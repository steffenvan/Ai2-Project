<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000039">
<title confidence="0.988905">
Labeled Morphological Segmentation with Semi-Markov Models
</title>
<author confidence="0.999788">
Ryan Cotterell1,2
</author>
<affiliation confidence="0.9917695">
Department of Computer Science1
Johns Hopkins University, USA
</affiliation>
<email confidence="0.992384">
ryan.cotterell@jhu.edu
</email>
<author confidence="0.994186">
Hinrich Schütze2
</author>
<affiliation confidence="0.9968975">
Center for Information and Language Processing2
University of Munich, Germany
</affiliation>
<email confidence="0.973293">
muellets@cis.lmu.de
</email>
<author confidence="0.620518">
Thomas Müller2 Alexander Fraser2
</author>
<sectionHeader confidence="0.959434" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957533333333">
We present labeled morphological
segmentation—an alternative view of
morphological processing that unifies sev-
eral tasks. We introduce a new hierarchy
of morphotactic tagsets and CHIPMUNK,
a discriminative morphological segmen-
tation system that, contrary to previous
work, explicitly models morphotactics.
We show improved performance on three
tasks for all six languages: (i) morpho-
logical segmentation, (ii) stemming and
(iii) morphological tag classification. For
morphological segmentation our method
shows absolute improvements of 2-6
points F1 over a strong baseline.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="categories and subject descriptors">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999339118644068">
Morphological processing is often an overlooked
problem since many well-studied languages (e.g.,
Chinese and English) are morphologically impov-
erished. But for languages with complex mor-
phology (e.g., Finnish and Turkish) morphological
processing is essential. A specific form of mor-
phological processing, morphological segmenta-
tion, has shown its utility for machine translation
(Dyer et al., 2008), sentiment analysis (Abdul-
Mageed et al., 2014), bilingual word alignment
(Eyigöz et al., 2013), speech processing (Creutz et
al., 2007b) and keyword spotting (Narasimhan et
al., 2014), inter alia. We advance the state-of-the-
art in supervised morphological segmentation by
describing a high-performance, data-driven tool
for handling complex morphology, even in low-
resource settings.
In this work, we make the distinction between
unlabeled morphological segmentation (UMS )
(often just called “morphological segmentation”)
and labeled morphological segmentation (LMS).
The labels in our supervised discriminative model
for LMS capture the distinctions between different
types of morphemes and directly model the mor-
photactics. We further create a hierarchical uni-
versal tagset for labeling morphemes, with differ-
ent levels appropriate for different tasks. Our hi-
erarchical tagset was designed by creating a stan-
dard representation from heterogeneous resources
for six languages. This allows us to use a single
unified framework to obtain strong performance
on three common morphological tasks that have
typically been viewed as separate problems and
addressed using different methods. We give an
overview of the tasks addressed in this paper in
Figure 1. The figure shows the expected output
for the Turkish word genCle¸smelerin ‘of rejuvenat-
ings’. In particular, it shows the full labeled mor-
phological segmentation, from which three repre-
sentations can be directly derived: the unlabeled
morphological segmentation, the stem/root 1 and
the structured morphological tag containing POS
and inflectional features.
We model these tasks with CHIPMUNK, a semi-
Markov conditional random field (semi-CRF)
(Sarawagi and Cohen, 2004), a model that is well-
suited for morphology. We provide a robust eval-
uation and analysis on six languages and CHIP-
MUNK yields strong results on all three tasks, in-
cluding state-of-the-art accuracy on morphologi-
cal segmentation.
Section 2 presents our LMS framework and the
morphotactic tagsets we use, i.e., the labels of the
sequence prediction task CHIPMUNK solves. Sec-
tion 3 introduces our semi-CRF model. Section 4
presents our novel features. Section 5 compares
CHIPMUNK to previous work. Section 6 presents
experiments on the three complementary tasks of
segmentation (UMS), stemming, and morpholog-
</bodyText>
<footnote confidence="0.9952208">
1Terminological notes: We use root to refer to a mor-
pheme with concrete meaning, stem to refer to the concate-
nation of all roots and derivational affixes, root detection to
refer to stripping both derivational and inflectional affixes,
and stemming to refer to stripping only inflectional affixes.
</footnote>
<page confidence="0.920434">
164
</page>
<note confidence="0.996811375">
Proceedings of the 19th Conference on Computational Language Learning, pages 164–174,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
gençle¸smelerin
UMS genç le¸s me ler in
Gloss young -ate -ion -s GENITIVE MARKER
LMS genç le¸s me ler in
ROOT:ADJECTIVAL SUFFIX:DERIV:VERB SUFFIX:DERIV:NOUN SUFFIX:INFL:NOUN:PLURAL SUFFIX:INFL:NOUN:GENITIVE
Root genç Stem gençle¸sme Morphological Tag PLURAL:GENITIVE
</note>
<figureCaption confidence="0.999695">
Figure 1: Examples of the tasks addressed for the Turkish word gençle¸smelerin ‘of rejuvenatings’: Traditional unlabeled
segmentation (UMS), Labeled morphological segmentation (LMS), stemming / root detection and (inflectional) morphological
tag classification. The morphotactic annotations produced by LMS allow us to solve these tasks using a single model.
</figureCaption>
<bodyText confidence="0.998526">
ical tag classification. Section 7 briefly discusses
finite-state morphology. Section 8 concludes.
The datasets created in this work, additional
description of our novel tagsets and CHIPMUNK
can be found at http://cistern.cis.lmu.de/
chipmunk.
</bodyText>
<sectionHeader confidence="0.902246" genericHeader="general terms">
2 Labeled Segmentation and Tagset
</sectionHeader>
<bodyText confidence="0.999940385714286">
We define the framework of labeled morphologi-
cal segmentation (LMS), an enhancement of mor-
phological segmentation that—in addition to iden-
tifying the boundaries of segments—assigns a
fine-grained morphotactic tag to each segment.
LMS leads to both better modeling of segmenta-
tion and subsumes several other tasks, e.g., stem-
ming.
Most previous approaches to morphological
segmentation are either unlabeled or use a small,
coarse-grained set such as prefix, root, suffix. In
contrast, our labels are fine-grained. This finer
granularity has two advantages. (i) The labels are
needed for many tasks, for instance in sentiment
analysis detecting morphologically encoded nega-
tion, as in Turkish, is crucial. In other words,
for many applications UMS is insufficient. (ii)
The LMS framework allows us to learn a prob-
abilistic model of morphotactics. Working with
LMS results in higher UMS accuracy. So even in
applications that only need segments and no la-
bels, LMS is beneficial. Note that the concate-
nation of labels across segments yields a bundle
of morphological attributes similar to those found
in the CoNLL datasets often used to train mor-
phological taggers (Buchholz and Marsi, 2006)—
thus LMS helps to unify UMS and morphological
tagging. We believe that LMS is a needed exten-
sion of current work in morphological segmenta-
tion. Our framework concisely allows the model
to capture interdependencies among various mor-
phemes and model relations between entire mor-
pheme classes—a neglected aspect of the problem.
We first create a hierarchical tagset with in-
creasing granularity, which we created by analyz-
ing the heterogeneous resources for the six lan-
guages we work on. The optimal level of gran-
ularity is task and language dependent: the level
is a trade-off between simplicity and expressivity.
We illustrate our tagset with the decomposition of
the German word Enteisungen ‘defrostings’ (Fig-
ure 2).
The level 0 tagset involves a single tag indi-
cating a segment. It ignores morphotactics com-
pletely and is similar to previous work. The level
1 tagset crudely approximates morphotactics: it
consists of the tags {PREFIX, ROOT, SUFFIX}.
This scheme has been successfully used by un-
supervised segmenters, e.g., MORFESSOR CAT-
MAP (Creutz et al., 2007a). It allows the model
to learn simple morphotactics, for instance that a
prefix cannot be followed by a suffix. This makes
a decomposition like reed → re+ed unlikely. We
also add an additional UNKNOWN tag for mor-
phemes that do not fit into this scheme. The level
2 tagset splits affixes into DERIVATIONAL and IN-
FLECTIONAL, effectively increasing the maximal
tagset size from 4 to 6. These tags can encode
that many languages allow for transitions from
derivational to inflectional endings, but rarely the
opposite. This makes the incorrect decomposi-
tion of German Offenheit ‘openness’ into Off, in-
flectional en and derivational heit unlikely2. This
tagset is also useful for building statistical stem-
mers. The level 3 tagset adds POS, i.e., whether a
root is VERBAL, NOMINAL or ADJECTIVAL, and
the POS of the word that an affix derives. The
level 4 tagset includes the inflectional feature a
suffix adds, e.g., CASE or NUMBER. This is help-
ful for certain agglutinative languages, in which,
</bodyText>
<footnote confidence="0.996403">
2Like en in English open, en in German Offen is part of
the root.
</footnote>
<page confidence="0.994486">
165
</page>
<table confidence="0.981585">
5 PREFIX:DERIV:VERB ROOT:NOUN SUFFIX:DERIV:NOUN SUFFIX:INFL:NOUN:PLURAL
4 PREFIX:DERIV:VERB ROOT:NOUN SUFFIX:DERIV:NOUN SUFFIX:INFL:NOUN:NUMBER
3 PREFIX:DERIV:VERB ROOT:NOUN SUFFIX:DERIV:NOUN SUFFIX:INFL:NOUN
2 PREFIX:DERIV ROOT SUFFIX:DERIV SUFFIX:INFL
1 PREFIX ROOT SUFFIX SUFFIX
0 SEGMENT SEGMENT SEGMENT SEGMENT
German Ent eis ung en
English de frost ing s
</table>
<figureCaption confidence="0.989095">
Figure 2: Example of the different morphotactic tagset granularities for German Enteisungen ‘defrostings’.
</figureCaption>
<bodyText confidence="0.900092">
level: 0 1 2 3 4
</bodyText>
<figure confidence="0.803378">
English 1 4 5 13 16
Finnish 1 4 6 14 17
German 1 4 6 13 17
Indonesian 1 4 4 8 8
Turkish 1 3 4 10 20
Zulu 1 4 6 14 17
</figure>
<tableCaption confidence="0.996608">
Table 1: Morphotactic tagset size at each level of granularity.
</tableCaption>
<bodyText confidence="0.999476625">
e.g., CASE must follow NUMBER. The level 5
tagset adds the actual value of the inflectional fea-
ture, e.g., PLURAL, and corresponds to the anno-
tation in the datasets. In preliminary experiments
we found that the level 5 tagset is too rich and does
not yield consistent improvements, we thus do not
explore it. Table 1 shows tagset sizes for the six
languages.3
</bodyText>
<sectionHeader confidence="0.993046" genericHeader="keywords">
3 Model
</sectionHeader>
<bodyText confidence="0.99742265">
CHIPMUNK is a supervised model implemented
using the well-understood semi-Markov condi-
tional random field (semi-CRF) (Sarawagi and
Cohen, 2004) that naturally fits the task of
LMS. Semi-CRFs generalize linear-chain CRFs
and model segmentation jointly with sequence la-
beling. Just as linear-chain CRFs are discrimina-
tive adaptations of hidden Markov models (Laf-
ferty et al., 2001), semi-CRFs are an analogous
adaptation of hidden semi-Markov models (Mur-
phy, 2002). Semi-CRFs allow us to elegantly inte-
grate new features that look at complete segments,
this is not possible with CRFs, making semi-CRFs
a natural choice for morphology.
A semi-CRF represents w (a word) as a se-
quence of segments s = (s1, ... , sn), each of
which is assigned a label `i. The concatenation
of all segments equals w. We seek a log-linear
distribution pθ(s, f  |w) overall possible segmen-
tations and label sequences for w, where θ is the
</bodyText>
<footnote confidence="0.863654">
3As converting segmentation datasets to tagsets is not al-
ways straightforward, we include tags that lack some fea-
tures, e.g., some level 4 German tags lack POS because our
German data does not specify it.
</footnote>
<bodyText confidence="0.9968451">
parameter vector. Note that we recover the stan-
dard CRF if we restrict the segment length to 1.
Formally, we define pθ as
where f is the feature function and
is the
partition function. To keep the notation unclut-
tered, we will write f without all its arguments in
the future. We use a generalization of the forward-
backward algorithm for efficient gradient compu-
tation (Sarawagi an
</bodyText>
<equation confidence="0.892657">
Zθ(w)
d Cohen, 2004). Inspection of
the semi-Markov forward recursion,
� f
α(t, l) = i,
i eθT
·α(t−
`&apos;), (2)
�
`&apos;
</equation>
<bodyText confidence="0.999528833333333">
shows that algorithm runs in
time where
n is the length of the word w and L is the number
of labels (size of the tagset).
We employ the maximum-likelihood criterion
to estimate the parameters with L-BFGS (Liu and
Nocedal, 1989), agradient-based optimization al-
gorithm. As in all exponential family models, the
gradient of the log-likelihood takes the form of the
difference between the observed and expected fea-
tures counts (Wainwright and Jordan, 2008) and
can be computed efficiently with the semi-Markov
extension of the forward-backward algorithm. We
use L2 regularization with a regularization coeffi-
cient tuned during cross-validation.
We note that semi-Markov models have the po-
tential to obviate typical errors made by standard
Markovian sequence models with an IOB label-
ing scheme over characters. For instance, con-
sider the incorrect segmentation of the English
verb sees into se+es. These are reasonable split
positions as many English stems end in se (e.g.,
consider abuse-s). Semi-CRFs have a major ad-
vantage here as they can
</bodyText>
<equation confidence="0.650534">
O(n2·L2)
</equation>
<bodyText confidence="0.7788695">
have segmental features
that allow them to learn se is not a good morph.
</bodyText>
<equation confidence="0.976455">
def
pθ(s, Z  |w) = 1
eθT f(si,`i,`i−1,i), (1)
Zθ(w) i
</equation>
<page confidence="0.992622">
166
</page>
<table confidence="0.791870571428571">
# Affixes Random Examples
English 394 -ard -taxy -odon -en -otic -fold
Finnish 120 -tä -llä -ja -t -nen -hön -jä -ton
German 112 -nomie -lichenes -ell -en -yl -iv
Indonesian 5 -kau -an -nya -ku -mu
Turkish 263 -ten -suz -mek -den -t -ünüz
Zulu 72 i- u- za- tsh- mi- obu- olu-
</table>
<tableCaption confidence="0.927563">
Table 2: Sizes of the various affix gazetteers.
Table 3: Number of words covered by the respective ASPELL
dictionary
</tableCaption>
<figure confidence="0.996064666666667">
English 119,839
Finnish 6,690,417
German 364,564
Indonesian 35,269
Turkish 80,261
Zulu 73,525
</figure>
<sectionHeader confidence="0.971748" genericHeader="introduction">
4 Features
</sectionHeader>
<bodyText confidence="0.992707578947369">
We introduce several novel features for LMS. We
exploit existing resources, e.g., spell checkers and
Wiktionary, to create straightforward and effective
features and we incorporate ideas from related ar-
eas: named-entity recognition (NER) and morpho-
logical tagging.
Affix Features and Gazetteers. In contrast to
syntax and semantics, the morphology of a lan-
guage is often simple to document and a list of the
most common morphs can be found in any good
grammar book. Wiktionary, for example, con-
tains affix lists for all the six languages used in
our experiments.4 Providing a supervised learner
with such a list is a great boon, just as gazetteer
features aid NER (Smith and Osborne, 2006)—
perhaps even more so since suffixes and prefixes
are generally closed-class; hence these lists are
likely to be comprehensive. These features are
binary and fire if a given substring occurs in the
gazetteer list. In this paper, we simply use suffix
lists from English Wiktionary, except for Zulu, for
which we use a prefix list, see Table 2.
We also include a feature that fires on the con-
junction of tags and substrings observed in the
training data. In the level 5 tagset this allows us
to link all allomorphs of a given morpheme. In the
lower level tagsets, this links related morphemes.
Virpioja et al. (2010) explored this idea for un-
supervised segmentation. Linking allomorphs to-
gether under a single tag helps combat sparsity in
modeling the morphotactics.
Stem Features. A major problem in statistical
segmentation is the reluctance to posit morphs not
observed in training; this particularly affects roots,
which are open-class. This makes it nearly im-
possible to correctly segment compounds that con-
tain unseen roots, e.g., to correctly segment home-
work you need to know that home and work are
independent English words. We solve this prob-
lem by incorporating spell-check features: binary
4A good example of such a resource is en.wiktio-
nary.org/wiki/Category:Turkish_suffixes.
features that fire if a segment is valid for a given
spell checker. Spell-check features function effec-
tively as a proxy for a “root detector”. We use
the open-source ASPELL dictionaries as they are
freely available in 91 languages. Table 3 shows
the coverage of these dictionaries.
Integrating the Features. Our model uses the
features discussed in this section and addition-
ally the simple n-gram context features of Ruoko-
lainen et al. (2013). The n-gram features look at
variable length substrings of the word on both the
right and left side of each potential boundary. We
create conjunctive features from the cross-product
between the morphotactic tagset (Section 2) and
the features.
</bodyText>
<sectionHeader confidence="0.999868" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99980948">
Van den Bosch and Daelemans (1999) and Marsi
et al. (2005) present memory-based approaches to
discriminative learning of morphological segmen-
tation. This is the previous work most similar to
our work. They address the problem of LMS. We
distinguish our work from theirs in that we define
a cross-lingual schema for defining a hierarchical
tagset for LMS. Morever, we tackle the problem
with a feature-rich log-linear model, allowing us
to easily incorporate disparate sources of knowl-
edge into a single framework, as we show in our
extensive evaluation.
UMS has been mainly addressed by unsu-
pervised algorithms. LINGUISTICA (Goldsmith,
2001) and MORFESSOR (Creutz and Lagus, 2002)
are built around an idea of optimally encoding the
data, in the sense of minimal description length
(MDL). MORFESSOR CAT-MAP (Creutz et al.,
2007a) formulates the model as sequence predic-
tion based on HMMs over a morph dictionary
and MAP estimation. The model also attempts
to induce basic morphotactic categories (PREFIX,
ROOT, SUFFIX). Kohonen et al. (2010a,b) and
Grönroos et al. (2014) present variations of MOR-
FESSOR for semi-supervised learning. Poon et
</bodyText>
<page confidence="0.988036">
167
</page>
<bodyText confidence="0.99992656097561">
al. (2009) introduces a Bayesian state-space model
with corpus-wide priors. The model resembles a
semi-CRF, but dynamic programming is no longer
possible due to the priors. They employ the three-
state tagset of Creutz and Lagus (2004) (row 1
in Figure 2) for Arabic and Hebrew UMS. Their
gradient and objective computation is based on an
enumeration of a heuristically chosen subset of the
exponentially many segmentations. This limits its
applicability to language with complex concatena-
tive morphology, e.g., Turkish and Finnish.
Ruokolainen et al. (2013) present an averaged
perceptron (Collins, 2002), a discriminative struc-
tured prediction method, for UMS. The model out-
performs the semi-supervised model of Poon et al.
(2009) on Arabic and Hebrew morpheme segmen-
tation as well as the semi-supervised model of Ko-
honen et al. (2010a) on English, Finnish and Turk-
ish.
Finally, Ruokolainen et al. (2014) get further
consistent improvements by using features ex-
tracted from large corpora, based on the letter suc-
cessor variety (LSV) model (Harris, 1995) and on
unsupervised segmentation models such as Mor-
fessor CatMAP (Creutz et al., 2007a). The idea
behind LSV is that for example talking should be
split into talk and ing, because talk can also be fol-
lowed by different letters then i such as e (talked)
and s (talks).
Chinese word segmentation (CWS) is related
to UMS. Andrew (2006) successfully apply semi-
CRFs to CWS. The problem of joint CWS and
POS tagging (Ng and Low, 2004; Zhang and
Clark, 2008) is related to LMS. To our knowl-
edge, joint CWS and POS tagging has not been
addressed by a simple single semi-CRF, possi-
bly because POS tagsets typically used in Chinese
treebanks are much bigger than our morphotactic
tagsets and the morphological poverty of Chinese
makes higher-order models necessary and the di-
rect application of semi-CRFs infeasible.
</bodyText>
<sectionHeader confidence="0.99978" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999824333333333">
We experimented on six languages from diverse
language families. The segmentation data for En-
glish, Finnish and Turkish was taken from Mor-
phoChallenge 2010 (Kurimo et al., 2010).5 De-
spite typically being used for UMS tasks, the Mor-
phoChallenge datasets do contain morpheme level
</bodyText>
<footnote confidence="0.995854">
5http://research.ics.aalto.fi/events/
morphochallenge2010/
</footnote>
<table confidence="0.99954875">
Un. Data Train+Tune+Dev Test
Train Tune Dev
English 878k 800 100 100 694
Finnish 2,928k 800 100 100 835
German 2,338k 800 100 100 751
Indonesian 88k 800 100 100 2500
Turkish 617k 800 100 100 763
Zulu 123k 800 100 100 9040
</table>
<tableCaption confidence="0.999566">
Table 4: Dataset sizes (number of types).
</tableCaption>
<bodyText confidence="0.999722694444444">
labels. The German data was extracted from the
CELEX2 collection (Baayen et al., 1993). The
Zulu data was taken from the Ukwabelana cor-
pus (Spiegler et al., 2010). Finally, the Indone-
sian portion was created applying the rule-based
analyzer MORPHIND (Larasati et al., 2011) to the
Indonesian portion of an Indonesian-English bilin-
gual corpus.6
We did not have access to the MorphoChallenge
test set and thus used the original development set
as our final evaluation set (Test). We developed
CHIPMUNK using 10-fold cross-validation on the
1000 word training set and split every fold into
training (Train), tuning (Tune) and development
sets (Dev).7 For German, Indonesian and Zulu we
randomly selected 1000 word forms as training set
and used the rest as evaluation set. For our final
evaluation we trained CHIPMUNK on the concate-
nation of Train, Tune and Dev (the original 1000
word training set), using the optimal parameters
from the cross-evaluation and tested on Test.
One of our baselines also uses unlabeled train-
ing data. MorphoChallenge provides word lists for
English, Finnish, German and Turkish. We use the
unannotated part of Ukwabelana for Zulu; and for
Indonesian, data from Wikipedia and the corpus of
Krisnawati and Schulz (2013).
Table 4 shows the important statistics of our
datasets.
In all evaluations, we use variants of the stan-
dard MorphoChallenge evaluation approach. Im-
portantly, for word types with multiple correct
segmentations, this involves finding the maximum
score by comparing our hypothesized segmenta-
tion with each correct segmentation, as is stan-
dardly done in MorphoChallenge.
</bodyText>
<footnote confidence="0.9958074">
6https://github.com/desmond86/
Indonesian-English-Bilingual-Corpus
7We used both Tune and Dev in order to both optimize
hyperparameters on held-out data (Tune) and perform quali-
tative error analysis on separate held-out data (Dev).
</footnote>
<page confidence="0.967679">
168
</page>
<table confidence="0.9995271">
English Finnish Indonesian German Turkish Zulu
CRF-MORPH 83.23 81.98 93.09 84.94 88.32 88.48
CRF-MORPH +LSV 84.45 84.35 93.50 86.90 89.98 89.06
First-order CRF 84.66 85.05 93.31 85.47 90.03 88.99
Higher-order CRF 84.66 84.78 93.88 85.40 90.65 88.85
CHIPMUNK 84.40 84.40 93.76 85.53 89.72 87.80
CHIPMUNK +Morph 83.27 84.71 93.17 84.84 90.48 90.03
CHIPMUNK +Affix 83.81 86.02 93.51 85.81 89.72 89.64
CHIPMUNK +Dict 86.10 86.11 95.39 87.76 90.45 88.66
CHIPMUNK +Dict,+Affix,+Morph 86.31 88.38 95.41 87.85 91.36 90.16
</table>
<tableCaption confidence="0.998335">
Table 5: Test Fl for UMS. Features: LSV = letter successor variety, Affix = affix, Dict = dictionary, Morph = optimal (on Tune)
morphotactic tagset.
</tableCaption>
<subsectionHeader confidence="0.983343">
6.1 UMS Experiments
</subsectionHeader>
<bodyText confidence="0.999976242424242">
We first evaluate CHIPMUNK on UMS, by pre-
dicting LMS and then discarding the labels. Our
primary baseline is the state-of-the-art super-
vised system CRF-MORPH of Ruokolainen et al.
(2013). We ran the version of the system that the
authors published on their website.8 We optimized
the model’s two hyperparameters on Tune: the
number of epochs and the maximal length of n-
gram character features. The system also supports
Harris’s letter successor variety (LSV) features
(Section 5), extracted from large unannotated cor-
pora, our second baseline. For completeness, we
also compare CHIPMUNK with a first-order CRF
and a higher-order CRF (Müller et al., 2013), both
used the same n-gram features as CRF-MORPH,
but without the LSV features.9 We evaluate all
models using the traditional macro F1 of the seg-
mentation boundaries.
Discussion. The UMS results on held-out data
are displayed in Table 5. Our most complex model
beats the best baseline by between 1 (German) and
3 (Finnish) points F1 on all six languages. We
additionally provide extensive ablation studies to
highlight the contribution of our novel features.
We find that the properties of each specific lan-
guage highly influences which features are most
effective. For the agglutinative languages, i.e,
Finnish, Turkish and Zulu, the affix based features
(+Affix) and the morphotactic tagset (+Morph)
yield consistent improvements over the semi-CRF
models with a single state. Improvements for the
affix features range from 0.2 for Turkish to 2.14
for Zulu. The morphological tagset yields im-
</bodyText>
<footnote confidence="0.95549925">
8http://users.ics.tkk.fi/tpruokol/
software/crfs_morph.zip
9Model order, maximal character n-gram length and reg-
ularization coefficients were optimized on Tune.
</footnote>
<bodyText confidence="0.999985945945946">
provements of 0.77 for Finnish, 1.89 for Turkish
and 2.10 for Zulu. We optimized tagset granularity
on Tune and found that levels 4 and level 2 yielded
the best results for the three agglutinative and the
three other languages, respectively.
The dictionary features (+Dict) help universally,
but their effects are particularly salient in lan-
guages with productive compounding, i.e., En-
glish, Finnish and German, where we see improve-
ments of &gt; 1.7.
In comparison with previous work (Ruoko-
lainen et al., 2013) we find that our most complex
model yields consistent improvements over CRF-
MORPH +LSV for all languages: The improve-
ments range from &gt; 1 for German over &gt; 1.5 for
Zulu, English, and Indonesian to &gt; 2 for Turkish
and &gt; 4 for Finnish.
To illustrate the effect of modeling morphotac-
tics through the larger morphotactic tagset on per-
formance, we provide a detailed analysis of Turk-
ish. See Table 6. We consider three different fea-
ture sets and increase the size of the morphotactic
tagsets depicted in Figure 2. The results evince the
general trend that improved morphotactic model-
ing benefits segmentation. Additionally, we ob-
serve that the improvements are complementary to
those from the other features.
As discussed earlier, a key problem in UMS, es-
pecially in low-resource settings, is the detection
of novel roots and affixes. Since many of our fea-
tures were designed to combat this problem specif-
ically, we investigated this aspect independently.
Table 7 shows the number of novel roots and af-
fixes found by our best model and the baseline. In
all languages, CHIPMUNK correctly identifies be-
tween 5% (English) and 22% (Finnish) more novel
roots than the baseline. We do not see major im-
</bodyText>
<page confidence="0.997673">
169
</page>
<bodyText confidence="0.992932">
+Affix +Dict,+Affix
Level 0 90.11 90.13 91.66
Level 1 90.73 90.68 92.80
Level 2 89.80 90.46 92.04
Level 3 91.03 90.83 92.31
Level 4 91.80 92.19 93.21
</bodyText>
<tableCaption confidence="0.489000833333333">
Table 6: Example of the effect of larger tagsets (Figure 2)
on Turkish segmentation measured on our development set.
As Turkish is an agglutinative language with hundreds of af-
fixes, the efficacy of our approach is expected to be partic-
ularly salient here. Recall we optimized for the best tagset
granularity for our experiments on Tune.
</tableCaption>
<figure confidence="0.516242">
Px-Px Px-Rt Rt-Rt Rt-Sx Sx-Sx Total
</figure>
<figureCaption confidence="0.981045375">
Figure 3: This figure represents a comparative analysis of un-
dersegmentation. Each column (labels at the bottom) shows
how often CRF-MORPH +LSV (top number in heatmap) and
CHIPMUNK (bottom number in heatmap) select a segment
that is two separate segments in the gold standard. E.g., Rt-
Sx indicates how a root and a suffix were treated as a sin-
gle segment. The color depends on the difference of the two
counts.
</figureCaption>
<bodyText confidence="0.999986916666667">
provements for affixes, but this is of less interest
as there are far fewer novel affixes.
We further explore how CHIPMUNK and the
baseline perform on different boundary types by
looking at missing boundaries between different
morphotactic types; this error type is also known
as undersegmentation. Figure 3 shows a heatmap
that overviews errors broken down by morphotac-
tic tag. We see that most errors are caused between
root and suffixes across all languages. This is re-
lated to the problem of finding new roots, as a new
root is often mistaken as a root-affix composition.
</bodyText>
<subsectionHeader confidence="0.999793">
6.2 Root Detection and Stemming
</subsectionHeader>
<bodyText confidence="0.998444833333333">
Root detection) and stemming) are two important
NLP problems that are closely related to morpho-
logical segmentation and used in applications such
as MT, information retrieval, parsing and infor-
mation extraction. Here we explore the utility of
CHIPMUNK as a statistical stemmer and root de-
</bodyText>
<table confidence="0.993738875">
CRF-MORPH CHIPMUNK
Roots Affixes Roots Affixes
English 614 6 644 12
Finnish 502 10 613 11
German 360 6 414 9
Indonesian 593 0 639 0
Turkish 435 22 514 19
Zulu 146 10 160 11
</table>
<tableCaption confidence="0.974901666666667">
Table 7: Dev number of unseen root and affix types cor-
rectly identified by CRF-MORPH +LSV and CHIPMUNK
+Affix,+Dict,+Morph.
</tableCaption>
<bodyText confidence="0.997041976190477">
tector.
Stemming is closely related to the task of
lemmatization, which involves the additional step
of normalizing to the canonical form.10 Con-
sider the German particle verb participle auf-
ge-schrieb-en ‘written down’. The participle is
built by applying an alternation to the verbal root
schreib ‘write’ adding the participial circumfix ge-
en and finally adding the verb particle auf. In our
segmentation-based definition, we would consider
schrieb ‘write’ as its root and auf-schrieb as its
stem. In order to additionally to restore the lemma,
we would also have to reverse the stem alternation
that replaced ei with ie and add the infinitival end-
ing en yielding the infinitive auf-schreib-en.
Our baseline MORFETTE (Chrupala et al., 2008)
is a statistical transducer that first extracts edit
paths between input and output and then uses a
perceptron classifier to decide which edit path to
apply. In short, MORFETTE treats the task as a
string-to-string transduction problem, whereas we
view it as a labeled segmentation problem.11 Note,
that MORFETTE would in principle be able to han-
dle stem alternations, although these usually lead
to an increase in the number of edit paths. We use
level 2 tagsets for all experiments—the smallest
tagsets complex enough for stemming—and ex-
tract the relevant segments.
Discussion. Our results are shown in Table 8.
We see consistent improvements across all tasks.
For the fusional languages (English, German and
Indonesian) we see modest gains in performance
on both root detection and stemming. However,
for the agglutinative languages (Finnish, Turkish
and Zulu) we see absolute gains as high as 50%
10Thus in our experiments there are no stem alternations.
The output is equivalent to that of the Porter stemmer (Porter,
1980).
11Note that MORFETTE is a pipeline that first tags and then
lemmatizes. We only make use of this second part of MOR-
FETTE for which it is a strong string-to-string transduction
baseline.
</bodyText>
<figure confidence="0.99804235">
0.0
0.0
0.0
0.0
2.0
0.0
250.0
214.0
374.0
285.0
308.0
267.0
49.0
44.0
43.0
27.0
6.0
9.0
49.0
58.0
56.0
46.0
87.0
73.0
217.0
168.0
146.0
108.0
154.0
143.0
48.0
23.0
43.0
36.0
3.0
3.0
0.0
0.0
0.0
0.0
28.0
29.0
40.0
16.0
0.0
0.0
30.0
34.0
0.0
0.0
3.0
3.0
1.0
2.0
37.0
19.0
233.0
158.0
152.0
103.0
108.0
40.0
0.0
0.0
17.0
17.0
82.0
41.0
401.0
250.0
318.0
264.0
Zul Tur Ind Ger Fin Eng
75
50
25
0
25
50
75
</figure>
<page confidence="0.970362">
170
</page>
<table confidence="0.988124125">
English Finnish German Indonesian Turkish Zulu
MORFETTE 62.82 39.28 43.81 86.00 26.08 30.76
CHIPMUNK 70.31 69.85 67.37 90.00 75.62 62.23
MORFETTE 91.35 51.74 79.49 86.00 28.57 58.12
CHIPMUNK 94.24 79.23 85.75 89.36 85.06 67.64
Root
Detection
Stemming
</table>
<tableCaption confidence="0.944318">
Table 8: Test Accuracies for root detection and stemming.
</tableCaption>
<table confidence="0.997240818181818">
Finnish Turkish
F1 MaxEnt 75.61 69.92
MaxEnt +Split 74.02 76.61
CHIPMUNK +All 80.34 85.07
Acc. MaxEnt 60.96 37.88
MaxEnt +Split 59.04 44.30
CHIPMUNK +All 65.00 56.06
Morpheme Tags
Full Word Tags
Finnish
Turkish
</table>
<tableCaption confidence="0.813932">
Table 10: Number of full word and morpheme tags in the
datasets.
</tableCaption>
<page confidence="0.79652">
43
50
172
636
</page>
<tableCaption confidence="0.9899675">
Table 9: Test F-Scores / accuracies for morphological tag
classification.
</tableCaption>
<bodyText confidence="0.999715375">
(Turkish) in accuracy. This significant improve-
ment is due to the complexity of the tasks in
these languages—their productive morphology in-
creases sparsity and makes the unstructured string-
to-string transduction approach suboptimal. We
view this as solid evidence that labeled segmen-
tation has utility in many components of the NLP
pipeline.
</bodyText>
<subsectionHeader confidence="0.999638">
6.3 Morphological Tag Classification
</subsectionHeader>
<bodyText confidence="0.967809952380952">
The joint modeling of segmentation and morpho-
tactic tags allows us to use CHIPMUNK for a crude
form of morphological analysis: the task of mor-
phological tag classification, which we define as
annotation of a word with its most likely inflec-
tional features.12 To be concrete, our task is to
predict the inflectional features of word type based
only on its character sequence and not its sen-
tential context. To this end, we take Finnish and
Turkish as two examples of languages that should
suit our approach particularly well as both have
highly complex inflectional morphologies. We use
our most fine-grained tagset and replace all non-
inflectional tags with a simple segment tag. The
tagset sizes are listed in Table 10.
We use the same experimental setup as in Sec-
tion 6.2 and compare CHIPMUNK to a maximum
entropy classifier (MaxEnt), whose features are
character n-grams of up to a maximal length of
12We recognize that this task is best performed with sen-
tential context (token-based). Integration with a POS tagger,
however, is beyond the scope of this paper.
k. 13 The maximum entropy classifier is L1-
regularized and its regularization coefficient as
well as the value for k are optimized on Tune.
As a second, stronger baseline we use a MaxEnt
classifier that splits tags into their constituents and
concatenates the features with every constituent as
well as the complete tag (MaxEnt +Split). Both
of the baselines in Table 9 are 0th-order versions
of the state-of-the-art CRF-based morphological
tagger MARMOT (Müller et al., 2013) (since our
model is type-based), making this a strong base-
line. We report full analysis accuracy and macro
F1 on the set of individual inflectional features.
Discussion. The results in Table 9 show that our
proposed method outperforms both baselines on
both performance metrics. We see gains of over
6% in accuracy in both languages. This is evi-
dence that our proposed approach could be suc-
cessfully integrated into a morphological tagger to
give a stronger character-based signal.
</bodyText>
<sectionHeader confidence="0.989917" genericHeader="method">
7 Comparison to Finite-State
Morphology
</sectionHeader>
<bodyText confidence="0.947447153846154">
A morphological finite-state analyzer is customar-
ily a hand-crafted tool that generates all the pos-
sible morphological readings with their associated
features. We believe that, for many applications,
high quality finite-state morphological analysis is
superior to our techniques. Finite-state morpho-
logical analyzers output a small set of linguis-
tically valid analyses of a type, typically with
only limited overgeneration. However, there are
two significant problems. The first is that signif-
icant effort is required to develop the transducers
modeling the “grammar” of the morphology and
13Prefixes and suffixes are explicitly marked.
</bodyText>
<page confidence="0.995976">
171
</page>
<bodyText confidence="0.999973304347826">
there is significant effort in creating and updating
the lexicon. The second is, it is difficult to use
finite-state morphology to guess analyses involv-
ing roots not covered in the lexicon.14 In fact,
this is usually solved by viewing it as a different
problem, morphological guessing, where linguis-
tic knowledge similar to the features we have pre-
sented is used to try to guess POS and morpholog-
ical analysis for types with no analysis.
In contrast, our training procedure learns a
probabilistic transducer, which is a soft version of
the type of hand-engineered grammar that is used
in finite-state analyzers. The 1-best labeled mor-
phological segmentation our model produces of-
fers a simple and clean representation which will
be of great use in many downstream applications.
Furthermore our model unifies analysis and guess-
ing into a single simple framework. Nevertheless,
finite-state morphologies are still extremely use-
ful, high-precision tools. A primary goal of fu-
ture work will be to use CHIPMUNK to attempt
to induce higher-quality morphological processing
systems.
</bodyText>
<sectionHeader confidence="0.976553" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999689875">
We have presented labeled morphological seg-
mentation (LMS) in this paper, a new ap-
proach to morphological processing. LMS uni-
fies three tasks that were solved before by differ-
ent methods—unlabeled morphological segmenta-
tion, stemming, and morphological tag classifica-
tion. LMS annotation itself has great potential for
use in downstream NLP applications. Our hierar-
chy of labeled morphological segmentation tagsets
can be used to map the heterogeneous data in six
languages we work with to universal representa-
tions of different granularities. We plan future cre-
ation of gold standard segmentations in more lan-
guages using our annotation scheme.
We further presented CHIPMUNK a semi-CRF-
based model for LMS that allows for the integra-
tion of various linguistic features and consistently
out-performs previously presented approaches to
unlabeled morphological segmentation. An im-
portant extension of CHIPMUNK is embedding it
in a context-sensitive POS tagger. Current state-
of-the-art models only employ character level n-
gram features to model word-internals (Müller et
al., 2013). We have demonstrated that our struc-
</bodyText>
<footnote confidence="0.8090805">
14While one can in theory put in wildcard root states, this
does not work in practice due to overgeneration.
</footnote>
<bodyText confidence="0.999849">
tured approach outperforms this baseline. We
leave this natural extension to future work.
The datasets used in this work, additional de-
scription of our novel tagsets and CHIPMUNK
can be found at http://cistern.cis.lmu.de/
chipmunk.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998277777777778">
We would like to thank Jason Eisner, Helmut
Schmid, Özlem Qetino˘glu and the anonymous re-
viewers for their comments. This material is
based upon work supported by a Fulbright fellow-
ship awarded to the first author by the German-
American Fulbright Commission and the National
Science Foundation under Grant No. 1423276.
The second author is a recipient of the Google
Europe Fellowship in Natural Language Process-
ing, and this research is supported by this Google
Fellowship. The fourth author was partially
supported by Deutsche Forschungsgemeinschaft
(grant SCHU 2246/10-1). This project has re-
ceived funding from the European Union’s Hori-
zon 2020 research and innovation programme un-
der grant agreement No 644402 (HimL) and the
DFG grant Models of Morphosyntax for Statisti-
cal Machine Translation.
</bodyText>
<sectionHeader confidence="0.998406" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999665791666667">
Muhammad Abdul-Mageed, Mona T. Diab, and Sandra
Kübler. 2014. SAMAR: Subjectivity and sentiment
analysis for Arabic social media. Computer Speech
&amp; Language.
Galen Andrew. 2006. A hybrid Markov/semi-Markov
conditional random field for sequence segmentation.
In Proceedings of EMNLP.
R Harald Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1993. The CELEX lexical database on CD-
ROM. Technical report, Linguistic Data Consor-
tium.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Grzegorz Chrupala, Georgiana Dinu, and Josef van
Genabith. 2008. Learning morphology with mor-
fette. In Proceedings of LREC.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Mathias Creutz and Krista Lagus. 2002. Unsupervised
discovery of morphemes. In Proceedings of SIG-
MORPHON.
</reference>
<page confidence="0.976111">
172
</page>
<reference confidence="0.999881637254902">
Mathias Creutz and Krista Lagus. 2004. Induction of a
simple morphology for highly-inflecting languages.
In Proceedings of SIGMORPHON.
Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo,
Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti
Varjokallio, Ebru Arisoy, Murat Saraclar, and An-
dreas Stolcke. 2007a. Analysis of morph-based
speech recognition and the modeling of out-of-
vocabulary words across languages. In Proceedings
of HLT-NAACL.
Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo,
Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti
Varjokallio, Ebru Arisoy, Murat Saraclar, and An-
dreas Stolcke. 2007b. Morph-based speech recog-
nition and modeling of out-of-vocabulary words
across languages. TSLP.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of ACL.
Elif Eyigöz, Daniel Gildea, and Kemal Oflazer. 2013.
Simultaneous word-morpheme alignment for statis-
tical machine translation. In Proceedings of HLT-
NAACL.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics.
Stig-Arne Grönroos, Sami Virpioja, Peter Smit, and
Mikko Kurimo. 2014. Morfessor FlatCat: An
HMM-based method for unsupervised and semi-
supervised learning of morphology. In Proceedings
of COLING.
Zellig Harris. 1995. From phoneme to morpheme.
Language.
Oskar Kohonen, Sami Virpioja, and Krista Lagus.
2010a. Semi-supervised learning of concatenative
morphology. In Proceedings of SIGMORPHON.
Oskar Kohonen, Sami Virpioja, Laura Leppänen, and
Krista Lagus. 2010b. Semi-supervised extensions to
Morfessor baseline. In Proceedings of the Morpho
Challenge Workshop.
Lucia D. Krisnawati and Klaus U. Schulz. 2013. Pla-
giarism detection for Indonesian texts. In Proceed-
ings of iiWAS.
Mikko Kurimo, Sami Virpioja, Ville Turunen, and
Krista Lagus. 2010. Morpho challenge competition
2005–2010: Evaluations and results. In Proceedings
of SIGMORPHON.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML.
Septina Dian Larasati, Vladislav Kuboˇn, and Daniel
Zeman. 2011. Indonesian morphology tool (mor-
phind): Towards an Indonesian corpus. In Sys-
tems and Frameworks for Computational Morphol-
ogy. Springer.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming.
Erwin Marsi, Antal van den Bosch, and Abdelhadi
Soudi. 2005. Memory-based morphological analy-
sis generation and part-of-speech tagging of Arabic.
In Proceedings of ACL Workshop: Computational
Approaches to Semitic Languages.
Kevin P Murphy. 2002. Hidden semi-Markov models
(hsmms). Technical report, Massachusetts Institute
of Technology.
Thomas Müller, Helmut Schmid, and Hinrich Schütze.
2013. Efficient higher-order CRFs for morphologi-
cal tagging. In Proceedings of EMNLP.
Karthik Narasimhan, Damianos Karakos, Richard
Schwartz, Stavros Tsakalidis, and Regina Barzilay.
2014. Morphological segmentation for keyword
spotting. In Proceedings of EMNLP.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proceedings of
EMNLP.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of NAACL.
Martin F Porter. 1980. An algorithm for suffix strip-
ping. Program.
Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja,
and Mikko Kurimo. 2013. Supervised morpholog-
ical segmentation in a low-resource learning setting
using conditional random fields. In Proceedings of
CoNLL.
Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja,
and Mikko Kurimo. 2014. Painless semi-supervised
morphological segmentation using conditional ran-
dom fields. Proceedings of EACL.
Sunita Sarawagi and William W Cohen. 2004. Semi-
Markov conditional random fields for information
extraction. In Proceedings of NIPS.
Andrew Smith and Miles Osborne. 2006. Using
gazetteers in discriminative information extraction.
In Proceedings of CoNLL.
Sebastian Spiegler, Andrew Van Der Spuy, and Peter A
Flach. 2010. Ukwabelana: An open-source mor-
phological Zulu corpus. In Proceedings of COL-
ING.
</reference>
<page confidence="0.9888">
173
</page>
<reference confidence="0.9996042">
Antal Van den Bosch and Walter Daelemans. 1999.
Memory-based morphological analysis. In Proceed-
ings of ACL. Association for Computational Lin-
guistics.
Sami Virpioja, Oskar Kohonen, and Krista Lagus.
2010. Unsupervised morpheme analysis with Al-
lomorfessor. In Multilingual Information Access
Evaluation. Springer.
Martin J Wainwright and Michael I Jordan. 2008.
Graphical models, exponential families, and varia-
tional inference. Foundations and Trends in Ma-
chine Learning.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and POS tagging using a single percep-
tron. In Proceedings of ACL.
</reference>
<page confidence="0.998423">
174
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.107084">
<title confidence="0.664281">Labeled Morphological Segmentation with Semi-Markov Models</title>
<affiliation confidence="0.521608">of Computer Johns Hopkins University, USA</affiliation>
<email confidence="0.99707">ryan.cotterell@jhu.edu</email>
<affiliation confidence="0.854869">for Information and Language University of Munich, Germany</affiliation>
<email confidence="0.99404">muellets@cis.lmu.de</email>
<author confidence="0.789378">Alexander</author>
<abstract confidence="0.998062">We present labeled morphological segmentation—an alternative view of morphological processing that unifies several tasks. We introduce a new hierarchy morphotactic tagsets and a discriminative morphological segmentation system that, contrary to previous work, explicitly models morphotactics. We show improved performance on three tasks for all six languages: (i) morphological segmentation, (ii) stemming and (iii) morphological tag classification. For morphological segmentation our method shows absolute improvements of 2-6 a strong baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Muhammad Abdul-Mageed</author>
<author>Mona T Diab</author>
<author>Sandra Kübler</author>
</authors>
<title>SAMAR: Subjectivity and sentiment analysis for Arabic social media.</title>
<date>2014</date>
<journal>Computer Speech &amp; Language.</journal>
<marker>Abdul-Mageed, Diab, Kübler, 2014</marker>
<rawString>Muhammad Abdul-Mageed, Mona T. Diab, and Sandra Kübler. 2014. SAMAR: Subjectivity and sentiment analysis for Arabic social media. Computer Speech &amp; Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
</authors>
<title>A hybrid Markov/semi-Markov conditional random field for sequence segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="17855" citStr="Andrew (2006)" startWordPosition="2785" endWordPosition="2786"> as well as the semi-supervised model of Kohonen et al. (2010a) on English, Finnish and Turkish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese word segmentation (CWS) is related to UMS. Andrew (2006) successfully apply semiCRFs to CWS. The problem of joint CWS and POS tagging (Ng and Low, 2004; Zhang and Clark, 2008) is related to LMS. To our knowledge, joint CWS and POS tagging has not been addressed by a simple single semi-CRF, possibly because POS tagsets typically used in Chinese treebanks are much bigger than our morphotactic tagsets and the morphological poverty of Chinese makes higher-order models necessary and the direct application of semi-CRFs infeasible. 6 Experiments We experimented on six languages from diverse language families. The segmentation data for English, Finnish and</context>
</contexts>
<marker>Andrew, 2006</marker>
<rawString>Galen Andrew. 2006. A hybrid Markov/semi-Markov conditional random field for sequence segmentation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Harald Baayen</author>
<author>Richard Piepenbrock</author>
<author>Leon Gulikers</author>
</authors>
<date>1993</date>
<booktitle>The CELEX lexical database on CDROM. Technical report, Linguistic Data Consortium.</booktitle>
<contexts>
<context position="19032" citStr="Baayen et al., 1993" startWordPosition="2974" endWordPosition="2977"> segmentation data for English, Finnish and Turkish was taken from MorphoChallenge 2010 (Kurimo et al., 2010).5 Despite typically being used for UMS tasks, the MorphoChallenge datasets do contain morpheme level 5http://research.ics.aalto.fi/events/ morphochallenge2010/ Un. Data Train+Tune+Dev Test Train Tune Dev English 878k 800 100 100 694 Finnish 2,928k 800 100 100 835 German 2,338k 800 100 100 751 Indonesian 88k 800 100 100 2500 Turkish 617k 800 100 100 763 Zulu 123k 800 100 100 9040 Table 4: Dataset sizes (number of types). labels. The German data was extracted from the CELEX2 collection (Baayen et al., 1993). The Zulu data was taken from the Ukwabelana corpus (Spiegler et al., 2010). Finally, the Indonesian portion was created applying the rule-based analyzer MORPHIND (Larasati et al., 2011) to the Indonesian portion of an Indonesian-English bilingual corpus.6 We did not have access to the MorphoChallenge test set and thus used the original development set as our final evaluation set (Test). We developed CHIPMUNK using 10-fold cross-validation on the 1000 word training set and split every fold into training (Train), tuning (Tune) and development sets (Dev).7 For German, Indonesian and Zulu we ran</context>
</contexts>
<marker>Baayen, Piepenbrock, Gulikers, 1993</marker>
<rawString>R Harald Baayen, Richard Piepenbrock, and Leon Gulikers. 1993. The CELEX lexical database on CDROM. Technical report, Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="6197" citStr="Buchholz and Marsi, 2006" startWordPosition="876" endWordPosition="879">e labels are needed for many tasks, for instance in sentiment analysis detecting morphologically encoded negation, as in Turkish, is crucial. In other words, for many applications UMS is insufficient. (ii) The LMS framework allows us to learn a probabilistic model of morphotactics. Working with LMS results in higher UMS accuracy. So even in applications that only need segments and no labels, LMS is beneficial. Note that the concatenation of labels across segments yields a bundle of morphological attributes similar to those found in the CoNLL datasets often used to train morphological taggers (Buchholz and Marsi, 2006)— thus LMS helps to unify UMS and morphological tagging. We believe that LMS is a needed extension of current work in morphological segmentation. Our framework concisely allows the model to capture interdependencies among various morphemes and model relations between entire morpheme classes—a neglected aspect of the problem. We first create a hierarchical tagset with increasing granularity, which we created by analyzing the heterogeneous resources for the six languages we work on. The optimal level of granularity is task and language dependent: the level is a trade-off between simplicity and e</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupala</author>
<author>Georgiana Dinu</author>
<author>Josef van Genabith</author>
</authors>
<title>Learning morphology with morfette.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>Chrupala, Dinu, van Genabith, 2008</marker>
<rawString>Grzegorz Chrupala, Georgiana Dinu, and Josef van Genabith. 2008. Learning morphology with morfette. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="17072" citStr="Collins, 2002" startWordPosition="2653" endWordPosition="2654">oon et 167 al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative morphology, e.g., Turkish and Finnish. Ruokolainen et al. (2013) present an averaged perceptron (Collins, 2002), a discriminative structured prediction method, for UMS. The model outperforms the semi-supervised model of Poon et al. (2009) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. (2010a) on English, Finnish and Turkish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be sp</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised discovery of morphemes.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGMORPHON.</booktitle>
<contexts>
<context position="16004" citStr="Creutz and Lagus, 2002" startWordPosition="2486" endWordPosition="2489">) present memory-based approaches to discriminative learning of morphological segmentation. This is the previous work most similar to our work. They address the problem of LMS. We distinguish our work from theirs in that we define a cross-lingual schema for defining a hierarchical tagset for LMS. Morever, we tackle the problem with a feature-rich log-linear model, allowing us to easily incorporate disparate sources of knowledge into a single framework, as we show in our extensive evaluation. UMS has been mainly addressed by unsupervised algorithms. LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002) are built around an idea of optimally encoding the data, in the sense of minimal description length (MDL). MORFESSOR CAT-MAP (Creutz et al., 2007a) formulates the model as sequence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (PREFIX, ROOT, SUFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of MORFESSOR for semi-supervised learning. Poon et 167 al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is </context>
</contexts>
<marker>Creutz, Lagus, 2002</marker>
<rawString>Mathias Creutz and Krista Lagus. 2002. Unsupervised discovery of morphemes. In Proceedings of SIGMORPHON.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Induction of a simple morphology for highly-inflecting languages.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGMORPHON.</booktitle>
<contexts>
<context position="16702" citStr="Creutz and Lagus (2004)" startWordPosition="2596" endWordPosition="2599">inimal description length (MDL). MORFESSOR CAT-MAP (Creutz et al., 2007a) formulates the model as sequence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (PREFIX, ROOT, SUFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of MORFESSOR for semi-supervised learning. Poon et 167 al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative morphology, e.g., Turkish and Finnish. Ruokolainen et al. (2013) present an averaged perceptron (Collins, 2002), a discriminative structured prediction method, for UMS. The model outperforms the semi-supervised model of Poon et al. (2009) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. (201</context>
</contexts>
<marker>Creutz, Lagus, 2004</marker>
<rawString>Mathias Creutz and Krista Lagus. 2004. Induction of a simple morphology for highly-inflecting languages. In Proceedings of SIGMORPHON.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
</authors>
<title>Teemu Hirsimäki, Mikko Kurimo, Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Saraclar, and Andreas Stolcke.</title>
<date>2007</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<marker>Creutz, 2007</marker>
<rawString>Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo, Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Saraclar, and Andreas Stolcke. 2007a. Analysis of morph-based speech recognition and the modeling of out-ofvocabulary words across languages. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
</authors>
<title>Teemu Hirsimäki, Mikko Kurimo, Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Saraclar, and Andreas Stolcke.</title>
<date>2007</date>
<publisher>TSLP.</publisher>
<marker>Creutz, 2007</marker>
<rawString>Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo, Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Saraclar, and Andreas Stolcke. 2007b. Morph-based speech recognition and modeling of out-of-vocabulary words across languages. TSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1307" citStr="Dyer et al., 2008" startWordPosition="165" endWordPosition="168">s: (i) morphological segmentation, (ii) stemming and (iii) morphological tag classification. For morphological segmentation our method shows absolute improvements of 2-6 points F1 over a strong baseline. 1 Introduction Morphological processing is often an overlooked problem since many well-studied languages (e.g., Chinese and English) are morphologically impoverished. But for languages with complex morphology (e.g., Finnish and Turkish) morphological processing is essential. A specific form of morphological processing, morphological segmentation, has shown its utility for machine translation (Dyer et al., 2008), sentiment analysis (AbdulMageed et al., 2014), bilingual word alignment (Eyigöz et al., 2013), speech processing (Creutz et al., 2007b) and keyword spotting (Narasimhan et al., 2014), inter alia. We advance the state-of-theart in supervised morphological segmentation by describing a high-performance, data-driven tool for handling complex morphology, even in lowresource settings. In this work, we make the distinction between unlabeled morphological segmentation (UMS ) (often just called “morphological segmentation”) and labeled morphological segmentation (LMS). The labels in our supervised di</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elif Eyigöz</author>
<author>Daniel Gildea</author>
<author>Kemal Oflazer</author>
</authors>
<title>Simultaneous word-morpheme alignment for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="1402" citStr="Eyigöz et al., 2013" startWordPosition="179" endWordPosition="182">For morphological segmentation our method shows absolute improvements of 2-6 points F1 over a strong baseline. 1 Introduction Morphological processing is often an overlooked problem since many well-studied languages (e.g., Chinese and English) are morphologically impoverished. But for languages with complex morphology (e.g., Finnish and Turkish) morphological processing is essential. A specific form of morphological processing, morphological segmentation, has shown its utility for machine translation (Dyer et al., 2008), sentiment analysis (AbdulMageed et al., 2014), bilingual word alignment (Eyigöz et al., 2013), speech processing (Creutz et al., 2007b) and keyword spotting (Narasimhan et al., 2014), inter alia. We advance the state-of-theart in supervised morphological segmentation by describing a high-performance, data-driven tool for handling complex morphology, even in lowresource settings. In this work, we make the distinction between unlabeled morphological segmentation (UMS ) (often just called “morphological segmentation”) and labeled morphological segmentation (LMS). The labels in our supervised discriminative model for LMS capture the distinctions between different types of morphemes and di</context>
</contexts>
<marker>Eyigöz, Gildea, Oflazer, 2013</marker>
<rawString>Elif Eyigöz, Daniel Gildea, and Kemal Oflazer. 2013. Simultaneous word-morpheme alignment for statistical machine translation. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language. Computational Linguistics.</title>
<date>2001</date>
<contexts>
<context position="15965" citStr="Goldsmith, 2001" startWordPosition="2482" endWordPosition="2483">ns (1999) and Marsi et al. (2005) present memory-based approaches to discriminative learning of morphological segmentation. This is the previous work most similar to our work. They address the problem of LMS. We distinguish our work from theirs in that we define a cross-lingual schema for defining a hierarchical tagset for LMS. Morever, we tackle the problem with a feature-rich log-linear model, allowing us to easily incorporate disparate sources of knowledge into a single framework, as we show in our extensive evaluation. UMS has been mainly addressed by unsupervised algorithms. LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002) are built around an idea of optimally encoding the data, in the sense of minimal description length (MDL). MORFESSOR CAT-MAP (Creutz et al., 2007a) formulates the model as sequence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (PREFIX, ROOT, SUFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of MORFESSOR for semi-supervised learning. Poon et 167 al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles </context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stig-Arne Grönroos</author>
<author>Sami Virpioja</author>
<author>Peter Smit</author>
<author>Mikko Kurimo</author>
</authors>
<title>Morfessor FlatCat: An HMM-based method for unsupervised and semisupervised learning of morphology.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="16394" citStr="Grönroos et al. (2014)" startWordPosition="2548" endWordPosition="2551">corporate disparate sources of knowledge into a single framework, as we show in our extensive evaluation. UMS has been mainly addressed by unsupervised algorithms. LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002) are built around an idea of optimally encoding the data, in the sense of minimal description length (MDL). MORFESSOR CAT-MAP (Creutz et al., 2007a) formulates the model as sequence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (PREFIX, ROOT, SUFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of MORFESSOR for semi-supervised learning. Poon et 167 al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative morphology, e.g., Turkish and Fin</context>
</contexts>
<marker>Grönroos, Virpioja, Smit, Kurimo, 2014</marker>
<rawString>Stig-Arne Grönroos, Sami Virpioja, Peter Smit, and Mikko Kurimo. 2014. Morfessor FlatCat: An HMM-based method for unsupervised and semisupervised learning of morphology. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1995</date>
<note>From phoneme to morpheme. Language.</note>
<contexts>
<context position="17522" citStr="Harris, 1995" startWordPosition="2726" endWordPosition="2727">pplicability to language with complex concatenative morphology, e.g., Turkish and Finnish. Ruokolainen et al. (2013) present an averaged perceptron (Collins, 2002), a discriminative structured prediction method, for UMS. The model outperforms the semi-supervised model of Poon et al. (2009) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. (2010a) on English, Finnish and Turkish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese word segmentation (CWS) is related to UMS. Andrew (2006) successfully apply semiCRFs to CWS. The problem of joint CWS and POS tagging (Ng and Low, 2004; Zhang and Clark, 2008) is related to LMS. To our knowledge, joint CWS and POS tagging has not been addressed by a simple single semi-CRF, possibly because POS tagsets typ</context>
</contexts>
<marker>Harris, 1995</marker>
<rawString>Zellig Harris. 1995. From phoneme to morpheme. Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oskar Kohonen</author>
<author>Sami Virpioja</author>
<author>Krista Lagus</author>
</authors>
<title>Semi-supervised learning of concatenative morphology.</title>
<date>2010</date>
<booktitle>In Proceedings of SIGMORPHON.</booktitle>
<contexts>
<context position="16363" citStr="Kohonen et al. (2010" startWordPosition="2543" endWordPosition="2546">del, allowing us to easily incorporate disparate sources of knowledge into a single framework, as we show in our extensive evaluation. UMS has been mainly addressed by unsupervised algorithms. LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002) are built around an idea of optimally encoding the data, in the sense of minimal description length (MDL). MORFESSOR CAT-MAP (Creutz et al., 2007a) formulates the model as sequence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (PREFIX, ROOT, SUFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of MORFESSOR for semi-supervised learning. Poon et 167 al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative mo</context>
</contexts>
<marker>Kohonen, Virpioja, Lagus, 2010</marker>
<rawString>Oskar Kohonen, Sami Virpioja, and Krista Lagus. 2010a. Semi-supervised learning of concatenative morphology. In Proceedings of SIGMORPHON.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oskar Kohonen</author>
<author>Sami Virpioja</author>
<author>Laura Leppänen</author>
<author>Krista Lagus</author>
</authors>
<title>Semi-supervised extensions to Morfessor baseline.</title>
<date>2010</date>
<booktitle>In Proceedings of the Morpho Challenge Workshop.</booktitle>
<contexts>
<context position="16363" citStr="Kohonen et al. (2010" startWordPosition="2543" endWordPosition="2546">del, allowing us to easily incorporate disparate sources of knowledge into a single framework, as we show in our extensive evaluation. UMS has been mainly addressed by unsupervised algorithms. LINGUISTICA (Goldsmith, 2001) and MORFESSOR (Creutz and Lagus, 2002) are built around an idea of optimally encoding the data, in the sense of minimal description length (MDL). MORFESSOR CAT-MAP (Creutz et al., 2007a) formulates the model as sequence prediction based on HMMs over a morph dictionary and MAP estimation. The model also attempts to induce basic morphotactic categories (PREFIX, ROOT, SUFFIX). Kohonen et al. (2010a,b) and Grönroos et al. (2014) present variations of MORFESSOR for semi-supervised learning. Poon et 167 al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative mo</context>
</contexts>
<marker>Kohonen, Virpioja, Leppänen, Lagus, 2010</marker>
<rawString>Oskar Kohonen, Sami Virpioja, Laura Leppänen, and Krista Lagus. 2010b. Semi-supervised extensions to Morfessor baseline. In Proceedings of the Morpho Challenge Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia D Krisnawati</author>
<author>Klaus U Schulz</author>
</authors>
<title>Plagiarism detection for Indonesian texts.</title>
<date>2013</date>
<booktitle>In Proceedings of iiWAS.</booktitle>
<contexts>
<context position="20191" citStr="Krisnawati and Schulz (2013)" startWordPosition="3159" endWordPosition="3162">) and development sets (Dev).7 For German, Indonesian and Zulu we randomly selected 1000 word forms as training set and used the rest as evaluation set. For our final evaluation we trained CHIPMUNK on the concatenation of Train, Tune and Dev (the original 1000 word training set), using the optimal parameters from the cross-evaluation and tested on Test. One of our baselines also uses unlabeled training data. MorphoChallenge provides word lists for English, Finnish, German and Turkish. We use the unannotated part of Ukwabelana for Zulu; and for Indonesian, data from Wikipedia and the corpus of Krisnawati and Schulz (2013). Table 4 shows the important statistics of our datasets. In all evaluations, we use variants of the standard MorphoChallenge evaluation approach. Importantly, for word types with multiple correct segmentations, this involves finding the maximum score by comparing our hypothesized segmentation with each correct segmentation, as is standardly done in MorphoChallenge. 6https://github.com/desmond86/ Indonesian-English-Bilingual-Corpus 7We used both Tune and Dev in order to both optimize hyperparameters on held-out data (Tune) and perform qualitative error analysis on separate held-out data (Dev).</context>
</contexts>
<marker>Krisnawati, Schulz, 2013</marker>
<rawString>Lucia D. Krisnawati and Klaus U. Schulz. 2013. Plagiarism detection for Indonesian texts. In Proceedings of iiWAS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Sami Virpioja</author>
<author>Ville Turunen</author>
<author>Krista Lagus</author>
</authors>
<title>Morpho challenge competition 2005–2010: Evaluations and results.</title>
<date>2010</date>
<booktitle>In Proceedings of SIGMORPHON.</booktitle>
<contexts>
<context position="18521" citStr="Kurimo et al., 2010" startWordPosition="2892" endWordPosition="2895">em of joint CWS and POS tagging (Ng and Low, 2004; Zhang and Clark, 2008) is related to LMS. To our knowledge, joint CWS and POS tagging has not been addressed by a simple single semi-CRF, possibly because POS tagsets typically used in Chinese treebanks are much bigger than our morphotactic tagsets and the morphological poverty of Chinese makes higher-order models necessary and the direct application of semi-CRFs infeasible. 6 Experiments We experimented on six languages from diverse language families. The segmentation data for English, Finnish and Turkish was taken from MorphoChallenge 2010 (Kurimo et al., 2010).5 Despite typically being used for UMS tasks, the MorphoChallenge datasets do contain morpheme level 5http://research.ics.aalto.fi/events/ morphochallenge2010/ Un. Data Train+Tune+Dev Test Train Tune Dev English 878k 800 100 100 694 Finnish 2,928k 800 100 100 835 German 2,338k 800 100 100 751 Indonesian 88k 800 100 100 2500 Turkish 617k 800 100 100 763 Zulu 123k 800 100 100 9040 Table 4: Dataset sizes (number of types). labels. The German data was extracted from the CELEX2 collection (Baayen et al., 1993). The Zulu data was taken from the Ukwabelana corpus (Spiegler et al., 2010). Finally, th</context>
</contexts>
<marker>Kurimo, Virpioja, Turunen, Lagus, 2010</marker>
<rawString>Mikko Kurimo, Sami Virpioja, Ville Turunen, and Krista Lagus. 2010. Morpho challenge competition 2005–2010: Evaluations and results. In Proceedings of SIGMORPHON.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="9735" citStr="Lafferty et al., 2001" startWordPosition="1449" endWordPosition="1453">ponds to the annotation in the datasets. In preliminary experiments we found that the level 5 tagset is too rich and does not yield consistent improvements, we thus do not explore it. Table 1 shows tagset sizes for the six languages.3 3 Model CHIPMUNK is a supervised model implemented using the well-understood semi-Markov conditional random field (semi-CRF) (Sarawagi and Cohen, 2004) that naturally fits the task of LMS. Semi-CRFs generalize linear-chain CRFs and model segmentation jointly with sequence labeling. Just as linear-chain CRFs are discriminative adaptations of hidden Markov models (Lafferty et al., 2001), semi-CRFs are an analogous adaptation of hidden semi-Markov models (Murphy, 2002). Semi-CRFs allow us to elegantly integrate new features that look at complete segments, this is not possible with CRFs, making semi-CRFs a natural choice for morphology. A semi-CRF represents w (a word) as a sequence of segments s = (s1, ... , sn), each of which is assigned a label `i. The concatenation of all segments equals w. We seek a log-linear distribution pθ(s, f |w) overall possible segmentations and label sequences for w, where θ is the 3As converting segmentation datasets to tagsets is not always stra</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Septina Dian Larasati</author>
<author>Vladislav Kuboˇn</author>
<author>Daniel Zeman</author>
</authors>
<title>Indonesian morphology tool (morphind): Towards an Indonesian corpus.</title>
<date>2011</date>
<booktitle>In Systems and Frameworks for Computational Morphology.</booktitle>
<publisher>Springer.</publisher>
<marker>Larasati, Kuboˇn, Zeman, 2011</marker>
<rawString>Septina Dian Larasati, Vladislav Kuboˇn, and Daniel Zeman. 2011. Indonesian morphology tool (morphind): Towards an Indonesian corpus. In Systems and Frameworks for Computational Morphology. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming.</booktitle>
<contexts>
<context position="11196" citStr="Liu and Nocedal, 1989" startWordPosition="1709" endWordPosition="1712">ally, we define pθ as where f is the feature function and is the partition function. To keep the notation uncluttered, we will write f without all its arguments in the future. We use a generalization of the forwardbackward algorithm for efficient gradient computation (Sarawagi an Zθ(w) d Cohen, 2004). Inspection of the semi-Markov forward recursion, � f α(t, l) = i, i eθT ·α(t− `&apos;), (2) � `&apos; shows that algorithm runs in time where n is the length of the word w and L is the number of labels (size of the tagset). We employ the maximum-likelihood criterion to estimate the parameters with L-BFGS (Liu and Nocedal, 1989), agradient-based optimization algorithm. As in all exponential family models, the gradient of the log-likelihood takes the form of the difference between the observed and expected features counts (Wainwright and Jordan, 2008) and can be computed efficiently with the semi-Markov extension of the forward-backward algorithm. We use L2 regularization with a regularization coefficient tuned during cross-validation. We note that semi-Markov models have the potential to obviate typical errors made by standard Markovian sequence models with an IOB labeling scheme over characters. For instance, consid</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erwin Marsi</author>
<author>Antal van den Bosch</author>
<author>Abdelhadi Soudi</author>
</authors>
<title>Memory-based morphological analysis generation and part-of-speech tagging of Arabic.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop: Computational Approaches to Semitic Languages.</booktitle>
<marker>Marsi, van den Bosch, Soudi, 2005</marker>
<rawString>Erwin Marsi, Antal van den Bosch, and Abdelhadi Soudi. 2005. Memory-based morphological analysis generation and part-of-speech tagging of Arabic. In Proceedings of ACL Workshop: Computational Approaches to Semitic Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Murphy</author>
</authors>
<title>Hidden semi-Markov models (hsmms).</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="9818" citStr="Murphy, 2002" startWordPosition="1463" endWordPosition="1465">5 tagset is too rich and does not yield consistent improvements, we thus do not explore it. Table 1 shows tagset sizes for the six languages.3 3 Model CHIPMUNK is a supervised model implemented using the well-understood semi-Markov conditional random field (semi-CRF) (Sarawagi and Cohen, 2004) that naturally fits the task of LMS. Semi-CRFs generalize linear-chain CRFs and model segmentation jointly with sequence labeling. Just as linear-chain CRFs are discriminative adaptations of hidden Markov models (Lafferty et al., 2001), semi-CRFs are an analogous adaptation of hidden semi-Markov models (Murphy, 2002). Semi-CRFs allow us to elegantly integrate new features that look at complete segments, this is not possible with CRFs, making semi-CRFs a natural choice for morphology. A semi-CRF represents w (a word) as a sequence of segments s = (s1, ... , sn), each of which is assigned a label `i. The concatenation of all segments equals w. We seek a log-linear distribution pθ(s, f |w) overall possible segmentations and label sequences for w, where θ is the 3As converting segmentation datasets to tagsets is not always straightforward, we include tags that lack some features, e.g., some level 4 German tag</context>
</contexts>
<marker>Murphy, 2002</marker>
<rawString>Kevin P Murphy. 2002. Hidden semi-Markov models (hsmms). Technical report, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Müller</author>
<author>Helmut Schmid</author>
<author>Hinrich Schütze</author>
</authors>
<title>Efficient higher-order CRFs for morphological tagging.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="22133" citStr="Müller et al., 2013" startWordPosition="3451" endWordPosition="3454"> by predicting LMS and then discarding the labels. Our primary baseline is the state-of-the-art supervised system CRF-MORPH of Ruokolainen et al. (2013). We ran the version of the system that the authors published on their website.8 We optimized the model’s two hyperparameters on Tune: the number of epochs and the maximal length of ngram character features. The system also supports Harris’s letter successor variety (LSV) features (Section 5), extracted from large unannotated corpora, our second baseline. For completeness, we also compare CHIPMUNK with a first-order CRF and a higher-order CRF (Müller et al., 2013), both used the same n-gram features as CRF-MORPH, but without the LSV features.9 We evaluate all models using the traditional macro F1 of the segmentation boundaries. Discussion. The UMS results on held-out data are displayed in Table 5. Our most complex model beats the best baseline by between 1 (German) and 3 (Finnish) points F1 on all six languages. We additionally provide extensive ablation studies to highlight the contribution of our novel features. We find that the properties of each specific language highly influences which features are most effective. For the agglutinative languages, </context>
<context position="31972" citStr="Müller et al., 2013" startWordPosition="5066" endWordPosition="5069"> this task is best performed with sentential context (token-based). Integration with a POS tagger, however, is beyond the scope of this paper. k. 13 The maximum entropy classifier is L1- regularized and its regularization coefficient as well as the value for k are optimized on Tune. As a second, stronger baseline we use a MaxEnt classifier that splits tags into their constituents and concatenates the features with every constituent as well as the complete tag (MaxEnt +Split). Both of the baselines in Table 9 are 0th-order versions of the state-of-the-art CRF-based morphological tagger MARMOT (Müller et al., 2013) (since our model is type-based), making this a strong baseline. We report full analysis accuracy and macro F1 on the set of individual inflectional features. Discussion. The results in Table 9 show that our proposed method outperforms both baselines on both performance metrics. We see gains of over 6% in accuracy in both languages. This is evidence that our proposed approach could be successfully integrated into a morphological tagger to give a stronger character-based signal. 7 Comparison to Finite-State Morphology A morphological finite-state analyzer is customarily a hand-crafted tool that</context>
<context position="35318" citStr="Müller et al., 2013" startWordPosition="5575" endWordPosition="5578">in six languages we work with to universal representations of different granularities. We plan future creation of gold standard segmentations in more languages using our annotation scheme. We further presented CHIPMUNK a semi-CRFbased model for LMS that allows for the integration of various linguistic features and consistently out-performs previously presented approaches to unlabeled morphological segmentation. An important extension of CHIPMUNK is embedding it in a context-sensitive POS tagger. Current stateof-the-art models only employ character level ngram features to model word-internals (Müller et al., 2013). We have demonstrated that our struc14While one can in theory put in wildcard root states, this does not work in practice due to overgeneration. tured approach outperforms this baseline. We leave this natural extension to future work. The datasets used in this work, additional description of our novel tagsets and CHIPMUNK can be found at http://cistern.cis.lmu.de/ chipmunk. Acknowledgments We would like to thank Jason Eisner, Helmut Schmid, Özlem Qetino˘glu and the anonymous reviewers for their comments. This material is based upon work supported by a Fulbright fellowship awarded to the first</context>
</contexts>
<marker>Müller, Schmid, Schütze, 2013</marker>
<rawString>Thomas Müller, Helmut Schmid, and Hinrich Schütze. 2013. Efficient higher-order CRFs for morphological tagging. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Narasimhan</author>
<author>Damianos Karakos</author>
<author>Richard Schwartz</author>
<author>Stavros Tsakalidis</author>
<author>Regina Barzilay</author>
</authors>
<title>Morphological segmentation for keyword spotting.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1491" citStr="Narasimhan et al., 2014" startWordPosition="192" endWordPosition="195">1 over a strong baseline. 1 Introduction Morphological processing is often an overlooked problem since many well-studied languages (e.g., Chinese and English) are morphologically impoverished. But for languages with complex morphology (e.g., Finnish and Turkish) morphological processing is essential. A specific form of morphological processing, morphological segmentation, has shown its utility for machine translation (Dyer et al., 2008), sentiment analysis (AbdulMageed et al., 2014), bilingual word alignment (Eyigöz et al., 2013), speech processing (Creutz et al., 2007b) and keyword spotting (Narasimhan et al., 2014), inter alia. We advance the state-of-theart in supervised morphological segmentation by describing a high-performance, data-driven tool for handling complex morphology, even in lowresource settings. In this work, we make the distinction between unlabeled morphological segmentation (UMS ) (often just called “morphological segmentation”) and labeled morphological segmentation (LMS). The labels in our supervised discriminative model for LMS capture the distinctions between different types of morphemes and directly model the morphotactics. We further create a hierarchical universal tagset for lab</context>
</contexts>
<marker>Narasimhan, Karakos, Schwartz, Tsakalidis, Barzilay, 2014</marker>
<rawString>Karthik Narasimhan, Damianos Karakos, Richard Schwartz, Stavros Tsakalidis, and Regina Barzilay. 2014. Morphological segmentation for keyword spotting. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based?</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="17950" citStr="Ng and Low, 2004" startWordPosition="2801" endWordPosition="2804">ish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese word segmentation (CWS) is related to UMS. Andrew (2006) successfully apply semiCRFs to CWS. The problem of joint CWS and POS tagging (Ng and Low, 2004; Zhang and Clark, 2008) is related to LMS. To our knowledge, joint CWS and POS tagging has not been addressed by a simple single semi-CRF, possibly because POS tagsets typically used in Chinese treebanks are much bigger than our morphotactic tagsets and the morphological poverty of Chinese makes higher-order models necessary and the direct application of semi-CRFs infeasible. 6 Experiments We experimented on six languages from diverse language families. The segmentation data for English, Finnish and Turkish was taken from MorphoChallenge 2010 (Kurimo et al., 2010).5 Despite typically being us</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese partof-speech tagging: One-at-a-time or all-at-once? word-based or character-based? In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="17199" citStr="Poon et al. (2009)" startWordPosition="2671" endWordPosition="2674">dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative morphology, e.g., Turkish and Finnish. Ruokolainen et al. (2013) present an averaged perceptron (Collins, 2002), a discriminative structured prediction method, for UMS. The model outperforms the semi-supervised model of Poon et al. (2009) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. (2010a) on English, Finnish and Turkish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese </context>
</contexts>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>Hoifung Poon, Colin Cherry, and Kristina Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<tech>Program.</tech>
<contexts>
<context position="28786" citStr="Porter, 1980" startWordPosition="4535" endWordPosition="4536">the number of edit paths. We use level 2 tagsets for all experiments—the smallest tagsets complex enough for stemming—and extract the relevant segments. Discussion. Our results are shown in Table 8. We see consistent improvements across all tasks. For the fusional languages (English, German and Indonesian) we see modest gains in performance on both root detection and stemming. However, for the agglutinative languages (Finnish, Turkish and Zulu) we see absolute gains as high as 50% 10Thus in our experiments there are no stem alternations. The output is equivalent to that of the Porter stemmer (Porter, 1980). 11Note that MORFETTE is a pipeline that first tags and then lemmatizes. We only make use of this second part of MORFETTE for which it is a strong string-to-string transduction baseline. 0.0 0.0 0.0 0.0 2.0 0.0 250.0 214.0 374.0 285.0 308.0 267.0 49.0 44.0 43.0 27.0 6.0 9.0 49.0 58.0 56.0 46.0 87.0 73.0 217.0 168.0 146.0 108.0 154.0 143.0 48.0 23.0 43.0 36.0 3.0 3.0 0.0 0.0 0.0 0.0 28.0 29.0 40.0 16.0 0.0 0.0 30.0 34.0 0.0 0.0 3.0 3.0 1.0 2.0 37.0 19.0 233.0 158.0 152.0 103.0 108.0 40.0 0.0 0.0 17.0 17.0 82.0 41.0 401.0 250.0 318.0 264.0 Zul Tur Ind Ger Fin Eng 75 50 25 0 25 50 75 170 English</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F Porter. 1980. An algorithm for suffix stripping. Program.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teemu Ruokolainen</author>
<author>Oskar Kohonen</author>
<author>Sami Virpioja</author>
<author>Mikko Kurimo</author>
</authors>
<title>Supervised morphological segmentation in a low-resource learning setting using conditional random fields.</title>
<date>2013</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="15064" citStr="Ruokolainen et al. (2013)" startWordPosition="2338" endWordPosition="2342">ndependent English words. We solve this problem by incorporating spell-check features: binary 4A good example of such a resource is en.wiktionary.org/wiki/Category:Turkish_suffixes. features that fire if a segment is valid for a given spell checker. Spell-check features function effectively as a proxy for a “root detector”. We use the open-source ASPELL dictionaries as they are freely available in 91 languages. Table 3 shows the coverage of these dictionaries. Integrating the Features. Our model uses the features discussed in this section and additionally the simple n-gram context features of Ruokolainen et al. (2013). The n-gram features look at variable length substrings of the word on both the right and left side of each potential boundary. We create conjunctive features from the cross-product between the morphotactic tagset (Section 2) and the features. 5 Related Work Van den Bosch and Daelemans (1999) and Marsi et al. (2005) present memory-based approaches to discriminative learning of morphological segmentation. This is the previous work most similar to our work. They address the problem of LMS. We distinguish our work from theirs in that we define a cross-lingual schema for defining a hierarchical t</context>
<context position="17025" citStr="Ruokolainen et al. (2013)" startWordPosition="2645" endWordPosition="2648">nt variations of MORFESSOR for semi-supervised learning. Poon et 167 al. (2009) introduces a Bayesian state-space model with corpus-wide priors. The model resembles a semi-CRF, but dynamic programming is no longer possible due to the priors. They employ the threestate tagset of Creutz and Lagus (2004) (row 1 in Figure 2) for Arabic and Hebrew UMS. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative morphology, e.g., Turkish and Finnish. Ruokolainen et al. (2013) present an averaged perceptron (Collins, 2002), a discriminative structured prediction method, for UMS. The model outperforms the semi-supervised model of Poon et al. (2009) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. (2010a) on English, Finnish and Turkish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behi</context>
<context position="21665" citStr="Ruokolainen et al. (2013)" startWordPosition="3377" endWordPosition="3380">8.85 CHIPMUNK 84.40 84.40 93.76 85.53 89.72 87.80 CHIPMUNK +Morph 83.27 84.71 93.17 84.84 90.48 90.03 CHIPMUNK +Affix 83.81 86.02 93.51 85.81 89.72 89.64 CHIPMUNK +Dict 86.10 86.11 95.39 87.76 90.45 88.66 CHIPMUNK +Dict,+Affix,+Morph 86.31 88.38 95.41 87.85 91.36 90.16 Table 5: Test Fl for UMS. Features: LSV = letter successor variety, Affix = affix, Dict = dictionary, Morph = optimal (on Tune) morphotactic tagset. 6.1 UMS Experiments We first evaluate CHIPMUNK on UMS, by predicting LMS and then discarding the labels. Our primary baseline is the state-of-the-art supervised system CRF-MORPH of Ruokolainen et al. (2013). We ran the version of the system that the authors published on their website.8 We optimized the model’s two hyperparameters on Tune: the number of epochs and the maximal length of ngram character features. The system also supports Harris’s letter successor variety (LSV) features (Section 5), extracted from large unannotated corpora, our second baseline. For completeness, we also compare CHIPMUNK with a first-order CRF and a higher-order CRF (Müller et al., 2013), both used the same n-gram features as CRF-MORPH, but without the LSV features.9 We evaluate all models using the traditional macro</context>
<context position="23695" citStr="Ruokolainen et al., 2013" startWordPosition="3688" endWordPosition="3692">/crfs_morph.zip 9Model order, maximal character n-gram length and regularization coefficients were optimized on Tune. provements of 0.77 for Finnish, 1.89 for Turkish and 2.10 for Zulu. We optimized tagset granularity on Tune and found that levels 4 and level 2 yielded the best results for the three agglutinative and the three other languages, respectively. The dictionary features (+Dict) help universally, but their effects are particularly salient in languages with productive compounding, i.e., English, Finnish and German, where we see improvements of &gt; 1.7. In comparison with previous work (Ruokolainen et al., 2013) we find that our most complex model yields consistent improvements over CRFMORPH +LSV for all languages: The improvements range from &gt; 1 for German over &gt; 1.5 for Zulu, English, and Indonesian to &gt; 2 for Turkish and &gt; 4 for Finnish. To illustrate the effect of modeling morphotactics through the larger morphotactic tagset on performance, we provide a detailed analysis of Turkish. See Table 6. We consider three different feature sets and increase the size of the morphotactic tagsets depicted in Figure 2. The results evince the general trend that improved morphotactic modeling benefits segmentat</context>
</contexts>
<marker>Ruokolainen, Kohonen, Virpioja, Kurimo, 2013</marker>
<rawString>Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja, and Mikko Kurimo. 2013. Supervised morphological segmentation in a low-resource learning setting using conditional random fields. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teemu Ruokolainen</author>
<author>Oskar Kohonen</author>
<author>Sami Virpioja</author>
<author>Mikko Kurimo</author>
</authors>
<title>Painless semi-supervised morphological segmentation using conditional random fields.</title>
<date>2014</date>
<booktitle>Proceedings of EACL.</booktitle>
<contexts>
<context position="17373" citStr="Ruokolainen et al. (2014)" startWordPosition="2701" endWordPosition="2704">. Their gradient and objective computation is based on an enumeration of a heuristically chosen subset of the exponentially many segmentations. This limits its applicability to language with complex concatenative morphology, e.g., Turkish and Finnish. Ruokolainen et al. (2013) present an averaged perceptron (Collins, 2002), a discriminative structured prediction method, for UMS. The model outperforms the semi-supervised model of Poon et al. (2009) on Arabic and Hebrew morpheme segmentation as well as the semi-supervised model of Kohonen et al. (2010a) on English, Finnish and Turkish. Finally, Ruokolainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese word segmentation (CWS) is related to UMS. Andrew (2006) successfully apply semiCRFs to CWS. The problem of joint CWS and POS tagging (Ng and Low, 2004; Zhang and Clark, 2008</context>
</contexts>
<marker>Ruokolainen, Kohonen, Virpioja, Kurimo, 2014</marker>
<rawString>Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja, and Mikko Kurimo. 2014. Painless semi-supervised morphological segmentation using conditional random fields. Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>SemiMarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="3031" citStr="Sarawagi and Cohen, 2004" startWordPosition="413" endWordPosition="416">ave typically been viewed as separate problems and addressed using different methods. We give an overview of the tasks addressed in this paper in Figure 1. The figure shows the expected output for the Turkish word genCle¸smelerin ‘of rejuvenatings’. In particular, it shows the full labeled morphological segmentation, from which three representations can be directly derived: the unlabeled morphological segmentation, the stem/root 1 and the structured morphological tag containing POS and inflectional features. We model these tasks with CHIPMUNK, a semiMarkov conditional random field (semi-CRF) (Sarawagi and Cohen, 2004), a model that is wellsuited for morphology. We provide a robust evaluation and analysis on six languages and CHIPMUNK yields strong results on all three tasks, including state-of-the-art accuracy on morphological segmentation. Section 2 presents our LMS framework and the morphotactic tagsets we use, i.e., the labels of the sequence prediction task CHIPMUNK solves. Section 3 introduces our semi-CRF model. Section 4 presents our novel features. Section 5 compares CHIPMUNK to previous work. Section 6 presents experiments on the three complementary tasks of segmentation (UMS), stemming, and morph</context>
<context position="9499" citStr="Sarawagi and Cohen, 2004" startWordPosition="1414" endWordPosition="1417">an 1 4 4 8 8 Turkish 1 3 4 10 20 Zulu 1 4 6 14 17 Table 1: Morphotactic tagset size at each level of granularity. e.g., CASE must follow NUMBER. The level 5 tagset adds the actual value of the inflectional feature, e.g., PLURAL, and corresponds to the annotation in the datasets. In preliminary experiments we found that the level 5 tagset is too rich and does not yield consistent improvements, we thus do not explore it. Table 1 shows tagset sizes for the six languages.3 3 Model CHIPMUNK is a supervised model implemented using the well-understood semi-Markov conditional random field (semi-CRF) (Sarawagi and Cohen, 2004) that naturally fits the task of LMS. Semi-CRFs generalize linear-chain CRFs and model segmentation jointly with sequence labeling. Just as linear-chain CRFs are discriminative adaptations of hidden Markov models (Lafferty et al., 2001), semi-CRFs are an analogous adaptation of hidden semi-Markov models (Murphy, 2002). Semi-CRFs allow us to elegantly integrate new features that look at complete segments, this is not possible with CRFs, making semi-CRFs a natural choice for morphology. A semi-CRF represents w (a word) as a sequence of segments s = (s1, ... , sn), each of which is assigned a lab</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W Cohen. 2004. SemiMarkov conditional random fields for information extraction. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Smith</author>
<author>Miles Osborne</author>
</authors>
<title>Using gazetteers in discriminative information extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="13332" citStr="Smith and Osborne, 2006" startWordPosition="2059" endWordPosition="2062">resources, e.g., spell checkers and Wiktionary, to create straightforward and effective features and we incorporate ideas from related areas: named-entity recognition (NER) and morphological tagging. Affix Features and Gazetteers. In contrast to syntax and semantics, the morphology of a language is often simple to document and a list of the most common morphs can be found in any good grammar book. Wiktionary, for example, contains affix lists for all the six languages used in our experiments.4 Providing a supervised learner with such a list is a great boon, just as gazetteer features aid NER (Smith and Osborne, 2006)— perhaps even more so since suffixes and prefixes are generally closed-class; hence these lists are likely to be comprehensive. These features are binary and fire if a given substring occurs in the gazetteer list. In this paper, we simply use suffix lists from English Wiktionary, except for Zulu, for which we use a prefix list, see Table 2. We also include a feature that fires on the conjunction of tags and substrings observed in the training data. In the level 5 tagset this allows us to link all allomorphs of a given morpheme. In the lower level tagsets, this links related morphemes. Virpioj</context>
</contexts>
<marker>Smith, Osborne, 2006</marker>
<rawString>Andrew Smith and Miles Osborne. 2006. Using gazetteers in discriminative information extraction. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Spiegler</author>
<author>Andrew Van Der Spuy</author>
<author>Peter A Flach</author>
</authors>
<title>Ukwabelana: An open-source morphological Zulu corpus.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<marker>Spiegler, Van Der Spuy, Flach, 2010</marker>
<rawString>Sebastian Spiegler, Andrew Van Der Spuy, and Peter A Flach. 2010. Ukwabelana: An open-source morphological Zulu corpus. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antal Van den Bosch</author>
<author>Walter Daelemans</author>
</authors>
<title>Memory-based morphological analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL. Association for Computational Linguistics.</booktitle>
<marker>Van den Bosch, Daelemans, 1999</marker>
<rawString>Antal Van den Bosch and Walter Daelemans. 1999. Memory-based morphological analysis. In Proceedings of ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sami Virpioja</author>
<author>Oskar Kohonen</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised morpheme analysis with Allomorfessor. In Multilingual Information Access Evaluation.</title>
<date>2010</date>
<publisher>Springer.</publisher>
<contexts>
<context position="13947" citStr="Virpioja et al. (2010)" startWordPosition="2166" endWordPosition="2169">, 2006)— perhaps even more so since suffixes and prefixes are generally closed-class; hence these lists are likely to be comprehensive. These features are binary and fire if a given substring occurs in the gazetteer list. In this paper, we simply use suffix lists from English Wiktionary, except for Zulu, for which we use a prefix list, see Table 2. We also include a feature that fires on the conjunction of tags and substrings observed in the training data. In the level 5 tagset this allows us to link all allomorphs of a given morpheme. In the lower level tagsets, this links related morphemes. Virpioja et al. (2010) explored this idea for unsupervised segmentation. Linking allomorphs together under a single tag helps combat sparsity in modeling the morphotactics. Stem Features. A major problem in statistical segmentation is the reluctance to posit morphs not observed in training; this particularly affects roots, which are open-class. This makes it nearly impossible to correctly segment compounds that contain unseen roots, e.g., to correctly segment homework you need to know that home and work are independent English words. We solve this problem by incorporating spell-check features: binary 4A good exampl</context>
</contexts>
<marker>Virpioja, Kohonen, Lagus, 2010</marker>
<rawString>Sami Virpioja, Oskar Kohonen, and Krista Lagus. 2010. Unsupervised morpheme analysis with Allomorfessor. In Multilingual Information Access Evaluation. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin J Wainwright</author>
<author>Michael I Jordan</author>
</authors>
<title>Graphical models, exponential families, and variational inference. Foundations and Trends</title>
<date>2008</date>
<booktitle>in Machine Learning.</booktitle>
<contexts>
<context position="11422" citStr="Wainwright and Jordan, 2008" startWordPosition="1742" endWordPosition="1745">rd algorithm for efficient gradient computation (Sarawagi an Zθ(w) d Cohen, 2004). Inspection of the semi-Markov forward recursion, � f α(t, l) = i, i eθT ·α(t− `&apos;), (2) � `&apos; shows that algorithm runs in time where n is the length of the word w and L is the number of labels (size of the tagset). We employ the maximum-likelihood criterion to estimate the parameters with L-BFGS (Liu and Nocedal, 1989), agradient-based optimization algorithm. As in all exponential family models, the gradient of the log-likelihood takes the form of the difference between the observed and expected features counts (Wainwright and Jordan, 2008) and can be computed efficiently with the semi-Markov extension of the forward-backward algorithm. We use L2 regularization with a regularization coefficient tuned during cross-validation. We note that semi-Markov models have the potential to obviate typical errors made by standard Markovian sequence models with an IOB labeling scheme over characters. For instance, consider the incorrect segmentation of the English verb sees into se+es. These are reasonable split positions as many English stems end in se (e.g., consider abuse-s). Semi-CRFs have a major advantage here as they can O(n2·L2) have </context>
</contexts>
<marker>Wainwright, Jordan, 2008</marker>
<rawString>Martin J Wainwright and Michael I Jordan. 2008. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="17974" citStr="Zhang and Clark, 2008" startWordPosition="2805" endWordPosition="2808">olainen et al. (2014) get further consistent improvements by using features extracted from large corpora, based on the letter successor variety (LSV) model (Harris, 1995) and on unsupervised segmentation models such as Morfessor CatMAP (Creutz et al., 2007a). The idea behind LSV is that for example talking should be split into talk and ing, because talk can also be followed by different letters then i such as e (talked) and s (talks). Chinese word segmentation (CWS) is related to UMS. Andrew (2006) successfully apply semiCRFs to CWS. The problem of joint CWS and POS tagging (Ng and Low, 2004; Zhang and Clark, 2008) is related to LMS. To our knowledge, joint CWS and POS tagging has not been addressed by a simple single semi-CRF, possibly because POS tagsets typically used in Chinese treebanks are much bigger than our morphotactic tagsets and the morphological poverty of Chinese makes higher-order models necessary and the direct application of semi-CRFs infeasible. 6 Experiments We experimented on six languages from diverse language families. The segmentation data for English, Finnish and Turkish was taken from MorphoChallenge 2010 (Kurimo et al., 2010).5 Despite typically being used for UMS tasks, the Mo</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>