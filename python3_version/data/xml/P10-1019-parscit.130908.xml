<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.987609">
Importance-Driven Turn-Bidding for Spoken Dialogue Systems
</title>
<author confidence="0.845441">
Ethan O. Selfridge and Peter A. Heeman
</author>
<affiliation confidence="0.686581">
Center for Spoken Language Understanding
Oregon Health &amp; Science University
</affiliation>
<address confidence="0.833384">
20000 NW Walker Rd., Beaverton, OR, 97006
</address>
<email confidence="0.999273">
selfridg@ohsu.edu, heemanp@ohsu.edu
</email>
<sectionHeader confidence="0.993922" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999689105263158">
Current turn-taking approaches for spoken
dialogue systems rely on the speaker re-
leasing the turn before the other can take it.
This reliance results in restricted interac-
tions that can lead to inefficient dialogues.
In this paper we present a model we re-
fer to as Importance-Driven Turn-Bidding
that treats turn-taking as a negotiative pro-
cess. Each conversant bids for the turn
based on the importance of the intended
utterance, and Reinforcement Learning is
used to indirectly learn this parameter. We
find that Importance-Driven Turn-Bidding
performs better than two current turn-
taking approaches in an artificial collabo-
rative slot-filling domain. The negotiative
nature of this model creates efficient dia-
logues, and supports the improvement of
mixed-initiative interaction.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931583333334">
As spoken dialogue systems are designed to
perform ever more elaborate tasks, the need
for mixed-initiative interaction necessarily grows.
Mixed-initiative interaction, where agents (both
artificial and human) may freely contribute to
reach a solution efficiently, has long been a focus
of dialogue systems research (Allen et al., 1999;
Guinn, 1996). Simple slot-filling tasks might
not require the flexible environment that mixed-
initiative interaction brings but those of greater
complexity, such as collaborative task comple-
tion or long-term planning, certainly do (Fergu-
son et al., 1996). However, translating this interac-
tion into working systems has proved problematic
(Walker et al., 1997), in part to issues surround-
ing turn-taking: the transition from one speaker to
another.
Many computational turn-taking approaches
seek to minimize silence and utterance overlap
during transitions. This leads to the speaker con-
trolling the turn transition. For example, systems
using the Keep-Or-Release approach will not at-
tempt to take the turn unless it is sure the user
has released it. One problem with this approach
is that the system might have important informa-
tion to give but will be unable to get the turn.
The speaker-centric nature of current approaches
does not enable mixed-initiative interaction and
results in inefficient dialogues. Primarily, these
approaches have been motivated by smooth tran-
sitions reported in the human turn-taking studies
of Sacks et al. (1974) among others.
Sacks et al. also acknowledge the negotiative
nature of turn-taking, stating that the “the turn as
unit is interactively determined”(p. 727). Other
studies have supported this, suggesting that hu-
mans negotiate the turn assignment through the
use of cues and that these cues are motivated by
the importance of what the conversant wishes to
contribute (Duncan and Niederehe, 1974; Yang
and Heeman, 2010; Schegloff, 2000). Given
this, any dialogue system hoping to interact with
humans efficiently and naturally should have a
negotiative and importance-driven quality to its
turn-taking protocol. We believe that, by focus-
ing on the rationale of human turn-taking be-
havior, a more effective turn-taking system may
be achieved. We propose the Importance-Driven
Turn-Bidding (IDTB) model where conversants
bid for the turn based on the importance of their
utterance. We use Reinforcement Learning to map
a given situation to the optimal utterance and bid-
ding behavior. By allowing conversants to bid for
the turn, the IDTB model enables negotiative turn-
taking and supports true mixed-initiative interac-
tion, and with it, greater dialogue efficiency.
We compare the IDTB model to current turn-
taking approaches. Using an artificial collab-
orative dialogue task, we show that the IDTB
model enables the system and user to complete
</bodyText>
<page confidence="0.966371">
177
</page>
<note confidence="0.9434915">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 177–185,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999828625">
the task more efficiently than the other approaches.
Though artificial dialogues are not ideal, they al-
low us to test the validity of the IDTB model be-
fore embarking on costly and time-consuming hu-
man studies. Since our primary evaluation criteria
is model comparison, consistent user simulations
provide a constant needed for such measures and
increase the external validity of our results.
</bodyText>
<sectionHeader confidence="0.822291" genericHeader="introduction">
2 Current Turn-Taking Approaches
</sectionHeader>
<bodyText confidence="0.999795859375">
Current dialogue systems focus on the release-turn
as the most important aspect of turn-taking, in
which a listener will only take the turn after the
speaker has released it. The simplest of these ap-
proaches only allows a single utterance per turn,
after which the turn necessarily transitions to the
next speaker. This Single-Utterance (SU) model
has been extended to allow the speaker to keep the
turn for multiple utterances: the Keep-Or-Release
(KR) approach. Since the KR approach gives the
speaker sole control of the turn, it is overwhelm-
ingly speaker-centric, and so necessarily unnego-
tiative. This restriction is meant to encourage
smooth turn-transitions, and is inspired by the or-
der, smoothness, and predictability reported in hu-
man turn-taking studies (Duncan, 1972; Sacks et
al., 1974).
Systems using the KR approach differ on how
they detect the user’s release-turn. Turn releases
are commonly identified in two ways: either us-
ing a silence-threshold (Sutton et al., 1996), or
the predictive nature of turn endings (Sacks et al.,
1974) and the cues associated with them (e.g. Gra-
vano and Hirschberg, 2009). Raux and Eskenazi
(2009) used decision theory with lexical cues to
predict appropriate places to take the turn. Simi-
larly, Jonsdottir, Thorisson, and Nivel (2008) used
Reinforcement Learning to reduce silences be-
tween turns and minimize overlap between utter-
ances by learning the specific turn-taking patterns
of individual speakers. Skantze and Schlangan
(2009) used incremental processing of speech and
prosodic turn-cues to reduce the reaction time of
the system, finding that that users rated this ap-
proach as more human-like than a baseline system.
In our view, systems built using the KR turn-
taking approach suffer from two deficits. First,
the speaker-centricity leads to inefficient dialogues
since the speaker may continue to hold the turn
even when the listener has vital information to
give. In addition, the lack of negotiation forces
the turn to necessarily transition to the listener af-
ter the speaker releases it. The possibility that the
dialogue may be better served if the listener does
not get the turn is not addressed by current ap-
proaches.
Barge-in, which generally refers to allowing
users to speak at any time (Strom and Seneff,
2000), has been the primary means to create a
more flexible turn-taking environment. Yet, since
barge-in recasts speaker-centric systems as user-
centric, the system’s contributions continue to be
limited. System barge-in has also been investi-
gated. Sato et al. (2002) used decision trees to de-
termine whether the system should take the turn or
not when the user pauses. An incremental method
by DeVault, Sagae, and Traum (2009) found pos-
sible points that a system could interrupt without
loss of user meaning, but failed to supply a rea-
sonable model as to when to use such information.
Despite these advances, barge-in capable systems
lack a negotiative turn-taking method, and con-
tinue to be deficient for reasons similar to those
described above.
</bodyText>
<sectionHeader confidence="0.995401" genericHeader="method">
3 Importance-Driven Turn-Bidding
(IDTB)
</sectionHeader>
<bodyText confidence="0.99958164">
We introduce the IDTB model to overcome the de-
ficiencies of current approaches. The IDTB model
has two foundational components: (1) The impor-
tance of speaking is the primary motivation behind
turn-taking behavior, and (2) conversants use turn-
cue strength to bid for the turn based on this impor-
tance. Importance may be broadly defined as how
well the utterance leads to some predetermined
conversational success, be it solely task comple-
tion or encompassing a myriad of social etiquette
components.
Importance-Driven Turn-Bidding is motivated
by empirical studies of human turn-conflict res-
olution. Yang and Heeman (2010) found an in-
crease of turn conflicts during tighter time con-
straints, which suggests that turn-taking is in-
fluenced by the importance of task completion.
Schlegoff (2000) proposed that persistent utter-
ance overlap was indicative of conversants hav-
ing a strong interest in holding the turn. Walker
and Whittaker (1990) show that people will inter-
rupt to remedy some understanding discrepancy,
which is certainly important to the conversation’s
success. People communicate the importance of
their utterance through turn-cues. Duncan and
</bodyText>
<page confidence="0.997037">
178
</page>
<bodyText confidence="0.9999516">
Niederehe (1974) found that turn-cue strength was
the best predictor of who won the turn, and this
finding is consistent with the use of volume to win
turns found by Yang and Heeman (2010).
The IDTB model uses turn-cue strength to bid
for the turn based on the importance of the utter-
ance. Stronger turn-cues should be used when the
intended utterance is important to the overall suc-
cess of the dialogue, and weaker ones when it is
not. In the prototype described in Section 5, both
the system and user agents bid for the turn after ev-
ery utterance and the bids are conceptualized here
as utterance onset: conversants should be quick
to speak important utterances but slow with less
important ones. This is relatively consistent with
Yang and Heeman (2010). A mature version of
our work will use cues in addition to utterance on-
set, such as those recently detailed in Gravano and
Hirshberg (2009).1
A crucial element of our model is the judgment
and quantization of utterance importance. We use
Reinforcement Learning (RL) to determine impor-
tance by conceptualizing it as maximizing the re-
ward over an entire dialogue. Whatever actions
lead to a higher return may be thought of as more
important than ones that do not.2 By using RL to
learn both the utterance and bid behavior, the sys-
tem can find an optimal pairing between them, and
choose the best combination for a given conversa-
tional situation.
</bodyText>
<sectionHeader confidence="0.9952" genericHeader="method">
4 Information State Update and
</sectionHeader>
<subsectionHeader confidence="0.527433">
Reinforcement Learning
</subsectionHeader>
<bodyText confidence="0.944963925">
We build our dialogue system using the Informa-
tion State Update approach (Larsson and Traum,
2000) and use Reinforcement Learning for action
selection (Sutton and Barto, 1998). The system
architecture consists of an Information State (IS)
that represents the agent’s knowledge and is up-
dated using a variety of rules. The IS also uses
rules to propose possible actions. A condensed
and compressed subset of the IS — the Reinforce-
ment Learning State — is used to learn which pro-
posed action to take (Heeman, 2007). It has been
shown that using RL to learn dialogue polices is
generally more effective than “hand crafted” di-
1Our work (present and future) is distinct from some re-
cent work on user pauses (Sato et al., 2002) since we treat
turn-taking as an integral piece of dialogue success.
2We gain an inherent flexibility in using RL since the re-
ward can be computed by a wide array of components. This
is consistent with the broad definition of importance.
alogue policies since the learning algorithm may
capture environmental dynamics that are unat-
tended to by human designers (Levin et al., 2000).
Reinforcement Learning learns an optimal pol-
icy, a mapping between a state s and action a,
where performing a in s leads to the lowest ex-
pected cost for the dialogue (we use minimum
cost instead of maximum reward). An c-greedy
search is used to estimate Q-scores, the expected
cost of some state–action pair, where the system
chooses a random action with c probability and the
argminaQ(s, a) action with 1-c probability. For
Q-learning, a popular RL algorithm and the one
used here, c is commonly set at 0.2 (Sutton and
Barto, 1998). Q-learning updates Q(s, a) based
on the best action of the next state, given by the
following equation, with the step size parameter
�
α = 1/ N(s, a) where N(s, a) is the number of
times the s, a pair has been seen since the begin-
ning of training.
</bodyText>
<equation confidence="0.9112515">
Q(st, at) = Q(st, at) + α[costt+1
+ argminaQ(st+1, a) − Q(st, at)]
</equation>
<bodyText confidence="0.999964">
The state space should be formulated as a
Markov Decision Process (MDP) for Q-learning
to update Q-scores properly. An MDP relies on
a first-order Markov assumption in that the transi-
tion and reward probability from some st, at pair
is completely contained by that pair and is unaf-
fected by the history st−1at−1, st−2at−2, . . .. For
this assumption to be met, care is required when
deciding which features to include for learning.
The RL State features we use are described in the
following section.
</bodyText>
<sectionHeader confidence="0.924889" genericHeader="method">
5 Domain and Turn-Taking Models
</sectionHeader>
<bodyText confidence="0.9999578">
In this section, we show how the IDTB ap-
proach can be implemented for a collaborative
slot filling domain. We also describe the Single-
Utterance and Keep-Or-Release domain imple-
mentations that we use for comparison.
</bodyText>
<subsectionHeader confidence="0.991994">
5.1 Domain Task
</subsectionHeader>
<bodyText confidence="0.999943777777778">
We use a food ordering domain with two partici-
pants, the system and a user, and three slots: drink,
burger, and side. The system’s objective is to fill
all three slots with the available fillers as quickly
as possible. The user’s role is to specify its de-
sired filler for each slot, though that specific filler
may not be available. The user simulation, while
intended to be realistic, is not based on empirical
data. Rather, it is designed to provide a rich turn-
</bodyText>
<page confidence="0.993574">
179
</page>
<bodyText confidence="0.999716266666667">
taking domain to evaluate the performance of dif-
ferent turn-taking designs. We consider this a col-
laborative slot-filling task since both conversants
must supply information to determine the intersec-
tion of available and desired fillers.
Users have two fillers for each slot.3 A user’s
top choice is either available, in which case we say
that the user has adequate filler knowledge, or their
second choice will be available, in which we say
it has inadequate filler knowledge. This assures
that at least one of the user’s filler is available.
Whether a user has adequate or inadequate filler
knowledge is probabilistically determined based
on user type, which will be described in Section
5.2.
</bodyText>
<tableCaption confidence="0.996552">
Table 1: Agent speech acts
</tableCaption>
<bodyText confidence="0.994420258064516">
Agent Actions
System query slot, inform [yes/no],
inform avail. slot fillers,
inform filler not available, bye
User inform slot filler,
query filler availability
We model conversations at the speech act level,
shown in Table 1, and so do not model the actual
words that the user and system might say. Each
agent has an Information State that proposes possi-
ble actions. The IS is made up of a number of vari-
ables that model the environment and is slightly
different for the system and the user. Shared vari-
ables include QUD, a stack which manages the
questions under discussion; lastUtterance, the pre-
vious utterance, and slotList, a list of the slot
names. The major system specific IS variables
that are not included in the RL State are availSlot-
Fillers, the available fillers for each slot; and three
slotFiller variables that hold the fillers given by the
user. The major user specific IS variables are three
desiredSlotFiller variables that hold an ordered list
of fillers, and unvisitedSlots, a list of slots that the
user believes are unfilled.
The system has a variety of speech actions: in-
form [yes/no], to answer when the user has asked a
filler availability question; inform filler not avail-
able, to inform the user when they have specified
an unavailable filler; three query slot actions (one
for each slot), a query which asks the user for a
filler and is proposed if that specific slot is unfilled;
</bodyText>
<footnote confidence="0.5071855">
3We use two fillers so as to minimize the length of train-
ing. This can be increased without substantial effort.
</footnote>
<bodyText confidence="0.99900884">
three inform available slot fillers actions, which
lists the available fillers for that slot and is pro-
posed if that specific slot is unfilled or filled with
an unavailable filler; and bye, which is always pro-
posed.
The user has two actions. They can inform the
system of a desired slot filler, inform slot filler, or
query the availability of a slot’s top filler, query
filler availability. A user will always respond with
the same slot as a system query, but may change
slots entirely for all other situations. Additional
details on user action selection are given in Section
5.2.
Specific information is used to produce an in-
stantiated speech action, what we refer to as an
utterance. For example, the speech action inform
slot filler results in the utterance of ”inform drink
d1.” A sample dialogue fragment using the Single-
Utterance approach is shown in Table 2. Notice
that in Line 3 the system informs the user that
their first filler, d1, is unavailable. The user then
asks asks about the availability of its second drink
choice, d2 (Line 4), and upon receiving an affirma-
tive response (Line 5), informs the system of that
filler preference (Line 6).
</bodyText>
<tableCaption confidence="0.883216">
Table 2: Single-Utterance dialogue
</tableCaption>
<table confidence="0.583883">
Spkr Speech Action Utterance
1 S: q. slot q. drink
</table>
<listItem confidence="0.938692166666667">
2 U: i. slot filler i. drink d1
3 S: i. filler not avail i. not have d1
4 U: q. filler avail q. drink have d2
5 S: i. slot i. yes
6 U: i. slot filler i. drink d2
7 S: i. avail slot fillers i. burger have b1
</listItem>
<bodyText confidence="0.992479615384615">
Implementation in RL: The system uses RL to
learn which of the IS proposed actions to take. In
this domain we use a cost function based on dia-
logue length and the number of slots filled with an
available filler: C = Number of Utterances + 25 �
unavailablyFilledSlots. In the present implemen-
tation the system’s bye utterance is costless. The
system chooses the action that minimizes the ex-
pected cost of the entire dialogue from the current
state.
The RL state for the speaker has seven vari-
ables:4 QUD-speaker, the stack of speakers who
have unresolved questions; Incorrect-Slot-Fillers,
</bodyText>
<footnote confidence="0.9708695">
4We experimented with a variety of RL States and this one
proved to be both small and effective.
</footnote>
<page confidence="0.994167">
180
</page>
<bodyText confidence="0.998852888888889">
a list of slot fillers (ordered chronologically on
when the user informed them) that are unavail-
able and have not been resolved; Last-Sys-Speech-
Action, the last speech action the system per-
formed; Given-Slot-Fillers, a list of slots that the
system has performed the inform available slot
filler action on; and three booleans variables, slot-
RL, that specify whether a slot has been filled cor-
rectly or not (e.g. Drink-RL).
</bodyText>
<subsectionHeader confidence="0.997694">
5.2 User Types
</subsectionHeader>
<bodyText confidence="0.999930023809524">
We define three different types of users — Experts,
Novices, and Intermediates. User types differ
probabilistically on two dimensions: slot knowl-
edge, and slot belief strength. We define experts to
have a 90 percent chance of having adequate filler
knowledge, intermediates a 50 percent chance,
and novices a 10 percent chance. These proba-
bilities are independent between slots. Slot belief
strength represents the user’s confidence that it has
adequate domain knowledge for the slot (i.e. the
top choice for that slot is available). It is either
a strong, warranted, or weak belief (Chu-Carroll
and Carberry, 1995). The intuition is that experts
should know when their top choice is available,
and novices should know that they do not know
the domain well.
Initial slot belief strength is dependent on user
type and whether their filler knowledge is ade-
quate (their initial top choice is available). Ex-
perts with adequate filler knowledge have a 70,
20, and 10 percent chance of having Strong, War-
ranted, and Weak beliefs respectfully. Similarly,
intermediates with adequate knowledge have a 50,
25, and 25 percent chance of the respective belief
strengths. When these user types have inadequate
filler knowledge the probabilities are reversed to
determine belief strength (e.g. Experts with inad-
equate domain knowledge for a slot have a 70%
chance of having a weak belief). Novice users al-
ways have a 10, 10, and 80 percent chance of the
respective belief strengths.
The user choses whether to use the query or
inform speech action based on the slot’s belief
strength. A strong belief will always result in an
inform, a warranted belief resulting in an inform
with p = 0.5, and weak belief will result in an in-
form with p = 0.25. If the user is informed of the
correct fillers by the system’s inform, that slot’s
belief strength is set to strong. If the user is in-
formed that a filler is not available, than that filler
is removed from the desired filler list and the belief
remains the same.5
</bodyText>
<subsectionHeader confidence="0.985235">
5.3 Turn-Taking Models
</subsectionHeader>
<bodyText confidence="0.603146678571429">
We now discuss how turn-taking works for the
IDTB model and the two competing models that
we use to evaluate our approach. The system
chooses its turn action based on the RL state and
we add a boolean variable turn-action to the RL
State to indicate when the system is performing a
turn action or a speech action. The user uses belief
to choose its turn action.
Turn-Bidding: Agents bid for the turn at the
end of each utterance to determine who will speak
next. Each bid is represented as a value between 0
and 1, and the agent with the lower value (stronger
bid) wins the turn. This is consistent with the
use of utterance onset. There are 5 types of bids,
highest, high, middle, low, and lowest, which are
spread over a portion of the range as shown in Fig-
ure 1. The system uses RL to choose a bid and
a random number (uniform distribution) is gener-
ated from that bid’s range. The users’ bids are de-
termined by their belief strength, which specifies
the mean of a Gaussian distribution, as shown in
Figure 1 (e.g Strong belief implies a p = 0.35).
Computing bids in this fashion leads to, on av-
erage, users with strong beliefs bidding highest,
warranted beliefs bidding in the middle, and weak
beliefs bidding lowest. The use of the probabil-
ity distributions allows us to randomly decide ties
between system and user bids.
</bodyText>
<figureCaption confidence="0.999582">
Figure 1: Bid Value Probability Distribution
</figureCaption>
<bodyText confidence="0.9684915">
Single-Utterance: The Single-Utterance (SU)
approach, as described in Section 2, has a rigid
</bodyText>
<footnote confidence="0.934526333333333">
5In this simple domain the next filler is guaranteed to be
available if the first is not. We do not model this with belief
strength since it is probably not representative of reality.
</footnote>
<page confidence="0.998138">
181
</page>
<bodyText confidence="0.993351878787879">
turn-taking mechanism. After a speaker makes a
single utterance the turn transitions to the listener.
Since the turn transitions after every utterance the
system must only choose appropriate utterances,
not turn-taking behavior. Similarly, user agents do
not have any turn-taking behavior and slot beliefs
are only used to choose between a query and an
inform.
Keep-Or-Release Model: The Keep-Or-
Release (KR) model, as described in Section
2, allows the speaker to either keep the turn to
make multiple utterances or release it. Taking the
same approach as English and Heeman (2005),
the system learns to keep or release the turn after
each utterance that it makes. We also use RL
to determine which conversant should begin the
dialogue. While the use of RL imparts some
importance onto the turn-taking behavior, it
is not influencing whether the system gets the
turn when it did not already have it. This is an
crucial distinction between KR and IDTB. IDTB
allows the conversants to negotiate the turn using
turn-bids motivated by importance, whereas in
KR only the speaker determines when the turn
can transition.
Users in the KR environment choose whether to
keep or release the turn similarly to bid decisions.6
After a user performs an utterance, it chooses the
slot that would be in the next utterance. A number,
k, is generated from a Gaussian distribution using
belief strength in the same manner as the IDTB
users’ bids are chosen. If k &lt; 0.55 then the user
keeps the turn, otherwise it releases it.
</bodyText>
<subsectionHeader confidence="0.999269">
5.4 Preliminary Turn-Bidding System
</subsectionHeader>
<bodyText confidence="0.999958928571429">
We described a preliminary turn-bidding system
in earlier work presented at a workshop (Selfridge
and Heeman, 2009). A major limitation was an
overly simplified user model. We used two user
types, expert and novice, who had fixed bids. Ex-
perts always bid high and had complete domain
knowledge, and the novices always bid low and
had incomplete domain knowledge. The system,
using all five bid types, was always able to out bid
and under bid the simulated users. Among other
things, this situation gives the system complete
control of the turn, which is at odds with the nego-
tiative nature of IDTB. The present contribution is
a more realistic and mature implementation.
</bodyText>
<footnote confidence="0.5816895">
6We experimented with a few different KR decision
strategies, and chose the one that performed the best.
</footnote>
<sectionHeader confidence="0.951789" genericHeader="method">
6 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.999532380952381">
We now evaluate the IDTB approach by compar-
ing it against the two competing models: Single-
Utterance and Keep-Or-Release. The three turn-
taking approaches are trained and tested in four
user conditions: novice, intermediate, expert, and
combined. In the combined condition, one of the
three user types is randomly selected for each dia-
logue. We train ten policies for each condition and
turn-taking approach. Policies are trained using Q-
learning, and c−greedy search for 10000 epochs
(1 epoch = 100 dialogues, after which the Q-scores
are updated) with c = 0.2. Each policy is then
ran over 10000 test dialogues with no exploration
(c = 0), and the mean dialogue cost for that pol-
icy is determined. The 10 separate policy values
are then averaged to create the mean policy cost.
The mean policy cost between the turn-taking ap-
proaches and user conditions are shown in Table 3.
Lower numbers are indicative of shorter dialogues,
since the system learns to successfully complete
the task in all cases.
</bodyText>
<tableCaption confidence="0.972555">
Table 3: Mean Policy Cost for Model and User
condition7
</tableCaption>
<table confidence="0.8708302">
Model Novice Int. Expert Combined
SU 7.61 7.09 6.43 7.05
KR 6.00 6.35 4.46 6.01
IDTB 6.09 5.77 4.35 5.52
Single User Conditions: Single user conditions
</table>
<bodyText confidence="0.999340722222222">
show how well each turn-taking approach can op-
timize its behavior for specific user populations
and handle slight differences found in those pop-
ulations. Table 3 shows that the mean policy cost
of the SU model is higher than the other two mod-
els which indicates longer dialogues on average.
Since the SU system must respond to every user
utterance and cannot learn a turn-taking strategy
to utilize user knowledge, the dialogues are neces-
sarily longer. For example, in the expert condition
the best possible dialogue for a SU interaction will
have a cost of five (three user utterances for each
slot, two system utterances in response). This cost
is in contrast to the best expert dialogue cost of
three (three user utterances) for KR and IDTB in-
teractions.
The IDTB turn-taking approach outperforms
the KR design in all single user conditions ex-
</bodyText>
<footnote confidence="0.8692">
7SD between policies &lt; 0.04
</footnote>
<page confidence="0.995648">
182
</page>
<bodyText confidence="0.999617428571429">
cept for novice (6.09 vs. 6.00). In this condi-
tion, the KR system takes the turn first, informs
the available fillers for each slot, and then releases
the turn. The user can then inform its filler eas-
ily. The IDTB system attempts a similar dialogue
strategy by using highest bids but sometimes loses
the turn when users also bid highest. If the user
uses the turn to query or inform an unavailable
filler the dialogue grows longer. However, this is
quite rare as shown by small difference in perfor-
mance between the two models. In all other single
user conditions, the IDTB approach has shorter di-
alogues than the KR approach (5.77 and 4.35 vs.
6.35 and 4.46). A detailed explanation of IDTB’s
performance will be given in Section 6.1.
Combined User Condition: We next measure
performance on the combined condition that
mixes all three user types. This condition is more
realistic than the other three, as it better mimics
how a system will be used in actual practice. The
IDTB approach (mean policy cost = 5.52) outper-
forms the KR (mean policy cost = 6.01) and SU
(mean policy cost = 7.05) approaches. We also
observe that KR outperforms SU. These results
suggest that the more a turn-taking design can be
flexible and negotiative, the more efficient the dia-
logues can be.
Exploiting User bidding differences: It fol-
lows that IDTB’s performance stems from its ne-
gotiative turn transitions. These transitions are dis-
tinctly different than KR transitions in that there is
information inherent in the users bids. A user that
has a stronger belief strength is more likely to be
have a higher bid and inform an available filler.
Policy analysis shows that the IDTB system takes
advantage of this information by using moderate
bids —neither highest nor lowest bids— to filter
users based on their turn behavior. The distribu-
tion of bids used over the ten learned policies is
shown in Table 4. The initial position refers to
the first bid of the dialogue; final position, the last
bid of the dialogue; and medial position, all other
bids. Notice that the system uses either the low or
mid bids as its initial policy and that 67.2% of di-
alogue medial bids are moderate. These distribu-
tions show that the system has learned to use the
entire bid range to filter the users, and is not seek-
ing to win or lose the turn outright. This behavior
is impossible in the KR approach.
</bodyText>
<tableCaption confidence="0.958292">
Table 4: Bid percentages over ten policies in the
</tableCaption>
<table confidence="0.8985642">
Combined User condition for IDTB
Position H-est High Mid Low L-est
Initial 0.0 0.0 70.0 30.0 0.0
Medial 20.5 19.4 24.5 23.3 12.3
Final 49.5 41.0 9.5 0.0 0.0
</table>
<subsectionHeader confidence="0.986732">
6.1 IDTB Performance:
</subsectionHeader>
<bodyText confidence="0.999527909090909">
In our domain, performance is measured by dia-
logue length and solution quality. However, since
solution quality never affects the dialogue cost for
a trained system, dialogue length is the only com-
ponent influencing the mean policy cost.
The primary cause of longer dialogues are un-
available filler inform and query (UFI–Q) utter-
ances by the user, which are easily identified.
These utterances lengthen the dialogue since the
system must inform the user of the available fillers
(the user would otherwise not know that the filler
was unavailable) and then the user must then in-
form the system of its second choice. The mean
number of UFI–Q utterance for each dialogue over
the ten learned policies are shown for all user con-
ditions in Table 5. Notice that these numbers are
inversely related to performance: the more UFI–
Q utterances, the worse the performance. For ex-
ample, in the combined condition the IDTB users
perform 0.38 UFI–Q utterances per dialogue (u/d)
compared to the 0.94 UFI–Q u/d for KR users.
While a KR user will release the turn if its planned
</bodyText>
<tableCaption confidence="0.942556">
Table 5: Mean number of UFI–Q utterances over
policies
</tableCaption>
<table confidence="0.998831">
Model Novice Int. Expert Combined
KR 0.0 1.15 0.53 0.94
IDTB 0.1 0.33 0.39 0.38
</table>
<bodyText confidence="0.999729307692308">
utterance has a weak belief, it may select that weak
utterance when first getting the turn (either after a
system utterance or at the start of the dialogue).
This may lead to a UFI–Q utterance. The IDTB
system, however, will outbid the same user, result-
ing in a shorter dialogue. This situation is shown
in Tables 6 and 7. The dialogue is the same un-
til utterance 3, where the IDTB system wins the
turn with a mid bid over the user’s low bid. In the
KR environment however, the user gets the turn
and performs an unavailable filler inform, which
the system must react to. This is an instance of
the second deficiency of the KR approach, where
</bodyText>
<page confidence="0.999396">
183
</page>
<tableCaption confidence="0.979377">
Table 6: Sample IDTB dialogue in Combined User
condition; Cost=6
</tableCaption>
<note confidence="0.726922">
Sys Usr Spkr Utt
1 low mid U: inform burger b1
2 h-est low S: inform burger have b3
3 mid low S: inform side have s1
4 mid h-est U: inform burger b3
5 mid high U: inform drink d1
6 l-est h-est U: inform side s1
7 high mid S: bye
</note>
<tableCaption confidence="0.9774465">
Table 7: Sample KR dialogue in Combined User
condition; Cost=7
</tableCaption>
<table confidence="0.965456">
Agent Utt Turn-Action
1 U: inform burger b1 Release
2 S: inform burger have b3 Release
3 U: inform side s1 Keep
4 U: inform drink d1 Keep
5 U: inform burger b3 Release
</table>
<sectionHeader confidence="0.729396" genericHeader="method">
6 S: inform side have s2 Release
7 U: inform side s2 Release
8 S: bye
</sectionHeader>
<bodyText confidence="0.999959945945946">
the speaking system should not have released the
turn. The user has the same belief in both scenar-
ios, but the negotiative nature of IDTB enables a
shorter dialogues. In short, the IDTB system can
win the turn when it should have it, but the KR
system cannot.
A lesser cause of longer dialogues is an instance
of the first deficiency of the KR systems; the lis-
tening user cannot get the turn when it should have
it. Usually, this situation presents itself when the
user releases the turn, having randomly chosen the
weaker of the two unfilled slots. The system then
has the turn for more than one utterance, inform-
ing the available fillers for two slots. However,
the user already had a strong belief and available
top filler for one of those slots, and the system
has increased the dialogue length unnecessarily. In
the combined condition, the KR system produces
0.06 unnecessary informs per dialogue, whereas
the IDTB system produces 0.045 per dialogue.
The novice and intermediate conditions mirror this
(IDTB: 0.009, 0.076 ; KR: 0.019, 0.096 respect-
fully), but the expert condition does not (IDTB:
0.011, KR: 0.0014). In this case, the IDTB system
wins the turn initially using a low bid and informs
one of the strong slots, whereas the expert user ini-
tiates the dialogue for the KR environment and un-
necessary informs are rarer. In general, however,
the KR approach has more unnecessary informs
since the KR system can only infer that one of the
user’s beliefs was probably weak, otherwise the
user would not have released the turn. The IDTB
system handles this situation by using a high bid,
allowing the user to outbid the system as its con-
tribution is more important. In other words, the
IDTB user can win the turn when it should have it,
but the KR user cannot.
</bodyText>
<sectionHeader confidence="0.998372" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999993352941176">
This paper presented the Importance-Driven Turn-
Bidding model of turn-taking. The IDTB model is
motivated by turn-conflict studies showing that the
interest in holding the turn influences conversant
turn-cues. A computational prototype using Re-
inforcement Learning to choose appropriate turn-
bids performs better than the standard KR and SU
approaches in an artificial collaborative dialogue
domain. In short, the Importance-Driven Turn-
Bidding model provides a negotiative turn-taking
framework that supports mixed-initiative interac-
tions.
In the previous section, we showed that the KR
approach is deficient for two reasons: the speak-
ing system might not keep the turn when it should
have, and might release the turn when it should
not have. This is driven by KR’s speaker-centric
nature; the speaker has no way of judging the
potential contribution of the listener. The IDTB
approach however, due to its negotiative quality,
does not have this problem.
Our performance differences arise from situa-
tions when the system is the speaker and the user
is the listener. The IDTB model also excels in the
opposite situation, when the system is the listener
and the user is the speaker, though our domain is
not sophisticated enough for this situation to oc-
cur. In the future we hope to develop a domain
with more realistic speech acts and a more diffi-
cult dialogue task that will, among other things,
highlight this situation. We also plan on imple-
menting a fully functional IDTB system, using an
incremental processing architecture that not only
detects, but generates, a wide array of turn-cues.
</bodyText>
<sectionHeader confidence="0.998384" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.923711666666667">
We gratefully acknowledge funding from the
National Science Foundation under grant IIS-
0713698.
</bodyText>
<page confidence="0.995894">
184
</page>
<bodyText confidence="0.9910365">
S. Larsson and D. Traum. 2000. Information state and
dialogue managment in the trindi dialogue move en-
gine toolkit. Natural Language Engineering, 6:323–
340.
</bodyText>
<sectionHeader confidence="0.893471" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988016099009901">
J.E Allen, C.I. Guinn, and Horvitz E. 1999. Mixed-
initiative interaction. IEEE Intelligent Systems,
14(5):14–23.
Jennifer Chu-Carroll and Sandra Carberry. 1995. Re-
sponse generation in collaborative negotiation. In
Proceedings of the 33rd annual meeting on Asso-
ciation for Computational Linguistics, pages 136–
143, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can i finish? learning when to respond to incre-
mental interpretation results in interactive dialogue.
In Proceedings of the SIGDIAL 2009 Conference,
pages 11–20, London, UK, September. Association
for Computational Linguistics.
S.J. Duncan and G. Niederehe. 1974. On signalling
that it’s your turn to speak. Journal of Experimental
Social Psychology, 10:234–247.
S.J. Duncan. 1972. Some signals and rules for taking
speaking turns in conversations. Journal of Person-
ality and Social Psychology, 23:283–292.
M. English and Peter A. Heeman. 2005. Learning
mixed initiative dialog strategies by using reinforce-
ment learning on both conversants. In Proceedings
of HLT/EMNLP, pages 1011–1018.
G. Ferguson, J. Allen, and B. Miller. 1996. TRAINS-
95: Towards a mixed-initiative planning assistant.
In Proceedings of the Third Conference on Artificial
Intelligence Planning Systems (AIPS-96), pages 70–
77.
A. Gravano and J. Hirschberg. 2009. Turn-yielding
cues in task-oriented dialogue. In Proceedings of the
SIGDIAL 2009 Conference: The 10th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 253–261. Association for Compu-
tational Linguistics.
C.I. Guinn. 1996. Mechanisms for mixed-initiative
human-computer collaborative discourse. In Pro-
ceedings of the 34th annual meeting on Association
for Computational Linguistics, pages 278–285. As-
sociation for Computational Linguistics.
P.A. Heeman. 2007. Combining reinforcement learn-
ing with information-state update rules. In Pro-
ceedings of the Annual Conference of the North
American Association for Computational Linguis-
tics, pages 268–275, Rochester, NY.
Gudny Ragna Jonsdottir, Kristinn R. Thorisson, and
Eric Nivel. 2008. Learning smooth, human-like
turntaking in realtime dialogue. In IVA ’08: Pro-
ceedings of the 8th international conference on In-
telligent VirtualAgents, pages 162–175, Berlin, Hei-
delberg. Springer-Verlag.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A
stochastic model of human-machine interaction for
learning dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1):11 – 23.
A. Raux and M. Eskenazi. 2009. A finite-state turn-
taking model for spoken dialog systems. In Pro-
ceedings of HLT/NAACL, pages 629–637. Associa-
tion for Computational Linguistics.
H. Sacks, E.A. Schegloff, and G. Jefferson. 1974. A
simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696–735.
R. Sato, R. Higashinaka, M. Tamoto, M. Nakano, and
K. Aikawa. 2002. Learning decision trees to de-
termine turn-taking by spoken dialogue systems. In
ICSLP, pages 861–864, Denver, CO.
E.A. Schegloff. 2000). Overlapping talk and the orga-
nization of turn-taking for conversation. Language
in Society, 29:1 – 63.
E. O. Selfridge and Peter A. Heeman. 2009. A bidding
approach to turn-taking. In 1st International Work-
shop on Spoken Dialogue Systems.
G. Skantze and D. Schlangen. 2009. Incremental di-
alogue processing in a micro-domain. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 745–753. Association for Computational Lin-
guistics.
N. Str6m and S. Seneff. 2000. Intelligent barge-in in
conversational systems. In Sixth International Con-
ference on Spoken Language Processing. Citeseer.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
S. Sutton, D. Novick, R. Cole, P. Vermeulen, J. de Vil-
liers, J. Schalkwyk, and M. Fanty. 1996. Build-
ing 10,000 spoken-dialogue systems. In ICSLP,
Philadelphia, Oct.
M. Walker and S. Whittaker. 1990. Mixed initiative
in dialoge: an investigation into discourse segmen-
tation. In Proceedings of the 28th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 70–76.
M. Walker, D. Hindle, J. Fromer, G.D. Fabbrizio, and
C. Mestel. 1997. Evaluating competing agent
strategies for a voice email agent. In Fifth European
Conference on Speech Communication and Technol-
ogy.
Fan Yang and Peter A. Heeman. 2010. Initiative con-
flicts in task-oriented dialogue”. Computer Speech
Language, 24(2):175 – 189.
</reference>
<page confidence="0.998847">
185
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.982147">
<title confidence="0.999948">Importance-Driven Turn-Bidding for Spoken Dialogue Systems</title>
<author confidence="0.999952">Ethan O Selfridge</author>
<author confidence="0.999952">Peter A Heeman</author>
<affiliation confidence="0.998835">Center for Spoken Language Understanding Oregon Health &amp; Science University</affiliation>
<address confidence="0.999825">20000 NW Walker Rd., Beaverton, OR, 97006</address>
<email confidence="0.999759">selfridg@ohsu.edu,heemanp@ohsu.edu</email>
<abstract confidence="0.999218">Current turn-taking approaches for spoken dialogue systems rely on the speaker releasing the turn before the other can take it. This reliance results in restricted interactions that can lead to inefficient dialogues. In this paper we present a model we reto as Turn-Bidding that treats turn-taking as a negotiative process. Each conversant bids for the turn based on the importance of the intended utterance, and Reinforcement Learning is used to indirectly learn this parameter. We find that Importance-Driven Turn-Bidding performs better than two current turntaking approaches in an artificial collaborative slot-filling domain. The negotiative nature of this model creates efficient dialogues, and supports the improvement of mixed-initiative interaction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J E Allen</author>
<author>C I Guinn</author>
<author>E Horvitz</author>
</authors>
<title>Mixedinitiative interaction.</title>
<date>1999</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>14</volume>
<issue>5</issue>
<contexts>
<context position="1392" citStr="Allen et al., 1999" startWordPosition="196" endWordPosition="199">t Importance-Driven Turn-Bidding performs better than two current turntaking approaches in an artificial collaborative slot-filling domain. The negotiative nature of this model creates efficient dialogues, and supports the improvement of mixed-initiative interaction. 1 Introduction As spoken dialogue systems are designed to perform ever more elaborate tasks, the need for mixed-initiative interaction necessarily grows. Mixed-initiative interaction, where agents (both artificial and human) may freely contribute to reach a solution efficiently, has long been a focus of dialogue systems research (Allen et al., 1999; Guinn, 1996). Simple slot-filling tasks might not require the flexible environment that mixedinitiative interaction brings but those of greater complexity, such as collaborative task completion or long-term planning, certainly do (Ferguson et al., 1996). However, translating this interaction into working systems has proved problematic (Walker et al., 1997), in part to issues surrounding turn-taking: the transition from one speaker to another. Many computational turn-taking approaches seek to minimize silence and utterance overlap during transitions. This leads to the speaker controlling the </context>
</contexts>
<marker>Allen, Guinn, Horvitz, 1999</marker>
<rawString>J.E Allen, C.I. Guinn, and Horvitz E. 1999. Mixedinitiative interaction. IEEE Intelligent Systems, 14(5):14–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Sandra Carberry</author>
</authors>
<title>Response generation in collaborative negotiation.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>136--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="18792" citStr="Chu-Carroll and Carberry, 1995" startWordPosition="3066" endWordPosition="3069"> 5.2 User Types We define three different types of users — Experts, Novices, and Intermediates. User types differ probabilistically on two dimensions: slot knowledge, and slot belief strength. We define experts to have a 90 percent chance of having adequate filler knowledge, intermediates a 50 percent chance, and novices a 10 percent chance. These probabilities are independent between slots. Slot belief strength represents the user’s confidence that it has adequate domain knowledge for the slot (i.e. the top choice for that slot is available). It is either a strong, warranted, or weak belief (Chu-Carroll and Carberry, 1995). The intuition is that experts should know when their top choice is available, and novices should know that they do not know the domain well. Initial slot belief strength is dependent on user type and whether their filler knowledge is adequate (their initial top choice is available). Experts with adequate filler knowledge have a 70, 20, and 10 percent chance of having Strong, Warranted, and Weak beliefs respectfully. Similarly, intermediates with adequate knowledge have a 50, 25, and 25 percent chance of the respective belief strengths. When these user types have inadequate filler knowledge t</context>
</contexts>
<marker>Chu-Carroll, Carberry, 1995</marker>
<rawString>Jennifer Chu-Carroll and Sandra Carberry. 1995. Response generation in collaborative negotiation. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 136– 143, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David DeVault</author>
<author>Kenji Sagae</author>
<author>David Traum</author>
</authors>
<title>Can i finish? learning when to respond to incremental interpretation results in interactive dialogue.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL 2009 Conference,</booktitle>
<pages>11--20</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>London, UK,</location>
<marker>DeVault, Sagae, Traum, 2009</marker>
<rawString>David DeVault, Kenji Sagae, and David Traum. 2009. Can i finish? learning when to respond to incremental interpretation results in interactive dialogue. In Proceedings of the SIGDIAL 2009 Conference, pages 11–20, London, UK, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Duncan</author>
<author>G Niederehe</author>
</authors>
<title>On signalling that it’s your turn to speak.</title>
<date>1974</date>
<journal>Journal of Experimental Social Psychology,</journal>
<pages>10--234</pages>
<contexts>
<context position="2928" citStr="Duncan and Niederehe, 1974" startWordPosition="431" endWordPosition="434">rrent approaches does not enable mixed-initiative interaction and results in inefficient dialogues. Primarily, these approaches have been motivated by smooth transitions reported in the human turn-taking studies of Sacks et al. (1974) among others. Sacks et al. also acknowledge the negotiative nature of turn-taking, stating that the “the turn as unit is interactively determined”(p. 727). Other studies have supported this, suggesting that humans negotiate the turn assignment through the use of cues and that these cues are motivated by the importance of what the conversant wishes to contribute (Duncan and Niederehe, 1974; Yang and Heeman, 2010; Schegloff, 2000). Given this, any dialogue system hoping to interact with humans efficiently and naturally should have a negotiative and importance-driven quality to its turn-taking protocol. We believe that, by focusing on the rationale of human turn-taking behavior, a more effective turn-taking system may be achieved. We propose the Importance-Driven Turn-Bidding (IDTB) model where conversants bid for the turn based on the importance of their utterance. We use Reinforcement Learning to map a given situation to the optimal utterance and bidding behavior. By allowing c</context>
</contexts>
<marker>Duncan, Niederehe, 1974</marker>
<rawString>S.J. Duncan and G. Niederehe. 1974. On signalling that it’s your turn to speak. Journal of Experimental Social Psychology, 10:234–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Duncan</author>
</authors>
<title>Some signals and rules for taking speaking turns in conversations.</title>
<date>1972</date>
<journal>Journal of Personality and Social Psychology,</journal>
<pages>23--283</pages>
<contexts>
<context position="5267" citStr="Duncan, 1972" startWordPosition="792" endWordPosition="793">ased it. The simplest of these approaches only allows a single utterance per turn, after which the turn necessarily transitions to the next speaker. This Single-Utterance (SU) model has been extended to allow the speaker to keep the turn for multiple utterances: the Keep-Or-Release (KR) approach. Since the KR approach gives the speaker sole control of the turn, it is overwhelmingly speaker-centric, and so necessarily unnegotiative. This restriction is meant to encourage smooth turn-transitions, and is inspired by the order, smoothness, and predictability reported in human turn-taking studies (Duncan, 1972; Sacks et al., 1974). Systems using the KR approach differ on how they detect the user’s release-turn. Turn releases are commonly identified in two ways: either using a silence-threshold (Sutton et al., 1996), or the predictive nature of turn endings (Sacks et al., 1974) and the cues associated with them (e.g. Gravano and Hirschberg, 2009). Raux and Eskenazi (2009) used decision theory with lexical cues to predict appropriate places to take the turn. Similarly, Jonsdottir, Thorisson, and Nivel (2008) used Reinforcement Learning to reduce silences between turns and minimize overlap between utt</context>
</contexts>
<marker>Duncan, 1972</marker>
<rawString>S.J. Duncan. 1972. Some signals and rules for taking speaking turns in conversations. Journal of Personality and Social Psychology, 23:283–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M English</author>
<author>Peter A Heeman</author>
</authors>
<title>Learning mixed initiative dialog strategies by using reinforcement learning on both conversants.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>1011--1018</pages>
<contexts>
<context position="22429" citStr="English and Heeman (2005)" startWordPosition="3695" endWordPosition="3698">probably not representative of reality. 181 turn-taking mechanism. After a speaker makes a single utterance the turn transitions to the listener. Since the turn transitions after every utterance the system must only choose appropriate utterances, not turn-taking behavior. Similarly, user agents do not have any turn-taking behavior and slot beliefs are only used to choose between a query and an inform. Keep-Or-Release Model: The Keep-OrRelease (KR) model, as described in Section 2, allows the speaker to either keep the turn to make multiple utterances or release it. Taking the same approach as English and Heeman (2005), the system learns to keep or release the turn after each utterance that it makes. We also use RL to determine which conversant should begin the dialogue. While the use of RL imparts some importance onto the turn-taking behavior, it is not influencing whether the system gets the turn when it did not already have it. This is an crucial distinction between KR and IDTB. IDTB allows the conversants to negotiate the turn using turn-bids motivated by importance, whereas in KR only the speaker determines when the turn can transition. Users in the KR environment choose whether to keep or release the </context>
</contexts>
<marker>English, Heeman, 2005</marker>
<rawString>M. English and Peter A. Heeman. 2005. Learning mixed initiative dialog strategies by using reinforcement learning on both conversants. In Proceedings of HLT/EMNLP, pages 1011–1018.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ferguson</author>
<author>J Allen</author>
<author>B Miller</author>
</authors>
<title>TRAINS95: Towards a mixed-initiative planning assistant.</title>
<date>1996</date>
<booktitle>In Proceedings of the Third Conference on Artificial Intelligence Planning Systems (AIPS-96),</booktitle>
<pages>70--77</pages>
<contexts>
<context position="1647" citStr="Ferguson et al., 1996" startWordPosition="232" endWordPosition="236">ive interaction. 1 Introduction As spoken dialogue systems are designed to perform ever more elaborate tasks, the need for mixed-initiative interaction necessarily grows. Mixed-initiative interaction, where agents (both artificial and human) may freely contribute to reach a solution efficiently, has long been a focus of dialogue systems research (Allen et al., 1999; Guinn, 1996). Simple slot-filling tasks might not require the flexible environment that mixedinitiative interaction brings but those of greater complexity, such as collaborative task completion or long-term planning, certainly do (Ferguson et al., 1996). However, translating this interaction into working systems has proved problematic (Walker et al., 1997), in part to issues surrounding turn-taking: the transition from one speaker to another. Many computational turn-taking approaches seek to minimize silence and utterance overlap during transitions. This leads to the speaker controlling the turn transition. For example, systems using the Keep-Or-Release approach will not attempt to take the turn unless it is sure the user has released it. One problem with this approach is that the system might have important information to give but will be u</context>
</contexts>
<marker>Ferguson, Allen, Miller, 1996</marker>
<rawString>G. Ferguson, J. Allen, and B. Miller. 1996. TRAINS95: Towards a mixed-initiative planning assistant. In Proceedings of the Third Conference on Artificial Intelligence Planning Systems (AIPS-96), pages 70– 77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gravano</author>
<author>J Hirschberg</author>
</authors>
<title>Turn-yielding cues in task-oriented dialogue.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>253--261</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5609" citStr="Gravano and Hirschberg, 2009" startWordPosition="846" endWordPosition="850"> the speaker sole control of the turn, it is overwhelmingly speaker-centric, and so necessarily unnegotiative. This restriction is meant to encourage smooth turn-transitions, and is inspired by the order, smoothness, and predictability reported in human turn-taking studies (Duncan, 1972; Sacks et al., 1974). Systems using the KR approach differ on how they detect the user’s release-turn. Turn releases are commonly identified in two ways: either using a silence-threshold (Sutton et al., 1996), or the predictive nature of turn endings (Sacks et al., 1974) and the cues associated with them (e.g. Gravano and Hirschberg, 2009). Raux and Eskenazi (2009) used decision theory with lexical cues to predict appropriate places to take the turn. Similarly, Jonsdottir, Thorisson, and Nivel (2008) used Reinforcement Learning to reduce silences between turns and minimize overlap between utterances by learning the specific turn-taking patterns of individual speakers. Skantze and Schlangan (2009) used incremental processing of speech and prosodic turn-cues to reduce the reaction time of the system, finding that that users rated this approach as more human-like than a baseline system. In our view, systems built using the KR turn</context>
</contexts>
<marker>Gravano, Hirschberg, 2009</marker>
<rawString>A. Gravano and J. Hirschberg. 2009. Turn-yielding cues in task-oriented dialogue. In Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 253–261. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C I Guinn</author>
</authors>
<title>Mechanisms for mixed-initiative human-computer collaborative discourse.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>278--285</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1406" citStr="Guinn, 1996" startWordPosition="200" endWordPosition="201">Turn-Bidding performs better than two current turntaking approaches in an artificial collaborative slot-filling domain. The negotiative nature of this model creates efficient dialogues, and supports the improvement of mixed-initiative interaction. 1 Introduction As spoken dialogue systems are designed to perform ever more elaborate tasks, the need for mixed-initiative interaction necessarily grows. Mixed-initiative interaction, where agents (both artificial and human) may freely contribute to reach a solution efficiently, has long been a focus of dialogue systems research (Allen et al., 1999; Guinn, 1996). Simple slot-filling tasks might not require the flexible environment that mixedinitiative interaction brings but those of greater complexity, such as collaborative task completion or long-term planning, certainly do (Ferguson et al., 1996). However, translating this interaction into working systems has proved problematic (Walker et al., 1997), in part to issues surrounding turn-taking: the transition from one speaker to another. Many computational turn-taking approaches seek to minimize silence and utterance overlap during transitions. This leads to the speaker controlling the turn transitio</context>
</contexts>
<marker>Guinn, 1996</marker>
<rawString>C.I. Guinn. 1996. Mechanisms for mixed-initiative human-computer collaborative discourse. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 278–285. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Heeman</author>
</authors>
<title>Combining reinforcement learning with information-state update rules.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Conference of the North American Association for Computational Linguistics,</booktitle>
<pages>268--275</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="10679" citStr="Heeman, 2007" startWordPosition="1671" endWordPosition="1672">bination for a given conversational situation. 4 Information State Update and Reinforcement Learning We build our dialogue system using the Information State Update approach (Larsson and Traum, 2000) and use Reinforcement Learning for action selection (Sutton and Barto, 1998). The system architecture consists of an Information State (IS) that represents the agent’s knowledge and is updated using a variety of rules. The IS also uses rules to propose possible actions. A condensed and compressed subset of the IS — the Reinforcement Learning State — is used to learn which proposed action to take (Heeman, 2007). It has been shown that using RL to learn dialogue polices is generally more effective than “hand crafted” di1Our work (present and future) is distinct from some recent work on user pauses (Sato et al., 2002) since we treat turn-taking as an integral piece of dialogue success. 2We gain an inherent flexibility in using RL since the reward can be computed by a wide array of components. This is consistent with the broad definition of importance. alogue policies since the learning algorithm may capture environmental dynamics that are unattended to by human designers (Levin et al., 2000). Reinforc</context>
</contexts>
<marker>Heeman, 2007</marker>
<rawString>P.A. Heeman. 2007. Combining reinforcement learning with information-state update rules. In Proceedings of the Annual Conference of the North American Association for Computational Linguistics, pages 268–275, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gudny Ragna Jonsdottir</author>
<author>Kristinn R Thorisson</author>
<author>Eric Nivel</author>
</authors>
<title>Learning smooth, human-like turntaking in realtime dialogue.</title>
<date>2008</date>
<booktitle>In IVA ’08: Proceedings of the 8th international conference on Intelligent VirtualAgents,</booktitle>
<pages>162--175</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Jonsdottir, Thorisson, Nivel, 2008</marker>
<rawString>Gudny Ragna Jonsdottir, Kristinn R. Thorisson, and Eric Nivel. 2008. Learning smooth, human-like turntaking in realtime dialogue. In IVA ’08: Proceedings of the 8th international conference on Intelligent VirtualAgents, pages 162–175, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
<author>W Eckert</author>
</authors>
<title>A stochastic model of human-machine interaction for learning dialog strategies.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing, 8(1):11 –</journal>
<volume>23</volume>
<contexts>
<context position="11269" citStr="Levin et al., 2000" startWordPosition="1770" endWordPosition="1773">action to take (Heeman, 2007). It has been shown that using RL to learn dialogue polices is generally more effective than “hand crafted” di1Our work (present and future) is distinct from some recent work on user pauses (Sato et al., 2002) since we treat turn-taking as an integral piece of dialogue success. 2We gain an inherent flexibility in using RL since the reward can be computed by a wide array of components. This is consistent with the broad definition of importance. alogue policies since the learning algorithm may capture environmental dynamics that are unattended to by human designers (Levin et al., 2000). Reinforcement Learning learns an optimal policy, a mapping between a state s and action a, where performing a in s leads to the lowest expected cost for the dialogue (we use minimum cost instead of maximum reward). An c-greedy search is used to estimate Q-scores, the expected cost of some state–action pair, where the system chooses a random action with c probability and the argminaQ(s, a) action with 1-c probability. For Q-learning, a popular RL algorithm and the one used here, c is commonly set at 0.2 (Sutton and Barto, 1998). Q-learning updates Q(s, a) based on the best action of the next </context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 2000</marker>
<rawString>E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochastic model of human-machine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing, 8(1):11 – 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>M Eskenazi</author>
</authors>
<title>A finite-state turntaking model for spoken dialog systems.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT/NAACL,</booktitle>
<pages>629--637</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5635" citStr="Raux and Eskenazi (2009)" startWordPosition="851" endWordPosition="854">e turn, it is overwhelmingly speaker-centric, and so necessarily unnegotiative. This restriction is meant to encourage smooth turn-transitions, and is inspired by the order, smoothness, and predictability reported in human turn-taking studies (Duncan, 1972; Sacks et al., 1974). Systems using the KR approach differ on how they detect the user’s release-turn. Turn releases are commonly identified in two ways: either using a silence-threshold (Sutton et al., 1996), or the predictive nature of turn endings (Sacks et al., 1974) and the cues associated with them (e.g. Gravano and Hirschberg, 2009). Raux and Eskenazi (2009) used decision theory with lexical cues to predict appropriate places to take the turn. Similarly, Jonsdottir, Thorisson, and Nivel (2008) used Reinforcement Learning to reduce silences between turns and minimize overlap between utterances by learning the specific turn-taking patterns of individual speakers. Skantze and Schlangan (2009) used incremental processing of speech and prosodic turn-cues to reduce the reaction time of the system, finding that that users rated this approach as more human-like than a baseline system. In our view, systems built using the KR turntaking approach suffer fro</context>
</contexts>
<marker>Raux, Eskenazi, 2009</marker>
<rawString>A. Raux and M. Eskenazi. 2009. A finite-state turntaking model for spoken dialog systems. In Proceedings of HLT/NAACL, pages 629–637. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sacks</author>
<author>E A Schegloff</author>
<author>G Jefferson</author>
</authors>
<title>A simplest systematics for the organization of turntaking for conversation.</title>
<date>1974</date>
<journal>Language,</journal>
<volume>50</volume>
<issue>4</issue>
<contexts>
<context position="2536" citStr="Sacks et al. (1974)" startWordPosition="370" endWordPosition="373">ance overlap during transitions. This leads to the speaker controlling the turn transition. For example, systems using the Keep-Or-Release approach will not attempt to take the turn unless it is sure the user has released it. One problem with this approach is that the system might have important information to give but will be unable to get the turn. The speaker-centric nature of current approaches does not enable mixed-initiative interaction and results in inefficient dialogues. Primarily, these approaches have been motivated by smooth transitions reported in the human turn-taking studies of Sacks et al. (1974) among others. Sacks et al. also acknowledge the negotiative nature of turn-taking, stating that the “the turn as unit is interactively determined”(p. 727). Other studies have supported this, suggesting that humans negotiate the turn assignment through the use of cues and that these cues are motivated by the importance of what the conversant wishes to contribute (Duncan and Niederehe, 1974; Yang and Heeman, 2010; Schegloff, 2000). Given this, any dialogue system hoping to interact with humans efficiently and naturally should have a negotiative and importance-driven quality to its turn-taking p</context>
<context position="5288" citStr="Sacks et al., 1974" startWordPosition="794" endWordPosition="797">implest of these approaches only allows a single utterance per turn, after which the turn necessarily transitions to the next speaker. This Single-Utterance (SU) model has been extended to allow the speaker to keep the turn for multiple utterances: the Keep-Or-Release (KR) approach. Since the KR approach gives the speaker sole control of the turn, it is overwhelmingly speaker-centric, and so necessarily unnegotiative. This restriction is meant to encourage smooth turn-transitions, and is inspired by the order, smoothness, and predictability reported in human turn-taking studies (Duncan, 1972; Sacks et al., 1974). Systems using the KR approach differ on how they detect the user’s release-turn. Turn releases are commonly identified in two ways: either using a silence-threshold (Sutton et al., 1996), or the predictive nature of turn endings (Sacks et al., 1974) and the cues associated with them (e.g. Gravano and Hirschberg, 2009). Raux and Eskenazi (2009) used decision theory with lexical cues to predict appropriate places to take the turn. Similarly, Jonsdottir, Thorisson, and Nivel (2008) used Reinforcement Learning to reduce silences between turns and minimize overlap between utterances by learning t</context>
</contexts>
<marker>Sacks, Schegloff, Jefferson, 1974</marker>
<rawString>H. Sacks, E.A. Schegloff, and G. Jefferson. 1974. A simplest systematics for the organization of turntaking for conversation. Language, 50(4):696–735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sato</author>
<author>R Higashinaka</author>
<author>M Tamoto</author>
<author>M Nakano</author>
<author>K Aikawa</author>
</authors>
<title>Learning decision trees to determine turn-taking by spoken dialogue systems.</title>
<date>2002</date>
<booktitle>In ICSLP,</booktitle>
<pages>861--864</pages>
<location>Denver, CO.</location>
<contexts>
<context position="7030" citStr="Sato et al. (2002)" startWordPosition="1071" endWordPosition="1074">. In addition, the lack of negotiation forces the turn to necessarily transition to the listener after the speaker releases it. The possibility that the dialogue may be better served if the listener does not get the turn is not addressed by current approaches. Barge-in, which generally refers to allowing users to speak at any time (Strom and Seneff, 2000), has been the primary means to create a more flexible turn-taking environment. Yet, since barge-in recasts speaker-centric systems as usercentric, the system’s contributions continue to be limited. System barge-in has also been investigated. Sato et al. (2002) used decision trees to determine whether the system should take the turn or not when the user pauses. An incremental method by DeVault, Sagae, and Traum (2009) found possible points that a system could interrupt without loss of user meaning, but failed to supply a reasonable model as to when to use such information. Despite these advances, barge-in capable systems lack a negotiative turn-taking method, and continue to be deficient for reasons similar to those described above. 3 Importance-Driven Turn-Bidding (IDTB) We introduce the IDTB model to overcome the deficiencies of current approaches</context>
<context position="10888" citStr="Sato et al., 2002" startWordPosition="1707" endWordPosition="1710">use Reinforcement Learning for action selection (Sutton and Barto, 1998). The system architecture consists of an Information State (IS) that represents the agent’s knowledge and is updated using a variety of rules. The IS also uses rules to propose possible actions. A condensed and compressed subset of the IS — the Reinforcement Learning State — is used to learn which proposed action to take (Heeman, 2007). It has been shown that using RL to learn dialogue polices is generally more effective than “hand crafted” di1Our work (present and future) is distinct from some recent work on user pauses (Sato et al., 2002) since we treat turn-taking as an integral piece of dialogue success. 2We gain an inherent flexibility in using RL since the reward can be computed by a wide array of components. This is consistent with the broad definition of importance. alogue policies since the learning algorithm may capture environmental dynamics that are unattended to by human designers (Levin et al., 2000). Reinforcement Learning learns an optimal policy, a mapping between a state s and action a, where performing a in s leads to the lowest expected cost for the dialogue (we use minimum cost instead of maximum reward). An</context>
</contexts>
<marker>Sato, Higashinaka, Tamoto, Nakano, Aikawa, 2002</marker>
<rawString>R. Sato, R. Higashinaka, M. Tamoto, M. Nakano, and K. Aikawa. 2002. Learning decision trees to determine turn-taking by spoken dialogue systems. In ICSLP, pages 861–864, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Schegloff</author>
</authors>
<title>Overlapping talk and the organization of turn-taking for conversation.</title>
<date>2000</date>
<journal>Language in Society, 29:1 –</journal>
<volume>63</volume>
<contexts>
<context position="2969" citStr="Schegloff, 2000" startWordPosition="439" endWordPosition="440">nteraction and results in inefficient dialogues. Primarily, these approaches have been motivated by smooth transitions reported in the human turn-taking studies of Sacks et al. (1974) among others. Sacks et al. also acknowledge the negotiative nature of turn-taking, stating that the “the turn as unit is interactively determined”(p. 727). Other studies have supported this, suggesting that humans negotiate the turn assignment through the use of cues and that these cues are motivated by the importance of what the conversant wishes to contribute (Duncan and Niederehe, 1974; Yang and Heeman, 2010; Schegloff, 2000). Given this, any dialogue system hoping to interact with humans efficiently and naturally should have a negotiative and importance-driven quality to its turn-taking protocol. We believe that, by focusing on the rationale of human turn-taking behavior, a more effective turn-taking system may be achieved. We propose the Importance-Driven Turn-Bidding (IDTB) model where conversants bid for the turn based on the importance of their utterance. We use Reinforcement Learning to map a given situation to the optimal utterance and bidding behavior. By allowing conversants to bid for the turn, the IDTB </context>
</contexts>
<marker>Schegloff, 2000</marker>
<rawString>E.A. Schegloff. 2000). Overlapping talk and the organization of turn-taking for conversation. Language in Society, 29:1 – 63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E O Selfridge</author>
<author>Peter A Heeman</author>
</authors>
<title>A bidding approach to turn-taking.</title>
<date>2009</date>
<booktitle>In 1st International Workshop on Spoken Dialogue Systems.</booktitle>
<contexts>
<context position="23507" citStr="Selfridge and Heeman, 2009" startWordPosition="3877" endWordPosition="3880">mportance, whereas in KR only the speaker determines when the turn can transition. Users in the KR environment choose whether to keep or release the turn similarly to bid decisions.6 After a user performs an utterance, it chooses the slot that would be in the next utterance. A number, k, is generated from a Gaussian distribution using belief strength in the same manner as the IDTB users’ bids are chosen. If k &lt; 0.55 then the user keeps the turn, otherwise it releases it. 5.4 Preliminary Turn-Bidding System We described a preliminary turn-bidding system in earlier work presented at a workshop (Selfridge and Heeman, 2009). A major limitation was an overly simplified user model. We used two user types, expert and novice, who had fixed bids. Experts always bid high and had complete domain knowledge, and the novices always bid low and had incomplete domain knowledge. The system, using all five bid types, was always able to out bid and under bid the simulated users. Among other things, this situation gives the system complete control of the turn, which is at odds with the negotiative nature of IDTB. The present contribution is a more realistic and mature implementation. 6We experimented with a few different KR dec</context>
</contexts>
<marker>Selfridge, Heeman, 2009</marker>
<rawString>E. O. Selfridge and Peter A. Heeman. 2009. A bidding approach to turn-taking. In 1st International Workshop on Spoken Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Skantze</author>
<author>D Schlangen</author>
</authors>
<title>Incremental dialogue processing in a micro-domain.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>745--753</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Skantze, Schlangen, 2009</marker>
<rawString>G. Skantze and D. Schlangen. 2009. Incremental dialogue processing in a micro-domain. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 745–753. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Str6m</author>
<author>S Seneff</author>
</authors>
<title>Intelligent barge-in in conversational systems.</title>
<date>2000</date>
<booktitle>In Sixth International Conference on Spoken Language Processing. Citeseer.</booktitle>
<publisher>MIT Press.</publisher>
<marker>Str6m, Seneff, 2000</marker>
<rawString>N. Str6m and S. Seneff. 2000. Intelligent barge-in in conversational systems. In Sixth International Conference on Spoken Language Processing. Citeseer. R. Sutton and A. Barto. 1998. Reinforcement Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sutton</author>
<author>D Novick</author>
<author>R Cole</author>
<author>P Vermeulen</author>
<author>J de Villiers</author>
<author>J Schalkwyk</author>
<author>M Fanty</author>
</authors>
<title>Building 10,000 spoken-dialogue systems.</title>
<date>1996</date>
<booktitle>In ICSLP,</booktitle>
<location>Philadelphia,</location>
<marker>Sutton, Novick, Cole, Vermeulen, de Villiers, Schalkwyk, Fanty, 1996</marker>
<rawString>S. Sutton, D. Novick, R. Cole, P. Vermeulen, J. de Villiers, J. Schalkwyk, and M. Fanty. 1996. Building 10,000 spoken-dialogue systems. In ICSLP, Philadelphia, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>S Whittaker</author>
</authors>
<title>Mixed initiative in dialoge: an investigation into discourse segmentation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>70--76</pages>
<contexts>
<context position="8490" citStr="Walker and Whittaker (1990)" startWordPosition="1302" endWordPosition="1305"> may be broadly defined as how well the utterance leads to some predetermined conversational success, be it solely task completion or encompassing a myriad of social etiquette components. Importance-Driven Turn-Bidding is motivated by empirical studies of human turn-conflict resolution. Yang and Heeman (2010) found an increase of turn conflicts during tighter time constraints, which suggests that turn-taking is influenced by the importance of task completion. Schlegoff (2000) proposed that persistent utterance overlap was indicative of conversants having a strong interest in holding the turn. Walker and Whittaker (1990) show that people will interrupt to remedy some understanding discrepancy, which is certainly important to the conversation’s success. People communicate the importance of their utterance through turn-cues. Duncan and 178 Niederehe (1974) found that turn-cue strength was the best predictor of who won the turn, and this finding is consistent with the use of volume to win turns found by Yang and Heeman (2010). The IDTB model uses turn-cue strength to bid for the turn based on the importance of the utterance. Stronger turn-cues should be used when the intended utterance is important to the overal</context>
</contexts>
<marker>Walker, Whittaker, 1990</marker>
<rawString>M. Walker and S. Whittaker. 1990. Mixed initiative in dialoge: an investigation into discourse segmentation. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, pages 70–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>D Hindle</author>
<author>J Fromer</author>
<author>G D Fabbrizio</author>
<author>C Mestel</author>
</authors>
<title>Evaluating competing agent strategies for a voice email agent. In</title>
<date>1997</date>
<booktitle>Fifth European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="1752" citStr="Walker et al., 1997" startWordPosition="248" endWordPosition="251">s, the need for mixed-initiative interaction necessarily grows. Mixed-initiative interaction, where agents (both artificial and human) may freely contribute to reach a solution efficiently, has long been a focus of dialogue systems research (Allen et al., 1999; Guinn, 1996). Simple slot-filling tasks might not require the flexible environment that mixedinitiative interaction brings but those of greater complexity, such as collaborative task completion or long-term planning, certainly do (Ferguson et al., 1996). However, translating this interaction into working systems has proved problematic (Walker et al., 1997), in part to issues surrounding turn-taking: the transition from one speaker to another. Many computational turn-taking approaches seek to minimize silence and utterance overlap during transitions. This leads to the speaker controlling the turn transition. For example, systems using the Keep-Or-Release approach will not attempt to take the turn unless it is sure the user has released it. One problem with this approach is that the system might have important information to give but will be unable to get the turn. The speaker-centric nature of current approaches does not enable mixed-initiative </context>
</contexts>
<marker>Walker, Hindle, Fromer, Fabbrizio, Mestel, 1997</marker>
<rawString>M. Walker, D. Hindle, J. Fromer, G.D. Fabbrizio, and C. Mestel. 1997. Evaluating competing agent strategies for a voice email agent. In Fifth European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Yang</author>
<author>Peter A Heeman</author>
</authors>
<title>Initiative conflicts in task-oriented dialogue”.</title>
<date>2010</date>
<journal>Computer Speech Language,</journal>
<volume>24</volume>
<issue>2</issue>
<pages>189</pages>
<contexts>
<context position="2951" citStr="Yang and Heeman, 2010" startWordPosition="435" endWordPosition="438">able mixed-initiative interaction and results in inefficient dialogues. Primarily, these approaches have been motivated by smooth transitions reported in the human turn-taking studies of Sacks et al. (1974) among others. Sacks et al. also acknowledge the negotiative nature of turn-taking, stating that the “the turn as unit is interactively determined”(p. 727). Other studies have supported this, suggesting that humans negotiate the turn assignment through the use of cues and that these cues are motivated by the importance of what the conversant wishes to contribute (Duncan and Niederehe, 1974; Yang and Heeman, 2010; Schegloff, 2000). Given this, any dialogue system hoping to interact with humans efficiently and naturally should have a negotiative and importance-driven quality to its turn-taking protocol. We believe that, by focusing on the rationale of human turn-taking behavior, a more effective turn-taking system may be achieved. We propose the Importance-Driven Turn-Bidding (IDTB) model where conversants bid for the turn based on the importance of their utterance. We use Reinforcement Learning to map a given situation to the optimal utterance and bidding behavior. By allowing conversants to bid for t</context>
<context position="8173" citStr="Yang and Heeman (2010)" startWordPosition="1252" endWordPosition="1255"> We introduce the IDTB model to overcome the deficiencies of current approaches. The IDTB model has two foundational components: (1) The importance of speaking is the primary motivation behind turn-taking behavior, and (2) conversants use turncue strength to bid for the turn based on this importance. Importance may be broadly defined as how well the utterance leads to some predetermined conversational success, be it solely task completion or encompassing a myriad of social etiquette components. Importance-Driven Turn-Bidding is motivated by empirical studies of human turn-conflict resolution. Yang and Heeman (2010) found an increase of turn conflicts during tighter time constraints, which suggests that turn-taking is influenced by the importance of task completion. Schlegoff (2000) proposed that persistent utterance overlap was indicative of conversants having a strong interest in holding the turn. Walker and Whittaker (1990) show that people will interrupt to remedy some understanding discrepancy, which is certainly important to the conversation’s success. People communicate the importance of their utterance through turn-cues. Duncan and 178 Niederehe (1974) found that turn-cue strength was the best pr</context>
<context position="9468" citStr="Yang and Heeman (2010)" startWordPosition="1465" endWordPosition="1468">of volume to win turns found by Yang and Heeman (2010). The IDTB model uses turn-cue strength to bid for the turn based on the importance of the utterance. Stronger turn-cues should be used when the intended utterance is important to the overall success of the dialogue, and weaker ones when it is not. In the prototype described in Section 5, both the system and user agents bid for the turn after every utterance and the bids are conceptualized here as utterance onset: conversants should be quick to speak important utterances but slow with less important ones. This is relatively consistent with Yang and Heeman (2010). A mature version of our work will use cues in addition to utterance onset, such as those recently detailed in Gravano and Hirshberg (2009).1 A crucial element of our model is the judgment and quantization of utterance importance. We use Reinforcement Learning (RL) to determine importance by conceptualizing it as maximizing the reward over an entire dialogue. Whatever actions lead to a higher return may be thought of as more important than ones that do not.2 By using RL to learn both the utterance and bid behavior, the system can find an optimal pairing between them, and choose the best combi</context>
</contexts>
<marker>Yang, Heeman, 2010</marker>
<rawString>Fan Yang and Peter A. Heeman. 2010. Initiative conflicts in task-oriented dialogue”. Computer Speech Language, 24(2):175 – 189.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>