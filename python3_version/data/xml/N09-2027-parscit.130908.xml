<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005867">
<title confidence="0.973332">
The Importance of Sub-Utterance Prosody in Predicting Level of Certainty
</title>
<author confidence="0.974605">
Heather Pon-Barry
</author>
<affiliation confidence="0.8806665">
School of Engineering and Applied Sciences
Harvard University
</affiliation>
<address confidence="0.830413">
Cambridge, MA 02138, USA
</address>
<email confidence="0.999069">
ponbarry@eecs.harvard.edu
</email>
<author confidence="0.975766">
Stuart Shieber
</author>
<affiliation confidence="0.8792505">
School of Engineering and Applied Sciences
Harvard University
</affiliation>
<address confidence="0.83077">
Cambridge, MA 02138, USA
</address>
<email confidence="0.999146">
shieber@seas.harvard.edu
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999636875">
We present an experiment aimed at under-
standing how to optimally use acoustic and
prosodic information to predict a speaker’s
level of certainty. With a corpus of utterances
where we can isolate a single word or phrase
that is responsible for the speaker’s level of
certainty we use different sets of sub-utterance
prosodic features to train models for predict-
ing an utterance’s perceived level of certainty.
Our results suggest that using prosodic fea-
tures of the word or phrase responsible for the
level of certainty and of its surrounding con-
text improves the prediction accuracy without
increasing the total number of features when
compared to using only features taken from
the utterance as a whole.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944818181818">
Prosody is a fundamental part of human-to-human
spoken communication; it can affect the syntac-
tic and semantic interpretation of an utterance
(Hirschberg, 2003) and it can be used by speakers
to convey their emotional state. In recent years, re-
searchers have found prosodic features to be useful
in automatically detecting emotions such as annoy-
ance and frustration (Ang et al., 2002) and in dis-
tinguishing positive from negative emotional states
(Lee and Narayanan, 2005).
In this paper, we address the problem of predict-
ing the perceived level of certainty of a spoken ut-
terance. Specifically, we have a corpus of utter-
ances where it is possible to isolate a single word
or phrase responsible for the speaker’s level of cer-
tainty. With this corpus we investigate whether us-
ing prosodic features of the word or phrase causing
uncertainty and of its surrounding context improves
the prediction accuracy when compared to using fea-
tures taken only from the utterance as a whole.
This work goes beyond existing research by look-
ing at the predictive power of prosodic features ex-
tracted from salient sub-utterance segments. Pre-
vious work on uncertainty has examined the pre-
dictive power of utterance- and intonational phrase-
level prosodic features (Liscombe et al., 2005) as
well as the relative strengths of correlations between
level of certainty and sub-utterance prosodic fea-
tures (Pon-Barry, 2008). Our results suggest that
we can do a better job at predicting an utterance’s
perceived level of certainty by using prosodic fea-
tures extracted from the whole utterance plus ones
extracted from salient pieces of the utterance, with-
out increasing the total number of features, than by
using only features from the whole utterance.
This work is relevant to spoken language applica-
tions in which the system knows specific words or
phrases that are likely to cause uncertainty. For ex-
ample, this would occur in a tutorial dialogue system
when the speaker answers a direct question (Pon-
Barry et al., 2006; Forbes-Riley et al., 2008), or in
language (foreign or ESL) learning systems and lit-
eracy systems (Alwan et al., 2007) when new vocab-
ulary is being introduced.
</bodyText>
<sectionHeader confidence="0.993678" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.9505852">
Researchers have examined certainty in spoken lan-
guage using data from tutorial dialogue systems
(Liscombe et al., 2005) and data from an uncertainty
corpus (Pon-Barry, 2008).
Liscombe et al. (2005) trained a decision tree
</bodyText>
<page confidence="0.988036">
105
</page>
<note confidence="0.3590025">
Proceedings of NAACL HLT 2009: Short Papers, pages 105–108,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.995239944444445">
classifier on utterance-level and intonational phrase-
level prosodic features to distinguish between cer-
tain, uncertain, and neutral utterances. They
achieved 76% accuracy, compared to a 66% accu-
racy baseline (choosing the most common class).
We have collected a corpus of utterances spoken
under varying levels of certainty (Pon-Barry, 2008).
The utterances were elicited by giving adult native
English speakers a written sentence containing one
or more gaps, then displaying multiple options for
filling in the gaps and telling the speakers to read
the sentence aloud with the gaps filled in according
to domain-specific criteria. We elicited utterances
in two domains: (1) using public transportation in
Boston, and (2) choosing vocabulary words to com-
plete a sentence. An example is shown below.
Q: How can I get from Harvard to the Silver Line?
A: Take the red line to
</bodyText>
<listItem confidence="0.958368">
a. South Station
b. Downtown Crossing
</listItem>
<bodyText confidence="0.998850862068965">
The term ‘context’ refers to the fixed part of the re-
sponse (“Take the red line to ”, in this exam-
ple) and the term ‘target word’ refers to the word or
phrase chosen to fill in the gap.
The corpus contains 600 utterances from 20
speakers. Each utterance was annotated for level
of certainty, on a 5-point scale, by five human
judges who listened to the utterances out of context.
The average inter-annotator agreement (Kappa) was
0.45. We refer to the average of the five ratings as
the ‘perceived level of certainty’ (the quantity we at-
tempt to predict in this paper).
We computed correlations between perceived
level of certainty and prosodic features extracted
from the whole utterance, the context, and the tar-
get word. Pauses preceding the target word were
considered part of the target word; all segmenta-
tion was done manually. Because the speakers had
unlimited time to read over the context before see-
ing the target words, the target word is considered
to be the source of the speaker’s confidence or un-
certainty; it corresponds to the decision that the
speaker had to make. Our correlation results sug-
gest that while some prosodic cues to level of cer-
tainty were strongest in the whole utterance, others
were strongest in the context or the target word. In
this paper, we extend this past work by testing the
prediction accuracy of models trained on different
subsets of these prosodic features.
</bodyText>
<sectionHeader confidence="0.96115" genericHeader="method">
3 Prediction Experiments
</sectionHeader>
<bodyText confidence="0.999662378378378">
In our experiments we used 480 of the 600 utter-
ances in the corpus, those which contained exactly
one gap. (Some had two or three gaps.) We ex-
tracted the following 20 prosodic feature-types from
each whole utterance, context, and target word (a to-
tal of 60 features) using WaveSurfer1 and Praat2.
Pitch: minf0, maxf0, meanf0, stdevf0, rangef0, rel-
ative position minf0, relative position maxf0,
absolute slope (Hz), absolute slope (semitones)
Intensity: minRMS, maxRMS, meanRMS, stdev-
RMS, relative position minRMS, relative posi-
tion maxRMS
Temporal: total silence, percent silence, total dura-
tion, speaking duration, speaking rate
These features are comparable to those used in Lis-
combe et al.’s (2005) prediction experiments. The
pitch and intensity features were represented as
z-scores normalized by speaker; the temporal fea-
tures were not normalized.
Next, we created a ‘combination’ set of 20 fea-
tures based on our correlation results. Figure 1 il-
lustrates how the combination set was created: for
each prosodic feature-type (each row in the table) we
chose either the whole utterance feature, the context
feature, or the target word feature, whichever one
had the strongest correlation with perceived level of
certainty. The selected features (highlighted in Fig-
ure 1) are listed below.
Whole Utterance: total silence, total duration,
speaking duration, relative position maxf0, rel-
ative position maxRMS, absolute slope (Hz),
absolute slope (semitones)
Context: minf0, maxf0, meanf0, stdevf0, rangef0,
minRMS, maxRMS, meanRMS, relative posi-
tion minRMS
Target Word: percent silence, speaking rate, rela-
tive position minf0, stdevRMS
</bodyText>
<footnote confidence="0.999972">
1http://www.speech.kth.se/wavesurfer/
2http://www.fon.hum.uva.nl/praat/
</footnote>
<page confidence="0.984805">
106
</page>
<figure confidence="0.6373">
Feature-type Whole Utterance Context Target Word
speaking rate 0.090 0.014 0.136
</figure>
<figureCaption confidence="0.999062">
Figure 1: The Combination feature set (highlighted in ta-
</figureCaption>
<bodyText confidence="0.976766285714286">
ble) was produced by selecting either the whole utterance
feature, the context feature, or the target word feature
for each prosodic feature-type, whichever one was most
strongly correlated with perceived level of certainty.
To compare the prediction accuracies of different
subsets of features, we fit five linear regression mod-
els to the feature sets. The five subsets are: (A)
whole utterance features only, (B) target word fea-
tures only, (C) context features only, (D) all fea-
tures, and (E) the combination feature set. We di-
vided the data into 20 folds (one fold per speaker)
and performed a 20-fold cross-validation for each
set of features. Each experiment fits a model us-
ing data from 19 speakers and tests on the remain-
</bodyText>
<equation confidence="0.604005">
1
</equation>
<bodyText confidence="0.999646941176471">
ing speaker. Thus, when we test our models, we are
testing the ability to classify utterances of an unseen
speaker.
Table 1 shows the accuracies of the models
trained on the five subsets of features. The num-
bers reported are averages of the 20 cross-validation
accuracies. We report results for two cases: 5 pre-
diction classes and 3 prediction classes. We first
computed the prediction accuracy over five classes
(the regression output was rounded to the nearest
integer). Next, in order to compare our results to
those of Liscombe et al. (2005), we recoded the
5-class results into 3-class results, following Pon-
Barry (2008), in the way that maximized inter-
annotator agreement. The naive baseline numbers
are the accuracies that would be achieved by always
choosing the most common class.
</bodyText>
<sectionHeader confidence="0.999023" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.998129574468086">
Assuming that the target word is responsible for the
speaker’s level of certainty, it is not surprising that
the target word feature set (B) yields higher accura-
cies than the context feature set (C). It is also not sur-
prising that the set of all features (D) yields higher
accuracies than sets (A), (B), and (C).
The key comparison to notice is that the combi-
nation feature set (E), with only 20 features, yields
higher average accuracies than the utterance fea-
ture set (A): a difference of 6.42% for 5 classes
and 5.83% for 3 classes. This suggests that using a
combination of features from the context and target
word in addition to features from the whole utter-
ance leads to better prediction of the perceived level
of certainty than using features from only the whole
utterance.
One might argue that these differences are just
due to noise. To address this issue, we compared
the prediction accuracies of sets (A) and (E) per fold.
This is illustrated in Figure 2. Each fold in our cross-
validation corresponds to a different speaker, so the
folds are not identically distributed and we do not
expect each fold to yield the same prediction accu-
racy. That means that we should compare predic-
tions of the two feature sets within folds rather than
between folds. Figure 2 shows the correlations be-
tween the predicted and perceived levels of certainty
for the models trained on sets (A) and (E). The com-
bination set (E) predictions were more strongly cor-
related than whole utterance set (A) predictions in
16 out of 20 folds. This result supports our claim
that using a combination of features from the con-
text and target word in addition to features from the
whole utterance leads to better prediction of level of
certainty.
Our best prediction accuracy for the 3 class case,
74.79%, was slightly lower than the accuracy re-
ported by Liscombe et al. (2005), 76.42%. However,
our difference from the naive baseline was 18.54%
where Liscombe et al.’s was 10.42%. Liscombe et
al. randomly divided their data into training and test
sets, so it is unclear whether they tested on seen or
unseen speakers. Further, they ran one experiment
rather than a cross-validation, so their reported ac-
curacy may not be indicative of the entire data set.
We also trained support vector models on these
subsets of features. The main result was the same:
</bodyText>
<figure confidence="0.774102060606061">
rel. position max RMS −0.039
−0.028 −0.007
total silence
−0.643
−0.507 −0.495
−0.532
percent silence −0.455 −0.225
min f0 0.107 0.119 0.041
max f0 −0.073 −0.153 −0.045
mean f0 0.033 0.070 −0.004
stdev f0 −0.035 −0.047 −0.043
range f0 −0.128 −0.211 −0.075
rel. position min f0 0.042 0.022 0.046
rel. position max f0
abs. slope f0 (Hz)
abs. slope f0 (Semi)
0.015
0.275
0.160
0.008 0.001
0.180 0.191
0.147 0.002
min RMS 0.101 0.172 0.027
max RMS −0.091 −0.110 −0.034
mean RMS −0.012 0.039 −0.031
stdev RMS −0.002 −0.003 −0.019
rel. position min RMS 0.101 0.172 0.027
−0.592
−0.430
−0.502 −0.590
−0.390 −0.386
total duration
speaking duration
</figure>
<page confidence="0.988173">
107
</page>
<tableCaption confidence="0.998678">
Table 1: Average prediction accuracies for the linear regression models trained on five subsets of prosodic features.
</tableCaption>
<figure confidence="0.972157361111111">
2 0.5645441 0.7745844 1 0.11812999
20 09862684 069875998 1 01001314
The models trained on the Combination feature set and the All feature set perform better than the other three models
19 0.61302941 0.70460363 1 0.09157422
63 07506 0
in both the 3- and 5-class settings.
5 054426476 061711862
Feature Set Num Features Accuracy (5 classes) Accuracy (3 classes)
Naive Baseline
149 070962379039662
N/A 31.46% 56.25%
40 07 716
(A) Utterance 20 39.00% 68.96%
1425
922 0.73071498
(B) Target Word
649 0.313 0666
20 43.13% 68.96%
157 03938164 0175
(C) Context
851 061984036 0184
20 37.71% 67.50%
148 067762751 1948
(D) All
033 0.802581
60 48.54% 74.58%
(E) Combination 20 45.42% 74.79%
1
0.8
0.6
Combination
Utterance
0.2
0
0 2 4 6 8 10 12 14 16 18 20
Fold
</figure>
<figureCaption confidence="0.929531166666667">
Figure 2: Correlations with perceived level of certainty
per fold for the Combination (O) and the Utterance (X)
feature set predictions, sorted by the size of the difference.
In 16 of the 20 experiments, the correlation coefficients
for the Combination feature set are greater than those of
the Utterance feature set.
</figureCaption>
<bodyText confidence="0.999754571428571">
the set of all features (D) and the combination set
(E) had better prediction accuracies than the utter-
ance feature set (A). In addition, the combination set
(E) had the best prediction accuracies (of all models)
in both the 3- and 5-class settings. The raw accura-
cies were approximately 5% lower than those of the
linear regression models.
</bodyText>
<sectionHeader confidence="0.993531" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999956363636364">
The results of our experiments suggest a better pre-
dictive model of level of certainty for systems where
words or phrases likely to cause uncertainty are
known ahead of time. Without increasing the total
number of features, combining select prosodic fea-
tures from the target word, the surrounding context
and the whole utterance leads to better prediction of
level of certainty than using features from the whole
utterance only. In the near future, we plan to exper-
iment with prediction models of the speaker’s self-
reported level of certainty.
</bodyText>
<sectionHeader confidence="0.998944" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9814675">
This work was supported by a National Defense Sci-
ence and Engineering Graduate Fellowship.
</bodyText>
<sectionHeader confidence="0.998951" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99966296969697">
Abeer Alwan, Yijian Bai, Matthew Black, et al. 2007. A
system for technology based assessment of language
and literacy in young children: the role of multiple in-
formation sources. Proc. of IEEE International Work-
shop on Multimedia Signal Processing, pp. 26–30,
Chania, Greece.
Jeremy Ang, Rajdip Dhillon, Ashley Krupski, et al.
2002. Prosody-based automatic detection of annoy-
ance and frustration in human-computer dialog. Proc.
of ICSLP 2002, pp. 2037–2040, Denver, CO.
Kate Forbes-Riley, Diane Litman, and Mihai Rotaru.
2008. Responding to student uncertainty during com-
puter tutoring: a preliminary evaluation. Proc. of the
9th International Conference on Intelligent Tutoring
Systems, Montreal, Canada.
Julia Hirschberg. 2003. Intonation and pragmatics. In
L. Horn and G. Ward (ed.), Handbook of Pragmatics,
Blackwell.
Chul Min Lee and Shrikanth Narayanan. 2005. Towards
detecting emotions in spoken dialogs. IEEE Transac-
tions on Speech and Audio Processing, 13(2):293–303.
Jackson Liscombe, Julia Hirschberg, and Jennifer Ven-
ditti. 2005. Detecting certainness in spoken tutorial
dialogues. Proceedings of Eurospeech 2005, Lisbon,
Portugal.
Heather Pon-Barry, Karl Schultz, Elizabeth Bratt, Brady
Clark, and Stanley Peters. 2006. Responding to stu-
dent uncertainty in spoken tutorial dialogue systems.
International Journal of Artificial Intelligence in Edu-
cation 16:171-194.
Heather Pon-Barry. 2008. Prosodic manifestations of
confidence and uncertainty in spoken language. Proc.
of Interspeech 2008, pp. 74–77, Brisbane, Australia.
</reference>
<figure confidence="0.8282815">
Correlation Coeff (R)
0.4
</figure>
<page confidence="0.9664">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.320960">
<title confidence="0.999398">The Importance of Sub-Utterance Prosody in Predicting Level of Certainty</title>
<author confidence="0.997198">Heather</author>
<affiliation confidence="0.802743">School of Engineering and Applied Harvard</affiliation>
<address confidence="0.999658">Cambridge, MA 02138,</address>
<email confidence="0.99957">ponbarry@eecs.harvard.edu</email>
<author confidence="0.930833">Stuart</author>
<affiliation confidence="0.80361">School of Engineering and Applied Harvard</affiliation>
<address confidence="0.999659">Cambridge, MA 02138,</address>
<email confidence="0.999907">shieber@seas.harvard.edu</email>
<abstract confidence="0.993853352941176">We present an experiment aimed at understanding how to optimally use acoustic and prosodic information to predict a speaker’s level of certainty. With a corpus of utterances where we can isolate a single word or phrase that is responsible for the speaker’s level of certainty we use different sets of sub-utterance prosodic features to train models for predicting an utterance’s perceived level of certainty. Our results suggest that using prosodic features of the word or phrase responsible for the level of certainty and of its surrounding context improves the prediction accuracy without increasing the total number of features when compared to using only features taken from the utterance as a whole.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abeer Alwan</author>
<author>Yijian Bai</author>
<author>Matthew Black</author>
</authors>
<title>A system for technology based assessment of language and literacy in young children: the role of multiple information sources.</title>
<date>2007</date>
<booktitle>Proc. of IEEE International Workshop on Multimedia Signal Processing,</booktitle>
<pages>26--30</pages>
<location>Chania, Greece.</location>
<contexts>
<context position="3178" citStr="Alwan et al., 2007" startWordPosition="500" endWordPosition="503">tainty by using prosodic features extracted from the whole utterance plus ones extracted from salient pieces of the utterance, without increasing the total number of features, than by using only features from the whole utterance. This work is relevant to spoken language applications in which the system knows specific words or phrases that are likely to cause uncertainty. For example, this would occur in a tutorial dialogue system when the speaker answers a direct question (PonBarry et al., 2006; Forbes-Riley et al., 2008), or in language (foreign or ESL) learning systems and literacy systems (Alwan et al., 2007) when new vocabulary is being introduced. 2 Previous Work Researchers have examined certainty in spoken language using data from tutorial dialogue systems (Liscombe et al., 2005) and data from an uncertainty corpus (Pon-Barry, 2008). Liscombe et al. (2005) trained a decision tree 105 Proceedings of NAACL HLT 2009: Short Papers, pages 105–108, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics classifier on utterance-level and intonational phraselevel prosodic features to distinguish between certain, uncertain, and neutral utterances. They achieved 76% accuracy, comp</context>
</contexts>
<marker>Alwan, Bai, Black, 2007</marker>
<rawString>Abeer Alwan, Yijian Bai, Matthew Black, et al. 2007. A system for technology based assessment of language and literacy in young children: the role of multiple information sources. Proc. of IEEE International Workshop on Multimedia Signal Processing, pp. 26–30, Chania, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy Ang</author>
<author>Rajdip Dhillon</author>
<author>Ashley Krupski</author>
</authors>
<title>Prosody-based automatic detection of annoyance and frustration in human-computer dialog.</title>
<date>2002</date>
<booktitle>Proc. of ICSLP</booktitle>
<pages>2037--2040</pages>
<location>Denver, CO.</location>
<contexts>
<context position="1444" citStr="Ang et al., 2002" startWordPosition="215" endWordPosition="218">ible for the level of certainty and of its surrounding context improves the prediction accuracy without increasing the total number of features when compared to using only features taken from the utterance as a whole. 1 Introduction Prosody is a fundamental part of human-to-human spoken communication; it can affect the syntactic and semantic interpretation of an utterance (Hirschberg, 2003) and it can be used by speakers to convey their emotional state. In recent years, researchers have found prosodic features to be useful in automatically detecting emotions such as annoyance and frustration (Ang et al., 2002) and in distinguishing positive from negative emotional states (Lee and Narayanan, 2005). In this paper, we address the problem of predicting the perceived level of certainty of a spoken utterance. Specifically, we have a corpus of utterances where it is possible to isolate a single word or phrase responsible for the speaker’s level of certainty. With this corpus we investigate whether using prosodic features of the word or phrase causing uncertainty and of its surrounding context improves the prediction accuracy when compared to using features taken only from the utterance as a whole. This wo</context>
</contexts>
<marker>Ang, Dhillon, Krupski, 2002</marker>
<rawString>Jeremy Ang, Rajdip Dhillon, Ashley Krupski, et al. 2002. Prosody-based automatic detection of annoyance and frustration in human-computer dialog. Proc. of ICSLP 2002, pp. 2037–2040, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Forbes-Riley</author>
<author>Diane Litman</author>
<author>Mihai Rotaru</author>
</authors>
<title>Responding to student uncertainty during computer tutoring: a preliminary evaluation.</title>
<date>2008</date>
<booktitle>Proc. of the 9th International Conference on Intelligent Tutoring Systems,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="3086" citStr="Forbes-Riley et al., 2008" startWordPosition="484" endWordPosition="487">Our results suggest that we can do a better job at predicting an utterance’s perceived level of certainty by using prosodic features extracted from the whole utterance plus ones extracted from salient pieces of the utterance, without increasing the total number of features, than by using only features from the whole utterance. This work is relevant to spoken language applications in which the system knows specific words or phrases that are likely to cause uncertainty. For example, this would occur in a tutorial dialogue system when the speaker answers a direct question (PonBarry et al., 2006; Forbes-Riley et al., 2008), or in language (foreign or ESL) learning systems and literacy systems (Alwan et al., 2007) when new vocabulary is being introduced. 2 Previous Work Researchers have examined certainty in spoken language using data from tutorial dialogue systems (Liscombe et al., 2005) and data from an uncertainty corpus (Pon-Barry, 2008). Liscombe et al. (2005) trained a decision tree 105 Proceedings of NAACL HLT 2009: Short Papers, pages 105–108, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics classifier on utterance-level and intonational phraselevel prosodic features to dist</context>
</contexts>
<marker>Forbes-Riley, Litman, Rotaru, 2008</marker>
<rawString>Kate Forbes-Riley, Diane Litman, and Mihai Rotaru. 2008. Responding to student uncertainty during computer tutoring: a preliminary evaluation. Proc. of the 9th International Conference on Intelligent Tutoring Systems, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
</authors>
<title>Intonation and pragmatics.</title>
<date>2003</date>
<booktitle>Handbook of Pragmatics,</booktitle>
<editor>In L. Horn and G. Ward (ed.),</editor>
<publisher>Blackwell.</publisher>
<contexts>
<context position="1220" citStr="Hirschberg, 2003" startWordPosition="179" endWordPosition="180">of certainty we use different sets of sub-utterance prosodic features to train models for predicting an utterance’s perceived level of certainty. Our results suggest that using prosodic features of the word or phrase responsible for the level of certainty and of its surrounding context improves the prediction accuracy without increasing the total number of features when compared to using only features taken from the utterance as a whole. 1 Introduction Prosody is a fundamental part of human-to-human spoken communication; it can affect the syntactic and semantic interpretation of an utterance (Hirschberg, 2003) and it can be used by speakers to convey their emotional state. In recent years, researchers have found prosodic features to be useful in automatically detecting emotions such as annoyance and frustration (Ang et al., 2002) and in distinguishing positive from negative emotional states (Lee and Narayanan, 2005). In this paper, we address the problem of predicting the perceived level of certainty of a spoken utterance. Specifically, we have a corpus of utterances where it is possible to isolate a single word or phrase responsible for the speaker’s level of certainty. With this corpus we investi</context>
</contexts>
<marker>Hirschberg, 2003</marker>
<rawString>Julia Hirschberg. 2003. Intonation and pragmatics. In L. Horn and G. Ward (ed.), Handbook of Pragmatics, Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chul Min Lee</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Towards detecting emotions in spoken dialogs.</title>
<date>2005</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="1532" citStr="Lee and Narayanan, 2005" startWordPosition="228" endWordPosition="231">ction accuracy without increasing the total number of features when compared to using only features taken from the utterance as a whole. 1 Introduction Prosody is a fundamental part of human-to-human spoken communication; it can affect the syntactic and semantic interpretation of an utterance (Hirschberg, 2003) and it can be used by speakers to convey their emotional state. In recent years, researchers have found prosodic features to be useful in automatically detecting emotions such as annoyance and frustration (Ang et al., 2002) and in distinguishing positive from negative emotional states (Lee and Narayanan, 2005). In this paper, we address the problem of predicting the perceived level of certainty of a spoken utterance. Specifically, we have a corpus of utterances where it is possible to isolate a single word or phrase responsible for the speaker’s level of certainty. With this corpus we investigate whether using prosodic features of the word or phrase causing uncertainty and of its surrounding context improves the prediction accuracy when compared to using features taken only from the utterance as a whole. This work goes beyond existing research by looking at the predictive power of prosodic features</context>
</contexts>
<marker>Lee, Narayanan, 2005</marker>
<rawString>Chul Min Lee and Shrikanth Narayanan. 2005. Towards detecting emotions in spoken dialogs. IEEE Transactions on Speech and Audio Processing, 13(2):293–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackson Liscombe</author>
<author>Julia Hirschberg</author>
<author>Jennifer Venditti</author>
</authors>
<title>Detecting certainness in spoken tutorial dialogues.</title>
<date>2005</date>
<booktitle>Proceedings of Eurospeech</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="2327" citStr="Liscombe et al., 2005" startWordPosition="360" endWordPosition="363">le to isolate a single word or phrase responsible for the speaker’s level of certainty. With this corpus we investigate whether using prosodic features of the word or phrase causing uncertainty and of its surrounding context improves the prediction accuracy when compared to using features taken only from the utterance as a whole. This work goes beyond existing research by looking at the predictive power of prosodic features extracted from salient sub-utterance segments. Previous work on uncertainty has examined the predictive power of utterance- and intonational phraselevel prosodic features (Liscombe et al., 2005) as well as the relative strengths of correlations between level of certainty and sub-utterance prosodic features (Pon-Barry, 2008). Our results suggest that we can do a better job at predicting an utterance’s perceived level of certainty by using prosodic features extracted from the whole utterance plus ones extracted from salient pieces of the utterance, without increasing the total number of features, than by using only features from the whole utterance. This work is relevant to spoken language applications in which the system knows specific words or phrases that are likely to cause uncerta</context>
<context position="9067" citStr="Liscombe et al. (2005)" startWordPosition="1435" endWordPosition="1438">experiment fits a model using data from 19 speakers and tests on the remain1 ing speaker. Thus, when we test our models, we are testing the ability to classify utterances of an unseen speaker. Table 1 shows the accuracies of the models trained on the five subsets of features. The numbers reported are averages of the 20 cross-validation accuracies. We report results for two cases: 5 prediction classes and 3 prediction classes. We first computed the prediction accuracy over five classes (the regression output was rounded to the nearest integer). Next, in order to compare our results to those of Liscombe et al. (2005), we recoded the 5-class results into 3-class results, following PonBarry (2008), in the way that maximized interannotator agreement. The naive baseline numbers are the accuracies that would be achieved by always choosing the most common class. 4 Discussion Assuming that the target word is responsible for the speaker’s level of certainty, it is not surprising that the target word feature set (B) yields higher accuracies than the context feature set (C). It is also not surprising that the set of all features (D) yields higher accuracies than sets (A), (B), and (C). The key comparison to notice </context>
<context position="11180" citStr="Liscombe et al. (2005)" startWordPosition="1799" endWordPosition="1802">ds rather than between folds. Figure 2 shows the correlations between the predicted and perceived levels of certainty for the models trained on sets (A) and (E). The combination set (E) predictions were more strongly correlated than whole utterance set (A) predictions in 16 out of 20 folds. This result supports our claim that using a combination of features from the context and target word in addition to features from the whole utterance leads to better prediction of level of certainty. Our best prediction accuracy for the 3 class case, 74.79%, was slightly lower than the accuracy reported by Liscombe et al. (2005), 76.42%. However, our difference from the naive baseline was 18.54% where Liscombe et al.’s was 10.42%. Liscombe et al. randomly divided their data into training and test sets, so it is unclear whether they tested on seen or unseen speakers. Further, they ran one experiment rather than a cross-validation, so their reported accuracy may not be indicative of the entire data set. We also trained support vector models on these subsets of features. The main result was the same: rel. position max RMS −0.039 −0.028 −0.007 total silence −0.643 −0.507 −0.495 −0.532 percent silence −0.455 −0.225 min f0</context>
</contexts>
<marker>Liscombe, Hirschberg, Venditti, 2005</marker>
<rawString>Jackson Liscombe, Julia Hirschberg, and Jennifer Venditti. 2005. Detecting certainness in spoken tutorial dialogues. Proceedings of Eurospeech 2005, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heather Pon-Barry</author>
<author>Karl Schultz</author>
<author>Elizabeth Bratt</author>
<author>Brady Clark</author>
<author>Stanley Peters</author>
</authors>
<title>Responding to student uncertainty in spoken tutorial dialogue systems.</title>
<date>2006</date>
<journal>International Journal of Artificial Intelligence in Education</journal>
<pages>16--171</pages>
<marker>Pon-Barry, Schultz, Bratt, Clark, Peters, 2006</marker>
<rawString>Heather Pon-Barry, Karl Schultz, Elizabeth Bratt, Brady Clark, and Stanley Peters. 2006. Responding to student uncertainty in spoken tutorial dialogue systems. International Journal of Artificial Intelligence in Education 16:171-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heather Pon-Barry</author>
</authors>
<title>Prosodic manifestations of confidence and uncertainty in spoken language.</title>
<date>2008</date>
<booktitle>Proc. of Interspeech</booktitle>
<pages>74--77</pages>
<location>Brisbane, Australia.</location>
<contexts>
<context position="2458" citStr="Pon-Barry, 2008" startWordPosition="381" endWordPosition="382">sodic features of the word or phrase causing uncertainty and of its surrounding context improves the prediction accuracy when compared to using features taken only from the utterance as a whole. This work goes beyond existing research by looking at the predictive power of prosodic features extracted from salient sub-utterance segments. Previous work on uncertainty has examined the predictive power of utterance- and intonational phraselevel prosodic features (Liscombe et al., 2005) as well as the relative strengths of correlations between level of certainty and sub-utterance prosodic features (Pon-Barry, 2008). Our results suggest that we can do a better job at predicting an utterance’s perceived level of certainty by using prosodic features extracted from the whole utterance plus ones extracted from salient pieces of the utterance, without increasing the total number of features, than by using only features from the whole utterance. This work is relevant to spoken language applications in which the system knows specific words or phrases that are likely to cause uncertainty. For example, this would occur in a tutorial dialogue system when the speaker answers a direct question (PonBarry et al., 2006</context>
<context position="3943" citStr="Pon-Barry, 2008" startWordPosition="614" endWordPosition="615">stems (Liscombe et al., 2005) and data from an uncertainty corpus (Pon-Barry, 2008). Liscombe et al. (2005) trained a decision tree 105 Proceedings of NAACL HLT 2009: Short Papers, pages 105–108, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics classifier on utterance-level and intonational phraselevel prosodic features to distinguish between certain, uncertain, and neutral utterances. They achieved 76% accuracy, compared to a 66% accuracy baseline (choosing the most common class). We have collected a corpus of utterances spoken under varying levels of certainty (Pon-Barry, 2008). The utterances were elicited by giving adult native English speakers a written sentence containing one or more gaps, then displaying multiple options for filling in the gaps and telling the speakers to read the sentence aloud with the gaps filled in according to domain-specific criteria. We elicited utterances in two domains: (1) using public transportation in Boston, and (2) choosing vocabulary words to complete a sentence. An example is shown below. Q: How can I get from Harvard to the Silver Line? A: Take the red line to a. South Station b. Downtown Crossing The term ‘context’ refers to t</context>
</contexts>
<marker>Pon-Barry, 2008</marker>
<rawString>Heather Pon-Barry. 2008. Prosodic manifestations of confidence and uncertainty in spoken language. Proc. of Interspeech 2008, pp. 74–77, Brisbane, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>