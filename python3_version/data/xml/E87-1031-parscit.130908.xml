<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001839">
<note confidence="0.9436205">
PLANNING FOR PROBLEM FORMULATION
IN ADVICE-GIVING DIALOGUE
</note>
<author confidence="0.5840235">
Paul Decitre, Thomas Grossi, Cleo Jullien, Jean-Philippe Solvay
Cap Sogeti Innovation
</author>
<affiliation confidence="0.745348">
Centre de Recherche de Grenoble
</affiliation>
<address confidence="0.4904995">
Chemin du Vieux Chene
38240 Meylan, France
</address>
<sectionHeader confidence="0.961982" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999983555555555">
We distinguish three main, overlapping activities in an
advice-giving dialogue: problem formulation, resolution,
and explanation. This paper focuses on a problem for-
mulation activity in a dialogue module which interacts
on one side with an expert problem solver for financial
investing and on the other side with a natural language
front-end. Several strategies which reflect specific aspects
of person-machine advice-giving dialogues are realized by
incorporating planning at a high-level of dialogue.
</bodyText>
<sectionHeader confidence="0.960082" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.989030346153846">
As performances and scope of intelligent systems in-
crease and the interaction of a system with a user gains in
complexity, it becomes desirable to provide an easy initial
access to a system for the novice user. Natural language
is a medium presumably known by most users. For the
system however, it not only requires understanding nat-
ural language &amp;quot;utterances&amp;quot; (on a keyboard) but also rec-
ognizing the intentions behind these utterances. It leads
to a full-fledged dialogue involving much reasoning at the
pragmatic level of the communication process. The com-
petence of most intelligent systems is usually bound to
a restricted application domain and we can imagine that
part of a dialogue is domain-dependent while another is
domain-independent. Our efforts aim at designing a dia-
logue module making these different aspects explicit and
interacting with other knowledge-based agents. This work
contributes to Esprit Project 316 Estearni: An architec-
ture for distributed problem solving by cooperating data
and knowledge bases. Advice-giving systems for financial
investment have been chosen as a first teethed application.
This paper describes preliminary research on the dialogue
module of such a system and the resulting prototype.
An advice-giving dialogue comprises three main activ-
ities, which may overlap:
— problem formulation, where the various needs and ca-
pabilities of the user are elicited;
</bodyText>
<listItem confidence="0.904299666666667">
— resolution, in which a possible solution to the problem
is determined;
— explanation, which aims at convincing the user that
</listItem>
<bodyText confidence="0.949472083333333">
&apos;The project is supported in part by the Commission of
the European Communities
the solution is in fact what she/he needs.
Our work concentrates on a problem formulation activity
in a dialogue module which cooperates with a problem
solver and a natural language front-end. The problem
solver selects adequate securities for basic investment sit-
uations of a private investor and is being developed as
part of the same Esprit project [Bruffaerts 1986]. The
natural language front-end based on functional grammars
is the result of separate research at Cap Sogeti Innovation
[Fimbel 1985, Lancel 1986].
</bodyText>
<subsectionHeader confidence="0.868031">
Computational Aspects
</subsectionHeader>
<bodyText confidence="0.999887742857143">
Dialogue and communication theory are a broad field
of studies drawing on several disciplines, among them phi-
losophy, cognitive science, and artificial intelligence. The
flurry of research devoted to these topics in recent years
is largely enough to convince us we could not seriously
hope to tackle the &amp;quot;general&amp;quot; problem. We have therefore
limited our interest to person-machine advice-giving dia-
logue and we focus on two essential characteristics of this
kind of dialogue:
— the system has intentions and extensive knowledge
about the domain which are a priori unknown to the
user;
— the user&apos;s intentions must be interpreted in terms of
the system&apos;s abilities or inabilities.
We can see the first point as a manifestation of the &amp;quot;ex-
pertness&amp;quot; of the system, and the second as a manifestation
of the &amp;quot;noviceness&amp;quot; of the user.
We briefly recall some other research issues connected
to our work and then elaborate on the specific aspects of
person-machine advice-giving dialogue.
Many efforts have been devoted to developing a general
theory for speech act understanding [Searle 1969, Allen
1980, Cohen 1979]. Recognizing the illocutionary force
of a speech act allows the system to reason about the
intentions of the user and to behave accordingly. Most
work in this field addresses only isolated speech acts or
sometimes single utterances and is not concerned with a
possible dialogue setting. Recent attempts, however, for
reformulating speech act analysis inside a general theory
of action [Cohen 1986] or for applying default reasoning
to speech act understanding [Perrault 1986] may yet facil-
itate an extension to a whole dialogue. Another line of re-
search has taken into account the dialogue dimension [Lit-
man 1984, Wilensky 1984, Carberry 1986] and shown the
strong interrelation between dialogue and plans but has
</bodyText>
<page confidence="0.998171">
186
</page>
<bodyText confidence="0.999727692307692">
been mostly concerned with information-seeking dialogues
in which the user seems to have an implicit knowledge of
what the system can or cannot do. These dialogues of-
ten produce patterns of the type &amp;quot;question from the user
requiring adequate answer from the system&amp;quot; and seldom
consider a possible initiative on the system&apos;s behalf. Dia-
logue parsing is yet another approach which attempts to
formalize the surface structure of a dialogue [Reichman
1984, Wachtel 1986]. It could however lead to some rigid-
ity in the interaction between the user and the system;
for instance, it may not provide the adequate primitive
elements to detect and repair communicative failures in
the dialogue. A possible way out would be to account
for this surface structure of dialogue within a theory of
pragmatics [Airenti 1986].
In person-machine advice-giving dialogue the challenge
we face is how to make the expertise of the system acces-
sible to the user in order to satisfy her/his needs: the &amp;quot;ex-
pertness&amp;quot; of the system and the &amp;quot;noviceness&amp;quot; of the user
force a compromise between the system controlling the
dialogue and the user expressing her/himself freely. We
chose to rely on the system for conducting the dialogue
without however ignoring the initiative of the user, which
is to be examined within the intentional framework of the
system. In the course of our research, we have derived a
few general strategies which typify our approach.
</bodyText>
<listItem confidence="0.997309315789474">
• Whenever possible, the system should set a clear back-
ground to the conversation. This is particularly true of
the beginning of a session, where the system should not
leave the user in the dark but should at once define its
own competence and suggest possible options to the user.
This initial setting will reflect the global purpose of the
dialogue and its expected unfolding.
• Each step of a dialogue takes place in a certain context.
We must ensure a common perception of this context by
the user and the system if we want a meaningful exchange
between them.
• It is worth taking advantage of what the system can
expect from the user when the latter &amp;quot;takes the floor&amp;quot; to
guide the search of a correct interpretation and quickly
decide the best-suited reaction. We should make these
expectations of the system apparent in our model of di-
alogue. Nevertheless, we want the system to allow for
user digressions, such as the introduction of a new topic
or the correction of a previous statement. It is impor-
</listItem>
<bodyText confidence="0.736892470588235">
tant to note that a sophisticated dialogue management
which would allow the system to react adequately to this
unexpected behavior of the user should not impair the
straightforward and most probable reaction described just
before. It should rather be called upon as a second best
choice when the first one has failed, thus defining a pref-
erence hierarchy among possible reactions of the system.
(In other words, first the clear-minded and obedient user,
then the muddle-minded one!)
• The form of the interaction between a user and an
advice-giver evolves with the experience they have of each
other and the increase of their mutual knowledge, either in
the course of a same session or through several sessions.
The dialogue system should gradually lead the user to-
ward a simpler and more efficient interface by suggesting
the adequate jargon and steps which would allow the user
to quicker and better formulate her/his problem [Slator
</bodyText>
<page confidence="0.590656">
1986].
</page>
<bodyText confidence="0.681118">
Description of the Prototype
</bodyText>
<listItem confidence="0.858982">
• World
</listItem>
<bodyText confidence="0.999891384615385">
The World of our dialogue module consists of a set
of objects among which several relations and inheritance
mechanisms are defined. For instance, there are classical
ia-a links, part-of links (a cash-need is part of the invest-
ment plan) and specification links (an amount is &amp;quot;specified&amp;quot;
by a number and a currency).
Parts of this semantic network are shared with other
agents than the dialogue agent, or at least have the same
representation in other agents. This is the case between
the dialogue module and the problem solver for the prob-
lem formulation phase: a model of the expected problem
is represented in the World.
For our application, the expected problem consists of
an investment plan expressed in terms of the basic invest-
ment situations for which the problem solver is able to se-
lect the adequate securities. It may include an emergency-
fund, ie., an amount of money which should be available
at random time within a given delay, or a cash-need, ie.,
an amount of money which should be available at a given
date. These financial objects in the problem model are
related to objects describing goals and situations of the
user&apos;s everyday world through requirement links. For in-
stance, buying a car in five years may necessitate a cash-
need, while covering unexpected expenses may ask for an
emergency-fund. These requirement links will guide the
recognition of the user plan when resolving references.
Other domain knowledge for the problem formulation di-
alogue is encoded in terms of the problem model objects
and includes preferred sequences for the interaction with
the user and constraints on these objects.
For the dialogue module, the user is considered as an-
other agent and her/his intentions and mental states are
represented in terms of positions toward objects of the di-
alogue. Examples of such positions are &apos;user understands
X&apos;, &apos;system wants to know the value of X&apos;, or &apos;user wants
X to take a certain value&apos;. We can view the objects and
positions as representing respectively static and dynamic
information in the system and allowing the exchange of
information between agents.
</bodyText>
<listItem confidence="0.883956">
• Focus-Stack and Agenda
</listItem>
<bodyText confidence="0.99992815">
We can characterize each step of the dialogue by a given
attentional focus and a given task for the system. In our
dialogue module these correspond respectively to a par-
ticular object — or set of objects — under discussion and
to an action of the system.
During the dialogue, the focus of attention obviously
evolves along a chronological dimension: one subject at
a time. But a deeper analysis (cf., for example, [Grosz
19851) reveals a layered structure . In the current pro-
totype, these layers of foci come into play in refinement
and digression. Refinement occurs when the treatment
of a complex object is split into sub-dialogues about its
parts: during such a sub-dialogue, the &amp;quot;parent&amp;quot; and &amp;quot;sib-
ling&amp;quot; objects constitute background context layers. A
typical digression takes place when the system suspends
information-gathering to give an explanation and comes
back to the suspended step of the dialogue. The system
keeps track of the active layers of foci in the Focus-Stack.
The sequence of actions the system has currently
planned to perform are stored in the Agenda.
</bodyText>
<page confidence="0.984034">
187
</page>
<listItem confidence="0.938578">
• Architecture
</listItem>
<bodyText confidence="0.985143555555556">
The dialogue module contains four sub-modules: the
INTERPRETER and the GENERATOR are in charge of relat-
ing logical form expressions of the natural language front-
end to meanings about the World, the EXECUTOR carries
out communicative-games for interacting with the user,
and the REACTOR activates metaplans for updating the
Agenda and the Focus-Stack.
The next sections of this paper investigates in greater
detail how the metaplans and communicative-games
model the possible actions and strategies of the system
and enter into the dialogue planning process. An account
on other aspects of this prototype may be found in a pre-
vious technical report [Decitre 1986].
High-Level Planning in the REACTOR
From the dialogue module&apos;s point of view, the entire
conversation results from the goal, &amp;quot;Obtain an investment
plan problem specification from the user&amp;quot;. The goal is ex-
panded according to the problem model into appropriate
subgoals, which are pushed onto the Agenda for sequen-
tial execution. As each subgoal is considered, it may be
further expanded as necessary. In other words, the de-
composition of the communicative intentions (obtaining
specifications) reflects the decomposition of the task in-
tentions (investing). There exist two types of metaplans:
the metaplans for expanding the Agenda and the meta-
plans for revising it according to some initiative from the
user.
</bodyText>
<listItem confidence="0.908598">
• Expansion
</listItem>
<bodyText confidence="0.9997297">
As an illustration of the first type of metaplans, let
us consider what happens at the beginning of an advice-
giving session. When the dialogue starts, the Agenda con-
sists solely of a single action treat(invest-plan). A treat action
basically corresponds to a sequence of three steps: presen-
tation of the object to the user, asking for values which
specify this object, and finally asking for confirmation.
But the expansion of treat actions can vary according to
the type of their argument. For instance, an object may be
either simple or complex, it may also be visible or trans-
parent. A transparent object is part of the structure of the
problem model but remains invisible to the user. This is
the case for partitiots(invest-pian) which consists of the set
of the parts of an investment plan, i.e., {emergency-fund,
cash-need, long-term}. These transparent objects attempt
to model the differences which may exist between how
the problem model is organized and how it may be per-
ceived by the user. For a complex object, the expansion
introduces treatments for the parts of the complex object,
whereas simple objects have only specifications.
Let us just show how these expansion metaplans ac-
count for the first two of our general strategies.
The expansion of the initial goal treat(invest-plan) posts
a present(invest-plan) onto the Agenda. The presentation
of a complex object such as invest-plan reflects how it will
be expanded, since the same source of information, ie.,
the problem model, is used for presentation and expan-
sion, and thus provides a background setting for the di-
alogue. The order — in this case obligatory — in which
the sub-objects of invest-plan are considered is: first, the
total-amount for the plan; second, the partition(invest-plan).
The latter is a transparent object for which adequate pre-
sentation rules are defined: the presentation of a partition
simply entails a presentation of all parts. The natural lan-
guage front-end actually generates the following descrip-
tion:
system -&amp;quot;Investment-plan: An investment plan is
characterized by a total amount and is usually com-
posed of an emergency-fund, one or several cash-needs
and a long-term investment.&amp;quot;
Update of the Focus-Stack is also governed by the expan-
sion, and a layer containing all the objects introduced in
this presentation is pushed onto the stack. The present
example gives [total-amount, emergency-fund, cash-need, long-
term]. We see again the effect of transparency: the parts
themselves are directly pushed onto the stack and not the
partition. This layer will constitute the backup layer of
the Focus-Stack associated to the overall dialogue setting.
At this stage, the next action on the Agenda is
treat (total-amount) which may be further expanded in push-
focus, ask-info-game, check-complete, pop-focus. The ask-info-
game is a communicative game which asks a question
about the total-amount object:
system -&amp;quot;What is the total amount of your plan of
investment?&amp;quot;
and waits for the response of the user. The communica-
tive game is designed to induce the user to specialize
her/his focus of attention toward the refined context total-
amount, and push-focus(total-amount) places this object on
the Focus-Stack, updating it correspondingly.
</bodyText>
<listItem confidence="0.938639">
• Revision
</listItem>
<bodyText confidence="0.999865151515151">
Our plan generation is simplified because the execution
of one subgoal cannot invalidate another, so a constant
monitoring of preconditions is obviated; but this is more
than made up by the difficulty in accommodating possible
changes to the plan necessitated by the user&apos;s input. The
choice of a planning process which either expands or re-
pairs an existing plan reflects our third strategy. Indeed,
the natural expansion of a plan can be seen as correspond-
ing to the expected behavior of the user and the revisions
only happen when the user takes the initiative. In this
approach, the reasoning which takes place when the user
follows the expected course is reduced to its minimum and
only digressions require extra efforts.
Interactions with the user are handled through com-
municative games and a special metaplan reacts when a
communicative game appears on top of the Agenda. This
metaplan triggers the execution of the game and ana-
lyzes the outcome of the execution to decide consequently
the updates to the Agenda. If the game has completely
succeeded, i.e. all responses of the user fit the expects.
tions, the communicative game is simply removed from
the Agenda and replaced by ok-react actions for each new
position expressed by the user. Otherwise there exist un-
expected responses and different actions are pushed onto
the Agenda in such a way that the expected positions will
be analyzed first by means of ok-react actions, then un-
expected positions concerning the current focus and un-
expected positions outside the current focus by means of
not-oh-react actions. For all these not-ok-react actions, there
are metaplans to consider the precise situation and to
decide an appropriate reaction, with rearrangement and
other modifications made as necessary to the Agenda of
pending actions. Delaying the expansion of plans until it
</bodyText>
<page confidence="0.997247">
188
</page>
<bodyText confidence="0.995437616">
becomes necessary to execute them facilitates taking into
account the effect of the user&apos;s responses on goals not
yet addressed, as in, for example, the verification of con-
straints which the various parts of the problem definition
impose on one another, or in noticing that the value of a
missing variable can be computed from the combination
of other values the user has already given.
What sorts of snags can occur in a dialogue that might
force the system to revise its plans? Our problem model
provides certain relations which must hold between val-
ues provided by the user. The user might, however,
give a value which is in conflict either with one of these
constraints or with values previously given. We must
point out the sticking-point and help the user resolve
the conflict. The verify-constraint metaplan pushes a meet-
constraint-game onto the Agenda. This game will present
the local constraint which led to refusing the new posi-
tion expressed by the user and the justifications which
relate this local constraint to the global constraints of the
problem model. Consider, for instance, a simple equality
constraint between the total amount and the sum of the
amounts of the parts. With a $20,000 total-amount and
a $5,000 amount for the emergency-fund, a $16,000 assign-
value position for the amount of the cash-need would bring
system -&amp;quot;The amount of your cash-need should be
less than or equal to *15.000 for consistency with the
total amount.&amp;quot;
We also have preferences (and sometimes obligations)
in the ordering of the various points to be addressed dur-
ing the conversation, but the user might not respect them.
For instance, the user might at any moment decide to
change subject, in which case we must consider the effects
of the switch: if, for example, she/he asks to back up in
the conversation to change something which was of neces-
sity addressed before the current subject, this could force
revision of all the values given since that point up to the
present. Based on the following situations, we identify
three classes of change-subject metaplans, which can trig-
ger when the new position expressed by the user bears
on a context which is not the current focus and modify
accordingly the Agenda:
- the current focus must be treated before the new
subject introduced by the user (according to se-
quencing policies in the problem model),
- the subject the user would like to examine has al-
ready been treated and a modification would have
consequences on what has been discussed since,
- there is no sequencing difficulty.
If the user asks for explanation of some point which
she/he doesn&apos;t understand, the system enters a digression
in the dialogue, after which the original topic is resumed.
Low-Level Planning and the EXECUTOR
As discussed above, the decomposition of a plan often
engenders the need for interaction with the user. This
is done through the communicative games. Basically a
communicative game aims at representing a pair of turns
between the user and the system, e.g., question/answer.
(In fact, we also need to model one-turn games for the
transitions between phases, e.g., introduction/resumption
of a new/old subject). Although we can never be sure
the second turn will take place as desired, the interest
of representing games is to provide local expectations for
the interpretation of the response of the user. It should
be noted that our intention in using these communicative
games is not to impose a structure on the dialogue be-
tween the user and the system: these games correspond
to an ideal dialogue in which the user would always re-
spond as expected. The actual dialogue is a succession
of communicative games which may fail, thereby reacti-
vating the high-level planning process described in the
previous section.
With each communicative game is associated an out-
meaning which indicates the semantic content to be con-
veyed to the user when the game is executed. This out-
meaning is expressed in the internal language of the dia-
logue module in which mostly appear objects of the prob-
lem model. Adequate references in logical form to these
objects are provided by the GENERATOR of the dialogue
module. The referring process utilizes:
- the semantic representation of the World;
- the Focus-Stack, especially the current focus which
may be elliptically referred to;
- the conceptual state of the user.
This conceptual state is based on initial assumptions, e.g.,
whether a concept is a priori familiar to the user, and
on what has already transpired during the dialogue, e.g.
whether a concept has already been explained, or how
the user has previously referred to an object of the prob-
lem model. The GENERATOR takes this information to
adapt its description and link unknown concepts to fa-
miliar ones. Thus the user progressively learns what the
problem model consists of and how it relates to her/his
familiar concepts: a simple but efficient approach to the
evolving interaction between the user and the system held
above as our fourth desirable strategy for person-machine
advice-giving dialogues.
Symmetrically a communicative game is also charac-
terized by an in-ezpected meaning which stands for the ex-
pected response of the user, usually in terms of positions
on the current focus or on parts of the current focus. The
user&apos;s sentence is put into logical form by the natural lan-
guage front-end and possible meanings are proposed by
the INTERPRETER. The latter has to determine which
object of the problem model the description of the user
could refer to. Each interpretation attempt is done within
a context, that is a particular object which is the root of
the search process. Interpretation is based on two search
strategies: the first uses specification links, while the sec-
ond uses discriminant properties and requirement links. Two
types of reference can be recognized. Direct reference uses
only the first strategy following the specification links start-
ing from the context object and allows for elliptical an-
swers to questions. Indirect reference uses successively
both strategies: a search based on the discriminant prop-
erties determines candidate objects with a requirement link
to the context object, then these candidates constitute the
starting points for searching along specification links. The
user does not have the same structured view of the finan-
cial world as the system do, and hence will not necessarily
refer to things as we would like. The user will talk about
&amp;quot;the car I want to buy in five years&amp;quot; which requires a
cash-need. Interpretation attempts are ordered according
to the stack of foci: the most salient focus (or layer of foci)
is selected as context (or set of contexts), then the deeper
foci are successively tried. The INTERPRETER only tries
</bodyText>
<page confidence="0.996827">
189
</page>
<bodyText confidence="0.999807190476191">
a deeper focus if no interpretation has been found at a
higher layer. Moreover, for each layer, the INTERPRETER
tries to solve the direct reference before the indirect one
and returns all possible interpretations within the first
layer and type of reference which permitted to solve the
reference. The structure of past foci partly reflects the
evolution of our task structure [Grosz 19851 and allows
the user to refer back to past segments of the dialogue.
This structure is more supple than a mechanism which
relies solely on unachieved goals because not only is the
focus of a completed task not lost, but its location within
this structure is influenced by the problem model in order
to optimize subsequent recovery.
Additional knowledge is contained in game descrip-
tions: a feature in-react complements in-expected in provid-
ing a set of game-specific rules for interpreting the literal
meaning of the user&apos;s response returned by the INTER-
PRETER into its intended meaning within the particular
game considered. A simple example consists of trans-
formation rules for yes-okino answers depending on the
game.
</bodyText>
<sectionHeader confidence="0.924956" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.99997475">
This work incorporates planning by the system at a
high level of dialogue, and nevertheless leaves a great deal
of initiative to the user. This flexibility is enhanced by
the wide range of input styles which are allowed by the
interpretation of input according to focus and indirect ref-
erence. At the moment we have a prototype of a dia-
logue module written in Prolog which implements general
strategies for person-machine advice-giving dialogue. The
natural-language front-end, written in C, has been inter-
faced with the prototype, but the generation side would
require further investigation. Generalizing the planning
component and integrating more sophisticated plan recog-
nition techniques are some of the other issues addressed
in a next prototype. Work is also under way to extend
the concept base in our knowledge world to enrich the
conversation with the user.
</bodyText>
<sectionHeader confidence="0.999148" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999921446428571">
Airenti G., Bara B.G., and Colombetti M., &amp;quot;Cogni-
tive Pragmatics,&amp;quot; Research Report URIA 86-1, Units di
Ricerca di Intelligenza Artiflciale, Universal&apos; di Milano,
1986.
Allen J., &amp;quot;A Plan-Based Analysis of Indirect Speech
Acts,&amp;quot; Journal of the Association of Computational Linguistics,
vol. 15, 1980.
Bruffaerts A., Henin E., and Marlair V., &amp;quot;An Expert Sys-
tem Prototype for Financial Counseling,&amp;quot; Research Re-
port 507, Philips Research Laboratory Brussels, 1986.
Carberry S., &amp;quot;User Models: the Problem of Disparity,&amp;quot;
Proceedings of the Xlth International Conference on Computa-
tional Linguistics, pp. 29-34, Bonn (FR Germany), 1986.
Cohen P.R. and Perrault C.R., &amp;quot;Elements of a Plan-Based
Theory of Speech Acts,&amp;quot; Cognitive Science, no. 3, pp. 177-
212, 1979.
Cohen P.R., &amp;quot;The Role of Speech Acts in Natural Lan-
guage Understanding,&amp;quot; Tutorials of the Xlth International
Conference on Computational Linguistics, Bonn (FR Ger-
many), 1986.
Decitre P., Grossi T., Jullien C., and Solvay J.P., &amp;quot;A Sum-
mary Description of a Dialoguer Prototype,&amp;quot; Technical
Report CRG 86-1, Cap Sogeti Innovation, 1986.
Fimbel E., Groscot H., Lancel J.M., and Simonin N., &amp;quot;Us-
ing a Text Model for Analysis and Generation,&amp;quot; Proceedings
of the Second Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, Geneva (Switzerland),
1985.
Grosz B.J., &amp;quot;Discourse Structure and the Proper Treat-
ment of Interruptions,&amp;quot; Proceedings of the IXth IJOAI, Los
Angeles (USA), 1985.
Lance! J.M., Rousselot F., and Simonin N., &amp;quot;A Gram-
mar Used for Parsing and Generation,&amp;quot; Proceedings of the
XIth International Conference on Computational Linguistics,
pp. 536-539, Bonn (FR Germany), 1986.
Litman D.J. and Allen J.F., &amp;quot;A Plan Recognition Model
for Subdialogues in Conversations,&amp;quot; Technical Report 141,
University of Rochester, 1984.
Perrault C.R., &amp;quot;An Application of Default Logic to Speech
Act Theory,&amp;quot; Proceedings of the NATO Workshop on Struc-
ture of Multimodal Dialogues Including Voice, Venaco (France),
1986.
Reichman R., &amp;quot;Extended Person-Machine Interface,&amp;quot; Ar-
tificial Intelligence, vol. 22, pp. 157-218, 1984.
Searle J., Speech Acts: An Essay in the Philosophy of Language,
Cambridge University Press, 1969.
Slator B.M., Anderson M.P., and Conley W., &amp;quot;Pygmalion
at the Interface,&amp;quot; Communications of the ACM, vol. 29 ,
no. 7 , pp. 599-604, 1986.
Wachtel T., &amp;quot;Pragmatic Sensitivity in NL Interfaces and
the Structure of Conversation,&amp;quot; Proceedings of the XIth Inter-
national Conference on Computational Linguistics, pp. 35-41,
Bonn (FR Germany), 1986.
Wilensky R., Arens Y., and Chin D., &amp;quot;Talking to UNIX
in English: An Overview of UC,&amp;quot; Communications of the
ACM, vol. 27, no. 6, pp. 574-593, 1984.
</reference>
<page confidence="0.997869">
190
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.739337">
<title confidence="0.9929935">PLANNING FOR PROBLEM FORMULATION IN ADVICE-GIVING DIALOGUE</title>
<author confidence="0.996033">Paul Decitre</author>
<author confidence="0.996033">Thomas Grossi</author>
<author confidence="0.996033">Cleo Jullien</author>
<author confidence="0.996033">Jean-Philippe Solvay</author>
<affiliation confidence="0.967127">Innovation Centre de Recherche de Grenoble</affiliation>
<address confidence="0.90594">Chemin du Vieux Chene 38240 Meylan, France</address>
<abstract confidence="0.9977882">We distinguish three main, overlapping activities in an advice-giving dialogue: problem formulation, resolution, and explanation. This paper focuses on a problem formulation activity in a dialogue module which interacts on one side with an expert problem solver for financial investing and on the other side with a natural language front-end. Several strategies which reflect specific aspects of person-machine advice-giving dialogues are realized by incorporating planning at a high-level of dialogue.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Airenti</author>
<author>B G Bara</author>
<author>M Colombetti</author>
</authors>
<title>Cognitive Pragmatics,&amp;quot; Research Report URIA 86-1, Units di Ricerca di Intelligenza Artiflciale, Universal&apos; di</title>
<date>1986</date>
<location>Milano,</location>
<marker>Airenti, Bara, Colombetti, 1986</marker>
<rawString>Airenti G., Bara B.G., and Colombetti M., &amp;quot;Cognitive Pragmatics,&amp;quot; Research Report URIA 86-1, Units di Ricerca di Intelligenza Artiflciale, Universal&apos; di Milano, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allen</author>
</authors>
<title>A Plan-Based Analysis of Indirect Speech Acts,&amp;quot;</title>
<date>1980</date>
<journal>Journal of the Association of Computational Linguistics,</journal>
<volume>15</volume>
<contexts>
<context position="3990" citStr="Allen 1980" startWordPosition="609" endWordPosition="610">tem has intentions and extensive knowledge about the domain which are a priori unknown to the user; — the user&apos;s intentions must be interpreted in terms of the system&apos;s abilities or inabilities. We can see the first point as a manifestation of the &amp;quot;expertness&amp;quot; of the system, and the second as a manifestation of the &amp;quot;noviceness&amp;quot; of the user. We briefly recall some other research issues connected to our work and then elaborate on the specific aspects of person-machine advice-giving dialogue. Many efforts have been devoted to developing a general theory for speech act understanding [Searle 1969, Allen 1980, Cohen 1979]. Recognizing the illocutionary force of a speech act allows the system to reason about the intentions of the user and to behave accordingly. Most work in this field addresses only isolated speech acts or sometimes single utterances and is not concerned with a possible dialogue setting. Recent attempts, however, for reformulating speech act analysis inside a general theory of action [Cohen 1986] or for applying default reasoning to speech act understanding [Perrault 1986] may yet facilitate an extension to a whole dialogue. Another line of research has taken into account the dialo</context>
</contexts>
<marker>Allen, 1980</marker>
<rawString>Allen J., &amp;quot;A Plan-Based Analysis of Indirect Speech Acts,&amp;quot; Journal of the Association of Computational Linguistics, vol. 15, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bruffaerts</author>
<author>E Henin</author>
<author>V Marlair</author>
</authors>
<title>An Expert System Prototype for Financial Counseling,&amp;quot;</title>
<date>1986</date>
<tech>Research Report 507,</tech>
<institution>Philips Research Laboratory Brussels,</institution>
<marker>Bruffaerts, Henin, Marlair, 1986</marker>
<rawString>Bruffaerts A., Henin E., and Marlair V., &amp;quot;An Expert System Prototype for Financial Counseling,&amp;quot; Research Report 507, Philips Research Laboratory Brussels, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Carberry</author>
</authors>
<title>User Models: the Problem of Disparity,&amp;quot;</title>
<date>1986</date>
<booktitle>Proceedings of the Xlth International Conference on Computational Linguistics,</booktitle>
<pages>29--34</pages>
<location>Bonn (FR</location>
<contexts>
<context position="4646" citStr="Carberry 1986" startWordPosition="713" endWordPosition="714">y force of a speech act allows the system to reason about the intentions of the user and to behave accordingly. Most work in this field addresses only isolated speech acts or sometimes single utterances and is not concerned with a possible dialogue setting. Recent attempts, however, for reformulating speech act analysis inside a general theory of action [Cohen 1986] or for applying default reasoning to speech act understanding [Perrault 1986] may yet facilitate an extension to a whole dialogue. Another line of research has taken into account the dialogue dimension [Litman 1984, Wilensky 1984, Carberry 1986] and shown the strong interrelation between dialogue and plans but has 186 been mostly concerned with information-seeking dialogues in which the user seems to have an implicit knowledge of what the system can or cannot do. These dialogues often produce patterns of the type &amp;quot;question from the user requiring adequate answer from the system&amp;quot; and seldom consider a possible initiative on the system&apos;s behalf. Dialogue parsing is yet another approach which attempts to formalize the surface structure of a dialogue [Reichman 1984, Wachtel 1986]. It could however lead to some rigidity in the interactio</context>
</contexts>
<marker>Carberry, 1986</marker>
<rawString>Carberry S., &amp;quot;User Models: the Problem of Disparity,&amp;quot; Proceedings of the Xlth International Conference on Computational Linguistics, pp. 29-34, Bonn (FR Germany), 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
<author>C R Perrault</author>
</authors>
<title>Elements of a Plan-Based Theory of Speech Acts,&amp;quot;</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<volume>3</volume>
<pages>177--212</pages>
<marker>Cohen, Perrault, 1979</marker>
<rawString>Cohen P.R. and Perrault C.R., &amp;quot;Elements of a Plan-Based Theory of Speech Acts,&amp;quot; Cognitive Science, no. 3, pp. 177-212, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Cohen</author>
</authors>
<title>The Role of Speech Acts in Natural Language Understanding,&amp;quot; Tutorials of the</title>
<date>1986</date>
<booktitle>Xlth International Conference on Computational Linguistics,</booktitle>
<location>Bonn (FR</location>
<contexts>
<context position="4400" citStr="Cohen 1986" startWordPosition="673" endWordPosition="674">and then elaborate on the specific aspects of person-machine advice-giving dialogue. Many efforts have been devoted to developing a general theory for speech act understanding [Searle 1969, Allen 1980, Cohen 1979]. Recognizing the illocutionary force of a speech act allows the system to reason about the intentions of the user and to behave accordingly. Most work in this field addresses only isolated speech acts or sometimes single utterances and is not concerned with a possible dialogue setting. Recent attempts, however, for reformulating speech act analysis inside a general theory of action [Cohen 1986] or for applying default reasoning to speech act understanding [Perrault 1986] may yet facilitate an extension to a whole dialogue. Another line of research has taken into account the dialogue dimension [Litman 1984, Wilensky 1984, Carberry 1986] and shown the strong interrelation between dialogue and plans but has 186 been mostly concerned with information-seeking dialogues in which the user seems to have an implicit knowledge of what the system can or cannot do. These dialogues often produce patterns of the type &amp;quot;question from the user requiring adequate answer from the system&amp;quot; and seldom c</context>
</contexts>
<marker>Cohen, 1986</marker>
<rawString>Cohen P.R., &amp;quot;The Role of Speech Acts in Natural Language Understanding,&amp;quot; Tutorials of the Xlth International Conference on Computational Linguistics, Bonn (FR Germany), 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Decitre</author>
<author>T Grossi</author>
<author>C Jullien</author>
<author>J P Solvay</author>
</authors>
<title>A Summary Description of a Dialoguer Prototype,&amp;quot;</title>
<date>1986</date>
<tech>Technical Report CRG 86-1,</tech>
<institution>Cap Sogeti Innovation,</institution>
<marker>Decitre, Grossi, Jullien, Solvay, 1986</marker>
<rawString>Decitre P., Grossi T., Jullien C., and Solvay J.P., &amp;quot;A Summary Description of a Dialoguer Prototype,&amp;quot; Technical Report CRG 86-1, Cap Sogeti Innovation, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Fimbel</author>
<author>H Groscot</author>
<author>J M Lancel</author>
<author>N Simonin</author>
</authors>
<title>Using a Text Model for Analysis and Generation,&amp;quot;</title>
<date>1985</date>
<booktitle>Proceedings of the Second Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Geneva</location>
<marker>Fimbel, Groscot, Lancel, Simonin, 1985</marker>
<rawString>Fimbel E., Groscot H., Lancel J.M., and Simonin N., &amp;quot;Using a Text Model for Analysis and Generation,&amp;quot; Proceedings of the Second Conference of the European Chapter of the Association for Computational Linguistics, Geneva (Switzerland), 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
</authors>
<title>Discourse Structure and the Proper Treatment of Interruptions,&amp;quot;</title>
<date>1985</date>
<booktitle>Proceedings of the IXth IJOAI,</booktitle>
<location>Los Angeles (USA),</location>
<contexts>
<context position="10687" citStr="Grosz 1985" startWordPosition="1724" endWordPosition="1725">ew the objects and positions as representing respectively static and dynamic information in the system and allowing the exchange of information between agents. • Focus-Stack and Agenda We can characterize each step of the dialogue by a given attentional focus and a given task for the system. In our dialogue module these correspond respectively to a particular object — or set of objects — under discussion and to an action of the system. During the dialogue, the focus of attention obviously evolves along a chronological dimension: one subject at a time. But a deeper analysis (cf., for example, [Grosz 19851) reveals a layered structure . In the current prototype, these layers of foci come into play in refinement and digression. Refinement occurs when the treatment of a complex object is split into sub-dialogues about its parts: during such a sub-dialogue, the &amp;quot;parent&amp;quot; and &amp;quot;sibling&amp;quot; objects constitute background context layers. A typical digression takes place when the system suspends information-gathering to give an explanation and comes back to the suspended step of the dialogue. The system keeps track of the active layers of foci in the Focus-Stack. The sequence of actions the system has curr</context>
<context position="24948" citStr="Grosz 1985" startWordPosition="4040" endWordPosition="4041">ation attempts are ordered according to the stack of foci: the most salient focus (or layer of foci) is selected as context (or set of contexts), then the deeper foci are successively tried. The INTERPRETER only tries 189 a deeper focus if no interpretation has been found at a higher layer. Moreover, for each layer, the INTERPRETER tries to solve the direct reference before the indirect one and returns all possible interpretations within the first layer and type of reference which permitted to solve the reference. The structure of past foci partly reflects the evolution of our task structure [Grosz 19851 and allows the user to refer back to past segments of the dialogue. This structure is more supple than a mechanism which relies solely on unachieved goals because not only is the focus of a completed task not lost, but its location within this structure is influenced by the problem model in order to optimize subsequent recovery. Additional knowledge is contained in game descriptions: a feature in-react complements in-expected in providing a set of game-specific rules for interpreting the literal meaning of the user&apos;s response returned by the INTERPRETER into its intended meaning within the p</context>
</contexts>
<marker>Grosz, 1985</marker>
<rawString>Grosz B.J., &amp;quot;Discourse Structure and the Proper Treatment of Interruptions,&amp;quot; Proceedings of the IXth IJOAI, Los Angeles (USA), 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Lance</author>
<author>F Rousselot</author>
<author>N Simonin</author>
</authors>
<title>A Grammar Used for Parsing and Generation,&amp;quot;</title>
<date>1986</date>
<booktitle>Proceedings of the XIth International Conference on Computational Linguistics,</booktitle>
<pages>536--539</pages>
<location>Bonn (FR</location>
<marker>Lance, Rousselot, Simonin, 1986</marker>
<rawString>Lance! J.M., Rousselot F., and Simonin N., &amp;quot;A Grammar Used for Parsing and Generation,&amp;quot; Proceedings of the XIth International Conference on Computational Linguistics, pp. 536-539, Bonn (FR Germany), 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman</author>
<author>J F Allen</author>
</authors>
<title>A Plan Recognition Model for Subdialogues in Conversations,&amp;quot;</title>
<date>1984</date>
<tech>Technical Report 141,</tech>
<institution>University of Rochester,</institution>
<marker>Litman, Allen, 1984</marker>
<rawString>Litman D.J. and Allen J.F., &amp;quot;A Plan Recognition Model for Subdialogues in Conversations,&amp;quot; Technical Report 141, University of Rochester, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Perrault</author>
</authors>
<title>An Application of Default Logic to Speech Act Theory,&amp;quot;</title>
<date>1986</date>
<booktitle>Proceedings of the NATO Workshop on Structure of Multimodal Dialogues Including Voice,</booktitle>
<location>Venaco</location>
<contexts>
<context position="4478" citStr="Perrault 1986" startWordPosition="684" endWordPosition="685">dialogue. Many efforts have been devoted to developing a general theory for speech act understanding [Searle 1969, Allen 1980, Cohen 1979]. Recognizing the illocutionary force of a speech act allows the system to reason about the intentions of the user and to behave accordingly. Most work in this field addresses only isolated speech acts or sometimes single utterances and is not concerned with a possible dialogue setting. Recent attempts, however, for reformulating speech act analysis inside a general theory of action [Cohen 1986] or for applying default reasoning to speech act understanding [Perrault 1986] may yet facilitate an extension to a whole dialogue. Another line of research has taken into account the dialogue dimension [Litman 1984, Wilensky 1984, Carberry 1986] and shown the strong interrelation between dialogue and plans but has 186 been mostly concerned with information-seeking dialogues in which the user seems to have an implicit knowledge of what the system can or cannot do. These dialogues often produce patterns of the type &amp;quot;question from the user requiring adequate answer from the system&amp;quot; and seldom consider a possible initiative on the system&apos;s behalf. Dialogue parsing is yet </context>
</contexts>
<marker>Perrault, 1986</marker>
<rawString>Perrault C.R., &amp;quot;An Application of Default Logic to Speech Act Theory,&amp;quot; Proceedings of the NATO Workshop on Structure of Multimodal Dialogues Including Voice, Venaco (France), 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reichman</author>
</authors>
<title>Extended Person-Machine Interface,&amp;quot;</title>
<date>1984</date>
<journal>Artificial Intelligence,</journal>
<volume>22</volume>
<pages>157--218</pages>
<contexts>
<context position="5173" citStr="Reichman 1984" startWordPosition="797" endWordPosition="798">s taken into account the dialogue dimension [Litman 1984, Wilensky 1984, Carberry 1986] and shown the strong interrelation between dialogue and plans but has 186 been mostly concerned with information-seeking dialogues in which the user seems to have an implicit knowledge of what the system can or cannot do. These dialogues often produce patterns of the type &amp;quot;question from the user requiring adequate answer from the system&amp;quot; and seldom consider a possible initiative on the system&apos;s behalf. Dialogue parsing is yet another approach which attempts to formalize the surface structure of a dialogue [Reichman 1984, Wachtel 1986]. It could however lead to some rigidity in the interaction between the user and the system; for instance, it may not provide the adequate primitive elements to detect and repair communicative failures in the dialogue. A possible way out would be to account for this surface structure of dialogue within a theory of pragmatics [Airenti 1986]. In person-machine advice-giving dialogue the challenge we face is how to make the expertise of the system accessible to the user in order to satisfy her/his needs: the &amp;quot;expertness&amp;quot; of the system and the &amp;quot;noviceness&amp;quot; of the user force a compro</context>
</contexts>
<marker>Reichman, 1984</marker>
<rawString>Reichman R., &amp;quot;Extended Person-Machine Interface,&amp;quot; Artificial Intelligence, vol. 22, pp. 157-218, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Searle</author>
</authors>
<title>Speech Acts: An Essay in the Philosophy of Language,</title>
<date>1969</date>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="3978" citStr="Searle 1969" startWordPosition="607" endWordPosition="608">ue: — the system has intentions and extensive knowledge about the domain which are a priori unknown to the user; — the user&apos;s intentions must be interpreted in terms of the system&apos;s abilities or inabilities. We can see the first point as a manifestation of the &amp;quot;expertness&amp;quot; of the system, and the second as a manifestation of the &amp;quot;noviceness&amp;quot; of the user. We briefly recall some other research issues connected to our work and then elaborate on the specific aspects of person-machine advice-giving dialogue. Many efforts have been devoted to developing a general theory for speech act understanding [Searle 1969, Allen 1980, Cohen 1979]. Recognizing the illocutionary force of a speech act allows the system to reason about the intentions of the user and to behave accordingly. Most work in this field addresses only isolated speech acts or sometimes single utterances and is not concerned with a possible dialogue setting. Recent attempts, however, for reformulating speech act analysis inside a general theory of action [Cohen 1986] or for applying default reasoning to speech act understanding [Perrault 1986] may yet facilitate an extension to a whole dialogue. Another line of research has taken into accou</context>
</contexts>
<marker>Searle, 1969</marker>
<rawString>Searle J., Speech Acts: An Essay in the Philosophy of Language, Cambridge University Press, 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B M Slator</author>
<author>M P Anderson</author>
<author>W Conley</author>
</authors>
<title>Pygmalion at the Interface,&amp;quot;</title>
<date>1986</date>
<journal>Communications of the ACM,</journal>
<volume>29</volume>
<pages>599--604</pages>
<marker>Slator, Anderson, Conley, 1986</marker>
<rawString>Slator B.M., Anderson M.P., and Conley W., &amp;quot;Pygmalion at the Interface,&amp;quot; Communications of the ACM, vol. 29 , no. 7 , pp. 599-604, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wachtel</author>
</authors>
<title>Pragmatic Sensitivity in NL Interfaces and the Structure of Conversation,&amp;quot;</title>
<date>1986</date>
<booktitle>Proceedings of the XIth International Conference on Computational Linguistics,</booktitle>
<pages>35--41</pages>
<location>Bonn (FR</location>
<contexts>
<context position="5187" citStr="Wachtel 1986" startWordPosition="799" endWordPosition="800">count the dialogue dimension [Litman 1984, Wilensky 1984, Carberry 1986] and shown the strong interrelation between dialogue and plans but has 186 been mostly concerned with information-seeking dialogues in which the user seems to have an implicit knowledge of what the system can or cannot do. These dialogues often produce patterns of the type &amp;quot;question from the user requiring adequate answer from the system&amp;quot; and seldom consider a possible initiative on the system&apos;s behalf. Dialogue parsing is yet another approach which attempts to formalize the surface structure of a dialogue [Reichman 1984, Wachtel 1986]. It could however lead to some rigidity in the interaction between the user and the system; for instance, it may not provide the adequate primitive elements to detect and repair communicative failures in the dialogue. A possible way out would be to account for this surface structure of dialogue within a theory of pragmatics [Airenti 1986]. In person-machine advice-giving dialogue the challenge we face is how to make the expertise of the system accessible to the user in order to satisfy her/his needs: the &amp;quot;expertness&amp;quot; of the system and the &amp;quot;noviceness&amp;quot; of the user force a compromise between t</context>
</contexts>
<marker>Wachtel, 1986</marker>
<rawString>Wachtel T., &amp;quot;Pragmatic Sensitivity in NL Interfaces and the Structure of Conversation,&amp;quot; Proceedings of the XIth International Conference on Computational Linguistics, pp. 35-41, Bonn (FR Germany), 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wilensky</author>
<author>Y Arens</author>
<author>D Chin</author>
</authors>
<title>Talking to UNIX in English: An Overview of UC,&amp;quot;</title>
<date>1984</date>
<journal>Communications of the ACM,</journal>
<volume>27</volume>
<pages>574--593</pages>
<marker>Wilensky, Arens, Chin, 1984</marker>
<rawString>Wilensky R., Arens Y., and Chin D., &amp;quot;Talking to UNIX in English: An Overview of UC,&amp;quot; Communications of the ACM, vol. 27, no. 6, pp. 574-593, 1984.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>