<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.983191">
Phoneme-to-Text Transcription System with an Infinite Vocabulary
</title>
<author confidence="0.898688">
Shinsuke Mori Daisuke Takuma Gakuto Kurata
</author>
<affiliation confidence="0.877844">
IBM Research, Tokyo Research Laboratory, IBM Japan, Ltd.
</affiliation>
<address confidence="0.893829">
1623-14 Shimotsuruma Yamato-shi, 242-8502, Japan
</address>
<email confidence="0.994759">
mori@fw.ipsj.or.jp
</email>
<sectionHeader confidence="0.994954" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999993882352941">
The noisy channel model approach is suc-
cessfully applied to various natural lan-
guage processing tasks. Currently the
main research focus of this approach is
adaptation methods, how to capture char-
acteristics of words and expressions in a
target domain given example sentences in
that domain. As a solution we describe a
method enlarging the vocabulary of a lan-
guage model to an almost infinite size and
capturing their context information. Espe-
cially the new method is suitable for lan-
guages in which words are not delimited
by whitespace. We applied our method
to a phoneme-to-text transcription task in
Japanese and reduced about 10% of the er-
rors in the results of an existing method.
</bodyText>
<sectionHeader confidence="0.998131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997968852459">
The noisy channel model approach is being suc-
cessfully applied to various natural language pro-
cessing (NLP) tasks, such as speech recognition
(Jelinek, 1985), spelling correction (Kernighan
et al., 1990), machine translation (Brown et al.,
1990), etc. In this approach an NLP system
is composed of two modules: one is a task-
dependent part (an acoustic model for speech
recognition) which describes a relationship be-
tween an input signal sequence and a word, the
other is a language model (LM) which measures
the likelihood of a sequence of words as a sen-
tence in the language. Since the LM is a common
part, its improvement augments the accuracies of
all NLP systems based on a noisy channel model.
Recently the main research focus of LM is shift-
ing to the adaptation method, how to capture the
characteristics of words and expressions in a tar-
get domain. The standard adaptation method is to
prepare a corpus in the application domain, count
the frequencies of words and word sequences, and
manually annotate new words with their input sig-
nal sequences to be added to the vocabulary. It is
now easy to gather machine-readable sentences in
various domains because of the ease of publication
and access via the Web (Kilgarriff and Grefen-
stette, 2003). In addition, traditional machine-
readable forms of medical reports or business re-
ports are also available. When we need to develop
an NLP system in various domains, there is a huge
but unannotated corpus.
For languages, such as Japanese and Chinese, in
which the words are not delimited by whitespace,
one encounters a word identification problem be-
fore counting the frequencies of words and word
sequences. To solve this problem one must have a
good word segmenter in the domain of the corpus.
The only robust and reliable word segmenter in the
domain is, however, a word segmenter based on
the statistics of the lexicons in the domain! Thus
we are obliged to pay a high cost for the manual
annotation of a corpus for each new subject do-
main.
In this paper, we propose a novel framework for
building an NLP system based on a noisy chan-
nel model with an almost infinite vocabulary. In
our method, first we estimate the probability of a
word boundary existing between two characters at
each point of a raw corpus in the target domain.
Using these probabilities we regard the corpus as
a stochastically segmented corpus (SSC). We then
estimate word n-gram probabilities from the SSC.
Then we build an NLP system, the phoneme-to-
text transcription system in this paper. To de-
scribe the stochastic relationship between a char-
acter sequence and its phoneme sequence, we also
propose a character-based unknown word model.
With this unknown word model and a word n-
gram model estimated from the SSC, the vocab-
ulary of our LM, a set of known words with their
context information, is expanded from words in a
</bodyText>
<note confidence="0.720552">
729
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 729–736,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.777305875">
pronounced with different intonations. Intona-
tional signals are, however, omitted in the input
of phoneme-to-text transcription.
• Lack of word boundaries: A word of a long
sequence of phonemes can be split into sev-
eral shorter words, such as frequent content
words, particles, etc. (ex. a-ri-ga-to-u/thanks
vs. a-ri/ant ga/is to-u/ten).
</bodyText>
<listItem confidence="0.708011666666667">
• Variations in writing: Some words have more
than one acceptable spellings. For example,
/fu-ri-ko-mi/bank-transfer is often writ-
</listItem>
<bodyText confidence="0.999205428571429">
ten as /fu-ri-ko-mi omitting two verbal end-
ings, especially in business writing.
Most of these ambiguities are not difficult to re-
solve for a native speaker who is familiar with the
domain. So the transcription system should offer
the candidate word sequences for each context and
domain.
</bodyText>
<subsectionHeader confidence="0.975281">
2.3 Available Resources
</subsectionHeader>
<bodyText confidence="0.96912">
Generally speaking, three resources are available
for a phoneme-to-text transcription based on the
noisy channel model:
</bodyText>
<figure confidence="0.914386">
• annotated corpus:
a small corpus in the general domain annotated
with word boundary information and phoneme
sequences for each word
• single character dictionary:
a dictionary containing all possible phoneme se-
quences for each single character
• raw corpus in the target domain:
a collection of text samples in the target do-
main extracted from the Web or documents in
machine-readable form
3 Language Model and its Application
</figure>
<bodyText confidence="0.992891">
A stochastic LM M is a function from a sequence
of characters x E X* to the probability. The sum-
mation over all possible sequences of characters
must be equal to or less than 1. This probability is
used as the likelihood in the NLP system.
</bodyText>
<subsectionHeader confidence="0.993875">
3.1 Word N-gram Model
</subsectionHeader>
<bodyText confidence="0.99995525">
The most famous LM is an n-gram model based
on words. In this model, a sentence is regarded as
a word sequence wh (= w1w2 • • • wh) and words
are predicted from beginning to end:
</bodyText>
<equation confidence="0.999576">
MW&apos;.(w) = h+111 P(wijw�—1
i=1 ~—n+1)1
</equation>
<bodyText confidence="0.999974666666667">
small annotated corpus to an almost infinite size,
including all substrings appearing in the large cor-
pus in the target domain. In experiments, we esti-
mated LMs from a relatively small annotated cor-
pus in the general domain and a large raw corpus
in the target domain. A phoneme-to-text transcrip-
tion system based on our LM and unknown word
model eliminated about 10% of the errors in the
results of an existing method.
</bodyText>
<sectionHeader confidence="0.94048" genericHeader="method">
2 Task Complexity
</sectionHeader>
<bodyText confidence="0.999876666666667">
In this section we explain the phoneme-to-text
transcription task which our new framework is ap-
plied to.
</bodyText>
<subsectionHeader confidence="0.975579">
2.1 Phoneme-to-text Transcription
</subsectionHeader>
<bodyText confidence="0.999985227272727">
To input a sentence in a language using a device
with fewer keys than the alphabet we need some
kind of transcription system. In French stenotypy,
for example, a special keyboard with 21 keys is
used to input French letters with accents (Der-
ouault and Merialdo, 1986). A similar problem
arises when we write an e-mail in any language
with a mobile phone or a PDA. For languages
with a much larger character set, such as Chi-
nese, Japanese, and Korean, a transcription system
called an input method is indispensable for writing
on a computer (Lunde, 1998).
The task we chose for the evaluation of
our method is phoneme-to-text transcription in
Japanese, which can also be regarded as a pseudo-
speech recognition in which the acoustic model
is perfect. In order to input Japanese to a com-
puter, the user types phoneme sequences and the
computer offers possible transcription candidates
in the descending order of their estimated simi-
larities to the characters the user wants to input.1
Then the user chooses the proper one.
</bodyText>
<subsectionHeader confidence="0.9958">
2.2 Ambiguities
</subsectionHeader>
<bodyText confidence="0.9986448">
A phoneme sequence in Japanese (written in sans-
serif font in this paper) is highly ambiguous for
a computer. There are many possible word se-
quences with similar pronunciations. These am-
biguities are mainly due to three factors:
</bodyText>
<listItem confidence="0.929561333333333">
• Homonyms: There are many words sharing the
same phoneme sequences. In the spoken lan-
guage, they are less ambiguous since they are
1 Generally one of Japanese phonogram sets is used as
phoneme. A phonogram is input by a combination of un-
ambiguous ASCII characters.
</listItem>
<page confidence="0.725389">
730
</page>
<bodyText confidence="0.999851857142857">
where wi (i &lt; 0) and wh+1 is a special symbol
called a BT (boundary token). Since it is impossi-
ble to define the complete vocabulary, we prepare
a special token UW for unknown words and an un-
known word spelling xh&apos; �is predicted by the fol-
lowing character-based n-gram model after UW is
predicted by Mw�n:
The other part in the above formula P (yIx) is a
PM representing the probability that a given sen-
tence x is pronounced as y. Since it is impossible
to collect the phoneme sequences y for all pos-
sible sentences x, the model is decomposed into
a word-based model My in which the words are
pronounced independently
</bodyText>
<equation confidence="0.900504">
M~~~( 11 P(x������ My,w(ylw) _ h P(yilwi), (3)
� ) _ �����), (1) ���
</equation>
<bodyText confidence="0.998804">
where xi (i &lt; 0) and xh +1 is a special symbol BT.
Thus, when wi is outside of the vocabulary W,
</bodyText>
<equation confidence="0.904116">
P(wilw���
~-n+1) _ Mx,n(wi)P(UWIw���
~-n+1).
</equation>
<subsectionHeader confidence="0.999358">
3.2 Automatic Word Segmentation
</subsectionHeader>
<bodyText confidence="0.9999494">
Nagata (1994) proposed a stochastic word seg-
menter based on a word n-gram model to solve
the word segmentation problem. According to this
method, the word segmenter divides a sentence x
into a word sequence with the highest probability
</bodyText>
<equation confidence="0.9904645">
w _ argmax
w=X
</equation>
<bodyText confidence="0.998409">
Nagata (1994) reported an accuracy of about 97%
on a test corpus in the same domain using a learn-
ing corpus of 10,945 sentences in Japanese.
</bodyText>
<subsectionHeader confidence="0.995006">
3.3 Phoneme-to-text Transcription
</subsectionHeader>
<bodyText confidence="0.9852732">
A phoneme-to-text transcription system based on
an LM T (Mori et al., 1999) receives a phoneme
sequence y and returns a list of candidate sen-
tences (X1, X2, • • •) in descending order of the
probability P(xIy):
</bodyText>
<equation confidence="0.793763">
T(y) _ (X1, X2, ...),
</equation>
<bodyText confidence="0.9977905">
where i &lt; j 4=� P(xily) &gt; P(xjly).
Similar to speech recognition, the probability is
decomposed into two independent parts: a pronun-
ciation model (PM) and an LM.
</bodyText>
<equation confidence="0.98952">
P(Xily) &gt;_ P(Xjly)
P(y)
4=� P(ylxi)P(xi) &gt;_ P(ylxj)P(xj) (2)
(•.• P (y) is independent of xi and xj.)
</equation>
<bodyText confidence="0.999945625">
In this formula P(x) is an LM representing the
likelihood of a sentence x. For the LM, we can
use a word n-gram model we explained above.
where yi is a phoneme sequence corresponding to
the word wi and the condition y _ y � is met.
The probabilities P (yiIwi) are estimated from
a corpus in which each word is annotated with a
phoneme sequence as follows:
</bodyText>
<equation confidence="0.993676">
P(yilwi) _ f(yi, wi) (4)
f (wi)
</equation>
<bodyText confidence="0.999972833333333">
where f(e) stands for the frequency of an event e
in the corpus. For unknown words no transcription
model has been proposed and the phoneme-to-text
transcription system (Mori et al., 1999) simply re-
turns the phoneme sequence itself.2 This is done
by replacing the unknown word model based on
the Japanese character set Mx,,(x) by a model
based on the phonemic alphabet My�n(y).
Thus the candidate evaluation metric of a
phoneme-to-text transcription (Mori et al., 1999)
composed of the word n-gram model and the
word-based pronunciation model is as follows:
</bodyText>
<equation confidence="0.994556333333333">
h
P(ylx)P(x) _ P(YiIwi)P(wi)
���
�P(yilwi)P(wi) (5)
P(wilWi—_n+JP(yilwi) if wi E W,
P(UWIwi-n+1)MY&gt;n(yi) if wi V W.
</equation>
<sectionHeader confidence="0.957078" genericHeader="method">
4 LM Estimation from a Stochastically
</sectionHeader>
<subsectionHeader confidence="0.931647">
Segmented Corpus (SSC)
</subsectionHeader>
<bodyText confidence="0.999562071428571">
To cope with segmentation errors, the concept
of stochastic segmentation is proposed (Mori and
Takuma, 2004). In this section, we briefly explain
a method of calculating word n-gram probabilities
on a stochastically segmented corpus in the target
domain. For a detailed explanation and proofs of
the mathematical soundness, please refer to the pa-
per (Mori and Takuma, 2004).
2 One of the Japanese syllabaries Katakana is used to spell
out imported words by imitating their Japanese-constrained
pronunciation and the phoneme sequence itself is the correct
transcription result for them. Mori et. al. (1999) reported that
approximately 33.0% of the unknown words in a test corpus
were imported words.
</bodyText>
<equation confidence="0.552957642857143">
Mw�n(W).
P(ylxi)P(xi) &gt; P(yl xj)P(xj)
P(y)
_
731
xi xb1 xe1 xb2 xe2
xbn xbn+1 xen
xk+1
wn
w1 w2
f (w n ) =
r 1
Pi(1 -Pb1) Pe1 (1 -Pb2) Pe2
(1 -Pbn) (1-Pbn+1) Pen
</equation>
<figureCaption confidence="0.996947">
Figure 1: Word n-gram frequency in a stochastically segmented corpus (SSC).
</figureCaption>
<subsectionHeader confidence="0.998749">
4.1 Stochastically Segmented Corpus (SSC)
</subsectionHeader>
<bodyText confidence="0.9993228">
A stochastically segmented corpus (SSC) is de-
fined as a combination of a raw corpus Cr (here-
after referred to as the character sequence ���
1 )
and word boundary probabilities P that a word
boundary exists between two characters x� and
xi+1. Since there are word boundaries before the
first character and after the last character of the
corpus, Pp = Pnr = 1.
In (Mori and Takuma, 2004), the word bound-
ary probabilities are defined as follows. First the
word boundary estimation accuracy a of an auto-
matic word segmenter is calculated on a test cor-
pus with word boundary information. Then the
raw corpus is segmented by the word segmenter.
Finally Pi is set to be a for each i where the word
segmenter put a word boundary and P is set to
be 1 — a for each i where it did not put a word
boundary. We adopted the same method in the ex-
periments.
</bodyText>
<subsectionHeader confidence="0.98091">
4.2 Word n-gram Frequency
</subsectionHeader>
<bodyText confidence="0.99067625">
Word n-gram frequencies on an SSC is calculated
as follows:
Word 0-gram frequency: This is defined as an
expected number of words in the SSC:
</bodyText>
<equation confidence="0.8494515">
n,-1
f(&apos;)=1+ Pi.
</equation>
<bodyText confidence="0.99317325">
Word n-gram frequency (n &gt; 1): Let us think
of a situation (see Figure 1) in which a word se-
quence wi occurs in the SSC as a subsequence
beginning at the (i + 1)-th character and end-
ing at the k-th character and each word wn,,
in the word sequence is equal to the character
sequence beginning at the bn-th character and
ending at the en,-th character (xbm = wn,, 1 &lt;
</bodyText>
<equation confidence="0.998687">
m —
Vm&lt;n;en,+l=bn,+1i 1&lt;Vm&lt;n—l;
b1 = i + l; en = h). The word n-gram fre-
</equation>
<bodyText confidence="0.950344">
quency of a word sequence fr(wl) in the SSC is
defined by the summation of the stochastic fre-
quency at each occurrence of the character se-
quence of the word sequence wi over all of the
occurrences in the SSC:
</bodyText>
<equation confidence="0.929267166666667">
��
1Z em-1
f, (W�1)
— Pi
��1111 �
(1 — Pj)Pem
</equation>
<bodyText confidence="0.9347015">
where e1 = (e1, e2, &apos; &apos; &apos; �en) and On =
{(i, el) Xbm = wn,, 1 &lt; m &lt; n}.
</bodyText>
<subsectionHeader confidence="0.983266">
4.3 Word n-gram probability
</subsectionHeader>
<bodyText confidence="0.9999352">
Similar to the word n-gram probability estimation
from a decisively segmented corpus, word n-gram
probabilities in an SSC are estimated by the maxi-
mum likelihood estimation method as relative val-
ues of word n-gram frequencies:
</bodyText>
<equation confidence="0.9994605">
P,(W) = f,(W)
frW �
~ ~~~ ~~-1
1 � = P �Wwi1) (n &gt; 2).
</equation>
<sectionHeader confidence="0.9141995" genericHeader="method">
5 Phoneme-to-Text Transcription with
an Infinite Vocabulary
</sectionHeader>
<bodyText confidence="0.999932857142857">
The vocabulary of an LM estimated from an
SSC consists of all subsequences occurring in it.
Adding a module describing a stochastic relation-
ship between these subsequences and input signal
sequences, we can build a phoneme-to-text tran-
scription system equipped with an almost infinite
vocabulary.
</bodyText>
<subsectionHeader confidence="0.988539">
5.1 Word Candidate Enumeration
</subsectionHeader>
<bodyText confidence="0.997825666666667">
Given a phoneme sequence as an input, the dic-
tionary of a phoneme-to-text transcription system
described in Subsection 3.3 returns pairs of a word
and a probability per Equation (4). Similarly, the
dictionary of a phoneme-to-text system with an in-
finite vocabulary must be able to take a phoneme
sequence y and return all possible pairs of a char-
acter sequence w and the probability P (y w) as
word candidates. This is done as follows:
</bodyText>
<listItem confidence="0.86257">
1. First we prepare a single character dictionary
containing all characters x in the language an-
notated with their all possible phoneme se-
</listItem>
<figure confidence="0.3354335">
quences Yx = {y1i y2i ..., yk}. For
��&gt;ei�EDn
m=1 j=b,,,
732
</figure>
<bodyText confidence="0.998884">
example, the Japanese single character dictio-
nary contains a character x = “ ” annotated
with its all possible phoneme sequences Y =
{bi, hi,jitsu, ka, ni, nichi, nit}.
</bodyText>
<listItem confidence="0.714556928571429">
2. Then we build a phoneme-to-text transcrip-
tion system for single characters equipped with
the vocabulary consisting of the union set of
phoneme sequences for all characters. Given
a phoneme sequence y, this module returns all
possible character sequences w with its gener-
ation probability P (yIw). For example, given
a subsequence of the input phoneme sequence
y = nittere, this module returns W = {
, , , , ,
, � �} as a word candidate set along with their
generation probabilities.
3. There are various methods to calculate the
probability P(yIw). The only condition is that
</listItem>
<bodyText confidence="0.777777">
given w = x1x2 • • • xr,,, P(ylw) must be a
stochastic language model (cf. Section 3) on the
alphabet Y. In the experiments, we assumed the
uniform distribution of phoneme sequences for
each character as follows:
</bodyText>
<equation confidence="0.99865">
P(YIw) = P(yIx1x2 ... xm) =
</equation>
<bodyText confidence="0.999984571428571">
The module we described above receives a
phoneme sequence and enumerates its decomposi-
tions to subsequences contained in the single char-
acter dictionary. This module is implemented us-
ing a dynamic programming method. In the ex-
periments we limited the maximum length of the
input to 16 phonemes.
</bodyText>
<subsectionHeader confidence="0.999084">
5.2 Modeling Contexts of Word Candidates
</subsectionHeader>
<bodyText confidence="0.99995225">
Word n-gram probability estimated from an SSC
may not be as accurate as an LM estimated from a
corpus segmented appropriately by hand. Thus we
use the following interpolation technique:
</bodyText>
<equation confidence="0.988258">
P (wiIHi) = A,Ps(wiIHi) + A,Pr(wiIHi),
</equation>
<bodyText confidence="0.992958">
where Hi is history before wi, PS is the probabil-
ity estimated from a segmented corpus CS, and Pr
is the probability estimated by our method from a
raw corpus Cr. The As and Ar are interpolation
coefficients which are estimated by the deleted in-
terpolation method (Jelinek et al., 1991).
3 More precisely, it may happen that the same phoneme
sequence is generated from a character sequence in multiple
ways. In this case the generation probability is calculated as
the summation over all possible generations.
In the experiments, the word bi-gram model in
our phoneme-to-text transcription system is com-
bined with word bi-gram probabilities estimated
from an SSC. Thus the phoneme-to-text transcrip-
tion system of our new framework refers to the
following LM to measure the likelihood of word
sequences:
</bodyText>
<equation confidence="0.970662166666667">
P(wz) (7)
{ AsPs(wilwi-1) + ArPr(wiIwi-1)
if w2 E W,
AsPs(UWIwi-1)Mx,n(wi)+ArPr(wilwi-1)
if wi V W n wi E Sr,
A,Ps(UWIwi-1)Mx,n(wi), P�(wi) = 0
</equation>
<bodyText confidence="0.988392678571429">
if wi V W n wi V Sr,
where Sr is the set of all subsequences appearing
in the SSC.
Our LM based on Equation (7) and an existing
LM (cf. Equation (5)) behave differently when
they predict an out-of-vocabulary word appearing
in the SSC, that is wi V W n wi E Sr. In
this case our LM has reliable context informa-
tion on the OOV word to help the system choose
the proper word. Our system also clearly func-
tions better than the LM interpolated with a word
n-gram model estimated from the automatic seg-
mentation result of the corpus when the result is a
wrong segmentation. For example, when the au-
tomatic segmentation result of the sequence “
” (the abbreviation of Japan TV broadcasting
corporation) has a word boundary between “ ”
and “ ,” the uni-gram probability P( ) is
equal to 0 and an OOV word “ ” is never
enumerated as a candidate.TTo the contrary, us-
ing our method P ( ) &gt; 0 when the sequence
“ ” appears in the SSC at least once. Thus
the sequence is enumerated as a candidate word.
In addition, when the sequence appears frequently
in the SSC, P( ) » 0 and the word may ap-
pear at a high position in the candidate list even if
the automatic segmenter always wrongly segments
the sequence into “ ” and “ .”
</bodyText>
<subsectionHeader confidence="0.995595">
5.3 Default Character for Phoneme
</subsectionHeader>
<bodyText confidence="0.95689375">
In very rare cases, it happens that the input
phoneme sequence cannot be decomposed into
phoneme sequences in the vocabulary and those
4 Two word fragments “ ” and “ ” may be enumer-
ated as word candidates. The notion of word may be neces-
sary for the user’s facility. However, we do not discuss the
necessity of the notion of word in the phoneme-to-text tran-
scription system.
</bodyText>
<equation confidence="0.9944188">
1 /
yxzl• l6)
m
i=1
=
</equation>
<page confidence="0.50346">
733
</page>
<bodyText confidence="0.999240363636364">
corresponding to subsequences of the SSC and,
as a result, the transcription system does not out-
put any candidate sentence. To avoid this sit-
uation, we prepare a default character for every
phoneme and the transcription system also enu-
merates the default character for each phoneme. In
Japanese from the viewpoint of transcription ac-
curacy, it is better to set the default characters to
katakana, which are used mainly for translitera-
tion of imported words. Since a katakana is pro-
nunced uniquely (IY,,I = 1),
</bodyText>
<equation confidence="0.691183">
P(yIw) = P(yIX1X2 ... Xm) = 1. (8)
From Equations (4), (6), and (8), the PM of our
transcription system is as follows:
P(yiIwi) (9)
f(yi, wi) if wi E W,
f (wi)
� I���I,if wi E�W n wi E Sr,
l, if wi E� W n wi E� Sr,
</equation>
<bodyText confidence="0.9986945">
where wi = X1X2 ... X
.
</bodyText>
<subsectionHeader confidence="0.938703">
5.4 Phoneme-to-Text Transcription with an
Infinite Vocabulary
</subsectionHeader>
<bodyText confidence="0.995210166666667">
Finally, the transcription system with an infinite
vocabulary enumerates candidate sentence x _
w1w2 . . . wh in the descending order of the follow-
ing evaluation function value composed of an LM
P (wi) defined by Equation (7) and a PM P (yiIwi)
defined by Equation (9):
</bodyText>
<equation confidence="0.988784">
h
P(yIx)P(x) � P(yiIwi)P(wi)
i=1
</equation>
<bodyText confidence="0.998178333333333">
Note that there are only three cases since the case
decompositions in Equation (7) and Equation (9)
are identical.
</bodyText>
<sectionHeader confidence="0.995926" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999783272727273">
As an evaluation of our phoneme-to-text transcrip-
tion system, we measured transcription accuracies
of several systems on test corpora in two domains:
one is a general domain in which we have a small
annotated corpus with word boundary information
and phoneme sequence for each word, and the
other is a target domain in which only a large raw
corpus is available. As the transcription result, we
took the word sequence of the highest probability.
In this section we show the results and evaluate
our new framework.
</bodyText>
<tableCaption confidence="0.999549">
Table 1: Annotated corpus in general domain
</tableCaption>
<table confidence="0.989167666666667">
#sentences #words #chars
learning 20,808 406,021 598,264
test 2,311 45,180 66,874
</table>
<tableCaption confidence="0.975567">
Table 2: Raw corpus in the target domain
</tableCaption>
<table confidence="0.986336666666667">
#sentences #words #chars
learning 797,345 — 17,645,920
test 1,000 — 20,935
</table>
<subsectionHeader confidence="0.994846">
6.1 Conditions on the Experiments
</subsectionHeader>
<bodyText confidence="0.999994166666667">
The segmented corpus used in our experiments is
composed of articles extracted from newspapers
and example sentences in a dictionary of daily
conversation. Each sentence in the corpus is seg-
mented into words and each word is annotated
with a phoneme sequence. The corpus was di-
vided into ten parts. The parameters of the model
were estimated from nine of them (learning) and
the model was tested on the remaining one (test).
Table 1 shows the corpus size. Another corpus
we used in the experiments is composed of daily
business reports. This corpus is not annotated
with word boundary information nor phoneme se-
quence for each word. For evaluation, we se-
lected 1,000 sentences randomly and annotated
them with the phoneme sequences to be used as
a test set. The rest was used for LM estimation
(see Table 2).
</bodyText>
<subsectionHeader confidence="0.99905">
6.2 Evaluation Criterion
</subsectionHeader>
<bodyText confidence="0.999980888888889">
The criterion we used for transcription systems is
precision and recall based on the number of char-
acters in the longest common subsequence (LCS)
(Aho, 1990). Let NCOR be the number of char-
acters in the correct sentence, NsYs be that in the
output of a system, and NLCS be that of the LCS
of the correct sentence and the output of the sys-
tem, so the recall is defined as NLCSINCOR and
the precision as NLCSINsY s.
</bodyText>
<subsectionHeader confidence="0.98962">
6.3 Models for Comparison
</subsectionHeader>
<bodyText confidence="0.99999075">
In order to clarify the difference in the usages of
the target domain corpus, we built four transcrip-
tion systems and compared their accuracies. Be-
low we explain the models in detail.
</bodyText>
<subsectionHeader confidence="0.410366">
Model S: Baseline
</subsectionHeader>
<bodyText confidence="0.9078965">
A word bi-gram model built from the segmented
general domain corpus.
</bodyText>
<equation confidence="0.77383775">
� {
m
H
j=1
</equation>
<page confidence="0.804974">
734
</page>
<tableCaption confidence="0.99739">
Table 3: Phoneme-to-text transcription accuracy.
</tableCaption>
<table confidence="0.996344">
word bi-gram from raw corpus unknown General domain Target domain Recall
the annotated corpus usage word model Precision Recall Precision
S Yes No No 89.80% 92.30% 68.62% 78.40%
D Yes Auto. Seg. No 92.67% 93.42% 80.59% 86.19%
D&apos; Yes Auto. Seg. Yes 92.52% 93.17% 90.35% 93.48%
S Yes Stoch. Seg. Yes 92.78% 93.40% 91.10% 94.09%
</table>
<bodyText confidence="0.88955475">
The vocabulary contains 10,728 words appearing
in more than one corpora of the nine learning cor-
pora. The automatic word segmenter used to build
the other three models is based on the method ex-
plained in Section 3 with this LM.
Model D: Decisive segmentation
A word bi-gram model estimated from the au-
tomatic segmentation result of the target corpus
interpolated with model S.
Model D&apos;: Decisive segmentation
Model D extended with our PM for unknown
words
</bodyText>
<subsectionHeader confidence="0.383576">
Model S: Stochastic segmentation
</subsectionHeader>
<bodyText confidence="0.999326666666667">
A word bi-gram model estimated from the SSC
in the target domain interpolated with model S
and equipped with our PM for unknown words
</bodyText>
<subsectionHeader confidence="0.920744">
6.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.998710083333333">
Table 3 shows the transcription accuracy of the
models. A comparison of the accuracies in the
target domain of the Model S and Model D con-
firms the well known fact that even an automatic
segmentation result containing errors helps an LM
improve its performance. The accuracy of Model
D in the general domain is also higher than that of
Model S. From this result we can say that over-
adaptation has not occurred.
Model D&apos;, equipped with our PM for unknown
words, is a natural extension of Model D, a model
based on an existing method. The accuracy of
Model D&apos; is higher than that of Model D in the tar-
get domain, but worse in the general domain. This
is because the vocabulary of Model D~ is enlarged
with the words and the word fragments contained
in the automatic segmentation result. Though no
study has been reported on the method of Model
D&apos;, below we take Model D&apos; as an existing method
for a more severe evaluation.
Comparing the accuracies of Model D~ and
Model S in both domain, it can be said that using
our method we can build a more accurate model
than the existing methods. The main reason is that
</bodyText>
<tableCaption confidence="0.946349">
Table 4: Relationship between the raw corpus size
and the accuracies.
</tableCaption>
<table confidence="0.9975015">
Raw corpus size Precision Recall
1.765 x 105 chars(1/100) 89.18% 92.32%
1.765 x 106 chars (1/10) 90.33% 93.40%
1.765 x 107 chars (1/1) 91.10% 94.09%
</table>
<bodyText confidence="0.999537222222222">
our phoneme model PM is able to enumerate tran-
scription candidates for out-of-vocabulary words
and word n-gram probabilities estimated from the
SSC helps the model choose the appropriate ones.
A detailed study of Table 3 tells us that the re-
duction rate of character error rate (100%—recall)
of Model S in the target domain (9.36%) is much
larger than that in the general domain (3.37%).
The reason for this is that the automatic word seg-
menter tends to make mistakes around character-
istic words and expressions in the target domain
and our method is much less influenced by those
segmentation errors than the existing method is.
In order to clarify the relationship between the
size of the SSC and the transcription accuracy, we
calculated the accuracies while changing the size
of the SSC (1/1, 1/10, 1/100). The result, shown
in Table 4, shows that we can still achieve a fur-
ther improvement just by gathering more example
sentences in the target domain.
The main difference between the models is the
LM part. Thus the accuracy increase is yielded by
the LM improvements. This fact indicates that we
can expect a similar improvement in other gener-
ative NLP systems using the noisy channel model
by expanding the LM vocabulary with context in-
formation to an infinite size.
</bodyText>
<sectionHeader confidence="0.999936" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.9998768">
The well-known methods for the unknown word
problem are classified into two groups: one is to
use an unknown word model and the other is to
extract word candidates from a corpus before the
application. Below we describe the relationship
</bodyText>
<page confidence="0.735873">
735
</page>
<bodyText confidence="0.999319739130435">
between these methods and the proposed method.
In the method using an unknown word model,
first the generation probability of an unknown
word is modeled by a character n-gram, and then
an NLP system, such as a morphological analyzer,
searches for the best solution considering the pos-
sibility that all subsequences might be unknown
words (Nagata, 1994; Bazzi and Glass, 2000).
In the same way, we can build a phoneme-to-
text transcription system which can enumerate un-
known word candidates, but the LM is not able to
refer to lexical context information to choose the
appropriate word, since the unknown words are
modeled to be generated from a single state. We
solved this problem by allowing the LM to refer to
information from an SSC.
When a machine-readable corpus in the target
domain is available, we can extract word candi-
dates from the corpus with a certain criterion and
use them in application. An advantage of this
method is that all of the occurrences of each can-
didate in the corpus are considered. Nagata (1996)
proposed a method calculating word candidates
with their uni-gram frequencies using a forward-
backward algorithm. and reported that the accu-
racy of a morphological analyzer can be improved
by adding the extracted words to its vocabulary.
Comparing our method with this research, it can
be said that our method executes the word can-
didate enumeration and their context calculation
dynamically at the time of the solution search for
an NLP task, phoneme-to-text transcription here.
One of the advantages of our framework is that
the system considers all substrings in the corpus
as word candidates (that is the recall of the word
extraction is 100%) and a higher accuracy is ex-
pected using a consistent criterion, namely the
generation probability, for the word candidate enu-
meration process and solution search process.
The framework we propose in this paper, en-
larging the vocabulary to an almost infinite size,
is general and applicable to many other NLP sys-
tems based on the noisy channel model, such as
speech recognition, statistical machine translation,
etc. Our framework is potentially capable of im-
proving the accuracies in these tasks as well.
</bodyText>
<sectionHeader confidence="0.998596" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999738692307692">
In this paper we proposed a generative NLP sys-
tem with an almost infinite vocabulary for lan-
guages without obvious word boundary informa-
tion in written texts. In the experiments we com-
pared four phoneme-to-text transcription systems
in Japanese. The transcription system equipped
with an infinite vocabulary showed a higher accu-
racy than the baseline model and the model based
on the existing method. These results show the
efficacy of our method and tell us that our ap-
proach is promising for the phoneme-to-text tran-
scription task or other NLP systems based on the
noisy channel model.
</bodyText>
<sectionHeader confidence="0.998469" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999956652173913">
Alfred V. Aho. 1990. Algorithms for finding pat-
terns in strings. In Handbook of Theoretical Com-
puter Science, volume A: Algorithms and Complex-
ity, pages 273–278. Elseveir Science Publishers.
Issam Bazzi and James R. Glass. 2000. Modeling out-
of-vocabulary words for robust speech recognition.
In Proc. of the ICSLP2000.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Frederick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Computational Linguistics, 16(2):79–85.
Anne-Marie Derouault and Bernard Merialdo. 1986.
Natural language modeling for phoneme-to-text
transcription. IEEE PAMI, 8(6):742–749.
Frederick Jelinek, Robert L. Mercer, and Salim
Roukos. 1991. Principles of lexical language
modeling for speech recognition. In Advances in
Speech Signal Processing, chapter 21, pages 651–
699. Dekker.
Frederick Jelinek. 1985. Self-organized language
modeling for speech recognition. Technical report,
IBM T. J. Watson Research Center.
Mark D. Kernighan, Kenneth W. Church, and
William A. Gale. 1990. A spelling correction pro-
gram based on a noisy channel model. In Proc. of
the COLING90, pages 205–210.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistics, 29(3):333–347.
Ken Lunde. 1998. CJKV Information Processing.
O’Reilly &amp; Associates.
Shinsuke Mori and Daisuke Takuma. 2004. Word
n-gram probability estimation from a Japanese raw
corpus. In Proc. of the ICSLP2004.
Shinsuke Mori, Tsuchiya Masatoshi, Osamu Yamaji,
and Makoto Nagao. 1999. Kana-kanji conver-
sion by a stochastic model. Transactions of IPSJ,
40(7):2946–2953. (in Japanese).
Masaaki Nagata. 1994. A stochastic Japanese morpho-
logical analyzer using a forward-DP backward-A*
n-best search algorithm. In Proc. of the COLING94,
pages 201–207.
Masaaki Nagata. 1996. Automatic extraction of
new words from Japanese texts using generalized
forward-backward search. In EMNLP.
</reference>
<page confidence="0.929299">
736
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.935099">
<title confidence="0.998965">Phoneme-to-Text Transcription System with an Infinite Vocabulary</title>
<author confidence="0.987973">Shinsuke Mori Daisuke Takuma Gakuto Kurata</author>
<affiliation confidence="0.999474">IBM Research, Tokyo Research Laboratory, IBM Japan, Ltd.</affiliation>
<address confidence="0.994577">1623-14 Shimotsuruma Yamato-shi, 242-8502, Japan</address>
<email confidence="0.988867">mori@fw.ipsj.or.jp</email>
<abstract confidence="0.9978395">The noisy channel model approach is successfully applied to various natural language processing tasks. Currently the main research focus of this approach is adaptation methods, how to capture characteristics of words and expressions in a target domain given example sentences in that domain. As a solution we describe a method enlarging the vocabulary of a language model to an almost infinite size and capturing their context information. Especially the new method is suitable for languages in which words are not delimited by whitespace. We applied our method to a phoneme-to-text transcription task in Japanese and reduced about 10% of the errors in the results of an existing method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
</authors>
<title>Algorithms for finding patterns in strings.</title>
<date>1990</date>
<booktitle>In Handbook of Theoretical Computer Science, volume A: Algorithms and Complexity,</booktitle>
<pages>273--278</pages>
<publisher>Elseveir Science Publishers.</publisher>
<contexts>
<context position="22094" citStr="Aho, 1990" startWordPosition="3812" endWordPosition="3813">del was tested on the remaining one (test). Table 1 shows the corpus size. Another corpus we used in the experiments is composed of daily business reports. This corpus is not annotated with word boundary information nor phoneme sequence for each word. For evaluation, we selected 1,000 sentences randomly and annotated them with the phoneme sequences to be used as a test set. The rest was used for LM estimation (see Table 2). 6.2 Evaluation Criterion The criterion we used for transcription systems is precision and recall based on the number of characters in the longest common subsequence (LCS) (Aho, 1990). Let NCOR be the number of characters in the correct sentence, NsYs be that in the output of a system, and NLCS be that of the LCS of the correct sentence and the output of the system, so the recall is defined as NLCSINCOR and the precision as NLCSINsY s. 6.3 Models for Comparison In order to clarify the difference in the usages of the target domain corpus, we built four transcription systems and compared their accuracies. Below we explain the models in detail. Model S: Baseline A word bi-gram model built from the segmented general domain corpus. � { m H j=1 734 Table 3: Phoneme-to-text trans</context>
</contexts>
<marker>Aho, 1990</marker>
<rawString>Alfred V. Aho. 1990. Algorithms for finding patterns in strings. In Handbook of Theoretical Computer Science, volume A: Algorithms and Complexity, pages 273–278. Elseveir Science Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Issam Bazzi</author>
<author>James R Glass</author>
</authors>
<title>Modeling outof-vocabulary words for robust speech recognition.</title>
<date>2000</date>
<booktitle>In Proc. of the ICSLP2000.</booktitle>
<contexts>
<context position="26911" citStr="Bazzi and Glass, 2000" startWordPosition="4647" endWordPosition="4650"> Work The well-known methods for the unknown word problem are classified into two groups: one is to use an unknown word model and the other is to extract word candidates from a corpus before the application. Below we describe the relationship 735 between these methods and the proposed method. In the method using an unknown word model, first the generation probability of an unknown word is modeled by a character n-gram, and then an NLP system, such as a morphological analyzer, searches for the best solution considering the possibility that all subsequences might be unknown words (Nagata, 1994; Bazzi and Glass, 2000). In the same way, we can build a phoneme-totext transcription system which can enumerate unknown word candidates, but the LM is not able to refer to lexical context information to choose the appropriate word, since the unknown words are modeled to be generated from a single state. We solved this problem by allowing the LM to refer to information from an SSC. When a machine-readable corpus in the target domain is available, we can extract word candidates from the corpus with a certain criterion and use them in application. An advantage of this method is that all of the occurrences of each cand</context>
</contexts>
<marker>Bazzi, Glass, 2000</marker>
<rawString>Issam Bazzi and James R. Glass. 2000. Modeling outof-vocabulary words for robust speech recognition. In Proc. of the ICSLP2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1190" citStr="Brown et al., 1990" startWordPosition="177" endWordPosition="180">larging the vocabulary of a language model to an almost infinite size and capturing their context information. Especially the new method is suitable for languages in which words are not delimited by whitespace. We applied our method to a phoneme-to-text transcription task in Japanese and reduced about 10% of the errors in the results of an existing method. 1 Introduction The noisy channel model approach is being successfully applied to various natural language processing (NLP) tasks, such as speech recognition (Jelinek, 1985), spelling correction (Kernighan et al., 1990), machine translation (Brown et al., 1990), etc. In this approach an NLP system is composed of two modules: one is a taskdependent part (an acoustic model for speech recognition) which describes a relationship between an input signal sequence and a word, the other is a language model (LM) which measures the likelihood of a sequence of words as a sentence in the language. Since the LM is a common part, its improvement augments the accuracies of all NLP systems based on a noisy channel model. Recently the main research focus of LM is shifting to the adaptation method, how to capture the characteristics of words and expressions in a targ</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne-Marie Derouault</author>
<author>Bernard Merialdo</author>
</authors>
<title>Natural language modeling for phoneme-to-text transcription.</title>
<date>1986</date>
<journal>IEEE PAMI,</journal>
<volume>8</volume>
<issue>6</issue>
<contexts>
<context position="6647" citStr="Derouault and Merialdo, 1986" startWordPosition="1094" endWordPosition="1098">l domain and a large raw corpus in the target domain. A phoneme-to-text transcription system based on our LM and unknown word model eliminated about 10% of the errors in the results of an existing method. 2 Task Complexity In this section we explain the phoneme-to-text transcription task which our new framework is applied to. 2.1 Phoneme-to-text Transcription To input a sentence in a language using a device with fewer keys than the alphabet we need some kind of transcription system. In French stenotypy, for example, a special keyboard with 21 keys is used to input French letters with accents (Derouault and Merialdo, 1986). A similar problem arises when we write an e-mail in any language with a mobile phone or a PDA. For languages with a much larger character set, such as Chinese, Japanese, and Korean, a transcription system called an input method is indispensable for writing on a computer (Lunde, 1998). The task we chose for the evaluation of our method is phoneme-to-text transcription in Japanese, which can also be regarded as a pseudospeech recognition in which the acoustic model is perfect. In order to input Japanese to a computer, the user types phoneme sequences and the computer offers possible transcript</context>
</contexts>
<marker>Derouault, Merialdo, 1986</marker>
<rawString>Anne-Marie Derouault and Bernard Merialdo. 1986. Natural language modeling for phoneme-to-text transcription. IEEE PAMI, 8(6):742–749.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Principles of lexical language modeling for speech recognition.</title>
<date>1991</date>
<booktitle>In Advances in Speech Signal Processing, chapter 21,</booktitle>
<pages>651--699</pages>
<publisher>Dekker.</publisher>
<contexts>
<context position="16762" citStr="Jelinek et al., 1991" startWordPosition="2871" endWordPosition="2874">periments we limited the maximum length of the input to 16 phonemes. 5.2 Modeling Contexts of Word Candidates Word n-gram probability estimated from an SSC may not be as accurate as an LM estimated from a corpus segmented appropriately by hand. Thus we use the following interpolation technique: P (wiIHi) = A,Ps(wiIHi) + A,Pr(wiIHi), where Hi is history before wi, PS is the probability estimated from a segmented corpus CS, and Pr is the probability estimated by our method from a raw corpus Cr. The As and Ar are interpolation coefficients which are estimated by the deleted interpolation method (Jelinek et al., 1991). 3 More precisely, it may happen that the same phoneme sequence is generated from a character sequence in multiple ways. In this case the generation probability is calculated as the summation over all possible generations. In the experiments, the word bi-gram model in our phoneme-to-text transcription system is combined with word bi-gram probabilities estimated from an SSC. Thus the phoneme-to-text transcription system of our new framework refers to the following LM to measure the likelihood of word sequences: P(wz) (7) { AsPs(wilwi-1) + ArPr(wiIwi-1) if w2 E W, AsPs(UWIwi-1)Mx,n(wi)+ArPr(wil</context>
</contexts>
<marker>Jelinek, Mercer, Roukos, 1991</marker>
<rawString>Frederick Jelinek, Robert L. Mercer, and Salim Roukos. 1991. Principles of lexical language modeling for speech recognition. In Advances in Speech Signal Processing, chapter 21, pages 651– 699. Dekker.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Self-organized language modeling for speech recognition.</title>
<date>1985</date>
<journal>IBM T. J. Watson Research Center.</journal>
<tech>Technical report,</tech>
<contexts>
<context position="1102" citStr="Jelinek, 1985" startWordPosition="167" endWordPosition="168">omain given example sentences in that domain. As a solution we describe a method enlarging the vocabulary of a language model to an almost infinite size and capturing their context information. Especially the new method is suitable for languages in which words are not delimited by whitespace. We applied our method to a phoneme-to-text transcription task in Japanese and reduced about 10% of the errors in the results of an existing method. 1 Introduction The noisy channel model approach is being successfully applied to various natural language processing (NLP) tasks, such as speech recognition (Jelinek, 1985), spelling correction (Kernighan et al., 1990), machine translation (Brown et al., 1990), etc. In this approach an NLP system is composed of two modules: one is a taskdependent part (an acoustic model for speech recognition) which describes a relationship between an input signal sequence and a word, the other is a language model (LM) which measures the likelihood of a sequence of words as a sentence in the language. Since the LM is a common part, its improvement augments the accuracies of all NLP systems based on a noisy channel model. Recently the main research focus of LM is shifting to the </context>
</contexts>
<marker>Jelinek, 1985</marker>
<rawString>Frederick Jelinek. 1985. Self-organized language modeling for speech recognition. Technical report, IBM T. J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark D Kernighan</author>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>A spelling correction program based on a noisy channel model.</title>
<date>1990</date>
<booktitle>In Proc. of the COLING90,</booktitle>
<pages>205--210</pages>
<contexts>
<context position="1148" citStr="Kernighan et al., 1990" startWordPosition="171" endWordPosition="174"> domain. As a solution we describe a method enlarging the vocabulary of a language model to an almost infinite size and capturing their context information. Especially the new method is suitable for languages in which words are not delimited by whitespace. We applied our method to a phoneme-to-text transcription task in Japanese and reduced about 10% of the errors in the results of an existing method. 1 Introduction The noisy channel model approach is being successfully applied to various natural language processing (NLP) tasks, such as speech recognition (Jelinek, 1985), spelling correction (Kernighan et al., 1990), machine translation (Brown et al., 1990), etc. In this approach an NLP system is composed of two modules: one is a taskdependent part (an acoustic model for speech recognition) which describes a relationship between an input signal sequence and a word, the other is a language model (LM) which measures the likelihood of a sequence of words as a sentence in the language. Since the LM is a common part, its improvement augments the accuracies of all NLP systems based on a noisy channel model. Recently the main research focus of LM is shifting to the adaptation method, how to capture the characte</context>
</contexts>
<marker>Kernighan, Church, Gale, 1990</marker>
<rawString>Mark D. Kernighan, Kenneth W. Church, and William A. Gale. 1990. A spelling correction program based on a noisy channel model. In Proc. of the COLING90, pages 205–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Gregory Grefenstette</author>
</authors>
<title>Introduction to the special issue on the web as corpus.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="2194" citStr="Kilgarriff and Grefenstette, 2003" startWordPosition="351" endWordPosition="355">rovement augments the accuracies of all NLP systems based on a noisy channel model. Recently the main research focus of LM is shifting to the adaptation method, how to capture the characteristics of words and expressions in a target domain. The standard adaptation method is to prepare a corpus in the application domain, count the frequencies of words and word sequences, and manually annotate new words with their input signal sequences to be added to the vocabulary. It is now easy to gather machine-readable sentences in various domains because of the ease of publication and access via the Web (Kilgarriff and Grefenstette, 2003). In addition, traditional machinereadable forms of medical reports or business reports are also available. When we need to develop an NLP system in various domains, there is a huge but unannotated corpus. For languages, such as Japanese and Chinese, in which the words are not delimited by whitespace, one encounters a word identification problem before counting the frequencies of words and word sequences. To solve this problem one must have a good word segmenter in the domain of the corpus. The only robust and reliable word segmenter in the domain is, however, a word segmenter based on the sta</context>
</contexts>
<marker>Kilgarriff, Grefenstette, 2003</marker>
<rawString>Adam Kilgarriff and Gregory Grefenstette. 2003. Introduction to the special issue on the web as corpus. Computational Linguistics, 29(3):333–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Lunde</author>
</authors>
<date>1998</date>
<journal>CJKV Information Processing. O’Reilly &amp; Associates.</journal>
<contexts>
<context position="6933" citStr="Lunde, 1998" startWordPosition="1147" endWordPosition="1148">framework is applied to. 2.1 Phoneme-to-text Transcription To input a sentence in a language using a device with fewer keys than the alphabet we need some kind of transcription system. In French stenotypy, for example, a special keyboard with 21 keys is used to input French letters with accents (Derouault and Merialdo, 1986). A similar problem arises when we write an e-mail in any language with a mobile phone or a PDA. For languages with a much larger character set, such as Chinese, Japanese, and Korean, a transcription system called an input method is indispensable for writing on a computer (Lunde, 1998). The task we chose for the evaluation of our method is phoneme-to-text transcription in Japanese, which can also be regarded as a pseudospeech recognition in which the acoustic model is perfect. In order to input Japanese to a computer, the user types phoneme sequences and the computer offers possible transcription candidates in the descending order of their estimated similarities to the characters the user wants to input.1 Then the user chooses the proper one. 2.2 Ambiguities A phoneme sequence in Japanese (written in sansserif font in this paper) is highly ambiguous for a computer. There ar</context>
</contexts>
<marker>Lunde, 1998</marker>
<rawString>Ken Lunde. 1998. CJKV Information Processing. O’Reilly &amp; Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Daisuke Takuma</author>
</authors>
<title>Word n-gram probability estimation from a Japanese raw corpus.</title>
<date>2004</date>
<booktitle>In Proc. of the ICSLP2004.</booktitle>
<contexts>
<context position="10929" citStr="Mori and Takuma, 2004" startWordPosition="1836" endWordPosition="1839">nce itself.2 This is done by replacing the unknown word model based on the Japanese character set Mx,,(x) by a model based on the phonemic alphabet My�n(y). Thus the candidate evaluation metric of a phoneme-to-text transcription (Mori et al., 1999) composed of the word n-gram model and the word-based pronunciation model is as follows: h P(ylx)P(x) _ P(YiIwi)P(wi) ��� �P(yilwi)P(wi) (5) P(wilWi—_n+JP(yilwi) if wi E W, P(UWIwi-n+1)MY&gt;n(yi) if wi V W. 4 LM Estimation from a Stochastically Segmented Corpus (SSC) To cope with segmentation errors, the concept of stochastic segmentation is proposed (Mori and Takuma, 2004). In this section, we briefly explain a method of calculating word n-gram probabilities on a stochastically segmented corpus in the target domain. For a detailed explanation and proofs of the mathematical soundness, please refer to the paper (Mori and Takuma, 2004). 2 One of the Japanese syllabaries Katakana is used to spell out imported words by imitating their Japanese-constrained pronunciation and the phoneme sequence itself is the correct transcription result for them. Mori et. al. (1999) reported that approximately 33.0% of the unknown words in a test corpus were imported words. Mw�n(W). </context>
<context position="12185" citStr="Mori and Takuma, 2004" startWordPosition="2051" endWordPosition="2054">_ 731 xi xb1 xe1 xb2 xe2 xbn xbn+1 xen xk+1 wn w1 w2 f (w n ) = r 1 Pi(1 -Pb1) Pe1 (1 -Pb2) Pe2 (1 -Pbn) (1-Pbn+1) Pen Figure 1: Word n-gram frequency in a stochastically segmented corpus (SSC). 4.1 Stochastically Segmented Corpus (SSC) A stochastically segmented corpus (SSC) is defined as a combination of a raw corpus Cr (hereafter referred to as the character sequence ��� 1 ) and word boundary probabilities P that a word boundary exists between two characters x� and xi+1. Since there are word boundaries before the first character and after the last character of the corpus, Pp = Pnr = 1. In (Mori and Takuma, 2004), the word boundary probabilities are defined as follows. First the word boundary estimation accuracy a of an automatic word segmenter is calculated on a test corpus with word boundary information. Then the raw corpus is segmented by the word segmenter. Finally Pi is set to be a for each i where the word segmenter put a word boundary and P is set to be 1 — a for each i where it did not put a word boundary. We adopted the same method in the experiments. 4.2 Word n-gram Frequency Word n-gram frequencies on an SSC is calculated as follows: Word 0-gram frequency: This is defined as an expected num</context>
</contexts>
<marker>Mori, Takuma, 2004</marker>
<rawString>Shinsuke Mori and Daisuke Takuma. 2004. Word n-gram probability estimation from a Japanese raw corpus. In Proc. of the ICSLP2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Tsuchiya Masatoshi</author>
<author>Osamu Yamaji</author>
<author>Makoto Nagao</author>
</authors>
<title>Kana-kanji conversion by a stochastic model.</title>
<date>1999</date>
<journal>Transactions of IPSJ,</journal>
<volume>40</volume>
<issue>7</issue>
<note>(in Japanese).</note>
<contexts>
<context position="9276" citStr="Mori et al., 1999" startWordPosition="1553" endWordPosition="1556">when wi is outside of the vocabulary W, P(wilw��� ~-n+1) _ Mx,n(wi)P(UWIw��� ~-n+1). 3.2 Automatic Word Segmentation Nagata (1994) proposed a stochastic word segmenter based on a word n-gram model to solve the word segmentation problem. According to this method, the word segmenter divides a sentence x into a word sequence with the highest probability w _ argmax w=X Nagata (1994) reported an accuracy of about 97% on a test corpus in the same domain using a learning corpus of 10,945 sentences in Japanese. 3.3 Phoneme-to-text Transcription A phoneme-to-text transcription system based on an LM T (Mori et al., 1999) receives a phoneme sequence y and returns a list of candidate sentences (X1, X2, • • •) in descending order of the probability P(xIy): T(y) _ (X1, X2, ...), where i &lt; j 4=� P(xily) &gt; P(xjly). Similar to speech recognition, the probability is decomposed into two independent parts: a pronunciation model (PM) and an LM. P(Xily) &gt;_ P(Xjly) P(y) 4=� P(ylxi)P(xi) &gt;_ P(ylxj)P(xj) (2) (•.• P (y) is independent of xi and xj.) In this formula P(x) is an LM representing the likelihood of a sentence x. For the LM, we can use a word n-gram model we explained above. where yi is a phoneme sequence correspon</context>
<context position="10555" citStr="Mori et al., 1999" startWordPosition="1780" endWordPosition="1783">babilities P (yiIwi) are estimated from a corpus in which each word is annotated with a phoneme sequence as follows: P(yilwi) _ f(yi, wi) (4) f (wi) where f(e) stands for the frequency of an event e in the corpus. For unknown words no transcription model has been proposed and the phoneme-to-text transcription system (Mori et al., 1999) simply returns the phoneme sequence itself.2 This is done by replacing the unknown word model based on the Japanese character set Mx,,(x) by a model based on the phonemic alphabet My�n(y). Thus the candidate evaluation metric of a phoneme-to-text transcription (Mori et al., 1999) composed of the word n-gram model and the word-based pronunciation model is as follows: h P(ylx)P(x) _ P(YiIwi)P(wi) ��� �P(yilwi)P(wi) (5) P(wilWi—_n+JP(yilwi) if wi E W, P(UWIwi-n+1)MY&gt;n(yi) if wi V W. 4 LM Estimation from a Stochastically Segmented Corpus (SSC) To cope with segmentation errors, the concept of stochastic segmentation is proposed (Mori and Takuma, 2004). In this section, we briefly explain a method of calculating word n-gram probabilities on a stochastically segmented corpus in the target domain. For a detailed explanation and proofs of the mathematical soundness, please ref</context>
</contexts>
<marker>Mori, Masatoshi, Yamaji, Nagao, 1999</marker>
<rawString>Shinsuke Mori, Tsuchiya Masatoshi, Osamu Yamaji, and Makoto Nagao. 1999. Kana-kanji conversion by a stochastic model. Transactions of IPSJ, 40(7):2946–2953. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A stochastic Japanese morphological analyzer using a forward-DP backward-A* n-best search algorithm.</title>
<date>1994</date>
<booktitle>In Proc. of the COLING94,</booktitle>
<pages>201--207</pages>
<contexts>
<context position="8788" citStr="Nagata (1994)" startWordPosition="1472" endWordPosition="1473"> n-gram model after UW is predicted by Mw�n: The other part in the above formula P (yIx) is a PM representing the probability that a given sentence x is pronounced as y. Since it is impossible to collect the phoneme sequences y for all possible sentences x, the model is decomposed into a word-based model My in which the words are pronounced independently M~~~( 11 P(x������ My,w(ylw) _ h P(yilwi), (3) � ) _ �����), (1) ��� where xi (i &lt; 0) and xh +1 is a special symbol BT. Thus, when wi is outside of the vocabulary W, P(wilw��� ~-n+1) _ Mx,n(wi)P(UWIw��� ~-n+1). 3.2 Automatic Word Segmentation Nagata (1994) proposed a stochastic word segmenter based on a word n-gram model to solve the word segmentation problem. According to this method, the word segmenter divides a sentence x into a word sequence with the highest probability w _ argmax w=X Nagata (1994) reported an accuracy of about 97% on a test corpus in the same domain using a learning corpus of 10,945 sentences in Japanese. 3.3 Phoneme-to-text Transcription A phoneme-to-text transcription system based on an LM T (Mori et al., 1999) receives a phoneme sequence y and returns a list of candidate sentences (X1, X2, • • •) in descending order of </context>
<context position="26887" citStr="Nagata, 1994" startWordPosition="4645" endWordPosition="4646">ize. 7 Related Work The well-known methods for the unknown word problem are classified into two groups: one is to use an unknown word model and the other is to extract word candidates from a corpus before the application. Below we describe the relationship 735 between these methods and the proposed method. In the method using an unknown word model, first the generation probability of an unknown word is modeled by a character n-gram, and then an NLP system, such as a morphological analyzer, searches for the best solution considering the possibility that all subsequences might be unknown words (Nagata, 1994; Bazzi and Glass, 2000). In the same way, we can build a phoneme-totext transcription system which can enumerate unknown word candidates, but the LM is not able to refer to lexical context information to choose the appropriate word, since the unknown words are modeled to be generated from a single state. We solved this problem by allowing the LM to refer to information from an SSC. When a machine-readable corpus in the target domain is available, we can extract word candidates from the corpus with a certain criterion and use them in application. An advantage of this method is that all of the </context>
</contexts>
<marker>Nagata, 1994</marker>
<rawString>Masaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-DP backward-A* n-best search algorithm. In Proc. of the COLING94, pages 201–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>Automatic extraction of new words from Japanese texts using generalized forward-backward search.</title>
<date>1996</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="27560" citStr="Nagata (1996)" startWordPosition="4763" endWordPosition="4764">oneme-totext transcription system which can enumerate unknown word candidates, but the LM is not able to refer to lexical context information to choose the appropriate word, since the unknown words are modeled to be generated from a single state. We solved this problem by allowing the LM to refer to information from an SSC. When a machine-readable corpus in the target domain is available, we can extract word candidates from the corpus with a certain criterion and use them in application. An advantage of this method is that all of the occurrences of each candidate in the corpus are considered. Nagata (1996) proposed a method calculating word candidates with their uni-gram frequencies using a forwardbackward algorithm. and reported that the accuracy of a morphological analyzer can be improved by adding the extracted words to its vocabulary. Comparing our method with this research, it can be said that our method executes the word candidate enumeration and their context calculation dynamically at the time of the solution search for an NLP task, phoneme-to-text transcription here. One of the advantages of our framework is that the system considers all substrings in the corpus as word candidates (tha</context>
</contexts>
<marker>Nagata, 1996</marker>
<rawString>Masaaki Nagata. 1996. Automatic extraction of new words from Japanese texts using generalized forward-backward search. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>