<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000090">
<title confidence="0.926236">
Semi-Supervised Training for Statistical Word Alignment
</title>
<note confidence="0.870244666666667">
Alexander Fraser
ISI / University of Southern California
4676 Admiralty Way, Suite 1001
</note>
<address confidence="0.464642">
Marina del Rey, CA 90292
</address>
<email confidence="0.996552">
fraser@isi.edu
</email>
<note confidence="0.871407666666667">
Daniel Marcu
ISI / University of Southern California
4676 Admiralty Way, Suite 1001
</note>
<address confidence="0.471218">
Marina del Rey, CA 90292
</address>
<email confidence="0.99843">
marcu@isi.edu
</email>
<sectionHeader confidence="0.994788" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999888454545454">
We introduce a semi-supervised approach
to training for statistical machine transla-
tion that alternates the traditional Expecta-
tion Maximization step that is applied on a
large training corpus with a discriminative
step aimed at increasing word-alignment
quality on a small, manually word-aligned
sub-corpus. We show that our algorithm
leads not only to improved alignments
but also to machine translation outputs of
higher quality.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.974366596153847">
The most widely applied training procedure for
statistical machine translation — IBM model 4
(Brown et al., 1993) unsupervised training fol-
lowed by post-processing with symmetrization
heuristics (Och and Ney, 2003) — yields low
quality word alignments. When compared with
gold standard parallel data which was manually
aligned using a high-recall/precision methodology
(Melamed, 1998), the word-level alignments pro-
duced automatically have an F-measure accuracy
of 64.6 and 76.4% (see Section 2 for details).
In this paper, we improve word alignment and,
subsequently, MT accuracy by developing a range
of increasingly sophisticated methods:
1. We first recast the problem of estimating the
IBM models (Brown et al., 1993) in a dis-
criminative framework, which leads to an ini-
tial increase in word-alignment accuracy.
2. We extend the IBM models with new
(sub)models, which leads to additional in-
creases in word-alignment accuracy. In the
process, we also show that these improve-
ments are explained not only by the power
of the new models, but also by a novel search
procedure for the alignment of highest prob-
ability.
3. Finally, we propose a training procedure that
interleaves discriminative training with max-
imum likelihood training.
These steps lead to word alignments of higher
accuracy which, in our case, correlate with higher
MT accuracy.
The rest of the paper is organized as follows.
In Section 2, we review the data sets we use to
validate experimentally our algorithms and the as-
sociated baselines. In Section 3, we present itera-
tively our contributions that eventually lead to ab-
solute increases in alignment quality of 4.8% for
French/English and 4.8% for Arabic/English, as
measured using F-measure for large word align-
ment tasks. These contributions pertain to the
casting of the training procedure in the discrim-
inative framework (Section 3.1); the IBM model
extensions and modified search procedure for the
Viterbi alignments (Section 3.2); and the in-
terleaved, minimum error/maximum likelihood,
training algorithm (Section 4). In Section 5, we as-
sess the impact that our improved alignments have
on MT quality. We conclude with a comparison of
our work with previous research on discriminative
training for word alignment and a short discussion
of semi-supervised learning.
</bodyText>
<sectionHeader confidence="0.906296" genericHeader="introduction">
2 Data Sets and Baseline
</sectionHeader>
<bodyText confidence="0.999489333333333">
We conduct experiments on alignment and
translation tasks using Arabic/English and
French/English data sets (see Table 1 for details).
Both sets have training data and two gold stan-
dard word alignments for small samples of the
training data, which we use as the alignment
</bodyText>
<page confidence="0.958401">
769
</page>
<note confidence="0.951679">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 769–776,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<table confidence="0.942356052631579">
ARABIC/ENGLISH FRENCH/ENGLISH
A E F E
SENTS 3,713,753 2,842,184
WORDS 102,473,086 119,994,972 75,794,254 67,366,819
TRAINING 489,534 231,255 149,568 114,907
VOCAB 199,749 104,155 60,651 47,765
SINGLETONS
SENTS 100 110
ALIGN DISCR. WORDS 1,712 2,010 1,888 1,726
LINKS 2,129 2,292
SENTS 55 110
ALIGN TEST WORDS 1,004 1,210 1,899 1,716
LINKS 1,368 2,176
SENTS 728 (4 REFERENCES) 833 (1 REFERENCE)
MAX BLEU 17664 22.0K TO 24.5K 20,562 17,454
WORDS
SENTS 663 (4 REFERENCES) 2,380 (1 REFERENCE)
TRANS. TEST 16,075 19.0K TO 21.6K 58,990 49,182
WORDS
</table>
<tableCaption confidence="0.992528">
Table 1: Datasets
</tableCaption>
<table confidence="0.998817666666667">
SYSTEM F-MEASURE F TO E F-MEASURE E TO F F-MEASURE BEST SYMM.
A/E MODEL 4: ITERATION 4 65.6 / 60.5 53.6 / 50.2 69.1 / 64.6 (UNION)
F/E MODEL 4: ITERATION 4 73.8 / 75.1 74.2 / 73.5 76.5 / 76.4 (REFINED)
</table>
<tableCaption confidence="0.976272">
Table 2: Baseline Results. F-measures are presented on both the alignment discriminative training set
and the alignment test set sub-corpora, separated by /.
</tableCaption>
<bodyText confidence="0.998525357142857">
discriminative training set and alignment test set.
Translation quality is evaluated by translating
a held-out translation test set. An additional
translation set called the Maximum BLEU set is
employed by the SMT system to train the weights
associated with the components of its log-linear
model (Och, 2003).
The training corpora are publicly avail-
able: both the Arabic/English data and the
French/English Hansards were released by
LDC. We created the manual word alignments
ourselves, following the Blinker guidelines
(Melamed, 1998).
To train our baseline systems we follow a stan-
dard procedure. The models were trained two
times, first using French or Arabic as the source
language and then using English as the source
language. For each training direction, we run
GIZA++ (Och and Ney, 2003), specifying 5 iter-
ations of Model 1, 4 iterations of the HMM model
(Vogel et al., 1996), and 4 iterations of Model 4.
We quantify the quality of the resulting hypothe-
sized alignments with F-measure using the manu-
ally aligned sets.
We present the results for three different con-
ditions in Table 2. For the “F to E” direction the
models assign non-zero probability to alignments
consisting of links from one Foreign word to zero
or more English words, while for “E to F” the
models assign non-zero probability to alignments
consisting of links from one English word to zero
or more Foreign words. It is standard practice to
improve the final alignments by combining the “F
to E” and “E to F” directions using symmetriza-
tion heuristics. We use the “union”, “refined” and
“intersection” heuristics defined in (Och and Ney,
2003) which are used in conjunction with IBM
Model 4 as the baseline in virtually all recent work
on word alignment. In Table 2, we report the best
symmetrized results.
The low F-measure scores of the baselines mo-
tivate our work.
</bodyText>
<sectionHeader confidence="0.996509" genericHeader="method">
3 Improving Word Alignments
</sectionHeader>
<subsectionHeader confidence="0.993824">
3.1 Discriminative Reranking of the IBM
Models
</subsectionHeader>
<bodyText confidence="0.99915625">
We reinterpret the five groups of parameters of
Model 4 listed in the first five lines of Table 3 as
sub-models of a log-linear model (see Equation 1).
Each sub-model hm has an associated weight am.
Given a vector of these weights a, the alignment
search problem, i.e. the search to return the best
alignment a� of the sentences e and f according to
the model, is specified by Equation 2.
</bodyText>
<equation confidence="0.9810168">
exp(Ei �ihi(a, e, f))
p�(f, a|e) = (1)
Ea, f, exp(Ei Aihi(a&apos;, e, f&apos;))
a� = argmax � aihi(f, a, e) (2)
a i
</equation>
<page confidence="0.986692">
770
</page>
<table confidence="0.840870928571429">
m Model 4 Description m Description
1 t(f|e) translation probs, f and e are words 9 translation table using approx. stems
2 n(ole) fertility probs, 0 is number of words generated by e 10 backoff fertility (fertility estimated
over all e)
3 null parameters used in generating Foreign words which 11 backoff fertility for words with count
are unaligned &lt;= 5
4 d1(Oj) movement probs of leftmost Foreign word translated 12 translation table from BMM iteration 4
from a particular e
5 d&gt;1(Oj) movement probs of other Foreign words translated 13 zero fertility English word penalty
from a particular e
6 translation table from refined combination of both 14 non-zero fertility English word penalty
alignments
7 translation table from union of both alignments 15 NULL Foreign word penalty
8 translation table from intersection of both alignments 16 non-NULL Foreign word penalty
</table>
<tableCaption confidence="0.997575">
Table 3: Sub-Models. Note that sub-models 1 to 5 are IBM Model 4, sub-models 6 to 16 are new.
</tableCaption>
<bodyText confidence="0.999670130434783">
Log-linear models are often trained to maxi-
mize entropy, but we will train our model di-
rectly on the final performance criterion. We use
1−F-measure as our error function, comparing hy-
pothesized word alignments for the discriminative
training set with the gold standard.
Och (2003) has described an efficient exact
one-dimensional error minimization technique for
a similar search problem in machine translation.
The technique involves calculating a piecewise
constant function f,,,(x) which evaluates the er-
ror of the hypotheses which would be picked by
equation 2 from a set of hypotheses if we hold all
weights constant, except for the weight An (which
is set to x).
The discriminative reranking algorithm is ini-
tialized with the parameters of the sub-models 0,
an initial choice of the A vector, gold standard
word alignments (labels) for the alignment dis-
criminative training set, the constant N specifying
the N-best list size used&apos;, and an empty master set
of hypothesized alignments. The algorithm is a
three step loop:
</bodyText>
<listItem confidence="0.743082666666667">
1. Enrich the master set of hypothesized align-
ments by producing an N-best list using A.
If all of the hypotheses in the N-best list are
already in the master set, the algorithm has
converged, so terminate the loop.
2. Consider the current A vector and 999 addi-
tional randomly generated vectors, setting A
to the vector with lowest error on the master
set.
3. Repeatedly run Och’s one-dimensional error
minimization step until there is no further er-
ror reduction (this results in a new vector A).
</listItem>
<footnote confidence="0.62076">
1N = 128 for our experiments
</footnote>
<subsectionHeader confidence="0.9765135">
3.2 Improvements to the Model and Search
3.2.1 New Sources of Knowledge
</subsectionHeader>
<bodyText confidence="0.920489264705882">
We define new sub-models to model factors not
captured by Model 4. These are lines 6 to 16
of Table 3, where we use the “E to F” align-
ment direction as an example. We use word-level
translation tables informed by both the “E to F”
and the “F to E” translation directions derived us-
ing the three symmetrization heuristics, the “E to
F” translation table from the final iteration of the
HMM model and an “E to F” translation table de-
rived using approximative stemming. The approx-
imative stemming sub-model (sub-model 9) uses
the first 4 letters of each vocabulary item as the
stem for English and French while for Arabic we
use the full word as the stem. We also use sub-
models for backed off fertility, and direct penal-
ization of unaligned English words (“zero fertil-
ity”) and aligned English words, and unaligned
Foreign words (“NULL-generated” words) and
aligned Foreign words. This is a small sampling
of the kinds of knowledge sources we can use in
this framework; many others have been proposed
in the literature.
Table 4 shows an evaluation of discriminative
reranking. We observe:
1. The first line is the starting point, which is
the Viterbi alignment of the 4th iteration of
HMM training.
2. The 1-to-many alignments generated by dis-
criminatively reranking Model 4 are better
than the 1-to-many alignments of four itera-
tions of Model 4.
3. The 1-to-many alignments of the discrimina-
tively reranked extended model are much bet-
ter than four iterations of Model 4.
</bodyText>
<page confidence="0.994475">
771
</page>
<table confidence="0.999684333333333">
SYSTEM F-MEASURE F TO E F-MEASURE E TO F F-MEASURE BEST SYMM.
A/E LAST ITERATION HMM 58.6 / 54.4 47.7 / 39.9 62.1 / 57.0 (UNION)
A/E MODEL 4 RERANKING 65.3 / 59.5 55.7 / 51.4 69.7 / 64.6 (UNION)
A/E EXTENDED MODEL RERANKING 68.4 / 62.2 61.6 / 57.7 72.0 / 66.4 (UNION)
A/E MODEL 4: ITERATION 4 65.6 / 60.5 53.6 / 50.2 69.1 / 64.6 (UNION)
F/E LAST ITERATION HMM 72.4 / 73.9 71.5 / 71.8 76.4 / 77.3 (REFINED)
F/E MODEL 4 RERANKING 77.9 / 77.9 78.4 / 77.7 79.2 / 79.4 (REFINED)
F/E EXTENDED MODEL RERANKING 78.7 / 80.2 79.3 / 79.6 79.6 / 80.4 (REFINED)
F/E MODEL 4: ITERATION 4 73.8 / 75.1 74.2 / 73.5 76.5 / 76.4 (REFINED)
</table>
<tableCaption confidence="0.960256">
Table 4: Discriminative Reranking with Improved Search. F-measures are presented on both the align-
ment discriminative training set and the alignment test set sub-corpora, separated by /.
</tableCaption>
<bodyText confidence="0.992537636363636">
4. The discriminatively reranked extended
model outperforms four iterations of Model
4 in both cases with the best heuristic
symmetrization, but some of the gain is
lost as we are optimizing the F-measure of
the 1-to-many alignments rather than the
F-measure of the many-to-many alignments
directly.
Overall, the results show our approach is better
than or competitive with running four iterations of
unsupervised Model 4 training.
</bodyText>
<subsectionHeader confidence="0.692376">
3.2.2 New Alignment Search Algorithm
</subsectionHeader>
<bodyText confidence="0.999818125">
Brown et al. (1993) introduced operations defin-
ing a hillclimbing search appropriate for Model 4.
Their search starts with a complete hypothesis and
exhaustively applies two operations to it, selecting
the best improved hypothesis it can find (or termi-
nating if no improved hypothesis is found). This
search makes many search errors2. We developed
a new alignment algorithm to reduce search errors:
</bodyText>
<listItem confidence="0.982038769230769">
• We perform an initial hillclimbing search (as
in the baseline algorithm) but construct a pri-
ority queue of possible other candidate align-
ments to consider.
• Alignments which are expanded are marked
so that they will not be returned to at a future
point in the search.
• The alignment search operates by consider-
ing complete hypotheses so it is an “anytime”
algorithm (meaning that it always has a cur-
rent best guess). Timers can therefore be
used to terminate the processing of the pri-
ority queue of candidate alignments.
</listItem>
<bodyText confidence="0.987056793103448">
The first two improvements are related to the
well-known Tabu local search algorithm (Glover,
2A search error in a word aligner is a failure to find the
best alignment according to the model, i.e. in our case a fail-
ure to maximize Equation 2.
1986). The third improvement is important for
restricting total time used when producing align-
ments for large training corpora.
We performed two experiments. The first evalu-
ates the number of search errors. For each corpus
we sampled 1000 sentence pairs randomly, with
no sentence length restriction. Model 4 parameters
are estimated from the final HMM Viterbi align-
ment of these sentence pairs. We then search to
try to find the Model 4 Viterbi alignment with both
the new and old algorithms, allowing them both
to process for the same amount of time. The per-
centage of known search errors is the percentage
of sentences from our sample in which we were
able to find a more probable candidate by apply-
ing our new algorithm using 24 hours of compu-
tation for just the 1000 sample sentences. Table
5 presents the results, showing that our new algo-
rithm reduced search errors in all cases, but fur-
ther reduction could be obtained. The second ex-
periment shows the impact of the new search on
discriminative reranking of Model 4 (see Table 6).
Reduced search errors lead to a better fit of the dis-
criminative training corpus.
</bodyText>
<sectionHeader confidence="0.998709" genericHeader="method">
4 Semi-Supervised Training for Word
Alignments
</sectionHeader>
<bodyText confidence="0.999816923076923">
Intuitively, in approximate EM training for Model
4 (Brown et al., 1993), the E-step corresponds to
calculating the probability of all alignments ac-
cording to the current model estimate, while the
M-step is the creation of a new model estimate
given a probability distribution over alignments
(calculated in the E-step).
In the E-step ideally all possible alignments
should be enumerated and labeled with p(a|e, f),
but this is intractable. For the M-step, we would
like to count over all possible alignments for each
sentence pair, weighted by their probability ac-
cording to the model estimated at the previous
</bodyText>
<page confidence="0.977689">
772
</page>
<table confidence="0.9958342">
SYSTEM F TO E ERRORS % E TO F ERRORS %
A/E OLD 19.4 22.3
A/E NEW 8.5 15.3
F/E OLD 32.5 25.9
F/E NEW 13.7 10.4
</table>
<tableCaption confidence="0.997174">
Table 5: Comparison of New Search Algorithm with Old Search Algorithm
</tableCaption>
<table confidence="0.9999566">
SYSTEM F-MEASURE F TO E F-MEASURE E TO F F-MEASURE BEST SYMM.
A/E MODEL 4 RERANKING OLD 64.1 / 58.1 54.0 / 48.8 67.9 / 63.0 (UNION)
A/E MODEL 4 RERANKING NEW 65.3 / 59.5 55.7 / 51.4 69.7 / 64.6 (UNION)
F/E MODEL 4 RERANKING OLD 77.3 / 77.8 78.3 / 77.2 79.2 / 79.1 (REFINED)
F/E MODEL 4 RERANKING NEW 77.9 / 77.9 78.4 / 77.7 79.2 / 79.4 (REFINED)
</table>
<tableCaption confidence="0.999303">
Table 6: Impact of Improved Search on Discriminative Reranking of Model 4
</tableCaption>
<bodyText confidence="0.999969578947369">
step. Because this is not tractable, we make the
assumption that the single assumed Viterbi align-
ment can be used to update our estimate in the M-
step. This approximation is called Viterbi training.
Neal and Hinton (1998) analyze approximate EM
training and motivate this type of variant.
We extend approximate EM training to perform
a new type of training which we call Minimum Er-
ror / Maximum Likelihood Training. The intuition
behind this approach to semi-supervised training
is that we wish to obtain the advantages of both
discriminative training (error minimization) and
approximate EM (which allows us to estimate a
large numbers of parameters even though we have
very few gold standard word alignments). We in-
troduce the EMD algorithm, in which discrimina-
tive training is used to control the contributions
of sub-models (thereby minimizing error), while a
procedure similar to one step of approximate EM
is used to estimate the large number of sub-model
parameters.
A brief sketch of the EMD algorithm applied
to our extended model is presented in Figure 1.
Parameters have a superscript t representing their
value at iteration t. We initialize the algorithm
with the gold standard word alignments (labels) of
the word alignment discriminative training set, an
initial A, N, and the starting alignments (the iter-
ation 4 HMM Viterbi alignment). In line 2, we
make iteration 0 estimates of the 5 sub-models of
Model 4 and the 6 heuristic sub-models which are
iteration dependent. In line 3, we run discrimi-
native training using the algorithm from Section
3.1. In line 4, we measure the error of the result-
ing A vector. In the main loop in line 7 we align
the full training set (similar to the E-step of EM),
while in line 8 we estimate the iteration-dependent
sub-models (similar to the M-step of EM). Then
</bodyText>
<figure confidence="0.947679647058824">
1: Algorithm EMD(labels, A&apos;, N, starting alignments)
2: estimate B0m for m = 1 to 11
3: A0 = Discrim(B0, A&apos;, labels, N)
4: e0 = E(A0, labels)
5: t = 1
6: loop
7: align full training set using At−1 and Bt−1
m
8: estimate Btm for m = 1 to 11
9: At = Discrim(Bt, A&apos;&apos;, labels, N)
10: et = E(At, labels)
11: if et &gt;= et−1 then
12: terminate loop
13: end if
14: t = t + 1
15: end loop
16: return hypothesized alignments of full training set
</figure>
<figureCaption confidence="0.999994">
Figure 1: Sketch of the EMD algorithm
</figureCaption>
<bodyText confidence="0.96353575">
we perform discriminative reranking in line 9 and
check for convergence in lines 10 and 11 (conver-
gence means that error was not decreased from the
previous iteration). The output of the algorithm is
new hypothesized alignments of the training cor-
pus.
Table 7 evaluates the EMD semi-supervised
training algorithm. We observe:
</bodyText>
<listItem confidence="0.672084375">
1. In both cases there is improved F-measure
on the second iteration of semi-supervised
training, indicating that the EMD algorithm
performs better than one step discriminative
reranking.
2. The French/English data set has converged
after the second iteration.
3. The Arabic/English data set converged after
</listItem>
<bodyText confidence="0.9162316">
improvement for the first, second and third
iterations.
We also performed an additional experiment for
French/English aimed at understanding the poten-
tial contribution of the word aligned data without
</bodyText>
<footnote confidence="0.890553">
3Convergence is achieved because error on the word
alignment discriminative training set does not improve.
</footnote>
<page confidence="0.993991">
773
</page>
<table confidence="0.999565375">
SYSTEM F-MEASURE F TO E F-MEASURE E TO F BEST SYMM.
A/E STARTING POINT 58.6 / 54.4 47.7 / 39.9 62.1 / 57.0 (UNION)
A/E EMD: ITERATION 1 68.4 / 62.2 61.6 / 57.7 72.0 / 66.4 (UNION)
A/E EMD: ITERATION 2 69.8 / 63.1 64.1 / 59.5 74.1 / 68.1 (UNION)
A/E EMD: ITERATION 3 70.6 / 65.4 64.3 / 59.2 74.7 / 69.4 (UNION)
F/E STARTING POINT 72.4 / 73.9 71.5 / 71.8 76.4 / 77.3 (REFINED)
F/E EMD: ITERATION 1 78.7 / 80.2 79.3 / 79.6 79.6 / 80.4 (REFINED)
F/E EMD: ITERATION 2 79.4 / 80.5 79.8 / 80.5 79.9 / 81.2 (REFINED)
</table>
<tableCaption confidence="0.999135">
Table 7: Semi-Supervised Training Task F-measure
</tableCaption>
<bodyText confidence="0.99996355">
the new algorithm4. Like Ittycheriah and Roukos
(2005), we converted the alignment discrimina-
tive training corpus links into a special corpus
consisting of parallel sentences where each sen-
tence consists only of a single word involved in
the link. We found that the information in the
links was “washed out” by the rest of the data and
resulted in no change in the alignment test set’s
F-Measure. Callison-Burch et al. (2004) showed
in their work on combining alignments of lower
and higher quality that the alignments of higher
quality should be given a much higher weight than
the lower quality alignments. Using this insight,
we found that adding 10,000 copies of the special
corpus to our training data resulted in the highest
alignment test set gain, which was a small gain
of 0.6 F-Measure. This result suggests that while
the link information is useful for improving F-
Measure, our improved methods for training are
producing much larger improvements.
</bodyText>
<sectionHeader confidence="0.811447" genericHeader="method">
5 Improvement of MT Quality
</sectionHeader>
<bodyText confidence="0.986527206896552">
The symmetrized alignments from the last iter-
ation of EMD were used to build phrasal SMT
systems, as were the symmetrized Model 4 align-
ments (the baseline). Aside from the final align-
ment, all other resources were held constant be-
tween the baseline and contrastive SMT systems,
including those based on lower level alignments
models such as IBM Model 1. For all of our ex-
periments, we use two language models, one built
using the English portion of the training data and
the other built using additional English news data.
We run Maximum BLEU (Och, 2003) for 25 iter-
ations individually for each system.
Table 8 shows our results. We report BLEU (Pa-
pineni et al., 2001) multiplied by 100. We also
show the F-measure after heuristic symmetrization
of the alignment test sets. The table shows that
4We would like to thank an anonymous reviewer for sug-
gesting that this experiment would be useful even when using
a small discriminative training corpus.
our algorithm produces heuristically symmetrized
final alignments of improved F-measure. Us-
ing these alignments in our phrasal SMT system,
we produced a statistically significant BLEU im-
provement (at a 95% confidence interval a gain of
0.78 is necessary) on the French/English task and
a statistically significant BLEU improvement on
the Arabic/English task (at a 95% confidence in-
terval a gain of 1.2 is necessary).
</bodyText>
<subsectionHeader confidence="0.764557">
5.1 Error Criterion
</subsectionHeader>
<bodyText confidence="0.999982777777778">
The error criterion we used for all experiments
is 1 − F-measure. The formula for F-measure is
shown in Equation 3. (Fraser and Marcu, 2006) es-
tablished that tuning the trade-off between Preci-
sion and Recall in the F-Measure formula will lead
to the best BLEU results. We tuned α by build-
ing a collection of alignments using our baseline
system, measuring Precision and Recall against
the alignment discriminative training set, build-
ing SMT systems and measuring resulting BLEU
scores, and then searching for an appropriate α
setting. We searched α = 0.1, 0.2, ..., 0.9 and set
α so that the resulting F-measure tracks BLEU to
the best extent possible. The best settings were
α = 0.2 for Arabic/English and α = 0.7 for
French/English, and these settings of α were used
for every result reported in this paper. See (Fraser
and Marcu, 2006) for further details.
</bodyText>
<equation confidence="0.99740825">
1
F(A, S, α) = (1_α) (3)
Precision(A,$) + α
Recall(A,$)
</equation>
<sectionHeader confidence="0.971875" genericHeader="method">
6 Previous Research
</sectionHeader>
<bodyText confidence="0.996055625">
Previous work on discriminative training for word-
alignment differed most strongly from our ap-
proach in that it generally views word-alignment
as a supervised task. Examples of this perspective
include (Liu et al., 2005; Ittycheriah and Roukos,
2005; Moore, 2005; Taskar et al., 2005). All
of these also used knowledge from one of the
IBM Models in order to obtain competitive results
</bodyText>
<page confidence="0.993398">
774
</page>
<table confidence="0.999284">
SYSTEM BLEU F-MEASURE
A/E UNSUP. MODEL 4 UNION 49.16 64.6
A/E EMD 3 UNION 50.84 69.4
F/E UNSUP. MODEL 4 REFINED 30.63 76.4
F/E EMD 2 REFINED 31.56 81.2
</table>
<tableCaption confidence="0.999884">
Table 8: Evaluation of Translation Quality
</tableCaption>
<bodyText confidence="0.999985862500001">
with the baseline (with the exception of (Moore,
2005)). We interleave discriminative training with
EM and are therefore performing semi-supervised
training. We show that semi-supervised training
leads to better word alignments than running unsu-
pervised training followed by discriminative train-
ing.
Another important difference with previous
work is that we are concerned with generating
many-to-many word alignments. Cherry and Lin
(2003) and Taskar et al. (2005) compared their re-
sults with Model 4 using “intersection” by look-
ing at AER (with the “Sure” versus “Possible” link
distinction), and restricted themselves to consider-
ing 1-to-1 alignments. However, “union” and “re-
fined” alignments, which are many-to-many, are
what are used to build competitive phrasal SMT
systems, because “intersection” performs poorly,
despite having been shown to have the best AER
scores for the French/English corpus we are using
(Och and Ney, 2003). (Fraser and Marcu, 2006)
recently found serious problems with AER both
empirically and analytically, which explains why
optimizing AER frequently results in poor ma-
chine translation performance.
Finally, we show better MT results by using F-
measure with a tuned α value. The only previous
discriminative approach which has been shown to
produce translations of similar or better quality to
those produced by the symmetrized baseline was
(Ittycheriah and Roukos, 2005). They had access
to 5000 gold standard word alignments, consider-
ably more than the 100 or 110 gold standard word
alignments used here. They also invested signif-
icant effort in sub-model engineering (producing
both sub-models specific to Arabic/English align-
ment and sub-models which would be useful for
other language pairs), while we use sub-models
which are simple extensions of Model 4 and lan-
guage independent.
The problem of semi-supervised learning is of-
ten defined as “using unlabeled data to help su-
pervised learning” (Seeger, 2000). Most work on
semi-supervised learning uses underlying distribu-
tions with a relatively small number of parame-
ters. An initial model is estimated in a supervised
fashion using the labeled data, and this supervised
model is used to attach labels (or a probability dis-
tribution over labels) to the unlabeled data, then a
new supervised model is estimated, and this is it-
erated. If these techniques are applied when there
are a small number of labels in relation to the num-
ber of parameters used, they will suffer from the
“overconfident pseudo-labeling problem” (Seeger,
2000), where the initial labels of poor quality as-
signed to the unlabeled data will dominate the
model estimated in the M-step. However, there
are tasks with large numbers of parameters where
there are sufficient labels. Nigam et al. (2000) ad-
dressed a text classification task. They estimate
a Naive Bayes classifier over the labeled data and
use it to provide initial MAP estimates for unla-
beled documents, followed by EM to further re-
fine the model. Callison-Burch et al. (2004) exam-
ined the issue of semi-supervised training for word
alignment, but under a scenario where they simu-
lated sufficient gold standard word alignments to
follow an approach similar to Nigam et al. (2000).
We do not have enough labels for this approach.
We are aware of two approaches to semi-
supervised learning which are more similar in
spirit to ours. Ivanov et al. (2001) used discrimi-
native training in a reinforcement learning context
in a similar way to our adding of a discriminative
training step to an unsupervised context. A large
body of work uses semi-supervised learning for
clustering by imposing constraints on clusters. For
instance, in (Basu et al., 2004), the clustering sys-
tem was supplied with pairs of instances labeled
as belonging to the same or different clusters.
</bodyText>
<sectionHeader confidence="0.999286" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999709">
We presented a semi-supervised algorithm based
on IBM Model 4, with modeling and search ex-
tensions, which produces alignments of improved
F-measure over unsupervised Model 4 training.
We used these alignments to produce transla-
tions of higher quality.
</bodyText>
<page confidence="0.994272">
775
</page>
<bodyText confidence="0.9996974">
The semi-supervised learning literature gen-
erally addresses augmenting supervised learning
tasks with unlabeled data (Seeger, 2000). In con-
trast, we augmented an unsupervised learning task
with labeled data. We hope that Minimum Error /
Maximum Likelihood training using the EMD al-
gorithm can be used for a wide diversity of tasks
where there is not enough labeled data to allow
supervised estimation of an initial model of rea-
sonable quality.
</bodyText>
<sectionHeader confidence="0.998608" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.997936833333333">
This work was partially supported under the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-C-0022. We would like to thank the USC Cen-
ter for High Performance Computing and Commu-
nications.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999939164705882">
Sugato Basu, Mikhail Bilenko, and Raymond J.
Mooney. 2004. A probabilistic framework for semi-
supervised clustering. In KDD ’04: Proc. of the
ACM SIGKDD international conference on knowl-
edge discovery and data mining, pages 59–68, New
York. ACM Press.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and R. L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263–
311.
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with
word- and sentence-aligned parallel corpora. In
Proc. of the 42nd Annual Meeting of the Association
for Computational Linguistics, Barcelona, Spain,
July.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proc. of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, Sapporo, Japan, July.
Alexander Fraser and Daniel Marcu. 2006. Measur-
ing word alignment quality for statistical machine
translation. In Technical Report ISI-TR-616. Avail-
able at http://www.isi.edu/ fraser/research.html,
ISI/University of Southern California, May.
Fred Glover. 1986. Future paths for integer program-
ming and links to artificial intelligence. Computers
and Operations Research, 13(5):533–549.
Abraham Ittycheriah and Salim Roukos. 2005. A
maximum entropy word aligner for Arabic-English
machine translation. In Proc. of Human Language
Technology Conf. and Conf. on Empirical Methods
in Natural Language Processing, Vancouver, BC.
Yuri A. Ivanov, Bruce Blumberg, and Alex Pentland.
2001. Expectation maximization for weakly labeled
data. In ICML ’01: Proc. of the Eighteenth Interna-
tional Conf. on Machine Learning, pages 218–225.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proc. of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 459–466, Ann Arbor, MI.
I. Dan Melamed. 1998. Manual annotation of trans-
lational equivalence: The blinker project. Techni-
cal Report 98-07, Institute for Research in Cognitive
Science, Philadelphia, PA.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proc. of Human
Language Technology Conf. and Conf. on Empirical
Methods in Natural Language Processing, Vancou-
ver, BC, October.
Radford M. Neal and Geoffrey E. Hinton. 1998. A
view of the EM algorithm that justifies incremental,
sparse, and other variants. In M. I. Jordan, editor,
Learning in Graphical Models. Kluwer.
Kamal Nigam, Andrew K. McCallum, Sebastian
Thrun, and Tom M. Mitchell. 2000. Text classifi-
cation from labeled and unlabeled documents using
EM. Machine Learning, 39(2/3):103–134.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 160–167, Sapporo, Japan.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: a method for auto-
matic evaluation of machine translation. Technical
Report RC22176 (W0109-022), IBM Research Di-
vision, Thomas J. Watson Research Center, York-
town Heights, NY, September.
Matthias Seeger. 2000. Learning with labeled and un-
labeled data. In Technical report, 2000. Available at
http://www.dai.ed.ac.uk/ seeger/papers.html.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proc. of Human Language Technol-
ogy Conf. and Conf. on Empirical Methods in Natu-
ral Language Processing, Vancouver, BC, October.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In COLING ’96: The 16th Int. Conf. on
Computational Linguistics, pages 836–841, Copen-
hagen, Denmark, August.
</reference>
<page confidence="0.998545">
776
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956540">
<title confidence="0.999421">Semi-Supervised Training for Statistical Word Alignment</title>
<author confidence="0.999204">Alexander Fraser</author>
<affiliation confidence="0.991411">ISI / University of Southern California</affiliation>
<address confidence="0.9971945">4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292</address>
<email confidence="0.999849">fraser@isi.edu</email>
<author confidence="0.999724">Daniel Marcu</author>
<affiliation confidence="0.993756">ISI / University of Southern California</affiliation>
<address confidence="0.994862">4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292</address>
<email confidence="0.997971">marcu@isi.edu</email>
<abstract confidence="0.999140416666667">We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus. We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sugato Basu</author>
<author>Mikhail Bilenko</author>
<author>Raymond J Mooney</author>
</authors>
<title>A probabilistic framework for semisupervised clustering.</title>
<date>2004</date>
<booktitle>In KDD ’04: Proc. of the ACM SIGKDD international conference on knowledge discovery and data mining,</booktitle>
<pages>59--68</pages>
<publisher>ACM Press.</publisher>
<location>New York.</location>
<contexts>
<context position="27135" citStr="Basu et al., 2004" startWordPosition="4521" endWordPosition="4524">ord alignment, but under a scenario where they simulated sufficient gold standard word alignments to follow an approach similar to Nigam et al. (2000). We do not have enough labels for this approach. We are aware of two approaches to semisupervised learning which are more similar in spirit to ours. Ivanov et al. (2001) used discriminative training in a reinforcement learning context in a similar way to our adding of a discriminative training step to an unsupervised context. A large body of work uses semi-supervised learning for clustering by imposing constraints on clusters. For instance, in (Basu et al., 2004), the clustering system was supplied with pairs of instances labeled as belonging to the same or different clusters. 7 Conclusion We presented a semi-supervised algorithm based on IBM Model 4, with modeling and search extensions, which produces alignments of improved F-measure over unsupervised Model 4 training. We used these alignments to produce translations of higher quality. 775 The semi-supervised learning literature generally addresses augmenting supervised learning tasks with unlabeled data (Seeger, 2000). In contrast, we augmented an unsupervised learning task with labeled data. We hop</context>
</contexts>
<marker>Basu, Bilenko, Mooney, 2004</marker>
<rawString>Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney. 2004. A probabilistic framework for semisupervised clustering. In KDD ’04: Proc. of the ACM SIGKDD international conference on knowledge discovery and data mining, pages 59–68, New York. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="877" citStr="Brown et al., 1993" startWordPosition="124" endWordPosition="127">uite 1001 Marina del Rey, CA 90292 marcu@isi.edu Abstract We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus. We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality. 1 Introduction The most widely applied training procedure for statistical machine translation — IBM model 4 (Brown et al., 1993) unsupervised training followed by post-processing with symmetrization heuristics (Och and Ney, 2003) — yields low quality word alignments. When compared with gold standard parallel data which was manually aligned using a high-recall/precision methodology (Melamed, 1998), the word-level alignments produced automatically have an F-measure accuracy of 64.6 and 76.4% (see Section 2 for details). In this paper, we improve word alignment and, subsequently, MT accuracy by developing a range of increasingly sophisticated methods: 1. We first recast the problem of estimating the IBM models (Brown et a</context>
<context position="12255" citStr="Brown et al. (1993)" startWordPosition="2015" endWordPosition="2018">F-measures are presented on both the alignment discriminative training set and the alignment test set sub-corpora, separated by /. 4. The discriminatively reranked extended model outperforms four iterations of Model 4 in both cases with the best heuristic symmetrization, but some of the gain is lost as we are optimizing the F-measure of the 1-to-many alignments rather than the F-measure of the many-to-many alignments directly. Overall, the results show our approach is better than or competitive with running four iterations of unsupervised Model 4 training. 3.2.2 New Alignment Search Algorithm Brown et al. (1993) introduced operations defining a hillclimbing search appropriate for Model 4. Their search starts with a complete hypothesis and exhaustively applies two operations to it, selecting the best improved hypothesis it can find (or terminating if no improved hypothesis is found). This search makes many search errors2. We developed a new alignment algorithm to reduce search errors: • We perform an initial hillclimbing search (as in the baseline algorithm) but construct a priority queue of possible other candidate alignments to consider. • Alignments which are expanded are marked so that they will n</context>
<context position="14643" citStr="Brown et al., 1993" startWordPosition="2420" endWordPosition="2423"> from our sample in which we were able to find a more probable candidate by applying our new algorithm using 24 hours of computation for just the 1000 sample sentences. Table 5 presents the results, showing that our new algorithm reduced search errors in all cases, but further reduction could be obtained. The second experiment shows the impact of the new search on discriminative reranking of Model 4 (see Table 6). Reduced search errors lead to a better fit of the discriminative training corpus. 4 Semi-Supervised Training for Word Alignments Intuitively, in approximate EM training for Model 4 (Brown et al., 1993), the E-step corresponds to calculating the probability of all alignments according to the current model estimate, while the M-step is the creation of a new model estimate given a probability distribution over alignments (calculated in the E-step). In the E-step ideally all possible alignments should be enumerated and labeled with p(a|e, f), but this is intractable. For the M-step, we would like to count over all possible alignments for each sentence pair, weighted by their probability according to the model estimated at the previous 772 SYSTEM F TO E ERRORS % E TO F ERRORS % A/E OLD 19.4 22.3</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Statistical machine translation with word- and sentence-aligned parallel corpora.</title>
<date>2004</date>
<booktitle>In Proc. of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="20001" citStr="Callison-Burch et al. (2004)" startWordPosition="3360" endWordPosition="3363">.4 / 77.3 (REFINED) F/E EMD: ITERATION 1 78.7 / 80.2 79.3 / 79.6 79.6 / 80.4 (REFINED) F/E EMD: ITERATION 2 79.4 / 80.5 79.8 / 80.5 79.9 / 81.2 (REFINED) Table 7: Semi-Supervised Training Task F-measure the new algorithm4. Like Ittycheriah and Roukos (2005), we converted the alignment discriminative training corpus links into a special corpus consisting of parallel sentences where each sentence consists only of a single word involved in the link. We found that the information in the links was “washed out” by the rest of the data and resulted in no change in the alignment test set’s F-Measure. Callison-Burch et al. (2004) showed in their work on combining alignments of lower and higher quality that the alignments of higher quality should be given a much higher weight than the lower quality alignments. Using this insight, we found that adding 10,000 copies of the special corpus to our training data resulted in the highest alignment test set gain, which was a small gain of 0.6 F-Measure. This result suggests that while the link information is useful for improving FMeasure, our improved methods for training are producing much larger improvements. 5 Improvement of MT Quality The symmetrized alignments from the las</context>
<context position="26464" citStr="Callison-Burch et al. (2004)" startWordPosition="4410" endWordPosition="4413"> a small number of labels in relation to the number of parameters used, they will suffer from the “overconfident pseudo-labeling problem” (Seeger, 2000), where the initial labels of poor quality assigned to the unlabeled data will dominate the model estimated in the M-step. However, there are tasks with large numbers of parameters where there are sufficient labels. Nigam et al. (2000) addressed a text classification task. They estimate a Naive Bayes classifier over the labeled data and use it to provide initial MAP estimates for unlabeled documents, followed by EM to further refine the model. Callison-Burch et al. (2004) examined the issue of semi-supervised training for word alignment, but under a scenario where they simulated sufficient gold standard word alignments to follow an approach similar to Nigam et al. (2000). We do not have enough labels for this approach. We are aware of two approaches to semisupervised learning which are more similar in spirit to ours. Ivanov et al. (2001) used discriminative training in a reinforcement learning context in a similar way to our adding of a discriminative training step to an unsupervised context. A large body of work uses semi-supervised learning for clustering by</context>
</contexts>
<marker>Callison-Burch, Talbot, Osborne, 2004</marker>
<rawString>Chris Callison-Burch, David Talbot, and Miles Osborne. 2004. Statistical machine translation with word- and sentence-aligned parallel corpora. In Proc. of the 42nd Annual Meeting of the Association for Computational Linguistics, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A probability model to improve word alignment.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sapporo, Japan,</location>
<contexts>
<context position="23907" citStr="Cherry and Lin (2003)" startWordPosition="4002" endWordPosition="4005">MEASURE A/E UNSUP. MODEL 4 UNION 49.16 64.6 A/E EMD 3 UNION 50.84 69.4 F/E UNSUP. MODEL 4 REFINED 30.63 76.4 F/E EMD 2 REFINED 31.56 81.2 Table 8: Evaluation of Translation Quality with the baseline (with the exception of (Moore, 2005)). We interleave discriminative training with EM and are therefore performing semi-supervised training. We show that semi-supervised training leads to better word alignments than running unsupervised training followed by discriminative training. Another important difference with previous work is that we are concerned with generating many-to-many word alignments. Cherry and Lin (2003) and Taskar et al. (2005) compared their results with Model 4 using “intersection” by looking at AER (with the “Sure” versus “Possible” link distinction), and restricted themselves to considering 1-to-1 alignments. However, “union” and “refined” alignments, which are many-to-many, are what are used to build competitive phrasal SMT systems, because “intersection” performs poorly, despite having been shown to have the best AER scores for the French/English corpus we are using (Och and Ney, 2003). (Fraser and Marcu, 2006) recently found serious problems with AER both empirically and analytically,</context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>Colin Cherry and Dekang Lin. 2003. A probability model to improve word alignment. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Measuring word alignment quality for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Technical Report ISI-TR-616. Available at http://www.isi.edu/ fraser/research.html, ISI/University of</booktitle>
<location>Southern California,</location>
<contexts>
<context position="22088" citStr="Fraser and Marcu, 2006" startWordPosition="3706" endWordPosition="3709">using a small discriminative training corpus. our algorithm produces heuristically symmetrized final alignments of improved F-measure. Using these alignments in our phrasal SMT system, we produced a statistically significant BLEU improvement (at a 95% confidence interval a gain of 0.78 is necessary) on the French/English task and a statistically significant BLEU improvement on the Arabic/English task (at a 95% confidence interval a gain of 1.2 is necessary). 5.1 Error Criterion The error criterion we used for all experiments is 1 − F-measure. The formula for F-measure is shown in Equation 3. (Fraser and Marcu, 2006) established that tuning the trade-off between Precision and Recall in the F-Measure formula will lead to the best BLEU results. We tuned α by building a collection of alignments using our baseline system, measuring Precision and Recall against the alignment discriminative training set, building SMT systems and measuring resulting BLEU scores, and then searching for an appropriate α setting. We searched α = 0.1, 0.2, ..., 0.9 and set α so that the resulting F-measure tracks BLEU to the best extent possible. The best settings were α = 0.2 for Arabic/English and α = 0.7 for French/English, and t</context>
<context position="24431" citStr="Fraser and Marcu, 2006" startWordPosition="4084" endWordPosition="4087">s work is that we are concerned with generating many-to-many word alignments. Cherry and Lin (2003) and Taskar et al. (2005) compared their results with Model 4 using “intersection” by looking at AER (with the “Sure” versus “Possible” link distinction), and restricted themselves to considering 1-to-1 alignments. However, “union” and “refined” alignments, which are many-to-many, are what are used to build competitive phrasal SMT systems, because “intersection” performs poorly, despite having been shown to have the best AER scores for the French/English corpus we are using (Och and Ney, 2003). (Fraser and Marcu, 2006) recently found serious problems with AER both empirically and analytically, which explains why optimizing AER frequently results in poor machine translation performance. Finally, we show better MT results by using Fmeasure with a tuned α value. The only previous discriminative approach which has been shown to produce translations of similar or better quality to those produced by the symmetrized baseline was (Ittycheriah and Roukos, 2005). They had access to 5000 gold standard word alignments, considerably more than the 100 or 110 gold standard word alignments used here. They also invested sig</context>
</contexts>
<marker>Fraser, Marcu, 2006</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2006. Measuring word alignment quality for statistical machine translation. In Technical Report ISI-TR-616. Available at http://www.isi.edu/ fraser/research.html, ISI/University of Southern California, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Glover</author>
</authors>
<title>Future paths for integer programming and links to artificial intelligence.</title>
<date>1986</date>
<journal>Computers and Operations Research,</journal>
<volume>13</volume>
<issue>5</issue>
<marker>Glover, 1986</marker>
<rawString>Fred Glover. 1986. Future paths for integer programming and links to artificial intelligence. Computers and Operations Research, 13(5):533–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy word aligner for Arabic-English machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of Human Language Technology Conf. and Conf. on Empirical Methods in Natural Language Processing,</booktitle>
<location>Vancouver, BC.</location>
<contexts>
<context position="19630" citStr="Ittycheriah and Roukos (2005)" startWordPosition="3297" endWordPosition="3300">not improve. 773 SYSTEM F-MEASURE F TO E F-MEASURE E TO F BEST SYMM. A/E STARTING POINT 58.6 / 54.4 47.7 / 39.9 62.1 / 57.0 (UNION) A/E EMD: ITERATION 1 68.4 / 62.2 61.6 / 57.7 72.0 / 66.4 (UNION) A/E EMD: ITERATION 2 69.8 / 63.1 64.1 / 59.5 74.1 / 68.1 (UNION) A/E EMD: ITERATION 3 70.6 / 65.4 64.3 / 59.2 74.7 / 69.4 (UNION) F/E STARTING POINT 72.4 / 73.9 71.5 / 71.8 76.4 / 77.3 (REFINED) F/E EMD: ITERATION 1 78.7 / 80.2 79.3 / 79.6 79.6 / 80.4 (REFINED) F/E EMD: ITERATION 2 79.4 / 80.5 79.8 / 80.5 79.9 / 81.2 (REFINED) Table 7: Semi-Supervised Training Task F-measure the new algorithm4. Like Ittycheriah and Roukos (2005), we converted the alignment discriminative training corpus links into a special corpus consisting of parallel sentences where each sentence consists only of a single word involved in the link. We found that the information in the links was “washed out” by the rest of the data and resulted in no change in the alignment test set’s F-Measure. Callison-Burch et al. (2004) showed in their work on combining alignments of lower and higher quality that the alignments of higher quality should be given a much higher weight than the lower quality alignments. Using this insight, we found that adding 10,0</context>
<context position="23132" citStr="Ittycheriah and Roukos, 2005" startWordPosition="3881" endWordPosition="3884">0.2, ..., 0.9 and set α so that the resulting F-measure tracks BLEU to the best extent possible. The best settings were α = 0.2 for Arabic/English and α = 0.7 for French/English, and these settings of α were used for every result reported in this paper. See (Fraser and Marcu, 2006) for further details. 1 F(A, S, α) = (1_α) (3) Precision(A,$) + α Recall(A,$) 6 Previous Research Previous work on discriminative training for wordalignment differed most strongly from our approach in that it generally views word-alignment as a supervised task. Examples of this perspective include (Liu et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005). All of these also used knowledge from one of the IBM Models in order to obtain competitive results 774 SYSTEM BLEU F-MEASURE A/E UNSUP. MODEL 4 UNION 49.16 64.6 A/E EMD 3 UNION 50.84 69.4 F/E UNSUP. MODEL 4 REFINED 30.63 76.4 F/E EMD 2 REFINED 31.56 81.2 Table 8: Evaluation of Translation Quality with the baseline (with the exception of (Moore, 2005)). We interleave discriminative training with EM and are therefore performing semi-supervised training. We show that semi-supervised training leads to better word alignments than running unsupervised training fo</context>
<context position="24873" citStr="Ittycheriah and Roukos, 2005" startWordPosition="4151" endWordPosition="4154">ems, because “intersection” performs poorly, despite having been shown to have the best AER scores for the French/English corpus we are using (Och and Ney, 2003). (Fraser and Marcu, 2006) recently found serious problems with AER both empirically and analytically, which explains why optimizing AER frequently results in poor machine translation performance. Finally, we show better MT results by using Fmeasure with a tuned α value. The only previous discriminative approach which has been shown to produce translations of similar or better quality to those produced by the symmetrized baseline was (Ittycheriah and Roukos, 2005). They had access to 5000 gold standard word alignments, considerably more than the 100 or 110 gold standard word alignments used here. They also invested significant effort in sub-model engineering (producing both sub-models specific to Arabic/English alignment and sub-models which would be useful for other language pairs), while we use sub-models which are simple extensions of Model 4 and language independent. The problem of semi-supervised learning is often defined as “using unlabeled data to help supervised learning” (Seeger, 2000). Most work on semi-supervised learning uses underlying dis</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2005. A maximum entropy word aligner for Arabic-English machine translation. In Proc. of Human Language Technology Conf. and Conf. on Empirical Methods in Natural Language Processing, Vancouver, BC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuri A Ivanov</author>
<author>Bruce Blumberg</author>
<author>Alex Pentland</author>
</authors>
<title>Expectation maximization for weakly labeled data.</title>
<date>2001</date>
<booktitle>In ICML ’01: Proc. of the Eighteenth International Conf. on Machine Learning,</booktitle>
<pages>218--225</pages>
<contexts>
<context position="26837" citStr="Ivanov et al. (2001)" startWordPosition="4474" endWordPosition="4477">(2000) addressed a text classification task. They estimate a Naive Bayes classifier over the labeled data and use it to provide initial MAP estimates for unlabeled documents, followed by EM to further refine the model. Callison-Burch et al. (2004) examined the issue of semi-supervised training for word alignment, but under a scenario where they simulated sufficient gold standard word alignments to follow an approach similar to Nigam et al. (2000). We do not have enough labels for this approach. We are aware of two approaches to semisupervised learning which are more similar in spirit to ours. Ivanov et al. (2001) used discriminative training in a reinforcement learning context in a similar way to our adding of a discriminative training step to an unsupervised context. A large body of work uses semi-supervised learning for clustering by imposing constraints on clusters. For instance, in (Basu et al., 2004), the clustering system was supplied with pairs of instances labeled as belonging to the same or different clusters. 7 Conclusion We presented a semi-supervised algorithm based on IBM Model 4, with modeling and search extensions, which produces alignments of improved F-measure over unsupervised Model </context>
</contexts>
<marker>Ivanov, Blumberg, Pentland, 2001</marker>
<rawString>Yuri A. Ivanov, Bruce Blumberg, and Alex Pentland. 2001. Expectation maximization for weakly labeled data. In ICML ’01: Proc. of the Eighteenth International Conf. on Machine Learning, pages 218–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Loglinear models for word alignment.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>459--466</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="23102" citStr="Liu et al., 2005" startWordPosition="3877" endWordPosition="3880">searched α = 0.1, 0.2, ..., 0.9 and set α so that the resulting F-measure tracks BLEU to the best extent possible. The best settings were α = 0.2 for Arabic/English and α = 0.7 for French/English, and these settings of α were used for every result reported in this paper. See (Fraser and Marcu, 2006) for further details. 1 F(A, S, α) = (1_α) (3) Precision(A,$) + α Recall(A,$) 6 Previous Research Previous work on discriminative training for wordalignment differed most strongly from our approach in that it generally views word-alignment as a supervised task. Examples of this perspective include (Liu et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005). All of these also used knowledge from one of the IBM Models in order to obtain competitive results 774 SYSTEM BLEU F-MEASURE A/E UNSUP. MODEL 4 UNION 49.16 64.6 A/E EMD 3 UNION 50.84 69.4 F/E UNSUP. MODEL 4 REFINED 30.63 76.4 F/E EMD 2 REFINED 31.56 81.2 Table 8: Evaluation of Translation Quality with the baseline (with the exception of (Moore, 2005)). We interleave discriminative training with EM and are therefore performing semi-supervised training. We show that semi-supervised training leads to better word alignments than ru</context>
</contexts>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2005. Loglinear models for word alignment. In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 459–466, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Manual annotation of translational equivalence: The blinker project.</title>
<date>1998</date>
<tech>Technical Report 98-07,</tech>
<institution>Institute for Research in Cognitive Science,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1148" citStr="Melamed, 1998" startWordPosition="162" endWordPosition="163">p aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus. We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality. 1 Introduction The most widely applied training procedure for statistical machine translation — IBM model 4 (Brown et al., 1993) unsupervised training followed by post-processing with symmetrization heuristics (Och and Ney, 2003) — yields low quality word alignments. When compared with gold standard parallel data which was manually aligned using a high-recall/precision methodology (Melamed, 1998), the word-level alignments produced automatically have an F-measure accuracy of 64.6 and 76.4% (see Section 2 for details). In this paper, we improve word alignment and, subsequently, MT accuracy by developing a range of increasingly sophisticated methods: 1. We first recast the problem of estimating the IBM models (Brown et al., 1993) in a discriminative framework, which leads to an initial increase in word-alignment accuracy. 2. We extend the IBM models with new (sub)models, which leads to additional increases in word-alignment accuracy. In the process, we also show that these improvements </context>
<context position="5011" citStr="Melamed, 1998" startWordPosition="771" endWordPosition="772">iminative training set and the alignment test set sub-corpora, separated by /. discriminative training set and alignment test set. Translation quality is evaluated by translating a held-out translation test set. An additional translation set called the Maximum BLEU set is employed by the SMT system to train the weights associated with the components of its log-linear model (Och, 2003). The training corpora are publicly available: both the Arabic/English data and the French/English Hansards were released by LDC. We created the manual word alignments ourselves, following the Blinker guidelines (Melamed, 1998). To train our baseline systems we follow a standard procedure. The models were trained two times, first using French or Arabic as the source language and then using English as the source language. For each training direction, we run GIZA++ (Och and Ney, 2003), specifying 5 iterations of Model 1, 4 iterations of the HMM model (Vogel et al., 1996), and 4 iterations of Model 4. We quantify the quality of the resulting hypothesized alignments with F-measure using the manually aligned sets. We present the results for three different conditions in Table 2. For the “F to E” direction the models assi</context>
</contexts>
<marker>Melamed, 1998</marker>
<rawString>I. Dan Melamed. 1998. Manual annotation of translational equivalence: The blinker project. Technical Report 98-07, Institute for Research in Cognitive Science, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>A discriminative framework for bilingual word alignment.</title>
<date>2005</date>
<booktitle>In Proc. of Human Language Technology Conf. and Conf. on Empirical Methods in Natural Language Processing,</booktitle>
<location>Vancouver, BC,</location>
<contexts>
<context position="23145" citStr="Moore, 2005" startWordPosition="3885" endWordPosition="3886">t the resulting F-measure tracks BLEU to the best extent possible. The best settings were α = 0.2 for Arabic/English and α = 0.7 for French/English, and these settings of α were used for every result reported in this paper. See (Fraser and Marcu, 2006) for further details. 1 F(A, S, α) = (1_α) (3) Precision(A,$) + α Recall(A,$) 6 Previous Research Previous work on discriminative training for wordalignment differed most strongly from our approach in that it generally views word-alignment as a supervised task. Examples of this perspective include (Liu et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005). All of these also used knowledge from one of the IBM Models in order to obtain competitive results 774 SYSTEM BLEU F-MEASURE A/E UNSUP. MODEL 4 UNION 49.16 64.6 A/E EMD 3 UNION 50.84 69.4 F/E UNSUP. MODEL 4 REFINED 30.63 76.4 F/E EMD 2 REFINED 31.56 81.2 Table 8: Evaluation of Translation Quality with the baseline (with the exception of (Moore, 2005)). We interleave discriminative training with EM and are therefore performing semi-supervised training. We show that semi-supervised training leads to better word alignments than running unsupervised training followed by dis</context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert C. Moore. 2005. A discriminative framework for bilingual word alignment. In Proc. of Human Language Technology Conf. and Conf. on Empirical Methods in Natural Language Processing, Vancouver, BC, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A view of the EM algorithm that justifies incremental, sparse, and other variants.</title>
<date>1998</date>
<booktitle>Learning in Graphical Models.</booktitle>
<editor>In M. I. Jordan, editor,</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="16007" citStr="Neal and Hinton (1998)" startWordPosition="2667" endWordPosition="2670"> TO E F-MEASURE E TO F F-MEASURE BEST SYMM. A/E MODEL 4 RERANKING OLD 64.1 / 58.1 54.0 / 48.8 67.9 / 63.0 (UNION) A/E MODEL 4 RERANKING NEW 65.3 / 59.5 55.7 / 51.4 69.7 / 64.6 (UNION) F/E MODEL 4 RERANKING OLD 77.3 / 77.8 78.3 / 77.2 79.2 / 79.1 (REFINED) F/E MODEL 4 RERANKING NEW 77.9 / 77.9 78.4 / 77.7 79.2 / 79.4 (REFINED) Table 6: Impact of Improved Search on Discriminative Reranking of Model 4 step. Because this is not tractable, we make the assumption that the single assumed Viterbi alignment can be used to update our estimate in the Mstep. This approximation is called Viterbi training. Neal and Hinton (1998) analyze approximate EM training and motivate this type of variant. We extend approximate EM training to perform a new type of training which we call Minimum Error / Maximum Likelihood Training. The intuition behind this approach to semi-supervised training is that we wish to obtain the advantages of both discriminative training (error minimization) and approximate EM (which allows us to estimate a large numbers of parameters even though we have very few gold standard word alignments). We introduce the EMD algorithm, in which discriminative training is used to control the contributions of sub-</context>
</contexts>
<marker>Neal, Hinton, 1998</marker>
<rawString>Radford M. Neal and Geoffrey E. Hinton. 1998. A view of the EM algorithm that justifies incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in Graphical Models. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew K McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom M Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="26223" citStr="Nigam et al. (2000)" startWordPosition="4369" endWordPosition="4372">ta, and this supervised model is used to attach labels (or a probability distribution over labels) to the unlabeled data, then a new supervised model is estimated, and this is iterated. If these techniques are applied when there are a small number of labels in relation to the number of parameters used, they will suffer from the “overconfident pseudo-labeling problem” (Seeger, 2000), where the initial labels of poor quality assigned to the unlabeled data will dominate the model estimated in the M-step. However, there are tasks with large numbers of parameters where there are sufficient labels. Nigam et al. (2000) addressed a text classification task. They estimate a Naive Bayes classifier over the labeled data and use it to provide initial MAP estimates for unlabeled documents, followed by EM to further refine the model. Callison-Burch et al. (2004) examined the issue of semi-supervised training for word alignment, but under a scenario where they simulated sufficient gold standard word alignments to follow an approach similar to Nigam et al. (2000). We do not have enough labels for this approach. We are aware of two approaches to semisupervised learning which are more similar in spirit to ours. Ivanov</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Kamal Nigam, Andrew K. McCallum, Sebastian Thrun, and Tom M. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="978" citStr="Och and Ney, 2003" startWordPosition="137" endWordPosition="140">raining for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus. We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality. 1 Introduction The most widely applied training procedure for statistical machine translation — IBM model 4 (Brown et al., 1993) unsupervised training followed by post-processing with symmetrization heuristics (Och and Ney, 2003) — yields low quality word alignments. When compared with gold standard parallel data which was manually aligned using a high-recall/precision methodology (Melamed, 1998), the word-level alignments produced automatically have an F-measure accuracy of 64.6 and 76.4% (see Section 2 for details). In this paper, we improve word alignment and, subsequently, MT accuracy by developing a range of increasingly sophisticated methods: 1. We first recast the problem of estimating the IBM models (Brown et al., 1993) in a discriminative framework, which leads to an initial increase in word-alignment accurac</context>
<context position="5271" citStr="Och and Ney, 2003" startWordPosition="814" endWordPosition="817">imum BLEU set is employed by the SMT system to train the weights associated with the components of its log-linear model (Och, 2003). The training corpora are publicly available: both the Arabic/English data and the French/English Hansards were released by LDC. We created the manual word alignments ourselves, following the Blinker guidelines (Melamed, 1998). To train our baseline systems we follow a standard procedure. The models were trained two times, first using French or Arabic as the source language and then using English as the source language. For each training direction, we run GIZA++ (Och and Ney, 2003), specifying 5 iterations of Model 1, 4 iterations of the HMM model (Vogel et al., 1996), and 4 iterations of Model 4. We quantify the quality of the resulting hypothesized alignments with F-measure using the manually aligned sets. We present the results for three different conditions in Table 2. For the “F to E” direction the models assign non-zero probability to alignments consisting of links from one Foreign word to zero or more English words, while for “E to F” the models assign non-zero probability to alignments consisting of links from one English word to zero or more Foreign words. It i</context>
<context position="24405" citStr="Och and Ney, 2003" startWordPosition="4080" endWordPosition="4083">fference with previous work is that we are concerned with generating many-to-many word alignments. Cherry and Lin (2003) and Taskar et al. (2005) compared their results with Model 4 using “intersection” by looking at AER (with the “Sure” versus “Possible” link distinction), and restricted themselves to considering 1-to-1 alignments. However, “union” and “refined” alignments, which are many-to-many, are what are used to build competitive phrasal SMT systems, because “intersection” performs poorly, despite having been shown to have the best AER scores for the French/English corpus we are using (Och and Ney, 2003). (Fraser and Marcu, 2006) recently found serious problems with AER both empirically and analytically, which explains why optimizing AER frequently results in poor machine translation performance. Finally, we show better MT results by using Fmeasure with a tuned α value. The only previous discriminative approach which has been shown to produce translations of similar or better quality to those produced by the symmetrized baseline was (Ittycheriah and Roukos, 2005). They had access to 5000 gold standard word alignments, considerably more than the 100 or 110 gold standard word alignments used he</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="4784" citStr="Och, 2003" startWordPosition="739" endWordPosition="740">MM. A/E MODEL 4: ITERATION 4 65.6 / 60.5 53.6 / 50.2 69.1 / 64.6 (UNION) F/E MODEL 4: ITERATION 4 73.8 / 75.1 74.2 / 73.5 76.5 / 76.4 (REFINED) Table 2: Baseline Results. F-measures are presented on both the alignment discriminative training set and the alignment test set sub-corpora, separated by /. discriminative training set and alignment test set. Translation quality is evaluated by translating a held-out translation test set. An additional translation set called the Maximum BLEU set is employed by the SMT system to train the weights associated with the components of its log-linear model (Och, 2003). The training corpora are publicly available: both the Arabic/English data and the French/English Hansards were released by LDC. We created the manual word alignments ourselves, following the Blinker guidelines (Melamed, 1998). To train our baseline systems we follow a standard procedure. The models were trained two times, first using French or Arabic as the source language and then using English as the source language. For each training direction, we run GIZA++ (Och and Ney, 2003), specifying 5 iterations of Model 1, 4 iterations of the HMM model (Vogel et al., 1996), and 4 iterations of Mod</context>
<context position="8142" citStr="Och (2003)" startWordPosition="1310" endWordPosition="1311">ion of both 14 non-zero fertility English word penalty alignments 7 translation table from union of both alignments 15 NULL Foreign word penalty 8 translation table from intersection of both alignments 16 non-NULL Foreign word penalty Table 3: Sub-Models. Note that sub-models 1 to 5 are IBM Model 4, sub-models 6 to 16 are new. Log-linear models are often trained to maximize entropy, but we will train our model directly on the final performance criterion. We use 1−F-measure as our error function, comparing hypothesized word alignments for the discriminative training set with the gold standard. Och (2003) has described an efficient exact one-dimensional error minimization technique for a similar search problem in machine translation. The technique involves calculating a piecewise constant function f,,,(x) which evaluates the error of the hypotheses which would be picked by equation 2 from a set of hypotheses if we hold all weights constant, except for the weight An (which is set to x). The discriminative reranking algorithm is initialized with the parameters of the sub-models 0, an initial choice of the A vector, gold standard word alignments (labels) for the alignment discriminative training </context>
<context position="21116" citStr="Och, 2003" startWordPosition="3550" endWordPosition="3551">g much larger improvements. 5 Improvement of MT Quality The symmetrized alignments from the last iteration of EMD were used to build phrasal SMT systems, as were the symmetrized Model 4 alignments (the baseline). Aside from the final alignment, all other resources were held constant between the baseline and contrastive SMT systems, including those based on lower level alignments models such as IBM Model 1. For all of our experiments, we use two language models, one built using the English portion of the training data and the other built using additional English news data. We run Maximum BLEU (Och, 2003) for 25 iterations individually for each system. Table 8 shows our results. We report BLEU (Papineni et al., 2001) multiplied by 100. We also show the F-measure after heuristic symmetrization of the alignment test sets. The table shows that 4We would like to thank an anonymous reviewer for suggesting that this experiment would be useful even when using a small discriminative training corpus. our algorithm produces heuristically symmetrized final alignments of improved F-measure. Using these alignments in our phrasal SMT system, we produced a statistically significant BLEU improvement (at a 95%</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore A Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research Division, Thomas J. Watson Research</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<location>Center, Yorktown Heights, NY,</location>
<contexts>
<context position="21230" citStr="Papineni et al., 2001" startWordPosition="3568" endWordPosition="3572">tion of EMD were used to build phrasal SMT systems, as were the symmetrized Model 4 alignments (the baseline). Aside from the final alignment, all other resources were held constant between the baseline and contrastive SMT systems, including those based on lower level alignments models such as IBM Model 1. For all of our experiments, we use two language models, one built using the English portion of the training data and the other built using additional English news data. We run Maximum BLEU (Och, 2003) for 25 iterations individually for each system. Table 8 shows our results. We report BLEU (Papineni et al., 2001) multiplied by 100. We also show the F-measure after heuristic symmetrization of the alignment test sets. The table shows that 4We would like to thank an anonymous reviewer for suggesting that this experiment would be useful even when using a small discriminative training corpus. our algorithm produces heuristically symmetrized final alignments of improved F-measure. Using these alignments in our phrasal SMT system, we produced a statistically significant BLEU improvement (at a 95% confidence interval a gain of 0.78 is necessary) on the French/English task and a statistically significant BLEU </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore A. Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center, Yorktown Heights, NY, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Seeger</author>
</authors>
<title>Learning with labeled and unlabeled data.</title>
<date>2000</date>
<booktitle>In Technical report,</booktitle>
<note>Available at http://www.dai.ed.ac.uk/ seeger/papers.html.</note>
<contexts>
<context position="25414" citStr="Seeger, 2000" startWordPosition="4238" endWordPosition="4239"> produced by the symmetrized baseline was (Ittycheriah and Roukos, 2005). They had access to 5000 gold standard word alignments, considerably more than the 100 or 110 gold standard word alignments used here. They also invested significant effort in sub-model engineering (producing both sub-models specific to Arabic/English alignment and sub-models which would be useful for other language pairs), while we use sub-models which are simple extensions of Model 4 and language independent. The problem of semi-supervised learning is often defined as “using unlabeled data to help supervised learning” (Seeger, 2000). Most work on semi-supervised learning uses underlying distributions with a relatively small number of parameters. An initial model is estimated in a supervised fashion using the labeled data, and this supervised model is used to attach labels (or a probability distribution over labels) to the unlabeled data, then a new supervised model is estimated, and this is iterated. If these techniques are applied when there are a small number of labels in relation to the number of parameters used, they will suffer from the “overconfident pseudo-labeling problem” (Seeger, 2000), where the initial labels</context>
</contexts>
<marker>Seeger, 2000</marker>
<rawString>Matthias Seeger. 2000. Learning with labeled and unlabeled data. In Technical report, 2000. Available at http://www.dai.ed.ac.uk/ seeger/papers.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-Julien</author>
<author>Dan Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In Proc. of Human Language Technology Conf. and Conf. on Empirical Methods in Natural Language Processing,</booktitle>
<location>Vancouver, BC,</location>
<contexts>
<context position="23167" citStr="Taskar et al., 2005" startWordPosition="3887" endWordPosition="3890">ng F-measure tracks BLEU to the best extent possible. The best settings were α = 0.2 for Arabic/English and α = 0.7 for French/English, and these settings of α were used for every result reported in this paper. See (Fraser and Marcu, 2006) for further details. 1 F(A, S, α) = (1_α) (3) Precision(A,$) + α Recall(A,$) 6 Previous Research Previous work on discriminative training for wordalignment differed most strongly from our approach in that it generally views word-alignment as a supervised task. Examples of this perspective include (Liu et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005). All of these also used knowledge from one of the IBM Models in order to obtain competitive results 774 SYSTEM BLEU F-MEASURE A/E UNSUP. MODEL 4 UNION 49.16 64.6 A/E EMD 3 UNION 50.84 69.4 F/E UNSUP. MODEL 4 REFINED 30.63 76.4 F/E EMD 2 REFINED 31.56 81.2 Table 8: Evaluation of Translation Quality with the baseline (with the exception of (Moore, 2005)). We interleave discriminative training with EM and are therefore performing semi-supervised training. We show that semi-supervised training leads to better word alignments than running unsupervised training followed by discriminative training. </context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A discriminative matching approach to word alignment. In Proc. of Human Language Technology Conf. and Conf. on Empirical Methods in Natural Language Processing, Vancouver, BC, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING ’96: The 16th Int. Conf. on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="5359" citStr="Vogel et al., 1996" startWordPosition="831" endWordPosition="834">mponents of its log-linear model (Och, 2003). The training corpora are publicly available: both the Arabic/English data and the French/English Hansards were released by LDC. We created the manual word alignments ourselves, following the Blinker guidelines (Melamed, 1998). To train our baseline systems we follow a standard procedure. The models were trained two times, first using French or Arabic as the source language and then using English as the source language. For each training direction, we run GIZA++ (Och and Ney, 2003), specifying 5 iterations of Model 1, 4 iterations of the HMM model (Vogel et al., 1996), and 4 iterations of Model 4. We quantify the quality of the resulting hypothesized alignments with F-measure using the manually aligned sets. We present the results for three different conditions in Table 2. For the “F to E” direction the models assign non-zero probability to alignments consisting of links from one Foreign word to zero or more English words, while for “E to F” the models assign non-zero probability to alignments consisting of links from one English word to zero or more Foreign words. It is standard practice to improve the final alignments by combining the “F to E” and “E to </context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In COLING ’96: The 16th Int. Conf. on Computational Linguistics, pages 836–841, Copenhagen, Denmark, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>