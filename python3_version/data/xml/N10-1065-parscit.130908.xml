<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000152">
<title confidence="0.785643">
Everybody loves a rich cousin: An empirical study of transliteration through
bridge languages
</title>
<author confidence="0.546749">
A Kumaran
</author>
<affiliation confidence="0.638104333333333">
Microsoft Research India,
Bangalore,
India
</affiliation>
<email confidence="0.890159">
a.kumaran@microsoft.com
</email>
<author confidence="0.933568">
Mitesh M. Khapra
</author>
<affiliation confidence="0.947923">
Indian Institute of Technology
</affiliation>
<address confidence="0.30767">
Bombay,
Powai, Mumbai 400076,
India
</address>
<email confidence="0.993088">
miteshk@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.9973" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999723272727273">
Most state of the art approaches for machine
transliteration are data driven and require sig-
nificant parallel names corpora between lan-
guages. As a result, developing translitera-
tion functionality among n languages could
be a resource intensive task requiring paral-
lel names corpora in the order of nC2. In this
paper, we explore ways of reducing this high
resource requirement by leveraging the avail-
able parallel data between subsets of the n lan-
guages, transitively. We propose, and show
empirically, that reasonable quality transliter-
ation engines may be developed between two
languages, X and Y , even when no direct par-
allel names data exists between them, but only
transitively through language Z. Such sys-
tems alleviate the need for O(nC2) corpora,
significantly. In addition we show that the per-
formance of such transitive transliteration sys-
tems is in par with direct transliteration sys-
tems, in practical applications, such as CLIR
systems.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999330636363637">
Names and Out Of Vocabulary (OOV) terms appear
very frequently in written and spoken text and hence
play a very important role in several Natural Lan-
guage Processing applications. Several studies have
shown that handling names correctly across lan-
guages can significantly improve the performance of
CLIR Systems (Mandl and Womser-Hacker, 2004)
and the utility of machine translation systems. The
fact that translation lexicons or even statistical dic-
tionaries derived from parallel data do not provide a
good coverage of name and OOV translations, un-
</bodyText>
<author confidence="0.25303">
Pushpak Bhattacharyya
</author>
<affiliation confidence="0.2812915">
Indian Institute of Technology
Bombay,
Powai, Mumbai 400076,
India
</affiliation>
<email confidence="0.811877">
pb@cse.iitb.ac.in
</email>
<bodyText confidence="0.999137555555556">
derscores the need for good transliteration engines
to transform them between the language.
The importance of machine transliteration, in the
above context, is well realized by the research com-
munity and several approaches have been proposed
to solve the problem. However, most state of the art
approaches are data driven and require significant
parallel names corpora between languages. Such
data may not always be available between every pair
of languages, thereby limiting our ability to support
transliteration functionality between many language
pairs, and subsequently information access between
languages. For example, let us consider a practi-
cal scenario where we have six languages from four
different language families as shown in Figure 1.
The nodes in the graph represent languages and the
edges indicate the availability of data between that
language pair and thus the availability of a Machine
Transliteration system for that pair. It is easy to see
the underlying characteristics of the graph. Data is
available between a language pair due to one of the
following three reasons:
Politically related languages: Due to the political
dominance of English it is easy to obtain parallel
names data between English and most languages.
Genealogically related languages: Arabic and He-
brew share a common origin and there is a signifi-
cant overlap between their phoneme and grapheme
inventory. It is easy to obtain parallel names data
between these two languages.
Demographically related languages: Hindi and
Kannada are languages spoken in the Indian sub-
continent, though they are from different language
families. However, due to the shared culture and de-
mographics, it is easy to create parallel names data
between these two languages.
</bodyText>
<page confidence="0.972543">
420
</page>
<note confidence="0.8483835">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 420–428,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999845">
Figure 1: A connected graph of languages
</figureCaption>
<bodyText confidence="0.999164342105263">
On the other hand, for politically, demographi-
cally and genealogically unrelated languages such
as, say, Hindi and Hebrew, parallel data is not readily
available, either due to the unavailability of skilled
bilingual speakers. Even the argument of using
Wikipedia resources for such creation of such par-
allel data does not hold good, as the amount of inter-
linking may be very small to yield data. For exam-
ple, only 800 name pairs between Hindi and Hebrew
were mined using a state of the art mining algorithm
(Udupa et al., 2009), from Wikipedia interwiki links.
We propose a methodology to develop a practi-
cal Machine Transliteration system between any two
nodes of the above graph, provided a two-step path
exists between them. That is, even when no parallel
data exists between X &amp; Y but sufficient data exists
between X &amp; Z and Z &amp; Y it is still possible to de-
velop transliteration functionality between X &amp; Y
by combining a X—* Z system with a Z Y
system. For example, given the graph of Figure 1,
we explore the possibility of developing translitera-
tion functionality between Hindi and Russian even
though no direct data is available between these two
languages. Further, we show that in many cases the
bridge language can be suitably selected to ensure
optimal MT accuracy.
To establish the practicality and utility of our ap-
proach we integrated such a bridge transliteration
system with a standard CLIR system and compared
its performance with that of a direct transliteration
system. We observed that such a bridge system
performs well in practice and in specific instances
results in improvement in CLIR performance over
a baseline system further strengthening our claims
that such bridge systems are good practical solutions
for alleviating the resource scarcity problem.
To summarize, our main contributions in this pa-
per are:
</bodyText>
<listItem confidence="0.998892454545455">
1. Constructing bridge transliteration systems and
establishing empirically their quality.
2. Demonstrating their utility in providing prac-
tical transliteration functionality between two
languages X &amp; Y with no direct parallel data
between them.
3. Demonstrating that in specific cases it is pos-
sible to select the bridge language so that op-
timal Machine Transliteration accuracy is en-
sured while stepping through the bridge lan-
guage.
</listItem>
<subsectionHeader confidence="0.99726">
1.1 Organization of the Paper
</subsectionHeader>
<bodyText confidence="0.999971307692308">
This paper is organized in the following manner. In
section 2 we present the related work and highlight
the lack of work on transliteration in resource scarce
scenarios. In section 3 we discuss the methodology
of bridge transliteration. Section 4 discusses the ex-
periments and datasets used. Section 4.3 discusses
the results and error analysis. Section 5 discusses or-
thographic characteristics to be considered while se-
lecting the bridge language. Section 6 demonstrates
the effectiveness of such bridge systems in a practi-
cal scenario, viz., Cross Language Information Re-
trieval. Section 7 concludes the paper, highlighting
future research issues.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999824818181818">
Current models for transliteration can be classi-
fied as grapheme-based, phoneme-based and hy-
brid models. Grapheme-based models, such as,
Source Channel Model (Lee and Choi, 1998), Max-
imum Entropy Model (Goto et al., 2003), Condi-
tional Random Fields (Veeravalli et al., 2008) and
Decision Trees (Kang and Choi, 2000) treat translit-
eration as an orthographic process and try to map
the source language graphemes directly to the tar-
get language graphemes. Phoneme based models,
such as, the ones based on Weighted Finite State
</bodyText>
<page confidence="0.998018">
421
</page>
<bodyText confidence="0.999927324324324">
Transducers (WFST) (Knight and Graehl, 1997)
and extended Markov window (Jung et al., 2000)
treat transliteration as a phonetic process rather than
an orthographic process. Under such frameworks,
transliteration is treated as a conversion from source
grapheme to source phoneme followed by a conver-
sion from source phoneme to target grapheme. Hy-
brid models either use a combination of a grapheme
based model and a phoneme based model (Stalls
and Knight, 1998) or capture the correspondence be-
tween source graphemes and source phonemes to
produce target language graphemes (Oh and Choi,
2002).
A significant shortcoming of all the previous
works was that none of them addressed the issue of
performing transliteration in a resource scarce sce-
nario, as there was always an implicit assumption
of availability of data between a pair of languages.
In particular, none of the above approaches address
the problem of developing transliteration functional-
ity between a pair of languages when no direct data
exists between them but sufficient data is available
between each of these languages and an intermedi-
ate language. Some work on similar lines has been
done in Machine Translation (Wu and Wang, 2007)
wherein an intermediate bridge language (say, Z) is
used to fill the data void that exists between a given
language pair (say, X and Y ). In fact, recently it has
been shown that the accuracy of a X —* Z Machine
Translation system can be improved by using addi-
tional X —* Y data provided Z and Y share some
common vocabulary and cognates (Nakov and Ng,
2009). However, no such effort has been made in the
area of Machine Transliteration. To the best of our
knowledge, this work is the first attempt at providing
a practical solution to the problem of transliteration
in the face of resource scarcity.
</bodyText>
<sectionHeader confidence="0.966518" genericHeader="method">
3 Bridge Transliteration Systems
</sectionHeader>
<bodyText confidence="0.995811375">
In this section, we explore the salient question “Is
it possible to develop a practical machine transliter-
ation system between X and Y, by composing two
intermediate X —* Z and Z —* Y transliteration
systems?” We use a standard transliteration method-
ology based on orthography for all experiments (as
outlined in section 3.1), to ensure the applicability
of the methodology to a variety of languages.
</bodyText>
<subsectionHeader confidence="0.96912">
3.1 CRF based transliteration engine
</subsectionHeader>
<bodyText confidence="0.9869294">
Conditional Random Fields ((Lafferty et al., 2001))
are undirected graphical models used for labeling
sequential data. Under this model, the conditional
probability distribution of the target word given the
source word is given by,
</bodyText>
<equation confidence="0.9848678">
P(Y |X,A) = 1 eEt1 E
N(X) 1 Ak A (Yt− 1,Yt,X,t)
(1)
where,
X = source word
Y = target word
T = length of source word
K = number of features
Ak = feature weight
N(X) = normalization constant
</equation>
<bodyText confidence="0.997972769230769">
CRF++ 1, an open source implementation of CRF
was used for training and decoding (i.e. transliter-
ating the names). GIZA++ (Och and Ney, 2003),
a freely available implementation of the IBM align-
ment models (Brown et al., 1993) was used to get
character level alignments for the name pairs in the
parallel names training corpora. Under this align-
ment, each character in the source word is aligned to
zero or more characters in the corresponding target
word. The following features are then generated us-
ing this character-aligned data (here ei and hi form
the i-th pair of aligned characters in the source word
and target word respectively):
</bodyText>
<listItem confidence="0.99979325">
• hi and ej such that i − 2 &lt; j &lt; i + 2
• hi and source character bigrams ( {ei−1, ei} or
{ei, ei+1})
• hi and source character trigrams ( {ei−2, ei−1,
ei} or {ei−1, ei, ei+1} or {ei, ei+1, ei+2})
• hi, hi−1 and ej such that i − 2 &lt; j &lt; i + 2
• hi, hi−1 and source character bigrams
• hi, hi−1 and source character trigrams
</listItem>
<footnote confidence="0.986555">
1http://crfpp.sourceforge.net/
</footnote>
<page confidence="0.993792">
422
</page>
<subsectionHeader confidence="0.995886">
3.2 Bridge Transliteration Methodology
</subsectionHeader>
<bodyText confidence="0.999919">
In this section, we outline our methodology for com-
posing transitive transliteration systems between X
and Y , using a bridge language Z, by chaining indi-
vidual direct transliteration systems. Our approach
of using bridge transliteration for finding the best
target string (Y *), given the input string X can be
represented by the following probabilistic expres-
sion:
</bodyText>
<equation confidence="0.9996145">
Y * = arg max
Y
�= P(Y, Z|X)
Z
�= P(Y |Z, X) * P(Z|X) (2)
Z
</equation>
<bodyText confidence="0.999832">
We simplify the above expression, by assuming that
Y is independent of X given Z; the linguistic intu-
ition behind this assumption is that the top-k outputs
of the X —* Z system corresponding to a string in
X, capture all the transliteration information neces-
sary for transliterating to Y . Subsequently, in sec-
tion 5 we discuss the characteristics of the effective
bridge languages to maximize the capture of neces-
sary information for the second stage of the translit-
eration, namely for generating correct strings of Z.
Thus,
</bodyText>
<equation confidence="0.9981895">
�Y * = P(Y |Z) * P(Z|X) (3)
Z
</equation>
<bodyText confidence="0.991746333333333">
The probabilities P(Y |Z) and P(Z|X) in Equation
(3) are derived from the two stages of the bridge sys-
tem. Specifically, we assume that the parallel names
corpora are available between the language pair, X
and Z, and the language pair, Z and Y . We train two
baseline CRF based transliteration systems (as out-
lined in Section 3.1), between the language X and
Z, and Z and Y . Each name in language X was
provided as an input into X —* Z transliteration sys-
tem, and the top-10 candidate strings in language Z
produced by this first stage system were given as an
input into the second stage system Z —* Y . The re-
sults were merged using Equation (2). Finally, the
top-10 outputs of this system were selected as the
output of the bridge system.
</bodyText>
<sectionHeader confidence="0.999839" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999969142857143">
It is a well known fact that transliteration is lossy,
and hence the transitive systems may be expected to
suffer from the accumulation of errors in each stage,
resulting in a system that is of much poorer quality
than a direct transliteration system. In this section,
we set out to quantify this expected loss in accuracy,
by a series of experiments in a set of languages us-
ing bridge transliteration systems and a baseline di-
rect systems. We conducted a comprehensive set of
experiments in a diverse set of languages, as shown
in Figure 1, that include English, Indic (Hindi and
Kannada), Slavic (Russian) and Semitic (Arabic and
Hebrew) languages. The datasets and results are de-
scribed in the following subsections.
</bodyText>
<subsectionHeader confidence="0.923811">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999993576923077">
To be consistent, for training each of these systems,
we used approximately 15K name pairs corpora (as
this was the maximum data available for some lan-
guage pairs). While we used the NEWS 2009 train-
ing corpus (Li et al., 2009) as a part of our train-
ing data, we enhanced the data set to about 15K by
adding more data of similar characteristics (such as,
name origin, domain, length of the name strings,
etc.), taken from the same source as the original
NEWS 2009 data. For languages such as Arabic
and Hebrew which were not part of the NEWS 2009
shared task, the data was created along the same
lines. All results are reported on the standard NEWS
2009 test set, wherever applicable. The test set con-
sists of about 1,000 name pairs in languages X and
Y ; to avoid any bias, it was made sure that there is
no overlap between the test set with the training sets
of both the X —* Z and Z —* Y systems. To estab-
lish a baseline, the same CRF based transliteration
system (outlined in Section 3.1) was trained with a
15K name pairs corpora between the languages X
—* Y . The same test set used for testing the transi-
tive systems was used for testing the direct system
as well. As before, to avoid any bias, we made sure
that there is no overlap between the test set and the
training set for the direct system as well.
</bodyText>
<sectionHeader confidence="0.792605" genericHeader="method">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999852166666667">
We produce top-10 outputs from the bridge system
as well from the direct system and compare their
performance. The performance is measured using
the following standard measures, viz., top-1 accu-
racy (ACC-1) and Mean F-score. These measures
are described in detail in (Li et al., 2009). Table 1
</bodyText>
<equation confidence="0.918569">
P(Y |X)
</equation>
<page confidence="0.965102">
423
</page>
<table confidence="0.9999112">
Language ACC-1 Relative change in Mean F-score Relative change in
Pair ACC-1 Mean F-score
Hin-Rus 0.507 -8.08% 0.903 -1.88%
Hin-Eng-Rus 0.466 0.886
Hin-Ara 0.458 -8.29% 0.897 -2.34%
Hin-Eng-Ara 0.420 0.876
Eng-Heb 0.544 0% 0.917 0%
Eng-Ara-Heb 0.544 0.917
Hin-Eng 0.422 -9.51% 0.884 -1.47%
Hin-Kan-Eng 0.382 0.871
</table>
<tableCaption confidence="0.999971">
Table 1: Stepping through an intermediate language
</tableCaption>
<bodyText confidence="0.999952555555556">
presents the performance measures, both for a di-
rect system (say, Hin-Rus), and a transitional sys-
tem (say, Hin-Eng-Rus), in 4 different transitional
systems, between English, Indic, Semitic and Slavic
languages. In each case, we observe that the transi-
tional systems have a slightly lower quality, with an
absolute drop in accuracy (ACC-1) of less than 0.05
(relative drop under 10%), and an absolute drop in
Mean F-Score of 0.02 (relative drop under 3%).
</bodyText>
<subsectionHeader confidence="0.999893">
4.3 Analysis of Results
</subsectionHeader>
<bodyText confidence="0.99991">
Intuitively, one would expect that the errors of the
two stages of the transitive transliteration system
(i.e., X -* Z, and Z -* Y ) to compound, leading
to a considerable loss in the overall performance of
the system. Given that the accuracies of the direct
transliteration systems are as given in Table 2, the
transitive systems are expected to have accuracies
close to the product of the accuracies of the individ-
ual stages, for independent systems.
</bodyText>
<table confidence="0.999695">
Language Pair ACC-1 Mean F-Score
Hin-Eng 0.422 0.884
Eng-Rus 0.672 0.935
Eng-Ara 0.514 0.905
Ara-Heb 1.000 1.000
Hin-Kan 0.433 0.879
Kan-Eng 0.434 0.886
</table>
<tableCaption confidence="0.99974">
Table 2: Performance of Direct Transliteration Systems
</tableCaption>
<bodyText confidence="0.999569942857143">
However, as we observe in Table 1, the relative
drop in the accuracy (ACC-1) is less than 10% from
that of the direct system, which goes against our in-
tuition. To identify the reasons for the better than
expected performance, we performed a detailed er-
ror analysis of each stage of the bridge translitera-
tion systems, and the results are reported in Tables 3
– 5. We draw attention to two interesting facts which
account for the better than expected performance of
the bridge system:
Improved 2nd stage performance on correct
inputs: In each one of the cases, as expected, the
ACC-1 of the first stage is same as the ACC-1 of the
X -* Z system. However, we notice that the ACC-1
of the second stage on the correct strings output
in the first stage, is significantly better than the the
ACC-1 of the Z -* Y system! For example, the
ACC-1 of the Eng-Rus system is 67.2% (see Table
2), but, that of the 2nd stage Eng-Rus system is
77.8%, namely, on the strings that are transliterated
correctly by the first stage. Our analysis indicate
that there are two reasons for such improvement:
First, the strings that get transliterated correctly in
the first stage are typically shorter or less ambigu-
ous and hence have a better probability of correct
transliterations in the both stages. This phenomenon
could be verified empirically: Names like ��4 19
{Gopal}, TIT741- {Ramesh}, TTIT {Ram} are
shorter and in general have less ambiguity on target
orthography. Second, also significantly, the use of
top-10 outputs from the first stage as input to the
second stage provides a better opportunity for the
second stage to produce correct string in Z. Again,
this phenomenon is verified by providing increasing
number of top-n results to the 2nd stage.
</bodyText>
<page confidence="0.997868">
424
</page>
<table confidence="0.997439833333333">
Hi→En→Ru En → Ru Stage-2
(Stage-2) Acc.
Correct Error
Hi→En Correct 263 75 77.81%
(Stage-1)
Error 119 362 24.74%
</table>
<tableCaption confidence="0.97889">
Table 3: Error Analysis for Hi→En→Ru
</tableCaption>
<table confidence="0.999531">
Hi→Ka→En Ka → En Stage-2
(Stage-2) Acc.
Correct Error
Hi→Ka Correct 225 196 53.44%
(Stage-1)
Error 151 400 27.40%
</table>
<tableCaption confidence="0.959714">
Table 5: Error Analysis for Hi→Ka→En
</tableCaption>
<table confidence="0.9987845">
Hi→En→Ar En → Ar Stage-2
(Stage-2) Acc.
Correct Error
Hi→En Correct 221 127 63.50%
(Stage-1)
Error 119 340 25.70%
</table>
<tableCaption confidence="0.99962">
Table 4: Error Analysis for Hi→En→Ar
</tableCaption>
<bodyText confidence="0.998839894736842">
2nd stage error correction on incorrect inputs:
The last rows in each of the above tables 3 – 5 re-
port the performance of the second stage system on
strings that were transliterated incorrectly by the first
stage. While we expected the second row to pro-
duce incorrect transliterations nearly for all inputs
(as the input themselves were incorrect in Z), we
find to our surprise that upto 25% of the erroneous
strings in Z were getting transliterated correctly in
Y ! This provides credence to our hypothesis that
sufficient transliteration information is captured in
the 1st stage output (even when incorrect) that may
be exploited in the 2nd stage. Empirically, we veri-
fied that in most cases (nearly 60%) the errors were
due to the incorrectly transliterated vowels, and in
many cases, they get corrected in the second stage,
and re-ranked higher in the output. Figure 2 shows a
few examples of such error corrections in the second
stage.
</bodyText>
<figureCaption confidence="0.997316">
Figure 2: Examples of error corrections
</figureCaption>
<sectionHeader confidence="0.637466" genericHeader="method">
5 Characteristics of the bridge language
</sectionHeader>
<bodyText confidence="0.999715571428571">
An interesting question that we explore in this sec-
tion is “how the choice of bridge language influence
the performance of the bridge system?”. The under-
lying assumption in transitive transliteration systems
(as expressed in Equation 3), is that “Y is indepen-
dent of X given Z”. In other words, we assume that
the representations in the language will Z “capture
sufficient transliteration information from X to pro-
duce correct strings in Y ”. We hypothesize that two
parameters of the bridge language, namely, the or-
thography inventory and the phoneme-to-grapheme
entropy, that has most influence on the quality of the
transitional systems, and provide empirical evidence
for this hypothesis.
</bodyText>
<subsectionHeader confidence="0.993151">
5.1 Richer Orthographic Inventory
</subsectionHeader>
<bodyText confidence="0.999831363636364">
In each of the successful bridge systems (that is,
those with a relative performance drop of less than
10%), presented in Table 1, namely, Hin-Eng-Ara,
Eng-Ara-Heb and Hin-Kan-Eng, the bridge lan-
guage has, in general, richer orthographic inven-
tory than the target language. Arabic has a reduced
set of vowels, and hence poorer orthographic inven-
tory compared with English. Similarly, between the
closely related Semitic languages Arabic-Hebrew,
there is a many-to-one mapping from Arabic to He-
brew, and between Kannada-English, Kannada has
nearly a superset of vowels and consonants as com-
pared to English or Hindi.
As an example for a poor choice of Z, we present
a transitional system, Hindi → Arabic → English, in
Table 6, in which the transitional language z (Ara-
bic) has smaller orthographic inventory than Y (En-
glish).
Arabic has a reduced set of vowels and, unlike En-
glish, in most contexts short vowels are optional. As
a result, when Arabic is used as the bridge language
the loss of information (in terms of vowels) is large
</bodyText>
<page confidence="0.998064">
425
</page>
<table confidence="0.9990592">
Language ACC Relative change in
-1
Pair ACC-1
Hin-Eng 0.422
Hin-Ara-Eng 0.155 -64.28%
</table>
<tableCaption confidence="0.999488">
Table 6: Incorrect choice of bridge language
</tableCaption>
<bodyText confidence="0.99633725">
and the second stage system has no possibility of re-
covering from such a loss. The performance of the
bridge system confirms such a drastic drop in ACC-
1 of nearly 64% compared with the direct system.
</bodyText>
<subsectionHeader confidence="0.998335">
5.2 Higher Phoneme-Grapheme Entropy
</subsectionHeader>
<bodyText confidence="0.9998416875">
We also find that the entropy in phoneme - grapheme
mapping of a language indicate a good correlation
with a good choice for a transition language. In
a good transitional system (say, Hin-Eng-Rus), En-
glish has a more ambiguous phoneme-to-grapheme
mapping than Russian; for example, in English the
phoneme ‘s’ as in Sam or Cecilia can be repre-
sented by the graphemes ‘c’ and ‘s’, whereas Rus-
sian uses only a single character to represent this
phoneme. In such cases, the ambiguity introduced
by the bridge language helps in recovering from er-
rors in the X —* Z system. The relative loss of
ACC-1 for this transitional system is only about 8%.
The Table 7 shows another transitional system, in
which a poor choice was for the transitional lan-
guage was made.
</bodyText>
<table confidence="0.9988646">
Language ACC Relative change in
-1
Pair ACC-1
Hin-Eng 0.422
Hin-Tam-Eng 0.231 -45.26%
</table>
<tableCaption confidence="0.999826">
Table 7: Incorrect choice of bridge language
</tableCaption>
<bodyText confidence="0.999907125">
Tamil has a reduced set of consonants compared
with Hindi or English. For example, the Hindi con-
sonants (k, kh, g, gh) are represented by a sin-
gle character in Tamil. As a result, when Tamil is
used as the bridge language it looses information (in
terms of consonants) and results in a significant drop
in performance (nearly a 45% drop in ACC-1) for
the bridge system.
</bodyText>
<sectionHeader confidence="0.468456" genericHeader="method">
6 Effectiveness of Bridge Transliteration
on CLIR System
</sectionHeader>
<bodyText confidence="0.999945166666667">
In this section, we demonstrate the effectiveness of
our bridge transliteration system on a downstream
application, namely, a Crosslingual Information Re-
trieval system. We used the standard document col-
lections from CLEF 2006 (Nardi and Peters, 2006),
CLEF 2007 (Nardi and Peters, 2007) and FIRE 2008
(FIRE, 2008). We used Hindi as the query language.
All the three fields (title, description and narration)
of the topics were used for the retrieval. Since the
collection and topics are from the previous years,
their relevance judgments were also available as a
reference for automatic evaluation.
</bodyText>
<subsectionHeader confidence="0.992226">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999968818181818">
We used primarily the statistical dictionaries gen-
erated by training statistical word alignment mod-
els on an existing Hindi-English parallel corpora.
As with any CLIR system that uses translation lex-
icon, we faced the problem of out-of-vocabulary
(OOV) query terms that need to be transliterated,
as they are typically proper names in the target lan-
guage. First, for comparison, we used the above
mentioned CLIR system with no transliteration en-
gine (Basic), and measured the crosslingual retrieval
performance. Clearly, the OOV terms would not be
converted into target language, and hence contribute
nothing to the retrieval performance. Second, we in-
tegrated a direct machine transliteration system be-
tween Hindi and English (D-HiEn), and calibrated
the improvement in performance. Third, we inte-
grate, instead of a direct system, a bridge transliter-
ation system between Hindi and English, transition-
ing through Kannada (B-HiKaEn). For both, direct
as well as bridge transliteration, we retained the top-
5 transliterations generated by the appropriate sys-
tem, for retrieval.
</bodyText>
<subsectionHeader confidence="0.87842">
6.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999869625">
The results of the above experiments are given in
Table 7. The current focus of these experiments is
to answer the question of whether the bridge ma-
chine transliteration systems used to transliterate
the OOV words in Hindi queries to English (by step-
ping through Kannada) performs at par with a di-
rect transliteration system. As expected, enhancing
the CLIR system with a machine transliteration sys-
</bodyText>
<page confidence="0.996577">
426
</page>
<table confidence="0.999808090909091">
Collection CLIR System MAP Relative MAP change Recall Relative Recall change
from Basic from Basic
CLEF 2006 Basic 0.1463 - 0.4952 -
D-HiEn 0.1536 +4.98% 0.5151 +4.01%
B-HiKaEn 0.1529 +4.51% 0.5302 +7.06%
CLEF 2007 Basic 0.2521 - 0.7156 -
D-HiEn 0.2556 +1.38% 0.7170 + 0.19%
B-HiKaEn 0.2748 +9.00% 0.7174 + 0.25%
FIRE 2008 Basic 0.4361 - 0.8457 -
D-HiEn 0.4505 +3.30% 0.8506 +0.57%
B-HiKaEn 0.4573 +4.86% 0.8621 +1.93%
</table>
<tableCaption confidence="0.999367">
Table 8: CLIR Experiments with bridge transliteration systems
</tableCaption>
<bodyText confidence="0.999724705882353">
tem (D-HiEn) gives better results over a CLIR sys-
tem with no transliteration functionality (Basic). On
the standard test collections, the bridge translitera-
tion system performs in par or better than the di-
rect transliteration system in terms of MAP as well
as recall. Even though, the bridged system is of
slightly lesser quality in ACC-1 in Hi-Ka-En, com-
pared to Hi-En (see Table 1), the top-5 results had
captured the correct transliteration, as shown in our
analysis. A detailed analysis of the query transla-
tions produced by the above systems showed that in
some cases the bridge systems does produce a bet-
ter transliteration thereby leading to a better MAP.
As an illustration, consider the OOV terms v�EVкn
{Vatican} and n~-l~ {Nestle} and the corre-
sponding transliterations generated by the different
systems. The Direct-HiEn system was unable to
</bodyText>
<table confidence="0.998664090909091">
OOV term D-HiEn B-HiKaEn
v�EVкn vetican vetican
(vatican) veticon vettican
vettican vatican
vetticon watican
wetican wetican
n~-l� nesle nestle
(nestle) nesly nesle
nesley nesley
nessle nestley
nesey nesly
</table>
<tableCaption confidence="0.999745">
Table 9: Sample output in direct and bridge systems
</tableCaption>
<bodyText confidence="0.99789675">
generate the correct transliteration in the top-5 re-
sults whereas the B-HiKaEn was able to produce the
correct transliteration in the top-5 results thereby re-
sulting in an improvement in MAP for these queries.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999819482758621">
In this paper, we introduced the idea of bridge
transliteration systems that were developed employ-
ing well-studied orthographic approaches between
constituent languages. We empirically established
the quality of such bridge transliteration systems
and showed that quite contrary to our expectations,
the quality of such systems does not degrade dras-
tically as compared to the direct systems. Our er-
ror analysis showed that these better-than-expected
results can be attributed to (i) Better performance
(—10-12%) of the second stage system on the strings
transliterated correctly by the first stage system and
(ii) Significant (-25%) error correction in the sec-
ond stage. Next, we highlighted that the perfor-
mance of such bridge systems will be satisfactory as
long as the orthographic inventory of the bridge lan-
guage is either richer or more ambiguous as com-
pared to the target language. We showed that our
results are consistent with this hypothesis and pro-
vided two examples where there is a significant drop
in the accuracy when the bridge language violates
the above constraints. Finally, we showed that a
state of the art CLIR system integrated with a bridge
transliteration system performs in par with the same
CLIR system integrated with a direct translitera-
tion system, vindicating our claim that such bridge
transliteration systems can be use in real-world ap-
plications to alleviate the resource requirement of
&apos;C2 parallel names corpora.
</bodyText>
<page confidence="0.997831">
427
</page>
<sectionHeader confidence="0.998335" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999867135802469">
Peter E Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19:263–311.
FIRE. 2008. Forum for information retrieval evaluation.
Isao Goto, Naoto Kato, Noriyoshi Uratani, and Terumasa
Ehara. 2003. Transliteration considering context in-
formation based on the maximum entropy method. In
Proceedings ofMT-Summit IX, pages 125–132.
Sung Young Jung, SungLim Hong, and Eunok Paek.
2000. An english to korean transliteration model of
extended markov window. In Proceedings of the 18th
conference on Computational linguistics, pages 383–
389.
Byung-Ju Kang and Key-Sun Choi. 2000. Automatic
transliteration and back-transliteration by decision tree
learning. In Proceedings of the 2nd International Con-
ference on Language Resources and Evaluation, pages
1135–1411.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Computational Linguistics, pages
128–135.
John D. Lafferty, Andrew Mccallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML ’01: Proceedings of the Eighteenth Interna-
tional Conference on Machine Learning, pages 282–
289, San Francisco, CA, USA.
Jae Sung Lee and Key-Sun Choi. 1998. English to ko-
rean statistical transliteration for information retrieval.
In Computer Processing of Oriental Languages, pages
17–37.
Haizhou Li, A Kumaran, , Min Zhang, and Vladimir Per-
vouvhine. 2009. Whitepaper of news 2009 machine
transliteration shared task. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Transliter-
ation (NEWS 2009), pages 19–26, Suntec, Singapore,
August. Association for Computational Linguistics.
Thomas Mandl and Christa Womser-Hacker. 2004. How
do named entities contribute to retrieval effectiveness?
In CLEF, pages 833–842.
Preslav Nakov and Hwee Tou Ng. 2009. Improved statis-
tical machine translation for resource-poor languages
using related resource-rich languages. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1358–1367, Singa-
pore, August. Association for Computational Linguis-
tics.
A Nardi and C Peters. 2006. Working notes for the clef
2006 workshop.
A Nardi and C Peters. 2007. Working notes for the clef
2007 workshop.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Jong-hoon Oh and Key-Sun Choi. 2002. An english-
korean transliteration model using pronunciation and
contextual rules. In Proceedings of the 19th In-
ternational Conference on Computational Linguistics
(COLING), pages 758–764.
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating names and technical terms in arabic text. In
Proceedings of COLING/ACL Workshop on Computa-
tional Approaches to Semitic Languages, pages 34–41.
Raghavendra Udupa, K Saravanan, Anton Bakalov, and
Abhijit Bhole. 2009. ”they are out there, if you know
where to look: Mining transliterations of oov query
terms for cross language information retrieval”. In
ECIR’09: Proceedings of the 31st European Confer-
ence on IR research on Advances in Information Re-
trieval, pages 437–448, Toulouse, France.
Suryaganesh Veeravalli, Sreeharsha Yella, Prasad Pin-
gali, and Vasudeva Varma. 2008. Statistical translit-
eration for cross language information retrieval using
hmm alignment model and crf. In Proceedings of the
2nd workshop on Cross Lingual Information Access
(CLIA) Addressing the Information Need of Multilin-
gual Societies.
Hua Wu and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine transla-
tion. Machine Translation, 21(3):165–181.
</reference>
<page confidence="0.998349">
428
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.308709">
<title confidence="0.764204333333333">loves a rich cousin: empirical study of transliteration through bridge languages A</title>
<affiliation confidence="0.832559">Microsoft Research</affiliation>
<email confidence="0.995211">a.kumaran@microsoft.com</email>
<author confidence="0.969521">M Mitesh</author>
<affiliation confidence="0.993728">Indian Institute of</affiliation>
<address confidence="0.643605">Powai, Mumbai</address>
<email confidence="0.973414">miteshk@cse.iitb.ac.in</email>
<abstract confidence="0.998283739130435">Most state of the art approaches for machine transliteration are data driven and require significant parallel names corpora between languages. As a result, developing transliterafunctionality among could be a resource intensive task requiring paralnames corpora in the order of In this paper, we explore ways of reducing this high resource requirement by leveraging the availparallel data between subsets of the languages, transitively. We propose, and show empirically, that reasonable quality transliteration engines may be developed between two even when no direct parallel names data exists between them, but only through language Such sysalleviate the need for significantly. In addition we show that the performance of such transitive transliteration systems is in par with direct transliteration systems, in practical applications, such as CLIR systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter E Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="10312" citStr="Brown et al., 1993" startWordPosition="1637" endWordPosition="1640">Lafferty et al., 2001)) are undirected graphical models used for labeling sequential data. Under this model, the conditional probability distribution of the target word given the source word is given by, P(Y |X,A) = 1 eEt1 E N(X) 1 Ak A (Yt− 1,Yt,X,t) (1) where, X = source word Y = target word T = length of source word K = number of features Ak = feature weight N(X) = normalization constant CRF++ 1, an open source implementation of CRF was used for training and decoding (i.e. transliterating the names). GIZA++ (Och and Ney, 2003), a freely available implementation of the IBM alignment models (Brown et al., 1993) was used to get character level alignments for the name pairs in the parallel names training corpora. Under this alignment, each character in the source word is aligned to zero or more characters in the corresponding target word. The following features are then generated using this character-aligned data (here ei and hi form the i-th pair of aligned characters in the source word and target word respectively): • hi and ej such that i − 2 &lt; j &lt; i + 2 • hi and source character bigrams ( {ei−1, ei} or {ei, ei+1}) • hi and source character trigrams ( {ei−2, ei−1, ei} or {ei−1, ei, ei+1} or {ei, ei</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter E Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>FIRE</author>
</authors>
<title>Forum for information retrieval evaluation.</title>
<date>2008</date>
<contexts>
<context position="23698" citStr="FIRE 2008" startWordPosition="3938" endWordPosition="3939"> are represented by a single character in Tamil. As a result, when Tamil is used as the bridge language it looses information (in terms of consonants) and results in a significant drop in performance (nearly a 45% drop in ACC-1) for the bridge system. 6 Effectiveness of Bridge Transliteration on CLIR System In this section, we demonstrate the effectiveness of our bridge transliteration system on a downstream application, namely, a Crosslingual Information Retrieval system. We used the standard document collections from CLEF 2006 (Nardi and Peters, 2006), CLEF 2007 (Nardi and Peters, 2007) and FIRE 2008 (FIRE, 2008). We used Hindi as the query language. All the three fields (title, description and narration) of the topics were used for the retrieval. Since the collection and topics are from the previous years, their relevance judgments were also available as a reference for automatic evaluation. 6.1 Experimental Setup We used primarily the statistical dictionaries generated by training statistical word alignment models on an existing Hindi-English parallel corpora. As with any CLIR system that uses translation lexicon, we faced the problem of out-of-vocabulary (OOV) query terms that need to </context>
<context position="25848" citStr="FIRE 2008" startWordPosition="4275" endWordPosition="4276">uestion of whether the bridge machine transliteration systems used to transliterate the OOV words in Hindi queries to English (by stepping through Kannada) performs at par with a direct transliteration system. As expected, enhancing the CLIR system with a machine transliteration sys426 Collection CLIR System MAP Relative MAP change Recall Relative Recall change from Basic from Basic CLEF 2006 Basic 0.1463 - 0.4952 - D-HiEn 0.1536 +4.98% 0.5151 +4.01% B-HiKaEn 0.1529 +4.51% 0.5302 +7.06% CLEF 2007 Basic 0.2521 - 0.7156 - D-HiEn 0.2556 +1.38% 0.7170 + 0.19% B-HiKaEn 0.2748 +9.00% 0.7174 + 0.25% FIRE 2008 Basic 0.4361 - 0.8457 - D-HiEn 0.4505 +3.30% 0.8506 +0.57% B-HiKaEn 0.4573 +4.86% 0.8621 +1.93% Table 8: CLIR Experiments with bridge transliteration systems tem (D-HiEn) gives better results over a CLIR system with no transliteration functionality (Basic). On the standard test collections, the bridge transliteration system performs in par or better than the direct transliteration system in terms of MAP as well as recall. Even though, the bridged system is of slightly lesser quality in ACC-1 in Hi-Ka-En, compared to Hi-En (see Table 1), the top-5 results had captured the correct transliterati</context>
</contexts>
<marker>FIRE, 2008</marker>
<rawString>FIRE. 2008. Forum for information retrieval evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Naoto Kato</author>
<author>Noriyoshi Uratani</author>
<author>Terumasa Ehara</author>
</authors>
<title>Transliteration considering context information based on the maximum entropy method.</title>
<date>2003</date>
<booktitle>In Proceedings ofMT-Summit IX,</booktitle>
<pages>125--132</pages>
<contexts>
<context position="7087" citStr="Goto et al., 2003" startWordPosition="1099" endWordPosition="1102">ents and datasets used. Section 4.3 discusses the results and error analysis. Section 5 discusses orthographic characteristics to be considered while selecting the bridge language. Section 6 demonstrates the effectiveness of such bridge systems in a practical scenario, viz., Cross Language Information Retrieval. Section 7 concludes the paper, highlighting future research issues. 2 Related Work Current models for transliteration can be classified as grapheme-based, phoneme-based and hybrid models. Grapheme-based models, such as, Source Channel Model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State 421 Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a c</context>
</contexts>
<marker>Goto, Kato, Uratani, Ehara, 2003</marker>
<rawString>Isao Goto, Naoto Kato, Noriyoshi Uratani, and Terumasa Ehara. 2003. Transliteration considering context information based on the maximum entropy method. In Proceedings ofMT-Summit IX, pages 125–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung Young Jung</author>
<author>SungLim Hong</author>
<author>Eunok Paek</author>
</authors>
<title>An english to korean transliteration model of extended markov window.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<pages>383--389</pages>
<contexts>
<context position="7485" citStr="Jung et al., 2000" startWordPosition="1163" endWordPosition="1166">urrent models for transliteration can be classified as grapheme-based, phoneme-based and hybrid models. Grapheme-based models, such as, Source Channel Model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State 421 Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grapheme based model and a phoneme based model (Stalls and Knight, 1998) or capture the correspondence between source graphemes and source phonemes to produce target language graphemes (Oh and Choi, 2002). A significant shortcoming of all the previous works was that none of them addressed the issue of p</context>
</contexts>
<marker>Jung, Hong, Paek, 2000</marker>
<rawString>Sung Young Jung, SungLim Hong, and Eunok Paek. 2000. An english to korean transliteration model of extended markov window. In Proceedings of the 18th conference on Computational linguistics, pages 383– 389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Byung-Ju Kang</author>
<author>Key-Sun Choi</author>
</authors>
<title>Automatic transliteration and back-transliteration by decision tree learning.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation,</booktitle>
<pages>1135--1411</pages>
<contexts>
<context position="7181" citStr="Kang and Choi, 2000" startWordPosition="1114" endWordPosition="1117">usses orthographic characteristics to be considered while selecting the bridge language. Section 6 demonstrates the effectiveness of such bridge systems in a practical scenario, viz., Cross Language Information Retrieval. Section 7 concludes the paper, highlighting future research issues. 2 Related Work Current models for transliteration can be classified as grapheme-based, phoneme-based and hybrid models. Grapheme-based models, such as, Source Channel Model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State 421 Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a </context>
</contexts>
<marker>Kang, Choi, 2000</marker>
<rawString>Byung-Ju Kang and Key-Sun Choi. 2000. Automatic transliteration and back-transliteration by decision tree learning. In Proceedings of the 2nd International Conference on Language Resources and Evaluation, pages 1135–1411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1997</date>
<booktitle>Machine transliteration. In Computational Linguistics,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="7438" citStr="Knight and Graehl, 1997" startWordPosition="1155" endWordPosition="1158">highlighting future research issues. 2 Related Work Current models for transliteration can be classified as grapheme-based, phoneme-based and hybrid models. Grapheme-based models, such as, Source Channel Model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State 421 Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grapheme based model and a phoneme based model (Stalls and Knight, 1998) or capture the correspondence between source graphemes and source phonemes to produce target language graphemes (Oh and Choi, 2002). A significant shortcoming of all the previous works</context>
</contexts>
<marker>Knight, Graehl, 1997</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1997. Machine transliteration. In Computational Linguistics, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew Mccallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML ’01: Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="9715" citStr="Lafferty et al., 2001" startWordPosition="1531" endWordPosition="1534">t providing a practical solution to the problem of transliteration in the face of resource scarcity. 3 Bridge Transliteration Systems In this section, we explore the salient question “Is it possible to develop a practical machine transliteration system between X and Y, by composing two intermediate X —* Z and Z —* Y transliteration systems?” We use a standard transliteration methodology based on orthography for all experiments (as outlined in section 3.1), to ensure the applicability of the methodology to a variety of languages. 3.1 CRF based transliteration engine Conditional Random Fields ((Lafferty et al., 2001)) are undirected graphical models used for labeling sequential data. Under this model, the conditional probability distribution of the target word given the source word is given by, P(Y |X,A) = 1 eEt1 E N(X) 1 Ak A (Yt− 1,Yt,X,t) (1) where, X = source word Y = target word T = length of source word K = number of features Ak = feature weight N(X) = normalization constant CRF++ 1, an open source implementation of CRF was used for training and decoding (i.e. transliterating the names). GIZA++ (Och and Ney, 2003), a freely available implementation of the IBM alignment models (Brown et al., 1993) wa</context>
</contexts>
<marker>Lafferty, Mccallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew Mccallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML ’01: Proceedings of the Eighteenth International Conference on Machine Learning, pages 282– 289, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jae Sung Lee</author>
<author>Key-Sun Choi</author>
</authors>
<title>English to korean statistical transliteration for information retrieval.</title>
<date>1998</date>
<booktitle>In Computer Processing of Oriental Languages,</booktitle>
<pages>17--37</pages>
<contexts>
<context position="7044" citStr="Lee and Choi, 1998" startWordPosition="1091" endWordPosition="1094">sliteration. Section 4 discusses the experiments and datasets used. Section 4.3 discusses the results and error analysis. Section 5 discusses orthographic characteristics to be considered while selecting the bridge language. Section 6 demonstrates the effectiveness of such bridge systems in a practical scenario, viz., Cross Language Information Retrieval. Section 7 concludes the paper, highlighting future research issues. 2 Related Work Current models for transliteration can be classified as grapheme-based, phoneme-based and hybrid models. Grapheme-based models, such as, Source Channel Model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State 421 Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source</context>
</contexts>
<marker>Lee, Choi, 1998</marker>
<rawString>Jae Sung Lee and Key-Sun Choi. 1998. English to korean statistical transliteration for information retrieval. In Computer Processing of Oriental Languages, pages 17–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Vladimir Pervouvhine</author>
</authors>
<title>Whitepaper of news 2009 machine transliteration shared task.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009),</booktitle>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<marker>Zhang, Pervouvhine, 2009</marker>
<rawString>Haizhou Li, A Kumaran, , Min Zhang, and Vladimir Pervouvhine. 2009. Whitepaper of news 2009 machine transliteration shared task. In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 19–26, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Mandl</author>
<author>Christa Womser-Hacker</author>
</authors>
<title>How do named entities contribute to retrieval effectiveness?</title>
<date>2004</date>
<booktitle>In CLEF,</booktitle>
<pages>833--842</pages>
<contexts>
<context position="1597" citStr="Mandl and Womser-Hacker, 2004" startWordPosition="237" endWordPosition="240">vely through language Z. Such systems alleviate the need for O(nC2) corpora, significantly. In addition we show that the performance of such transitive transliteration systems is in par with direct transliteration systems, in practical applications, such as CLIR systems. 1 Introduction Names and Out Of Vocabulary (OOV) terms appear very frequently in written and spoken text and hence play a very important role in several Natural Language Processing applications. Several studies have shown that handling names correctly across languages can significantly improve the performance of CLIR Systems (Mandl and Womser-Hacker, 2004) and the utility of machine translation systems. The fact that translation lexicons or even statistical dictionaries derived from parallel data do not provide a good coverage of name and OOV translations, unPushpak Bhattacharyya Indian Institute of Technology Bombay, Powai, Mumbai 400076, India pb@cse.iitb.ac.in derscores the need for good transliteration engines to transform them between the language. The importance of machine transliteration, in the above context, is well realized by the research community and several approaches have been proposed to solve the problem. However, most state of</context>
</contexts>
<marker>Mandl, Womser-Hacker, 2004</marker>
<rawString>Thomas Mandl and Christa Womser-Hacker. 2004. How do named entities contribute to retrieval effectiveness? In CLEF, pages 833–842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Improved statistical machine translation for resource-poor languages using related resource-rich languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1358--1367</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8951" citStr="Nakov and Ng, 2009" startWordPosition="1408" endWordPosition="1411">functionality between a pair of languages when no direct data exists between them but sufficient data is available between each of these languages and an intermediate language. Some work on similar lines has been done in Machine Translation (Wu and Wang, 2007) wherein an intermediate bridge language (say, Z) is used to fill the data void that exists between a given language pair (say, X and Y ). In fact, recently it has been shown that the accuracy of a X —* Z Machine Translation system can be improved by using additional X —* Y data provided Z and Y share some common vocabulary and cognates (Nakov and Ng, 2009). However, no such effort has been made in the area of Machine Transliteration. To the best of our knowledge, this work is the first attempt at providing a practical solution to the problem of transliteration in the face of resource scarcity. 3 Bridge Transliteration Systems In this section, we explore the salient question “Is it possible to develop a practical machine transliteration system between X and Y, by composing two intermediate X —* Z and Z —* Y transliteration systems?” We use a standard transliteration methodology based on orthography for all experiments (as outlined in section 3.1</context>
</contexts>
<marker>Nakov, Ng, 2009</marker>
<rawString>Preslav Nakov and Hwee Tou Ng. 2009. Improved statistical machine translation for resource-poor languages using related resource-rich languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1358–1367, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nardi</author>
<author>C Peters</author>
</authors>
<title>Working notes for the clef</title>
<date>2006</date>
<pages>workshop.</pages>
<contexts>
<context position="23648" citStr="Nardi and Peters, 2006" startWordPosition="3927" endWordPosition="3930">ndi or English. For example, the Hindi consonants (k, kh, g, gh) are represented by a single character in Tamil. As a result, when Tamil is used as the bridge language it looses information (in terms of consonants) and results in a significant drop in performance (nearly a 45% drop in ACC-1) for the bridge system. 6 Effectiveness of Bridge Transliteration on CLIR System In this section, we demonstrate the effectiveness of our bridge transliteration system on a downstream application, namely, a Crosslingual Information Retrieval system. We used the standard document collections from CLEF 2006 (Nardi and Peters, 2006), CLEF 2007 (Nardi and Peters, 2007) and FIRE 2008 (FIRE, 2008). We used Hindi as the query language. All the three fields (title, description and narration) of the topics were used for the retrieval. Since the collection and topics are from the previous years, their relevance judgments were also available as a reference for automatic evaluation. 6.1 Experimental Setup We used primarily the statistical dictionaries generated by training statistical word alignment models on an existing Hindi-English parallel corpora. As with any CLIR system that uses translation lexicon, we faced the problem of</context>
</contexts>
<marker>Nardi, Peters, 2006</marker>
<rawString>A Nardi and C Peters. 2006. Working notes for the clef 2006 workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nardi</author>
<author>C Peters</author>
</authors>
<title>Working notes for the clef</title>
<date>2007</date>
<pages>workshop.</pages>
<contexts>
<context position="23684" citStr="Nardi and Peters, 2007" startWordPosition="3933" endWordPosition="3936">di consonants (k, kh, g, gh) are represented by a single character in Tamil. As a result, when Tamil is used as the bridge language it looses information (in terms of consonants) and results in a significant drop in performance (nearly a 45% drop in ACC-1) for the bridge system. 6 Effectiveness of Bridge Transliteration on CLIR System In this section, we demonstrate the effectiveness of our bridge transliteration system on a downstream application, namely, a Crosslingual Information Retrieval system. We used the standard document collections from CLEF 2006 (Nardi and Peters, 2006), CLEF 2007 (Nardi and Peters, 2007) and FIRE 2008 (FIRE, 2008). We used Hindi as the query language. All the three fields (title, description and narration) of the topics were used for the retrieval. Since the collection and topics are from the previous years, their relevance judgments were also available as a reference for automatic evaluation. 6.1 Experimental Setup We used primarily the statistical dictionaries generated by training statistical word alignment models on an existing Hindi-English parallel corpora. As with any CLIR system that uses translation lexicon, we faced the problem of out-of-vocabulary (OOV) query terms</context>
</contexts>
<marker>Nardi, Peters, 2007</marker>
<rawString>A Nardi and C Peters. 2007. Working notes for the clef 2007 workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="10228" citStr="Och and Ney, 2003" startWordPosition="1623" endWordPosition="1626">ety of languages. 3.1 CRF based transliteration engine Conditional Random Fields ((Lafferty et al., 2001)) are undirected graphical models used for labeling sequential data. Under this model, the conditional probability distribution of the target word given the source word is given by, P(Y |X,A) = 1 eEt1 E N(X) 1 Ak A (Yt− 1,Yt,X,t) (1) where, X = source word Y = target word T = length of source word K = number of features Ak = feature weight N(X) = normalization constant CRF++ 1, an open source implementation of CRF was used for training and decoding (i.e. transliterating the names). GIZA++ (Och and Ney, 2003), a freely available implementation of the IBM alignment models (Brown et al., 1993) was used to get character level alignments for the name pairs in the parallel names training corpora. Under this alignment, each character in the source word is aligned to zero or more characters in the corresponding target word. The following features are then generated using this character-aligned data (here ei and hi form the i-th pair of aligned characters in the source word and target word respectively): • hi and ej such that i − 2 &lt; j &lt; i + 2 • hi and source character bigrams ( {ei−1, ei} or {ei, ei+1}) </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-hoon Oh</author>
<author>Key-Sun Choi</author>
</authors>
<title>An englishkorean transliteration model using pronunciation and contextual rules.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>758--764</pages>
<contexts>
<context position="7985" citStr="Oh and Choi, 2002" startWordPosition="1241" endWordPosition="1244"> Weighted Finite State 421 Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grapheme based model and a phoneme based model (Stalls and Knight, 1998) or capture the correspondence between source graphemes and source phonemes to produce target language graphemes (Oh and Choi, 2002). A significant shortcoming of all the previous works was that none of them addressed the issue of performing transliteration in a resource scarce scenario, as there was always an implicit assumption of availability of data between a pair of languages. In particular, none of the above approaches address the problem of developing transliteration functionality between a pair of languages when no direct data exists between them but sufficient data is available between each of these languages and an intermediate language. Some work on similar lines has been done in Machine Translation (Wu and Wang</context>
</contexts>
<marker>Oh, Choi, 2002</marker>
<rawString>Jong-hoon Oh and Key-Sun Choi. 2002. An englishkorean transliteration model using pronunciation and contextual rules. In Proceedings of the 19th International Conference on Computational Linguistics (COLING), pages 758–764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Glover Stalls</author>
<author>Kevin Knight</author>
</authors>
<title>Translating names and technical terms in arabic text.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>34--41</pages>
<contexts>
<context position="7853" citStr="Stalls and Knight, 1998" startWordPosition="1221" endWordPosition="1224">s and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State 421 Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hybrid models either use a combination of a grapheme based model and a phoneme based model (Stalls and Knight, 1998) or capture the correspondence between source graphemes and source phonemes to produce target language graphemes (Oh and Choi, 2002). A significant shortcoming of all the previous works was that none of them addressed the issue of performing transliteration in a resource scarce scenario, as there was always an implicit assumption of availability of data between a pair of languages. In particular, none of the above approaches address the problem of developing transliteration functionality between a pair of languages when no direct data exists between them but sufficient data is available betwee</context>
</contexts>
<marker>Stalls, Knight, 1998</marker>
<rawString>Bonnie Glover Stalls and Kevin Knight. 1998. Translating names and technical terms in arabic text. In Proceedings of COLING/ACL Workshop on Computational Approaches to Semitic Languages, pages 34–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raghavendra Udupa</author>
<author>K Saravanan</author>
<author>Anton Bakalov</author>
<author>Abhijit Bhole</author>
</authors>
<title>they are out there, if you know where to look: Mining transliterations of oov query terms for cross language information retrieval”.</title>
<date>2009</date>
<booktitle>In ECIR’09: Proceedings of the 31st European Conference on IR research on Advances in Information Retrieval,</booktitle>
<pages>437--448</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="4426" citStr="Udupa et al., 2009" startWordPosition="677" endWordPosition="680"> c�2010 Association for Computational Linguistics Figure 1: A connected graph of languages On the other hand, for politically, demographically and genealogically unrelated languages such as, say, Hindi and Hebrew, parallel data is not readily available, either due to the unavailability of skilled bilingual speakers. Even the argument of using Wikipedia resources for such creation of such parallel data does not hold good, as the amount of interlinking may be very small to yield data. For example, only 800 name pairs between Hindi and Hebrew were mined using a state of the art mining algorithm (Udupa et al., 2009), from Wikipedia interwiki links. We propose a methodology to develop a practical Machine Transliteration system between any two nodes of the above graph, provided a two-step path exists between them. That is, even when no parallel data exists between X &amp; Y but sufficient data exists between X &amp; Z and Z &amp; Y it is still possible to develop transliteration functionality between X &amp; Y by combining a X—* Z system with a Z Y system. For example, given the graph of Figure 1, we explore the possibility of developing transliteration functionality between Hindi and Russian even though no direct data is</context>
</contexts>
<marker>Udupa, Saravanan, Bakalov, Bhole, 2009</marker>
<rawString>Raghavendra Udupa, K Saravanan, Anton Bakalov, and Abhijit Bhole. 2009. ”they are out there, if you know where to look: Mining transliterations of oov query terms for cross language information retrieval”. In ECIR’09: Proceedings of the 31st European Conference on IR research on Advances in Information Retrieval, pages 437–448, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suryaganesh Veeravalli</author>
<author>Sreeharsha Yella</author>
<author>Prasad Pingali</author>
<author>Vasudeva Varma</author>
</authors>
<title>Statistical transliteration for cross language information retrieval using hmm alignment model and crf.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2nd workshop on Cross Lingual Information Access (CLIA) Addressing the Information Need of Multilingual Societies.</booktitle>
<contexts>
<context position="7140" citStr="Veeravalli et al., 2008" startWordPosition="1107" endWordPosition="1110">he results and error analysis. Section 5 discusses orthographic characteristics to be considered while selecting the bridge language. Section 6 demonstrates the effectiveness of such bridge systems in a practical scenario, viz., Cross Language Information Retrieval. Section 7 concludes the paper, highlighting future research issues. 2 Related Work Current models for transliteration can be classified as grapheme-based, phoneme-based and hybrid models. Grapheme-based models, such as, Source Channel Model (Lee and Choi, 1998), Maximum Entropy Model (Goto et al., 2003), Conditional Random Fields (Veeravalli et al., 2008) and Decision Trees (Kang and Choi, 2000) treat transliteration as an orthographic process and try to map the source language graphemes directly to the target language graphemes. Phoneme based models, such as, the ones based on Weighted Finite State 421 Transducers (WFST) (Knight and Graehl, 1997) and extended Markov window (Jung et al., 2000) treat transliteration as a phonetic process rather than an orthographic process. Under such frameworks, transliteration is treated as a conversion from source grapheme to source phoneme followed by a conversion from source phoneme to target grapheme. Hyb</context>
</contexts>
<marker>Veeravalli, Yella, Pingali, Varma, 2008</marker>
<rawString>Suryaganesh Veeravalli, Sreeharsha Yella, Prasad Pingali, and Vasudeva Varma. 2008. Statistical transliteration for cross language information retrieval using hmm alignment model and crf. In Proceedings of the 2nd workshop on Cross Lingual Information Access (CLIA) Addressing the Information Need of Multilingual Societies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Pivot language approach for phrase-based statistical machine translation.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="8592" citStr="Wu and Wang, 2007" startWordPosition="1339" endWordPosition="1342">Choi, 2002). A significant shortcoming of all the previous works was that none of them addressed the issue of performing transliteration in a resource scarce scenario, as there was always an implicit assumption of availability of data between a pair of languages. In particular, none of the above approaches address the problem of developing transliteration functionality between a pair of languages when no direct data exists between them but sufficient data is available between each of these languages and an intermediate language. Some work on similar lines has been done in Machine Translation (Wu and Wang, 2007) wherein an intermediate bridge language (say, Z) is used to fill the data void that exists between a given language pair (say, X and Y ). In fact, recently it has been shown that the accuracy of a X —* Z Machine Translation system can be improved by using additional X —* Y data provided Z and Y share some common vocabulary and cognates (Nakov and Ng, 2009). However, no such effort has been made in the area of Machine Transliteration. To the best of our knowledge, this work is the first attempt at providing a practical solution to the problem of transliteration in the face of resource scarcity</context>
</contexts>
<marker>Wu, Wang, 2007</marker>
<rawString>Hua Wu and Haifeng Wang. 2007. Pivot language approach for phrase-based statistical machine translation. Machine Translation, 21(3):165–181.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>