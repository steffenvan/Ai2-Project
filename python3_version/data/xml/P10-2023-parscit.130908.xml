<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008280">
<title confidence="0.97275">
Automatically Generating Term-frequency-induced Taxonomies
</title>
<author confidence="0.4505345">
Karin Murthy Tanveer A Faruquie L Venkata Subramaniam
K Hima Prasad Mukesh Mohania
</author>
<affiliation confidence="0.478961">
IBM Research - India
</affiliation>
<email confidence="0.987136">
{karinmur|ftanveer|lvsubram|hkaranam|mkmukesh}@in.ibm.com
</email>
<sectionHeader confidence="0.997252" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999932666666667">
We propose a novel method to automati-
cally acquire a term-frequency-based tax-
onomy from a corpus using an unsuper-
vised method. A term-frequency-based
taxonomy is useful for application do-
mains where the frequency with which
terms occur on their own and in combi-
nation with other terms imposes a natural
term hierarchy. We highlight an applica-
tion for our approach and demonstrate its
effectiveness and robustness in extracting
knowledge from real-world data.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970283018868">
Taxonomy deduction is an important task to under-
stand and manage information. However, building
taxonomies manually for specific domains or data
sources is time consuming and expensive. Tech-
niques to automatically deduce a taxonomy in an
unsupervised manner are thus indispensable. Au-
tomatic deduction of taxonomies consist of two
tasks: extracting relevant terms to represent con-
cepts of the taxonomy and discovering relation-
ships between concepts. For unstructured text, the
extraction of relevant terms relies on information
extraction methods (Etzioni et al., 2005).
The relationship extraction task can be classi-
fied into two categories. Approaches in the first
category use lexical-syntactic formulation to de-
fine patterns, either manually (Kozareva et al.,
2008) or automatically (Girju et al., 2006), and
apply those patterns to mine instances of the pat-
terns. Though producing accurate results, these
approaches usually have low coverage for many
domains and suffer from the problem of incon-
sistency between terms when connecting the in-
stances as chains to form a taxonomy. The second
category of approaches uses clustering to discover
terms and the relationships between them (Roy
and Subramaniam, 2006), even if those relation-
ships do not explicitly appear in the text. Though
these methods tackle inconsistency by addressing
taxonomy deduction globally, the relationships ex-
tracted are often difficult to interpret by humans.
We show that for certain domains, the frequency
with which terms appear in a corpus on their own
and in conjunction with other terms induces a nat-
ural taxonomy. We formally define the concept
of a term-frequency-based taxonomy and show
its applicability for an example application. We
present an unsupervised method to generate such
a taxonomy from scratch and outline how domain-
specific constraints can easily be integrated into
the generation process. An advantage of the new
method is that it can also be used to extend an ex-
isting taxonomy.
We evaluated our method on a large corpus of
real-life addresses. For addresses from emerging
geographies no standard postal address scheme
exists and our objective was to produce a postal
taxonomy that is useful in standardizing addresses
(Kothari et al., 2010). Specifically, the experi-
ments were designed to investigate the effective-
ness of our approach on noisy terms with lots of
variations. The results show that our method is
able to induce a taxonomy without using any kind
of lexical-semantic patterns.
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999909454545455">
One approach for taxonomy deduction is to use
explicit expressions (Iwaska et al., 2000) or lexi-
cal and semantic patterns such as is a (Snow et al.,
2004), similar usage (Kozareva et al., 2008), syn-
onyms and antonyms (Lin et al., 2003), purpose
(Cimiano and Wenderoth, 2007), and employed by
(Bunescu and Mooney, 2007) to extract and orga-
nize terms. The quality of extraction is often con-
trolled using statistical measures (Pantel and Pen-
nacchiotti, 2006) and external resources such as
wordnet (Girju et al., 2006). However, there are
</bodyText>
<page confidence="0.980176">
126
</page>
<note confidence="0.5076305">
Proceedings of the ACL 2010 Conference Short Papers, pages 126–131,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999676558139535">
domains (such as the one introduced in Section
3.2) where the text does not allow the derivation
of linguistic relations.
Supervised methods for taxonomy induction
provide training instances with global seman-
tic information about concepts (Fleischman and
Hovy, 2002) and use bootstrapping to induce new
seeds to extract further patterns (Cimiano et al.,
2005). Semi-supervised approaches start with
known terms belonging to a category, construct
context vectors of classified terms, and associate
categories to previously unclassified terms de-
pending on the similarity of their context (Tanev
and Magnini, 2006). However, providing train-
ing data and hand-crafted patterns can be tedious.
Moreover in some domains (such as the one pre-
sented in Section 3.2) it is not possible to construct
a context vector or determine the replacement fit.
Unsupervised methods use clustering of word-
context vectors (Lin, 1998), co-occurrence (Yang
and Callan, 2008), and conjunction features (Cara-
ballo, 1999) to discover implicit relationships.
However, these approaches do not perform well
for small corpora. Also, it is difficult to label the
obtained clusters which poses challenges for eval-
uation. To avoid these problems, incremental clus-
tering approaches have been proposed (Yang and
Callan, 2009). Recently, lexical entailment has
been used where the term is assigned to a cate-
gory if its occurrence in the corpus can be replaced
by the lexicalization of the category (Giuliano and
Gliozzo, 2008). In our method, terms are incre-
mentally added to the taxonomy based on their
support and context.
Association rule mining (Agrawal and Srikant,
1994) discovers interesting relations between
terms, based on the frequency with which terms
appear together. However, the amount of patterns
generated is often huge and constructing a tax-
onomy from all the patterns can be challenging.
In our approach, we employ similar concepts but
make taxonomy construction part of the relation-
ship discovery process.
</bodyText>
<sectionHeader confidence="0.999168" genericHeader="method">
3 Term-frequency-induced Taxonomies
</sectionHeader>
<bodyText confidence="0.9977094">
For some application domains, a taxonomy is in-
duced by the frequency in which terms appear in a
corpus on their own and in combination with other
terms. We first introduce the problem formally and
then motivate it with an example application.
</bodyText>
<figureCaption confidence="0.998799">
Figure 1: Part of an address taxonomy
</figureCaption>
<subsectionHeader confidence="0.984775">
3.1 Definition
</subsectionHeader>
<bodyText confidence="0.998884375">
Let C be a corpus of records r. Each record is
represented as a set of terms t. Let T = {t  |t ∈
r ∧ r ∈ C} be the set of all terms of C. Let f(t)
denote the frequency of term t, that is the number
of records in C that contain t. Let F(t, T+, T−)
denote the frequency of term t given a set of must-
also-appear terms T+ and a set of cannot-also-
appear terms T−. F(t, T+, T−) =  |{r ∈ C |
</bodyText>
<equation confidence="0.972857">
t ∈ r∧ ∀ t&apos; ∈ T+ : t&apos; ∈ r ∧ ∀ t&apos; ∈ T− : t&apos; ∈� r} |.
</equation>
<bodyText confidence="0.999104421052632">
A term-frequency-induced taxonomy (TFIT), is
an ordered tree over terms in T. For a node n in
the tree, n.t is the term at n, A(n) the ancestors of
n, and P(n) the predecessors of n.
A TFIT has a root node with the special term ⊥
and the conditional frequency ∞. The following
condition is true for any other node n:
∀t ∈ T, F(n.t, A(n), P(n)) ≥ F(t, A(n), P(n)).
That is, each node’s term has the highest condi-
tional frequency in the context of the node’s an-
cestors and predecessors. Only terms with a con-
ditional frequency above zero are added to a TFIT.
We show in Section 4 how a TFIT taxonomy
can be automatically induced from a given corpus.
But before that, we show that TFITs are useful in
practice and reflect a natural ordering of terms for
application domains where the concept hierarchy
is expressed through the frequency in which terms
appear.
</bodyText>
<subsectionHeader confidence="0.998434">
3.2 Example Domain: Address Data
</subsectionHeader>
<bodyText confidence="0.999964333333333">
An address taxonomy is a key enabler for address
standardization. Figure 1 shows part of such an ad-
dress taxonomy where the root contains the most
generic term and leaf-level nodes contain the most
specific terms. For emerging economies building
a standardized address taxonomy is a huge chal-
</bodyText>
<page confidence="0.993395">
127
</page>
<table confidence="0.997743125">
Row Term Part of address Category
1 D-15 house number alphanumerical
2 Rawal building name proper noun
3 Complex building name proper noun
4 Behind landmark marker
5 Hotel landmark marker
6 Ruchira landmark proper noun
7 Katre street proper noun
8 Road street marker
9 Jeevan area proper noun
10 Nagar area marker
11 Andheri city (taluk) proper noun
12 East city (taluk) direction
13 Mumbai district proper noun
14 Maharashtra state proper noun
15 400069 ZIP code 6 digit string
</table>
<tableCaption confidence="0.999892">
Table 1: Example of a tokenized address
</tableCaption>
<bodyText confidence="0.999595377777778">
lenge. First, new areas and with it new addresses
constantly emerge. Second, there are very limited
conventions for specifying an address (Faruquie et
al., 2010). However, while many developing coun-
tries do not have a postal taxonomy, there is often
no lack of address data to learn a taxonomy from.
Column 2 of Table 1 shows an example of an
Indian address. Although Indian addresses tend to
follow the general principal that more specific in-
formation is mentioned earlier, there is no fixed or-
der for different elements of an address. For exam-
ple, the ZIP code of an address may be mentioned
before or after the state information and, although
ZIP code information is more specific than city in-
formation, it is generally mentioned later in the
address. Also, while ZIP codes often exist, their
use by people is very limited. Instead, people tend
to mention copious amounts of landmark informa-
tion (see for example rows 4-6 in Table 1).
Taking all this into account, there is often not
enough structure available to automatically infer a
taxonomy purely based on the structural or seman-
tic aspects of an address. However, for address
data, the general-to-specific concept hierarchy is
reflected in the frequency with which terms appear
on their own and together with other terms.
It mostly holds that f(s) &gt; f(d) &gt; f(c) &gt;
f(z) where s is a state name, d is a district name,
c is a city name, and z is a ZIP code. How-
ever, sometimes the name of a large city may be
more frequent than the name of a small state. For
example, in a given corpus, the term ’Houston’
(a populous US city) may appear more frequent
than the term ’Vermont’ (a small US state). To
avoid that ’Houston’ is picked as a node at the first
level of the taxonomy (which should only contain
states), the conditional-frequency constraint intro-
duced in Section 3.1 is enforced for each node in a
TFIT. ’Houston’s state ’Texas’ (which is more fre-
quent) is picked before ’Houston’. After ’Texas’ is
picked it appears in the ”cannot-also-appear”’ list
for all further siblings on the first level, thus giving
’Houston’ has a conditional frequency of zero.
We show in Section 5 that an address taxonomy
can be inferred by generating a TFIT taxonomy.
</bodyText>
<sectionHeader confidence="0.9826" genericHeader="method">
4 Automatically Generating TFITs
</sectionHeader>
<bodyText confidence="0.999592666666667">
We describe a basic algorithm to generate a TFIT
and then show extensions to adapt to different ap-
plication domains.
</bodyText>
<subsectionHeader confidence="0.920933">
4.1 Base Algorithm
</subsectionHeader>
<figure confidence="0.931720066666667">
Algorithm 1 Algorithm for generating a TFIT.
// For initialization T+, T− are empty
// For initialization l,w are zero
genTFIT(T+, T−, C, l, w)
//select most frequent term
tneyt = tj with F(tj, T+, T−) is maximal amongst all
tj E C;
fneyt = F(tneyt, T+, T−);
if fneyt &gt; support then
//Output node (tj, l, w)
...
//Generate child node
genTFIT(T+ U {tneyt}, T−, C, l + 1, w)
//Generate sibling node
genTFIT(T+, T− U {tneyt}, C, l, w + 1)
</figure>
<subsectionHeader confidence="0.479263">
end if
</subsectionHeader>
<bodyText confidence="0.99987925">
To generate a TFIT taxonomy as defined in Sec-
tion 3.1 we recursively pick the most frequent term
given previously chosen terms. The basic algo-
rithm genTFIT is sketched out in Algorithm 1.
When genTFIT is called the first time, T+ and
T− are empty and both level l and width w are
zero. With each call of genTFIT a new node
n in the taxonomy is created with (t, l, w) where
t is the most frequent term given T+ and T−
and l and w capture the position in the taxonomy.
genTFIT is recursively called to generate a child
of n and a sibling for n.
The only input parameter required by our al-
gorithm is support. Instead of adding all terms
with a conditional frequency above zero, we only
add terms with a conditional frequency equal to or
higher than support. The support parameter con-
trols the precision of the resulting TFIT and also
the runtime of the algorithm. Increasing support
increases the precision but also lowers the recall.
</bodyText>
<page confidence="0.990031">
128
</page>
<subsectionHeader confidence="0.914143">
4.2 Integrating Constraints
</subsectionHeader>
<bodyText confidence="0.999996315789474">
Structural as well as semantic constraints can eas-
ily be integrated into the TFIT generation.
We distinguish between taxonomy-level and
node-level structural constraints. For example,
limiting the depth of the taxonomy by introduc-
ing a maxLevel constraint and checking before
each recursive call if maxLevel is reached, is
a taxonomy-level constraint. A node-level con-
straint applies to each node and affects the way
the frequency of terms is determined.
For our example application, we introduce the
following node-level constraint: at each node we
only count terms that appear at specific positions
in records with respect to the current level of the
node. Specifically, we slide (or incrementally in-
crease) a window over the address records start-
ing from the end. For example, when picking the
term ’Washington’ as a state name, occurrences of
’Washington’ as city or street name are ignored.
Using a window instead of an exact position ac-
counts for positional variability. Also, to accom-
modate varying amounts of landmark information
we length-normalize the position of terms. That is,
we divide all positions in an address by the average
length of an address (which is 10 for our 40 Mil-
lion addresses). Accordingly, we adjust the size of
the window and use increments of 0.1 for sliding
(or increasing) the window.
In addition to syntactical constraints, semantic
constraints can be integrated by classifying terms
for use when picking the next frequent term. In our
example application, markers tend to appear much
more often than any proper noun. For example,
the term ’Road’ appears in almost all addresses,
and might be picked up as the most frequent term
very early in the process. Thus, it is beneficial to
ignore marker terms during taxonomy generation
and adding them as a post-processing step.
</bodyText>
<subsectionHeader confidence="0.999678">
4.3 Handling Noise
</subsectionHeader>
<bodyText confidence="0.999966846153846">
The approach we propose naturally handles noise
by ignoring it, unless the noise level exceeds the
support threshold. Misspelled terms are generally
infrequent and will as such not become part of
the taxonomy. The same applies to incorrect ad-
dresses. Incomplete addresses partially contribute
to the taxonomy and only cause a problem if the
same information is missing too often. For ex-
ample, if more than support addresses with the
city ’Houston’ are missing the state ’Texas’, then
’Houston’ may become a node at the first level and
appear to be a state. Generally, such cases only ap-
pear at the far right of the taxonomy.
</bodyText>
<sectionHeader confidence="0.998263" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9999962">
We present an evaluation of our approach for ad-
dress data from an emerging economy. We imple-
mented our algorithm in Java and store the records
in a DB2 database. We rely on the DB2 optimizer
to efficiently retrieve the next frequent term.
</bodyText>
<subsectionHeader confidence="0.9288">
5.1 Dataset
</subsectionHeader>
<bodyText confidence="0.99998528">
The results are based on 40 Million Indian ad-
dresses. Each address record was given to us as
a single string and was first tokenized into a se-
quence of terms as shown in Table 1. In a second
step, we addressed spelling variations. There is no
fixed way of transliterating Indian alphabets to En-
glish and most Indian proper nouns have various
spellings in English. We used tools to detect syn-
onyms with the same context to generate a list of
rules to map terms to a standard form (Lin, 1998).
For example, in Table 1 ’Maharashtra’ can also be
spelled ’Maharastra’. We also used a list of key-
words to classify some terms as markers such as
’Road’ and ’Nagar’ shown in Table 1.
Our evaluation consists of two parts. First, we
show results for constructing a TFIT from scratch.
To evaluate the precision and recall we also re-
trieved post office addresses from India Post1,
cleaned them, and organized them in a tree.
Second, we use our approach to enrich the ex-
isting hierarchy created from post office addresses
with additional area terms. To validate the result,
we also retrieved data about which area names ap-
pear within a ZIP code.2 We also verified whether
Google Maps shows an area on its map.3
</bodyText>
<subsectionHeader confidence="0.99885">
5.2 Taxonomy Generation
</subsectionHeader>
<bodyText confidence="0.999974125">
We generated a taxonomy O using all 40 million
addresses. We compare the terms assigned to
category levels district and taluk4 in O with the
tree P constructed from post office addresses.
Each district and taluk has at least one post office.
Thus P covers all districts and taluks and allows
us to test coverage and precision. We compute the
precision and recall for each category level CL as
</bodyText>
<footnote confidence="0.99926775">
1http://www.indiapost.gov.in/Pin/pinsearch.aspx
2http://www.whereincity.com/india/pincode/search
3maps.google.com
4Administrative division in some South-Asian countries.
</footnote>
<page confidence="0.990222">
129
</page>
<table confidence="0.9999306">
Support Recall % Precision %
100 District 93.9 57.4
Taluk 50.9 60.5
200 District 87.9 64.4
Taluk 49.6 66.1
</table>
<tableCaption confidence="0.9291885">
Table 2: Precision and recall for categorizing
terms belonging to the state Maharashtra
</tableCaption>
<equation confidence="0.8077635">
RecallCL = # correct paths from root to CL in O
# paths from root to CL in P
Prec252oTZCL = # correct paths from root to CL in O
# paths from root to CL in O
</equation>
<bodyText confidence="0.999466421052632">
Table 2 shows precision and recall for district
and taluk for the large state Maharashtra. Recall
is good for district. For taluk it is lower because a
major part of the data belongs to urban areas where
taluk information is missing. The precision seems
to be low but it has to be noted that in almost 75%
of the addresses either district or taluk informa-
tion is missing or noisy. Given that, we were able
to recover a significant portion of the knowledge
structure.
We also examined a branch for a smaller state
(Kerala). Again, both districts and taluks appear
at the next level of the taxonomy. For a support
of 200 there are 19 entries in O of which all but
two appear in P as district or taluk. One entry is a
taluk that actually belongs to Maharashtra and one
entry is a name variation of a taluk in P. There
were not enough addresses to get a good coverage
of all districts and taluks.
</bodyText>
<subsectionHeader confidence="0.996987">
5.3 Taxonomy Augmentation
</subsectionHeader>
<bodyText confidence="0.999938526315789">
We used P and ran our algorithm for each branch
in P to include area information. We focus our
evaluation on the city Mumbai. The recall is low
because many addresses do not mention a ZIP
code or use an incorrect ZIP code. However,
the precision is good implying that our approach
works even in the presence of large amounts of
noise.
Table 3 shows the results for ZIP code 400002
and 400004 for a support of 100. We get simi-
lar results for other ZIP codes. For each detected
area we compared whether the area is also listed
on whereincity.com, part of a post office name
(PO), or shown on google maps. All but four
areas found are confirmed by at least one of the
three external sources. Out of the unconfirmed
terms Fanaswadi and MarineDrive seem to
be genuine area names but we could not confirm
DhakurdwarRoad. The term th is due to our
</bodyText>
<table confidence="0.999843894736842">
Area Whereincity PO Google
Bhuleshwar yes no yes
Chira Bazar yes no yes
Dhobi Talao no no yes
Fanaswadi no no no
Kalbadevi Road yes yes yes
Marine Drive no no no
Marine Lines yes yes yes
Princess Street no no yes
th no no no
Thakurdwar Road no no no
Zaveri Bazar yes no yes
Charni Road no yes no
Girgaon yes yes yes
Khadilkar Road yes no yes
Khetwadi Road yes no no
Kumbharwada no no yes
Opera House no yes no
Prathna Samaj yes no no
</table>
<tableCaption confidence="0.8664325">
Table 3: Areas found for ZIP code 400002 (top)
and 400004 (bottom)
</tableCaption>
<bodyText confidence="0.991655909090909">
tokenization process. 16 correct terms out of 18
terms results in a precision of 89%.
We also ran experiments to measure the cov-
erage of area detection for Mumbai without us-
ing ZIP codes. Initializing our algorithm with
Maharshtra and Mumbai yielded over 100 ar-
eas with a support of 300 and more. However,
again the precision is low because quite a few of
those areas are actually taluk names.
Using a large number of addresses is necessary
to achieve good recall and precision.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999812823529412">
In this paper, we presented a novel approach to
generate a taxonomy for data where terms ex-
hibit an inherent frequency-based hierarchy. We
showed that term frequency can be used to gener-
ate a meaningful taxonomy from address records.
The presented approach can also be used to extend
an existing taxonomy which is a big advantage
for emerging countries where geographical areas
evolve continuously.
While we have evaluated our approach on ad-
dress data, it is applicable to all data sources where
the inherent hierarchical structure is encoded in
the frequency with which terms appear on their
own and together with other terms. Preliminary
experiments on real-time analyst’s stock market
tips 5 produced a taxonomy of (TV station, An-
alyst, Affiliation) with decent precision and recall.
</bodyText>
<footnote confidence="0.994627">
5See Live Market voices at:
http://money.rediff.com/money/jsp/markets home.jsp
</footnote>
<page confidence="0.996067">
130
</page>
<sectionHeader confidence="0.995432" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999604618181818">
Rakesh Agrawal and Ramakrishnan Srikant. 1994.
Fast algorithms for mining association rules in large
databases. In Proceedings of the 20th International
Conference on Very Large Data Bases, pages 487–
499.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 576–583.
Sharon A. Caraballo. 1999. Automatic construction
of a hypernym-labeled noun hierarchy from text. In
Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics on Compu-
tational Linguistics, pages 120–126.
Philipp Cimiano and Johanna Wenderoth. 2007. Au-
tomatic acquisition of ranked qualia structures from
the web. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 888–895.
Philipp Cimiano, G¨unter Ladwig, and Steffen Staab.
2005. Gimme’ the context: context-driven auto-
matic semantic annotation with c-pankow. In Pro-
ceedings of the 14th International Conference on
World Wide Web, pages 332–341.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
an experimental study. Artificial Intelligence,
165(1):91–134.
Tanveer A. Faruquie, K. Hima Prasad, L. Venkata
Subramaniam, Mukesh K. Mohania, Girish Venkat-
achaliah, Shrinivas Kulkarni, and Pramit Basu.
2010. Data cleansing as a transient service. In
Proceedings of the 26th International Conference on
Data Engineering, pages 1025–1036.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proceed-
ings of the 19th International Conference on Com-
putational Linguistics, pages 1–7.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83–135.
Claudio Giuliano and Alfio Gliozzo. 2008. Instance-
based ontology population exploiting named-entity
substitution. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 265–272.
Lucja M. Iwaska, Naveen Mata, and Kellyn Kruger.
2000. Fully automatic acquisition of taxonomic
knowledge from large corpora of texts. In Lucja M.
Iwaska and Stuart C. Shapiro, editors, Natural Lan-
guage Processing and Knowledge Representation:
Language for Knowledge and Knowledge for Lan-
guage, pages 335–345.
Govind Kothari, Tanveer A Faruquie, L V Subrama-
niam, K H Prasad, and Mukesh Mohania. 2010.
Transfer of supervision for improved address stan-
dardization. In Proceedings of the 20th Interna-
tional Conference on Pattern Recognition.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1048–1056.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distri-
butionally similar words. In Proceedings of the 18th
International Joint Conference on Artificial Intelli-
gence, pages 1492–1493.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th Inter-
national Conference on Computational Linguistics,
pages 768–774.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 113–120.
Shourya Roy and L Venkata Subramaniam. 2006. Au-
tomatic generation of domain models for call cen-
ters from noisy transcriptions. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 737–
744.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in Neural Information Pro-
cessing Systems, pages 1297–1304.
Hristo Tanev and Bernardo Magnini. 2006. Weakly
supervised approaches for ontology population. In
Proceedings of the 11th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 3–7.
Hui Yang and Jamie Callan. 2008. Learning the dis-
tance metric in a personal ontology. In Proceed-
ing of the 2nd International Workshop on Ontolo-
gies and Information Systems for the Semantic Web,
pages 17–24.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 271–279.
</reference>
<page confidence="0.998296">
131
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.729815">
<title confidence="0.999801">Automatically Generating Term-frequency-induced Taxonomies</title>
<author confidence="0.8661405">Karin Murthy Tanveer A Faruquie L Venkata Subramaniam K Hima Prasad Mukesh Mohania</author>
<affiliation confidence="0.990203">IBM Research - India</affiliation>
<abstract confidence="0.999688">We propose a novel method to automatically acquire a term-frequency-based taxonomy from a corpus using an unsupervised method. A term-frequency-based taxonomy is useful for application domains where the frequency with which terms occur on their own and in combination with other terms imposes a natural term hierarchy. We highlight an application for our approach and demonstrate its effectiveness and robustness in extracting knowledge from real-world data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
<author>Ramakrishnan Srikant</author>
</authors>
<title>Fast algorithms for mining association rules in large databases.</title>
<date>1994</date>
<booktitle>In Proceedings of the 20th International Conference on Very Large Data Bases,</booktitle>
<pages>487--499</pages>
<contexts>
<context position="5542" citStr="Agrawal and Srikant, 1994" startWordPosition="843" endWordPosition="846">it relationships. However, these approaches do not perform well for small corpora. Also, it is difficult to label the obtained clusters which poses challenges for evaluation. To avoid these problems, incremental clustering approaches have been proposed (Yang and Callan, 2009). Recently, lexical entailment has been used where the term is assigned to a category if its occurrence in the corpus can be replaced by the lexicalization of the category (Giuliano and Gliozzo, 2008). In our method, terms are incrementally added to the taxonomy based on their support and context. Association rule mining (Agrawal and Srikant, 1994) discovers interesting relations between terms, based on the frequency with which terms appear together. However, the amount of patterns generated is often huge and constructing a taxonomy from all the patterns can be challenging. In our approach, we employ similar concepts but make taxonomy construction part of the relationship discovery process. 3 Term-frequency-induced Taxonomies For some application domains, a taxonomy is induced by the frequency in which terms appear in a corpus on their own and in combination with other terms. We first introduce the problem formally and then motivate it </context>
</contexts>
<marker>Agrawal, Srikant, 1994</marker>
<rawString>Rakesh Agrawal and Ramakrishnan Srikant. 1994. Fast algorithms for mining association rules in large databases. In Proceedings of the 20th International Conference on Very Large Data Bases, pages 487– 499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>576--583</pages>
<contexts>
<context position="3532" citStr="Bunescu and Mooney, 2007" startWordPosition="537" endWordPosition="540">dardizing addresses (Kothari et al., 2010). Specifically, the experiments were designed to investigate the effectiveness of our approach on noisy terms with lots of variations. The results show that our method is able to induce a taxonomy without using any kind of lexical-semantic patterns. 2 Related Work One approach for taxonomy deduction is to use explicit expressions (Iwaska et al., 2000) or lexical and semantic patterns such as is a (Snow et al., 2004), similar usage (Kozareva et al., 2008), synonyms and antonyms (Lin et al., 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. The quality of extraction is often controlled using statistical measures (Pantel and Pennacchiotti, 2006) and external resources such as wordnet (Girju et al., 2006). However, there are 126 Proceedings of the ACL 2010 Conference Short Papers, pages 126–131, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics domains (such as the one introduced in Section 3.2) where the text does not allow the derivation of linguistic relations. Supervised methods for taxonomy induction provide training instances with global semantic information abo</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 576–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
</authors>
<title>Automatic construction of a hypernym-labeled noun hierarchy from text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>120--126</pages>
<contexts>
<context position="4897" citStr="Caraballo, 1999" startWordPosition="742" endWordPosition="744"> approaches start with known terms belonging to a category, construct context vectors of classified terms, and associate categories to previously unclassified terms depending on the similarity of their context (Tanev and Magnini, 2006). However, providing training data and hand-crafted patterns can be tedious. Moreover in some domains (such as the one presented in Section 3.2) it is not possible to construct a context vector or determine the replacement fit. Unsupervised methods use clustering of wordcontext vectors (Lin, 1998), co-occurrence (Yang and Callan, 2008), and conjunction features (Caraballo, 1999) to discover implicit relationships. However, these approaches do not perform well for small corpora. Also, it is difficult to label the obtained clusters which poses challenges for evaluation. To avoid these problems, incremental clustering approaches have been proposed (Yang and Callan, 2009). Recently, lexical entailment has been used where the term is assigned to a category if its occurrence in the corpus can be replaced by the lexicalization of the category (Giuliano and Gliozzo, 2008). In our method, terms are incrementally added to the taxonomy based on their support and context. Associ</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>Sharon A. Caraballo. 1999. Automatic construction of a hypernym-labeled noun hierarchy from text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, pages 120–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Johanna Wenderoth</author>
</authors>
<title>Automatic acquisition of ranked qualia structures from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>888--895</pages>
<contexts>
<context position="3488" citStr="Cimiano and Wenderoth, 2007" startWordPosition="530" endWordPosition="533">roduce a postal taxonomy that is useful in standardizing addresses (Kothari et al., 2010). Specifically, the experiments were designed to investigate the effectiveness of our approach on noisy terms with lots of variations. The results show that our method is able to induce a taxonomy without using any kind of lexical-semantic patterns. 2 Related Work One approach for taxonomy deduction is to use explicit expressions (Iwaska et al., 2000) or lexical and semantic patterns such as is a (Snow et al., 2004), similar usage (Kozareva et al., 2008), synonyms and antonyms (Lin et al., 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. The quality of extraction is often controlled using statistical measures (Pantel and Pennacchiotti, 2006) and external resources such as wordnet (Girju et al., 2006). However, there are 126 Proceedings of the ACL 2010 Conference Short Papers, pages 126–131, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics domains (such as the one introduced in Section 3.2) where the text does not allow the derivation of linguistic relations. Supervised methods for taxonomy induction provide training in</context>
</contexts>
<marker>Cimiano, Wenderoth, 2007</marker>
<rawString>Philipp Cimiano and Johanna Wenderoth. 2007. Automatic acquisition of ranked qualia structures from the web. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 888–895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>G¨unter Ladwig</author>
<author>Steffen Staab</author>
</authors>
<title>Gimme’ the context: context-driven automatic semantic annotation with c-pankow.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th International Conference on World Wide Web,</booktitle>
<pages>332--341</pages>
<contexts>
<context position="4264" citStr="Cimiano et al., 2005" startWordPosition="647" endWordPosition="650">l and Pennacchiotti, 2006) and external resources such as wordnet (Girju et al., 2006). However, there are 126 Proceedings of the ACL 2010 Conference Short Papers, pages 126–131, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics domains (such as the one introduced in Section 3.2) where the text does not allow the derivation of linguistic relations. Supervised methods for taxonomy induction provide training instances with global semantic information about concepts (Fleischman and Hovy, 2002) and use bootstrapping to induce new seeds to extract further patterns (Cimiano et al., 2005). Semi-supervised approaches start with known terms belonging to a category, construct context vectors of classified terms, and associate categories to previously unclassified terms depending on the similarity of their context (Tanev and Magnini, 2006). However, providing training data and hand-crafted patterns can be tedious. Moreover in some domains (such as the one presented in Section 3.2) it is not possible to construct a context vector or determine the replacement fit. Unsupervised methods use clustering of wordcontext vectors (Lin, 1998), co-occurrence (Yang and Callan, 2008), and conju</context>
</contexts>
<marker>Cimiano, Ladwig, Staab, 2005</marker>
<rawString>Philipp Cimiano, G¨unter Ladwig, and Steffen Staab. 2005. Gimme’ the context: context-driven automatic semantic annotation with c-pankow. In Proceedings of the 14th International Conference on World Wide Web, pages 332–341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="1273" citStr="Etzioni et al., 2005" startWordPosition="176" endWordPosition="179">ting knowledge from real-world data. 1 Introduction Taxonomy deduction is an important task to understand and manage information. However, building taxonomies manually for specific domains or data sources is time consuming and expensive. Techniques to automatically deduce a taxonomy in an unsupervised manner are thus indispensable. Automatic deduction of taxonomies consist of two tasks: extracting relevant terms to represent concepts of the taxonomy and discovering relationships between concepts. For unstructured text, the extraction of relevant terms relies on information extraction methods (Etzioni et al., 2005). The relationship extraction task can be classified into two categories. Approaches in the first category use lexical-syntactic formulation to define patterns, either manually (Kozareva et al., 2008) or automatically (Girju et al., 2006), and apply those patterns to mine instances of the patterns. Though producing accurate results, these approaches usually have low coverage for many domains and suffer from the problem of inconsistency between terms when connecting the instances as chains to form a taxonomy. The second category of approaches uses clustering to discover terms and the relationsh</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tanveer A Faruquie</author>
<author>K Hima Prasad</author>
<author>L Venkata Subramaniam</author>
<author>Mukesh K Mohania</author>
<author>Girish Venkatachaliah</author>
<author>Shrinivas Kulkarni</author>
<author>Pramit Basu</author>
</authors>
<title>Data cleansing as a transient service.</title>
<date>2010</date>
<booktitle>In Proceedings of the 26th International Conference on Data Engineering,</booktitle>
<pages>1025--1036</pages>
<contexts>
<context position="8525" citStr="Faruquie et al., 2010" startWordPosition="1387" endWordPosition="1390">anumerical 2 Rawal building name proper noun 3 Complex building name proper noun 4 Behind landmark marker 5 Hotel landmark marker 6 Ruchira landmark proper noun 7 Katre street proper noun 8 Road street marker 9 Jeevan area proper noun 10 Nagar area marker 11 Andheri city (taluk) proper noun 12 East city (taluk) direction 13 Mumbai district proper noun 14 Maharashtra state proper noun 15 400069 ZIP code 6 digit string Table 1: Example of a tokenized address lenge. First, new areas and with it new addresses constantly emerge. Second, there are very limited conventions for specifying an address (Faruquie et al., 2010). However, while many developing countries do not have a postal taxonomy, there is often no lack of address data to learn a taxonomy from. Column 2 of Table 1 shows an example of an Indian address. Although Indian addresses tend to follow the general principal that more specific information is mentioned earlier, there is no fixed order for different elements of an address. For example, the ZIP code of an address may be mentioned before or after the state information and, although ZIP code information is more specific than city information, it is generally mentioned later in the address. Also, </context>
</contexts>
<marker>Faruquie, Prasad, Subramaniam, Mohania, Venkatachaliah, Kulkarni, Basu, 2010</marker>
<rawString>Tanveer A. Faruquie, K. Hima Prasad, L. Venkata Subramaniam, Mukesh K. Mohania, Girish Venkatachaliah, Shrinivas Kulkarni, and Pramit Basu. 2010. Data cleansing as a transient service. In Proceedings of the 26th International Conference on Data Engineering, pages 1025–1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Eduard Hovy</author>
</authors>
<title>Fine grained classification of named entities.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="4171" citStr="Fleischman and Hovy, 2002" startWordPosition="632" endWordPosition="635">nd organize terms. The quality of extraction is often controlled using statistical measures (Pantel and Pennacchiotti, 2006) and external resources such as wordnet (Girju et al., 2006). However, there are 126 Proceedings of the ACL 2010 Conference Short Papers, pages 126–131, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics domains (such as the one introduced in Section 3.2) where the text does not allow the derivation of linguistic relations. Supervised methods for taxonomy induction provide training instances with global semantic information about concepts (Fleischman and Hovy, 2002) and use bootstrapping to induce new seeds to extract further patterns (Cimiano et al., 2005). Semi-supervised approaches start with known terms belonging to a category, construct context vectors of classified terms, and associate categories to previously unclassified terms depending on the similarity of their context (Tanev and Magnini, 2006). However, providing training data and hand-crafted patterns can be tedious. Moreover in some domains (such as the one presented in Section 3.2) it is not possible to construct a context vector or determine the replacement fit. Unsupervised methods use cl</context>
</contexts>
<marker>Fleischman, Hovy, 2002</marker>
<rawString>Michael Fleischman and Eduard Hovy. 2002. Fine grained classification of named entities. In Proceedings of the 19th International Conference on Computational Linguistics, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan Moldovan</author>
</authors>
<title>Automatic discovery of part-whole relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="1511" citStr="Girju et al., 2006" startWordPosition="211" endWordPosition="214">chniques to automatically deduce a taxonomy in an unsupervised manner are thus indispensable. Automatic deduction of taxonomies consist of two tasks: extracting relevant terms to represent concepts of the taxonomy and discovering relationships between concepts. For unstructured text, the extraction of relevant terms relies on information extraction methods (Etzioni et al., 2005). The relationship extraction task can be classified into two categories. Approaches in the first category use lexical-syntactic formulation to define patterns, either manually (Kozareva et al., 2008) or automatically (Girju et al., 2006), and apply those patterns to mine instances of the patterns. Though producing accurate results, these approaches usually have low coverage for many domains and suffer from the problem of inconsistency between terms when connecting the instances as chains to form a taxonomy. The second category of approaches uses clustering to discover terms and the relationships between them (Roy and Subramaniam, 2006), even if those relationships do not explicitly appear in the text. Though these methods tackle inconsistency by addressing taxonomy deduction globally, the relationships extracted are often dif</context>
<context position="3729" citStr="Girju et al., 2006" startWordPosition="569" endWordPosition="572">method is able to induce a taxonomy without using any kind of lexical-semantic patterns. 2 Related Work One approach for taxonomy deduction is to use explicit expressions (Iwaska et al., 2000) or lexical and semantic patterns such as is a (Snow et al., 2004), similar usage (Kozareva et al., 2008), synonyms and antonyms (Lin et al., 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. The quality of extraction is often controlled using statistical measures (Pantel and Pennacchiotti, 2006) and external resources such as wordnet (Girju et al., 2006). However, there are 126 Proceedings of the ACL 2010 Conference Short Papers, pages 126–131, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics domains (such as the one introduced in Section 3.2) where the text does not allow the derivation of linguistic relations. Supervised methods for taxonomy induction provide training instances with global semantic information about concepts (Fleischman and Hovy, 2002) and use bootstrapping to induce new seeds to extract further patterns (Cimiano et al., 2005). Semi-supervised approaches start with known terms belonging to </context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2006</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2006. Automatic discovery of part-whole relations. Computational Linguistics, 32(1):83–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alfio Gliozzo</author>
</authors>
<title>Instancebased ontology population exploiting named-entity substitution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>265--272</pages>
<contexts>
<context position="5392" citStr="Giuliano and Gliozzo, 2008" startWordPosition="819" endWordPosition="822">use clustering of wordcontext vectors (Lin, 1998), co-occurrence (Yang and Callan, 2008), and conjunction features (Caraballo, 1999) to discover implicit relationships. However, these approaches do not perform well for small corpora. Also, it is difficult to label the obtained clusters which poses challenges for evaluation. To avoid these problems, incremental clustering approaches have been proposed (Yang and Callan, 2009). Recently, lexical entailment has been used where the term is assigned to a category if its occurrence in the corpus can be replaced by the lexicalization of the category (Giuliano and Gliozzo, 2008). In our method, terms are incrementally added to the taxonomy based on their support and context. Association rule mining (Agrawal and Srikant, 1994) discovers interesting relations between terms, based on the frequency with which terms appear together. However, the amount of patterns generated is often huge and constructing a taxonomy from all the patterns can be challenging. In our approach, we employ similar concepts but make taxonomy construction part of the relationship discovery process. 3 Term-frequency-induced Taxonomies For some application domains, a taxonomy is induced by the frequ</context>
</contexts>
<marker>Giuliano, Gliozzo, 2008</marker>
<rawString>Claudio Giuliano and Alfio Gliozzo. 2008. Instancebased ontology population exploiting named-entity substitution. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 265–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucja M Iwaska</author>
<author>Naveen Mata</author>
<author>Kellyn Kruger</author>
</authors>
<title>Fully automatic acquisition of taxonomic knowledge from large corpora of texts.</title>
<date>2000</date>
<booktitle>Natural Language Processing and Knowledge Representation: Language for Knowledge and Knowledge for Language,</booktitle>
<pages>335--345</pages>
<editor>In Lucja M. Iwaska and Stuart C. Shapiro, editors,</editor>
<contexts>
<context position="3302" citStr="Iwaska et al., 2000" startWordPosition="497" endWordPosition="500">omy. We evaluated our method on a large corpus of real-life addresses. For addresses from emerging geographies no standard postal address scheme exists and our objective was to produce a postal taxonomy that is useful in standardizing addresses (Kothari et al., 2010). Specifically, the experiments were designed to investigate the effectiveness of our approach on noisy terms with lots of variations. The results show that our method is able to induce a taxonomy without using any kind of lexical-semantic patterns. 2 Related Work One approach for taxonomy deduction is to use explicit expressions (Iwaska et al., 2000) or lexical and semantic patterns such as is a (Snow et al., 2004), similar usage (Kozareva et al., 2008), synonyms and antonyms (Lin et al., 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. The quality of extraction is often controlled using statistical measures (Pantel and Pennacchiotti, 2006) and external resources such as wordnet (Girju et al., 2006). However, there are 126 Proceedings of the ACL 2010 Conference Short Papers, pages 126–131, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguisti</context>
</contexts>
<marker>Iwaska, Mata, Kruger, 2000</marker>
<rawString>Lucja M. Iwaska, Naveen Mata, and Kellyn Kruger. 2000. Fully automatic acquisition of taxonomic knowledge from large corpora of texts. In Lucja M. Iwaska and Stuart C. Shapiro, editors, Natural Language Processing and Knowledge Representation: Language for Knowledge and Knowledge for Language, pages 335–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Govind Kothari</author>
<author>Tanveer A Faruquie</author>
<author>L V Subramaniam</author>
<author>K H Prasad</author>
<author>Mukesh Mohania</author>
</authors>
<title>Transfer of supervision for improved address standardization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 20th International Conference on Pattern Recognition.</booktitle>
<contexts>
<context position="2949" citStr="Kothari et al., 2010" startWordPosition="440" endWordPosition="443">ncept of a term-frequency-based taxonomy and show its applicability for an example application. We present an unsupervised method to generate such a taxonomy from scratch and outline how domainspecific constraints can easily be integrated into the generation process. An advantage of the new method is that it can also be used to extend an existing taxonomy. We evaluated our method on a large corpus of real-life addresses. For addresses from emerging geographies no standard postal address scheme exists and our objective was to produce a postal taxonomy that is useful in standardizing addresses (Kothari et al., 2010). Specifically, the experiments were designed to investigate the effectiveness of our approach on noisy terms with lots of variations. The results show that our method is able to induce a taxonomy without using any kind of lexical-semantic patterns. 2 Related Work One approach for taxonomy deduction is to use explicit expressions (Iwaska et al., 2000) or lexical and semantic patterns such as is a (Snow et al., 2004), similar usage (Kozareva et al., 2008), synonyms and antonyms (Lin et al., 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and o</context>
</contexts>
<marker>Kothari, Faruquie, Subramaniam, Prasad, Mohania, 2010</marker>
<rawString>Govind Kothari, Tanveer A Faruquie, L V Subramaniam, K H Prasad, and Mukesh Mohania. 2010. Transfer of supervision for improved address standardization. In Proceedings of the 20th International Conference on Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1048--1056</pages>
<contexts>
<context position="1473" citStr="Kozareva et al., 2008" startWordPosition="205" endWordPosition="208">urces is time consuming and expensive. Techniques to automatically deduce a taxonomy in an unsupervised manner are thus indispensable. Automatic deduction of taxonomies consist of two tasks: extracting relevant terms to represent concepts of the taxonomy and discovering relationships between concepts. For unstructured text, the extraction of relevant terms relies on information extraction methods (Etzioni et al., 2005). The relationship extraction task can be classified into two categories. Approaches in the first category use lexical-syntactic formulation to define patterns, either manually (Kozareva et al., 2008) or automatically (Girju et al., 2006), and apply those patterns to mine instances of the patterns. Though producing accurate results, these approaches usually have low coverage for many domains and suffer from the problem of inconsistency between terms when connecting the instances as chains to form a taxonomy. The second category of approaches uses clustering to discover terms and the relationships between them (Roy and Subramaniam, 2006), even if those relationships do not explicitly appear in the text. Though these methods tackle inconsistency by addressing taxonomy deduction globally, the</context>
<context position="3407" citStr="Kozareva et al., 2008" startWordPosition="517" endWordPosition="520">raphies no standard postal address scheme exists and our objective was to produce a postal taxonomy that is useful in standardizing addresses (Kothari et al., 2010). Specifically, the experiments were designed to investigate the effectiveness of our approach on noisy terms with lots of variations. The results show that our method is able to induce a taxonomy without using any kind of lexical-semantic patterns. 2 Related Work One approach for taxonomy deduction is to use explicit expressions (Iwaska et al., 2000) or lexical and semantic patterns such as is a (Snow et al., 2004), similar usage (Kozareva et al., 2008), synonyms and antonyms (Lin et al., 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. The quality of extraction is often controlled using statistical measures (Pantel and Pennacchiotti, 2006) and external resources such as wordnet (Girju et al., 2006). However, there are 126 Proceedings of the ACL 2010 Conference Short Papers, pages 126–131, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics domains (such as the one introduced in Section 3.2) where the text does not allow the derivation of li</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1048–1056.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Shaojun Zhao</author>
<author>Lijuan Qin</author>
<author>Ming Zhou</author>
</authors>
<title>Identifying synonyms among distributionally similar words.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1492--1493</pages>
<contexts>
<context position="3449" citStr="Lin et al., 2003" startWordPosition="525" endWordPosition="528">s and our objective was to produce a postal taxonomy that is useful in standardizing addresses (Kothari et al., 2010). Specifically, the experiments were designed to investigate the effectiveness of our approach on noisy terms with lots of variations. The results show that our method is able to induce a taxonomy without using any kind of lexical-semantic patterns. 2 Related Work One approach for taxonomy deduction is to use explicit expressions (Iwaska et al., 2000) or lexical and semantic patterns such as is a (Snow et al., 2004), similar usage (Kozareva et al., 2008), synonyms and antonyms (Lin et al., 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. The quality of extraction is often controlled using statistical measures (Pantel and Pennacchiotti, 2006) and external resources such as wordnet (Girju et al., 2006). However, there are 126 Proceedings of the ACL 2010 Conference Short Papers, pages 126–131, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics domains (such as the one introduced in Section 3.2) where the text does not allow the derivation of linguistic relations. Supervised methods for</context>
</contexts>
<marker>Lin, Zhao, Qin, Zhou, 2003</marker>
<rawString>Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou. 2003. Identifying synonyms among distributionally similar words. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, pages 1492–1493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="4814" citStr="Lin, 1998" startWordPosition="732" endWordPosition="733">new seeds to extract further patterns (Cimiano et al., 2005). Semi-supervised approaches start with known terms belonging to a category, construct context vectors of classified terms, and associate categories to previously unclassified terms depending on the similarity of their context (Tanev and Magnini, 2006). However, providing training data and hand-crafted patterns can be tedious. Moreover in some domains (such as the one presented in Section 3.2) it is not possible to construct a context vector or determine the replacement fit. Unsupervised methods use clustering of wordcontext vectors (Lin, 1998), co-occurrence (Yang and Callan, 2008), and conjunction features (Caraballo, 1999) to discover implicit relationships. However, these approaches do not perform well for small corpora. Also, it is difficult to label the obtained clusters which poses challenges for evaluation. To avoid these problems, incremental clustering approaches have been proposed (Yang and Callan, 2009). Recently, lexical entailment has been used where the term is assigned to a category if its occurrence in the corpus can be replaced by the lexicalization of the category (Giuliano and Gliozzo, 2008). In our method, terms</context>
<context position="15357" citStr="Lin, 1998" startWordPosition="2578" endWordPosition="2579">the records in a DB2 database. We rely on the DB2 optimizer to efficiently retrieve the next frequent term. 5.1 Dataset The results are based on 40 Million Indian addresses. Each address record was given to us as a single string and was first tokenized into a sequence of terms as shown in Table 1. In a second step, we addressed spelling variations. There is no fixed way of transliterating Indian alphabets to English and most Indian proper nouns have various spellings in English. We used tools to detect synonyms with the same context to generate a list of rules to map terms to a standard form (Lin, 1998). For example, in Table 1 ’Maharashtra’ can also be spelled ’Maharastra’. We also used a list of keywords to classify some terms as markers such as ’Road’ and ’Nagar’ shown in Table 1. Our evaluation consists of two parts. First, we show results for constructing a TFIT from scratch. To evaluate the precision and recall we also retrieved post office addresses from India Post1, cleaned them, and organized them in a tree. Second, we use our approach to enrich the existing hierarchy created from post office addresses with additional area terms. To validate the result, we also retrieved data about </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th International Conference on Computational Linguistics, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="3669" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="558" endWordPosition="562">roach on noisy terms with lots of variations. The results show that our method is able to induce a taxonomy without using any kind of lexical-semantic patterns. 2 Related Work One approach for taxonomy deduction is to use explicit expressions (Iwaska et al., 2000) or lexical and semantic patterns such as is a (Snow et al., 2004), similar usage (Kozareva et al., 2008), synonyms and antonyms (Lin et al., 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. The quality of extraction is often controlled using statistical measures (Pantel and Pennacchiotti, 2006) and external resources such as wordnet (Girju et al., 2006). However, there are 126 Proceedings of the ACL 2010 Conference Short Papers, pages 126–131, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics domains (such as the one introduced in Section 3.2) where the text does not allow the derivation of linguistic relations. Supervised methods for taxonomy induction provide training instances with global semantic information about concepts (Fleischman and Hovy, 2002) and use bootstrapping to induce new seeds to extract further patterns (Cimiano et al., 2005). Sem</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shourya Roy</author>
<author>L Venkata Subramaniam</author>
</authors>
<title>Automatic generation of domain models for call centers from noisy transcriptions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>737--744</pages>
<contexts>
<context position="1917" citStr="Roy and Subramaniam, 2006" startWordPosition="275" endWordPosition="278"> extraction task can be classified into two categories. Approaches in the first category use lexical-syntactic formulation to define patterns, either manually (Kozareva et al., 2008) or automatically (Girju et al., 2006), and apply those patterns to mine instances of the patterns. Though producing accurate results, these approaches usually have low coverage for many domains and suffer from the problem of inconsistency between terms when connecting the instances as chains to form a taxonomy. The second category of approaches uses clustering to discover terms and the relationships between them (Roy and Subramaniam, 2006), even if those relationships do not explicitly appear in the text. Though these methods tackle inconsistency by addressing taxonomy deduction globally, the relationships extracted are often difficult to interpret by humans. We show that for certain domains, the frequency with which terms appear in a corpus on their own and in conjunction with other terms induces a natural taxonomy. We formally define the concept of a term-frequency-based taxonomy and show its applicability for an example application. We present an unsupervised method to generate such a taxonomy from scratch and outline how do</context>
</contexts>
<marker>Roy, Subramaniam, 2006</marker>
<rawString>Shourya Roy and L Venkata Subramaniam. 2006. Automatic generation of domain models for call centers from noisy transcriptions. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 737– 744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1297--1304</pages>
<contexts>
<context position="3368" citStr="Snow et al., 2004" startWordPosition="511" endWordPosition="514">s. For addresses from emerging geographies no standard postal address scheme exists and our objective was to produce a postal taxonomy that is useful in standardizing addresses (Kothari et al., 2010). Specifically, the experiments were designed to investigate the effectiveness of our approach on noisy terms with lots of variations. The results show that our method is able to induce a taxonomy without using any kind of lexical-semantic patterns. 2 Related Work One approach for taxonomy deduction is to use explicit expressions (Iwaska et al., 2000) or lexical and semantic patterns such as is a (Snow et al., 2004), similar usage (Kozareva et al., 2008), synonyms and antonyms (Lin et al., 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. The quality of extraction is often controlled using statistical measures (Pantel and Pennacchiotti, 2006) and external resources such as wordnet (Girju et al., 2006). However, there are 126 Proceedings of the ACL 2010 Conference Short Papers, pages 126–131, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics domains (such as the one introduced in Section 3.2) where the t</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2004</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004. Learning syntactic patterns for automatic hypernym discovery. In Advances in Neural Information Processing Systems, pages 1297–1304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hristo Tanev</author>
<author>Bernardo Magnini</author>
</authors>
<title>Weakly supervised approaches for ontology population.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>3--7</pages>
<contexts>
<context position="4516" citStr="Tanev and Magnini, 2006" startWordPosition="682" endWordPosition="685"> Linguistics domains (such as the one introduced in Section 3.2) where the text does not allow the derivation of linguistic relations. Supervised methods for taxonomy induction provide training instances with global semantic information about concepts (Fleischman and Hovy, 2002) and use bootstrapping to induce new seeds to extract further patterns (Cimiano et al., 2005). Semi-supervised approaches start with known terms belonging to a category, construct context vectors of classified terms, and associate categories to previously unclassified terms depending on the similarity of their context (Tanev and Magnini, 2006). However, providing training data and hand-crafted patterns can be tedious. Moreover in some domains (such as the one presented in Section 3.2) it is not possible to construct a context vector or determine the replacement fit. Unsupervised methods use clustering of wordcontext vectors (Lin, 1998), co-occurrence (Yang and Callan, 2008), and conjunction features (Caraballo, 1999) to discover implicit relationships. However, these approaches do not perform well for small corpora. Also, it is difficult to label the obtained clusters which poses challenges for evaluation. To avoid these problems, </context>
</contexts>
<marker>Tanev, Magnini, 2006</marker>
<rawString>Hristo Tanev and Bernardo Magnini. 2006. Weakly supervised approaches for ontology population. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 3–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Yang</author>
<author>Jamie Callan</author>
</authors>
<title>Learning the distance metric in a personal ontology.</title>
<date>2008</date>
<booktitle>In Proceeding of the 2nd International Workshop on Ontologies and Information Systems for the Semantic Web,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="4853" citStr="Yang and Callan, 2008" startWordPosition="735" endWordPosition="738">r patterns (Cimiano et al., 2005). Semi-supervised approaches start with known terms belonging to a category, construct context vectors of classified terms, and associate categories to previously unclassified terms depending on the similarity of their context (Tanev and Magnini, 2006). However, providing training data and hand-crafted patterns can be tedious. Moreover in some domains (such as the one presented in Section 3.2) it is not possible to construct a context vector or determine the replacement fit. Unsupervised methods use clustering of wordcontext vectors (Lin, 1998), co-occurrence (Yang and Callan, 2008), and conjunction features (Caraballo, 1999) to discover implicit relationships. However, these approaches do not perform well for small corpora. Also, it is difficult to label the obtained clusters which poses challenges for evaluation. To avoid these problems, incremental clustering approaches have been proposed (Yang and Callan, 2009). Recently, lexical entailment has been used where the term is assigned to a category if its occurrence in the corpus can be replaced by the lexicalization of the category (Giuliano and Gliozzo, 2008). In our method, terms are incrementally added to the taxonom</context>
</contexts>
<marker>Yang, Callan, 2008</marker>
<rawString>Hui Yang and Jamie Callan. 2008. Learning the distance metric in a personal ontology. In Proceeding of the 2nd International Workshop on Ontologies and Information Systems for the Semantic Web, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Yang</author>
<author>Jamie Callan</author>
</authors>
<title>A metric-based framework for automatic taxonomy induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>271--279</pages>
<contexts>
<context position="5192" citStr="Yang and Callan, 2009" startWordPosition="785" endWordPosition="788">tterns can be tedious. Moreover in some domains (such as the one presented in Section 3.2) it is not possible to construct a context vector or determine the replacement fit. Unsupervised methods use clustering of wordcontext vectors (Lin, 1998), co-occurrence (Yang and Callan, 2008), and conjunction features (Caraballo, 1999) to discover implicit relationships. However, these approaches do not perform well for small corpora. Also, it is difficult to label the obtained clusters which poses challenges for evaluation. To avoid these problems, incremental clustering approaches have been proposed (Yang and Callan, 2009). Recently, lexical entailment has been used where the term is assigned to a category if its occurrence in the corpus can be replaced by the lexicalization of the category (Giuliano and Gliozzo, 2008). In our method, terms are incrementally added to the taxonomy based on their support and context. Association rule mining (Agrawal and Srikant, 1994) discovers interesting relations between terms, based on the frequency with which terms appear together. However, the amount of patterns generated is often huge and constructing a taxonomy from all the patterns can be challenging. In our approach, we</context>
</contexts>
<marker>Yang, Callan, 2009</marker>
<rawString>Hui Yang and Jamie Callan. 2009. A metric-based framework for automatic taxonomy induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 271–279.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>