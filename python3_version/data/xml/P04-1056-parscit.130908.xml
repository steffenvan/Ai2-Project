<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007882">
<title confidence="0.989886">
Collective Information Extraction
with Relational Markov Networks
</title>
<author confidence="0.955485">
Razvan Bunescu
</author>
<affiliation confidence="0.992430666666667">
Department of Computer Sciences
University of Texas at Austin
1 University Station C0500
</affiliation>
<address confidence="0.86725">
Austin, TX 78712
</address>
<email confidence="0.990715">
razvan@cs.utexas.edu
</email>
<author confidence="0.588698">
Raymond J. Mooney
</author>
<affiliation confidence="0.990404666666667">
Department of Computer Sciences
University of Texas at Austin
1 University Station C0500
</affiliation>
<address confidence="0.872422">
Austin, TX 78712
</address>
<email confidence="0.998528">
mooney@cs.utexas.edu
</email>
<sectionHeader confidence="0.988449" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999879888888889">
Most information extraction (IE) systems treat
separate potential extractions as independent.
However, in many cases, considering influences
between different potential extractions could im-
prove overall accuracy. Statistical methods
based on undirected graphical models, such as
conditional random fields (CRFs), have been
shown to be an effective approach to learning
accurate IE systems. We present a new IE
method that employs Relational Markov Net-
works (a generalization of CRFs), which can
represent arbitrary dependencies between ex-
tractions. This allows for &amp;quot;collective informa-
tion extraction&amp;quot; that exploits the mutual in-
fluence between possible extractions. Experi-
ments on learning to extract protein names from
biomedical text demonstrate the advantages of
this approach.
</bodyText>
<sectionHeader confidence="0.995571" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999809981481481">
Information extraction (IE), locating references
to specific types of items in natural-language
documents, is an important task with many
practical applications. Since IE systems are dif-
ficult and time-consuming to construct, most
recent research has focused on empirical tech-
niques that automatically construct information
extractors by training on supervised corpora
(Cardie, 1997; Califf, 1999). One of the current
best empirical approaches to IE is conditional
random fields (CRF&apos;s) (Lafferty et al., 2001).
CRF&apos;s are a restricted class of undirected graphi-
cal models (Jordan, 1999) designed for sequence
segmentation tasks such as IE, part-of-speech
(POS) tagging (Lafferty et al., 2001), and shal-
low parsing (Sha and Pereira, 2003). In a re-
cent follow-up to previously published experi-
ments comparing a large variety of IE-learning
methods (including HMM, SVM, MaxEnt, and
rule-based methods) on the task of tagging ref-
erences to human proteins in Medline abstracts
(Bunescu et al., 2004), CRF&apos;s were found to sig-
nificantly out-perform competing techniques.
As typically applied, CRF&apos;s, like almost all IE
methods, assume separate extractions are inde-
pendent and treat each potential extraction in
isolation. However, in many cases, considering
influences between extractions can be very use-
ful. For example, in our protein-tagging task,
repeated references to the same protein are com-
mon. If the context surrounding one occurrence
of a phrase is very indicative of it being a pro-
tein, then this should also influence the tagging
of another occurrence of the same phrase in a
different context which is not indicative of pro-
tein references.
Relational Markov Networks (RMN&apos;s)
(Taskar et al., 2002) are a generalization of
CRF&apos;s that allow for collective classification
of a set of related entities by integrating
information from features of individual entities
as well as the relations between them. Results
on classifying connected sets of web pages have
verified the advantage of this approach (Taskar
et al., 2002). In this paper, we present an
approach to collective information extraction
using RMN&apos;s that simultaneously extracts
all of the information from a document by
exploiting the textual content and context of
each relevant substring as well as the document
relationships between them. Experiments
on human protein tagging demonstrate the
advantages of collective extraction on several
annotated corpora of Medline abstracts.
</bodyText>
<sectionHeader confidence="0.9781175" genericHeader="introduction">
2 The RMN Framework for Entity
Recognition
</sectionHeader>
<bodyText confidence="0.999153107142857">
Given a collection of documents D, we associate
with each document dE Da set of candidate
entities d.E, in our case a restricted set of to-
ken sequences from the document. Each entity
e E d.E is characterized by a predefined set of
boolean features e.F. This set of features is the
same for all candidate entities, and it can be
assimilated with the relational database defini-
tion of a table. One particular feature is e.label
which is set to 1 if e is considered a valid extrac-
tion, and 0 otherwise. In this document model,
labels are the only hidden features, and the in-
ference procedure will try to find a most prob-
able assignment of values to labels, given the
current model parameters.
Each document is associated with an undi-
rected graphical model, with nodes correspond-
ing directly to entity features, one node for each
feature of each candidate entity in the docu-
ment. The set of edges is created by matching
clique templates against the entire set of enti-
ties d.E. A clique template is a procedure that
finds all subsets of entities satisfying a given
constraint, after which, for each entity subset, it
connects a selected set of feature nodes so that
they form a clique.
Formally, there is a set of clique templates C,
with each template c E C specified by:
</bodyText>
<listItem confidence="0.9910037">
1. A matching operator M, for selecting sub-
sets of entities.
2. A selected set of features S, = K) for
entities returned by the matching operator.
X, denotes the observed features, while
refers to the hidden labels.
3. A clique potential 0, that gives the com-
patibility of each possible configuration of
values for the features in 5,, s.t. cbc(s) &gt;
0, Vs E Sc.
</listItem>
<bodyText confidence="0.999458466666667">
Given a set, E, of nodes, Mc(E) C 2E con-
sists of subsets of entities whose feature nodes
S, are to be connected in a clique. In previ-
ous applications of RMNs, the selected subsets
of entities for a given template have the same
size; however, our clique templates may match
a variable number of entities. The set 5, may
contain the same feature from different entities.
Usually, for each entity in the matching set, its
label is included in Sc. All these will be illus-
trated with examples in Sections 4 and 5 where
the clique templates used in our model are de-
scribed in detail.
Depending on the number of hidden labels in
we define two categories of clique templates:
</bodyText>
<listItem confidence="0.9813375">
• Local Templates are all templates c E C
for which IY,I = 1. They model the correla-
tions between an entity&apos;s observed features
and its label.
• Global Templates are all templates c E
C for which I Ye I &gt; 1. They capture in-
fluences between multiple entities from the
same document.
</listItem>
<bodyText confidence="0.9997472">
After the graph model for a document d has
been completed with cliques from all templates,
the probability distribution over the random
field of hidden entity labels d.Y given the ob-
served features d.X is computed as:
</bodyText>
<equation confidence="0.92587175">
P(d.Y1d.X)=
Z(d1.X) H H q5c(G.Xe, G.Ye)
cEC GEMc(d.E)
(1)
where Z(d.X) is the normalizing partition func-
tion:
Z(d.X) = fl fl q5c(G.Xe,G.Ye) (2)
Y cEC GEM,(d.E)
</equation>
<bodyText confidence="0.999957">
The above distribution presents the RMN as
a Markov random field (MRF) with the clique
templates as a method for tying potential values
across different cliques in the graphical model.
</bodyText>
<sectionHeader confidence="0.928647" genericHeader="method">
3 Candidate Entities and Entity
Features
</sectionHeader>
<bodyText confidence="0.925302951219512">
Like most entity names, almost all proteins in
our data are base noun phrases or parts of them.
Therefore, such substrings are used to deter-
mine candidate entities. To avoid missing op-
tions, we adopt a very broad definition of base
noun phrase.
Definition 1: A base noun phrase is a max-
imal contiguous sequence of tokens whose POS
tags are from { &amp;quot;JJ&amp;quot;, &amp;quot;VBN&amp;quot;, &amp;quot;VBG&amp;quot;, &amp;quot;POS&amp;quot;,
&amp;quot;NN&amp;quot;, &amp;quot;NNS&amp;quot;, &amp;quot;NNP&amp;quot;, &amp;quot;NNPS&amp;quot;, &amp;quot;CD&amp;quot;, &amp;quot;-&amp;quot;},
and whose last word (the head) is tagged either
as a noun, or a number.
Candidate extractions consist of base NPs,
augmented with all their contiguous subse-
quences headed by a noun or number.
The set of features associated with each can-
didate is based on the feature templates intro-
duced in (Collins, 2002), used there for train-
ing a ranking algorithm on the extractions re-
turned by a maximum-entropy tagger. Many
of these features use the concept of word type,
which allows a different form of token general-
ization than POS tags. The short type of a word
is created by replacing any maximal contiguous
sequences of capital letters with &apos;A&apos;, of lower-
case letters with &apos;a&apos;, and of digits with &apos;0&apos;. For
example, the word TGF-1 would be mapped to
type A-0.
Consequently, each token position i in a can-
didate extraction provides three types of infor-
mation: the word itself wi, its POS tag t, and
its short type si. The full set of features types
is listed in Table 1, where we consider a generic
elabel
φHD=enzyme
elabel
φPF=A0_a
...
φSF=A0_a
...
φSF=a
</bodyText>
<equation confidence="0.88856275">
φPF=A0
f1=v; ef2=v; efh=v;
1 2 h
e
</equation>
<bodyText confidence="0.999974857142857">
Note that the factor graph above has an
equivalent RMN graph consisting of a one-node
clique only, on which it is hard to visualize the
various potentials involved. There are cases
where different factor graphs may yield the
same underlying RMN graph, which makes the
factor graph representation preferable.
</bodyText>
<sectionHeader confidence="0.995214" genericHeader="method">
5 Global Clique Templates
</sectionHeader>
<bodyText confidence="0.997945739130435">
Global clique templates enable us to model hy-
pothesized influences between entities from the
same document. They connect the label nodes
of two or more entities, which, in the factor
graph, translates into potential nodes connected
to at least two label nodes. In our experiments
we use three global templates:
Overlap Template (OT): No two entity
names overlap in the text i.e if the span of one
entity is [Si, el] and the span of another entity
is [82, e2], and Si &lt; 82, then el &lt; 82.
Repeat Template (RT): If multiple enti-
ties in the same document are repetitions of the
same name, their labels tend to have the same
value (i.e. most of them are protein names, or
most of them are not protein names). Later
we discuss situations in which repetitions of the
same protein name are not tagged as proteins,
and design an approach to handle this.
Acronym Template (AT): It is common
convention that a protein is first introduced
by its long name, immediately followed by its
short-form (acronym) in parentheses.
</bodyText>
<subsectionHeader confidence="0.875856">
5.1 The Overlap Template
</subsectionHeader>
<bodyText confidence="0.999631225806452">
The definition of a candidate extraction from
Section 3 leads to many overlapping entities.
For example, &apos;glutathione S - transferase&apos; is a base
NP, and it generates five candidate extractions:
&apos;glutathione&apos;, &apos;glutathione S&apos;, &apos;glutathione S - trans-
ferase&apos;, &apos;S - transferase&apos;, and &apos;transferase&apos;. If &apos;gin-
tathione S - transferase&apos; has label-value 1, be-
cause the other four entities overlap with it,
they should all have label-value 0.
This type of constraint is enforced by the
overlap template whose M operator matches
any two overlapping candidate entities, and
which connects their label nodes (specified in S)
through a potential node with a potential func-
tion cb that allows at most one of them to have
label-value 1, as illustrated in Table 2. Contin-
uing with the previous example, because &apos;gin-
tathione S&apos; and &apos;S - transferase&apos; are two overlap-
ping entities, the factor graph model will con-
tain an overlap potential node connected to the
label nodes of these two entities.
An alternative solution for the overlap tem-
plate is to create a potential node for each token
position that is covered by at least two candi-
date entities in the document, and connect it
to their label nodes. The difference in this case
is that the potential node will be connected to
a variable number of entity label nodes. How-
ever this second approach has the advantage of
creating fewer potential nodes in the document
factor graph, which results in faster inference.
</bodyText>
<equation confidence="0.901533333333333">
OOT el .label = 0 ei.label = 1
e2.1abel = 0 1 1
e2.1abel = 1 1 0
</equation>
<tableCaption confidence="0.952046">
Table 2: Overlap Potential.
</tableCaption>
<subsectionHeader confidence="0.736133">
5.2 The Repeat Template
</subsectionHeader>
<bodyText confidence="0.993312916666667">
We could specify the potential for the repeat
template in a similar 2-by-2 table, this time
leaving the table entries to be learned, given
that it is not a hard constraint. However we
can do better by noting that the vast majority
of cases where a repeated protein name is not
also tagged as a protein happens when it is part
of a larger phrase that is tagged. For exam-
ple, &apos;HDAC1 enzyme&apos; is a protein name, there-
fore &apos;HDAC1&apos; is not tagged in this phrase, even
though it may have been tagged previously in
the abstract where it was not followed by &apos;en-
zyme&apos;. We need a potential that allows two en-
tities with the same text to have different labels
if the entity with label-value 0 is inside another
entity with label-value 1. But a candidate en-
tity may be inside more than one &amp;quot;including&amp;quot;
entity, and the number of including entities may
vary from one candidate extraction to another.
Using the example from Section 5.1, the candi-
date entity &apos;glutathione&apos; is included in two other
entities: &apos;glutathione S&apos; and &apos;glutathione S - trans-
ferase&apos;.
In order to instantiate potentials over vari-
able number of label nodes, we introduce a log-
ical OR clique template that matches a vari-
able number of entities. When this template
matches a subset of entities el, e2, ... , en, it will
create an auxiliary OR entity e,, with a single
feature e„ .1 abel . The potential function is set
so that it assigns a non-zero potential only when
e, .1 abel = el .1 abel V e2.1abel V ...V en .1 abel . The
cliques are only created as needed, e.g. when the
auxiliary OR variable is required by repeat and
acronym clique templates.
Figure 3 shows the factor graph for a sam-
</bodyText>
<figure confidence="0.9875366875">
φAT
u v
or
φor
φ
RT
u u v
or
vor
...
un
u1 u2
φ φ
or or
... ...
u1 u2 un v1 v2 vm
</figure>
<bodyText confidence="0.999763">
verges, it gives a good approximation to the cor-
rect marginals. The algorithm works by altering
the belief at each label node by repeatedly pass-
ing messages between the node and all potential
nodes connected to it (Kschischang et al., 2001).
As many of the label nodes are indirectly con-
nected through potential nodes instantiated by
global templates, their belief values will propa-
gate in the graph and mutually influence each
other, leading in the end to a collective labeling
decision.
The time complexity of computing messages
from a potential node to a label node is expo-
nential in the number of label nodes attached to
the potential. Since this &amp;quot;fan-in&amp;quot; can be large
for OR potential nodes, this step required opti-
mization. Fortunately, due to the special form
of the OR potential, and the normalization be-
fore each message-passing step, we were able to
develop a linear-time algorithm for this special
case. Details are omitted due to limited space.
</bodyText>
<sectionHeader confidence="0.97745" genericHeader="method">
7 Learning Potentials in Factor
Graphs
</sectionHeader>
<bodyText confidence="0.997609">
Following a maximum likelihood estimation, we
shall use the log-linear representation of poten-
tials:
</bodyText>
<equation confidence="0.957566">
95c (G.Xe,G.Y,) = exp{w,f,(G.X,,G.K)} (4)
</equation>
<bodyText confidence="0.999951090909091">
where A is a vector of binary features, one for
each configuration of values for X, and K.
Let w be the concatenated vector of all po-
tential parameters wc. One approach to finding
the maximum-likelihood solution for w is to use
a gradient-based method, which requires com-
puting the gradient of the log-likelihood with
respect to potential parameters wc. It can be
shown that this gradient is equal with the dif-
ference between the empirical counts of fc and
their expectation under the current set of pa-
rameters w. This expectation is expensive to
compute, since it requires summing over all pos-
sible configurations of candidate entity labels
from a given document. To circumvent this
complexity, we use Collins&apos; voted perceptron ap-
proach (Collins, 2002), which approximates the
full expectation of fc with the fc counts for the
most likely labeling under the current parame-
ters, w. In all our experiments, the perceptron
was run for 50 epochs, with a learning rate set
at 0.01.
</bodyText>
<sectionHeader confidence="0.979568" genericHeader="evaluation">
8 Experimental Results
</sectionHeader>
<bodyText confidence="0.999862864864865">
We have tested the RMN approach on two
datasets that have been hand-tagged for hu-
man protein names. The first dataset is Yapexl
which consists of 200 Medline abstracts. Of
these, 147 have been randomly selected by pos-
ing a query containing the (Mesh) terms protein
binding, interaction, and molecular to Medline,
while the rest of 53 have been extracted ran-
domly from the GENIA corpus (Collier et al.,
1999). It contains a total of 3713 protein refer-
ences. The second dataset is Aimed2 which has
been previously used for training the protein in-
teraction extraction systems in (Bunescu et al.,
2004). It consists of 225 Medline abstracts, of
which 200 are known to describe interactions
between human proteins, while the other 25 do
not refer to any interaction. There are 4084 pro-
tein references in this dataset. We compared
the performance of three systems: LT-RMN is
the RMN approach using local templates and
the overlap template, GLT-RMN is the full
RMN approach, using both local and global
templates, and CRF, which uses a CRF for la-
beling token sequences. We used the CRF im-
plementation from (McCallum, 2002) with the
set of tags and features used by the Maximum-
Entropy tagger described in (Bunescu et al.,
2004). All Medline abstracts were tokenized
and then POS tagged using Brill&apos;s tagger (Brill,
1995). Each extracted protein name in the test
data was compared to the human-tagged data,
with the positions taken into account. Two ex-
tractions are considered a match if they consist
of the same character sequence in the same po-
sition in the text. Results are shown in Tables 3
and 4 which give average precision, recall, and
F-measure using 10-fold cross validation.
</bodyText>
<table confidence="0.9832265">
Method Precision Recall F-measure
LT-RMN 70.79 53.81 61.14
GLT-RMN 69.71 65.76 67.68
CRF 72.45 58.64 64.81
</table>
<tableCaption confidence="0.991095">
Table 3: Extraction Performance on Yapex.
</tableCaption>
<table confidence="0.98245825">
Method Precision Recall F-measure
LT-RMN 81.33 72.79 76.82
GLT-RMN 82.79 80.04 81.39
CRF 85.37 75.90 80.36
</table>
<tableCaption confidence="0.999907">
Table 4: Extraction Performance on Aimed.
</tableCaption>
<bodyText confidence="0.997102">
These tables show that, in terms of F-
measure, the use of global templates for mod-
</bodyText>
<footnote confidence="0.9740955">
&apos;URL: www.sics.se/humle/projects/prothalt/
2 URL: ftp.cs.utexas.edu/mooney/bio-data/
</footnote>
<figure confidence="0.985641909090909">
100
90
GLT-RMN
LT-RMN
Precision (%)
80
70
60
50
0 20 40 60 80 100
Recall (%)
100
90
Precision (%)
80
70
60
GLT-RMN
LT-RMN
50
0 20 40 60 80 100
Recall (%)
</figure>
<bodyText confidence="0.9973456">
to improve a Maximum-Entropy tagger; how-
ever, these features do not fully capture the mu-
tual influence between the labels of acronyms
and their long forms, or between entity repeti-
tions. In particular, they only allow earlier ex-
tractions in a document to influence later ones
and not vice-versa. The RMN approach handles
these and potentially other mutual influences
between entities in a more complete, probabilis-
tically sound manner.
</bodyText>
<sectionHeader confidence="0.991717" genericHeader="conclusions">
10 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999962">
We have presented an approach to collec-
tive information extraction that uses Relational
Markov Networks to reason about the mutual
influences between multiple extractions. A new
type of clique template — the logical OR tem-
plate — was introduced, allowing a variable num-
ber of relevant entities to be used by other clique
templates. Soft correlations between repetitions
and acronyms and their long form in the same
document have been captured by global clique
templates, allowing for local extraction deci-
sions to propagate and mutually influence each
other.
Regarding future work, a richer set of features
for the local templates would likely improve per-
formance. Currently, LT-RMN&apos;s accuracy is
still significantly less than CRF&apos;s, which lim-
its the performance of the full system. Another
limitation is the approximate inference used by
both RMN methods. The number of factor
graphs for which the sum-product algorithm did
not converge was non-negligible, and our ap-
proach stopped after a fix number of iterations.
Besides exploring improvements to loopy belief
propagation that increase computational cost
(Yedidia et al., 2000), we intend to examine al-
ternative approximate-inference methods.
</bodyText>
<sectionHeader confidence="0.988548" genericHeader="acknowledgments">
11 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9835905">
This work was partially supported by grants
IIS-0117308 and IIS-0325116 from the NSF.
</bodyText>
<sectionHeader confidence="0.997307" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988352493333334">
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543-565.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M.
Marcotte, Raymond J. Mooney, Arun Kumar Ra-
mani, and Yuk Wah Wong. 2004. Comparative exper-
iments on learning information extractors for proteins
and their interactions. Special Issue in the Journal
Artificial Intelligence in Medicine on Summarization
and Information Extraction from Medical Documents.
To appear.
Mary Elaine Califf, editor. 1999. Papers from the AAAI-
1999 Workshop on Machine Learning for Information
Extraction, Orlando, FL. AAAI Press.
Claire Cardie. 1997. Empirical methods in information
extraction. Al Magazine, 18(4):65-79.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In Proceedings of the Seventh Conference on Natu-
ral Language Learning (CoNLL-2003), pages 160-163,
Edmonton, Canada.
N. Collier, H. Park, N. Ogata, Y. Tateisi, C. Nobata,
T.Ohta, T. Sekimizu, H. Imai, K. Ibushi, and J. Tsu-
jii. 1999. The GENIA project: Corpus-based knowl-
edge acquisition and information extraction from
genome research papers. In Ninth Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-99), pages 271-272, Bergen.
Michael Collins. 2002. Ranking algorithms for named-
entity extraction: Boosting and the voted perceptron.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-02), pages
489-496, Philadelphia, PA.
Michael I. Jordan, editor. 1999. Learning in Graphical
Models. MIT Press, Cambridge, MA.
F. R. Kschischang, B. Frey, and II.-A. Loeliger. 2001.
Factor graphs and the sum-product algorithm. IEEE
Transactions on Information Theory, 47(2):498-519.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of 18th International Conference
on Machine Learning (ICML-2001), pages 282-289,
Williams College, MA.
Andrew Kachites McCallum. 2002. Mallet:
A machine learning for language toolkit.
http:/ /mallet .cs.umass.edu.
Judea Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann, San Mateo,CA.
Ariel S. Schwartz and Marti A. Hearst. 2003. A sim-
ple algorithm for identifying abbreviation definitions
in biomedical text. In Proceedings of the 8th Pacific
Symposium on Biocomputing, pages 451-462, Lihue,
HI, January.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of Hu-
man Language Technology and the Meeting of the
North American Association for Computational Lin-
guistics, pages 134-141, Edmonton, Canada.
Benjamin Taskar, Pieter Abbeel, and D. Koller. 2002.
Discriminative probabilistic models for relational
data. In Proceedings of 18th Conference on Uncer-
tainty in Artificial Intelligence (UAI-02), pages 485-
492, Edmonton, Canada.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
Proceedings of CoNLL-2003, pages 142-147. Edmon-
ton, Canada.
Jonathan S. Yedidia, William T. Freeman, and Yair
Weiss. 2000. Generalized belief propagation. In Ad-
vances in Neural Information Processing Systems 12,
pages 689-695, Denver, CO.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.270111">
<title confidence="0.9996355">Collective Information Extraction with Relational Markov Networks</title>
<author confidence="0.999643">Razvan Bunescu</author>
<affiliation confidence="0.90517">Department of Computer Sciences University of Texas at Austin 1 University Station C0500</affiliation>
<address confidence="0.70456">Austin, TX 78712</address>
<email confidence="0.999564">razvan@cs.utexas.edu</email>
<author confidence="0.999997">Raymond J Mooney</author>
<affiliation confidence="0.86955275">Department of Computer Sciences University of Texas at Austin 1 University Station C0500 Austin, TX 78712</affiliation>
<email confidence="0.999538">mooney@cs.utexas.edu</email>
<abstract confidence="0.998841894736842">Most information extraction (IE) systems treat separate potential extractions as independent. However, in many cases, considering influences potential extractions could improve overall accuracy. Statistical methods on models, such as random fields have been shown to be an effective approach to learning accurate IE systems. We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions. This allows for &amp;quot;collective information extraction&amp;quot; that exploits the mutual influence between possible extractions. Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<contexts>
<context position="16440" citStr="Brill, 1995" startWordPosition="2777" endWordPosition="2778">proteins, while the other 25 do not refer to any interaction. There are 4084 protein references in this dataset. We compared the performance of three systems: LT-RMN is the RMN approach using local templates and the overlap template, GLT-RMN is the full RMN approach, using both local and global templates, and CRF, which uses a CRF for labeling token sequences. We used the CRF implementation from (McCallum, 2002) with the set of tags and features used by the MaximumEntropy tagger described in (Bunescu et al., 2004). All Medline abstracts were tokenized and then POS tagged using Brill&apos;s tagger (Brill, 1995). Each extracted protein name in the test data was compared to the human-tagged data, with the positions taken into account. Two extractions are considered a match if they consist of the same character sequence in the same position in the text. Results are shown in Tables 3 and 4 which give average precision, recall, and F-measure using 10-fold cross validation. Method Precision Recall F-measure LT-RMN 70.79 53.81 61.14 GLT-RMN 69.71 65.76 67.68 CRF 72.45 58.64 64.81 Table 3: Extraction Performance on Yapex. Method Precision Recall F-measure LT-RMN 81.33 72.79 76.82 GLT-RMN 82.79 80.04 81.39 C</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Ruifang Ge</author>
<author>Rohit J Kate</author>
<author>Edward M Marcotte</author>
<author>Raymond J Mooney</author>
<author>Arun Kumar Ramani</author>
<author>Yuk Wah Wong</author>
</authors>
<title>Comparative experiments on learning information extractors for proteins and their interactions.</title>
<date>2004</date>
<booktitle>Special Issue in the Journal Artificial Intelligence in Medicine on Summarization and Information Extraction from Medical Documents.</booktitle>
<note>To appear.</note>
<contexts>
<context position="2152" citStr="Bunescu et al., 2004" startWordPosition="302" endWordPosition="305">die, 1997; Califf, 1999). One of the current best empirical approaches to IE is conditional random fields (CRF&apos;s) (Lafferty et al., 2001). CRF&apos;s are a restricted class of undirected graphical models (Jordan, 1999) designed for sequence segmentation tasks such as IE, part-of-speech (POS) tagging (Lafferty et al., 2001), and shallow parsing (Sha and Pereira, 2003). In a recent follow-up to previously published experiments comparing a large variety of IE-learning methods (including HMM, SVM, MaxEnt, and rule-based methods) on the task of tagging references to human proteins in Medline abstracts (Bunescu et al., 2004), CRF&apos;s were found to significantly out-perform competing techniques. As typically applied, CRF&apos;s, like almost all IE methods, assume separate extractions are independent and treat each potential extraction in isolation. However, in many cases, considering influences between extractions can be very useful. For example, in our protein-tagging task, repeated references to the same protein are common. If the context surrounding one occurrence of a phrase is very indicative of it being a protein, then this should also influence the tagging of another occurrence of the same phrase in a different co</context>
<context position="15726" citStr="Bunescu et al., 2004" startWordPosition="2655" endWordPosition="2658">8 Experimental Results We have tested the RMN approach on two datasets that have been hand-tagged for human protein names. The first dataset is Yapexl which consists of 200 Medline abstracts. Of these, 147 have been randomly selected by posing a query containing the (Mesh) terms protein binding, interaction, and molecular to Medline, while the rest of 53 have been extracted randomly from the GENIA corpus (Collier et al., 1999). It contains a total of 3713 protein references. The second dataset is Aimed2 which has been previously used for training the protein interaction extraction systems in (Bunescu et al., 2004). It consists of 225 Medline abstracts, of which 200 are known to describe interactions between human proteins, while the other 25 do not refer to any interaction. There are 4084 protein references in this dataset. We compared the performance of three systems: LT-RMN is the RMN approach using local templates and the overlap template, GLT-RMN is the full RMN approach, using both local and global templates, and CRF, which uses a CRF for labeling token sequences. We used the CRF implementation from (McCallum, 2002) with the set of tags and features used by the MaximumEntropy tagger described in (</context>
</contexts>
<marker>Bunescu, Ge, Kate, Marcotte, Mooney, Ramani, Wong, 2004</marker>
<rawString>Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M. Marcotte, Raymond J. Mooney, Arun Kumar Ramani, and Yuk Wah Wong. 2004. Comparative experiments on learning information extractors for proteins and their interactions. Special Issue in the Journal Artificial Intelligence in Medicine on Summarization and Information Extraction from Medical Documents. To appear.</rawString>
</citation>
<citation valid="true">
<date>1999</date>
<booktitle>Papers from the AAAI1999 Workshop on Machine Learning for Information Extraction,</booktitle>
<editor>Mary Elaine Califf, editor.</editor>
<publisher>AAAI Press.</publisher>
<location>Orlando, FL.</location>
<marker>1999</marker>
<rawString>Mary Elaine Califf, editor. 1999. Papers from the AAAI1999 Workshop on Machine Learning for Information Extraction, Orlando, FL. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
</authors>
<title>Empirical methods in information extraction.</title>
<date>1997</date>
<journal>Al Magazine,</journal>
<pages>18--4</pages>
<contexts>
<context position="1540" citStr="Cardie, 1997" startWordPosition="208" endWordPosition="209">collective information extraction&amp;quot; that exploits the mutual influence between possible extractions. Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach. 1 Introduction Information extraction (IE), locating references to specific types of items in natural-language documents, is an important task with many practical applications. Since IE systems are difficult and time-consuming to construct, most recent research has focused on empirical techniques that automatically construct information extractors by training on supervised corpora (Cardie, 1997; Califf, 1999). One of the current best empirical approaches to IE is conditional random fields (CRF&apos;s) (Lafferty et al., 2001). CRF&apos;s are a restricted class of undirected graphical models (Jordan, 1999) designed for sequence segmentation tasks such as IE, part-of-speech (POS) tagging (Lafferty et al., 2001), and shallow parsing (Sha and Pereira, 2003). In a recent follow-up to previously published experiments comparing a large variety of IE-learning methods (including HMM, SVM, MaxEnt, and rule-based methods) on the task of tagging references to human proteins in Medline abstracts (Bunescu e</context>
</contexts>
<marker>Cardie, 1997</marker>
<rawString>Claire Cardie. 1997. Empirical methods in information extraction. Al Magazine, 18(4):65-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Named entity recognition with a maximum entropy approach.</title>
<date>2003</date>
<booktitle>In Proceedings of the Seventh Conference on Natural Language Learning (CoNLL-2003),</booktitle>
<pages>160--163</pages>
<location>Edmonton, Canada.</location>
<marker>Chieu, Ng, 2003</marker>
<rawString>Hai Leong Chieu and Hwee Tou Ng. 2003. Named entity recognition with a maximum entropy approach. In Proceedings of the Seventh Conference on Natural Language Learning (CoNLL-2003), pages 160-163, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Collier</author>
<author>H Park</author>
<author>N Ogata</author>
<author>Y Tateisi</author>
<author>C Nobata</author>
<author>T Sekimizu T Ohta</author>
<author>H Imai</author>
<author>K Ibushi</author>
<author>J Tsujii</author>
</authors>
<title>The GENIA project: Corpus-based knowledge acquisition and information extraction from genome research papers.</title>
<date>1999</date>
<booktitle>In Ninth Conference of the European Chapter of the Association for Computational Linguistics (EACL-99),</booktitle>
<pages>271--272</pages>
<location>Bergen.</location>
<contexts>
<context position="15535" citStr="Collier et al., 1999" startWordPosition="2623" endWordPosition="2626">ctation of fc with the fc counts for the most likely labeling under the current parameters, w. In all our experiments, the perceptron was run for 50 epochs, with a learning rate set at 0.01. 8 Experimental Results We have tested the RMN approach on two datasets that have been hand-tagged for human protein names. The first dataset is Yapexl which consists of 200 Medline abstracts. Of these, 147 have been randomly selected by posing a query containing the (Mesh) terms protein binding, interaction, and molecular to Medline, while the rest of 53 have been extracted randomly from the GENIA corpus (Collier et al., 1999). It contains a total of 3713 protein references. The second dataset is Aimed2 which has been previously used for training the protein interaction extraction systems in (Bunescu et al., 2004). It consists of 225 Medline abstracts, of which 200 are known to describe interactions between human proteins, while the other 25 do not refer to any interaction. There are 4084 protein references in this dataset. We compared the performance of three systems: LT-RMN is the RMN approach using local templates and the overlap template, GLT-RMN is the full RMN approach, using both local and global templates, </context>
</contexts>
<marker>Collier, Park, Ogata, Tateisi, Nobata, Ohta, Imai, Ibushi, Tsujii, 1999</marker>
<rawString>N. Collier, H. Park, N. Ogata, Y. Tateisi, C. Nobata, T.Ohta, T. Sekimizu, H. Imai, K. Ibushi, and J. Tsujii. 1999. The GENIA project: Corpus-based knowledge acquisition and information extraction from genome research papers. In Ninth Conference of the European Chapter of the Association for Computational Linguistics (EACL-99), pages 271-272, Bergen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking algorithms for namedentity extraction: Boosting and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-02),</booktitle>
<pages>489--496</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="7550" citStr="Collins, 2002" startWordPosition="1227" endWordPosition="1228"> such substrings are used to determine candidate entities. To avoid missing options, we adopt a very broad definition of base noun phrase. Definition 1: A base noun phrase is a maximal contiguous sequence of tokens whose POS tags are from { &amp;quot;JJ&amp;quot;, &amp;quot;VBN&amp;quot;, &amp;quot;VBG&amp;quot;, &amp;quot;POS&amp;quot;, &amp;quot;NN&amp;quot;, &amp;quot;NNS&amp;quot;, &amp;quot;NNP&amp;quot;, &amp;quot;NNPS&amp;quot;, &amp;quot;CD&amp;quot;, &amp;quot;-&amp;quot;}, and whose last word (the head) is tagged either as a noun, or a number. Candidate extractions consist of base NPs, augmented with all their contiguous subsequences headed by a noun or number. The set of features associated with each candidate is based on the feature templates introduced in (Collins, 2002), used there for training a ranking algorithm on the extractions returned by a maximum-entropy tagger. Many of these features use the concept of word type, which allows a different form of token generalization than POS tags. The short type of a word is created by replacing any maximal contiguous sequences of capital letters with &apos;A&apos;, of lowercase letters with &apos;a&apos;, and of digits with &apos;0&apos;. For example, the word TGF-1 would be mapped to type A-0. Consequently, each token position i in a candidate extraction provides three types of information: the word itself wi, its POS tag t, and its short type</context>
<context position="14880" citStr="Collins, 2002" startWordPosition="2512" endWordPosition="2513">l parameters wc. One approach to finding the maximum-likelihood solution for w is to use a gradient-based method, which requires computing the gradient of the log-likelihood with respect to potential parameters wc. It can be shown that this gradient is equal with the difference between the empirical counts of fc and their expectation under the current set of parameters w. This expectation is expensive to compute, since it requires summing over all possible configurations of candidate entity labels from a given document. To circumvent this complexity, we use Collins&apos; voted perceptron approach (Collins, 2002), which approximates the full expectation of fc with the fc counts for the most likely labeling under the current parameters, w. In all our experiments, the perceptron was run for 50 epochs, with a learning rate set at 0.01. 8 Experimental Results We have tested the RMN approach on two datasets that have been hand-tagged for human protein names. The first dataset is Yapexl which consists of 200 Medline abstracts. Of these, 147 have been randomly selected by posing a query containing the (Mesh) terms protein binding, interaction, and molecular to Medline, while the rest of 53 have been extracte</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Ranking algorithms for namedentity extraction: Boosting and the voted perceptron. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-02), pages 489-496, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael I Jordan</author>
<author>editor</author>
</authors>
<title>Learning in Graphical Models.</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Jordan, editor, 1999</marker>
<rawString>Michael I. Jordan, editor. 1999. Learning in Graphical Models. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F R Kschischang</author>
<author>B Frey</author>
<author>II-A Loeliger</author>
</authors>
<title>Factor graphs and the sum-product algorithm.</title>
<date>2001</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>47--2</pages>
<contexts>
<context position="13228" citStr="Kschischang et al., 2001" startWordPosition="2240" endWordPosition="2243">he potential function is set so that it assigns a non-zero potential only when e, .1 abel = el .1 abel V e2.1abel V ...V en .1 abel . The cliques are only created as needed, e.g. when the auxiliary OR variable is required by repeat and acronym clique templates. Figure 3 shows the factor graph for a samφAT u v or φor φ RT u u v or vor ... un u1 u2 φ φ or or ... ... u1 u2 un v1 v2 vm verges, it gives a good approximation to the correct marginals. The algorithm works by altering the belief at each label node by repeatedly passing messages between the node and all potential nodes connected to it (Kschischang et al., 2001). As many of the label nodes are indirectly connected through potential nodes instantiated by global templates, their belief values will propagate in the graph and mutually influence each other, leading in the end to a collective labeling decision. The time complexity of computing messages from a potential node to a label node is exponential in the number of label nodes attached to the potential. Since this &amp;quot;fan-in&amp;quot; can be large for OR potential nodes, this step required optimization. Fortunately, due to the special form of the OR potential, and the normalization before each message-passing st</context>
</contexts>
<marker>Kschischang, Frey, Loeliger, 2001</marker>
<rawString>F. R. Kschischang, B. Frey, and II.-A. Loeliger. 2001. Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory, 47(2):498-519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of 18th International Conference on Machine Learning (ICML-2001),</booktitle>
<pages>282--289</pages>
<location>Williams College, MA.</location>
<contexts>
<context position="1668" citStr="Lafferty et al., 2001" startWordPosition="226" endWordPosition="229">rning to extract protein names from biomedical text demonstrate the advantages of this approach. 1 Introduction Information extraction (IE), locating references to specific types of items in natural-language documents, is an important task with many practical applications. Since IE systems are difficult and time-consuming to construct, most recent research has focused on empirical techniques that automatically construct information extractors by training on supervised corpora (Cardie, 1997; Califf, 1999). One of the current best empirical approaches to IE is conditional random fields (CRF&apos;s) (Lafferty et al., 2001). CRF&apos;s are a restricted class of undirected graphical models (Jordan, 1999) designed for sequence segmentation tasks such as IE, part-of-speech (POS) tagging (Lafferty et al., 2001), and shallow parsing (Sha and Pereira, 2003). In a recent follow-up to previously published experiments comparing a large variety of IE-learning methods (including HMM, SVM, MaxEnt, and rule-based methods) on the task of tagging references to human proteins in Medline abstracts (Bunescu et al., 2004), CRF&apos;s were found to significantly out-perform competing techniques. As typically applied, CRF&apos;s, like almost all I</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of 18th International Conference on Machine Learning (ICML-2001), pages 282-289, Williams College, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit. http:/ /mallet .cs.umass.edu.</title>
<date>2002</date>
<contexts>
<context position="16243" citStr="McCallum, 2002" startWordPosition="2744" endWordPosition="2745"> previously used for training the protein interaction extraction systems in (Bunescu et al., 2004). It consists of 225 Medline abstracts, of which 200 are known to describe interactions between human proteins, while the other 25 do not refer to any interaction. There are 4084 protein references in this dataset. We compared the performance of three systems: LT-RMN is the RMN approach using local templates and the overlap template, GLT-RMN is the full RMN approach, using both local and global templates, and CRF, which uses a CRF for labeling token sequences. We used the CRF implementation from (McCallum, 2002) with the set of tags and features used by the MaximumEntropy tagger described in (Bunescu et al., 2004). All Medline abstracts were tokenized and then POS tagged using Brill&apos;s tagger (Brill, 1995). Each extracted protein name in the test data was compared to the human-tagged data, with the positions taken into account. Two extractions are considered a match if they consist of the same character sequence in the same position in the text. Results are shown in Tables 3 and 4 which give average precision, recall, and F-measure using 10-fold cross validation. Method Precision Recall F-measure LT-R</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http:/ /mallet .cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo,CA.</location>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo,CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel S Schwartz</author>
<author>Marti A Hearst</author>
</authors>
<title>A simple algorithm for identifying abbreviation definitions in biomedical text.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th Pacific Symposium on Biocomputing,</booktitle>
<pages>451--462</pages>
<location>Lihue, HI,</location>
<marker>Schwartz, Hearst, 2003</marker>
<rawString>Ariel S. Schwartz and Marti A. Hearst. 2003. A simple algorithm for identifying abbreviation definitions in biomedical text. In Proceedings of the 8th Pacific Symposium on Biocomputing, pages 451-462, Lihue, HI, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology and the Meeting of the North American Association for Computational Linguistics,</booktitle>
<pages>134--141</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1895" citStr="Sha and Pereira, 2003" startWordPosition="261" endWordPosition="264">portant task with many practical applications. Since IE systems are difficult and time-consuming to construct, most recent research has focused on empirical techniques that automatically construct information extractors by training on supervised corpora (Cardie, 1997; Califf, 1999). One of the current best empirical approaches to IE is conditional random fields (CRF&apos;s) (Lafferty et al., 2001). CRF&apos;s are a restricted class of undirected graphical models (Jordan, 1999) designed for sequence segmentation tasks such as IE, part-of-speech (POS) tagging (Lafferty et al., 2001), and shallow parsing (Sha and Pereira, 2003). In a recent follow-up to previously published experiments comparing a large variety of IE-learning methods (including HMM, SVM, MaxEnt, and rule-based methods) on the task of tagging references to human proteins in Medline abstracts (Bunescu et al., 2004), CRF&apos;s were found to significantly out-perform competing techniques. As typically applied, CRF&apos;s, like almost all IE methods, assume separate extractions are independent and treat each potential extraction in isolation. However, in many cases, considering influences between extractions can be very useful. For example, in our protein-tagging</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of Human Language Technology and the Meeting of the North American Association for Computational Linguistics, pages 134-141, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Taskar</author>
<author>Pieter Abbeel</author>
<author>D Koller</author>
</authors>
<title>Discriminative probabilistic models for relational data.</title>
<date>2002</date>
<booktitle>In Proceedings of 18th Conference on Uncertainty in Artificial Intelligence (UAI-02),</booktitle>
<pages>485--492</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2861" citStr="Taskar et al., 2002" startWordPosition="413" endWordPosition="416">d, CRF&apos;s, like almost all IE methods, assume separate extractions are independent and treat each potential extraction in isolation. However, in many cases, considering influences between extractions can be very useful. For example, in our protein-tagging task, repeated references to the same protein are common. If the context surrounding one occurrence of a phrase is very indicative of it being a protein, then this should also influence the tagging of another occurrence of the same phrase in a different context which is not indicative of protein references. Relational Markov Networks (RMN&apos;s) (Taskar et al., 2002) are a generalization of CRF&apos;s that allow for collective classification of a set of related entities by integrating information from features of individual entities as well as the relations between them. Results on classifying connected sets of web pages have verified the advantage of this approach (Taskar et al., 2002). In this paper, we present an approach to collective information extraction using RMN&apos;s that simultaneously extracts all of the information from a document by exploiting the textual content and context of each relevant substring as well as the document relationships between the</context>
</contexts>
<marker>Taskar, Abbeel, Koller, 2002</marker>
<rawString>Benjamin Taskar, Pieter Abbeel, and D. Koller. 2002. Discriminative probabilistic models for relational data. In Proceedings of 18th Conference on Uncertainty in Artificial Intelligence (UAI-02), pages 485-492, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003,</booktitle>
<pages>142--147</pages>
<location>Edmonton, Canada.</location>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of CoNLL-2003, pages 142-147. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan S Yedidia</author>
<author>William T Freeman</author>
<author>Yair Weiss</author>
</authors>
<title>Generalized belief propagation.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems 12,</booktitle>
<pages>689--695</pages>
<location>Denver, CO.</location>
<marker>Yedidia, Freeman, Weiss, 2000</marker>
<rawString>Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. 2000. Generalized belief propagation. In Advances in Neural Information Processing Systems 12, pages 689-695, Denver, CO.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>