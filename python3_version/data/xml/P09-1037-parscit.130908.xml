<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000456">
<title confidence="0.9980555">
Topological Ordering of Function Words
in Hierarchical Phrase-based Translation
</title>
<author confidence="0.999167">
Hendra Setiawan1 and Min-Yen Kan2 and Haizhou Li3 and Philip Resnik1
</author>
<affiliation confidence="0.997069">
1University of Maryland Institute for Advanced Computer Studies
2School of Computing, National University of Singapore
3Human Language Technology, Institute for Infocomm Research, Singapore
</affiliation>
<email confidence="0.991429">
{hendra,resnik}@umiacs.umd.edu,
kanmy@comp.nus.edu.sg, hli@i2r.a-star.edu.sg
</email>
<sectionHeader confidence="0.995516" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899888888889">
Hierarchical phrase-based models are at-
tractive because they provide a consis-
tent framework within which to character-
ize both local and long-distance reorder-
ings, but they also make it difficult to
distinguish many implausible reorderings
from those that are linguistically plausi-
ble. Rather than appealing to annotation-
driven syntactic modeling, we address this
problem by observing the influential role
of function words in determining syntac-
tic structure, and introducing soft con-
straints on function word relationships as
part of a standard log-linear hierarchi-
cal phrase-based model. Experimentation
on Chinese-English and Arabic-English
translation demonstrates that the approach
yields significant gains in performance.
</bodyText>
<sectionHeader confidence="0.998523" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99276655">
Hierarchical phrase-based models (Chiang, 2005;
Chiang, 2007) offer a number of attractive bene-
fits in statistical machine translation (SMT), while
maintaining the strengths of phrase-based systems
(Koehn et al., 2003). The most important of these
is the ability to model long-distance reordering ef-
ficiently. To model such a reordering, a hierar-
chical phrase-based system demands no additional
parameters, since long and short distance reorder-
ings are modeled identically using synchronous
context free grammar (SCFG) rules. The same
rule, depending on its topological ordering – i.e.
its position in the hierarchical structure – can af-
fect both short and long spans of text. Interest-
ingly, hierarchical phrase-based models provide
this benefit without making any linguistic commit-
ments beyond the structure of the model.
However, the system&apos;s lack of linguistic com-
mitment is also responsible for one of its great-
est drawbacks. In the absence of linguistic knowl-
edge, the system models linguistic structure using
an SCFG that contains only one type of nontermi-
nal symbol&apos;. As a result, the system is susceptible
to the overgeneration problem: the grammar may
suggest more reordering choices than appropriate,
and many of those choices lead to ungrammatical
translations.
Chiang (2005) hypothesized that incorrect re-
ordering choices would often correspond to hier-
archical phrases that violate syntactic boundaries
in the source language, and he explored the use
of a “constituent feature” intended to reward the
application of hierarchical phrases which respect
source language syntactic categories. Although
this did not yield significant improvements, Mar-
ton and Resnik (2008) and Chiang et al. (2008)
extended this approach by introducing soft syn-
tactic constraints similar to the constituent feature,
but more fine-grained and sensitive to distinctions
among syntactic categories; these led to substan-
tial improvements in performance. Zollman et al.
(2006)took a complementary approach, constrain-
ing the application of hierarchical rules to respect
syntactic boundaries in the target language syn-
tax. Whether the focus is on constraints from the
source language or the target language, the main
ingredient in both previous approaches is the idea
of constraining the spans of hierarchical phrases to
respect syntactic boundaries.
In this paper, we pursue a different approach
to improving reordering choices in a hierarchical
phrase-based model. Instead of biasing the model
toward hierarchical phrases whose spans respect
syntactic boundaries, we focus on the topologi-
cal ordering of phrases in the hierarchical struc-
ture. We conjecture that since incorrect reorder-
ing choices correspond to incorrect topological or-
derings, boosting the probability of correct topo-
&apos;Tn practice, one additional nonterminal symbol is used in
“glue rules”. This is not relevant in the present discussion.
</bodyText>
<page confidence="0.972603">
324
</page>
<note confidence="0.999613">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 324–332,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999853047619048">
logical ordering choices should improve the sys-
tem. Although related to previous proposals (cor-
rect topological orderings lead to correct spans
and vice versa), our proposal incorporates broader
context and is structurally more aware, since we
look at the topological ordering of a phrase relative
to other phrases, rather than modeling additional
properties of a phrase in isolation. In addition, our
proposal requires no monolingual parsing or lin-
guistically informed syntactic modeling for either
the source or target language.
The key to our approach is the observation that
we can approximate the topological ordering of
hierarchical phrases via the topological ordering
of function words. We introduce a statistical re-
ordering model that we call the pairwise domi-
nance model, which characterizes reorderings of
phrases around a pair of function words. In mod-
eling function words, our model can be viewed as
a successor to the function words-centric reorder-
ing model (Setiawan et al., 2007), expanding on
the previous approach by modeling pairs of func-
tion words rather than individual function words
in isolation.
The rest of the paper is organized as follows. In
Section 2, we briefly review hierarchical phrase-
based models. In Section 3, we first describe the
overgeneration problem in more detail with a con-
crete example, and then motivate our idea of us-
ing the topological ordering of function words to
address the problem. In Section 4, we develop
our idea by introducing the pairwise dominance
model, expressing function word relationships in
terms of what we call the the dominance predi-
cate. In Section 5, we describe an algorithm to es-
timate the parameters of the dominance predicate
from parallel text. In Sections 6 and 7, we describe
our experiments, and in Section 8, we analyze the
output of our system and lay out a possible future
direction. Section 9 discusses the relation of our
approach to prior work and Section 10 wraps up
with our conclusions.
</bodyText>
<sectionHeader confidence="0.885859" genericHeader="introduction">
2 Hierarchical Phrase-based System
</sectionHeader>
<bodyText confidence="0.9998476">
Formally, a hierarchical phrase-based SMT sys-
tem is based on a weighted synchronous context
free grammar (SCFG) with one type of nonter-
minal symbol. Synchronous rules in hierarchical
phrase-based models take the following form:
</bodyText>
<equation confidence="0.998972">
X — (γ, α, —) (1)
</equation>
<bodyText confidence="0.998379538461538">
where X is the nonterminal symbol and γ and α
are strings that contain the combination of lexical
items and nonterminals in the source and target
languages, respectively. The — symbol indicates
that nonterminals in γ and α are synchronized
through co-indexation; i.e., nonterminals with the
same index are aligned. Nonterminal correspon-
dences are strictly one-to-one, and in practice the
number of nonterminals on the right hand side is
constrained to at most two, which must be sepa-
rated by lexical items.
Each rule is associated with a score that is com-
puted via the following log linear formula:
</bodyText>
<equation confidence="0.998403333333333">
11 w(X � (γ, α, �)) = f�� (2)
i
i
</equation>
<bodyText confidence="0.9998914375">
where fi is a feature describing one particular as-
pect of the rule and λi is the corresponding weight
of that feature. Given e� and f as the source
and target phrases associated with the rule, typi-
cal features used are rule&apos;s translation probability
Ptrans( �f|e) and its inverse Ptrans(�e |�f), the lexi-
cal probability Ple�(�f|e) and its inverse Plea(e |�f).
Systems generally also employ a word penalty, a
phrase penalty, and target language model feature.
(See (Chiang, 2005) for more detailed discussion.)
Our pairwise dominance model will be expressed
as an additional rule-level feature in the model.
Translation of a source sentence e using hier-
archical phrase-based models is formulated as a
search for the most probable derivation D* whose
source side is equal to e:
</bodyText>
<equation confidence="0.9890905">
D* = argmax P(D),where source(D)=e.
D = Xi, i E 1...|D |is a set of rules following a
</equation>
<bodyText confidence="0.989602">
certain topological ordering, indicated here by the
use of the superscript.
</bodyText>
<sectionHeader confidence="0.946513" genericHeader="method">
3 Overgeneration and Topological
</sectionHeader>
<subsectionHeader confidence="0.820492">
Ordering of Function Words
</subsectionHeader>
<bodyText confidence="0.926723545454545">
The use of only one type of nonterminal allows a
flexible permutation of the topological ordering of
the same set of rules, resulting in a huge number of
possible derivations from a given source sentence.
In that respect, the overgeneration problem is not
new to SMT: Bracketing Transduction Grammar
(BTG) (Wu, 1997) uses a single type of nontermi-
nal and is subject to overgeneration problems, as
well.�
Note, however, that overgeneration in BTG can be
viewed as a feature, not a bug, since the formalism was origi-
</bodyText>
<page confidence="0.998678">
325
</page>
<bodyText confidence="0.9949027">
The problem may be less severe in hierarchi-
cal phrase-based MT than in BTG, since lexical
items on the rules&apos; right hand sides often limit the
span of nonterminals. Nonetheless overgeneration
of reorderings is still problematic, as we illustrate
using the hypothetical Chinese-to-English exam-
ple in Fig. 1.
Suppose we want to translate the Chinese sen-
tence in Fig. 1 into English using the following set
of rules:
</bodyText>
<listItem confidence="0.9987754">
1. Xa → h žJB 3M X1, computers and X1i
2. Xb → hX1 A X2, X1 are X2i
3. X, → h -T-K , cell phones i
4. Xd → hX1 { AHJR , inventions of X1i
5. Xe → h �^��� , the last century i
</listItem>
<bodyText confidence="0.999491390625">
Co-indexation of nonterminals on the right hand
side is indicated by subscripts, and for our ex-
amples the label of the nonterminal on the left
hand side is used as the rule&apos;s unique identifier.
To correctly translate the sentence, a hierarchical
phrase-based system needs to model the subject
noun phrase, object noun phrase and copula con-
structions; these are captured by rules Xa, Xd and
Xb respectively, so this set of rules represents a
hierarchical phrase-based system that can be used
to correctly translate the Chinese sentence. Note
that the Chinese word order is correctly preserved
in the subject (Xa) as well as copula constructions
(Xb), and correctly inverted in the object construc-
tion (Xd).
However, although it can generate the correct
translation in Fig. 2, the grammar has no mech-
anism to prevent the generation of an incorrect
translation like the one illustrated in Fig. 3. If
we contrast the topological ordering of the rules
in Fig. 2 and Fig. 3, we observe that the difference
is small but quite significant. Using precede sym-
bol (≺) to indicate the first operand immediately
dominates the second operand in the hierarchical
structure, the topological orderings in Fig. 2 and
Fig. 3 are Xa ≺ Xb ≺ X, ≺ Xd ≺ Xe and
Xd ≺ Xa ≺ Xb ≺ X, ≺ Xe, respectively. The
only difference is the topological ordering of Xd:
in Fig. 2, it appears below most of the other hier-
archical phrases, while in Fig. 3, it appears above
all the other hierarchical phrases.
nally introduced for bilingual analysis rather than generation
of translations.
Modeling the topological ordering of hierarchi-
cal phrases is computationally prohibitive, since
there are literally millions of hierarchical rules in
the system&apos;s automatically-learned grammar and
millions of possible ways to order their applica-
tion. To avoid this computational problem and
still model the topological ordering, we propose
to use the topological ordering of function words
as a practical approximation. This is motivated by
the fact that function words tend to carry crucial
syntactic information in sentences, serving as the
“glue” for content-bearing phrases. Moreover, the
positional relationships between function words
and content phrases tends to be fixed (e.g., in En-
glish, prepositions invariably precede their object
noun phrase), at least for the languages we have
worked with thus far.
In the Chinese sentence above, there are three
function words involved: the conjunction3M (and),
the copula A (are), and the noun phrase marker
{(of).3 Using the function words as approximate
representations of the rules in which they appear,
the topological ordering of hierarchical phrases in
Fig. 2 is 3M(and) ≺ A(are) ≺ {(of), while that
in Fig. 3 is {(of) ≺ 3M(and) ≺ A(are).4 We
can distinguish the correct and incorrect reorder-
ing choices by looking at this simple information.
In the correct reordering choice, {(of) appears at
the lower level of the hierarchy while in the incor-
rect one, {(of) appears at the highest level of the
hierarchy.
</bodyText>
<sectionHeader confidence="0.997397" genericHeader="method">
4 Pairwise Dominance Model
</sectionHeader>
<bodyText confidence="0.987926095238095">
Our example suggests that we may be able to im-
prove the translation model&apos;s sensitivity to correct
versus incorrect reordering choices by modeling
the topological ordering of function words. We do
so by introducing a predicate capturing the domi-
nance relationship in a derivation between pairs of
neighboring function words.5
Let us define a predicate d(Y &apos;, Y&amp;quot;) that takes
two function words as input and outputs one of
3 W use the term “noun phrase marker” here in a general
sense, meaning that in this example it helps tell us that the
phrase is part of an NP, not as a technical linguistic term. Tt
serves in other grammatical roles, as well. Disambiguating
the syntactic roles of function words might be a particularly
useful thing to do in the model we are proposing; this is a
question for future research.
¢Note that for expository purposes, we designed our sim-
ple grammar to ensure that these function words appear in
separate rules.
5Two function words are considered neighbors iff no other
function word appears between them in the source sentence.
</bodyText>
<page confidence="0.953201">
326
</page>
<equation confidence="0.6074488">
žrM Z Cå 4 _L^-�E { AÒ
✘✾ ✘✘✘✘
❳❳❳❳❳③
❄
❄ ❄ ❄ ❄
</equation>
<bodyText confidence="0.316532">
computers and cell phones are inventions of the last century
</bodyText>
<figureCaption confidence="0.991871">
Figure 1: A running example of Chinese-to-English translation.
</figureCaption>
<equation confidence="0.9254716">
Xa==&gt;.( ž Z Xb, computers and Xb)
�( žJB Z Xc 4 Xd, computers and Xc are Xd)
�( žJM Z Cå 4 Xd, computers and cell phones are Xd)
( žJM Z Cå 4 Xe { AÒ , computers and cell phones are inventions of Xe)
( žJB Z Cå 4 ±^-2&apos; { AÒ , computers and cell phones are inventions of the last century)
</equation>
<figureCaption confidence="0.972801">
Figure 2: The derivation that leads to the correct translation
</figureCaption>
<equation confidence="0.9922554">
Xd==&gt;.(Xa { AÒ , inventions of Xa)
�(žJrm Z Xb { AÒ , inventions of computers and Xb)
�(žJrm Z Xc 4 Xe { AÒ , inventions of computers and Xc are Xe)
�(žu Z Cå 4 Xe { AÒ , inventions of computers and cell phones are Xe)
�(žJB Z Cå 4 ±^-�E { AÒ , inventions of computers and cell phones are the last century)
</equation>
<figureCaption confidence="0.999502">
Figure 3: The derivation that leads to the incorrect translation
</figureCaption>
<bodyText confidence="0.975363724137931">
four values: {leftFirst, rightFirst, dontCare, nei-
ther}, where Y &apos; appears to the left of Y &apos;&apos; in the
source sentence. The value leftFirst indicates that
in the derivation&apos;s topological ordering, Y &apos; pre-
cedes Y &apos;&apos; (i.e. Y &apos; dominates Y &apos;&apos; in the hierarchi-
cal structure), while rightFirst indicates that Y &apos;&apos;
dominates Y &apos;. In Fig. 2, d(Y &apos;,Y &apos;&apos;) = leftFirst
for Y &apos; = the copula 4 (are) and Y &apos;&apos; = the noun
phrase marker { (of).
The dontCare and neither values capture two
additional relationships: dontCare indicates that
the topological ordering of the function words is
flexible, and neither indicates that the topologi-
cal ordering of the function words is disjoint. The
former is useful in cases where the hierarchical
phrases suggest the same kind of reordering, and
therefore restricting their topological ordering is
not necessary. This is illustrated in Fig. 2 by the
pair Z(and) and the copula 4(are), where putting
either one above the other does not change the fi-
nal word order. The latter is useful in cases where
the two function words do not share a same parent.
Formally, this model requires several changes in
the design of the hierarchical phrase-based system.
1. To facilitate topological ordering of function
words, the hierarchical phrases must be sub-
categorized with function words. Taking Xb
in Fig. 2 as a case in point, subcategorization
using function words would yield:6
</bodyText>
<equation confidence="0.999053">
Xb(4 � {) — Xc 4 Xd({) (3)
</equation>
<bodyText confidence="0.998961095238095">
The subcategorization (indicated by the
information in parentheses following the
nonterminal) propagates the function word
4(are) of Xb to the higher level structure to-
gether with the function word {(of) of Xd.
This propagation process generalizes to other
rules by maintaining the ordering of the func-
tion words according to their appearance in
the source sentence. Note that the subcate-
gorized nonterminals often resemble genuine
syntactic categories, for instance X({) can
frequently be interpreted as a noun phrase.
2. To facilitate the computation of the domi-
nance relationship, the coindexing in syn-
chronized rules (indicated by the — symbol
in Eq. 1) must be expanded to include infor-
mation not only about the nonterminal corre-
spondences but also about the alignment of
the lexical items. For example, adding lexi-
cal alignment information to rule Xd would
yield:
</bodyText>
<equation confidence="0.90939">
Xd — (X1{2AÒ3, inventions3 of2 X1)
(4)
</equation>
<footnote confidence="0.989291">
6The target language side is concealed for clarity.
</footnote>
<page confidence="0.995117">
327
</page>
<bodyText confidence="0.9998528">
The computation of the dominance relation-
ship using this alignment information will be
discussed in detail in the next section.
Again taking Xb in Fig. 2 as a case in point, the
dominance feature takes the following form:
</bodyText>
<equation confidence="0.9995425">
fdorn.(Xb) Pz dom(d(4, {)j4, {)) (5)
dom(d(YL,YR)jYL,YR)) (6)
</equation>
<bodyText confidence="0.998871583333333">
where the probability of 4 � { is estimated ac-
cording to the probability of d(4, {).
In practice, both 4(are) and {(of) may ap-
pear together in one same rule. In such a case, a
dominance score is not calculated since the topo-
logical ordering of the two function words is un-
ambiguous. Hence, in our implementation, a
dominance score is only calculated at the points
where the topological ordering of the hierarchical
phrases needs to be resolved, i.e. the two function
words always come from two different hierarchi-
cal phrases.
</bodyText>
<sectionHeader confidence="0.987495" genericHeader="method">
5 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.999925923076923">
Learning the dominance model involves extract-
ing d values for every pair of neighboring func-
tion words in the training bitext. Such statistics
are not directly observable in parallel corpora, so
estimation is needed. Our estimation method is
based on two facts: (1) the topological ordering
of hierarchical phrases is tightly coupled with the
span of the hierarchical phrases, and (2) the span
of a hierarchical phrase at a higher level is al-
ways a superset of the span of all other hierarchical
phrases at the lower level of its substructure. Thus,
to establish soft estimates of dominance counts,
we utilize alignment information available in the
rule together with the consistent alignment heuris-
tic (Och and Ney, 2004) traditionally used to guess
phrase alignments.
Specifically, we define the span of a function
word as a maximal, consistent alignment in the
source language that either starts from or ends
with the function word. (Requiring that spans be
maximal ensures their uniqueness.) We will re-
fer to such spans as Maximal Consistent Align-
ments (MCA). Note that each function word has
two such Maximal Consistent Alignments: one
that ends with the function word (MCAR)and an-
other that starts from the function word (MCAL).
</bodyText>
<table confidence="0.9982505">
Y 0 Y 00 left- right- dont- nei-
First First Care ther
fn (and) 4 (are) 0.11 0.16 0.68 0.05
4 (are) { (of) 0.57 0.15 0.06 0.22
</table>
<tableCaption confidence="0.99905">
Table 1: The distribution of the dominance values
</tableCaption>
<bodyText confidence="0.9817858">
of the function words involved in Fig. 1. The value
with the highest probability is in bold.
Given two function words Y 0 and Y 00, with Y 0
preceding Y 00, we define the value of d by exam-
ining the MCAs of the two function words.
</bodyText>
<equation confidence="0.997334">
d(Y 0, Y 00) =
{ leftFirst, Y 0 E� MCAR(Y 00) ∧ Y 00 E MCAL(Y 0)
rightFirst, Y 0E MCAR(Y 00) ∧ Y 00E� MCAL(Y 0)
dontCare, Y 0E MCAR(Y 00) ∧ Y 00E MCAL(Y 0)
neither, Y 0E� MCAR(Y 00) ∧ Y 00E� MCAL(Y 0)
(6)
</equation>
<bodyText confidence="0.997510846153846">
Fig. 4a illustrates the leftFirst dominance value
where the intersection of the MCAs contains only
the second function word ({(of)). Fig. 4b illus-
trates the dontCare value, where the intersection
contains both function words. Similarly, rightFirst
and neither are represented by an intersection that
contains only Y 0, or by an empty intersection, re-
spectively. Once all the d values are counted, the
pairwise dominance model of neighboring func-
tion words can be estimated simply from counts
using maximum likelihood. Table 1 illustrates es-
timated dominance values that correctly resolve
the topological ordering for our running example.
</bodyText>
<sectionHeader confidence="0.999141" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.946137705882353">
We tested the effect of introducing the pairwise
dominance model into hierarchical phrase-based
translation on Chinese-to-English and Arabic-to-
English translation tasks, thus studying its effect
in two languages where the use of function words
differs significantly. Following Setiawan et al.
(2007), we identify function words as the N most
frequent words in the corpus, rather than identify-
ing them according to linguistic criteria; this ap-
proximation removes the need for any additional
language-specific resources. We report results
for N = 32, 64,128, 256, 512,1024, 2048.7 For
7 W observe that even N = 2048 represents less than
1.5% and 0.8% of the words in the Chinese and Arabic vo-
cabularies, respectively. The validity of the frequency-based
strategy, relative to linguistically-defined function words, is
discussed in Section 8
</bodyText>
<page confidence="0.972925">
328
</page>
<figure confidence="0.989713934782608">
C
SIL
fn
� q C
3M SIL
♥a
computers
and
cell phones
are
innovations
of
the last century
❥
③
❥
③
❥
�
�
in,
�
-
4 E
❥
❥
�
-
4 � �
�
irm
♥b
❥
③
❥
③
❥
❥
❥
computer
and
cell phones
are
innovations
of
the last century
</figure>
<bodyText confidence="0.999939117647059">
“subsampling” from the full training data using a
method proposed by Kishore Papineni (personal
communication). The subsampling algorithm se-
lects sentence pairs from the training data in a
way that seeks reasonable representation for all n-
grams appearing in the test set. For the language
model, we used a 5-gram model trained on the En-
glish portion of the whole training data plus por-
tions of the Gigaword v2 corpus. We used the
NIST MT03 test set as the development set for
optimizing the interpolation weights using MERT.
We carried out the evaluation of the systems on the
NIST 2006 evaluation set (MT06) and the NIST
2008 evaluation set (MT08). Arabic source text
was preprocessed by separating clitics, the defi-
niteness marker, and the future tense marker from
their stems.
</bodyText>
<sectionHeader confidence="0.975386" genericHeader="method">
7 Experimental Results
</sectionHeader>
<figureCaption confidence="0.922744333333333">
Figure 4: Illustrations for: a) the leftFirst value,
and b) the dontCare value. Thickly bordered
boxes are MCAs of the function words while solid
circles are the alignment points of the function
words. The gray boxes are the intersections of the
two MCAs.
</figureCaption>
<bodyText confidence="0.9988322">
all experiments, we report performance using the
BLEU score (Papineni et al., 2002), and we assess
statistical significance using the standard boot-
strapping approach introduced by (Koehn, 2004).
Chinese-to-English experiments. We trained
the system on the NIST MT06 Eval corpus ex-
cluding the UN data (approximately 900K sen-
tence pairs). For the language model, we used a 5-
gram model with modified Kneser-Ney smoothing
(Kneser and Ney, 1995) trained on the English side
of our training data as well as portions of the Giga-
word v2 English corpus. We used the NIST MT03
test set as the development set for optimizing inter-
polation weights using minimum error rate train-
ing (MERT; (Och and Ney, 2002)). We carried out
evaluation of the systems on the NIST 2006 eval-
uation test (MT06) and the NIST 2008 evaluation
test (MT08). We segmented Chinese as a prepro-
cessing step using the Harbin segmenter (Zhao et
al., 2001).
Arabic-to-English experiments. We trained
the system on a subset of 950K sentence pairs
from the NIST MT08 training data, selected by
Chinese-to-English experiments. Table 2 sum-
marizes the results of our Chinese-to-English ex-
periments. These results confirm that the pairwise
dominance model can significantly increase per-
formance as measured by the BLEU score, with a
consistent pattern of results across the MT06 and
MT08 test sets. Modeling N = 32 drops the per-
formance marginally below baseline, suggesting
that perhaps there are not enough words for the
pairwise dominance model to work with. Dou-
bling the number of words (N = 64) produces
a small gain, and defining the pairwise dominance
model using N = 128 most frequent words pro-
duces a statistically significant 1-point gain over
the baseline (p &lt; 0.01). Larger values of N
yield statistically significant performance above
the baseline, but without further improvements
over N = 128.
Arabic-to-English experiments. Table 3 sum-
marizes the results of our Arabic-to-English ex-
periments. This set of experiments shows a pat-
tern consistent with what we observed in Chinese-
to-English translation, again generally consistent
across MT06 and MT08 test sets although mod-
eling a small number of lexical items (N = 32)
brings a marginal improvement over the baseline.
In addition, we again find that the pairwise dom-
inance model with N = 128 produces the most
significant gain over the baseline in the MT06,
although, interestingly, modeling a much larger
number of lexical items (N = 2048) yields the
strongest improvement for the MT08 test set.
</bodyText>
<page confidence="0.996211">
329
</page>
<table confidence="0.533123777777778">
MT06 MT08
baseline 30.58 24.08
+dom(N = 32) 30.43 23.91
+dom(N = 64) 30.96 24.45
+dom(N = 128) 31.59 24.91
+dom(N = 256) 31.24 24.26
+dom(N = 512) 31.33 24.39
+dom(N = 1024) 31.22 24.79
+dom(N = 2048) 30.75 23.92
</table>
<tableCaption confidence="0.708071">
Table 2: Experimental results on Chinese-to-
</tableCaption>
<bodyText confidence="0.9922928">
English translation with the pairwise dominance
model (dom) of different N. The baseline (the
first line) is the original hierarchical phrase-based
system. Statistically significant results (p &lt; 0.01)
over the baseline are in bold.
</bodyText>
<equation confidence="0.975609888888889">
MT06 MT08
baseline 41.56 40.06
+dom(N = 32) 41.66 40.26
+dom(N = 64) 42.03 40.73
+dom(N = 128) 42.66 41.08
+dom(N = 256) 42.28 40.69
+dom(N = 512) 41.97 40.95
+dom(N = 1024) 42.05 40.55
+dom(N = 2048) 42.48 41.47
</equation>
<tableCaption confidence="0.55456">
Table 3: Experimental results on Arabic-to-
</tableCaption>
<bodyText confidence="0.9986928">
English translation with the pairwise dominance
model (dom) of different N. The baseline (the
first line) is the original hierarchical phrase-based
system. Statistically significant results over the
baseline (p &lt; 0.01) are in bold.
</bodyText>
<sectionHeader confidence="0.969481" genericHeader="discussions">
8 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999711153846154">
The results in both sets of experiments show con-
sistently that we have achieved a significant gains
by modeling the topological ordering of function
words. When we visually inspect and compare
the outputs of our system with those of the base-
line, we observe that improved BLEU score often
corresponds to visible improvements in the sub-
jective translation quality. For example, the trans-
lations for the Chinese sentence “29&apos;r1 �,02 :3
IMM4 r±5 X296 8ñ7 T8 R9 À10 õ11 È12
?13”, taken from Chinese MT06 test set, are as
follows (co-indexing subscripts represent recon-
structed word alignments):
</bodyText>
<listItem confidence="0.740630333333333">
• baseline: “military1 intelligence2 un-
der observation8 in5 u.s.6 air raids7 :3 iran4
to9 how11 long12 ?13 ”
• +dom(N=128): “ military1 survey2 :3 how11
long12 iran4 under8 air strikes7 of the u.s6
can9 hold out10 ?13 ”
</listItem>
<bodyText confidence="0.999749931818182">
In addition to some lexical translation errors
(e.g. X296 should be translated to U.S. Army),
the baseline system also makes mistakes in re-
ordering. The most obvious, perhaps, is its fail-
ure to capture the wh-movement involving the in-
terrogative word õ11 (how); this should move
to the beginning of the translated clause, consis-
tent with English wh-fronting as opposed to Chi-
nese wh in situ. The pairwise dominance model
helps, since the dominance value between the in-
terrogative word and its previous function word,
the modal verb f&amp;quot;9(can) in the baseline system&apos;s
output, is neither, rather than rightFirst as in the
better translation.
The fact that performance tends to be best us-
ing a frequency threshold of N = 128 strikes
us as intuitively sensible, given what we know
about word frequency rankings.$ In English,
for example, the most frequent 128 words in-
clude virtually all common conjunctions, deter-
miners, prepositions, auxiliaries, and comple-
mentizers – the crucial elements of “syntactic
glue” that characterize the types of linguistic
phrases and the ordering relationships between
them – and a very small proportion of con-
tent words. Using Adam Kilgarriff&apos;s lemma-
tized frequency list from the British National Cor-
pus, http://www.kilgarriff.co.uk/bnc-readme.html,
the most frequent 128 words in English are heav-
ily dominated by determiners, “functional” ad-
verbs like not and when, “particle” adverbs like
up, prepositions, pronouns, and conjunctions, with
some arguably “functional” auxiliary and light
verbs like be, have, do, give, make, take. Con-
tent words are generally limited to a small number
of frequent verbs like think and want and a very
small handful of frequent nouns. In contrast, ranks
129-256 are heavily dominated by the traditional
content-word categories, i.e. nouns, verbs, adjec-
tives and adverbs, with a small number of left-over
function words such as less frequent conjunctions
while, when, and although.
Consistent with these observations for English,
the empirical results for Chinese suggest that our
</bodyText>
<footnote confidence="0.968956333333333">
8Tn fact, we initially simply chose N = 128 for our exper-
imentation, and then did runs with alternative N to confirm
our intuitions.
</footnote>
<page confidence="0.996707">
330
</page>
<bodyText confidence="0.99995235">
approximation of function words using word fre-
quency is reasonable. Using a list of approxi-
mately 900 linguistically identified function words
in Chinese extracted from (Howard, 2002), we ob-
serve that that the performance drops when in-
creasing N above 128 corresponds to a large in-
crease in the number of non-function words used
in the model. For example, with N = 2048, the
proportion of non-function words is 88%, com-
pared to 60% when N = 128.9
One natural extension of this work, therefore,
would be to tighten up our characterization of
function words, whether statistically, distribution-
ally, or simply using manually created resources
that exist for many languages. As a first step, we
did a version of the Chinese-English experiment
using the list of approximately 900 genuine func-
tion words, testing on the Chinese MT06 set. Per-
haps surprisingly, translation performance, 30.90
BLEU, was around the level we obtained when
using frequency to approximate function words at
N = 64. However, we observe that many of
the words in the linguistically motivated function
word list are quite infrequent; this suggests that
data sparseness may be an additional factor worth
investigating.
Finally, although we believe there are strong
motivations for focusing on the role of function
words in reordering, there may well be value in
extending the dominance model to include content
categories. Verbs and many nouns have subcat-
egorization properties that may influence phrase
ordering, for example, and this may turn out to ex-
plain the increase in Arabic-English performance
for N = 2048 using the MT08 test set. More gen-
erally, the approach we are taking can be viewed
as a way of selectively lexicalizing the automati-
cally extracted grammar, and there is a large range
of potentially interesting choices in how such lex-
icalization could be done.
</bodyText>
<sectionHeader confidence="0.999844" genericHeader="related work">
9 Related Work
</sectionHeader>
<bodyText confidence="0.977465515151515">
In the introduction, we discussed Chiang&apos;s (2005)
constituency feature, related ideas explored by
Marton and Resnik (2008) and Chiang et al.
(2008), and the target-side variation investigated
by Zollman et al. (2006). These methods differ
from each other mainly in terms of the specific lin-
9We plan to do corresponding experimentation and anal-
ysis for Arabic once we identify a suitable list of manually
identified function words.
guistic knowledge being used and on which side
the constraints are applied.
Shen et al. (2008) proposed to use lin-
guistic knowledge expressed in terms of a de-
pendency grammar, instead of a syntactic con-
stituency grammar. Villar et al. (2008) attempted
to use syntactic constituency on both the source
and target languages in the same spirit as the con-
stituency feature, along with some simple pattern-
based heuristics – an approach also investigated by
Iglesias et al. (2009). Aiming at improving the se-
lection of derivations, Zhou et al. (2008) proposed
prior derivation models utilizing syntactic annota-
tion of the source language, which can be seen as
smoothing the probabilities of hierarchical phrase
features.
A key point is that the model we have intro-
duced in this paper does not require the linguistic
supervision needed in most of this prior work. We
estimate the parameters of our model from parallel
text without any linguistic annotation. That said,
we would emphasize that our approach is, in fact,
motivated in linguistic terms by the role of func-
tion words in natural language syntax.
</bodyText>
<sectionHeader confidence="0.995734" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999898875">
We have presented a pairwise dominance model
to address reordering issues that are not handled
particularly well by standard hierarchical phrase-
based modeling. In particular, the minimal lin-
guistic commitment in hierarchical phrase-based
models renders them susceptible to overgenera-
tion of reordering choices. Our proposal han-
dles the overgeneration problem by identifying
hierarchical phrases with function words and by
using function word relationships to incorporate
soft constraints on topological orderings. Our
experimental results demonstrate that introducing
the pairwise dominance model into hierarchical
phrase-based modeling improves performance sig-
nificantly in large-scale Chinese-to-English and
Arabic-to-English translation tasks.
</bodyText>
<sectionHeader confidence="0.999318" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993934857142857">
This research was supported in part by the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-2-001. Any opinions, findings, conclusions or
recommendations expressed in this paper are those
of the authors and do not necessarily reflect the
view of the sponsors.
</bodyText>
<page confidence="0.998304">
331
</page>
<sectionHeader confidence="0.99536" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998386822916667">
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 224–233, Honolulu,
Hawaii, October.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL&apos;05), pages
263–270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Jiaying Howard. 2002. A Student Handbook for Chi-
nese Function Words. The Chinese University Press.
Gonzalo Iglesias, Adria de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings
of the 12th Conference of the European Chapter of
the Association of Computational Linguistics (to ap-
pear).
R. Kneser and H. Ney. 1995. Improved backing-
off for m-gram language modeling. In Proceed-
ings of IEEE International Conference on Acoustics,
Speech, and Signal Processing95, pages 181–184,
Detroit, MI, May.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 127–133,
Edmonton, Alberta, Canada, May. Association for
Computational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388–395, Barcelona, Spain,
July.
Yuval Marton and Philip Resnik. 2008. Soft syntac-
tic constraints for hierarchical phrased-based trans-
lation. In Proceedings of The 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1003–
1011, Columbus, Ohio, June.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 295–302, Philadelphia,
Pennsylvania, USA, July.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417–449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 712–
719, Prague, Czech Republic, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of The 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 577–585, Columbus,
Ohio, June.
David Vilar, Daniel Stein, and Hermann Ney. 2008.
Analysing soft syntax features and heuristics for hi-
erarchical phrase based machine translation. Inter-
national Workshop on Spoken Language Translation
2008, pages 190–197, October.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404, Sep.
Tiejun Zhao, Yajuan Lv, Jianmin Yao, Hao Yu, Muyun
Yang, and Fang Liu. 2001. Increasing accuracy
of chinese segmentation with strategy of multi-step
processing. Journal of Chinese Information Pro-
cessing (Chinese Version), 1:13–18.
Bowen Zhou, Bing Xiang, Xiaodan Zhu, and Yuqing
Gao. 2008. Prior derivation models for formally
syntax-based translation using linguistically syntac-
tic parsing and tree kernels. In Proceedings of
the ACL-08: HLT Second Workshop on Syntax and
Structure in Statistical Translation (SSST-2), pages
19–27, Columbus, Ohio, June.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138–141, New York City,
June.
</reference>
<page confidence="0.998186">
332
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.235283">
<title confidence="0.732282">Function Words in Hierarchical Phrase-based Translation of Maryland Institute for Advanced Computer Studies</title>
<affiliation confidence="0.750605">of National University of Institute for Infocomm Research,</affiliation>
<email confidence="0.987304">hendra@umiacs.umd.edu,</email>
<email confidence="0.987304">resnik@umiacs.umd.edu,</email>
<abstract confidence="0.99924252631579">Hierarchical phrase-based models are attractive because they provide a consistent framework within which to characterboth local and reorderbut they also make it difficult to many implausible those that are plausi- Rather than annotationsyntactic we address this by influential role function words in syntacstructure, and constraints on function word relationships as of a standard hierarchical phrase-based model. Experimentation and translation demonstrates that the approach in performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>224--233</pages>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="2871" citStr="Chiang et al. (2008)" startWordPosition="402" endWordPosition="405">lt, the system is susceptible to the overgeneration problem: the grammar may suggest more reordering choices than appropriate, and many of those choices lead to ungrammatical translations. Chiang (2005) hypothesized that incorrect reordering choices would often correspond to hierarchical phrases that violate syntactic boundaries in the source language, and he explored the use of a “constituent feature” intended to reward the application of hierarchical phrases which respect source language syntactic categories. Although this did not yield significant improvements, Marton and Resnik (2008) and Chiang et al. (2008) extended this approach by introducing soft syntactic constraints similar to the constituent feature, but more fine-grained and sensitive to distinctions among syntactic categories; these led to substantial improvements in performance. Zollman et al. (2006)took a complementary approach, constraining the application of hierarchical rules to respect syntactic boundaries in the target language syntax. Whether the focus is on constraints from the source language or the target language, the main ingredient in both previous approaches is the idea of constraining the spans of hierarchical phrases to </context>
<context position="30563" citStr="Chiang et al. (2008)" startWordPosition="5025" endWordPosition="5028">ies. Verbs and many nouns have subcategorization properties that may influence phrase ordering, for example, and this may turn out to explain the increase in Arabic-English performance for N = 2048 using the MT08 test set. More generally, the approach we are taking can be viewed as a way of selectively lexicalizing the automatically extracted grammar, and there is a large range of potentially interesting choices in how such lexicalization could be done. 9 Related Work In the introduction, we discussed Chiang&apos;s (2005) constituency feature, related ideas explored by Marton and Resnik (2008) and Chiang et al. (2008), and the target-side variation investigated by Zollman et al. (2006). These methods differ from each other mainly in terms of the specific lin9We plan to do corresponding experimentation and analysis for Arabic once we identify a suitable list of manually identified function words. guistic knowledge being used and on which side the constraints are applied. Shen et al. (2008) proposed to use linguistic knowledge expressed in terms of a dependency grammar, instead of a syntactic constituency grammar. Villar et al. (2008) attempted to use syntactic constituency on both the source and target lang</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 224–233, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05),</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1213" citStr="Chiang, 2005" startWordPosition="153" endWordPosition="154"> make it difficult to distinguish many implausible reorderings from those that are linguistically plausible. Rather than appealing to annotationdriven syntactic modeling, we address this problem by observing the influential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields significant gains in performance. 1 Introduction Hierarchical phrase-based models (Chiang, 2005; Chiang, 2007) offer a number of attractive benefits in statistical machine translation (SMT), while maintaining the strengths of phrase-based systems (Koehn et al., 2003). The most important of these is the ability to model long-distance reordering efficiently. To model such a reordering, a hierarchical phrase-based system demands no additional parameters, since long and short distance reorderings are modeled identically using synchronous context free grammar (SCFG) rules. The same rule, depending on its topological ordering – i.e. its position in the hierarchical structure – can affect both</context>
<context position="2453" citStr="Chiang (2005)" startWordPosition="343" endWordPosition="344">xt. Interestingly, hierarchical phrase-based models provide this benefit without making any linguistic commitments beyond the structure of the model. However, the system&apos;s lack of linguistic commitment is also responsible for one of its greatest drawbacks. In the absence of linguistic knowledge, the system models linguistic structure using an SCFG that contains only one type of nonterminal symbol&apos;. As a result, the system is susceptible to the overgeneration problem: the grammar may suggest more reordering choices than appropriate, and many of those choices lead to ungrammatical translations. Chiang (2005) hypothesized that incorrect reordering choices would often correspond to hierarchical phrases that violate syntactic boundaries in the source language, and he explored the use of a “constituent feature” intended to reward the application of hierarchical phrases which respect source language syntactic categories. Although this did not yield significant improvements, Marton and Resnik (2008) and Chiang et al. (2008) extended this approach by introducing soft syntactic constraints similar to the constituent feature, but more fine-grained and sensitive to distinctions among syntactic categories; </context>
<context position="7581" citStr="Chiang, 2005" startWordPosition="1159" endWordPosition="1160">l items. Each rule is associated with a score that is computed via the following log linear formula: 11 w(X � (γ, α, �)) = f�� (2) i i where fi is a feature describing one particular aspect of the rule and λi is the corresponding weight of that feature. Given e� and f as the source and target phrases associated with the rule, typical features used are rule&apos;s translation probability Ptrans( �f|e) and its inverse Ptrans(�e |�f), the lexical probability Ple�(�f|e) and its inverse Plea(e |�f). Systems generally also employ a word penalty, a phrase penalty, and target language model feature. (See (Chiang, 2005) for more detailed discussion.) Our pairwise dominance model will be expressed as an additional rule-level feature in the model. Translation of a source sentence e using hierarchical phrase-based models is formulated as a search for the most probable derivation D* whose source side is equal to e: D* = argmax P(D),where source(D)=e. D = Xi, i E 1...|D |is a set of rules following a certain topological ordering, indicated here by the use of the superscript. 3 Overgeneration and Topological Ordering of Function Words The use of only one type of nonterminal allows a flexible permutation of the top</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05), pages 263–270, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1228" citStr="Chiang, 2007" startWordPosition="155" endWordPosition="156">cult to distinguish many implausible reorderings from those that are linguistically plausible. Rather than appealing to annotationdriven syntactic modeling, we address this problem by observing the influential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields significant gains in performance. 1 Introduction Hierarchical phrase-based models (Chiang, 2005; Chiang, 2007) offer a number of attractive benefits in statistical machine translation (SMT), while maintaining the strengths of phrase-based systems (Koehn et al., 2003). The most important of these is the ability to model long-distance reordering efficiently. To model such a reordering, a hierarchical phrase-based system demands no additional parameters, since long and short distance reorderings are modeled identically using synchronous context free grammar (SCFG) rules. The same rule, depending on its topological ordering – i.e. its position in the hierarchical structure – can affect both short and long</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiaying Howard</author>
</authors>
<title>A Student Handbook for Chinese Function Words. The Chinese</title>
<date>2002</date>
<publisher>University Press.</publisher>
<contexts>
<context position="28740" citStr="Howard, 2002" startWordPosition="4728" endWordPosition="4729">the traditional content-word categories, i.e. nouns, verbs, adjectives and adverbs, with a small number of left-over function words such as less frequent conjunctions while, when, and although. Consistent with these observations for English, the empirical results for Chinese suggest that our 8Tn fact, we initially simply chose N = 128 for our experimentation, and then did runs with alternative N to confirm our intuitions. 330 approximation of function words using word frequency is reasonable. Using a list of approximately 900 linguistically identified function words in Chinese extracted from (Howard, 2002), we observe that that the performance drops when increasing N above 128 corresponds to a large increase in the number of non-function words used in the model. For example, with N = 2048, the proportion of non-function words is 88%, compared to 60% when N = 128.9 One natural extension of this work, therefore, would be to tighten up our characterization of function words, whether statistically, distributionally, or simply using manually created resources that exist for many languages. As a first step, we did a version of the Chinese-English experiment using the list of approximately 900 genuine</context>
</contexts>
<marker>Howard, 2002</marker>
<rawString>Jiaying Howard. 2002. A Student Handbook for Chinese Function Words. The Chinese University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Iglesias</author>
<author>Adria de Gispert</author>
<author>Eduardo R Banga</author>
<author>William Byrne</author>
</authors>
<title>Rule filtering by pattern for efficient hierarchical translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association of Computational Linguistics</booktitle>
<note>(to appear).</note>
<marker>Iglesias, de Gispert, Banga, Byrne, 2009</marker>
<rawString>Gonzalo Iglesias, Adria de Gispert, Eduardo R. Banga, and William Byrne. 2009. Rule filtering by pattern for efficient hierarchical translation. In Proceedings of the 12th Conference of the European Chapter of the Association of Computational Linguistics (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backingoff for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing95,</booktitle>
<pages>181--184</pages>
<location>Detroit, MI,</location>
<contexts>
<context position="22488" citStr="Kneser and Ney, 1995" startWordPosition="3713" endWordPosition="3716">e. Thickly bordered boxes are MCAs of the function words while solid circles are the alignment points of the function words. The gray boxes are the intersections of the two MCAs. all experiments, we report performance using the BLEU score (Papineni et al., 2002), and we assess statistical significance using the standard bootstrapping approach introduced by (Koehn, 2004). Chinese-to-English experiments. We trained the system on the NIST MT06 Eval corpus excluding the UN data (approximately 900K sentence pairs). For the language model, we used a 5- gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) trained on the English side of our training data as well as portions of the Gigaword v2 English corpus. We used the NIST MT03 test set as the development set for optimizing interpolation weights using minimum error rate training (MERT; (Och and Ney, 2002)). We carried out evaluation of the systems on the NIST 2006 evaluation test (MT06) and the NIST 2008 evaluation test (MT08). We segmented Chinese as a preprocessing step using the Harbin segmenter (Zhao et al., 2001). Arabic-to-English experiments. We trained the system on a subset of 950K sentence pairs from the NIST MT08 training data, sel</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backingoff for m-gram language modeling. In Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing95, pages 181–184, Detroit, MI, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Alberta, Canada,</location>
<contexts>
<context position="1385" citStr="Koehn et al., 2003" startWordPosition="176" endWordPosition="179">ling, we address this problem by observing the influential role of function words in determining syntactic structure, and introducing soft constraints on function word relationships as part of a standard log-linear hierarchical phrase-based model. Experimentation on Chinese-English and Arabic-English translation demonstrates that the approach yields significant gains in performance. 1 Introduction Hierarchical phrase-based models (Chiang, 2005; Chiang, 2007) offer a number of attractive benefits in statistical machine translation (SMT), while maintaining the strengths of phrase-based systems (Koehn et al., 2003). The most important of these is the ability to model long-distance reordering efficiently. To model such a reordering, a hierarchical phrase-based system demands no additional parameters, since long and short distance reorderings are modeled identically using synchronous context free grammar (SCFG) rules. The same rule, depending on its topological ordering – i.e. its position in the hierarchical structure – can affect both short and long spans of text. Interestingly, hierarchical phrase-based models provide this benefit without making any linguistic commitments beyond the structure of the mo</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 127–133, Edmonton, Alberta, Canada, May. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="22239" citStr="Koehn, 2004" startWordPosition="3675" endWordPosition="3676">t (MT08). Arabic source text was preprocessed by separating clitics, the definiteness marker, and the future tense marker from their stems. 7 Experimental Results Figure 4: Illustrations for: a) the leftFirst value, and b) the dontCare value. Thickly bordered boxes are MCAs of the function words while solid circles are the alignment points of the function words. The gray boxes are the intersections of the two MCAs. all experiments, we report performance using the BLEU score (Papineni et al., 2002), and we assess statistical significance using the standard bootstrapping approach introduced by (Koehn, 2004). Chinese-to-English experiments. We trained the system on the NIST MT06 Eval corpus excluding the UN data (approximately 900K sentence pairs). For the language model, we used a 5- gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) trained on the English side of our training data as well as portions of the Gigaword v2 English corpus. We used the NIST MT03 test set as the development set for optimizing interpolation weights using minimum error rate training (MERT; (Och and Ney, 2002)). We carried out evaluation of the systems on the NIST 2006 evaluation test (MT06) and the NIS</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft syntactic constraints for hierarchical phrased-based translation.</title>
<date>2008</date>
<booktitle>In Proceedings of The 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1003--1011</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2846" citStr="Marton and Resnik (2008)" startWordPosition="396" endWordPosition="400">onterminal symbol&apos;. As a result, the system is susceptible to the overgeneration problem: the grammar may suggest more reordering choices than appropriate, and many of those choices lead to ungrammatical translations. Chiang (2005) hypothesized that incorrect reordering choices would often correspond to hierarchical phrases that violate syntactic boundaries in the source language, and he explored the use of a “constituent feature” intended to reward the application of hierarchical phrases which respect source language syntactic categories. Although this did not yield significant improvements, Marton and Resnik (2008) and Chiang et al. (2008) extended this approach by introducing soft syntactic constraints similar to the constituent feature, but more fine-grained and sensitive to distinctions among syntactic categories; these led to substantial improvements in performance. Zollman et al. (2006)took a complementary approach, constraining the application of hierarchical rules to respect syntactic boundaries in the target language syntax. Whether the focus is on constraints from the source language or the target language, the main ingredient in both previous approaches is the idea of constraining the spans of</context>
<context position="30538" citStr="Marton and Resnik (2008)" startWordPosition="5020" endWordPosition="5023">el to include content categories. Verbs and many nouns have subcategorization properties that may influence phrase ordering, for example, and this may turn out to explain the increase in Arabic-English performance for N = 2048 using the MT08 test set. More generally, the approach we are taking can be viewed as a way of selectively lexicalizing the automatically extracted grammar, and there is a large range of potentially interesting choices in how such lexicalization could be done. 9 Related Work In the introduction, we discussed Chiang&apos;s (2005) constituency feature, related ideas explored by Marton and Resnik (2008) and Chiang et al. (2008), and the target-side variation investigated by Zollman et al. (2006). These methods differ from each other mainly in terms of the specific lin9We plan to do corresponding experimentation and analysis for Arabic once we identify a suitable list of manually identified function words. guistic knowledge being used and on which side the constraints are applied. Shen et al. (2008) proposed to use linguistic knowledge expressed in terms of a dependency grammar, instead of a syntactic constituency grammar. Villar et al. (2008) attempted to use syntactic constituency on both t</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In Proceedings of The 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1003– 1011, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="22744" citStr="Och and Ney, 2002" startWordPosition="3760" endWordPosition="3763">02), and we assess statistical significance using the standard bootstrapping approach introduced by (Koehn, 2004). Chinese-to-English experiments. We trained the system on the NIST MT06 Eval corpus excluding the UN data (approximately 900K sentence pairs). For the language model, we used a 5- gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) trained on the English side of our training data as well as portions of the Gigaword v2 English corpus. We used the NIST MT03 test set as the development set for optimizing interpolation weights using minimum error rate training (MERT; (Och and Ney, 2002)). We carried out evaluation of the systems on the NIST 2006 evaluation test (MT06) and the NIST 2008 evaluation test (MT08). We segmented Chinese as a preprocessing step using the Harbin segmenter (Zhao et al., 2001). Arabic-to-English experiments. We trained the system on a subset of 950K sentence pairs from the NIST MT08 training data, selected by Chinese-to-English experiments. Table 2 summarizes the results of our Chinese-to-English experiments. These results confirm that the pairwise dominance model can significantly increase performance as measured by the BLEU score, with a consistent p</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 295–302, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="18150" citStr="Och and Ney, 2004" startWordPosition="2967" endWordPosition="2970">n the training bitext. Such statistics are not directly observable in parallel corpora, so estimation is needed. Our estimation method is based on two facts: (1) the topological ordering of hierarchical phrases is tightly coupled with the span of the hierarchical phrases, and (2) the span of a hierarchical phrase at a higher level is always a superset of the span of all other hierarchical phrases at the lower level of its substructure. Thus, to establish soft estimates of dominance counts, we utilize alignment information available in the rule together with the consistent alignment heuristic (Och and Ney, 2004) traditionally used to guess phrase alignments. Specifically, we define the span of a function word as a maximal, consistent alignment in the source language that either starts from or ends with the function word. (Requiring that spans be maximal ensures their uniqueness.) We will refer to such spans as Maximal Consistent Alignments (MCA). Note that each function word has two such Maximal Consistent Alignments: one that ends with the function word (MCAR)and another that starts from the function word (MCAL). Y 0 Y 00 left- right- dont- neiFirst First Care ther fn (and) 4 (are) 0.11 0.16 0.68 0.</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="22129" citStr="Papineni et al., 2002" startWordPosition="3658" endWordPosition="3661">ERT. We carried out the evaluation of the systems on the NIST 2006 evaluation set (MT06) and the NIST 2008 evaluation set (MT08). Arabic source text was preprocessed by separating clitics, the definiteness marker, and the future tense marker from their stems. 7 Experimental Results Figure 4: Illustrations for: a) the leftFirst value, and b) the dontCare value. Thickly bordered boxes are MCAs of the function words while solid circles are the alignment points of the function words. The gray boxes are the intersections of the two MCAs. all experiments, we report performance using the BLEU score (Papineni et al., 2002), and we assess statistical significance using the standard bootstrapping approach introduced by (Koehn, 2004). Chinese-to-English experiments. We trained the system on the NIST MT06 Eval corpus excluding the UN data (approximately 900K sentence pairs). For the language model, we used a 5- gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) trained on the English side of our training data as well as portions of the Gigaword v2 English corpus. We used the NIST MT03 test set as the development set for optimizing interpolation weights using minimum error rate training (MERT; (Och</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendra Setiawan</author>
<author>Min-Yen Kan</author>
<author>Haizhou Li</author>
</authors>
<title>Ordering phrases with function words.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>712--719</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5211" citStr="Setiawan et al., 2007" startWordPosition="758" endWordPosition="761"> isolation. In addition, our proposal requires no monolingual parsing or linguistically informed syntactic modeling for either the source or target language. The key to our approach is the observation that we can approximate the topological ordering of hierarchical phrases via the topological ordering of function words. We introduce a statistical reordering model that we call the pairwise dominance model, which characterizes reorderings of phrases around a pair of function words. In modeling function words, our model can be viewed as a successor to the function words-centric reordering model (Setiawan et al., 2007), expanding on the previous approach by modeling pairs of function words rather than individual function words in isolation. The rest of the paper is organized as follows. In Section 2, we briefly review hierarchical phrasebased models. In Section 3, we first describe the overgeneration problem in more detail with a concrete example, and then motivate our idea of using the topological ordering of function words to address the problem. In Section 4, we develop our idea by introducing the pairwise dominance model, expressing function word relationships in terms of what we call the the dominance </context>
<context position="20232" citStr="Setiawan et al. (2007)" startWordPosition="3324" endWordPosition="3327">tersection, respectively. Once all the d values are counted, the pairwise dominance model of neighboring function words can be estimated simply from counts using maximum likelihood. Table 1 illustrates estimated dominance values that correctly resolve the topological ordering for our running example. 6 Experimental Setup We tested the effect of introducing the pairwise dominance model into hierarchical phrase-based translation on Chinese-to-English and Arabic-toEnglish translation tasks, thus studying its effect in two languages where the use of function words differs significantly. Following Setiawan et al. (2007), we identify function words as the N most frequent words in the corpus, rather than identifying them according to linguistic criteria; this approximation removes the need for any additional language-specific resources. We report results for N = 32, 64,128, 256, 512,1024, 2048.7 For 7 W observe that even N = 2048 represents less than 1.5% and 0.8% of the words in the Chinese and Arabic vocabularies, respectively. The validity of the frequency-based strategy, relative to linguistically-defined function words, is discussed in Section 8 328 C SIL fn � q C 3M SIL ♥a computers and cell phones are i</context>
</contexts>
<marker>Setiawan, Kan, Li, 2007</marker>
<rawString>Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007. Ordering phrases with function words. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 712– 719, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of The 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>577--585</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="30941" citStr="Shen et al. (2008)" startWordPosition="5086" endWordPosition="5089"> of potentially interesting choices in how such lexicalization could be done. 9 Related Work In the introduction, we discussed Chiang&apos;s (2005) constituency feature, related ideas explored by Marton and Resnik (2008) and Chiang et al. (2008), and the target-side variation investigated by Zollman et al. (2006). These methods differ from each other mainly in terms of the specific lin9We plan to do corresponding experimentation and analysis for Arabic once we identify a suitable list of manually identified function words. guistic knowledge being used and on which side the constraints are applied. Shen et al. (2008) proposed to use linguistic knowledge expressed in terms of a dependency grammar, instead of a syntactic constituency grammar. Villar et al. (2008) attempted to use syntactic constituency on both the source and target languages in the same spirit as the constituency feature, along with some simple patternbased heuristics – an approach also investigated by Iglesias et al. (2009). Aiming at improving the selection of derivations, Zhou et al. (2008) proposed prior derivation models utilizing syntactic annotation of the source language, which can be seen as smoothing the probabilities of hierarchi</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of The 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 577–585, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Hermann Ney</author>
</authors>
<title>Analysing soft syntax features and heuristics for hierarchical phrase based machine translation. International Workshop on Spoken Language Translation</title>
<date>2008</date>
<pages>190--197</pages>
<marker>Vilar, Stein, Ney, 2008</marker>
<rawString>David Vilar, Daniel Stein, and Hermann Ney. 2008. Analysing soft syntax features and heuristics for hierarchical phrase based machine translation. International Workshop on Spoken Language Translation 2008, pages 190–197, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="8417" citStr="Wu, 1997" startWordPosition="1296" endWordPosition="1297">h for the most probable derivation D* whose source side is equal to e: D* = argmax P(D),where source(D)=e. D = Xi, i E 1...|D |is a set of rules following a certain topological ordering, indicated here by the use of the superscript. 3 Overgeneration and Topological Ordering of Function Words The use of only one type of nonterminal allows a flexible permutation of the topological ordering of the same set of rules, resulting in a huge number of possible derivations from a given source sentence. In that respect, the overgeneration problem is not new to SMT: Bracketing Transduction Grammar (BTG) (Wu, 1997) uses a single type of nonterminal and is subject to overgeneration problems, as well.� Note, however, that overgeneration in BTG can be viewed as a feature, not a bug, since the formalism was origi325 The problem may be less severe in hierarchical phrase-based MT than in BTG, since lexical items on the rules&apos; right hand sides often limit the span of nonterminals. Nonetheless overgeneration of reorderings is still problematic, as we illustrate using the hypothetical Chinese-to-English example in Fig. 1. Suppose we want to translate the Chinese sentence in Fig. 1 into English using the followin</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiejun Zhao</author>
<author>Yajuan Lv</author>
<author>Jianmin Yao</author>
<author>Hao Yu</author>
<author>Muyun Yang</author>
<author>Fang Liu</author>
</authors>
<title>Increasing accuracy of chinese segmentation with strategy of multi-step processing.</title>
<date>2001</date>
<journal>Journal of Chinese Information Processing (Chinese Version),</journal>
<pages>1--13</pages>
<contexts>
<context position="22961" citStr="Zhao et al., 2001" startWordPosition="3798" endWordPosition="3801">ta (approximately 900K sentence pairs). For the language model, we used a 5- gram model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) trained on the English side of our training data as well as portions of the Gigaword v2 English corpus. We used the NIST MT03 test set as the development set for optimizing interpolation weights using minimum error rate training (MERT; (Och and Ney, 2002)). We carried out evaluation of the systems on the NIST 2006 evaluation test (MT06) and the NIST 2008 evaluation test (MT08). We segmented Chinese as a preprocessing step using the Harbin segmenter (Zhao et al., 2001). Arabic-to-English experiments. We trained the system on a subset of 950K sentence pairs from the NIST MT08 training data, selected by Chinese-to-English experiments. Table 2 summarizes the results of our Chinese-to-English experiments. These results confirm that the pairwise dominance model can significantly increase performance as measured by the BLEU score, with a consistent pattern of results across the MT06 and MT08 test sets. Modeling N = 32 drops the performance marginally below baseline, suggesting that perhaps there are not enough words for the pairwise dominance model to work with. </context>
</contexts>
<marker>Zhao, Lv, Yao, Yu, Yang, Liu, 2001</marker>
<rawString>Tiejun Zhao, Yajuan Lv, Jianmin Yao, Hao Yu, Muyun Yang, and Fang Liu. 2001. Increasing accuracy of chinese segmentation with strategy of multi-step processing. Journal of Chinese Information Processing (Chinese Version), 1:13–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bowen Zhou</author>
<author>Bing Xiang</author>
<author>Xiaodan Zhu</author>
<author>Yuqing Gao</author>
</authors>
<title>Prior derivation models for formally syntax-based translation using linguistically syntactic parsing and tree kernels.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2),</booktitle>
<pages>pages</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="31391" citStr="Zhou et al. (2008)" startWordPosition="5161" endWordPosition="5164">c once we identify a suitable list of manually identified function words. guistic knowledge being used and on which side the constraints are applied. Shen et al. (2008) proposed to use linguistic knowledge expressed in terms of a dependency grammar, instead of a syntactic constituency grammar. Villar et al. (2008) attempted to use syntactic constituency on both the source and target languages in the same spirit as the constituency feature, along with some simple patternbased heuristics – an approach also investigated by Iglesias et al. (2009). Aiming at improving the selection of derivations, Zhou et al. (2008) proposed prior derivation models utilizing syntactic annotation of the source language, which can be seen as smoothing the probabilities of hierarchical phrase features. A key point is that the model we have introduced in this paper does not require the linguistic supervision needed in most of this prior work. We estimate the parameters of our model from parallel text without any linguistic annotation. That said, we would emphasize that our approach is, in fact, motivated in linguistic terms by the role of function words in natural language syntax. 10 Conclusion We have presented a pairwise d</context>
</contexts>
<marker>Zhou, Xiang, Zhu, Gao, 2008</marker>
<rawString>Bowen Zhou, Bing Xiang, Xiaodan Zhu, and Yuqing Gao. 2008. Prior derivation models for formally syntax-based translation using linguistically syntactic parsing and tree kernels. In Proceedings of the ACL-08: HLT Second Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 19–27, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>138--141</pages>
<location>New York City,</location>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings on the Workshop on Statistical Machine Translation, pages 138–141, New York City, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>