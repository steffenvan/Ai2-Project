<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.9966285">
Learning Word Senses With Feature Selection and Order Identification
Capabilities
</title>
<author confidence="0.981825">
Zheng-Yu Niu, Dong-Hong Ji
</author>
<affiliation confidence="0.946547">
Institute for Infocomm Research
</affiliation>
<address confidence="0.859471">
21 Heng Mui Keng Terrace
119613 Singapore
</address>
<email confidence="0.991711">
{zniu, dhji}@i2r.a-star.edu.sg
</email>
<author confidence="0.957719">
Chew-Lim Tan
</author>
<affiliation confidence="0.901485333333333">
Department of Computer Science
National University of Singapore
3 Science Drive 2
</affiliation>
<address confidence="0.7058">
117543 Singapore
</address>
<email confidence="0.994858">
tancl@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.9947" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999366">
This paper presents an unsupervised word sense
learning algorithm, which induces senses of target
word by grouping its occurrences into a “natural”
number of clusters based on the similarity of their
contexts. For removing noisy words in feature set,
feature selection is conducted by optimizing a clus-
ter validation criterion subject to some constraint in
an unsupervised manner. Gaussian mixture model
and Minimum Description Length criterion are used
to estimate cluster structure and cluster number.
Experimental results show that our algorithm can
find important feature subset, estimate model or-
der (cluster number) and achieve better performance
than another algorithm which requires cluster num-
ber to be provided.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982527272728">
Sense disambiguation is essential for many lan-
guage applications such as machine translation, in-
formation retrieval, and speech processing (Ide and
V´eronis, 1998). Almost all of sense disambigua-
tion methods are heavily dependant on manually
compiled lexical resources. However these lexical
resources often miss domain specific word senses,
even many new words are not included inside.
Learning word senses from free text will help us
dispense of outside knowledge source for defining
sense by only discriminating senses of words. An-
other application of word sense learning is to help
enriching or even constructing semantic lexicons
(Widdows, 2003).
The solution of word sense learning is closely re-
lated to the interpretation of word senses. Different
interpretations of word senses result in different so-
lutions to word sense learning.
One interpretation strategy is to treat a word sense
as a set of synonyms like synset in WordNet. The
committee based word sense discovery algorithm
(Pantel and Lin, 2002) followed this strategy, which
treated senses as clusters of words occurring in sim-
ilar contexts. Their algorithm initially discovered
tight clusters called committees by grouping top
n words similar with target word using average-
link clustering. Then the target word was assigned
to committees if the similarity between them was
above a given threshold. Each committee that the
target word belonged to was interpreted as one of
its senses.
There are two difficulties with this committee
based sense learning. The first difficulty is about
derivation of feature vectors. A feature for target
word here consists of a contextual content word and
its grammatical relationship with target word. Ac-
quisition of grammatical relationship depends on
the output of a syntactic parser. But for some lan-
guages, ex. Chinese, the performance of syntactic
parsing is still a problem. The second difficulty with
this solution is that two parameters are required to
be provided, which control the number of commit-
tees and the number of senses of target word.
Another interpretation strategy is to treat a word
sense as a group of similar contexts of target word.
The context group discrimination (CGD) algorithm
presented in (Sch¨utze, 1998) adopted this strategy.
Firstly, their algorithm selected important contex-
tual words using x2 or local frequency criterion.
With the x2 based criterion, those contextual words
whose occurrence depended on whether the am-
biguous word occurred were chosen as features.
When using local frequency criterion, their algo-
rithm selected top n most frequent contextual words
as features. Then each context of occurrences of
target word was represented by second order co-
occurrence based context vector. Singular value de-
composition (SVD) was conducted to reduce the di-
mensionality of context vectors. Then the reduced
context vectors were grouped into a pre-defined
number of clusters whose centroids corresponded to
senses of target word.
Some observations can be made about their fea-
ture selection and clustering procedure. One ob-
servation is that their feature selection uses only
first order information although the second order co-
occurrence data is available. The other observation
is about their clustering procedure. Similar with
committee based sense discovery algorithm, their
clustering procedure also requires the predefinition
of cluster number. Their method can capture both
coarse-gained and fine-grained sense distinction as
the predefined cluster number varies. But from a
point of statistical view, there should exist a parti-
tioning of data at which the most reliable, “natural”
sense clusters appear.
In this paper, we follow the second order repre-
sentation method for contexts of target word, since
it is supposed to be less sparse and more robust than
first order information (Sch¨utze, 1998). We intro-
duce a cluster validation based unsupervised fea-
ture wrapper to remove noises in contextual words,
which works by measuring the consistency between
cluster structures estimated from disjoint data sub-
sets in selected feature space. It is based on the
assumption that if selected feature subset is impor-
tant and complete, cluster structure estimated from
data subset in this feature space should be stable
and robust against random sampling. After deter-
mination of important contextual words, we use a
Gaussian mixture model (GMM) based clustering
algorithm (Bouman et al., 1998) to estimate cluster
structure and cluster number by minimizing Min-
imum Description Length (MDL) criterion (Ris-
sanen, 1978). We construct several subsets from
widely used benchmark corpus as test data. Experi-
mental results show that our algorithm (FSGMM)
can find important feature subset, estimate cluster
number and achieve better performance compared
with CGD algorithm.
This paper is organized as follows. In section
2 we will introduce our word sense learning al-
gorithm, which incorporates unsupervised feature
selection and model order identification technique.
Then we will give out the experimental results of
our algorithm and discuss some findings from these
results in section 3. Section 4 will be devoted to
a brief review of related efforts on word sense dis-
crimination. In section 5 we will conclude our work
and suggest some possible improvements.
</bodyText>
<sectionHeader confidence="0.879666" genericHeader="introduction">
2 Learning Procedure
</sectionHeader>
<subsectionHeader confidence="0.979397">
2.1 Feature selection
</subsectionHeader>
<bodyText confidence="0.999964157894737">
Feature selection for word sense learning is to find
important contextual words which help to discrim-
inate senses of target word without using class la-
bels in data set. This problem can be generalized
as selecting important feature subset in an unsuper-
vised manner. Many unsupervised feature selection
algorithms have been presented, which can be cate-
gorized as feature filter (Dash et al., 2002; Talav-
era, 1999) and feature wrapper (Dy and Brodley,
2000; Law et al., 2002; Mitra et al., 2002; Modha
and Spangler, 2003).
In this paper we propose a cluster valida-
tion based unsupervised feature subset evaluation
method. Cluster validation has been used to solve
model order identification problem (Lange et al.,
2002; Levine and Domany, 2001). Table 1 gives
out our feature subset evaluation algorithm. If some
features in feature subset are noises, the estimated
cluster structure on data subset in selected feature
space is not stable, which is more likely to be the
artifact of random splitting. Then the consistency
between cluster structures estimated from disjoint
data subsets will be lower. Otherwise the estimated
cluster structures should be more consistent. Here
we assume that splitting does not eliminate some of
the underlying modes in data set.
For comparison of different clustering structures,
predictors are constructed based on these clustering
solutions, then we use these predictors to classify
the same data subset. The agreement between class
memberships computed by different predictors can
be used as the measure of consistency between clus-
ter structures. We use the stability measure (Lange
et al., 2002) (given in Table 1) to assess the agree-
ment between class memberships.
For each occurrence, one strategy is to construct
its second order context vector by summing the vec-
tors of contextual words, then let the feature selec-
tion procedure start to work on these second order
contextual vectors to select features. However, since
the sense associated with a word’s occurrence is al-
ways determined by very few feature words in its
contexts, it is always the case that there exist more
noisy words than the real features in the contexts.
So, simply summing the contextual word’s vectors
together may result in noise-dominated second or-
der context vectors.
To deal with this problem, we extend the feature
selection procedure further to the construction of
second order context vectors: to select better feature
words in contexts to construct better second order
context vectors enabling better feature selection.
Since the sense associated with a word’s occur-
rence is always determined by some feature words
in its contexts, it is reasonable to suppose that the
selected features should cover most of occurrences.
Formally, let coverage(D,T) be the coverage rate
of the feature set T with respect to a set of con-
texts D, i.e., the ratio of the number of the occur-
rences with at least one feature in their local con-
texts against the total number of occurrences, then
we assume that coverage(D,T) ≥ τ. In practice,
we set τ = 0.9.
This assumption also helps to avoid the bias to-
ward the selection of fewer features, since with
fewer features, there are more occurrences without
features in contexts, and their context vectors will
be zero valued, which tends to result in more stable
cluster structure.
Let D be a set of local contexts of occurrences of
target word, then D = {di}Ni=1, where di represents
local context of the i-th occurrence, and N is the
total number of this word’s occurrences.
W is used to denote bag of words occurring in
context set D, then W = {wi}Mi=1, where wi de-
notes a word occurring in D, and M is the total
number of different contextual words.
Let V denote a M x M second-order co-
occurrence symmetric matrix. Suppose that the i-th
, 1 &lt; i &lt; M, row in the second order matrix corre-
sponds to word wi and the j-th , 1 &lt; j &lt; M, col-
umn corresponds to word wj, then the entry speci-
fied by i-th row and j-th column records the number
of times that word wi occurs close to wj in corpus.
We use v(wi) to represent the word vector of con-
textual word wi, which is the i-th row in matrix V .
HT is a weight matrix of contextual word subset
T, T C_ W. Then each entry hi,j represents the
weight of word wj in di, wj E T, 1 &lt; i &lt; N. We
use binary term weighting method to derive context
vectors: hi,j = 1 if word wj occurs in di, otherwise
zero.
Let CT = {cTi }Ni=1 be a set of context vectors in
feature space T, where cT i is the context vector of
the i-th occurrence. cTi is defined as:
</bodyText>
<equation confidence="0.606265">
T=Eci (hi,jv(wj)), wj E T, 1 &lt; i &lt; N. (1)
j
</equation>
<bodyText confidence="0.9132795">
The feature subset selection in word set W can be
formulated as:
</bodyText>
<equation confidence="0.995272">
Tˆ = arg maxfcriterion(T, H, V, q)1, T C W, (2)
T
</equation>
<bodyText confidence="0.999104923076923">
subject to coverage(D, T) &gt; τ, where Tˆ is the op-
timal feature subset, criterion is the cluster valida-
tion based evaluation function (the function in Ta-
ble 1), q is the resampling frequency for estimate
of stability, and coverage(D,T) is the proportion
of contexts with occurrences of features in T. This
constrained optimization results in a solution which
maximizes the criterion and meets the given con-
straint at the same time. In this paper we use se-
quential greedy forward floating search (Pudil et al.,
1994) in sorted word list based on χ2 or local fre-
quency criterion. We set l = 1, m = 1, where l is
plus step, and m is take-away step.
</bodyText>
<subsectionHeader confidence="0.999827">
2.2 Clustering with order identification
</subsectionHeader>
<bodyText confidence="0.995116">
After feature selection, we employ a Gaussian mix-
ture modelling algorithm, Cluster (Bouman et al.,
</bodyText>
<tableCaption confidence="0.7780695">
Table 1: Unsupervised Feature Subset Evaluation Algorithm.
Intuitively, for a given feature subset T, we iteratively split data
set into disjoint halves, and compute the agreement of cluster-
ing solutions estimated from these sets using stability measure.
The average of stability over q resampling is the estimation of
the score of T.
</tableCaption>
<construct confidence="0.4838845">
Function criterion(T, H, V , q)
Input parameter: feature subset T, weight matrix H,
second order co-occurrence matrix V , resampling
frequency q;
</construct>
<listItem confidence="0.954730166666667">
(1) ST = 0;
(2) For i = 1 to q do
(2.1) Randomly split CT into disjoint halves, denoted
as CTA and CTB;
(2.2) Estimate GMM parameter and cluster number on CTA
using Cluster, and the parameter set is denoted as ˆBA;
</listItem>
<figure confidence="0.4402233">
The solution ˆBA can be used to construct a predictor
PA;
(2.3) Estimate GMM parameter and cluster number on CTB
ˆBB,
The solution ˆBB can be used to construct a predictor
PB;
(2.4) Classify CTB using PA and PB;
The class labels assigned by PA and PB are denoted
as LA and LB;
(2.5) ST+ = maxπ CTB|�i 1 f it(LA(cTBi)) = LB(cTBi)1,
</figure>
<bodyText confidence="0.875739">
where it denotes possible permutation relating indices
between LA and LB, and cTBi E CTB;
</bodyText>
<listItem confidence="0.994615">
(3) ST = q1ST;
(4) Return ST;
</listItem>
<bodyText confidence="0.999709818181818">
1998), to estimate cluster structure and cluster num-
ber. Let Y = {yn}Nn=1 be a set of M dimen-
sional vectors to be modelled by GMM. Assuming
that this model has K subclasses, let Irk denote the
prior probability of subclass k, µk denote the M di-
mensional mean vector for subclass k, Rk denote
the M x M dimensional covariance matrix for sub-
class k, 1 &lt; k &lt; K. The subclass label for pixel
yn is represented by xn. MDL criterion is used
for GMM parameter estimation and order identifi-
cation, which is given by:
</bodyText>
<equation confidence="0.999498333333333">
1
log (Pyn|xn(yn|Θ)) + L log (NM),
2
(3)
Pyn|xn(yn|k, B)itk, (4)
L = K(1 + M + (M +2 1)M) − 1, (5)
</equation>
<bodyText confidence="0.977383666666667">
The log likelihood measures the goodness of fit of
a model to data sample, while the second term pe-
nalizes complex model. This estimator works by at-
tempting to find a model order with minimum code
length to describe the data sample Y and parameter
set O.
If the cluster number is fixed, the estimation of
GMM parameter can be solved using EM algorithm
using Cluster, and the parameter set is denoted as
</bodyText>
<equation confidence="0.998019375">
N
E
n=1
MDL(K, B) = −
K
E
k=1
Pyn|xn(yn|Θ) =
</equation>
<bodyText confidence="0.998306333333333">
to address this type of incomplete data problem
(Dempster et al., 1977). The initialization of mix-
ture parameter 0M is given by:
</bodyText>
<equation confidence="0.998912888888889">
1
πk =
(1) (6)
Ko
µk = yn, where n = L(k − 1)(N − 1)/(Ko − 1)� + 1 (7)
(1)
R(1)
k = N 1 ΣN n=1ynyt (8)
n
</equation>
<bodyText confidence="0.9449814">
Ko is a given initial subclass number.
Then EM algorithm is used to estimate model pa-
rameters by minimizing MDL:
E-step: re-estimate the expectations based on pre-
vious iteration:
</bodyText>
<equation confidence="0.990492333333333">
pxn|yn(k|yn, θ(i)) = pyn|xn(yn|k, θ(i))πk
PK , (9)
l=1(pyn|xn(yn|l, θ(i))πl)
</equation>
<bodyText confidence="0.96521">
M-step: estimate the model parameter 0(i) to
maximize the log-likelihood in MDL:
</bodyText>
<equation confidence="0.982463285714286">
pxn|yn(k|yn, θ(i)) (10)
πk = Nk (11)
N
ynpxn|yn(k|yn, θ(i)) (12)
(yn − µk)(yn − µk)tpxn|yn(k|yn, θ(i))
(13)
pyn|xn(yn|k, θ(i)) = 1
(2π)M/2 |Rk|−1/2 exp{λ} (14)
λ = − 2 (yn − µk)tR−1
1 k (yn − µk) (15)
The EM iteration is terminated when the change
of MDL(K, 0) is less than E:
100 (1 + M + (M + 1)M
1 2 )log(NM) (16)
</equation>
<bodyText confidence="0.999921416666667">
For inferring the cluster number, EM algorithm
is applied for each value of K, 1 G K G Ko, and
the value Kˆ which minimizes the value of MDL
is chosen as the correct cluster number. To make
this process more efficient, two cluster pair l and m
are selected to minimize the change in MDL crite-
ria when reducing K to K − 1. These two clusters
l and m are then merged. The resulting parameter
set is chosen as an initial condition for EM iteration
with K − 1 subclasses. This operation will avoid a
complete minimization with respect to π, µ, and R
for each value of K.
</bodyText>
<tableCaption confidence="0.910607">
Table 2: Four ambiguous words, their senses and frequency
</tableCaption>
<table confidence="0.946253047619048">
distribution of each sense.
Word Sense Percentage
hard not easy (difficult) 82.8%
(adjective) not soft (metaphoric) 9.6%
not soft (physical) 7.6%
interest money paid for the use of money 52.4%
a share in a company or business 20.4%
readiness to give attention 14%
advantage, advancement or favor 9.4%
activity that one gives attention to 3.6%
causing attention to be given to 0.2%
line product 56%
(noun) telephone connection 10.6%
written or spoken text 9.8%
cord 8.6%
division 8.2%
formation 6.8%
serve supply with food 42.6%
(verb) hold an office 33.6%
function as something 16%
provide a service 7.8%
</table>
<sectionHeader confidence="0.986593" genericHeader="background">
3 Experiments and Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999699">
3.1 Test data
</subsectionHeader>
<bodyText confidence="0.9999715">
We constructed four datasets from hand-tagged cor-
pus 1 by randomly selecting 500 instances for each
ambiguous word - “hard”, “interest”, “line”, and
“serve”. The details of these datasets are given in
Table 2. Our preprocessing included lowering the
upper case characters, ignoring all words that con-
tain digits or non alpha-numeric characters, remov-
ing words from a stop word list, and filtering out
low frequency words which appeared only once in
entire set. We did not use stemming procedure.
The sense tags were removed when they were used
by FSGMM and CGD. In evaluation procedure,
these sense tags were used as ground truth classes.
A second order co-occurrence matrix for English
words was constructed using English version of
Xinhua News (Jan. 1998-Dec. 1999). The win-
dow size for counting second order co-occurrence
was 50 words.
</bodyText>
<subsectionHeader confidence="0.996229">
3.2 Evaluation method for feature selection
</subsectionHeader>
<bodyText confidence="0.99991975">
For evaluation of feature selection, we used mutual
information between feature subset and class label
set to assess the importance of selected feature sub-
set. Our assessment measure is defined as:
</bodyText>
<equation confidence="0.9700185">
p(w, l)log p(w, l)
p(w)p(l), (17)
</equation>
<bodyText confidence="0.994525">
where T is the feature subset to be evaluated, T C
W, L is class label set, p(w, l) is the joint distri-
bution of two variables w and l, p(w) and p(l) are
marginal probabilities. p(w, l) is estimated based
</bodyText>
<footnote confidence="0.759446">
1http://www.d.umn.edu/∼tpederse/data.html
</footnote>
<equation confidence="0.994275375">
N
X
n=1
Nk =
1
Nk
µk =
XN
n=1
1
Rk =
Nk
N
X
n=1
2 =
X
1
M(T) =
|T
|
w∈T
X
l∈L
</equation>
<bodyText confidence="0.99996875">
on contingency table of contextual word set W and
class label set L. Intuitively, if M(T1) &gt; M(T2),
T1 is more important than T2 since T1 contains more
information about L.
</bodyText>
<subsectionHeader confidence="0.986706">
3.3 Evaluation method for clustering result
</subsectionHeader>
<bodyText confidence="0.952944882352941">
When assessing the agreement between clustering
result and hand-tagged senses (ground truth classes)
in benchmark data, we encountered the difficulty
that there was no sense tag for each cluster.
In (Lange et al., 2002), they defined a permu-
tation procedure for calculating the agreement be-
tween two cluster memberships assigned by differ-
ent unsupervised learners. In this paper, we applied
their method to assign different sense tags to only
min(|U|,|C|) clusters by maximizing the accuracy,
where |U |is the number of clusters, and |C |is the
number of ground truth classes. The underlying as-
sumption here is that each cluster is considered as
a class, and for any two clusters, they do not share
same class labels. At most |C |clusters are assigned
sense tags, since there are only |C |classes in bench-
mark data.
Given the contingency table Q between clusters
and ground truth classes, each entry Qj,j gives the
number of occurrences which fall into both the i-
th cluster and the j-th ground truth class. If |U |&lt;
|C|, we constructed empty clusters so that |U |=
|C|. Let Q represent a one-to-one mapping function
from C to U. It means that Q(j1) =6 Q(j2) if j1 =6
j2 and vice versa, 1 ≤ j1, j2 ≤ |C|. Then Q(j)
is the index of the cluster associated with the j-th
class. Searching a mapping function to maximize
the accuracy of U can be formulated as:
Top max(|W |×
20%, 100) words in contextual word list was se-
lected as features using frequency or k2 based rank-
ing. Then k-means clustering2 was performed on
context vector matrix using normalized Euclidean
distance. K-means clustering was repeated 5 times
</bodyText>
<footnote confidence="0.686032">
2We used k-means function in statistics toolbox ofMatlab.
</footnote>
<bodyText confidence="0.999376351851852">
and the partition with best quality was chosen as fi-
nal result. The number of clusters used by k-means
was set to be identical with the number of ground
truth classes. We tested CGDterm using various
word vector weighting methods when deriving con-
text vectors, ex. binary, idf, tf · idf.
CGDSV D: The context vector matrix was de-
rived using same method in CGDterm. Then k-
means clustering was conducted on latent seman-
tic space transformed from context vector matrix,
using normalized Euclidean distance. Specifically,
context vectors were reduced to 100 dimensions us-
ing SVD. If the dimension of context vector was
less than 100, all of latent semantic vectors with
non-zero eigenvalue were used for subsequent clus-
tering. We also tested it using different weighting
methods, ex. binary, idf, tf · idf.
F5GMM: We performed cluster validation
based feature selection in feature set used by CGD.
Then Cluster algorithm was used to group target
word’s instances using Euclidean distance measure.
τ was set as 0.90 in feature subset search procedure.
The random splitting frequency is set as 10 for es-
timation of the score of feature subset. The initial
subclass number was 20 and full covariance matrix
was used for parameter estimation of each subclass.
For investigating the effect of different context
window size on the performance of three proce-
dures, we tested these procedures using various con-
text window sizes: ±1, ±5, ±15, ±25, and all of
contextual words. The average length of sentences
in 4 datasets is 32 words before preprocessing. Per-
formance on each dataset was assessed by equation
19.
The scores of feature subsets selected by
F5GMM and CGD are listed in Table 3 and
4. The average accuracy of three procedures with
different feature ranking and weighting method is
given in Table 5. Each figure is the average over 5
different context window size and 4 datasets. We
give out the detailed results of these three proce-
dures in Figure 1. Several results should be noted
specifically:
From Table 3 and 4, we can find that F5GMM
achieved better score on mutual information (MI)
measure than CGD over 35 out of total 40 cases.
This is the evidence that our feature selection pro-
cedure can remove noise and retain important fea-
tures.
As it was shown in Table 5, with both k2 and
freq based feature ranking, F5GMM algorithm
performed better than CGDterm and CGDSV D if
we used average accuracy to evaluate their per-
formance. Specifically, with k2 based feature
</bodyText>
<equation confidence="0.99054125">
Ωˆ = arg max |C |QΩ(j),j. (18)
Ω L
j=1
Then the accuracy of solution U is given by
E
Accuracy(U) = E j QˆΩ(j),j
i,j Qi,j
. (19)
</equation>
<bodyText confidence="0.994492">
In fact, Ejj Qj ,j is equal to N, the number of
occurrences of target word in test set.
</bodyText>
<subsectionHeader confidence="0.641992">
3.4 Experiments and results
</subsectionHeader>
<bodyText confidence="0.997343649122807">
For each dataset, we tested following procedures:
CGDterm:We implemented the context group
discrimination algorithm.
ranking, FSGMM attained 55.4% average accu-
racy, while the best average accuracy of CGDterm
and CGDSV D were 40.9% and 51.3% respec-
tively. With freq based feature ranking, FSGMM
achieved 51.2% average accuracy, while the best av-
erage accuracy of CGDterm and CGDSV D were
45.1% and 50.2%.
The automatically estimated cluster numbers by
FSGMM over 4 datasets are given in Table 6.
The estimated cluster number was 2 - 4 for “hard”,
3 - 6 for “interest”, 3 - 6 for “line”, and 2 - 4
for “serve”. It is noted that the estimated cluster
number was less than the number of ground truth
classes in most cases. There are some reasons for
this phenomenon. First, the data is not balanced,
which may lead to that some important features can-
not be retrieved. For example, the fourth sense of
“serve”, and the sixth sense of “line”, their corre-
sponding features are not up to the selection criteria.
Second, some senses can not be distinguished using
only bag-of-words information, and their difference
lies in syntactic information held by features. For
example, the third sense and the sixth sense of “in-
terest” may be distinguished by syntactic relation of
feature words, while the bag of feature words occur-
ring in their context are similar. Third, some senses
are determined by global topics, rather than local
contexts. For example, according to global topics, it
may be easier to distinguish the first and the second
sense of “interest”.
Figure 2 shows the average accuracy over three
procedures in Figure 1 as a function of context
window size for 4 datasets. For “hard”, the per-
formance dropped as window size increased, and
the best accuracy(77.0%) was achieved at win-
dow size 1. For “interest”, sense discrimination
did not benefit from large window size and the
best accuracy(40.1%) was achieved at window size
5. For “line”, accuracy dropped when increas-
ing window size and the best accuracy(50.2%) was
achieved at window size 1. For “serve”, the per-
formance benefitted from large window size and the
best accuracy(46.8%) was achieved at window size
15.
In (Leacock et al., 1998), they used Bayesian ap-
proach for sense disambiguation of three ambiguous
words, “hard”, “line”, and “serve”, based on cues
from topical and local context. They observed that
local context was more reliable than topical context
as an indicator of senses for this verb and adjective,
but slightly less reliable for this noun. Compared
with their conclusion, we can find that our result
is consistent with it for “hard”. But there is some
differences for verb “serve” and noun “line”. For
</bodyText>
<tableCaption confidence="0.9951045">
Table 3: Mutual information between feature subset and class
label with x2 based feature ranking.
</tableCaption>
<table confidence="0.9996292">
Word Cont. Size of MI Size of MI
wind. feature x10−2 feature x10−2
size subset subset
of CGD of
FSGMM
hard 1 18 6.4495 14 8.1070
5 100 0.4018 80 0.4300
15 100 0.1362 80 0.1416
25 133 0.0997 102 0.1003
all 145 0.0937 107 0.0890
interest 1 64 1.9697 55 2.0639
5 100 0.3234 89 0.3355
15 157 0.1558 124 0.1531
25 190 0.1230 138 0.1267
all 200 0.1163 140 0.1191
line 1 39 4.2089 32 4.6456
5 100 0.4628 84 0.4871
15 183 0.1488 128 0.1429
25 263 0.1016 163 0.0962
all 351 0.0730 192 0.0743
serve 1 22 6.8169 20 6.7043
5 100 0.5057 85 0.5227
15 188 0.2078 164 0.2094
25 255 0.1503 225 0.1536
all 320 0.1149 244 0.1260
</table>
<tableCaption confidence="0.9984075">
Table 4: Mutual information between feature subset and class
label with freq based feature ranking.
</tableCaption>
<table confidence="0.99928924">
Word Cont. Size of MI Size of MI
wind. feature x10−2 feature x10−2
size subset subset
of CGD of
FSGMM
hard 1 18 6.4495 14 8.1070
5 100 0.4194 80 0.4832
15 100 0.1647 80 0.1774
25 133 0.1150 102 0.1259
all 145 0.1064 107 0.1269
interest 1 64 1.9697 55 2.7051
5 100 0.6015 89 0.8309
15 157 0.2526 124 0.3495
25 190 0.1928 138 0.2982
all 200 0.1811 140 0.2699
line 1 39 4.2089 32 4.4606
5 100 0.6895 84 0.7816
15 183 0.2301 128 0.2929
25 263 0.1498 163 0.2181
all 351 0.1059 192 0.1630
serve 1 22 6.8169 20 7.0021
5 100 0.7045 85 0.8422
15 188 0.2763 164 0.3418
25 255 0.1901 225 0.2734
all 320 0.1490 244 0.2309
</table>
<bodyText confidence="0.997484166666667">
“serve”, the possible reason is that we do not use
position of local word and part of speech informa-
tion, which may deteriorate the performance when
local context(≤ 5 words) is used. For “line”, the
reason might come from the feature subset, which
is not good enough to provide improvement when
</bodyText>
<tableCaption confidence="0.916725">
Table 5: Average accuracy of three procedures with various
</tableCaption>
<table confidence="0.938613055555556">
settings over 4 datasets.
Algorithm Feature Feature Average
ranking weighting accuracy
method method
FSGMM x2 binary 0.554
CGDterm x2 binary 0.404
CGDterm x2 idf 0.407
CGDterm x2 tf · idf 0.409
CGDSVD x2 binary 0.513
CGDSVD x2 idf 0.512
CGDSVD x2 tf · idf 0.508
FSGMM freq binary 0.512
CGDterm freq binary 0.451
CGDterm freq idf 0.437
CGDterm freq tf · idf 0.447
CGDSVD freq binary 0.502
CGDSVD freq idf 0.498
CGDSVD freq tf · idf 0.485
</table>
<figure confidence="0.990228">
0.9
0.8
0.7
0.6
0.5
0.4
0 1 5 15 25 all
Hard dataset
0.7
0.6
0.5
0.4
0.3
0.2
0 1 5 15 25 all
Line dataset
0.6
0.5
0.4
0.3
0.2
0 1 5 15 25 all
Interest dataset
0 1 5 15 25 all
Serve dataset
0.55
0.35
0.6
0.5
0.4
0.3
.45
</figure>
<tableCaption confidence="0.932068">
Table 6: Automatically determined mixture component num-
</tableCaption>
<figure confidence="0.86367612">
ber.
Word Context Model Model
window order order
size with x2 with freq
hard 1 3 4
5 2 2
15 2 3
25 2 3
all 2 3
interest 1 5 4
5 3 4
15 4 6
25 4 6
all 3 4
line 1 5 6
5 4 3
15 5 4
25 5 4
all 3 4
serve 1 3 3
5 3 4
15 3 3
25 3 3
all 2 4
context window size is no less than 5.
</figure>
<sectionHeader confidence="0.99946" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999788363636364">
Besides the two works (Pantel and Lin, 2002;
Sch¨utze, 1998), there are other related efforts on
word sense discrimination (Dorow and Widdows,
2003; Fukumoto and Suzuki, 1999; Pedersen and
Bruce, 1997).
In (Pedersen and Bruce, 1997), they described an
experimental comparison of three clustering algo-
rithms for word sense discrimination. Their feature
sets included morphology of target word, part of
speech of contextual words, absence or presence of
particular contextual words, and collocation of fre-
</bodyText>
<figureCaption confidence="0.953330714285714">
Figure 1: Results for three procedures over 4 datases. The
horizontal axis corresponds to the context window size. Solid
line represents the result of FSGMM + binary, dashed line
denotes the result of CGDSV D + idf, and dotted line is the
result of CGDterm + idf. Square marker denotes x2 based
feature ranking, while cross marker denotes f req based feature
ranking.
</figureCaption>
<figure confidence="0.9609085">
r
A
</figure>
<figureCaption confidence="0.930933666666667">
Figure 2: Average accuracy over three procedures in Figure
1 as a function of context window size (horizontal axis) for 4
datasets.
</figureCaption>
<bodyText confidence="0.999010333333333">
quent words. Then occurrences of target word were
grouped into a pre-defined number of clusters. Sim-
ilar with many other algorithms, their algorithm also
required the cluster number to be provided.
In (Fukumoto and Suzuki, 1999), a term weight
learning algorithm was proposed for verb sense dis-
ambiguation, which can automatically extract nouns
co-occurring with verbs and identify the number of
senses of an ambiguous verb. The weakness of their
method is to assume that nouns co-occurring with
verbs are disambiguated in advance and the number
of senses of target verb is no less than two.
</bodyText>
<figure confidence="0.9960515">
0.8
0.75
0.7
0.65
0.6
0.55
0.5
0.45
0.4
0.35
0.3
0 1 5 15 25 all
Hard dataset
Interest dataset
Line dataset
Serve dataset
</figure>
<bodyText confidence="0.9999203">
The algorithm in (Dorow and Widdows, 2003)
represented target noun word, its neighbors and
their relationships using a graph in which each node
denoted a noun and two nodes had an edge between
them if they co-occurred with more than a given
number of times. Then senses of target word were
iteratively learned by clustering the local graph of
similar words around target word. Their algorithm
required a threshold as input, which controlled the
number of senses.
</bodyText>
<sectionHeader confidence="0.995254" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99998144">
Our word sense learning algorithm combined two
novel ingredients: feature selection and order iden-
tification. Feature selection was formalized as a
constrained optimization problem, the output of
which was a set of important features to determine
word senses. Both cluster structure and cluster num-
ber were estimated by minimizing a MDL crite-
rion. Experimental results showed that our algo-
rithm can retrieve important features, estimate clus-
ter number automatically, and achieve better per-
formance in terms of average accuracy than CGD
algorithm which required cluster number as input.
Our word sense learning algorithm is unsupervised
in two folds: no requirement of sense tagged data,
and no requirement of predefinition of sense num-
ber, which enables the automatic discovery of word
senses from free text.
In our algorithm, we treat bag of words in lo-
cal contexts as features. It has been shown that
local collocations and morphology of target word
play important roles in word sense disambiguation
or discrimination (Leacock et al., 1998; Widdows,
2003). It is necessary to incorporate these more
structural information to improve the performance
of word sense learning.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999713109589041">
Bouman, C. A., Shapiro, M., Cook, G. W., Atkins,
C. B., &amp; Cheng, H. (1998) Cluster: An
Unsupervsied Algorithm for Modeling Gaus-
sian Mixtures. http://dynamo.ecn.purdue.edu/
∼bouman/software/cluster/.
Dash, M., Choi, K., Scheuermann, P., &amp; Liu, H. (2002)
Feature Selection for Clustering - A Filter Solution.
Proc. of IEEE Int. Conf. on Data Mining(pp. 115–
122).
Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977)
Maximum likelihood from incomplete data using the
EM algorithm. Journal of the Royal Statistical Soci-
ety, 39(B).
Dorow, B, &amp; Widdows, D. (2003) Discovering Corpus-
Specific Word Senses. Proc. of the 10th Conf. of the
European Chapter of the Association for Computa-
tional Linguistics, Conference Companion (research
notes and demos)(pp.79–82).
Dy, J. G., &amp; Brodley, C. E. (2000) Feature Subset Selec-
tion and Order Identification for Unsupervised Learn-
ing. Proc. of the 17th Int. Conf. on Machine Learn-
ing(pp. 247–254).
Fukumoto, F., &amp; Suzuki, Y. (1999) Word Sense Disam-
biguation in Untagged Text Based on Term Weight
Learning. Proc. of the 9th Conf. of European Chapter
of the Association for Computational Linguistics(pp.
209–216).
Ide, N., &amp; V´eronis, J. (1998) Word Sense Disambigua-
tion: The State of the Art. Computational Linguistics,
24:1, 1–41.
Lange, T., Braun, M., Roth, V., &amp; Buhmann, J. M. (2002)
Stability-Based Model Selection. Advances in Neural
Information Processing Systems 15.
Law, M. H., Figueiredo, M., &amp; Jain, A. K. (2002) Fea-
ture Selection in Mixture-Based Clustering. Advances
in Neural Information Processing Systems 15.
Leacock, C., Chodorow, M., &amp; Miller A. G. (1998) Us-
ing Corpus Statistics and WordNet Relations for Sense
Identification. Computational Linguistics, 24:1, 147–
165.
Levine, E., &amp; Domany, E. (2001) Resampling Method
for Unsupervised Estimation of Cluster Validity. Neu-
ral Computation, Vol. 13, 2573–2593.
Mitra, P., Murthy, A. C., &amp; Pal, K. S. (2002) Unsu-
pervised Feature Selection Using Feature Similarity.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 24:4, 301–312.
Modha, D. S., &amp; Spangler, W. S. (2003) Feature Weight-
ing in k-Means Clustering. Machine Learning, 52:3,
217–237.
Pantel, P. &amp; Lin, D. K. (2002) Discovering Word Senses
from Text. Proc. of ACM SIGKDD Conf. on Knowl-
edge Discovery and Data Mining(pp. 613-619).
Pedersen, T., &amp; Bruce, R. (1997) Distinguishing Word
Senses in Untagged Text. Proceedings of the 2nd
Conference on Empirical Methods in Natural Lan-
guage Processing(pp. 197–207).
Pudil, P., Novovicova, J., &amp; Kittler, J. (1994) Floating
Search Methods in Feature Selection. Pattern Recog-
nigion Letters, Vol. 15, 1119-1125.
Rissanen, J. (1978) Modeling by Shortest Data Descrip-
tion. Automatica, Vol. 14, 465–471.
Sch¨utze, H. (1998) Automatic Word Sense Discrimina-
tion. Computational Linguistics, 24:1, 97–123.
Talavera, L. (1999) Feature Selection as a Preprocessing
Step for Hierarchical Clustering. Proc. of the 16th Int.
Conf. on Machine Learning(pp. 389–397).
Widdows, D. (2003) Unsupervised methods for devel-
oping taxonomies by combining syntactic and statisti-
cal information. Proc. of the Human Language Tech-
nology / Conference of the North American Chapter
of the Association for Computational Linguistics(pp.
276–283).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.526984">
<title confidence="0.998423">Learning Word Senses With Feature Selection and Order Identification Capabilities</title>
<author confidence="0.999142">Zheng-Yu Niu</author>
<author confidence="0.999142">Dong-Hong Ji</author>
<affiliation confidence="0.999978">Institute for Infocomm Research</affiliation>
<address confidence="0.977">21 Heng Mui Keng Terrace 119613 Singapore</address>
<author confidence="0.986813">Chew-Lim Tan</author>
<affiliation confidence="0.898286666666667">Department of Computer Science National University of Singapore 3 Science Drive 2</affiliation>
<address confidence="0.980205">117543 Singapore</address>
<email confidence="0.996331">tancl@comp.nus.edu.sg</email>
<abstract confidence="0.988815125">This paper presents an unsupervised word sense learning algorithm, which induces senses of target word by grouping its occurrences into a “natural” number of clusters based on the similarity of their contexts. For removing noisy words in feature set, feature selection is conducted by optimizing a cluster validation criterion subject to some constraint in an unsupervised manner. Gaussian mixture model and Minimum Description Length criterion are used to estimate cluster structure and cluster number. Experimental results show that our algorithm can find important feature subset, estimate model order (cluster number) and achieve better performance than another algorithm which requires cluster number to be provided.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C A Bouman</author>
<author>M Shapiro</author>
<author>G W Cook</author>
<author>C B Atkins</author>
<author>H Cheng</author>
</authors>
<title>Cluster: An Unsupervsied Algorithm for Modeling Gaussian Mixtures.</title>
<date>1998</date>
<note>http://dynamo.ecn.purdue.edu/ ∼bouman/software/cluster/.</note>
<contexts>
<context position="5521" citStr="Bouman et al., 1998" startWordPosition="834" endWordPosition="837">rder information (Sch¨utze, 1998). We introduce a cluster validation based unsupervised feature wrapper to remove noises in contextual words, which works by measuring the consistency between cluster structures estimated from disjoint data subsets in selected feature space. It is based on the assumption that if selected feature subset is important and complete, cluster structure estimated from data subset in this feature space should be stable and robust against random sampling. After determination of important contextual words, we use a Gaussian mixture model (GMM) based clustering algorithm (Bouman et al., 1998) to estimate cluster structure and cluster number by minimizing Minimum Description Length (MDL) criterion (Rissanen, 1978). We construct several subsets from widely used benchmark corpus as test data. Experimental results show that our algorithm (FSGMM) can find important feature subset, estimate cluster number and achieve better performance compared with CGD algorithm. This paper is organized as follows. In section 2 we will introduce our word sense learning algorithm, which incorporates unsupervised feature selection and model order identification technique. Then we will give out the experi</context>
</contexts>
<marker>Bouman, Shapiro, Cook, Atkins, Cheng, 1998</marker>
<rawString>Bouman, C. A., Shapiro, M., Cook, G. W., Atkins, C. B., &amp; Cheng, H. (1998) Cluster: An Unsupervsied Algorithm for Modeling Gaussian Mixtures. http://dynamo.ecn.purdue.edu/ ∼bouman/software/cluster/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dash</author>
<author>K Choi</author>
<author>P Scheuermann</author>
<author>H Liu</author>
</authors>
<title>Feature Selection for Clustering - A Filter Solution.</title>
<date>2002</date>
<booktitle>Proc. of IEEE Int. Conf. on Data Mining(pp. 115–</booktitle>
<pages>122</pages>
<contexts>
<context position="6822" citStr="Dash et al., 2002" startWordPosition="1037" endWordPosition="1040">ion 3. Section 4 will be devoted to a brief review of related efforts on word sense discrimination. In section 5 we will conclude our work and suggest some possible improvements. 2 Learning Procedure 2.1 Feature selection Feature selection for word sense learning is to find important contextual words which help to discriminate senses of target word without using class labels in data set. This problem can be generalized as selecting important feature subset in an unsupervised manner. Many unsupervised feature selection algorithms have been presented, which can be categorized as feature filter (Dash et al., 2002; Talavera, 1999) and feature wrapper (Dy and Brodley, 2000; Law et al., 2002; Mitra et al., 2002; Modha and Spangler, 2003). In this paper we propose a cluster validation based unsupervised feature subset evaluation method. Cluster validation has been used to solve model order identification problem (Lange et al., 2002; Levine and Domany, 2001). Table 1 gives out our feature subset evaluation algorithm. If some features in feature subset are noises, the estimated cluster structure on data subset in selected feature space is not stable, which is more likely to be the artifact of random splitti</context>
</contexts>
<marker>Dash, Choi, Scheuermann, Liu, 2002</marker>
<rawString>Dash, M., Choi, K., Scheuermann, P., &amp; Liu, H. (2002) Feature Selection for Clustering - A Filter Solution. Proc. of IEEE Int. Conf. on Data Mining(pp. 115– 122).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data using the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<contexts>
<context position="14176" citStr="Dempster et al., 1977" startWordPosition="2358" endWordPosition="2361">by: 1 log (Pyn|xn(yn|Θ)) + L log (NM), 2 (3) Pyn|xn(yn|k, B)itk, (4) L = K(1 + M + (M +2 1)M) − 1, (5) The log likelihood measures the goodness of fit of a model to data sample, while the second term penalizes complex model. This estimator works by attempting to find a model order with minimum code length to describe the data sample Y and parameter set O. If the cluster number is fixed, the estimation of GMM parameter can be solved using EM algorithm using Cluster, and the parameter set is denoted as N E n=1 MDL(K, B) = − K E k=1 Pyn|xn(yn|Θ) = to address this type of incomplete data problem (Dempster et al., 1977). The initialization of mixture parameter 0M is given by: 1 πk = (1) (6) Ko µk = yn, where n = L(k − 1)(N − 1)/(Ko − 1)� + 1 (7) (1) R(1) k = N 1 ΣN n=1ynyt (8) n Ko is a given initial subclass number. Then EM algorithm is used to estimate model parameters by minimizing MDL: E-step: re-estimate the expectations based on previous iteration: pxn|yn(k|yn, θ(i)) = pyn|xn(yn|k, θ(i))πk PK , (9) l=1(pyn|xn(yn|l, θ(i))πl) M-step: estimate the model parameter 0(i) to maximize the log-likelihood in MDL: pxn|yn(k|yn, θ(i)) (10) πk = Nk (11) N ynpxn|yn(k|yn, θ(i)) (12) (yn − µk)(yn − µk)tpxn|yn(k|yn, θ(i</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977) Maximum likelihood from incomplete data using the EM algorithm. Journal of the Royal Statistical Society, 39(B).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dorow</author>
<author>D Widdows</author>
</authors>
<title>Discovering CorpusSpecific Word Senses.</title>
<date>2003</date>
<booktitle>Proc. of the 10th Conf. of the European Chapter of the Association for Computational Linguistics, Conference Companion (research notes and demos)(pp.79–82).</booktitle>
<contexts>
<context position="27907" citStr="Dorow and Widdows, 2003" startWordPosition="4822" endWordPosition="4825"> 25 all Line dataset 0.6 0.5 0.4 0.3 0.2 0 1 5 15 25 all Interest dataset 0 1 5 15 25 all Serve dataset 0.55 0.35 0.6 0.5 0.4 0.3 .45 Table 6: Automatically determined mixture component number. Word Context Model Model window order order size with x2 with freq hard 1 3 4 5 2 2 15 2 3 25 2 3 all 2 3 interest 1 5 4 5 3 4 15 4 6 25 4 6 all 3 4 line 1 5 6 5 4 3 15 5 4 25 5 4 all 3 4 serve 1 3 3 5 3 4 15 3 3 25 3 3 all 2 4 context window size is no less than 5. 4 Related Work Besides the two works (Pantel and Lin, 2002; Sch¨utze, 1998), there are other related efforts on word sense discrimination (Dorow and Widdows, 2003; Fukumoto and Suzuki, 1999; Pedersen and Bruce, 1997). In (Pedersen and Bruce, 1997), they described an experimental comparison of three clustering algorithms for word sense discrimination. Their feature sets included morphology of target word, part of speech of contextual words, absence or presence of particular contextual words, and collocation of freFigure 1: Results for three procedures over 4 datases. The horizontal axis corresponds to the context window size. Solid line represents the result of FSGMM + binary, dashed line denotes the result of CGDSV D + idf, and dotted line is the resul</context>
<context position="29523" citStr="Dorow and Widdows, 2003" startWordPosition="5089" endWordPosition="5092"> their algorithm also required the cluster number to be provided. In (Fukumoto and Suzuki, 1999), a term weight learning algorithm was proposed for verb sense disambiguation, which can automatically extract nouns co-occurring with verbs and identify the number of senses of an ambiguous verb. The weakness of their method is to assume that nouns co-occurring with verbs are disambiguated in advance and the number of senses of target verb is no less than two. 0.8 0.75 0.7 0.65 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0 1 5 15 25 all Hard dataset Interest dataset Line dataset Serve dataset The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times. Then senses of target word were iteratively learned by clustering the local graph of similar words around target word. Their algorithm required a threshold as input, which controlled the number of senses. 5 Conclusion and Future Work Our word sense learning algorithm combined two novel ingredients: feature selection and order identification. Feature selection was formalized as a co</context>
</contexts>
<marker>Dorow, Widdows, 2003</marker>
<rawString>Dorow, B, &amp; Widdows, D. (2003) Discovering CorpusSpecific Word Senses. Proc. of the 10th Conf. of the European Chapter of the Association for Computational Linguistics, Conference Companion (research notes and demos)(pp.79–82).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Dy</author>
<author>C E Brodley</author>
</authors>
<title>Feature Subset Selection and Order Identification for Unsupervised Learning.</title>
<date>2000</date>
<booktitle>Proc. of the 17th Int. Conf. on Machine Learning(pp.</booktitle>
<pages>247--254</pages>
<contexts>
<context position="6881" citStr="Dy and Brodley, 2000" startWordPosition="1047" endWordPosition="1050">lated efforts on word sense discrimination. In section 5 we will conclude our work and suggest some possible improvements. 2 Learning Procedure 2.1 Feature selection Feature selection for word sense learning is to find important contextual words which help to discriminate senses of target word without using class labels in data set. This problem can be generalized as selecting important feature subset in an unsupervised manner. Many unsupervised feature selection algorithms have been presented, which can be categorized as feature filter (Dash et al., 2002; Talavera, 1999) and feature wrapper (Dy and Brodley, 2000; Law et al., 2002; Mitra et al., 2002; Modha and Spangler, 2003). In this paper we propose a cluster validation based unsupervised feature subset evaluation method. Cluster validation has been used to solve model order identification problem (Lange et al., 2002; Levine and Domany, 2001). Table 1 gives out our feature subset evaluation algorithm. If some features in feature subset are noises, the estimated cluster structure on data subset in selected feature space is not stable, which is more likely to be the artifact of random splitting. Then the consistency between cluster structures estimat</context>
</contexts>
<marker>Dy, Brodley, 2000</marker>
<rawString>Dy, J. G., &amp; Brodley, C. E. (2000) Feature Subset Selection and Order Identification for Unsupervised Learning. Proc. of the 17th Int. Conf. on Machine Learning(pp. 247–254).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Fukumoto</author>
<author>Y Suzuki</author>
</authors>
<title>Word Sense Disambiguation in Untagged Text Based on Term Weight Learning.</title>
<date>1999</date>
<booktitle>Proc. of the 9th Conf. of European Chapter of the Association for Computational Linguistics(pp.</booktitle>
<pages>209--216</pages>
<contexts>
<context position="27934" citStr="Fukumoto and Suzuki, 1999" startWordPosition="4826" endWordPosition="4829">0.5 0.4 0.3 0.2 0 1 5 15 25 all Interest dataset 0 1 5 15 25 all Serve dataset 0.55 0.35 0.6 0.5 0.4 0.3 .45 Table 6: Automatically determined mixture component number. Word Context Model Model window order order size with x2 with freq hard 1 3 4 5 2 2 15 2 3 25 2 3 all 2 3 interest 1 5 4 5 3 4 15 4 6 25 4 6 all 3 4 line 1 5 6 5 4 3 15 5 4 25 5 4 all 3 4 serve 1 3 3 5 3 4 15 3 3 25 3 3 all 2 4 context window size is no less than 5. 4 Related Work Besides the two works (Pantel and Lin, 2002; Sch¨utze, 1998), there are other related efforts on word sense discrimination (Dorow and Widdows, 2003; Fukumoto and Suzuki, 1999; Pedersen and Bruce, 1997). In (Pedersen and Bruce, 1997), they described an experimental comparison of three clustering algorithms for word sense discrimination. Their feature sets included morphology of target word, part of speech of contextual words, absence or presence of particular contextual words, and collocation of freFigure 1: Results for three procedures over 4 datases. The horizontal axis corresponds to the context window size. Solid line represents the result of FSGMM + binary, dashed line denotes the result of CGDSV D + idf, and dotted line is the result of CGDterm + idf. Square </context>
</contexts>
<marker>Fukumoto, Suzuki, 1999</marker>
<rawString>Fukumoto, F., &amp; Suzuki, Y. (1999) Word Sense Disambiguation in Untagged Text Based on Term Weight Learning. Proc. of the 9th Conf. of European Chapter of the Association for Computational Linguistics(pp. 209–216).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>J V´eronis</author>
</authors>
<title>Word Sense Disambiguation: The State of the Art.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>1--41</pages>
<marker>Ide, V´eronis, 1998</marker>
<rawString>Ide, N., &amp; V´eronis, J. (1998) Word Sense Disambiguation: The State of the Art. Computational Linguistics, 24:1, 1–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lange</author>
<author>M Braun</author>
<author>V Roth</author>
<author>J M Buhmann</author>
</authors>
<title>Stability-Based Model Selection.</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems 15.</booktitle>
<contexts>
<context position="7143" citStr="Lange et al., 2002" startWordPosition="1089" endWordPosition="1092">riminate senses of target word without using class labels in data set. This problem can be generalized as selecting important feature subset in an unsupervised manner. Many unsupervised feature selection algorithms have been presented, which can be categorized as feature filter (Dash et al., 2002; Talavera, 1999) and feature wrapper (Dy and Brodley, 2000; Law et al., 2002; Mitra et al., 2002; Modha and Spangler, 2003). In this paper we propose a cluster validation based unsupervised feature subset evaluation method. Cluster validation has been used to solve model order identification problem (Lange et al., 2002; Levine and Domany, 2001). Table 1 gives out our feature subset evaluation algorithm. If some features in feature subset are noises, the estimated cluster structure on data subset in selected feature space is not stable, which is more likely to be the artifact of random splitting. Then the consistency between cluster structures estimated from disjoint data subsets will be lower. Otherwise the estimated cluster structures should be more consistent. Here we assume that splitting does not eliminate some of the underlying modes in data set. For comparison of different clustering structures, predi</context>
<context position="18142" citStr="Lange et al., 2002" startWordPosition="3063" endWordPosition="3066">nd p(l) are marginal probabilities. p(w, l) is estimated based 1http://www.d.umn.edu/∼tpederse/data.html N X n=1 Nk = 1 Nk µk = XN n=1 1 Rk = Nk N X n=1 2 = X 1 M(T) = |T | w∈T X l∈L on contingency table of contextual word set W and class label set L. Intuitively, if M(T1) &gt; M(T2), T1 is more important than T2 since T1 contains more information about L. 3.3 Evaluation method for clustering result When assessing the agreement between clustering result and hand-tagged senses (ground truth classes) in benchmark data, we encountered the difficulty that there was no sense tag for each cluster. In (Lange et al., 2002), they defined a permutation procedure for calculating the agreement between two cluster memberships assigned by different unsupervised learners. In this paper, we applied their method to assign different sense tags to only min(|U|,|C|) clusters by maximizing the accuracy, where |U |is the number of clusters, and |C |is the number of ground truth classes. The underlying assumption here is that each cluster is considered as a class, and for any two clusters, they do not share same class labels. At most |C |clusters are assigned sense tags, since there are only |C |classes in benchmark data. Giv</context>
</contexts>
<marker>Lange, Braun, Roth, Buhmann, 2002</marker>
<rawString>Lange, T., Braun, M., Roth, V., &amp; Buhmann, J. M. (2002) Stability-Based Model Selection. Advances in Neural Information Processing Systems 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H Law</author>
<author>M Figueiredo</author>
<author>A K Jain</author>
</authors>
<title>Feature Selection in Mixture-Based Clustering.</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems 15.</booktitle>
<contexts>
<context position="6899" citStr="Law et al., 2002" startWordPosition="1051" endWordPosition="1054">sense discrimination. In section 5 we will conclude our work and suggest some possible improvements. 2 Learning Procedure 2.1 Feature selection Feature selection for word sense learning is to find important contextual words which help to discriminate senses of target word without using class labels in data set. This problem can be generalized as selecting important feature subset in an unsupervised manner. Many unsupervised feature selection algorithms have been presented, which can be categorized as feature filter (Dash et al., 2002; Talavera, 1999) and feature wrapper (Dy and Brodley, 2000; Law et al., 2002; Mitra et al., 2002; Modha and Spangler, 2003). In this paper we propose a cluster validation based unsupervised feature subset evaluation method. Cluster validation has been used to solve model order identification problem (Lange et al., 2002; Levine and Domany, 2001). Table 1 gives out our feature subset evaluation algorithm. If some features in feature subset are noises, the estimated cluster structure on data subset in selected feature space is not stable, which is more likely to be the artifact of random splitting. Then the consistency between cluster structures estimated from disjoint d</context>
</contexts>
<marker>Law, Figueiredo, Jain, 2002</marker>
<rawString>Law, M. H., Figueiredo, M., &amp; Jain, A. K. (2002) Feature Selection in Mixture-Based Clustering. Advances in Neural Information Processing Systems 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
<author>A G Miller</author>
</authors>
<title>Using Corpus Statistics and WordNet Relations for Sense Identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>147--165</pages>
<contexts>
<context position="24503" citStr="Leacock et al., 1998" startWordPosition="4153" endWordPosition="4156"> accuracy over three procedures in Figure 1 as a function of context window size for 4 datasets. For “hard”, the performance dropped as window size increased, and the best accuracy(77.0%) was achieved at window size 1. For “interest”, sense discrimination did not benefit from large window size and the best accuracy(40.1%) was achieved at window size 5. For “line”, accuracy dropped when increasing window size and the best accuracy(50.2%) was achieved at window size 1. For “serve”, the performance benefitted from large window size and the best accuracy(46.8%) was achieved at window size 15. In (Leacock et al., 1998), they used Bayesian approach for sense disambiguation of three ambiguous words, “hard”, “line”, and “serve”, based on cues from topical and local context. They observed that local context was more reliable than topical context as an indicator of senses for this verb and adjective, but slightly less reliable for this noun. Compared with their conclusion, we can find that our result is consistent with it for “hard”. But there is some differences for verb “serve” and noun “line”. For Table 3: Mutual information between feature subset and class label with x2 based feature ranking. Word Cont. Size</context>
</contexts>
<marker>Leacock, Chodorow, Miller, 1998</marker>
<rawString>Leacock, C., Chodorow, M., &amp; Miller A. G. (1998) Using Corpus Statistics and WordNet Relations for Sense Identification. Computational Linguistics, 24:1, 147– 165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levine</author>
<author>E Domany</author>
</authors>
<title>Resampling Method for Unsupervised Estimation of Cluster Validity.</title>
<date>2001</date>
<journal>Neural Computation,</journal>
<volume>13</volume>
<pages>2573--2593</pages>
<contexts>
<context position="7169" citStr="Levine and Domany, 2001" startWordPosition="1093" endWordPosition="1096">arget word without using class labels in data set. This problem can be generalized as selecting important feature subset in an unsupervised manner. Many unsupervised feature selection algorithms have been presented, which can be categorized as feature filter (Dash et al., 2002; Talavera, 1999) and feature wrapper (Dy and Brodley, 2000; Law et al., 2002; Mitra et al., 2002; Modha and Spangler, 2003). In this paper we propose a cluster validation based unsupervised feature subset evaluation method. Cluster validation has been used to solve model order identification problem (Lange et al., 2002; Levine and Domany, 2001). Table 1 gives out our feature subset evaluation algorithm. If some features in feature subset are noises, the estimated cluster structure on data subset in selected feature space is not stable, which is more likely to be the artifact of random splitting. Then the consistency between cluster structures estimated from disjoint data subsets will be lower. Otherwise the estimated cluster structures should be more consistent. Here we assume that splitting does not eliminate some of the underlying modes in data set. For comparison of different clustering structures, predictors are constructed base</context>
</contexts>
<marker>Levine, Domany, 2001</marker>
<rawString>Levine, E., &amp; Domany, E. (2001) Resampling Method for Unsupervised Estimation of Cluster Validity. Neural Computation, Vol. 13, 2573–2593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Mitra</author>
<author>A C Murthy</author>
<author>K S Pal</author>
</authors>
<title>Unsupervised Feature Selection Using Feature Similarity.</title>
<date>2002</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>24</volume>
<pages>301--312</pages>
<contexts>
<context position="6919" citStr="Mitra et al., 2002" startWordPosition="1055" endWordPosition="1058">on. In section 5 we will conclude our work and suggest some possible improvements. 2 Learning Procedure 2.1 Feature selection Feature selection for word sense learning is to find important contextual words which help to discriminate senses of target word without using class labels in data set. This problem can be generalized as selecting important feature subset in an unsupervised manner. Many unsupervised feature selection algorithms have been presented, which can be categorized as feature filter (Dash et al., 2002; Talavera, 1999) and feature wrapper (Dy and Brodley, 2000; Law et al., 2002; Mitra et al., 2002; Modha and Spangler, 2003). In this paper we propose a cluster validation based unsupervised feature subset evaluation method. Cluster validation has been used to solve model order identification problem (Lange et al., 2002; Levine and Domany, 2001). Table 1 gives out our feature subset evaluation algorithm. If some features in feature subset are noises, the estimated cluster structure on data subset in selected feature space is not stable, which is more likely to be the artifact of random splitting. Then the consistency between cluster structures estimated from disjoint data subsets will be </context>
</contexts>
<marker>Mitra, Murthy, Pal, 2002</marker>
<rawString>Mitra, P., Murthy, A. C., &amp; Pal, K. S. (2002) Unsupervised Feature Selection Using Feature Similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24:4, 301–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Modha</author>
<author>W S Spangler</author>
</authors>
<title>Feature Weighting in k-Means Clustering.</title>
<date>2003</date>
<booktitle>Machine Learning,</booktitle>
<volume>52</volume>
<pages>217--237</pages>
<contexts>
<context position="6946" citStr="Modha and Spangler, 2003" startWordPosition="1059" endWordPosition="1062">will conclude our work and suggest some possible improvements. 2 Learning Procedure 2.1 Feature selection Feature selection for word sense learning is to find important contextual words which help to discriminate senses of target word without using class labels in data set. This problem can be generalized as selecting important feature subset in an unsupervised manner. Many unsupervised feature selection algorithms have been presented, which can be categorized as feature filter (Dash et al., 2002; Talavera, 1999) and feature wrapper (Dy and Brodley, 2000; Law et al., 2002; Mitra et al., 2002; Modha and Spangler, 2003). In this paper we propose a cluster validation based unsupervised feature subset evaluation method. Cluster validation has been used to solve model order identification problem (Lange et al., 2002; Levine and Domany, 2001). Table 1 gives out our feature subset evaluation algorithm. If some features in feature subset are noises, the estimated cluster structure on data subset in selected feature space is not stable, which is more likely to be the artifact of random splitting. Then the consistency between cluster structures estimated from disjoint data subsets will be lower. Otherwise the estima</context>
</contexts>
<marker>Modha, Spangler, 2003</marker>
<rawString>Modha, D. S., &amp; Spangler, W. S. (2003) Feature Weighting in k-Means Clustering. Machine Learning, 52:3, 217–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D K Lin</author>
</authors>
<title>Discovering Word Senses from Text.</title>
<date>2002</date>
<booktitle>Proc. of ACM SIGKDD Conf. on Knowledge Discovery and Data Mining(pp.</booktitle>
<pages>613--619</pages>
<contexts>
<context position="2105" citStr="Pantel and Lin, 2002" startWordPosition="306" endWordPosition="309">rning word senses from free text will help us dispense of outside knowledge source for defining sense by only discriminating senses of words. Another application of word sense learning is to help enriching or even constructing semantic lexicons (Widdows, 2003). The solution of word sense learning is closely related to the interpretation of word senses. Different interpretations of word senses result in different solutions to word sense learning. One interpretation strategy is to treat a word sense as a set of synonyms like synset in WordNet. The committee based word sense discovery algorithm (Pantel and Lin, 2002) followed this strategy, which treated senses as clusters of words occurring in similar contexts. Their algorithm initially discovered tight clusters called committees by grouping top n words similar with target word using averagelink clustering. Then the target word was assigned to committees if the similarity between them was above a given threshold. Each committee that the target word belonged to was interpreted as one of its senses. There are two difficulties with this committee based sense learning. The first difficulty is about derivation of feature vectors. A feature for target word her</context>
<context position="27803" citStr="Pantel and Lin, 2002" startWordPosition="4807" endWordPosition="4810"> tf · idf 0.485 0.9 0.8 0.7 0.6 0.5 0.4 0 1 5 15 25 all Hard dataset 0.7 0.6 0.5 0.4 0.3 0.2 0 1 5 15 25 all Line dataset 0.6 0.5 0.4 0.3 0.2 0 1 5 15 25 all Interest dataset 0 1 5 15 25 all Serve dataset 0.55 0.35 0.6 0.5 0.4 0.3 .45 Table 6: Automatically determined mixture component number. Word Context Model Model window order order size with x2 with freq hard 1 3 4 5 2 2 15 2 3 25 2 3 all 2 3 interest 1 5 4 5 3 4 15 4 6 25 4 6 all 3 4 line 1 5 6 5 4 3 15 5 4 25 5 4 all 3 4 serve 1 3 3 5 3 4 15 3 3 25 3 3 all 2 4 context window size is no less than 5. 4 Related Work Besides the two works (Pantel and Lin, 2002; Sch¨utze, 1998), there are other related efforts on word sense discrimination (Dorow and Widdows, 2003; Fukumoto and Suzuki, 1999; Pedersen and Bruce, 1997). In (Pedersen and Bruce, 1997), they described an experimental comparison of three clustering algorithms for word sense discrimination. Their feature sets included morphology of target word, part of speech of contextual words, absence or presence of particular contextual words, and collocation of freFigure 1: Results for three procedures over 4 datases. The horizontal axis corresponds to the context window size. Solid line represents the</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Pantel, P. &amp; Lin, D. K. (2002) Discovering Word Senses from Text. Proc. of ACM SIGKDD Conf. on Knowledge Discovery and Data Mining(pp. 613-619).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>R Bruce</author>
</authors>
<title>Distinguishing Word Senses in Untagged Text.</title>
<date>1997</date>
<booktitle>Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing(pp.</booktitle>
<contexts>
<context position="27961" citStr="Pedersen and Bruce, 1997" startWordPosition="4830" endWordPosition="4833"> all Interest dataset 0 1 5 15 25 all Serve dataset 0.55 0.35 0.6 0.5 0.4 0.3 .45 Table 6: Automatically determined mixture component number. Word Context Model Model window order order size with x2 with freq hard 1 3 4 5 2 2 15 2 3 25 2 3 all 2 3 interest 1 5 4 5 3 4 15 4 6 25 4 6 all 3 4 line 1 5 6 5 4 3 15 5 4 25 5 4 all 3 4 serve 1 3 3 5 3 4 15 3 3 25 3 3 all 2 4 context window size is no less than 5. 4 Related Work Besides the two works (Pantel and Lin, 2002; Sch¨utze, 1998), there are other related efforts on word sense discrimination (Dorow and Widdows, 2003; Fukumoto and Suzuki, 1999; Pedersen and Bruce, 1997). In (Pedersen and Bruce, 1997), they described an experimental comparison of three clustering algorithms for word sense discrimination. Their feature sets included morphology of target word, part of speech of contextual words, absence or presence of particular contextual words, and collocation of freFigure 1: Results for three procedures over 4 datases. The horizontal axis corresponds to the context window size. Solid line represents the result of FSGMM + binary, dashed line denotes the result of CGDSV D + idf, and dotted line is the result of CGDterm + idf. Square marker denotes x2 based fea</context>
</contexts>
<marker>Pedersen, Bruce, 1997</marker>
<rawString>Pedersen, T., &amp; Bruce, R. (1997) Distinguishing Word Senses in Untagged Text. Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing(pp. 197–207).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pudil</author>
<author>J Novovicova</author>
<author>J Kittler</author>
</authors>
<title>Floating Search Methods in Feature Selection.</title>
<date>1994</date>
<journal>Pattern Recognigion Letters,</journal>
<volume>15</volume>
<pages>1119--1125</pages>
<contexts>
<context position="11631" citStr="Pudil et al., 1994" startWordPosition="1887" endWordPosition="1890">e subset selection in word set W can be formulated as: Tˆ = arg maxfcriterion(T, H, V, q)1, T C W, (2) T subject to coverage(D, T) &gt; τ, where Tˆ is the optimal feature subset, criterion is the cluster validation based evaluation function (the function in Table 1), q is the resampling frequency for estimate of stability, and coverage(D,T) is the proportion of contexts with occurrences of features in T. This constrained optimization results in a solution which maximizes the criterion and meets the given constraint at the same time. In this paper we use sequential greedy forward floating search (Pudil et al., 1994) in sorted word list based on χ2 or local frequency criterion. We set l = 1, m = 1, where l is plus step, and m is take-away step. 2.2 Clustering with order identification After feature selection, we employ a Gaussian mixture modelling algorithm, Cluster (Bouman et al., Table 1: Unsupervised Feature Subset Evaluation Algorithm. Intuitively, for a given feature subset T, we iteratively split data set into disjoint halves, and compute the agreement of clustering solutions estimated from these sets using stability measure. The average of stability over q resampling is the estimation of the score </context>
</contexts>
<marker>Pudil, Novovicova, Kittler, 1994</marker>
<rawString>Pudil, P., Novovicova, J., &amp; Kittler, J. (1994) Floating Search Methods in Feature Selection. Pattern Recognigion Letters, Vol. 15, 1119-1125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rissanen</author>
</authors>
<title>Modeling by Shortest Data Description.</title>
<date>1978</date>
<journal>Automatica,</journal>
<volume>14</volume>
<pages>465--471</pages>
<contexts>
<context position="5644" citStr="Rissanen, 1978" startWordPosition="853" endWordPosition="855">extual words, which works by measuring the consistency between cluster structures estimated from disjoint data subsets in selected feature space. It is based on the assumption that if selected feature subset is important and complete, cluster structure estimated from data subset in this feature space should be stable and robust against random sampling. After determination of important contextual words, we use a Gaussian mixture model (GMM) based clustering algorithm (Bouman et al., 1998) to estimate cluster structure and cluster number by minimizing Minimum Description Length (MDL) criterion (Rissanen, 1978). We construct several subsets from widely used benchmark corpus as test data. Experimental results show that our algorithm (FSGMM) can find important feature subset, estimate cluster number and achieve better performance compared with CGD algorithm. This paper is organized as follows. In section 2 we will introduce our word sense learning algorithm, which incorporates unsupervised feature selection and model order identification technique. Then we will give out the experimental results of our algorithm and discuss some findings from these results in section 3. Section 4 will be devoted to a b</context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>Rissanen, J. (1978) Modeling by Shortest Data Description. Automatica, Vol. 14, 465–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Automatic Word Sense Discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>97--123</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Sch¨utze, H. (1998) Automatic Word Sense Discrimination. Computational Linguistics, 24:1, 97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Talavera</author>
</authors>
<title>Feature Selection as a Preprocessing Step for Hierarchical Clustering.</title>
<date>1999</date>
<booktitle>Proc. of the 16th Int. Conf. on Machine Learning(pp.</booktitle>
<pages>389--397</pages>
<contexts>
<context position="6839" citStr="Talavera, 1999" startWordPosition="1041" endWordPosition="1043">ll be devoted to a brief review of related efforts on word sense discrimination. In section 5 we will conclude our work and suggest some possible improvements. 2 Learning Procedure 2.1 Feature selection Feature selection for word sense learning is to find important contextual words which help to discriminate senses of target word without using class labels in data set. This problem can be generalized as selecting important feature subset in an unsupervised manner. Many unsupervised feature selection algorithms have been presented, which can be categorized as feature filter (Dash et al., 2002; Talavera, 1999) and feature wrapper (Dy and Brodley, 2000; Law et al., 2002; Mitra et al., 2002; Modha and Spangler, 2003). In this paper we propose a cluster validation based unsupervised feature subset evaluation method. Cluster validation has been used to solve model order identification problem (Lange et al., 2002; Levine and Domany, 2001). Table 1 gives out our feature subset evaluation algorithm. If some features in feature subset are noises, the estimated cluster structure on data subset in selected feature space is not stable, which is more likely to be the artifact of random splitting. Then the cons</context>
</contexts>
<marker>Talavera, 1999</marker>
<rawString>Talavera, L. (1999) Feature Selection as a Preprocessing Step for Hierarchical Clustering. Proc. of the 16th Int. Conf. on Machine Learning(pp. 389–397).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
</authors>
<title>Unsupervised methods for developing taxonomies by combining syntactic and statistical information.</title>
<date>2003</date>
<booktitle>Proc. of the Human Language Technology / Conference of the North American Chapter of the Association for Computational Linguistics(pp.</booktitle>
<pages>276--283</pages>
<contexts>
<context position="1744" citStr="Widdows, 2003" startWordPosition="249" endWordPosition="250">r many language applications such as machine translation, information retrieval, and speech processing (Ide and V´eronis, 1998). Almost all of sense disambiguation methods are heavily dependant on manually compiled lexical resources. However these lexical resources often miss domain specific word senses, even many new words are not included inside. Learning word senses from free text will help us dispense of outside knowledge source for defining sense by only discriminating senses of words. Another application of word sense learning is to help enriching or even constructing semantic lexicons (Widdows, 2003). The solution of word sense learning is closely related to the interpretation of word senses. Different interpretations of word senses result in different solutions to word sense learning. One interpretation strategy is to treat a word sense as a set of synonyms like synset in WordNet. The committee based word sense discovery algorithm (Pantel and Lin, 2002) followed this strategy, which treated senses as clusters of words occurring in similar contexts. Their algorithm initially discovered tight clusters called committees by grouping top n words similar with target word using averagelink clus</context>
<context position="27907" citStr="Widdows, 2003" startWordPosition="4824" endWordPosition="4825">ne dataset 0.6 0.5 0.4 0.3 0.2 0 1 5 15 25 all Interest dataset 0 1 5 15 25 all Serve dataset 0.55 0.35 0.6 0.5 0.4 0.3 .45 Table 6: Automatically determined mixture component number. Word Context Model Model window order order size with x2 with freq hard 1 3 4 5 2 2 15 2 3 25 2 3 all 2 3 interest 1 5 4 5 3 4 15 4 6 25 4 6 all 3 4 line 1 5 6 5 4 3 15 5 4 25 5 4 all 3 4 serve 1 3 3 5 3 4 15 3 3 25 3 3 all 2 4 context window size is no less than 5. 4 Related Work Besides the two works (Pantel and Lin, 2002; Sch¨utze, 1998), there are other related efforts on word sense discrimination (Dorow and Widdows, 2003; Fukumoto and Suzuki, 1999; Pedersen and Bruce, 1997). In (Pedersen and Bruce, 1997), they described an experimental comparison of three clustering algorithms for word sense discrimination. Their feature sets included morphology of target word, part of speech of contextual words, absence or presence of particular contextual words, and collocation of freFigure 1: Results for three procedures over 4 datases. The horizontal axis corresponds to the context window size. Solid line represents the result of FSGMM + binary, dashed line denotes the result of CGDSV D + idf, and dotted line is the resul</context>
<context position="29523" citStr="Widdows, 2003" startWordPosition="5091" endWordPosition="5092">orithm also required the cluster number to be provided. In (Fukumoto and Suzuki, 1999), a term weight learning algorithm was proposed for verb sense disambiguation, which can automatically extract nouns co-occurring with verbs and identify the number of senses of an ambiguous verb. The weakness of their method is to assume that nouns co-occurring with verbs are disambiguated in advance and the number of senses of target verb is no less than two. 0.8 0.75 0.7 0.65 0.6 0.55 0.5 0.45 0.4 0.35 0.3 0 1 5 15 25 all Hard dataset Interest dataset Line dataset Serve dataset The algorithm in (Dorow and Widdows, 2003) represented target noun word, its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times. Then senses of target word were iteratively learned by clustering the local graph of similar words around target word. Their algorithm required a threshold as input, which controlled the number of senses. 5 Conclusion and Future Work Our word sense learning algorithm combined two novel ingredients: feature selection and order identification. Feature selection was formalized as a co</context>
</contexts>
<marker>Widdows, 2003</marker>
<rawString>Widdows, D. (2003) Unsupervised methods for developing taxonomies by combining syntactic and statistical information. Proc. of the Human Language Technology / Conference of the North American Chapter of the Association for Computational Linguistics(pp. 276–283).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>