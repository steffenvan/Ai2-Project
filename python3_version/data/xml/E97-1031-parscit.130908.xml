<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.975407">
A Flexible POS Tagger Using an Automatically Acquired
Language Model*
</title>
<author confidence="0.628546">
Lluis Marquez
</author>
<address confidence="0.759571875">
LSI - UPC
c/ Jordi Girona 1-3
08034 Barcelona. Catalonia
lluismOlsi.upc.es
Lluis Padr6
LSI - UPC
c/ Jordi Girona 1-3
08034 Barcelona. Catalonia
</address>
<email confidence="0.918921">
padroOlsi.upc.es
</email>
<sectionHeader confidence="0.992639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999962692307692">
We present an algorithm that automati-
cally learns context constraints using sta-
tistical decision trees. We then use the ac-
quired constraints in a flexible POS tag-
ger. The tagger is able to use informa-
tion of any degree: n-grams, automati-
cally learned context constraints, linguis-
tically motivated manually written con-
straints, etc. The sources and kinds of con-
straints are unrestricted, and the language
model can be easily extended, improving
the results. The tagger has been tested and
evaluated on the WSJ corpus.
</bodyText>
<sectionHeader confidence="0.997907" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999671">
In NLP, it is necessary to model the language in a
representation suitable for the task to be performed.
The language models more commonly used are based
on two main approaches: first, the linguistic ap-
proach, in which the model is written by a linguist,
generally in the form of rules or constraints (Vouti-
lainen and Jarvinen, 1995). Second, the automatic
approach, in which the model is automatically ob-
tained from corpora (either raw or annotated)&apos;, and
consists of n—grams (Garside et at., 1987; Cutting
et at., 1992), rules (Hindle, 1989) or neural nets
(Schmid, 1994). In the automatic approach we can
distinguish two main trends: The low—level data
trend collects statistics from the training corpora in
the form of n—grams, probabilities, weights, etc. The
high level data trend acquires more sophisticated in-
formation, such as context rules, constraints, or de-
cision trees (Daelemans et al., 1996; Marquez and
Rodriguez, 1995; Samuelsson et al., 1996). The ac-
quisition methods range from supervised—inductive—
learning—from—example algorithms (Quinlan, 1986;
</bodyText>
<footnote confidence="0.599373166666667">
•This research has been partially funded by the Span-
ish Research Department (CICYT) and inscribed as
TIC96-1243-0O3-02
&apos;When the model is obtained from annotated corpora
we talk about supervised learning, when it is obtained
from raw corpora training is considered unsupervised.
</footnote>
<bodyText confidence="0.9991915625">
Aha et al., 1991) to genetic algorithm strategies
(Losee, 1994), through the transformation—based
error—driven algorithm used in (Brill, 1995). Still
another possibility are the hybrid models, which try
to join the advantages of both approaches ( Vouti-
lainen and PadrO, 1997).
We present in this paper a hybrid approach that
puts together both trends in automatic approach
and the linguistic approach. We describe a POS tag-
ger based on the work described in (Padr6, 1996),
that is able to use bi/trigram information, auto-
matically learned context constraints and linguisti-
cally motivated manually written constraints. The
sources and kinds of constraints are unrestricted,
and the language model can be easily extended. The
structure of the tagger is presented in figure 1.
</bodyText>
<figure confidence="0.45205">
Language Model
</figure>
<figureCaption confidence="0.999608">
Figure 1: Tagger architecture.
</figureCaption>
<bodyText confidence="0.9999572">
We also present a constraint—acquisition algo-
rithm that uses statistical decision trees to learn con-
text constraints from annotated corpora and we use
the acquired constraints to feed the POS tagger.
The paper is organized as follows. In section 2 we
describe our language model, in section 3 we describe
the constraint acquisition algorithm, and in section
4 we expose the tagging algorithm. Descriptions of
the corpus used, the experiments performed and the
results obtained can be found in sections 5 and 6.
</bodyText>
<sectionHeader confidence="0.995334" genericHeader="method">
2 Language Model
</sectionHeader>
<bodyText confidence="0.994062">
We will use a hybrid language model consisting of an
automatically acquired part and a linguist—written
part.
</bodyText>
<figure confidence="0.987846285714286">
[Lexicon Bigrams f Manually
Trigrams Automatically written
learned constraints
constraints
Raw Corpus
Tagging algorithm
Tagged Corpus
</figure>
<page confidence="0.995268">
238
</page>
<bodyText confidence="0.972078916666667">
The automatically acquired part is divided in two
kinds of information: on the one hand, we have bi-
grams and trigrams collected from the annotated
training corpus (see section 5 for details). On the
other hand, we have context constraints learned
from the same training corpus using statistical deci-
sion trees, as described in section 3.
The linguistic part is very small —since there were
no available resources to develop it further— and
covers only very few cases, but it is included to il-
lustrate the flexibility of the algorithm.
A sample rule of the linguistic part:
</bodyText>
<equation confidence="0.849470333333333">
10.0 (&apos;vauxiliar%)
(--C1/BN IN • : JJ JJS JJRD+
&lt;VBN&gt; ;
</equation>
<bodyText confidence="0.999169428571429">
This rule states that a tag past participle (VBN) is
very compatible (10.0) with a left context consisting
of a %vauxiliar% (previously defined macro which
includes all forms of &amp;quot;have&amp;quot; and &amp;quot;be&amp;quot;) provided that
all the words in between don&apos;t have any of the tags
in the set [VBN IN , : JJ JJS JJR]. That is,
this rule raises the support for the tag past partici-
ple when there is an auxiliary verb to the left but
only if there is not another candidate to be a past
participle or an adjective inbetween. The tags [IN
, :] prevent the rule from being applied when the
auxiliary verb and the participle are in two different
phrases (a comma, a colon or a preposition are con-
sidered to mark the beginning of another phrase).
The constraint language is able to express the
same kind of patterns than the Constraint Gram-
mar formalism (Karlsson et al., 1995), although in a
different formalism. In addition, each constraint has
a compatibility value that indicates its strength. In
the middle run, the system will be adapted to accept
CGs.
</bodyText>
<sectionHeader confidence="0.99035" genericHeader="method">
3 Constraint Acquisition
</sectionHeader>
<bodyText confidence="0.999687552631579">
Choosing, from a set of possible tags, the proper syn-
tactic tag for a word in a particular context can be
seen as a problem of classification. Decision trees,
recently used in NLP basic tasks such as tagging
and parsing (McCarthy and Lehnert, 1995; Daele-
mans et al., 1996; Magerman, 1996), are suitable for
performing this task.
A decision tree is a n-ary branching tree that rep-
resents a classification rule for classifying the objects
of a certain domain into a set of mutually exclusive
classes. The domain objects are described as a set
of attribute-value pairs, where each attribute mea-
sures a relevant feature of an object taking a (ideally
small) set of discrete, mutually incompatible values.
Each non-terminal node of a decision tree represents
a question on (usually) one attribute. For each possi-
ble value of this attribute there is a branch to follow.
Leaf nodes represent concrete classes.
Classify a new object with a decision tree is simply
following the convenient path through the tree until
a leaf is reached.
Statistical decision trees only differs from common
decision trees in that leaf nodes define a conditional
probability distribution on the set of classes.
It is important to note that decision trees can be
directly translated to rules considering, for each path
from the root to a leaf, the conjunction of all ques-
tions involved in this path as a condition and the
class assigned to the leaf as the consequence. Statis-
tical decision trees would generate rules in the same
manner but assigning a certain degree of probability
to each answer.
So the learning process of contextual constraints
is performed by means of learning one statistical de-
cision tree for each class of POS ambiguity` and con-
verting them to constraints (rules) expressing com-
patibility/incompatibility of concrete tags in certain
contexts.
</bodyText>
<subsectionHeader confidence="0.93811">
Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.993286833333333">
The algorithm we used for constructing the statisti-
cal decision trees is a non-incremental supervised
learning-from-examples algorithm of the TDIDT
(Top Down Induction of Decision Trees) family. It
constructs the trees in a top-down way, guided by
the distributional information of the examples, but
not on the examples order (Quinlan, 1986). Briefly.
the algorithm works as a recursive process that de-
parts from considering the whole set of examples at
the root level and constructs the tree in a top-down
way branching at any non-terminal node according
to a certain selected attribute. The different val-
ues of this attribute induce a partition of the set
of examples in the corresponding subsets, in which
the process is applied recursively in order to gener-
ate the different subtrees. The recursion ends, in a
certain node, either when all (or almost all) the re-
maining examples belong to the same class, or when
the number of examples is too small. These nodes
are the leafs of the tree and contain the conditional
probability distribution, of its associated subset of
examples, on the possible classes.
The heuristic function for selecting the most
useful attribute at each step is of a cru-
cial importance in order to obtain simple trees,
since no backtracking is performed. There ex-
ist two main families of attribute-selecting func-
tions: information-based (Quinlan, 1986; Lopez,
1991) and statistically-based (Breiman et al., 1984;
Mingers, 1989).
</bodyText>
<subsectionHeader confidence="0.84995">
Training Set
</subsectionHeader>
<bodyText confidence="0.7954786">
For each class of POS ambiguity the initial exam-
ple set is built by selecting from the training corpus
2Classes of ambiguity are determined by the groups
of possible tags for the words in the corpus, i.e, noun-
adjective, noun-adjective-verb, preposition-adverb, etc.
</bodyText>
<page confidence="0.997263">
239
</page>
<bodyText confidence="0.999947818181818">
all the occurrences of the words belonging to this
ambiguity class. More particularly, the set of at-
tributes that describe each example consists of the
part-of-speech tags of the neighbour words, and the
information about the word itself (orthography and
the proper tag in its context). The window consid-
ered in the experiments reported in section 6 is 3
words to the left and 2 to the right. The follow-
ing are two real examples from the training set for
the words that can be preposition and adverb at the
same time (IN-RB conflict).
</bodyText>
<equation confidence="0.9544055">
VB DT NN &lt;uas&amp;quot;,IN&gt; DT JJ
NN IN NN &lt;uonce&amp;quot;,RB&gt; VBN TO
</equation>
<bodyText confidence="0.999256">
Approximately 90% of this set of examples is used
for the construction of the tree. The remaining 10%
is used as fresh test corpus for the pruning process.
</bodyText>
<subsubsectionHeader confidence="0.865563">
Attribute Selection Function
</subsubsectionHeader>
<bodyText confidence="0.9999566875">
For the experiments reported in section 6 we used a
attribute selection function due to Lopez de Nlanta-
ras (Lopez. 1991), which belongs to the information-
based family. Roughly speaking, it defines a distance
measure between partitions and selects for branch-
ing the attribute that generates the closest partition
to the correct partition, namely the one that joins
together all the examples of the same class.
Let X be a set of examples, C the set of classes and
Pc(X) the partition of X according to the values of
C. The selected attribute will be the one that gen-
erates the closest partition of X to Pc(X). For that
we need to define a distance measure between parti-
tions. Let PA(X) be the partition of X induced by
the values of attribute .4. The average information
of such partition is defined as follows:
</bodyText>
<equation confidence="0.5938785">
/( PA (X)) = - E p(X, a) log, p(X, a) ,
aE PA (X)
</equation>
<bodyText confidence="0.999992333333333">
where p( X . a) is the probability for an element of X
belonging to the set a which is the subset of X whose
examples have a certain value for the attribute .4,
and it is estimated by the ratio IV. This average
information measure reflects the randomness of dis-
tribution of the elements of X between the classes of
the partition induced by .4. If we consider now the
intersection between two different partitions induced
by attributes .4 and B we obtain
</bodyText>
<equation confidence="0.9390485">
/(PA(X)n Ps(X)) =
- E E P(X•anb) log2 p(X. anb) .
aEPAIX1(5EPBX)
Conditioned information of PB(X) given PA(X) is
/(Pe(X)IP.4(X)) =
/(PA(X)n PB(X)) I(PA(X)) =
AX,anb)log,p()Hanb)
p(Aa)
(2EPA(XbEPBLXI
It is easy to show that the measure
d(PA(X)• Pe(X))=
I(Ps(X)IPA(X))± I(P.4(X)IPB(X))
is a distance. Normalizing we obtain
dN(PA(X).PB(X))= d(P.4(X)•Ps(X))
I(PA(X) n PB(X))
with values in [0.11.
</equation>
<bodyText confidence="0.997611">
So the selected attribute will be that one that min-
imizes the measure: d.v( Pc(X), PA(X)).
</bodyText>
<subsectionHeader confidence="0.75407">
Branching Strategy
</subsectionHeader>
<bodyText confidence="0.993018515151515">
Usual TDIDT algorithms consider a branch for each
value of the selected attribute. This strategy is not
feasible when the number of values is big (or even in-
finite). In our case the greatest number of values for
an attribute is 45 —the tag set size— which is con-
siderably big (this means that the branching factor
could be 45 at every level of the tree3). Some sys-
tems perform a previous recasting of the attributes
in order to have only binary-valued attributes and to
deal with binary trees (Magerman, 1996). This can
always be done but the resulting features lose their
intuition and direct interpretation, and explode in
number. We have chosen a mixed approach which
consist of splitting for all values and afterwards join-
ing the resulting subsets into groups for which we
have not enough statistical evidence of being differ-
ent distributions. This statistical evidence is tested
with a X2 test at a 5% level of significance. In order
to avoid zero probabilities the following smoothing
is performed. In a certain set of examples, the prob-
ability of a tag ti is estimated by
p(ti) =fl+i - .
where m is the number of possible tags and n the
number of examples.
Additionally, all the subsets that don&apos;t imply a
reduction in the classification error are joined to-
gether in order to have a bigger set of examples to
be treated in the following step of the tree construc-
tion. The classification error of a certain node is
simply: 1 - max; &lt;i‹, ((ti )) .
Experiments reported in (Marquez
and Rodriguez. 1995) show that in this way more
compact and predictive trees are obtained.
</bodyText>
<subsubsectionHeader confidence="0.568526">
Pruning the Tree
</subsubsectionHeader>
<bodyText confidence="0.985613125">
Decision trees that correctly classify all examples of
the training set are not always the most predictive
ones. This is due to the phenomenon known as over-
fitting. It occurs when the training set has a certain
amount of misclassified examples. which is obviously
the case of our training corpus (see section 5). If we
&apos;In real cases the branching factor is much lower since
not all tags appear always in all positions of the context.
</bodyText>
<equation confidence="0.717159">
-E E
</equation>
<page confidence="0.95987">
240
</page>
<bodyText confidence="0.992847642857143">
force the learning algorithm to completely classify
the examples then the resulting trees would fit also
the noisy examples.
The usual solutions to this problem are: 1) Prune
the tree. either during the construction process
(Quinlan. 1993) or afterwards (Mingers. 1989): 2)
Smooth the conditional probability distributions us-
ing fresh corpus4 (Magerman, 1996).
Since another important requirement of our prob-
lem is to have small trees we have implemented
a post-pruning technique. In a first step the
tree is completely expanded and afterwards it is
pruned following a minimal cost-complexity crite-
rion (Breiman et al.. 1984). Roughly speaking this
is a process that iteratively cut thos.e subtrees pro-
ducing only marginal benefits in accuracy, obtaining
smaller trees at each step. The trees of this sequence
are tested using a, comparatively small, fresh part of
the training set in order to decide which is the one
with the highest degree of accuracy on new exam-
ples. Experimental tests (Marquez and Rodriguez,
1995) have shown that the pruning process reduces
tree sizes at about 50% and improves their accuracy
in a 2-5%.
An Example
Finally, we present a real example of the simple ac-
quired contextual constraints for the conflict IN-RB
(preposition-adverb).
</bodyText>
<figure confidence="0.821570857142857">
P(IN)=0.81 I Pm). probability
p(zB)-_-0.19 dutnbwoon
others &amp;quot;as&amp;quot; &amp;quot;As&amp;quot;
1st right tag
others RB
C&amp;quot;&apos;&amp;quot;&amp;quot;&amp;quot;di • Pa N1.--0.0 I 3
NRB1=0.987
</figure>
<figureCaption confidence="0.999854">
Figure 2: Example of a decision tree branch.
</figureCaption>
<bodyText confidence="0.97178725">
The tree branch in figure 2 is translated into the
following constraints:
-5.81 &lt;[&amp;quot;as&amp;quot; &amp;quot;As&amp;quot;] ,IN&gt; ([RB]) ( [IN] ) ;
2.366 &lt;[&amp;quot;as&amp;quot; &amp;quot;Asi ,RB&gt; ([RB]) ( [IN] );
which express the compatibility (either positive or
negative) of the word-tag pair in angle brackets with
the given context. The compatibility value for each
constraint is the mutual information between the tag
and the context (Cover and Thomas, 1991). It is
directly computed from the probabilities in the tree.
&apos;Of course. this can be done only in the case of sta-
tistical decision trees.
</bodyText>
<sectionHeader confidence="0.990679" genericHeader="method">
4 Tagging Algorithm
</sectionHeader>
<bodyText confidence="0.99922143902439">
Usual tagging algorithms are either n-gram oriented
-such as Viterbi algorithm (Viterbi. 1967)- or ad-
hoc for every case when they must deal with more
complex information.
We use relaxation labelling as a tagging algorithm.
Relaxation labelling is a generic name for a family
of iterative algorithms which perform function opti-
mization, based on local information. See (Torras.
1989) for a summary. Its most remarkable feature is
that it can deal with any kind of constraints. thus the
model can be improved by adding any constraints
available and it makes the tagging algorithm inde-
pendent of the complexity of the model.
The algorithm has been applied to part-of-speech
tagging (PadrO. 1996), and to shallow parsing
(Voutilainen and Padro. 1997).
The algorithm is described as follows:
Let V = {v1. u2 t.„} be a set of variables
(words).
Let ti = tz„.. I be the set of possible
labels (POS tags) for variable vi.
Let CS be a set of constraints between the labels
of the variables. Each constraint C E CS states a
&amp;quot;compatibility value&amp;quot; C,. for a combination of pairs
variable-label. Any number of variables may be in-
volved in a constraint.
The aim of the algorithm is to find a weighted
labelling5 such that -global consistency&amp;quot; is maxi-
mized. Maximizing &amp;quot;global consistency&amp;quot; is defined
as maximizing for all vi, Eip x S, where pjli is
the weight for label j in variable vi and Sii the sup-
port received by the same combination. The support
for the pair variable-label expresses how compatible
that pair is with the labels of neighbouring variables,
according to the constraint set. It is a vector opti-
mization and doesn&apos;t maximize only the sum of the
supports of all variables. It finds a weighted labelling
such that any other choice wouldn&apos;t increase the sup-
port for any variable.
The support is defined as the sum of the influence
of every constraint on a label.
</bodyText>
<equation confidence="0.760709">
E In f(r)
r€R,,
</equation>
<bodyText confidence="0.922626888888889">
where:
Rj is the set of constraints on label j for variable
i, i.e. the constraints formed by any combination of
variable-label pairs that includes the pair (
/nf (r) = Cr x prkli(m) x x p.rad(m). is the prod-
uct of the current weights&apos; for the labels appearing
&apos;A weighted labelling is a weight assignment for each
label of each variable such that the weights for the labels
of the same variable add up to one.
</bodyText>
<footnote confidence="0.5410725">
674;(m) is the weight assigned to label k for variable
r at time m.
2nd nght tag
others
</footnote>
<page confidence="0.982963">
241
</page>
<bodyText confidence="0.996352">
in the constraint except (vi, t)) (representing how
applicable the constraint is in the current context)
multiplied by Cr which is the constraint compatibil-
ity value (stating how compatible the pair is with the
context).
Briefly, what the algorithm does is:
</bodyText>
<listItem confidence="0.9956295">
1. Start with a random weight assignment&amp;quot;.
2. Compute the support value for each label of
each variable.
3. Increase the weights of the labels more compat-
</listItem>
<bodyText confidence="0.52545025">
ible with the context (support greater than 0)
and decrease those of the less compatible labels
(support less than 0)8, using the updating func-
tion:
</bodyText>
<equation confidence="0.999376333333333">
pt;(m) x (1 + Sij)
/4 011 + 1) = ksi
Ep(m) x (1+ Sik)
</equation>
<bodyText confidence="0.939007333333333">
where — 1 &lt;S &lt;+1
4. If a stopping/convergence criterion&apos; is satisfied,
stop, otherwise go to step 2.
The cost of the algorithm is proportional to the
product of the number of words by the number of
constraints.
</bodyText>
<sectionHeader confidence="0.564732" genericHeader="method">
5 Description of the corpus
</sectionHeader>
<bodyText confidence="0.999965555555556">
We used the Wall Street Journal corpus to train and
test the system. We divided it in three parts: 1, 100
Kw were used as a training set, 20 Kw as a model—
tuning set, and 50 Kw as a test set.
The tag set size is 45 tags. 36.4% of the words in
the corpus are ambiguous, and the ambiguity ratio
is 2.44 tags/word over the ambiguous words, 1.52
overall.
We used a lexicon derived from training corpora,
that contains all possible tags for a word, as well
as their lexical probabilities. For the words in test
corpora not appearing in the train set, we stored
all possible tags, but no lexical probability (i.e. we
assume uniform distribution)10.
The noise in the lexicon was filtered by manually
checking the lexicon entries for the most frequent 200
words in the corpus&amp;quot; to eliminate the tags due to
errors in the training set. For instance the original
</bodyText>
<footnote confidence="0.955557125">
7 We use lexical probabilities as a starting point.
8Negative values for support indicate incompatibility.
9We use the criterion of stopping when there are no
more changes, although more sophisticated heuristic pro-
cedures are also used to stop relaxation processes (Ek-
lundh and Rosenfeld, 1978; Richards et al. , 1981).
°That is, we assumed a morphological analyzer that
provides all possible tags for unknown words.
</footnote>
<figureCaption confidence="0.5112745">
&amp;quot;The 200 most frequent words in the corpus cover
over half of it.
</figureCaption>
<bodyText confidence="0.997899266666667">
lexicon entry (numbers indicate frequencies in the
training corpus) for the very common word the was
the CD 1 DT 47715 ..1.1 7 NN 1 NNP 6 VBP 1.
since it appears in the corpus with the six differ-
ent tags: CD (cardinal), DT (determiner), JJ (ad-
jective), NN (noun), NNP (proper noun) and VBP
(verb-personal form). It is obvious that the only
correct reading for the is determiner.
The training set was used to estimate bi/trigram
statistics and to perform the constraint learning.
The model—tuning set was used to tune the algo-
rithm parameterizations, and to write the linguistic
part of the model.
The resulting models were tested in the fresh test
set.
</bodyText>
<sectionHeader confidence="0.978607" genericHeader="evaluation">
6 Experiments and results
</sectionHeader>
<bodyText confidence="0.985096615384616">
The whole WSJ corpus contains 241 different classes
of ambiguity. The 40 most representative classes12
were selected for acquiring the corresponding deci-
sion trees. That produced 40 trees totaling up to
2995 leaf nodes, and covering 83.95% of the ambigu-
ous words. Given that each tree branch produces as
many constraints as tags its leaf involves, these trees
were translated into 8473 context constraints.
We also extracted the 1404 bigram restrictions
and the 17387 trigram restrictions appearing in the
training corpus.
Finally, the model—tuning set was tagged using
a bigram model. The most common errors com-
mited by the bigram tagger were selected for manu-
ally writing the sample linguistic part of the model,
consisting of a set of 20 hand-written constraints.
From now on C will stands for the set of acquired
context constraints. B for the bigram model, T for
the trigram model, and H for the hand-written con-
straints. Any combination of these letters will indi-
cate the joining of the corresponding models (BT,
BC, BTC, etc.).
In addition, ML indicates a baseline model con-
taining no constraints (this will result in a most-
likely tagger) and HMM stands for a hidden
Markov model bigram tagger (Elworthy, 1992).
We tested the tagger on the 50 Kw test set using
all the combinations of the language models. Results
are reported below.
The effect of the acquired rules on the number of
errors for some of the most common cases is shown
in table 1. XX/YY stands for an error consisting
of a word tagged YY when it should have been XX.
Table 2 contains the meaning of all the involved tags.
Figures in table 1 show that in all cases the learned
constraints led to an improvement.
It is remarkable that when using C alone, the
number of errors is lower than with any bigram
&amp;quot;In terms of number of examples.
</bodyText>
<page confidence="0.986688">
242
</page>
<table confidence="0.999724666666667">
ML I C B BC T I TC BT &apos; BTC
JJ/NN+NN/JJ 73+137 70+94 73+112 69+102 57+103 61+95 67+101 62+93
VBD/VBN+VBN/VBD 176+190 71+66 88+69 63+56 56+57 55+57 65+60 59+61
IN/RB+RB/IN 31+132 40+69 66+107 43+17 77+68 47+67 65+98 46+8:3
VB/VBP+VBP/VB 128+147 30+26 49+43 32+27 31+32 32+18 28+:32 28+32
NN/NNP+NNP/NN 70+11 44+12 72+17 45+16 69+27 50+18 71+20 62+15
NNP/NYPS+NNPS/NNP 45+14 37+19 45+13 46+15 .54+12 51+12 .53+14 51+14
that&amp;quot; 187 53 66 45 60 40 57 45
Total 1341 631 820 630 703 603 731 651
</table>
<tableCaption confidence="0.996336">
Table 1: Number of some common errors commited by each model
</tableCaption>
<table confidence="0.890190303030303">
NN
JJ
VBD
VBN
RB
IN
VB
VBP
NNP
NNPS
Noun
Adjective
Verb - past tense
Verb - past participle
Adverb
Preposition
Verb - base form
Verb - personal form
Proper noun
Plural proper noun
C 91.96% 97.08%
BC 92.72% 97.36%
TC 92.32% 97.39%
BTC 92.5:5% 97.29%
ambiguous
overall
91.35%
91.82%
96.86%
97.03%
BT
91.92%
97.06%
</table>
<tableCaption confidence="0.980638333333333">
Table 4: Results of our tagger using every combination
of constraint kinds
Table 2: Tag meanings
</tableCaption>
<bodyText confidence="0.999697733333333">
and/or trigram model, that is, the acquired model
performs better than the others estimated from the
same training corpus.
We also find that the cooperation of a bigram or
trigram model with the acquired one, produces even
better results. This is not true in the cooperation
of bigrams and trigrams with acquired constraints
(BTC), in this case the synergy is not enough to get
a better joint result. This might be due to the fact
that the noise in B and T adds up and overwhelms
the context constraints.
The results obtained by the baseline taggers can
be found in table 3 and the results obtained using all
the learned constraints together with the bi/trigram
models in table 4.
</bodyText>
<tableCaption confidence="0.993239">
Table 3: Results of the baseline taggers
</tableCaption>
<bodyText confidence="0.999256071428572">
On the one hand, the results in tables 3 and 4
show that our tagger performs slightly worse than a
HMM tagger in the same conditions13, that is, when
using only bigram information.
13Hand analysis of the errors commited by the algo-
rithm suggest that the worse results may be due to noise
in the training and test corpora, i.e., relaxation algo-
rithm seems to be more noise-sensitive than a Markov
model. Further research is required on this point.
On the other hand, those results also show that
since our tagger is more flexible than a HNIM, it can
easily accept more complex information to improve
its results up to 97.39% without modifying the algo-
rithm.
</bodyText>
<table confidence="0.934172777777778">
ambiguous overall
H 86.41% 95.06%
BH 91.88% 97.05%
TH 92.04% 97.11%
BTH 92.32% 97.21%
CH 91.97% 97.08%
BCH 92.76% 97.37%
TCH 92.98% 97.45%
BTCH 92.71% 97.35%
</table>
<tableCaption confidence="0.8555675">
Table 5: Results of our tagger using every combination
of constraint kinds and hand written constraints
</tableCaption>
<bodyText confidence="0.998946071428571">
Table 5 shows the results adding the hand written
constraints. The hand written set is very small and
only covers a few common error cases. That pro-
duces poor results when using them alone (H). but
they are good enough to raise the results given by
the automatically acquired models up to 97.45%.
Although the improvement obtained might seem
small, it must be taken into account that we are
moving very close to the best achievable result with
these techniques.
First, some ambiguities can only be solved with
semantic information, such as the Noun-Adjective
ambiguity for word principal in the phrase the prin-
cipal office. It could be an adjective, meaning the
</bodyText>
<figure confidence="0.791962">
ambiguous 1 overall
ML 85.31%
HMM 91.75%
94,66%
97.00%
</figure>
<page confidence="0.996452">
243
</page>
<bodyText confidence="0.995917909090909">
mazn office, or a noun, meaning the school head of-
fice.
Second, the WSJ corpus contains noise (mistagged
words) that affects both the training and the test
sets. The noise in the training set produces noisy
—and so less precise— models. In the test set, it pro-
duces a wrong estimation of accuracy, since correct
answers are computed as wrong and vice-versa.
For instance, verb participle forms are sometimes
tagged as such ( VBN) and also as adjectives (JJ) in
other sentences with no structural differences:
</bodyText>
<listItem confidence="0.92654">
• ... f ailing_VBG to_TO voluntarily_RB
submit_VB the_DT reguested_VBN
inf ormation_NN . . .
• . . . a_DT large_JJ sample_NN of _IN
married_JJ women.NNS with_IN at_IN
least _JJS one_CD child_NN . . .
</listItem>
<bodyText confidence="0.992058333333333">
Another structure not coherently tagged are noun
chains when the nouns are ambiguous and can be
also adjectives:
</bodyText>
<listItem confidence="0.7518885">
• ... Mr . _NNP Hahn_NNP , the_DT
62-year-old_JJ chairman_NN and_CC
</listItem>
<figure confidence="0.5872246">
chief_NN executive_JJ officer_NN of _IN
Georgia-Pacif ic_NNP Corp . _NNP . . .
• . . . Burger_NNP King_NNP
&apos; s_POS chief_JJ executive_NN officer_NN
Barry_NNP Gibbons_NNP , stars_VBZ
in_IN ads_NNS saying_VBG
• . . . and_CC Barrett_NNP B . _NNP
Weekes_BNP , chairman_NN ,
president_NN and_CC chief_JJ executive_JJ
officer_NN
</figure>
<listItem confidence="0.62264225">
• ... the_DT company_NN includes_VBZ
Neil_NNP Davenport_NNP , 47_CD ,
president_NN and_CC chief_NN executive_NN
officer_NN ;_:
</listItem>
<bodyText confidence="0.999777333333333">
All this makes that the performance cannot reach
100%, and that an accurate analysis of the noise in
WSJ corpus should be performed to estimate the
actual upper bound that a tagger can achieve on
these data. This issue will be addressed in further
work.
</bodyText>
<sectionHeader confidence="0.999124" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999973956521739">
We have presented an automatic constraint learning
algorithm based on statistical decision trees.
We have used the acquired constraints in a part—
of—speech tagger that allows combining any kind of
constraints in the language model.
The results obtained show a clear improvement in
the performance when the automatically acquired
constraints are added to the model. That indicates
that relaxation labelling is a flexible algorithm able
to combine properly different information kinds, and
that the constraints acquired by the learning algo-
rithm capture relevant context information that was
not included in the n—gram models.
It is difficult to compare the results to other works,
since the accuracy varies greatly depending on the
corpus, the tag set, and the lexicon or morphological
analyzer used. The more similar conditions reported
in previous work are those experiments performed
on the WSJ corpus: (Brill, 1992) reports 3-4% er-
ror rate, and (Daelemans et al., 1996) report 96.7%
accuracy. We obtained a 97.39% accuracy with tri-
grams plus automatically acquired constraints, and
97.45% when hand written constraints were added.
</bodyText>
<sectionHeader confidence="0.997905" genericHeader="acknowledgments">
8 Further Work
</sectionHeader>
<bodyText confidence="0.9888075">
Further work is still to be done in the following di-
rections:
</bodyText>
<listItem confidence="0.82006225">
• Perform a thorough analysis of the noise in
the WSJ corpus to determine a realistic upper
bound for the performance that can be expected
from a POS tagger.
</listItem>
<bodyText confidence="0.821443">
On the constraint learning algorithm:
</bodyText>
<listItem confidence="0.96404405882353">
• Consider more complex context features, such
as non—limited distance or barrier rules in the
style of (Samuelsson et al., 1996).
• Take into account morphological, semantic and
other kinds of information.
• Perform a global smoothing to deal with low—
frequency ambiguity classes.
On the tagging algorithms
• Study the convergence properties of the algo-
rithm to decide whether the lower results at
convergence are produced by the noise in the
corpus.
• Use back-off techniques to minimize inter-
ferences between statistical and learned con-
straints.
• Use the algorithm to perform simultaneously
POS tagging and word sense disambiguation,
</listItem>
<bodyText confidence="0.7470465">
to take advantage of cross influences between
both kinds of information.
</bodyText>
<sectionHeader confidence="0.997427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998551875">
D.W. Aha, D. Kibler and M. Albert. 1991 Instance—
based learning algorithms. In Machine Learning.
7:37-66. Belmont, California.
L. Breiman, J.H. Friedman, R.A. Olshen and
C.J. Stone. 1984 Classification and Regression
Trees. The Wadsworth Statistics/ Probability Se-
ries. Wadsworth International Group, Belmont,
California.
</reference>
<page confidence="0.977899">
244
</page>
<reference confidence="0.999735435643564">
E. Brill. 1992 A Simple Rule-Based Part-of-Speech.
In Proceedings of the Third Conference on Applied
Natural Language Processing. ACL.
E. Brill. 1995 Unsupervised Learning of Disam-
biguation Rules for Part-of-speech Tagging. In
Proceedings of 3rd Workshop on Very Large Cor-
pora. Massachusetts.
T.M. Cover and J.A. Thomas (Editors) 1991 Ele-
ments of information theory. John Wiley &amp; Sons.
D. Cutting, J. Kupiec, J. Pederson and P. Sibun.
1992 A Practical Part-of-Speech Tagger. In Pro-
ceedings of the Third Conference on Applied Nat-
ural Language Processing., ACL,
J. Eklundh and A. Rosenfeld. 1978 Convergence
Properties of Relaxation Labelling. Technical Re-
port no. 701. Computer Science Center. Univer-
sity of Maryland.
D. Elworthy. 1993 Part-of-Speech and Phrasal
Tagging. Technical report, SPRIT BRA-7315 Ac-
quilex II, Working Paper WP #10.
W. Daelemans, J. Zavrel, P. Berck and S. Gillis.
1996 MTB: A Memory-Based Part-of-Speech
Tagger Generator. In Proceedings of 4th Work-
shop on Very Large Corpora. Copenhagen, Den-
mark.
R. Garside, G. Leech and G. Sampson (Editors)
1987 The Computational Analysis of English.
London and New York: Longman.
D. Hindle. 1989 Acquiring disambiguation rules
from text. In Proceedings ACL&apos;89.
F. Karlsson 1990 Constraint Grammar as a Frame-
work for Parsing Running Text. In H. Karlgren
(ed.), Papers presented to the 13th International
Conference on Computational Linguistics, Vol. 3.
Helsinki. 168-173.
F. Karlsson, A. Voutilainen, J. Heikkila and
A. Anttila. (Editors) 1995 Constraint Grammar:
A Language-Independent System for Parsing Un-
restricted Text. Mouton de Gruyter, Berlin and
New York.
R,. Lopez. 1991 A Distance-Based Attribute Selec-
tion Measure for Decision Tree Induction. Ma-
chine Learning. Kluwer Academic.
R.M. Losee. 1994 Learning Syntactic Rules and
Tags with Genetic Algorithms for Information
Retrieval and Filtering: An Empirical Basis for
Grammatical Rules. Information Processing
Management, May.
M. Magerman. 1996 Learning Grammatical Struc-
ture Using Statistical Decision-Trees. In Lecture
Notes in Artificial Intelligence 1147. Grammatical
Inference: Learning Syntax from Sentences. Pro-
ceedings ICGI-96. Springer.
. Marquez and H. Rodriguez. 1995 Towards Learn-
ing a Constraint Grammar from Annotated Cor-
pora Using Decision Trees. ESPRIT BRA-7315
Acquilex II, Working Paper.
F. McCarthy and W.G. Lehnert. 1995 Using De-
cision Trees for Coreference Resolution. In Pro-
ceedings of 14th International Joint Conference on
Artificial Intelligence (IJCAP95).
Mingers. 1989 An Empirical Comparison of Se-
lection Measures for Decision-Tree Induction. In
Machine Learning. 3:319-342.
Mingers. 1989 An Empirical Comparison of Prun-
ing Methods for Decision-Tree Induction. In Ma-
chine Learning. 4:227-243.
. PadrO. 1996 POS Tagging Using Relaxation
Labelling. In Proceedings of 16th International
Conference on Computational Linguistics. Copen-
hagen, Denmark.
R. Quinlan. 1986 Induction of Decision Trees. In
Machine Learning. 1:81-106.
R. Quinlan. 1993 C4.5: Programs for Machine
Learning. San Mateo, CA. Morgan Kaufmann.
Richards, D. Landgrebe and P. Swain. 1981 On
the accuracy of pixel relaxation labelling. IEEE
Transactions on System, Man and Cybernetics.
Vol. SMC-11
. Samuelsson, P. Tapanainen and A. Voutilainen.
1996 Inducing Constraint Grammars. In Pro-
ceedings of the 3rd International Colloquium on
Grammatical Inference.
. Schmid 1994 Part-of-speech tagging with neu-
ral networks. In Proceedings of 15th International
Conference on Computational Linguistics. Kyoto,
Japan.
. Torras. 1989 Relaxation and Neural Learning:
Points of Convergence and Divergence. Journal
of Parallel and Distributed Computing. 6:217-244
.J. Viterbi. 1967 Error bounds for convolutional
codes and an asymptotically optimal decoding al-
gorithm. In IEEE Transactions on Information
Theory. pg 260-269, April.
Voutilainen and T. Jarvinen. 1995 Specifying
a shallow grammatical representation for parsing
purposes. In Proceedings of the 7th meeting of the
European Association for Computational Linguis-
tics. 210-214.
. Voutilainen and L. Padre). 1997 Developing a
Hybrid NP Parser. In Proceedings of A NLP&apos;97.
</reference>
<page confidence="0.998715">
245
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.224309">
<title confidence="0.995672">A Flexible POS Tagger Using an Automatically Acquired Language Model*</title>
<author confidence="0.964218">Lluis Marquez</author>
<affiliation confidence="0.96461">LSI - UPC</affiliation>
<address confidence="0.959311">c/ Jordi Girona 1-3 08034 Barcelona. Catalonia</address>
<email confidence="0.947747">lluismOlsi.upc.es</email>
<author confidence="0.354426">Lluis Padr</author>
<affiliation confidence="0.603571">LSI - UPC</affiliation>
<address confidence="0.8765945">c/ Jordi Girona 1-3 08034 Barcelona. Catalonia</address>
<email confidence="0.960265">padroOlsi.upc.es</email>
<abstract confidence="0.993507142857143">We present an algorithm that automatically learns context constraints using statistical decision trees. We then use the acquired constraints in a flexible POS tagger. The tagger is able to use information of any degree: n-grams, automatically learned context constraints, linguistically motivated manually written constraints, etc. The sources and kinds of constraints are unrestricted, and the language model can be easily extended, improving the results. The tagger has been tested and evaluated on the WSJ corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D W Aha</author>
<author>D Kibler</author>
<author>M Albert</author>
</authors>
<title>Instance— based learning algorithms.</title>
<date>1991</date>
<booktitle>In Machine Learning.</booktitle>
<pages>7--37</pages>
<location>Belmont, California.</location>
<contexts>
<context position="2150" citStr="Aha et al., 1991" startWordPosition="330" endWordPosition="333">weights, etc. The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al., 1996; Marquez and Rodriguez, 1995; Samuelsson et al., 1996). The acquisition methods range from supervised—inductive— learning—from—example algorithms (Quinlan, 1986; •This research has been partially funded by the Spanish Research Department (CICYT) and inscribed as TIC96-1243-0O3-02 &apos;When the model is obtained from annotated corpora we talk about supervised learning, when it is obtained from raw corpora training is considered unsupervised. Aha et al., 1991) to genetic algorithm strategies (Losee, 1994), through the transformation—based error—driven algorithm used in (Brill, 1995). Still another possibility are the hybrid models, which try to join the advantages of both approaches ( Voutilainen and PadrO, 1997). We present in this paper a hybrid approach that puts together both trends in automatic approach and the linguistic approach. We describe a POS tagger based on the work described in (Padr6, 1996), that is able to use bi/trigram information, automatically learned context constraints and linguistically motivated manually written constraints.</context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>D.W. Aha, D. Kibler and M. Albert. 1991 Instance— based learning algorithms. In Machine Learning. 7:37-66. Belmont, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
<author>J H Friedman</author>
<author>R A Olshen</author>
<author>C J Stone</author>
</authors>
<title>Classification and Regression Trees. The Wadsworth Statistics/ Probability Series.</title>
<date>1984</date>
<publisher>Wadsworth International Group,</publisher>
<location>Belmont, California.</location>
<contexts>
<context position="8708" citStr="Breiman et al., 1984" startWordPosition="1405" endWordPosition="1408">tain node, either when all (or almost all) the remaining examples belong to the same class, or when the number of examples is too small. These nodes are the leafs of the tree and contain the conditional probability distribution, of its associated subset of examples, on the possible classes. The heuristic function for selecting the most useful attribute at each step is of a crucial importance in order to obtain simple trees, since no backtracking is performed. There exist two main families of attribute-selecting functions: information-based (Quinlan, 1986; Lopez, 1991) and statistically-based (Breiman et al., 1984; Mingers, 1989). Training Set For each class of POS ambiguity the initial example set is built by selecting from the training corpus 2Classes of ambiguity are determined by the groups of possible tags for the words in the corpus, i.e, nounadjective, noun-adjective-verb, preposition-adverb, etc. 239 all the occurrences of the words belonging to this ambiguity class. More particularly, the set of attributes that describe each example consists of the part-of-speech tags of the neighbour words, and the information about the word itself (orthography and the proper tag in its context). The window c</context>
</contexts>
<marker>Breiman, Friedman, Olshen, Stone, 1984</marker>
<rawString>L. Breiman, J.H. Friedman, R.A. Olshen and C.J. Stone. 1984 Classification and Regression Trees. The Wadsworth Statistics/ Probability Series. Wadsworth International Group, Belmont, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A Simple Rule-Based Part-of-Speech.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing.</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="28556" citStr="Brill, 1992" startWordPosition="4801" endWordPosition="4802">automatically acquired constraints are added to the model. That indicates that relaxation labelling is a flexible algorithm able to combine properly different information kinds, and that the constraints acquired by the learning algorithm capture relevant context information that was not included in the n—gram models. It is difficult to compare the results to other works, since the accuracy varies greatly depending on the corpus, the tag set, and the lexicon or morphological analyzer used. The more similar conditions reported in previous work are those experiments performed on the WSJ corpus: (Brill, 1992) reports 3-4% error rate, and (Daelemans et al., 1996) report 96.7% accuracy. We obtained a 97.39% accuracy with trigrams plus automatically acquired constraints, and 97.45% when hand written constraints were added. 8 Further Work Further work is still to be done in the following directions: • Perform a thorough analysis of the noise in the WSJ corpus to determine a realistic upper bound for the performance that can be expected from a POS tagger. On the constraint learning algorithm: • Consider more complex context features, such as non—limited distance or barrier rules in the style of (Samuel</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>E. Brill. 1992 A Simple Rule-Based Part-of-Speech. In Proceedings of the Third Conference on Applied Natural Language Processing. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Unsupervised Learning of Disambiguation Rules for Part-of-speech Tagging.</title>
<date>1995</date>
<booktitle>In Proceedings of 3rd Workshop on Very Large Corpora.</booktitle>
<location>Massachusetts.</location>
<contexts>
<context position="2275" citStr="Brill, 1995" startWordPosition="347" endWordPosition="348">rees (Daelemans et al., 1996; Marquez and Rodriguez, 1995; Samuelsson et al., 1996). The acquisition methods range from supervised—inductive— learning—from—example algorithms (Quinlan, 1986; •This research has been partially funded by the Spanish Research Department (CICYT) and inscribed as TIC96-1243-0O3-02 &apos;When the model is obtained from annotated corpora we talk about supervised learning, when it is obtained from raw corpora training is considered unsupervised. Aha et al., 1991) to genetic algorithm strategies (Losee, 1994), through the transformation—based error—driven algorithm used in (Brill, 1995). Still another possibility are the hybrid models, which try to join the advantages of both approaches ( Voutilainen and PadrO, 1997). We present in this paper a hybrid approach that puts together both trends in automatic approach and the linguistic approach. We describe a POS tagger based on the work described in (Padr6, 1996), that is able to use bi/trigram information, automatically learned context constraints and linguistically motivated manually written constraints. The sources and kinds of constraints are unrestricted, and the language model can be easily extended. The structure of the t</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995 Unsupervised Learning of Disambiguation Rules for Part-of-speech Tagging. In Proceedings of 3rd Workshop on Very Large Corpora. Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of information theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="15485" citStr="Cover and Thomas, 1991" startWordPosition="2561" endWordPosition="2564">e conflict IN-RB (preposition-adverb). P(IN)=0.81 I Pm). probability p(zB)-_-0.19 dutnbwoon others &amp;quot;as&amp;quot; &amp;quot;As&amp;quot; 1st right tag others RB C&amp;quot;&apos;&amp;quot;&amp;quot;&amp;quot;di • Pa N1.--0.0 I 3 NRB1=0.987 Figure 2: Example of a decision tree branch. The tree branch in figure 2 is translated into the following constraints: -5.81 &lt;[&amp;quot;as&amp;quot; &amp;quot;As&amp;quot;] ,IN&gt; ([RB]) ( [IN] ) ; 2.366 &lt;[&amp;quot;as&amp;quot; &amp;quot;Asi ,RB&gt; ([RB]) ( [IN] ); which express the compatibility (either positive or negative) of the word-tag pair in angle brackets with the given context. The compatibility value for each constraint is the mutual information between the tag and the context (Cover and Thomas, 1991). It is directly computed from the probabilities in the tree. &apos;Of course. this can be done only in the case of statistical decision trees. 4 Tagging Algorithm Usual tagging algorithms are either n-gram oriented -such as Viterbi algorithm (Viterbi. 1967)- or adhoc for every case when they must deal with more complex information. We use relaxation labelling as a tagging algorithm. Relaxation labelling is a generic name for a family of iterative algorithms which perform function optimization, based on local information. See (Torras. 1989) for a summary. Its most remarkable feature is that it can </context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>T.M. Cover and J.A. Thomas (Editors) 1991 Elements of information theory. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cutting</author>
<author>J Kupiec</author>
<author>J Pederson</author>
<author>P Sibun</author>
</authors>
<title>A Practical Part-of-Speech Tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing., ACL,</booktitle>
<marker>Cutting, Kupiec, Pederson, Sibun, 1992</marker>
<rawString>D. Cutting, J. Kupiec, J. Pederson and P. Sibun. 1992 A Practical Part-of-Speech Tagger. In Proceedings of the Third Conference on Applied Natural Language Processing., ACL,</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eklundh</author>
<author>A Rosenfeld</author>
</authors>
<title>Convergence Properties of Relaxation Labelling.</title>
<date>1978</date>
<tech>Technical Report no. 701.</tech>
<institution>Computer Science Center. University of Maryland.</institution>
<contexts>
<context position="20028" citStr="Eklundh and Rosenfeld, 1978" startWordPosition="3360" endWordPosition="3364">appearing in the train set, we stored all possible tags, but no lexical probability (i.e. we assume uniform distribution)10. The noise in the lexicon was filtered by manually checking the lexicon entries for the most frequent 200 words in the corpus&amp;quot; to eliminate the tags due to errors in the training set. For instance the original 7 We use lexical probabilities as a starting point. 8Negative values for support indicate incompatibility. 9We use the criterion of stopping when there are no more changes, although more sophisticated heuristic procedures are also used to stop relaxation processes (Eklundh and Rosenfeld, 1978; Richards et al. , 1981). °That is, we assumed a morphological analyzer that provides all possible tags for unknown words. &amp;quot;The 200 most frequent words in the corpus cover over half of it. lexicon entry (numbers indicate frequencies in the training corpus) for the very common word the was the CD 1 DT 47715 ..1.1 7 NN 1 NNP 6 VBP 1. since it appears in the corpus with the six different tags: CD (cardinal), DT (determiner), JJ (adjective), NN (noun), NNP (proper noun) and VBP (verb-personal form). It is obvious that the only correct reading for the is determiner. The training set was used to es</context>
</contexts>
<marker>Eklundh, Rosenfeld, 1978</marker>
<rawString>J. Eklundh and A. Rosenfeld. 1978 Convergence Properties of Relaxation Labelling. Technical Report no. 701. Computer Science Center. University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Part-of-Speech and Phrasal Tagging.</title>
<date>1993</date>
<booktitle>SPRIT BRA-7315 Acquilex II, Working Paper WP #10.</booktitle>
<tech>Technical report,</tech>
<marker>Elworthy, 1993</marker>
<rawString>D. Elworthy. 1993 Part-of-Speech and Phrasal Tagging. Technical report, SPRIT BRA-7315 Acquilex II, Working Paper WP #10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>MTB: A Memory-Based Part-of-Speech Tagger Generator.</title>
<date>1996</date>
<booktitle>In Proceedings of 4th Workshop on Very Large Corpora.</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1691" citStr="Daelemans et al., 1996" startWordPosition="266" endWordPosition="269">es or constraints (Voutilainen and Jarvinen, 1995). Second, the automatic approach, in which the model is automatically obtained from corpora (either raw or annotated)&apos;, and consists of n—grams (Garside et at., 1987; Cutting et at., 1992), rules (Hindle, 1989) or neural nets (Schmid, 1994). In the automatic approach we can distinguish two main trends: The low—level data trend collects statistics from the training corpora in the form of n—grams, probabilities, weights, etc. The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al., 1996; Marquez and Rodriguez, 1995; Samuelsson et al., 1996). The acquisition methods range from supervised—inductive— learning—from—example algorithms (Quinlan, 1986; •This research has been partially funded by the Spanish Research Department (CICYT) and inscribed as TIC96-1243-0O3-02 &apos;When the model is obtained from annotated corpora we talk about supervised learning, when it is obtained from raw corpora training is considered unsupervised. Aha et al., 1991) to genetic algorithm strategies (Losee, 1994), through the transformation—based error—driven algorithm used in (Brill, 1995). Still another </context>
<context position="5685" citStr="Daelemans et al., 1996" startWordPosition="916" endWordPosition="920">ther phrase). The constraint language is able to express the same kind of patterns than the Constraint Grammar formalism (Karlsson et al., 1995), although in a different formalism. In addition, each constraint has a compatibility value that indicates its strength. In the middle run, the system will be adapted to accept CGs. 3 Constraint Acquisition Choosing, from a set of possible tags, the proper syntactic tag for a word in a particular context can be seen as a problem of classification. Decision trees, recently used in NLP basic tasks such as tagging and parsing (McCarthy and Lehnert, 1995; Daelemans et al., 1996; Magerman, 1996), are suitable for performing this task. A decision tree is a n-ary branching tree that represents a classification rule for classifying the objects of a certain domain into a set of mutually exclusive classes. The domain objects are described as a set of attribute-value pairs, where each attribute measures a relevant feature of an object taking a (ideally small) set of discrete, mutually incompatible values. Each non-terminal node of a decision tree represents a question on (usually) one attribute. For each possible value of this attribute there is a branch to follow. Leaf no</context>
<context position="28610" citStr="Daelemans et al., 1996" startWordPosition="4809" endWordPosition="4812">d to the model. That indicates that relaxation labelling is a flexible algorithm able to combine properly different information kinds, and that the constraints acquired by the learning algorithm capture relevant context information that was not included in the n—gram models. It is difficult to compare the results to other works, since the accuracy varies greatly depending on the corpus, the tag set, and the lexicon or morphological analyzer used. The more similar conditions reported in previous work are those experiments performed on the WSJ corpus: (Brill, 1992) reports 3-4% error rate, and (Daelemans et al., 1996) report 96.7% accuracy. We obtained a 97.39% accuracy with trigrams plus automatically acquired constraints, and 97.45% when hand written constraints were added. 8 Further Work Further work is still to be done in the following directions: • Perform a thorough analysis of the noise in the WSJ corpus to determine a realistic upper bound for the performance that can be expected from a POS tagger. On the constraint learning algorithm: • Consider more complex context features, such as non—limited distance or barrier rules in the style of (Samuelsson et al., 1996). • Take into account morphological,</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>W. Daelemans, J. Zavrel, P. Berck and S. Gillis. 1996 MTB: A Memory-Based Part-of-Speech Tagger Generator. In Proceedings of 4th Workshop on Very Large Corpora. Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garside</author>
<author>G Leech</author>
<author>G Sampson</author>
</authors>
<title>The Computational Analysis of English. London and</title>
<date>1987</date>
<location>New York: Longman.</location>
<marker>Garside, Leech, Sampson, 1987</marker>
<rawString>R. Garside, G. Leech and G. Sampson (Editors) 1987 The Computational Analysis of English. London and New York: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Acquiring disambiguation rules from text.</title>
<date>1989</date>
<booktitle>In Proceedings ACL&apos;89.</booktitle>
<contexts>
<context position="1329" citStr="Hindle, 1989" startWordPosition="212" endWordPosition="213">agger has been tested and evaluated on the WSJ corpus. 1 Introduction In NLP, it is necessary to model the language in a representation suitable for the task to be performed. The language models more commonly used are based on two main approaches: first, the linguistic approach, in which the model is written by a linguist, generally in the form of rules or constraints (Voutilainen and Jarvinen, 1995). Second, the automatic approach, in which the model is automatically obtained from corpora (either raw or annotated)&apos;, and consists of n—grams (Garside et at., 1987; Cutting et at., 1992), rules (Hindle, 1989) or neural nets (Schmid, 1994). In the automatic approach we can distinguish two main trends: The low—level data trend collects statistics from the training corpora in the form of n—grams, probabilities, weights, etc. The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al., 1996; Marquez and Rodriguez, 1995; Samuelsson et al., 1996). The acquisition methods range from supervised—inductive— learning—from—example algorithms (Quinlan, 1986; •This research has been partially funded by the Spanish Research Department</context>
</contexts>
<marker>Hindle, 1989</marker>
<rawString>D. Hindle. 1989 Acquiring disambiguation rules from text. In Proceedings ACL&apos;89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Karlsson</author>
</authors>
<title>Constraint Grammar as a Framework for Parsing Running Text.</title>
<date>1990</date>
<booktitle>Papers presented to the 13th International Conference on Computational Linguistics,</booktitle>
<volume>3</volume>
<pages>168--173</pages>
<editor>In H. Karlgren (ed.),</editor>
<marker>Karlsson, 1990</marker>
<rawString>F. Karlsson 1990 Constraint Grammar as a Framework for Parsing Running Text. In H. Karlgren (ed.), Papers presented to the 13th International Conference on Computational Linguistics, Vol. 3. Helsinki. 168-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Karlsson</author>
<author>A Voutilainen</author>
<author>J Heikkila</author>
<author>A Anttila</author>
</authors>
<title>Constraint Grammar: A Language-Independent System for Parsing Unrestricted Text. Mouton de Gruyter,</title>
<date>1995</date>
<location>Berlin and New York.</location>
<contexts>
<context position="5207" citStr="Karlsson et al., 1995" startWordPosition="836" endWordPosition="839">etween don&apos;t have any of the tags in the set [VBN IN , : JJ JJS JJR]. That is, this rule raises the support for the tag past participle when there is an auxiliary verb to the left but only if there is not another candidate to be a past participle or an adjective inbetween. The tags [IN , :] prevent the rule from being applied when the auxiliary verb and the participle are in two different phrases (a comma, a colon or a preposition are considered to mark the beginning of another phrase). The constraint language is able to express the same kind of patterns than the Constraint Grammar formalism (Karlsson et al., 1995), although in a different formalism. In addition, each constraint has a compatibility value that indicates its strength. In the middle run, the system will be adapted to accept CGs. 3 Constraint Acquisition Choosing, from a set of possible tags, the proper syntactic tag for a word in a particular context can be seen as a problem of classification. Decision trees, recently used in NLP basic tasks such as tagging and parsing (McCarthy and Lehnert, 1995; Daelemans et al., 1996; Magerman, 1996), are suitable for performing this task. A decision tree is a n-ary branching tree that represents a clas</context>
</contexts>
<marker>Karlsson, Voutilainen, Heikkila, Anttila, 1995</marker>
<rawString>F. Karlsson, A. Voutilainen, J. Heikkila and A. Anttila. (Editors) 1995 Constraint Grammar: A Language-Independent System for Parsing Unrestricted Text. Mouton de Gruyter, Berlin and New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R</author>
</authors>
<title>A Distance-Based Attribute Selection Measure for Decision Tree Induction. Machine Learning.</title>
<date>1991</date>
<publisher>Kluwer Academic.</publisher>
<marker>R, 1991</marker>
<rawString>R,. Lopez. 1991 A Distance-Based Attribute Selection Measure for Decision Tree Induction. Machine Learning. Kluwer Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Losee</author>
</authors>
<title>Learning Syntactic Rules and Tags with Genetic Algorithms for Information Retrieval and Filtering: An Empirical Basis for Grammatical Rules. Information Processing</title>
<date>1994</date>
<tech>Management,</tech>
<contexts>
<context position="2196" citStr="Losee, 1994" startWordPosition="338" endWordPosition="339">re sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al., 1996; Marquez and Rodriguez, 1995; Samuelsson et al., 1996). The acquisition methods range from supervised—inductive— learning—from—example algorithms (Quinlan, 1986; •This research has been partially funded by the Spanish Research Department (CICYT) and inscribed as TIC96-1243-0O3-02 &apos;When the model is obtained from annotated corpora we talk about supervised learning, when it is obtained from raw corpora training is considered unsupervised. Aha et al., 1991) to genetic algorithm strategies (Losee, 1994), through the transformation—based error—driven algorithm used in (Brill, 1995). Still another possibility are the hybrid models, which try to join the advantages of both approaches ( Voutilainen and PadrO, 1997). We present in this paper a hybrid approach that puts together both trends in automatic approach and the linguistic approach. We describe a POS tagger based on the work described in (Padr6, 1996), that is able to use bi/trigram information, automatically learned context constraints and linguistically motivated manually written constraints. The sources and kinds of constraints are unre</context>
</contexts>
<marker>Losee, 1994</marker>
<rawString>R.M. Losee. 1994 Learning Syntactic Rules and Tags with Genetic Algorithms for Information Retrieval and Filtering: An Empirical Basis for Grammatical Rules. Information Processing Management, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Magerman</author>
</authors>
<title>Learning Grammatical Structure Using Statistical Decision-Trees.</title>
<date>1996</date>
<booktitle>In Lecture Notes in Artificial Intelligence 1147. Grammatical Inference: Learning Syntax from Sentences. Proceedings ICGI-96.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="5702" citStr="Magerman, 1996" startWordPosition="921" endWordPosition="922">aint language is able to express the same kind of patterns than the Constraint Grammar formalism (Karlsson et al., 1995), although in a different formalism. In addition, each constraint has a compatibility value that indicates its strength. In the middle run, the system will be adapted to accept CGs. 3 Constraint Acquisition Choosing, from a set of possible tags, the proper syntactic tag for a word in a particular context can be seen as a problem of classification. Decision trees, recently used in NLP basic tasks such as tagging and parsing (McCarthy and Lehnert, 1995; Daelemans et al., 1996; Magerman, 1996), are suitable for performing this task. A decision tree is a n-ary branching tree that represents a classification rule for classifying the objects of a certain domain into a set of mutually exclusive classes. The domain objects are described as a set of attribute-value pairs, where each attribute measures a relevant feature of an object taking a (ideally small) set of discrete, mutually incompatible values. Each non-terminal node of a decision tree represents a question on (usually) one attribute. For each possible value of this attribute there is a branch to follow. Leaf nodes represent con</context>
<context position="12108" citStr="Magerman, 1996" startWordPosition="1997" endWordPosition="1998">he selected attribute will be that one that minimizes the measure: d.v( Pc(X), PA(X)). Branching Strategy Usual TDIDT algorithms consider a branch for each value of the selected attribute. This strategy is not feasible when the number of values is big (or even infinite). In our case the greatest number of values for an attribute is 45 —the tag set size— which is considerably big (this means that the branching factor could be 45 at every level of the tree3). Some systems perform a previous recasting of the attributes in order to have only binary-valued attributes and to deal with binary trees (Magerman, 1996). This can always be done but the resulting features lose their intuition and direct interpretation, and explode in number. We have chosen a mixed approach which consist of splitting for all values and afterwards joining the resulting subsets into groups for which we have not enough statistical evidence of being different distributions. This statistical evidence is tested with a X2 test at a 5% level of significance. In order to avoid zero probabilities the following smoothing is performed. In a certain set of examples, the probability of a tag ti is estimated by p(ti) =fl+i - . where m is the</context>
<context position="13996" citStr="Magerman, 1996" startWordPosition="2319" endWordPosition="2320">ning set has a certain amount of misclassified examples. which is obviously the case of our training corpus (see section 5). If we &apos;In real cases the branching factor is much lower since not all tags appear always in all positions of the context. -E E 240 force the learning algorithm to completely classify the examples then the resulting trees would fit also the noisy examples. The usual solutions to this problem are: 1) Prune the tree. either during the construction process (Quinlan. 1993) or afterwards (Mingers. 1989): 2) Smooth the conditional probability distributions using fresh corpus4 (Magerman, 1996). Since another important requirement of our problem is to have small trees we have implemented a post-pruning technique. In a first step the tree is completely expanded and afterwards it is pruned following a minimal cost-complexity criterion (Breiman et al.. 1984). Roughly speaking this is a process that iteratively cut thos.e subtrees producing only marginal benefits in accuracy, obtaining smaller trees at each step. The trees of this sequence are tested using a, comparatively small, fresh part of the training set in order to decide which is the one with the highest degree of accuracy on ne</context>
</contexts>
<marker>Magerman, 1996</marker>
<rawString>M. Magerman. 1996 Learning Grammatical Structure Using Statistical Decision-Trees. In Lecture Notes in Artificial Intelligence 1147. Grammatical Inference: Learning Syntax from Sentences. Proceedings ICGI-96. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marquez</author>
<author>H Rodriguez</author>
</authors>
<title>Towards Learning a Constraint Grammar from Annotated Corpora Using Decision Trees.</title>
<date>1995</date>
<booktitle>ESPRIT BRA-7315 Acquilex II, Working Paper.</booktitle>
<contexts>
<context position="1720" citStr="Marquez and Rodriguez, 1995" startWordPosition="270" endWordPosition="273">lainen and Jarvinen, 1995). Second, the automatic approach, in which the model is automatically obtained from corpora (either raw or annotated)&apos;, and consists of n—grams (Garside et at., 1987; Cutting et at., 1992), rules (Hindle, 1989) or neural nets (Schmid, 1994). In the automatic approach we can distinguish two main trends: The low—level data trend collects statistics from the training corpora in the form of n—grams, probabilities, weights, etc. The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al., 1996; Marquez and Rodriguez, 1995; Samuelsson et al., 1996). The acquisition methods range from supervised—inductive— learning—from—example algorithms (Quinlan, 1986; •This research has been partially funded by the Spanish Research Department (CICYT) and inscribed as TIC96-1243-0O3-02 &apos;When the model is obtained from annotated corpora we talk about supervised learning, when it is obtained from raw corpora training is considered unsupervised. Aha et al., 1991) to genetic algorithm strategies (Losee, 1994), through the transformation—based error—driven algorithm used in (Brill, 1995). Still another possibility are the hybrid mo</context>
<context position="14656" citStr="Marquez and Rodriguez, 1995" startWordPosition="2425" endWordPosition="2428">ement of our problem is to have small trees we have implemented a post-pruning technique. In a first step the tree is completely expanded and afterwards it is pruned following a minimal cost-complexity criterion (Breiman et al.. 1984). Roughly speaking this is a process that iteratively cut thos.e subtrees producing only marginal benefits in accuracy, obtaining smaller trees at each step. The trees of this sequence are tested using a, comparatively small, fresh part of the training set in order to decide which is the one with the highest degree of accuracy on new examples. Experimental tests (Marquez and Rodriguez, 1995) have shown that the pruning process reduces tree sizes at about 50% and improves their accuracy in a 2-5%. An Example Finally, we present a real example of the simple acquired contextual constraints for the conflict IN-RB (preposition-adverb). P(IN)=0.81 I Pm). probability p(zB)-_-0.19 dutnbwoon others &amp;quot;as&amp;quot; &amp;quot;As&amp;quot; 1st right tag others RB C&amp;quot;&apos;&amp;quot;&amp;quot;&amp;quot;di • Pa N1.--0.0 I 3 NRB1=0.987 Figure 2: Example of a decision tree branch. The tree branch in figure 2 is translated into the following constraints: -5.81 &lt;[&amp;quot;as&amp;quot; &amp;quot;As&amp;quot;] ,IN&gt; ([RB]) ( [IN] ) ; 2.366 &lt;[&amp;quot;as&amp;quot; &amp;quot;Asi ,RB&gt; ([RB]) ( [IN] ); which express the comp</context>
</contexts>
<marker>Marquez, Rodriguez, 1995</marker>
<rawString>. Marquez and H. Rodriguez. 1995 Towards Learning a Constraint Grammar from Annotated Corpora Using Decision Trees. ESPRIT BRA-7315 Acquilex II, Working Paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F McCarthy</author>
<author>W G Lehnert</author>
</authors>
<title>Using Decision Trees for Coreference Resolution.</title>
<date>1995</date>
<booktitle>In Proceedings of 14th International Joint Conference on Artificial Intelligence (IJCAP95).</booktitle>
<contexts>
<context position="5661" citStr="McCarthy and Lehnert, 1995" startWordPosition="912" endWordPosition="915">to mark the beginning of another phrase). The constraint language is able to express the same kind of patterns than the Constraint Grammar formalism (Karlsson et al., 1995), although in a different formalism. In addition, each constraint has a compatibility value that indicates its strength. In the middle run, the system will be adapted to accept CGs. 3 Constraint Acquisition Choosing, from a set of possible tags, the proper syntactic tag for a word in a particular context can be seen as a problem of classification. Decision trees, recently used in NLP basic tasks such as tagging and parsing (McCarthy and Lehnert, 1995; Daelemans et al., 1996; Magerman, 1996), are suitable for performing this task. A decision tree is a n-ary branching tree that represents a classification rule for classifying the objects of a certain domain into a set of mutually exclusive classes. The domain objects are described as a set of attribute-value pairs, where each attribute measures a relevant feature of an object taking a (ideally small) set of discrete, mutually incompatible values. Each non-terminal node of a decision tree represents a question on (usually) one attribute. For each possible value of this attribute there is a b</context>
</contexts>
<marker>McCarthy, Lehnert, 1995</marker>
<rawString>F. McCarthy and W.G. Lehnert. 1995 Using Decision Trees for Coreference Resolution. In Proceedings of 14th International Joint Conference on Artificial Intelligence (IJCAP95).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingers</author>
</authors>
<title>An Empirical Comparison of Selection Measures for Decision-Tree Induction.</title>
<date>1989</date>
<booktitle>In Machine Learning.</booktitle>
<pages>3--319</pages>
<contexts>
<context position="8724" citStr="Mingers, 1989" startWordPosition="1409" endWordPosition="1410"> all (or almost all) the remaining examples belong to the same class, or when the number of examples is too small. These nodes are the leafs of the tree and contain the conditional probability distribution, of its associated subset of examples, on the possible classes. The heuristic function for selecting the most useful attribute at each step is of a crucial importance in order to obtain simple trees, since no backtracking is performed. There exist two main families of attribute-selecting functions: information-based (Quinlan, 1986; Lopez, 1991) and statistically-based (Breiman et al., 1984; Mingers, 1989). Training Set For each class of POS ambiguity the initial example set is built by selecting from the training corpus 2Classes of ambiguity are determined by the groups of possible tags for the words in the corpus, i.e, nounadjective, noun-adjective-verb, preposition-adverb, etc. 239 all the occurrences of the words belonging to this ambiguity class. More particularly, the set of attributes that describe each example consists of the part-of-speech tags of the neighbour words, and the information about the word itself (orthography and the proper tag in its context). The window considered in the</context>
</contexts>
<marker>Mingers, 1989</marker>
<rawString>Mingers. 1989 An Empirical Comparison of Selection Measures for Decision-Tree Induction. In Machine Learning. 3:319-342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingers</author>
</authors>
<title>An Empirical Comparison of Pruning Methods for Decision-Tree Induction.</title>
<date>1989</date>
<booktitle>In Machine Learning.</booktitle>
<pages>4--227</pages>
<contexts>
<context position="8724" citStr="Mingers, 1989" startWordPosition="1409" endWordPosition="1410"> all (or almost all) the remaining examples belong to the same class, or when the number of examples is too small. These nodes are the leafs of the tree and contain the conditional probability distribution, of its associated subset of examples, on the possible classes. The heuristic function for selecting the most useful attribute at each step is of a crucial importance in order to obtain simple trees, since no backtracking is performed. There exist two main families of attribute-selecting functions: information-based (Quinlan, 1986; Lopez, 1991) and statistically-based (Breiman et al., 1984; Mingers, 1989). Training Set For each class of POS ambiguity the initial example set is built by selecting from the training corpus 2Classes of ambiguity are determined by the groups of possible tags for the words in the corpus, i.e, nounadjective, noun-adjective-verb, preposition-adverb, etc. 239 all the occurrences of the words belonging to this ambiguity class. More particularly, the set of attributes that describe each example consists of the part-of-speech tags of the neighbour words, and the information about the word itself (orthography and the proper tag in its context). The window considered in the</context>
</contexts>
<marker>Mingers, 1989</marker>
<rawString>Mingers. 1989 An Empirical Comparison of Pruning Methods for Decision-Tree Induction. In Machine Learning. 4:227-243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>PadrO</author>
</authors>
<title>POS Tagging Using Relaxation Labelling.</title>
<date>1996</date>
<booktitle>In Proceedings of 16th International Conference on Computational Linguistics.</booktitle>
<location>Copenhagen, Denmark.</location>
<marker>PadrO, 1996</marker>
<rawString>. PadrO. 1996 POS Tagging Using Relaxation Labelling. In Proceedings of 16th International Conference on Computational Linguistics. Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<title>Induction of Decision Trees.</title>
<date>1986</date>
<booktitle>In Machine Learning.</booktitle>
<pages>1--81</pages>
<contexts>
<context position="1852" citStr="Quinlan, 1986" startWordPosition="287" endWordPosition="288">and consists of n—grams (Garside et at., 1987; Cutting et at., 1992), rules (Hindle, 1989) or neural nets (Schmid, 1994). In the automatic approach we can distinguish two main trends: The low—level data trend collects statistics from the training corpora in the form of n—grams, probabilities, weights, etc. The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al., 1996; Marquez and Rodriguez, 1995; Samuelsson et al., 1996). The acquisition methods range from supervised—inductive— learning—from—example algorithms (Quinlan, 1986; •This research has been partially funded by the Spanish Research Department (CICYT) and inscribed as TIC96-1243-0O3-02 &apos;When the model is obtained from annotated corpora we talk about supervised learning, when it is obtained from raw corpora training is considered unsupervised. Aha et al., 1991) to genetic algorithm strategies (Losee, 1994), through the transformation—based error—driven algorithm used in (Brill, 1995). Still another possibility are the hybrid models, which try to join the advantages of both approaches ( Voutilainen and PadrO, 1997). We present in this paper a hybrid approach</context>
<context position="7612" citStr="Quinlan, 1986" startWordPosition="1227" endWordPosition="1228"> process of contextual constraints is performed by means of learning one statistical decision tree for each class of POS ambiguity` and converting them to constraints (rules) expressing compatibility/incompatibility of concrete tags in certain contexts. Learning Algorithm The algorithm we used for constructing the statistical decision trees is a non-incremental supervised learning-from-examples algorithm of the TDIDT (Top Down Induction of Decision Trees) family. It constructs the trees in a top-down way, guided by the distributional information of the examples, but not on the examples order (Quinlan, 1986). Briefly. the algorithm works as a recursive process that departs from considering the whole set of examples at the root level and constructs the tree in a top-down way branching at any non-terminal node according to a certain selected attribute. The different values of this attribute induce a partition of the set of examples in the corresponding subsets, in which the process is applied recursively in order to generate the different subtrees. The recursion ends, in a certain node, either when all (or almost all) the remaining examples belong to the same class, or when the number of examples i</context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>R. Quinlan. 1986 Induction of Decision Trees. In Machine Learning. 1:81-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA.</location>
<marker>Quinlan, 1993</marker>
<rawString>R. Quinlan. 1993 C4.5: Programs for Machine Learning. San Mateo, CA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Landgrebe Richards</author>
<author>P Swain</author>
</authors>
<title>On the accuracy of pixel relaxation labelling.</title>
<date>1981</date>
<journal>IEEE Transactions on System, Man and Cybernetics.</journal>
<volume>Vol.</volume>
<pages>11</pages>
<marker>Richards, Swain, 1981</marker>
<rawString>Richards, D. Landgrebe and P. Swain. 1981 On the accuracy of pixel relaxation labelling. IEEE Transactions on System, Man and Cybernetics. Vol. SMC-11</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Tapanainen Samuelsson</author>
<author>A Voutilainen</author>
</authors>
<title>Inducing Constraint Grammars.</title>
<date>1996</date>
<booktitle>In Proceedings of the 3rd International Colloquium on Grammatical Inference.</booktitle>
<marker>Samuelsson, Voutilainen, 1996</marker>
<rawString>. Samuelsson, P. Tapanainen and A. Voutilainen. 1996 Inducing Constraint Grammars. In Proceedings of the 3rd International Colloquium on Grammatical Inference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Schmid</author>
</authors>
<title>Part-of-speech tagging with neural networks.</title>
<date>1994</date>
<booktitle>In Proceedings of 15th International Conference on Computational Linguistics. Kyoto,</booktitle>
<contexts>
<context position="1359" citStr="Schmid, 1994" startWordPosition="217" endWordPosition="218">uated on the WSJ corpus. 1 Introduction In NLP, it is necessary to model the language in a representation suitable for the task to be performed. The language models more commonly used are based on two main approaches: first, the linguistic approach, in which the model is written by a linguist, generally in the form of rules or constraints (Voutilainen and Jarvinen, 1995). Second, the automatic approach, in which the model is automatically obtained from corpora (either raw or annotated)&apos;, and consists of n—grams (Garside et at., 1987; Cutting et at., 1992), rules (Hindle, 1989) or neural nets (Schmid, 1994). In the automatic approach we can distinguish two main trends: The low—level data trend collects statistics from the training corpora in the form of n—grams, probabilities, weights, etc. The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al., 1996; Marquez and Rodriguez, 1995; Samuelsson et al., 1996). The acquisition methods range from supervised—inductive— learning—from—example algorithms (Quinlan, 1986; •This research has been partially funded by the Spanish Research Department (CICYT) and inscribed as TIC9</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>. Schmid 1994 Part-of-speech tagging with neural networks. In Proceedings of 15th International Conference on Computational Linguistics. Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torras</author>
</authors>
<title>Relaxation and Neural Learning: Points of Convergence and Divergence.</title>
<date>1989</date>
<journal>Journal of Parallel and Distributed Computing.</journal>
<pages>6--217</pages>
<marker>Torras, 1989</marker>
<rawString>. Torras. 1989 Relaxation and Neural Learning: Points of Convergence and Divergence. Journal of Parallel and Distributed Computing. 6:217-244</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimal decoding algorithm.</title>
<date>1967</date>
<booktitle>In IEEE Transactions on Information Theory.</booktitle>
<pages>260--269</pages>
<marker>Viterbi, 1967</marker>
<rawString>.J. Viterbi. 1967 Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. In IEEE Transactions on Information Theory. pg 260-269, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Voutilainen</author>
<author>T Jarvinen</author>
</authors>
<title>Specifying a shallow grammatical representation for parsing purposes.</title>
<date>1995</date>
<booktitle>In Proceedings of the 7th meeting of the European Association for Computational Linguistics.</booktitle>
<pages>210--214</pages>
<contexts>
<context position="1119" citStr="Voutilainen and Jarvinen, 1995" startWordPosition="176" endWordPosition="180">atically learned context constraints, linguistically motivated manually written constraints, etc. The sources and kinds of constraints are unrestricted, and the language model can be easily extended, improving the results. The tagger has been tested and evaluated on the WSJ corpus. 1 Introduction In NLP, it is necessary to model the language in a representation suitable for the task to be performed. The language models more commonly used are based on two main approaches: first, the linguistic approach, in which the model is written by a linguist, generally in the form of rules or constraints (Voutilainen and Jarvinen, 1995). Second, the automatic approach, in which the model is automatically obtained from corpora (either raw or annotated)&apos;, and consists of n—grams (Garside et at., 1987; Cutting et at., 1992), rules (Hindle, 1989) or neural nets (Schmid, 1994). In the automatic approach we can distinguish two main trends: The low—level data trend collects statistics from the training corpora in the form of n—grams, probabilities, weights, etc. The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al., 1996; Marquez and Rodriguez, 199</context>
</contexts>
<marker>Voutilainen, Jarvinen, 1995</marker>
<rawString>Voutilainen and T. Jarvinen. 1995 Specifying a shallow grammatical representation for parsing purposes. In Proceedings of the 7th meeting of the European Association for Computational Linguistics. 210-214.</rawString>
</citation>
<citation valid="true">
<title>Developing a Hybrid NP Parser.</title>
<date>1997</date>
<booktitle>In Proceedings of A NLP&apos;97.</booktitle>
<marker>1997</marker>
<rawString>. Voutilainen and L. Padre). 1997 Developing a Hybrid NP Parser. In Proceedings of A NLP&apos;97.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>