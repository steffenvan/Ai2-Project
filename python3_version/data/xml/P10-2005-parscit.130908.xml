<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002189">
<title confidence="0.9934975">
Diversify and Combine: Improving Word Alignment for Machine
Translation on Low-Resource Languages
</title>
<author confidence="0.827786">
Bing Xiang, Yonggang Deng, and Bowen Zhou
</author>
<note confidence="0.493971">
IBM T. J. Watson Research Center
</note>
<address confidence="0.550772">
Yorktown Heights, NY 10598
</address>
<email confidence="0.996872">
{bxiang,ydeng,zhou}@us.ibm.com
</email>
<sectionHeader confidence="0.99859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999916055555556">
We present a novel method to improve
word alignment quality and eventually the
translation performance by producing and
combining complementary word align-
ments for low-resource languages. Instead
of focusing on the improvement of a single
set of word alignments, we generate mul-
tiple sets of diversified alignments based
on different motivations, such as linguis-
tic knowledge, morphology and heuris-
tics. We demonstrate this approach on an
English-to-Pashto translation task by com-
bining the alignments obtained from syn-
tactic reordering, stemming, and partial
words. The combined alignment outper-
forms the baseline alignment, with signif-
icantly higher F-scores and better transla-
tion performance.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999856166666667">
Word alignment usually serves as the starting
point and foundation for a statistical machine
translation (SMT) system. It has received a signif-
icant amount of research over the years, notably in
(Brown et al., 1993; Ittycheriah and Roukos, 2005;
Fraser and Marcu, 2007; Hermjakob, 2009). They
all focused on the improvement of word alignment
models. In this work, we leverage existing align-
ers and generate multiple sets of word alignments
based on complementary information, then com-
bine them to get the final alignment for phrase
training. The resource required for this approach
is little, compared to what is needed to build a rea-
sonable discriminative alignment model, for ex-
ample. This makes the approach especially ap-
pealing for SMT on low-resource languages.
Most of the research on alignment combination
in the past has focused on how to combine the
alignments from two different directions, source-
to-target and target-to-source. Usually people start
from the intersection of two sets of alignments,
and gradually add links in the union based on
certain heuristics, as in (Koehn et al., 2003), to
achieve a better balance compared to using either
intersection (high precision) or union (high recall).
In (Ayan and Dorr, 2006) a maximum entropy ap-
proach was proposed to combine multiple align-
ments based on a set of linguistic and alignment
features. A different approach was presented in
(Deng and Zhou, 2009), which again concentrated
on the combination of two sets of alignments, but
with a different criterion. It tries to maximize the
number of phrases that can be extracted in the
combined alignments. A greedy search method
was utilized and it achieved higher translation per-
formance than the baseline.
More recently, an alignment selection approach
was proposed in (Huang, 2009), which com-
putes confidence scores for each link and prunes
the links from multiple sets of alignments using
a hand-picked threshold. The alignments used
in that work were generated from different align-
ers (HMM, block model, and maximum entropy
model). In this work, we use soft voting with
weighted confidence scores, where the weights
can be tuned with a specific objective function.
There is no need for a pre-determined threshold
as used in (Huang, 2009). Also, we utilize var-
ious knowledge sources to enrich the alignments
instead of using different aligners. Our strategy is
to diversify and then combine in order to catch any
complementary information captured in the word
alignments for low-resource languages.
The rest of the paper is organized as follows.
</bodyText>
<page confidence="0.966306">
22
</page>
<note confidence="0.500906">
Proceedings of the ACL 2010 Conference Short Papers, pages 22–26,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999284">
We present three different sets of alignments in
Section 2 for an English-to-Pashto MT task. In
Section 3, we propose the alignment combination
algorithm. The experimental results are reported
in Section 4. We conclude the paper in Section 5.
</bodyText>
<sectionHeader confidence="0.974809" genericHeader="method">
2 Diversified Word Alignments
</sectionHeader>
<bodyText confidence="0.999975">
We take an English-to-Pashto MT task as an exam-
ple and create three sets of additional alignments
on top of the baseline alignment.
</bodyText>
<subsectionHeader confidence="0.997381">
2.1 Syntactic Reordering
</subsectionHeader>
<bodyText confidence="0.9998739">
Pashto is a subject-object-verb (SOV) language,
which puts verbs after objects. People have pro-
posed different syntactic rules to pre-reorder SOV
languages, either based on a constituent parse tree
(Dr´abek and Yarowsky, 2004; Wang et al., 2007)
or dependency parse tree (Xu et al., 2009). In
this work, we apply syntactic reordering for verb
phrases (VP) based on the English constituent
parse. The VP-based reordering rule we apply in
the work is:
</bodyText>
<listItem confidence="0.986786">
• VP(VB*, *) --� V P(*, V B*)
</listItem>
<bodyText confidence="0.996719631578948">
where V B* represents V B, VBD, VBG, VBN,
VBP and VBZ.
In Figure 1, we show the reference alignment
between an English sentence and the correspond-
ing Pashto translation, where E is the original En-
glish sentence, P is the Pashto sentence (in ro-
manized text), and E′ is the English sentence after
reordering. As we can see, after the VP-based re-
ordering, the alignment between the two sentences
becomes monotone, which makes it easier for the
aligner to get the alignment correct. During the
reordering of English sentences, we store the in-
dex changes for the English words. After getting
the alignment trained on the reordered English and
original Pashto sentence pairs, we map the English
words back to the original order, along with the
learned alignment links. In this way, the align-
ment is ready to be combined with the baseline
alignment and any other alternatives.
</bodyText>
<subsectionHeader confidence="0.998914">
2.2 Stemming
</subsectionHeader>
<bodyText confidence="0.999471">
Pashto is one of the morphologically rich lan-
guages. In addition to the linguistic knowledge ap-
plied in the syntactic reordering described above,
we also utilize morphological analysis by applying
stemming on both the English and Pashto sides.
For English, we use Porter stemming (Porter,
</bodyText>
<figureCaption confidence="0.9451">
Figure 1: Alignment before/after VP-based re-
ordering.
</figureCaption>
<bodyText confidence="0.999901272727273">
1980), a widely applied algorithm to remove the
common morphological and inflexional endings
from words in English. For Pashto, we utilize
a morphological decompostion algorithm that has
been shown to be effective for Arabic speech
recognition (Xiang et al., 2006). We start from a
fixed set of affixes with 8 prefixes and 21 suffixes.
The prefixes and suffixes are stripped off from
the Pashto words under the two constraints:(1)
Longest matched affixes first; (2) Remaining stem
must be at least two characters long.
</bodyText>
<subsectionHeader confidence="0.998757">
2.3 Partial Word
</subsectionHeader>
<bodyText confidence="0.999939181818182">
For low-resource languages, we usually suffer
from the data sparsity issue. Recently, a simple
method was presented in (Chiang et al., 2009),
which keeps partial English and Urdu words in the
training data for alignment training. This is similar
to the stemming method, but is more heuristics-
based, and does not rely on a set of available af-
fixes. With the same motivation, we keep the first
4 characters of each English and Pashto word to
generate one more alternative for the word align-
ment.
</bodyText>
<sectionHeader confidence="0.983437" genericHeader="method">
3 Confidence-Based Alignment
Combination
</sectionHeader>
<bodyText confidence="0.999251">
Now we describe the algorithm to combine mul-
tiple sets of word alignments based on weighted
confidence scores. Suppose aijk is an alignment
link in the i-th set of alignments between the j-th
source word and k-th target word in sentence pair
(S,T). Similar to (Huang, 2009), we define the
confidence of aijk as
</bodyText>
<equation confidence="0.752392">
&apos;c(aijk|S, T) = qs2t(aijk|S,T)qt2s(aijk|T,S),
(1)
</equation>
<figure confidence="0.569633428571429">
S
S CC
S
NP VP
NP VP
PRP VBP NP
PRP$ NNS
E: they are your employees and you know them well
P: hQvy stAsO kArvAl dy Av tAsO hQvy smh pOEnB
E’: they your employees are and you them well know
NP ADVP
VBP
PRP
RB
</figure>
<page confidence="0.991963">
23
</page>
<bodyText confidence="0.864154">
where the source-to-target link posterior probabil-
ity
qs2t(aijk|S, T) = EkK (tk s9)
A , ′=1 pi (tk′  |sj ) (2)
and the target-to-source link posterior probability
qt2s(aijk|T, S) is defined similarly. pi(tk|sj) is
the lexical translation probability between source
word sj and target word tk in the i-th set of align-
ments.
Our alignment combination algorithm is as fol-
lows.
</bodyText>
<listItem confidence="0.986028666666667">
1. Each candidate link ajk gets soft votes from
N sets of alignments via weighted confidence
scores:
</listItem>
<equation confidence="0.984813">
N
v(ajk|S,T) = wi * c(aijk|S,T), (3)
i=1
</equation>
<bodyText confidence="0.998773">
where the weight wi for each set of alignment
can be optimized under various criteria. In
this work, we tune it on a hand-aligned de-
velopment set to maximize the alignment F-
score.
</bodyText>
<listItem confidence="0.9936324">
2. All candidates are sorted by soft votes in de-
scending order and evaluated sequentially. A
candidate link ajk is included if one of the
following is true:
• Neither sj nor tk is aligned so far;
• sj is not aligned and its left or right
neighboring word is aligned to tk so far;
• tk is not aligned and its left or right
neighboring word is aligned to sj so far.
3. Repeat scanning all candidate links until no
</listItem>
<bodyText confidence="0.90903125">
more links can be added.
In this way, those alignment links with higher
confidence scores have higher priority to be in-
cluded in the combined alignment.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.859408">
4.1 Baseline
</subsectionHeader>
<bodyText confidence="0.999956285714286">
Our training data contains around 70K English-
Pashto sentence pairs released under the DARPA
TRANSTAC project, with about 900K words on
the English side. The baseline is a phrase-based
MT system similar to (Koehn et al., 2003). We
use GIZA++ (Och and Ney, 2000) to generate
the baseline alignment for each direction and then
apply grow-diagonal-final (gdf). The decoding
weights are optimized with minimum error rate
training (MERT) (Och, 2003) to maximize BLEU
scores (Papineni et al., 2002). There are 2028 sen-
tences in the tuning set and 1019 sentences in the
test set, both with one reference. We use another
150 sentence pairs as a heldout hand-aligned set
to measure the word alignment quality. The three
sets of alignments described in Section 2 are gen-
erated on the same training data separately with
GIZA++ and enhanced by gdf as for the baseline
alignment. The English parse tree used for the
syntactic reordering was produced by a maximum
entropy based parser (Ratnaparkhi, 1997).
</bodyText>
<subsectionHeader confidence="0.970877">
4.2 Improvement in Word Alignment
</subsectionHeader>
<bodyText confidence="0.986649828571429">
In Table 1 we show the precision, recall and F-
score of each set of word alignments for the 150-
sentence set. Using partial word provides the high-
est F-score among all individual alignments. The
F-score is 5% higher than for the baseline align-
ment. The VP-based reordering itself does not im-
prove the F-score, which could be due to the parse
errors on the conversational training data. We ex-
periment with three options (c0, c1, c2) when com-
bining the baseline and reordering-based align-
ments. In c0, the weights wi and confidence scores
c(aijk|S, T) in Eq. (3) are all set to 1. In c1,
we set confidence scores to 1, while tuning the
weights with hill climbing to maximize the F-
score on a hand-aligned tuning set. In c2, we com-
pute the confidence scores as in Eq. (1) and tune
the weights as in c1. The numbers in Table 1 show
the effectiveness of having both weights and con-
fidence scores during the combination.
Similarly, we combine the baseline with each
of the other sets of alignments using c2. They
all result in significantly higher F-scores. We
also generate alignments on VP-reordered partial
words (X in Table 1) and compared B + X and
B + V + P. The better results with B + V + P
show the benefit of keeping the alignments as di-
versified as possible before the combination. Fi-
nally, we compare the proposed alignment combi-
nation c2 with the heuristics-based method (gdf),
where the latter starts from the intersection of all 4
sets of alignments and then applies grow-diagonal-
final (Koehn et al., 2003) based on the links in
the union. The proposed combination approach on
B + V + S + P results in close to 7% higher F-
scores than the baseline and also 2% higher than
</bodyText>
<page confidence="0.997085">
24
</page>
<bodyText confidence="0.99528">
gdf. We also notice that its higher F-score is
mainly due to the higher precision, which should
result from the consideration of confidence scores.
</bodyText>
<table confidence="0.9996148">
Alignment Comb P R F
Baseline 0.6923 0.6414 0.6659
V 0.6934 0.6388 0.6650
S 0.7376 0.6495 0.6907
P 0.7665 0.6643 0.7118
X 0.7615 0.6641 0.7095
B+V c0 0.7639 0.6312 0.6913
B+V c1 0.7645 0.6373 0.6951
B+V c2 0.7895 0.6505 0.7133
B+S c2 0.7942 0.6553 0.7181
B+P c2 0.8006 0.6612 0.7242
B+X c2 0.7827 0.6670 0.7202
B+V+P c2 0.7912 0.6755 0.7288
B+V+S+P gdf 0.7238 0.7042 0.7138
B+V+S+P c2 0.7906 0.6852 0.7342
</table>
<tableCaption confidence="0.94656575">
Table 1: Alignment precision, recall and F-score
(B: baseline; V: VP-based reordering; S: stem-
ming; P: partial word; X: VP-reordered partial
word).
</tableCaption>
<subsectionHeader confidence="0.994835">
4.3 Improvement in MT Performance
</subsectionHeader>
<bodyText confidence="0.9993">
In Table 2, we show the corresponding BLEU
scores on the test set for the systems built on each
set of word alignment in Table 1. Similar to the
observation from Table 1, c2 outperforms c0 and
c1, and B + V + S + P with c2 outperforms
B + V + S + P with gdf. We also ran one ex-
periment in which we concatenated all 4 sets of
alignments into one big set (shown as cat). Over-
all, the BLEU score with confidence-based com-
bination was increased by 1 point compared to the
baseline, 0.6 compared to gdf, and 0.7 compared
to cat. All results are statistically significant with
p &lt; 0.05 using the sign-test described in (Collins
et al., 2005).
</bodyText>
<sectionHeader confidence="0.996491" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999935222222222">
In this work, we have presented a word alignment
combination method that improves both the align-
ment quality and the translation performance. We
generated multiple sets of diversified alignments
based on linguistics, morphology, and heuris-
tics, and demonstrated the effectiveness of com-
bination on the English-to-Pashto translation task.
We showed that the combined alignment signif-
icantly outperforms the baseline alignment with
</bodyText>
<table confidence="0.999317625">
Alignment Comb Links Phrase BLEU
Baseline 963K 565K 12.67
V 965K 624K 12.82
S 915K 692K 13.04
P 906K 716K 13.30
X 911K 689K 13.00
B+V c0 870K 890K 13.20
B+V c1 865K 899K 13.32
B+V c2 874K 879K 13.60
B+S c2 864K 948K 13.41
B+P c2 863K 942K 13.40
B+X c2 871K 905K 13.37
B+V+P c2 880K 914K 13.60
B+V+S+P cat 3749K 1258K 13.01
B+V+S+P gdf 1021K 653K 13.14
B+V+S+P c2 907K 771K 13.73
</table>
<tableCaption confidence="0.926324333333333">
Table 2: Improvement in BLEU scores (B: base-
line; V: VP-based reordering; S: stemming; P: par-
tial word; X: VP-reordered partial word).
</tableCaption>
<bodyText confidence="0.9996935">
both higher F-score and higher BLEU score. The
combination approach itself is not limited to any
specific alignment. It provides a general frame-
work that can take advantage of as many align-
ments as possible, which could differ in prepro-
cessing, alignment modeling, or any other aspect.
</bodyText>
<sectionHeader confidence="0.998631" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999866">
This work was supported by the DARPA
TRANSTAC program. We would like to thank
Upendra Chaudhari, Sameer Maskey and Xiao-
qiang Luo for providing useful resources and the
anonymous reviewers for their constructive com-
ments.
</bodyText>
<sectionHeader confidence="0.997869" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.984001846153846">
Necip Fazil Ayan and Bonnie J. Dorr. 2006. A max-
imum entropy approach to combining word align-
ments. In Proc. HLT/NAACL, June.
Peter Brown, Vincent Della Pietra, Stephen Della
Pietra, and Robert Mercer. 1993. The mathematics
of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19(2):263–311.
David Chiang, Kevin Knight, Samad Echihabi, et al.
2009. Isi/language weaver nist 2009 systems. In
Presentation at NIST MT 2009 Workshop, August.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proc. ofACL, pages 531–540.
</reference>
<page confidence="0.96447">
25
</page>
<reference confidence="0.999752333333333">
Yonggang Deng and Bowen Zhou. 2009. Optimizing
word alignment combination for phrase table train-
ing. In Proc. ACL, pages 229–232, August.
Elliott Franco Dr´abek and David Yarowsky. 2004. Im-
proving bitext word alignments via syntax-based re-
ordering of english. In Proc. ACL.
Alexander Fraser and Daniel Marcu. 2007. Getting the
structure right for word alignment: Leaf. In Proc. of
EMNLP, pages 51–60, June.
Ulf Hermjakob. 2009. Improved word alignment with
statistics and linguistic heuristics. In Proc. EMNLP,
pages 229–237, August.
Fei Huang. 2009. Confidence measure for word align-
ment. In Proc. ACL, pages 932–940, August.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for arabic-english ma-
chine translation. In Proc. of HLT/EMNLP, pages
89–96, October.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc.
NAACL/HLT.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proc. ofACL, pages
440–447, Hong Kong, China, October.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of ACL,
pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proc. ofACL, pages
311–318.
Martin Porter. 1980. An algorithm for suffix stripping.
In Program, volume 14, pages 130–137.
Adwait Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy models.
In Proc. ofEMNLP, pages 1–10.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proc. EMNLP, pages 737–
745.
Bing Xiang, Kham Nguyen, Long Nguyen, Richard
Schwartz, and John Makhoul. 2006. Morphological
decomposition for arabic broadcast news transcrip-
tion. In Proc. ICASSP.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proc.
NAACL/HLT, pages 245–253, June.
</reference>
<page confidence="0.998054">
26
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961722">
<title confidence="0.999906">Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</title>
<author confidence="0.999951">Bing Xiang</author>
<author confidence="0.999951">Yonggang Deng</author>
<author confidence="0.999951">Bowen Zhou</author>
<affiliation confidence="0.999968">IBM T. J. Watson Research Center</affiliation>
<address confidence="0.986903">Yorktown Heights, NY 10598</address>
<abstract confidence="0.998661421052632">We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages. Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversified alignments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better translation performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>A maximum entropy approach to combining word alignments.</title>
<date>2006</date>
<booktitle>In Proc. HLT/NAACL,</booktitle>
<contexts>
<context position="2186" citStr="Ayan and Dorr, 2006" startWordPosition="327" endWordPosition="330">eeded to build a reasonable discriminative alignment model, for example. This makes the approach especially appealing for SMT on low-resource languages. Most of the research on alignment combination in the past has focused on how to combine the alignments from two different directions, sourceto-target and target-to-source. Usually people start from the intersection of two sets of alignments, and gradually add links in the union based on certain heuristics, as in (Koehn et al., 2003), to achieve a better balance compared to using either intersection (high precision) or union (high recall). In (Ayan and Dorr, 2006) a maximum entropy approach was proposed to combine multiple alignments based on a set of linguistic and alignment features. A different approach was presented in (Deng and Zhou, 2009), which again concentrated on the combination of two sets of alignments, but with a different criterion. It tries to maximize the number of phrases that can be extracted in the combined alignments. A greedy search method was utilized and it achieved higher translation performance than the baseline. More recently, an alignment selection approach was proposed in (Huang, 2009), which computes confidence scores for e</context>
</contexts>
<marker>Ayan, Dorr, 2006</marker>
<rawString>Necip Fazil Ayan and Bonnie J. Dorr. 2006. A maximum entropy approach to combining word alignments. In Proc. HLT/NAACL, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Vincent Della Pietra</author>
<author>Stephen Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1166" citStr="Brown et al., 1993" startWordPosition="165" endWordPosition="168">nments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better translation performance. 1 Introduction Word alignment usually serves as the starting point and foundation for a statistical machine translation (SMT) system. It has received a significant amount of research over the years, notably in (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007; Hermjakob, 2009). They all focused on the improvement of word alignment models. In this work, we leverage existing aligners and generate multiple sets of word alignments based on complementary information, then combine them to get the final alignment for phrase training. The resource required for this approach is little, compared to what is needed to build a reasonable discriminative alignment model, for example. This makes the approach especially appealing for SMT on low-resource languages. Most of the research on alignment combination i</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter Brown, Vincent Della Pietra, Stephen Della Pietra, and Robert Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Samad Echihabi</author>
</authors>
<title>Isi/language weaver nist</title>
<date>2009</date>
<booktitle>In Presentation at NIST MT 2009 Workshop,</booktitle>
<contexts>
<context position="6476" citStr="Chiang et al., 2009" startWordPosition="1021" endWordPosition="1024">orphological and inflexional endings from words in English. For Pashto, we utilize a morphological decompostion algorithm that has been shown to be effective for Arabic speech recognition (Xiang et al., 2006). We start from a fixed set of affixes with 8 prefixes and 21 suffixes. The prefixes and suffixes are stripped off from the Pashto words under the two constraints:(1) Longest matched affixes first; (2) Remaining stem must be at least two characters long. 2.3 Partial Word For low-resource languages, we usually suffer from the data sparsity issue. Recently, a simple method was presented in (Chiang et al., 2009), which keeps partial English and Urdu words in the training data for alignment training. This is similar to the stemming method, but is more heuristicsbased, and does not rely on a set of available affixes. With the same motivation, we keep the first 4 characters of each English and Pashto word to generate one more alternative for the word alignment. 3 Confidence-Based Alignment Combination Now we describe the algorithm to combine multiple sets of word alignments based on weighted confidence scores. Suppose aijk is an alignment link in the i-th set of alignments between the j-th source word a</context>
</contexts>
<marker>Chiang, Knight, Echihabi, 2009</marker>
<rawString>David Chiang, Kevin Knight, Samad Echihabi, et al. 2009. Isi/language weaver nist 2009 systems. In Presentation at NIST MT 2009 Workshop, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>531--540</pages>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proc. ofACL, pages 531–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>Bowen Zhou</author>
</authors>
<title>Optimizing word alignment combination for phrase table training.</title>
<date>2009</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>229--232</pages>
<contexts>
<context position="2370" citStr="Deng and Zhou, 2009" startWordPosition="358" endWordPosition="361">nt combination in the past has focused on how to combine the alignments from two different directions, sourceto-target and target-to-source. Usually people start from the intersection of two sets of alignments, and gradually add links in the union based on certain heuristics, as in (Koehn et al., 2003), to achieve a better balance compared to using either intersection (high precision) or union (high recall). In (Ayan and Dorr, 2006) a maximum entropy approach was proposed to combine multiple alignments based on a set of linguistic and alignment features. A different approach was presented in (Deng and Zhou, 2009), which again concentrated on the combination of two sets of alignments, but with a different criterion. It tries to maximize the number of phrases that can be extracted in the combined alignments. A greedy search method was utilized and it achieved higher translation performance than the baseline. More recently, an alignment selection approach was proposed in (Huang, 2009), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold. The alignments used in that work were generated from different aligners (HMM, block model,</context>
</contexts>
<marker>Deng, Zhou, 2009</marker>
<rawString>Yonggang Deng and Bowen Zhou. 2009. Optimizing word alignment combination for phrase table training. In Proc. ACL, pages 229–232, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elliott Franco Dr´abek</author>
<author>David Yarowsky</author>
</authors>
<title>Improving bitext word alignments via syntax-based reordering of english.</title>
<date>2004</date>
<booktitle>In Proc. ACL.</booktitle>
<marker>Dr´abek, Yarowsky, 2004</marker>
<rawString>Elliott Franco Dr´abek and David Yarowsky. 2004. Improving bitext word alignments via syntax-based reordering of english. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Getting the structure right for word alignment: Leaf.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>51--60</pages>
<contexts>
<context position="1220" citStr="Fraser and Marcu, 2007" startWordPosition="173" endWordPosition="176">guistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better translation performance. 1 Introduction Word alignment usually serves as the starting point and foundation for a statistical machine translation (SMT) system. It has received a significant amount of research over the years, notably in (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007; Hermjakob, 2009). They all focused on the improvement of word alignment models. In this work, we leverage existing aligners and generate multiple sets of word alignments based on complementary information, then combine them to get the final alignment for phrase training. The resource required for this approach is little, compared to what is needed to build a reasonable discriminative alignment model, for example. This makes the approach especially appealing for SMT on low-resource languages. Most of the research on alignment combination in the past has focused on how to combine the alignment</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2007. Getting the structure right for word alignment: Leaf. In Proc. of EMNLP, pages 51–60, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
</authors>
<title>Improved word alignment with statistics and linguistic heuristics.</title>
<date>2009</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>229--237</pages>
<contexts>
<context position="1238" citStr="Hermjakob, 2009" startWordPosition="177" endWordPosition="178">ology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better translation performance. 1 Introduction Word alignment usually serves as the starting point and foundation for a statistical machine translation (SMT) system. It has received a significant amount of research over the years, notably in (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007; Hermjakob, 2009). They all focused on the improvement of word alignment models. In this work, we leverage existing aligners and generate multiple sets of word alignments based on complementary information, then combine them to get the final alignment for phrase training. The resource required for this approach is little, compared to what is needed to build a reasonable discriminative alignment model, for example. This makes the approach especially appealing for SMT on low-resource languages. Most of the research on alignment combination in the past has focused on how to combine the alignments from two differe</context>
</contexts>
<marker>Hermjakob, 2009</marker>
<rawString>Ulf Hermjakob. 2009. Improved word alignment with statistics and linguistic heuristics. In Proc. EMNLP, pages 229–237, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
</authors>
<title>Confidence measure for word alignment.</title>
<date>2009</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>932--940</pages>
<contexts>
<context position="2746" citStr="Huang, 2009" startWordPosition="419" endWordPosition="420">on) or union (high recall). In (Ayan and Dorr, 2006) a maximum entropy approach was proposed to combine multiple alignments based on a set of linguistic and alignment features. A different approach was presented in (Deng and Zhou, 2009), which again concentrated on the combination of two sets of alignments, but with a different criterion. It tries to maximize the number of phrases that can be extracted in the combined alignments. A greedy search method was utilized and it achieved higher translation performance than the baseline. More recently, an alignment selection approach was proposed in (Huang, 2009), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold. The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model). In this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function. There is no need for a pre-determined threshold as used in (Huang, 2009). Also, we utilize various knowledge sources to enrich the alignments instead of using different aligners. Our strategy is to diversify and th</context>
<context position="7144" citStr="Huang, 2009" startWordPosition="1137" endWordPosition="1138">ning data for alignment training. This is similar to the stemming method, but is more heuristicsbased, and does not rely on a set of available affixes. With the same motivation, we keep the first 4 characters of each English and Pashto word to generate one more alternative for the word alignment. 3 Confidence-Based Alignment Combination Now we describe the algorithm to combine multiple sets of word alignments based on weighted confidence scores. Suppose aijk is an alignment link in the i-th set of alignments between the j-th source word and k-th target word in sentence pair (S,T). Similar to (Huang, 2009), we define the confidence of aijk as &apos;c(aijk|S, T) = qs2t(aijk|S,T)qt2s(aijk|T,S), (1) S S CC S NP VP NP VP PRP VBP NP PRP$ NNS E: they are your employees and you know them well P: hQvy stAsO kArvAl dy Av tAsO hQvy smh pOEnB E’: they your employees are and you them well know NP ADVP VBP PRP RB 23 where the source-to-target link posterior probability qs2t(aijk|S, T) = EkK (tk s9) A , ′=1 pi (tk′ |sj ) (2) and the target-to-source link posterior probability qt2s(aijk|T, S) is defined similarly. pi(tk|sj) is the lexical translation probability between source word sj and target word tk in the i-t</context>
</contexts>
<marker>Huang, 2009</marker>
<rawString>Fei Huang. 2009. Confidence measure for word alignment. In Proc. ACL, pages 932–940, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy word aligner for arabic-english machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP,</booktitle>
<pages>89--96</pages>
<contexts>
<context position="1196" citStr="Ittycheriah and Roukos, 2005" startWordPosition="169" endWordPosition="172">erent motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better translation performance. 1 Introduction Word alignment usually serves as the starting point and foundation for a statistical machine translation (SMT) system. It has received a significant amount of research over the years, notably in (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007; Hermjakob, 2009). They all focused on the improvement of word alignment models. In this work, we leverage existing aligners and generate multiple sets of word alignments based on complementary information, then combine them to get the final alignment for phrase training. The resource required for this approach is little, compared to what is needed to build a reasonable discriminative alignment model, for example. This makes the approach especially appealing for SMT on low-resource languages. Most of the research on alignment combination in the past has focused on how </context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2005. A maximum entropy word aligner for arabic-english machine translation. In Proc. of HLT/EMNLP, pages 89–96, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. NAACL/HLT.</booktitle>
<contexts>
<context position="2053" citStr="Koehn et al., 2003" startWordPosition="306" endWordPosition="309">ombine them to get the final alignment for phrase training. The resource required for this approach is little, compared to what is needed to build a reasonable discriminative alignment model, for example. This makes the approach especially appealing for SMT on low-resource languages. Most of the research on alignment combination in the past has focused on how to combine the alignments from two different directions, sourceto-target and target-to-source. Usually people start from the intersection of two sets of alignments, and gradually add links in the union based on certain heuristics, as in (Koehn et al., 2003), to achieve a better balance compared to using either intersection (high precision) or union (high recall). In (Ayan and Dorr, 2006) a maximum entropy approach was proposed to combine multiple alignments based on a set of linguistic and alignment features. A different approach was presented in (Deng and Zhou, 2009), which again concentrated on the combination of two sets of alignments, but with a different criterion. It tries to maximize the number of phrases that can be extracted in the combined alignments. A greedy search method was utilized and it achieved higher translation performance th</context>
<context position="8955" citStr="Koehn et al., 2003" startWordPosition="1463" endWordPosition="1466">sj is not aligned and its left or right neighboring word is aligned to tk so far; • tk is not aligned and its left or right neighboring word is aligned to sj so far. 3. Repeat scanning all candidate links until no more links can be added. In this way, those alignment links with higher confidence scores have higher priority to be included in the combined alignment. 4 Experiments 4.1 Baseline Our training data contains around 70K EnglishPashto sentence pairs released under the DARPA TRANSTAC project, with about 900K words on the English side. The baseline is a phrase-based MT system similar to (Koehn et al., 2003). We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf). The decoding weights are optimized with minimum error rate training (MERT) (Och, 2003) to maximize BLEU scores (Papineni et al., 2002). There are 2028 sentences in the tuning set and 1019 sentences in the test set, both with one reference. We use another 150 sentence pairs as a heldout hand-aligned set to measure the word alignment quality. The three sets of alignments described in Section 2 are generated on the same training data separately with GIZA++ and enhanc</context>
<context position="11269" citStr="Koehn et al., 2003" startWordPosition="1867" endWordPosition="1870">g the combination. Similarly, we combine the baseline with each of the other sets of alignments using c2. They all result in significantly higher F-scores. We also generate alignments on VP-reordered partial words (X in Table 1) and compared B + X and B + V + P. The better results with B + V + P show the benefit of keeping the alignments as diversified as possible before the combination. Finally, we compare the proposed alignment combination c2 with the heuristics-based method (gdf), where the latter starts from the intersection of all 4 sets of alignments and then applies grow-diagonalfinal (Koehn et al., 2003) based on the links in the union. The proposed combination approach on B + V + S + P results in close to 7% higher Fscores than the baseline and also 2% higher than 24 gdf. We also notice that its higher F-score is mainly due to the higher precision, which should result from the consideration of confidence scores. Alignment Comb P R F Baseline 0.6923 0.6414 0.6659 V 0.6934 0.6388 0.6650 S 0.7376 0.6495 0.6907 P 0.7665 0.6643 0.7118 X 0.7615 0.6641 0.7095 B+V c0 0.7639 0.6312 0.6913 B+V c1 0.7645 0.6373 0.6951 B+V c2 0.7895 0.6505 0.7133 B+S c2 0.7942 0.6553 0.7181 B+P c2 0.8006 0.6612 0.7242 B</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. NAACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>440--447</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="8990" citStr="Och and Ney, 2000" startWordPosition="1470" endWordPosition="1473">ght neighboring word is aligned to tk so far; • tk is not aligned and its left or right neighboring word is aligned to sj so far. 3. Repeat scanning all candidate links until no more links can be added. In this way, those alignment links with higher confidence scores have higher priority to be included in the combined alignment. 4 Experiments 4.1 Baseline Our training data contains around 70K EnglishPashto sentence pairs released under the DARPA TRANSTAC project, with about 900K words on the English side. The baseline is a phrase-based MT system similar to (Koehn et al., 2003). We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf). The decoding weights are optimized with minimum error rate training (MERT) (Och, 2003) to maximize BLEU scores (Papineni et al., 2002). There are 2028 sentences in the tuning set and 1019 sentences in the test set, both with one reference. We use another 150 sentence pairs as a heldout hand-aligned set to measure the word alignment quality. The three sets of alignments described in Section 2 are generated on the same training data separately with GIZA++ and enhanced by gdf as for the baseline align</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proc. ofACL, pages 440–447, Hong Kong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="9173" citStr="Och, 2003" startWordPosition="1498" endWordPosition="1499">n be added. In this way, those alignment links with higher confidence scores have higher priority to be included in the combined alignment. 4 Experiments 4.1 Baseline Our training data contains around 70K EnglishPashto sentence pairs released under the DARPA TRANSTAC project, with about 900K words on the English side. The baseline is a phrase-based MT system similar to (Koehn et al., 2003). We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf). The decoding weights are optimized with minimum error rate training (MERT) (Och, 2003) to maximize BLEU scores (Papineni et al., 2002). There are 2028 sentences in the tuning set and 1019 sentences in the test set, both with one reference. We use another 150 sentence pairs as a heldout hand-aligned set to measure the word alignment quality. The three sets of alignments described in Section 2 are generated on the same training data separately with GIZA++ and enhanced by gdf as for the baseline alignment. The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997). 4.2 Improvement in Word Alignment In Table 1 we show</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="9221" citStr="Papineni et al., 2002" startWordPosition="1504" endWordPosition="1507">ment links with higher confidence scores have higher priority to be included in the combined alignment. 4 Experiments 4.1 Baseline Our training data contains around 70K EnglishPashto sentence pairs released under the DARPA TRANSTAC project, with about 900K words on the English side. The baseline is a phrase-based MT system similar to (Koehn et al., 2003). We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf). The decoding weights are optimized with minimum error rate training (MERT) (Och, 2003) to maximize BLEU scores (Papineni et al., 2002). There are 2028 sentences in the tuning set and 1019 sentences in the test set, both with one reference. We use another 150 sentence pairs as a heldout hand-aligned set to measure the word alignment quality. The three sets of alignments described in Section 2 are generated on the same training data separately with GIZA++ and enhanced by gdf as for the baseline alignment. The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997). 4.2 Improvement in Word Alignment In Table 1 we show the precision, recall and Fscore of each set of</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. ofACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<booktitle>In Program,</booktitle>
<volume>14</volume>
<pages>130--137</pages>
<marker>Porter, 1980</marker>
<rawString>Martin Porter. 1980. An algorithm for suffix stripping. In Program, volume 14, pages 130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proc. ofEMNLP,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="9719" citStr="Ratnaparkhi, 1997" startWordPosition="1591" endWordPosition="1592">eights are optimized with minimum error rate training (MERT) (Och, 2003) to maximize BLEU scores (Papineni et al., 2002). There are 2028 sentences in the tuning set and 1019 sentences in the test set, both with one reference. We use another 150 sentence pairs as a heldout hand-aligned set to measure the word alignment quality. The three sets of alignments described in Section 2 are generated on the same training data separately with GIZA++ and enhanced by gdf as for the baseline alignment. The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997). 4.2 Improvement in Word Alignment In Table 1 we show the precision, recall and Fscore of each set of word alignments for the 150- sentence set. Using partial word provides the highest F-score among all individual alignments. The F-score is 5% higher than for the baseline alignment. The VP-based reordering itself does not improve the F-score, which could be due to the parse errors on the conversational training data. We experiment with three options (c0, c1, c2) when combining the baseline and reordering-based alignments. In c0, the weights wi and confidence scores c(aijk|S, T) in Eq. (3) are</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Proc. ofEMNLP, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>737--745</pages>
<contexts>
<context position="4339" citStr="Wang et al., 2007" startWordPosition="667" endWordPosition="670">2 for an English-to-Pashto MT task. In Section 3, we propose the alignment combination algorithm. The experimental results are reported in Section 4. We conclude the paper in Section 5. 2 Diversified Word Alignments We take an English-to-Pashto MT task as an example and create three sets of additional alignments on top of the baseline alignment. 2.1 Syntactic Reordering Pashto is a subject-object-verb (SOV) language, which puts verbs after objects. People have proposed different syntactic rules to pre-reorder SOV languages, either based on a constituent parse tree (Dr´abek and Yarowsky, 2004; Wang et al., 2007) or dependency parse tree (Xu et al., 2009). In this work, we apply syntactic reordering for verb phrases (VP) based on the English constituent parse. The VP-based reordering rule we apply in the work is: • VP(VB*, *) --� V P(*, V B*) where V B* represents V B, VBD, VBG, VBN, VBP and VBZ. In Figure 1, we show the reference alignment between an English sentence and the corresponding Pashto translation, where E is the original English sentence, P is the Pashto sentence (in romanized text), and E′ is the English sentence after reordering. As we can see, after the VP-based reordering, the alignmen</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proc. EMNLP, pages 737– 745.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Xiang</author>
<author>Kham Nguyen</author>
<author>Long Nguyen</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Morphological decomposition for arabic broadcast news transcription.</title>
<date>2006</date>
<booktitle>In Proc. ICASSP.</booktitle>
<contexts>
<context position="6064" citStr="Xiang et al., 2006" startWordPosition="953" endWordPosition="956">.2 Stemming Pashto is one of the morphologically rich languages. In addition to the linguistic knowledge applied in the syntactic reordering described above, we also utilize morphological analysis by applying stemming on both the English and Pashto sides. For English, we use Porter stemming (Porter, Figure 1: Alignment before/after VP-based reordering. 1980), a widely applied algorithm to remove the common morphological and inflexional endings from words in English. For Pashto, we utilize a morphological decompostion algorithm that has been shown to be effective for Arabic speech recognition (Xiang et al., 2006). We start from a fixed set of affixes with 8 prefixes and 21 suffixes. The prefixes and suffixes are stripped off from the Pashto words under the two constraints:(1) Longest matched affixes first; (2) Remaining stem must be at least two characters long. 2.3 Partial Word For low-resource languages, we usually suffer from the data sparsity issue. Recently, a simple method was presented in (Chiang et al., 2009), which keeps partial English and Urdu words in the training data for alignment training. This is similar to the stemming method, but is more heuristicsbased, and does not rely on a set of</context>
</contexts>
<marker>Xiang, Nguyen, Nguyen, Schwartz, Makhoul, 2006</marker>
<rawString>Bing Xiang, Kham Nguyen, Long Nguyen, Richard Schwartz, and John Makhoul. 2006. Morphological decomposition for arabic broadcast news transcription. In Proc. ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Jaeho Kang</author>
<author>Michael Ringgaard</author>
<author>Franz Och</author>
</authors>
<title>Using a dependency parser to improve smt for subject-object-verb languages. In</title>
<date>2009</date>
<booktitle>Proc. NAACL/HLT,</booktitle>
<pages>245--253</pages>
<contexts>
<context position="4382" citStr="Xu et al., 2009" startWordPosition="675" endWordPosition="678">n 3, we propose the alignment combination algorithm. The experimental results are reported in Section 4. We conclude the paper in Section 5. 2 Diversified Word Alignments We take an English-to-Pashto MT task as an example and create three sets of additional alignments on top of the baseline alignment. 2.1 Syntactic Reordering Pashto is a subject-object-verb (SOV) language, which puts verbs after objects. People have proposed different syntactic rules to pre-reorder SOV languages, either based on a constituent parse tree (Dr´abek and Yarowsky, 2004; Wang et al., 2007) or dependency parse tree (Xu et al., 2009). In this work, we apply syntactic reordering for verb phrases (VP) based on the English constituent parse. The VP-based reordering rule we apply in the work is: • VP(VB*, *) --� V P(*, V B*) where V B* represents V B, VBD, VBG, VBN, VBP and VBZ. In Figure 1, we show the reference alignment between an English sentence and the corresponding Pashto translation, where E is the original English sentence, P is the Pashto sentence (in romanized text), and E′ is the English sentence after reordering. As we can see, after the VP-based reordering, the alignment between the two sentences becomes monoton</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a dependency parser to improve smt for subject-object-verb languages. In Proc. NAACL/HLT, pages 245–253, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>