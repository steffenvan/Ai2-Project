<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000272">
<title confidence="0.998832">
A Classification Algorithm for Predicting the Structure of Summaries
</title>
<author confidence="0.995103">
Horacio Saggion
</author>
<affiliation confidence="0.996912">
University of Sheffield
</affiliation>
<address confidence="0.922870666666667">
211 Portobello Street
Sheffield - S1 4DP
United Kingdom
</address>
<email confidence="0.916849">
http://www.dcs.shef.ac.uk/~saggion
H.Saggion@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.993746" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999748125">
We investigate the problem of generating
the structure of short domain independent
abstracts. We apply a supervised machine
learning approach trained over a set of ab-
stracts collected from abstracting services
and automatically annotated with a text
analysis tool. We design a set of features
for learning inspired from past research
in content selection, information order-
ing, and rhetorical analysis for training
an algorithm which then predicts the dis-
course structure of unseen abstracts. The
proposed approach to the problem which
combines local and contextual features is
able to predict the local structure of the ab-
stracts in just over 60% of the cases.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947590909091">
Mani (2001) defines an abstract as “a summary
at least some of whose material is not present in
the input”. In a study of professional abstracting,
Endres-Niggemeyer (2000) concluded that profes-
sional abstractors produce abstracts by “cut-and-
paste” operations, and that standard sentence pat-
terns are used in their production. Examples of
abstracts produced by a professional abstractor are
shown in Figures 1 and 2. They contain fragments
“copied” from the input documents together with
phrases (underlined in the figures) inserted by the
professional abstractors. In a recent study in hu-
man abstracting (restricted to the amendment of
authors abstracts) Montesi and Owen (2007) noted
that professional abstractors prepend third person
singular verbs in present tense and without subject
to the author abstract, a phenomenon related – yet
different – from the problem we are investigating
in this paper.
Note that the phrases or predicates prepended to
the selected sentence fragments copied from the
input document have a communicative function:
</bodyText>
<figureCaption confidence="0.764427">
Presents a model instructional session that was prepared and
taught by librarians to introduce college students, faculty,
and staff to the Internet by teaching them how to join list-
servs and topic- centered discussion groups. Describes the
sessions’ audience, learning objectives, facility, and course
design. Presents a checklist for preparing an Internet instruc-
tion session.
Figure 1: Professional Abstracts with Inserted
Predicates from LISA Abstracting Service
</figureCaption>
<bodyText confidence="0.994406">
Talks about Newsblaster, an experimental software tool that
scans and summarizes electronic news stories, developed by
Columbia University’s Natural Language Processing Group.
Reports that Newsblaster is a cross between a search en-
gine and ... Explains that Newsblaster publishes the sum-
maries in a Web page that divides the day summaries into ....
Mentions that Newsblaster is considered an aid to those who
have to quickly canvas large amounts of information from
many sources.
</bodyText>
<figureCaption confidence="0.801836">
Figure 2: Professional Abstract with Inserted
Predicates from Internet &amp; Personal Computing
Abstracts
</figureCaption>
<bodyText confidence="0.999205809523809">
they inform or alert the reader about the content
of the abstracted document by explicitly mark-
ing what the author says or mentions, presents or
introduces, concludes, or includes, in her paper.
Montesi and Owen (2007) observe that the revi-
sion of abstracts is carried out to improve com-
prehensibility and style and to make the abstract
objective.
We investigate how to create the discourse
structure of the abstracts: more specifically we are
interested in predicting the inserted predicates or
phrases and at which positions in the abstract they
should be prepended.
Abstractive techniques in text summarization
include sentence compression (Cohn and Lapata,
2008), headline generation (Soricut and Marcu,
2007), and canned-based generation (Oakes and
Paice, 2001). Close to the problem studied here
is Jing and McKeown’s (Jing and McKeown,
2000) cut-and-paste method founded on Endres-
Niggemeyer’s observations. The cut-and-paste
</bodyText>
<page confidence="0.998635">
31
</page>
<note confidence="0.9989895">
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 31–38,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999837416666666">
method includes such operations as sentence trun-
cation, aggregation, specialization/generalization,
reference adjustment and rewording. None of
these operations account for the transformations
observed in the abstracts of Figures 1 and 2. The
formulaic expressions or predicates inserted in the
abstract “glue” together the extracted fragments,
thus creating the abstract’s discourse structure.
To the best of our knowledge, and with the ex-
ception of Saggion and Lapalme (2002) indicative
generation approach which included operations to
add extra linguistic material to generate an indica-
tive abstract, the work presented here is the first
to investigate this relevant operation in the field of
text abstracting and to propose a robust computa-
tional method for its simulation.
In this paper we are interested in the process
of generating the structure of the abstract by au-
tomatic means. In order to study this problem, we
have collected a corpus of abstracts written by ab-
stractors; we have designed an algorithm for pre-
dicting the structure; implemented the algorithm;
and evaluated the structure predicted by the auto-
matic system against the true structure.
</bodyText>
<sectionHeader confidence="0.811196" genericHeader="introduction">
2 Problem Specification, Data Collection,
</sectionHeader>
<subsectionHeader confidence="0.823689">
and Annotation
</subsectionHeader>
<bodyText confidence="0.987134527777778">
The abstracts we study in this research follow the
pattern:
Abstract ≡ ®ni=1 Predi ⊕ βi
where Predi is a phrase used to introduce the “con-
tent” βi of sentence i, n is the number of sentences
in the abstract, ® indicates multiple concatena-
tion, and X ⊕Y indicates the concatenation of X
and Y. In this paper we concentrate only on this
“linear” structure, we plan to study more complex
(e.g., tree-like representations) in future work.
The problem we are interested in solving is the
following: given sentence fragments βi extracted
from the document, how to create the Abstract.
Note that if N is the number of different phrases
(Predi) used in the model, then a priori there are
Nn possible discourse structures to select from for
the abstract, generating all possibilities and select-
ing the most appropriate would be impractical. We
present an algorithm that decides which predicate
or phrase is most suitable for each sentence, do-
ing this by considering the sentence content and
the abstract generated so far. For the experiments
to be reported in this paper, the discourse structure
of the abstracts is created using predicates or ex-
pressions learned from a corpus a subset of which
is shown in Table 1.
We have collected abstracts from various
databases including LISA, ERIC, and Internet
&amp; Personal Computing Abstracts, using our in-
stitutional library’s facilities and the abstracts’
providers’ keyword search facilities. Electronic
copies of the abstracted documents can also be
accessed through our institution following a link,
thus allowing us to check abstracts against ab-
stracted document (additional information on the
abstracts is given in the Appendix).
</bodyText>
<subsectionHeader confidence="0.999101">
2.1 Document Processing and Annotation
</subsectionHeader>
<bodyText confidence="0.99883584375">
Each electronic version of the abstract was pro-
cessed using the freely available GATE text analy-
sis software (Cunningham et al., 2002). First each
abstract was analyzed by a text structure analy-
sis program to identify meta-data such as title, au-
thor, source document, the text of the abstract, etc.
Each sentence in the abstract was stripped from
the predicate or phrase inserted by the abstractor
(e.g., “Mentions that”, “Concludes with”) and a
normalised version of the expression was used to
annotate the sentence, in a way similar to the ab-
stracts in Figures 1 and 2. After this each abstract
and document title was tokenised, sentence split-
ted, part-of-speech tagged, and morphologically
analyzed. A rule-based system was used to carry
out partial, robust syntactic and semantic analy-
sis of the abstracts (Gaizauskas et al., 2005) pro-
ducing predicate-argument representations where
predicates which are used to represent entities are
created from the morphological roots of nouns or
verbs in the text (unary predicates) and predicates
with are used to represent binary relations are a
closed set of names representing grammatical re-
lations such as the verb logical object, or the verb
logical subject or a prepositional attachment, etc.
This predicate-argument structure representation
was further analysed in order to extract “seman-
tic” triples which are used in the experiments re-
ported here. Output of this analysis is shown in
Figure 3. Note that the representation also con-
tains the tokens of the text, their parts of speech,
lemmas, noun phrases, verb phrases, etc.
</bodyText>
<sectionHeader confidence="0.987663" genericHeader="method">
3 Proposed Solution
</sectionHeader>
<bodyText confidence="0.99968275">
Our algorithm (see Algorithm 1) takes as in-
put an ordered list of sentence fragments obtained
from the source document and decides how to
“paste” the fragments together into an abstract;
</bodyText>
<page confidence="0.996015">
32
</page>
<bodyText confidence="0.994334">
to address; to add; to advise; to assert; to claim; to comment; to compare; to conclude; to define; to
describe; to discuss; to evaluate; to examine; to explain; to focus; to give; to highlight; to include;
to indicate; to note; to observe; to overview; to point out; to present; to recommend; to report; to
say; to show; to suggest; ...
</bodyText>
<listItem confidence="0.5402395">
to report + to indicate + to note + to declare + to include; to provide + to explain + to indicate +
to mention; to point out + to report + to mention + to include; to discuss + to list + to suggest +
to conclude; to present + to say + to add + to conclude + to contain; to discuss + to explain + to
recommend; to discuss + to cite + to say; ...
</listItem>
<tableCaption confidence="0.94282">
Table 1: Subset of predicates or expressions used by professional abstractors and some of the discourse
structures used.
Sentence: Features a listing of ten family-oriented pro-
grams, including vendor information, registration fees, and
a short review of each.
</tableCaption>
<construct confidence="0.583824333333333">
Representation: listing-det-a; listing-of-program; family-
oriented-adj-program; fee-qual-registration; information-
qual-vendor; listing-apposed-information; ...
</construct>
<figureCaption confidence="0.995445">
Figure 3: Sentence Representation (partial)
</figureCaption>
<figure confidence="0.826059769230769">
Algorithm 1 Discourse Structure Prediction Al-
gorithm
Given: a list of n sorted text fragments Pi
begin
Abstract ← ““;
Context ← START;
for all i : 0 ≤ i ≤ n−1; do
Pred ← PredictPredicate(Context,Pi);
Abstract ← Abstract ⊕ Pred ⊕ Pi ⊕ “.”;
Context ← ExtractContext(Abstract);
end for
return Abstract
end
</figure>
<bodyText confidence="0.998824846153846">
at each iteration the algorithm selects the “best”
available phrase or predicate to prepend to the cur-
rent fragment from a finite vocabulary (induced
from the analysed corpus) based on local and
contextual information. One could rely on ex-
isting trainable sentence selection (Kupiec et al.,
1995) or even phrase selection (Banko et al., 2000)
strategies to pick up appropriate Pi’s from the doc-
ument to be abstracted and rely on recent informa-
tion ordering techniques to sort the Pi fragments
(Lapata, 2003). This is the reason why we only ad-
dress here the discourse structure generation prob-
lem.
</bodyText>
<subsectionHeader confidence="0.9823045">
3.1 Predicting Discourse Structure as
Classification
</subsectionHeader>
<bodyText confidence="0.999994628571429">
There are various possible ways of predicting what
expression to insert at each point in the genera-
tion process (i.e., the PredictPredicate function
in Algorithm 1). In the experiments reported here
we use a classification algorithm based on lexical,
syntactic, and discursive features, which decides
which of the N possible available phrases is most
suitable. The algorithm is trained over the anno-
tated abstracts and used to predict the structure of
unseen test abstracts.
Where the classification algorithm is concerned,
we have decided to use Support Vector Machines
which have recently been used in different tasks
in natural language processing, they have been
shown particularly suitable for text categorization
(Joachims, 1998). We have tried other machine
learning algorithms such as Decision Trees, Naive
Bayes Classification, and Nearest Neighbor from
the Weka toolkit (Witten and Frank, 1999), but the
support vector machines gave us the best classifi-
cation accuracy (a comparison with Naive Bayes
will be presented in Section 4).
The features used for the experiments reported
here are inspired by previous work in text summa-
rization on content selection (Kupiec et al., 1995),
rhetorical classification (Teufel and Moens, 2002),
and information ordering (Lapata, 2003). The
features are extracted from the analyzed abstracts
with specialized programs. In particular we use
positional features (position of the predicate to be
generated in the structure), length features (num-
ber of words in the sentence), title features (e.g.,
presence of title words in sentence), content fea-
tures computed as the syntactic head of noun and
verb phrases, semantic features computed as the
</bodyText>
<page confidence="0.995199">
33
</page>
<bodyText confidence="0.97849525">
to add; to conclude; to contain; to describe; to
discuss; to explain; to feature; to include; to indi-
cate; to mention; to note; to point out; to present;
to provide; to report; to say
</bodyText>
<tableCaption confidence="0.975054">
Table 2: Predicates in the reduced corpus
</tableCaption>
<bodyText confidence="0.999914529411765">
arguments of “semantic” triples (Section 2.1) ex-
tracted from the parsed abstracts. Features occur-
ring less than 4 times in the corpus were removed
for the experiments. For each sentence, a cohe-
sion feature is also computed as the number of
nouns in common with the previous sentence frag-
ment (or title if first sentence). Cohesion infor-
mation has been used in rhetorical-based parsing
for summarization (Marcu, 1997) in order to de-
cide between “list” or “elaboration” relations and
also in content selection for summarization (Barzi-
lay and Elhadad, 1997). For some experiments
we also use word-level information (lemmas) and
part-of-speech tags. For some of the experiments
reported here the variable Context at iteration i in
Algorithm 1 is instantiated with the predicates pre-
dicted at iterations i −1 and i − 2.
</bodyText>
<sectionHeader confidence="0.998121" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.997538583333333">
The experiments reported here correspond to the
use of different features as input for the classifier.
In these experiments we have used a subset of the
collected abstracts, they contain predicates which
appeared at least 5 times in the corpus. With this
restriction in place the original set of predicates
used to create the discourse structure is reduced to
sixteen (See Table 2), however, the number of pos-
sible structures in the reduced corpus is still con-
siderable with a total of 179 different structures.
In the experiments we compare several classi-
fiers:
</bodyText>
<listItem confidence="0.9652196">
• Random Generation selects a predicate at
random at each iteration of the algorithm;
• Predicate-based Generation is a SVM classi-
fier which uses the two previous predicates to
generate the current predicate ignoring sen-
tence content;
• Position-based Generation is a SVM classi-
fier which also ignores sentence content but
uses as features for classification the absolute
position of the sentence to be generated;
</listItem>
<table confidence="0.999651666666667">
Configuration Avg.Acc
Random Generation 10%
Predicate-based Generation 35%
Position-based Generation 38%
tf*idf-based Generation 55%
Summarization-based Generation 60%
</table>
<tableCaption confidence="0.955282">
Table 3: Average accuracy of different classifica-
tion configurations.
</tableCaption>
<listItem confidence="0.971642">
• tf*idf-based Generation is a SVM classifier
which uses lemmas of the sentence fragment
to be generated to pick up one predicate (note
that position features and predicates were
added to the mix without improving the clas-
sifier);
• Summarization-based Generation is a SVM
which uses the summarization and discourse
features discussed in the previous section in-
cluding contextual information (Predi−2 and
Predi−1 – with special values when i = 0 and
i = 1).
</listItem>
<bodyText confidence="0.999845892857143">
We measure the performance of each instance
of the algorithm by comparing the predicted struc-
ture against the true structure. We compute two
metrics: (i) accuracy at the sentence level (as in
classification), which is the proportion of predi-
cates which were correctly generated; and (ii) ac-
curacy at the textual level, which is the proportion
of abstracts correctly generated. For the latter we
compute the proportion of abstracts with zero er-
rors, less than two errors, and less than three er-
rors.
For every instance of the algorithm we perform
a cross-validation experiment, selecting for each
experiment 20 abstracts for testing and the rest of
the abstracts for training. Accuracy measures at
sentence and text levels are averages of the cross-
validation experiments.
Results of the algorithms are presented in Ta-
bles 3 and 4. Random generation has very poor
performance with only 10% local accuracy and
less than 1% of full correct structures. Knowledge
of the predicates selected for previous sentences
improves performance over the random system
(35% local accuracy and 5% of full correct struc-
tures predicted). As in previous summarization
studies, position proved to contribute to the task:
the positional classifier predicts individual predi-
cates with a 38% accuracy; however only 8% of
</bodyText>
<page confidence="0.99551">
34
</page>
<bodyText confidence="0.999824045454545">
the structures are recalled. Differences between
the accuracies of the two algorithms (predicate-
based and position-based) are significant at 95%
confidence level (a t-test was used). As it is usu-
ally the case in text classification experiments,
the use of word level information (lemmas in our
case) achieves good performance: 55% classifica-
tion accuracy at sentence level, and 18% of full
structures correctly predicted. The use of lex-
ical (noun and verb heads, arguments), syntac-
tic (parts of speech information), and discourse
(predicted predicates, position, cohesion) features
has the better performance with 60% classifica-
tion accuracy at sentence level predicting 21%
of all structures with 73% of the structures con-
taining less than 3 errors. The differences in
accuracy between the word-based classifier and
the summarization-based classifier are statistically
significant at 95% confidence level (a t-test was
used). A Naive Bayes classifier which uses the
summarization features achieves 50% classifica-
tion accuracy.
</bodyText>
<table confidence="0.981435">
Conf. 0 errs &lt; 2 errrs &lt; 3 errs
Random 0.3% 4% 20%
Predicate-based 5% 24% 48%
Position-based 8% 33% 50%
tf*idf-based 18% 42% 67%
Summ-based 21% 55% 73%
</table>
<tableCaption confidence="0.967088">
Table 4: Percent of correct and partially correct
structures predicted. Averaged over all runs.
</tableCaption>
<bodyText confidence="0.9631482">
Table 5 shows a partial confusion table for pred-
icates “to add”, “to conclude”, “to explain”, and
“to present” while and Table 6 reports individual
classification accuracy. All these results are based
on averages of the summarization-based classifier.
</bodyText>
<sectionHeader confidence="0.999299" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999970457142857">
We have presented here a problem which has not
been investigated before in the field of text sum-
marization: the addition of extra linguistic mate-
rial (i.e., not present in the source document) to the
abstract “informational content” in order to create
the structure of the abstract. We have proposed an
algorithm which uses a classification component
at each iteration to predict predicates or phrases to
be prepended to fragments extracted from a doc-
ument. We have shown that this classifier based
on summarization features including linguistic, se-
mantic, positional, cohesive, and discursive infor-
mation can predict the local discourse structures in
over 60% of the cases. There is a mixed picture on
the prediction of individual predicates, with most
predicates correctly classified in most of the cases
except for predicates such as “to describe”, “to
note”, and “to report” which are confused with
other phrases. Predicates such as “to present” and
“to include” have the tendency of appearing to-
wards the very beginning or the very end of the ab-
stract been therefore predicted by position-based
features (Edmundson, 1969; Lin and Hovy, 1997).
Note that in this work we have decided to evaluate
the predicted structure against the true structure (a
hard evaluation measure), in future work we will
assess the abstracts with a set of quality questions
similar to those put forward by the Document Un-
derstanding Conference Evaluations (also in a way
similar to (Kan and McKeown, 2002) who eval-
uated their abstracts in a retrieval environment).
We expect to obtain a reasonable evaluation result
given that it appears that some of the predicates or
phrases are “interchangeable” (e.g., “to contain”
and “to include”).
</bodyText>
<figureCaption confidence="0.998725692307692">
Actual Pred. Predicted Pred. Conf.Freq.
to add to add 32%
to explain 16%
to say 10%
to conclude to conclude 35%
to say 29%
to add 7%
to explain to explain 35%
to say 15%
to add 11%
to present to present 86%
to discuss 7%
to provide 1%
</figureCaption>
<tableCaption confidence="0.634311666666667">
Table 5: Classification Confusion Table for a Sub-
set of Predicates in the Corpus (Average Fre-
quency).
</tableCaption>
<sectionHeader confidence="0.999889" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.99991925">
Liddy (1991) produced a formal model of the in-
formational or conceptual structure of abstracts
of empirical research. This structure was elicited
from abstractors of two organizations ERIC and
PsycINFO through a series of tasks. Lexical clues
which predict the components of the structure
were latter induced by corpus analysis. In the do-
main of indicative summarization, Kan and McK-
</bodyText>
<page confidence="0.995656">
35
</page>
<table confidence="0.999855176470588">
Predicate Avg. Accuracy
to add 31.40
to conclude 34.78
to contain 10.96
to describe 15.69
to discuss 54.55
to explain 35.63
to feature 34.38
to include 85.86
to indicate 20.69
to mention 26.47
to note 6.78
to point out 91.67
to present 86.19
to provide 40.94
to report 1.59
to say 75.86
</table>
<tableCaption confidence="0.996802">
Table 6: Predicate Classification Accuracy
</tableCaption>
<bodyText confidence="0.999858555555556">
marization (Oakes and Paice, 2001) in limited do-
mains. Saggion and Lapalme (2002) have studied
and implemented a rule-based “verb selection” op-
eration in their SumUM system which has been
applied to introduce document topics during in-
dicative summary generation.
Our discourse structure generation procedure is
in principle generic but depends on the availability
of a corpus for training.
</bodyText>
<sectionHeader confidence="0.993707" genericHeader="method">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999711842105263">
In text summarization research, most attention
has been paid to the problem of what information
to select for a summary. Here, we have focused
on the problem of how to combine the selected
content with extra linguistic information in order
to create the structure of the summary.
There are several contributions of this work:
eown (2002) studied the problem of generating ab-
stracts for bibliographical data which although in a
restricted domain has some contact points with the
work described here. As in their work we use the
abstracts in our corpus to induce the model. They
rely on a more or less fixed discourse structure to
accommodate the generation process. In our ap-
proach the discourse structure is not fixed but pre-
dicted for each particular abstract. Related to our
classification experiments is work on semantic or
rhetorical classification of “structured” abstracts
(Saggion, 2008) from the MEDLINE abstracting
database where similar features to those presented
here were used to identify in abstracts semantic
categories such as objective, method, results, and
conclusions. Related to this is the work by Teufel
and Moens (2002) on rhetorical classification for
content selection. In cut-and-paste summarization
(Jing and McKeown, 2000), sentence combina-
tion operations were implemented manually fol-
lowing the study of a set of professionally written
abstracts; however the particular “pasting” oper-
ation presented here was not implemented. Pre-
vious studies on text-to-text abstracting (Banko et
al., 2000; Knight and Marcu, 2000) have studied
problems such as sentence compression and sen-
tence combination but not the “pasting” procedure
presented here. The insertion in the abstract of
linguistic material not present in the input docu-
ment has been addressed in paraphrase generation
(Barzilay and Lee, 2004) and canned-based sum-
</bodyText>
<listItem confidence="0.960883">
• First, we have presented the problem of gen-
erating the discourse structures of an abstract
and proposed a meta algorithm for predicting
it. This problem has not been investigated be-
fore.
• Second, we have proposed – based on pre-
vious summarization research – a number of
features to be used for solving this problem;
and
• Finally, we have propose several instantia-
tions of the algorithm to solve the problem
and achieved a reasonable accuracy using the
designed features;
</listItem>
<bodyText confidence="0.999928222222222">
There is however much space for improvement
even though the algorithm recalls some “partial
structures”, many “full structures” can not be gen-
erated. We are currently investigating the use
of induced rules to address the problem and will
compare a rule-based approach with our classi-
fier. Less superficial cohesion features are being
investigated and will be tested in this classification
framework.
</bodyText>
<sectionHeader confidence="0.980152" genericHeader="method">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999605833333333">
We would like to thank three anonymous review-
ers for their suggestions and comments. We thank
Adam Funk who helped us improve the quality of
our paper. Part of this research was carried out
while the author was working for the EU-funded
MUSING project (IST-2004-027097).
</bodyText>
<page confidence="0.997513">
36
</page>
<sectionHeader confidence="0.990338" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.999556676767677">
Michele Banko, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statisti-
cal translation. In ACL ’00: Proceedings of the 38th
Annual Meeting on Association for Computational
Linguistics, pages 318–325, Morristown, NJ, USA.
Association for Computational Linguistics.
Regina Barzilay and Michael Elhadad. 1997. Using
Lexical Chains for Text Summarization. In Proceed-
ings of the ACL/EACL’97 Workshop on Intelligent
Scalable Text Summarization, pages 10–17, Madrid,
Spain, July.
R. Barzilay and L. Lee. 2004. Catching the Drift:
Probabilistic Content Models, with Applications to
Generation and Summarization. In Proceedings of
HLT-NAACL 2004.
T. Cohn and M. Lapata. 2008. Sentence compression
beyond word deletion. In Proceedings of COLING
2008, Manchester.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graph-
ical development environment for robust NLP tools
and applications. In ACL 2002.
H.P. Edmundson. 1969. New Methods in Automatic
Extracting. Journal of the Association for Comput-
ing Machinery, 16(2):264–285, April.
Brigitte Endres-Niggemeyer. 2000. SimSum: an em-
pirically founded simulation of summarizing. Infor-
mation Processing &amp; Management, 36:659–682.
R. Gaizauskas, M. Hepple, H. Saggion, and M. Green-
wood. 2005. SUPPLE: A Practical Parser for Natu-
ral Language Engineering Applications.
Hongyan Jing and Kathleen McKeown. 2000. Cut
and Paste Based Text Summarization. In Proceed-
ings of the 1st Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 178–185, Seattle, Washington, USA, April 29
-May 4.
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. In European Conference on Machine Learn-
ing (ECML), pages 137–142, Berlin. Springer.
Min-Yen Kan and Kathleen R.. McKeown. 2002.
Corpus-trained text generation for summarization.
In Proceedings of the Second International Natu-
ral Language Generation Conference (INLG 2002),
pages 1–8, Harriman, New York, USA.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of the 17th National Confer-
ence of the American Association for Artificial In-
telligence. AAAI, July 30 - August 3.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A Trainable Document Summarizer. In Proc. of the
18th ACM-SIGIR Conference, pages 68–73, Seattle,
Washington, United States.
M. Lapata. 2003. Probabilistic Text Structuring: Ex-
periments with Sentence Ordering. In Proceedings
of the 41st Meeting of the Association of Computa-
tional Linguistics, pages 545–552, Sapporo, Japan.
Elizabeth D. Liddy. 1991. The Discourse-Level Struc-
ture of Empirical Abstracts: An Exploratory Study.
Information Processing &amp; Management, 27(1):55–
81.
C. Lin and E. Hovy. 1997. Identifying Topics by Po-
sition. In Fifth Conference on Applied Natural Lan-
guage Processing, pages 283–290. Association for
Computational Linguistics, 31 March-3 April.
Inderjeet Mani. 2001. Automatic Text Summarization.
John Benjamins Publishing Company.
D. Marcu. 1997. The Rhetorical Parsing, Summa-
rization, and Generation of Natural Language Texts.
Ph.D. thesis, Department of Computer Science, Uni-
versity of Toronto.
M. Montesi and J. M. Owen. 2007. Revision of au-
thor abstracts: how it is carried out by LISA editors.
Aslib Proceedings, 59(1):26–45.
M.P. Oakes and C.D. Paice. 2001. Term extrac-
tion for automatic abstracting. In D. Bourigault,
C. Jacquemin, and M-C. L’Homme, editors, Recent
Advances in Computational Terminology, volume 2
of Natural Language Processing, chapter 17, pages
353–370. John Benjamins Publishing Company.
H. Saggion and G. Lapalme. 2002. Generating
Indicative-Informative Summaries with SumUM.
Computational Linguistics, 28(4):497–526.
H. Saggion. 2008. Automatic Summarization: An
Overview. Revue Franc¸aise de Linguistique Ap-
pliqu´ee , XIII(1), Juin.
R. Soricut and D. Marcu. 2007. Abstractive headline
generation using WIDL-expressions. Inf. Process.
Manage., 43(6):1536–1548.
S. Teufel and M. Moens. 2002. Summarizing
Scientific Articles: Experiments with Relevance
and Rhetorical Status. Computational Linguistics,
28(4):409–445.
Ian H. Witten and Eibe Frank. 1999. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations. Morgan Kaufmann, Oc-
tober.
</reference>
<sectionHeader confidence="0.9909835" genericHeader="method">
Appendix I: Corpus Statistics and
Examples
</sectionHeader>
<bodyText confidence="0.987681">
The corpus of abstracts following the specification
given in Section 2 contains 693 abstracts, 10,423
</bodyText>
<page confidence="0.997551">
37
</page>
<bodyText confidence="0.999325">
sentences, and 305,105 tokens. The reduced
corpus used for the experiments contains 300
abstracts.
</bodyText>
<subsectionHeader confidence="0.910833">
Examples
</subsectionHeader>
<bodyText confidence="0.979448576271186">
Here we list one example of the use of each of the
predicates in the reduced set of 300 abstracts used
for the experiments.
Adds that it uses search commands and
features that are similar to those of
traditional online commercial database
services, has the ability to do nested
Boolean queries as well as truncation
when needed, and provides detailed doc-
umentation that offers plenty of exam-
ples.
Concludes CNET is a network of sites,
each dealing with a specialized aspect of
computers that are accessible from the
home page and elsewhere around the site.
Contains a step-by-step guide to using
PGP.
Describes smallbizNet, the LEXIS-
NEXIS Small Business Service, Small
Business Administration, Small Business
Advancement National Center, and other
small business-related sites.
Discusses connections and links between
differing electronic mail systems.
Explains DataStar was one of the first on-
line hosts to offer a Web interface, and
was upgraded in 1997.
Features tables showing the number of
relevant, non-relevant, and use retrievals
on both LEXIS and WIN for federal and
for state court queries.
Includes an electronic organizer, an er-
gonomically correct keyboard, an on-
line idle-disconnect, a video capture de-
vice, a color photo scanner, a real-time
Web audio player, laptop speakers, a
personal information manager (PIM), a
mouse with built-in scrolling, and a voice
fax-modem.
Indicates that the University of Califor-
nia, Berkeley, has the School of Informa-
tion Management and Systems, the Uni-
versity of Washington has the Informa-
tion School, and the University of Mary-
land has the College of Information Stud-
ies.
Mentions that overall, the interface is
effective because the menus and sear
screens permit very precise searches with
no knowledge of searching or Dialog
databases.
Notes that Magazine Index was origi-
nally offered on Lyle Priest’s invention,
a unique microfilm reader.
Points out the strong competition that the
Internet has created for the traditional on-
line information services, and the move
of these services to the Internet.
Presents searching tips and techniques.
</bodyText>
<reference confidence="0.7577196">
Provides summaries of African art; Allen
Memorial Art Museum of Oberlin Col-
lege; Art crimes; Asian arts; Da Vinci,
Leonardo; Gallery Walk; and Native
American Art Gallery.
</reference>
<bodyText confidence="0.988264111111111">
Reports that Dialog has announced ma-
jor enhancements to its alerting system on
the DialogClassic, DialogClassic Web,
and DialogWeb services.
Says that dads from all over the country
share advice on raising children, educa-
tional resources, kids’ software, and other
related topics using their favorite online
service provider.
</bodyText>
<page confidence="0.998823">
38
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.190799">
<title confidence="0.99992">A Classification Algorithm for Predicting the Structure of Summaries</title>
<author confidence="0.994803">Horacio</author>
<affiliation confidence="0.998506">University of</affiliation>
<address confidence="0.852603">211 Portobello</address>
<note confidence="0.4278045">Sheffield - S1 United</note>
<email confidence="0.959313">H.Saggion@dcs.shef.ac.uk</email>
<abstract confidence="0.995257705882353">We investigate the problem of generating the structure of short domain independent abstracts. We apply a supervised machine learning approach trained over a set of abstracts collected from abstracting services and automatically annotated with a text analysis tool. We design a set of features for learning inspired from past research in content selection, information ordering, and rhetorical analysis for training an algorithm which then predicts the discourse structure of unseen abstracts. The proposed approach to the problem which combines local and contextual features is able to predict the local structure of the abstracts in just over 60% of the cases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Vibhu O Mittal</author>
<author>Michael J Witbrock</author>
</authors>
<title>Headline generation based on statistical translation.</title>
<date>2000</date>
<booktitle>In ACL ’00: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>318--325</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10608" citStr="Banko et al., 2000" startWordPosition="1653" endWordPosition="1656">cture Prediction Algorithm Given: a list of n sorted text fragments Pi begin Abstract ← ““; Context ← START; for all i : 0 ≤ i ≤ n−1; do Pred ← PredictPredicate(Context,Pi); Abstract ← Abstract ⊕ Pred ⊕ Pi ⊕ “.”; Context ← ExtractContext(Abstract); end for return Abstract end at each iteration the algorithm selects the “best” available phrase or predicate to prepend to the current fragment from a finite vocabulary (induced from the analysed corpus) based on local and contextual information. One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate Pi’s from the document to be abstracted and rely on recent information ordering techniques to sort the Pi fragments (Lapata, 2003). This is the reason why we only address here the discourse structure generation problem. 3.1 Predicting Discourse Structure as Classification There are various possible ways of predicting what expression to insert at each point in the generation process (i.e., the PredictPredicate function in Algorithm 1). In the experiments reported here we use a classification algorithm based on lexical, syntactic, and discursive features, which</context>
<context position="22897" citStr="Banko et al., 2000" startWordPosition="3601" endWordPosition="3604">he MEDLINE abstracting database where similar features to those presented here were used to identify in abstracts semantic categories such as objective, method, results, and conclusions. Related to this is the work by Teufel and Moens (2002) on rhetorical classification for content selection. In cut-and-paste summarization (Jing and McKeown, 2000), sentence combination operations were implemented manually following the study of a set of professionally written abstracts; however the particular “pasting” operation presented here was not implemented. Previous studies on text-to-text abstracting (Banko et al., 2000; Knight and Marcu, 2000) have studied problems such as sentence compression and sentence combination but not the “pasting” procedure presented here. The insertion in the abstract of linguistic material not present in the input document has been addressed in paraphrase generation (Barzilay and Lee, 2004) and canned-based sum• First, we have presented the problem of generating the discourse structures of an abstract and proposed a meta algorithm for predicting it. This problem has not been investigated before. • Second, we have proposed – based on previous summarization research – a number of f</context>
</contexts>
<marker>Banko, Mittal, Witbrock, 2000</marker>
<rawString>Michele Banko, Vibhu O. Mittal, and Michael J. Witbrock. 2000. Headline generation based on statistical translation. In ACL ’00: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, pages 318–325, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using Lexical Chains for Text Summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/EACL’97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>10--17</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="13387" citStr="Barzilay and Elhadad, 1997" startWordPosition="2091" endWordPosition="2095"> to provide; to report; to say Table 2: Predicates in the reduced corpus arguments of “semantic” triples (Section 2.1) extracted from the parsed abstracts. Features occurring less than 4 times in the corpus were removed for the experiments. For each sentence, a cohesion feature is also computed as the number of nouns in common with the previous sentence fragment (or title if first sentence). Cohesion information has been used in rhetorical-based parsing for summarization (Marcu, 1997) in order to decide between “list” or “elaboration” relations and also in content selection for summarization (Barzilay and Elhadad, 1997). For some experiments we also use word-level information (lemmas) and part-of-speech tags. For some of the experiments reported here the variable Context at iteration i in Algorithm 1 is instantiated with the predicates predicted at iterations i −1 and i − 2. 4 Experiments and Results The experiments reported here correspond to the use of different features as input for the classifier. In these experiments we have used a subset of the collected abstracts, they contain predicates which appeared at least 5 times in the corpus. With this restriction in place the original set of predicates used t</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Regina Barzilay and Michael Elhadad. 1997. Using Lexical Chains for Text Summarization. In Proceedings of the ACL/EACL’97 Workshop on Intelligent Scalable Text Summarization, pages 10–17, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="23202" citStr="Barzilay and Lee, 2004" startWordPosition="3648" endWordPosition="3651">-and-paste summarization (Jing and McKeown, 2000), sentence combination operations were implemented manually following the study of a set of professionally written abstracts; however the particular “pasting” operation presented here was not implemented. Previous studies on text-to-text abstracting (Banko et al., 2000; Knight and Marcu, 2000) have studied problems such as sentence compression and sentence combination but not the “pasting” procedure presented here. The insertion in the abstract of linguistic material not present in the input document has been addressed in paraphrase generation (Barzilay and Lee, 2004) and canned-based sum• First, we have presented the problem of generating the discourse structures of an abstract and proposed a meta algorithm for predicting it. This problem has not been investigated before. • Second, we have proposed – based on previous summarization research – a number of features to be used for solving this problem; and • Finally, we have propose several instantiations of the algorithm to solve the problem and achieved a reasonable accuracy using the designed features; There is however much space for improvement even though the algorithm recalls some “partial structures”,</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>R. Barzilay and L. Lee. 2004. Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization. In Proceedings of HLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>M Lapata</author>
</authors>
<title>Sentence compression beyond word deletion.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING 2008,</booktitle>
<location>Manchester.</location>
<contexts>
<context position="3673" citStr="Cohn and Lapata, 2008" startWordPosition="545" endWordPosition="548">ntent of the abstracted document by explicitly marking what the author says or mentions, presents or introduces, concludes, or includes, in her paper. Montesi and Owen (2007) observe that the revision of abstracts is carried out to improve comprehensibility and style and to make the abstract objective. We investigate how to create the discourse structure of the abstracts: more specifically we are interested in predicting the inserted predicates or phrases and at which positions in the abstract they should be prepended. Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). Close to the problem studied here is Jing and McKeown’s (Jing and McKeown, 2000) cut-and-paste method founded on EndresNiggemeyer’s observations. The cut-and-paste 31 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 31–38, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP method includes such operations as sentence truncation, aggregation, specialization/generalization, reference adjustment and rewording. None of these operations account fo</context>
</contexts>
<marker>Cohn, Lapata, 2008</marker>
<rawString>T. Cohn and M. Lapata. 2008. Sentence compression beyond word deletion. In Proceedings of COLING 2008, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
</authors>
<title>GATE: A framework and graphical development environment for robust NLP tools and applications. In</title>
<date>2002</date>
<booktitle>ACL</booktitle>
<contexts>
<context position="7163" citStr="Cunningham et al., 2002" startWordPosition="1088" endWordPosition="1091">llected abstracts from various databases including LISA, ERIC, and Internet &amp; Personal Computing Abstracts, using our institutional library’s facilities and the abstracts’ providers’ keyword search facilities. Electronic copies of the abstracted documents can also be accessed through our institution following a link, thus allowing us to check abstracts against abstracted document (additional information on the abstracts is given in the Appendix). 2.1 Document Processing and Annotation Each electronic version of the abstract was processed using the freely available GATE text analysis software (Cunningham et al., 2002). First each abstract was analyzed by a text structure analysis program to identify meta-data such as title, author, source document, the text of the abstract, etc. Each sentence in the abstract was stripped from the predicate or phrase inserted by the abstractor (e.g., “Mentions that”, “Concludes with”) and a normalised version of the expression was used to annotate the sentence, in a way similar to the abstracts in Figures 1 and 2. After this each abstract and document title was tokenised, sentence splitted, part-of-speech tagged, and morphologically analyzed. A rule-based system was used to</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan. 2002. GATE: A framework and graphical development environment for robust NLP tools and applications. In ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New Methods in Automatic Extracting.</title>
<date>1969</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="19312" citStr="Edmundson, 1969" startWordPosition="3023" endWordPosition="3024">ation features including linguistic, semantic, positional, cohesive, and discursive information can predict the local discourse structures in over 60% of the cases. There is a mixed picture on the prediction of individual predicates, with most predicates correctly classified in most of the cases except for predicates such as “to describe”, “to note”, and “to report” which are confused with other phrases. Predicates such as “to present” and “to include” have the tendency of appearing towards the very beginning or the very end of the abstract been therefore predicted by position-based features (Edmundson, 1969; Lin and Hovy, 1997). Note that in this work we have decided to evaluate the predicted structure against the true structure (a hard evaluation measure), in future work we will assess the abstracts with a set of quality questions similar to those put forward by the Document Understanding Conference Evaluations (also in a way similar to (Kan and McKeown, 2002) who evaluated their abstracts in a retrieval environment). We expect to obtain a reasonable evaluation result given that it appears that some of the predicates or phrases are “interchangeable” (e.g., “to contain” and “to include”). Actual</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>H.P. Edmundson. 1969. New Methods in Automatic Extracting. Journal of the Association for Computing Machinery, 16(2):264–285, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brigitte Endres-Niggemeyer</author>
</authors>
<title>SimSum: an empirically founded simulation of summarizing.</title>
<date>2000</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<pages>36--659</pages>
<contexts>
<context position="1083" citStr="Endres-Niggemeyer (2000)" startWordPosition="158" endWordPosition="159">ly annotated with a text analysis tool. We design a set of features for learning inspired from past research in content selection, information ordering, and rhetorical analysis for training an algorithm which then predicts the discourse structure of unseen abstracts. The proposed approach to the problem which combines local and contextual features is able to predict the local structure of the abstracts in just over 60% of the cases. 1 Introduction Mani (2001) defines an abstract as “a summary at least some of whose material is not present in the input”. In a study of professional abstracting, Endres-Niggemeyer (2000) concluded that professional abstractors produce abstracts by “cut-andpaste” operations, and that standard sentence patterns are used in their production. Examples of abstracts produced by a professional abstractor are shown in Figures 1 and 2. They contain fragments “copied” from the input documents together with phrases (underlined in the figures) inserted by the professional abstractors. In a recent study in human abstracting (restricted to the amendment of authors abstracts) Montesi and Owen (2007) noted that professional abstractors prepend third person singular verbs in present tense and</context>
</contexts>
<marker>Endres-Niggemeyer, 2000</marker>
<rawString>Brigitte Endres-Niggemeyer. 2000. SimSum: an empirically founded simulation of summarizing. Information Processing &amp; Management, 36:659–682.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gaizauskas</author>
<author>M Hepple</author>
<author>H Saggion</author>
<author>M Greenwood</author>
</authors>
<title>SUPPLE: A Practical Parser for Natural Language Engineering Applications.</title>
<date>2005</date>
<contexts>
<context position="7864" citStr="Gaizauskas et al., 2005" startWordPosition="1202" endWordPosition="1205">entify meta-data such as title, author, source document, the text of the abstract, etc. Each sentence in the abstract was stripped from the predicate or phrase inserted by the abstractor (e.g., “Mentions that”, “Concludes with”) and a normalised version of the expression was used to annotate the sentence, in a way similar to the abstracts in Figures 1 and 2. After this each abstract and document title was tokenised, sentence splitted, part-of-speech tagged, and morphologically analyzed. A rule-based system was used to carry out partial, robust syntactic and semantic analysis of the abstracts (Gaizauskas et al., 2005) producing predicate-argument representations where predicates which are used to represent entities are created from the morphological roots of nouns or verbs in the text (unary predicates) and predicates with are used to represent binary relations are a closed set of names representing grammatical relations such as the verb logical object, or the verb logical subject or a prepositional attachment, etc. This predicate-argument structure representation was further analysed in order to extract “semantic” triples which are used in the experiments reported here. Output of this analysis is shown in</context>
</contexts>
<marker>Gaizauskas, Hepple, Saggion, Greenwood, 2005</marker>
<rawString>R. Gaizauskas, M. Hepple, H. Saggion, and M. Greenwood. 2005. SUPPLE: A Practical Parser for Natural Language Engineering Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen McKeown</author>
</authors>
<title>Cut and Paste Based Text Summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>178--185</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="3855" citStr="Jing and McKeown, 2000" startWordPosition="572" endWordPosition="575">e that the revision of abstracts is carried out to improve comprehensibility and style and to make the abstract objective. We investigate how to create the discourse structure of the abstracts: more specifically we are interested in predicting the inserted predicates or phrases and at which positions in the abstract they should be prepended. Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). Close to the problem studied here is Jing and McKeown’s (Jing and McKeown, 2000) cut-and-paste method founded on EndresNiggemeyer’s observations. The cut-and-paste 31 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 31–38, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP method includes such operations as sentence truncation, aggregation, specialization/generalization, reference adjustment and rewording. None of these operations account for the transformations observed in the abstracts of Figures 1 and 2. The formulaic expressions or predicates inserted in the abstract “glue” together the extracted fragments, thus cre</context>
<context position="22628" citStr="Jing and McKeown, 2000" startWordPosition="3562" endWordPosition="3565">o accommodate the generation process. In our approach the discourse structure is not fixed but predicted for each particular abstract. Related to our classification experiments is work on semantic or rhetorical classification of “structured” abstracts (Saggion, 2008) from the MEDLINE abstracting database where similar features to those presented here were used to identify in abstracts semantic categories such as objective, method, results, and conclusions. Related to this is the work by Teufel and Moens (2002) on rhetorical classification for content selection. In cut-and-paste summarization (Jing and McKeown, 2000), sentence combination operations were implemented manually following the study of a set of professionally written abstracts; however the particular “pasting” operation presented here was not implemented. Previous studies on text-to-text abstracting (Banko et al., 2000; Knight and Marcu, 2000) have studied problems such as sentence compression and sentence combination but not the “pasting” procedure presented here. The insertion in the abstract of linguistic material not present in the input document has been addressed in paraphrase generation (Barzilay and Lee, 2004) and canned-based sum• Fir</context>
</contexts>
<marker>Jing, McKeown, 2000</marker>
<rawString>Hongyan Jing and Kathleen McKeown. 2000. Cut and Paste Based Text Summarization. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics, pages 178–185, Seattle, Washington, USA, April 29 -May 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In European Conference on Machine Learning (ECML),</booktitle>
<pages>137--142</pages>
<publisher>Springer.</publisher>
<location>Berlin.</location>
<contexts>
<context position="11651" citStr="Joachims, 1998" startWordPosition="1816" endWordPosition="1817">he PredictPredicate function in Algorithm 1). In the experiments reported here we use a classification algorithm based on lexical, syntactic, and discursive features, which decides which of the N possible available phrases is most suitable. The algorithm is trained over the annotated abstracts and used to predict the structure of unseen test abstracts. Where the classification algorithm is concerned, we have decided to use Support Vector Machines which have recently been used in different tasks in natural language processing, they have been shown particularly suitable for text categorization (Joachims, 1998). We have tried other machine learning algorithms such as Decision Trees, Naive Bayes Classification, and Nearest Neighbor from the Weka toolkit (Witten and Frank, 1999), but the support vector machines gave us the best classification accuracy (a comparison with Naive Bayes will be presented in Section 4). The features used for the experiments reported here are inspired by previous work in text summarization on content selection (Kupiec et al., 1995), rhetorical classification (Teufel and Moens, 2002), and information ordering (Lapata, 2003). The features are extracted from the analyzed abstra</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In European Conference on Machine Learning (ECML), pages 137–142, Berlin. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Yen Kan</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Corpus-trained text generation for summarization.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Natural Language Generation Conference (INLG</booktitle>
<pages>1--8</pages>
<location>Harriman, New York, USA.</location>
<contexts>
<context position="19673" citStr="Kan and McKeown, 2002" startWordPosition="3082" endWordPosition="3085">nd “to report” which are confused with other phrases. Predicates such as “to present” and “to include” have the tendency of appearing towards the very beginning or the very end of the abstract been therefore predicted by position-based features (Edmundson, 1969; Lin and Hovy, 1997). Note that in this work we have decided to evaluate the predicted structure against the true structure (a hard evaluation measure), in future work we will assess the abstracts with a set of quality questions similar to those put forward by the Document Understanding Conference Evaluations (also in a way similar to (Kan and McKeown, 2002) who evaluated their abstracts in a retrieval environment). We expect to obtain a reasonable evaluation result given that it appears that some of the predicates or phrases are “interchangeable” (e.g., “to contain” and “to include”). Actual Pred. Predicted Pred. Conf.Freq. to add to add 32% to explain 16% to say 10% to conclude to conclude 35% to say 29% to add 7% to explain to explain 35% to say 15% to add 11% to present to present 86% to discuss 7% to provide 1% Table 5: Classification Confusion Table for a Subset of Predicates in the Corpus (Average Frequency). 6 Related Work Liddy (1991) pr</context>
</contexts>
<marker>Kan, McKeown, 2002</marker>
<rawString>Min-Yen Kan and Kathleen R.. McKeown. 2002. Corpus-trained text generation for summarization. In Proceedings of the Second International Natural Language Generation Conference (INLG 2002), pages 1–8, Harriman, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticsbased summarization - step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th National Conference of the American Association for Artificial Intelligence. AAAI,</booktitle>
<contexts>
<context position="22922" citStr="Knight and Marcu, 2000" startWordPosition="3605" endWordPosition="3608">ng database where similar features to those presented here were used to identify in abstracts semantic categories such as objective, method, results, and conclusions. Related to this is the work by Teufel and Moens (2002) on rhetorical classification for content selection. In cut-and-paste summarization (Jing and McKeown, 2000), sentence combination operations were implemented manually following the study of a set of professionally written abstracts; however the particular “pasting” operation presented here was not implemented. Previous studies on text-to-text abstracting (Banko et al., 2000; Knight and Marcu, 2000) have studied problems such as sentence compression and sentence combination but not the “pasting” procedure presented here. The insertion in the abstract of linguistic material not present in the input document has been addressed in paraphrase generation (Barzilay and Lee, 2004) and canned-based sum• First, we have presented the problem of generating the discourse structures of an abstract and proposed a meta algorithm for predicting it. This problem has not been investigated before. • Second, we have proposed – based on previous summarization research – a number of features to be used for so</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization - step one: Sentence compression. In Proceedings of the 17th National Conference of the American Association for Artificial Intelligence. AAAI, July 30 - August 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A Trainable Document Summarizer.</title>
<date>1995</date>
<booktitle>In Proc. of the 18th ACM-SIGIR Conference,</booktitle>
<pages>68--73</pages>
<location>Seattle, Washington, United States.</location>
<contexts>
<context position="10562" citStr="Kupiec et al., 1995" startWordPosition="1645" endWordPosition="1648">esentation (partial) Algorithm 1 Discourse Structure Prediction Algorithm Given: a list of n sorted text fragments Pi begin Abstract ← ““; Context ← START; for all i : 0 ≤ i ≤ n−1; do Pred ← PredictPredicate(Context,Pi); Abstract ← Abstract ⊕ Pred ⊕ Pi ⊕ “.”; Context ← ExtractContext(Abstract); end for return Abstract end at each iteration the algorithm selects the “best” available phrase or predicate to prepend to the current fragment from a finite vocabulary (induced from the analysed corpus) based on local and contextual information. One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate Pi’s from the document to be abstracted and rely on recent information ordering techniques to sort the Pi fragments (Lapata, 2003). This is the reason why we only address here the discourse structure generation problem. 3.1 Predicting Discourse Structure as Classification There are various possible ways of predicting what expression to insert at each point in the generation process (i.e., the PredictPredicate function in Algorithm 1). In the experiments reported here we use a classification algorithm based on lexi</context>
<context position="12105" citStr="Kupiec et al., 1995" startWordPosition="1886" endWordPosition="1889">s which have recently been used in different tasks in natural language processing, they have been shown particularly suitable for text categorization (Joachims, 1998). We have tried other machine learning algorithms such as Decision Trees, Naive Bayes Classification, and Nearest Neighbor from the Weka toolkit (Witten and Frank, 1999), but the support vector machines gave us the best classification accuracy (a comparison with Naive Bayes will be presented in Section 4). The features used for the experiments reported here are inspired by previous work in text summarization on content selection (Kupiec et al., 1995), rhetorical classification (Teufel and Moens, 2002), and information ordering (Lapata, 2003). The features are extracted from the analyzed abstracts with specialized programs. In particular we use positional features (position of the predicate to be generated in the structure), length features (number of words in the sentence), title features (e.g., presence of title words in sentence), content features computed as the syntactic head of noun and verb phrases, semantic features computed as the 33 to add; to conclude; to contain; to describe; to discuss; to explain; to feature; to include; to i</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A Trainable Document Summarizer. In Proc. of the 18th ACM-SIGIR Conference, pages 68–73, Seattle, Washington, United States.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
</authors>
<title>Probabilistic Text Structuring: Experiments with Sentence Ordering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association of Computational Linguistics,</booktitle>
<pages>545--552</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="10773" citStr="Lapata, 2003" startWordPosition="1683" endWordPosition="1684">); Abstract ← Abstract ⊕ Pred ⊕ Pi ⊕ “.”; Context ← ExtractContext(Abstract); end for return Abstract end at each iteration the algorithm selects the “best” available phrase or predicate to prepend to the current fragment from a finite vocabulary (induced from the analysed corpus) based on local and contextual information. One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate Pi’s from the document to be abstracted and rely on recent information ordering techniques to sort the Pi fragments (Lapata, 2003). This is the reason why we only address here the discourse structure generation problem. 3.1 Predicting Discourse Structure as Classification There are various possible ways of predicting what expression to insert at each point in the generation process (i.e., the PredictPredicate function in Algorithm 1). In the experiments reported here we use a classification algorithm based on lexical, syntactic, and discursive features, which decides which of the N possible available phrases is most suitable. The algorithm is trained over the annotated abstracts and used to predict the structure of unsee</context>
<context position="12198" citStr="Lapata, 2003" startWordPosition="1899" endWordPosition="1900">own particularly suitable for text categorization (Joachims, 1998). We have tried other machine learning algorithms such as Decision Trees, Naive Bayes Classification, and Nearest Neighbor from the Weka toolkit (Witten and Frank, 1999), but the support vector machines gave us the best classification accuracy (a comparison with Naive Bayes will be presented in Section 4). The features used for the experiments reported here are inspired by previous work in text summarization on content selection (Kupiec et al., 1995), rhetorical classification (Teufel and Moens, 2002), and information ordering (Lapata, 2003). The features are extracted from the analyzed abstracts with specialized programs. In particular we use positional features (position of the predicate to be generated in the structure), length features (number of words in the sentence), title features (e.g., presence of title words in sentence), content features computed as the syntactic head of noun and verb phrases, semantic features computed as the 33 to add; to conclude; to contain; to describe; to discuss; to explain; to feature; to include; to indicate; to mention; to note; to point out; to present; to provide; to report; to say Table 2</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>M. Lapata. 2003. Probabilistic Text Structuring: Experiments with Sentence Ordering. In Proceedings of the 41st Meeting of the Association of Computational Linguistics, pages 545–552, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth D Liddy</author>
</authors>
<title>The Discourse-Level Structure of Empirical Abstracts: An Exploratory Study.</title>
<date>1991</date>
<journal>Information Processing &amp; Management,</journal>
<volume>27</volume>
<issue>1</issue>
<pages>81</pages>
<contexts>
<context position="20270" citStr="Liddy (1991)" startWordPosition="3191" endWordPosition="3192"> McKeown, 2002) who evaluated their abstracts in a retrieval environment). We expect to obtain a reasonable evaluation result given that it appears that some of the predicates or phrases are “interchangeable” (e.g., “to contain” and “to include”). Actual Pred. Predicted Pred. Conf.Freq. to add to add 32% to explain 16% to say 10% to conclude to conclude 35% to say 29% to add 7% to explain to explain 35% to say 15% to add 11% to present to present 86% to discuss 7% to provide 1% Table 5: Classification Confusion Table for a Subset of Predicates in the Corpus (Average Frequency). 6 Related Work Liddy (1991) produced a formal model of the informational or conceptual structure of abstracts of empirical research. This structure was elicited from abstractors of two organizations ERIC and PsycINFO through a series of tasks. Lexical clues which predict the components of the structure were latter induced by corpus analysis. In the domain of indicative summarization, Kan and McK35 Predicate Avg. Accuracy to add 31.40 to conclude 34.78 to contain 10.96 to describe 15.69 to discuss 54.55 to explain 35.63 to feature 34.38 to include 85.86 to indicate 20.69 to mention 26.47 to note 6.78 to point out 91.67 t</context>
</contexts>
<marker>Liddy, 1991</marker>
<rawString>Elizabeth D. Liddy. 1991. The Discourse-Level Structure of Empirical Abstracts: An Exploratory Study. Information Processing &amp; Management, 27(1):55– 81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>E Hovy</author>
</authors>
<title>Identifying Topics by Position.</title>
<date>1997</date>
<booktitle>In Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>283--290</pages>
<contexts>
<context position="19333" citStr="Lin and Hovy, 1997" startWordPosition="3025" endWordPosition="3028">cluding linguistic, semantic, positional, cohesive, and discursive information can predict the local discourse structures in over 60% of the cases. There is a mixed picture on the prediction of individual predicates, with most predicates correctly classified in most of the cases except for predicates such as “to describe”, “to note”, and “to report” which are confused with other phrases. Predicates such as “to present” and “to include” have the tendency of appearing towards the very beginning or the very end of the abstract been therefore predicted by position-based features (Edmundson, 1969; Lin and Hovy, 1997). Note that in this work we have decided to evaluate the predicted structure against the true structure (a hard evaluation measure), in future work we will assess the abstracts with a set of quality questions similar to those put forward by the Document Understanding Conference Evaluations (also in a way similar to (Kan and McKeown, 2002) who evaluated their abstracts in a retrieval environment). We expect to obtain a reasonable evaluation result given that it appears that some of the predicates or phrases are “interchangeable” (e.g., “to contain” and “to include”). Actual Pred. Predicted Pred</context>
</contexts>
<marker>Lin, Hovy, 1997</marker>
<rawString>C. Lin and E. Hovy. 1997. Identifying Topics by Position. In Fifth Conference on Applied Natural Language Processing, pages 283–290. Association for Computational Linguistics, 31 March-3 April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Automatic Text Summarization.</title>
<date>2001</date>
<publisher>John Benjamins Publishing Company.</publisher>
<contexts>
<context position="922" citStr="Mani (2001)" startWordPosition="132" endWordPosition="133">ndent abstracts. We apply a supervised machine learning approach trained over a set of abstracts collected from abstracting services and automatically annotated with a text analysis tool. We design a set of features for learning inspired from past research in content selection, information ordering, and rhetorical analysis for training an algorithm which then predicts the discourse structure of unseen abstracts. The proposed approach to the problem which combines local and contextual features is able to predict the local structure of the abstracts in just over 60% of the cases. 1 Introduction Mani (2001) defines an abstract as “a summary at least some of whose material is not present in the input”. In a study of professional abstracting, Endres-Niggemeyer (2000) concluded that professional abstractors produce abstracts by “cut-andpaste” operations, and that standard sentence patterns are used in their production. Examples of abstracts produced by a professional abstractor are shown in Figures 1 and 2. They contain fragments “copied” from the input documents together with phrases (underlined in the figures) inserted by the professional abstractors. In a recent study in human abstracting (restr</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Inderjeet Mani. 2001. Automatic Text Summarization. John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The Rhetorical Parsing, Summarization, and Generation of Natural Language Texts.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Toronto.</institution>
<contexts>
<context position="13249" citStr="Marcu, 1997" startWordPosition="2072" endWordPosition="2073">n; to describe; to discuss; to explain; to feature; to include; to indicate; to mention; to note; to point out; to present; to provide; to report; to say Table 2: Predicates in the reduced corpus arguments of “semantic” triples (Section 2.1) extracted from the parsed abstracts. Features occurring less than 4 times in the corpus were removed for the experiments. For each sentence, a cohesion feature is also computed as the number of nouns in common with the previous sentence fragment (or title if first sentence). Cohesion information has been used in rhetorical-based parsing for summarization (Marcu, 1997) in order to decide between “list” or “elaboration” relations and also in content selection for summarization (Barzilay and Elhadad, 1997). For some experiments we also use word-level information (lemmas) and part-of-speech tags. For some of the experiments reported here the variable Context at iteration i in Algorithm 1 is instantiated with the predicates predicted at iterations i −1 and i − 2. 4 Experiments and Results The experiments reported here correspond to the use of different features as input for the classifier. In these experiments we have used a subset of the collected abstracts, t</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>D. Marcu. 1997. The Rhetorical Parsing, Summarization, and Generation of Natural Language Texts. Ph.D. thesis, Department of Computer Science, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Montesi</author>
<author>J M Owen</author>
</authors>
<title>Revision of author abstracts: how it is carried out</title>
<date>2007</date>
<booktitle>Aslib Proceedings,</booktitle>
<pages>59--1</pages>
<editor>by LISA editors.</editor>
<contexts>
<context position="1590" citStr="Montesi and Owen (2007)" startWordPosition="232" endWordPosition="235">ome of whose material is not present in the input”. In a study of professional abstracting, Endres-Niggemeyer (2000) concluded that professional abstractors produce abstracts by “cut-andpaste” operations, and that standard sentence patterns are used in their production. Examples of abstracts produced by a professional abstractor are shown in Figures 1 and 2. They contain fragments “copied” from the input documents together with phrases (underlined in the figures) inserted by the professional abstractors. In a recent study in human abstracting (restricted to the amendment of authors abstracts) Montesi and Owen (2007) noted that professional abstractors prepend third person singular verbs in present tense and without subject to the author abstract, a phenomenon related – yet different – from the problem we are investigating in this paper. Note that the phrases or predicates prepended to the selected sentence fragments copied from the input document have a communicative function: Presents a model instructional session that was prepared and taught by librarians to introduce college students, faculty, and staff to the Internet by teaching them how to join listservs and topic- centered discussion groups. Descr</context>
<context position="3225" citStr="Montesi and Owen (2007)" startWordPosition="477" endWordPosition="480">eports that Newsblaster is a cross between a search engine and ... Explains that Newsblaster publishes the summaries in a Web page that divides the day summaries into .... Mentions that Newsblaster is considered an aid to those who have to quickly canvas large amounts of information from many sources. Figure 2: Professional Abstract with Inserted Predicates from Internet &amp; Personal Computing Abstracts they inform or alert the reader about the content of the abstracted document by explicitly marking what the author says or mentions, presents or introduces, concludes, or includes, in her paper. Montesi and Owen (2007) observe that the revision of abstracts is carried out to improve comprehensibility and style and to make the abstract objective. We investigate how to create the discourse structure of the abstracts: more specifically we are interested in predicting the inserted predicates or phrases and at which positions in the abstract they should be prepended. Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). Close to the problem studied here is Jing and McKe</context>
</contexts>
<marker>Montesi, Owen, 2007</marker>
<rawString>M. Montesi and J. M. Owen. 2007. Revision of author abstracts: how it is carried out by LISA editors. Aslib Proceedings, 59(1):26–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Oakes</author>
<author>C D Paice</author>
</authors>
<title>Term extraction for automatic abstracting.</title>
<date>2001</date>
<booktitle>Recent Advances in Computational Terminology,</booktitle>
<volume>2</volume>
<pages>353--370</pages>
<editor>In D. Bourigault, C. Jacquemin, and M-C. L’Homme, editors,</editor>
<publisher>John Benjamins Publishing Company.</publisher>
<contexts>
<context position="3773" citStr="Oakes and Paice, 2001" startWordPosition="558" endWordPosition="561"> introduces, concludes, or includes, in her paper. Montesi and Owen (2007) observe that the revision of abstracts is carried out to improve comprehensibility and style and to make the abstract objective. We investigate how to create the discourse structure of the abstracts: more specifically we are interested in predicting the inserted predicates or phrases and at which positions in the abstract they should be prepended. Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). Close to the problem studied here is Jing and McKeown’s (Jing and McKeown, 2000) cut-and-paste method founded on EndresNiggemeyer’s observations. The cut-and-paste 31 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 31–38, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP method includes such operations as sentence truncation, aggregation, specialization/generalization, reference adjustment and rewording. None of these operations account for the transformations observed in the abstracts of Figures 1 and 2. The formulaic expressions or pre</context>
<context position="21008" citStr="Oakes and Paice, 2001" startWordPosition="3310" endWordPosition="3313">ture was elicited from abstractors of two organizations ERIC and PsycINFO through a series of tasks. Lexical clues which predict the components of the structure were latter induced by corpus analysis. In the domain of indicative summarization, Kan and McK35 Predicate Avg. Accuracy to add 31.40 to conclude 34.78 to contain 10.96 to describe 15.69 to discuss 54.55 to explain 35.63 to feature 34.38 to include 85.86 to indicate 20.69 to mention 26.47 to note 6.78 to point out 91.67 to present 86.19 to provide 40.94 to report 1.59 to say 75.86 Table 6: Predicate Classification Accuracy marization (Oakes and Paice, 2001) in limited domains. Saggion and Lapalme (2002) have studied and implemented a rule-based “verb selection” operation in their SumUM system which has been applied to introduce document topics during indicative summary generation. Our discourse structure generation procedure is in principle generic but depends on the availability of a corpus for training. 7 Conclusions In text summarization research, most attention has been paid to the problem of what information to select for a summary. Here, we have focused on the problem of how to combine the selected content with extra linguistic information</context>
</contexts>
<marker>Oakes, Paice, 2001</marker>
<rawString>M.P. Oakes and C.D. Paice. 2001. Term extraction for automatic abstracting. In D. Bourigault, C. Jacquemin, and M-C. L’Homme, editors, Recent Advances in Computational Terminology, volume 2 of Natural Language Processing, chapter 17, pages 353–370. John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>G Lapalme</author>
</authors>
<date>2002</date>
<booktitle>Generating Indicative-Informative Summaries with SumUM. Computational Linguistics,</booktitle>
<pages>28--4</pages>
<contexts>
<context position="4579" citStr="Saggion and Lapalme (2002)" startWordPosition="672" endWordPosition="675"> of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 31–38, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP method includes such operations as sentence truncation, aggregation, specialization/generalization, reference adjustment and rewording. None of these operations account for the transformations observed in the abstracts of Figures 1 and 2. The formulaic expressions or predicates inserted in the abstract “glue” together the extracted fragments, thus creating the abstract’s discourse structure. To the best of our knowledge, and with the exception of Saggion and Lapalme (2002) indicative generation approach which included operations to add extra linguistic material to generate an indicative abstract, the work presented here is the first to investigate this relevant operation in the field of text abstracting and to propose a robust computational method for its simulation. In this paper we are interested in the process of generating the structure of the abstract by automatic means. In order to study this problem, we have collected a corpus of abstracts written by abstractors; we have designed an algorithm for predicting the structure; implemented the algorithm; and e</context>
<context position="21055" citStr="Saggion and Lapalme (2002)" startWordPosition="3318" endWordPosition="3321">rganizations ERIC and PsycINFO through a series of tasks. Lexical clues which predict the components of the structure were latter induced by corpus analysis. In the domain of indicative summarization, Kan and McK35 Predicate Avg. Accuracy to add 31.40 to conclude 34.78 to contain 10.96 to describe 15.69 to discuss 54.55 to explain 35.63 to feature 34.38 to include 85.86 to indicate 20.69 to mention 26.47 to note 6.78 to point out 91.67 to present 86.19 to provide 40.94 to report 1.59 to say 75.86 Table 6: Predicate Classification Accuracy marization (Oakes and Paice, 2001) in limited domains. Saggion and Lapalme (2002) have studied and implemented a rule-based “verb selection” operation in their SumUM system which has been applied to introduce document topics during indicative summary generation. Our discourse structure generation procedure is in principle generic but depends on the availability of a corpus for training. 7 Conclusions In text summarization research, most attention has been paid to the problem of what information to select for a summary. Here, we have focused on the problem of how to combine the selected content with extra linguistic information in order to create the structure of the summar</context>
</contexts>
<marker>Saggion, Lapalme, 2002</marker>
<rawString>H. Saggion and G. Lapalme. 2002. Generating Indicative-Informative Summaries with SumUM. Computational Linguistics, 28(4):497–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
</authors>
<title>Automatic Summarization: An Overview. Revue Franc¸aise de Linguistique Appliqu´ee ,</title>
<date>2008</date>
<location>XIII(1), Juin.</location>
<contexts>
<context position="22272" citStr="Saggion, 2008" startWordPosition="3513" endWordPosition="3514">There are several contributions of this work: eown (2002) studied the problem of generating abstracts for bibliographical data which although in a restricted domain has some contact points with the work described here. As in their work we use the abstracts in our corpus to induce the model. They rely on a more or less fixed discourse structure to accommodate the generation process. In our approach the discourse structure is not fixed but predicted for each particular abstract. Related to our classification experiments is work on semantic or rhetorical classification of “structured” abstracts (Saggion, 2008) from the MEDLINE abstracting database where similar features to those presented here were used to identify in abstracts semantic categories such as objective, method, results, and conclusions. Related to this is the work by Teufel and Moens (2002) on rhetorical classification for content selection. In cut-and-paste summarization (Jing and McKeown, 2000), sentence combination operations were implemented manually following the study of a set of professionally written abstracts; however the particular “pasting” operation presented here was not implemented. Previous studies on text-to-text abstra</context>
</contexts>
<marker>Saggion, 2008</marker>
<rawString>H. Saggion. 2008. Automatic Summarization: An Overview. Revue Franc¸aise de Linguistique Appliqu´ee , XIII(1), Juin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Abstractive headline generation using WIDL-expressions.</title>
<date>2007</date>
<journal>Inf. Process. Manage.,</journal>
<volume>43</volume>
<issue>6</issue>
<contexts>
<context position="3720" citStr="Soricut and Marcu, 2007" startWordPosition="551" endWordPosition="554">y marking what the author says or mentions, presents or introduces, concludes, or includes, in her paper. Montesi and Owen (2007) observe that the revision of abstracts is carried out to improve comprehensibility and style and to make the abstract objective. We investigate how to create the discourse structure of the abstracts: more specifically we are interested in predicting the inserted predicates or phrases and at which positions in the abstract they should be prepended. Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). Close to the problem studied here is Jing and McKeown’s (Jing and McKeown, 2000) cut-and-paste method founded on EndresNiggemeyer’s observations. The cut-and-paste 31 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 31–38, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP method includes such operations as sentence truncation, aggregation, specialization/generalization, reference adjustment and rewording. None of these operations account for the transformations observed in the abstracts</context>
</contexts>
<marker>Soricut, Marcu, 2007</marker>
<rawString>R. Soricut and D. Marcu. 2007. Abstractive headline generation using WIDL-expressions. Inf. Process. Manage., 43(6):1536–1548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>Summarizing Scientific Articles: Experiments with Relevance and Rhetorical Status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="12157" citStr="Teufel and Moens, 2002" startWordPosition="1892" endWordPosition="1895">s in natural language processing, they have been shown particularly suitable for text categorization (Joachims, 1998). We have tried other machine learning algorithms such as Decision Trees, Naive Bayes Classification, and Nearest Neighbor from the Weka toolkit (Witten and Frank, 1999), but the support vector machines gave us the best classification accuracy (a comparison with Naive Bayes will be presented in Section 4). The features used for the experiments reported here are inspired by previous work in text summarization on content selection (Kupiec et al., 1995), rhetorical classification (Teufel and Moens, 2002), and information ordering (Lapata, 2003). The features are extracted from the analyzed abstracts with specialized programs. In particular we use positional features (position of the predicate to be generated in the structure), length features (number of words in the sentence), title features (e.g., presence of title words in sentence), content features computed as the syntactic head of noun and verb phrases, semantic features computed as the 33 to add; to conclude; to contain; to describe; to discuss; to explain; to feature; to include; to indicate; to mention; to note; to point out; to prese</context>
<context position="22520" citStr="Teufel and Moens (2002)" startWordPosition="3549" endWordPosition="3552">use the abstracts in our corpus to induce the model. They rely on a more or less fixed discourse structure to accommodate the generation process. In our approach the discourse structure is not fixed but predicted for each particular abstract. Related to our classification experiments is work on semantic or rhetorical classification of “structured” abstracts (Saggion, 2008) from the MEDLINE abstracting database where similar features to those presented here were used to identify in abstracts semantic categories such as objective, method, results, and conclusions. Related to this is the work by Teufel and Moens (2002) on rhetorical classification for content selection. In cut-and-paste summarization (Jing and McKeown, 2000), sentence combination operations were implemented manually following the study of a set of professionally written abstracts; however the particular “pasting” operation presented here was not implemented. Previous studies on text-to-text abstracting (Banko et al., 2000; Knight and Marcu, 2000) have studied problems such as sentence compression and sentence combination but not the “pasting” procedure presented here. The insertion in the abstract of linguistic material not present in the i</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>S. Teufel and M. Moens. 2002. Summarizing Scientific Articles: Experiments with Relevance and Rhetorical Status. Computational Linguistics, 28(4):409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>1999</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="11820" citStr="Witten and Frank, 1999" startWordPosition="1839" endWordPosition="1842">ures, which decides which of the N possible available phrases is most suitable. The algorithm is trained over the annotated abstracts and used to predict the structure of unseen test abstracts. Where the classification algorithm is concerned, we have decided to use Support Vector Machines which have recently been used in different tasks in natural language processing, they have been shown particularly suitable for text categorization (Joachims, 1998). We have tried other machine learning algorithms such as Decision Trees, Naive Bayes Classification, and Nearest Neighbor from the Weka toolkit (Witten and Frank, 1999), but the support vector machines gave us the best classification accuracy (a comparison with Naive Bayes will be presented in Section 4). The features used for the experiments reported here are inspired by previous work in text summarization on content selection (Kupiec et al., 1995), rhetorical classification (Teufel and Moens, 2002), and information ordering (Lapata, 2003). The features are extracted from the analyzed abstracts with specialized programs. In particular we use positional features (position of the predicate to be generated in the structure), length features (number of words in</context>
</contexts>
<marker>Witten, Frank, 1999</marker>
<rawString>Ian H. Witten and Eibe Frank. 1999. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann, October.</rawString>
</citation>
<citation valid="false">
<title>Provides summaries of African art; Allen Memorial Art Museum of Oberlin College; Art crimes; Asian arts; Da Vinci, Leonardo; Gallery Walk; and Native American Art Gallery.</title>
<marker></marker>
<rawString>Provides summaries of African art; Allen Memorial Art Museum of Oberlin College; Art crimes; Asian arts; Da Vinci, Leonardo; Gallery Walk; and Native American Art Gallery.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>