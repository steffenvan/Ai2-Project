<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.113816">
<title confidence="0.989462">
Improved Modeling of Out-Of-Vocabulary Words Using Morphological
Classes
</title>
<author confidence="0.996352">
Thomas M¨uller and Hinrich Sch¨utze
</author>
<affiliation confidence="0.998278">
Institute for Natural Language Processing
University of Stuttgart, Germany
</affiliation>
<email confidence="0.994833">
muellets@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.994939" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.921274555555555">
We present a class-based language model that
clusters rare words of similar morphology
together. The model improves the predic-
tion of words after histories containing out-
of-vocabulary words. The morphological fea-
tures used are obtained without the use of la-
beled data. The perplexity improvement com-
pared to a state of the art Kneser-Ney model is
4% overall and 81% on unknown histories.
</bodyText>
<sectionHeader confidence="0.998555" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997256">
One of the challenges in statistical language mod-
eling are words that appear in the recognition task
at hand, but not in the training set, so called out-
of-vocabulary (OOV) words. Especially for produc-
tive language it is often necessary to at least reduce
the number of OOVs. We present a novel approach
based on morphological classes to handling OOV
words in language modeling for English. Previous
work on morphological classes in English has not
been able to show noticeable improvements in per-
plexity. In this article class-based language mod-
els as proposed by Brown et al. (1992) are used to
tackle the problem. Our model improves perplex-
ity of a Kneser-Ney (KN) model for English by 4%,
the largest improvement of a state-of-the-art model
for English due to morphological modeling that we
are aware of. A class-based language model groups
words into classes and replaces the word transition
probability by a class transition probability and a
word emission probability:
</bodyText>
<equation confidence="0.999344">
P(w3|w1w2) = P(c3|c1c2) · P(w3|c3). (1)
</equation>
<bodyText confidence="0.999892181818182">
Brown et al. and many other authors primarily use
context information for clustering. Niesler et al.
(1998) showed that context clustering works better
than clusters based on part-of-speech tags. How-
ever, since the context of an OOV word is unknown
and it therefore cannot be assigned to a cluster, OOV
words are as much a problem to a context-based
class model as to a word model. That is why we
use non-distributional features – features like mor-
phological suffixes that only depend on the shape of
the word itself – to design a new class-based model
that can naturally integrate unknown words.
In related work, factored language models
(Bilmes and Kirchhoff, 2003) were proposed to
make use of morphological information in highly
inflecting languages such as Finnish (Creutz et al.,
2007), Turkish (Creutz et al., 2007; Yuret and Bic¸ici,
2009) and Arabic (Creutz et al., 2007; Vergyri et
al., 2004) or compounding languages like German
(Berton et al., 1996). The main idea is to replace
words by sequences of factors or features and to
apply statistical language modeling to the resulting
factor sequences. If, for example, words were seg-
mented into morphemes, an unknown word would
be split into an unseen sequence, which could be rec-
ognized using discounting techniques. However, if
one morpheme, e.g. the stem, is unknown to the sys-
tem, the fundamental problem remains unsolved.
Our class-based model uses a number of features
that have not been used in factored models (e.g.,
shape and length features) and achieves – in con-
trast to factored models – good perplexity gains for
English.
</bodyText>
<page confidence="0.973107">
524
</page>
<note confidence="0.7002005">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 524–528,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<equation confidence="0.8044733125">
is capital(w)
is all capital(w)
capital character(w)
appears in lowercase(w)
first character of w is an uppercase letter
V c E w : c is an uppercase letter
I c E w : c is an uppercase letter
capital character(w) V w′ E ET
special character(w)
digit(w)
is number(w)
not special(w)
I c E w : c is not a letter or digit
I c E w : c is a digit
w E L([+ − ǫ][0 − 9] (([., ][0 − 9])I[0 − 9]) *)
(special character(w) V digit(w) V is number(w))
</equation>
<tableCaption confidence="0.977259">
Table 1: Predicates of the capitalization and special character groups. ET is the vocabulary of the training corpus T,
w′ is obtained from w by changing all uppercase letters to lowercase and L(expr) is the language generated by the
regular expression expr.
</tableCaption>
<sectionHeader confidence="0.975051" genericHeader="method">
2 Morphological Features
</sectionHeader>
<bodyText confidence="0.999982767857143">
The feature vector of a word consists of four parts
that represent information about suffixes, capitaliza-
tion, special characters and word length. For the
suffix group, we define a binary feature for each
of the 100 most frequent suffixes learned on the
training corpus by the Reports algorithm (Keshava,
2006), a general purpose unsupervised morphology
learning algorithm. One additional binary feature is
used for all other suffixes learned by Reports, in-
cluding the empty suffix.
The feature groups capitalization and special
characters are motivated by the analysis shown in
Table 2. Our goal is to improve OOV modeling.
The table shows that most OOV words (f = 0) are
numbers (CD), names (NP), and nouns and adjec-
tives (NN, NNS, JJ). This distribution is similar to
hapax legomena (f = 1), but different from the POS
distribution of all tokens. Capitalization and special
character features are of obvious utility in identify-
ing the POS classes NP and CD since names in En-
glish are usually capitalized and numbers are writ-
ten with digits and special characters such as comma
and period. To capture these “shape” properties of a
word, we define the features listed in Table 1.
The fourth feature group is length. Short words
often have unusual distributional properties. Exam-
ples are abbreviations and bond credit ratings like
Aaa. To represent this information in the length
part of the vector, we define four binary features for
lengths 1, 2, 3 and greater than 3. The four parts
of the vector (suffixes, capitalization, special char-
acters, length) are weighted equally by normalizing
the subvector of each subgroup to unit length.
We designed the four feature groups to group
word types to either resemble POS classes or to in-
duce an even finer sub-partitioning. Unsupervised
POS clustering is a hard task in English and it is vir-
tually impossible if a word’s context (which is not
available for OOV items) is not taken into account.
For example, there is no way we can learn that “the”
and “a” are similar or that “child” has the same re-
lationship to “children” as “kid” does to “kids”. But
as our analysis in Table 2 shows, part of the benefit
of morphological analysis for OOVs comes from an
appropriate treatment of names and numbers. The
suffix feature group is useful for categorizing OOV
nouns and adjectives because there are very few ir-
regular morphemes like “ren” in children in English
and OOV words are likely to be regular words.
So even though morphological learning based on
the limited information we use is not possible in gen-
eral, it can be partially solved for the special case of
OOV words. Our experimental results in Section 5
confirm that this is the case. We also testes prefixes
and features based on word stems. However, they
produced inferior clustering solutions.
</bodyText>
<sectionHeader confidence="0.997802" genericHeader="method">
3 The Language Model
</sectionHeader>
<bodyText confidence="0.9998243">
As mentioned before in the literature, e.g. by Mal-
tese and Mancini (1992), class-based models only
outperform word models in cases of insufficient
data. That is why we use a frequency-based ap-
proach and only include words below a certain to-
ken frequency threshold θ in the clustering process.
A second motivation is that the contexts of low fre-
quency words are more similar to the expected con-
texts of OOV words.
Given a training corpus, all words with a fre-
</bodyText>
<page confidence="0.985813">
525
</page>
<table confidence="0.999867">
tag f = 1 types tokens
f = 0 (OOV)
CD 0.39 0.38 0.05
NP 0.35 0.35 0.14
NN 0.10 0.10 0.17
NNS 0.05 0.06 0.07
JJ 0.05 0.06 0.07
V* 0.04 0.05 0.15
E 0.98 0.99 0.66
</table>
<tableCaption confidence="0.96236">
Table 2: Proportion of dominant POS for types with train-
ing set frequencies f E 10, 11 and for tokens. V* consists
of all verb POS tags.
</tableCaption>
<bodyText confidence="0.999772">
quency below the threshold 0 are partitioned into
k clusters using the bisecting k-means algorithm
(Steinbach et al., 2000). The cluster of an OOV
word w can be defined as the cluster whose centroid
is closest to the feature vector of w. The formerly
removed high-frequency words are added as single-
ton clusters to produce a complete clustering. How-
ever, OOV words can only be assigned to the orig-
inal k-means clusters. Over this clustering a class-
based trigram model can be defined, as introduced
by Brown et al. (1992). The word transition proba-
bility of such a model is given by equation 1, where
cz denotes the cluster of the word wz. The class
transition probability P(c3|c1c2) is estimated using
the unsmoothed maximum likelihood estimate. The
emission probability is defined as follows:
</bodyText>
<equation confidence="0.9924774">
P(w3|c3) = { 1 if c(w3) &gt; 0
(1 − E) c(w3)
if 0&gt;c(w3)&gt;0
L�wEc3 c(w)
E if c(w3) = 0
</equation>
<bodyText confidence="0.997656666666667">
where c(w) is the frequency of w in the training set.
E is estimated on held-out data. The morphologi-
cal language model is then interpolated with a modi-
fied Kneser-Ney trigram model. In this interpolation
the parameters A depend on the cluster c2 of the his-
tory word w2, i.e.:
</bodyText>
<equation confidence="0.9993265">
P(w3|w1w2) = A(c2) - PM(w3|w1w2)
+ (1 − A(c2)) - PKN(w3|w1w2).
</equation>
<bodyText confidence="0.999567285714286">
This setup may cause overfitting as every high fre-
quent word w2 corresponds to a singleton class. A
grouping of several words into equivalence classes
could therefore further improve the model; this,
however, is beyond the scope of this article. We es-
timate optimal parameters A(c2) using the algorithm
described by Bahl et al. (1991).
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999156387096774">
We compare the performance of the described model
with a Kneser-Ney model and an interpolated model
based on part-of-speech (POS) tags. The relation be-
tween words and POS tags is many-to-many, but we
transform it to a many-to-one relation by labeling
every word – independent of its context – with its
most frequent tag. OOV words are treated equally
even though their POS classes would not be known
in a real application. Treetagger (Schmid, 1994) was
used to tag the entire corpus.
The experiments are carried out on a Wall Street
Journal (WSJ) corpus of 50 million words that is
split into training set (80%), valdev (5%), valtst
(5%), and test set (10%). The number of distinct fea-
ture vectors in training set, valdev and validation set
(valdev+valtst) are 632, 466, and 512, respectively.
As mentioned above, the training set is used to learn
suffixes and the maximum likelihood n-gram esti-
mates. The unknown word rate of the validation set
is E Pz� 0.028.
We use two setups to evaluate our methods. The
first uses valdev for parameter estimation and valtst
for testing and the second the entire validation set for
parameter estimation and the test set for testing. All
models with a threshold greater or equal to the fre-
quency of the most frequent word type are identical.
We use oc as the threshold to refer to these models.
In a similar manner, the cluster count oc denotes a
clustering where two words are in the same cluster
if and only if their features are identical. This is the
finest possible clustering of the feature vectors.
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999717">
Table 3 shows the results of our experiments. The
KN model yields a perplexity of 88.06 on valtst (top
row). For small frequency thresholds overfitting ef-
fects cause that the interpolated models are worse
than the KN model. We can see that a clustering
of the feature vectors is not necessary as the differ-
ences between all cluster models are small and c∞
is the overall best model. Surprisingly, morphologi-
cal clustering and POS classes are close even though
</bodyText>
<page confidence="0.995158">
526
</page>
<table confidence="0.999909181818182">
θ cPOS c1 c50 c100 c∞
0 88.06 88.06 88.06 88.06 88.06
1 89.74 89.84 89.73 89.74 89.74
5 89.07 89.36 89.07 89.06 89.07
10 88.59 89.01 88.58 88.57 88.58
50 86.72 87.58 86.69 86.68 86.68
102 85.92 87.06 85.92 85.91 85.89
103 84.43 86.88 84.83 84.77 84.56
104 85.22 87.59 85.89 85.73 85.26
105 86.82 87.99 87.44 87.32 86.79
oc 87.31 88.06 87.96 87.92 87.62
θ cPOS c1 c50 c100 c∞
0 813.50 813.50 813.50 813.50 813.50
1 181.25 206.17 182.78 183.62 184.43
5 152.51 185.54 154.52 152.98 153.83
10 147.48 186.12 149.34 147.98 147.48
50 146.21 203.10 142.21 140.67 140.46
102 149.06 215.54 143.95 142.48 141.67
103 173.91 279.02 164.22 159.04 150.13
104 239.72 349.54 221.42 208.85 180.57
105 317.13 373.98 318.04 297.18 236.90
oc 348.76 378.38 366.92 357.80 292.34
</table>
<tableCaption confidence="0.917403333333333">
Table 3: Perplexities for different frequency thresholds θ and cluster models. In the left table, perplexity is calculated
over all events P(w3|w1w2) of the valtst set. On the right side, the subset of events where w1 or w2 are unknown is
taken into account. The overall best results for class models and POS models are highlighted in bold.
</tableCaption>
<bodyText confidence="0.999930743589744">
the POS class model uses oracle information to as-
sign the right POS to an unknown word. The optimal
threshold is θ = 103 – the bolded perplexity values
84.43 and 84.56; that means that only 1.35% of the
word types were excluded from the morphological
clustering (86% of the tokens). The improvement
over the KN model is 4%.
In a second evaluation we reduce the perplexity
calculations to predictions of the form P(w3|w1w2)
where w1 or w2 are OOV words. On such an event
the KN model has to back off to a bigram or even
unigram estimate, which results in inferior predic-
tions and higher perplexity. The perplexity for the
KN model is 813.50 (top row). A first observation
is that the perplexity of model c1 starts at a good
value, but worsens with rising values for θ &gt; 10.
The reason is the dominance of proper nouns and
cardinal numbers at a frequency threshold of one and
in the distribution of OOV words (cf. Table 2). The
c1 model with θ = 1 is specialized for predicting
words after unknown nouns and cardinal numbers
and two thirds of the unknown words are of exactly
that type. However, with rising θ, other word classes
get a higher influence and different probability dis-
tributions are superimposed. The best morphologi-
cal model c∞ reduces the KN perplexity of 813.50
to 140.46 (bolded), an improvement of 83%.
As a final experiment, we evaluated our method
on the test set. In this case, we used the entire
validation set for parameter tuning (i.e., valdev and
valtst). The overall perplexity of the KN model is
88.28, the perplexities for the best POS and c∞ clus-
ter model for θ = 1000 are 84.59 and 84.71 respec-
tively, which corresponds again to an improvement
of 4%. For unknown histories the KN model per-
plexity is 767.25 and the POS and c∞ cluster model
perplexities at θ = 50 are 150.90 and 144.77. Thus,
the morphological model reduces perplexity by 81%
compared to the KN model.
</bodyText>
<sectionHeader confidence="0.999234" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999814">
We have presented a new class-based morphological
language model. In an experiment the model outper-
formed a modified Kneser-Ney model, especially in
the prediction of the continuations of histories con-
taining OOV words. The model is entirely unsuper-
vised, but works as well as a model using part-of-
speech information.
Future Work. We plan to use our model for do-
main adaptation in applications like machine trans-
lation. We then want to extend our model to other
languages, which could be more challenging, as cer-
tain languages have a more complex morphology
than English, but also worthwhile, if the unknown
word rate is higher. Preliminary experiments on
German and Finnish show promising results. The
model could be further improved by using contex-
tual information for the word clustering and training
a classifier based on morphological features to as-
sign OOV words to these clusters.
Acknowledgments. This research was funded by
DFG (grant SFB 732). We would like to thank Hel-
mut Schmid and the anonymous reviewers for their
valuable comments.
</bodyText>
<page confidence="0.995047">
527
</page>
<sectionHeader confidence="0.990044" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991216943396227">
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza,
Robert L. Mercer, and David Nahamoo. 1991. A fast
algorithm for deleted interpolation. In Speech Com-
munication and Technology, pages 1209–1212.
Andre Berton, Pablo Fetter, and Peter Regel-Brietzmann.
1996. Compound words in large-vocabulary German
speech recognition systems. In Spoken Language, vol-
ume 2, pages 1165 –1168 vol.2, October.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
Human Language Technology, NAACL ’03, pages 4–
6. Association for Computational Linguistics.
Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467–479, December.
Mathias Creutz, Teemu Hirsim¨aki, Mikko Kurimo, Antti
Puurula, Janne Pylkk¨onen, Vesa Siivola, Matti Var-
jokallio, Ebru Arisoy, Murat Sarac¸lar, and Andreas
Stolcke. 2007. Morph-based speech recognition
and modeling of out-of-vocabulary words across lan-
guages. ACM Transactions on Speech and Language
Processing, 5:3:1–3:29, December.
Samarth Keshava. 2006. A simpler, intuitive approach
to morpheme induction. In PASCAL Challenge Work-
shop on Unsupervised Segmentation of Words into
Morphemes, pages 31–35.
Giulio Maltese and Federico Mancini. 1992. An auto-
matic technique to include grammatical and morpho-
logical information in a trigram-based statistical lan-
guage model. In Acoustics, Speech, and Signal Pro-
cessing, volume 1, pages 157 –160 vol.1, March.
Thomas R. Niesler, Edward W.D. Whittaker, and
Philip C. Woodland. 1998. Comparison of part-of-
speech and automatically derived category-based lan-
guage models for speech recognition. In Acoustics,
Speech and Signal Processing, volume 1, pages 177
–180 vol.1, May.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In New Methods in Lan-
guage Processing, pages 44–49.
Michael Steinbach, George Karypis, and Vipin Kumar.
2000. A comparison of document clustering tech-
niques. In KDD Workshop on Text Mining.
Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and An-
dreas Stolcke. 2004. Morphology-based language
modeling for Arabic speech recognition. In Spoken
Language Processing, pages 2245–2248.
Deniz Yuret and Ergun Bic¸ici. 2009. Modeling morpho-
logically rich languages using split words and unstruc-
tured dependencies. In International Joint Conference
on Natural Language Processing, pages 345–348. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.996309">
528
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985236">
<title confidence="0.9992475">Improved Modeling of Out-Of-Vocabulary Words Using Morphological Classes</title>
<author confidence="0.994681">Thomas M¨uller</author>
<author confidence="0.994681">Hinrich</author>
<affiliation confidence="0.999543">Institute for Natural Language University of Stuttgart,</affiliation>
<email confidence="0.99739">muellets@ims.uni-stuttgart.de</email>
<abstract confidence="0.9993479">We present a class-based language model that clusters rare words of similar morphology together. The model improves the prediction of words after histories containing outof-vocabulary words. The morphological features used are obtained without the use of labeled data. The perplexity improvement compared to a state of the art Kneser-Ney model is and unknown histories.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Peter F Brown</author>
<author>Peter V de Souza</author>
<author>Robert L Mercer</author>
<author>David Nahamoo</author>
</authors>
<title>A fast algorithm for deleted interpolation.</title>
<date>1991</date>
<booktitle>In Speech Communication and Technology,</booktitle>
<pages>1209--1212</pages>
<marker>Bahl, Brown, de Souza, Mercer, Nahamoo, 1991</marker>
<rawString>Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, Robert L. Mercer, and David Nahamoo. 1991. A fast algorithm for deleted interpolation. In Speech Communication and Technology, pages 1209–1212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Berton</author>
<author>Pablo Fetter</author>
<author>Peter Regel-Brietzmann</author>
</authors>
<title>Compound words in large-vocabulary German speech recognition systems.</title>
<date>1996</date>
<journal>In Spoken Language,</journal>
<volume>2</volume>
<pages>1165--1168</pages>
<contexts>
<context position="2602" citStr="Berton et al., 1996" startWordPosition="410" endWordPosition="413">xt-based class model as to a word model. That is why we use non-distributional features – features like morphological suffixes that only depend on the shape of the word itself – to design a new class-based model that can naturally integrate unknown words. In related work, factored language models (Bilmes and Kirchhoff, 2003) were proposed to make use of morphological information in highly inflecting languages such as Finnish (Creutz et al., 2007), Turkish (Creutz et al., 2007; Yuret and Bic¸ici, 2009) and Arabic (Creutz et al., 2007; Vergyri et al., 2004) or compounding languages like German (Berton et al., 1996). The main idea is to replace words by sequences of factors or features and to apply statistical language modeling to the resulting factor sequences. If, for example, words were segmented into morphemes, an unknown word would be split into an unseen sequence, which could be recognized using discounting techniques. However, if one morpheme, e.g. the stem, is unknown to the system, the fundamental problem remains unsolved. Our class-based model uses a number of features that have not been used in factored models (e.g., shape and length features) and achieves – in contrast to factored models – go</context>
</contexts>
<marker>Berton, Fetter, Regel-Brietzmann, 1996</marker>
<rawString>Andre Berton, Pablo Fetter, and Peter Regel-Brietzmann. 1996. Compound words in large-vocabulary German speech recognition systems. In Spoken Language, volume 2, pages 1165 –1168 vol.2, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff A Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In Human Language Technology, NAACL ’03,</booktitle>
<pages>pages</pages>
<contexts>
<context position="2308" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="363" endWordPosition="366">arily use context information for clustering. Niesler et al. (1998) showed that context clustering works better than clusters based on part-of-speech tags. However, since the context of an OOV word is unknown and it therefore cannot be assigned to a cluster, OOV words are as much a problem to a context-based class model as to a word model. That is why we use non-distributional features – features like morphological suffixes that only depend on the shape of the word itself – to design a new class-based model that can naturally integrate unknown words. In related work, factored language models (Bilmes and Kirchhoff, 2003) were proposed to make use of morphological information in highly inflecting languages such as Finnish (Creutz et al., 2007), Turkish (Creutz et al., 2007; Yuret and Bic¸ici, 2009) and Arabic (Creutz et al., 2007; Vergyri et al., 2004) or compounding languages like German (Berton et al., 1996). The main idea is to replace words by sequences of factors or features and to apply statistical language modeling to the resulting factor sequences. If, for example, words were segmented into morphemes, an unknown word would be split into an unseen sequence, which could be recognized using discounting te</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored language models and generalized parallel backoff. In Human Language Technology, NAACL ’03, pages 4– 6. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V de Souza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<marker>Brown, de Souza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18:467–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
</authors>
<title>Teemu Hirsim¨aki, Mikko Kurimo, Antti Puurula, Janne Pylkk¨onen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Sarac¸lar, and Andreas Stolcke.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<pages>5--3</pages>
<marker>Creutz, 2007</marker>
<rawString>Mathias Creutz, Teemu Hirsim¨aki, Mikko Kurimo, Antti Puurula, Janne Pylkk¨onen, Vesa Siivola, Matti Varjokallio, Ebru Arisoy, Murat Sarac¸lar, and Andreas Stolcke. 2007. Morph-based speech recognition and modeling of out-of-vocabulary words across languages. ACM Transactions on Speech and Language Processing, 5:3:1–3:29, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samarth Keshava</author>
</authors>
<title>A simpler, intuitive approach to morpheme induction.</title>
<date>2006</date>
<booktitle>In PASCAL Challenge Workshop on Unsupervised Segmentation of Words into Morphemes,</booktitle>
<pages>31--35</pages>
<contexts>
<context position="4471" citStr="Keshava, 2006" startWordPosition="730" endWordPosition="731">t(w) V is number(w)) Table 1: Predicates of the capitalization and special character groups. ET is the vocabulary of the training corpus T, w′ is obtained from w by changing all uppercase letters to lowercase and L(expr) is the language generated by the regular expression expr. 2 Morphological Features The feature vector of a word consists of four parts that represent information about suffixes, capitalization, special characters and word length. For the suffix group, we define a binary feature for each of the 100 most frequent suffixes learned on the training corpus by the Reports algorithm (Keshava, 2006), a general purpose unsupervised morphology learning algorithm. One additional binary feature is used for all other suffixes learned by Reports, including the empty suffix. The feature groups capitalization and special characters are motivated by the analysis shown in Table 2. Our goal is to improve OOV modeling. The table shows that most OOV words (f = 0) are numbers (CD), names (NP), and nouns and adjectives (NN, NNS, JJ). This distribution is similar to hapax legomena (f = 1), but different from the POS distribution of all tokens. Capitalization and special character features are of obvious</context>
</contexts>
<marker>Keshava, 2006</marker>
<rawString>Samarth Keshava. 2006. A simpler, intuitive approach to morpheme induction. In PASCAL Challenge Workshop on Unsupervised Segmentation of Words into Morphemes, pages 31–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giulio Maltese</author>
<author>Federico Mancini</author>
</authors>
<title>An automatic technique to include grammatical and morphological information in a trigram-based statistical language model.</title>
<date>1992</date>
<booktitle>In Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>157--160</pages>
<contexts>
<context position="7054" citStr="Maltese and Mancini (1992)" startWordPosition="1167" endWordPosition="1171">oup is useful for categorizing OOV nouns and adjectives because there are very few irregular morphemes like “ren” in children in English and OOV words are likely to be regular words. So even though morphological learning based on the limited information we use is not possible in general, it can be partially solved for the special case of OOV words. Our experimental results in Section 5 confirm that this is the case. We also testes prefixes and features based on word stems. However, they produced inferior clustering solutions. 3 The Language Model As mentioned before in the literature, e.g. by Maltese and Mancini (1992), class-based models only outperform word models in cases of insufficient data. That is why we use a frequency-based approach and only include words below a certain token frequency threshold θ in the clustering process. A second motivation is that the contexts of low frequency words are more similar to the expected contexts of OOV words. Given a training corpus, all words with a fre525 tag f = 1 types tokens f = 0 (OOV) CD 0.39 0.38 0.05 NP 0.35 0.35 0.14 NN 0.10 0.10 0.17 NNS 0.05 0.06 0.07 JJ 0.05 0.06 0.07 V* 0.04 0.05 0.15 E 0.98 0.99 0.66 Table 2: Proportion of dominant POS for types with</context>
</contexts>
<marker>Maltese, Mancini, 1992</marker>
<rawString>Giulio Maltese and Federico Mancini. 1992. An automatic technique to include grammatical and morphological information in a trigram-based statistical language model. In Acoustics, Speech, and Signal Processing, volume 1, pages 157 –160 vol.1, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas R Niesler</author>
<author>Edward W D Whittaker</author>
<author>Philip C Woodland</author>
</authors>
<title>Comparison of part-ofspeech and automatically derived category-based language models for speech recognition.</title>
<date>1998</date>
<booktitle>In Acoustics, Speech and Signal Processing,</booktitle>
<volume>1</volume>
<pages>177--180</pages>
<contexts>
<context position="1748" citStr="Niesler et al. (1998)" startWordPosition="268" endWordPosition="271">ty. In this article class-based language models as proposed by Brown et al. (1992) are used to tackle the problem. Our model improves perplexity of a Kneser-Ney (KN) model for English by 4%, the largest improvement of a state-of-the-art model for English due to morphological modeling that we are aware of. A class-based language model groups words into classes and replaces the word transition probability by a class transition probability and a word emission probability: P(w3|w1w2) = P(c3|c1c2) · P(w3|c3). (1) Brown et al. and many other authors primarily use context information for clustering. Niesler et al. (1998) showed that context clustering works better than clusters based on part-of-speech tags. However, since the context of an OOV word is unknown and it therefore cannot be assigned to a cluster, OOV words are as much a problem to a context-based class model as to a word model. That is why we use non-distributional features – features like morphological suffixes that only depend on the shape of the word itself – to design a new class-based model that can naturally integrate unknown words. In related work, factored language models (Bilmes and Kirchhoff, 2003) were proposed to make use of morphologi</context>
</contexts>
<marker>Niesler, Whittaker, Woodland, 1998</marker>
<rawString>Thomas R. Niesler, Edward W.D. Whittaker, and Philip C. Woodland. 1998. Comparison of part-ofspeech and automatically derived category-based language models for speech recognition. In Acoustics, Speech and Signal Processing, volume 1, pages 177 –180 vol.1, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<contexts>
<context position="9762" citStr="Schmid, 1994" startWordPosition="1652" endWordPosition="1653">; this, however, is beyond the scope of this article. We estimate optimal parameters A(c2) using the algorithm described by Bahl et al. (1991). 4 Experimental Setup We compare the performance of the described model with a Kneser-Ney model and an interpolated model based on part-of-speech (POS) tags. The relation between words and POS tags is many-to-many, but we transform it to a many-to-one relation by labeling every word – independent of its context – with its most frequent tag. OOV words are treated equally even though their POS classes would not be known in a real application. Treetagger (Schmid, 1994) was used to tag the entire corpus. The experiments are carried out on a Wall Street Journal (WSJ) corpus of 50 million words that is split into training set (80%), valdev (5%), valtst (5%), and test set (10%). The number of distinct feature vectors in training set, valdev and validation set (valdev+valtst) are 632, 466, and 512, respectively. As mentioned above, the training set is used to learn suffixes and the maximum likelihood n-gram estimates. The unknown word rate of the validation set is E Pz� 0.028. We use two setups to evaluate our methods. The first uses valdev for parameter estimat</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In New Methods in Language Processing, pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Steinbach</author>
<author>George Karypis</author>
<author>Vipin Kumar</author>
</authors>
<title>A comparison of document clustering techniques.</title>
<date>2000</date>
<booktitle>In KDD Workshop on Text Mining.</booktitle>
<contexts>
<context position="7864" citStr="Steinbach et al., 2000" startWordPosition="1321" endWordPosition="1324">old θ in the clustering process. A second motivation is that the contexts of low frequency words are more similar to the expected contexts of OOV words. Given a training corpus, all words with a fre525 tag f = 1 types tokens f = 0 (OOV) CD 0.39 0.38 0.05 NP 0.35 0.35 0.14 NN 0.10 0.10 0.17 NNS 0.05 0.06 0.07 JJ 0.05 0.06 0.07 V* 0.04 0.05 0.15 E 0.98 0.99 0.66 Table 2: Proportion of dominant POS for types with training set frequencies f E 10, 11 and for tokens. V* consists of all verb POS tags. quency below the threshold 0 are partitioned into k clusters using the bisecting k-means algorithm (Steinbach et al., 2000). The cluster of an OOV word w can be defined as the cluster whose centroid is closest to the feature vector of w. The formerly removed high-frequency words are added as singleton clusters to produce a complete clustering. However, OOV words can only be assigned to the original k-means clusters. Over this clustering a classbased trigram model can be defined, as introduced by Brown et al. (1992). The word transition probability of such a model is given by equation 1, where cz denotes the cluster of the word wz. The class transition probability P(c3|c1c2) is estimated using the unsmoothed maximu</context>
</contexts>
<marker>Steinbach, Karypis, Kumar, 2000</marker>
<rawString>Michael Steinbach, George Karypis, and Vipin Kumar. 2000. A comparison of document clustering techniques. In KDD Workshop on Text Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitra Vergyri</author>
<author>Katrin Kirchhoff</author>
<author>Kevin Duh</author>
<author>Andreas Stolcke</author>
</authors>
<title>Morphology-based language modeling for Arabic speech recognition.</title>
<date>2004</date>
<booktitle>In Spoken Language Processing,</booktitle>
<pages>2245--2248</pages>
<contexts>
<context position="2543" citStr="Vergyri et al., 2004" startWordPosition="401" endWordPosition="404">ned to a cluster, OOV words are as much a problem to a context-based class model as to a word model. That is why we use non-distributional features – features like morphological suffixes that only depend on the shape of the word itself – to design a new class-based model that can naturally integrate unknown words. In related work, factored language models (Bilmes and Kirchhoff, 2003) were proposed to make use of morphological information in highly inflecting languages such as Finnish (Creutz et al., 2007), Turkish (Creutz et al., 2007; Yuret and Bic¸ici, 2009) and Arabic (Creutz et al., 2007; Vergyri et al., 2004) or compounding languages like German (Berton et al., 1996). The main idea is to replace words by sequences of factors or features and to apply statistical language modeling to the resulting factor sequences. If, for example, words were segmented into morphemes, an unknown word would be split into an unseen sequence, which could be recognized using discounting techniques. However, if one morpheme, e.g. the stem, is unknown to the system, the fundamental problem remains unsolved. Our class-based model uses a number of features that have not been used in factored models (e.g., shape and length f</context>
</contexts>
<marker>Vergyri, Kirchhoff, Duh, Stolcke, 2004</marker>
<rawString>Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and Andreas Stolcke. 2004. Morphology-based language modeling for Arabic speech recognition. In Spoken Language Processing, pages 2245–2248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
<author>Ergun Bic¸ici</author>
</authors>
<title>Modeling morphologically rich languages using split words and unstructured dependencies.</title>
<date>2009</date>
<booktitle>In International Joint Conference on Natural Language Processing,</booktitle>
<pages>345--348</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Yuret, Bic¸ici, 2009</marker>
<rawString>Deniz Yuret and Ergun Bic¸ici. 2009. Modeling morphologically rich languages using split words and unstructured dependencies. In International Joint Conference on Natural Language Processing, pages 345–348. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>