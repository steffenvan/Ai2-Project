<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005615">
<title confidence="0.997409">
Linguistic Regularities in Continuous Space Word Representations
</title>
<author confidence="0.995215">
Tomas Mikolov* , Wen-tau Yih, Geoffrey Zweig
</author>
<affiliation confidence="0.933448">
Microsoft Research
</affiliation>
<address confidence="0.747859">
Redmond, WA 98052
</address>
<sectionHeader confidence="0.970411" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99970296">
Continuous space language models have re-
cently demonstrated outstanding results across
a variety of tasks. In this paper, we ex-
amine the vector-space word representations
that are implicitly learned by the input-layer
weights. We find that these representations
are surprisingly good at capturing syntactic
and semantic regularities in language, and
that each relationship is characterized by a
relation-specific vector offset. This allows
vector-oriented reasoning based on the offsets
between words. For example, the male/female
relationship is automatically learned, and with
the induced vector representations, “King -
Man + Woman” results in a vector very close
to “Queen.” We demonstrate that the word
vectors capture syntactic regularities by means
of syntactic analogy questions (provided with
this paper), and are able to correctly answer
almost 40% of the questions. We demonstrate
that the word vectors capture semantic regu-
larities by using the vector offset method to
answer SemEval-2012 Task 2 questions. Re-
markably, this method outperforms the best
previous systems.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999848">
A defining feature of neural network language mod-
els is their representation of words as high dimen-
sional real valued vectors. In these models (Ben-
gio et al., 2003; Schwenk, 2007; Mikolov et al.,
2010), words are converted via a learned lookup-
table into real valued vectors which are used as the
</bodyText>
<subsectionHeader confidence="0.188269">
*Currently at Google, Inc.
</subsectionHeader>
<bodyText confidence="0.99968655882353">
inputs to a neural network. As pointed out by the
original proposers, one of the main advantages of
these models is that the distributed representation
achieves a level of generalization that is not possi-
ble with classical n-gram language models; whereas
a n-gram model works in terms of discrete units that
have no inherent relationship to one another, a con-
tinuous space model works in terms of word vectors
where similar words are likely to have similar vec-
tors. Thus, when the model parameters are adjusted
in response to a particular word or word-sequence,
the improvements will carry over to occurrences of
similar words and sequences.
By training a neural network language model, one
obtains not just the model itself, but also the learned
word representations, which may be used for other,
potentially unrelated, tasks. This has been used to
good effect, for example in (Collobert and Weston,
2008; Turian et al., 2010) where induced word rep-
resentations are used with sophisticated classifiers to
improve performance in many NLP tasks.
In this work, we find that the learned word repre-
sentations in fact capture meaningful syntactic and
semantic regularities in a very simple way. Specif-
ically, the regularities are observed as constant vec-
tor offsets between pairs of words sharing a par-
ticular relationship. For example, if we denote the
vector for word i as xi, and focus on the singu-
lar/plural relation, we observe that x apple−xapples ≈
xcar−xcars, xfamily−xfamilies ≈ xcar−xcars, and
so on. Perhaps more surprisingly, we find that this
is also the case for a variety of semantic relations, as
measured by the SemEval 2012 task of measuring
relation similarity.
</bodyText>
<page confidence="0.977847">
746
</page>
<subsectionHeader confidence="0.2952">
Proceedings of NAACL-HLT 2013, pages 746–751,
</subsectionHeader>
<bodyText confidence="0.955498875">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
The remainder of this paper is organized as fol-
lows. In Section 2, we discuss related work; Section
3 describes the recurrent neural network language
model we used to obtain word vectors; Section 4 dis-
cusses the test sets; Section 5 describes our proposed
vector offset method; Section 6 summarizes our ex-
periments, and we conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999903583333333">
Distributed word representations have a long his-
tory, with early proposals including (Hinton, 1986;
Pollack, 1990; Elman, 1991; Deerwester et al.,
1990). More recently, neural network language
models have been proposed for the classical lan-
guage modeling task of predicting a probability dis-
tribution over the “next” word, given some preced-
ing words. These models were first studied in the
context of feed-forward networks (Bengio et al.,
2003; Bengio et al., 2006), and later in the con-
text of recurrent neural network models (Mikolov et
al., 2010; Mikolov et al., 2011b). This early work
demonstrated outstanding performance in terms of
word-prediction, but also the need for more compu-
tationally efficient models. This has been addressed
by subsequent work using hierarchical prediction
(Morin and Bengio, 2005; Mnih and Hinton, 2009;
Le et al., 2011; Mikolov et al., 2011b; Mikolov et
al., 2011a). Also of note, the use of distributed
topic representations has been studied in (Hinton
and Salakhutdinov, 2006; Hinton and Salakhutdi-
nov, 2010), and (Bordes et al., 2012) presents a se-
mantically driven method for obtaining word repre-
sentations.
</bodyText>
<sectionHeader confidence="0.968437" genericHeader="method">
3 Recurrent Neural Network Model
</sectionHeader>
<bodyText confidence="0.99995775">
The word representations we study are learned by a
recurrent neural network language model (Mikolov
et al., 2010), as illustrated in Figure 1. This architec-
ture consists of an input layer, a hidden layer with re-
current connections, plus the corresponding weight
matrices. The input vector w(t) represents input
word at time t encoded using 1-of-N coding, and the
output layer y(t) produces a probability distribution
over words. The hidden layer s(t) maintains a rep-
resentation of the sentence history. The input vector
w(t) and the output vector y(t) have dimensional-
ity of the vocabulary. The values in the hidden and
</bodyText>
<figureCaption confidence="0.997126">
Figure 1: Recurrent Neural Network Language Model.
</figureCaption>
<bodyText confidence="0.895835">
output layers are computed as follows:
</bodyText>
<equation confidence="0.9999895">
s(t) = f (Uw(t) + Ws(t−1)) (1)
y(t) = g (Vs(t)) , (2)
</equation>
<bodyText confidence="0.720127">
where
</bodyText>
<equation confidence="0.998125333333333">
1 e
z�
f (z) = 1 + e_z , g(zm) = ezk (3)
</equation>
<bodyText confidence="0.999991333333333">
In this framework, the word representations are
found in the columns of U, with each column rep-
resenting a word. The RNN is trained with back-
propagation to maximize the data log-likelihood un-
der the model. The model itself has no knowledge
of syntax or morphology or semantics. Remark-
ably, training such a purely lexical model to max-
imize likelihood will induce word representations
with striking syntactic and semantic properties.
</bodyText>
<sectionHeader confidence="0.926545" genericHeader="method">
4 Measuring Linguistic Regularity
</sectionHeader>
<subsectionHeader confidence="0.996789">
4.1 A Syntactic Test Set
</subsectionHeader>
<bodyText confidence="0.999975">
To understand better the syntactic regularities which
are inherent in the learned representation, we created
a test set of analogy questions of the form “a is to b
as c is to ” testing base/comparative/superlative
forms of adjectives; singular/plural forms of com-
mon nouns; possessive/non-possessive forms of
common nouns; and base, past and 3rd person
present tense forms of verbs. More precisely, we
tagged 267M words of newspaper text with Penn
</bodyText>
<page confidence="0.991797">
747
</page>
<table confidence="0.999459533333333">
Category Relation Patterns Tested # Questions Example
Adjectives Base/Comparative JJ/JJR, JJR/JJ 1000 good:better rough:
Adjectives Base/Superlative JJ/JJS, JJS/JJ 1000 good:best rough:
Adjectives Comparative/ JJS/JJR, JJR/JJS 1000 better:best rougher:
Superlative
Nouns Singular/Plural NN/NNS, 1000 year:years law:
NNS/NN
Nouns Non-possessive/ NN/NN POS, 1000 city:city’s bank:
Possessive NN POS/NN
Verbs Base/Past VB/VBD, 1000 see:saw return:
VBD/VB
Verbs Base/3rd Person VB/VBZ, VBZ/VB 1000 see:sees return:
Singular Present
Verbs Past/3rd Person VBD/VBZ, 1000 saw:sees returned:
Singular Present VBZ/VBD
</table>
<tableCaption confidence="0.995329">
Table 1: Test set patterns. For a given pattern and word-pair, both orderings occur in the test set. For example, if
“see:saw return: ” occurs, so will “saw:see returned: ”.
</tableCaption>
<bodyText confidence="0.998922909090909">
Treebank POS tags (Marcus et al., 1993). We then
selected 100 of the most frequent comparative adjec-
tives (words labeled JJR); 100 of the most frequent
plural nouns (NNS); 100 of the most frequent pos-
sessive nouns (NN POS); and 100 of the most fre-
quent base form verbs (VB). We then systematically
generated analogy questions by randomly matching
each of the 100 words with 5 other words from the
same category, and creating variants as indicated in
Table 1. The total test set size is 8000. The test set
is available online. 1
</bodyText>
<subsectionHeader confidence="0.987969">
4.2 A Semantic Test Set
</subsectionHeader>
<bodyText confidence="0.999338076923077">
In addition to syntactic analogy questions, we used
the SemEval-2012 Task 2, Measuring Relation Sim-
ilarity (Jurgens et al., 2012), to estimate the extent
to which RNNLM word vectors contain semantic
information. The dataset contains 79 fine-grained
word relations, where 10 are used for training and
69 testing. Each relation is exemplified by 3 or
4 gold word pairs. Given a group of word pairs
that supposedly have the same relation, the task is
to order the target pairs according to the degree to
which this relation holds. This can be viewed as an-
other analogy problem. For example, take the Class-
Inclusion:Singular Collective relation with the pro-
</bodyText>
<footnote confidence="0.7811285">
1http://research.microsoft.com/en-
us/projects/rnn/default.aspx
</footnote>
<bodyText confidence="0.9973035">
totypical word pair clothing:shirt. To measure the
degree that a target word pair dish:bowl has the same
relation, we form the analogy “clothing is to shirt as
dish is to bowl,” and ask how valid it is.
</bodyText>
<sectionHeader confidence="0.988159" genericHeader="method">
5 The Vector Offset Method
</sectionHeader>
<bodyText confidence="0.999579722222222">
As we have seen, both the syntactic and semantic
tasks have been formulated as analogy questions.
We have found that a simple vector offset method
based on cosine distance is remarkably effective in
solving these questions. In this method, we assume
relationships are present as vector offsets, so that in
the embedding space, all pairs of words sharing a
particular relation are related by the same constant
offset. This is illustrated in Figure 2.
In this model, to answer the analogy question a:b
c:d where d is unknown, we find the embedding
vectors xa, xb, xc (all normalized to unit norm), and
compute y = xb − xa + xc. y is the continuous
space representation of the word we expect to be the
best answer. Of course, no word might exist at that
exact position, so we then search for the word whose
embedding vector has the greatest cosine similarity
to y and output it:
</bodyText>
<equation confidence="0.9490606">
xwy
w� = argmaxw
When d is given, as in our semantic test set, we
simply use cos(xb − xa + xc, xd) for the words
kxwkkyk
</equation>
<page confidence="0.991731">
748
</page>
<figureCaption confidence="0.9971302">
Figure 2: Left panel shows vector offsets for three word
pairs illustrating the gender relation. Right panel shows
a different projection, and the singular/plural relation for
two words. In high-dimensional space, multiple relations
can be embedded for a single word.
</figureCaption>
<bodyText confidence="0.985463714285714">
provided. We have explored several related meth-
ods and found that the proposed method performs
well for both syntactic and semantic relations. We
note that this measure is qualitatively similar to rela-
tional similarity model of (Turney, 2012), which pre-
dicts similarity between members of the word pairs
(xb, xd), (x,-, xd) and dis-similarity for (xa, xd).
</bodyText>
<sectionHeader confidence="0.99724" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999980695652174">
To evaluate the vector offset method, we used
vectors generated by the RNN toolkit of Mikolov
(2012). Vectors of dimensionality 80, 320, and 640
were generated, along with a composite of several
systems, with total dimensionality 1600. The sys-
tems were trained with 320M words of Broadcast
News data as described in (Mikolov et al., 2011a),
and had an 82k vocabulary. Table 2 shows results
for both RNNLM and LSA vectors on the syntactic
task. LSA was trained on the same data as the RNN.
We see that the RNN vectors capture significantly
more syntactic regularity than the LSA vectors, and
do remarkably well in an absolute sense, answering
more than one in three questions correctly. 2
In Table 3 we compare the RNN vectors with
those based on the methods of Collobert and We-
ston (2008) and Mnih and Hinton (2009), as imple-
mented by (Turian et al., 2010) and available online
3 Since different words are present in these datasets,
we computed the intersection of the vocabularies of
the RNN vectors and the new vectors, and restricted
the test set and word vectors to those. This resulted
in a 36k word vocabulary, and a test set with 6632
</bodyText>
<footnote confidence="0.9997355">
2Guessing gets a small fraction of a percent.
3http://metaoptimize.com/projects/wordreprs/
</footnote>
<table confidence="0.99912125">
Method Adjectives Nouns Verbs All
LSA-80 9.2 11.1 17.4 12.8
LSA-320 11.3 18.1 20.7 16.5
LSA-640 9.6 10.1 13.8 11.3
RNN-80 9.3 5.2 30.4 16.2
RNN-320 18.2 19.0 45.0 28.5
RNN-640 21.0 25.2 54.8 34.7
RNN-1600 23.9 29.2 62.2 39.6
</table>
<tableCaption confidence="0.7963105">
Table 2: Results for identifying syntactic regularities for
different word representations. Percent correct.
</tableCaption>
<table confidence="0.999586666666667">
Method Adjectives Nouns Verbs All
RNN-80 10.1 8.1 30.4 19.0
CW-50 1.1 2.4 8.1 4.5
CW-100 1.3 4.1 8.6 5.0
HLBL-50 4.4 5.4 23.1 13.0
HLBL-100 7.6 13.2 30.2 18.7
</table>
<tableCaption confidence="0.961677666666667">
Table 3: Comparison of RNN vectors with Turian’s Col-
lobert and Weston based vectors and the Hierarchical
Log-Bilinear model of Mnih and Hinton. Percent correct.
</tableCaption>
<bodyText confidence="0.9998545">
questions. Turian’s Collobert and Weston based vec-
tors do poorly on this task, whereas the Hierarchical
Log-Bilinear Model vectors of (Mnih and Hinton,
2009) do essentially as well as the RNN vectors.
These representations were trained on 37M words
of data and this may indicate a greater robustness of
the HLBL method.
We conducted similar experiments with the se-
mantic test set. For each target word pair in a rela-
tion category, the model measures its relational sim-
ilarity to each of the prototypical word pairs, and
then uses the average as the final score. The results
are evaluated using the two standard metrics defined
in the task, Spearman’s rank correlation coefficient
p and MaxDiff accuracy. In both cases, larger val-
ues are better. To compare to previous systems, we
report the average over all 69 relations in the test set.
From Table 4, we see that as with the syntac-
tic regularity study, the RNN-based representations
perform best. In this case, however, Turian’s CW
vectors are comparable in performance to the HLBL
vectors. With the RNN vectors, the performance im-
proves as the number of dimensions increases. Sur-
prisingly, we found that even though the RNN vec-
</bodyText>
<page confidence="0.995396">
749
</page>
<table confidence="0.999142636363636">
Method Spearman’s p MaxDiff Acc.
LSA-640 0.149 0.364
RNN-80 0.211 0.389
RNN-320 0.259 0.408
RNN-640 0.270 0.416
RNN-1600 0.275 0.418
CW-50 0.159 0.363
CW-100 0.154 0.363
HLBL-50 0.149 0.363
HLBL-100 0.146 0.362
UTD-NB 0.230 0.395
</table>
<tableCaption confidence="0.999963">
Table 4: Results in measuring relation similarity
</tableCaption>
<bodyText confidence="0.99628575">
tors are not trained or tuned specifically for this task,
the model achieves better results (RNN-320, RNN-
640 &amp; RNN-1600) than the previously best perform-
ing system, UTD-NB (Rink and Harabagiu, 2012).
</bodyText>
<sectionHeader confidence="0.998374" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999980153846154">
We have presented a generally applicable vector off-
set method for identifying linguistic regularities in
continuous space word representations. We have
shown that the word representations learned by a
RNNLM do an especially good job in capturing
these regularities. We present a new dataset for mea-
suring syntactic performance, and achieve almost
40% correct. We also evaluate semantic general-
ization on the SemEval 2012 task, and outperform
the previous state-of-the-art. Surprisingly, both re-
sults are the byproducts of an unsupervised maxi-
mum likelihood training criterion that simply oper-
ates on a large amount of text data.
</bodyText>
<sectionHeader confidence="0.998476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999523765625">
Y. Bengio, R. Ducharme, Vincent, P., and C. Jauvin.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Reseach, 3(6).
Y. Bengio, H. Schwenk, J.S. Sen´ecal, F. Morin, and J.L.
Gauvain. 2006. Neural probabilistic language models.
Innovations in Machine Learning, pages 137–186.
A. Bordes, X. Glorot, J. Weston, and Y. Bengio. 2012.
Joint learning of words and meaning representations
for open-text semantic parsing. In Proceedings of 15th
International Conference on Artificial Intelligence and
Statistics.
R. Collobert and J. Weston. 2008. A unified architecture
for natural language processing: Deep neural networks
with multitask learning. In Proceedings of the 25th
international conference on Machine learning, pages
160–167. ACM.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society forInforma-
tion Science, 41(96).
J.L. Elman. 1991. Distributed representations, simple re-
current networks, and grammatical structure. Machine
learning, 7(2):195–225.
G.E. Hinton and R.R. Salakhutdinov. 2006. Reducing
the dimensionality of data with neural networks. Sci-
ence, 313(5786):504–507.
G. Hinton and R. Salakhutdinov. 2010. Discovering bi-
nary codes for documents by learning deep generative
models. Topics in Cognitive Science, 3(1):74–91.
G.E. Hinton. 1986. Learning distributed representations
of concepts. In Proceedings of the eighth annual con-
ference of the cognitive science society, pages 1–12.
Amherst, MA.
David Jurgens, Saif Mohammad, Peter Turney, and Keith
Holyoak. 2012. Semeval-2012 task 2: Measuring de-
grees of relational similarity. In *SEM 2012: The First
Joint Conference on Lexical and Computational Se-
mantics (SemEval 2012), pages 356–364. Association
for Computational Linguistics.
Hai-Son Le, I. Oparin, A. Allauzen, J.-L. Gauvain, and
F. Yvon. 2011. Structured output layer neural network
language model. In Proceedings of ICASSP 2011.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the penn treebank. Computational Lin-
guistics, 19(2):313–330.
Tomas Mikolov, Martin Karafiat, Jan Cernocky, and San-
jeev Khudanpur. 2010. Recurrent neural network
based language model. In Proceedings of Interspeech
2010.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011a. Strategies for
Training Large Scale Neural Network Language Mod-
els. In Proceedings of ASRU 2011.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky, and Sanjeev Khudanpur. 2011b. Ex-
tensions of recurrent neural network based language
model. In Proceedings of ICASSP 2011.
Tomas Mikolov. 2012. RNN toolkit.
A. Mnih and G.E. Hinton. 2009. A scalable hierarchical
distributed language model. Advances in neural infor-
mation processing systems, 21:1081–1088.
F. Morin and Y. Bengio. 2005. Hierarchical probabilistic
neural network language model. In Proceedings of the
</reference>
<page confidence="0.969047">
750
</page>
<reference confidence="0.998436578947368">
international workshop on artificial intelligence and
statistics, pages 246–252.
J.B. Pollack. 1990. Recursive distributed representa-
tions. Artificial Intelligence, 46(1):77–105.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
*SEM 2012: The First Joint Conference on Lexical
and Computational Semantics (SemEval 2012), pages
413–418. Association for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492
– 518.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings ofAssociation for
Computational Linguistics (ACL 2010).
P.D. Turney. 2012. Domain and function: A dual-space
model of semantic relations and compositions. Jour-
nal of Artificial Intelligence Research, 44:533–585.
</reference>
<page confidence="0.998294">
751
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.884459">
<title confidence="0.99753">Linguistic Regularities in Continuous Space Word Representations</title>
<author confidence="0.936225">Wen-tau Yih</author>
<author confidence="0.936225">Geoffrey</author>
<affiliation confidence="0.971338">Microsoft</affiliation>
<address confidence="0.999802">Redmond, WA 98052</address>
<abstract confidence="0.998364423076923">Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Reseach,</journal>
<volume>3</volume>
<issue>6</issue>
<contexts>
<context position="1416" citStr="Bengio et al., 2003" startWordPosition="204" endWordPosition="208">ts in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems. 1 Introduction A defining feature of neural network language models is their representation of words as high dimensional real valued vectors. In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the *Currently at Google, Inc. inputs to a neural network. As pointed out by the original proposers, one of the main advantages of these models is that the distributed representation achieves a level of generalization that is not possible with classical n-gram language models; whereas a n-gram model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely</context>
<context position="4189" citStr="Bengio et al., 2003" startWordPosition="653" endWordPosition="656">s; Section 4 discusses the test sets; Section 5 describes our proposed vector offset method; Section 6 summarizes our experiments, and we conclude in Section 7. 2 Related Work Distributed word representations have a long history, with early proposals including (Hinton, 1986; Pollack, 1990; Elman, 1991; Deerwester et al., 1990). More recently, neural network language models have been proposed for the classical language modeling task of predicting a probability distribution over the “next” word, given some preceding words. These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models. This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a). Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 201</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Y. Bengio, R. Ducharme, Vincent, P., and C. Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Reseach, 3(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>H Schwenk</author>
<author>J S Sen´ecal</author>
<author>F Morin</author>
<author>J L Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, 2006</marker>
<rawString>Y. Bengio, H. Schwenk, J.S. Sen´ecal, F. Morin, and J.L. Gauvain. 2006. Neural probabilistic language models. Innovations in Machine Learning, pages 137–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bordes</author>
<author>X Glorot</author>
<author>J Weston</author>
<author>Y Bengio</author>
</authors>
<title>Joint learning of words and meaning representations for open-text semantic parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of 15th International Conference on Artificial Intelligence and Statistics.</booktitle>
<contexts>
<context position="4818" citStr="Bordes et al., 2012" startWordPosition="753" endWordPosition="756">et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models. This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a). Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations. 3 Recurrent Neural Network Model The word representations we study are learned by a recurrent neural network language model (Mikolov et al., 2010), as illustrated in Figure 1. This architecture consists of an input layer, a hidden layer with recurrent connections, plus the corresponding weight matrices. The input vector w(t) represents input word at time t encoded using 1-of-N coding, and the output layer y(t) produces a probability distribution over words. The hidden layer s(t) maintains a representation of the sentenc</context>
</contexts>
<marker>Bordes, Glorot, Weston, Bengio, 2012</marker>
<rawString>A. Bordes, X. Glorot, J. Weston, and Y. Bengio. 2012. Joint learning of words and meaning representations for open-text semantic parsing. In Proceedings of 15th International Conference on Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2481" citStr="Collobert and Weston, 2008" startWordPosition="381" endWordPosition="384">terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectors. Thus, when the model parameters are adjusted in response to a particular word or word-sequence, the improvements will carry over to occurrences of similar words and sequences. By training a neural network language model, one obtains not just the model itself, but also the learned word representations, which may be used for other, potentially unrelated, tasks. This has been used to good effect, for example in (Collobert and Weston, 2008; Turian et al., 2010) where induced word representations are used with sophisticated classifiers to improve performance in many NLP tasks. In this work, we find that the learned word representations in fact capture meaningful syntactic and semantic regularities in a very simple way. Specifically, the regularities are observed as constant vector offsets between pairs of words sharing a particular relationship. For example, if we denote the vector for word i as xi, and focus on the singular/plural relation, we observe that x apple−xapples ≈ xcar−xcars, xfamily−xfamilies ≈ xcar−xcars, and so on.</context>
<context position="11396" citStr="Collobert and Weston (2008)" startWordPosition="1836" endWordPosition="1840">along with a composite of several systems, with total dimensionality 1600. The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary. Table 2 shows results for both RNNLM and LSA vectors on the syntactic task. LSA was trained on the same data as the RNN. We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly. 2 In Table 3 we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and Mnih and Hinton (2009), as implemented by (Turian et al., 2010) and available online 3 Since different words are present in these datasets, we computed the intersection of the vocabularies of the RNN vectors and the new vectors, and restricted the test set and word vectors to those. This resulted in a 36k word vocabulary, and a test set with 6632 2Guessing gets a small fraction of a percent. 3http://metaoptimize.com/projects/wordreprs/ Method Adjectives Nouns Verbs All LSA-80 9.2 11.1 17.4 12.8 LSA-320 11.3 18.1 20.7 16.5 LSA-640 9.6 10.1 13.8 11.3 RNN-80 9.3 5.2 30.4 16.2 RNN-320 18.2 19</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society forInformation Science,</journal>
<volume>41</volume>
<issue>96</issue>
<contexts>
<context position="3898" citStr="Deerwester et al., 1990" startWordPosition="607" endWordPosition="610">CL-HLT 2013, pages 746–751, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics The remainder of this paper is organized as follows. In Section 2, we discuss related work; Section 3 describes the recurrent neural network language model we used to obtain word vectors; Section 4 discusses the test sets; Section 5 describes our proposed vector offset method; Section 6 summarizes our experiments, and we conclude in Section 7. 2 Related Work Distributed word representations have a long history, with early proposals including (Hinton, 1986; Pollack, 1990; Elman, 1991; Deerwester et al., 1990). More recently, neural network language models have been proposed for the classical language modeling task of predicting a probability distribution over the “next” word, given some preceding words. These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models. This has been addressed by subseque</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society forInformation Science, 41(96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Distributed representations, simple recurrent networks, and grammatical structure.</title>
<date>1991</date>
<booktitle>Machine learning,</booktitle>
<pages>7--2</pages>
<contexts>
<context position="3872" citStr="Elman, 1991" startWordPosition="605" endWordPosition="606">edings of NAACL-HLT 2013, pages 746–751, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics The remainder of this paper is organized as follows. In Section 2, we discuss related work; Section 3 describes the recurrent neural network language model we used to obtain word vectors; Section 4 discusses the test sets; Section 5 describes our proposed vector offset method; Section 6 summarizes our experiments, and we conclude in Section 7. 2 Related Work Distributed word representations have a long history, with early proposals including (Hinton, 1986; Pollack, 1990; Elman, 1991; Deerwester et al., 1990). More recently, neural network language models have been proposed for the classical language modeling task of predicting a probability distribution over the “next” word, given some preceding words. These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models. This has </context>
</contexts>
<marker>Elman, 1991</marker>
<rawString>J.L. Elman. 1991. Distributed representations, simple recurrent networks, and grammatical structure. Machine learning, 7(2):195–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
<author>R R Salakhutdinov</author>
</authors>
<title>Reducing the dimensionality of data with neural networks.</title>
<date>2006</date>
<journal>Science,</journal>
<volume>313</volume>
<issue>5786</issue>
<contexts>
<context position="4758" citStr="Hinton and Salakhutdinov, 2006" startWordPosition="743" endWordPosition="746"> in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models. This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a). Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations. 3 Recurrent Neural Network Model The word representations we study are learned by a recurrent neural network language model (Mikolov et al., 2010), as illustrated in Figure 1. This architecture consists of an input layer, a hidden layer with recurrent connections, plus the corresponding weight matrices. The input vector w(t) represents input word at time t encoded using 1-of-N coding, and the output layer y(t) produces a probability distribution over words. The</context>
</contexts>
<marker>Hinton, Salakhutdinov, 2006</marker>
<rawString>G.E. Hinton and R.R. Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hinton</author>
<author>R Salakhutdinov</author>
</authors>
<title>Discovering binary codes for documents by learning deep generative models.</title>
<date>2010</date>
<journal>Topics in Cognitive Science,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="4791" citStr="Hinton and Salakhutdinov, 2010" startWordPosition="747" endWordPosition="751">networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models. This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a). Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations. 3 Recurrent Neural Network Model The word representations we study are learned by a recurrent neural network language model (Mikolov et al., 2010), as illustrated in Figure 1. This architecture consists of an input layer, a hidden layer with recurrent connections, plus the corresponding weight matrices. The input vector w(t) represents input word at time t encoded using 1-of-N coding, and the output layer y(t) produces a probability distribution over words. The hidden layer s(t) maintains a re</context>
</contexts>
<marker>Hinton, Salakhutdinov, 2010</marker>
<rawString>G. Hinton and R. Salakhutdinov. 2010. Discovering binary codes for documents by learning deep generative models. Topics in Cognitive Science, 3(1):74–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton</author>
</authors>
<title>Learning distributed representations of concepts.</title>
<date>1986</date>
<booktitle>In Proceedings of the eighth annual conference of the cognitive science society,</booktitle>
<pages>1--12</pages>
<location>Amherst, MA.</location>
<contexts>
<context position="3844" citStr="Hinton, 1986" startWordPosition="601" endWordPosition="602">elation similarity. 746 Proceedings of NAACL-HLT 2013, pages 746–751, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics The remainder of this paper is organized as follows. In Section 2, we discuss related work; Section 3 describes the recurrent neural network language model we used to obtain word vectors; Section 4 discusses the test sets; Section 5 describes our proposed vector offset method; Section 6 summarizes our experiments, and we conclude in Section 7. 2 Related Work Distributed word representations have a long history, with early proposals including (Hinton, 1986; Pollack, 1990; Elman, 1991; Deerwester et al., 1990). More recently, neural network language models have been proposed for the classical language modeling task of predicting a probability distribution over the “next” word, given some preceding words. These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally</context>
</contexts>
<marker>Hinton, 1986</marker>
<rawString>G.E. Hinton. 1986. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society, pages 1–12. Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Saif Mohammad</author>
<author>Peter Turney</author>
<author>Keith Holyoak</author>
</authors>
<title>Semeval-2012 task 2: Measuring degrees of relational similarity.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics (SemEval</booktitle>
<pages>356--364</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8143" citStr="Jurgens et al., 2012" startWordPosition="1284" endWordPosition="1287">0 of the most frequent comparative adjectives (words labeled JJR); 100 of the most frequent plural nouns (NNS); 100 of the most frequent possessive nouns (NN POS); and 100 of the most frequent base form verbs (VB). We then systematically generated analogy questions by randomly matching each of the 100 words with 5 other words from the same category, and creating variants as indicated in Table 1. The total test set size is 8000. The test set is available online. 1 4.2 A Semantic Test Set In addition to syntactic analogy questions, we used the SemEval-2012 Task 2, Measuring Relation Similarity (Jurgens et al., 2012), to estimate the extent to which RNNLM word vectors contain semantic information. The dataset contains 79 fine-grained word relations, where 10 are used for training and 69 testing. Each relation is exemplified by 3 or 4 gold word pairs. Given a group of word pairs that supposedly have the same relation, the task is to order the target pairs according to the degree to which this relation holds. This can be viewed as another analogy problem. For example, take the ClassInclusion:Singular Collective relation with the pro1http://research.microsoft.com/enus/projects/rnn/default.aspx totypical word</context>
</contexts>
<marker>Jurgens, Mohammad, Turney, Holyoak, 2012</marker>
<rawString>David Jurgens, Saif Mohammad, Peter Turney, and Keith Holyoak. 2012. Semeval-2012 task 2: Measuring degrees of relational similarity. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics (SemEval 2012), pages 356–364. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>I Oparin</author>
<author>A Allauzen</author>
<author>J-L Gauvain</author>
<author>F Yvon</author>
</authors>
<title>Structured output layer neural network language model.</title>
<date>2011</date>
<booktitle>In Proceedings of ICASSP</booktitle>
<contexts>
<context position="4599" citStr="Le et al., 2011" startWordPosition="718" endWordPosition="721">anguage modeling task of predicting a probability distribution over the “next” word, given some preceding words. These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models. This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a). Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations. 3 Recurrent Neural Network Model The word representations we study are learned by a recurrent neural network language model (Mikolov et al., 2010), as illustrated in Figure 1. This architecture consists of an input layer, a hidden layer with recurrent connections, plus the corresponding weight matrices. T</context>
</contexts>
<marker>Le, Oparin, Allauzen, Gauvain, Yvon, 2011</marker>
<rawString>Hai-Son Le, I. Oparin, A. Allauzen, J.-L. Gauvain, and F. Yvon. 2011. Structured output layer neural network language model. In Proceedings of ICASSP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="7501" citStr="Marcus et al., 1993" startWordPosition="1172" endWordPosition="1175">arative/ JJS/JJR, JJR/JJS 1000 better:best rougher: Superlative Nouns Singular/Plural NN/NNS, 1000 year:years law: NNS/NN Nouns Non-possessive/ NN/NN POS, 1000 city:city’s bank: Possessive NN POS/NN Verbs Base/Past VB/VBD, 1000 see:saw return: VBD/VB Verbs Base/3rd Person VB/VBZ, VBZ/VB 1000 see:sees return: Singular Present Verbs Past/3rd Person VBD/VBZ, 1000 saw:sees returned: Singular Present VBZ/VBD Table 1: Test set patterns. For a given pattern and word-pair, both orderings occur in the test set. For example, if “see:saw return: ” occurs, so will “saw:see returned: ”. Treebank POS tags (Marcus et al., 1993). We then selected 100 of the most frequent comparative adjectives (words labeled JJR); 100 of the most frequent plural nouns (NNS); 100 of the most frequent possessive nouns (NN POS); and 100 of the most frequent base form verbs (VB). We then systematically generated analogy questions by randomly matching each of the 100 words with 5 other words from the same category, and creating variants as indicated in Table 1. The total test set size is 8000. The test set is available online. 1 4.2 A Semantic Test Set In addition to syntactic analogy questions, we used the SemEval-2012 Task 2, Measuring </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafiat</author>
<author>Jan Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<contexts>
<context position="1454" citStr="Mikolov et al., 2010" startWordPosition="211" endWordPosition="214">” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems. 1 Introduction A defining feature of neural network language models is their representation of words as high dimensional real valued vectors. In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the *Currently at Google, Inc. inputs to a neural network. As pointed out by the original proposers, one of the main advantages of these models is that the distributed representation achieves a level of generalization that is not possible with classical n-gram language models; whereas a n-gram model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectors. Thus, when t</context>
<context position="4294" citStr="Mikolov et al., 2010" startWordPosition="672" endWordPosition="675">ummarizes our experiments, and we conclude in Section 7. 2 Related Work Distributed word representations have a long history, with early proposals including (Hinton, 1986; Pollack, 1990; Elman, 1991; Deerwester et al., 1990). More recently, neural network language models have been proposed for the classical language modeling task of predicting a probability distribution over the “next” word, given some preceding words. These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models. This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a). Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations. 3</context>
</contexts>
<marker>Mikolov, Karafiat, Cernocky, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafiat, Jan Cernocky, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of Interspeech 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Anoop Deoras</author>
<author>Daniel Povey</author>
<author>Lukas Burget</author>
<author>Jan Cernocky</author>
</authors>
<title>Strategies for Training Large Scale Neural Network Language Models.</title>
<date>2011</date>
<booktitle>In Proceedings of ASRU</booktitle>
<contexts>
<context position="4316" citStr="Mikolov et al., 2011" startWordPosition="676" endWordPosition="679">nts, and we conclude in Section 7. 2 Related Work Distributed word representations have a long history, with early proposals including (Hinton, 1986; Pollack, 1990; Elman, 1991; Deerwester et al., 1990). More recently, neural network language models have been proposed for the classical language modeling task of predicting a probability distribution over the “next” word, given some preceding words. These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models. This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a). Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations. 3 Recurrent Neural Netw</context>
<context position="10945" citStr="Mikolov et al., 2011" startWordPosition="1756" endWordPosition="1759">ll for both syntactic and semantic relations. We note that this measure is qualitatively similar to relational similarity model of (Turney, 2012), which predicts similarity between members of the word pairs (xb, xd), (x,-, xd) and dis-similarity for (xa, xd). 6 Experimental Results To evaluate the vector offset method, we used vectors generated by the RNN toolkit of Mikolov (2012). Vectors of dimensionality 80, 320, and 640 were generated, along with a composite of several systems, with total dimensionality 1600. The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary. Table 2 shows results for both RNNLM and LSA vectors on the syntactic task. LSA was trained on the same data as the RNN. We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly. 2 In Table 3 we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and Mnih and Hinton (2009), as implemented by (Turian et al., 2010) and available online 3 Since different words are present in these datasets, we c</context>
</contexts>
<marker>Mikolov, Deoras, Povey, Burget, Cernocky, 2011</marker>
<rawString>Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget, and Jan Cernocky. 2011a. Strategies for Training Large Scale Neural Network Language Models. In Proceedings of ASRU 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Lukas Burget</author>
<author>Jan Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network based language model.</title>
<date>2011</date>
<booktitle>In Proceedings of ICASSP</booktitle>
<contexts>
<context position="4316" citStr="Mikolov et al., 2011" startWordPosition="676" endWordPosition="679">nts, and we conclude in Section 7. 2 Related Work Distributed word representations have a long history, with early proposals including (Hinton, 1986; Pollack, 1990; Elman, 1991; Deerwester et al., 1990). More recently, neural network language models have been proposed for the classical language modeling task of predicting a probability distribution over the “next” word, given some preceding words. These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models. This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a). Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations. 3 Recurrent Neural Netw</context>
<context position="10945" citStr="Mikolov et al., 2011" startWordPosition="1756" endWordPosition="1759">ll for both syntactic and semantic relations. We note that this measure is qualitatively similar to relational similarity model of (Turney, 2012), which predicts similarity between members of the word pairs (xb, xd), (x,-, xd) and dis-similarity for (xa, xd). 6 Experimental Results To evaluate the vector offset method, we used vectors generated by the RNN toolkit of Mikolov (2012). Vectors of dimensionality 80, 320, and 640 were generated, along with a composite of several systems, with total dimensionality 1600. The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary. Table 2 shows results for both RNNLM and LSA vectors on the syntactic task. LSA was trained on the same data as the RNN. We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly. 2 In Table 3 we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and Mnih and Hinton (2009), as implemented by (Turian et al., 2010) and available online 3 Since different words are present in these datasets, we c</context>
</contexts>
<marker>Mikolov, Kombrink, Burget, Cernocky, Khudanpur, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. 2011b. Extensions of recurrent neural network based language model. In Proceedings of ICASSP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
</authors>
<date>2012</date>
<note>RNN toolkit.</note>
<contexts>
<context position="10708" citStr="Mikolov (2012)" startWordPosition="1719" endWordPosition="1720">ion, and the singular/plural relation for two words. In high-dimensional space, multiple relations can be embedded for a single word. provided. We have explored several related methods and found that the proposed method performs well for both syntactic and semantic relations. We note that this measure is qualitatively similar to relational similarity model of (Turney, 2012), which predicts similarity between members of the word pairs (xb, xd), (x,-, xd) and dis-similarity for (xa, xd). 6 Experimental Results To evaluate the vector offset method, we used vectors generated by the RNN toolkit of Mikolov (2012). Vectors of dimensionality 80, 320, and 640 were generated, along with a composite of several systems, with total dimensionality 1600. The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary. Table 2 shows results for both RNNLM and LSA vectors on the syntactic task. LSA was trained on the same data as the RNN. We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly. 2 In Table 3 w</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tomas Mikolov. 2012. RNN toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model. Advances in neural information processing systems,</title>
<date>2009</date>
<pages>21--1081</pages>
<contexts>
<context position="4582" citStr="Mnih and Hinton, 2009" startWordPosition="714" endWordPosition="717">sed for the classical language modeling task of predicting a probability distribution over the “next” word, given some preceding words. These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models. This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a). Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations. 3 Recurrent Neural Network Model The word representations we study are learned by a recurrent neural network language model (Mikolov et al., 2010), as illustrated in Figure 1. This architecture consists of an input layer, a hidden layer with recurrent connections, plus the corresponding w</context>
<context position="11423" citStr="Mnih and Hinton (2009)" startWordPosition="1842" endWordPosition="1845">l systems, with total dimensionality 1600. The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary. Table 2 shows results for both RNNLM and LSA vectors on the syntactic task. LSA was trained on the same data as the RNN. We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly. 2 In Table 3 we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and Mnih and Hinton (2009), as implemented by (Turian et al., 2010) and available online 3 Since different words are present in these datasets, we computed the intersection of the vocabularies of the RNN vectors and the new vectors, and restricted the test set and word vectors to those. This resulted in a 36k word vocabulary, and a test set with 6632 2Guessing gets a small fraction of a percent. 3http://metaoptimize.com/projects/wordreprs/ Method Adjectives Nouns Verbs All LSA-80 9.2 11.1 17.4 12.8 LSA-320 11.3 18.1 20.7 16.5 LSA-640 9.6 10.1 13.8 11.3 RNN-80 9.3 5.2 30.4 16.2 RNN-320 18.2 19.0 45.0 28.5 RNN-640 21.0 2</context>
<context position="12652" citStr="Mnih and Hinton, 2009" startWordPosition="2041" endWordPosition="2044">8 34.7 RNN-1600 23.9 29.2 62.2 39.6 Table 2: Results for identifying syntactic regularities for different word representations. Percent correct. Method Adjectives Nouns Verbs All RNN-80 10.1 8.1 30.4 19.0 CW-50 1.1 2.4 8.1 4.5 CW-100 1.3 4.1 8.6 5.0 HLBL-50 4.4 5.4 23.1 13.0 HLBL-100 7.6 13.2 30.2 18.7 Table 3: Comparison of RNN vectors with Turian’s Collobert and Weston based vectors and the Hierarchical Log-Bilinear model of Mnih and Hinton. Percent correct. questions. Turian’s Collobert and Weston based vectors do poorly on this task, whereas the Hierarchical Log-Bilinear Model vectors of (Mnih and Hinton, 2009) do essentially as well as the RNN vectors. These representations were trained on 37M words of data and this may indicate a greater robustness of the HLBL method. We conducted similar experiments with the semantic test set. For each target word pair in a relation category, the model measures its relational similarity to each of the prototypical word pairs, and then uses the average as the final score. The results are evaluated using the two standard metrics defined in the task, Spearman’s rank correlation coefficient p and MaxDiff accuracy. In both cases, larger values are better. To compare t</context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>A. Mnih and G.E. Hinton. 2009. A scalable hierarchical distributed language model. Advances in neural information processing systems, 21:1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Morin</author>
<author>Y Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In Proceedings of the international workshop on artificial intelligence and statistics,</booktitle>
<pages>246--252</pages>
<contexts>
<context position="4559" citStr="Morin and Bengio, 2005" startWordPosition="710" endWordPosition="713">e models have been proposed for the classical language modeling task of predicting a probability distribution over the “next” word, given some preceding words. These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models. This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a). Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations. 3 Recurrent Neural Network Model The word representations we study are learned by a recurrent neural network language model (Mikolov et al., 2010), as illustrated in Figure 1. This architecture consists of an input layer, a hidden layer with recurrent connections, p</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>F. Morin and Y. Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceedings of the international workshop on artificial intelligence and statistics, pages 246–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Pollack</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<volume>46</volume>
<issue>1</issue>
<contexts>
<context position="3859" citStr="Pollack, 1990" startWordPosition="603" endWordPosition="604">rity. 746 Proceedings of NAACL-HLT 2013, pages 746–751, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics The remainder of this paper is organized as follows. In Section 2, we discuss related work; Section 3 describes the recurrent neural network language model we used to obtain word vectors; Section 4 discusses the test sets; Section 5 describes our proposed vector offset method; Section 6 summarizes our experiments, and we conclude in Section 7. 2 Related Work Distributed word representations have a long history, with early proposals including (Hinton, 1986; Pollack, 1990; Elman, 1991; Deerwester et al., 1990). More recently, neural network language models have been proposed for the classical language modeling task of predicting a probability distribution over the “next” word, given some preceding words. These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b). This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient mode</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>J.B. Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46(1):77–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Rink</author>
<author>Sanda Harabagiu</author>
</authors>
<title>UTD: Determining relational similarity using lexical patterns.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics (SemEval</booktitle>
<pages>413--418</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14156" citStr="Rink and Harabagiu, 2012" startWordPosition="2290" endWordPosition="2293">tors. With the RNN vectors, the performance improves as the number of dimensions increases. Surprisingly, we found that even though the RNN vec749 Method Spearman’s p MaxDiff Acc. LSA-640 0.149 0.364 RNN-80 0.211 0.389 RNN-320 0.259 0.408 RNN-640 0.270 0.416 RNN-1600 0.275 0.418 CW-50 0.159 0.363 CW-100 0.154 0.363 HLBL-50 0.149 0.363 HLBL-100 0.146 0.362 UTD-NB 0.230 0.395 Table 4: Results in measuring relation similarity tors are not trained or tuned specifically for this task, the model achieves better results (RNN-320, RNN640 &amp; RNN-1600) than the previously best performing system, UTD-NB (Rink and Harabagiu, 2012). 7 Conclusion We have presented a generally applicable vector offset method for identifying linguistic regularities in continuous space word representations. We have shown that the word representations learned by a RNNLM do an especially good job in capturing these regularities. We present a new dataset for measuring syntactic performance, and achieve almost 40% correct. We also evaluate semantic generalization on the SemEval 2012 task, and outperform the previous state-of-the-art. Surprisingly, both results are the byproducts of an unsupervised maximum likelihood training criterion that simp</context>
</contexts>
<marker>Rink, Harabagiu, 2012</marker>
<rawString>Bryan Rink and Sanda Harabagiu. 2012. UTD: Determining relational similarity using lexical patterns. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics (SemEval 2012), pages 413–418. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>3</issue>
<pages>518</pages>
<contexts>
<context position="1431" citStr="Schwenk, 2007" startWordPosition="209" endWordPosition="210">lose to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems. 1 Introduction A defining feature of neural network language models is their representation of words as high dimensional real valued vectors. In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the *Currently at Google, Inc. inputs to a neural network. As pointed out by the original proposers, one of the main advantages of these models is that the distributed representation achieves a level of generalization that is not possible with classical n-gram language models; whereas a n-gram model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have simila</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21(3):492 – 518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: a simple and general method for semisupervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings ofAssociation for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="2503" citStr="Turian et al., 2010" startWordPosition="385" endWordPosition="388"> have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectors. Thus, when the model parameters are adjusted in response to a particular word or word-sequence, the improvements will carry over to occurrences of similar words and sequences. By training a neural network language model, one obtains not just the model itself, but also the learned word representations, which may be used for other, potentially unrelated, tasks. This has been used to good effect, for example in (Collobert and Weston, 2008; Turian et al., 2010) where induced word representations are used with sophisticated classifiers to improve performance in many NLP tasks. In this work, we find that the learned word representations in fact capture meaningful syntactic and semantic regularities in a very simple way. Specifically, the regularities are observed as constant vector offsets between pairs of words sharing a particular relationship. For example, if we denote the vector for word i as xi, and focus on the singular/plural relation, we observe that x apple−xapples ≈ xcar−xcars, xfamily−xfamilies ≈ xcar−xcars, and so on. Perhaps more surprisi</context>
<context position="11464" citStr="Turian et al., 2010" startWordPosition="1850" endWordPosition="1853">The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary. Table 2 shows results for both RNNLM and LSA vectors on the syntactic task. LSA was trained on the same data as the RNN. We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly. 2 In Table 3 we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and Mnih and Hinton (2009), as implemented by (Turian et al., 2010) and available online 3 Since different words are present in these datasets, we computed the intersection of the vocabularies of the RNN vectors and the new vectors, and restricted the test set and word vectors to those. This resulted in a 36k word vocabulary, and a test set with 6632 2Guessing gets a small fraction of a percent. 3http://metaoptimize.com/projects/wordreprs/ Method Adjectives Nouns Verbs All LSA-80 9.2 11.1 17.4 12.8 LSA-320 11.3 18.1 20.7 16.5 LSA-640 9.6 10.1 13.8 11.3 RNN-80 9.3 5.2 30.4 16.2 RNN-320 18.2 19.0 45.0 28.5 RNN-640 21.0 25.2 54.8 34.7 RNN-1600 23.9 29.2 62.2 39.</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: a simple and general method for semisupervised learning. In Proceedings ofAssociation for Computational Linguistics (ACL 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Domain and function: A dual-space model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>44--533</pages>
<contexts>
<context position="10470" citStr="Turney, 2012" startWordPosition="1680" endWordPosition="1681">d is given, as in our semantic test set, we simply use cos(xb − xa + xc, xd) for the words kxwkkyk 748 Figure 2: Left panel shows vector offsets for three word pairs illustrating the gender relation. Right panel shows a different projection, and the singular/plural relation for two words. In high-dimensional space, multiple relations can be embedded for a single word. provided. We have explored several related methods and found that the proposed method performs well for both syntactic and semantic relations. We note that this measure is qualitatively similar to relational similarity model of (Turney, 2012), which predicts similarity between members of the word pairs (xb, xd), (x,-, xd) and dis-similarity for (xa, xd). 6 Experimental Results To evaluate the vector offset method, we used vectors generated by the RNN toolkit of Mikolov (2012). Vectors of dimensionality 80, 320, and 640 were generated, along with a composite of several systems, with total dimensionality 1600. The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary. Table 2 shows results for both RNNLM and LSA vectors on the syntactic task. LSA was trained on</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>P.D. Turney. 2012. Domain and function: A dual-space model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533–585.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>