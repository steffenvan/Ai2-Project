<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004118">
<note confidence="0.907359">
Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 392-399.
</note>
<bodyText confidence="0.9467385">
lit Section 4, we consider the implications of our
experimental results and discuss future work.
</bodyText>
<sectionHeader confidence="0.9285535" genericHeader="method">
2 The Direct Correspondence
Assumption
</sectionHeader>
<bodyText confidence="0.998662382352941">
To our knowledge, the direct correspondence as-
sumption underlies all statistical models that at-
tempt to capture a relationship between syntac-
tic structures in two languages, be they con-
stituent models or dependency models. As
art example of the former, consider Wu&apos;s
(1995) stochastic inversion transduction gram-
mar (SITG), in which paired sentences are si-
multaneously generated using context-free rules;
word order differences are accounted for by
allowing each rule to be read in a left-to-
right or right-to-left fashion, depending ort
the language. For example, SITG can gen-
erate verb initial (English) and verb final
(Japanese) verb phrases using the same rule
VP V NP. For arty derivation using this
rule, if VE and NPE are the English verb
and noun phrase, and they are respectively
aligned with Japanese verb and noun phrase
v j and NP j, then VERB-OBJECT(VE, NPE) and
VERB-OBJECT(V j, NP j) mug both be true.
As art example where the DCA relates
dependency structures, consider the hier-
archical alignment algorithm proposed by
Alshawi et al. (2000). In this framework, word-
level alignments and paired dependency struc-
tures are constructed simultaneously. The
English-Basque example (1) illustrates: if the
English word buy is aligned to the Basque word
erosi and gift is aligned to opari, the creation
of the head-modifier relationship between buy
and gift is accompanied by the creation of a cor-
responding head-modifier relationship between
erosi and opari.
</bodyText>
<listItem confidence="0.91981375">
(1) a. I got a gift for my brother
b. Nik (I) lire (MY) anaiari (BROTHER-
DAT) (Vali (GIFT) bat (A) erosi (Buy)
nion (PAST)
</listItem>
<subsectionHeader confidence="0.887903">
2.1 Formalizing the DCA
</subsectionHeader>
<bodyText confidence="0.996137666666667">
Let us formalize this intuitive idea about corre-
sponding syntactic relationships in the following
more general way:
</bodyText>
<sectionHeader confidence="0.349153" genericHeader="method">
Direct Correspondence Assumption
</sectionHeader>
<bodyText confidence="0.986918771428572">
(DCA): Given a pair of sentences E and F
that are (literal) translations of each other with
syntactic structures TreeE and TreeF, if nodes
E and YE of TreeE are aligned with nodes xF
and YF of TreeF, respectively, and if syntactic
relationship R(x E, YE) holds in TreeE, then
R(x F , YF) holds in TreeF.
Here, R(x , y) may specify a head-modifier
relationship between words in a dependency
tree, or a sisterhood relationship between non-
terminals in a constituency tree. As stated, the
DCA amounts to art assumption that the cross-
language alignment resembles a Itomomorphism
relating the syntactic graph of E to the syntactic
graph of F.2
Wu&apos;s SITG makes this assumption, under the
interpretation that R is the head-modifier re-
lation expressed in a rewrite rule. The IBM
MT models (Brown et al., 1993) do not re-
spect the DCA, but neither do they attempt to
model arty higher level syntactic relationship be-
tween constituents within or across languages
the translation model (alignments) and the lan-
guage model are statistically independent. In
Yamada and Knight&apos;s (2001) extension of the
IBM models, on the other hand, grammatical
information from the source language is prop-
agated into the noisy channel, and the gram-
matical transformations in their channel model
appear to respect direct correspondence.3 The
simultaneous parsing and alignment algorithm
of Alshawi et al. (2000) is essentially art imple-
mentation of the DCA in which relationship R
has no linguistic import (i.e. anything can be a
head).
</bodyText>
<footnote confidence="0.986151538461538">
2Some models embody a stronger version of the DCA
that more closely resembles an isomorphism between de-
pendency graphs(Shieber, 1994), though we will not pur-
sue this idea further here.
3 Knight and Yamada actually pre-process the English
input in cases that most transparently violate direct cor-
respondence; for example, they permute English verbs to
sentence-final position in the model transforming English
into Japanese. Most models we looked at have addressed
some effects of DCA failure, but they have not acknowl-
edged it explicitly as an underlying assumption, nor have
they gone beyond expedient measures to the type of prin-
cipled analysis that we propose below.
</footnote>
<table confidence="0.9778074">
xEng YEng xBsq YBsq
verb-subj got I erosi nik
verb-obj got gift erosi opari
noun-det gift a opari bat
noun-mod brother my anaiari nire
</table>
<tableCaption confidence="0.999841">
Table 1: Correspondences preserved in (1)
</tableCaption>
<subsectionHeader confidence="0.99564">
2.2 Problems with the DCA
</subsectionHeader>
<bodyText confidence="0.999960589285714">
The DCA seems to be a reasonable principle, es-
pecially when expressed in terms of syntactic de-
pendencies that abstract away word order. That
is, the thematic (who-did-what-to-whom) rela-
tionships are likely to hold true across transla-
tions evert for typologically different languages.
Consider example (1) again: despite the fact
that the Basque sentence has a different word
order, with the verb appearing at the far right
of the sentence, the syntactic dependency rela-
tionships of English (subject, object, noun mod-
ifier, etc.) are largely preserved across the align-
ment, as illustrated in Table 1. Moreover, the
DCA makes possible more elegant formalisms
(e.g. SITG) and more efficient algorithms It
may allow us to use the syntactic analysis for
one language to infer annotations for the corre-
sponding sentence in another language, helping
to reduce the labor and expense of creating tree-
banks in new languages (Cabezas et al., 2001;
Yarowsky and Ngai, 2001).
Unfortunately, the DCA is flawed, even for
literal translations. For example, in sentence
pair (1), the indirect object of the verb is ex-
pressed in English using a prepositional phrase
(headed by the word for) that attaches to
the verb, but it is expressed with the dative
case marking ort anaiari (BROTHER-DAT) it
Basque. If we aligned both for and brother
to anaiari, then a many-to-one mapping would
be formed, and the DCA would be violated:
R(f or, brother) holds in the English tree but
R(analari,analari) does not hold in the Basque
tree. Similarly, a one-to-many mapping (e.g.,
aligning got with erosi (Buy) and nion (PAST)
in this example) can also be problematic for the
DCA.
The inadequacy of the DCA should come as
no surprise. The syntax literature dating back
to Cltomsky (1981), together with a rich com-
putational literature on translation divergences
(e.g. (Abeille et al., 1990; Dorr, 1994; Han
et al., 2000)), is concerned with characterizing
in a systematic way the apparent diversity of
mechanisms used by languages to express mean-
ings syntactically. For example, current theo-
ries claim that languages employ stable head-
complement orders across construction types. In
English, the head of a phrase is uniformly to the
left of modifying prepositional phrases, senten-
tial complements, etc. In Chinese, verbal and
prepositional phrases respect the English order-
ing but heads in the nominal system uniformly
appear to the right. Systematic application of
this sort of linguistic knowledge turns out to be
the key in getting beyond the DCA&apos;s limitations.
</bodyText>
<sectionHeader confidence="0.8261495" genericHeader="method">
3 Evaluating the DCA using
Annotation Projection
</sectionHeader>
<bodyText confidence="0.999980688888889">
Thus far, we have argued that the DCA is a use-
ful and widely assumed principle; at the same
time we have illustrated that it is incapable of
accounting for some well known and fundamen-
tal linguistic facts. Yet this is not art unfamil-
iar situation. For years, stochastic modeling of
language has depended on the linguistically im-
plausible assumptions underlying&apos;n-gram mod-
els, hidden Markov models, context-free gram-
mars, and the like, with remarkable success.
Having made the DCA explicit, we would sug-
gest that the right questions are: to what extent
is it true, and how useful is it when it holds?
In the remainder of the paper, we focus on an-
swering the first question empirically by consid-
ering the syntactic relationships and alignments
between translated sentence pairs in two distant
languages (English and Chinese). In our experi-
mental framework, a system is given the &amp;quot;ideal&amp;quot;
syntactic analyses for the English sentences and
English-Chinese word-alignments, and it uses a
Direct Projection Algorithm (described below)
to project the English syntactic annotations di-
rectly across to the Chinese sentences in accor-
dance with the DCA. The resulting Chinese de-
pendency analyses are then compared with an
independently derived gold standard, enabling
us to determine recall and precision figures for
syntactic dependencies (cf. (Lin, 1998)) and to
perform a qualitative error analysis. This error
analysis led us to revise our projection approach,
and the resulting linguistically informed projec-
tion improved significantly the ability to obtain
accurate Chinese parses.
This experimental framework for the first
question is designed with art eye toward the sec-
ond, concerning the usefulness of making the
direct correspondence assumption. If the DCA
holds true more often than not, then one might
speculate that the projected syntactic structures
could be useful as a treebank (albeit a noisy
one) for training Chinese parsers, and could
help more generally in overcoming the syntactic
annotation bottleneck for languages other than
English.
</bodyText>
<subsectionHeader confidence="0.997483">
3.1 The Direct Projection Algorithm
</subsectionHeader>
<bodyText confidence="0.999945333333333">
The DCA translates fairly directly into art algo-
rithm for projecting English dependency analy-
ses across to Chinese using word alignments as
the bridge. More formally, given sentence pair
(F, F), the English syntactic relations are pro-
jected for the following situations:
</bodyText>
<listItem confidence="0.9989745">
• one-to-one if hE E E is aligned with a
unique hF E F and mE is aligned with a
unique in F E F, then if R(hE , rn E), con-
clude R(hF , F).
• unaligned (English) if WE EE is not
aligned with arty word in F, then create a
new empty word nF E F such that for arty
XE aligned with a unique xF, R(xE, WE)
R(x F, nF) and R(tvE, xE)
• one-to-many if WE E E is aligned with
</listItem>
<bodyText confidence="0.883115473684211">
tvi,, tvn,, then create a new empty
word mF E F such that mF is the parent
of tvi,, tvn, and set WE to align to m F
instead.
• many-to-one if wjE, wnE E E are all
uniquely aligned to WF E F, then delete all
alignments between wiE (1 &lt; i &lt; n) and WF
except for the head (denoted as whE ); more-
over, if wiE , a modifier of .tchE, had its own
modifiers, R(tviE, wjE) R(tch, wJF).
The many-to-many case is decomposed into
a two-step process: first perform one-to-many,
then perform many-to-one. In the cases of un-
aligned Chinese words, they are left out of the
projected syntactic tree. The asymmetry in the
treatment of one-to-many and many-to-one
and of the unaligned words for the two languages
arises from the asymmetric nature of the projec-
tion.
</bodyText>
<subsectionHeader confidence="0.991777">
3.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.993625175438596">
The corpus for this experiment was constructed
by obtaining manual English translations for
124 Chinese newswire sentences (with 40 words
or less) contained in sections 001-015 of the Penn
Chinese Treebank (Xia et al., 2000). The Chi-
nese data in our set ranged from simple sen-
tences to some complicated constructions such
as complex relative clauses, multiple run-on
clauses, embeddings, nominal constructions, etc.
Average sentence length was 23.7 words.
Parses for the English sentences were con-
structed by a process of automatic analy-
sis followed by hand correction; output trees
from a broad-coverage lexicalized English parser
(Collins, 1997) were automatically converted
into dependencies to be corrected. The gold-
standard dependency analyses for the Chinese
sentences were constructed manually by two flu-
ent speakers of Chinese, working independently
and using the Chinese Treebank&apos;s (manually
constructed) constituency parses for reference.4
Inter-annotator agreement ort unlabeled syntac-
tic dependencies is 92.4%. Manual English-
Chinese alignments were constructed by two an-
notators who are native speakers of Chinese us-
ing a software environment similar to that de-
scribed by Melamed (1998).
The direct projection of English dependen-
cies to Chinese yielded poor results as measured
by precision and recall over unlabeled syntactic
dependencies: precision was 30.1% and recall
39.1%. Inspection of the results revealed that
our manually aligned parallel corpus contained
many instances of multiply aligned or unaligned
tokens, owing either to freeness of translation
40ne author of this paper served as one of the anno-
tators.
(a violation of the assumption that translations
are literal) or to differences in how the two lan-
guages express the same meaning. For example,
to quantify a Chinese noun with a determiner,
one also needs to supply a measure word in ad-
dition to the quantity. Thus, the noun phrase
an apple is expressed as yee (AN) ge (-mEAs)
ping-guo (APPLE). Chinese also includes sepa-
rate words to indicate aspectual categories such
as continued action, in contrast to verbal suf-
fixes in English such as the -ing in running.
Because Chinese classifiers, aspectual particles,
and other functional words do not appear in the
English sentence, there is no way for a projected
English analysis to correctly account for them.
As a result, the Chinese dependency trees usu-
ally fail to contain an appropriate grammatical
relation for these items. Because they are fre-
quent, the failure to properly account for them
significantly hurts performance.
</bodyText>
<subsectionHeader confidence="0.968902">
3.3 Revised Projection
</subsectionHeader>
<bodyText confidence="0.998986083333333">
Our error analysis led to the conclusion that the
correspondence of syntactic relationships would
be improved by a better handling of the one-to-
many mappings and the unaligned cases. We
investigated two ways of addressing this issue.
First, we adopted a simple strategy informed
by the tendency of languages to have a consis-
tent direction for &amp;quot;headedness&amp;quot;. Chinese and
English share the property that they are head-
initial for most phrase types. Thus, if an English
word aligns to multiple Chinese words cj, , cm,
the leftmost word c1 is treated as the head and
c2, ..., cm are analyzed as its dependents. If
a Chinese empty node was introduced to align
with an untranslated English word, it is deleted
and its left-most child is promoted to replace it.
Looking at language in this non-construction-
dependent way allows us to make simple changes
that have wide ranging effects. This is illustra-
tive of how our approach tries to rein in cases
where the DCA breaks down by using linguisti-
cally informed constraints that are as general as
possible.
Second, we used more detailed linguistic
knowledge of Chinese to develop a small set of
rules, expressed in a tree-based pattern-action
formalism, that perform local modifications of a
projected analysis on the Chinese side. To avoid
the slippery slope of unending language-specific
rule tweaking, we strictly constrained the possi-
ble rules. Rules were permitted to refer only to
closed class items, to parts of speech projected
from the English analysis, or to easily enumer-
ated lexical categories (e.g. {dollar, RMB, $,
For example, one such rule deals with noun
modification:
</bodyText>
<listItem confidence="0.84869925">
• If nj, nk are a set of Chinese words
aligned to an English noun, replace the
empty node introduced in the Direct Pro-
jection Algorithm by promoting the last
</listItem>
<bodyText confidence="0.9285404">
word nk to its place with nj, nk_j as
dependents.
Another deals with aspectual markers for verbs:
• If vi, vk, a sequence of Chinese words
aligned with English verbs, is followed by
a, an aspect marker, make a into a modifier
of the last verb vk.
The most involved transformation places a lin-
guistic constraint on the Chinese functional
word de, which may be translated as that (the
head of a relative clause), as the preposition of,
or as &apos;s (a marker for possessives). This com-
mon Chinese functional word is almost always
either unaligned or multiply aligned to an En-
glish word.
</bodyText>
<listItem confidence="0.982467142857143">
• If c, is the Chinese word that appeared im-
mediately to the left of de and c3 is the Chi-
nese word that appeared immediately to the
right of it, then find the lowest ancestors cp
and cq for c, and ci, respectively, such that
R(cp, cq) exists; remove that relationship;
and replace it with R(de, cp) and R(cq, de).
</listItem>
<bodyText confidence="0.999979166666667">
The latter two changes may seem unrelated,
but they both take advantage of the fact that
Chinese violates the head-initial rule in its nom-
inal system, where noun phrases are uniformly
head-final. More generally, the majority of rule
patterns are variations on the same solution to
the same problem. Viewing the problem from
a higher level of linguistic abstraction made it
possible to find all the relevant cases in a short
time (a few days) and express the solution com-
pactly (&lt; 20 rules). The complete set of rules
can be found in (Hwa et al., 2002).
</bodyText>
<subsectionHeader confidence="0.740243">
3.4 A New Experiment
</subsectionHeader>
<bodyText confidence="0.99990321875">
Because our error analysis and subsequent al-
gorithm refinements made use of our original
Chinese-English data set, we created a new test
set based on 88 new Chinese sentences from
the Penn Chinese Treebank, already manually
translated into English as part of the NIST MT
evaluation preview.5 These sentences averaged
19.0 words in length.
As described above, parses ort the English
side were created semi-automatically, and word
alignments were acquired manually. However, in
order to reduce our reliance ort linguistically so-
phisticated human annotators for Chinese syn-
tax, we adopted art alternative strategy for ob-
taining the gold standard: we automatically
converted the Treebank&apos;s constituency parses of
the Chinese sentences into syntactic dependency
representations, using art algorithm similar to
the one described in Section 2 of the paper by
Xia and Palmer (2001).6
The recall and precision figures for the new ex-
periment are summarized in Table 2. The first
row of the table shows the results comparing the
output of the Direct Projection Algorithm with
the gold standard. As we have already seen pre-
viously, the quality of these trees is not very
good. The second row of the table shows that af-
ter applying the single transformation based ort
the head-initial assumption, precision and recall
both improve significantly: using the F-measure
to combine precision and recall into a single fig-
ure of merit (Van Rijsbergen, 1979), the increase
</bodyText>
<footnote confidence="0.989666181818182">
5 See http://www.nist.govispeechitestsimti. We
used sentences from sections 038, 039, 067, 122, 191, 207,
249 because, according to the distributor, the translation
of these sections (files with .spc suffix) have been more
carefully verified.
6The strategy was validated by performing the same
process on the original data set; the agreement rate with
the human-generated dependency trees was 97.5%. This
led us to be confident that Treebank constituency parses
could be used automatically to create a gold standard for
syntactic dependencies.
</footnote>
<table confidence="0.9855765">
Method Precision Recall F-measure
Direct 34.5 42.5 38.1
Head-initial 59.4 59.4 59.4
Rules 68.0 66.6 67.3
</table>
<tableCaption confidence="0.999728">
Table 2: Performance ort Chinese analyses (Vo)
</tableCaption>
<bodyText confidence="0.999939333333333">
from 38.1% to 59.4% represents a 55.9% relative
improvement. The third row of the table shows
that by applying the small set of tree modifica-
tion rules after direct projection (one of which
is default assignment of the head-initial analysis
to multi-word phrases when no other rule ap-
plies), we obtain art evert larger improvement,
the 67.3% F-measure representing a 76.6% rela-
tive gain over baseline performance.
</bodyText>
<sectionHeader confidence="0.979353" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999935342465754">
To what extent is the DCA a valid assumption?
Our experiments confirm the linguistic intuition,
indicating that one cannot safely assume a direct
mapping between the syntactic dependencies of
one language and the syntactic dependencies of
another.
How useful is the DCA? The experimental re-
sults show that evert the simplistic DCA can
be useful when operating in conjunction with
small quantities of systematic linguistic knowl-
edge. Syntactic analyses projected from English
to Chinese can, in principle, yield Chinese analy-
ses that are nearly 70% accurate (in terms of un-
labeled dependencies) after application of a set
of linguistically principled rules. In the near fu-
ture we will address the remaining errors, which
also seem to be amenable to a uniform linguis-
tic treatment: in large part they involve differ-
ences in category expression (nominal expres-
sions translated as verbs or vice versa) and we
believe that we can use context to effect the cor-
rect category transformations. We will also ex-
plore correction of errors via statistical learning
t echni goes .
The implication of this work for statistical
translation modeling is that a little bit of knowl-
edge can be a good thing. The approach de-
scribed here strikes a balance somewhere be-
tween the endless construction-by-construction
tuning of rule-based approaches, ort the one
hand, and, ort the other, the development of in-
sufficiently constrained stochastic models.
We have systematically diagnosed a common
assumption that has been dealt with previously
ort a case by case basis, but not named. Most
of the models we know of from early work at
IBM to second-generation models such as that of
Knight and Yamada rectify glaring problems
caused by the failure of the DCA using a range
of pre- or post-processing techniques.
We have identified the source for a host of
these problems and have suggested diagnostics
for future cases where we might expect these
problems to arise. More important, we have
shown that linguistically informed strategies can
be developed efficiently to improve output that
is otherwise compromised by situations where
the DCA does not hold.
In addition to resolving the remaining prob-
lematic cases for our projection framework, we
are exploring ways to automatically create large
quantities of syntactically annotated data. This
will break the bottleneck in developing appro-
priately annotated training corpora. Currently,
we are following two research directions. Our
first goal is to minimize the degree of degrada-
tion in the quality of the projected trees when
the input analyses and word alignments are au-
tomatically generated by a statistical parser and
word alignment model. To improve the quality
of the input analyses, we are adapting active
learning and co-training techniques (Hwa, 2000;
Sarkar, 2001) to exploit the most reliable data.
We are also actively developing art alternative
alignment model that makes more use of the
syntactic structure (Lopez et al., 2002). Our
second goal is to detect and reduce the noise
in the projected trees so that they might re-
place the expensive human-annotated corpora
as training examples for statistical parsers. We
are investigating the use of filtering strategies to
localize the potentially problematic parts of the
projected syntactic trees.
</bodyText>
<sectionHeader confidence="0.996108" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998877454545454">
This work has been supported, in part, by ONR
MUM Contract FCP0.810548265, NSA RD-
02-5700, DARPA/ITO Cooperative Agreement
N660010028910, and Mitre Contract 010418-
7712. The authors would like to thank Edward
Hung, Gina Levow, and Lingling Zhang for their
assistance as annotators; Michael Collins for the
use of his parser; Franz Josef Och for his help
with GIZA++; and Lillian Lee, the students
of CM5C828, and the anonymous reviewers for
their comments ort this paper.
</bodyText>
<sectionHeader confidence="0.997771" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993917921052632">
Anne Abeille, Kathleen Bishop, Sharon Cote, and
Yves Schabes. 1990. A lexicalized tree adjoining
grammar for English. Technical Report MS-CIS-
90-24, University of Pennsylvania.
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou-
glas. 2000. Learning dependency transduction
models as collections of finite state head trans-
ducers. Computational Linguistics, 26(1).
Peter F. Brown, John Cocke, Stephen A. DellaPietra,
Vincent J. DellaPietra, Frederick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine trans-
lation. Computational Linguistics, 16(2):79-85,
June.
Peter F. Brown, Stephen A. DellaPietra, Vincent J.
DellaPietra, and Robert L. Mercer. 1993. The
mathematics of machine translation: Parameter
estimation. Computational Linguistics.
Clara Cabezas, Bonnie Dorr, and Philip Resnik.
2001. Spanish language processing at university of
maryland: Building infrastructure for multilingual
applications. In Proceedings of the Second Inter-
national Workshop on Spanish Language Process-
ing and Language Technologies (SLPLT-2).
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proc. of the 39th Meeting of
the ACL.
Ciprian Chelba and Fredrick Jelinek. 1998. Ex-
ploiting syntactic structure for language modeling.
In Proceedings of COLING-ACL, volume 1, pages
225-231.
Noam Chomsky. 1981. Lectures on Government and
Binding. Foris.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of The ACL, pages 16-
23, Madrid, Spain.
Bonnie J. Dorr. 1994. Machine Translation Diver-
gences: A Formal Description and Proposed Solu-
tion. Computational Linguistics, 20(4):597-633.
Jason Eisner. 1997. Bilexical grammars and a cubic-
time probabilistic parser. In Proceedings of the
International Workshop on Parsing Technologies.
Chung-Hye Han, Benoi Lavoi, Martha Palmer, Owen
Rambow, Richard Kittredge, Tanya Korelsky,
Nan i Kim, and Myunghee Kim. 2000. Handling
structural divergences and recovering dropped ar-
guments in a Korean/English machine translation
system. In Proceedings of The AMTA.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational cor-
respondence using annotation projection. Techni-
cal report, University of Maryland.
Rebecca Hwa. 2000. Sample selection for statistical
grammar induction. In Proceedings of The 2000
Joint SIGDAT Conference on EMNLP and VLC,
pages 45-52, Hong Kong, China, October.
Benoit Lavoie, Michael White, and Tanya Korelsky.
2001. Including Lexico-Structural Transfer Rules
from Parsed Bi-texts. In Proceedings of the 39th
Annual Meeting of the Association for Computa-
tional Linguistics — DDMT Workshop, Toulouse,
France.
Dekang Lin. 1998. Dependency-Based Evaluation
of MINIPAR. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, First Inter-
national Conference on Language Resources and
Evaluation, Granada, Spain, May.
Adam Lopez, Michael Nossal, Rebecca Hwa, and
Philip Resnik. 2002. Word-level alignment for
multilingual resource acquisition. In Proceedings
of the Workshop on Linguistic Knowledge Acqui-
sition and Representation: Bootstrapping Anno-
tated Language Data. To appear.
I. Dan Melamed. 1998. Annotation style guide for
the blinker project. Technical Report IRCS 98-06,
University of Pennsylvania.
Arul Menezes and Stephen D. Richardson. 2001. A
best-first alignment algorithm for automatic ex-
traction of transfer mappings from bilingual cor-
pora. In Proceedings of the 39th Annual Meeting
of the Association for Computational Linguistics
— DDMT Workshop, Toulouse, France.
Anoop Sarkar. 2001. Applying co-training methods
to statistical parsing. In Proc. of NAACL, June.
Stuart Shieber. 1994. Restricting the weak-
generative capacity of synchronous tree-
adjoining grammars. Computational Intelligence,
10(4):371-385, November.
C. J. Van Rijsbergen. 1979. Information Retrieval.
Butterworth.
Dekai Wu. 1995. Stochastic inversion transduc-
tion grammars, with application to segmentation,
bracketing, and alignment of parallel corpora. In
Proc. of the 14th Intl. Joint Conf. on Artificial
Intelligence, pages 1328-1335, Aug.
Fei Xia and Martha Palmer. 2001. Converting de-
pendency structures to phrase structures. In Proc.
of the HLT Conference, March.
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Ocurowski, John Kovarik, Fu-Dong Chiou, Shizhe
Huang, Tony Kroch, and Mitch Marcus. 2000.
Developing guidelines and ensuring consistency for
chinese text annotation. In Proceedings of the Sec-
ond Language Resources and Evaluation Confer-
ence, June.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proc. of The
Conference of The Association for Computational
Linguistics, pages 523-529.
David Yarowsky and Grace Ngai. 2001. Inducing
multilingual pos taggers and np bracketers via ro-
bust projection across aligned corpora. In Proc.
of NAACL-2001, pages 200-207.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.820985428571429">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp. 392-399. lit Section 4, we consider the implications of our experimental results and discuss future work. 2 The Direct Correspondence Assumption To our knowledge, the direct correspondence as-</note>
<abstract confidence="0.998312479919679">sumption underlies all statistical models that attempt to capture a relationship between syntactic structures in two languages, be they constituent models or dependency models. As art example of the former, consider Wu&apos;s (1995) stochastic inversion transduction grammar (SITG), in which paired sentences are simultaneously generated using context-free rules; word order differences are accounted for by allowing each rule to be read in a left-toright or right-to-left fashion, depending ort the language. For example, SITG can generate verb initial (English) and verb final (Japanese) verb phrases using the same rule VP V NP. For arty derivation using this rule, if VE and NPE are the English verb and noun phrase, and they are respectively aligned with Japanese verb and noun phrase NP j, then NPE) j) mug be true. As art example where the DCA relates dependency structures, consider the hierarchical alignment algorithm proposed by Alshawi et al. (2000). In this framework, wordalignments and paired dependency structures are constructed simultaneously. The English-Basque example (1) illustrates: if the English word buy is aligned to the Basque word aligned to creation of the head-modifier relationship between buy accompanied by the creation of a corresponding head-modifier relationship between (1) a. I got a gift for my brother Nik (I) lire (BROTHER- (Vali (GIFT) (Buy) 2.1 Formalizing the DCA Let us formalize this intuitive idea about corresponding syntactic relationships in the following more general way: Direct Correspondence Assumption a pair of sentences that are (literal) translations of each other with structures nodes YEof aligned with nodes YFof and if syntactic E, YE)holds in F , YF)holds in , may specify a head-modifier relationship between words in a dependency tree, or a sisterhood relationship between nonterminals in a constituency tree. As stated, the DCA amounts to art assumption that the crosslanguage alignment resembles a Itomomorphism the syntactic graph of the syntactic of Wu&apos;s SITG makes this assumption, under the that the head-modifier reexpressed in a rewrite rule. The (Brown et al., 1993) do not respect the DCA, but neither do they attempt to model arty higher level syntactic relationship between constituents within or across languages the translation model (alignments) and the language model are statistically independent. In Yamada and Knight&apos;s (2001) extension of the on the other hand, grammatical information from the source language is propagated into the noisy channel, and the grammatical transformations in their channel model to respect direct The simultaneous parsing and alignment algorithm of Alshawi et al. (2000) is essentially art impleof the DCA in which relationship has no linguistic import (i.e. anything can be a head). models embody a stronger version of the DCA that more closely resembles an isomorphism between dependency graphs(Shieber, 1994), though we will not pursue this idea further here. 3Knight and Yamada actually pre-process the English input in cases that most transparently violate direct correspondence; for example, they permute English verbs to sentence-final position in the model transforming English into Japanese. Most models we looked at have addressed some effects of DCA failure, but they have not acknowledged it explicitly as an underlying assumption, nor have they gone beyond expedient measures to the type of principled analysis that we propose below. xEng YEng xBsq YBsq verb-subj got I erosi nik verb-obj got gift erosi opari noun-det gift a opari bat noun-mod brother my anaiari nire Table 1: Correspondences preserved in (1) 2.2 Problems with the DCA The DCA seems to be a reasonable principle, especially when expressed in terms of syntactic dependencies that abstract away word order. That is, the thematic (who-did-what-to-whom) relationships are likely to hold true across translations evert for typologically different languages. Consider example (1) again: despite the fact that the Basque sentence has a different word order, with the verb appearing at the far right of the sentence, the syntactic dependency relationships of English (subject, object, noun modifier, etc.) are largely preserved across the alignment, as illustrated in Table 1. Moreover, the DCA makes possible more elegant formalisms (e.g. SITG) and more efficient algorithms It may allow us to use the syntactic analysis for one language to infer annotations for the corresponding sentence in another language, helping to reduce the labor and expense of creating treein new languages (Cabezas et al., Yarowsky and Ngai, 2001). Unfortunately, the DCA is flawed, even for literal translations. For example, in sentence pair (1), the indirect object of the verb is expressed in English using a prepositional phrase by the word attaches to the verb, but it is expressed with the dative marking ort If we aligned both anaiari, a many-to-one mapping would be formed, and the DCA would be violated: in the English tree but not hold in the Basque tree. Similarly, a one-to-many mapping (e.g., and in this example) can also be problematic for the DCA. The inadequacy of the DCA should come as no surprise. The syntax literature dating back to Cltomsky (1981), together with a rich computational literature on translation divergences (e.g. (Abeille et al., 1990; Dorr, 1994; Han et al., 2000)), is concerned with characterizing in a systematic way the apparent diversity of mechanisms used by languages to express meanings syntactically. For example, current theories claim that languages employ stable headorders across construction types. English, the head of a phrase is uniformly to the left of modifying prepositional phrases, sentential complements, etc. In Chinese, verbal and prepositional phrases respect the English ordering but heads in the nominal system uniformly appear to the right. Systematic application of this sort of linguistic knowledge turns out to be the key in getting beyond the DCA&apos;s limitations. 3 Evaluating the DCA using Annotation Projection Thus far, we have argued that the DCA is a useful and widely assumed principle; at the same time we have illustrated that it is incapable of accounting for some well known and fundamental linguistic facts. Yet this is not art unfamiliar situation. For years, stochastic modeling of language has depended on the linguistically implausible assumptions underlying&apos;n-gram models, hidden Markov models, context-free grammars, and the like, with remarkable success. Having made the DCA explicit, we would sugthat the right questions are: what extent true, and useful is when it holds? In the remainder of the paper, we focus on answering the first question empirically by considering the syntactic relationships and alignments between translated sentence pairs in two distant languages (English and Chinese). In our experimental framework, a system is given the &amp;quot;ideal&amp;quot; syntactic analyses for the English sentences and English-Chinese word-alignments, and it uses a Direct Projection Algorithm (described below) to project the English syntactic annotations directly across to the Chinese sentences in accordance with the DCA. The resulting Chinese dependency analyses are then compared with an independently derived gold standard, enabling us to determine recall and precision figures for syntactic dependencies (cf. (Lin, 1998)) and to perform a qualitative error analysis. This error analysis led us to revise our projection approach, and the resulting linguistically informed projection improved significantly the ability to obtain accurate Chinese parses. This experimental framework for the first question is designed with art eye toward the second, concerning the usefulness of making the direct correspondence assumption. If the DCA holds true more often than not, then one might speculate that the projected syntactic structures could be useful as a treebank (albeit a noisy one) for training Chinese parsers, and could help more generally in overcoming the syntactic annotation bottleneck for languages other than English. 3.1 The Direct Projection Algorithm The DCA translates fairly directly into art algorithm for projecting English dependency analyses across to Chinese using word alignments as the bridge. More formally, given sentence pair F), English syntactic relations are projected for the following situations: one-to-one E E aligned with a E F is aligned with a F E F, if , rn E), con- , F). unaligned (English) EE not with arty word in create a empty word that for arty with a unique WE) F, nF) xE) one-to-many E E aligned with create a new empty that the parent set align to F instead. many-to-one E E all aligned to E F, delete all between (1 &lt; &lt; and for the head (denoted as ); moreif , a modifier of its own is decomposed into a two-step process: first perform one-to-many, then perform many-to-one. In the cases of unaligned Chinese words, they are left out of the projected syntactic tree. The asymmetry in the of and of the unaligned words for the two languages arises from the asymmetric nature of the projection. 3.2 Experimental Setup The corpus for this experiment was constructed by obtaining manual English translations for 124 Chinese newswire sentences (with 40 words less) contained in sections the Penn Chinese Treebank (Xia et al., 2000). The Chinese data in our set ranged from simple sentences to some complicated constructions such as complex relative clauses, multiple run-on clauses, embeddings, nominal constructions, etc. Average sentence length was 23.7 words. Parses for the English sentences were constructed by a process of automatic analysis followed by hand correction; output trees from a broad-coverage lexicalized English parser (Collins, 1997) were automatically converted into dependencies to be corrected. The goldstandard dependency analyses for the Chinese sentences were constructed manually by two fluent speakers of Chinese, working independently and using the Chinese Treebank&apos;s (manually constituency parses for Inter-annotator agreement ort unlabeled syntactic dependencies is 92.4%. Manual English- Chinese alignments were constructed by two annotators who are native speakers of Chinese using a software environment similar to that described by Melamed (1998). The direct projection of English dependencies to Chinese yielded poor results as measured by precision and recall over unlabeled syntactic dependencies: precision was 30.1% and recall 39.1%. Inspection of the results revealed that our manually aligned parallel corpus contained many instances of multiply aligned or unaligned tokens, owing either to freeness of translation author of this paper served as one of the annotators. (a violation of the assumption that translations are literal) or to differences in how the two languages express the same meaning. For example, to quantify a Chinese noun with a determiner, one also needs to supply a measure word in addition to the quantity. Thus, the noun phrase apple expressed as (AN) ge (-mEAs) also includes separate words to indicate aspectual categories such as continued action, in contrast to verbal sufin English such as the Because Chinese classifiers, aspectual particles, and other functional words do not appear in the English sentence, there is no way for a projected English analysis to correctly account for them. As a result, the Chinese dependency trees usually fail to contain an appropriate grammatical relation for these items. Because they are frequent, the failure to properly account for them significantly hurts performance. 3.3 Revised Projection Our error analysis led to the conclusion that the correspondence of syntactic relationships would improved by a better handling of the one-toand the We investigated two ways of addressing this issue. First, we adopted a simple strategy informed by the tendency of languages to have a consistent direction for &amp;quot;headedness&amp;quot;. Chinese and English share the property that they are headinitial for most phrase types. Thus, if an English aligns to multiple Chinese words , leftmost word is treated as the head and ..., are analyzed as its dependents. If a Chinese empty node was introduced to align with an untranslated English word, it is deleted and its left-most child is promoted to replace it. Looking at language in this non-constructiondependent way allows us to make simple changes that have wide ranging effects. This is illustrative of how our approach tries to rein in cases where the DCA breaks down by using linguistically informed constraints that are as general as possible. Second, we used more detailed linguistic knowledge of Chinese to develop a small set of rules, expressed in a tree-based pattern-action formalism, that perform local modifications of a projected analysis on the Chinese side. To avoid the slippery slope of unending language-specific rule tweaking, we strictly constrained the possible rules. Rules were permitted to refer only to closed class items, to parts of speech projected from the English analysis, or to easily enumerlexical categories (e.g. $, For example, one such rule deals with noun modification: If a set of Chinese words aligned to an English noun, replace the empty node introduced in the Direct Projection Algorithm by promoting the last its place with as dependents. Another deals with aspectual markers for verbs: If vk, a sequence of Chinese words aligned with English verbs, is followed by aspect marker, make a modifier of the last verb vk. The most involved transformation places a linguistic constraint on the Chinese functional may be translated as of a relative clause), as the preposition as marker for possessives). This common Chinese functional word is almost always either unaligned or multiply aligned to an English word. If the Chinese word that appeared imto the left of is the Chinese word that appeared immediately to the of it, then find the lowest ancestors for such that remove that relationship; replace it with de). The latter two changes may seem unrelated, but they both take advantage of the fact that Chinese violates the head-initial rule in its nominal system, where noun phrases are uniformly head-final. More generally, the majority of rule patterns are variations on the same solution to the same problem. Viewing the problem from a higher level of linguistic abstraction made it possible to find all the relevant cases in a short time (a few days) and express the solution compactly (&lt; 20 rules). The complete set of rules can be found in (Hwa et al., 2002). 3.4 A New Experiment Because our error analysis and subsequent algorithm refinements made use of our original Chinese-English data set, we created a new test set based on 88 new Chinese sentences from the Penn Chinese Treebank, already manually translated into English as part of the NIST MT These sentences averaged in length. As described above, parses ort the English side were created semi-automatically, and word alignments were acquired manually. However, in order to reduce our reliance ort linguistically sophisticated human annotators for Chinese syntax, we adopted art alternative strategy for obtaining the gold standard: we automatically converted the Treebank&apos;s constituency parses of the Chinese sentences into syntactic dependency representations, using art algorithm similar to the one described in Section 2 of the paper by and Palmer The recall and precision figures for the new experiment are summarized in Table 2. The first row of the table shows the results comparing the output of the Direct Projection Algorithm with the gold standard. As we have already seen previously, the quality of these trees is not very good. The second row of the table shows that after applying the single transformation based ort the head-initial assumption, precision and recall both improve significantly: using the F-measure to combine precision and recall into a single figure of merit (Van Rijsbergen, 1979), the increase 5See http://www.nist.govispeechitestsimti. We used sentences from sections 038, 039, 067, 122, 191, 207, 249 because, according to the distributor, the translation of these sections (files with .spc suffix) have been more carefully verified. strategy was validated by performing the same process on the original data set; the agreement rate with the human-generated dependency trees was 97.5%. This led us to be confident that Treebank constituency parses could be used automatically to create a gold standard for syntactic dependencies. Method Precision Recall F-measure Direct 34.5 42.5 38.1 Head-initial 59.4 59.4 59.4 Rules 68.0 66.6 67.3 Table 2: Performance ort Chinese analyses (Vo) from 38.1% to 59.4% represents a 55.9% relative improvement. The third row of the table shows that by applying the small set of tree modification rules after direct projection (one of which is default assignment of the head-initial analysis to multi-word phrases when no other rule applies), we obtain art evert larger improvement, the 67.3% F-measure representing a 76.6% relative gain over baseline performance. 4 Conclusions and Future Work To what extent is the DCA a valid assumption? Our experiments confirm the linguistic intuition, indicating that one cannot safely assume a direct mapping between the syntactic dependencies of one language and the syntactic dependencies of another. How useful is the DCA? The experimental results show that evert the simplistic DCA can be useful when operating in conjunction with small quantities of systematic linguistic knowledge. Syntactic analyses projected from English to Chinese can, in principle, yield Chinese analyses that are nearly 70% accurate (in terms of unlabeled dependencies) after application of a set of linguistically principled rules. In the near future we will address the remaining errors, which also seem to be amenable to a uniform linguistic treatment: in large part they involve differences in category expression (nominal expressions translated as verbs or vice versa) and we believe that we can use context to effect the correct category transformations. We will also explore correction of errors via statistical learning t echni goes . The implication of this work for statistical translation modeling is that a little bit of knowledge can be a good thing. The approach described here strikes a balance somewhere between the endless construction-by-construction tuning of rule-based approaches, ort the one hand, and, ort the other, the development of insufficiently constrained stochastic models. We have systematically diagnosed a common assumption that has been dealt with previously ort a case by case basis, but not named. Most of the models we know of from early work at IBM to second-generation models such as that of Knight and Yamada rectify glaring problems caused by the failure of the DCA using a range of preor post-processing techniques. We have identified the source for a host of these problems and have suggested diagnostics for future cases where we might expect these problems to arise. More important, we have shown that linguistically informed strategies can be developed efficiently to improve output that is otherwise compromised by situations where the DCA does not hold. In addition to resolving the remaining problematic cases for our projection framework, we are exploring ways to automatically create large quantities of syntactically annotated data. This will break the bottleneck in developing appropriately annotated training corpora. Currently, we are following two research directions. Our first goal is to minimize the degree of degradation in the quality of the projected trees when the input analyses and word alignments are automatically generated by a statistical parser and word alignment model. To improve the quality of the input analyses, we are adapting active learning and co-training techniques (Hwa, 2000; Sarkar, 2001) to exploit the most reliable data. We are also actively developing art alternative alignment model that makes more use of the syntactic structure (Lopez et al., 2002). Our second goal is to detect and reduce the noise in the projected trees so that they might replace the expensive human-annotated corpora as training examples for statistical parsers. We are investigating the use of filtering strategies to localize the potentially problematic parts of the projected syntactic trees.</abstract>
<note confidence="0.8731725">Acknowledgments This work has been supported, in part, by ONR Contract FCP0.810548265, NSA RD- 02-5700, DARPA/ITO Cooperative Agreement N660010028910, and Mitre Contract 010418- 7712. The authors would like to thank Edward</note>
<abstract confidence="0.881730071428571">Hung, Gina Levow, and Lingling Zhang for their assistance as annotators; Michael Collins for the use of his parser; Franz Josef Och for his help with GIZA++; and Lillian Lee, the students of CM5C828, and the anonymous reviewers for their comments ort this paper. References Anne Abeille, Kathleen Bishop, Sharon Cote, and Yves Schabes. 1990. A lexicalized tree adjoining grammar for English. Technical Report MS-CIS- 90-24, University of Pennsylvania. Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 2000. Learning dependency transduction models as collections of finite state head trans-</abstract>
<title confidence="0.442768">Linguistics,</title>
<author confidence="0.9678005">Peter F Brown</author>
<author confidence="0.9678005">John Cocke</author>
<author confidence="0.9678005">Stephen A DellaPietra</author>
<author confidence="0.9678005">Vincent J DellaPietra</author>
<author confidence="0.9678005">Frederick Jelinek</author>
<author confidence="0.9678005">D John</author>
<note confidence="0.9338135">Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine trans- Linguistics, June. Peter F. Brown, Stephen A. DellaPietra, Vincent J. DellaPietra, and Robert L. Mercer. 1993. The mathematics of machine translation: Parameter Linguistics. Clara Cabezas, Bonnie Dorr, and Philip Resnik. 2001. Spanish language processing at university of</note>
<abstract confidence="0.749487888888889">maryland: Building infrastructure for multilingual In of the Second International Workshop on Spanish Language Processing and Language Technologies (SLPLT-2). Eugene Charniak. 2001. Immediate-head parsing for models. In of the 39th Meeting of the ACL. Ciprian Chelba and Fredrick Jelinek. 1998. Exploiting syntactic structure for language modeling.</abstract>
<note confidence="0.59466503030303">of COLING-ACL, 1, pages 225-231. Chomsky. 1981. on Government and Michael Collins. 1997. Three generative, lexicalised for statistical parsing. In of 35th Annual Meeting of The ACL, 16- 23, Madrid, Spain. Bonnie J. Dorr. 1994. Machine Translation Divergences: A Formal Description and Proposed Solu- Linguistics, Jason Eisner. 1997. Bilexical grammars and a cubicprobabilistic parser. In of the International Workshop on Parsing Technologies. Chung-Hye Han, Benoi Lavoi, Martha Palmer, Owen Rambow, Richard Kittredge, Tanya Korelsky, Nan i Kim, and Myunghee Kim. 2000. Handling structural divergences and recovering dropped arguments in a Korean/English machine translation In of The AMTA. Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan Kolak. 2002. Evaluating translational correspondence using annotation projection. Technical report, University of Maryland. Rebecca Hwa. 2000. Sample selection for statistical induction. In of The 2000 Joint SIGDAT Conference on EMNLP and VLC, pages 45-52, Hong Kong, China, October. Benoit Lavoie, Michael White, and Tanya Korelsky. 2001. Including Lexico-Structural Transfer Rules Parsed Bi-texts. In of the 39th Annual Meeting of the Association for Computa- Linguistics — DDMT Workshop, France.</note>
<title confidence="0.632468">Dekang Lin. 1998. Dependency-Based Evaluation</title>
<author confidence="0.693336">In of the Workshop on</author>
<affiliation confidence="0.556372333333333">the Evaluation of Parsing Systems, First International Conference on Language Resources and Spain, May.</affiliation>
<address confidence="0.328272">Adam Lopez, Michael Nossal, Rebecca Hwa, and</address>
<author confidence="0.752176">Word-level alignment for</author>
<abstract confidence="0.902305777777778">resource acquisition. In of the Workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Anno- Language Data. appear. I. Dan Melamed. 1998. Annotation style guide for the blinker project. Technical Report IRCS 98-06, University of Pennsylvania. Arul Menezes and Stephen D. Richardson. 2001. A best-first alignment algorithm for automatic extraction of transfer mappings from bilingual cor- In of the 39th Annual Meeting of the Association for Computational Linguistics DDMT Workshop, France. Anoop Sarkar. 2001. Applying co-training methods statistical parsing. In of NAACL, Shieber. 1994. Restricting the weakgenerative capacity of synchronous treegrammars. Intelligence,</abstract>
<address confidence="0.555873">10(4):371-385, November.</address>
<note confidence="0.909313769230769">J. Van Rijsbergen. 1979. Retrieval. Butterworth. Dekai Wu. 1995. Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora. In Proc. of the 14th Intl. Joint Conf. on Artificial 1328-1335, Aug. Fei Xia and Martha Palmer. 2001. Converting destructures to phrase structures. In the HLT Conference, Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen Ocurowski, John Kovarik, Fu-Dong Chiou, Shizhe Huang, Tony Kroch, and Mitch Marcus. 2000.</note>
<title confidence="0.733583666666667">Developing guidelines and ensuring consistency for text annotation. In of the Second Language Resources and Evaluation Confer-</title>
<author confidence="0.628731">A syntax-</author>
<affiliation confidence="0.5661465">statistical translation model. In of The Conference of The Association for Computational</affiliation>
<note confidence="0.8440042">523-529. David Yarowsky and Grace Ngai. 2001. Inducing multilingual pos taggers and np bracketers via roprojection across aligned corpora. In NAACL-2001, 200-207.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeille</author>
<author>Kathleen Bishop</author>
<author>Sharon Cote</author>
<author>Yves Schabes</author>
</authors>
<title>A lexicalized tree adjoining grammar for English.</title>
<date>1990</date>
<tech>Technical Report MS-CIS90-24,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="6264" citStr="Abeille et al., 1990" startWordPosition="1011" endWordPosition="1014">case marking ort anaiari (BROTHER-DAT) it Basque. If we aligned both for and brother to anaiari, then a many-to-one mapping would be formed, and the DCA would be violated: R(f or, brother) holds in the English tree but R(analari,analari) does not hold in the Basque tree. Similarly, a one-to-many mapping (e.g., aligning got with erosi (Buy) and nion (PAST) in this example) can also be problematic for the DCA. The inadequacy of the DCA should come as no surprise. The syntax literature dating back to Cltomsky (1981), together with a rich computational literature on translation divergences (e.g. (Abeille et al., 1990; Dorr, 1994; Han et al., 2000)), is concerned with characterizing in a systematic way the apparent diversity of mechanisms used by languages to express meanings syntactically. For example, current theories claim that languages employ stable headcomplement orders across construction types. In English, the head of a phrase is uniformly to the left of modifying prepositional phrases, sentential complements, etc. In Chinese, verbal and prepositional phrases respect the English ordering but heads in the nominal system uniformly appear to the right. Systematic application of this sort of linguistic</context>
</contexts>
<marker>Abeille, Bishop, Cote, Schabes, 1990</marker>
<rawString>Anne Abeille, Kathleen Bishop, Sharon Cote, and Yves Schabes. 1990. A lexicalized tree adjoining grammar for English. Technical Report MS-CIS90-24, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Srinivas Bangalore</author>
<author>Shona Douglas</author>
</authors>
<title>Learning dependency transduction models as collections of finite state head transducers.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="1324" citStr="Alshawi et al. (2000)" startWordPosition="206" endWordPosition="209">counted for by allowing each rule to be read in a left-toright or right-to-left fashion, depending ort the language. For example, SITG can generate verb initial (English) and verb final (Japanese) verb phrases using the same rule VP V NP. For arty derivation using this rule, if VE and NPE are the English verb and noun phrase, and they are respectively aligned with Japanese verb and noun phrase v j and NP j, then VERB-OBJECT(VE, NPE) and VERB-OBJECT(V j, NP j) mug both be true. As art example where the DCA relates dependency structures, consider the hierarchical alignment algorithm proposed by Alshawi et al. (2000). In this framework, wordlevel alignments and paired dependency structures are constructed simultaneously. The English-Basque example (1) illustrates: if the English word buy is aligned to the Basque word erosi and gift is aligned to opari, the creation of the head-modifier relationship between buy and gift is accompanied by the creation of a corresponding head-modifier relationship between erosi and opari. (1) a. I got a gift for my brother b. Nik (I) lire (MY) anaiari (BROTHERDAT) (Vali (GIFT) bat (A) erosi (Buy) nion (PAST) 2.1 Formalizing the DCA Let us formalize this intuitive idea about </context>
<context position="3409" citStr="Alshawi et al. (2000)" startWordPosition="545" endWordPosition="548">le. The IBM MT models (Brown et al., 1993) do not respect the DCA, but neither do they attempt to model arty higher level syntactic relationship between constituents within or across languages the translation model (alignments) and the language model are statistically independent. In Yamada and Knight&apos;s (2001) extension of the IBM models, on the other hand, grammatical information from the source language is propagated into the noisy channel, and the grammatical transformations in their channel model appear to respect direct correspondence.3 The simultaneous parsing and alignment algorithm of Alshawi et al. (2000) is essentially art implementation of the DCA in which relationship R has no linguistic import (i.e. anything can be a head). 2Some models embody a stronger version of the DCA that more closely resembles an isomorphism between dependency graphs(Shieber, 1994), though we will not pursue this idea further here. 3 Knight and Yamada actually pre-process the English input in cases that most transparently violate direct correspondence; for example, they permute English verbs to sentence-final position in the model transforming English into Japanese. Most models we looked at have addressed some effec</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 2000. Learning dependency transduction models as collections of finite state head transducers. Computational Linguistics, 26(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A DellaPietra</author>
<author>Vincent J DellaPietra</author>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--2</pages>
<marker>Brown, Cocke, DellaPietra, DellaPietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F. Brown, John Cocke, Stephen A. DellaPietra, Vincent J. DellaPietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79-85, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A DellaPietra</author>
<author>Vincent J DellaPietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="2830" citStr="Brown et al., 1993" startWordPosition="456" endWordPosition="459">F and YF of TreeF, respectively, and if syntactic relationship R(x E, YE) holds in TreeE, then R(x F , YF) holds in TreeF. Here, R(x , y) may specify a head-modifier relationship between words in a dependency tree, or a sisterhood relationship between nonterminals in a constituency tree. As stated, the DCA amounts to art assumption that the crosslanguage alignment resembles a Itomomorphism relating the syntactic graph of E to the syntactic graph of F.2 Wu&apos;s SITG makes this assumption, under the interpretation that R is the head-modifier relation expressed in a rewrite rule. The IBM MT models (Brown et al., 1993) do not respect the DCA, but neither do they attempt to model arty higher level syntactic relationship between constituents within or across languages the translation model (alignments) and the language model are statistically independent. In Yamada and Knight&apos;s (2001) extension of the IBM models, on the other hand, grammatical information from the source language is propagated into the noisy channel, and the grammatical transformations in their channel model appear to respect direct correspondence.3 The simultaneous parsing and alignment algorithm of Alshawi et al. (2000) is essentially art i</context>
</contexts>
<marker>Brown, DellaPietra, DellaPietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. DellaPietra, Vincent J. DellaPietra, and Robert L. Mercer. 1993. The mathematics of machine translation: Parameter estimation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clara Cabezas</author>
<author>Bonnie Dorr</author>
<author>Philip Resnik</author>
</authors>
<title>Spanish language processing at university of maryland: Building infrastructure for multilingual applications.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second International Workshop on Spanish Language Processing and Language Technologies (SLPLT-2).</booktitle>
<contexts>
<context position="5343" citStr="Cabezas et al., 2001" startWordPosition="858" endWordPosition="861">pite the fact that the Basque sentence has a different word order, with the verb appearing at the far right of the sentence, the syntactic dependency relationships of English (subject, object, noun modifier, etc.) are largely preserved across the alignment, as illustrated in Table 1. Moreover, the DCA makes possible more elegant formalisms (e.g. SITG) and more efficient algorithms It may allow us to use the syntactic analysis for one language to infer annotations for the corresponding sentence in another language, helping to reduce the labor and expense of creating treebanks in new languages (Cabezas et al., 2001; Yarowsky and Ngai, 2001). Unfortunately, the DCA is flawed, even for literal translations. For example, in sentence pair (1), the indirect object of the verb is expressed in English using a prepositional phrase (headed by the word for) that attaches to the verb, but it is expressed with the dative case marking ort anaiari (BROTHER-DAT) it Basque. If we aligned both for and brother to anaiari, then a many-to-one mapping would be formed, and the DCA would be violated: R(f or, brother) holds in the English tree but R(analari,analari) does not hold in the Basque tree. Similarly, a one-to-many ma</context>
</contexts>
<marker>Cabezas, Dorr, Resnik, 2001</marker>
<rawString>Clara Cabezas, Bonnie Dorr, and Philip Resnik. 2001. Spanish language processing at university of maryland: Building infrastructure for multilingual applications. In Proceedings of the Second International Workshop on Spanish Language Processing and Language Technologies (SLPLT-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proc. of the 39th Meeting of the ACL.</booktitle>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proc. of the 39th Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Fredrick Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<volume>1</volume>
<pages>225--231</pages>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>Ciprian Chelba and Fredrick Jelinek. 1998. Exploiting syntactic structure for language modeling. In Proceedings of COLING-ACL, volume 1, pages 225-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding. Foris.</booktitle>
<marker>Chomsky, 1981</marker>
<rawString>Noam Chomsky. 1981. Lectures on Government and Binding. Foris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of The ACL,</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="11126" citStr="Collins, 1997" startWordPosition="1820" endWordPosition="1821">s constructed by obtaining manual English translations for 124 Chinese newswire sentences (with 40 words or less) contained in sections 001-015 of the Penn Chinese Treebank (Xia et al., 2000). The Chinese data in our set ranged from simple sentences to some complicated constructions such as complex relative clauses, multiple run-on clauses, embeddings, nominal constructions, etc. Average sentence length was 23.7 words. Parses for the English sentences were constructed by a process of automatic analysis followed by hand correction; output trees from a broad-coverage lexicalized English parser (Collins, 1997) were automatically converted into dependencies to be corrected. The goldstandard dependency analyses for the Chinese sentences were constructed manually by two fluent speakers of Chinese, working independently and using the Chinese Treebank&apos;s (manually constructed) constituency parses for reference.4 Inter-annotator agreement ort unlabeled syntactic dependencies is 92.4%. Manual EnglishChinese alignments were constructed by two annotators who are native speakers of Chinese using a software environment similar to that described by Melamed (1998). The direct projection of English dependencies t</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of The ACL, pages 16-23, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
</authors>
<title>Machine Translation Divergences: A Formal Description and Proposed Solution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--4</pages>
<contexts>
<context position="6276" citStr="Dorr, 1994" startWordPosition="1015" endWordPosition="1016">ri (BROTHER-DAT) it Basque. If we aligned both for and brother to anaiari, then a many-to-one mapping would be formed, and the DCA would be violated: R(f or, brother) holds in the English tree but R(analari,analari) does not hold in the Basque tree. Similarly, a one-to-many mapping (e.g., aligning got with erosi (Buy) and nion (PAST) in this example) can also be problematic for the DCA. The inadequacy of the DCA should come as no surprise. The syntax literature dating back to Cltomsky (1981), together with a rich computational literature on translation divergences (e.g. (Abeille et al., 1990; Dorr, 1994; Han et al., 2000)), is concerned with characterizing in a systematic way the apparent diversity of mechanisms used by languages to express meanings syntactically. For example, current theories claim that languages employ stable headcomplement orders across construction types. In English, the head of a phrase is uniformly to the left of modifying prepositional phrases, sentential complements, etc. In Chinese, verbal and prepositional phrases respect the English ordering but heads in the nominal system uniformly appear to the right. Systematic application of this sort of linguistic knowledge t</context>
</contexts>
<marker>Dorr, 1994</marker>
<rawString>Bonnie J. Dorr. 1994. Machine Translation Divergences: A Formal Description and Proposed Solution. Computational Linguistics, 20(4):597-633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Bilexical grammars and a cubictime probabilistic parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies.</booktitle>
<marker>Eisner, 1997</marker>
<rawString>Jason Eisner. 1997. Bilexical grammars and a cubictime probabilistic parser. In Proceedings of the International Workshop on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung-Hye Han</author>
<author>Benoi Lavoi</author>
<author>Martha Palmer</author>
<author>Owen Rambow</author>
<author>Richard Kittredge</author>
<author>Tanya Korelsky</author>
<author>Nan i Kim</author>
<author>Myunghee Kim</author>
</authors>
<title>Handling structural divergences and recovering dropped arguments in a Korean/English machine translation system.</title>
<date>2000</date>
<booktitle>In Proceedings of The AMTA.</booktitle>
<contexts>
<context position="6295" citStr="Han et al., 2000" startWordPosition="1017" endWordPosition="1020">DAT) it Basque. If we aligned both for and brother to anaiari, then a many-to-one mapping would be formed, and the DCA would be violated: R(f or, brother) holds in the English tree but R(analari,analari) does not hold in the Basque tree. Similarly, a one-to-many mapping (e.g., aligning got with erosi (Buy) and nion (PAST) in this example) can also be problematic for the DCA. The inadequacy of the DCA should come as no surprise. The syntax literature dating back to Cltomsky (1981), together with a rich computational literature on translation divergences (e.g. (Abeille et al., 1990; Dorr, 1994; Han et al., 2000)), is concerned with characterizing in a systematic way the apparent diversity of mechanisms used by languages to express meanings syntactically. For example, current theories claim that languages employ stable headcomplement orders across construction types. In English, the head of a phrase is uniformly to the left of modifying prepositional phrases, sentential complements, etc. In Chinese, verbal and prepositional phrases respect the English ordering but heads in the nominal system uniformly appear to the right. Systematic application of this sort of linguistic knowledge turns out to be the </context>
</contexts>
<marker>Han, Lavoi, Palmer, Rambow, Kittredge, Korelsky, Kim, Kim, 2000</marker>
<rawString>Chung-Hye Han, Benoi Lavoi, Martha Palmer, Owen Rambow, Richard Kittredge, Tanya Korelsky, Nan i Kim, and Myunghee Kim. 2000. Handling structural divergences and recovering dropped arguments in a Korean/English machine translation system. In Proceedings of The AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Okan Kolak</author>
</authors>
<title>Evaluating translational correspondence using annotation projection.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>University of Maryland.</institution>
<contexts>
<context position="16289" citStr="Hwa et al., 2002" startWordPosition="2678" endWordPosition="2681">e that relationship; and replace it with R(de, cp) and R(cq, de). The latter two changes may seem unrelated, but they both take advantage of the fact that Chinese violates the head-initial rule in its nominal system, where noun phrases are uniformly head-final. More generally, the majority of rule patterns are variations on the same solution to the same problem. Viewing the problem from a higher level of linguistic abstraction made it possible to find all the relevant cases in a short time (a few days) and express the solution compactly (&lt; 20 rules). The complete set of rules can be found in (Hwa et al., 2002). 3.4 A New Experiment Because our error analysis and subsequent algorithm refinements made use of our original Chinese-English data set, we created a new test set based on 88 new Chinese sentences from the Penn Chinese Treebank, already manually translated into English as part of the NIST MT evaluation preview.5 These sentences averaged 19.0 words in length. As described above, parses ort the English side were created semi-automatically, and word alignments were acquired manually. However, in order to reduce our reliance ort linguistically sophisticated human annotators for Chinese syntax, we</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Kolak, 2002</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan Kolak. 2002. Evaluating translational correspondence using annotation projection. Technical report, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Sample selection for statistical grammar induction.</title>
<date>2000</date>
<booktitle>In Proceedings of The 2000 Joint SIGDAT Conference on EMNLP and VLC,</booktitle>
<pages>45--52</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="21687" citStr="Hwa, 2000" startWordPosition="3534" endWordPosition="3535">matic cases for our projection framework, we are exploring ways to automatically create large quantities of syntactically annotated data. This will break the bottleneck in developing appropriately annotated training corpora. Currently, we are following two research directions. Our first goal is to minimize the degree of degradation in the quality of the projected trees when the input analyses and word alignments are automatically generated by a statistical parser and word alignment model. To improve the quality of the input analyses, we are adapting active learning and co-training techniques (Hwa, 2000; Sarkar, 2001) to exploit the most reliable data. We are also actively developing art alternative alignment model that makes more use of the syntactic structure (Lopez et al., 2002). Our second goal is to detect and reduce the noise in the projected trees so that they might replace the expensive human-annotated corpora as training examples for statistical parsers. We are investigating the use of filtering strategies to localize the potentially problematic parts of the projected syntactic trees. Acknowledgments This work has been supported, in part, by ONR MUM Contract FCP0.810548265, NSA RD02</context>
</contexts>
<marker>Hwa, 2000</marker>
<rawString>Rebecca Hwa. 2000. Sample selection for statistical grammar induction. In Proceedings of The 2000 Joint SIGDAT Conference on EMNLP and VLC, pages 45-52, Hong Kong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Lavoie</author>
<author>Michael White</author>
<author>Tanya Korelsky</author>
</authors>
<title>Including Lexico-Structural Transfer Rules from Parsed Bi-texts.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics — DDMT Workshop,</booktitle>
<location>Toulouse, France.</location>
<marker>Lavoie, White, Korelsky, 2001</marker>
<rawString>Benoit Lavoie, Michael White, and Tanya Korelsky. 2001. Including Lexico-Structural Transfer Rules from Parsed Bi-texts. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics — DDMT Workshop, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-Based Evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on the Evaluation of Parsing Systems, First International Conference on Language Resources and Evaluation,</booktitle>
<location>Granada, Spain,</location>
<contexts>
<context position="8332" citStr="Lin, 1998" startWordPosition="1339" endWordPosition="1340">ignments between translated sentence pairs in two distant languages (English and Chinese). In our experimental framework, a system is given the &amp;quot;ideal&amp;quot; syntactic analyses for the English sentences and English-Chinese word-alignments, and it uses a Direct Projection Algorithm (described below) to project the English syntactic annotations directly across to the Chinese sentences in accordance with the DCA. The resulting Chinese dependency analyses are then compared with an independently derived gold standard, enabling us to determine recall and precision figures for syntactic dependencies (cf. (Lin, 1998)) and to perform a qualitative error analysis. This error analysis led us to revise our projection approach, and the resulting linguistically informed projection improved significantly the ability to obtain accurate Chinese parses. This experimental framework for the first question is designed with art eye toward the second, concerning the usefulness of making the direct correspondence assumption. If the DCA holds true more often than not, then one might speculate that the projected syntactic structures could be useful as a treebank (albeit a noisy one) for training Chinese parsers, and could </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-Based Evaluation of MINIPAR. In Proceedings of the Workshop on the Evaluation of Parsing Systems, First International Conference on Language Resources and Evaluation, Granada, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
<author>Michael Nossal</author>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
</authors>
<title>Word-level alignment for multilingual resource acquisition.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Linguistic Knowledge Acquisition</booktitle>
<note>To appear.</note>
<contexts>
<context position="21869" citStr="Lopez et al., 2002" startWordPosition="3561" endWordPosition="3564">developing appropriately annotated training corpora. Currently, we are following two research directions. Our first goal is to minimize the degree of degradation in the quality of the projected trees when the input analyses and word alignments are automatically generated by a statistical parser and word alignment model. To improve the quality of the input analyses, we are adapting active learning and co-training techniques (Hwa, 2000; Sarkar, 2001) to exploit the most reliable data. We are also actively developing art alternative alignment model that makes more use of the syntactic structure (Lopez et al., 2002). Our second goal is to detect and reduce the noise in the projected trees so that they might replace the expensive human-annotated corpora as training examples for statistical parsers. We are investigating the use of filtering strategies to localize the potentially problematic parts of the projected syntactic trees. Acknowledgments This work has been supported, in part, by ONR MUM Contract FCP0.810548265, NSA RD02-5700, DARPA/ITO Cooperative Agreement N660010028910, and Mitre Contract 010418- 7712. The authors would like to thank Edward Hung, Gina Levow, and Lingling Zhang for their assistanc</context>
</contexts>
<marker>Lopez, Nossal, Hwa, Resnik, 2002</marker>
<rawString>Adam Lopez, Michael Nossal, Rebecca Hwa, and Philip Resnik. 2002. Word-level alignment for multilingual resource acquisition. In Proceedings of the Workshop on Linguistic Knowledge Acquisition and Representation: Bootstrapping Annotated Language Data. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Annotation style guide for the blinker project.</title>
<date>1998</date>
<tech>Technical Report IRCS 98-06,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="11677" citStr="Melamed (1998)" startWordPosition="1898" endWordPosition="1899">rom a broad-coverage lexicalized English parser (Collins, 1997) were automatically converted into dependencies to be corrected. The goldstandard dependency analyses for the Chinese sentences were constructed manually by two fluent speakers of Chinese, working independently and using the Chinese Treebank&apos;s (manually constructed) constituency parses for reference.4 Inter-annotator agreement ort unlabeled syntactic dependencies is 92.4%. Manual EnglishChinese alignments were constructed by two annotators who are native speakers of Chinese using a software environment similar to that described by Melamed (1998). The direct projection of English dependencies to Chinese yielded poor results as measured by precision and recall over unlabeled syntactic dependencies: precision was 30.1% and recall 39.1%. Inspection of the results revealed that our manually aligned parallel corpus contained many instances of multiply aligned or unaligned tokens, owing either to freeness of translation 40ne author of this paper served as one of the annotators. (a violation of the assumption that translations are literal) or to differences in how the two languages express the same meaning. For example, to quantify a Chinese</context>
</contexts>
<marker>Melamed, 1998</marker>
<rawString>I. Dan Melamed. 1998. Annotation style guide for the blinker project. Technical Report IRCS 98-06, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arul Menezes</author>
<author>Stephen D Richardson</author>
</authors>
<title>A best-first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics — DDMT Workshop,</booktitle>
<location>Toulouse, France.</location>
<marker>Menezes, Richardson, 2001</marker>
<rawString>Arul Menezes and Stephen D. Richardson. 2001. A best-first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics — DDMT Workshop, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In Proc. of NAACL,</booktitle>
<contexts>
<context position="21702" citStr="Sarkar, 2001" startWordPosition="3536" endWordPosition="3537"> for our projection framework, we are exploring ways to automatically create large quantities of syntactically annotated data. This will break the bottleneck in developing appropriately annotated training corpora. Currently, we are following two research directions. Our first goal is to minimize the degree of degradation in the quality of the projected trees when the input analyses and word alignments are automatically generated by a statistical parser and word alignment model. To improve the quality of the input analyses, we are adapting active learning and co-training techniques (Hwa, 2000; Sarkar, 2001) to exploit the most reliable data. We are also actively developing art alternative alignment model that makes more use of the syntactic structure (Lopez et al., 2002). Our second goal is to detect and reduce the noise in the projected trees so that they might replace the expensive human-annotated corpora as training examples for statistical parsers. We are investigating the use of filtering strategies to localize the potentially problematic parts of the projected syntactic trees. Acknowledgments This work has been supported, in part, by ONR MUM Contract FCP0.810548265, NSA RD02-5700, DARPA/IT</context>
</contexts>
<marker>Sarkar, 2001</marker>
<rawString>Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proc. of NAACL, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>Restricting the weakgenerative capacity of synchronous treeadjoining grammars.</title>
<date>1994</date>
<journal>Computational Intelligence,</journal>
<pages>10--4</pages>
<contexts>
<context position="3668" citStr="Shieber, 1994" startWordPosition="589" endWordPosition="590">ly independent. In Yamada and Knight&apos;s (2001) extension of the IBM models, on the other hand, grammatical information from the source language is propagated into the noisy channel, and the grammatical transformations in their channel model appear to respect direct correspondence.3 The simultaneous parsing and alignment algorithm of Alshawi et al. (2000) is essentially art implementation of the DCA in which relationship R has no linguistic import (i.e. anything can be a head). 2Some models embody a stronger version of the DCA that more closely resembles an isomorphism between dependency graphs(Shieber, 1994), though we will not pursue this idea further here. 3 Knight and Yamada actually pre-process the English input in cases that most transparently violate direct correspondence; for example, they permute English verbs to sentence-final position in the model transforming English into Japanese. Most models we looked at have addressed some effects of DCA failure, but they have not acknowledged it explicitly as an underlying assumption, nor have they gone beyond expedient measures to the type of principled analysis that we propose below. xEng YEng xBsq YBsq verb-subj got I erosi nik verb-obj got gift</context>
</contexts>
<marker>Shieber, 1994</marker>
<rawString>Stuart Shieber. 1994. Restricting the weakgenerative capacity of synchronous treeadjoining grammars. Computational Intelligence, 10(4):371-385, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<location>Butterworth.</location>
<marker>Van Rijsbergen, 1979</marker>
<rawString>C. J. Van Rijsbergen. 1979. Information Retrieval. Butterworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora.</title>
<date>1995</date>
<booktitle>In Proc. of the 14th Intl. Joint Conf. on Artificial Intelligence,</booktitle>
<pages>1328--1335</pages>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. 1995. Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora. In Proc. of the 14th Intl. Joint Conf. on Artificial Intelligence, pages 1328-1335, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Martha Palmer</author>
</authors>
<title>Converting dependency structures to phrase structures.</title>
<date>2001</date>
<booktitle>In Proc. of the HLT Conference,</booktitle>
<contexts>
<context position="17185" citStr="Xia and Palmer (2001)" startWordPosition="2815" endWordPosition="2818">art of the NIST MT evaluation preview.5 These sentences averaged 19.0 words in length. As described above, parses ort the English side were created semi-automatically, and word alignments were acquired manually. However, in order to reduce our reliance ort linguistically sophisticated human annotators for Chinese syntax, we adopted art alternative strategy for obtaining the gold standard: we automatically converted the Treebank&apos;s constituency parses of the Chinese sentences into syntactic dependency representations, using art algorithm similar to the one described in Section 2 of the paper by Xia and Palmer (2001).6 The recall and precision figures for the new experiment are summarized in Table 2. The first row of the table shows the results comparing the output of the Direct Projection Algorithm with the gold standard. As we have already seen previously, the quality of these trees is not very good. The second row of the table shows that after applying the single transformation based ort the head-initial assumption, precision and recall both improve significantly: using the F-measure to combine precision and recall into a single figure of merit (Van Rijsbergen, 1979), the increase 5 See http://www.nist</context>
</contexts>
<marker>Xia, Palmer, 2001</marker>
<rawString>Fei Xia and Martha Palmer. 2001. Converting dependency structures to phrase structures. In Proc. of the HLT Conference, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Martha Palmer</author>
<author>Nianwen Xue</author>
<author>Mary Ellen Ocurowski</author>
<author>John Kovarik</author>
<author>Fu-Dong Chiou</author>
<author>Shizhe Huang</author>
<author>Tony Kroch</author>
<author>Mitch Marcus</author>
</authors>
<title>Developing guidelines and ensuring consistency for chinese text annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second Language Resources and Evaluation Conference,</booktitle>
<contexts>
<context position="10703" citStr="Xia et al., 2000" startWordPosition="1755" endWordPosition="1758">The many-to-many case is decomposed into a two-step process: first perform one-to-many, then perform many-to-one. In the cases of unaligned Chinese words, they are left out of the projected syntactic tree. The asymmetry in the treatment of one-to-many and many-to-one and of the unaligned words for the two languages arises from the asymmetric nature of the projection. 3.2 Experimental Setup The corpus for this experiment was constructed by obtaining manual English translations for 124 Chinese newswire sentences (with 40 words or less) contained in sections 001-015 of the Penn Chinese Treebank (Xia et al., 2000). The Chinese data in our set ranged from simple sentences to some complicated constructions such as complex relative clauses, multiple run-on clauses, embeddings, nominal constructions, etc. Average sentence length was 23.7 words. Parses for the English sentences were constructed by a process of automatic analysis followed by hand correction; output trees from a broad-coverage lexicalized English parser (Collins, 1997) were automatically converted into dependencies to be corrected. The goldstandard dependency analyses for the Chinese sentences were constructed manually by two fluent speakers </context>
</contexts>
<marker>Xia, Palmer, Xue, Ocurowski, Kovarik, Chiou, Huang, Kroch, Marcus, 2000</marker>
<rawString>Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen Ocurowski, John Kovarik, Fu-Dong Chiou, Shizhe Huang, Tony Kroch, and Mitch Marcus. 2000. Developing guidelines and ensuring consistency for chinese text annotation. In Proceedings of the Second Language Resources and Evaluation Conference, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. of The Conference of The Association for Computational Linguistics,</booktitle>
<pages>523--529</pages>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proc. of The Conference of The Association for Computational Linguistics, pages 523-529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora. In</title>
<date>2001</date>
<booktitle>Proc. of NAACL-2001,</booktitle>
<pages>200--207</pages>
<contexts>
<context position="5369" citStr="Yarowsky and Ngai, 2001" startWordPosition="862" endWordPosition="865"> Basque sentence has a different word order, with the verb appearing at the far right of the sentence, the syntactic dependency relationships of English (subject, object, noun modifier, etc.) are largely preserved across the alignment, as illustrated in Table 1. Moreover, the DCA makes possible more elegant formalisms (e.g. SITG) and more efficient algorithms It may allow us to use the syntactic analysis for one language to infer annotations for the corresponding sentence in another language, helping to reduce the labor and expense of creating treebanks in new languages (Cabezas et al., 2001; Yarowsky and Ngai, 2001). Unfortunately, the DCA is flawed, even for literal translations. For example, in sentence pair (1), the indirect object of the verb is expressed in English using a prepositional phrase (headed by the word for) that attaches to the verb, but it is expressed with the dative case marking ort anaiari (BROTHER-DAT) it Basque. If we aligned both for and brother to anaiari, then a many-to-one mapping would be formed, and the DCA would be violated: R(f or, brother) holds in the English tree but R(analari,analari) does not hold in the Basque tree. Similarly, a one-to-many mapping (e.g., aligning got </context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. 2001. Inducing multilingual pos taggers and np bracketers via robust projection across aligned corpora. In Proc. of NAACL-2001, pages 200-207.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>