<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.966065">
Direct Parsing of Discontinuous Constituents in German
</title>
<author confidence="0.969982">
Wolfgang Maier
</author>
<affiliation confidence="0.752426">
SFB 833, University of T¨ubingen
</affiliation>
<address confidence="0.7037815">
Nauklerstr. 35
72074 T¨ubingen, Germany
</address>
<email confidence="0.995473">
wmaier@sfs.uni-tuebingen.de
</email>
<sectionHeader confidence="0.998567" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995275625">
Discontinuities occur especially frequently in
languages with a relatively free word order,
such as German. Generally, due to the long-
distance dependencies they induce, they lie
beyond the expressivity of Probabilistic CFG,
i.e., they cannot be directly reconstructed by
a PCFG parser. In this paper, we use a
parser for Probabilistic Linear Context-Free
Rewriting Systems (PLCFRS), a formalism
with high expressivity, to directly parse the
German NeGra and TIGER treebanks. In both
treebanks, discontinuities are annotated with
crossing branches. Based on an evaluation us-
ing different metrics, we show that an output
quality can be achieved which is comparable
to the output quality of PCFG-based systems.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99773325">
Languages with a rather free word order, like Ger-
man, display discontinuous constituents particularly
frequently. In (1), the discontinuity is caused by an
extraposed relative clause.
</bodyText>
<equation confidence="0.4376475">
(1) wieder
again
</equation>
<bodyText confidence="0.999583533333333">
In most constituency treebanks, sentence annota-
tion is restricted to having the shape of trees with-
out crossing branches, and the non-local dependen-
cies induced by the discontinuities are modeled by
an additional mechanism. In the Penn Treebank
(PTB) (Marcus et al., 1994), e.g., this mechanism
is a combination of special labels and empty nodes,
establishing implicit additional edges. In the Ger-
man T¨uBa-D/Z (Telljohann et al., 2006), additional
edges are established by a combination of topolog-
ical field annotation and special edge labels. As an
example, Fig. 1 shows a tree from T¨uBa-D/Z with
the annotation of (1). Note here the edge label ON-
MOD on the relative clause which indicates that the
subject of the sentence (alle Attribute) is modified.
</bodyText>
<figure confidence="0.921689777777778">
treffen
match
Attribute
attributes
zu,
VPART
alle
all
auch
also
die
which
sonst
otherwise
immer
always
passen
fit
</figure>
<figureCaption confidence="0.207828">
‘Again, the same attributes as always apply.’
</figureCaption>
<bodyText confidence="0.93793075">
Another language with a rather free word order is
Bulgarian. In (2), the discontinuity is caused by top-
icalization.
‘As for pens, I only buy expensive ones.’
</bodyText>
<page confidence="0.994921">
58
</page>
<figureCaption confidence="0.999895">
Figure 1: A tree from T¨uBa-D/Z
</figureCaption>
<bodyText confidence="0.9836364">
However, in a few other treebanks, such as the
German NeGra and TIGER treebanks (Skut et al.,
1997; Brants et al., 2002), crossing branches are al-
lowed. This way, all dependents of a long-distance
dependency can be grouped under a single node.
</bodyText>
<footnote confidence="0.499212833333333">
(2) X14M14Kaai41 a3
Pens1 I
KyHysaM
buy
caMo esTHHH t1
only expensive t1
</footnote>
<note confidence="0.9794085">
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 58–66,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.5436585">
Fig. 2 shows a tree from NeGra with the annotation
of (3).
</figureCaption>
<figure confidence="0.912476">
(3) Noch
Yet
‘Never have I had that much choice.’
Note the direct annotation of the discontinuous VP.
</figure>
<figureCaption confidence="0.999667">
Figure 2: A tree from NeGra
</figureCaption>
<bodyText confidence="0.999963452830189">
Since in general, the annotation mechanisms for
non-local dependencies lie beyond the expressivity
of Context-Free Grammar, non-local information is
inaccessible for PCFG parsing and therefore gener-
ally discarded. In NeGra/TIGER annotation, e.g.,
tree transformation algorithms are applied before
parsing in order to resolve the crossing branches.
See, e.g., K¨ubler et al. (2008) and Boyd (2007) for
details. If one wants to avoid the loss of annotation
information which is implied with such transforma-
tions, one possibility is to use a probabilistic parser
for a formalism which is more expressive than CFG.
In this paper, we tackle the question if qualita-
tively good results can be achieved when parsing
German with such a parser. Concretely, we use a
parser for Probabilistic Linear Context-Free Rewrit-
ing Systems (PLCFRS) (Kallmeyer and Maier,
2010). LCFRS (Vijay-Shanker et al., 1987) are a
natural extension of CFG in which a single non-
terminal node can dominate more than one contin-
uous span of terminals. We can directly interpret
NeGra-style trees as its derivation structures, i.e., we
can extract grammars without making further lin-
guistic assumptions (Maier and Lichte, 2009) (see
Sect. 2.3), as it is necessary for other formalisms
such as Probabilistic Tree Adjoining Grammars
(Chiang, 2003). Since the non-local dependencies
are immediately accessible in NeGra and TIGER,
we choose these treebanks as our data source. In
order to judge parser output quality, we use four dif-
ferent evaluation types. We use an EVALB-style
measure, adapted for LCFRS, in order to compare
our parser to previous work on parsing German tree-
banks. In order to address the known shortcomings
of EVALB, we perform an additional evaluation us-
ing the tree distance metric of Zhang and Shasha
(1989), which works independently of the fact if
there are crossing branches in the trees or not, and a
dependency evaluation (Lin, 1995), which has also
be applied before in the context of parsing German
(K¨ubler et al., 2008). Last, we evaluate certain diffi-
cult phenomena by hand on TePaCoC (K¨ubler et al.,
2008), a set of sentences hand-picked from TIGER.
The evaluations show that with a PLCFRS parser,
competitive results can be achieved.
The remainder of the article is structured as fol-
lows. In Sect. 2, we present the formalism, the
parser, and how we obtain our grammars. In
Sect. 3, we discuss the evaluation methods we em-
ploy. Sect. 4 contains our experimental results.
Sect. 5 is dedicated to related work. Sect. 6 con-
tains the conclusion and presents some possible fu-
ture work.
</bodyText>
<sectionHeader confidence="0.989515" genericHeader="method">
2 A Parser for PLCFRS
</sectionHeader>
<subsectionHeader confidence="0.9896895">
2.1 Probabilistic Linear Context-Free
Rewriting Systems
</subsectionHeader>
<bodyText confidence="0.870593375">
LCFRS are an extension of CFG where the non-
terminals can span not only single strings but, in-
stead, tuples of strings. We will notate LCFRS with
the syntax of simple Range Concatenation Gram-
mars (SRCG) (Boullier, 1998), a formalism that is
equivalent to LCFRS.
A LCFRS (Vijay-Shanker et al., 1987) is a tuple
G = (N, T, V, P, 5) where
</bodyText>
<listItem confidence="0.825142">
a) N is a finite set of non-terminals with a func-
tion dim: N , N that determines the fan-out
of each A E N;
b) T and V are disjoint finite sets of terminals and
variables;
c) 5 E N is the start symbol with dim(5) = 1;
</listItem>
<equation confidence="0.950446142857143">
d) P is a finite set of rewriting rules
A(α1, ... , αdim(A)) , A1(X(1)
1 , ... , X(1)
dim(A1))
··· Am(X(m)
1 , ... , X(m)
dim(A,n))
</equation>
<figure confidence="0.980457705882353">
Noch nie habe ich so viel gewählt
ADV ADV VAFIN PPER ADV ADV VVPP $.
1.Sg.Pres.Ind 1.Sg.*.Nom
MO HD
AVP
MO
HD
S
OC
VP
SB
MO HD
AVP
MO
HD
nie habe ich so viel gew¨ahlt
never have I so much chosen
</figure>
<page confidence="0.935727">
59
</page>
<bodyText confidence="0.968365976744186">
form &gt; 0 where A, A1, ... , Am E N, X(i)
j E V
for 1 &lt; i &lt; m,1 &lt; j &lt; dim(Ai) and αi E (T U
V)* for 1 &lt; i &lt; dim(A). For all r E P, it holds
that every variable X occurring in r occurs exactly
once in the left-hand side (LHS) and exactly once in
the right-hand side (RHS).
The fan-out of an LCFRS G is the maximal fan-
out of all non-terminals in G. Furthermore, the RHS
length of a rewriting rules r E P is called the rank
of r and the maximal rank of all rules in P is called
the rank of G. An LCFRS is called ordered if for
every r E P and every RHS non-terminal A in r and
each pair X1, X2 of arguments of A in the RHS of
r, X1 precedes X2 in the RHS iff X1 precedes X2
in the LHS.
Borrowed from SRCG, we specify the language
of an LCFRS based on the notion of ranges. For
some input word w = w1 · · · wn, a range is a pair
(i, j) of integers with 0 &lt; i &lt; n denoting the sub-
string wi+1 · · · wj. Note that a range denotes E iff
i = j. Only consecutive ranges can be concatenated
into new ranges. We can replace the variables and
terminals of the rewriting rules with ranges. E.g.,
A((g, h)) —* B((g + 1, h — 1)) is a replacement of
the clause A(aX1b) —* B(X1) if the input word w is
such that wg+1 = a and wh = b. A rewriting rule in
which all elements of all arguments have been con-
sistently replaced by ranges is called an instantiated
rule. A derivation is built by successively rewriting
the LHSs of instantiated rules with its RHSs. The
language L(G) of some LCFRS G consists of all
words w = w1 · · · wn for which it holds that there is
a rule with the start symbol on the LHS which can
be instantiated to (0, n) and rewritten to E.
A probabilistic LCFRS (PLCFRS) is a tu-
ple (N, T, V, P, S, p) such that (N, T, V, P, S) is a
LCFRS and p : P —* [0..1] a function such that for
all A E N: ΣA(9)11$EPp(A(x) —* -6) = 1. There
are possibly other ways to extend LCFRS with prob-
abilities. This definition is supported by the fact that
probabilistic MCFGs1 have been defined in the same
way (Kato et al., 2006).
</bodyText>
<footnote confidence="0.776994">
1MCFGs are equivalent to LCFRSs and SRCGs (Boullier,
1998).
</footnote>
<bodyText confidence="0.584032">
Scan: 0 : [A, ((i, i + 1))] A POS tag ofwi+1
</bodyText>
<equation confidence="0.9123508">
Unary: in : [B, �ρ]p : A(�ρ) —* B(�ρ) E P
in + |log(p) |: [A, �ρ]
inB : [B, PB], inC : [C, Pc]
Binary: inB
+ inC + log(p) : [A, Pa]
</equation>
<bodyText confidence="0.968716">
where p : A( pA) —* B( pB)C( �ρC) is an instantiated rule.
Goal: [S, ((0, n))]
</bodyText>
<figureCaption confidence="0.993835">
Figure 3: Weighted CYK deduction system
</figureCaption>
<subsectionHeader confidence="0.991915">
2.2 A CYK Parser for PLCFRS
</subsectionHeader>
<bodyText confidence="0.999622457142857">
We use the parser of Kallmeyer and Maier (2010).
It is a probabilistic CYK parser (Seki et al., 1991),
using the technique of weighted deductive parsing
(Nederhof, 2003). While for symbolic parsing, other
elaborate algorithms exist (Kallmeyer and Maier,
2009), for probabilistic parsing, CYK is a natural
choice.
It is assumed for the parser that our LCFRSs are
of rank 2 and do not contain rules where some of the
LHS components are E. Both assumptions can be
made without loss of generality since every LCFRS
can be binarized (G´omez-Rodriguez et al., 2009)
and E-components on LHS of rules can be removed
(Boullier, 1998). We make the assumption that POS
tagging is done before parsing. The POS tags are
special non-terminals of fan-out 1. Consequently,
the rules are either of the form A(a) —* E where A
is a POS tag and a E T or of the form A(a) —* B(x)
or A(a) —* B(x)C(y) where α� E (V+)di-(A), i.e.,
only the rules for POS tags contain terminals in their
LHSs.
The parser items have the form [A, pl, with A E
N and ρ� a vector of ranges characterizing all com-
ponents of the span of A. We specify the set
of weighted parse items via the deduction rules in
Fig. 3.
Parsing time can be reduced by reordering the
agenda during parsing such that those items are pro-
cessed first which lead to a complete parse more
quickly than others (Klein and Manning, 2003a).
The parser uses for this purpose an admissible, but
not monotonic estimate called LR estimate. It gives
(relative to a sentence length) an estimate of the out-
side probability of some non-terminal A with a span
of a certain length (the sum of the lengths of all the
</bodyText>
<page confidence="0.991162">
60
</page>
<bodyText confidence="0.999966833333333">
components of the span), a certain number of ter-
minals to the left of the first and to the right of the
last component and a certain number of terminals
gaps in between the components of the A span, i.e.,
filling the gaps. A discussion of other estimates is
presented at length in Kallmeyer and Maier (2010).
</bodyText>
<subsectionHeader confidence="0.760783">
2.3 LCFRS for Modeling Discontinuities
</subsectionHeader>
<bodyText confidence="0.991277589285714">
We use the algorithm from Maier and Søgaard
(2008) to extract LCFRS rules from our data sets.
For all nonterminals A0 with the children A1 · · · Am
(i.e., for all non-terminals which are not pretermi-
nals), we create a clause 00 —* 01 · · · 0m with 0Z,
0 &lt; i &lt; m, labeled AZ. The arguments of each
0Z, 1 &lt; i &lt; m, are single variables, one for each
of the continuous yield part dominated by the node
AZ. The arguments of 00 are concatenations of these
variables that describe how the discontinuous parts
of the yield of A0 are obtained from the yields of its
daughters. For all preterminals A dominating some
terminal a, we extract a production A(a) —* ε. Since
by definition, a label is associated with a certain
fan-out, we distinguish the labels by correspond-
ing subscripts. Note that this extraction algorithm
yields only ordered LCFRS. Furthermore, note that
for trees without crossing branches, this algorithm
yields a PLCFRS with fan-out 1, i.e., a PCFG.
As mentioned before, the advantage of using
LCFRS is that grammar extraction is straight-
forward and that no separate assumptions must be
made. Note that unlike, e.g., Range Concatenation
Grammar (RCG) (Boullier, 1998), LCFRS cannot
model re-entrancies, i.e., nodes with more than one
incoming edge. While those do not occur in NeGra-
style annotation, some of the annotation in the PTB,
e.g., the annotation for right node raising, can be in-
terpreted as re-entrancies. This topic is left for fu-
ture work. See Maier and Lichte (2009) for further
details, especially on how treebank properties relate
to properties of extracted grammars.
Before parsing, we binarize our grammar. We first
mark the head daughters of all non-terminal nodes
using Collins-style head rules based on the NeGra
rules of the Stanford Parser (Klein and Manning,
2003b) and the reorder the RHSs of all LCFRS rules
such that sequence of elements to the right of the
head daughter is reversed and moved to the begin-
ning of the RHS. From this point, the binarization
works like the transformation into Chomsky Normal
Form for CFGs. For each rule with an RHS of length
&gt; 3, we introduce a new non-terminal which cov-
ers the RHS without the first element and continue
successively from left to right. The rightmost new
rule, which covers the head daughter, is binarized to
unary.
We markovize the grammar as in the CFG case.
To the new symbols introduced during the binariza-
tion, a variable number of symbols from the vertical
and horizontal context of the original rule is added.
Following the literature, we call the respective quan-
tities v and h. As an example, Fig. 4 shows the out-
put for the production for the VP in the left tree in
Fig. 2.
After extraction and head marking:
</bodyText>
<equation confidence="0.994135">
VP2(X1,X2X3) → AVP1(X1) AVP1(X2) VVPP1’(X3)
After binarization and markovization with v = 1, h = 2:
VP2(X1,X2) → AVP1(X1) @-VP2v-AVP1h-VVPP1h(X2)
@-VP2v-AVP1h-VVPP1h(X1X2)
→ AVP1(X1) @-VP2v-VVPP1h(X2)
@-VP2v-VVPP1h(X1) → VVPP1(X1)
After binarization and markovization with v = 2, h = 1:
VP2(X1,X2) → AVP1(X1) @-VP2v-S2v-AVP1h(X2)
@-VP2v-S2v-AVP1h(X1X2)
→ AVP1(X1) @-VP2v-S2v-VVPP1h(X2)
@-VP2v-S2v-VVPP1h(X1) → VVPP1(X1)
</equation>
<figureCaption confidence="0.996649">
Figure 4: Grammar extraction and binarization example
</figureCaption>
<bodyText confidence="0.999919333333333">
The probabilities are then computed based on the
number of occurrences of rules in the transformed
treebank, using a Maximum Likelihood estimator.
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="method">
3 Evaluation methods
</sectionHeader>
<bodyText confidence="0.994201846153846">
We assess the quality of our parser output using dif-
ferent methods.
The first is an EVALB-style metric (henceforth
EVALB), i.e., we compare phrase boundaries. In
spite of its shortcomings (Rehbein and van Gen-
abith, 2007), it allows us to compare to previ-
ous work on parsing NeGra. In the context of
LCFRS, we compare sets of tuples of the form
[A,(i1� , i1�), ... , (i� � , i� �)�, where A is a non-terminal
in some derivation tree with dim(A) = k and each
(im�, im�), 1 &lt; m &lt; k, is a tuple of indices denot-
ling a continuous sequence of terminals dominated
by A. One set is obtained from the parser output, and
</bodyText>
<page confidence="0.978961">
61
</page>
<figure confidence="0.9136174">
B&lt;
B C
B t3 B &gt;B t4
t1 t2 z t1 t2 z
x y x y
</figure>
<figureCaption confidence="0.999786">
Figure 5: TDIST example
</figureCaption>
<bodyText confidence="0.999604714285714">
one from the corresponding treebank trees. Using
these tuple sets, we compute labeled and unlabeled
recall (LR/UR), precision (LP/UP), and the F1 mea-
sure (LF1/UF1) in the usual way. Note that if k = 1,
our metric is identical to its PCFG version.
EVALB does not necessarily reflect parser output
quality (Rehbein and van Genabith, 2007; Emms,
2008; K¨ubler et al., 2008). One of its major prob-
lems is that attachment errors are penalized too
hard. As the second evaluation method, we there-
fore choose the tree-distance measure (henceforth
TDIST) (Zhang and Shasha, 1989), which levitates
this problem. It has been proposed for parser evalu-
ation by Emms (2008). TDIST is an ideal candidate
for evaluation of the output of a PLCFRS, since it the
fact if trees have crossing branches or not is not rel-
evant to it. Two trees τk and τA are compared on the
basis of T-mappings from τk to τA. A T-mapping
is a partial mapping Q of nodes of τk to nodes of τA
where all node mappings preserve left-to-right or-
der and ancestry. Within the mappings, node inser-
tion, node deletion, and label swap operations are
identified, represented resp. by the sets I, D and
S. Furthermore, we consider the set M represent-
ing the matched (i.e., unchanged) nodes. The cost of
a T-mapping is the total number of operations, i.e.
|I|+|D|+|S|. The tree distance between two trees
τK and τA is the cost of the cheapest T-mapping.
Fig. 5, borrowed from Emms, shows an example for
a T-mapping. Inserted nodes are prefixed with &gt;,
deleted nodes are suffixed with &lt;, and nodes with
swapped labels are linked with arrows. Since in to-
tal, four operations are involved, to this T-mapping,
a cost of 4 is assigned. For more details, especially
on algorithms which compute TDIST, refer to Bille
(2005). In order to convert the tree distance measure
into a similarity measure like EVALB, we use the
macro-averaged Dice and Jaccard normalizations as
defined by Emms. Let τK and τA be two trees with
|τK |and |τA |nodes, respectively. For a T-mapping
Q from τK to τA with the sets D, I, S and M, we
compute them as follows.
</bodyText>
<equation confidence="0.9998944">
dice(Q) = 1 − |D |+ |+ I
 |+ IS|
|τK  |A |
jaccard(Q) = 1 − |D |+ |I |+ |S|
|D |+ |I |+ |S |+ |M|
</equation>
<bodyText confidence="0.999979769230769">
where, in order to achieve macro-averaging, we sum
the numerators and denominators over all tree pairs
before dividing. See Emms (2008) for further de-
tails.
The third method is dependency evaluation
(henceforth DEP), as described by Lin (1995). It
consists of comparing dependency graphs extracted
from the gold data and from the parser output. The
dependency extraction algorithm as given by Lin
does also not rely on trees to be free of crossing
branches. It only relies on a method to identify the
head of each phrase. We use our own implementa-
tion of the algorithm which is described in Sect. 4
of Lin (1995), combined with the head finding algo-
rithm of the parser. Dependency evaluation abstracts
away from another bias of EVALB. Concretely, it
does not prefer trees with a high node/token ratio,
since two dependency graphs to be compared neces-
sarily have the same number of (terminal) nodes. In
the context of parsing German, this evaluation has
been employed previously by K¨ubler et al. (2008).
Last, we evaluate on TePaCoC (Testing
Parser Performance on Complex Grammatical
Constructions), a set of particularly difficult sen-
tences hand-picked from TIGER (K¨ubler et al.,
2008).
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999821090909091">
Our data sources are the German NeGra (Skut et
al., 1997) and TIGER (Brants et al., 2002) tree-
banks. In a preprocessing step, following common
practice, we attach all punctuation to nodes within
the tree, since it is not included in the NeGra an-
notation. In a first pass, using heuristics, we at-
tach all nodes to the in each case highest available
phrasal node such that ideally, we do not introduce
new crossing branches. In a second pass, paren-
theses and quotation marks are preferably attached
to the same node. Grammatical function labels are
</bodyText>
<page confidence="0.998262">
62
</page>
<bodyText confidence="0.999847958333333">
discarded. After this preprocessing step, we create
a separate version of the data set, in which we re-
solve the crossing branches in the trees, using the
common approach of re-attaching nodes to higher
constituents. We use the first 90% of our data sets
for training and the remaining 10% for testing. Due
to memory limitations, we restrict ourselves to sen-
tences of a maximal length of 30 words. Our TIGER
data sets (TIGER and T-CF) have 31,568 sentences
of an average length of 14.81, splitted into 31,568
sentences for training and 3,508 sentences for test-
ing. Our NeGra data sets (NeGra and N-CF) have
18,335 sentences, splitted into 16,501 sentences for
training and 1,834 sentences for testing.
We parse the data sets described above with acti-
vated LR estimate. For all our experiments, we use
the markovization settings v = 2 and h = 1, which
have proven to be successful in previous work on
parsing NeGra (Rafferty and Manning, 2008). We
provide the parser with the gold tagging. Fig. 6
shows the average parsing times for all data sets on
an AMD Opteron node with 8GB of RAM (pure
Java implementation), Tab. 1 shows the percentage
of parsed sentences.
</bodyText>
<figureCaption confidence="0.992639">
Figure 6: Parsing times
</figureCaption>
<table confidence="0.99350325">
NeGra TIGER N-CF T-CF
total 1834 3508 1834 3508
parsed 1779 3462 1804 3462
(97.0%) (98.7%) (98.4%) (98.7%)
</table>
<tableCaption confidence="0.999845">
Table 1: Parsed sentences
</tableCaption>
<subsectionHeader confidence="0.987168">
4.1 Evaluation Using EVALB
</subsectionHeader>
<bodyText confidence="0.99824425">
Tab. 2 shows the evaluation of the parser output us-
ing EVALB, as described in the previous section.
We report labeled and unlabeled precision, recall
and F1 measure.
</bodyText>
<tableCaption confidence="0.977626">
Table 2: EVALB results
</tableCaption>
<bodyText confidence="0.999349777777778">
Not surprisingly, reconstructing discontinuities is
hard. Therefore, when parsing without crossing
branches, the results are slightly better. In order to
see the influence of discontinuous structures during
parsing on the underlying phrase structure, we re-
solve the crossing branches in the parser output of
NeGra and TIGER and compare it to the respective
gold test data of N-CF and T-CF. Tab. 3 shows the
results.
</bodyText>
<table confidence="0.992853666666667">
LP LR LFI UP UR UFI
NeGra 72.75 71.04 71.89 76.38 74.58 75.47
TIGER 75.28 72.25 73.74 78.81 75.64 77.20
</table>
<tableCaption confidence="0.999725">
Table 3: EVALB results (resolved crossing branches)
</tableCaption>
<bodyText confidence="0.997530476190476">
The results deteriorate slightly in comparison
with N-CF and T-CF, however, they are slightly
higher than for than for NeGra and TIGER. This
is due to the fact that during the transformation,
some errors in the LCFRS parses get “corrected”:
Wrongly attached phrasal nodes are re-attached to
unique higher positions in the trees.
In order to give a point of comparison with previ-
ous work on parsing TIGER and NeGra, in Tab. 4,
we report some of the results from the literature. All
of them were obtained using PCFG parsers: K¨ubler
(2005) (Tab. 1, plain PCFG for NeGra), K¨ubler et al.
(2008) (Tab. 3, plain PCFG and Stanford parser with
markovization v = 2 and h = 1 for TIGER), and
Petrov and Klein (2007) (Tab. 1, Berkeley parser, la-
tent variables). We include the results for N-CF and
T-CF.
Our results are slightly better than for the plain
PCFG models. We would expect the result for T-
CF to be closer to the corresponding result for the
Stanford parser, since we are using a comparable
</bodyText>
<figure confidence="0.894910058823529">
5 10 15 20 25
Sentence length
time in sec. (log scale)
0.01
100
0.1
10
1
NeGra
TIGER
N-CF
T-CF
LP LR LFI UP UR UFI
NeGra 72.39 70.68 71.52 76.01 74.22 75.10
TIGER 74.97 71.95 73.43 78.58 75.42 76.97
N-CF 74.85 73.26 74.04 78.11 76.45 77.28
T-CF 77.51 73.73 75.57 80.59 76.66 78.57
</figure>
<page confidence="0.935065">
63
</page>
<table confidence="0.888606">
plain this work markov. latent
69.94 74.04 80.1
74.00 75.57 77.30 –
</table>
<tableCaption confidence="0.999606">
Table 4: PCFG parsing of NeGra, Labeled Fl
</tableCaption>
<bodyText confidence="0.999942214285714">
model. This difference is mostly likely due to losses
induced by the LR estimate. All items to which the
estimate assigns an outside log probability estimate
of −oc get blocked and are not put on the agenda.
This blocking has an extremely beneficial effect on
parser speed. However, it is paid by a worse recall,
as experiments with smaller data sets have shown.
A complete discussion of the effects of estimates, as
well as a discussion of other possible optimizations,
is presented in Kallmeyer and Maier (2010).
Recall finally that LCFRS parses are more infor-
mative than PCFG parses – a lower score for LCFRS
EVALB than for PCFG EVALB does not necessarily
mean that the PCFG parse is “better”.
</bodyText>
<subsectionHeader confidence="0.995124">
4.2 Evaluation Using Tree Distance
</subsectionHeader>
<bodyText confidence="0.9997076">
Tab. 5 shows the results of evaluating with TDIST,
excluding unparsed sentences. We report the dice
and jaccard normalizations, as well as a summary
of the distribution of the tree distances between gold
trees and trees from the parser output (see Sect. 3).
</bodyText>
<table confidence="0.990169666666667">
dice jaccard tree distance distrib.
0 &lt;3 &gt;10
NeGra 88.86 79.79 31.65 53.77 15.08
TIGER 89.47 80.84 29.87 56.78 18.18
N-CF 92.50 85.99 33.43 61.92 6.71
T-CF 92.70 86.46 31.80 63.81 4.56
</table>
<tableCaption confidence="0.99966">
Table 5: Tree distance evaluation
</tableCaption>
<bodyText confidence="0.999981692307692">
Again, we can observe that parsing LCFRS is
harder than parsing PCFG. As for EVALB, the re-
sults for TIGER are slightly higher than the ones for
NeGra. The distribution of the tree distances shows
that about a third of all sentences receive a com-
pletely correct parse. More than a half, resp. a third
of all parser output trees require ≤ 3 operations to be
mapped to the corresponding gold tree, and a only a
small percentage requires ≥ 10 operations.
To our knowledge, TDIST has not been used to
evaluate parser output for NeGra and TIGER. How-
ever, Emms (2008) reports results for the PTB using
different parsers. Collins’ Model 1 (Collins, 1999),
e.g., lies at 93.62 (Dice) and 87.87 (Jaccard). For
the Berkeley Parser (Petrov and Klein, 2007), 94.72
and 89.87 is reported. We see that our results lie in
them same range. However, Jaccard scores are lower
since this normalization punishes a higher number
of edit operations more severely than Dice. In or-
der to meaningfully interpret which treebank prop-
erties are responsible for the fact that between the
gold trees and the trees from the parser, the German
data requires more tree edit operations than the En-
glish data, a TDIST evaluation of the output of an
off-the-shelf PCFG parser would be necessary. This
is left for future work.
</bodyText>
<subsectionHeader confidence="0.998028">
4.3 Dependency Evaluation
</subsectionHeader>
<bodyText confidence="0.9977558">
For the dependency evaluation, we extract depen-
dency graphs from both the gold data and the test
data and compare the unlabeled accuracy. Tab. 6
shows the results. We report unlabeled attachment
score (UAS).
</bodyText>
<table confidence="0.9911394">
UAS
NeGra 76.50
TIGER 77.84
N-CF 77.52
T-CF 78.67
</table>
<tableCaption confidence="0.996627">
Table 6: Dependency evaluation
</tableCaption>
<bodyText confidence="0.9998904">
The dependency results are consistent with the
previous results in as much as the scores for PCFG
parsing are again higher. The dependency re-
sults reported in K¨ubler et al. (2008) however are
much higher (85.6 UAS for the markovized Stan-
ford parser). While a part of the losses can again
be attributed to the LR estimate, another reason lies
undoubtedly in the different dependency conversion
method which we employ, and in further treebank
transformations which K¨ubler et al. perform. In or-
der to get a more fine grained result, in future work,
we will consider graph modifications as proposed by
Lin (1995) as well as including annotation-specific
information from NeGra/TIGER in our conversion
procedure.
</bodyText>
<subsectionHeader confidence="0.888112">
4.4 TePaCoC
</subsectionHeader>
<bodyText confidence="0.7605734">
The TePaCoC data set (K¨ubler et al., 2008) provides
100 hand-picked sentences from TIGER which con-
tain constructions that are especially difficult to
NeGra
TIGER
</bodyText>
<page confidence="0.990931">
64
</page>
<bodyText confidence="0.998446153846154">
parse. Out of these 100 sentences, we only consider
69. The remaining 31 sentences are either longer
than 30 words or not included in the TIGER 2003
release (K¨ubler et al. use the 2005 release). The
data is partitioned in groups of sentences with extra-
posed relative clauses (ERC), forward conjunction
reduction (FCR), noun PP attachment (PPN), verb
PP attachment (PPV), subject gap with finite/fronted
verbs (SGF) and coordination of unlike constituents
(CUC). Tab. 7 shows the EVALB results for the (dis-
continuous) TePaCoC. We parse these sentences us-
ing the same training set as before with all TePaCoC
sentences removed.
</bodyText>
<table confidence="0.99834575">
LP LR LFl UP UR UFl
ERC 59.34 61.36 60.34 64.84 67.05 65.92
FCR 78.03 76.70 77.36 82.66 81.25 81.95
PPN 72.15 72.15 72.15 75.95 75.95 75.95
PPV 73.33 73.33 73.33 76.66 76.66 76.66
CUC 58.76 57.58 58.16 69.07 67.68 68.37
SGF 82.67 81.05 81.85 85.33 83.66 84.49
all 72.27 71.83 72.05 77.26 76.78 77.02
</table>
<tableCaption confidence="0.999576">
Table 7: EVALB scores for TePaCoC
</tableCaption>
<bodyText confidence="0.999981263157895">
While we cannot compare our results directly
with the PCFG results (using grammatical function
labels) of K¨ubler et al., their results nevertheless give
an orientation.
We take a closer look at all sentence groups. Our
result for ERC is more than 15 points worse than
the result of K¨ubler et al. The relative clause itself
is mostly recognized as a sentence (though not ex-
plicitly marked as a relative clause, since we do not
consider grammatical functions). However, it is al-
most consistently attached too high (on the VP or
on clause level). While this is correct for K¨ubler et
al., with crossing branches, it treated as an error and
punished especially hard by EVALB. FCR is parsed
mostly well and with comparable results to K¨ubler
et al. There are too few sentences to make a strong
claim about PP attachment. However, in both PPN
and PPV flat phrases seem to be preferred, which
has as a consequence that in PPN, PPs are attached
too high and in PPV too low. Our output confirms
the claim of K¨ubler et al.’s that unlike coordinations
is the most difficult of all TePaCoC phenomena. The
conjuncts themselves are correctly identified in most
cases, however then coordinated at the wrong level.
SGF is parsed best. K¨ubler et al. report for this group
only 78.6 labeled F1 for the Stanford Parser. Our
overall results are slightly worse than the results of
K¨ubler et al., but show less variance.
To sum up, not surprisingly, getting the right at-
tachment positions seems to be hard for LCFRS,
too. Additionally, with crossing branches, the out-
put is rated worse, since some attachments are not
present anymore without crossing branches. Since
especially for the relative clauses, attachment posi-
tions are in fact a matter of discussion from a syntac-
tic point of view, we will consider in future studies
to selectively resolve some of the crossing branches,
e.g., by attaching relative clauses to higher positions.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999913181818182">
The use of formalisms with a high expressivity has
been explored before (Plaehn, 2004; Levy, 2005).
To our knowledge, Plaehn is the only one to re-
port evaluation results. He uses the formalism of
Discontinuous Phrase Structure Grammar (DPSG).
Limiting the sentence length to 15, he obtains 73.16
labeled F1 on NeGra. Evaluating all sentences of
our NeGra data with a length of up to 15 words re-
sults, however, in 81.27 labeled F1. For a compari-
son between DPSG and LCFRS, refer to Maier and
Søgaard (2008).
</bodyText>
<sectionHeader confidence="0.99842" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999913473684211">
We have investigated the possibility of using Prob-
abilistic Linear Context-Free Rewriting Systems for
direct parsing of discontinuous constituents. Conse-
quently, we have applied a PLCFRS parser on the
German NeGra and TIGER treebanks. Our evalu-
ation, which used different metrics, showed that a
PLCFRS parser can achieve competitive results.
In future work, all of the presented evaluation
methods will be investigated to greater detail. In
order to do this, we will parse our data sets with
current state-of-the-art systems. Especially a more
elaborate dependency conversion should enable a
more informative comparison between the output of
PCFG parsers and the output of the PLCFRS parser.
Last, since an algorithm is available which extracts
LCFRSs from dependency structures (Kuhlmann
and Satta, 2009), the parser is instantly ready for
parsing them. We are currently performing the cor-
responding experiments.
</bodyText>
<page confidence="0.999497">
65
</page>
<sectionHeader confidence="0.998344" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994607316831683">
Philip Bille. 2005. A survey on tree edit distance and
related problems. Theoretical Computer Science, 337.
Pierre Boullier. 1998. A Proposal for a Natural Lan-
guage Processing Syntactic Backbone. Technical Re-
port 3342, INRIA.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
The Linguistic Annotation Workshop at ACL 2007.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER Tree-
bank. In Proceedings of Treebanks and Linguistic
Theories.
David Chiang. 2003. Statistical parsing with an automat-
ically extracted Tree Adjoining Grammar. In Data-
Oriented Parsing. CSLI Publications.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Martin Emms. 2008. Tree Distance and some other vari-
ants of Evalb. In Proceedings ofLREC 08.
Carlos G´omez-Rodriguez, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009. Optimal reduction of
rule length in linear context-free rewriting systems. In
Proceedings ofNAACL-HLT.
Laura Kallmeyer and Wolfgang Maier. 2009. An in-
cremental earley parser for simple range concatenation
grammar. In Proceedings ofIWPT 09.
Laura Kallmeyer and Wolfgang Maier. 2010. Data-
driven parsing with probabilistic linear context-free
rewriting systems. Unpublished Manuscript.
Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2006.
RNA pseudoknotted structure prediction using
stochastic multiple context-free grammar. IPSJ
Digital Courier, 2.
Dan Klein and Christopher D. Manning. 2003a. A* Pars-
ing: Fast Exact Viterbi Parse Selection. In Proceed-
ings ofNAACL-HLT.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural lan-
guage parsing. In In Advances in Neural Information
Processing Systems 15 (NIPS). MIT Press.
Sandra K¨ubler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings ofLREC 08.
Sandra K¨ubler. 2005. How do treebank annotation
schemes influence parsing results? Or how not to com-
pare apples and oranges. In Proceedings of RANLP
2005.
Marco Kuhlmann and Giorgio Satta. 2009. Treebank
grammar techniques for non-projective dependency
parsing. In Proceedings ofEACL.
Roger Levy. 2005. Probabilistic Models of Word Or-
der and Syntactic Discontinuity. Ph.D. thesis, Stan-
ford University.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings of
IJCAI 95.
Wolfgang Maier and Timm Lichte. 2009. Characterizing
discontinuity in constituent treebanks. In Proceedings
ofFormal Grammar 2009.
Wolfgang Maier and Anders Søgaard. 2008. Treebanks
and mild context-sensitivity. In Proceedings ofFormal
Grammar 2008.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings ofHLT.
Mark-Jan Nederhof. 2003. Weighted Deductive Parsing
and Knuth’s Algorithm. Computational Linguistics,
29(1).
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL 2007.
Oliver Plaehn. 2004. Computing the most probable parse
for a discontinuous phrase-structure grammar. In New
developments in parsing technology. Kluwer.
Anna Rafferty and Christopher D. Manning. 2008. Pars-
ing three German treebanks: Lexicalized and unlexi-
calized baselines. In Proceedings of the Workshop on
Parsing German at ACL 2008.
Ines Rehbein and Josef van Genabith. 2007. Evaluating
evaluation measures. In Proceedings of NODALIDA
2007.
Hiroyuki Seki, Takahashi Matsumura, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free gram-
mars. Theoretical Computer Science, 88(2).
Wojciech Skut, Brigitte Krenn, Thorten Brants, and Hans
Uszkoreit. 1997. An annotation scheme for free word
order languages. In Proceedings ofANLP.
Heike Telljohann, Erhard Hinrichs, Sandra K¨ubler, and
Heike Zinsmeister. 2006. Stylebook for the T¨ubingen
Treebank of Written German (T¨uBa-D/Z). Technis-
cher Bericht, Universit¨at T¨ubingen.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions produced
by various grammatical formalisms. In Proceedings of
ACL.
Kaizhong Zhang and Dennis Shasha. 1989. Simple fast
algorithms for the editing distance between trees and
related problems. SIAMJournal of Computing, 18.
</reference>
<page confidence="0.987481">
66
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.354101">
<title confidence="0.7123715">Direct Parsing of Discontinuous Constituents in German Wolfgang</title>
<note confidence="0.797897">SFB 833, University of Nauklerstr. 72074 T¨ubingen,</note>
<email confidence="0.989653">wmaier@sfs.uni-tuebingen.de</email>
<abstract confidence="0.999441117647059">Discontinuities occur especially frequently in languages with a relatively free word order, such as German. Generally, due to the longdistance dependencies they induce, they lie beyond the expressivity of Probabilistic CFG, i.e., they cannot be directly reconstructed by a PCFG parser. In this paper, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS), a formalism with high expressivity, to directly parse the German NeGra and TIGER treebanks. In both treebanks, discontinuities are annotated with crossing branches. Based on an evaluation using different metrics, we show that an output quality can be achieved which is comparable to the output quality of PCFG-based systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philip Bille</author>
</authors>
<title>A survey on tree edit distance and related problems.</title>
<date>2005</date>
<journal>Theoretical Computer Science,</journal>
<volume>337</volume>
<contexts>
<context position="16629" citStr="Bille (2005)" startWordPosition="2943" endWordPosition="2944">S. Furthermore, we consider the set M representing the matched (i.e., unchanged) nodes. The cost of a T-mapping is the total number of operations, i.e. |I|+|D|+|S|. The tree distance between two trees τK and τA is the cost of the cheapest T-mapping. Fig. 5, borrowed from Emms, shows an example for a T-mapping. Inserted nodes are prefixed with &gt;, deleted nodes are suffixed with &lt;, and nodes with swapped labels are linked with arrows. Since in total, four operations are involved, to this T-mapping, a cost of 4 is assigned. For more details, especially on algorithms which compute TDIST, refer to Bille (2005). In order to convert the tree distance measure into a similarity measure like EVALB, we use the macro-averaged Dice and Jaccard normalizations as defined by Emms. Let τK and τA be two trees with |τK |and |τA |nodes, respectively. For a T-mapping Q from τK to τA with the sets D, I, S and M, we compute them as follows. dice(Q) = 1 − |D |+ |+ I |+ IS| |τK |A | jaccard(Q) = 1 − |D |+ |I |+ |S| |D |+ |I |+ |S |+ |M| where, in order to achieve macro-averaging, we sum the numerators and denominators over all tree pairs before dividing. See Emms (2008) for further details. The third method is depende</context>
</contexts>
<marker>Bille, 2005</marker>
<rawString>Philip Bille. 2005. A survey on tree edit distance and related problems. Theoretical Computer Science, 337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Boullier</author>
</authors>
<title>A Proposal for a Natural Language Processing Syntactic Backbone.</title>
<date>1998</date>
<tech>Technical Report 3342, INRIA.</tech>
<contexts>
<context position="5796" citStr="Boullier, 1998" startWordPosition="919" endWordPosition="920"> is structured as follows. In Sect. 2, we present the formalism, the parser, and how we obtain our grammars. In Sect. 3, we discuss the evaluation methods we employ. Sect. 4 contains our experimental results. Sect. 5 is dedicated to related work. Sect. 6 contains the conclusion and presents some possible future work. 2 A Parser for PLCFRS 2.1 Probabilistic Linear Context-Free Rewriting Systems LCFRS are an extension of CFG where the nonterminals can span not only single strings but, instead, tuples of strings. We will notate LCFRS with the syntax of simple Range Concatenation Grammars (SRCG) (Boullier, 1998), a formalism that is equivalent to LCFRS. A LCFRS (Vijay-Shanker et al., 1987) is a tuple G = (N, T, V, P, 5) where a) N is a finite set of non-terminals with a function dim: N , N that determines the fan-out of each A E N; b) T and V are disjoint finite sets of terminals and variables; c) 5 E N is the start symbol with dim(5) = 1; d) P is a finite set of rewriting rules A(α1, ... , αdim(A)) , A1(X(1) 1 , ... , X(1) dim(A1)) ··· Am(X(m) 1 , ... , X(m) dim(A,n)) Noch nie habe ich so viel gewählt ADV ADV VAFIN PPER ADV ADV VVPP $. 1.Sg.Pres.Ind 1.Sg.*.Nom MO HD AVP MO HD S OC VP SB MO HD AVP MO</context>
<context position="8529" citStr="Boullier, 1998" startWordPosition="1509" endWordPosition="1510">G) of some LCFRS G consists of all words w = w1 · · · wn for which it holds that there is a rule with the start symbol on the LHS which can be instantiated to (0, n) and rewritten to E. A probabilistic LCFRS (PLCFRS) is a tuple (N, T, V, P, S, p) such that (N, T, V, P, S) is a LCFRS and p : P —* [0..1] a function such that for all A E N: ΣA(9)11$EPp(A(x) —* -6) = 1. There are possibly other ways to extend LCFRS with probabilities. This definition is supported by the fact that probabilistic MCFGs1 have been defined in the same way (Kato et al., 2006). 1MCFGs are equivalent to LCFRSs and SRCGs (Boullier, 1998). Scan: 0 : [A, ((i, i + 1))] A POS tag ofwi+1 Unary: in : [B, �ρ]p : A(�ρ) —* B(�ρ) E P in + |log(p) |: [A, �ρ] inB : [B, PB], inC : [C, Pc] Binary: inB + inC + log(p) : [A, Pa] where p : A( pA) —* B( pB)C( �ρC) is an instantiated rule. Goal: [S, ((0, n))] Figure 3: Weighted CYK deduction system 2.2 A CYK Parser for PLCFRS We use the parser of Kallmeyer and Maier (2010). It is a probabilistic CYK parser (Seki et al., 1991), using the technique of weighted deductive parsing (Nederhof, 2003). While for symbolic parsing, other elaborate algorithms exist (Kallmeyer and Maier, 2009), for probabili</context>
<context position="12013" citStr="Boullier, 1998" startWordPosition="2149" endWordPosition="2150">ters. For all preterminals A dominating some terminal a, we extract a production A(a) —* ε. Since by definition, a label is associated with a certain fan-out, we distinguish the labels by corresponding subscripts. Note that this extraction algorithm yields only ordered LCFRS. Furthermore, note that for trees without crossing branches, this algorithm yields a PLCFRS with fan-out 1, i.e., a PCFG. As mentioned before, the advantage of using LCFRS is that grammar extraction is straightforward and that no separate assumptions must be made. Note that unlike, e.g., Range Concatenation Grammar (RCG) (Boullier, 1998), LCFRS cannot model re-entrancies, i.e., nodes with more than one incoming edge. While those do not occur in NeGrastyle annotation, some of the annotation in the PTB, e.g., the annotation for right node raising, can be interpreted as re-entrancies. This topic is left for future work. See Maier and Lichte (2009) for further details, especially on how treebank properties relate to properties of extracted grammars. Before parsing, we binarize our grammar. We first mark the head daughters of all non-terminal nodes using Collins-style head rules based on the NeGra rules of the Stanford Parser (Kle</context>
</contexts>
<marker>Boullier, 1998</marker>
<rawString>Pierre Boullier. 1998. A Proposal for a Natural Language Processing Syntactic Backbone. Technical Report 3342, INRIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriane Boyd</author>
</authors>
<title>Discontinuity revisited: An improved conversion to context-free representations.</title>
<date>2007</date>
<booktitle>In The Linguistic Annotation Workshop at ACL</booktitle>
<contexts>
<context position="3320" citStr="Boyd (2007)" startWordPosition="509" endWordPosition="510">Computational Linguistics Fig. 2 shows a tree from NeGra with the annotation of (3). (3) Noch Yet ‘Never have I had that much choice.’ Note the direct annotation of the discontinuous VP. Figure 2: A tree from NeGra Since in general, the annotation mechanisms for non-local dependencies lie beyond the expressivity of Context-Free Grammar, non-local information is inaccessible for PCFG parsing and therefore generally discarded. In NeGra/TIGER annotation, e.g., tree transformation algorithms are applied before parsing in order to resolve the crossing branches. See, e.g., K¨ubler et al. (2008) and Boyd (2007) for details. If one wants to avoid the loss of annotation information which is implied with such transformations, one possibility is to use a probabilistic parser for a formalism which is more expressive than CFG. In this paper, we tackle the question if qualitatively good results can be achieved when parsing German with such a parser. Concretely, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS) (Kallmeyer and Maier, 2010). LCFRS (Vijay-Shanker et al., 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous </context>
</contexts>
<marker>Boyd, 2007</marker>
<rawString>Adriane Boyd. 2007. Discontinuity revisited: An improved conversion to context-free representations. In The Linguistic Annotation Workshop at ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER Treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of Treebanks and Linguistic Theories.</booktitle>
<contexts>
<context position="2333" citStr="Brants et al., 2002" startWordPosition="357" endWordPosition="360">ation of (1). Note here the edge label ONMOD on the relative clause which indicates that the subject of the sentence (alle Attribute) is modified. treffen match Attribute attributes zu, VPART alle all auch also die which sonst otherwise immer always passen fit ‘Again, the same attributes as always apply.’ Another language with a rather free word order is Bulgarian. In (2), the discontinuity is caused by topicalization. ‘As for pens, I only buy expensive ones.’ 58 Figure 1: A tree from T¨uBa-D/Z However, in a few other treebanks, such as the German NeGra and TIGER treebanks (Skut et al., 1997; Brants et al., 2002), crossing branches are allowed. This way, all dependents of a long-distance dependency can be grouped under a single node. (2) X14M14Kaai41 a3 Pens1 I KyHysaM buy caMo esTHHH t1 only expensive t1 Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 58–66, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Fig. 2 shows a tree from NeGra with the annotation of (3). (3) Noch Yet ‘Never have I had that much choice.’ Note the direct annotation of the discontinuous VP. Figure 2: A tree from NeGra Since in </context>
<context position="18337" citStr="Brants et al., 2002" startWordPosition="3243" endWordPosition="3246">pendency evaluation abstracts away from another bias of EVALB. Concretely, it does not prefer trees with a high node/token ratio, since two dependency graphs to be compared necessarily have the same number of (terminal) nodes. In the context of parsing German, this evaluation has been employed previously by K¨ubler et al. (2008). Last, we evaluate on TePaCoC (Testing Parser Performance on Complex Grammatical Constructions), a set of particularly difficult sentences hand-picked from TIGER (K¨ubler et al., 2008). 4 Experiments Our data sources are the German NeGra (Skut et al., 1997) and TIGER (Brants et al., 2002) treebanks. In a preprocessing step, following common practice, we attach all punctuation to nodes within the tree, since it is not included in the NeGra annotation. In a first pass, using heuristics, we attach all nodes to the in each case highest available phrasal node such that ideally, we do not introduce new crossing branches. In a second pass, parentheses and quotation marks are preferably attached to the same node. Grammatical function labels are 62 discarded. After this preprocessing step, we create a separate version of the data set, in which we resolve the crossing branches in the tr</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER Treebank. In Proceedings of Treebanks and Linguistic Theories.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically extracted Tree Adjoining Grammar. In DataOriented Parsing.</title>
<date>2003</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="4231" citStr="Chiang, 2003" startWordPosition="655" endWordPosition="656"> when parsing German with such a parser. Concretely, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS) (Kallmeyer and Maier, 2010). LCFRS (Vijay-Shanker et al., 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals. We can directly interpret NeGra-style trees as its derivation structures, i.e., we can extract grammars without making further linguistic assumptions (Maier and Lichte, 2009) (see Sect. 2.3), as it is necessary for other formalisms such as Probabilistic Tree Adjoining Grammars (Chiang, 2003). Since the non-local dependencies are immediately accessible in NeGra and TIGER, we choose these treebanks as our data source. In order to judge parser output quality, we use four different evaluation types. We use an EVALB-style measure, adapted for LCFRS, in order to compare our parser to previous work on parsing German treebanks. In order to address the known shortcomings of EVALB, we perform an additional evaluation using the tree distance metric of Zhang and Shasha (1989), which works independently of the fact if there are crossing branches in the trees or not, and a dependency evaluatio</context>
</contexts>
<marker>Chiang, 2003</marker>
<rawString>David Chiang. 2003. Statistical parsing with an automatically extracted Tree Adjoining Grammar. In DataOriented Parsing. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="24145" citStr="Collins, 1999" startWordPosition="4251" endWordPosition="4252">ng LCFRS is harder than parsing PCFG. As for EVALB, the results for TIGER are slightly higher than the ones for NeGra. The distribution of the tree distances shows that about a third of all sentences receive a completely correct parse. More than a half, resp. a third of all parser output trees require ≤ 3 operations to be mapped to the corresponding gold tree, and a only a small percentage requires ≥ 10 operations. To our knowledge, TDIST has not been used to evaluate parser output for NeGra and TIGER. However, Emms (2008) reports results for the PTB using different parsers. Collins’ Model 1 (Collins, 1999), e.g., lies at 93.62 (Dice) and 87.87 (Jaccard). For the Berkeley Parser (Petrov and Klein, 2007), 94.72 and 89.87 is reported. We see that our results lie in them same range. However, Jaccard scores are lower since this normalization punishes a higher number of edit operations more severely than Dice. In order to meaningfully interpret which treebank properties are responsible for the fact that between the gold trees and the trees from the parser, the German data requires more tree edit operations than the English data, a TDIST evaluation of the output of an off-the-shelf PCFG parser would b</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Emms</author>
</authors>
<title>Tree Distance and some other variants of Evalb.</title>
<date>2008</date>
<booktitle>In Proceedings ofLREC 08.</booktitle>
<contexts>
<context position="15213" citStr="Emms, 2008" startWordPosition="2694" endWordPosition="2695">ivation tree with dim(A) = k and each (im�, im�), 1 &lt; m &lt; k, is a tuple of indices denotling a continuous sequence of terminals dominated by A. One set is obtained from the parser output, and 61 B&lt; B C B t3 B &gt;B t4 t1 t2 z t1 t2 z x y x y Figure 5: TDIST example one from the corresponding treebank trees. Using these tuple sets, we compute labeled and unlabeled recall (LR/UR), precision (LP/UP), and the F1 measure (LF1/UF1) in the usual way. Note that if k = 1, our metric is identical to its PCFG version. EVALB does not necessarily reflect parser output quality (Rehbein and van Genabith, 2007; Emms, 2008; K¨ubler et al., 2008). One of its major problems is that attachment errors are penalized too hard. As the second evaluation method, we therefore choose the tree-distance measure (henceforth TDIST) (Zhang and Shasha, 1989), which levitates this problem. It has been proposed for parser evaluation by Emms (2008). TDIST is an ideal candidate for evaluation of the output of a PLCFRS, since it the fact if trees have crossing branches or not is not relevant to it. Two trees τk and τA are compared on the basis of T-mappings from τk to τA. A T-mapping is a partial mapping Q of nodes of τk to nodes of</context>
<context position="17180" citStr="Emms (2008)" startWordPosition="3053" endWordPosition="3054">lly on algorithms which compute TDIST, refer to Bille (2005). In order to convert the tree distance measure into a similarity measure like EVALB, we use the macro-averaged Dice and Jaccard normalizations as defined by Emms. Let τK and τA be two trees with |τK |and |τA |nodes, respectively. For a T-mapping Q from τK to τA with the sets D, I, S and M, we compute them as follows. dice(Q) = 1 − |D |+ |+ I |+ IS| |τK |A | jaccard(Q) = 1 − |D |+ |I |+ |S| |D |+ |I |+ |S |+ |M| where, in order to achieve macro-averaging, we sum the numerators and denominators over all tree pairs before dividing. See Emms (2008) for further details. The third method is dependency evaluation (henceforth DEP), as described by Lin (1995). It consists of comparing dependency graphs extracted from the gold data and from the parser output. The dependency extraction algorithm as given by Lin does also not rely on trees to be free of crossing branches. It only relies on a method to identify the head of each phrase. We use our own implementation of the algorithm which is described in Sect. 4 of Lin (1995), combined with the head finding algorithm of the parser. Dependency evaluation abstracts away from another bias of EVALB. </context>
<context position="24059" citStr="Emms (2008)" startWordPosition="4238" endWordPosition="4239">31.80 63.81 4.56 Table 5: Tree distance evaluation Again, we can observe that parsing LCFRS is harder than parsing PCFG. As for EVALB, the results for TIGER are slightly higher than the ones for NeGra. The distribution of the tree distances shows that about a third of all sentences receive a completely correct parse. More than a half, resp. a third of all parser output trees require ≤ 3 operations to be mapped to the corresponding gold tree, and a only a small percentage requires ≥ 10 operations. To our knowledge, TDIST has not been used to evaluate parser output for NeGra and TIGER. However, Emms (2008) reports results for the PTB using different parsers. Collins’ Model 1 (Collins, 1999), e.g., lies at 93.62 (Dice) and 87.87 (Jaccard). For the Berkeley Parser (Petrov and Klein, 2007), 94.72 and 89.87 is reported. We see that our results lie in them same range. However, Jaccard scores are lower since this normalization punishes a higher number of edit operations more severely than Dice. In order to meaningfully interpret which treebank properties are responsible for the fact that between the gold trees and the trees from the parser, the German data requires more tree edit operations than the </context>
</contexts>
<marker>Emms, 2008</marker>
<rawString>Martin Emms. 2008. Tree Distance and some other variants of Evalb. In Proceedings ofLREC 08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos G´omez-Rodriguez</author>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
<author>David Weir</author>
</authors>
<title>Optimal reduction of rule length in linear context-free rewriting systems.</title>
<date>2009</date>
<booktitle>In Proceedings ofNAACL-HLT.</booktitle>
<marker>G´omez-Rodriguez, Kuhlmann, Satta, Weir, 2009</marker>
<rawString>Carlos G´omez-Rodriguez, Marco Kuhlmann, Giorgio Satta, and David Weir. 2009. Optimal reduction of rule length in linear context-free rewriting systems. In Proceedings ofNAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Kallmeyer</author>
<author>Wolfgang Maier</author>
</authors>
<title>An incremental earley parser for simple range concatenation grammar.</title>
<date>2009</date>
<booktitle>In Proceedings ofIWPT 09.</booktitle>
<contexts>
<context position="9114" citStr="Kallmeyer and Maier, 2009" startWordPosition="1622" endWordPosition="1625">lent to LCFRSs and SRCGs (Boullier, 1998). Scan: 0 : [A, ((i, i + 1))] A POS tag ofwi+1 Unary: in : [B, �ρ]p : A(�ρ) —* B(�ρ) E P in + |log(p) |: [A, �ρ] inB : [B, PB], inC : [C, Pc] Binary: inB + inC + log(p) : [A, Pa] where p : A( pA) —* B( pB)C( �ρC) is an instantiated rule. Goal: [S, ((0, n))] Figure 3: Weighted CYK deduction system 2.2 A CYK Parser for PLCFRS We use the parser of Kallmeyer and Maier (2010). It is a probabilistic CYK parser (Seki et al., 1991), using the technique of weighted deductive parsing (Nederhof, 2003). While for symbolic parsing, other elaborate algorithms exist (Kallmeyer and Maier, 2009), for probabilistic parsing, CYK is a natural choice. It is assumed for the parser that our LCFRSs are of rank 2 and do not contain rules where some of the LHS components are E. Both assumptions can be made without loss of generality since every LCFRS can be binarized (G´omez-Rodriguez et al., 2009) and E-components on LHS of rules can be removed (Boullier, 1998). We make the assumption that POS tagging is done before parsing. The POS tags are special non-terminals of fan-out 1. Consequently, the rules are either of the form A(a) —* E where A is a POS tag and a E T or of the form A(a) —* B(x) </context>
</contexts>
<marker>Kallmeyer, Maier, 2009</marker>
<rawString>Laura Kallmeyer and Wolfgang Maier. 2009. An incremental earley parser for simple range concatenation grammar. In Proceedings ofIWPT 09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Kallmeyer</author>
<author>Wolfgang Maier</author>
</authors>
<title>Datadriven parsing with probabilistic linear context-free rewriting systems.</title>
<date>2010</date>
<note>Unpublished Manuscript.</note>
<contexts>
<context position="3779" citStr="Kallmeyer and Maier, 2010" startWordPosition="582" endWordPosition="585">annotation, e.g., tree transformation algorithms are applied before parsing in order to resolve the crossing branches. See, e.g., K¨ubler et al. (2008) and Boyd (2007) for details. If one wants to avoid the loss of annotation information which is implied with such transformations, one possibility is to use a probabilistic parser for a formalism which is more expressive than CFG. In this paper, we tackle the question if qualitatively good results can be achieved when parsing German with such a parser. Concretely, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS) (Kallmeyer and Maier, 2010). LCFRS (Vijay-Shanker et al., 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals. We can directly interpret NeGra-style trees as its derivation structures, i.e., we can extract grammars without making further linguistic assumptions (Maier and Lichte, 2009) (see Sect. 2.3), as it is necessary for other formalisms such as Probabilistic Tree Adjoining Grammars (Chiang, 2003). Since the non-local dependencies are immediately accessible in NeGra and TIGER, we choose these treebanks as our data source. In order to judge pa</context>
<context position="8902" citStr="Kallmeyer and Maier (2010)" startWordPosition="1591" endWordPosition="1594">) = 1. There are possibly other ways to extend LCFRS with probabilities. This definition is supported by the fact that probabilistic MCFGs1 have been defined in the same way (Kato et al., 2006). 1MCFGs are equivalent to LCFRSs and SRCGs (Boullier, 1998). Scan: 0 : [A, ((i, i + 1))] A POS tag ofwi+1 Unary: in : [B, �ρ]p : A(�ρ) —* B(�ρ) E P in + |log(p) |: [A, �ρ] inB : [B, PB], inC : [C, Pc] Binary: inB + inC + log(p) : [A, Pa] where p : A( pA) —* B( pB)C( �ρC) is an instantiated rule. Goal: [S, ((0, n))] Figure 3: Weighted CYK deduction system 2.2 A CYK Parser for PLCFRS We use the parser of Kallmeyer and Maier (2010). It is a probabilistic CYK parser (Seki et al., 1991), using the technique of weighted deductive parsing (Nederhof, 2003). While for symbolic parsing, other elaborate algorithms exist (Kallmeyer and Maier, 2009), for probabilistic parsing, CYK is a natural choice. It is assumed for the parser that our LCFRSs are of rank 2 and do not contain rules where some of the LHS components are E. Both assumptions can be made without loss of generality since every LCFRS can be binarized (G´omez-Rodriguez et al., 2009) and E-components on LHS of rules can be removed (Boullier, 1998). We make the assumptio</context>
<context position="10795" citStr="Kallmeyer and Maier (2010)" startWordPosition="1935" endWordPosition="1938">re quickly than others (Klein and Manning, 2003a). The parser uses for this purpose an admissible, but not monotonic estimate called LR estimate. It gives (relative to a sentence length) an estimate of the outside probability of some non-terminal A with a span of a certain length (the sum of the lengths of all the 60 components of the span), a certain number of terminals to the left of the first and to the right of the last component and a certain number of terminals gaps in between the components of the A span, i.e., filling the gaps. A discussion of other estimates is presented at length in Kallmeyer and Maier (2010). 2.3 LCFRS for Modeling Discontinuities We use the algorithm from Maier and Søgaard (2008) to extract LCFRS rules from our data sets. For all nonterminals A0 with the children A1 · · · Am (i.e., for all non-terminals which are not preterminals), we create a clause 00 —* 01 · · · 0m with 0Z, 0 &lt; i &lt; m, labeled AZ. The arguments of each 0Z, 1 &lt; i &lt; m, are single variables, one for each of the continuous yield part dominated by the node AZ. The arguments of 00 are concatenations of these variables that describe how the discontinuous parts of the yield of A0 are obtained from the yields of its da</context>
<context position="22803" citStr="Kallmeyer and Maier (2010)" startWordPosition="4016" endWordPosition="4019">3 plain this work markov. latent 69.94 74.04 80.1 74.00 75.57 77.30 – Table 4: PCFG parsing of NeGra, Labeled Fl model. This difference is mostly likely due to losses induced by the LR estimate. All items to which the estimate assigns an outside log probability estimate of −oc get blocked and are not put on the agenda. This blocking has an extremely beneficial effect on parser speed. However, it is paid by a worse recall, as experiments with smaller data sets have shown. A complete discussion of the effects of estimates, as well as a discussion of other possible optimizations, is presented in Kallmeyer and Maier (2010). Recall finally that LCFRS parses are more informative than PCFG parses – a lower score for LCFRS EVALB than for PCFG EVALB does not necessarily mean that the PCFG parse is “better”. 4.2 Evaluation Using Tree Distance Tab. 5 shows the results of evaluating with TDIST, excluding unparsed sentences. We report the dice and jaccard normalizations, as well as a summary of the distribution of the tree distances between gold trees and trees from the parser output (see Sect. 3). dice jaccard tree distance distrib. 0 &lt;3 &gt;10 NeGra 88.86 79.79 31.65 53.77 15.08 TIGER 89.47 80.84 29.87 56.78 18.18 N-CF 9</context>
</contexts>
<marker>Kallmeyer, Maier, 2010</marker>
<rawString>Laura Kallmeyer and Wolfgang Maier. 2010. Datadriven parsing with probabilistic linear context-free rewriting systems. Unpublished Manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuki Kato</author>
<author>Hiroyuki Seki</author>
<author>Tadao Kasami</author>
</authors>
<title>RNA pseudoknotted structure prediction using stochastic multiple context-free grammar.</title>
<date>2006</date>
<journal>IPSJ Digital Courier,</journal>
<volume>2</volume>
<contexts>
<context position="8469" citStr="Kato et al., 2006" startWordPosition="1498" endWordPosition="1501">g the LHSs of instantiated rules with its RHSs. The language L(G) of some LCFRS G consists of all words w = w1 · · · wn for which it holds that there is a rule with the start symbol on the LHS which can be instantiated to (0, n) and rewritten to E. A probabilistic LCFRS (PLCFRS) is a tuple (N, T, V, P, S, p) such that (N, T, V, P, S) is a LCFRS and p : P —* [0..1] a function such that for all A E N: ΣA(9)11$EPp(A(x) —* -6) = 1. There are possibly other ways to extend LCFRS with probabilities. This definition is supported by the fact that probabilistic MCFGs1 have been defined in the same way (Kato et al., 2006). 1MCFGs are equivalent to LCFRSs and SRCGs (Boullier, 1998). Scan: 0 : [A, ((i, i + 1))] A POS tag ofwi+1 Unary: in : [B, �ρ]p : A(�ρ) —* B(�ρ) E P in + |log(p) |: [A, �ρ] inB : [B, PB], inC : [C, Pc] Binary: inB + inC + log(p) : [A, Pa] where p : A( pA) —* B( pB)C( �ρC) is an instantiated rule. Goal: [S, ((0, n))] Figure 3: Weighted CYK deduction system 2.2 A CYK Parser for PLCFRS We use the parser of Kallmeyer and Maier (2010). It is a probabilistic CYK parser (Seki et al., 1991), using the technique of weighted deductive parsing (Nederhof, 2003). While for symbolic parsing, other elaborate</context>
</contexts>
<marker>Kato, Seki, Kasami, 2006</marker>
<rawString>Yuki Kato, Hiroyuki Seki, and Tadao Kasami. 2006. RNA pseudoknotted structure prediction using stochastic multiple context-free grammar. IPSJ Digital Courier, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>A* Parsing: Fast Exact Viterbi Parse Selection.</title>
<date>2003</date>
<booktitle>In Proceedings ofNAACL-HLT.</booktitle>
<contexts>
<context position="10216" citStr="Klein and Manning, 2003" startWordPosition="1829" endWordPosition="1832">t 1. Consequently, the rules are either of the form A(a) —* E where A is a POS tag and a E T or of the form A(a) —* B(x) or A(a) —* B(x)C(y) where α� E (V+)di-(A), i.e., only the rules for POS tags contain terminals in their LHSs. The parser items have the form [A, pl, with A E N and ρ� a vector of ranges characterizing all components of the span of A. We specify the set of weighted parse items via the deduction rules in Fig. 3. Parsing time can be reduced by reordering the agenda during parsing such that those items are processed first which lead to a complete parse more quickly than others (Klein and Manning, 2003a). The parser uses for this purpose an admissible, but not monotonic estimate called LR estimate. It gives (relative to a sentence length) an estimate of the outside probability of some non-terminal A with a span of a certain length (the sum of the lengths of all the 60 components of the span), a certain number of terminals to the left of the first and to the right of the last component and a certain number of terminals gaps in between the components of the A span, i.e., filling the gaps. A discussion of other estimates is presented at length in Kallmeyer and Maier (2010). 2.3 LCFRS for Model</context>
<context position="12633" citStr="Klein and Manning, 2003" startWordPosition="2248" endWordPosition="2251">98), LCFRS cannot model re-entrancies, i.e., nodes with more than one incoming edge. While those do not occur in NeGrastyle annotation, some of the annotation in the PTB, e.g., the annotation for right node raising, can be interpreted as re-entrancies. This topic is left for future work. See Maier and Lichte (2009) for further details, especially on how treebank properties relate to properties of extracted grammars. Before parsing, we binarize our grammar. We first mark the head daughters of all non-terminal nodes using Collins-style head rules based on the NeGra rules of the Stanford Parser (Klein and Manning, 2003b) and the reorder the RHSs of all LCFRS rules such that sequence of elements to the right of the head daughter is reversed and moved to the beginning of the RHS. From this point, the binarization works like the transformation into Chomsky Normal Form for CFGs. For each rule with an RHS of length &gt; 3, we introduce a new non-terminal which covers the RHS without the first element and continue successively from left to right. The rightmost new rule, which covers the head daughter, is binarized to unary. We markovize the grammar as in the CFG case. To the new symbols introduced during the binariz</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003a. A* Parsing: Fast Exact Viterbi Parse Selection. In Proceedings ofNAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In In Advances in Neural Information Processing Systems 15 (NIPS).</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10216" citStr="Klein and Manning, 2003" startWordPosition="1829" endWordPosition="1832">t 1. Consequently, the rules are either of the form A(a) —* E where A is a POS tag and a E T or of the form A(a) —* B(x) or A(a) —* B(x)C(y) where α� E (V+)di-(A), i.e., only the rules for POS tags contain terminals in their LHSs. The parser items have the form [A, pl, with A E N and ρ� a vector of ranges characterizing all components of the span of A. We specify the set of weighted parse items via the deduction rules in Fig. 3. Parsing time can be reduced by reordering the agenda during parsing such that those items are processed first which lead to a complete parse more quickly than others (Klein and Manning, 2003a). The parser uses for this purpose an admissible, but not monotonic estimate called LR estimate. It gives (relative to a sentence length) an estimate of the outside probability of some non-terminal A with a span of a certain length (the sum of the lengths of all the 60 components of the span), a certain number of terminals to the left of the first and to the right of the last component and a certain number of terminals gaps in between the components of the A span, i.e., filling the gaps. A discussion of other estimates is presented at length in Kallmeyer and Maier (2010). 2.3 LCFRS for Model</context>
<context position="12633" citStr="Klein and Manning, 2003" startWordPosition="2248" endWordPosition="2251">98), LCFRS cannot model re-entrancies, i.e., nodes with more than one incoming edge. While those do not occur in NeGrastyle annotation, some of the annotation in the PTB, e.g., the annotation for right node raising, can be interpreted as re-entrancies. This topic is left for future work. See Maier and Lichte (2009) for further details, especially on how treebank properties relate to properties of extracted grammars. Before parsing, we binarize our grammar. We first mark the head daughters of all non-terminal nodes using Collins-style head rules based on the NeGra rules of the Stanford Parser (Klein and Manning, 2003b) and the reorder the RHSs of all LCFRS rules such that sequence of elements to the right of the head daughter is reversed and moved to the beginning of the RHS. From this point, the binarization works like the transformation into Chomsky Normal Form for CFGs. For each rule with an RHS of length &gt; 3, we introduce a new non-terminal which covers the RHS without the first element and continue successively from left to right. The rightmost new rule, which covers the head daughter, is binarized to unary. We markovize the grammar as in the CFG case. To the new symbols introduced during the binariz</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003b. Fast exact inference with a factored model for natural language parsing. In In Advances in Neural Information Processing Systems 15 (NIPS). MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Wolfgang Maier</author>
<author>Ines Rehbein</author>
<author>Yannick Versley</author>
</authors>
<title>How to compare treebanks.</title>
<date>2008</date>
<booktitle>In Proceedings ofLREC 08.</booktitle>
<marker>K¨ubler, Maier, Rehbein, Versley, 2008</marker>
<rawString>Sandra K¨ubler, Wolfgang Maier, Ines Rehbein, and Yannick Versley. 2008. How to compare treebanks. In Proceedings ofLREC 08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
</authors>
<title>How do treebank annotation schemes influence parsing results? Or how not to compare apples and oranges.</title>
<date>2005</date>
<booktitle>In Proceedings of RANLP</booktitle>
<marker>K¨ubler, 2005</marker>
<rawString>Sandra K¨ubler. 2005. How do treebank annotation schemes influence parsing results? Or how not to compare apples and oranges. In Proceedings of RANLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Giorgio Satta</author>
</authors>
<title>Treebank grammar techniques for non-projective dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings ofEACL.</booktitle>
<marker>Kuhlmann, Satta, 2009</marker>
<rawString>Marco Kuhlmann and Giorgio Satta. 2009. Treebank grammar techniques for non-projective dependency parsing. In Proceedings ofEACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
</authors>
<title>Probabilistic Models of Word Order and Syntactic Discontinuity.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="28985" citStr="Levy, 2005" startWordPosition="5060" endWordPosition="5061">risingly, getting the right attachment positions seems to be hard for LCFRS, too. Additionally, with crossing branches, the output is rated worse, since some attachments are not present anymore without crossing branches. Since especially for the relative clauses, attachment positions are in fact a matter of discussion from a syntactic point of view, we will consider in future studies to selectively resolve some of the crossing branches, e.g., by attaching relative clauses to higher positions. 5 Related Work The use of formalisms with a high expressivity has been explored before (Plaehn, 2004; Levy, 2005). To our knowledge, Plaehn is the only one to report evaluation results. He uses the formalism of Discontinuous Phrase Structure Grammar (DPSG). Limiting the sentence length to 15, he obtains 73.16 labeled F1 on NeGra. Evaluating all sentences of our NeGra data with a length of up to 15 words results, however, in 81.27 labeled F1. For a comparison between DPSG and LCFRS, refer to Maier and Søgaard (2008). 6 Conclusion and Future Work We have investigated the possibility of using Probabilistic Linear Context-Free Rewriting Systems for direct parsing of discontinuous constituents. Consequently, </context>
</contexts>
<marker>Levy, 2005</marker>
<rawString>Roger Levy. 2005. Probabilistic Models of Word Order and Syntactic Discontinuity. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A dependency-based method for evaluating broad-coverage parsers.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI 95.</booktitle>
<contexts>
<context position="4844" citStr="Lin, 1995" startWordPosition="757" endWordPosition="758">ince the non-local dependencies are immediately accessible in NeGra and TIGER, we choose these treebanks as our data source. In order to judge parser output quality, we use four different evaluation types. We use an EVALB-style measure, adapted for LCFRS, in order to compare our parser to previous work on parsing German treebanks. In order to address the known shortcomings of EVALB, we perform an additional evaluation using the tree distance metric of Zhang and Shasha (1989), which works independently of the fact if there are crossing branches in the trees or not, and a dependency evaluation (Lin, 1995), which has also be applied before in the context of parsing German (K¨ubler et al., 2008). Last, we evaluate certain difficult phenomena by hand on TePaCoC (K¨ubler et al., 2008), a set of sentences hand-picked from TIGER. The evaluations show that with a PLCFRS parser, competitive results can be achieved. The remainder of the article is structured as follows. In Sect. 2, we present the formalism, the parser, and how we obtain our grammars. In Sect. 3, we discuss the evaluation methods we employ. Sect. 4 contains our experimental results. Sect. 5 is dedicated to related work. Sect. 6 contains</context>
<context position="17288" citStr="Lin (1995)" startWordPosition="3070" endWordPosition="3071">nto a similarity measure like EVALB, we use the macro-averaged Dice and Jaccard normalizations as defined by Emms. Let τK and τA be two trees with |τK |and |τA |nodes, respectively. For a T-mapping Q from τK to τA with the sets D, I, S and M, we compute them as follows. dice(Q) = 1 − |D |+ |+ I |+ IS| |τK |A | jaccard(Q) = 1 − |D |+ |I |+ |S| |D |+ |I |+ |S |+ |M| where, in order to achieve macro-averaging, we sum the numerators and denominators over all tree pairs before dividing. See Emms (2008) for further details. The third method is dependency evaluation (henceforth DEP), as described by Lin (1995). It consists of comparing dependency graphs extracted from the gold data and from the parser output. The dependency extraction algorithm as given by Lin does also not rely on trees to be free of crossing branches. It only relies on a method to identify the head of each phrase. We use our own implementation of the algorithm which is described in Sect. 4 of Lin (1995), combined with the head finding algorithm of the parser. Dependency evaluation abstracts away from another bias of EVALB. Concretely, it does not prefer trees with a high node/token ratio, since two dependency graphs to be compare</context>
<context position="25713" citStr="Lin (1995)" startWordPosition="4513" endWordPosition="4514">The dependency results are consistent with the previous results in as much as the scores for PCFG parsing are again higher. The dependency results reported in K¨ubler et al. (2008) however are much higher (85.6 UAS for the markovized Stanford parser). While a part of the losses can again be attributed to the LR estimate, another reason lies undoubtedly in the different dependency conversion method which we employ, and in further treebank transformations which K¨ubler et al. perform. In order to get a more fine grained result, in future work, we will consider graph modifications as proposed by Lin (1995) as well as including annotation-specific information from NeGra/TIGER in our conversion procedure. 4.4 TePaCoC The TePaCoC data set (K¨ubler et al., 2008) provides 100 hand-picked sentences from TIGER which contain constructions that are especially difficult to NeGra TIGER 64 parse. Out of these 100 sentences, we only consider 69. The remaining 31 sentences are either longer than 30 words or not included in the TIGER 2003 release (K¨ubler et al. use the 2005 release). The data is partitioned in groups of sentences with extraposed relative clauses (ERC), forward conjunction reduction (FCR), no</context>
</contexts>
<marker>Lin, 1995</marker>
<rawString>Dekang Lin. 1995. A dependency-based method for evaluating broad-coverage parsers. In Proceedings of IJCAI 95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
<author>Timm Lichte</author>
</authors>
<title>Characterizing discontinuity in constituent treebanks.</title>
<date>2009</date>
<booktitle>In Proceedings ofFormal Grammar</booktitle>
<contexts>
<context position="4113" citStr="Maier and Lichte, 2009" startWordPosition="635" endWordPosition="638">formalism which is more expressive than CFG. In this paper, we tackle the question if qualitatively good results can be achieved when parsing German with such a parser. Concretely, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS) (Kallmeyer and Maier, 2010). LCFRS (Vijay-Shanker et al., 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals. We can directly interpret NeGra-style trees as its derivation structures, i.e., we can extract grammars without making further linguistic assumptions (Maier and Lichte, 2009) (see Sect. 2.3), as it is necessary for other formalisms such as Probabilistic Tree Adjoining Grammars (Chiang, 2003). Since the non-local dependencies are immediately accessible in NeGra and TIGER, we choose these treebanks as our data source. In order to judge parser output quality, we use four different evaluation types. We use an EVALB-style measure, adapted for LCFRS, in order to compare our parser to previous work on parsing German treebanks. In order to address the known shortcomings of EVALB, we perform an additional evaluation using the tree distance metric of Zhang and Shasha (1989)</context>
<context position="12326" citStr="Maier and Lichte (2009)" startWordPosition="2201" endWordPosition="2204">rees without crossing branches, this algorithm yields a PLCFRS with fan-out 1, i.e., a PCFG. As mentioned before, the advantage of using LCFRS is that grammar extraction is straightforward and that no separate assumptions must be made. Note that unlike, e.g., Range Concatenation Grammar (RCG) (Boullier, 1998), LCFRS cannot model re-entrancies, i.e., nodes with more than one incoming edge. While those do not occur in NeGrastyle annotation, some of the annotation in the PTB, e.g., the annotation for right node raising, can be interpreted as re-entrancies. This topic is left for future work. See Maier and Lichte (2009) for further details, especially on how treebank properties relate to properties of extracted grammars. Before parsing, we binarize our grammar. We first mark the head daughters of all non-terminal nodes using Collins-style head rules based on the NeGra rules of the Stanford Parser (Klein and Manning, 2003b) and the reorder the RHSs of all LCFRS rules such that sequence of elements to the right of the head daughter is reversed and moved to the beginning of the RHS. From this point, the binarization works like the transformation into Chomsky Normal Form for CFGs. For each rule with an RHS of le</context>
</contexts>
<marker>Maier, Lichte, 2009</marker>
<rawString>Wolfgang Maier and Timm Lichte. 2009. Characterizing discontinuity in constituent treebanks. In Proceedings ofFormal Grammar 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
<author>Anders Søgaard</author>
</authors>
<title>Treebanks and mild context-sensitivity.</title>
<date>2008</date>
<booktitle>In Proceedings ofFormal Grammar</booktitle>
<contexts>
<context position="10886" citStr="Maier and Søgaard (2008)" startWordPosition="1949" endWordPosition="1952">ible, but not monotonic estimate called LR estimate. It gives (relative to a sentence length) an estimate of the outside probability of some non-terminal A with a span of a certain length (the sum of the lengths of all the 60 components of the span), a certain number of terminals to the left of the first and to the right of the last component and a certain number of terminals gaps in between the components of the A span, i.e., filling the gaps. A discussion of other estimates is presented at length in Kallmeyer and Maier (2010). 2.3 LCFRS for Modeling Discontinuities We use the algorithm from Maier and Søgaard (2008) to extract LCFRS rules from our data sets. For all nonterminals A0 with the children A1 · · · Am (i.e., for all non-terminals which are not preterminals), we create a clause 00 —* 01 · · · 0m with 0Z, 0 &lt; i &lt; m, labeled AZ. The arguments of each 0Z, 1 &lt; i &lt; m, are single variables, one for each of the continuous yield part dominated by the node AZ. The arguments of 00 are concatenations of these variables that describe how the discontinuous parts of the yield of A0 are obtained from the yields of its daughters. For all preterminals A dominating some terminal a, we extract a production A(a) —*</context>
<context position="29392" citStr="Maier and Søgaard (2008)" startWordPosition="5130" endWordPosition="5133">ctively resolve some of the crossing branches, e.g., by attaching relative clauses to higher positions. 5 Related Work The use of formalisms with a high expressivity has been explored before (Plaehn, 2004; Levy, 2005). To our knowledge, Plaehn is the only one to report evaluation results. He uses the formalism of Discontinuous Phrase Structure Grammar (DPSG). Limiting the sentence length to 15, he obtains 73.16 labeled F1 on NeGra. Evaluating all sentences of our NeGra data with a length of up to 15 words results, however, in 81.27 labeled F1. For a comparison between DPSG and LCFRS, refer to Maier and Søgaard (2008). 6 Conclusion and Future Work We have investigated the possibility of using Probabilistic Linear Context-Free Rewriting Systems for direct parsing of discontinuous constituents. Consequently, we have applied a PLCFRS parser on the German NeGra and TIGER treebanks. Our evaluation, which used different metrics, showed that a PLCFRS parser can achieve competitive results. In future work, all of the presented evaluation methods will be investigated to greater detail. In order to do this, we will parse our data sets with current state-of-the-art systems. Especially a more elaborate dependency conv</context>
</contexts>
<marker>Maier, Søgaard, 2008</marker>
<rawString>Wolfgang Maier and Anders Søgaard. 2008. Treebanks and mild context-sensitivity. In Proceedings ofFormal Grammar 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings ofHLT.</booktitle>
<contexts>
<context position="1376" citStr="Marcus et al., 1994" startWordPosition="197" endWordPosition="200"> different metrics, we show that an output quality can be achieved which is comparable to the output quality of PCFG-based systems. 1 Introduction Languages with a rather free word order, like German, display discontinuous constituents particularly frequently. In (1), the discontinuity is caused by an extraposed relative clause. (1) wieder again In most constituency treebanks, sentence annotation is restricted to having the shape of trees without crossing branches, and the non-local dependencies induced by the discontinuities are modeled by an additional mechanism. In the Penn Treebank (PTB) (Marcus et al., 1994), e.g., this mechanism is a combination of special labels and empty nodes, establishing implicit additional edges. In the German T¨uBa-D/Z (Telljohann et al., 2006), additional edges are established by a combination of topological field annotation and special edge labels. As an example, Fig. 1 shows a tree from T¨uBa-D/Z with the annotation of (1). Note here the edge label ONMOD on the relative clause which indicates that the subject of the sentence (alle Attribute) is modified. treffen match Attribute attributes zu, VPART alle all auch also die which sonst otherwise immer always passen fit ‘A</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings ofHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Weighted Deductive Parsing and Knuth’s Algorithm.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="9024" citStr="Nederhof, 2003" startWordPosition="1612" endWordPosition="1613">MCFGs1 have been defined in the same way (Kato et al., 2006). 1MCFGs are equivalent to LCFRSs and SRCGs (Boullier, 1998). Scan: 0 : [A, ((i, i + 1))] A POS tag ofwi+1 Unary: in : [B, �ρ]p : A(�ρ) —* B(�ρ) E P in + |log(p) |: [A, �ρ] inB : [B, PB], inC : [C, Pc] Binary: inB + inC + log(p) : [A, Pa] where p : A( pA) —* B( pB)C( �ρC) is an instantiated rule. Goal: [S, ((0, n))] Figure 3: Weighted CYK deduction system 2.2 A CYK Parser for PLCFRS We use the parser of Kallmeyer and Maier (2010). It is a probabilistic CYK parser (Seki et al., 1991), using the technique of weighted deductive parsing (Nederhof, 2003). While for symbolic parsing, other elaborate algorithms exist (Kallmeyer and Maier, 2009), for probabilistic parsing, CYK is a natural choice. It is assumed for the parser that our LCFRSs are of rank 2 and do not contain rules where some of the LHS components are E. Both assumptions can be made without loss of generality since every LCFRS can be binarized (G´omez-Rodriguez et al., 2009) and E-components on LHS of rules can be removed (Boullier, 1998). We make the assumption that POS tagging is done before parsing. The POS tags are special non-terminals of fan-out 1. Consequently, the rules ar</context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>Mark-Jan Nederhof. 2003. Weighted Deductive Parsing and Knuth’s Algorithm. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL</booktitle>
<contexts>
<context position="21610" citStr="Petrov and Klein (2007)" startWordPosition="3802" endWordPosition="3805">e slightly higher than for than for NeGra and TIGER. This is due to the fact that during the transformation, some errors in the LCFRS parses get “corrected”: Wrongly attached phrasal nodes are re-attached to unique higher positions in the trees. In order to give a point of comparison with previous work on parsing TIGER and NeGra, in Tab. 4, we report some of the results from the literature. All of them were obtained using PCFG parsers: K¨ubler (2005) (Tab. 1, plain PCFG for NeGra), K¨ubler et al. (2008) (Tab. 3, plain PCFG and Stanford parser with markovization v = 2 and h = 1 for TIGER), and Petrov and Klein (2007) (Tab. 1, Berkeley parser, latent variables). We include the results for N-CF and T-CF. Our results are slightly better than for the plain PCFG models. We would expect the result for TCF to be closer to the corresponding result for the Stanford parser, since we are using a comparable 5 10 15 20 25 Sentence length time in sec. (log scale) 0.01 100 0.1 10 1 NeGra TIGER N-CF T-CF LP LR LFI UP UR UFI NeGra 72.39 70.68 71.52 76.01 74.22 75.10 TIGER 74.97 71.95 73.43 78.58 75.42 76.97 N-CF 74.85 73.26 74.04 78.11 76.45 77.28 T-CF 77.51 73.73 75.57 80.59 76.66 78.57 63 plain this work markov. latent </context>
<context position="24243" citStr="Petrov and Klein, 2007" startWordPosition="4265" endWordPosition="4268">gher than the ones for NeGra. The distribution of the tree distances shows that about a third of all sentences receive a completely correct parse. More than a half, resp. a third of all parser output trees require ≤ 3 operations to be mapped to the corresponding gold tree, and a only a small percentage requires ≥ 10 operations. To our knowledge, TDIST has not been used to evaluate parser output for NeGra and TIGER. However, Emms (2008) reports results for the PTB using different parsers. Collins’ Model 1 (Collins, 1999), e.g., lies at 93.62 (Dice) and 87.87 (Jaccard). For the Berkeley Parser (Petrov and Klein, 2007), 94.72 and 89.87 is reported. We see that our results lie in them same range. However, Jaccard scores are lower since this normalization punishes a higher number of edit operations more severely than Dice. In order to meaningfully interpret which treebank properties are responsible for the fact that between the gold trees and the trees from the parser, the German data requires more tree edit operations than the English data, a TDIST evaluation of the output of an off-the-shelf PCFG parser would be necessary. This is left for future work. 4.3 Dependency Evaluation For the dependency evaluation</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLTNAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Plaehn</author>
</authors>
<title>Computing the most probable parse for a discontinuous phrase-structure grammar. In New developments in parsing technology.</title>
<date>2004</date>
<publisher>Kluwer.</publisher>
<contexts>
<context position="28972" citStr="Plaehn, 2004" startWordPosition="5058" endWordPosition="5059">m up, not surprisingly, getting the right attachment positions seems to be hard for LCFRS, too. Additionally, with crossing branches, the output is rated worse, since some attachments are not present anymore without crossing branches. Since especially for the relative clauses, attachment positions are in fact a matter of discussion from a syntactic point of view, we will consider in future studies to selectively resolve some of the crossing branches, e.g., by attaching relative clauses to higher positions. 5 Related Work The use of formalisms with a high expressivity has been explored before (Plaehn, 2004; Levy, 2005). To our knowledge, Plaehn is the only one to report evaluation results. He uses the formalism of Discontinuous Phrase Structure Grammar (DPSG). Limiting the sentence length to 15, he obtains 73.16 labeled F1 on NeGra. Evaluating all sentences of our NeGra data with a length of up to 15 words results, however, in 81.27 labeled F1. For a comparison between DPSG and LCFRS, refer to Maier and Søgaard (2008). 6 Conclusion and Future Work We have investigated the possibility of using Probabilistic Linear Context-Free Rewriting Systems for direct parsing of discontinuous constituents. C</context>
</contexts>
<marker>Plaehn, 2004</marker>
<rawString>Oliver Plaehn. 2004. Computing the most probable parse for a discontinuous phrase-structure grammar. In New developments in parsing technology. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing three German treebanks: Lexicalized and unlexicalized baselines.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Parsing German at ACL</booktitle>
<contexts>
<context position="19739" citStr="Rafferty and Manning, 2008" startWordPosition="3484" endWordPosition="3487">y limitations, we restrict ourselves to sentences of a maximal length of 30 words. Our TIGER data sets (TIGER and T-CF) have 31,568 sentences of an average length of 14.81, splitted into 31,568 sentences for training and 3,508 sentences for testing. Our NeGra data sets (NeGra and N-CF) have 18,335 sentences, splitted into 16,501 sentences for training and 1,834 sentences for testing. We parse the data sets described above with activated LR estimate. For all our experiments, we use the markovization settings v = 2 and h = 1, which have proven to be successful in previous work on parsing NeGra (Rafferty and Manning, 2008). We provide the parser with the gold tagging. Fig. 6 shows the average parsing times for all data sets on an AMD Opteron node with 8GB of RAM (pure Java implementation), Tab. 1 shows the percentage of parsed sentences. Figure 6: Parsing times NeGra TIGER N-CF T-CF total 1834 3508 1834 3508 parsed 1779 3462 1804 3462 (97.0%) (98.7%) (98.4%) (98.7%) Table 1: Parsed sentences 4.1 Evaluation Using EVALB Tab. 2 shows the evaluation of the parser output using EVALB, as described in the previous section. We report labeled and unlabeled precision, recall and F1 measure. Table 2: EVALB results Not sur</context>
</contexts>
<marker>Rafferty, Manning, 2008</marker>
<rawString>Anna Rafferty and Christopher D. Manning. 2008. Parsing three German treebanks: Lexicalized and unlexicalized baselines. In Proceedings of the Workshop on Parsing German at ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef van Genabith</author>
</authors>
<title>Evaluating evaluation measures.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA</booktitle>
<marker>Rehbein, van Genabith, 2007</marker>
<rawString>Ines Rehbein and Josef van Genabith. 2007. Evaluating evaluation measures. In Proceedings of NODALIDA 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Seki</author>
<author>Takahashi Matsumura</author>
<author>Mamoru Fujii</author>
<author>Tadao Kasami</author>
</authors>
<title>On multiple context-free grammars.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<volume>88</volume>
<issue>2</issue>
<contexts>
<context position="8956" citStr="Seki et al., 1991" startWordPosition="1601" endWordPosition="1604">abilities. This definition is supported by the fact that probabilistic MCFGs1 have been defined in the same way (Kato et al., 2006). 1MCFGs are equivalent to LCFRSs and SRCGs (Boullier, 1998). Scan: 0 : [A, ((i, i + 1))] A POS tag ofwi+1 Unary: in : [B, �ρ]p : A(�ρ) —* B(�ρ) E P in + |log(p) |: [A, �ρ] inB : [B, PB], inC : [C, Pc] Binary: inB + inC + log(p) : [A, Pa] where p : A( pA) —* B( pB)C( �ρC) is an instantiated rule. Goal: [S, ((0, n))] Figure 3: Weighted CYK deduction system 2.2 A CYK Parser for PLCFRS We use the parser of Kallmeyer and Maier (2010). It is a probabilistic CYK parser (Seki et al., 1991), using the technique of weighted deductive parsing (Nederhof, 2003). While for symbolic parsing, other elaborate algorithms exist (Kallmeyer and Maier, 2009), for probabilistic parsing, CYK is a natural choice. It is assumed for the parser that our LCFRSs are of rank 2 and do not contain rules where some of the LHS components are E. Both assumptions can be made without loss of generality since every LCFRS can be binarized (G´omez-Rodriguez et al., 2009) and E-components on LHS of rules can be removed (Boullier, 1998). We make the assumption that POS tagging is done before parsing. The POS tag</context>
</contexts>
<marker>Seki, Matsumura, Fujii, Kasami, 1991</marker>
<rawString>Hiroyuki Seki, Takahashi Matsumura, Mamoru Fujii, and Tadao Kasami. 1991. On multiple context-free grammars. Theoretical Computer Science, 88(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Skut</author>
<author>Brigitte Krenn</author>
<author>Thorten Brants</author>
<author>Hans Uszkoreit</author>
</authors>
<title>An annotation scheme for free word order languages.</title>
<date>1997</date>
<booktitle>In Proceedings ofANLP.</booktitle>
<contexts>
<context position="2311" citStr="Skut et al., 1997" startWordPosition="353" endWordPosition="356">-D/Z with the annotation of (1). Note here the edge label ONMOD on the relative clause which indicates that the subject of the sentence (alle Attribute) is modified. treffen match Attribute attributes zu, VPART alle all auch also die which sonst otherwise immer always passen fit ‘Again, the same attributes as always apply.’ Another language with a rather free word order is Bulgarian. In (2), the discontinuity is caused by topicalization. ‘As for pens, I only buy expensive ones.’ 58 Figure 1: A tree from T¨uBa-D/Z However, in a few other treebanks, such as the German NeGra and TIGER treebanks (Skut et al., 1997; Brants et al., 2002), crossing branches are allowed. This way, all dependents of a long-distance dependency can be grouped under a single node. (2) X14M14Kaai41 a3 Pens1 I KyHysaM buy caMo esTHHH t1 only expensive t1 Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 58–66, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics Fig. 2 shows a tree from NeGra with the annotation of (3). (3) Noch Yet ‘Never have I had that much choice.’ Note the direct annotation of the discontinuous VP. Figure 2: A tre</context>
<context position="18305" citStr="Skut et al., 1997" startWordPosition="3237" endWordPosition="3240">ng algorithm of the parser. Dependency evaluation abstracts away from another bias of EVALB. Concretely, it does not prefer trees with a high node/token ratio, since two dependency graphs to be compared necessarily have the same number of (terminal) nodes. In the context of parsing German, this evaluation has been employed previously by K¨ubler et al. (2008). Last, we evaluate on TePaCoC (Testing Parser Performance on Complex Grammatical Constructions), a set of particularly difficult sentences hand-picked from TIGER (K¨ubler et al., 2008). 4 Experiments Our data sources are the German NeGra (Skut et al., 1997) and TIGER (Brants et al., 2002) treebanks. In a preprocessing step, following common practice, we attach all punctuation to nodes within the tree, since it is not included in the NeGra annotation. In a first pass, using heuristics, we attach all nodes to the in each case highest available phrasal node such that ideally, we do not introduce new crossing branches. In a second pass, parentheses and quotation marks are preferably attached to the same node. Grammatical function labels are 62 discarded. After this preprocessing step, we create a separate version of the data set, in which we resolve</context>
</contexts>
<marker>Skut, Krenn, Brants, Uszkoreit, 1997</marker>
<rawString>Wojciech Skut, Brigitte Krenn, Thorten Brants, and Hans Uszkoreit. 1997. An annotation scheme for free word order languages. In Proceedings ofANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heike Telljohann</author>
<author>Erhard Hinrichs</author>
<author>Sandra K¨ubler</author>
<author>Heike Zinsmeister</author>
</authors>
<title>Stylebook for the T¨ubingen Treebank of Written German (T¨uBa-D/Z). Technischer Bericht, Universit¨at T¨ubingen.</title>
<date>2006</date>
<marker>Telljohann, Hinrichs, K¨ubler, Zinsmeister, 2006</marker>
<rawString>Heike Telljohann, Erhard Hinrichs, Sandra K¨ubler, and Heike Zinsmeister. 2006. Stylebook for the T¨ubingen Treebank of Written German (T¨uBa-D/Z). Technischer Bericht, Universit¨at T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
<author>Aravind K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3815" citStr="Vijay-Shanker et al., 1987" startWordPosition="587" endWordPosition="590">on algorithms are applied before parsing in order to resolve the crossing branches. See, e.g., K¨ubler et al. (2008) and Boyd (2007) for details. If one wants to avoid the loss of annotation information which is implied with such transformations, one possibility is to use a probabilistic parser for a formalism which is more expressive than CFG. In this paper, we tackle the question if qualitatively good results can be achieved when parsing German with such a parser. Concretely, we use a parser for Probabilistic Linear Context-Free Rewriting Systems (PLCFRS) (Kallmeyer and Maier, 2010). LCFRS (Vijay-Shanker et al., 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals. We can directly interpret NeGra-style trees as its derivation structures, i.e., we can extract grammars without making further linguistic assumptions (Maier and Lichte, 2009) (see Sect. 2.3), as it is necessary for other formalisms such as Probabilistic Tree Adjoining Grammars (Chiang, 2003). Since the non-local dependencies are immediately accessible in NeGra and TIGER, we choose these treebanks as our data source. In order to judge parser output quality, we use four dif</context>
<context position="5875" citStr="Vijay-Shanker et al., 1987" startWordPosition="930" endWordPosition="933">e parser, and how we obtain our grammars. In Sect. 3, we discuss the evaluation methods we employ. Sect. 4 contains our experimental results. Sect. 5 is dedicated to related work. Sect. 6 contains the conclusion and presents some possible future work. 2 A Parser for PLCFRS 2.1 Probabilistic Linear Context-Free Rewriting Systems LCFRS are an extension of CFG where the nonterminals can span not only single strings but, instead, tuples of strings. We will notate LCFRS with the syntax of simple Range Concatenation Grammars (SRCG) (Boullier, 1998), a formalism that is equivalent to LCFRS. A LCFRS (Vijay-Shanker et al., 1987) is a tuple G = (N, T, V, P, 5) where a) N is a finite set of non-terminals with a function dim: N , N that determines the fan-out of each A E N; b) T and V are disjoint finite sets of terminals and variables; c) 5 E N is the start symbol with dim(5) = 1; d) P is a finite set of rewriting rules A(α1, ... , αdim(A)) , A1(X(1) 1 , ... , X(1) dim(A1)) ··· Am(X(m) 1 , ... , X(m) dim(A,n)) Noch nie habe ich so viel gewählt ADV ADV VAFIN PPER ADV ADV VVPP $. 1.Sg.Pres.Ind 1.Sg.*.Nom MO HD AVP MO HD S OC VP SB MO HD AVP MO HD nie habe ich so viel gew¨ahlt never have I so much chosen 59 form &gt; 0 where</context>
</contexts>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi. 1987. Characterizing structural descriptions produced by various grammatical formalisms. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaizhong Zhang</author>
<author>Dennis Shasha</author>
</authors>
<title>Simple fast algorithms for the editing distance between trees and related problems.</title>
<date>1989</date>
<journal>SIAMJournal of Computing,</journal>
<volume>18</volume>
<contexts>
<context position="4713" citStr="Zhang and Shasha (1989)" startWordPosition="733" endWordPosition="736">Maier and Lichte, 2009) (see Sect. 2.3), as it is necessary for other formalisms such as Probabilistic Tree Adjoining Grammars (Chiang, 2003). Since the non-local dependencies are immediately accessible in NeGra and TIGER, we choose these treebanks as our data source. In order to judge parser output quality, we use four different evaluation types. We use an EVALB-style measure, adapted for LCFRS, in order to compare our parser to previous work on parsing German treebanks. In order to address the known shortcomings of EVALB, we perform an additional evaluation using the tree distance metric of Zhang and Shasha (1989), which works independently of the fact if there are crossing branches in the trees or not, and a dependency evaluation (Lin, 1995), which has also be applied before in the context of parsing German (K¨ubler et al., 2008). Last, we evaluate certain difficult phenomena by hand on TePaCoC (K¨ubler et al., 2008), a set of sentences hand-picked from TIGER. The evaluations show that with a PLCFRS parser, competitive results can be achieved. The remainder of the article is structured as follows. In Sect. 2, we present the formalism, the parser, and how we obtain our grammars. In Sect. 3, we discuss </context>
<context position="15436" citStr="Zhang and Shasha, 1989" startWordPosition="2728" endWordPosition="2731">B t4 t1 t2 z t1 t2 z x y x y Figure 5: TDIST example one from the corresponding treebank trees. Using these tuple sets, we compute labeled and unlabeled recall (LR/UR), precision (LP/UP), and the F1 measure (LF1/UF1) in the usual way. Note that if k = 1, our metric is identical to its PCFG version. EVALB does not necessarily reflect parser output quality (Rehbein and van Genabith, 2007; Emms, 2008; K¨ubler et al., 2008). One of its major problems is that attachment errors are penalized too hard. As the second evaluation method, we therefore choose the tree-distance measure (henceforth TDIST) (Zhang and Shasha, 1989), which levitates this problem. It has been proposed for parser evaluation by Emms (2008). TDIST is an ideal candidate for evaluation of the output of a PLCFRS, since it the fact if trees have crossing branches or not is not relevant to it. Two trees τk and τA are compared on the basis of T-mappings from τk to τA. A T-mapping is a partial mapping Q of nodes of τk to nodes of τA where all node mappings preserve left-to-right order and ancestry. Within the mappings, node insertion, node deletion, and label swap operations are identified, represented resp. by the sets I, D and S. Furthermore, we </context>
</contexts>
<marker>Zhang, Shasha, 1989</marker>
<rawString>Kaizhong Zhang and Dennis Shasha. 1989. Simple fast algorithms for the editing distance between trees and related problems. SIAMJournal of Computing, 18.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>