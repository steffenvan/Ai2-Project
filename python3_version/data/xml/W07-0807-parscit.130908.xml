<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.803365">
Finding Variants of Out-of-Vocabulary Words in Arabic
</title>
<author confidence="0.370748">
Abdusalam F.A. Nwesri S.M.M. Tahaghoghi Falk Scholer
</author>
<affiliation confidence="0.3352085">
School of Computer Science and Information Technology
RMIT University, GPO Box 2476V, Melbourne 3001, Australia
</affiliation>
<email confidence="0.966807">
{nwesri,saied,fscholer}@cs.rmit.edu.au
</email>
<sectionHeader confidence="0.997033" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999756363636363">
Transliteration of a word into another lan-
guage often leads to multiple spellings. Un-
less an information retrieval system recog-
nises different forms of transliterated words,
a significant number of documents will be
missed when users specify only one spelling
variant. Using two different datasets, we
evaluate several approaches to finding vari-
ants of foreign words in Arabic, and show
that the longest common subsequence (LCS)
technique is the best overall.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999548181818182">
The pronunciation of a word in one language is
converted into the phonemes of another language
through transliteration. This is particularly com-
mon with proper nouns. However, phonetics dif-
fer across languages, and transliteration usually re-
sults in many spellings for the same word. This
is an issue even across languages that use substan-
tially the same character set; simple examples would
be “colour” and “color” across British and Ameri-
can usage, and “ambience” and “ambiance” across
French and English.
A change in character sets compounds the prob-
lem: for instance, there are at least 32 English
forms for the Arabic name of the Libyan leader
“Kaddafi”,1 and Nwesri et al. (2006) have identi-
fied 28 different spellings for the name of the for-
mer Serbian president Milosevic in the eleventh Text
REtrieval Conference (TREC) Arabic newswire col-
lection. Users typically submit only one spelling
variant in their query, and current Arabic text re-
trieval systems return only documents that contain
that variant (Abdelali et al., 2004). We apply tech-
</bodyText>
<footnote confidence="0.906219">
1http://www.geocities.com/Athens/8744/
spelling.htm
</footnote>
<bodyText confidence="0.990564">
niques used to identify similar strings in other lan-
guages such as English, and present a novel ap-
proach to identify and retrieve different variants of
foreign words in Arabic.
</bodyText>
<sectionHeader confidence="0.963045" genericHeader="introduction">
2 The Arabic Language
</sectionHeader>
<bodyText confidence="0.97212844">
Arabic is a Semitic language written from right to
left, with most words derived from three-character
root words. The Arabic alphabet has 28 characters,
each with a distinct sound. Short vowels do not have
any associated characters, but are instead indicated
by diacritics attached to other characters. For ex-
ample, the letter  /f/ with the diacritic Fatha 
is pronounced /fa/,2 with the diacritic Kasra  is

pronounced /fI/, and with the diacritic Damma  is

pronounced /fU/.
In general written Arabic, diacritics are not in-
dicated; readers must rely on context to determine
implicit diacritics, and so how the word should be
pronounced. For example, some of the variants of
the word  are  /kataba/ (he wrote),  /kU-

tUb/ (books), or  /kUtIba/ (is written).
There are also three long vowels — represented
by the letters {  } — that are more pronounced

than short vowels. For instance, the letter  can
be followed by the long vowel  /a:/ to form  /fa:/,
by /u:/ to form  /fu:/, and by  /i:/ to form   /fi:/.
</bodyText>
<subsectionHeader confidence="0.56683">
2.1 Foreign Words
</subsectionHeader>
<bodyText confidence="0.999823">
From an information retrieval (IR) perspective, for-
eign words in Arabic can be classified into two gen-
eral categories: translated and transliterated (Nwesri
et al., 2006). Translated words, sometimes referred
to as Arabised words, are foreign words that are
modified or remodelled to conform to Arabic word
</bodyText>
<footnote confidence="0.722368">
2We use the International Phonetic Alphabet.
</footnote>
<page confidence="0.983502">
49
</page>
<note confidence="0.9205265">
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 49–56,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999717774193548">
paradigms, and are well assimilated into the lan-
guage. The assimilation process includes changes
in the structure of the borrowed word, including
segmental and vowel changes, addition or dele-
tion of syllables, and modification of stress pat-
terns (Al-Qinal, 2002). Foreign words of this cate-
gory usually have a single consistent spelling vari-
ant, for example  virus,    archive,
and radio.
Where equivalent native terms are not available
early enough for widespread adoption, foreign terms
are used directly with their original pronunciation
represented using Arabic letters. As these do not
appear in standard Arabic lexicons — that may in-
clude adopted words — they are considered to be
Out-Of-Vocabulary (OOV) words.
With transliterated words, the phonemes of a for-
eign word are replaced with their nearest Arabic
equivalents. Since Arabic phonemes cannot repre-
sent all phonemes found in other languages, the orig-
inal phonemes are usually not represented uniformly
by different transliterators, resulting in multiple
spelling variants for the same foreign word (Stalls
and Knight, 1998).
Faced with the need to use new foreign terms, na-
tive speakers often cannot wait for formal equiva-
lents to be defined. This is particularly true for news
agencies, which encounter new foreign nouns and
technical terms daily. This urgency leads to more
transliteration than translation, with the associated
problem of multiple spellings.
</bodyText>
<subsectionHeader confidence="0.999807">
2.2 Spelling Variants
</subsectionHeader>
<bodyText confidence="0.992990260869565">
In Arabic, short vowels must be indicated using dia-
critics, but these are rarely used in general text, and
there are no standard rules on when and where di-
acritics must be indicated. Context does not help
in predicting diacritics for foreign words such as
proper nouns or technical terms, and so long vowels
are often used to make the pronunciation explicit in
the spelling of the word without relying on diacrit-
ics. This, too, is subject to variation; some translit-
erators add a long vowel after each consonant in the
word, while others add just enough long vowels to
clarify word segments with ambiguous pronuncia-
tion.
The absence of certain sounds in Arabic, and
varying pronunciations across dialects, also con-
tributes to the multiplicity of spellings. For instance,
the sound /g/ has no standard equivalent in Ara-
bic, since transliterators represent it according to
how they pronounce it. For instance, the English
letter G /g/ is at times mapped to the Arabic let-
ters  /G/,  /q/, or  /Z/ (Abduljaleel and Larkey,
2003); we have also observed it mapped to the let-
ter  /k/:
</bodyText>
<equation confidence="0.588462">
  ,   ,   ,
</equation>
<bodyText confidence="0.990971285714286">
and are transliterations of Gorbachev
we have found on the Web.
Similarly, the interpretation of character combi-
nations varies between transliterators. Moreover,
Typographical and phonetic errors during translit-
eration may add even more variants (Borgman and
Siegfried, 1992).
</bodyText>
<subsectionHeader confidence="0.999676">
2.3 Retrieval of Variants
</subsectionHeader>
<bodyText confidence="0.99980047826087">
When different variants of a word exist, only a sub-
set of related documents can be found when the
search uses only one variant. Typical search en-
gine users are unlikely to recognise the problem and
hence do not add other variants to their query. Cur-
rently, major search engines such as Google, Yahoo,
and MSN search use exact match for Arabic search,
and no publicly available Arabic Information Re-
trieval (AIR) system has been reported to retrieve
different spelling variants (Abdelali et al., 2004).
In this paper we explore how the different vari-
ants of a foreign word may be captured. We test
existing similarity techniques, and introduce three
techniques to search for variants of foreign words
in Arabic. In the first technique, we convert differ-
ent variants to a single normalised form by remov-
ing vowels and conflating homophones. In the sec-
ond technique, we extend the well-known Soundex
technique — commonly used to identify variants of
names in English — to the OOV problem in Arabic,
and in the third technique, we modify the English
Editex algorithm to identify similar foreign words
in Arabic.
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.99758875">
Approaches to identify similar-sounding but differ-
ently spelt words have been heavily investigated in
English; among these are techniques that make use
of string or phonetic similarity.
</bodyText>
<page confidence="0.992393">
50
</page>
<bodyText confidence="0.99945125">
String similarity approaches include the Edit Dis-
tance (Hall and Dowling, 1980), used to measure
the similarity of two strings by counting the minimal
number of character insertions, deletions, or replace-
ments needed to transform one string into another.
To transpose a string s of length n into a string t
of length m, edit(m, n) computes the minimal steps
required as follows:
</bodyText>
<equation confidence="0.993809">
edit(0, 0) = 0
edit(i, 0) = i
edit(0, j) = j
edit(i, j) = min[edit(i − 1, j) + 1,
edit(i, j − 1) + 1,
edit(i − 1, j − 1) + d(si, ti)]
</equation>
<bodyText confidence="0.9952659375">
where d(si, ti) = 1 if si = ti, 0 otherwise.
This measure can be used to rank words in the
collection with respect to a query word. Zobel and
Dart (1995) showed that Edit Distance performed
the best among the techniques they evaluated for
matching English names. It is not known how this
technique will perform with Arabic words.
Another candidate approach that can be used
to identify similar foreign words in Arabic is
n-grams (Hall and Dowling, 1980). This approach
is language independent; the strings are divided into
grams (substrings) of length n, and the similarity of
the strings is computed on the basis of the similarity
of their n-grams. Pfeifer et al. (1996) compute the
similarity as the number of shared grams divided by
the total number of distinct grams in the two strings.
</bodyText>
<equation confidence="0.7112865">
gramCount =  |G3 n Gt |
 |G3 U Gt |
</equation>
<bodyText confidence="0.985411352941176">
where G3 is the set of grams in string s. For ex-
ample, with n=2, the similarity of “ahmed” and
“ahmmed” using this measure is 0.8 because both
strings contain the four 2-grams ah, hm, me, and
ed, while there are five distinct 2-grams across the
two strings.
Gram distance (Ukkonen, 1992) is another string
similarity technique. When grams are not repeated
– which is the case in names — the similarity is
computed as (Zobel and Dart, 1996):
gramDist(s, t) = |G3  |+  |Gt  |−2  |G3 n Gt |
According to this measure, the similarity between
“ahmed” and “ahmmed” is 1.
With the Dice (1945) measure, the similarity of
strings s and t is computed as twice the number of
common n-grams between s and t, divided by the
total number of n-grams in the two strings:
</bodyText>
<equation confidence="0.998331">
2x  |G3 n Gt |
Dice(s, t) =
 |G3  |+  |Gt |
</equation>
<bodyText confidence="0.996897720930232">
where G3 denotes the set of n-grams in s, and Gt
denotes the set of n-grams in t.
The longest common subsequence (LCS) algo-
rithm measures the similarity between two strings
based on the common characters in the two
strings (Wagner and Fischer, 1974; Stephen, 1992).
Similarity is normalised by dividing the length of
the common subsequence by the length of the longer
string (Melamed, 1995). The similarity between be-
tween “ahmed” and “ahmmed” is (5/6=0.833).
Phonetic approaches to determine similarity be-
tween two words include the well-known Soundex
algorithm developed by Odell and Russell, patented
in 1918 and 1922 (Hall and Dowling, 1980). This
has predefined codes for the sounds in a language,
with similar-sounding letters grouped under one
code. During comparisons, all letters in a word bar
the first one are encoded, and the resulting represen-
tation is truncated to be at most four characters long.
A variant of Soundex is the Phonix algorithm (Gadd,
1990), which transforms letter groups to letters and
then to codes; the actual mappings are different from
Soundex. Both Soundex and Phonix have been re-
ported to have poorer precision in identifying vari-
ants of English names than both Edit Distance and
n-grams (Zobel and Dart, 1995).
Aqeel et al. (2006) propose an Arabic version of
English Soundex (Asoundex-final). They include di-
acritics in a list of Arabic names, and created queries
by altering some of these names by adding, deleting,
or inserting characters.
Most Arabic names are meaningful words — for
example,  (the praised one) — and rarely do
have spelling variants. This leads to morphological
ambiguity as names may match verbs, pronouns and
other categories of the language. We have found that
using Asoundex-final with the misspelt query 
on an Arabic collection with 35 949 unique
words returns   (exaggeration),   (she
becomes sad),  (she resolves),  (she
helps),  (improvement),  (immun-
isation),(she governs),  (she defeats).
Moreover, it is not clear when and how diacritics
</bodyText>
<page confidence="0.991657">
51
</page>
<bodyText confidence="0.9778813">
are removed, nor where the long vowel  belongs
in their implementation.
Editex, developed by Zobel and Dart (1996),
enhances the Edit Distance technique by incorpo-
rating the letter-grouping strategy used by Soundex
and Phonix, and has been shown to have better
performance than these two algorithms, as well as
Edit Distance, on a collection of 30 000 distinct
English names. The similarity between two strings
s and t is computed as:
</bodyText>
<equation confidence="0.9957728">
edit(0, 0) = 0
edit(i, 0) = edit(i − 1, 0) + d(sz − 1, s1)
edit(0.j) = edit(0, j − 1) + d(tj − 1, tj)
edit(i.j) = min[edit(i − 1, j) + d(sz − 1, sz),
edit(i, j − 1) + d(tj − 1, tj),
edit(i − 1, j − 1) + r(sz, tj)]
where: r(sz,tj) is 0 if sz=tj, 1 if
group(sz)=group(tj), and 2 otherwise; and
d(sz, tj) is 1 if sz =� tj and sz is “h” or “w”, and
r(sz, tj) otherwise.
</equation>
<sectionHeader confidence="0.993389" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.99988725">
We used two different data sets. The first set is gen-
erated from text crawled from the Web, and the sec-
ond is prepared by manual transliteration of foreign
words from English to Arabic.
</bodyText>
<subsectionHeader confidence="0.990944">
4.1 Crawled Data
</subsectionHeader>
<bodyText confidence="0.995518578947369">
This set is derived from a one-gigabyte crawl of Ara-
bic web pages from twelve different online news
sites. From this data we extracted 18 873 073 Ara-
bic words, 383 649 of them unique. We used the
Microsoft Office 2003 Arabic spellchecker to build
a reference list of OOV words. To avoid dupli-
cates in the 40 514 OOV words returned by the
spellchecker, we removed the first character if it is
an Arabic preposition, and if the string remaining
after that character exists in the collectionWe also
removed the definite article “Al” to obtain a list
of 32 583 words. Through manual inspection, we
identified 2 039 foreign words.
To evaluate alternative techniques, we use a ref-
erence list of foreign words and their variants. To
identify variants, we generated all possible spelling
variants of each word according to the patterns we
describe in Section 4.1.1, and kept only the patterns
that exist in our collection; 556 clusters of foreign
</bodyText>
<tableCaption confidence="0.88106">
Table 1: Variants of the word “Beckham” generated
by adding vowels
</tableCaption>
<table confidence="0.9175862">
   
   
   
   
words remain.
</table>
<subsectionHeader confidence="0.777714">
4.1.1 Generation of Variants
</subsectionHeader>
<bodyText confidence="0.99951509375">
To generate foreing words variants, we first re-
move any vowels and then reinsert vowel combi-
nations of the three long vowels 1  1 between
the consonants that remain. For a word of length n,
this process generates 4(n−1) variants. Consider the
word  (Beckham). We remove vowels to ob-
tain , and then add all possible vowels to obtain
the variants shown in Table 1.
As discussed in Section 2.2, inconsistent repre-
sentation of sounds between transliterators adds to
the variations in spelling. Thus, the number of
possible transliterations for a foreign word is given
by 4(n−1) multiplied by the number of possible
transliterations for each of its consonants. In our ex-
ample, the letter  /q/ may also be used in place
of  /k/, and so we generate another set; since the
representation tends to be consistent within a given
word, we need to create only as many sets as there
are Arabic representations for the sound.
We validate the generated variants against our
collection and keep only those that appear in the
crawled text. For our example word “Beckham”,
we found only two correct variants:  and
Some of the generated variants could be correct Ara-
bic words that would be valid when checked against
the collection. Many of the generated clusters were
found to be noisy – that is, they included many na-
tive Arabic words. We manually corrected these
clusters by removing unrelated Arabic words. The
average cluster length is 2.8 words; the smallest
cluster has two variants, and the largest has nine,
with a total of 1 718 words.
</bodyText>
<subsectionHeader confidence="0.885628">
4.2 Transliterated Data
</subsectionHeader>
<bodyText confidence="0.972064666666667">
Our second collection reflects one pattern in which
OOV words are introduced by ordinary users
.
</bodyText>
<page confidence="0.991065">
52
</page>
<bodyText confidence="0.999698176470588">
transliterating English words into Arabic. We ex-
tracted a list of 1 134 foreign words from the
TREC 2002 Arabic collection, and passed these to
the Google translation engine to obtain their En-
glish equivalents. We manually inspected these and
corrected any incorrect translations. We also re-
moved the 57 words mapped by Google to multi-
ple English words. These are usually a word and
a possible conjunction or preposition. For example
the word r_k*...591 (Luxembourg) is transliterated
to (For June). We passed the English list to seven
Arabic native speakers and asked them to translit-
erate each word in the list back into Arabic, even
if the word has an Arabic equivalent. Four of the
translators are PhD candidates in the sciences or en-
gineering, and have finished an advanced-level En-
glish course; the other three are currently enrolled
in an intermediate-level English course. Participants
were asked to type in their transliteration next to
each English word. We noticed that some translit-
erators had only basic computing skills, and made
many spelling mistakes. For example, instead of
typing the character 1, we found that transliterators
sometimes mistakenly type J.
We clustered transliterations by the original En-
glish words, removed duplicates from each cluster,
and also removed 103 clusters where all transliter-
ators agreed on the same version of transliteration.
This left 3 582 words in 207 clusters of size 2, 252
clusters of size 3, 192 clusters of size 4, 149 clus-
ters of size 5, 93 clusters of size 6, and 47 clusters
of size 7. Finally, we incorporated these transliter-
ations into a list with 35 949 unique Arabic native
words prepared by Nwesri et al. (2006).
</bodyText>
<sectionHeader confidence="0.997745" genericHeader="method">
5 Algorithms
</sectionHeader>
<bodyText confidence="0.998243166666667">
We propose three algorithms to identify foreign
words in Arabic text. The first is normalisation,
which aims to handle different types of typograph-
ical errors described in Section 2.1. The second
and third techniques are extensions to the English
Soundex and Editex techniques.
</bodyText>
<subsectionHeader confidence="0.927929">
5.1 Normalisation
</subsectionHeader>
<bodyText confidence="0.952700333333333">
To deal with different typographical styles in writ-
ing foreign words, we first remove vowels from ev-
ery foreign term. We keep vowels unchanged if they
</bodyText>
<tableCaption confidence="0.9740025">
Table 2: Normalisation of equivalent consonants to
a single form
</tableCaption>
<subsectionHeader confidence="0.402986">
Original Normalised
</subsectionHeader>
<bodyText confidence="0.999915620689655">
are the first or the last characters of the word, since
they are generally pronounced in Arabic. The long
vowel letters are sometimes used as consonants, and
these may be followed immediately by another long
vowel. For example, the vowel letter S /i/ may be
followed by the long vowel 9 /u:/ to form 9= /ju:/.
For such cases, we keep the first vowel and remove
the second. Two vowels can also be used together
as diphthongs, as in 91 /aw/ and SI /aj/. We re-
tain vowels that are followed by another vowel or
preceded by a vowel that forms a diphthong. We
also conflate similar consonants based on statisti-
cal analysis of letter mappings between English and
Arabic (Abduljaleel and Larkey, 2003; Stalls and
Knight, 1998), and confirming through a web search
that these consonants are used interchangeably in
web documents.3 Table 2 shows all consonants we
consider to be equivalent.
Our process may lead to ambiguity where a simi-
lar native word exists; for instance, the spelling vari-
ants �1�,, and �L for (Beckham) are normalised
to, which is identical to the Arabic word mean-
ing either (how much) or (in you). Adding a cus-
tom prefix to the normalised form is one way to ad-
dress this issue; we add the letter “o to the begin-
ning of each normalised word. For example, variants
for Beckham are thus normalised to r- S. Since the
letter S never occurs at the beginning of any Arabic
word, no ambiguity remains.
</bodyText>
<subsectionHeader confidence="0.998406">
5.2 Phonetic Approach
</subsectionHeader>
<bodyText confidence="0.99981075">
Our phonetic algorithm aims to replace similar
sounds with a single code. As noted earlier, we do
not envisage that this algorithm has use for native
Arabic words, as these are usually distinct, and pro-
</bodyText>
<tableCaption confidence="0.881272">
3All phonetic groups are created based on transliteration
mapping between English and Arabic letters
</tableCaption>
<figure confidence="0.9945705">
k-r
J0
t
C-)
A
k-r k-r
u J0
J 00
C t� J v
u
</figure>
<page confidence="0.99616">
53
</page>
<tableCaption confidence="0.996942">
Table 3: Mappings for our phonetic approach
</tableCaption>
<subsectionHeader confidence="0.296677">
Characters Code
</subsectionHeader>
<equation confidence="0.995980857142857">
   0

      1
    2
  3
    4
  5
 6
 7
 8
 9
 A
 B
 C
</equation>
<bodyText confidence="0.999967666666667">
nunciation is rarely ambiguous. Table 3 shows Ara-
bic letters and their corresponding codes. To nor-
malise foreign words, we replace each letter but the
first by its phonetic code, and drop any vowels. We
have found — as have (Aqeel et al., 2006) and (Zo-
bel and Dart, 1996) — that it is better to encode all
letters, rather than only the first four characters; for
brevity, we show only the results for coding all char-
acters, under the label “Soutex”.
</bodyText>
<subsectionHeader confidence="0.999102">
5.3 Arabic Editex
</subsectionHeader>
<bodyText confidence="0.990983888888889">
Based on groups identified in Table 4, we have mod-
ified the Editex algorithm of Zobel and Dart (1996).
It works as in English except that we drop the func-
tionality used to consider the two silent characters
in the English version as silent characters in Arabic
are rare and usually occur at the beginning or at the
end of the word. Specifically, we replace d(sZ7 tj)
by r(sZ7 tj). We call the Arabic version of this algo-
rithm “AEditex”.
</bodyText>
<sectionHeader confidence="0.997815" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999102166666667">
To evaluate the effectiveness of our approaches, we
consider each word in the list to be a query, and
pose this to the entire collection. The query result
should be other words in the same cluster. We con-
sider every word to be a query to avoid any bias to-
wards string similarity techniques as phonetic based
</bodyText>
<tableCaption confidence="0.995199">
Table 4: AEditex letter groups
</tableCaption>
<table confidence="0.9990719">
Characters Group
   0
   1
  2
  3
  4
  5
  6
  7
    8
</table>
<bodyText confidence="0.995531259259259">
techniques fail to capture misspelled words whereas
string similarity techniques do.
The results returned by the different algorithms
described in the previous section are not directly
comparable, as some algorithms return ranked
results and others return unranked results. Ranked
results could also form a weak ordering in which
multiple results belong to the same rank (Ragha-
van et al., 1989). Standard information retrieval
measures are not appropriate for evaluating such
techniques. Zobel and Dart (1996) address this by
using standard precision and recall, but randomly
permute results of equal ranks and calculate the
average of recall and precision over ten different
permutations. Raghavan et al (1989) propose
a different measure called Probability of Rele-
vance (PRR). This measure assumes that the user
randomly selects a document from the topmost
ranks. At any point the precision is defined as the
probability that the random examined document is
relevant. Recall is the number of relevant documents
that the user has seen so far. If we require NR
relevant documents — in our case, words — from a
ranked result, we start by looking at the top answer
and continue until we get to the NRth relevant
word at rank k. The PRR measure is calculated
as (Raghavan et al., 1989):
</bodyText>
<equation confidence="0.9822425">
NR
PRR = NR + j + (i.s)/(r + 1)
</equation>
<bodyText confidence="0.99970975">
Where j is the number of non-relevant words found
in ranks before k, s is the number of remaining rel-
evant words still to be retrieved in rank k, i is the
number of non-relevant words in rank k, and r is the
</bodyText>
<page confidence="0.986062">
54
</page>
<figure confidence="0.957087666666667">
Precision
0 0.5 1
Recall
</figure>
<figureCaption confidence="0.999968">
Figure 1: Results on the crawled data
Figure 2: Results on the transliterated data
</figureCaption>
<figure confidence="0.999555342857143">
0.8
0.6
0.4
0.2
0
1
NORM
Soutex
LCS
AEditex
Edit Distance
gramCount
Dice
Asoundex-Final
gramDist
Exact match
0 0.5 1
Recall
Precision
0.8
0.6
0.4
0.2
0
1
LCS
Edit Distance
AEditex
Soutex
gramCount
Dice
NORM
Asoundex-Final
gramDist
Exact match
</figure>
<bodyText confidence="0.698372333333333">
number of relevant words in rank k. Interpolation
is used to smooth results and calculate an average
across all queries.
</bodyText>
<subsectionHeader confidence="0.756672">
6.1 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.99787048">
Results from running algorithms using queries in
both datasets against their respective collection are
shown in Figure 1 and Figure 2. The average preci-
sion (average PRR in our case) for each algorithm is
shown in Table 5. The algorithms produce signifi-
cantly better results than exact match (p&lt;0.0001).
On the first data set, NORM performs the best.
LCS is the second best algorithm, followed by AEdi-
tex and Edit Distance. Soutex shows better perfor-
mance than all other algorithms except NORM af-
ter 50% recall, but performs poorly at lower recall
levels. Both the gramCount and Dice algorithms
have similar performance with average precision at
around 46%. Asoundex-final and gramDist show
poorer performance than other algorithms, with av-
erage precision at 38%.
Asoundex-final performs poorly; As mentioned
earlier, the absence of diacritics in typical Arabic
text makes it hard to generalise this technique to re-
trieve names.
Results from the transliterated dataset generally
favour the string similarity algorithms. LCS outper-
forms all other techniques with an average precision
of 78%, followed by Edit Distance at 70%, and then
AEditex at 62%. Soutex performs better than both
</bodyText>
<tableCaption confidence="0.993234">
Table 5: Average precision results
</tableCaption>
<table confidence="0.98658475">
Data set
Algorithm First Second
NORM 0.660 0.536
LCS 0.619 0.782
Edit Distance 0.572 0.700
AEditex 0.576 0.624
Soutex 0.530 0.590
gramCount 0.451 0.595
Dice 0.457 0.568
Asoundex-final 0.368 0.446
gramDist 0.376 0.401
Exact Match 0.300 0.261
</table>
<bodyText confidence="0.999847769230769">
the gramCount and Dice algorithms. It performs
better than AEditex at 50% and higher recall lev-
els. NORM performs better than the Asoundex-final
and gramDist algorithms. The gramDist algorithm
is again the worst. All algorithms showed signifi-
cant improvements above the baseline (p&lt;0.0001).
Although NORM and Soutex algorithms do not
produce the best performance, they have the advan-
tage of being run at index time to encode foreign
words which can be later used in retrieval. The al-
ternative algorithms such as Edit Distance are more
computationally expensive and can only be used at
query time.
</bodyText>
<page confidence="0.998157">
55
</page>
<sectionHeader confidence="0.999106" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999987606060606">
Foreign words transliterated into Arabic can appear
with multiple spellings, hindering effective recall in
a text-retrieval system. In this work, we have eval-
uated nine techniques to find such variants. Edit
Distance, Gram Count, Dice, Asoundex-final, Gram
Distance, and Longest Common Subsequence are
language independent techniques used to find vari-
ant names in other languages; Soutex and AEdi-
tex are extended techniques to accommodate Ara-
bic Words; and NORM is a novel technique to find
OOV variants in Arabic. We show that these tech-
niques are effective for finding foreign word vari-
ants. The phonetic approaches generally perform
better on a collection of newswire text than on a
manually transliterated word list, although our Sou-
tex algorithm performs well on both datasets. LCS
was the best of the string-similarity techniques, es-
pecially with the manually transliterated dataset, and
is the most robust choice overall.
The way the transliterated dataset was created af-
fected the results of phonetic approaches; the dataset
has many spelling mistakes, with words interpreted
differently and often wrongly by users not fluent in
English. Often users only hear these words in the
news, and are not even familiar with the spelling of
the word in the original language. To construct a
more realistic data set, we could ask Arabic writers
to transliterate words from a recording; this would
allow pronunciation to be accurately captured by
users not fluent in English.
Information retrieval systems must cater for com-
mon spelling variants; our results help understand
how to identify these in Arabic text.
</bodyText>
<sectionHeader confidence="0.99945" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999801538461539">
Ahmed Abdelali, Jim Cowie, and Hamdy S. Soliman. 2004.
Arabic information retrieval perspectives. In Proceedings
of the 11th Conference on Natural Language Processing,
Journes d’Etude sur la Parole - TraitementAutomatique des
Langues Naturelles (JEP-TALN), Fez, Morocco.
Nasreen Abduljaleel and Leah S. Larkey. 2003. Statistical
transliteration for English-Arabic cross-language informa-
tion retrieval. In Proceedings of the International Confer-
ence on Information and Knowledge Management, pages
139–146, New Orleans, LA, USA. ACM Press.
Jamal B. S. Al-Qinal. 2002. Morphophonemics of loanwords
in translation. Journal of King Saud University, 13:1–132.
Syed Uzair Aqeel, Steve Beitzel, Eric Jensen, David Grossman,
and Ophir Frieder. 2006. On the development of name
search techniques for Arabic. Journal of the American Soci-
ety for Information Science and Technology, 57(6):728–739.
Christine L. Borgman and Susan L. Siegfried. 1992. Getty’s
synoname and its cousins: A survey of applications of per-
sonal name-matching algorithms. Journal of the American
Society for Information Science, 43(7):459–476.
Lee R. Dice. 1945. Measures of the amount of ecologic associ-
ation between species. Ecology, 26(3):297–302, July.
T. Gadd. 1990. Phonix: the algorithm. Program, 24(4):363–
369.
Patrick A. V. Hall and Geoff R. Dowling. 1980. Approximate
string matching. ACM Computing Surveys, 12(4):381–402.
Dan Melamed. 1995. Automatic evaluation and uniform filter
cascades for inducing N-best translation lexicons. In David
Yarovsky and Kenneth Church, editors, Proceedings of the
Third Workshop on Very Large Corpora, pages 184–198,
Somerset, New Jersey. Association for Computational Lin-
guistics.
Abdusalam F Ahmad Nwesri, S. M. M. Tahaghoghi, and Falk
Scholer. 2006. Capturing out-of-vocabulary words in Ara-
bic text. In Proceedings of the 2006 Conference on Em-
pirical Methods in Natural Language Processing (EMNLP
2006), pages 258–266, Sydney, Australia, 22–23 July. Asso-
ciation for Computational Linguistics.
Ulrich Pfeifer, Thomas Poersch, and Norbert Fuhr. 1996. Re-
trieval effectiveness of proper name search methods. Inf.
Process. Manage., 32(6):667–679.
Vijay Raghavan, Peter Bollmann, and Gwang S. Jung. 1989.
A critical investigation of recall and precision as measures
of retrieval system performance. ACM Trans. Inf. Syst.,
7(3):205–229.
Bonnie Glover Stalls and Kevin Knight. 1998. Translating
names and technical terms in Arabic text. In COLING/ACL
Workshop on Computational Approaches to Semitic Lan-
guages, pages 34–41, Montreal, Quebc, Canada.
Graham A Stephen. 1992. String search. Technical report,
School of Electronic Engineering Science, University Col-
lege of North Wales.
Esko Ukkonen. 1992. Approximate string-matching with
q-grams and maximal matches. Theor. Comput. Sci.,
92(1):191–211.
Robert A. Wagner and Michael J. Fischer. 1974. The string-to-
string correction problem. J. ACM, 21(1):168–173.
Justin Zobel and Philip Dart. 1995. Finding approximate
matches in large lexicons. Software - Practice and Expe-
rience, 25(3):331–345.
Justin Zobel and Philip Dart. 1996. Phonetic string matching:
lessons from information retrieval. In The 19th annual in-
ternational ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 166–172, New York,
NY, USA. ACM Press.
</reference>
<page confidence="0.998423">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.762818">
<title confidence="0.999747">Finding Variants of Out-of-Vocabulary Words in Arabic</title>
<author confidence="0.999062">Abdusalam F A Nwesri S M M Tahaghoghi Falk</author>
<affiliation confidence="0.895324">School of Computer Science and Information RMIT University, GPO Box 2476V, Melbourne 3001,</affiliation>
<abstract confidence="0.996125833333333">Transliteration of a word into another language often leads to multiple spellings. Unless an information retrieval system recognises different forms of transliterated words, a significant number of documents will be missed when users specify only one spelling variant. Using two different datasets, we evaluate several approaches to finding variants of foreign words in Arabic, and show that the longest common subsequence (LCS) technique is the best overall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmed Abdelali</author>
<author>Jim Cowie</author>
<author>Hamdy S Soliman</author>
</authors>
<title>Arabic information retrieval perspectives.</title>
<date>2004</date>
<booktitle>In Proceedings of the 11th Conference on Natural Language Processing, Journes d’Etude sur la Parole - TraitementAutomatique des Langues Naturelles (JEP-TALN),</booktitle>
<location>Fez, Morocco.</location>
<contexts>
<context position="1777" citStr="Abdelali et al., 2004" startWordPosition="267" endWordPosition="270">r” across British and American usage, and “ambience” and “ambiance” across French and English. A change in character sets compounds the problem: for instance, there are at least 32 English forms for the Arabic name of the Libyan leader “Kaddafi”,1 and Nwesri et al. (2006) have identified 28 different spellings for the name of the former Serbian president Milosevic in the eleventh Text REtrieval Conference (TREC) Arabic newswire collection. Users typically submit only one spelling variant in their query, and current Arabic text retrieval systems return only documents that contain that variant (Abdelali et al., 2004). We apply tech1http://www.geocities.com/Athens/8744/ spelling.htm niques used to identify similar strings in other languages such as English, and present a novel approach to identify and retrieve different variants of foreign words in Arabic. 2 The Arabic Language Arabic is a Semitic language written from right to left, with most words derived from three-character root words. The Arabic alphabet has 28 characters, each with a distinct sound. Short vowels do not have any associated characters, but are instead indicated by diacritics attached to other characters. For ex- ample, the letter  /</context>
<context position="7076" citStr="Abdelali et al., 2004" startWordPosition="1121" endWordPosition="1124">honetic errors during transliteration may add even more variants (Borgman and Siegfried, 1992). 2.3 Retrieval of Variants When different variants of a word exist, only a subset of related documents can be found when the search uses only one variant. Typical search engine users are unlikely to recognise the problem and hence do not add other variants to their query. Currently, major search engines such as Google, Yahoo, and MSN search use exact match for Arabic search, and no publicly available Arabic Information Retrieval (AIR) system has been reported to retrieve different spelling variants (Abdelali et al., 2004). In this paper we explore how the different variants of a foreign word may be captured. We test existing similarity techniques, and introduce three techniques to search for variants of foreign words in Arabic. In the first technique, we convert different variants to a single normalised form by removing vowels and conflating homophones. In the second technique, we extend the well-known Soundex technique — commonly used to identify variants of names in English — to the OOV problem in Arabic, and in the third technique, we modify the English Editex algorithm to identify similar foreign words in </context>
</contexts>
<marker>Abdelali, Cowie, Soliman, 2004</marker>
<rawString>Ahmed Abdelali, Jim Cowie, and Hamdy S. Soliman. 2004. Arabic information retrieval perspectives. In Proceedings of the 11th Conference on Natural Language Processing, Journes d’Etude sur la Parole - TraitementAutomatique des Langues Naturelles (JEP-TALN), Fez, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nasreen Abduljaleel</author>
<author>Leah S Larkey</author>
</authors>
<title>Statistical transliteration for English-Arabic cross-language information retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Information and Knowledge Management,</booktitle>
<pages>139--146</pages>
<publisher>ACM Press.</publisher>
<location>New Orleans, LA, USA.</location>
<contexts>
<context position="6149" citStr="Abduljaleel and Larkey, 2003" startWordPosition="973" endWordPosition="976"> diacritics. This, too, is subject to variation; some transliterators add a long vowel after each consonant in the word, while others add just enough long vowels to clarify word segments with ambiguous pronunciation. The absence of certain sounds in Arabic, and varying pronunciations across dialects, also contributes to the multiplicity of spellings. For instance, the sound /g/ has no standard equivalent in Arabic, since transliterators represent it according to how they pronounce it. For instance, the English letter G /g/ is at times mapped to the Arabic letters  /G/,  /q/, or  /Z/ (Abduljaleel and Larkey, 2003); we have also observed it mapped to the letter  /k/:   ,   ,   , and are transliterations of Gorbachev we have found on the Web. Similarly, the interpretation of character combinations varies between transliterators. Moreover, Typographical and phonetic errors during transliteration may add even more variants (Borgman and Siegfried, 1992). 2.3 Retrieval of Variants When different variants of a word exist, only a subset of related documents can be found when the search uses only one variant. Typical search engine users are unlikely to r</context>
<context position="18871" citStr="Abduljaleel and Larkey, 2003" startWordPosition="3151" endWordPosition="3154"> generally pronounced in Arabic. The long vowel letters are sometimes used as consonants, and these may be followed immediately by another long vowel. For example, the vowel letter S /i/ may be followed by the long vowel 9 /u:/ to form 9= /ju:/. For such cases, we keep the first vowel and remove the second. Two vowels can also be used together as diphthongs, as in 91 /aw/ and SI /aj/. We retain vowels that are followed by another vowel or preceded by a vowel that forms a diphthong. We also conflate similar consonants based on statistical analysis of letter mappings between English and Arabic (Abduljaleel and Larkey, 2003; Stalls and Knight, 1998), and confirming through a web search that these consonants are used interchangeably in web documents.3 Table 2 shows all consonants we consider to be equivalent. Our process may lead to ambiguity where a similar native word exists; for instance, the spelling variants �1�,, and �L for (Beckham) are normalised to, which is identical to the Arabic word meaning either (how much) or (in you). Adding a custom prefix to the normalised form is one way to address this issue; we add the letter “o to the beginning of each normalised word. For example, variants for Beckham are t</context>
</contexts>
<marker>Abduljaleel, Larkey, 2003</marker>
<rawString>Nasreen Abduljaleel and Leah S. Larkey. 2003. Statistical transliteration for English-Arabic cross-language information retrieval. In Proceedings of the International Conference on Information and Knowledge Management, pages 139–146, New Orleans, LA, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jamal B S Al-Qinal</author>
</authors>
<title>Morphophonemics of loanwords in translation.</title>
<date>2002</date>
<journal>Journal of King Saud University,</journal>
<pages>13--1</pages>
<contexts>
<context position="3904" citStr="Al-Qinal, 2002" startWordPosition="614" endWordPosition="615"> 2006). Translated words, sometimes referred to as Arabised words, are foreign words that are modified or remodelled to conform to Arabic word 2We use the International Phonetic Alphabet. 49 Proceedings of the 5th Workshop on Important Unresolved Matters, pages 49–56, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics paradigms, and are well assimilated into the language. The assimilation process includes changes in the structure of the borrowed word, including segmental and vowel changes, addition or deletion of syllables, and modification of stress patterns (Al-Qinal, 2002). Foreign words of this category usually have a single consistent spelling variant, for example  virus,    archive, and radio. Where equivalent native terms are not available early enough for widespread adoption, foreign terms are used directly with their original pronunciation represented using Arabic letters. As these do not appear in standard Arabic lexicons — that may include adopted words — they are considered to be Out-Of-Vocabulary (OOV) words. With transliterated words, the phonemes of a foreign word are replaced with their nearest Arabic equivalents. Since A</context>
</contexts>
<marker>Al-Qinal, 2002</marker>
<rawString>Jamal B. S. Al-Qinal. 2002. Morphophonemics of loanwords in translation. Journal of King Saud University, 13:1–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Syed Uzair Aqeel</author>
<author>Steve Beitzel</author>
<author>Eric Jensen</author>
<author>David Grossman</author>
<author>Ophir Frieder</author>
</authors>
<title>On the development of name search techniques for Arabic.</title>
<date>2006</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>57</volume>
<issue>6</issue>
<contexts>
<context position="11285" citStr="Aqeel et al. (2006)" startWordPosition="1858" endWordPosition="1861">is has predefined codes for the sounds in a language, with similar-sounding letters grouped under one code. During comparisons, all letters in a word bar the first one are encoded, and the resulting representation is truncated to be at most four characters long. A variant of Soundex is the Phonix algorithm (Gadd, 1990), which transforms letter groups to letters and then to codes; the actual mappings are different from Soundex. Both Soundex and Phonix have been reported to have poorer precision in identifying variants of English names than both Edit Distance and n-grams (Zobel and Dart, 1995). Aqeel et al. (2006) propose an Arabic version of English Soundex (Asoundex-final). They include diacritics in a list of Arabic names, and created queries by altering some of these names by adding, deleting, or inserting characters. Most Arabic names are meaningful words — for example,  (the praised one) — and rarely do have spelling variants. This leads to morphological ambiguity as names may match verbs, pronouns and other categories of the language. We have found that using Asoundex-final with the misspelt query  on an Arabic collection with 35 949 unique words returns   (exaggeration),  </context>
<context position="20361" citStr="Aqeel et al., 2006" startWordPosition="3443" endWordPosition="3446"> use for native Arabic words, as these are usually distinct, and pro3All phonetic groups are created based on transliteration mapping between English and Arabic letters k-r J0 t C-) A k-r k-r u J0 J 00 C t� J v u 53 Table 3: Mappings for our phonetic approach Characters Code    0        1     2   3     4   5  6  7  8  9  A  B  C nunciation is rarely ambiguous. Table 3 shows Arabic letters and their corresponding codes. To normalise foreign words, we replace each letter but the first by its phonetic code, and drop any vowels. We have found — as have (Aqeel et al., 2006) and (Zobel and Dart, 1996) — that it is better to encode all letters, rather than only the first four characters; for brevity, we show only the results for coding all characters, under the label “Soutex”. 5.3 Arabic Editex Based on groups identified in Table 4, we have modified the Editex algorithm of Zobel and Dart (1996). It works as in English except that we drop the functionality used to consider the two silent characters in the English version as silent characters in Arabic are rare and usually occur at the beginning or at the end of the word. Specifically, we replace d(sZ7 tj) by r(sZ7 </context>
</contexts>
<marker>Aqeel, Beitzel, Jensen, Grossman, Frieder, 2006</marker>
<rawString>Syed Uzair Aqeel, Steve Beitzel, Eric Jensen, David Grossman, and Ophir Frieder. 2006. On the development of name search techniques for Arabic. Journal of the American Society for Information Science and Technology, 57(6):728–739.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine L Borgman</author>
<author>Susan L Siegfried</author>
</authors>
<title>Getty’s synoname and its cousins: A survey of applications of personal name-matching algorithms.</title>
<date>1992</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>43</volume>
<issue>7</issue>
<contexts>
<context position="6548" citStr="Borgman and Siegfried, 1992" startWordPosition="1032" endWordPosition="1035">ivalent in Arabic, since transliterators represent it according to how they pronounce it. For instance, the English letter G /g/ is at times mapped to the Arabic letters  /G/,  /q/, or  /Z/ (Abduljaleel and Larkey, 2003); we have also observed it mapped to the letter  /k/:   ,   ,   , and are transliterations of Gorbachev we have found on the Web. Similarly, the interpretation of character combinations varies between transliterators. Moreover, Typographical and phonetic errors during transliteration may add even more variants (Borgman and Siegfried, 1992). 2.3 Retrieval of Variants When different variants of a word exist, only a subset of related documents can be found when the search uses only one variant. Typical search engine users are unlikely to recognise the problem and hence do not add other variants to their query. Currently, major search engines such as Google, Yahoo, and MSN search use exact match for Arabic search, and no publicly available Arabic Information Retrieval (AIR) system has been reported to retrieve different spelling variants (Abdelali et al., 2004). In this paper we explore how the different variants of a foreign word </context>
</contexts>
<marker>Borgman, Siegfried, 1992</marker>
<rawString>Christine L. Borgman and Susan L. Siegfried. 1992. Getty’s synoname and its cousins: A survey of applications of personal name-matching algorithms. Journal of the American Society for Information Science, 43(7):459–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee R Dice</author>
</authors>
<title>Measures of the amount of ecologic association between species.</title>
<date>1945</date>
<journal>Ecology,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="9807" citStr="Dice (1945)" startWordPosition="1608" endWordPosition="1609">Count = |G3 n Gt | |G3 U Gt | where G3 is the set of grams in string s. For example, with n=2, the similarity of “ahmed” and “ahmmed” using this measure is 0.8 because both strings contain the four 2-grams ah, hm, me, and ed, while there are five distinct 2-grams across the two strings. Gram distance (Ukkonen, 1992) is another string similarity technique. When grams are not repeated – which is the case in names — the similarity is computed as (Zobel and Dart, 1996): gramDist(s, t) = |G3 |+ |Gt |−2 |G3 n Gt | According to this measure, the similarity between “ahmed” and “ahmmed” is 1. With the Dice (1945) measure, the similarity of strings s and t is computed as twice the number of common n-grams between s and t, divided by the total number of n-grams in the two strings: 2x |G3 n Gt | Dice(s, t) = |G3 |+ |Gt | where G3 denotes the set of n-grams in s, and Gt denotes the set of n-grams in t. The longest common subsequence (LCS) algorithm measures the similarity between two strings based on the common characters in the two strings (Wagner and Fischer, 1974; Stephen, 1992). Similarity is normalised by dividing the length of the common subsequence by the length of the longer string (Melamed, 1995)</context>
</contexts>
<marker>Dice, 1945</marker>
<rawString>Lee R. Dice. 1945. Measures of the amount of ecologic association between species. Ecology, 26(3):297–302, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Gadd</author>
</authors>
<title>Phonix: the algorithm.</title>
<date>1990</date>
<journal>Program,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>369</pages>
<contexts>
<context position="10986" citStr="Gadd, 1990" startWordPosition="1810" endWordPosition="1811">he longer string (Melamed, 1995). The similarity between between “ahmed” and “ahmmed” is (5/6=0.833). Phonetic approaches to determine similarity between two words include the well-known Soundex algorithm developed by Odell and Russell, patented in 1918 and 1922 (Hall and Dowling, 1980). This has predefined codes for the sounds in a language, with similar-sounding letters grouped under one code. During comparisons, all letters in a word bar the first one are encoded, and the resulting representation is truncated to be at most four characters long. A variant of Soundex is the Phonix algorithm (Gadd, 1990), which transforms letter groups to letters and then to codes; the actual mappings are different from Soundex. Both Soundex and Phonix have been reported to have poorer precision in identifying variants of English names than both Edit Distance and n-grams (Zobel and Dart, 1995). Aqeel et al. (2006) propose an Arabic version of English Soundex (Asoundex-final). They include diacritics in a list of Arabic names, and created queries by altering some of these names by adding, deleting, or inserting characters. Most Arabic names are meaningful words — for example,  (the praised one) — and rarel</context>
</contexts>
<marker>Gadd, 1990</marker>
<rawString>T. Gadd. 1990. Phonix: the algorithm. Program, 24(4):363– 369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick A V Hall</author>
<author>Geoff R Dowling</author>
</authors>
<title>Approximate string matching.</title>
<date>1980</date>
<journal>ACM Computing Surveys,</journal>
<volume>12</volume>
<issue>4</issue>
<contexts>
<context position="7967" citStr="Hall and Dowling, 1980" startWordPosition="1266" endWordPosition="1269">a single normalised form by removing vowels and conflating homophones. In the second technique, we extend the well-known Soundex technique — commonly used to identify variants of names in English — to the OOV problem in Arabic, and in the third technique, we modify the English Editex algorithm to identify similar foreign words in Arabic. 3 Related Work Approaches to identify similar-sounding but differently spelt words have been heavily investigated in English; among these are techniques that make use of string or phonetic similarity. 50 String similarity approaches include the Edit Distance (Hall and Dowling, 1980), used to measure the similarity of two strings by counting the minimal number of character insertions, deletions, or replacements needed to transform one string into another. To transpose a string s of length n into a string t of length m, edit(m, n) computes the minimal steps required as follows: edit(0, 0) = 0 edit(i, 0) = i edit(0, j) = j edit(i, j) = min[edit(i − 1, j) + 1, edit(i, j − 1) + 1, edit(i − 1, j − 1) + d(si, ti)] where d(si, ti) = 1 if si = ti, 0 otherwise. This measure can be used to rank words in the collection with respect to a query word. Zobel and Dart (1995) showed that </context>
<context position="10662" citStr="Hall and Dowling, 1980" startWordPosition="1754" endWordPosition="1757">et of n-grams in s, and Gt denotes the set of n-grams in t. The longest common subsequence (LCS) algorithm measures the similarity between two strings based on the common characters in the two strings (Wagner and Fischer, 1974; Stephen, 1992). Similarity is normalised by dividing the length of the common subsequence by the length of the longer string (Melamed, 1995). The similarity between between “ahmed” and “ahmmed” is (5/6=0.833). Phonetic approaches to determine similarity between two words include the well-known Soundex algorithm developed by Odell and Russell, patented in 1918 and 1922 (Hall and Dowling, 1980). This has predefined codes for the sounds in a language, with similar-sounding letters grouped under one code. During comparisons, all letters in a word bar the first one are encoded, and the resulting representation is truncated to be at most four characters long. A variant of Soundex is the Phonix algorithm (Gadd, 1990), which transforms letter groups to letters and then to codes; the actual mappings are different from Soundex. Both Soundex and Phonix have been reported to have poorer precision in identifying variants of English names than both Edit Distance and n-grams (Zobel and Dart, 199</context>
</contexts>
<marker>Hall, Dowling, 1980</marker>
<rawString>Patrick A. V. Hall and Geoff R. Dowling. 1980. Approximate string matching. ACM Computing Surveys, 12(4):381–402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Melamed</author>
</authors>
<title>Automatic evaluation and uniform filter cascades for inducing N-best translation lexicons.</title>
<date>1995</date>
<booktitle>In David Yarovsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>184--198</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="10407" citStr="Melamed, 1995" startWordPosition="1718" endWordPosition="1719">he Dice (1945) measure, the similarity of strings s and t is computed as twice the number of common n-grams between s and t, divided by the total number of n-grams in the two strings: 2x |G3 n Gt | Dice(s, t) = |G3 |+ |Gt | where G3 denotes the set of n-grams in s, and Gt denotes the set of n-grams in t. The longest common subsequence (LCS) algorithm measures the similarity between two strings based on the common characters in the two strings (Wagner and Fischer, 1974; Stephen, 1992). Similarity is normalised by dividing the length of the common subsequence by the length of the longer string (Melamed, 1995). The similarity between between “ahmed” and “ahmmed” is (5/6=0.833). Phonetic approaches to determine similarity between two words include the well-known Soundex algorithm developed by Odell and Russell, patented in 1918 and 1922 (Hall and Dowling, 1980). This has predefined codes for the sounds in a language, with similar-sounding letters grouped under one code. During comparisons, all letters in a word bar the first one are encoded, and the resulting representation is truncated to be at most four characters long. A variant of Soundex is the Phonix algorithm (Gadd, 1990), which transforms le</context>
</contexts>
<marker>Melamed, 1995</marker>
<rawString>Dan Melamed. 1995. Automatic evaluation and uniform filter cascades for inducing N-best translation lexicons. In David Yarovsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora, pages 184–198, Somerset, New Jersey. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdusalam F Ahmad Nwesri</author>
<author>S M M Tahaghoghi</author>
<author>Falk Scholer</author>
</authors>
<title>Capturing out-of-vocabulary words in Arabic text.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>258--266</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1427" citStr="Nwesri et al. (2006)" startWordPosition="212" endWordPosition="215"> phonemes of another language through transliteration. This is particularly common with proper nouns. However, phonetics differ across languages, and transliteration usually results in many spellings for the same word. This is an issue even across languages that use substantially the same character set; simple examples would be “colour” and “color” across British and American usage, and “ambience” and “ambiance” across French and English. A change in character sets compounds the problem: for instance, there are at least 32 English forms for the Arabic name of the Libyan leader “Kaddafi”,1 and Nwesri et al. (2006) have identified 28 different spellings for the name of the former Serbian president Milosevic in the eleventh Text REtrieval Conference (TREC) Arabic newswire collection. Users typically submit only one spelling variant in their query, and current Arabic text retrieval systems return only documents that contain that variant (Abdelali et al., 2004). We apply tech1http://www.geocities.com/Athens/8744/ spelling.htm niques used to identify similar strings in other languages such as English, and present a novel approach to identify and retrieve different variants of foreign words in Arabic. 2 The </context>
<context position="3295" citStr="Nwesri et al., 2006" startWordPosition="523" endWordPosition="526">ould be pronounced. For example, some of the variants of the word  are  /kataba/ (he wrote),  /kU tUb/ (books), or  /kUtIba/ (is written). There are also three long vowels — represented by the letters {  } — that are more pronounced  than short vowels. For instance, the letter  can be followed by the long vowel  /a:/ to form  /fa:/, by /u:/ to form  /fu:/, and by  /i:/ to form   /fi:/. 2.1 Foreign Words From an information retrieval (IR) perspective, foreign words in Arabic can be classified into two general categories: translated and transliterated (Nwesri et al., 2006). Translated words, sometimes referred to as Arabised words, are foreign words that are modified or remodelled to conform to Arabic word 2We use the International Phonetic Alphabet. 49 Proceedings of the 5th Workshop on Important Unresolved Matters, pages 49–56, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics paradigms, and are well assimilated into the language. The assimilation process includes changes in the structure of the borrowed word, including segmental and vowel changes, addition or deletion of syllables, and modification of stress patterns (Al-Qin</context>
<context position="17632" citStr="Nwesri et al. (2006)" startWordPosition="2942" endWordPosition="2945">istakes. For example, instead of typing the character 1, we found that transliterators sometimes mistakenly type J. We clustered transliterations by the original English words, removed duplicates from each cluster, and also removed 103 clusters where all transliterators agreed on the same version of transliteration. This left 3 582 words in 207 clusters of size 2, 252 clusters of size 3, 192 clusters of size 4, 149 clusters of size 5, 93 clusters of size 6, and 47 clusters of size 7. Finally, we incorporated these transliterations into a list with 35 949 unique Arabic native words prepared by Nwesri et al. (2006). 5 Algorithms We propose three algorithms to identify foreign words in Arabic text. The first is normalisation, which aims to handle different types of typographical errors described in Section 2.1. The second and third techniques are extensions to the English Soundex and Editex techniques. 5.1 Normalisation To deal with different typographical styles in writing foreign words, we first remove vowels from every foreign term. We keep vowels unchanged if they Table 2: Normalisation of equivalent consonants to a single form Original Normalised are the first or the last characters of the word, sin</context>
</contexts>
<marker>Nwesri, Tahaghoghi, Scholer, 2006</marker>
<rawString>Abdusalam F Ahmad Nwesri, S. M. M. Tahaghoghi, and Falk Scholer. 2006. Capturing out-of-vocabulary words in Arabic text. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 258–266, Sydney, Australia, 22–23 July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Pfeifer</author>
<author>Thomas Poersch</author>
<author>Norbert Fuhr</author>
</authors>
<title>Retrieval effectiveness of proper name search methods.</title>
<date>1996</date>
<journal>Inf. Process. Manage.,</journal>
<volume>32</volume>
<issue>6</issue>
<contexts>
<context position="9072" citStr="Pfeifer et al. (1996)" startWordPosition="1469" endWordPosition="1472">measure can be used to rank words in the collection with respect to a query word. Zobel and Dart (1995) showed that Edit Distance performed the best among the techniques they evaluated for matching English names. It is not known how this technique will perform with Arabic words. Another candidate approach that can be used to identify similar foreign words in Arabic is n-grams (Hall and Dowling, 1980). This approach is language independent; the strings are divided into grams (substrings) of length n, and the similarity of the strings is computed on the basis of the similarity of their n-grams. Pfeifer et al. (1996) compute the similarity as the number of shared grams divided by the total number of distinct grams in the two strings. gramCount = |G3 n Gt | |G3 U Gt | where G3 is the set of grams in string s. For example, with n=2, the similarity of “ahmed” and “ahmmed” using this measure is 0.8 because both strings contain the four 2-grams ah, hm, me, and ed, while there are five distinct 2-grams across the two strings. Gram distance (Ukkonen, 1992) is another string similarity technique. When grams are not repeated – which is the case in names — the similarity is computed as (Zobel and Dart, 1996): gramD</context>
</contexts>
<marker>Pfeifer, Poersch, Fuhr, 1996</marker>
<rawString>Ulrich Pfeifer, Thomas Poersch, and Norbert Fuhr. 1996. Retrieval effectiveness of proper name search methods. Inf. Process. Manage., 32(6):667–679.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vijay Raghavan</author>
<author>Peter Bollmann</author>
<author>Gwang S Jung</author>
</authors>
<title>A critical investigation of recall and precision as measures of retrieval system performance.</title>
<date>1989</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="21858" citStr="Raghavan et al., 1989" startWordPosition="3720" endWordPosition="3724">consider every word to be a query to avoid any bias towards string similarity techniques as phonetic based Table 4: AEditex letter groups Characters Group    0    1   2   3   4   5   6   7     8 techniques fail to capture misspelled words whereas string similarity techniques do. The results returned by the different algorithms described in the previous section are not directly comparable, as some algorithms return ranked results and others return unranked results. Ranked results could also form a weak ordering in which multiple results belong to the same rank (Raghavan et al., 1989). Standard information retrieval measures are not appropriate for evaluating such techniques. Zobel and Dart (1996) address this by using standard precision and recall, but randomly permute results of equal ranks and calculate the average of recall and precision over ten different permutations. Raghavan et al (1989) propose a different measure called Probability of Relevance (PRR). This measure assumes that the user randomly selects a document from the topmost ranks. At any point the precision is defined as the probability that the random examined document is relevant. Recall is the number of </context>
</contexts>
<marker>Raghavan, Bollmann, Jung, 1989</marker>
<rawString>Vijay Raghavan, Peter Bollmann, and Gwang S. Jung. 1989. A critical investigation of recall and precision as measures of retrieval system performance. ACM Trans. Inf. Syst., 7(3):205–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Glover Stalls</author>
<author>Kevin Knight</author>
</authors>
<title>Translating names and technical terms in Arabic text.</title>
<date>1998</date>
<booktitle>In COLING/ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>34--41</pages>
<location>Montreal, Quebc, Canada.</location>
<contexts>
<context position="4756" citStr="Stalls and Knight, 1998" startWordPosition="741" endWordPosition="744">d adoption, foreign terms are used directly with their original pronunciation represented using Arabic letters. As these do not appear in standard Arabic lexicons — that may include adopted words — they are considered to be Out-Of-Vocabulary (OOV) words. With transliterated words, the phonemes of a foreign word are replaced with their nearest Arabic equivalents. Since Arabic phonemes cannot represent all phonemes found in other languages, the original phonemes are usually not represented uniformly by different transliterators, resulting in multiple spelling variants for the same foreign word (Stalls and Knight, 1998). Faced with the need to use new foreign terms, native speakers often cannot wait for formal equivalents to be defined. This is particularly true for news agencies, which encounter new foreign nouns and technical terms daily. This urgency leads to more transliteration than translation, with the associated problem of multiple spellings. 2.2 Spelling Variants In Arabic, short vowels must be indicated using diacritics, but these are rarely used in general text, and there are no standard rules on when and where diacritics must be indicated. Context does not help in predicting diacritics for foreig</context>
<context position="18897" citStr="Stalls and Knight, 1998" startWordPosition="3155" endWordPosition="3158">c. The long vowel letters are sometimes used as consonants, and these may be followed immediately by another long vowel. For example, the vowel letter S /i/ may be followed by the long vowel 9 /u:/ to form 9= /ju:/. For such cases, we keep the first vowel and remove the second. Two vowels can also be used together as diphthongs, as in 91 /aw/ and SI /aj/. We retain vowels that are followed by another vowel or preceded by a vowel that forms a diphthong. We also conflate similar consonants based on statistical analysis of letter mappings between English and Arabic (Abduljaleel and Larkey, 2003; Stalls and Knight, 1998), and confirming through a web search that these consonants are used interchangeably in web documents.3 Table 2 shows all consonants we consider to be equivalent. Our process may lead to ambiguity where a similar native word exists; for instance, the spelling variants �1�,, and �L for (Beckham) are normalised to, which is identical to the Arabic word meaning either (how much) or (in you). Adding a custom prefix to the normalised form is one way to address this issue; we add the letter “o to the beginning of each normalised word. For example, variants for Beckham are thus normalised to r- S. Si</context>
</contexts>
<marker>Stalls, Knight, 1998</marker>
<rawString>Bonnie Glover Stalls and Kevin Knight. 1998. Translating names and technical terms in Arabic text. In COLING/ACL Workshop on Computational Approaches to Semitic Languages, pages 34–41, Montreal, Quebc, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham A Stephen</author>
</authors>
<title>String search.</title>
<date>1992</date>
<tech>Technical report,</tech>
<institution>School of Electronic Engineering Science, University College of North Wales.</institution>
<contexts>
<context position="10281" citStr="Stephen, 1992" startWordPosition="1698" endWordPosition="1699">gramDist(s, t) = |G3 |+ |Gt |−2 |G3 n Gt | According to this measure, the similarity between “ahmed” and “ahmmed” is 1. With the Dice (1945) measure, the similarity of strings s and t is computed as twice the number of common n-grams between s and t, divided by the total number of n-grams in the two strings: 2x |G3 n Gt | Dice(s, t) = |G3 |+ |Gt | where G3 denotes the set of n-grams in s, and Gt denotes the set of n-grams in t. The longest common subsequence (LCS) algorithm measures the similarity between two strings based on the common characters in the two strings (Wagner and Fischer, 1974; Stephen, 1992). Similarity is normalised by dividing the length of the common subsequence by the length of the longer string (Melamed, 1995). The similarity between between “ahmed” and “ahmmed” is (5/6=0.833). Phonetic approaches to determine similarity between two words include the well-known Soundex algorithm developed by Odell and Russell, patented in 1918 and 1922 (Hall and Dowling, 1980). This has predefined codes for the sounds in a language, with similar-sounding letters grouped under one code. During comparisons, all letters in a word bar the first one are encoded, and the resulting representation i</context>
</contexts>
<marker>Stephen, 1992</marker>
<rawString>Graham A Stephen. 1992. String search. Technical report, School of Electronic Engineering Science, University College of North Wales.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esko Ukkonen</author>
</authors>
<title>Approximate string-matching with q-grams and maximal matches.</title>
<date>1992</date>
<journal>Theor. Comput. Sci.,</journal>
<volume>92</volume>
<issue>1</issue>
<contexts>
<context position="9513" citStr="Ukkonen, 1992" startWordPosition="1554" endWordPosition="1555">strings are divided into grams (substrings) of length n, and the similarity of the strings is computed on the basis of the similarity of their n-grams. Pfeifer et al. (1996) compute the similarity as the number of shared grams divided by the total number of distinct grams in the two strings. gramCount = |G3 n Gt | |G3 U Gt | where G3 is the set of grams in string s. For example, with n=2, the similarity of “ahmed” and “ahmmed” using this measure is 0.8 because both strings contain the four 2-grams ah, hm, me, and ed, while there are five distinct 2-grams across the two strings. Gram distance (Ukkonen, 1992) is another string similarity technique. When grams are not repeated – which is the case in names — the similarity is computed as (Zobel and Dart, 1996): gramDist(s, t) = |G3 |+ |Gt |−2 |G3 n Gt | According to this measure, the similarity between “ahmed” and “ahmmed” is 1. With the Dice (1945) measure, the similarity of strings s and t is computed as twice the number of common n-grams between s and t, divided by the total number of n-grams in the two strings: 2x |G3 n Gt | Dice(s, t) = |G3 |+ |Gt | where G3 denotes the set of n-grams in s, and Gt denotes the set of n-grams in t. The longest co</context>
</contexts>
<marker>Ukkonen, 1992</marker>
<rawString>Esko Ukkonen. 1992. Approximate string-matching with q-grams and maximal matches. Theor. Comput. Sci., 92(1):191–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Wagner</author>
<author>Michael J Fischer</author>
</authors>
<title>The string-tostring correction problem.</title>
<date>1974</date>
<journal>J. ACM,</journal>
<volume>21</volume>
<issue>1</issue>
<contexts>
<context position="10265" citStr="Wagner and Fischer, 1974" startWordPosition="1694" endWordPosition="1697">s (Zobel and Dart, 1996): gramDist(s, t) = |G3 |+ |Gt |−2 |G3 n Gt | According to this measure, the similarity between “ahmed” and “ahmmed” is 1. With the Dice (1945) measure, the similarity of strings s and t is computed as twice the number of common n-grams between s and t, divided by the total number of n-grams in the two strings: 2x |G3 n Gt | Dice(s, t) = |G3 |+ |Gt | where G3 denotes the set of n-grams in s, and Gt denotes the set of n-grams in t. The longest common subsequence (LCS) algorithm measures the similarity between two strings based on the common characters in the two strings (Wagner and Fischer, 1974; Stephen, 1992). Similarity is normalised by dividing the length of the common subsequence by the length of the longer string (Melamed, 1995). The similarity between between “ahmed” and “ahmmed” is (5/6=0.833). Phonetic approaches to determine similarity between two words include the well-known Soundex algorithm developed by Odell and Russell, patented in 1918 and 1922 (Hall and Dowling, 1980). This has predefined codes for the sounds in a language, with similar-sounding letters grouped under one code. During comparisons, all letters in a word bar the first one are encoded, and the resulting </context>
</contexts>
<marker>Wagner, Fischer, 1974</marker>
<rawString>Robert A. Wagner and Michael J. Fischer. 1974. The string-tostring correction problem. J. ACM, 21(1):168–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Zobel</author>
<author>Philip Dart</author>
</authors>
<title>Finding approximate matches in large lexicons.</title>
<date>1995</date>
<booktitle>Software - Practice and Experience,</booktitle>
<pages>25--3</pages>
<contexts>
<context position="8554" citStr="Zobel and Dart (1995)" startWordPosition="1385" endWordPosition="1388"> Distance (Hall and Dowling, 1980), used to measure the similarity of two strings by counting the minimal number of character insertions, deletions, or replacements needed to transform one string into another. To transpose a string s of length n into a string t of length m, edit(m, n) computes the minimal steps required as follows: edit(0, 0) = 0 edit(i, 0) = i edit(0, j) = j edit(i, j) = min[edit(i − 1, j) + 1, edit(i, j − 1) + 1, edit(i − 1, j − 1) + d(si, ti)] where d(si, ti) = 1 if si = ti, 0 otherwise. This measure can be used to rank words in the collection with respect to a query word. Zobel and Dart (1995) showed that Edit Distance performed the best among the techniques they evaluated for matching English names. It is not known how this technique will perform with Arabic words. Another candidate approach that can be used to identify similar foreign words in Arabic is n-grams (Hall and Dowling, 1980). This approach is language independent; the strings are divided into grams (substrings) of length n, and the similarity of the strings is computed on the basis of the similarity of their n-grams. Pfeifer et al. (1996) compute the similarity as the number of shared grams divided by the total number </context>
<context position="11264" citStr="Zobel and Dart, 1995" startWordPosition="1854" endWordPosition="1857"> and Dowling, 1980). This has predefined codes for the sounds in a language, with similar-sounding letters grouped under one code. During comparisons, all letters in a word bar the first one are encoded, and the resulting representation is truncated to be at most four characters long. A variant of Soundex is the Phonix algorithm (Gadd, 1990), which transforms letter groups to letters and then to codes; the actual mappings are different from Soundex. Both Soundex and Phonix have been reported to have poorer precision in identifying variants of English names than both Edit Distance and n-grams (Zobel and Dart, 1995). Aqeel et al. (2006) propose an Arabic version of English Soundex (Asoundex-final). They include diacritics in a list of Arabic names, and created queries by altering some of these names by adding, deleting, or inserting characters. Most Arabic names are meaningful words — for example,  (the praised one) — and rarely do have spelling variants. This leads to morphological ambiguity as names may match verbs, pronouns and other categories of the language. We have found that using Asoundex-final with the misspelt query  on an Arabic collection with 35 949 unique words returns  </context>
</contexts>
<marker>Zobel, Dart, 1995</marker>
<rawString>Justin Zobel and Philip Dart. 1995. Finding approximate matches in large lexicons. Software - Practice and Experience, 25(3):331–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Zobel</author>
<author>Philip Dart</author>
</authors>
<title>Phonetic string matching: lessons from information retrieval.</title>
<date>1996</date>
<booktitle>In The 19th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>166--172</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9665" citStr="Zobel and Dart, 1996" startWordPosition="1579" endWordPosition="1582">grams. Pfeifer et al. (1996) compute the similarity as the number of shared grams divided by the total number of distinct grams in the two strings. gramCount = |G3 n Gt | |G3 U Gt | where G3 is the set of grams in string s. For example, with n=2, the similarity of “ahmed” and “ahmmed” using this measure is 0.8 because both strings contain the four 2-grams ah, hm, me, and ed, while there are five distinct 2-grams across the two strings. Gram distance (Ukkonen, 1992) is another string similarity technique. When grams are not repeated – which is the case in names — the similarity is computed as (Zobel and Dart, 1996): gramDist(s, t) = |G3 |+ |Gt |−2 |G3 n Gt | According to this measure, the similarity between “ahmed” and “ahmmed” is 1. With the Dice (1945) measure, the similarity of strings s and t is computed as twice the number of common n-grams between s and t, divided by the total number of n-grams in the two strings: 2x |G3 n Gt | Dice(s, t) = |G3 |+ |Gt | where G3 denotes the set of n-grams in s, and Gt denotes the set of n-grams in t. The longest common subsequence (LCS) algorithm measures the similarity between two strings based on the common characters in the two strings (Wagner and Fischer, 1974</context>
<context position="12212" citStr="Zobel and Dart (1996)" startWordPosition="2001" endWordPosition="2004">have spelling variants. This leads to morphological ambiguity as names may match verbs, pronouns and other categories of the language. We have found that using Asoundex-final with the misspelt query  on an Arabic collection with 35 949 unique words returns   (exaggeration),   (she becomes sad),  (she resolves),  (she helps),  (improvement),  (immunisation),(she governs),  (she defeats). Moreover, it is not clear when and how diacritics 51 are removed, nor where the long vowel  belongs in their implementation. Editex, developed by Zobel and Dart (1996), enhances the Edit Distance technique by incorporating the letter-grouping strategy used by Soundex and Phonix, and has been shown to have better performance than these two algorithms, as well as Edit Distance, on a collection of 30 000 distinct English names. The similarity between two strings s and t is computed as: edit(0, 0) = 0 edit(i, 0) = edit(i − 1, 0) + d(sz − 1, s1) edit(0.j) = edit(0, j − 1) + d(tj − 1, tj) edit(i.j) = min[edit(i − 1, j) + d(sz − 1, sz), edit(i, j − 1) + d(tj − 1, tj), edit(i − 1, j − 1) + r(sz, tj)] where: r(sz,tj) is 0 if sz=tj, 1 if group(sz)=group(tj), and 2 ot</context>
<context position="20388" citStr="Zobel and Dart, 1996" startWordPosition="3448" endWordPosition="3452">rds, as these are usually distinct, and pro3All phonetic groups are created based on transliteration mapping between English and Arabic letters k-r J0 t C-) A k-r k-r u J0 J 00 C t� J v u 53 Table 3: Mappings for our phonetic approach Characters Code    0        1     2   3     4   5  6  7  8  9  A  B  C nunciation is rarely ambiguous. Table 3 shows Arabic letters and their corresponding codes. To normalise foreign words, we replace each letter but the first by its phonetic code, and drop any vowels. We have found — as have (Aqeel et al., 2006) and (Zobel and Dart, 1996) — that it is better to encode all letters, rather than only the first four characters; for brevity, we show only the results for coding all characters, under the label “Soutex”. 5.3 Arabic Editex Based on groups identified in Table 4, we have modified the Editex algorithm of Zobel and Dart (1996). It works as in English except that we drop the functionality used to consider the two silent characters in the English version as silent characters in Arabic are rare and usually occur at the beginning or at the end of the word. Specifically, we replace d(sZ7 tj) by r(sZ7 tj). We call the Arabic ver</context>
<context position="21973" citStr="Zobel and Dart (1996)" startWordPosition="3736" endWordPosition="3739">AEditex letter groups Characters Group    0    1   2   3   4   5   6   7     8 techniques fail to capture misspelled words whereas string similarity techniques do. The results returned by the different algorithms described in the previous section are not directly comparable, as some algorithms return ranked results and others return unranked results. Ranked results could also form a weak ordering in which multiple results belong to the same rank (Raghavan et al., 1989). Standard information retrieval measures are not appropriate for evaluating such techniques. Zobel and Dart (1996) address this by using standard precision and recall, but randomly permute results of equal ranks and calculate the average of recall and precision over ten different permutations. Raghavan et al (1989) propose a different measure called Probability of Relevance (PRR). This measure assumes that the user randomly selects a document from the topmost ranks. At any point the precision is defined as the probability that the random examined document is relevant. Recall is the number of relevant documents that the user has seen so far. If we require NR relevant documents — in our case, words — from a</context>
</contexts>
<marker>Zobel, Dart, 1996</marker>
<rawString>Justin Zobel and Philip Dart. 1996. Phonetic string matching: lessons from information retrieval. In The 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 166–172, New York, NY, USA. ACM Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>