<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002577">
<title confidence="0.963702">
Semantic classification with WordNet kernels
</title>
<author confidence="0.926907">
Diarmuid O´ S´eaghdha
</author>
<affiliation confidence="0.973371">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.68868">
United Kingdom
</address>
<email confidence="0.994386">
do242@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.996594" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995635">
This paper presents methods for performing
graph-based semantic classification using ker-
nel functions defined on the WordNet lexi-
cal hierarchy. These functions are evaluated
on the SemEval Task 4 relation classification
dataset and their performance is shown to be
competitive with that of more complex sys-
tems. A number of possible future develop-
ments are suggested to illustrate the flexibility
of the approach.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999857055555556">
The estimation of semantic similarity between
words is one of the longest-established tasks in Nat-
ural Language Processing and many approaches to
the problem have been proposed. The two domi-
nant lexical similarity paradigms are distributional
similarity, which compares words on the basis of
their observed co-occurrence behaviour in corpora,
and semantic network similarity, which compares
words based on their position in a graph such as
the WordNet hierarchy. In this paper we consider
measures of network similarity for the purpose of
supervised classification with kernel methods. The
utility of kernel functions related to popular distribu-
tional similarity measures has recently been demon-
strated by O´ S´eaghdha and Copestake (2008); we
show here that kernel analogues of WordNet simi-
larity can likewise give good performance on a se-
mantic classification task.
</bodyText>
<sectionHeader confidence="0.917779" genericHeader="method">
2 Kernels derived from graphs
</sectionHeader>
<bodyText confidence="0.968179">
Kernel-based classifiers such as support vector ma-
chines (SVMs) make use of functions called kernel
functions (or simply kernels) to compute the similar-
ity between data points (Shawe-Taylor and Cristian-
ini, 2004). Valid kernels are restricted to the set of
positive semi-definite (psd) functions, i.e., those that
correspond to an inner product in some vector space.
Kernel methods have been widely adopted in NLP
over the past decade, in part due to the good perfor-
mance of SVMs on many tasks and in part due to the
ability to exploit prior knowledge about a given task
through the choice of an appropriate kernel function.
In this section we consider kernel functions that use
spectral properties of a graph to compute the sim-
ilarity between its nodes. The theoretical founda-
tions and some machine learning applications of the
adopted approach have been developed by Kondor
and Lafferty (2002), Smola and Kondor (2003) and
Herbster et al. (2008).
Let G be a graph with vertex set V = vi, ... , vn
and edge set E C V x V . We assume that G is
connected and undirected and that all edges have a
positive weight wij &gt; 0. Let A be the symmetric
nxn matrix with entries Aij = wij if an edge exists
between vertices vi and vj, and Aij = 0 otherwise.
Let D be the diagonal matrix with entries Dii =
E
j∈V Aij. The graph Laplacian L is then defined
as
</bodyText>
<equation confidence="0.896516666666667">
L = D − A (1)
The normalised Laplacian is defined as Lˆ =
D− 2 LD−
</equation>
<bodyText confidence="0.4401315">
1 2 . Both Lˆ and L are positive semi-
definite, but they are typically used as starting points
</bodyText>
<page confidence="0.945212">
237
</page>
<note confidence="0.359076">
Proceedings of NAACL HLT 2009: Short Papers, pages 237–240,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.717359714285714">
for the derivation of kernels rather than as kernels
themselves.
Let λ1 &gt; · · · &gt; λn be the eigenvalues of L and
u1, ... , un the corresponding eigenvectors. Note
that un = 0 for all graphs. L is singular and hence
has no well-defined inverse, but its pseudoinverse
L+ is defined as
</bodyText>
<equation confidence="0.997705333333333">
λ−1
i uiuT (2)
i
</equation>
<bodyText confidence="0.9712668">
L+ is positive definite, and its entries are related to
the resistance distance between points in an elec-
trical circuit (Herbster et al., 2008) and to the av-
erage commute-time distance, i.e., the average dis-
tance of a random walk from one node to another
and back again (Fouss et al., 2007). The similar-
ity measure defined by L+ hence takes information
about the connectivity of the graph into account as
well as information about adjacency. An analogous
pseudoinverse ˆL+ can be defined for the normalised
Laplacian.
A second class of graph-based kernel functions
are the diffusion kernels introduced by Kondor and
Lafferty (2002). The kernel Ht is defined as Ht =
e−tˆL
</bodyText>
<equation confidence="0.9827675">
exp(−tˆλi)ˆuiˆuT (3)
i
</equation>
<bodyText confidence="0.9998728">
where t &gt; 0, and ˆλ1 &gt; · · · &gt; ˆλn and ˆu1, . . . , ˆun
are the eigenvalues and eigenvectors of ˆL+ respec-
tively. Ht can be interpreted in terms of heat diffu-
sion or the distribution of a lazy random walk ema-
nating from a given point at a time point t.
</bodyText>
<sectionHeader confidence="0.999572" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999908">
3.1 Graph construction
</subsectionHeader>
<bodyText confidence="0.999973272727273">
WordNet (Fellbaum, 1998) is a semantic network in
which nodes correspond to word senses (or synsets)
and edges correspond to relations between senses.
In this work we restrict ourselves to the noun com-
ponent of WordNet and use only hyponymy and in-
stance hyponymy relations for graph construction.
The version of WordNet used is WordNet 3.0.
To evaluate the utility of the graph-based kernels
described in Section 2 for computing lexical sim-
ilarity, we use the dataset developed for the task
on Classifying Semantic Relations Between Nom-
inals at the 2007 SemEval competition (Girju et
al., 2007). The dataset comprises candidate exam-
ple sentences for seven two-argument semantic rela-
tions, with 140 training sentences and approximately
80 test sentences for each relation. It is a particularly
suitable task for evaluating WordNet kernels, as the
candidate relation arguments for each sentence are
tagged with their WordNet sense and it has been pre-
viously shown that a kernel model based on distribu-
tional lexical similarity can attain very good perfor-
mance ( O´ S´eaghdha and Copestake, 2008).
</bodyText>
<subsectionHeader confidence="0.999751">
3.2 Calculating the WordNet kernels
</subsectionHeader>
<bodyText confidence="0.99943">
The noun hierarchy in WordNet 3.0 contains 82,115
senses; computing kernel similarities on a graph of
this size raises significant computational issues. The
calculation of the Laplacian pseudoinverse is com-
plicated by the fact that while L and Lˆ are very
sparse, their pseudoinverses are invariably dense and
require very large amounts of memory. To circum-
vent this problem, we follow Fouss et al. (2007)
in computing L+ and ˆL+ one column at a time
through a Cholesky factorisation procedure. Only
those columns required for the classification task
need be calculated, and the kernel computation for
each relation subtask can be performed in a mat-
ter of minutes. Calculating the diffusion kernel in-
volves an eigendecomposition of ˆL, meaning that
computing the kernel exactly is infeasible. The so-
lution used here is to approximate Ht by using the
m smallest components of the spectrum of Lˆ when
computing (3); from (2) it can be seen that a similar
approximation can be made to speed up computation
of L+ and ˆL+.
</bodyText>
<subsectionHeader confidence="0.993358">
3.3 Experimental setup
</subsectionHeader>
<bodyText confidence="0.997527636363637">
For all kernels and relation datasets, the kernel ma-
trix for each argument position was precomputed
and normalised so that every diagonal entry equalled
1. A small number of candidate arguments are not
annotated with a WordNet sense or are assigned a
non-noun sense; these arguments were assumed to
have self-similarity equal to 1 and zero similarity to
all other arguments. This does not affect the pos-
itive semi-definiteness of the kernel matrices. The
per-argument kernel matrices were summed to give
the kernel matrix for each relation subtask. The ker-
</bodyText>
<equation confidence="0.988207666666667">
n−1�
i=1
L+ =
, or equivalently:
Ht = n−1�
i=1
</equation>
<page confidence="0.967808">
238
</page>
<table confidence="0.9997565">
Full graph m = 500 m = 1000
Kernel Acc F Acc F Acc F
B 72.1 68.4 - - - -
L+ 73.3 69.4 73.2 70.5 73.6 70.6
ˆL+ 72.5 70.0 72.7 70.0 74.1 71.0
Ht - - 68.6 64.7 69.8 65.1
</table>
<tableCaption confidence="0.999975">
Table 1: Results on SemEval Task 4
</tableCaption>
<bodyText confidence="0.9958125">
nels described in Section 2 were compared to a base-
line kernel B. This baseline represents each word as
a binary feature vector describing its synset and all
its hypernym synsets in the WordNet hierarchy, and
calculates the linear kernel between vectors.
All experiments were run using the LIBSVM sup-
port vector machine library (Chang and Lin, 2001).
For each relation the SVM cost parameter was op-
timised in the range (2−6, 2−4, ... , 212) through
cross-validation on the training set. The diffusion
kernel parameter t was optimised in the same way,
in the range (10−3,10−2, ... ,103).
</bodyText>
<sectionHeader confidence="0.99993" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999488692307692">
Macro-averaged accuracy and F-score for each ker-
nel are reported in Table 1. There is little difference
between the Laplacian and normalised Laplacian
pseudoinverses; both achieve better performance
than the baseline B. The results also suggest that the
reduced-eigenspectrum approximations to L+ and
ˆL+ may bring benefits in terms of performance as
well as efficiency via a smoothing effect. The best
performance is attained by the approximation to ˆL+
with m = 1, 000 eigencomponents. The heat ker-
nel Ht fares less well; the problem here may be that
the optimal range for the t parameter has not been
identified.
Comparing these results to those of the partici-
pants in the 2007 SemEval task, the WordNet-based
lexical similarity model fares very well. All versions
of L+ and ˆL+ attain higher accuracy than all but one
of 15 systems in the competition and higher F-score
than all but three. Even the baseline B ranks above
all but the top three systems, suggesting that this too
can be a useful model. This is in spite of the fact that
all systems which made use of the sense annotations
also used a rich variety of other information sources
such as features extracted from the sentence context,
while the models presented here use only the graph
structure of WordNet.1
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999988739130435">
There is a large body of work on using WordNet
to compute measures of lexical similarity (Budanit-
sky and Hirst, 2006). However, many of these mea-
sures are not amenable for use as kernel functions as
they rely on properties which cannot be expressed
as a vector inner product, such as the lowest com-
mon subsumer of two vertices. Hughes and Ram-
age (2007) present a lexical similarity model based
on random walks on graphs derived from WordNet;
Rao et al. (2008) propose the Laplacian pseudoin-
verse on such graphs as a lexical similarity measure.
Both of these works share aspects of the current pa-
per; however, neither address supervised learning or
present an application-oriented evaluation.
Extracting features from WordNet for use in su-
pervised learning is a standard technique (Scott and
Matwin, 1999). Siolas and d’Alche-Buc (2000) and
Basili et al. (2006) use a measure of lexical similar-
ity from WordNet as an intermediary to smooth bag-
of-words kernels on documents. Siolas and d’Alche-
Buc use an inverse path-based similarity measure,
while Basili et al. use a measure of “conceptual den-
sity” that is not proven to be positive semi-definite.
</bodyText>
<sectionHeader confidence="0.946718" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999987357142857">
The main purpose of this paper has been to demon-
strate how kernels that capture spectral aspects of
graph structure can be used to compare nodes in
a lexical hierarchy and thus provide a kernelised
measure of WordNet similarity. As far as we are
aware, these measures have not previously been in-
vestigated in the context of semantic classification.
The resulting WordNet kernels have been evaluated
on the SemEval Task 4 dataset and shown to attain
a higher level of performance than many more com-
plicated systems that participated in that task.
Two obvious shortcomings of the kernels dis-
cussed here are that they are defined on senses
rather than words and that they are computed on a
</bodyText>
<footnote confidence="0.984423">
1Of course, information about lexical similarity is not suf-
ficient to classify all examples. In particular, the models pre-
sented here perform relatively badly on the ORIGIN-ENTITY
and THEME-TOOL relations, while scoring better than all
SemEval entrants on INSTRUMENT-AGENCY and PRODUCT-
PRODUCER.
</footnote>
<page confidence="0.997776">
239
</page>
<bodyText confidence="0.9566144">
rather impoverished graph structure (the WordNet
hyponym hierarchy is quite tree-like). One of the
significant benefits of spectral graph kernels is that
they can be computed on arbitrary graphs and are
most powerful when graphs have a rich connectiv-
ity structure. Some potential future directions that
would make greater use of this flexibility include the
following:
• A simple extension from sense-kernels to
word-kernels involves adding word nodes to
the WordNet graph, with an edge linking each
word to each of its possible senses. This is sim-
ilar to the graph construction method of Hughes
and Ramage (2007) and Rao et al. (2008).
However, preliminary experiments on the Se-
mEval Task 4 dataset indicate that further re-
finement of this approach may be necessary
in order to match the performance of kernels
based on distributional lexical similarity ( O´
S´eaghdha and Copestake, 2008).
</bodyText>
<listItem confidence="0.971092142857143">
• Incorporating other WordNet relations such as
meronymy and topicality gives a way of ker-
nelising semantic association or relatedness;
one application of this might be in develop-
ing supervised methods for spelling correction
(Budanitsky and Hirst, 2006).
• A WordNet graph can be augmented with in-
</listItem>
<bodyText confidence="0.945037">
formation from other sources, such as links
based on corpus-derived similarity. Alterna-
tively, the graph-based kernel functions could
be applied to graphs constructed from parsed
corpora (Minkov and Cohen, 2008).
</bodyText>
<sectionHeader confidence="0.997872" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999616338461539">
Roberto Basili, Marco Cammisa, and Alessandro Mos-
chitti. 2006. A semantic kernel to classify texts with
very few training examples. Informatica, 30(2):163–
172.
Alexander Budanitsky and Graeme Hirst. 2006. Evalu-
ating WordNet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32(1):13–47.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: A library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.
tw/˜cjlin/libsvm.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Francois Fouss, Alain Pirotte, Jean-Michel Renders, and
Marco Saerens. 2007. Random-walk computation of
similarities between nodes of a graph with application
to collaborative recommendation. IEEE Transactions
on Knowledge and Data Engineering, 19(3):355–369.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
SemEval-2007 Task 04: Classification of semantic
relations between nominals. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-07).
Mark Herbster, Massimiliano Pontil, and Sergio Rojas
Galeano. 2008. Fast prediction on a tree. In Pro-
ceedings of the 22nd Annual Conference on Neural In-
formation Processing Systems (NIPS-08).
Thad Hughes and Daniel Ramage. 2007. Lexical seman-
tic relatedness with random graph walks. In Proceed-
ings of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL-
Risi Imre Kondor and John Lafferty. 2002. Diffusion
kernels on graphs and other discrete input spaces. In
Proceedings of the 19th International Conference on
Machine Learning (ICML-02).
Einat Minkov and William W. Cohen. 2008. Learning
graph walk based similarity measures for parsed text.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
Diarmuid O´ S´eaghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proceedings of the 22nd International Conference on
Computational Linguistics (COLING-08).
Delip Rao, David Yarowsky, and Chris Callison-Burch.
2008. Affinity measures based on the graph Lapla-
cian. In Proceedings of the 3rd TextGraphs Workshop
on Graph-based Algorithms for NLP.
Sam Scott and Stan Matwin. 1999. Feature engineering
for text classification. In Proceedings of the 16th In-
ternational Conference on Machine Learning (ICML-
99).
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press, Cambridge.
Georges Siolas and Florence d’Alche-Buc. 2000. Sup-
port vector machines based on a semantic kernel for
text categorization. In Proceedings of the IEEE-INNS-
ENNS International Joint Conference on Neural Net-
works.
Alexander J. Smola and Risi Kondor. 2003. Kernels and
regularization on graphs. In Proceedings of the the
16th Annual Conference on Learning Theory and 7th
Workshop on Kernel Machines (COLT-03).
</reference>
<page confidence="0.996991">
240
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.333176">
<title confidence="0.946693">Semantic classification with WordNet kernels</title>
<affiliation confidence="0.698633333333333">Computer University of United</affiliation>
<email confidence="0.817151">do242@cl.cam.ac.uk</email>
<abstract confidence="0.998357636363636">This paper presents methods for performing graph-based semantic classification using kernel functions defined on the WordNet lexical hierarchy. These functions are evaluated on the SemEval Task 4 relation classification dataset and their performance is shown to be competitive with that of more complex systems. A number of possible future developments are suggested to illustrate the flexibility of the approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Marco Cammisa</author>
<author>Alessandro Moschitti</author>
</authors>
<title>A semantic kernel to classify texts with very few training examples.</title>
<date>2006</date>
<journal>Informatica,</journal>
<volume>30</volume>
<issue>2</issue>
<pages>172</pages>
<contexts>
<context position="10142" citStr="Basili et al. (2006)" startWordPosition="1719" endWordPosition="1722">xpressed as a vector inner product, such as the lowest common subsumer of two vertices. Hughes and Ramage (2007) present a lexical similarity model based on random walks on graphs derived from WordNet; Rao et al. (2008) propose the Laplacian pseudoinverse on such graphs as a lexical similarity measure. Both of these works share aspects of the current paper; however, neither address supervised learning or present an application-oriented evaluation. Extracting features from WordNet for use in supervised learning is a standard technique (Scott and Matwin, 1999). Siolas and d’Alche-Buc (2000) and Basili et al. (2006) use a measure of lexical similarity from WordNet as an intermediary to smooth bagof-words kernels on documents. Siolas and d’AlcheBuc use an inverse path-based similarity measure, while Basili et al. use a measure of “conceptual density” that is not proven to be positive semi-definite. 6 Conclusion and future work The main purpose of this paper has been to demonstrate how kernels that capture spectral aspects of graph structure can be used to compare nodes in a lexical hierarchy and thus provide a kernelised measure of WordNet similarity. As far as we are aware, these measures have not previo</context>
</contexts>
<marker>Basili, Cammisa, Moschitti, 2006</marker>
<rawString>Roberto Basili, Marco Cammisa, and Alessandro Moschitti. 2006. A semantic kernel to classify texts with very few training examples. Informatica, 30(2):163– 172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="9399" citStr="Budanitsky and Hirst, 2006" startWordPosition="1596" endWordPosition="1600">ˆL+ attain higher accuracy than all but one of 15 systems in the competition and higher F-score than all but three. Even the baseline B ranks above all but the top three systems, suggesting that this too can be a useful model. This is in spite of the fact that all systems which made use of the sense annotations also used a rich variety of other information sources such as features extracted from the sentence context, while the models presented here use only the graph structure of WordNet.1 5 Related work There is a large body of work on using WordNet to compute measures of lexical similarity (Budanitsky and Hirst, 2006). However, many of these measures are not amenable for use as kernel functions as they rely on properties which cannot be expressed as a vector inner product, such as the lowest common subsumer of two vertices. Hughes and Ramage (2007) present a lexical similarity model based on random walks on graphs derived from WordNet; Rao et al. (2008) propose the Laplacian pseudoinverse on such graphs as a lexical similarity measure. Both of these works share aspects of the current paper; however, neither address supervised learning or present an application-oriented evaluation. Extracting features from </context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines. Software available at http://www.csie.ntu.edu.</title>
<date>2001</date>
<tech>tw/˜cjlin/libsvm.</tech>
<contexts>
<context position="7746" citStr="Chang and Lin, 2001" startWordPosition="1315" endWordPosition="1318">n subtask. The kern−1� i=1 L+ = , or equivalently: Ht = n−1� i=1 238 Full graph m = 500 m = 1000 Kernel Acc F Acc F Acc F B 72.1 68.4 - - - - L+ 73.3 69.4 73.2 70.5 73.6 70.6 ˆL+ 72.5 70.0 72.7 70.0 74.1 71.0 Ht - - 68.6 64.7 69.8 65.1 Table 1: Results on SemEval Task 4 nels described in Section 2 were compared to a baseline kernel B. This baseline represents each word as a binary feature vector describing its synset and all its hypernym synsets in the WordNet hierarchy, and calculates the linear kernel between vectors. All experiments were run using the LIBSVM support vector machine library (Chang and Lin, 2001). For each relation the SVM cost parameter was optimised in the range (2−6, 2−4, ... , 212) through cross-validation on the training set. The diffusion kernel parameter t was optimised in the same way, in the range (10−3,10−2, ... ,103). 4 Results Macro-averaged accuracy and F-score for each kernel are reported in Table 1. There is little difference between the Laplacian and normalised Laplacian pseudoinverses; both achieve better performance than the baseline B. The results also suggest that the reduced-eigenspectrum approximations to L+ and ˆL+ may bring benefits in terms of performance as w</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: A library for support vector machines. Software available at http://www.csie.ntu.edu. tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francois Fouss</author>
<author>Alain Pirotte</author>
<author>Jean-Michel Renders</author>
<author>Marco Saerens</author>
</authors>
<title>Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation.</title>
<date>2007</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="3721" citStr="Fouss et al., 2007" startWordPosition="626" endWordPosition="629">or Computational Linguistics for the derivation of kernels rather than as kernels themselves. Let λ1 &gt; · · · &gt; λn be the eigenvalues of L and u1, ... , un the corresponding eigenvectors. Note that un = 0 for all graphs. L is singular and hence has no well-defined inverse, but its pseudoinverse L+ is defined as λ−1 i uiuT (2) i L+ is positive definite, and its entries are related to the resistance distance between points in an electrical circuit (Herbster et al., 2008) and to the average commute-time distance, i.e., the average distance of a random walk from one node to another and back again (Fouss et al., 2007). The similarity measure defined by L+ hence takes information about the connectivity of the graph into account as well as information about adjacency. An analogous pseudoinverse ˆL+ can be defined for the normalised Laplacian. A second class of graph-based kernel functions are the diffusion kernels introduced by Kondor and Lafferty (2002). The kernel Ht is defined as Ht = e−tˆL exp(−tˆλi)ˆuiˆuT (3) i where t &gt; 0, and ˆλ1 &gt; · · · &gt; ˆλn and ˆu1, . . . , ˆun are the eigenvalues and eigenvectors of ˆL+ respectively. Ht can be interpreted in terms of heat diffusion or the distribution of a lazy ra</context>
<context position="5952" citStr="Fouss et al. (2007)" startWordPosition="997" endWordPosition="1000">rdNet sense and it has been previously shown that a kernel model based on distributional lexical similarity can attain very good performance ( O´ S´eaghdha and Copestake, 2008). 3.2 Calculating the WordNet kernels The noun hierarchy in WordNet 3.0 contains 82,115 senses; computing kernel similarities on a graph of this size raises significant computational issues. The calculation of the Laplacian pseudoinverse is complicated by the fact that while L and Lˆ are very sparse, their pseudoinverses are invariably dense and require very large amounts of memory. To circumvent this problem, we follow Fouss et al. (2007) in computing L+ and ˆL+ one column at a time through a Cholesky factorisation procedure. Only those columns required for the classification task need be calculated, and the kernel computation for each relation subtask can be performed in a matter of minutes. Calculating the diffusion kernel involves an eigendecomposition of ˆL, meaning that computing the kernel exactly is infeasible. The solution used here is to approximate Ht by using the m smallest components of the spectrum of Lˆ when computing (3); from (2) it can be seen that a similar approximation can be made to speed up computation of</context>
</contexts>
<marker>Fouss, Pirotte, Renders, Saerens, 2007</marker>
<rawString>Francois Fouss, Alain Pirotte, Jean-Michel Renders, and Marco Saerens. 2007. Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. IEEE Transactions on Knowledge and Data Engineering, 19(3):355–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Preslav Nakov</author>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
<author>Peter Turney</author>
<author>Deniz Yuret</author>
</authors>
<title>SemEval-2007 Task 04: Classification of semantic relations between nominals.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-07).</booktitle>
<contexts>
<context position="5010" citStr="Girju et al., 2007" startWordPosition="849" endWordPosition="852">y 3.1 Graph construction WordNet (Fellbaum, 1998) is a semantic network in which nodes correspond to word senses (or synsets) and edges correspond to relations between senses. In this work we restrict ourselves to the noun component of WordNet and use only hyponymy and instance hyponymy relations for graph construction. The version of WordNet used is WordNet 3.0. To evaluate the utility of the graph-based kernels described in Section 2 for computing lexical similarity, we use the dataset developed for the task on Classifying Semantic Relations Between Nominals at the 2007 SemEval competition (Girju et al., 2007). The dataset comprises candidate example sentences for seven two-argument semantic relations, with 140 training sentences and approximately 80 test sentences for each relation. It is a particularly suitable task for evaluating WordNet kernels, as the candidate relation arguments for each sentence are tagged with their WordNet sense and it has been previously shown that a kernel model based on distributional lexical similarity can attain very good performance ( O´ S´eaghdha and Copestake, 2008). 3.2 Calculating the WordNet kernels The noun hierarchy in WordNet 3.0 contains 82,115 senses; compu</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. SemEval-2007 Task 04: Classification of semantic relations between nominals. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Herbster</author>
<author>Massimiliano Pontil</author>
<author>Sergio Rojas Galeano</author>
</authors>
<title>Fast prediction on a tree.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd Annual Conference on Neural Information Processing Systems (NIPS-08).</booktitle>
<contexts>
<context position="2428" citStr="Herbster et al. (2008)" startWordPosition="370" endWordPosition="373">to an inner product in some vector space. Kernel methods have been widely adopted in NLP over the past decade, in part due to the good performance of SVMs on many tasks and in part due to the ability to exploit prior knowledge about a given task through the choice of an appropriate kernel function. In this section we consider kernel functions that use spectral properties of a graph to compute the similarity between its nodes. The theoretical foundations and some machine learning applications of the adopted approach have been developed by Kondor and Lafferty (2002), Smola and Kondor (2003) and Herbster et al. (2008). Let G be a graph with vertex set V = vi, ... , vn and edge set E C V x V . We assume that G is connected and undirected and that all edges have a positive weight wij &gt; 0. Let A be the symmetric nxn matrix with entries Aij = wij if an edge exists between vertices vi and vj, and Aij = 0 otherwise. Let D be the diagonal matrix with entries Dii = E j∈V Aij. The graph Laplacian L is then defined as L = D − A (1) The normalised Laplacian is defined as Lˆ = D− 2 LD− 1 2 . Both Lˆ and L are positive semidefinite, but they are typically used as starting points 237 Proceedings of NAACL HLT 2009: Short</context>
</contexts>
<marker>Herbster, Pontil, Galeano, 2008</marker>
<rawString>Mark Herbster, Massimiliano Pontil, and Sergio Rojas Galeano. 2008. Fast prediction on a tree. In Proceedings of the 22nd Annual Conference on Neural Information Processing Systems (NIPS-08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thad Hughes</author>
<author>Daniel Ramage</author>
</authors>
<title>Lexical semantic relatedness with random graph walks.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>(EMNLP-CoNLL-</pages>
<contexts>
<context position="9634" citStr="Hughes and Ramage (2007)" startWordPosition="1639" endWordPosition="1643">spite of the fact that all systems which made use of the sense annotations also used a rich variety of other information sources such as features extracted from the sentence context, while the models presented here use only the graph structure of WordNet.1 5 Related work There is a large body of work on using WordNet to compute measures of lexical similarity (Budanitsky and Hirst, 2006). However, many of these measures are not amenable for use as kernel functions as they rely on properties which cannot be expressed as a vector inner product, such as the lowest common subsumer of two vertices. Hughes and Ramage (2007) present a lexical similarity model based on random walks on graphs derived from WordNet; Rao et al. (2008) propose the Laplacian pseudoinverse on such graphs as a lexical similarity measure. Both of these works share aspects of the current paper; however, neither address supervised learning or present an application-oriented evaluation. Extracting features from WordNet for use in supervised learning is a standard technique (Scott and Matwin, 1999). Siolas and d’Alche-Buc (2000) and Basili et al. (2006) use a measure of lexical similarity from WordNet as an intermediary to smooth bagof-words k</context>
<context position="12058" citStr="Hughes and Ramage (2007)" startWordPosition="2031" endWordPosition="2034">DUCER. 239 rather impoverished graph structure (the WordNet hyponym hierarchy is quite tree-like). One of the significant benefits of spectral graph kernels is that they can be computed on arbitrary graphs and are most powerful when graphs have a rich connectivity structure. Some potential future directions that would make greater use of this flexibility include the following: • A simple extension from sense-kernels to word-kernels involves adding word nodes to the WordNet graph, with an edge linking each word to each of its possible senses. This is similar to the graph construction method of Hughes and Ramage (2007) and Rao et al. (2008). However, preliminary experiments on the SemEval Task 4 dataset indicate that further refinement of this approach may be necessary in order to match the performance of kernels based on distributional lexical similarity ( O´ S´eaghdha and Copestake, 2008). • Incorporating other WordNet relations such as meronymy and topicality gives a way of kernelising semantic association or relatedness; one application of this might be in developing supervised methods for spelling correction (Budanitsky and Hirst, 2006). • A WordNet graph can be augmented with information from other so</context>
</contexts>
<marker>Hughes, Ramage, 2007</marker>
<rawString>Thad Hughes and Daniel Ramage. 2007. Lexical semantic relatedness with random graph walks. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Risi Imre Kondor</author>
<author>John Lafferty</author>
</authors>
<title>Diffusion kernels on graphs and other discrete input spaces.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Machine Learning (ICML-02).</booktitle>
<contexts>
<context position="2376" citStr="Kondor and Lafferty (2002)" startWordPosition="361" endWordPosition="364">i-definite (psd) functions, i.e., those that correspond to an inner product in some vector space. Kernel methods have been widely adopted in NLP over the past decade, in part due to the good performance of SVMs on many tasks and in part due to the ability to exploit prior knowledge about a given task through the choice of an appropriate kernel function. In this section we consider kernel functions that use spectral properties of a graph to compute the similarity between its nodes. The theoretical foundations and some machine learning applications of the adopted approach have been developed by Kondor and Lafferty (2002), Smola and Kondor (2003) and Herbster et al. (2008). Let G be a graph with vertex set V = vi, ... , vn and edge set E C V x V . We assume that G is connected and undirected and that all edges have a positive weight wij &gt; 0. Let A be the symmetric nxn matrix with entries Aij = wij if an edge exists between vertices vi and vj, and Aij = 0 otherwise. Let D be the diagonal matrix with entries Dii = E j∈V Aij. The graph Laplacian L is then defined as L = D − A (1) The normalised Laplacian is defined as Lˆ = D− 2 LD− 1 2 . Both Lˆ and L are positive semidefinite, but they are typically used as star</context>
<context position="4062" citStr="Kondor and Lafferty (2002)" startWordPosition="678" endWordPosition="681"> positive definite, and its entries are related to the resistance distance between points in an electrical circuit (Herbster et al., 2008) and to the average commute-time distance, i.e., the average distance of a random walk from one node to another and back again (Fouss et al., 2007). The similarity measure defined by L+ hence takes information about the connectivity of the graph into account as well as information about adjacency. An analogous pseudoinverse ˆL+ can be defined for the normalised Laplacian. A second class of graph-based kernel functions are the diffusion kernels introduced by Kondor and Lafferty (2002). The kernel Ht is defined as Ht = e−tˆL exp(−tˆλi)ˆuiˆuT (3) i where t &gt; 0, and ˆλ1 &gt; · · · &gt; ˆλn and ˆu1, . . . , ˆun are the eigenvalues and eigenvectors of ˆL+ respectively. Ht can be interpreted in terms of heat diffusion or the distribution of a lazy random walk emanating from a given point at a time point t. 3 Methodology 3.1 Graph construction WordNet (Fellbaum, 1998) is a semantic network in which nodes correspond to word senses (or synsets) and edges correspond to relations between senses. In this work we restrict ourselves to the noun component of WordNet and use only hyponymy and i</context>
</contexts>
<marker>Kondor, Lafferty, 2002</marker>
<rawString>Risi Imre Kondor and John Lafferty. 2002. Diffusion kernels on graphs and other discrete input spaces. In Proceedings of the 19th International Conference on Machine Learning (ICML-02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>William W Cohen</author>
</authors>
<title>Learning graph walk based similarity measures for parsed text.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-</booktitle>
<marker>Minkov, Cohen, 2008</marker>
<rawString>Einat Minkov and William W. Cohen. 2008. Learning graph walk based similarity measures for parsed text. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
<author>Ann Copestake</author>
</authors>
<title>Semantic classification with distributional kernels.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING-08).</booktitle>
<marker>S´eaghdha, Copestake, 2008</marker>
<rawString>Diarmuid O´ S´eaghdha and Ann Copestake. 2008. Semantic classification with distributional kernels. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING-08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>David Yarowsky</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Affinity measures based on the graph Laplacian.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd TextGraphs Workshop on Graph-based Algorithms for NLP.</booktitle>
<contexts>
<context position="9741" citStr="Rao et al. (2008)" startWordPosition="1658" endWordPosition="1661">mation sources such as features extracted from the sentence context, while the models presented here use only the graph structure of WordNet.1 5 Related work There is a large body of work on using WordNet to compute measures of lexical similarity (Budanitsky and Hirst, 2006). However, many of these measures are not amenable for use as kernel functions as they rely on properties which cannot be expressed as a vector inner product, such as the lowest common subsumer of two vertices. Hughes and Ramage (2007) present a lexical similarity model based on random walks on graphs derived from WordNet; Rao et al. (2008) propose the Laplacian pseudoinverse on such graphs as a lexical similarity measure. Both of these works share aspects of the current paper; however, neither address supervised learning or present an application-oriented evaluation. Extracting features from WordNet for use in supervised learning is a standard technique (Scott and Matwin, 1999). Siolas and d’Alche-Buc (2000) and Basili et al. (2006) use a measure of lexical similarity from WordNet as an intermediary to smooth bagof-words kernels on documents. Siolas and d’AlcheBuc use an inverse path-based similarity measure, while Basili et al</context>
<context position="12080" citStr="Rao et al. (2008)" startWordPosition="2036" endWordPosition="2039">d graph structure (the WordNet hyponym hierarchy is quite tree-like). One of the significant benefits of spectral graph kernels is that they can be computed on arbitrary graphs and are most powerful when graphs have a rich connectivity structure. Some potential future directions that would make greater use of this flexibility include the following: • A simple extension from sense-kernels to word-kernels involves adding word nodes to the WordNet graph, with an edge linking each word to each of its possible senses. This is similar to the graph construction method of Hughes and Ramage (2007) and Rao et al. (2008). However, preliminary experiments on the SemEval Task 4 dataset indicate that further refinement of this approach may be necessary in order to match the performance of kernels based on distributional lexical similarity ( O´ S´eaghdha and Copestake, 2008). • Incorporating other WordNet relations such as meronymy and topicality gives a way of kernelising semantic association or relatedness; one application of this might be in developing supervised methods for spelling correction (Budanitsky and Hirst, 2006). • A WordNet graph can be augmented with information from other sources, such as links b</context>
</contexts>
<marker>Rao, Yarowsky, Callison-Burch, 2008</marker>
<rawString>Delip Rao, David Yarowsky, and Chris Callison-Burch. 2008. Affinity measures based on the graph Laplacian. In Proceedings of the 3rd TextGraphs Workshop on Graph-based Algorithms for NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sam Scott</author>
<author>Stan Matwin</author>
</authors>
<title>Feature engineering for text classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th International Conference on Machine Learning (ICML99).</booktitle>
<contexts>
<context position="10086" citStr="Scott and Matwin, 1999" startWordPosition="1710" endWordPosition="1713">rnel functions as they rely on properties which cannot be expressed as a vector inner product, such as the lowest common subsumer of two vertices. Hughes and Ramage (2007) present a lexical similarity model based on random walks on graphs derived from WordNet; Rao et al. (2008) propose the Laplacian pseudoinverse on such graphs as a lexical similarity measure. Both of these works share aspects of the current paper; however, neither address supervised learning or present an application-oriented evaluation. Extracting features from WordNet for use in supervised learning is a standard technique (Scott and Matwin, 1999). Siolas and d’Alche-Buc (2000) and Basili et al. (2006) use a measure of lexical similarity from WordNet as an intermediary to smooth bagof-words kernels on documents. Siolas and d’AlcheBuc use an inverse path-based similarity measure, while Basili et al. use a measure of “conceptual density” that is not proven to be positive semi-definite. 6 Conclusion and future work The main purpose of this paper has been to demonstrate how kernels that capture spectral aspects of graph structure can be used to compare nodes in a lexical hierarchy and thus provide a kernelised measure of WordNet similarity</context>
</contexts>
<marker>Scott, Matwin, 1999</marker>
<rawString>Sam Scott and Stan Matwin. 1999. Feature engineering for text classification. In Proceedings of the 16th International Conference on Machine Learning (ICML99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1693" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="245" endWordPosition="249">r we consider measures of network similarity for the purpose of supervised classification with kernel methods. The utility of kernel functions related to popular distributional similarity measures has recently been demonstrated by O´ S´eaghdha and Copestake (2008); we show here that kernel analogues of WordNet similarity can likewise give good performance on a semantic classification task. 2 Kernels derived from graphs Kernel-based classifiers such as support vector machines (SVMs) make use of functions called kernel functions (or simply kernels) to compute the similarity between data points (Shawe-Taylor and Cristianini, 2004). Valid kernels are restricted to the set of positive semi-definite (psd) functions, i.e., those that correspond to an inner product in some vector space. Kernel methods have been widely adopted in NLP over the past decade, in part due to the good performance of SVMs on many tasks and in part due to the ability to exploit prior knowledge about a given task through the choice of an appropriate kernel function. In this section we consider kernel functions that use spectral properties of a graph to compute the similarity between its nodes. The theoretical foundations and some machine learning app</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georges Siolas</author>
<author>Florence d’Alche-Buc</author>
</authors>
<title>Support vector machines based on a semantic kernel for text categorization.</title>
<date>2000</date>
<booktitle>In Proceedings of the IEEE-INNSENNS International Joint Conference on Neural Networks.</booktitle>
<marker>Siolas, d’Alche-Buc, 2000</marker>
<rawString>Georges Siolas and Florence d’Alche-Buc. 2000. Support vector machines based on a semantic kernel for text categorization. In Proceedings of the IEEE-INNSENNS International Joint Conference on Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander J Smola</author>
<author>Risi Kondor</author>
</authors>
<title>Kernels and regularization on graphs.</title>
<date>2003</date>
<booktitle>In Proceedings of the the 16th Annual Conference on Learning Theory and 7th Workshop on Kernel Machines (COLT-03).</booktitle>
<contexts>
<context position="2401" citStr="Smola and Kondor (2003)" startWordPosition="365" endWordPosition="368">i.e., those that correspond to an inner product in some vector space. Kernel methods have been widely adopted in NLP over the past decade, in part due to the good performance of SVMs on many tasks and in part due to the ability to exploit prior knowledge about a given task through the choice of an appropriate kernel function. In this section we consider kernel functions that use spectral properties of a graph to compute the similarity between its nodes. The theoretical foundations and some machine learning applications of the adopted approach have been developed by Kondor and Lafferty (2002), Smola and Kondor (2003) and Herbster et al. (2008). Let G be a graph with vertex set V = vi, ... , vn and edge set E C V x V . We assume that G is connected and undirected and that all edges have a positive weight wij &gt; 0. Let A be the symmetric nxn matrix with entries Aij = wij if an edge exists between vertices vi and vj, and Aij = 0 otherwise. Let D be the diagonal matrix with entries Dii = E j∈V Aij. The graph Laplacian L is then defined as L = D − A (1) The normalised Laplacian is defined as Lˆ = D− 2 LD− 1 2 . Both Lˆ and L are positive semidefinite, but they are typically used as starting points 237 Proceedin</context>
</contexts>
<marker>Smola, Kondor, 2003</marker>
<rawString>Alexander J. Smola and Risi Kondor. 2003. Kernels and regularization on graphs. In Proceedings of the the 16th Annual Conference on Learning Theory and 7th Workshop on Kernel Machines (COLT-03).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>