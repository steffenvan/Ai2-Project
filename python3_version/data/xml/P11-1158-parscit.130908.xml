<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003947">
<title confidence="0.978078">
Efficient CCG Parsing: A* versus Adaptive Supertagging
</title>
<author confidence="0.99675">
Michael Auli Adam Lopez
</author>
<affiliation confidence="0.99785">
School of Informatics HLTCOE
University of Edinburgh Johns Hopkins University
</affiliation>
<email confidence="0.998484">
m.auli@sms.ed.ac.uk alopez@cs.jhu.edu
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999775173913044">
We present a systematic comparison and com-
bination of two orthogonal techniques for
efficient parsing of Combinatory Categorial
Grammar (CCG). First we consider adap-
tive supertagging, a widely used approximate
search technique that prunes most lexical cat-
egories from the parser’s search space using
a separate sequence model. Next we con-
sider several variants on A*, a classic exact
search technique which to our knowledge has
not been applied to more expressive grammar
formalisms like CCG. In addition to standard
hardware-independent measures of parser ef-
fort we also present what we believe is the first
evaluation of A* parsing on the more realistic
but more stringent metric of CPU time. By it-
self, A* substantially reduces parser effort as
measured by the number of edges considered
during parsing, but we show that for CCG this
does not always correspond to improvements
in CPU time over a CKY baseline. Combining
A* with adaptive supertagging decreases CPU
time by 15% for our best model.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999501047619047">
Efficient parsing of Combinatorial Categorial Gram-
mar (CCG; Steedman, 2000) is a longstanding prob-
lem in computational linguistics. Even with practi-
cal CCG that are strongly context-free (Fowler and
Penn, 2010), parsing can be much harder than with
Penn Treebank-style context-free grammars, since
the number of nonterminal categories is generally
much larger, leading to increased grammar con-
stants. Where a typical Penn Treebank grammar
may have fewer than 100 nonterminals (Hocken-
maier and Steedman, 2002), we found that a CCG
grammar derived from CCGbank contained nearly
1600. The same grammar assigns an average of 26
lexical categories per word, resulting in a very large
space of possible derivations.
The most successful strategy to date for efficient
parsing of CCG is to first prune the set of lexi-
cal categories considered for each word, using the
output of a supertagger, a sequence model over
these categories (Bangalore and Joshi, 1999; Clark,
2002). Variations on this approach drive the widely-
used, broad coverage C&amp;C parser (Clark and Cur-
ran, 2004; Clark and Curran, 2007). However, prun-
ing means approximate search: if a lexical category
used by the highest probability derivation is pruned,
the parser will not find that derivation (§2). Since the
supertagger enforces no grammaticality constraints
it may even prefer a sequence of lexical categories
that cannot be combined into any derivation (Fig-
ure 1). Empirically, we show that supertagging im-
proves efficiency by an order of magnitude, but the
tradeoff is a significant loss in accuracy (§3).
Can we improve on this tradeoff? The line of in-
vestigation we pursue in this paper is to consider
more efficient exact algorithms. In particular, we
test different variants of the classical A* algorithm
(Hart et al., 1968), which has met with success in
Penn Treebank parsing with context-free grammars
(Klein and Manning, 2003; Pauls and Klein, 2009a;
Pauls and Klein, 2009b). We can substitute A* for
standard CKY on either the unpruned set of lexi-
cal categories, or the pruned set resulting from su-
</bodyText>
<page confidence="0.946989">
1577
</page>
<note confidence="0.961131">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1577–1585,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
I like tea I like tea
</note>
<figureCaption confidence="0.998235333333333">
Figure 1: The relationship between supertagger and
parser search spaces based on the intersection of their cor-
responding tag sequences.
</figureCaption>
<bodyText confidence="0.999087142857143">
pertagging. Our empirical results show that on the
unpruned set of lexical categories, heuristics em-
ployed for context-free grammars show substantial
speedups in hardware-independent metrics of parser
effort (§4). To understand how this compares to the
CKY baseline, we conduct a carefully controlled set
of timing experiments. Although our results show
that improvements on hardware-independent met-
rics do not always translate into improvements in
CPU time due to increased processing costs that are
hidden by these metrics, they also show that when
the lexical categories are pruned using the output of
a supertagger, then we can still improve efficiency
by 15% with A* techniques (§5).
</bodyText>
<sectionHeader confidence="0.96866" genericHeader="introduction">
2 CCG and Parsing Algorithms
</sectionHeader>
<bodyText confidence="0.999448411764706">
CCG is a lexicalized grammar formalism encoding
for each word lexical categories which are either
basic (eg. NN, JJ) or complex. Complex lexical
categories specify the number and directionality of
arguments. For example, one lexical category (of
over 100 in our model) for the transitive verb like is
(S\NP2)/NP1, specifying the first argument as an
NP to the right and the second as an NP to the left. In
parsing, adjacent spans are combined using a small
number of binary combinatory rules like forward ap-
plication or composition on the spanning categories
(Steedman, 2000; Fowler and Penn, 2010). In the
first derivation below, (S\NP)/NP and NP com-
bine to form the spanning category S\NP, which
only requires an NP to its left to form a complete
sentence-spanning S. The second derivation uses
type-raising to change the category type of I.
</bodyText>
<equation confidence="0.995951285714286">
NP (S\NP)/NP NP
&gt;
S/(S\NP)
&gt;�
NP (S\NP)/NP NP
S S/NP
S
</equation>
<bodyText confidence="0.9998236">
Because of the number of lexical categories and their
complexity, a key difficulty in parsing CCG is that
the number of analyses for each span of the sentence
quickly becomes extremely large, even with efficient
dynamic programming.
</bodyText>
<subsectionHeader confidence="0.995821">
2.1 Adaptive Supertagging
</subsectionHeader>
<bodyText confidence="0.99993965625">
Supertagging (Bangalore and Joshi, 1999) treats the
assignment of lexical categories (or supertags) as a
sequence tagging problem. Once the supertagger
has been run, lexical categories that apply to each
word in the input sentence are pruned to contain only
those with high posterior probability (or other figure
of merit) under the supertagging model (Clark and
Curran, 2004). The posterior probabilities are then
discarded; it is the extensive pruning of lexical cate-
gories that leads to substantially faster parsing times.
Pruning the categories in advance this way has a
specific failure mode: sometimes it is not possible
to produce a sentence-spanning derivation from the
tag sequences preferred by the supertagger, since it
does not enforce grammaticality. A workaround for
this problem is the adaptive supertagging (AST) ap-
proach of Clark and Curran (2004). It is based on a
step function over supertagger beam ratios, relaxing
the pruning threshold for lexical categories when-
ever the parser fails to find an analysis. The pro-
cess either succeeds and returns a parse after some
iteration or gives up after a predefined number of
iterations. As Clark and Curran (2004) show, most
sentences can be parsed with a very small number of
supertags per word. However, the technique is inher-
ently approximate: it will return a lower probability
parse under the parsing model if a higher probabil-
ity parse can only be constructed from a supertag
sequence returned by a subsequent iteration. In this
way it prioritizes speed over accuracy, although the
tradeoff can be modified by adjusting the beam step
function.
</bodyText>
<subsectionHeader confidence="0.999063">
2.2 A* Parsing
</subsectionHeader>
<bodyText confidence="0.998574666666667">
Irrespective of whether lexical categories are pruned
in advance using the output of a supertagger, the
CCG parsers we are aware of all use some vari-
</bodyText>
<figure confidence="0.998826916666667">
Attainable parses
Desirable parses
High scoring
supertags
Valid parses
High scoring
parses
Valid supertag-sequences
S\NP
�
&gt;e
&gt;
</figure>
<page confidence="0.987435">
1578
</page>
<bodyText confidence="0.999917181818182">
ant of the CKY algorithm. Although CKY is easy
to implement, it is exhaustive: it explores all pos-
sible analyses of all possible spans, irrespective of
whether such analyses are likely to be part of the
highest probability derivation. Hence it seems nat-
ural to consider exact algorithms that are more effi-
cient than CKY.
A* search is an agenda-based best-first graph
search algorithm which finds the lowest cost parse
exactly without necessarily traversing the entire
search space (Klein and Manning, 2003). In contrast
to CKY, items are not processed in topological order
using a simple control loop. Instead, they are pro-
cessed from a priority queue, which orders them by
the product of their inside probability and a heuris-
tic estimate of their outside probability. Provided
that the heuristic never underestimates the true out-
side probability (i.e. it is admissible) the solution is
guaranteed to be exact. Heuristics are model specific
and we consider several variants in our experiments
based on the CFG heuristics developed by Klein and
Manning (2003) and Pauls and Klein (2009a).
</bodyText>
<sectionHeader confidence="0.98833" genericHeader="method">
3 Adaptive Supertagging Experiments
</sectionHeader>
<bodyText confidence="0.989073842105263">
Parser. For our experiments we used the generative
CCG parser of Hockenmaier and Steedman (2002).
Generative parsers have the property that all edge
weights are non-negative, which is required for A*
techniques.1 Although not quite as accurate as the
discriminative parser of Clark and Curran (2007) in
our preliminary experiments, this parser is still quite
competitive. It is written in Java and implements
the CKY algorithm with a global pruning threshold
of 10−4 for the models we use. We focus on two
parsing models: PCFG, the baseline of Hockenmaier
and Steedman (2002) which treats the grammar as a
PCFG (Table 1); and HWDep, a headword depen-
dency model which is the best performing model of
the parser. The PCFG model simply generates a tree
top down and uses very simple structural probabili-
ties while the HWDep model conditions node expan-
sions on headwords and their lexical categories.
Supertagger. For supertagging we used Den-
nis Mehay’s implementation, which follows Clark
1Indeed, all of the past work on A* parsing that we are aware of
uses generative parsers (Pauls and Klein, 2009b, inter alia).
(2002).2 Due to differences in smoothing of the
supertagging and parsing models, we occasionally
drop supertags returned by the supertagger because
they do not appear in the parsing model 3.
Evaluation. All experiments were conducted on
CCGBank (Hockenmaier and Steedman, 2007), a
right-most normal-form CCG version of the Penn
Treebank. Models were trained on sections 2-21,
tuned on section 00, and tested on section 23. Pars-
ing accuracy is measured using labelled and unla-
belled predicate argument structure recovery (Clark
and Hockenmaier, 2002); we evaluate on all sen-
tences and thus penalise lower coverage. All tim-
ing experiments reported in the paper were run on a
2.5 GHz Xeon machine with 32 GB memory and are
averaged over ten runs4.
</bodyText>
<subsectionHeader confidence="0.865309">
3.1 Results
</subsectionHeader>
<bodyText confidence="0.9999565">
Supertagging has been shown to improve the speed
of a generative parser, although little analysis has
been reported beyond the speedups (Clark, 2002)
We ran experiments to understand the time/accuracy
tradeoff of adaptive supertagging, and to serve as
baselines.
Adaptive supertagging is parametrized by a beam
size Q and a dictionary cutoff k that bounds the
number of lexical categories considered for each
word (Clark and Curran, 2007). Table 3 shows both
the standard beam levels (AST) used for the C&amp;C
parser and looser beam levels: AST-covA, a sim-
ple extension of AST with increased coverage and
AST-covB, also increasing coverage but with bet-
ter performance for the HWDep model.
Parsing results for the AST settings (Tables 4
and 5) confirm that it improves speed by an order of
magnitude over a baseline parser without AST. Per-
haps surprisingly, the number of parse failures de-
creases with AST in some cases. This is because the
parser prunes more aggressively as the search space
increases.5
</bodyText>
<footnote confidence="0.9974842">
2http://code.google.com/p/statopenccg
3Less than 2% of supertags are affected by this.
4The timing results reported differ from an earlier draft since
we used a different machine
5Hockenmaier and Steedman (2002) saw a similar effect.
</footnote>
<page confidence="0.995899">
1579
</page>
<bodyText confidence="0.9697055">
Expansion probability p(exp|P) exp E {leaf, unary, left-head, right-head}
Head probability p(H|P, exp) H is the head daughter
Non-head probability p(S|P, exp, H) S is the non-head daughter
Lexical probability p(w|P)
</bodyText>
<tableCaption confidence="0.640885333333333">
Table 1: Factorisation of the PCFG model. H,P, and S are categories, and w is a word.
Expansion probability p(exp|P, cP#wP) exp E {leaf, unary, left-head, right-head}
Head probability p(H|P, exp, cP#wP) H is the head daughter
Non-head probability p(S|P, exp, H#cP#wP) S is the non-head daughter
Lexcat probability p(cS|S#P,H,S) p(cTOP|P=TOP)
Headword probability p(wS|cS#P, H, S, wP) p(wTOP|cTOP)
Table 2: Headword dependency model factorisation, backoff levels are denoted by ’#’ between conditioning variables:
A # B # C indicates that P�(... |A, B, C) is interpolated with P�(... |A, B), which is an interpolation of P� ... |A, B)
and P�(... |A). Variables cp and wp represent, respectively, the head lexical category and headword of category P.
</tableCaption>
<table confidence="0.999961714285714">
Condition Parameter Iteration 1 2 3 4 5 6
AST Q (beam width) 0.075 0.03 0.01 0.005 0.001
k (dictionary cutoff) 20 20 20 20 150
AST-covA Q 0.075 0.03 0.01 0.005 0.001 0.0001
k 20 20 20 20 150 150
AST-covB Q 0.03 0.01 0.005 0.001 0.0001 0.0001
k 20 20 20 20 20 150
</table>
<tableCaption confidence="0.987482">
Table 3: Beam step function used for standard (AST) and high-coverage (AST-covA and AST-covB) supertagging.
</tableCaption>
<table confidence="0.999869555555555">
Time(sec) Sent/sec Cats/word Fail UP UR UF LP LR LF
PCFG 290 6.6 26.2 5 86.4 86.5 86.5 77.2 77.3 77.3
PCFG (AST) 65 29.5 1.5 14 87.4 85.9 86.6 79.5 78.0 78.8
PCFG (AST-covA) 67 28.6 1.5 6 87.3 86.9 87.1 79.1 78.8 78.9
PCFG (AST-covB) 69 27.7 1.7 5 87.3 86.2 86.7 79.1 78.1 78.6
HWDep 1512 1.3 26.2 5 90.2 90.1 90.2 83.2 83.0 83.1
HWDep (AST) 133 14.4 1.5 16 89.8 88.0 88.9 82.6 80.9 81.8
HWDep (AST-covA) 139 13.7 1.5 9 89.8 88.3 89.0 82.6 81.1 81.9
HWDep (AST-covB) 155 12.3 1.7 7 90.1 88.7 89.4 83.0 81.8 82.4
</table>
<tableCaption confidence="0.991409333333333">
Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative
CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall
(LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis.
</tableCaption>
<subsectionHeader confidence="0.994656">
3.2 Efficiency versus Accuracy
</subsectionHeader>
<bodyText confidence="0.9996358">
The most interesting result is the effect of the
speedup on accuracy. As shown in Table 6, the
vast majority of sentences are actually parsed with
a very tight supertagger beam, raising the question
of whether many higher-scoring parses are pruned.6
</bodyText>
<footnote confidence="0.897544">
6Similar results are reported by Clark and Curran (2007).
</footnote>
<bodyText confidence="0.999000166666667">
Despite this, labeled F-score improves by up to 1.6
F-measure for the PCFG model, although it harms
accuracy for HWDep as expected.
In order to understand this effect, we filtered sec-
tion 00 to include only sentences of between 18
and 26 words (resulting in 610 sentences) for which
</bodyText>
<page confidence="0.893086">
1580
</page>
<table confidence="0.999838111111111">
Time(sec) Sent/sec Cats/word Fail UP UR UF LP LR LF
PCFG 326 7.4 25.7 29 85.9 85.4 85.7 76.6 76.2 76.4
PCFG (AST) 82 29.4 1.5 34 86.7 84.8 85.7 78.6 76.9 77.7
PCFG (AST-covA) 85 28.3 1.6 15 86.6 85.5 86.0 78.5 77.5 78.0
PCFG (AST-covB) 86 27.9 1.7 14 86.6 85.6 86.1 78.1 77.3 77.7
HWDep 1754 1.4 25.7 30 90.2 89.3 89.8 83.5 82.7 83.1
HWDep (AST) 167 14.4 1.5 27 89.5 87.6 88.5 82.3 80.6 81.5
HWDep (AST-covA) 177 13.6 1.6 14 89.4 88.1 88.8 82.2 81.1 81.7
HWDep (AST-covB) 188 12.8 1.7 14 89.7 88.5 89.1 82.5 81.4 82.0
</table>
<tableCaption confidence="0.997164">
Table 5: Results on CCGbank section 23 when applying adaptive supertagging (AST) to two models of a CCG parser.
</tableCaption>
<table confidence="0.999884285714286">
Q Cats/word Parses %
0.075 1.33 1676 87.6
0.03 1.56 114 6.0
0.01 1.97 60 3.1
0.005 2.36 15 0.8
0.001k =150 3.84 32 1.7
Fail 16 0.9
</table>
<tableCaption confidence="0.89838125">
Table 6: Breakdown of the number of sentences parsed
for the HWDep (AST) model (see Table 4) at each of
the supertagger beam levels from the most to the least
restrictive setting.
</tableCaption>
<figure confidence="0.980560555555555">
N
fo
N
m
P
PCFGrF -­‐s corepHWDepiF-­‐sc or eaFigu re2:Log- pr obabi lityo
fpar ses relative to exact so lu-t ion vs.labell edF-scoreat
%
100.0
99.5
99.0
98.5
98.0
97.5
t obabi�� l
97.0
96.5
96.0
95.5
95.0
Supe rtagge
PCF ��Log� � Probab
�� beam
lity HWDeprL
gWPro ability
88
87
86
85
84
83
82
81
80
79
Labeled F­-‐score
</figure>
<bodyText confidence="0.999033611111111">
we can perform exhaustive search without pruning7,
and for which we could parse without failure at all
of the tested beam settings. We then measured the
log probability of the highest probability parse found
under a variety of beam settings, relative to the log
probability of the unpruned exact parse, along with
the labeled F-Score of the Viterbi parse under these
settings (Figure 2). The results show that PCFG ac-
tually finds worse results as it considers more of the
search space. In other words, the supertagger can ac-
tually “fix” a bad parsing model by restricting it to
a small portion of the search space. With the more
accurate HWDep model, this does not appear to be
a problem and there is a clear opportunity for im-
provement by considering the larger search space.
The next question is whether we can exploit this
larger search space without paying as high a cost in
efficiency.
</bodyText>
<footnote confidence="0.95500725">
7The fact that only a subset of short sentences could be exhaus-
tively parsed demonstrates the need for efficient search algo-
rithms.
l we u
</footnote>
<bodyText confidence="0.980683571428571">
e ac hsupert aggingbeam-
le vel.4A* ParsingExpe ri mentsToc omp areappro
aches, we extende d o urbasel ineparser tosupp ort
A*search . Foll ow ing (Klei nan dManning,20 03 ) wer
estric to urexp er ime ntstose n-ten cesonw hic h we
can per form exacts ea rchviau s- in gt he sam esubse
tof section0 0as inm3. 2. Befo reconsid eri ngCPUt
im e,we first ev alu atethe amoun tofwo rkdonebyt
heparserusi ngthree h ar dware-i nde penden tm
etric s.We me asure t hen umbero fedges pus hed(P
aulsand Klein,2009a)a nd edg espopped,correspond
ingtothein ser t/decr ease-keyo pe rat ionsandr
emoveo perationofthe priority qu eue,res pec tive
ly. Fi nally,wemea sure t henum- ber oftrav er sals
,which c ountsthe n umberof edg ew eightsc omp uted,r
eg ardlessof whe th ert hewei ghtisdisc ar d edduet
othepri orex istenc eofabe tterw ei gh t.T hisl att
ermetr ic seems to bet hemo stac -c ura teaccou
nto ft heworkdone b yt hep arse r.D uetod iffe
renc es inthePCFG a ndHWDepmo d- ddifferent A* variants: for els, the weco n sid ere
se a simple A* with aPCFGprecm-mode
</bodyText>
<page confidence="0.981681">
1581
</page>
<bodyText confidence="0.9999584">
puted heuristic, while for the the more complex
HWDep model, we used a hierarchical A* algo-
rithm (Pauls and Klein, 2009a; Felzenszwalb and
McAllester, 2007) based on a simple grammar pro-
jection that we designed.
</bodyText>
<subsectionHeader confidence="0.998618">
4.1 Hardware-Independent Results: PCFG
</subsectionHeader>
<bodyText confidence="0.967926875">
For the PCFG model, we compared three agenda-
based parsers: EXH prioritizes edges by their span
length, thereby simulating the exhaustive CKY algo-
rithm; NULL prioritizes edges by their inside proba-
bility; and SX is an A* parser that prioritizes edges
by their inside probability times an admissible out-
side probability estimate.8 We use the SX estimate
devised by Klein and Manning (2003) for CFG pars-
ing, where they found it offered very good perfor-
mance for relatively little computation. It gives a
bound on the outside probability of a nonterminal P
with i words to the right and j words to the left, and
can be computed from a grammar using a simple dy-
namic program.
The parsers are tested with and without adap-
tive supertagging where the former can be seen as
performing exact search (via A*) over the pruned
search space created by AST.
Table 7 shows that A* with the SX heuristic de-
creases the number of edges pushed by up to 39%
on the unpruned search space. Although encourag-
ing, this is not as impressive as the 95% speedup
obtained by Klein and Manning (2003) with this
heuristic on their CFG. On the other hand, the NULL
heuristic works better for CCG than for CFG, with
speedups of 29% and 11%, respectively. These re-
sults carry over to the AST setting which shows that
A* can improve search even on the highly pruned
search graph. Note that A* only saves work in the
final iteration of AST, since for earlier iterations it
must process the entire agenda to determine that
there is no spanning analysis.
Since there are many more categories in the CCG
grammar we might have expected the SX heuristic to
work better than for a CFG. Why doesn’t it? We can
measure how well a heuristic bounds the true cost in
8The NULL parser is a special case of A*, also called uni-
form cost search, which in the case of parsing corresponds to
Knuth’s algorithm (Knuth, 1977; Klein and Manning, 2001),
the extension of Dijkstra’s algorithm to hypergraphs.
</bodyText>
<subsectionHeader confidence="0.482007">
Outside Span
</subsectionHeader>
<figureCaption confidence="0.99371325">
Figure 3: Average slack of the SX heuristic. The figure
aggregates the ratio of the difference between the esti-
mated outside cost and true outside costs relative to the
true cost across the development set.
</figureCaption>
<bodyText confidence="0.999980888888889">
terms of slack: the difference between the true and
estimated outside cost. Lower slack means that the
heuristic bounds the true cost better and guides us to
the exact solution more quickly. Figure 3 plots the
average slack for the SX heuristic against the num-
ber of words in the outside context. Comparing this
with an analysis of the same heuristic when applied
to a CFG by Klein and Manning (2003), we find that
it is less effective in our setting9. There is a steep
increase in slack for outside contexts with size more
than one. The main reason for this is because a sin-
gle word in the outside context is in many cases the
full stop at the end of the sentence, which is very pre-
dictable. However for longer spans the flexibility of
CCG to analyze spans in many different ways means
that the outside estimate for a nonterminal can be
based on many high probability outside derivations
which do not bound the true probability very well.
</bodyText>
<subsectionHeader confidence="0.999333">
4.2 Hardware-Independent Results: HWDep
</subsectionHeader>
<bodyText confidence="0.977769625">
Lexicalization in the HWDep model makes the pre-
computed SX estimate impractical, so for this model
we designed two hierarchical A* (HA*) variants
based on simple grammar projections of the model.
The basic idea of HA* is to compute Viterbi in-
side probabilities using the easier-to-parse projected
9Specifically, we refer to Figure 9 of their paper which uses a
slightly different representation of estimate sharpness
</bodyText>
<figure confidence="0.998848181818182">
1 4 7 10 13 16 19 22 25
Average Slack
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
</figure>
<page confidence="0.978709">
1582
</page>
<table confidence="0.9966224">
Parser Edges pushed Edges popped Traversals
Std % AST % Std % AST % Std % AST %
EXH 34 100 6.3 100 15.7 100 4.2 100 133.4 100 13.3 100
NULL 24.3 71 4.9 78 13.5 86 3.5 83 113.8 85 11.1 83
SX 20.9 61 4.3 68 10.0 64 2.6 62 96.5 72 9.7 73
</table>
<tableCaption confidence="0.9943205">
Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges
pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging.
</tableCaption>
<bodyText confidence="0.999897382352941">
grammar, use these to compute Viterbi outside prob-
abilities for the simple grammar, and then use these
as outside estimates for the true grammar; all com-
putations are prioritized in a single agenda follow-
ing the algorithm of Felzenszwalb and McAllester
(2007) and Pauls and Klein (2009a). We designed
two simple grammar projections, each simplifying
the HWDep model: PCFGProj completely re-
moves lexicalization and projects the grammar to
a PCFG, while as LexcatProj removes only the
headwords but retains the lexical categories.
Figure 4 compares exhaustive search, A* with no
heuristic (NULL), and HA*. For HA*, parsing ef-
fort is broken down into the different edge types
computed at each stage: We distinguish between the
work carried out to compute the inside and outside
edges of the projection, where the latter represent
the heuristic estimates, and finally, the work to com-
pute the edges of the target grammar. We find that
A* NULL saves about 44% of edges pushed which
makes it slightly more effective than for the PCFG
model. However, the effort to compute the gram-
mar projections outweighs their benefit. We suspect
that this is due to the large difference between the
target grammar and the projection: The PCFG pro-
jection is a simple grammar and so we improve the
probability of a traversal less often than in the target
grammar.
The Lexcat projection performs worst, for two
reasons. First, the projection requires about as much
work to compute as the target grammar without a
heuristic (NULL). Second, the projection itself does
not save a large amount of work as can be seen in
the statistics for the target grammar.
</bodyText>
<sectionHeader confidence="0.98613" genericHeader="method">
5 CPU Timing Experiments
</sectionHeader>
<bodyText confidence="0.999966972972973">
Hardware-independent metrics are useful for under-
standing agenda-based algorithms, but what we ac-
tually care about is CPU time. We were not aware of
any past work that measures A* parsers in terms of
CPU time, but as this is the real objective we feel that
experiments of this type are important. This is espe-
cially true in real implementations because the sav-
ings in edges processed by an agenda parser come at
a cost: operations on the priority queue data struc-
ture can add significant runtime.
Timing experiments of this type are very
implementation-dependent, so we took care to im-
plement the algorithms as cleanly as possible and
to reuse as much of the existing parser code as we
could. An important implementation decision for
agenda-based algorithms is the data structure used
to implement the priority queue. Preliminary experi-
ments showed that a Fibonacci heap implementation
outperformed several alternatives: Brodal queues
(Brodal, 1996), binary heaps, binomial heaps, and
pairing heaps.10
We carried out timing experiments on the best A*
parsers for each model (SX and NULL for PCFG and
HWDep, respectively), comparing them with our
CKY implementation and the agenda-based CKY
simulation EXH; we used the same data as in §3.2.
Table 8 presents the cumulative running times with
and without adaptive supertagging average over ten
runs, while Table 9 reports F-scores.
The results (Table 8) are striking. Although the
timing results of the agenda-based parsers track the
hardware-independent metrics, they start at a signif-
icant disadvantage to exhaustive CKY with a sim-
ple control loop. This is most evident when looking
at the timing results for EXH, which in the case of
the full PCFG model requires more than twice the
time than the CKY algorithm that it simulates. A*
</bodyText>
<footnote confidence="0.9900375">
10We used the Fibonacci heap implementation at
http://www.jgrapht.org
</footnote>
<page confidence="0.966344">
1583
</page>
<figureCaption confidence="0.9808335">
Figure 4: Comparsion between a CKY simulation (EXH), A* with no heuristic (NULL), hierarchical A* (HA*) using
two grammar projections for standard search (left) and AST (right). The breakdown of the inside/outside edges for the
grammar projection as well as the target grammar shows that the projections, serving as the heuristic estimates for the
target grammar, are costly to compute.
</figureCaption>
<table confidence="0.999197833333333">
Standard AST
PCFG HWDep PCFG HWDep
CKY 536 24489 34 143
EXH 1251 26889 41 155
A* NULL 1032 21830 36 121
A* SX 889 - 34 -
</table>
<tableCaption confidence="0.971022">
Table 8: Parsing time in seconds of CKY and agenda-
based parsers with and without adaptive supertagging.
</tableCaption>
<table confidence="0.999691833333333">
Standard AST
PCFG HWDep PCFG HWDep
CKY 80.4 85.5 81.7 83.8
EXH 79.4 85.5 80.3 83.8
A* NULL 79.6 85.5 80.7 83.8
A* SX 79.4 - 80.4 -
</table>
<tableCaption confidence="0.6104155">
Table 9: Labelled F-score of exact CKY and agenda-
based parsers with/without adaptive supertagging. All
parses have the same probabilities, thus variances are due
to implementation-dependent differences in tiebreaking.
</tableCaption>
<bodyText confidence="0.9998">
makes modest CPU-time improvements in parsing
the full space of the HWDep model. Although this
decreases the time required to obtain the highest ac-
curacy, it is still a substantial tradeoff in speed com-
pared with AST.
On the other hand, the AST tradeoff improves sig-
nificantly: by combining AST with A* we observe
a decrease in running time of 15% for the A* NULL
parser of the HWDep model over CKY. As the CKY
baseline with AST is very strong, this result shows
that A* holds real promise for CCG parsing.
</bodyText>
<sectionHeader confidence="0.998117" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999981260869565">
Adaptive supertagging is a strong technique for ef-
ficient CCG parsing. Our analysis confirms tremen-
dous speedups, and shows that for weak models, it
can even result in improved accuracy. However, for
better models, the efficiency gains of adaptive su-
pertagging come at the cost of accuracy. One way to
look at this is that the supertagger has good precision
with respect to the parser’s search space, but low re-
call. For instance, we might combine both parsing
and supertagging models in a principled way to ex-
ploit these observations, eg. by making the supertag-
ger output a soft constraint on the parser rather than
a hard constraint. Principled, efficient search algo-
rithms will be crucial to such an approach.
To our knowledge, we are the first to measure
A* parsing speed both in terms of running time and
commonly used hardware-independent metrics. It
is clear from our results that the gains from A* do
not come as easily for CCG as for CFG, and that
agenda-based algorithms like A* must make very
large reductions in the number of edges processed
to result in realtime savings, due to the added ex-
pense of keeping a priority queue. However, we
</bodyText>
<page confidence="0.982577">
1584
</page>
<bodyText confidence="0.999970357142857">
have shown that A* can yield real improvements
even over the highly optimized technique of adaptive
supertagging: in this pruned search space, a 44%
reduction in the number of edges pushed results in
a 15% speedup in CPU time. Furthermore, just as
A* can be combined with adaptive supertagging, it
should also combine easily with other search-space
pruning methods, such as those of Djordjevic et
al. (2007), Kummerfeld et al. (2010), Zhang et al.
(2010) and Roark and Hollingshead (2009). In fu-
ture work we plan to examine better A* heuristics
for CCG, and to look at principled approaches to
combine the strengths of A*, adaptive supertagging,
and other techniques to the best advantage.
</bodyText>
<sectionHeader confidence="0.999024" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999819933333333">
We would like to thank Prachya Boonkwan, Juri
Ganitkevitch, Philipp Koehn, Tom Kwiatkowski,
Matt Post, Mark Steedman, Emily Thomforde, and
Luke Zettlemoyer for helpful discussion related to
this work and comments on previous drafts; Julia
Hockenmaier for furnishing us with her parser; and
the anonymous reviewers for helpful commentary.
We also acknowledge funding from EPSRC grant
EP/P504171/1 (Auli); the EuroMatrixPlus project
funded by the European Commission, 7th Frame-
work Programme (Lopez); and the resources pro-
vided by the Edinburgh Compute and Data Fa-
cility (http://www.ecdf.ed.ac.uk/). The
ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk/).
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999977098360656">
S. Bangalore and A. K. Joshi. 1999. Supertagging: An
Approach to Almost Parsing. Computational Linguis-
tics, 25(2):238–265, June.
G. S. Brodal. 1996. Worst-case efficient priority queues.
In Proc. of SODA, pages 52–58.
S. Clark and J. R. Curran. 2004. The importance of su-
pertagging for wide-coverage CCG parsing. In Proc.
of COLING.
S. Clark and J. R. Curran. 2007. Wide-coverage effi-
cient statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4):493–552.
S. Clark and J. Hockenmaier. 2002. Evaluating a wide-
coverage CCG parser. In Proc. of LREC Beyond Par-
seval Workshop, pages 60–66.
S. Clark. 2002. Supertagging for Combinatory Catego-
rial Grammar. In Proc. of TAG+6, pages 19–24.
B. Djordjevic, J. R. Curran, and S. Clark. 2007. Improv-
ing the efficiency of a wide-coverage CCG parser. In
Proc. of IWPT.
P. F. Felzenszwalb and D. McAllester. 2007. The Gener-
alized A* Architecture. In Journal of Artificial Intelli-
gence Research, volume 29, pages 153–190.
T. A. D. Fowler and G. Penn. 2010. Accurate context-
free parsing with combinatory categorial grammar. In
Proc. of ACL.
P. Hart, N. Nilsson, and B. Raphael. 1968. A formal
basis for the heuristic determination of minimum cost
paths. Transactions on Systems Science and Cybernet-
ics, 4, Jul.
J. Hockenmaier and M. Steedman. 2002. Generative
models for statistical parsing with Combinatory Cat-
egorial Grammar. In Proc. of ACL, pages 335–342.
Association for Computational Linguistics.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
A corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355–396.
D. Klein and C. D. Manning. 2001. Parsing and hyper-
graphs. In Proc. of IWPT.
D. Klein and C. D. Manning. 2003. A* parsing: Fast
exact Viterbi parse selection. In Proc. of HLT-NAACL,
pages 119–126, May.
D. E. Knuth. 1977. A generalization of Dijkstra’s algo-
rithm. Information Processing Letters, 6:1–5.
J. K. Kummerfeld, J. Roesner, T. Dawborn, J. Haggerty,
J. R. Curran, and S. Clark. 2010. Faster parsing by
supertagger adaptation. In Proc. of ACL.
A. Pauls and D. Klein. 2009a. Hierarchical search for
parsing. In Proc. of HLT-NAACL, pages 557–565,
June.
A. Pauls and D. Klein. 2009b. k-best A* Parsing. In
Proc. of ACL-IJCNLP, ACL-IJCNLP ’09, pages 958–
966.
B. Roark and K. Hollingshead. 2009. Linear complexity
context-free parsing pipelines via chart constraints. In
Proc. of HLT-NAACL.
M. Steedman. 2000. The syntactic process. MIT Press,
Cambridge, MA.
Y. Zhang, B.-G. Ahn, S. Clark, C. V. Wyk, J. R. Cur-
ran, and L. Rimell. 2010. Chart pruning for fast
lexicalised-grammar parsing. In Proc. of COLING.
</reference>
<page confidence="0.992372">
1585
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.397982">
<title confidence="0.999952">Efficient CCG Parsing: A* versus Adaptive Supertagging</title>
<author confidence="0.999959">Michael Auli Adam Lopez</author>
<affiliation confidence="0.999971">School of Informatics HLTCOE University of Edinburgh Johns Hopkins University</affiliation>
<email confidence="0.995507">m.auli@sms.ed.ac.ukalopez@cs.jhu.edu</email>
<abstract confidence="0.990659363636364">We present a systematic comparison and combination of two orthogonal techniques for efficient parsing of Combinatory Categorial Grammar (CCG). First we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser’s search space using a separate sequence model. Next we consider several variants on A*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG. In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining</abstract>
<note confidence="0.6258035">A* with adaptive supertagging decreases CPU time by 15% for our best model.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>A K Joshi</author>
</authors>
<title>Supertagging: An Approach to Almost Parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="2168" citStr="Bangalore and Joshi, 1999" startWordPosition="333" endWordPosition="336">ategories is generally much larger, leading to increased grammar constants. Where a typical Penn Treebank grammar may have fewer than 100 nonterminals (Hockenmaier and Steedman, 2002), we found that a CCG grammar derived from CCGbank contained nearly 1600. The same grammar assigns an average of 26 lexical categories per word, resulting in a very large space of possible derivations. The most successful strategy to date for efficient parsing of CCG is to first prune the set of lexical categories considered for each word, using the output of a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). Variations on this approach drive the widelyused, broad coverage C&amp;C parser (Clark and Curran, 2004; Clark and Curran, 2007). However, pruning means approximate search: if a lexical category used by the highest probability derivation is pruned, the parser will not find that derivation (§2). Since the supertagger enforces no grammaticality constraints it may even prefer a sequence of lexical categories that cannot be combined into any derivation (Figure 1). Empirically, we show that supertagging improves efficiency by an order of magnitude, but the tradeoff is a significant loss</context>
<context position="5558" citStr="Bangalore and Joshi, 1999" startWordPosition="874" endWordPosition="877">00; Fowler and Penn, 2010). In the first derivation below, (S\NP)/NP and NP combine to form the spanning category S\NP, which only requires an NP to its left to form a complete sentence-spanning S. The second derivation uses type-raising to change the category type of I. NP (S\NP)/NP NP &gt; S/(S\NP) &gt;� NP (S\NP)/NP NP S S/NP S Because of the number of lexical categories and their complexity, a key difficulty in parsing CCG is that the number of analyses for each span of the sentence quickly becomes extremely large, even with efficient dynamic programming. 2.1 Adaptive Supertagging Supertagging (Bangalore and Joshi, 1999) treats the assignment of lexical categories (or supertags) as a sequence tagging problem. Once the supertagger has been run, lexical categories that apply to each word in the input sentence are pruned to contain only those with high posterior probability (or other figure of merit) under the supertagging model (Clark and Curran, 2004). The posterior probabilities are then discarded; it is the extensive pruning of lexical categories that leads to substantially faster parsing times. Pruning the categories in advance this way has a specific failure mode: sometimes it is not possible to produce a </context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>S. Bangalore and A. K. Joshi. 1999. Supertagging: An Approach to Almost Parsing. Computational Linguistics, 25(2):238–265, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Brodal</author>
</authors>
<title>Worst-case efficient priority queues.</title>
<date>1996</date>
<booktitle>In Proc. of SODA,</booktitle>
<pages>52--58</pages>
<contexts>
<context position="24781" citStr="Brodal, 1996" startWordPosition="4159" endWordPosition="4160">ntations because the savings in edges processed by an agenda parser come at a cost: operations on the priority queue data structure can add significant runtime. Timing experiments of this type are very implementation-dependent, so we took care to implement the algorithms as cleanly as possible and to reuse as much of the existing parser code as we could. An important implementation decision for agenda-based algorithms is the data structure used to implement the priority queue. Preliminary experiments showed that a Fibonacci heap implementation outperformed several alternatives: Brodal queues (Brodal, 1996), binary heaps, binomial heaps, and pairing heaps.10 We carried out timing experiments on the best A* parsers for each model (SX and NULL for PCFG and HWDep, respectively), comparing them with our CKY implementation and the agenda-based CKY simulation EXH; we used the same data as in §3.2. Table 8 presents the cumulative running times with and without adaptive supertagging average over ten runs, while Table 9 reports F-scores. The results (Table 8) are striking. Although the timing results of the agenda-based parsers track the hardware-independent metrics, they start at a significant disadvant</context>
</contexts>
<marker>Brodal, 1996</marker>
<rawString>G. S. Brodal. 1996. Worst-case efficient priority queues. In Proc. of SODA, pages 52–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="2283" citStr="Clark and Curran, 2004" startWordPosition="351" endWordPosition="355">have fewer than 100 nonterminals (Hockenmaier and Steedman, 2002), we found that a CCG grammar derived from CCGbank contained nearly 1600. The same grammar assigns an average of 26 lexical categories per word, resulting in a very large space of possible derivations. The most successful strategy to date for efficient parsing of CCG is to first prune the set of lexical categories considered for each word, using the output of a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). Variations on this approach drive the widelyused, broad coverage C&amp;C parser (Clark and Curran, 2004; Clark and Curran, 2007). However, pruning means approximate search: if a lexical category used by the highest probability derivation is pruned, the parser will not find that derivation (§2). Since the supertagger enforces no grammaticality constraints it may even prefer a sequence of lexical categories that cannot be combined into any derivation (Figure 1). Empirically, we show that supertagging improves efficiency by an order of magnitude, but the tradeoff is a significant loss in accuracy (§3). Can we improve on this tradeoff? The line of investigation we pursue in this paper is to conside</context>
<context position="5894" citStr="Clark and Curran, 2004" startWordPosition="927" endWordPosition="930">he number of lexical categories and their complexity, a key difficulty in parsing CCG is that the number of analyses for each span of the sentence quickly becomes extremely large, even with efficient dynamic programming. 2.1 Adaptive Supertagging Supertagging (Bangalore and Joshi, 1999) treats the assignment of lexical categories (or supertags) as a sequence tagging problem. Once the supertagger has been run, lexical categories that apply to each word in the input sentence are pruned to contain only those with high posterior probability (or other figure of merit) under the supertagging model (Clark and Curran, 2004). The posterior probabilities are then discarded; it is the extensive pruning of lexical categories that leads to substantially faster parsing times. Pruning the categories in advance this way has a specific failure mode: sometimes it is not possible to produce a sentence-spanning derivation from the tag sequences preferred by the supertagger, since it does not enforce grammaticality. A workaround for this problem is the adaptive supertagging (AST) approach of Clark and Curran (2004). It is based on a step function over supertagger beam ratios, relaxing the pruning threshold for lexical catego</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>S. Clark and J. R. Curran. 2004. The importance of supertagging for wide-coverage CCG parsing. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>Wide-coverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="2308" citStr="Clark and Curran, 2007" startWordPosition="356" endWordPosition="359">erminals (Hockenmaier and Steedman, 2002), we found that a CCG grammar derived from CCGbank contained nearly 1600. The same grammar assigns an average of 26 lexical categories per word, resulting in a very large space of possible derivations. The most successful strategy to date for efficient parsing of CCG is to first prune the set of lexical categories considered for each word, using the output of a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). Variations on this approach drive the widelyused, broad coverage C&amp;C parser (Clark and Curran, 2004; Clark and Curran, 2007). However, pruning means approximate search: if a lexical category used by the highest probability derivation is pruned, the parser will not find that derivation (§2). Since the supertagger enforces no grammaticality constraints it may even prefer a sequence of lexical categories that cannot be combined into any derivation (Figure 1). Empirically, we show that supertagging improves efficiency by an order of magnitude, but the tradeoff is a significant loss in accuracy (§3). Can we improve on this tradeoff? The line of investigation we pursue in this paper is to consider more efficient exact al</context>
<context position="8851" citStr="Clark and Curran (2007)" startWordPosition="1401" endWordPosition="1404">er underestimates the true outside probability (i.e. it is admissible) the solution is guaranteed to be exact. Heuristics are model specific and we consider several variants in our experiments based on the CFG heuristics developed by Klein and Manning (2003) and Pauls and Klein (2009a). 3 Adaptive Supertagging Experiments Parser. For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). Generative parsers have the property that all edge weights are non-negative, which is required for A* techniques.1 Although not quite as accurate as the discriminative parser of Clark and Curran (2007) in our preliminary experiments, this parser is still quite competitive. It is written in Java and implements the CKY algorithm with a global pruning threshold of 10−4 for the models we use. We focus on two parsing models: PCFG, the baseline of Hockenmaier and Steedman (2002) which treats the grammar as a PCFG (Table 1); and HWDep, a headword dependency model which is the best performing model of the parser. The PCFG model simply generates a tree top down and uses very simple structural probabilities while the HWDep model conditions node expansions on headwords and their lexical categories. Su</context>
<context position="10860" citStr="Clark and Curran, 2007" startWordPosition="1727" endWordPosition="1730">ntences and thus penalise lower coverage. All timing experiments reported in the paper were run on a 2.5 GHz Xeon machine with 32 GB memory and are averaged over ten runs4. 3.1 Results Supertagging has been shown to improve the speed of a generative parser, although little analysis has been reported beyond the speedups (Clark, 2002) We ran experiments to understand the time/accuracy tradeoff of adaptive supertagging, and to serve as baselines. Adaptive supertagging is parametrized by a beam size Q and a dictionary cutoff k that bounds the number of lexical categories considered for each word (Clark and Curran, 2007). Table 3 shows both the standard beam levels (AST) used for the C&amp;C parser and looser beam levels: AST-covA, a simple extension of AST with increased coverage and AST-covB, also increasing coverage but with better performance for the HWDep model. Parsing results for the AST settings (Tables 4 and 5) confirm that it improves speed by an order of magnitude over a baseline parser without AST. Perhaps surprisingly, the number of parse failures decreases with AST in some cases. This is because the parser prunes more aggressively as the search space increases.5 2http://code.google.com/p/statopenccg</context>
<context position="14183" citStr="Clark and Curran (2007)" startWordPosition="2285" endWordPosition="2288">ng adaptive supertagging (AST) to two models of a generative CCG parser. Performance is measured in terms of parse failures, labelled and unlabelled precision (LP/UP), recall (LR/UR) and F-score (LF/UF). Evaluation is based only on sentences for which each parser returned an analysis. 3.2 Efficiency versus Accuracy The most interesting result is the effect of the speedup on accuracy. As shown in Table 6, the vast majority of sentences are actually parsed with a very tight supertagger beam, raising the question of whether many higher-scoring parses are pruned.6 6Similar results are reported by Clark and Curran (2007). Despite this, labeled F-score improves by up to 1.6 F-measure for the PCFG model, although it harms accuracy for HWDep as expected. In order to understand this effect, we filtered section 00 to include only sentences of between 18 and 26 words (resulting in 610 sentences) for which 1580 Time(sec) Sent/sec Cats/word Fail UP UR UF LP LR LF PCFG 326 7.4 25.7 29 85.9 85.4 85.7 76.6 76.2 76.4 PCFG (AST) 82 29.4 1.5 34 86.7 84.8 85.7 78.6 76.9 77.7 PCFG (AST-covA) 85 28.3 1.6 15 86.6 85.5 86.0 78.5 77.5 78.0 PCFG (AST-covB) 86 27.9 1.7 14 86.6 85.6 86.1 78.1 77.3 77.7 HWDep 1754 1.4 25.7 30 90.2 8</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>S. Clark and J. R. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Hockenmaier</author>
</authors>
<title>Evaluating a widecoverage CCG parser.</title>
<date>2002</date>
<booktitle>In Proc. of LREC Beyond Parseval Workshop,</booktitle>
<pages>60--66</pages>
<contexts>
<context position="10214" citStr="Clark and Hockenmaier, 2002" startWordPosition="1620" endWordPosition="1623"> we are aware of uses generative parsers (Pauls and Klein, 2009b, inter alia). (2002).2 Due to differences in smoothing of the supertagging and parsing models, we occasionally drop supertags returned by the supertagger because they do not appear in the parsing model 3. Evaluation. All experiments were conducted on CCGBank (Hockenmaier and Steedman, 2007), a right-most normal-form CCG version of the Penn Treebank. Models were trained on sections 2-21, tuned on section 00, and tested on section 23. Parsing accuracy is measured using labelled and unlabelled predicate argument structure recovery (Clark and Hockenmaier, 2002); we evaluate on all sentences and thus penalise lower coverage. All timing experiments reported in the paper were run on a 2.5 GHz Xeon machine with 32 GB memory and are averaged over ten runs4. 3.1 Results Supertagging has been shown to improve the speed of a generative parser, although little analysis has been reported beyond the speedups (Clark, 2002) We ran experiments to understand the time/accuracy tradeoff of adaptive supertagging, and to serve as baselines. Adaptive supertagging is parametrized by a beam size Q and a dictionary cutoff k that bounds the number of lexical categories con</context>
</contexts>
<marker>Clark, Hockenmaier, 2002</marker>
<rawString>S. Clark and J. Hockenmaier. 2002. Evaluating a widecoverage CCG parser. In Proc. of LREC Beyond Parseval Workshop, pages 60–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
</authors>
<title>Supertagging for Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In Proc. of TAG+6,</booktitle>
<pages>19--24</pages>
<contexts>
<context position="2182" citStr="Clark, 2002" startWordPosition="337" endWordPosition="338"> larger, leading to increased grammar constants. Where a typical Penn Treebank grammar may have fewer than 100 nonterminals (Hockenmaier and Steedman, 2002), we found that a CCG grammar derived from CCGbank contained nearly 1600. The same grammar assigns an average of 26 lexical categories per word, resulting in a very large space of possible derivations. The most successful strategy to date for efficient parsing of CCG is to first prune the set of lexical categories considered for each word, using the output of a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). Variations on this approach drive the widelyused, broad coverage C&amp;C parser (Clark and Curran, 2004; Clark and Curran, 2007). However, pruning means approximate search: if a lexical category used by the highest probability derivation is pruned, the parser will not find that derivation (§2). Since the supertagger enforces no grammaticality constraints it may even prefer a sequence of lexical categories that cannot be combined into any derivation (Figure 1). Empirically, we show that supertagging improves efficiency by an order of magnitude, but the tradeoff is a significant loss in accuracy (</context>
<context position="10571" citStr="Clark, 2002" startWordPosition="1684" endWordPosition="1685">rmal-form CCG version of the Penn Treebank. Models were trained on sections 2-21, tuned on section 00, and tested on section 23. Parsing accuracy is measured using labelled and unlabelled predicate argument structure recovery (Clark and Hockenmaier, 2002); we evaluate on all sentences and thus penalise lower coverage. All timing experiments reported in the paper were run on a 2.5 GHz Xeon machine with 32 GB memory and are averaged over ten runs4. 3.1 Results Supertagging has been shown to improve the speed of a generative parser, although little analysis has been reported beyond the speedups (Clark, 2002) We ran experiments to understand the time/accuracy tradeoff of adaptive supertagging, and to serve as baselines. Adaptive supertagging is parametrized by a beam size Q and a dictionary cutoff k that bounds the number of lexical categories considered for each word (Clark and Curran, 2007). Table 3 shows both the standard beam levels (AST) used for the C&amp;C parser and looser beam levels: AST-covA, a simple extension of AST with increased coverage and AST-covB, also increasing coverage but with better performance for the HWDep model. Parsing results for the AST settings (Tables 4 and 5) confirm t</context>
</contexts>
<marker>Clark, 2002</marker>
<rawString>S. Clark. 2002. Supertagging for Combinatory Categorial Grammar. In Proc. of TAG+6, pages 19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Djordjevic</author>
<author>J R Curran</author>
<author>S Clark</author>
</authors>
<title>Improving the efficiency of a wide-coverage CCG parser.</title>
<date>2007</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="28751" citStr="Djordjevic et al. (2007)" startWordPosition="4830" endWordPosition="4833">for CFG, and that agenda-based algorithms like A* must make very large reductions in the number of edges processed to result in realtime savings, due to the added expense of keeping a priority queue. However, we 1584 have shown that A* can yield real improvements even over the highly optimized technique of adaptive supertagging: in this pruned search space, a 44% reduction in the number of edges pushed results in a 15% speedup in CPU time. Furthermore, just as A* can be combined with adaptive supertagging, it should also combine easily with other search-space pruning methods, such as those of Djordjevic et al. (2007), Kummerfeld et al. (2010), Zhang et al. (2010) and Roark and Hollingshead (2009). In future work we plan to examine better A* heuristics for CCG, and to look at principled approaches to combine the strengths of A*, adaptive supertagging, and other techniques to the best advantage. Acknowledgements We would like to thank Prachya Boonkwan, Juri Ganitkevitch, Philipp Koehn, Tom Kwiatkowski, Matt Post, Mark Steedman, Emily Thomforde, and Luke Zettlemoyer for helpful discussion related to this work and comments on previous drafts; Julia Hockenmaier for furnishing us with her parser; and the anonym</context>
</contexts>
<marker>Djordjevic, Curran, Clark, 2007</marker>
<rawString>B. Djordjevic, J. R. Curran, and S. Clark. 2007. Improving the efficiency of a wide-coverage CCG parser. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Felzenszwalb</author>
<author>D McAllester</author>
</authors>
<title>The Generalized A* Architecture.</title>
<date>2007</date>
<journal>In Journal of Artificial Intelligence Research,</journal>
<volume>29</volume>
<pages>153--190</pages>
<contexts>
<context position="17980" citStr="Felzenszwalb and McAllester, 2007" startWordPosition="2977" endWordPosition="2980">veo perationofthe priority qu eue,res pec tive ly. Fi nally,wemea sure t henum- ber oftrav er sals ,which c ountsthe n umberof edg ew eightsc omp uted,r eg ardlessof whe th ert hewei ghtisdisc ar d edduet othepri orex istenc eofabe tterw ei gh t.T hisl att ermetr ic seems to bet hemo stac -c ura teaccou nto ft heworkdone b yt hep arse r.D uetod iffe renc es inthePCFG a ndHWDepmo d- ddifferent A* variants: for els, the weco n sid ere se a simple A* with aPCFGprecm-mode 1581 puted heuristic, while for the the more complex HWDep model, we used a hierarchical A* algorithm (Pauls and Klein, 2009a; Felzenszwalb and McAllester, 2007) based on a simple grammar projection that we designed. 4.1 Hardware-Independent Results: PCFG For the PCFG model, we compared three agendabased parsers: EXH prioritizes edges by their span length, thereby simulating the exhaustive CKY algorithm; NULL prioritizes edges by their inside probability; and SX is an A* parser that prioritizes edges by their inside probability times an admissible outside probability estimate.8 We use the SX estimate devised by Klein and Manning (2003) for CFG parsing, where they found it offered very good performance for relatively little computation. It gives a boun</context>
<context position="22434" citStr="Felzenszwalb and McAllester (2007)" startWordPosition="3769" endWordPosition="3772">EXH 34 100 6.3 100 15.7 100 4.2 100 133.4 100 13.3 100 NULL 24.3 71 4.9 78 13.5 86 3.5 83 113.8 85 11.1 83 SX 20.9 61 4.3 68 10.0 64 2.6 62 96.5 72 9.7 73 Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging. grammar, use these to compute Viterbi outside probabilities for the simple grammar, and then use these as outside estimates for the true grammar; all computations are prioritized in a single agenda following the algorithm of Felzenszwalb and McAllester (2007) and Pauls and Klein (2009a). We designed two simple grammar projections, each simplifying the HWDep model: PCFGProj completely removes lexicalization and projects the grammar to a PCFG, while as LexcatProj removes only the headwords but retains the lexical categories. Figure 4 compares exhaustive search, A* with no heuristic (NULL), and HA*. For HA*, parsing effort is broken down into the different edge types computed at each stage: We distinguish between the work carried out to compute the inside and outside edges of the projection, where the latter represent the heuristic estimates, and fin</context>
</contexts>
<marker>Felzenszwalb, McAllester, 2007</marker>
<rawString>P. F. Felzenszwalb and D. McAllester. 2007. The Generalized A* Architecture. In Journal of Artificial Intelligence Research, volume 29, pages 153–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T A D Fowler</author>
<author>G Penn</author>
</authors>
<title>Accurate contextfree parsing with combinatory categorial grammar.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1428" citStr="Fowler and Penn, 2010" startWordPosition="215" endWordPosition="218"> first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model. 1 Introduction Efficient parsing of Combinatorial Categorial Grammar (CCG; Steedman, 2000) is a longstanding problem in computational linguistics. Even with practical CCG that are strongly context-free (Fowler and Penn, 2010), parsing can be much harder than with Penn Treebank-style context-free grammars, since the number of nonterminal categories is generally much larger, leading to increased grammar constants. Where a typical Penn Treebank grammar may have fewer than 100 nonterminals (Hockenmaier and Steedman, 2002), we found that a CCG grammar derived from CCGbank contained nearly 1600. The same grammar assigns an average of 26 lexical categories per word, resulting in a very large space of possible derivations. The most successful strategy to date for efficient parsing of CCG is to first prune the set of lexic</context>
<context position="4958" citStr="Fowler and Penn, 2010" startWordPosition="775" endWordPosition="778"> and Parsing Algorithms CCG is a lexicalized grammar formalism encoding for each word lexical categories which are either basic (eg. NN, JJ) or complex. Complex lexical categories specify the number and directionality of arguments. For example, one lexical category (of over 100 in our model) for the transitive verb like is (S\NP2)/NP1, specifying the first argument as an NP to the right and the second as an NP to the left. In parsing, adjacent spans are combined using a small number of binary combinatory rules like forward application or composition on the spanning categories (Steedman, 2000; Fowler and Penn, 2010). In the first derivation below, (S\NP)/NP and NP combine to form the spanning category S\NP, which only requires an NP to its left to form a complete sentence-spanning S. The second derivation uses type-raising to change the category type of I. NP (S\NP)/NP NP &gt; S/(S\NP) &gt;� NP (S\NP)/NP NP S S/NP S Because of the number of lexical categories and their complexity, a key difficulty in parsing CCG is that the number of analyses for each span of the sentence quickly becomes extremely large, even with efficient dynamic programming. 2.1 Adaptive Supertagging Supertagging (Bangalore and Joshi, 1999)</context>
</contexts>
<marker>Fowler, Penn, 2010</marker>
<rawString>T. A. D. Fowler and G. Penn. 2010. Accurate contextfree parsing with combinatory categorial grammar. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hart</author>
<author>N Nilsson</author>
<author>B Raphael</author>
</authors>
<title>A formal basis for the heuristic determination of minimum cost paths.</title>
<date>1968</date>
<booktitle>Transactions on Systems Science and Cybernetics,</booktitle>
<volume>4</volume>
<contexts>
<context position="3009" citStr="Hart et al., 1968" startWordPosition="469" endWordPosition="472">est probability derivation is pruned, the parser will not find that derivation (§2). Since the supertagger enforces no grammaticality constraints it may even prefer a sequence of lexical categories that cannot be combined into any derivation (Figure 1). Empirically, we show that supertagging improves efficiency by an order of magnitude, but the tradeoff is a significant loss in accuracy (§3). Can we improve on this tradeoff? The line of investigation we pursue in this paper is to consider more efficient exact algorithms. In particular, we test different variants of the classical A* algorithm (Hart et al., 1968), which has met with success in Penn Treebank parsing with context-free grammars (Klein and Manning, 2003; Pauls and Klein, 2009a; Pauls and Klein, 2009b). We can substitute A* for standard CKY on either the unpruned set of lexical categories, or the pruned set resulting from su1577 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1577–1585, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics I like tea I like tea Figure 1: The relationship between supertagger and parser search spaces based on the intersection of th</context>
</contexts>
<marker>Hart, Nilsson, Raphael, 1968</marker>
<rawString>P. Hart, N. Nilsson, and B. Raphael. 1968. A formal basis for the heuristic determination of minimum cost paths. Transactions on Systems Science and Cybernetics, 4, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hockenmaier</author>
<author>M Steedman</author>
</authors>
<title>Generative models for statistical parsing with Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>335--342</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1726" citStr="Hockenmaier and Steedman, 2002" startWordPosition="258" endWordPosition="262"> over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model. 1 Introduction Efficient parsing of Combinatorial Categorial Grammar (CCG; Steedman, 2000) is a longstanding problem in computational linguistics. Even with practical CCG that are strongly context-free (Fowler and Penn, 2010), parsing can be much harder than with Penn Treebank-style context-free grammars, since the number of nonterminal categories is generally much larger, leading to increased grammar constants. Where a typical Penn Treebank grammar may have fewer than 100 nonterminals (Hockenmaier and Steedman, 2002), we found that a CCG grammar derived from CCGbank contained nearly 1600. The same grammar assigns an average of 26 lexical categories per word, resulting in a very large space of possible derivations. The most successful strategy to date for efficient parsing of CCG is to first prune the set of lexical categories considered for each word, using the output of a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). Variations on this approach drive the widelyused, broad coverage C&amp;C parser (Clark and Curran, 2004; Clark and Curran, 2007). However, pruning</context>
<context position="8648" citStr="Hockenmaier and Steedman (2002)" startWordPosition="1370" endWordPosition="1373">ontrol loop. Instead, they are processed from a priority queue, which orders them by the product of their inside probability and a heuristic estimate of their outside probability. Provided that the heuristic never underestimates the true outside probability (i.e. it is admissible) the solution is guaranteed to be exact. Heuristics are model specific and we consider several variants in our experiments based on the CFG heuristics developed by Klein and Manning (2003) and Pauls and Klein (2009a). 3 Adaptive Supertagging Experiments Parser. For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). Generative parsers have the property that all edge weights are non-negative, which is required for A* techniques.1 Although not quite as accurate as the discriminative parser of Clark and Curran (2007) in our preliminary experiments, this parser is still quite competitive. It is written in Java and implements the CKY algorithm with a global pruning threshold of 10−4 for the models we use. We focus on two parsing models: PCFG, the baseline of Hockenmaier and Steedman (2002) which treats the grammar as a PCFG (Table 1); and HWDep, a headword dependency model which is the best performing model </context>
<context position="11634" citStr="Hockenmaier and Steedman (2002)" startWordPosition="1852" endWordPosition="1855">creased coverage and AST-covB, also increasing coverage but with better performance for the HWDep model. Parsing results for the AST settings (Tables 4 and 5) confirm that it improves speed by an order of magnitude over a baseline parser without AST. Perhaps surprisingly, the number of parse failures decreases with AST in some cases. This is because the parser prunes more aggressively as the search space increases.5 2http://code.google.com/p/statopenccg 3Less than 2% of supertags are affected by this. 4The timing results reported differ from an earlier draft since we used a different machine 5Hockenmaier and Steedman (2002) saw a similar effect. 1579 Expansion probability p(exp|P) exp E {leaf, unary, left-head, right-head} Head probability p(H|P, exp) H is the head daughter Non-head probability p(S|P, exp, H) S is the non-head daughter Lexical probability p(w|P) Table 1: Factorisation of the PCFG model. H,P, and S are categories, and w is a word. Expansion probability p(exp|P, cP#wP) exp E {leaf, unary, left-head, right-head} Head probability p(H|P, exp, cP#wP) H is the head daughter Non-head probability p(S|P, exp, H#cP#wP) S is the non-head daughter Lexcat probability p(cS|S#P,H,S) p(cTOP|P=TOP) Headword proba</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>J. Hockenmaier and M. Steedman. 2002. Generative models for statistical parsing with Combinatory Categorial Grammar. In Proc. of ACL, pages 335–342. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hockenmaier</author>
<author>M Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="9942" citStr="Hockenmaier and Steedman, 2007" startWordPosition="1578" endWordPosition="1581">and uses very simple structural probabilities while the HWDep model conditions node expansions on headwords and their lexical categories. Supertagger. For supertagging we used Dennis Mehay’s implementation, which follows Clark 1Indeed, all of the past work on A* parsing that we are aware of uses generative parsers (Pauls and Klein, 2009b, inter alia). (2002).2 Due to differences in smoothing of the supertagging and parsing models, we occasionally drop supertags returned by the supertagger because they do not appear in the parsing model 3. Evaluation. All experiments were conducted on CCGBank (Hockenmaier and Steedman, 2007), a right-most normal-form CCG version of the Penn Treebank. Models were trained on sections 2-21, tuned on section 00, and tested on section 23. Parsing accuracy is measured using labelled and unlabelled predicate argument structure recovery (Clark and Hockenmaier, 2002); we evaluate on all sentences and thus penalise lower coverage. All timing experiments reported in the paper were run on a 2.5 GHz Xeon machine with 32 GB memory and are averaged over ten runs4. 3.1 Results Supertagging has been shown to improve the speed of a generative parser, although little analysis has been reported beyo</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>J. Hockenmaier and M. Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Parsing and hypergraphs.</title>
<date>2001</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="19970" citStr="Klein and Manning, 2001" startWordPosition="3331" endWordPosition="3334"> shows that A* can improve search even on the highly pruned search graph. Note that A* only saves work in the final iteration of AST, since for earlier iterations it must process the entire agenda to determine that there is no spanning analysis. Since there are many more categories in the CCG grammar we might have expected the SX heuristic to work better than for a CFG. Why doesn’t it? We can measure how well a heuristic bounds the true cost in 8The NULL parser is a special case of A*, also called uniform cost search, which in the case of parsing corresponds to Knuth’s algorithm (Knuth, 1977; Klein and Manning, 2001), the extension of Dijkstra’s algorithm to hypergraphs. Outside Span Figure 3: Average slack of the SX heuristic. The figure aggregates the ratio of the difference between the estimated outside cost and true outside costs relative to the true cost across the development set. terms of slack: the difference between the true and estimated outside cost. Lower slack means that the heuristic bounds the true cost better and guides us to the exact solution more quickly. Figure 3 plots the average slack for the SX heuristic against the number of words in the outside context. Comparing this with an anal</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>D. Klein and C. D. Manning. 2001. Parsing and hypergraphs. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>A* parsing: Fast exact Viterbi parse selection.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>119--126</pages>
<contexts>
<context position="3114" citStr="Klein and Manning, 2003" startWordPosition="485" endWordPosition="488">tagger enforces no grammaticality constraints it may even prefer a sequence of lexical categories that cannot be combined into any derivation (Figure 1). Empirically, we show that supertagging improves efficiency by an order of magnitude, but the tradeoff is a significant loss in accuracy (§3). Can we improve on this tradeoff? The line of investigation we pursue in this paper is to consider more efficient exact algorithms. In particular, we test different variants of the classical A* algorithm (Hart et al., 1968), which has met with success in Penn Treebank parsing with context-free grammars (Klein and Manning, 2003; Pauls and Klein, 2009a; Pauls and Klein, 2009b). We can substitute A* for standard CKY on either the unpruned set of lexical categories, or the pruned set resulting from su1577 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1577–1585, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics I like tea I like tea Figure 1: The relationship between supertagger and parser search spaces based on the intersection of their corresponding tag sequences. pertagging. Our empirical results show that on the unpruned set of lexic</context>
<context position="7934" citStr="Klein and Manning, 2003" startWordPosition="1257" endWordPosition="1260">es Desirable parses High scoring supertags Valid parses High scoring parses Valid supertag-sequences S\NP � &gt;e &gt; 1578 ant of the CKY algorithm. Although CKY is easy to implement, it is exhaustive: it explores all possible analyses of all possible spans, irrespective of whether such analyses are likely to be part of the highest probability derivation. Hence it seems natural to consider exact algorithms that are more efficient than CKY. A* search is an agenda-based best-first graph search algorithm which finds the lowest cost parse exactly without necessarily traversing the entire search space (Klein and Manning, 2003). In contrast to CKY, items are not processed in topological order using a simple control loop. Instead, they are processed from a priority queue, which orders them by the product of their inside probability and a heuristic estimate of their outside probability. Provided that the heuristic never underestimates the true outside probability (i.e. it is admissible) the solution is guaranteed to be exact. Heuristics are model specific and we consider several variants in our experiments based on the CFG heuristics developed by Klein and Manning (2003) and Pauls and Klein (2009a). 3 Adaptive Superta</context>
<context position="18462" citStr="Klein and Manning (2003)" startWordPosition="3054" endWordPosition="3057">c, while for the the more complex HWDep model, we used a hierarchical A* algorithm (Pauls and Klein, 2009a; Felzenszwalb and McAllester, 2007) based on a simple grammar projection that we designed. 4.1 Hardware-Independent Results: PCFG For the PCFG model, we compared three agendabased parsers: EXH prioritizes edges by their span length, thereby simulating the exhaustive CKY algorithm; NULL prioritizes edges by their inside probability; and SX is an A* parser that prioritizes edges by their inside probability times an admissible outside probability estimate.8 We use the SX estimate devised by Klein and Manning (2003) for CFG parsing, where they found it offered very good performance for relatively little computation. It gives a bound on the outside probability of a nonterminal P with i words to the right and j words to the left, and can be computed from a grammar using a simple dynamic program. The parsers are tested with and without adaptive supertagging where the former can be seen as performing exact search (via A*) over the pruned search space created by AST. Table 7 shows that A* with the SX heuristic decreases the number of edges pushed by up to 39% on the unpruned search space. Although encouraging</context>
<context position="20646" citStr="Klein and Manning (2003)" startWordPosition="3446" endWordPosition="3449">hs. Outside Span Figure 3: Average slack of the SX heuristic. The figure aggregates the ratio of the difference between the estimated outside cost and true outside costs relative to the true cost across the development set. terms of slack: the difference between the true and estimated outside cost. Lower slack means that the heuristic bounds the true cost better and guides us to the exact solution more quickly. Figure 3 plots the average slack for the SX heuristic against the number of words in the outside context. Comparing this with an analysis of the same heuristic when applied to a CFG by Klein and Manning (2003), we find that it is less effective in our setting9. There is a steep increase in slack for outside contexts with size more than one. The main reason for this is because a single word in the outside context is in many cases the full stop at the end of the sentence, which is very predictable. However for longer spans the flexibility of CCG to analyze spans in many different ways means that the outside estimate for a nonterminal can be based on many high probability outside derivations which do not bound the true probability very well. 4.2 Hardware-Independent Results: HWDep Lexicalization in th</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. A* parsing: Fast exact Viterbi parse selection. In Proc. of HLT-NAACL, pages 119–126, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>A generalization of Dijkstra’s algorithm.</title>
<date>1977</date>
<journal>Information Processing Letters,</journal>
<pages>6--1</pages>
<contexts>
<context position="19944" citStr="Knuth, 1977" startWordPosition="3329" endWordPosition="3330">setting which shows that A* can improve search even on the highly pruned search graph. Note that A* only saves work in the final iteration of AST, since for earlier iterations it must process the entire agenda to determine that there is no spanning analysis. Since there are many more categories in the CCG grammar we might have expected the SX heuristic to work better than for a CFG. Why doesn’t it? We can measure how well a heuristic bounds the true cost in 8The NULL parser is a special case of A*, also called uniform cost search, which in the case of parsing corresponds to Knuth’s algorithm (Knuth, 1977; Klein and Manning, 2001), the extension of Dijkstra’s algorithm to hypergraphs. Outside Span Figure 3: Average slack of the SX heuristic. The figure aggregates the ratio of the difference between the estimated outside cost and true outside costs relative to the true cost across the development set. terms of slack: the difference between the true and estimated outside cost. Lower slack means that the heuristic bounds the true cost better and guides us to the exact solution more quickly. Figure 3 plots the average slack for the SX heuristic against the number of words in the outside context. C</context>
</contexts>
<marker>Knuth, 1977</marker>
<rawString>D. E. Knuth. 1977. A generalization of Dijkstra’s algorithm. Information Processing Letters, 6:1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Kummerfeld</author>
<author>J Roesner</author>
<author>T Dawborn</author>
<author>J Haggerty</author>
<author>J R Curran</author>
<author>S Clark</author>
</authors>
<title>Faster parsing by supertagger adaptation.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="28777" citStr="Kummerfeld et al. (2010)" startWordPosition="4834" endWordPosition="4837">ased algorithms like A* must make very large reductions in the number of edges processed to result in realtime savings, due to the added expense of keeping a priority queue. However, we 1584 have shown that A* can yield real improvements even over the highly optimized technique of adaptive supertagging: in this pruned search space, a 44% reduction in the number of edges pushed results in a 15% speedup in CPU time. Furthermore, just as A* can be combined with adaptive supertagging, it should also combine easily with other search-space pruning methods, such as those of Djordjevic et al. (2007), Kummerfeld et al. (2010), Zhang et al. (2010) and Roark and Hollingshead (2009). In future work we plan to examine better A* heuristics for CCG, and to look at principled approaches to combine the strengths of A*, adaptive supertagging, and other techniques to the best advantage. Acknowledgements We would like to thank Prachya Boonkwan, Juri Ganitkevitch, Philipp Koehn, Tom Kwiatkowski, Matt Post, Mark Steedman, Emily Thomforde, and Luke Zettlemoyer for helpful discussion related to this work and comments on previous drafts; Julia Hockenmaier for furnishing us with her parser; and the anonymous reviewers for helpful </context>
</contexts>
<marker>Kummerfeld, Roesner, Dawborn, Haggerty, Curran, Clark, 2010</marker>
<rawString>J. K. Kummerfeld, J. Roesner, T. Dawborn, J. Haggerty, J. R. Curran, and S. Clark. 2010. Faster parsing by supertagger adaptation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pauls</author>
<author>D Klein</author>
</authors>
<title>Hierarchical search for parsing.</title>
<date>2009</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>557--565</pages>
<contexts>
<context position="3137" citStr="Pauls and Klein, 2009" startWordPosition="489" endWordPosition="492">ticality constraints it may even prefer a sequence of lexical categories that cannot be combined into any derivation (Figure 1). Empirically, we show that supertagging improves efficiency by an order of magnitude, but the tradeoff is a significant loss in accuracy (§3). Can we improve on this tradeoff? The line of investigation we pursue in this paper is to consider more efficient exact algorithms. In particular, we test different variants of the classical A* algorithm (Hart et al., 1968), which has met with success in Penn Treebank parsing with context-free grammars (Klein and Manning, 2003; Pauls and Klein, 2009a; Pauls and Klein, 2009b). We can substitute A* for standard CKY on either the unpruned set of lexical categories, or the pruned set resulting from su1577 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1577–1585, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics I like tea I like tea Figure 1: The relationship between supertagger and parser search spaces based on the intersection of their corresponding tag sequences. pertagging. Our empirical results show that on the unpruned set of lexical categories, heuristi</context>
<context position="8512" citStr="Pauls and Klein (2009" startWordPosition="1351" endWordPosition="1354">tire search space (Klein and Manning, 2003). In contrast to CKY, items are not processed in topological order using a simple control loop. Instead, they are processed from a priority queue, which orders them by the product of their inside probability and a heuristic estimate of their outside probability. Provided that the heuristic never underestimates the true outside probability (i.e. it is admissible) the solution is guaranteed to be exact. Heuristics are model specific and we consider several variants in our experiments based on the CFG heuristics developed by Klein and Manning (2003) and Pauls and Klein (2009a). 3 Adaptive Supertagging Experiments Parser. For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). Generative parsers have the property that all edge weights are non-negative, which is required for A* techniques.1 Although not quite as accurate as the discriminative parser of Clark and Curran (2007) in our preliminary experiments, this parser is still quite competitive. It is written in Java and implements the CKY algorithm with a global pruning threshold of 10−4 for the models we use. We focus on two parsing models: PCFG, the baseline of Hockenmaier and </context>
<context position="17943" citStr="Pauls and Klein, 2009" startWordPosition="2973" endWordPosition="2976">keyo pe rat ionsandr emoveo perationofthe priority qu eue,res pec tive ly. Fi nally,wemea sure t henum- ber oftrav er sals ,which c ountsthe n umberof edg ew eightsc omp uted,r eg ardlessof whe th ert hewei ghtisdisc ar d edduet othepri orex istenc eofabe tterw ei gh t.T hisl att ermetr ic seems to bet hemo stac -c ura teaccou nto ft heworkdone b yt hep arse r.D uetod iffe renc es inthePCFG a ndHWDepmo d- ddifferent A* variants: for els, the weco n sid ere se a simple A* with aPCFGprecm-mode 1581 puted heuristic, while for the the more complex HWDep model, we used a hierarchical A* algorithm (Pauls and Klein, 2009a; Felzenszwalb and McAllester, 2007) based on a simple grammar projection that we designed. 4.1 Hardware-Independent Results: PCFG For the PCFG model, we compared three agendabased parsers: EXH prioritizes edges by their span length, thereby simulating the exhaustive CKY algorithm; NULL prioritizes edges by their inside probability; and SX is an A* parser that prioritizes edges by their inside probability times an admissible outside probability estimate.8 We use the SX estimate devised by Klein and Manning (2003) for CFG parsing, where they found it offered very good performance for relativel</context>
<context position="22460" citStr="Pauls and Klein (2009" startWordPosition="3774" endWordPosition="3777">.4 100 13.3 100 NULL 24.3 71 4.9 78 13.5 86 3.5 83 113.8 85 11.1 83 SX 20.9 61 4.3 68 10.0 64 2.6 62 96.5 72 9.7 73 Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging. grammar, use these to compute Viterbi outside probabilities for the simple grammar, and then use these as outside estimates for the true grammar; all computations are prioritized in a single agenda following the algorithm of Felzenszwalb and McAllester (2007) and Pauls and Klein (2009a). We designed two simple grammar projections, each simplifying the HWDep model: PCFGProj completely removes lexicalization and projects the grammar to a PCFG, while as LexcatProj removes only the headwords but retains the lexical categories. Figure 4 compares exhaustive search, A* with no heuristic (NULL), and HA*. For HA*, parsing effort is broken down into the different edge types computed at each stage: We distinguish between the work carried out to compute the inside and outside edges of the projection, where the latter represent the heuristic estimates, and finally, the work to compute </context>
</contexts>
<marker>Pauls, Klein, 2009</marker>
<rawString>A. Pauls and D. Klein. 2009a. Hierarchical search for parsing. In Proc. of HLT-NAACL, pages 557–565, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pauls</author>
<author>D Klein</author>
</authors>
<title>k-best A* Parsing.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP, ACL-IJCNLP ’09,</booktitle>
<pages>958--966</pages>
<contexts>
<context position="3137" citStr="Pauls and Klein, 2009" startWordPosition="489" endWordPosition="492">ticality constraints it may even prefer a sequence of lexical categories that cannot be combined into any derivation (Figure 1). Empirically, we show that supertagging improves efficiency by an order of magnitude, but the tradeoff is a significant loss in accuracy (§3). Can we improve on this tradeoff? The line of investigation we pursue in this paper is to consider more efficient exact algorithms. In particular, we test different variants of the classical A* algorithm (Hart et al., 1968), which has met with success in Penn Treebank parsing with context-free grammars (Klein and Manning, 2003; Pauls and Klein, 2009a; Pauls and Klein, 2009b). We can substitute A* for standard CKY on either the unpruned set of lexical categories, or the pruned set resulting from su1577 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1577–1585, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics I like tea I like tea Figure 1: The relationship between supertagger and parser search spaces based on the intersection of their corresponding tag sequences. pertagging. Our empirical results show that on the unpruned set of lexical categories, heuristi</context>
<context position="8512" citStr="Pauls and Klein (2009" startWordPosition="1351" endWordPosition="1354">tire search space (Klein and Manning, 2003). In contrast to CKY, items are not processed in topological order using a simple control loop. Instead, they are processed from a priority queue, which orders them by the product of their inside probability and a heuristic estimate of their outside probability. Provided that the heuristic never underestimates the true outside probability (i.e. it is admissible) the solution is guaranteed to be exact. Heuristics are model specific and we consider several variants in our experiments based on the CFG heuristics developed by Klein and Manning (2003) and Pauls and Klein (2009a). 3 Adaptive Supertagging Experiments Parser. For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). Generative parsers have the property that all edge weights are non-negative, which is required for A* techniques.1 Although not quite as accurate as the discriminative parser of Clark and Curran (2007) in our preliminary experiments, this parser is still quite competitive. It is written in Java and implements the CKY algorithm with a global pruning threshold of 10−4 for the models we use. We focus on two parsing models: PCFG, the baseline of Hockenmaier and </context>
<context position="17943" citStr="Pauls and Klein, 2009" startWordPosition="2973" endWordPosition="2976">keyo pe rat ionsandr emoveo perationofthe priority qu eue,res pec tive ly. Fi nally,wemea sure t henum- ber oftrav er sals ,which c ountsthe n umberof edg ew eightsc omp uted,r eg ardlessof whe th ert hewei ghtisdisc ar d edduet othepri orex istenc eofabe tterw ei gh t.T hisl att ermetr ic seems to bet hemo stac -c ura teaccou nto ft heworkdone b yt hep arse r.D uetod iffe renc es inthePCFG a ndHWDepmo d- ddifferent A* variants: for els, the weco n sid ere se a simple A* with aPCFGprecm-mode 1581 puted heuristic, while for the the more complex HWDep model, we used a hierarchical A* algorithm (Pauls and Klein, 2009a; Felzenszwalb and McAllester, 2007) based on a simple grammar projection that we designed. 4.1 Hardware-Independent Results: PCFG For the PCFG model, we compared three agendabased parsers: EXH prioritizes edges by their span length, thereby simulating the exhaustive CKY algorithm; NULL prioritizes edges by their inside probability; and SX is an A* parser that prioritizes edges by their inside probability times an admissible outside probability estimate.8 We use the SX estimate devised by Klein and Manning (2003) for CFG parsing, where they found it offered very good performance for relativel</context>
<context position="22460" citStr="Pauls and Klein (2009" startWordPosition="3774" endWordPosition="3777">.4 100 13.3 100 NULL 24.3 71 4.9 78 13.5 86 3.5 83 113.8 85 11.1 83 SX 20.9 61 4.3 68 10.0 64 2.6 62 96.5 72 9.7 73 Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging. grammar, use these to compute Viterbi outside probabilities for the simple grammar, and then use these as outside estimates for the true grammar; all computations are prioritized in a single agenda following the algorithm of Felzenszwalb and McAllester (2007) and Pauls and Klein (2009a). We designed two simple grammar projections, each simplifying the HWDep model: PCFGProj completely removes lexicalization and projects the grammar to a PCFG, while as LexcatProj removes only the headwords but retains the lexical categories. Figure 4 compares exhaustive search, A* with no heuristic (NULL), and HA*. For HA*, parsing effort is broken down into the different edge types computed at each stage: We distinguish between the work carried out to compute the inside and outside edges of the projection, where the latter represent the heuristic estimates, and finally, the work to compute </context>
</contexts>
<marker>Pauls, Klein, 2009</marker>
<rawString>A. Pauls and D. Klein. 2009b. k-best A* Parsing. In Proc. of ACL-IJCNLP, ACL-IJCNLP ’09, pages 958– 966.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>K Hollingshead</author>
</authors>
<title>Linear complexity context-free parsing pipelines via chart constraints.</title>
<date>2009</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="28832" citStr="Roark and Hollingshead (2009)" startWordPosition="4843" endWordPosition="4846">tions in the number of edges processed to result in realtime savings, due to the added expense of keeping a priority queue. However, we 1584 have shown that A* can yield real improvements even over the highly optimized technique of adaptive supertagging: in this pruned search space, a 44% reduction in the number of edges pushed results in a 15% speedup in CPU time. Furthermore, just as A* can be combined with adaptive supertagging, it should also combine easily with other search-space pruning methods, such as those of Djordjevic et al. (2007), Kummerfeld et al. (2010), Zhang et al. (2010) and Roark and Hollingshead (2009). In future work we plan to examine better A* heuristics for CCG, and to look at principled approaches to combine the strengths of A*, adaptive supertagging, and other techniques to the best advantage. Acknowledgements We would like to thank Prachya Boonkwan, Juri Ganitkevitch, Philipp Koehn, Tom Kwiatkowski, Matt Post, Mark Steedman, Emily Thomforde, and Luke Zettlemoyer for helpful discussion related to this work and comments on previous drafts; Julia Hockenmaier for furnishing us with her parser; and the anonymous reviewers for helpful commentary. We also acknowledge funding from EPSRC gran</context>
</contexts>
<marker>Roark, Hollingshead, 2009</marker>
<rawString>B. Roark and K. Hollingshead. 2009. Linear complexity context-free parsing pipelines via chart constraints. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The syntactic process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1293" citStr="Steedman, 2000" startWordPosition="196" endWordPosition="197">rmalisms like CCG. In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model. 1 Introduction Efficient parsing of Combinatorial Categorial Grammar (CCG; Steedman, 2000) is a longstanding problem in computational linguistics. Even with practical CCG that are strongly context-free (Fowler and Penn, 2010), parsing can be much harder than with Penn Treebank-style context-free grammars, since the number of nonterminal categories is generally much larger, leading to increased grammar constants. Where a typical Penn Treebank grammar may have fewer than 100 nonterminals (Hockenmaier and Steedman, 2002), we found that a CCG grammar derived from CCGbank contained nearly 1600. The same grammar assigns an average of 26 lexical categories per word, resulting in a very la</context>
<context position="4934" citStr="Steedman, 2000" startWordPosition="773" endWordPosition="774">ques (§5). 2 CCG and Parsing Algorithms CCG is a lexicalized grammar formalism encoding for each word lexical categories which are either basic (eg. NN, JJ) or complex. Complex lexical categories specify the number and directionality of arguments. For example, one lexical category (of over 100 in our model) for the transitive verb like is (S\NP2)/NP1, specifying the first argument as an NP to the right and the second as an NP to the left. In parsing, adjacent spans are combined using a small number of binary combinatory rules like forward application or composition on the spanning categories (Steedman, 2000; Fowler and Penn, 2010). In the first derivation below, (S\NP)/NP and NP combine to form the spanning category S\NP, which only requires an NP to its left to form a complete sentence-spanning S. The second derivation uses type-raising to change the category type of I. NP (S\NP)/NP NP &gt; S/(S\NP) &gt;� NP (S\NP)/NP NP S S/NP S Because of the number of lexical categories and their complexity, a key difficulty in parsing CCG is that the number of analyses for each span of the sentence quickly becomes extremely large, even with efficient dynamic programming. 2.1 Adaptive Supertagging Supertagging (Ba</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>M. Steedman. 2000. The syntactic process. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>B-G Ahn</author>
<author>S Clark</author>
<author>C V Wyk</author>
<author>J R Curran</author>
<author>L Rimell</author>
</authors>
<title>Chart pruning for fast lexicalised-grammar parsing.</title>
<date>2010</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="28798" citStr="Zhang et al. (2010)" startWordPosition="4838" endWordPosition="4841">st make very large reductions in the number of edges processed to result in realtime savings, due to the added expense of keeping a priority queue. However, we 1584 have shown that A* can yield real improvements even over the highly optimized technique of adaptive supertagging: in this pruned search space, a 44% reduction in the number of edges pushed results in a 15% speedup in CPU time. Furthermore, just as A* can be combined with adaptive supertagging, it should also combine easily with other search-space pruning methods, such as those of Djordjevic et al. (2007), Kummerfeld et al. (2010), Zhang et al. (2010) and Roark and Hollingshead (2009). In future work we plan to examine better A* heuristics for CCG, and to look at principled approaches to combine the strengths of A*, adaptive supertagging, and other techniques to the best advantage. Acknowledgements We would like to thank Prachya Boonkwan, Juri Ganitkevitch, Philipp Koehn, Tom Kwiatkowski, Matt Post, Mark Steedman, Emily Thomforde, and Luke Zettlemoyer for helpful discussion related to this work and comments on previous drafts; Julia Hockenmaier for furnishing us with her parser; and the anonymous reviewers for helpful commentary. We also a</context>
</contexts>
<marker>Zhang, Ahn, Clark, Wyk, Curran, Rimell, 2010</marker>
<rawString>Y. Zhang, B.-G. Ahn, S. Clark, C. V. Wyk, J. R. Curran, and L. Rimell. 2010. Chart pruning for fast lexicalised-grammar parsing. In Proc. of COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>