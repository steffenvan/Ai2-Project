<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.819920333333333">
Squibs and Discussions
Dependency Unification Grammar for
PROLOG
</title>
<author confidence="0.999484">
Friedrich Steimann* Christoph Brzoska*
</author>
<affiliation confidence="0.992679">
Universitat Karlsruhe Universitat Karlsruhe
</affiliation>
<sectionHeader confidence="0.987987" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9999675">
The programming language PROLOG has proved to be an excellent tool for imple-
menting natural language processing systems. Its built-in resolution and unification
mechanisms are well suited to both accept and generate sentences of artificial and nat-
ural languages. Although supporting many different linguistic formalisms, its straight-
forwardness and elegance have perhaps best been demonstrated with definite clause
grammars (DCGs) (Pereira and Warren 1980), an extension to PROLOG&apos;s syntax al-
lowing direct implementation of rules of context-free grammars as Horn clauses.
While context-free grammars and DCGs—strongly related to the huge linguistic
field of constituency or phrase structure grammars and their descendants—have be-
come very popular among logic programmers, dependency grammars (DGs) have long
remained a widely unnoticed linguistic alternative. DG is based on the observation
that each word of a sentence has individual slots to be filled by others, its so-called
dependents. Which dependents a particular word takes depends not only on its func-
tion within the sentence, but also on its meaning—like other contemporary linguistic
frameworks, DG integrates both syntactic and semantic aspects of natural language.
DG was first formalized by Tesniere (1959) and later, among others, by Gaifman
(1965) and Hays (1964). The formalization presented in this paper is based on Hellwig&apos;s
Dependency Unification Grammar (DUG) (Hellwig 1986). We merely add a framework
for automatic translation of DUG rules to Horn clauses that makes DUGs as easy to
implement as classic DCGs.
</bodyText>
<sectionHeader confidence="0.703904" genericHeader="method">
2. Dependency Grammar as Context-Free Grammar
</sectionHeader>
<bodyText confidence="0.999811166666667">
Whereas context-free grammars differentiate between terminals (coding the words of
a language) and non-terminals (representing the constituents that are to be expanded),
the symbols of a DG uniformly serve both purposes: like terminals they must be part
of the sentence to be accepted (or generated), and like non-terminals, they call for ad-
ditional constituents of the sentence. Despite this significant difference, DG can be de-
fined in terms of context-free grammar, making the twofold role of its symbols explicit:
</bodyText>
<subsectionHeader confidence="0.679284">
Definition
</subsectionHeader>
<bodyText confidence="0.866976333333333">
A context-free grammar G = (T, N, P. S) where
—terminals and non-terminals are related by a one-to-one mapping
f: T —&gt; NWS} and
</bodyText>
<footnote confidence="0.76960325">
* Institut fiir Logik, Komplexitat und Deduktionssysteme, Universitat Karlsruhe, Germany. E-mail:
{steimann,brzoska}@ira.uka.de
C) 1995 Association for Computational Linguistics
Computational Linguistics Volume 21, Number 1
</footnote>
<equation confidence="0.772312">
—each production in P is either of the form
s —4 ni ... nm
</equation>
<bodyText confidence="0.718065666666667">
or of the form
n —&gt; ni . . . f 1 (n) • • • nm,
where n, n1,. . . , nm are elements of NVS} and s -= S
</bodyText>
<equation confidence="0.74843875">
is a dependency grammar.
Accordingly, if atomic symbols are replaced by first-order terms, the following toy
DG can be implemented in PROLOG using the DCG rule format:
s --&gt; n(_, verb(_)).
</equation>
<construct confidence="0.719857">
n(give, verb(N)) --&gt;
n(_, noun(N)),
[n(give, verb(N))],
n(_, noun(_)),
n(_, noun(_)).
n(sleep, verb(N)) --&gt;
n(_, noun(N)),
[n(sleep, verb(N))].
</construct>
<bodyText confidence="0.916556588235294">
n(&apos;Peter&apos;, noun(N)) --&gt;
[n(&apos;Peter&apos;, noun(N))].
n(qilark&apos;, noun(N)) --&gt;
[n(cMark&apos;, noun(N))].
n(book, noun(N)) --&gt;
n(_, det),
[n(book, noun(N))].
n(a, det) --&gt;
[n(a, det)].
The terms n(.,.) provide space for feature structures commonly employed to capture
syntactic and semantic properties of words (Shieber 1986; Knight 1989). They serve
only as an example here; other structures, including that used by Hellwig (1986), can
also be employed.
Prior to parsing, each sentence must be converted to a string of terms holding
the features derived through lexical analysis. This preprocessing step also resolves
lexical ambiguities by representing words with alternative meanings through different
symbols. Parsing the sentences &amp;quot;Peter gives Mark a book&amp;quot; and &amp;quot;Mark sleeps&amp;quot; with the
</bodyText>
<page confidence="0.964629">
96
</page>
<figure confidence="0.890836375">
Friedrich Steimann and Christoph Brzoska Dependency Unification Grammar for PROLOG
above DCG produces the following dependency trees:
gives sleeps
Peter Mark Mark
book
a
Pe er gives Mark Mark sleeps
a book
</figure>
<sectionHeader confidence="0.376879" genericHeader="method">
3. Direct Transformation of DUG Rules to Horn Clauses
</sectionHeader>
<bodyText confidence="0.997142636363637">
Although implementing DUG as DCG works acceptably, it makes no use of the rules&apos;
regular form: note how, when parsing the sentence &amp;quot;Mark sleeps,&amp;quot; the parser calls
several rules before it realizes that the rule for give must fail (because the sentence
does not contain give), even though the head already indicates that give is required for
the rule to succeed. If, however, the word partially specified as n(_, verb(_)) in the body
of the start rule is accepted before the next rule is selected, an intelligent parser can
exploit the fact that the sentence&apos;s verb is sleep and immediately call the appropriate
rule. We therefore suggest an alternative syntax and translation scheme that produces
a more efficient DUG parser.
In our DUG syntax, the head of a rule is separated from its body (holding the
dependents of the word in the head) by the binary infix operator :&gt;. The start rule
</bodyText>
<equation confidence="0.8688964">
s :&gt; n(_, verb(_)) .
is translated to the Horn clause
s(_Gl, _G2) :-
accept(n(_G3, verb(_G4)), _G1, _G5),
n(_G3, verb(_G4), _G5, _G2).
</equation>
<bodyText confidence="0.998092666666667">
where the arguments appended to each predicate hold input and output sentence,
respectively, and where an accept predicate is inserted before each literal of the rule
body.&apos; Accordingly,
</bodyText>
<equation confidence="0.8109982">
n(sleep, verb(N)) :&gt; n(_, noun(N)).
becomes
n(sleep, verb(N), _Gl, _G2) :-
accept(n(_G3, noun(N)), _Gl, _G4),
n(_G3, noun(N), _G4, _G2).
</equation>
<bodyText confidence="0.94701675">
Note that the head literal of the sleep rule need not be repeated in the body because
the respective word is removed from the input sentence before the rule is called (in
this case in the start rule). The fact that a word has no dependent is coded by
n(well, adverb) :&gt; .
</bodyText>
<footnote confidence="0.643506">
1 The implementation of accept(. . .) can be found in the appendix.
</footnote>
<page confidence="0.995265">
97
</page>
<note confidence="0.613784">
Computational Linguistics Volume 21, Number 1
</note>
<bodyText confidence="0.92212775">
and translated to
n(well, adverb, _Gl, _G1).
Like other contemporary grammar formalisms, DUG comes with syntactic extensions
that code optionality and references.
</bodyText>
<subsectionHeader confidence="0.999076">
3.1 Optionality
</subsectionHeader>
<bodyText confidence="0.9951898">
Many dependents are optional. Rather than providing an alternative rule for every
possible combination of dependents, it is more convenient to declare a dependent
optional, meaning that a sentence is correct independent of its presence. For example,
n(sleep, verb(N)) :&gt; n(_, noun(N)), ? n(_, adverb).
where ? precedes the optional dependent, is implemented as
</bodyText>
<equation confidence="0.99528625">
n(sleep, verb(N), _G2) :-
accept(n(_G3, noun(N)), _Gl, _G4), n(_G3, noun(N), _G4, _G5),
((accept(n(_G6, adverb)), _G5, _G7), n(_G6, adverb, _G7, _G2))
_G5=_G2) .
</equation>
<bodyText confidence="0.962909">
accepting &amp;quot;Mark sleeps&amp;quot; as well as &amp;quot;Mark sleeps well.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.999764">
3.2 Referencing
</subsectionHeader>
<bodyText confidence="0.941393666666667">
References account for the fact that many words are similar in terms of the dependents
they take. In order not to repeat the same set of rules over and over again, a reference
operator = (read &apos;goes like&apos;) is introduced that causes branching to the rule of an
analogous word, as in
n(yawn, verb(N)) :&gt; ==&gt; n(sleep, verb(N))
In this case, the word sleep being referred to is not a dependent of yawn, the PROLOG
translation
n(yawn, verb(N) , _G1, _G2) :- n(sleep, verb(N) , , _G2) .
therefore branches to the rule for sleep without accepting the word sleep.
As a side effect, references introduce quasi non-terminals to DUG. For example,
by factoring out common dependency patterns, it is possible to generalize the rules
for transitive verbs and allow for exceptions to the rule at the same time:
</bodyText>
<table confidence="0.891777625">
Y. standard dependents of transitive verbs in active voice
transverb(N, active) :&gt;
word(_, noun(N)), % subject
word(_, noun(_)). % object
% standard dependents of transitive verbs in passive voice
transverb(N, passive) :&gt;
word(_, noun(N)), % subject
? word(by, preposition). % optional agent
</table>
<footnote confidence="0.674139666666667">
% standard transitive verb
word(like, verb(N, Voice)) :&gt;
==&gt; transverb(N, Voice).
</footnote>
<page confidence="0.975864">
98
</page>
<bodyText confidence="0.5884368">
Friedrich Steimann and Christoph Brzoska Dependency Unification Grammar for PROLOG
% transitive verb with additional indirect object
word(give, verb(N, Voice)) :&gt;
==&gt; transverb(N, Voice),
word(_, noun(_)).
</bodyText>
<sectionHeader confidence="0.607689" genericHeader="method">
4. A Word about Word Order
</sectionHeader>
<bodyText confidence="0.968052818181818">
Following Hellwig&apos;s DUG formalism, our PROLOG implementation does not code
word order directly in the rules. Other DG formalisms, such as the one proposed by
Gaifman (1965) and Hays (1964), mark the position of the head among its depen-
dents by a special symbol in the body. The DUG parser can be adapted to follow this
convention by accepting the symbol self in the rule body as in
n(sleep, noun(N)) :&gt; n(_, noun(N)), self.
and by modifying both the preprocessor and the accept predicate so that the input
sentence is split at the position of the dependent accepted and left and right remainders
are passed to the next rules separately. However, many natural languages leave word
order rather unconstrained, and its adequate handling is not a problem specific to DGs
(see, for example, Pereira 1981, and Covington 1990).
</bodyText>
<sectionHeader confidence="0.757919" genericHeader="method">
5. Notes on Performance
</sectionHeader>
<bodyText confidence="0.999752611111111">
The presented DUG formalism with free word order has successfully been employed
to parse Latin sentences. Tracing showed that backtracking was considerably reduced
as compared with an equivalent phrase structure grammar, although no good upper
bound for complexity could be found (Steimann 1991). Although the pure DG for-
malism proved to be particularly practical for integration of idioms and exceptions,
its lack of constituent symbols, i.e., non-terminals, would have lead to a grammar
of enormous size and made it difficult to integrate special Latin constructs such as
accusative cum infinitive or ablative absolute.
However, as shown above, DUG is a hybrid grammar: although dependency rules
are the backbone of the formalism, it allows the introduction of quasi non-terminals
that are integrated into the grammar via references. If desired, phrase structure rules
can thus easily be combined with ordinary dependency rules.
The size of a grammar can be further reduced by introduction of order-sorted
feature types (Alt-Kaci and Nasr 1986) supporting variable numbers of labeled argu-
ments and subtyping. Using feature types instead of constructor terms for represent-
ing the words of a language increases readability and enables abstraction of rules as
well as implementation of semantic type hierarchies supporting selectional restrictions
(Steimann 1991).
</bodyText>
<sectionHeader confidence="0.846235" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.880936052631579">
Ait-Kaci, H., and Nasr, R. (1986). &amp;quot;LOGIN:
A logic programming language with
built-in inheritance.&amp;quot; The Journal of Logic
Programming 3:185-215.
Covington, M. A. (1990). &amp;quot;Parsing
discontinuous constituents in dependency
grammar.&amp;quot; Computational Linguistics
16(4):234-236.
Gaifman, H. (1965). &amp;quot;Dependency systems
and phrase-structure systems.&amp;quot;
Information and Control 8:304-337.
Hays, D. G. (1964). &amp;quot;Dependency theory: A
formalism and some observations.&amp;quot;
Language 40(4):511-525.
Hellwig, P. (1986). &amp;quot;Dependency unification
grammar.&amp;quot; In Proceedings, 11th International
Conference on Computational Linguistics
(COLING 1986). University of Bonn,
Bonn. 195-198.
</reference>
<page confidence="0.995525">
99
</page>
<note confidence="0.517033">
Computational Linguistics Volume 21, Number 1
</note>
<reference confidence="0.877150541666667">
Knight, K. (1989). &amp;quot;Unification: A
multidisciplinary survey.&amp;quot; ACM
Computing Surveys 21(1):105-113.
Pereira, F. (1981). &amp;quot;Extraposition
grammars.&amp;quot; American Journal of
Computational Linguistics 7(4):243-255.
Pereira, F., and Warren, D. H. D. (1980).
&amp;quot;Definite clause grammars for language
analysis—a survey of the formalism and a
comparison with augmented transition
networks.&amp;quot; Artificial Intelligence
13:231-278.
Shieber, S. M. (1986). &amp;quot;An introduction to
unification-based approaches to
grammar.&amp;quot; CLSI Lecture Notes, No. 4,
Stanford University, Stanford, California.
Steimann, F. (1991). &amp;quot;Ordnungssortierte
feature-Logik und
Dependenzgrammatiken in der
Computerlinguistik.&amp;quot; Diplomarbeit
Universitat Karlsruhe, Fakultat fiir
Informatik, Karlsruhe, Germany.
Tesniere, L. (1959). Elements de syntaxe
structurale. Paris: Librairie Klincksiek.
</reference>
<page confidence="0.984543">
100
</page>
<note confidence="0.501712">
Friedrich Steimann and Christoph Brzoska Dependency Unification Grammar for PROLOG
</note>
<sectionHeader confidence="0.978558" genericHeader="method">
Appendix A
</sectionHeader>
<subsectionHeader confidence="0.696904">
The DUG Preprocessor
</subsectionHeader>
<bodyText confidence="0.955740142857143">
The following PROLOG source code implements a simple preprocessor that converts
source files containing DUG rules into target files consisting of Horn clauses only.
Automatic creation of the parse tree has also been implemented. However, it is omitted
here for clarity.
Note that every call to a start rule must be extended by two list arguments: the
input and the output sentence (the latter usually being the empty list [D.
% operator directives (priorities must be adapted)
</bodyText>
<reference confidence="0.94833040625">
op(1200, xfx, &apos;:&gt;&apos;).
op(600, fx, &apos;7&apos;)•
op(500, fx, &apos;==&gt;&apos;).
dug(Source, Target) :-
see(Source),
tell (Target),
convert,
seen,
told.
convert :-
read(DUGClause),
(DUGClause = end_of_file
convert(DUGClause, PClause),
displayq(PClause), write(&apos;.&apos;), nl,
convert).
% DUG rule
convert((HeadIn :&gt; BodyIn), (HeadOut BodyOut)) :-
!, HeadIn =.. [PredlArgs],
append(Args, [In, Out], Expanded),
HeadOut =.. [PredlExpanded],
convert(BodyIn, BodyOut, In, Out).
% other
convert(Clause, Clause).
% conjunction
convert((AIn, BIn), (AOut, BOut), In, Out) :-
!, convert(AIn, AOut, In, Intermediate),
convert(BIn, BOut, Intermediate, Out).
% option
convert(? AIn, ((AOut); In = Out), In, Out) :-
!, convert(AIn, AOut, In, Out).
% reference
convert (==&gt; AIn, AOut, In, Out) :-
</reference>
<page confidence="0.955495">
101
</page>
<note confidence="0.363613">
Computational Linguistics Volume 21, Number 1
</note>
<reference confidence="0.9591614">
!, AIn =.. [PredlArgs],
append(Args, [In, Out], Expanded),
AOut =.. [PredlExpanded].
% no dependents
convert([], true, In, In) :- !.
% dependent (introduces call to &apos;accept&apos;)
convert(AIn, (accept(AIn, In, Intermediate), AOut), In, Out) :-
Amn =.. [PredlArgs],
append(Args, [Intermediate, Out], Expanded),
AOut =.. [PredlExpanded].
The accept predicate that must be included in every program containing DUG rules
can be implemented as follows:
accept (Element, [ElementIString], String).
accept (Element, [OtherlStringIn], [OtherlStringOut]) :-
accept(Element, StringIn, StringOut).
</reference>
<page confidence="0.998624">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.966102">
<title confidence="0.989408">Squibs and Discussions Dependency Unification Grammar for PROLOG</title>
<author confidence="0.999713">Friedrich Steimann Christoph Brzoska</author>
<affiliation confidence="0.990346">Universitat Karlsruhe Universitat Karlsruhe</affiliation>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Ait-Kaci</author>
<author>R Nasr</author>
</authors>
<title>LOGIN: A logic programming language with built-in inheritance.&amp;quot;</title>
<date>1986</date>
<journal>The Journal of Logic Programming</journal>
<pages>3--185</pages>
<marker>Ait-Kaci, Nasr, 1986</marker>
<rawString>Ait-Kaci, H., and Nasr, R. (1986). &amp;quot;LOGIN: A logic programming language with built-in inheritance.&amp;quot; The Journal of Logic Programming 3:185-215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Covington</author>
</authors>
<title>Parsing discontinuous constituents in dependency grammar.&amp;quot;</title>
<date>1990</date>
<journal>Computational Linguistics</journal>
<pages>16--4</pages>
<contexts>
<context position="8940" citStr="Covington 1990" startWordPosition="1401" endWordPosition="1402">ion of the head among its dependents by a special symbol in the body. The DUG parser can be adapted to follow this convention by accepting the symbol self in the rule body as in n(sleep, noun(N)) :&gt; n(_, noun(N)), self. and by modifying both the preprocessor and the accept predicate so that the input sentence is split at the position of the dependent accepted and left and right remainders are passed to the next rules separately. However, many natural languages leave word order rather unconstrained, and its adequate handling is not a problem specific to DGs (see, for example, Pereira 1981, and Covington 1990). 5. Notes on Performance The presented DUG formalism with free word order has successfully been employed to parse Latin sentences. Tracing showed that backtracking was considerably reduced as compared with an equivalent phrase structure grammar, although no good upper bound for complexity could be found (Steimann 1991). Although the pure DG formalism proved to be particularly practical for integration of idioms and exceptions, its lack of constituent symbols, i.e., non-terminals, would have lead to a grammar of enormous size and made it difficult to integrate special Latin constructs such as </context>
</contexts>
<marker>Covington, 1990</marker>
<rawString>Covington, M. A. (1990). &amp;quot;Parsing discontinuous constituents in dependency grammar.&amp;quot; Computational Linguistics 16(4):234-236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Gaifman</author>
</authors>
<title>Dependency systems and phrase-structure systems.&amp;quot;</title>
<date>1965</date>
<journal>Information and Control</journal>
<pages>8--304</pages>
<contexts>
<context position="1477" citStr="Gaifman (1965)" startWordPosition="207" endWordPosition="208"> and their descendants—have become very popular among logic programmers, dependency grammars (DGs) have long remained a widely unnoticed linguistic alternative. DG is based on the observation that each word of a sentence has individual slots to be filled by others, its so-called dependents. Which dependents a particular word takes depends not only on its function within the sentence, but also on its meaning—like other contemporary linguistic frameworks, DG integrates both syntactic and semantic aspects of natural language. DG was first formalized by Tesniere (1959) and later, among others, by Gaifman (1965) and Hays (1964). The formalization presented in this paper is based on Hellwig&apos;s Dependency Unification Grammar (DUG) (Hellwig 1986). We merely add a framework for automatic translation of DUG rules to Horn clauses that makes DUGs as easy to implement as classic DCGs. 2. Dependency Grammar as Context-Free Grammar Whereas context-free grammars differentiate between terminals (coding the words of a language) and non-terminals (representing the constituents that are to be expanded), the symbols of a DG uniformly serve both purposes: like terminals they must be part of the sentence to be accepted</context>
<context position="8293" citStr="Gaifman (1965)" startWordPosition="1290" endWordPosition="1291">itive verbs in passive voice transverb(N, passive) :&gt; word(_, noun(N)), % subject ? word(by, preposition). % optional agent % standard transitive verb word(like, verb(N, Voice)) :&gt; ==&gt; transverb(N, Voice). 98 Friedrich Steimann and Christoph Brzoska Dependency Unification Grammar for PROLOG % transitive verb with additional indirect object word(give, verb(N, Voice)) :&gt; ==&gt; transverb(N, Voice), word(_, noun(_)). 4. A Word about Word Order Following Hellwig&apos;s DUG formalism, our PROLOG implementation does not code word order directly in the rules. Other DG formalisms, such as the one proposed by Gaifman (1965) and Hays (1964), mark the position of the head among its dependents by a special symbol in the body. The DUG parser can be adapted to follow this convention by accepting the symbol self in the rule body as in n(sleep, noun(N)) :&gt; n(_, noun(N)), self. and by modifying both the preprocessor and the accept predicate so that the input sentence is split at the position of the dependent accepted and left and right remainders are passed to the next rules separately. However, many natural languages leave word order rather unconstrained, and its adequate handling is not a problem specific to DGs (see,</context>
</contexts>
<marker>Gaifman, 1965</marker>
<rawString>Gaifman, H. (1965). &amp;quot;Dependency systems and phrase-structure systems.&amp;quot; Information and Control 8:304-337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Hays</author>
</authors>
<title>Dependency theory: A formalism and some observations.&amp;quot;</title>
<date>1964</date>
<journal>Language</journal>
<pages>40--4</pages>
<contexts>
<context position="1493" citStr="Hays (1964)" startWordPosition="210" endWordPosition="211">nts—have become very popular among logic programmers, dependency grammars (DGs) have long remained a widely unnoticed linguistic alternative. DG is based on the observation that each word of a sentence has individual slots to be filled by others, its so-called dependents. Which dependents a particular word takes depends not only on its function within the sentence, but also on its meaning—like other contemporary linguistic frameworks, DG integrates both syntactic and semantic aspects of natural language. DG was first formalized by Tesniere (1959) and later, among others, by Gaifman (1965) and Hays (1964). The formalization presented in this paper is based on Hellwig&apos;s Dependency Unification Grammar (DUG) (Hellwig 1986). We merely add a framework for automatic translation of DUG rules to Horn clauses that makes DUGs as easy to implement as classic DCGs. 2. Dependency Grammar as Context-Free Grammar Whereas context-free grammars differentiate between terminals (coding the words of a language) and non-terminals (representing the constituents that are to be expanded), the symbols of a DG uniformly serve both purposes: like terminals they must be part of the sentence to be accepted (or generated),</context>
<context position="8309" citStr="Hays (1964)" startWordPosition="1293" endWordPosition="1294">ive voice transverb(N, passive) :&gt; word(_, noun(N)), % subject ? word(by, preposition). % optional agent % standard transitive verb word(like, verb(N, Voice)) :&gt; ==&gt; transverb(N, Voice). 98 Friedrich Steimann and Christoph Brzoska Dependency Unification Grammar for PROLOG % transitive verb with additional indirect object word(give, verb(N, Voice)) :&gt; ==&gt; transverb(N, Voice), word(_, noun(_)). 4. A Word about Word Order Following Hellwig&apos;s DUG formalism, our PROLOG implementation does not code word order directly in the rules. Other DG formalisms, such as the one proposed by Gaifman (1965) and Hays (1964), mark the position of the head among its dependents by a special symbol in the body. The DUG parser can be adapted to follow this convention by accepting the symbol self in the rule body as in n(sleep, noun(N)) :&gt; n(_, noun(N)), self. and by modifying both the preprocessor and the accept predicate so that the input sentence is split at the position of the dependent accepted and left and right remainders are passed to the next rules separately. However, many natural languages leave word order rather unconstrained, and its adequate handling is not a problem specific to DGs (see, for example, Pe</context>
</contexts>
<marker>Hays, 1964</marker>
<rawString>Hays, D. G. (1964). &amp;quot;Dependency theory: A formalism and some observations.&amp;quot; Language 40(4):511-525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hellwig</author>
</authors>
<title>Dependency unification grammar.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings, 11th International Conference on Computational Linguistics (COLING</booktitle>
<institution>University of Bonn,</institution>
<location>Bonn.</location>
<contexts>
<context position="1610" citStr="Hellwig 1986" startWordPosition="226" endWordPosition="227">ed linguistic alternative. DG is based on the observation that each word of a sentence has individual slots to be filled by others, its so-called dependents. Which dependents a particular word takes depends not only on its function within the sentence, but also on its meaning—like other contemporary linguistic frameworks, DG integrates both syntactic and semantic aspects of natural language. DG was first formalized by Tesniere (1959) and later, among others, by Gaifman (1965) and Hays (1964). The formalization presented in this paper is based on Hellwig&apos;s Dependency Unification Grammar (DUG) (Hellwig 1986). We merely add a framework for automatic translation of DUG rules to Horn clauses that makes DUGs as easy to implement as classic DCGs. 2. Dependency Grammar as Context-Free Grammar Whereas context-free grammars differentiate between terminals (coding the words of a language) and non-terminals (representing the constituents that are to be expanded), the symbols of a DG uniformly serve both purposes: like terminals they must be part of the sentence to be accepted (or generated), and like non-terminals, they call for additional constituents of the sentence. Despite this significant difference, </context>
<context position="3586" citStr="Hellwig (1986)" startWordPosition="538" endWordPosition="539">LOG using the DCG rule format: s --&gt; n(_, verb(_)). n(give, verb(N)) --&gt; n(_, noun(N)), [n(give, verb(N))], n(_, noun(_)), n(_, noun(_)). n(sleep, verb(N)) --&gt; n(_, noun(N)), [n(sleep, verb(N))]. n(&apos;Peter&apos;, noun(N)) --&gt; [n(&apos;Peter&apos;, noun(N))]. n(qilark&apos;, noun(N)) --&gt; [n(cMark&apos;, noun(N))]. n(book, noun(N)) --&gt; n(_, det), [n(book, noun(N))]. n(a, det) --&gt; [n(a, det)]. The terms n(.,.) provide space for feature structures commonly employed to capture syntactic and semantic properties of words (Shieber 1986; Knight 1989). They serve only as an example here; other structures, including that used by Hellwig (1986), can also be employed. Prior to parsing, each sentence must be converted to a string of terms holding the features derived through lexical analysis. This preprocessing step also resolves lexical ambiguities by representing words with alternative meanings through different symbols. Parsing the sentences &amp;quot;Peter gives Mark a book&amp;quot; and &amp;quot;Mark sleeps&amp;quot; with the 96 Friedrich Steimann and Christoph Brzoska Dependency Unification Grammar for PROLOG above DCG produces the following dependency trees: gives sleeps Peter Mark Mark book a Pe er gives Mark Mark sleeps a book 3. Direct Transformation of DUG R</context>
</contexts>
<marker>Hellwig, 1986</marker>
<rawString>Hellwig, P. (1986). &amp;quot;Dependency unification grammar.&amp;quot; In Proceedings, 11th International Conference on Computational Linguistics (COLING 1986). University of Bonn, Bonn. 195-198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
</authors>
<title>Unification: A multidisciplinary survey.&amp;quot;</title>
<date>1989</date>
<journal>ACM Computing Surveys</journal>
<pages>21--1</pages>
<contexts>
<context position="3493" citStr="Knight 1989" startWordPosition="523" endWordPosition="524">c symbols are replaced by first-order terms, the following toy DG can be implemented in PROLOG using the DCG rule format: s --&gt; n(_, verb(_)). n(give, verb(N)) --&gt; n(_, noun(N)), [n(give, verb(N))], n(_, noun(_)), n(_, noun(_)). n(sleep, verb(N)) --&gt; n(_, noun(N)), [n(sleep, verb(N))]. n(&apos;Peter&apos;, noun(N)) --&gt; [n(&apos;Peter&apos;, noun(N))]. n(qilark&apos;, noun(N)) --&gt; [n(cMark&apos;, noun(N))]. n(book, noun(N)) --&gt; n(_, det), [n(book, noun(N))]. n(a, det) --&gt; [n(a, det)]. The terms n(.,.) provide space for feature structures commonly employed to capture syntactic and semantic properties of words (Shieber 1986; Knight 1989). They serve only as an example here; other structures, including that used by Hellwig (1986), can also be employed. Prior to parsing, each sentence must be converted to a string of terms holding the features derived through lexical analysis. This preprocessing step also resolves lexical ambiguities by representing words with alternative meanings through different symbols. Parsing the sentences &amp;quot;Peter gives Mark a book&amp;quot; and &amp;quot;Mark sleeps&amp;quot; with the 96 Friedrich Steimann and Christoph Brzoska Dependency Unification Grammar for PROLOG above DCG produces the following dependency trees: gives sleeps</context>
</contexts>
<marker>Knight, 1989</marker>
<rawString>Knight, K. (1989). &amp;quot;Unification: A multidisciplinary survey.&amp;quot; ACM Computing Surveys 21(1):105-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>Extraposition grammars.&amp;quot;</title>
<date>1981</date>
<journal>American Journal of Computational Linguistics</journal>
<pages>7--4</pages>
<contexts>
<context position="8919" citStr="Pereira 1981" startWordPosition="1398" endWordPosition="1399">4), mark the position of the head among its dependents by a special symbol in the body. The DUG parser can be adapted to follow this convention by accepting the symbol self in the rule body as in n(sleep, noun(N)) :&gt; n(_, noun(N)), self. and by modifying both the preprocessor and the accept predicate so that the input sentence is split at the position of the dependent accepted and left and right remainders are passed to the next rules separately. However, many natural languages leave word order rather unconstrained, and its adequate handling is not a problem specific to DGs (see, for example, Pereira 1981, and Covington 1990). 5. Notes on Performance The presented DUG formalism with free word order has successfully been employed to parse Latin sentences. Tracing showed that backtracking was considerably reduced as compared with an equivalent phrase structure grammar, although no good upper bound for complexity could be found (Steimann 1991). Although the pure DG formalism proved to be particularly practical for integration of idioms and exceptions, its lack of constituent symbols, i.e., non-terminals, would have lead to a grammar of enormous size and made it difficult to integrate special Lati</context>
</contexts>
<marker>Pereira, 1981</marker>
<rawString>Pereira, F. (1981). &amp;quot;Extraposition grammars.&amp;quot; American Journal of Computational Linguistics 7(4):243-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>D H D Warren</author>
</authors>
<title>Definite clause grammars for language analysis—a survey of the formalism and a comparison with augmented transition networks.&amp;quot;</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<pages>13--231</pages>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, F., and Warren, D. H. D. (1980). &amp;quot;Definite clause grammars for language analysis—a survey of the formalism and a comparison with augmented transition networks.&amp;quot; Artificial Intelligence 13:231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>An introduction to unification-based approaches to grammar.&amp;quot;</title>
<date>1986</date>
<journal>CLSI Lecture Notes,</journal>
<volume>4</volume>
<institution>Stanford University,</institution>
<location>Stanford, California.</location>
<contexts>
<context position="3479" citStr="Shieber 1986" startWordPosition="521" endWordPosition="522">ngly, if atomic symbols are replaced by first-order terms, the following toy DG can be implemented in PROLOG using the DCG rule format: s --&gt; n(_, verb(_)). n(give, verb(N)) --&gt; n(_, noun(N)), [n(give, verb(N))], n(_, noun(_)), n(_, noun(_)). n(sleep, verb(N)) --&gt; n(_, noun(N)), [n(sleep, verb(N))]. n(&apos;Peter&apos;, noun(N)) --&gt; [n(&apos;Peter&apos;, noun(N))]. n(qilark&apos;, noun(N)) --&gt; [n(cMark&apos;, noun(N))]. n(book, noun(N)) --&gt; n(_, det), [n(book, noun(N))]. n(a, det) --&gt; [n(a, det)]. The terms n(.,.) provide space for feature structures commonly employed to capture syntactic and semantic properties of words (Shieber 1986; Knight 1989). They serve only as an example here; other structures, including that used by Hellwig (1986), can also be employed. Prior to parsing, each sentence must be converted to a string of terms holding the features derived through lexical analysis. This preprocessing step also resolves lexical ambiguities by representing words with alternative meanings through different symbols. Parsing the sentences &amp;quot;Peter gives Mark a book&amp;quot; and &amp;quot;Mark sleeps&amp;quot; with the 96 Friedrich Steimann and Christoph Brzoska Dependency Unification Grammar for PROLOG above DCG produces the following dependency trees</context>
</contexts>
<marker>Shieber, 1986</marker>
<rawString>Shieber, S. M. (1986). &amp;quot;An introduction to unification-based approaches to grammar.&amp;quot; CLSI Lecture Notes, No. 4, Stanford University, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Steimann</author>
</authors>
<title>Ordnungssortierte feature-Logik und Dependenzgrammatiken in der Computerlinguistik.&amp;quot;</title>
<date>1991</date>
<institution>Diplomarbeit Universitat Karlsruhe, Fakultat fiir Informatik,</institution>
<location>Karlsruhe, Germany.</location>
<contexts>
<context position="9261" citStr="Steimann 1991" startWordPosition="1448" endWordPosition="1449"> the position of the dependent accepted and left and right remainders are passed to the next rules separately. However, many natural languages leave word order rather unconstrained, and its adequate handling is not a problem specific to DGs (see, for example, Pereira 1981, and Covington 1990). 5. Notes on Performance The presented DUG formalism with free word order has successfully been employed to parse Latin sentences. Tracing showed that backtracking was considerably reduced as compared with an equivalent phrase structure grammar, although no good upper bound for complexity could be found (Steimann 1991). Although the pure DG formalism proved to be particularly practical for integration of idioms and exceptions, its lack of constituent symbols, i.e., non-terminals, would have lead to a grammar of enormous size and made it difficult to integrate special Latin constructs such as accusative cum infinitive or ablative absolute. However, as shown above, DUG is a hybrid grammar: although dependency rules are the backbone of the formalism, it allows the introduction of quasi non-terminals that are integrated into the grammar via references. If desired, phrase structure rules can thus easily be combi</context>
</contexts>
<marker>Steimann, 1991</marker>
<rawString>Steimann, F. (1991). &amp;quot;Ordnungssortierte feature-Logik und Dependenzgrammatiken in der Computerlinguistik.&amp;quot; Diplomarbeit Universitat Karlsruhe, Fakultat fiir Informatik, Karlsruhe, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tesniere</author>
</authors>
<title>Elements de syntaxe structurale. Paris: Librairie Klincksiek.</title>
<date>1959</date>
<contexts>
<context position="1434" citStr="Tesniere (1959)" startWordPosition="200" endWordPosition="201">of constituency or phrase structure grammars and their descendants—have become very popular among logic programmers, dependency grammars (DGs) have long remained a widely unnoticed linguistic alternative. DG is based on the observation that each word of a sentence has individual slots to be filled by others, its so-called dependents. Which dependents a particular word takes depends not only on its function within the sentence, but also on its meaning—like other contemporary linguistic frameworks, DG integrates both syntactic and semantic aspects of natural language. DG was first formalized by Tesniere (1959) and later, among others, by Gaifman (1965) and Hays (1964). The formalization presented in this paper is based on Hellwig&apos;s Dependency Unification Grammar (DUG) (Hellwig 1986). We merely add a framework for automatic translation of DUG rules to Horn clauses that makes DUGs as easy to implement as classic DCGs. 2. Dependency Grammar as Context-Free Grammar Whereas context-free grammars differentiate between terminals (coding the words of a language) and non-terminals (representing the constituents that are to be expanded), the symbols of a DG uniformly serve both purposes: like terminals they </context>
</contexts>
<marker>Tesniere, 1959</marker>
<rawString>Tesniere, L. (1959). Elements de syntaxe structurale. Paris: Librairie Klincksiek.</rawString>
</citation>
<citation valid="false">
<booktitle>op(1200, xfx, &apos;:&gt;&apos;). op(600, fx, &apos;7&apos;)• op(500, fx, &apos;==&gt;&apos;).</booktitle>
<marker></marker>
<rawString>op(1200, xfx, &apos;:&gt;&apos;). op(600, fx, &apos;7&apos;)• op(500, fx, &apos;==&gt;&apos;).</rawString>
</citation>
<citation valid="false">
<authors>
<author>dug</author>
</authors>
<note>Target) :-see(Source), tell (Target), convert,</note>
<marker>dug, </marker>
<rawString>dug(Source, Target) :-see(Source), tell (Target), convert,</rawString>
</citation>
<citation valid="false">
<authors>
<author>told seen</author>
</authors>
<marker>seen, </marker>
<rawString>seen, told.</rawString>
</citation>
<citation valid="false">
<authors>
<author>convert -read</author>
</authors>
<title>DUGClause = end_of_file convert(DUGClause,</title>
<note>PClause), displayq(PClause), write(&apos;.&apos;), nl, convert).</note>
<marker>-read, </marker>
<rawString>convert :-read(DUGClause), (DUGClause = end_of_file convert(DUGClause, PClause), displayq(PClause), write(&apos;.&apos;), nl, convert).</rawString>
</citation>
<citation valid="false">
<authors>
<author>DUG</author>
</authors>
<title>rule convert((HeadIn :&gt;</title>
<journal>BodyIn), (HeadOut BodyOut)) :-!, HeadIn =.. [PredlArgs], append(Args, [In, Out], Expanded), HeadOut =.. [PredlExpanded], convert(BodyIn, BodyOut, In, Out).</journal>
<marker>DUG, </marker>
<rawString>% DUG rule convert((HeadIn :&gt; BodyIn), (HeadOut BodyOut)) :-!, HeadIn =.. [PredlArgs], append(Args, [In, Out], Expanded), HeadOut =.. [PredlExpanded], convert(BodyIn, BodyOut, In, Out).</rawString>
</citation>
<citation valid="false">
<authors>
<author>other convert</author>
</authors>
<title>conjunction convert((AIn, BIn), (AOut, BOut), In, Out) :-!, convert(AIn, AOut, In, Intermediate), convert(BIn, BOut, Intermediate, Out). % option convert(? AIn, ((AOut);</title>
<booktitle>In = Out), In, Out) :-!, convert(AIn, AOut, In, Out). % reference convert (==&gt; AIn, AOut, In, Out) :-!, AIn =.. [PredlArgs], append(Args, [In, Out], Expanded), AOut =..</booktitle>
<publisher>[PredlExpanded].</publisher>
<marker>convert, </marker>
<rawString>% other convert(Clause, Clause). % conjunction convert((AIn, BIn), (AOut, BOut), In, Out) :-!, convert(AIn, AOut, In, Intermediate), convert(BIn, BOut, Intermediate, Out). % option convert(? AIn, ((AOut); In = Out), In, Out) :-!, convert(AIn, AOut, In, Out). % reference convert (==&gt; AIn, AOut, In, Out) :-!, AIn =.. [PredlArgs], append(Args, [In, Out], Expanded), AOut =.. [PredlExpanded].</rawString>
</citation>
<citation valid="false">
<authors>
<author>no dependents convert</author>
</authors>
<title>dependent (introduces call to &apos;accept&apos;) convert(AIn, (accept(AIn,</title>
<journal>In, Intermediate), AOut), In, Out) :-Amn =.. [PredlArgs], append(Args, [Intermediate, Out], Expanded), AOut =.. [PredlExpanded].</journal>
<marker>convert, </marker>
<rawString>% no dependents convert([], true, In, In) :- !. % dependent (introduces call to &apos;accept&apos;) convert(AIn, (accept(AIn, In, Intermediate), AOut), In, Out) :-Amn =.. [PredlArgs], append(Args, [Intermediate, Out], Expanded), AOut =.. [PredlExpanded].</rawString>
</citation>
<citation valid="false">
<title>The accept predicate that must be included in every program containing DUG rules</title>
<marker></marker>
<rawString>The accept predicate that must be included in every program containing DUG rules</rawString>
</citation>
<citation valid="false">
<title>can be implemented as follows: accept (Element,</title>
<location>[ElementIString], String).</location>
<marker></marker>
<rawString>can be implemented as follows: accept (Element, [ElementIString], String).</rawString>
</citation>
<citation valid="false">
<authors>
<author>accept</author>
</authors>
<location>[OtherlStringIn], [OtherlStringOut]) :-accept(Element, StringIn, StringOut).</location>
<marker>accept, </marker>
<rawString>accept (Element, [OtherlStringIn], [OtherlStringOut]) :-accept(Element, StringIn, StringOut).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>