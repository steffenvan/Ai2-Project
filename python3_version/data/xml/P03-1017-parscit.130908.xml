<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001453">
<title confidence="0.982423">
Constructing Semantic Space Models from Parsed Corpora
</title>
<author confidence="0.99755">
Sebastian Padó
</author>
<affiliation confidence="0.996006">
Department of Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.9921365">
PO Box 15 11 50
66041 Saarbrücken, Germany
</address>
<email confidence="0.997661">
pado@coli.uni-sb.de
</email>
<author confidence="0.990702">
Mirella Lapata
</author>
<affiliation confidence="0.997816">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.9271955">
Regent Court, 211 Portobello Street
Sheffield S1 4DP, UK
</address>
<email confidence="0.998539">
mlap@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994925">
Traditional vector-based models use word
co-occurrence counts from large corpora
to represent lexical meaning. In this pa-
per we present a novel approach for con-
structing semantic spaces that takes syn-
tactic relations into account. We introduce
a formalisation for this class of models
and evaluate their adequacy on two mod-
elling tasks: semantic priming and auto-
matic discrimination of lexical relations.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999880823529412">
Vector-based models of word co-occurrence have
proved a useful representational framework for a
variety of natural language processing (NLP) tasks
such as word sense discrimination (Schütze, 1998),
text segmentation (Choi et al., 2001), contextual
spelling correction (Jones and Martin, 1997), auto-
matic thesaurus extraction (Grefenstette, 1994), and
notably information retrieval (Salton et al., 1975).
Vector-based representations of lexical meaning
have been also popular in cognitive science and
figure prominently in a variety of modelling stud-
ies ranging from similarity judgements (McDonald,
2000) to semantic priming (Lund and Burgess, 1996;
Lowe and McDonald, 2000) and text comprehension
(Landauer and Dumais, 1997).
In this approach semantic information is extracted
from large bodies of text under the assumption that
the context surrounding a given word provides im-
portant information about its meaning. The semantic
properties of words are represented by vectors that
are constructed from the observed distributional pat-
terns of co-occurrence of their neighbouring words.
Co-occurrence information is typically collected in
a frequency matrix, where each row corresponds to
a unique target word and each column represents its
linguistic context.
Contexts are defined as a small number of words
surrounding the target word (Lund and Burgess,
1996; Lowe and McDonald, 2000) or as entire para-
graphs, even documents (Landauer and Dumais,
1997). Context is typically treated as a set of
unordered words, although in some cases syntac-
tic information is taken into account (Lin, 1998;
Grefenstette, 1994; Lee, 1999). A word can be
thus viewed as a point in an n-dimensional semantic
space. The semantic similarity between words can
be then mathematically computed by measuring the
distance between points in the semantic space using
a metric such as cosine or Euclidean distance.
In the variants of vector-based models where no
linguistic knowledge is used, differences among
parts of speech for the same word (e.g., to drink
vs. a drink) are not taken into account in the con-
struction of the semantic space, although in some
cases word lexemes are used rather than word sur-
face forms (Lowe and McDonald, 2000; McDonald,
2000). Minimal assumptions are made with respect
to syntactic dependencies among words. In fact it is
assumed that all context words within a certain dis-
tance from the target word are semantically relevant.
The lack of syntactic information makes the build-
ing of semantic space models relatively straightfor-
ward and language independent (all that is needed is
a corpus of written or spoken text). However, this
entails that contextual information contributes indis-
criminately to a word’s meaning.
Some studies have tried to incorporate syntactic
information into vector-based models. In this view,
the semantic space is constructed from words that
bear a syntactic relationship to the target word of in-
terest. This makes semantic spaces more flexible,
different types of contexts can be selected and words
do not have to physically co-occur to be considered
contextually relevant. However, existing models ei-
ther concentrate on specific relations for construct-
ing the semantic space such as objects (e.g., Lee,
1999) or collapse all types of syntactic relations
available for a given target word (Grefenstette, 1994;
Lin, 1998). Although syntactic information is now
used to select a word’s appropriate contexts, this in-
formation is not explicitly captured in the contexts
themselves (which are still represented by words)
and is therefore not amenable to further processing.
A commonly raised criticism for both types of se-
mantic space models (i.e., word-based and syntax-
based) concerns the notion of semantic similarity.
Proximity between two words in the semantic space
cannot indicate the nature of the lexical relations be-
tween them. Distributionally similar words can be
antonyms, synonyms, hyponyms or in some cases
semantically unrelated. This limits the application
of semantic space models for NLP tasks which re-
quire distinguishing between lexical relations.
In this paper we generalise semantic space models
by proposing a flexible conceptualisation of context
which is parametrisable in terms of syntactic rela-
tions. We develop a general framework for vector-
based models which can be optimised for different
tasks. Our framework allows the construction of se-
mantic space to take place over words or syntactic
relations thus bridging the distance between word-
based and syntax-based models. Furthermore, we
show how our model can incorporate well-defined,
informative contexts in a principled way which re-
tains information about the syntactic relations avail-
able for a given target word.
We first evaluate our model on semantic prim-
ing, a phenomenon that has received much attention
in computational psycholinguistics and is typically
modelled using word-based semantic spaces. We
next conduct a study that shows that our model is
sensitive to different types of lexical relations.
</bodyText>
<sectionHeader confidence="0.961285" genericHeader="method">
2 Dependency-based Vector Space Models
</sectionHeader>
<bodyText confidence="0.989612666666667">
Once we move away from words as the basic con-
text unit, the issue of representation of syntactic in-
formation becomes pertinent. Information about the
dependency relations between words abstracts over
word order and can be considered as an intermediate
layer between surface syntax and semantics. More
</bodyText>
<equation confidence="0.463808">
N
apples
</equation>
<figureCaption confidence="0.999581">
Figure 1: A dependency parse of a short sentence
</figureCaption>
<bodyText confidence="0.999878416666667">
formally, dependencies are asymmetric binary rela-
tionships between a head and a modifier (Tesnière,
1959). The structure of a sentence can be repre-
sented by a set of dependency relationships that form
a tree as shown in Figure 1. Here the head of the sen-
tence is the verb carry which is in turn modified by
its subject lorry and its object apples.
It is the dependencies in Figure 1 that will form
the context over which the semantic space will be
constructed. The construction mechanism sets out
by identifying the local context of a target word,
which is a subset of all dependency paths starting
from it. The paths consist of the dependency edges
of the tree labelled with dependency relations such
as subj, obj, or aux (see Figure 1). The paths can be
ranked by a path value function which gives differ-
ent weight to different dependency types (for exam-
ple, it can be argued that subjects and objects convey
more semantic information than determiners). Tar-
get words are then represented in terms of syntactic
features which form the dimensions of the seman-
tic space. Paths are mapped to features by the path
equivalence relation and the appropriate cells in the
matrix are incremented.
</bodyText>
<subsectionHeader confidence="0.999503">
2.1 Definition of Semantic Space
</subsectionHeader>
<bodyText confidence="0.99895525">
We assume the semantic space formalisation pro-
posed by Lowe (2001). A semantic space is a matrix
whose rows correspond to target words and columns
to dimensions which Lowe calls basis elements:
</bodyText>
<listItem confidence="0.6046122">
Definition 1. A Semantic Space Model is a matrix
K = B × T, where bZ E B denotes the basis element
of column i, tj E T denotes the target word of row j,
and KZj the cell (i, j).
T is the set of words for which the matrix con-
tains representations; this can be either word types
or word tokens. In this paper, we assume that co-
occurrence counts are constructed over word types,
but the framework can be easily adapted to represent
word tokens instead.
</listItem>
<figure confidence="0.994883375">
Det
a
N Aux
lorry might
V
carry
A
sweet
</figure>
<bodyText confidence="0.9981102">
In traditional semantic spaces, the cells Kij of
the matrix correspond to word co-occurrence counts.
This is no longer the case for dependency-based
models. In the following we explain how co-
occurrence counts are constructed.
</bodyText>
<subsectionHeader confidence="0.999385">
2.2 Building the Context
</subsectionHeader>
<bodyText confidence="0.848768090909091">
The first step in constructing a semantic space from
a large collection of dependency relations is to con-
struct a word’s local context.
Definition 2. The dependency parse p of a sentence
s is an undirected graph p(s) = (Vp,Ep). The set of
nodes corresponds to words of the sentence: Vp =
{w1,... ,wn}. The set of edges is Ep  Vp ×Vp.
Definition 3. A class q is a three-tuple consisting
of a POS-tag, a relation, and another POS-tag. We
write Q for the set of all classes Cat × R ×Cat. For
each parse p, the labelling function Lp : Ep  Q as-
signs a class to every edge of the parse.
In Figure 1, the labelling function labels the left-
most edge as Lp((a,lorry)) = Det,det,N. Note that
Det represents the POS-tag “determiner” and det the
dependency relation “determiner”.
In traditional models, the target words are sur-
rounded by context words. In a dependency-based
model, the target words are surrounded by depen-
dency paths.
Definition 4. A path 0 is an ordered tuple of edges
e1,...,en  Enp so that
</bodyText>
<equation confidence="0.932774">
i : (ei−1 = (v1,v2)  ei = (v3,v4))  v2 = v3
</equation>
<bodyText confidence="0.978796466666667">
Definition 5. A path anchored at a word w is a path
e1,...,en so that e1 = (v1,v2) and w = v1. Write
(Dw for the set of all paths over Ep anchored at w.
In words, a path is a tuple of connected edges in
a parse graph and it is anchored at w if it starts at w.
In Figure 1, the set of paths anchored at lorry1 is:
{(lorry,carry),(lorry,carry),(carry,apples),
(lorry,a),(lorry,carry),(carry,might),...}
The local context of a word is the set or a subset of
its anchored paths. The class information can always
be recovered by means of the labelling function.
Definition 6. A local context of a word w from a
sentence s is a subset of the anchored paths at w. A
function c : W  2Φw which assigns a local context
to a word is called a context specification function.
</bodyText>
<footnote confidence="0.851053">
1For the sake of brevity, we only show paths up to length 2.
</footnote>
<bodyText confidence="0.99951">
The context specification function allows to elim-
inate paths on the basis of their classes. For exam-
ple, it is possible to eliminate all paths from the set
of anchored paths but those which contain immedi-
ate subject and direct object relations. This can be
formalised as:
</bodyText>
<equation confidence="0.9999035">
c(w) = {0  (Dw |0 = e 
(Lp(e) = V,obj,N Lp(e) = V,subj,N)}
</equation>
<bodyText confidence="0.9989194">
In Figure 1, the labels of the two edges which
form paths of length 1 and conform to this context
specification are marked in boldface. Notice that the
local context of lorry contains only one anchored
path (c(lorry) = {(lorry,carry)}).
</bodyText>
<subsectionHeader confidence="0.999519">
2.3 Quantifying the Context
</subsectionHeader>
<bodyText confidence="0.999857266666667">
The second step in the construction of the
dependency-based semantic models is to specify the
relative importance of different paths. Linguistic in-
formation can be incorporated into our framework
through the path value function.
Definition 7. The path value function v assigns a
real number to a path: v : (D  R.
For instance, the path value function could pe-
nalise longer paths for only expressing indirect re-
lationships between words. An example of a length-
based path value function is v(0) = 1n where 0 =
e1,...,en. This function assigns a value of 1 to the
one path from c(lorry) and fractions to longer paths.
Once the value of all paths in the local context
is determined, the dimensions of the space must be
specified. Unlike word-based models, our contexts
contain syntactic information and dimensions can
be defined in terms of syntactic features. The path
equivalence relation combines functionally equiva-
lent dependency paths that share a syntactic feature
into equivalence classes.
Definition 8. Let  be the path equivalence relation
on (D. The partition induced by this equivalence re-
lation is the set of basis elements B.
For example, it is possible to combine all paths
which end at the same word: A path which starts
at wi and ends at wj, irrespectively of its length and
class, will be the co-occurrence of wi and wj. This
word-based equivalence function can be defined in
the following manner:
</bodyText>
<equation confidence="0.831077">
(v1,v2),...,(vn−1,vn)  (v1,v2),...,(vm−1,vm)
iff vn = vm
</equation>
<bodyText confidence="0.999741533333333">
This means that in Figure 1 the set of basis elements
is the set of words at which paths end. Although co-
occurrence counts are constructed over words like in
traditional semantic space models, it is only words
which stand in a syntactic relationship to the target
that are taken into account.
Once the value of all paths in the local context
is determined, the local observed frequency for the
co-occurrence of a basis element b with the target
word w is just the sum of values of all paths f in
this context which express the basis element b. The
global observed frequency is the sum of the local
observed frequencies for all occurrences of a target
word type t and is therefore a measure for the co-
occurrence of t and b over the whole corpus.
</bodyText>
<equation confidence="0.868649666666667">
Definition 9. Global observed frequency:
fˆ(b,t) = ∑ ∑ v(f)
wEW (t) φEC(w)Aφ—b
</equation>
<bodyText confidence="0.9999158">
As Lowe (2001) notes, raw frequency counts are
likely to give misleading results. Due to the Zip-
fian distribution of word types, words occurring
with similar frequencies will be judged more similar
than they actually are. A lexical association func-
tion can be used to explicitly factor out chance co-
occurrences.
Definition 10. Write A for the lexical association
function which computes the value of a cell of the
matrix from a co-occurrence frequency:
</bodyText>
<equation confidence="0.644535">
Kij = A(fˆ(bi,tj))
</equation>
<sectionHeader confidence="0.999589" genericHeader="method">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999616">
3.1 Parameter Settings
</subsectionHeader>
<bodyText confidence="0.9998045625">
All our experiments were conducted on the British
National Corpus (BNC), a 100 million word col-
lection of samples of written and spoken language
(Burnard, 1995). We used Lin’s (1998) broad cover-
age dependency parser MINIPAR to obtain a parsed
version of the corpus. MINIPAR employs a man-
ually constructed grammar and a lexicon derived
from WordNet with the addition of proper names
(130,000 entries in total). Lexicon entries con-
tain part-of-speech and subcategorization informa-
tion. The grammar is represented as a network of
35 nodes (i.e., grammatical categories) and 59 edges
(i.e., types of syntactic (dependency) relationships).
MINIPAR uses a distributed chart parsing algorithm.
Grammar rules are implemented as constraints asso-
ciated with the nodes and edges.
</bodyText>
<figure confidence="0.83100375">
Cosine distance cos(x,y) = �∑ix2�∑iy2
i i
Skew divergence sα(x,y) = åixi log xi
αxi+(1−α)yi
</figure>
<figureCaption confidence="0.995193">
Figure 2: Distance measures
</figureCaption>
<bodyText confidence="0.999832840909091">
The dependency-based semantic space was con-
structed with the word-based path equivalence func-
tion from Section 2.3. As basis elements for our se-
mantic space the 1000 most frequent words in the
BNC were used. Each element of the resulting vec-
tor was replaced with its log-likelihood value (see
Definition 10 in Section 2.3) which can be consid-
ered as an estimate of how surprising or distinctive
a co-occurrence pair is (Dunning, 1993).
We experimented with a variety of distance mea-
sures such as cosine, Euclidean distance, L1 norm,
Jaccard’s coefficient, Kullback-Leibler divergence
and the Skew divergence (see Lee 1999 for an
overview). We obtained the best results for co-
sine (Experiment 1) and Skew divergence (Experi-
ment 2). The two measures are shown in Figure 2.
The Skew divergence represents a generalisation of
the Kullback-Leibler divergence and was proposed
by Lee (1999) as a linguistically motivated distance
measure. We use a value of a = .99.
We explored in detail the influence of different
types and sizes of context by varying the context
specification and path value functions. Contexts
were defined over a set of 23 most frequent depen-
dency relations which accounted for half of the de-
pendency edges found in our corpus. From these,
we constructed four context specification functions:
(a) minimum contexts containing paths of length 1
(in Figure 1 sweet and carry are the minimum con-
text for apples), (b) np context adds dependency in-
formation relevant for noun compounds to minimum
context, (c) wide takes into account paths of length
longer than 1 that represent meaningful linguistic re-
lations such as argument structure, but also prepo-
sitional phrases and embedded clauses (in Figure 1
the wide context of apples is sweet, carry, lorry, and
might), and (d) maximum combined all of the above
into a rich context representation.
Four path valuation functions were used: (a) plain
assigns the same value to every path, (b) length
assigns a value inversely proportional to a path’s
length, (c) oblique ranks paths according to the
obliqueness hierarchy of grammatical relations
(Keenan and Comrie, 1977), and (d) oblength
</bodyText>
<table confidence="0.9955405">
∑ixiyi
context specification path value function
1 minimum plain
2 minimum oblique
3 np plain
4 np length
5 np oblique
6 np oblength
7 wide plain
8 wide length
9 wide oblique
10 wide oblength
11 maximum plain
12 maximum length
13 maximum oblique
14 maximum oblength
</table>
<tableCaption confidence="0.999873">
Table 1: The fourteen models
</tableCaption>
<bodyText confidence="0.99977265">
combines length and oblique. The resulting 14
parametrisations are shown in Table 1. Length-
based and length-neutral path value functions are
collapsed for the minimum context specification
since it only considers paths of length 1.
We further compare in Experiments 1 and 2 our
dependency-based model against a state-of-the-art
vector-based model where context is defined as a
“bag of words”. Note that considerable latitude is
allowed in setting parameters for vector-based mod-
els. In order to allow a fair comparison, we se-
lected parameters for the traditional model that have
been considered optimal in the literature (Patel et al.,
1998), namely a symmetric 10 word window and
the most frequent 500 content words from the BNC
as dimensions. These parameters were similar to
those used by Lowe and McDonald (2000) (symmet-
ric 10 word window and 536 content words). Again
the log-likelihood score is used to factor out chance
co-occurrences.
</bodyText>
<subsectionHeader confidence="0.9996">
3.2 Experiment 1: Priming
</subsectionHeader>
<bodyText confidence="0.999976724137931">
A large number of modelling studies in psycholin-
guistics have focused on simulating semantic prim-
ing studies. The semantic priming paradigm pro-
vides a natural test bed for semantic space models
as it concentrates on the semantic similarity or dis-
similarity between a prime and its target, and it is
precisely this type of lexical relations that vector-
based models capture.
In this experiment we focus on Balota and Lorch’s
(1986) mediated priming study. In semantic priming
transient presentation of a prime word like tiger di-
rectly facilitates pronunciation or lexical decision on
a target word like lion. Mediated priming extends
this paradigm by additionally allowing indirectly re-
lated words as primes – like stripes, which is only
related to lion by means of the intermediate concept
tiger. Balota and Lorch (1986) obtained small medi-
ated priming effects for pronunciation tasks but not
for lexical decision. For the pronunciation task, re-
action times were reduced significantly for both di-
rect and mediated primes, however the effect was
larger for direct primes.
There are at least two semantic space simulations
that attempt to shed light on the mediated priming
effect. Lowe and McDonald (2000) replicated both
the direct and mediated priming effects, whereas
Livesay and Burgess (1997) could only replicate di-
rect priming. In their study, mediated primes were
farther from their targets than unrelated words.
</bodyText>
<subsectionHeader confidence="0.511626">
3.2.1 Materials and Design
</subsectionHeader>
<bodyText confidence="0.999712666666667">
Materials were taken form Balota and Lorch
(1986). They consist of 48 target words, each paired
with a related and a mediated prime (e.g., lion-tiger-
stripes). Each related-mediated prime tuple was
paired with an unrelated control randomly selected
from the complement set of related primes.
</bodyText>
<subsectionHeader confidence="0.799044">
3.2.2 Procedure
</subsectionHeader>
<bodyText confidence="0.999899">
One stimulus was removed as it had a low cor-
pus frequency (less than 100), which meant that
the resulting vector would be unreliable. We con-
structed vectors from the BNC for all stimuli with
the dependency-based models and the traditional
model, using the parametrisations given in Sec-
tion 3.1 and cosine as a distance measure. We calcu-
lated the distance in semantic space between targets
and their direct primes (TarDirP), targets and their
mediated primes (TarMedP), targets and their unre-
lated controls (TarUnC) for both models.
</bodyText>
<sectionHeader confidence="0.855473" genericHeader="method">
3.2.3 Results
</sectionHeader>
<bodyText confidence="0.999982533333333">
We carried out a one-way Analysis of Variance
(ANOVA) with the distance as dependent variable
(TarDirP, TarMedP, TarUnC). Recall from Table 1
that we experimented with fourteen different con-
text definitions. A reliable effect of distance was
observed for all models (p &lt; .001). We used the
12 statistic to calculate the amount of variance ac-
counted for by the different models. Figure 3 plots
12 against the different contexts. The best result
was obtained for model 7 which accounts for 23.1%
of the variance (F(2,140) = 20.576, p &lt; .001) and
corresponds to the wide context specification and
the plain path value function. A reliable distance
effect was also observed for the traditional vector-
based model (F(2,138) = 9.384, p &lt; .001).
</bodyText>
<figureCaption confidence="0.985391">
Figure 3: η2 scores for mediated priming materials
</figureCaption>
<table confidence="0.9910285">
Model TarDirP – TarUnC TarMedP – TarUnC
Model 7 F = 25.290 (p &lt; .001) F = .001 (p = .790)
Traditional F = 12.185 (p = .001) F = .172 (p = .680)
L&amp;McD F = 24.105 (p &lt; .001) F = 13.107 (p &lt; .001)
</table>
<tableCaption confidence="0.999196">
Table 2: Size of direct and mediated priming effects
</tableCaption>
<bodyText confidence="0.99997806">
Pairwise ANOVAs were further performed to ex-
amine the size of the direct and mediated priming ef-
fects individually (see Table 2). There was a reliable
direct priming effect (F(1,94) = 25.290, p &lt; .001)
but we failed to find a reliable mediated priming
effect (F(1,93) = .001, p = .790). A reliable di-
rect priming effect (F(1,92) = 12.185, p = .001)
but no mediated priming effect was also obtained for
the traditional vector-based model. We used the η2
statistic to compare the effect sizes obtained for the
dependency-based and traditional model. The best
dependency-based model accounted for 23.1% of
the variance, whereas the traditional model ac-
counted for 12.2% (see also Table 2).
Our results indicate that dependency-based mod-
els are able to model direct priming across a wide
range of parameters. Our results also show that
larger contexts (see models 7 and 11 in Figure 3) are
more informative than smaller contexts (see mod-
els 1 and 3 in Figure 3), but note that the wide con-
text specification performed better than maximum. At
least for mediated priming, a uniform path value as
assigned by the plain path value function outper-
forms all other functions (see Figure 3).
Neither our dependency-based model nor the tra-
ditional model were able to replicate the mediated
priming effect reported by Lowe and McDonald
(2000) (see L &amp; McD in Table 2). This may be
due to differences in lemmatisation of the BNC,
the parametrisations of the model or the choice of
context words (Lowe and McDonald use a spe-
cial procedure to identify “reliable” context words).
Our results also differ from Livesay and Burgess
(1997) who found that mediated primes were fur-
ther from their targets than unrelated controls, us-
ing however a model and corpus different from the
ones we employed for our comparative studies. In
the dependency-based model, mediated primes were
virtually indistinguishable from unrelated words.
In sum, our results indicate that a model which
takes syntactic information into account outper-
forms a traditional vector-based model which sim-
ply relies on word occurrences. Our model is able
to reproduce the well-established direct priming ef-
fect but not the more controversial mediated prim-
ing effect. Our results point to the need for further
comparative studies among semantic space models
where variables such as corpus choice and size as
well as preprocessing (e.g., lemmatisation, tokeni-
sation) are controlled for.
</bodyText>
<subsectionHeader confidence="0.998337">
3.3 Experiment 2: Encoding of Relations
</subsectionHeader>
<bodyText confidence="0.999927714285714">
In this experiment we examine whether dependency-
based models construct a semantic space that encap-
sulates different lexical relations. More specifically,
we will assess whether word pairs capturing differ-
ent types of semantic relations (e.g., hyponymy, syn-
onymy) can be distinguished in terms of their dis-
tances in the semantic space.
</bodyText>
<subsectionHeader confidence="0.86638">
3.3.1 Materials and Design
</subsectionHeader>
<bodyText confidence="0.9999415">
Our experimental materials were taken from
Hodgson (1991) who in an attempt to investigate
which types of lexical relations induce priming col-
lected a set of 142 word pairs exemplifying the fol-
lowing semantic relations: (a) synonymy (words
with the same meaning, value and worth), (b) su-
perordination and subordination (one word is an in-
stance of the kind expressed by the other word, pain
and sensation), (c) category coordination (words
which express two instances of a common super-
ordinate concept, truck and train), (d) antonymy
(words with opposite meaning, friend and enemy),
(e) conceptual association (the first word subjects
produce in free association given the other word,
leash and dog), and (f) phrasal association (words
which co-occur in phrases private and property).
The pairs were selected to be unambiguous exam-
ples of the relation type they instantiate and were
matched for frequency. The pairs cover a wide range
of parts of speech, like adjectives, verbs, and nouns.
</bodyText>
<figure confidence="0.995309357142857">
eta squared
0.25
0.15
0.05
0.2
0.1
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14
model
TarDirP -- TarMedP -- TarUnC
TarDirP -- TarUnC
TarMedP -- TarUnC
1 2 3 4 5 6 7 8 9 10 11 12 13 14
model
</figure>
<figureCaption confidence="0.999403">
Figure 4: η2 scores for the Hodgson materials
</figureCaption>
<table confidence="0.999277857142857">
Mean PA SUP CO ANT SYN
CA 16.25 x x x x
PA 15.13 x x
SUP 11.04
CO 10.45
ANT 10.07
SYN 8.87
</table>
<tableCaption confidence="0.9766325">
Table 3: Mean skew divergences and Tukey test re-
sults for model 7
</tableCaption>
<subsectionHeader confidence="0.929739">
3.3.2 Procedure
</subsectionHeader>
<bodyText confidence="0.999928">
As in Experiment 1, six words with low fre-
quencies (less than 100) were removed from the
materials. Vectors were computed for the re-
maining 278 words for both the traditional and
the dependency-based models, again with the
parametrisations detailed in Section 3.1. We calcu-
lated the semantic distance for every word pair, this
time using Skew divergence as distance measure.
</bodyText>
<sectionHeader confidence="0.951539" genericHeader="evaluation">
3.3.3 Results
</sectionHeader>
<bodyText confidence="0.999934940298508">
We carried out an ANOVA with the lexical rela-
tion as factor and the distance as dependent variable.
The lexical relation factor had six levels, namely the
relations detailed in Section 3.3.1. We found no ef-
fect of semantic distance for the traditional semantic
space model (F(5,141) = 1.481, p = .200). The η2
statistic revealed that only 5.2% of the variance was
accounted for. On the other hand, a reliable effect
of distance was observed for all dependency-based
models (p &lt; .001). Model 7 (wide context specifi-
cation and plain path value function) accounted for
the highest amount of variance in our data (20.3%).
Our results can be seen in Figure 4.
We examined whether there are any significant
differences among the six relations using Post-hoc
Tukey tests. The pairwise comparisons for model 7
are given in Table 3. The mean distances for concep-
tual associates (CA), phrasal associates (PA), super-
ordinates/subordinates (SUP), category coordinates
(CO), antonyms (ANT), and synonyms (SYN) are
also shown in Table 3. There is no significant differ-
ence between PA and CA, although SUP, CO, ANT,
and SYN, are all significantly different from CA (see
Table 3, where × indicates statistical significance,
a = .05). Furthermore, ANT and SYN are signifi-
cantly different from PA.
Kilgarriff and Yallop (2000) point out that man-
ually constructed taxonomies or thesauri are typ-
ically organised according to synonymy and hy-
ponymy for nouns and verbs and antonymy for ad-
jectives. They further argue that for automatically
constructed thesauri similar words are words that
either co-occur with each other or with the same
words. The relations SYN, SUP, CO, and ANT can be
thought of as representing taxonomy-related knowl-
edge, whereas CA and PA correspond to the word
clusters found in automatically constructed thesauri.
In fact an ANOVA reveals that the distinction be-
tween these two classes of relations can be made
reliably (F(1,136) = 15.347, p &lt; .001), after col-
lapsing SYN, SUP, CO, and ANT into one class and
CA and PA into another.
Our results suggest that dependency-based vector
space models can, at least to a certain degree, dis-
tinguish among different types of lexical relations,
while this seems to be more difficult for traditional
semantic space models. The Tukey test revealed that
category coordination is reliably distinguished from
all other relations and that phrasal association is re-
liably different from antonymy and synonymy. Tax-
onomy related relations (e.g., synonymy, antonymy,
hyponymy) can be reliably distinguished from con-
ceptual and phrasal association. However, no reli-
able differences were found between closely associ-
ated relations such as antonymy and synonymy.
Our results further indicate that context encoding
plays an important role in discriminating lexical re-
lations. As in Experiment 1 our best results were
obtained with the wide context specification. Also,
weighting schemes such as the obliqueness hierar-
chy length again decreased the model’s performance
(see conditions 2, 5, 9, and 13 in Figure 4), show-
ing that dependency relations contribute equally to
the representation of a word’s meaning. This points
to the fact that rich context encodings with a wide
range of dependency relations are promising for cap-
turing lexical semantic distinctions. However, the
</bodyText>
<figure confidence="0.983164111111111">
Hodgson skew divergence
eta squared 0.21
0.2
0.19
0.18
0.17
0.16
0.15
0.14
</figure>
<bodyText confidence="0.9985885">
performance for maximum context specification was
lower, which indicates that collapsing all depen-
dency relations is not the optimal method, at least
for the tasks attempted here.
</bodyText>
<sectionHeader confidence="0.999538" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999662853658537">
In this paper we presented a novel semantic space
model that enriches traditional vector-based models
with syntactic information. The model is highly gen-
eral and can be optimised for different tasks. It ex-
tends prior work on syntax-based models (Grefen-
stette, 1994; Lin, 1998), by providing a general
framework for defining context so that a large num-
ber of syntactic relations can be used in the construc-
tion of the semantic space.
Our approach differs from Lin (1998) in three
important ways: (a) by introducing dependency
paths we can capture non-immediate relationships
between words (i.e., between subjects and objects),
whereas Lin considers only local context (depen-
dency edges in our terminology); the semantic
space is therefore constructed solely from isolated
head/modifier pairs and their inter-dependencies are
not taken into account; (b) Lin creates the semantic
space from the set of dependency edges that are rel-
evant for a given word; by introducing dependency
labels and the path value function we can selectively
weight the importance of different labels (e.g., sub-
ject, object, modifier) and parametrize the space ac-
cordingly for different tasks; (c) considerable flexi-
bility is allowed in our formulation for selecting the
dimensions of the semantic space; the latter can be
words (see the leaves in Figure 1), parts of speech
or dependency edges; in Lin’s approach, it is only
dependency edges (features in his terminology) that
form the dimensions of the semantic space.
Experiment 1 revealed that the dependency-based
model adequately simulates semantic priming. Ex-
periment 2 showed that a model that relies on rich
context specifications can reliably distinguish be-
tween different types of lexical relations. Our re-
sults indicate that a number of NLP tasks could
potentially benefit from dependency-based models.
These are particularly relevant for word sense dis-
crimination, automatic thesaurus construction, auto-
matic clustering and in general similarity-based ap-
proaches to NLP.
</bodyText>
<sectionHeader confidence="0.999251" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999897546875">
Balota, David A. and Robert Lorch, Jr. 1986. Depth of au-
tomatic spreading activation: Mediated priming effects in
pronunciation but not in lexical decision. Journal of Ex-
perimental Psychology: Learning, Memory and Cognition
12(3):336–45.
Burnard, Lou. 1995. Users Guide for the British National Cor-
pus. British National Corpus Consortium, Oxford University
Computing Service.
Choi, Freddy, Peter Wiemer-Hastings, and Johanna Moore.
2001. Latent Semantic Analysis for text segmentation. In
Proceedings ofEMNLP 2001. Seattle, WA.
Dunning, Ted. 1993. Accurate methods for the statistics of sur-
prise and coincidence. Computational Linguistics 19:61–74.
Grefenstette, Gregory. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers.
Hodgson, James M. 1991. Informational constraints on pre-
lexical priming. Language and Cognitive Processes 6:169–
205.
Jones, Michael P. and James H. Martin. 1997. Contextual
spelling correction using Latent Semantic Analysis. In Pro-
ceedings of the ANLP 97.
Keenan, E. and B. Comrie. 1977. Noun phrase accessibility and
universal grammar. Linguistic Inquiry (8):62–100.
Kilgarriff, Adam and Colin Yallop. 2000. What’s in a thesaurus.
In Proceedings ofLREC 2000. pages 1371–1379.
Landauer, T. and S. Dumais. 1997. A solution to Platos prob-
lem: the latent semantic analysis theory of acquisition, in-
duction, and representation of knowledge. Psychological Re-
view 104(2):211–240.
Lee, Lillian. 1999. Measures of distributional similarity. In
Proceedings ofACL ’99. pages 25–32.
Lin, Dekang. 1998. Automatic retrieval and clustering of simi-
lar words. In Proceedings of COLING-ACL 1998. Montréal,
Canada, pages 768–511.
Lin, Dekang. 2001. LaTaT: Language and text analysis tools.
In J. Allan, editor, Proceedings of HLT 2001. Morgan Kauf-
mann, San Francisco.
Livesay, K. and C. Burgess. 1997. Mediated priming in high-
dimensional meaning space: What is &amp;quot;mediated&amp;quot; in mediated
priming? In Proceedings of COGSCI 1997. Lawrence Erl-
baum Associates.
Lowe, Will. 2001. Towards a theory of semantic space. In Pro-
ceedings of COGSCI 2001. Lawrence Erlbaum Associates,
pages 576–81.
Lowe, Will and Scott McDonald. 2000. The direct route: Medi-
ated priming in semantic space. In Proceedings of COGSCI
2000. Lawrence Erlbaum Associates, pages 675–80.
Lund, Kevin and Curt Burgess. 1996. Producing high-
dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instruments, and Computers
28:203–8.
McDonald, Scott. 2000. Environmental Determinants ofLexical
Processing Effort. Ph.D. thesis, University of Edinburgh.
Patel, Malti, John A. Bullinaria, and Joseph P. Levy. 1998. Ex-
tracting semantic representations from large text corpora. In
Proceedings of the 4th Neural Computation and Psychology
Workshop. London, pages 199–212.
Salton, G, A Wang, and C Yang. 1975. A vector-space model
for information retrieval. Journal of the American Society
for Information Science 18(613–620).
Schütze, Hinrich. 1998. Automatic word sense discrimination.
Computational Linguistics 24(1):97–124.
Tesnière, Lucien. 1959. Elements de syntaxe structurale.
Klincksieck, Paris.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872705">
<title confidence="0.999965">Constructing Semantic Space Models from Parsed Corpora</title>
<author confidence="0.999723">Sebastian Padó</author>
<affiliation confidence="0.9999105">Department of Computational Linguistics Saarland University</affiliation>
<address confidence="0.9720295">PO Box 15 11 50 66041 Saarbrücken, Germany</address>
<email confidence="0.997143">pado@coli.uni-sb.de</email>
<author confidence="0.998744">Mirella Lapata</author>
<affiliation confidence="0.9998875">Department of Computer Science University of Sheffield</affiliation>
<address confidence="0.975584">Regent Court, 211 Portobello Street Sheffield S1 4DP, UK</address>
<email confidence="0.997892">mlap@dcs.shef.ac.uk</email>
<abstract confidence="0.997869">Traditional vector-based models use word co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David A Balota</author>
<author>Robert Lorch</author>
</authors>
<title>Depth of automatic spreading activation: Mediated priming effects in pronunciation but not in lexical decision.</title>
<date>1986</date>
<journal>Journal of Experimental Psychology: Learning, Memory and Cognition</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="18775" citStr="Balota and Lorch (1986)" startWordPosition="3079" endWordPosition="3082"> it concentrates on the semantic similarity or dissimilarity between a prime and its target, and it is precisely this type of lexical relations that vectorbased models capture. In this experiment we focus on Balota and Lorch’s (1986) mediated priming study. In semantic priming transient presentation of a prime word like tiger directly facilitates pronunciation or lexical decision on a target word like lion. Mediated priming extends this paradigm by additionally allowing indirectly related words as primes – like stripes, which is only related to lion by means of the intermediate concept tiger. Balota and Lorch (1986) obtained small mediated priming effects for pronunciation tasks but not for lexical decision. For the pronunciation task, reaction times were reduced significantly for both direct and mediated primes, however the effect was larger for direct primes. There are at least two semantic space simulations that attempt to shed light on the mediated priming effect. Lowe and McDonald (2000) replicated both the direct and mediated priming effects, whereas Livesay and Burgess (1997) could only replicate direct priming. In their study, mediated primes were farther from their targets than unrelated words. </context>
</contexts>
<marker>Balota, Lorch, 1986</marker>
<rawString>Balota, David A. and Robert Lorch, Jr. 1986. Depth of automatic spreading activation: Mediated priming effects in pronunciation but not in lexical decision. Journal of Experimental Psychology: Learning, Memory and Cognition 12(3):336–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>Users Guide for the British National Corpus.</title>
<date>1995</date>
<institution>British National Corpus Consortium, Oxford University Computing Service.</institution>
<contexts>
<context position="13814" citStr="Burnard, 1995" startWordPosition="2282" endWordPosition="2283">to give misleading results. Due to the Zipfian distribution of word types, words occurring with similar frequencies will be judged more similar than they actually are. A lexical association function can be used to explicitly factor out chance cooccurrences. Definition 10. Write A for the lexical association function which computes the value of a cell of the matrix from a co-occurrence frequency: Kij = A(fˆ(bi,tj)) 3 Evaluation 3.1 Parameter Settings All our experiments were conducted on the British National Corpus (BNC), a 100 million word collection of samples of written and spoken language (Burnard, 1995). We used Lin’s (1998) broad coverage dependency parser MINIPAR to obtain a parsed version of the corpus. MINIPAR employs a manually constructed grammar and a lexicon derived from WordNet with the addition of proper names (130,000 entries in total). Lexicon entries contain part-of-speech and subcategorization information. The grammar is represented as a network of 35 nodes (i.e., grammatical categories) and 59 edges (i.e., types of syntactic (dependency) relationships). MINIPAR uses a distributed chart parsing algorithm. Grammar rules are implemented as constraints associated with the nodes an</context>
</contexts>
<marker>Burnard, 1995</marker>
<rawString>Burnard, Lou. 1995. Users Guide for the British National Corpus. British National Corpus Consortium, Oxford University Computing Service.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Choi</author>
<author>Peter Wiemer-Hastings</author>
<author>Johanna Moore</author>
</authors>
<title>Latent Semantic Analysis for text segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings ofEMNLP</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="1004" citStr="Choi et al., 2001" startWordPosition="137" endWordPosition="140">ord co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations. 1 Introduction Vector-based models of word co-occurrence have proved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies of text under the assumption that the c</context>
</contexts>
<marker>Choi, Wiemer-Hastings, Moore, 2001</marker>
<rawString>Choi, Freddy, Peter Wiemer-Hastings, and Johanna Moore. 2001. Latent Semantic Analysis for text segmentation. In Proceedings ofEMNLP 2001. Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<pages>19--61</pages>
<contexts>
<context position="14981" citStr="Dunning, 1993" startWordPosition="2466" endWordPosition="2467">ted as constraints associated with the nodes and edges. Cosine distance cos(x,y) = �∑ix2�∑iy2 i i Skew divergence sα(x,y) = åixi log xi αxi+(1−α)yi Figure 2: Distance measures The dependency-based semantic space was constructed with the word-based path equivalence function from Section 2.3. As basis elements for our semantic space the 1000 most frequent words in the BNC were used. Each element of the resulting vector was replaced with its log-likelihood value (see Definition 10 in Section 2.3) which can be considered as an estimate of how surprising or distinctive a co-occurrence pair is (Dunning, 1993). We experimented with a variety of distance measures such as cosine, Euclidean distance, L1 norm, Jaccard’s coefficient, Kullback-Leibler divergence and the Skew divergence (see Lee 1999 for an overview). We obtained the best results for cosine (Experiment 1) and Skew divergence (Experiment 2). The two measures are shown in Figure 2. The Skew divergence represents a generalisation of the Kullback-Leibler divergence and was proposed by Lee (1999) as a linguistically motivated distance measure. We use a value of a = .99. We explored in detail the influence of different types and sizes of contex</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Dunning, Ted. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics 19:61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1114" citStr="Grefenstette, 1994" startWordPosition="152" endWordPosition="153">proach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations. 1 Introduction Vector-based models of word co-occurrence have proved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies of text under the assumption that the context surrounding a given word provides important information about its meaning. The semantic properties of w</context>
<context position="2379" citStr="Grefenstette, 1994" startWordPosition="341" endWordPosition="342">ucted from the observed distributional patterns of co-occurrence of their neighbouring words. Co-occurrence information is typically collected in a frequency matrix, where each row corresponds to a unique target word and each column represents its linguistic context. Contexts are defined as a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000) or as entire paragraphs, even documents (Landauer and Dumais, 1997). Context is typically treated as a set of unordered words, although in some cases syntactic information is taken into account (Lin, 1998; Grefenstette, 1994; Lee, 1999). A word can be thus viewed as a point in an n-dimensional semantic space. The semantic similarity between words can be then mathematically computed by measuring the distance between points in the semantic space using a metric such as cosine or Euclidean distance. In the variants of vector-based models where no linguistic knowledge is used, differences among parts of speech for the same word (e.g., to drink vs. a drink) are not taken into account in the construction of the semantic space, although in some cases word lexemes are used rather than word surface forms (Lowe and McDonald</context>
<context position="4112" citStr="Grefenstette, 1994" startWordPosition="617" endWordPosition="618">Some studies have tried to incorporate syntactic information into vector-based models. In this view, the semantic space is constructed from words that bear a syntactic relationship to the target word of interest. This makes semantic spaces more flexible, different types of contexts can be selected and words do not have to physically co-occur to be considered contextually relevant. However, existing models either concentrate on specific relations for constructing the semantic space such as objects (e.g., Lee, 1999) or collapse all types of syntactic relations available for a given target word (Grefenstette, 1994; Lin, 1998). Although syntactic information is now used to select a word’s appropriate contexts, this information is not explicitly captured in the contexts themselves (which are still represented by words) and is therefore not amenable to further processing. A commonly raised criticism for both types of semantic space models (i.e., word-based and syntaxbased) concerns the notion of semantic similarity. Proximity between two words in the semantic space cannot indicate the nature of the lexical relations between them. Distributionally similar words can be antonyms, synonyms, hyponyms or in som</context>
<context position="29737" citStr="Grefenstette, 1994" startWordPosition="4890" endWordPosition="4892">ncy relations are promising for capturing lexical semantic distinctions. However, the Hodgson skew divergence eta squared 0.21 0.2 0.19 0.18 0.17 0.16 0.15 0.14 performance for maximum context specification was lower, which indicates that collapsing all dependency relations is not the optimal method, at least for the tasks attempted here. 4 Discussion In this paper we presented a novel semantic space model that enriches traditional vector-based models with syntactic information. The model is highly general and can be optimised for different tasks. It extends prior work on syntax-based models (Grefenstette, 1994; Lin, 1998), by providing a general framework for defining context so that a large number of syntactic relations can be used in the construction of the semantic space. Our approach differs from Lin (1998) in three important ways: (a) by introducing dependency paths we can capture non-immediate relationships between words (i.e., between subjects and objects), whereas Lin considers only local context (dependency edges in our terminology); the semantic space is therefore constructed solely from isolated head/modifier pairs and their inter-dependencies are not taken into account; (b) Lin creates </context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette, Gregory. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James M Hodgson</author>
</authors>
<title>Informational constraints on prelexical priming.</title>
<date>1991</date>
<journal>Language and Cognitive Processes</journal>
<volume>6</volume>
<pages>205</pages>
<contexts>
<context position="24179" citStr="Hodgson (1991)" startWordPosition="3969" endWordPosition="3970">mong semantic space models where variables such as corpus choice and size as well as preprocessing (e.g., lemmatisation, tokenisation) are controlled for. 3.3 Experiment 2: Encoding of Relations In this experiment we examine whether dependencybased models construct a semantic space that encapsulates different lexical relations. More specifically, we will assess whether word pairs capturing different types of semantic relations (e.g., hyponymy, synonymy) can be distinguished in terms of their distances in the semantic space. 3.3.1 Materials and Design Our experimental materials were taken from Hodgson (1991) who in an attempt to investigate which types of lexical relations induce priming collected a set of 142 word pairs exemplifying the following semantic relations: (a) synonymy (words with the same meaning, value and worth), (b) superordination and subordination (one word is an instance of the kind expressed by the other word, pain and sensation), (c) category coordination (words which express two instances of a common superordinate concept, truck and train), (d) antonymy (words with opposite meaning, friend and enemy), (e) conceptual association (the first word subjects produce in free associa</context>
</contexts>
<marker>Hodgson, 1991</marker>
<rawString>Hodgson, James M. 1991. Informational constraints on prelexical priming. Language and Cognitive Processes 6:169– 205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael P Jones</author>
<author>James H Martin</author>
</authors>
<title>Contextual spelling correction using Latent Semantic Analysis.</title>
<date>1997</date>
<booktitle>In Proceedings of the ANLP 97.</booktitle>
<contexts>
<context position="1061" citStr="Jones and Martin, 1997" startWordPosition="144" endWordPosition="147">sent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations. 1 Introduction Vector-based models of word co-occurrence have proved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies of text under the assumption that the context surrounding a given word provides important inform</context>
</contexts>
<marker>Jones, Martin, 1997</marker>
<rawString>Jones, Michael P. and James H. Martin. 1997. Contextual spelling correction using Latent Semantic Analysis. In Proceedings of the ANLP 97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Keenan</author>
<author>B Comrie</author>
</authors>
<title>Noun phrase accessibility and universal grammar. Linguistic Inquiry</title>
<date>1977</date>
<contexts>
<context position="16673" citStr="Keenan and Comrie, 1977" startWordPosition="2737" endWordPosition="2740">mum context, (c) wide takes into account paths of length longer than 1 that represent meaningful linguistic relations such as argument structure, but also prepositional phrases and embedded clauses (in Figure 1 the wide context of apples is sweet, carry, lorry, and might), and (d) maximum combined all of the above into a rich context representation. Four path valuation functions were used: (a) plain assigns the same value to every path, (b) length assigns a value inversely proportional to a path’s length, (c) oblique ranks paths according to the obliqueness hierarchy of grammatical relations (Keenan and Comrie, 1977), and (d) oblength ∑ixiyi context specification path value function 1 minimum plain 2 minimum oblique 3 np plain 4 np length 5 np oblique 6 np oblength 7 wide plain 8 wide length 9 wide oblique 10 wide oblength 11 maximum plain 12 maximum length 13 maximum oblique 14 maximum oblength Table 1: The fourteen models combines length and oblique. The resulting 14 parametrisations are shown in Table 1. Lengthbased and length-neutral path value functions are collapsed for the minimum context specification since it only considers paths of length 1. We further compare in Experiments 1 and 2 our dependen</context>
</contexts>
<marker>Keenan, Comrie, 1977</marker>
<rawString>Keenan, E. and B. Comrie. 1977. Noun phrase accessibility and universal grammar. Linguistic Inquiry (8):62–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Colin Yallop</author>
</authors>
<title>What’s in a thesaurus.</title>
<date>2000</date>
<booktitle>In Proceedings ofLREC</booktitle>
<pages>1371--1379</pages>
<contexts>
<context position="27207" citStr="Kilgarriff and Yallop (2000)" startWordPosition="4491" endWordPosition="4494"> are any significant differences among the six relations using Post-hoc Tukey tests. The pairwise comparisons for model 7 are given in Table 3. The mean distances for conceptual associates (CA), phrasal associates (PA), superordinates/subordinates (SUP), category coordinates (CO), antonyms (ANT), and synonyms (SYN) are also shown in Table 3. There is no significant difference between PA and CA, although SUP, CO, ANT, and SYN, are all significantly different from CA (see Table 3, where × indicates statistical significance, a = .05). Furthermore, ANT and SYN are significantly different from PA. Kilgarriff and Yallop (2000) point out that manually constructed taxonomies or thesauri are typically organised according to synonymy and hyponymy for nouns and verbs and antonymy for adjectives. They further argue that for automatically constructed thesauri similar words are words that either co-occur with each other or with the same words. The relations SYN, SUP, CO, and ANT can be thought of as representing taxonomy-related knowledge, whereas CA and PA correspond to the word clusters found in automatically constructed thesauri. In fact an ANOVA reveals that the distinction between these two classes of relations can be</context>
</contexts>
<marker>Kilgarriff, Yallop, 2000</marker>
<rawString>Kilgarriff, Adam and Colin Yallop. 2000. What’s in a thesaurus. In Proceedings ofLREC 2000. pages 1371–1379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Landauer</author>
<author>S Dumais</author>
</authors>
<title>A solution to Platos problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="1494" citStr="Landauer and Dumais, 1997" startWordPosition="204" endWordPosition="207">iety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies of text under the assumption that the context surrounding a given word provides important information about its meaning. The semantic properties of words are represented by vectors that are constructed from the observed distributional patterns of co-occurrence of their neighbouring words. Co-occurrence information is typically collected in a frequency matrix, where each row corresponds to a unique target word and each column represents its linguistic context. Contexts are defined as a small number of words surrounding the t</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, T. and S. Dumais. 1997. A solution to Platos problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL ’99.</booktitle>
<pages>25--32</pages>
<contexts>
<context position="2391" citStr="Lee, 1999" startWordPosition="343" endWordPosition="344">ved distributional patterns of co-occurrence of their neighbouring words. Co-occurrence information is typically collected in a frequency matrix, where each row corresponds to a unique target word and each column represents its linguistic context. Contexts are defined as a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000) or as entire paragraphs, even documents (Landauer and Dumais, 1997). Context is typically treated as a set of unordered words, although in some cases syntactic information is taken into account (Lin, 1998; Grefenstette, 1994; Lee, 1999). A word can be thus viewed as a point in an n-dimensional semantic space. The semantic similarity between words can be then mathematically computed by measuring the distance between points in the semantic space using a metric such as cosine or Euclidean distance. In the variants of vector-based models where no linguistic knowledge is used, differences among parts of speech for the same word (e.g., to drink vs. a drink) are not taken into account in the construction of the semantic space, although in some cases word lexemes are used rather than word surface forms (Lowe and McDonald, 2000; McDo</context>
<context position="4013" citStr="Lee, 1999" startWordPosition="602" endWordPosition="603">this entails that contextual information contributes indiscriminately to a word’s meaning. Some studies have tried to incorporate syntactic information into vector-based models. In this view, the semantic space is constructed from words that bear a syntactic relationship to the target word of interest. This makes semantic spaces more flexible, different types of contexts can be selected and words do not have to physically co-occur to be considered contextually relevant. However, existing models either concentrate on specific relations for constructing the semantic space such as objects (e.g., Lee, 1999) or collapse all types of syntactic relations available for a given target word (Grefenstette, 1994; Lin, 1998). Although syntactic information is now used to select a word’s appropriate contexts, this information is not explicitly captured in the contexts themselves (which are still represented by words) and is therefore not amenable to further processing. A commonly raised criticism for both types of semantic space models (i.e., word-based and syntaxbased) concerns the notion of semantic similarity. Proximity between two words in the semantic space cannot indicate the nature of the lexical r</context>
<context position="15168" citStr="Lee 1999" startWordPosition="2493" endWordPosition="2494">y-based semantic space was constructed with the word-based path equivalence function from Section 2.3. As basis elements for our semantic space the 1000 most frequent words in the BNC were used. Each element of the resulting vector was replaced with its log-likelihood value (see Definition 10 in Section 2.3) which can be considered as an estimate of how surprising or distinctive a co-occurrence pair is (Dunning, 1993). We experimented with a variety of distance measures such as cosine, Euclidean distance, L1 norm, Jaccard’s coefficient, Kullback-Leibler divergence and the Skew divergence (see Lee 1999 for an overview). We obtained the best results for cosine (Experiment 1) and Skew divergence (Experiment 2). The two measures are shown in Figure 2. The Skew divergence represents a generalisation of the Kullback-Leibler divergence and was proposed by Lee (1999) as a linguistically motivated distance measure. We use a value of a = .99. We explored in detail the influence of different types and sizes of context by varying the context specification and path value functions. Contexts were defined over a set of 23 most frequent dependency relations which accounted for half of the dependency edges</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lee, Lillian. 1999. Measures of distributional similarity. In Proceedings ofACL ’99. pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>768--511</pages>
<location>Montréal, Canada,</location>
<contexts>
<context position="2359" citStr="Lin, 1998" startWordPosition="339" endWordPosition="340"> are constructed from the observed distributional patterns of co-occurrence of their neighbouring words. Co-occurrence information is typically collected in a frequency matrix, where each row corresponds to a unique target word and each column represents its linguistic context. Contexts are defined as a small number of words surrounding the target word (Lund and Burgess, 1996; Lowe and McDonald, 2000) or as entire paragraphs, even documents (Landauer and Dumais, 1997). Context is typically treated as a set of unordered words, although in some cases syntactic information is taken into account (Lin, 1998; Grefenstette, 1994; Lee, 1999). A word can be thus viewed as a point in an n-dimensional semantic space. The semantic similarity between words can be then mathematically computed by measuring the distance between points in the semantic space using a metric such as cosine or Euclidean distance. In the variants of vector-based models where no linguistic knowledge is used, differences among parts of speech for the same word (e.g., to drink vs. a drink) are not taken into account in the construction of the semantic space, although in some cases word lexemes are used rather than word surface form</context>
<context position="4124" citStr="Lin, 1998" startWordPosition="619" endWordPosition="620">ied to incorporate syntactic information into vector-based models. In this view, the semantic space is constructed from words that bear a syntactic relationship to the target word of interest. This makes semantic spaces more flexible, different types of contexts can be selected and words do not have to physically co-occur to be considered contextually relevant. However, existing models either concentrate on specific relations for constructing the semantic space such as objects (e.g., Lee, 1999) or collapse all types of syntactic relations available for a given target word (Grefenstette, 1994; Lin, 1998). Although syntactic information is now used to select a word’s appropriate contexts, this information is not explicitly captured in the contexts themselves (which are still represented by words) and is therefore not amenable to further processing. A commonly raised criticism for both types of semantic space models (i.e., word-based and syntaxbased) concerns the notion of semantic similarity. Proximity between two words in the semantic space cannot indicate the nature of the lexical relations between them. Distributionally similar words can be antonyms, synonyms, hyponyms or in some cases sema</context>
<context position="29749" citStr="Lin, 1998" startWordPosition="4893" endWordPosition="4894">omising for capturing lexical semantic distinctions. However, the Hodgson skew divergence eta squared 0.21 0.2 0.19 0.18 0.17 0.16 0.15 0.14 performance for maximum context specification was lower, which indicates that collapsing all dependency relations is not the optimal method, at least for the tasks attempted here. 4 Discussion In this paper we presented a novel semantic space model that enriches traditional vector-based models with syntactic information. The model is highly general and can be optimised for different tasks. It extends prior work on syntax-based models (Grefenstette, 1994; Lin, 1998), by providing a general framework for defining context so that a large number of syntactic relations can be used in the construction of the semantic space. Our approach differs from Lin (1998) in three important ways: (a) by introducing dependency paths we can capture non-immediate relationships between words (i.e., between subjects and objects), whereas Lin considers only local context (dependency edges in our terminology); the semantic space is therefore constructed solely from isolated head/modifier pairs and their inter-dependencies are not taken into account; (b) Lin creates the semantic</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, Dekang. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL 1998. Montréal, Canada, pages 768–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>LaTaT: Language and text analysis tools.</title>
<date>2001</date>
<booktitle>Proceedings of HLT 2001.</booktitle>
<editor>In J. Allan, editor,</editor>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<marker>Lin, 2001</marker>
<rawString>Lin, Dekang. 2001. LaTaT: Language and text analysis tools. In J. Allan, editor, Proceedings of HLT 2001. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Livesay</author>
<author>C Burgess</author>
</authors>
<title>Mediated priming in highdimensional meaning space: What is &amp;quot;mediated&amp;quot; in mediated priming?</title>
<date>1997</date>
<booktitle>In Proceedings of COGSCI</booktitle>
<contexts>
<context position="19251" citStr="Livesay and Burgess (1997)" startWordPosition="3153" endWordPosition="3156">g indirectly related words as primes – like stripes, which is only related to lion by means of the intermediate concept tiger. Balota and Lorch (1986) obtained small mediated priming effects for pronunciation tasks but not for lexical decision. For the pronunciation task, reaction times were reduced significantly for both direct and mediated primes, however the effect was larger for direct primes. There are at least two semantic space simulations that attempt to shed light on the mediated priming effect. Lowe and McDonald (2000) replicated both the direct and mediated priming effects, whereas Livesay and Burgess (1997) could only replicate direct priming. In their study, mediated primes were farther from their targets than unrelated words. 3.2.1 Materials and Design Materials were taken form Balota and Lorch (1986). They consist of 48 target words, each paired with a related and a mediated prime (e.g., lion-tigerstripes). Each related-mediated prime tuple was paired with an unrelated control randomly selected from the complement set of related primes. 3.2.2 Procedure One stimulus was removed as it had a low corpus frequency (less than 100), which meant that the resulting vector would be unreliable. We const</context>
<context position="22908" citStr="Livesay and Burgess (1997)" startWordPosition="3774" endWordPosition="3777">pecification performed better than maximum. At least for mediated priming, a uniform path value as assigned by the plain path value function outperforms all other functions (see Figure 3). Neither our dependency-based model nor the traditional model were able to replicate the mediated priming effect reported by Lowe and McDonald (2000) (see L &amp; McD in Table 2). This may be due to differences in lemmatisation of the BNC, the parametrisations of the model or the choice of context words (Lowe and McDonald use a special procedure to identify “reliable” context words). Our results also differ from Livesay and Burgess (1997) who found that mediated primes were further from their targets than unrelated controls, using however a model and corpus different from the ones we employed for our comparative studies. In the dependency-based model, mediated primes were virtually indistinguishable from unrelated words. In sum, our results indicate that a model which takes syntactic information into account outperforms a traditional vector-based model which simply relies on word occurrences. Our model is able to reproduce the well-established direct priming effect but not the more controversial mediated priming effect. Our re</context>
</contexts>
<marker>Livesay, Burgess, 1997</marker>
<rawString>Livesay, K. and C. Burgess. 1997. Mediated priming in highdimensional meaning space: What is &amp;quot;mediated&amp;quot; in mediated priming? In Proceedings of COGSCI 1997. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Lowe</author>
</authors>
<title>Towards a theory of semantic space.</title>
<date>2001</date>
<booktitle>In Proceedings of COGSCI 2001. Lawrence Erlbaum Associates,</booktitle>
<pages>576--81</pages>
<contexts>
<context position="7476" citStr="Lowe (2001)" startWordPosition="1157" endWordPosition="1158">ependency relations such as subj, obj, or aux (see Figure 1). The paths can be ranked by a path value function which gives different weight to different dependency types (for example, it can be argued that subjects and objects convey more semantic information than determiners). Target words are then represented in terms of syntactic features which form the dimensions of the semantic space. Paths are mapped to features by the path equivalence relation and the appropriate cells in the matrix are incremented. 2.1 Definition of Semantic Space We assume the semantic space formalisation proposed by Lowe (2001). A semantic space is a matrix whose rows correspond to target words and columns to dimensions which Lowe calls basis elements: Definition 1. A Semantic Space Model is a matrix K = B × T, where bZ E B denotes the basis element of column i, tj E T denotes the target word of row j, and KZj the cell (i, j). T is the set of words for which the matrix contains representations; this can be either word types or word tokens. In this paper, we assume that cooccurrence counts are constructed over word types, but the framework can be easily adapted to represent word tokens instead. Det a N Aux lorry migh</context>
<context position="13160" citStr="Lowe (2001)" startWordPosition="2177" endWordPosition="2178">syntactic relationship to the target that are taken into account. Once the value of all paths in the local context is determined, the local observed frequency for the co-occurrence of a basis element b with the target word w is just the sum of values of all paths f in this context which express the basis element b. The global observed frequency is the sum of the local observed frequencies for all occurrences of a target word type t and is therefore a measure for the cooccurrence of t and b over the whole corpus. Definition 9. Global observed frequency: fˆ(b,t) = ∑ ∑ v(f) wEW (t) φEC(w)Aφ—b As Lowe (2001) notes, raw frequency counts are likely to give misleading results. Due to the Zipfian distribution of word types, words occurring with similar frequencies will be judged more similar than they actually are. A lexical association function can be used to explicitly factor out chance cooccurrences. Definition 10. Write A for the lexical association function which computes the value of a cell of the matrix from a co-occurrence frequency: Kij = A(fˆ(bi,tj)) 3 Evaluation 3.1 Parameter Settings All our experiments were conducted on the British National Corpus (BNC), a 100 million word collection of </context>
</contexts>
<marker>Lowe, 2001</marker>
<rawString>Lowe, Will. 2001. Towards a theory of semantic space. In Proceedings of COGSCI 2001. Lawrence Erlbaum Associates, pages 576–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Lowe</author>
<author>Scott McDonald</author>
</authors>
<title>The direct route: Mediated priming in semantic space.</title>
<date>2000</date>
<booktitle>In Proceedings of COGSCI 2000. Lawrence Erlbaum Associates,</booktitle>
<pages>675--80</pages>
<contexts>
<context position="1443" citStr="Lowe and McDonald, 2000" startWordPosition="197" endWordPosition="200">ved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies of text under the assumption that the context surrounding a given word provides important information about its meaning. The semantic properties of words are represented by vectors that are constructed from the observed distributional patterns of co-occurrence of their neighbouring words. Co-occurrence information is typically collected in a frequency matrix, where each row corresponds to a unique target word and each column represents its linguistic context. Contexts are d</context>
<context position="2985" citStr="Lowe and McDonald, 2000" startWordPosition="442" endWordPosition="445">refenstette, 1994; Lee, 1999). A word can be thus viewed as a point in an n-dimensional semantic space. The semantic similarity between words can be then mathematically computed by measuring the distance between points in the semantic space using a metric such as cosine or Euclidean distance. In the variants of vector-based models where no linguistic knowledge is used, differences among parts of speech for the same word (e.g., to drink vs. a drink) are not taken into account in the construction of the semantic space, although in some cases word lexemes are used rather than word surface forms (Lowe and McDonald, 2000; McDonald, 2000). Minimal assumptions are made with respect to syntactic dependencies among words. In fact it is assumed that all context words within a certain distance from the target word are semantically relevant. The lack of syntactic information makes the building of semantic space models relatively straightforward and language independent (all that is needed is a corpus of written or spoken text). However, this entails that contextual information contributes indiscriminately to a word’s meaning. Some studies have tried to incorporate syntactic information into vector-based models. In t</context>
<context position="17803" citStr="Lowe and McDonald (2000)" startWordPosition="2923" endWordPosition="2926">ce it only considers paths of length 1. We further compare in Experiments 1 and 2 our dependency-based model against a state-of-the-art vector-based model where context is defined as a “bag of words”. Note that considerable latitude is allowed in setting parameters for vector-based models. In order to allow a fair comparison, we selected parameters for the traditional model that have been considered optimal in the literature (Patel et al., 1998), namely a symmetric 10 word window and the most frequent 500 content words from the BNC as dimensions. These parameters were similar to those used by Lowe and McDonald (2000) (symmetric 10 word window and 536 content words). Again the log-likelihood score is used to factor out chance co-occurrences. 3.2 Experiment 1: Priming A large number of modelling studies in psycholinguistics have focused on simulating semantic priming studies. The semantic priming paradigm provides a natural test bed for semantic space models as it concentrates on the semantic similarity or dissimilarity between a prime and its target, and it is precisely this type of lexical relations that vectorbased models capture. In this experiment we focus on Balota and Lorch’s (1986) mediated priming </context>
<context position="19159" citStr="Lowe and McDonald (2000)" startWordPosition="3140" endWordPosition="3143">on a target word like lion. Mediated priming extends this paradigm by additionally allowing indirectly related words as primes – like stripes, which is only related to lion by means of the intermediate concept tiger. Balota and Lorch (1986) obtained small mediated priming effects for pronunciation tasks but not for lexical decision. For the pronunciation task, reaction times were reduced significantly for both direct and mediated primes, however the effect was larger for direct primes. There are at least two semantic space simulations that attempt to shed light on the mediated priming effect. Lowe and McDonald (2000) replicated both the direct and mediated priming effects, whereas Livesay and Burgess (1997) could only replicate direct priming. In their study, mediated primes were farther from their targets than unrelated words. 3.2.1 Materials and Design Materials were taken form Balota and Lorch (1986). They consist of 48 target words, each paired with a related and a mediated prime (e.g., lion-tigerstripes). Each related-mediated prime tuple was paired with an unrelated control randomly selected from the complement set of related primes. 3.2.2 Procedure One stimulus was removed as it had a low corpus fr</context>
<context position="22619" citStr="Lowe and McDonald (2000)" startWordPosition="3723" endWordPosition="3726">ate that dependency-based models are able to model direct priming across a wide range of parameters. Our results also show that larger contexts (see models 7 and 11 in Figure 3) are more informative than smaller contexts (see models 1 and 3 in Figure 3), but note that the wide context specification performed better than maximum. At least for mediated priming, a uniform path value as assigned by the plain path value function outperforms all other functions (see Figure 3). Neither our dependency-based model nor the traditional model were able to replicate the mediated priming effect reported by Lowe and McDonald (2000) (see L &amp; McD in Table 2). This may be due to differences in lemmatisation of the BNC, the parametrisations of the model or the choice of context words (Lowe and McDonald use a special procedure to identify “reliable” context words). Our results also differ from Livesay and Burgess (1997) who found that mediated primes were further from their targets than unrelated controls, using however a model and corpus different from the ones we employed for our comparative studies. In the dependency-based model, mediated primes were virtually indistinguishable from unrelated words. In sum, our results in</context>
</contexts>
<marker>Lowe, McDonald, 2000</marker>
<rawString>Lowe, Will and Scott McDonald. 2000. The direct route: Mediated priming in semantic space. In Proceedings of COGSCI 2000. Lawrence Erlbaum Associates, pages 675–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing highdimensional semantic spaces from lexical co-occurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, and Computers</journal>
<pages>28--203</pages>
<contexts>
<context position="1417" citStr="Lund and Burgess, 1996" startWordPosition="193" endWordPosition="196">d co-occurrence have proved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies of text under the assumption that the context surrounding a given word provides important information about its meaning. The semantic properties of words are represented by vectors that are constructed from the observed distributional patterns of co-occurrence of their neighbouring words. Co-occurrence information is typically collected in a frequency matrix, where each row corresponds to a unique target word and each column represents its linguist</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Lund, Kevin and Curt Burgess. 1996. Producing highdimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instruments, and Computers 28:203–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott McDonald</author>
</authors>
<title>Environmental Determinants ofLexical Processing Effort.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="1373" citStr="McDonald, 2000" startWordPosition="188" endWordPosition="189">troduction Vector-based models of word co-occurrence have proved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies of text under the assumption that the context surrounding a given word provides important information about its meaning. The semantic properties of words are represented by vectors that are constructed from the observed distributional patterns of co-occurrence of their neighbouring words. Co-occurrence information is typically collected in a frequency matrix, where each row corresponds to a unique target </context>
<context position="2985" citStr="McDonald, 2000" startWordPosition="444" endWordPosition="445">te, 1994; Lee, 1999). A word can be thus viewed as a point in an n-dimensional semantic space. The semantic similarity between words can be then mathematically computed by measuring the distance between points in the semantic space using a metric such as cosine or Euclidean distance. In the variants of vector-based models where no linguistic knowledge is used, differences among parts of speech for the same word (e.g., to drink vs. a drink) are not taken into account in the construction of the semantic space, although in some cases word lexemes are used rather than word surface forms (Lowe and McDonald, 2000; McDonald, 2000). Minimal assumptions are made with respect to syntactic dependencies among words. In fact it is assumed that all context words within a certain distance from the target word are semantically relevant. The lack of syntactic information makes the building of semantic space models relatively straightforward and language independent (all that is needed is a corpus of written or spoken text). However, this entails that contextual information contributes indiscriminately to a word’s meaning. Some studies have tried to incorporate syntactic information into vector-based models. In t</context>
<context position="17803" citStr="McDonald (2000)" startWordPosition="2925" endWordPosition="2926">y considers paths of length 1. We further compare in Experiments 1 and 2 our dependency-based model against a state-of-the-art vector-based model where context is defined as a “bag of words”. Note that considerable latitude is allowed in setting parameters for vector-based models. In order to allow a fair comparison, we selected parameters for the traditional model that have been considered optimal in the literature (Patel et al., 1998), namely a symmetric 10 word window and the most frequent 500 content words from the BNC as dimensions. These parameters were similar to those used by Lowe and McDonald (2000) (symmetric 10 word window and 536 content words). Again the log-likelihood score is used to factor out chance co-occurrences. 3.2 Experiment 1: Priming A large number of modelling studies in psycholinguistics have focused on simulating semantic priming studies. The semantic priming paradigm provides a natural test bed for semantic space models as it concentrates on the semantic similarity or dissimilarity between a prime and its target, and it is precisely this type of lexical relations that vectorbased models capture. In this experiment we focus on Balota and Lorch’s (1986) mediated priming </context>
<context position="19159" citStr="McDonald (2000)" startWordPosition="3142" endWordPosition="3143">et word like lion. Mediated priming extends this paradigm by additionally allowing indirectly related words as primes – like stripes, which is only related to lion by means of the intermediate concept tiger. Balota and Lorch (1986) obtained small mediated priming effects for pronunciation tasks but not for lexical decision. For the pronunciation task, reaction times were reduced significantly for both direct and mediated primes, however the effect was larger for direct primes. There are at least two semantic space simulations that attempt to shed light on the mediated priming effect. Lowe and McDonald (2000) replicated both the direct and mediated priming effects, whereas Livesay and Burgess (1997) could only replicate direct priming. In their study, mediated primes were farther from their targets than unrelated words. 3.2.1 Materials and Design Materials were taken form Balota and Lorch (1986). They consist of 48 target words, each paired with a related and a mediated prime (e.g., lion-tigerstripes). Each related-mediated prime tuple was paired with an unrelated control randomly selected from the complement set of related primes. 3.2.2 Procedure One stimulus was removed as it had a low corpus fr</context>
<context position="22619" citStr="McDonald (2000)" startWordPosition="3725" endWordPosition="3726">dependency-based models are able to model direct priming across a wide range of parameters. Our results also show that larger contexts (see models 7 and 11 in Figure 3) are more informative than smaller contexts (see models 1 and 3 in Figure 3), but note that the wide context specification performed better than maximum. At least for mediated priming, a uniform path value as assigned by the plain path value function outperforms all other functions (see Figure 3). Neither our dependency-based model nor the traditional model were able to replicate the mediated priming effect reported by Lowe and McDonald (2000) (see L &amp; McD in Table 2). This may be due to differences in lemmatisation of the BNC, the parametrisations of the model or the choice of context words (Lowe and McDonald use a special procedure to identify “reliable” context words). Our results also differ from Livesay and Burgess (1997) who found that mediated primes were further from their targets than unrelated controls, using however a model and corpus different from the ones we employed for our comparative studies. In the dependency-based model, mediated primes were virtually indistinguishable from unrelated words. In sum, our results in</context>
</contexts>
<marker>McDonald, 2000</marker>
<rawString>McDonald, Scott. 2000. Environmental Determinants ofLexical Processing Effort. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malti Patel</author>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from large text corpora.</title>
<date>1998</date>
<booktitle>In Proceedings of the 4th Neural Computation and Psychology Workshop.</booktitle>
<pages>199--212</pages>
<location>London,</location>
<contexts>
<context position="17628" citStr="Patel et al., 1998" startWordPosition="2893" endWordPosition="2896">lique. The resulting 14 parametrisations are shown in Table 1. Lengthbased and length-neutral path value functions are collapsed for the minimum context specification since it only considers paths of length 1. We further compare in Experiments 1 and 2 our dependency-based model against a state-of-the-art vector-based model where context is defined as a “bag of words”. Note that considerable latitude is allowed in setting parameters for vector-based models. In order to allow a fair comparison, we selected parameters for the traditional model that have been considered optimal in the literature (Patel et al., 1998), namely a symmetric 10 word window and the most frequent 500 content words from the BNC as dimensions. These parameters were similar to those used by Lowe and McDonald (2000) (symmetric 10 word window and 536 content words). Again the log-likelihood score is used to factor out chance co-occurrences. 3.2 Experiment 1: Priming A large number of modelling studies in psycholinguistics have focused on simulating semantic priming studies. The semantic priming paradigm provides a natural test bed for semantic space models as it concentrates on the semantic similarity or dissimilarity between a prime</context>
</contexts>
<marker>Patel, Bullinaria, Levy, 1998</marker>
<rawString>Patel, Malti, John A. Bullinaria, and Joseph P. Levy. 1998. Extracting semantic representations from large text corpora. In Proceedings of the 4th Neural Computation and Psychology Workshop. London, pages 199–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wang</author>
<author>C Yang</author>
</authors>
<title>A vector-space model for information retrieval.</title>
<date>1975</date>
<journal>Journal of the American Society for Information Science</journal>
<volume>18</volume>
<issue>613</issue>
<contexts>
<context position="1171" citStr="Salton et al., 1975" startWordPosition="158" endWordPosition="161">ctic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations. 1 Introduction Vector-based models of word co-occurrence have proved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies of text under the assumption that the context surrounding a given word provides important information about its meaning. The semantic properties of words are represented by vectors that are constructed from</context>
</contexts>
<marker>Salton, Wang, Yang, 1975</marker>
<rawString>Salton, G, A Wang, and C Yang. 1975. A vector-space model for information retrieval. Journal of the American Society for Information Science 18(613–620).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schütze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="965" citStr="Schütze, 1998" startWordPosition="133" endWordPosition="134">aditional vector-based models use word co-occurrence counts from large corpora to represent lexical meaning. In this paper we present a novel approach for constructing semantic spaces that takes syntactic relations into account. We introduce a formalisation for this class of models and evaluate their adequacy on two modelling tasks: semantic priming and automatic discrimination of lexical relations. 1 Introduction Vector-based models of word co-occurrence have proved a useful representational framework for a variety of natural language processing (NLP) tasks such as word sense discrimination (Schütze, 1998), text segmentation (Choi et al., 2001), contextual spelling correction (Jones and Martin, 1997), automatic thesaurus extraction (Grefenstette, 1994), and notably information retrieval (Salton et al., 1975). Vector-based representations of lexical meaning have been also popular in cognitive science and figure prominently in a variety of modelling studies ranging from similarity judgements (McDonald, 2000) to semantic priming (Lund and Burgess, 1996; Lowe and McDonald, 2000) and text comprehension (Landauer and Dumais, 1997). In this approach semantic information is extracted from large bodies </context>
</contexts>
<marker>Schütze, 1998</marker>
<rawString>Schütze, Hinrich. 1998. Automatic word sense discrimination. Computational Linguistics 24(1):97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesnière</author>
</authors>
<title>Elements de syntaxe structurale.</title>
<date>1959</date>
<location>Klincksieck, Paris.</location>
<contexts>
<context position="6293" citStr="Tesnière, 1959" startWordPosition="950" endWordPosition="951">spaces. We next conduct a study that shows that our model is sensitive to different types of lexical relations. 2 Dependency-based Vector Space Models Once we move away from words as the basic context unit, the issue of representation of syntactic information becomes pertinent. Information about the dependency relations between words abstracts over word order and can be considered as an intermediate layer between surface syntax and semantics. More N apples Figure 1: A dependency parse of a short sentence formally, dependencies are asymmetric binary relationships between a head and a modifier (Tesnière, 1959). The structure of a sentence can be represented by a set of dependency relationships that form a tree as shown in Figure 1. Here the head of the sentence is the verb carry which is in turn modified by its subject lorry and its object apples. It is the dependencies in Figure 1 that will form the context over which the semantic space will be constructed. The construction mechanism sets out by identifying the local context of a target word, which is a subset of all dependency paths starting from it. The paths consist of the dependency edges of the tree labelled with dependency relations such as </context>
</contexts>
<marker>Tesnière, 1959</marker>
<rawString>Tesnière, Lucien. 1959. Elements de syntaxe structurale. Klincksieck, Paris.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>