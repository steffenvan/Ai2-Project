<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.659015">
Chunk-based Statistical Translation
</title>
<author confidence="0.602101">
Taro Watanabe†, Eiichiro Sumita† and Hiroshi G. Okuno$
</author>
<email confidence="0.894367">
{taro.watanabe, eiichiro.sumita}@atr.co.jp
</email>
<note confidence="0.3098345">
† ATR Spoken Language Translation $Department of Intelligence Science
Research Laboratories and Technology
2-2-2 Hikaridai, Keihanna Science City Graduate School of Informatics, Kyoto Uniersity
Kyoto 619-0288 JAPAN Kyoto 606-8501 JAPAN
</note>
<sectionHeader confidence="0.980586" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968272727273">
This paper describes an alternative trans-
lation model based on a text chunk un-
der the framework of statistical machine
translation. The translation model sug-
gested here first performs chunking. Then,
each word in a chunk is translated. Fi-
nally, translated chunks are reordered.
Under this scenario of translation model-
ing, we have experimented on a broad-
coverage Japanese-English traveling cor-
pus and achieved improved performance.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968046511628">
The framework of statistical machine translation for-
mulates the problem of translating a source sentence
in a language J into a target language E as the
maximization problem of the conditional probability
Eˆ = argmaxE P(E|J). The application of the Bayes
Rule resulted in Eˆ = argmaxE P(E)P(J|E). The for-
mer term P(E) is called a language model, repre-
senting the likelihood of E. The latter term P(J|E)
is called a translation model, representing the gener-
ation probability from E into J.
As an implementation of P(J|E), the word align-
ment based statistical translation (Brown et al.,
1993) has been successfully applied to similar lan-
guage pairs, such as French–English and German–
English, but not to drastically different ones, such
as Japanese–English. This failure has been due to
the limited representation by word alignment and
the weak model structure for handling complicated
word correspondence.
This paper provides a chunk-based statistical
translation as an alternative to the word alignment
based statistical translation. The translation process
inside the translation model is structured as follows.
A source sentence is first chunked, and then each
chunk is translated into target language with local
word alignments. Next, translated chunks are re-
ordered to match the target language constraints.
Based on this scenario, the chunk-based statis-
tical translation model is structured with several
components and trained by a variation of the EM-
algorithm. A translation experiment was carried
out with a decoder based on the left-to-right beam
search. It was observed that the translation quality
improved from 46.5% to 52.1% in BLEU score and
from 59.2% to 65.1% in subjective evaluation.
The next section briefly reviews the word align-
ment based statistical machine translation (Brown et
al., 1993). Section 3 discusses an alternative ap-
proach, a chunk-based translation model, ranging
from its structure to training procedure and decod-
ing algorithm. Then, Section 4 provides experimen-
tal results on Japanese-to-English translation in the
traveling domain, followed by discussion.
</bodyText>
<sectionHeader confidence="0.9941895" genericHeader="introduction">
2 Word Alignment Based Statistical
Translation
</sectionHeader>
<bodyText confidence="0.998319714285714">
Word alignment based statistical translation rep-
resents bilingual correspondence by the notion of
word alignment A, allowing one-to-many generation
from each source word. Figure 1 illustrates an exam-
ple of English and Japanese sentences, E and J, with
sample word alignments. In this example, “show1”
has generated two words, “mise5” and “tekudasai6”.
</bodyText>
<equation confidence="0.880607">
E = NULL0 show1 me2 the3 one4 in5 the6 window7
J = uindo1 no2 shinamono3 o4 mise5 tekudasai6
A = ( 7 0 4 0 1 1 )
</equation>
<figureCaption confidence="0.999387">
Figure 1: Example of word alignment
</figureCaption>
<bodyText confidence="0.998859">
Under this word alignment assumption, the transla-
tion model P(J|E) can be further decomposed with-
out approximation.
</bodyText>
<equation confidence="0.833469">
P(J|E) = Z P(J, A|E)
A
</equation>
<sectionHeader confidence="0.917486" genericHeader="method">
2.1 IBM Model
</sectionHeader>
<bodyText confidence="0.999328">
During the generation process from E to J, P(J, A|E)
is assumed to be structured with a couple of pro-
cesses, such as insertion, deletion and reorder. A
scenario for the word alignment based translation
model defined by Brown et al. (1993), for instance
IBM Model 4, goes as follows (refer to Figure 2).
</bodyText>
<listItem confidence="0.9598876875">
1. Choose the number of words to generate for
each source word according to the Fertility
Model. For example, “show” was increased to
2 words, while “me” was deleted.
2. Insert NULLs at appropriate positions by the
NULL Generation Model. Two NULLs were
inserted after each “show” in Figure 2.
3. Translate word-by-word for each generated
word by looking up the Lexicon Model. One of
the two “show” words was translated to “mise.”
4. Reorder the translated words by referring to the
Distortion Model. The word “mise” was re-
ordered to the 5th position, and “uindo” was
reordered to the 1st position. Positioning is de-
termined by the previous word’s alignment to
capture phrasal constraints.
</listItem>
<bodyText confidence="0.97533">
For the meanings of each symbol in each model, re-
fer to Brown et al. (1993).
</bodyText>
<subsectionHeader confidence="0.999106">
2.2 Problems of Word Alignment Based
Translation Model
</subsectionHeader>
<bodyText confidence="0.98936225">
The strategy for the word alignment based transla-
tion model is to translate each word by generating
multiple single words (a bag of words) and to deter-
mine the position of each translated word. Although
</bodyText>
<figureCaption confidence="0.876633">
Figure 2: Word alignment based translation model
P(J, A|E) (IBM Model 4)
</figureCaption>
<bodyText confidence="0.999586">
this procedure is sufficient to capture the bilingual
correspondence for similar language pairs, some is-
sues remain for drastically different pairs:
Insertion/Deletion Modeling Although deletion
was modeled in the Fertility Model, it merely as-
signs zero to each deleted word without considering
context. Similarly, inserted words are selected by
the Lexical Model parameter and inserted at the po-
sitions determined by a binomial distribution.
This insertion/deletion scheme contributed to the
simplicity of this representation of the translation
processes, allowing a sophisticated application to
run on an enormous bilingual sentence collection.
However, it is apparent that the weak modeling of
those phenomena will lead to inferior performance
for language pairs such as Japanese and English.
Local Alignment Modeling The IBM Model 4
(and 5) simulates phrasal constraints, although there
were implicitly implemented as its Distortion Model
parameters. In addition, the entire reordering is
determined by a collection of local reorderings in-
sufficient to capture the long-distance phrasal con-
straints.
The next section introduces an alternative model-
ing, chunk-based statistical translation, which was
intended to resolve the above two issues.
</bodyText>
<sectionHeader confidence="0.94122" genericHeader="method">
3 Chunk-based Statistical Translation
</sectionHeader>
<bodyText confidence="0.995655666666667">
Chunk-based statistical translation models the pro-
cess of chunking for both the source and target sen-
tences, E and J,
</bodyText>
<equation confidence="0.996086">
P(J|E) = Z P(J, J, E|E)
J E
</equation>
<bodyText confidence="0.96547">
where J and E are the chunked sentences for J
and E, respectively, defined as two-dimentional ar-
show1 show show mise uindo1
</bodyText>
<equation confidence="0.941424551724138">
me2 show NULL no no2
the3 one show tekudasai shinamono3
one4 window NULL o o4
in5 one shinamono mise5
the6 window uindo tekudasai6
window7
Distortion
Fertility
Lexicon
d1(1 − f 3 1 ~|E4, J1)
d1(3 − � 5+6
2 ~|E1, J3)
d1(5 − f 2+4
2 1|NULL, J5)
d&gt;1(6 − 5|J6)
(24)p41,,4-2 _,2
4-2p2
/� 0 1
n(2|E1)
n(0|E2)
n(0|E3)
...
NULL t(J5|E1)
t(J6|E1)
t(J3|E4)
...
in5 the6 window7
3
mise5 tekudasai6
</equation>
<figureCaption confidence="0.999437">
Figure 3: Example of chunk-based alignment
</figureCaption>
<bodyText confidence="0.997586875">
rays. For instance, Ji,j represents the jth word of
the ith chunk. The number of chunks for source
and target is assumed to be equal, |J |= |E|,
so that each chunk can convey a unit of meaning
without added/subtracted information. The term
P(J, J, E|E) is further decomposed by chunk align-
ment A and word alignment for each chunk transla-
tion A.
</bodyText>
<equation confidence="0.721758">
P(J, J, E|E) = � Z P(J, J, A, A, E|E)
A A
</equation>
<bodyText confidence="0.999985454545454">
The notion of alignment A is the same as those found
in the word alignment based translation model,
which assigns a source chunk index for each target
chunk. A is a two-dimensional array which assigns
a source word index for each target word per chunk.
For example, Figure 3 shows two-level alignments
taken from the example in Figure 1. The target
chunk at position 3, J3, “mise tekudasai” is aligned
to the first position (A3 = 1), and both the words
“mise” and “tekudasai” are aligned to the first posi-
tion of the source sentence (A3,1 = 1, A3,2 = 1).
</bodyText>
<subsectionHeader confidence="0.978106">
3.1 Translation Model Structure
</subsectionHeader>
<bodyText confidence="0.960147434782609">
The term P(J, J, A, A, E|E) is further decomposed
with approximation according to the scenario de-
scribed below (refer to Figure 4).
1. Perform chunking for source sentence E by
P(E|E). For instance, chunks of “show me” and
“the one” were derived. The process is mod-
eled by two steps:
(a) Selection of chunk size (Head Model).
For each word Ei, assign the chunk size
ϕi using the head model c(ϕi|Ei). A word
with chunk size more than 0 (ϕi &gt; 0) is
treated as a head word, otherwise a non-
head (refer to the words in bold in Figure
4).
(b) Associate each non-head word to a head
word (Chunk Model). Each non-head
word Ei is associated to a head word Eh by
the probability η(c(Eh)|h − i, c(Ei)), where
h is the position of a head word and c(E)
is a function to map a word E to its word
class (i.e. POS). For instance, “the3” is
associated with the head word “one4” lo-
cated at 4 − 3 = +1.
</bodyText>
<listItem confidence="0.55280624">
2. Select words to be translated with Deletion and
Fertility Model.
(a) Select the number of head words. For each
head word Eh (ϕh &gt; 0), choose fertility φh
according to the Fertility Model ν(φh|Eh).
We assume that the head word must be
translated, therefore φh &gt; 0. In addition,
one of them is selected as a head word at
target position using a uniform distribu-
tion 1/φh.
(b) Delete some non-head words. For
each non-head word Ei (ϕi = 0),
delete it according to the Deletion Model
δ(di|c(Ei), c(Eh)), where Eh is the head
word in the same chunk and di is 1 if Ei
is deleted, otherwise 0.
3. Insert some words. In Figure 4, NULLs were
inserted for two chunks. For each chunk Ei,
select the number of spurious words φJ by In-
sertion Model ι(φZ|c(Eh)), where Eh is the head
word of Ei.
4. Translate word-by-word. Each source word Ei,
including spurious words, is translated to Jj ac-
cording to the Lexicon Model, τ(Jj|Ei).
5. Reorder words. Each word in a chunk is
</listItem>
<bodyText confidence="0.995549777777778">
reordered according to the Reorder Model
P(Aj|EAj,Jj). The chunk reordering is taken
after the Distortion Model of IBM Model 4,
where the position is determined by the relative
position from the head word,
ρ(k − h|c(EAAj,k), c(Jj,k))
where h is the position of a head word for the
chunk Jj. For example, “no” is positioned −1
of “uindo”.
</bodyText>
<equation confidence="0.994078615384615">
E =
show1 me2 1
the3 one4
2
shinamono3 o4
uindo1 no2
J= uindo1 no2 1 shinamono3 o4 mise5 tekudasai6 3
2
A = ( 3 2 1 )
A = ( [ 7, 0 ] [ 4, 0 ] [ 1, 1 ] )
P(Aj|EAj, Jj) =
|Aj |
H
</equation>
<page confidence="0.673514">
k=1
</page>
<figure confidence="0.98888572">
window7 window window window
show1 show show
one4 one one one
me2 me show
the3 the NULL
the6 the
in5 in mise5
NULL
show
show
shinamono
tekudasai
uindo
mise
no
o
shinamono shinamono3
tekudasai no2
uindo
no
mise uindo1
o o4
tekudasai6
Chunking Deletion Insertion Lexicon Reorder Chunk
&amp; Fertility Reorder
</figure>
<figureCaption confidence="0.999817">
Figure 4: Chunk-based translation model. The words in bold are head words.
</figureCaption>
<listItem confidence="0.687611">
6. Reorder chunks. All of the chunks are
</listItem>
<bodyText confidence="0.9994364">
reordered according to the Chunk Reorder
Model, P(A|E,J). The chunk reordering is
also similar to the Distortion Model, where the
positioning is determined by the relative posi-
tion from the previous alignment
</bodyText>
<equation confidence="0.989661">
|J|
P(A|E, J) = H P(j − j�|c(EAj−1,h�), c(Jj,h))
j=1
</equation>
<bodyText confidence="0.998147166666667">
where j~ is the chunk alignment of the the pre-
vious chunk aEAj−1. h and h&apos; are the head word
indices for Jj and EAj−1, respectively. Note
that the reordering is dependent on head words.
To summarize, the chunk-based translation model
can be formulated as
</bodyText>
<equation confidence="0.963562357142857">
Z
P(J|E) =
E,J,A,A
H×
i:SOi=0
H× v(0i|Ei)/0i
i:SOi&gt;0
H×
i:SOi=0
H×
i:SOi&gt;0
H× P(Aj|EAj, Jj) × P(A|E,J)
j
.
</equation>
<subsectionHeader confidence="0.9514005">
3.2 Characteristics of chunk-based Translation
Model
</subsectionHeader>
<bodyText confidence="0.999991333333333">
The main difference to the word alignment based
translation model is the treatment of the bag of
word translations. The word alignment based trans-
lation model generates a bag of words for each
source word, while the chunk-based model con-
structs a set of target words from a set of source
words. The behavior is modeled as a chunking pro-
cedure by first associating words to the head word
of its chunk and then performing chunk-wise trans-
lation/insertion/deletion.
The complicated word alignment is handled by
the determination of word positions in two stages:
translation of chunk and chunk reordering. The for-
mer structures local orderings while the latter con-
stitutes global orderings. In addition, the concept of
head associated with each chunk plays the central
role in constraining different levels of the reordering
by the relative positions from heads.
</bodyText>
<subsectionHeader confidence="0.9933">
3.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999937045454545">
The parameter estimation for the chunk-based trans-
lation model relies on the EM-algorithm (Dempster
et al., 1977). Given a large bilingual corpus the
conditional probability of P(J, A, A, E|J, E) =
P(J, J, A, A, E|E)/ EJ,A,A,E P(J, J, A, A, E|E) is
first estimated for each pair of J and E (E-step),
then each model parameters is computed based
on the estimated conditional probability (M-step).
The above procedure is iterated until the set of
parameters converge.
However, this naive algorithm will suffer from se-
vere computational problems. The enumeration of
all possible chunkings J and E together with word
alignment A and chunk alignment A requires a sig-
nificant amount of computation. Therefore, we have
introduced a variation of the Inside-Outside algo-
rithm as seen in (Yamada and Knight, 2001) for E-
step computation. The details of the procedure are
described in Appendix A.
In addition to the computational problem, there
exists a local-maximum problem, where the EM-
Algorithm converges to a maximum solution but
</bodyText>
<equation confidence="0.9753725">
H c(SOi|Ei)
i
η(c(Ehi)|hi − i, c(Ei))
S(di|c(Ei), c(Ehi))
ι(0�i|c(Ei)) × H H τ(Jj,k|EAj,k)
j k
</equation>
<bodyText confidence="0.999948571428572">
does not guarantee finding the global maximum. In
order to solve this problem and to make the pa-
rameters converge quickly, IBM Model 4 parame-
ters were used as the initial parameters for training.
We directly applied the Lexicon Model and Fertility
Model to the chunk-based translation model but set
other parameters as uniform.
</bodyText>
<subsectionHeader confidence="0.953598">
3.4 Decoding
</subsectionHeader>
<bodyText confidence="0.999848428571429">
The decoding algorithm employed for this chunk-
based statistical translation is based on the beam
search algorithm for word alignment statistical
translation presented in (Tillmann and Ney, 2000),
which generates outputs in left-to-right order by
consuming input in an arbitrary order.
The decoder consists of two stages:
</bodyText>
<listItem confidence="0.996358">
1. Generate possible output chunks for all possi-
ble input chunks.
2. Generate hypothesized output by consuming
input chunks in arbitrary order and combining
possible output chunks in left-to-right order.
</listItem>
<bodyText confidence="0.960719304347826">
The generation of possible output chunks is es-
timated through an inverted lexicon model and
sequences of inserted strings (Tillmann and Ney,
2000). In addition, an example-based method is
also introduced, which generates candidate chunks
by looking up the viterbi chunking and alignment
from a training corpus.
Since the combination of all possible chunks is
computationally very expensive, we have introduced
the following pruning and scoring strategies.
beam pruning: Since the search space is enor-
mous, we have set up a size threshold to main-
tain partial hypotheses for both of the above
two stages. We also incorporated a threshold
for scoring, which allows partial hypotheses
with a certain score to be processed.
example-based scoring: Input/output chunk pairs
that appeared in a training corpus are “re-
warded” so that they are more likely kept in
the beam. During the decoding process, when
a pair of chunks appeared in the first stage, the
score is boosted by using this formula in the log
domain,
</bodyText>
<equation confidence="0.712307">
log Ptm(J|E) + log Plm(E)
</equation>
<tableCaption confidence="0.996811">
Table 1: Basic Travel Expression Corpus
</tableCaption>
<table confidence="0.8766595">
Japanese English
# of sentences 171,894
# of words 1,181,188 1,009,065
vocabulary size 20472 16232
# of singletons 82,06 5,854
3-gram perplexity 23.7 35.8
�+ weight × freq(EAj, Jj)
j
</table>
<bodyText confidence="0.9999494">
in which Ptm(J|E) and Plm(E) are translation
model and language model probability, respec-
tively1, freq(EAj,Jj) is the frequency for the
pair EAj and Jj appearing in the training cor-
pus, and weight is a tuning parameter.
</bodyText>
<sectionHeader confidence="0.999676" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.995854833333333">
The corpus for this experiment was extracted from
the Basic Travel Expression Corpus (BTEC), a col-
lection of conversational travel phrases for Japanese
and English (Takezawa et al., 2002) as seen in Ta-
ble 1. The entire corpus was split into three parts:
152,169 sentences for training, 4,846 sentences for
testing, and the remaining 10,148 sentences for pa-
rameter tuning, such as the termination criteria for
the training iteration and the parameter tuning for
decoders.
Three translation systems were tested for compar-
ison:
model4: Word alignment based translation model,
IBM Model 4 with a beam search decoder.
chunk3: Chunk-based translation model, limiting
the maximum allowed chunk size to 3.
model3+: chunk3 with example-based chunk can-
didate generation.
Figure 5 shows some examples of viterbi chunking
and chunk alignment for chunk3.
Translations were carried out on 510 sentences se-
lected randomly from the test set and evaluated ac-
cording to the following criteria with 16 reference
sets.
</bodyText>
<footnote confidence="0.9917405">
WER: Word-error-rate, which penalizes the edit distance
against reference translations.
1For simplicity of notation, dependence on other variables
are omitted, such as J.
</footnote>
<equation confidence="0.9993005">
[ i * have] [ the * number ] [ of my * passport ]
[ * e][ * ] [ * ]
[ i * have ] [ a * stomach ache ][ please * give me ] [ some * medicine ]
[ * ] [ * ] [ * ] [ * ]
[ * i ] [ * ’d ] [*like] [a*table] [ * for ] [ * two ] [ by the * window ] [ * if possible]
[ * ][ ][ * ][ * ][ * ][ * ] [ * ] [ * ]
[ i ∗ have ] [ a ∗ reservation ] [ ∗ for ] [ two ∗ nights ] [ my ∗ name is ] [ ∗ risa kobayashi ]
[ ∗ ] [ ∗ ] [ ∗ ][ ∗ ] [ ∗ ] [ ∗ ]
</equation>
<figureCaption confidence="0.9961135">
Figure 5: Examples of viterbi chunking and chunk alignment for English-to-Japanese translation model.
Chunks are bracketed and the words with ∗ to the left are head words.
</figureCaption>
<tableCaption confidence="0.8710525">
Table 2: Experimental results for Japanese–English
translation
</tableCaption>
<table confidence="0.954718714285714">
Model WER PER BLEU SE [%]
[%] [%] [%] A A+B A+B+C
model4 43.3 37.2 46.5 59.2 74.1 80.2
chunk3 40.9 36.1 48.4 59.8 73.5 78.8
chunk3+ 38.5 33.7 52.1 65.1 76.3 80.6
PER: Position independent WER, which penalizes without
considering positional disfluencies.
</table>
<figure confidence="0.736982">
input:
reference:
model4:
chunk3:
</figure>
<figureCaption confidence="0.564559">
is this all the baggage from flight one five two
is this all you baggage for flight one five two
is this all the baggage from flight one five two
</figureCaption>
<figure confidence="0.760558095238095">
input:
reference:
model4:
chunk3:
may i have room service for breakfast please
please give me some room service please
i ’d like room service for breakfast
hello i ’d like to change my reservation for march nineteenth
i ’d like to change my reservation for ninety days be march hello
hello i ’d like to change my reservation on march nineteenth
input:
reference:
model4:
chunk3:
input:
reference:
model4:
chunk3:
wait a couple of minutes i ’m telephoning now
is this the line is busy now a few minutes
i ’m on another phone now please wait a couple of minutes
</figure>
<figureCaption confidence="0.985015166666667">
BLEU: BLEU score, which computes the ratio of n-gram for
the translation results found in reference translations (Pa-
pineni et al., 2002).
SE: Subjective evaluation ranks ranging from A to D
(A:Perfect, B:Fair, C:Acceptable and D:Nonsense),
judged by native speakers.
</figureCaption>
<bodyText confidence="0.999914222222222">
Table 2 summarizes the evaluation of Japanese-to-
English translations, and Figure 6 presents some of
the results by model4 and chunk3+.
As Table 2 indicates, chunk3 performs better than
model4 in terms of the non-subjective evaluations,
although it scores almost equally in subjective eval-
uations. With the help of example-based decoding,
chunk3+ was evaluated as the best among the three
systems.
</bodyText>
<sectionHeader confidence="0.995407" genericHeader="method">
5 Discussion
</sectionHeader>
<footnote confidence="0.7141975">
The chunk-based translation model was originally
inspired by transfer-based machine translation but
modeled by chunks in order to capture syntax-based
correspondence. However, the structures evolved
into complicated modeling: The translation model
involves many stages, notably chunking and two
kinds of reordering, word-based and chunk-based
alignments. This is directly reflected in parameter
</footnote>
<figureCaption confidence="0.9015435">
Figure 6: Translation examples by word alignment
based model and chunk-based model
</figureCaption>
<bodyText confidence="0.999904036363637">
estimation, where chunk3 took 20 days for 40 iter-
ations, which is roughly the same amount of time
required for training IBM Model 5 with pegging.
The unit of chunk in the statistical machine
translation framework has been extensively dis-
cussed in the literature. Och et al. (1999) pro-
posed a translation template approach that com-
putes phrasal mappings from the viterbi align-
ments of a training corpus. Watanabe et al. (2002)
used syntax-based phrase alignment to obtain
chunks. Marcu and Wong (2002) argued for a dif-
ferent phrase-based translation modeling that di-
rectly induces a phrase-by-phrase lexicon model
from word-wise data. All of these methods bias
the training and/or decoding with phrase-level ex-
amples obtained by preprocessing a corpus (Och et
al., 1999; Watanabe et al., 2002) or by allowing a
lexicon model to hold phrases (Marcu and Wong,
2002). On the other hand, the chunk-based transla-
tion model holds the knowledge of how to construct
a sequence of chunks from a sequence of words. The
former approach is suitable for inputs with less de-
viation from a training corpus, while the latter ap-
proach will be able to perform well on unseen word
sequences, although chunk-based examples are also
useful for decoding to overcome the limited context
of a n-gram based language model.
Wang (1998) presented a different chunk-based
method by treating the translation model as a phrase-
to-string process. Yamada and Knight (2001) fur-
ther extended the model to a syntax-to-string trans-
lation modeling. Both assume that the source part
of a translation model is structured either with a se-
quence of chunks or with a parse tree, while our
method directly models a string-to-string procedure.
It is clear that the string-to-string modeling with hi-
den chunk-layers is computationally more expensive
than those structure-to-string models. However, the
structure-to-string approaches are already biased by
a monolingual chunking or parsing, which, in turn,
might not be able to uncover the bilingual phrasal or
syntactical constraints often observed in a corpus.
Alshawi et al. (2000) also presented a two-level
arranged word ordering and chunk ordering by a hi-
erarchically organized collection of finite state trans-
ducers. The main difference from our work is that
their approach is basically deterministic, while the
chunk-based translation model is non-deterministic.
The former method, of course, performs more ef-
ficient decoding but requires stronger heuristics to
generate a set of transducers. Although the latter
approach demands a large amount of decoding time
and hypothesis space, it can operate on a very broad-
coverage corpus with appropriate translation model-
ing.
</bodyText>
<sectionHeader confidence="0.996979" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999432">
The research reported here was supported in part by
a contract with the Telecommunications Advance-
ment Organization of Japan entitled “A study of
speech dialogue translation technology based on a
large corpus”.
</bodyText>
<sectionHeader confidence="0.997321" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.997599818181818">
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45–60.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
A. P. Dempster, N.M. Laird, and D.B.Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society,
B(39):1–38.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. ofEMNLP-2002, Philadelphia, PA, July.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proc. of EMNLP/WVLC, Univer-
sity of Maryland, College Park, MD, June.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002,
pages 311–318.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech
translation of travel conversations in the real world. In
Proc. ofLREC 2002, pages 147–152, Las Palmas, Ca-
nary Islands, Spain, May.
Christoph Tillmann and Hermann Ney. 2000. Word
re-ordering and dp-based search in statistical machine
translation. In Proc. of the COLING 2000, July-
August.
Ye-Yi Wang. 1998. Grammar Inference and Statis-
tical Machine Translation. Ph.D. thesis, School of
Computer Science, Language Technologies Institute,
Carnegie Mellon University.
Taro Watanabe, Kenji Imamura, and Eiichiro Sumita.
2002. Statistical machine translation based on hierar-
chical phrase alignment. In Proc. of TMI 2002, pages
188–198, Keihanna, Japan, March.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. of ACL 2001,
Toulouse, France.
</reference>
<sectionHeader confidence="0.989687333333333" genericHeader="method">
Appendix A Inside-Outside Algorithm for
Chunk-based Translation
Model
</sectionHeader>
<bodyText confidence="0.973098476190476">
The basic idea of inside-outside computation is to
separate the whole process into two parts, chunk
translation and chunk reordering. Chunk transla-
tion handles translation of each chunk, while chunk
reordering performs chunking and chunk reoprder-
ing. The inside (backward or beta) probabilities
can be derived, which represent the probability of
source/target paring of chunks and sentences. The
outside (forward or alpha) probabilities can be de-
fined as the probability of a particular source and
target pair appearing at a particular chunking and re-
ordering.
Inside Probability First, given E and J, compute
chunk translation inside probabilities for all the pos-
sible source and target chunks pairing Ei�
i and J j~
j in
which Ei�
i is the chunk ranging from index i to i~,
First, the counts for each model parameter θ with
associated random variables countθ(Θ) is
</bodyText>
<equation confidence="0.84433175">
�countθ(Θ) =
&lt;E,J&gt;
Pθ,(A&apos;, J j~
j , Ei�
</equation>
<bodyText confidence="0.761229333333333">
i )
.
Second, the count for chunk reordering with asso-
</bodyText>
<figure confidence="0.989471129032258">
ciated random variables count�(Θ) is
� α(Ei�
Θ(A�,Ei� i , Jj~
i ,Jj&apos; j )/β(E, J)
j )
�×
θ&apos;
α(E, J)/β(E, J)
P(A, J j~
j |Ei�
i )
H
θ
β(EAk, Jk)
Pθ(A�, J j~
j , Ei�
i )
�
P�(A|E, J)
k
z
Θ(A,E,J)
�
β(Ei�
i , J j~
j ) =
A&apos;
�=
A&apos;
�count�(Θ) =
&lt;E,J&gt;
</figure>
<bodyText confidence="0.969283">
where Pθ is the probability of a model with asso-
ciated values for corresponding random variables,
such as E(ϕi|Ei) or τ(Jj|Ei), except for the chunk re-
order model p. A&apos; is a word alignment for the chunks
Ei~ iand Jj&apos;
j .
Second, compute the inside probability for sen-
tence pairs E and J by considering all possible
chunkings and chunk alignments.
</bodyText>
<equation confidence="0.9891645">
β(E, J) = � z P(A, E,J, J|E)
E,J:|E|=|J |A
= L� z P(A|E, J) �
E,J:|E|=|J |A j
</equation>
<bodyText confidence="0.7686725">
Outside Probability The outside probability for
sentence pairing is always 1.
α(E, J) = 1.0
The outside probabilities for each chunk pair is
�
α(Ei�
i , J j~
j ) = α(E, J)
</bodyText>
<equation confidence="0.760778833333333">
E,J:|E|=|J|
�× β(EAk,Jk)
EAk#Ei�
i ,Jk#Jjr
j
.
</equation>
<bodyText confidence="0.99771208">
Inside-Outside Computation The combination
of the above inside-outside probabilities yields the
following formulas for the accumulated counts of
pair occurrences.
.
Approximation Even with the introduction of the
inside-outside parameter estimation paradigm, the
enumeration of all possible chunk pairing and word
alignment requires O(lmk4(k + 1)k) computations,
where l and m are sentence length for E and J, re-
spectively, and k is the maximum allowed number
of words per chunk. In addition, the enumeration
of all possible alignments for all possible chunked
sentences is O(2l2mn!), where n = |J |= |E|.
In order to handle the massive amount of compu-
tational demand, we have applied an approximation
to the inside-outside estimation procedure. First,
the enumeration of word alignment computation for
chunk translations was approximated by a set of
alignments, the viterbi alignment and neighboring
alignment through move/swap operations of partic-
ular word alignments.
Second, the chunk alignment enumeration was
also approximated by a set of chunking and chunk
alignments as follows.
</bodyText>
<listItem confidence="0.901777">
1. Determines the number of chunks per sentence
2. Determine initial chunking and alignment
3. Compute viterbi chunking-alignment via hill-climbing
using the following operators
• Move boundary of chunk
• Swap chunk alignment
• Move head position
4. Compute neighboring chunking-alignment using the
above operators
</listItem>
<figure confidence="0.973830333333333">
β(EAj, Jj)
z P(A|E, J)
A
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.369323">
<title confidence="0.930959">Chunk-based Statistical Translation</title>
<author confidence="0.672063">G Eiichiro</author>
<affiliation confidence="0.9880475">Spoken Language Translation of Intelligence Science Research Laboratories and Technology</affiliation>
<address confidence="0.7816385">2-2-2 Hikaridai, Keihanna Science City Graduate School of Informatics, Kyoto Uniersity Kyoto 619-0288 JAPAN Kyoto 606-8501 JAPAN</address>
<abstract confidence="0.993961">This paper describes an alternative translation model based on a text chunk under the framework of statistical machine translation. The translation model suggested here first performs chunking. Then, each word in a chunk is translated. Finally, translated chunks are reordered. Under this scenario of translation modeling, we have experimented on a broadcoverage Japanese-English traveling corpus and achieved improved performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Srinivas Bangalore</author>
<author>Shona Douglas</author>
</authors>
<title>Learning dependency translation models as collections of finite state head transducers.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="21870" citStr="Alshawi et al. (2000)" startWordPosition="3663" endWordPosition="3666"> to a syntax-to-string translation modeling. Both assume that the source part of a translation model is structured either with a sequence of chunks or with a parse tree, while our method directly models a string-to-string procedure. It is clear that the string-to-string modeling with hiden chunk-layers is computationally more expensive than those structure-to-string models. However, the structure-to-string approaches are already biased by a monolingual chunking or parsing, which, in turn, might not be able to uncover the bilingual phrasal or syntactical constraints often observed in a corpus. Alshawi et al. (2000) also presented a two-level arranged word ordering and chunk ordering by a hierarchically organized collection of finite state transducers. The main difference from our work is that their approach is basically deterministic, while the chunk-based translation model is non-deterministic. The former method, of course, performs more efficient decoding but requires stronger heuristics to generate a set of transducers. Although the latter approach demands a large amount of decoding time and hypothesis space, it can operate on a very broadcoverage corpus with appropriate translation modeling. Acknowl</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 2000. Learning dependency translation models as collections of finite state head transducers. Computational Linguistics, 26(1):45–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1416" citStr="Brown et al., 1993" startWordPosition="206" endWordPosition="209"> performance. 1 Introduction The framework of statistical machine translation formulates the problem of translating a source sentence in a language J into a target language E as the maximization problem of the conditional probability Eˆ = argmaxE P(E|J). The application of the Bayes Rule resulted in Eˆ = argmaxE P(E)P(J|E). The former term P(E) is called a language model, representing the likelihood of E. The latter term P(J|E) is called a translation model, representing the generation probability from E into J. As an implementation of P(J|E), the word alignment based statistical translation (Brown et al., 1993) has been successfully applied to similar language pairs, such as French–English and German– English, but not to drastically different ones, such as Japanese–English. This failure has been due to the limited representation by word alignment and the weak model structure for handling complicated word correspondence. This paper provides a chunk-based statistical translation as an alternative to the word alignment based statistical translation. The translation process inside the translation model is structured as follows. A source sentence is first chunked, and then each chunk is translated into t</context>
<context position="2638" citStr="Brown et al., 1993" startWordPosition="390" endWordPosition="393">et language with local word alignments. Next, translated chunks are reordered to match the target language constraints. Based on this scenario, the chunk-based statistical translation model is structured with several components and trained by a variation of the EMalgorithm. A translation experiment was carried out with a decoder based on the left-to-right beam search. It was observed that the translation quality improved from 46.5% to 52.1% in BLEU score and from 59.2% to 65.1% in subjective evaluation. The next section briefly reviews the word alignment based statistical machine translation (Brown et al., 1993). Section 3 discusses an alternative approach, a chunk-based translation model, ranging from its structure to training procedure and decoding algorithm. Then, Section 4 provides experimental results on Japanese-to-English translation in the traveling domain, followed by discussion. 2 Word Alignment Based Statistical Translation Word alignment based statistical translation represents bilingual correspondence by the notion of word alignment A, allowing one-to-many generation from each source word. Figure 1 illustrates an example of English and Japanese sentences, E and J, with sample word alignm</context>
<context position="3860" citStr="Brown et al. (1993)" startWordPosition="589" endWordPosition="592">s. In this example, “show1” has generated two words, “mise5” and “tekudasai6”. E = NULL0 show1 me2 the3 one4 in5 the6 window7 J = uindo1 no2 shinamono3 o4 mise5 tekudasai6 A = ( 7 0 4 0 1 1 ) Figure 1: Example of word alignment Under this word alignment assumption, the translation model P(J|E) can be further decomposed without approximation. P(J|E) = Z P(J, A|E) A 2.1 IBM Model During the generation process from E to J, P(J, A|E) is assumed to be structured with a couple of processes, such as insertion, deletion and reorder. A scenario for the word alignment based translation model defined by Brown et al. (1993), for instance IBM Model 4, goes as follows (refer to Figure 2). 1. Choose the number of words to generate for each source word according to the Fertility Model. For example, “show” was increased to 2 words, while “me” was deleted. 2. Insert NULLs at appropriate positions by the NULL Generation Model. Two NULLs were inserted after each “show” in Figure 2. 3. Translate word-by-word for each generated word by looking up the Lexicon Model. One of the two “show” words was translated to “mise.” 4. Reorder the translated words by referring to the Distortion Model. The word “mise” was reordered to th</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, B(39):1–38.</journal>
<contexts>
<context position="12438" citStr="Dempster et al., 1977" startWordPosition="2082" endWordPosition="2085">e head word of its chunk and then performing chunk-wise translation/insertion/deletion. The complicated word alignment is handled by the determination of word positions in two stages: translation of chunk and chunk reordering. The former structures local orderings while the latter constitutes global orderings. In addition, the concept of head associated with each chunk plays the central role in constraining different levels of the reordering by the relative positions from heads. 3.3 Parameter Estimation The parameter estimation for the chunk-based translation model relies on the EM-algorithm (Dempster et al., 1977). Given a large bilingual corpus the conditional probability of P(J, A, A, E|J, E) = P(J, J, A, A, E|E)/ EJ,A,A,E P(J, J, A, A, E|E) is first estimated for each pair of J and E (E-step), then each model parameters is computed based on the estimated conditional probability (M-step). The above procedure is iterated until the set of parameters converge. However, this naive algorithm will suffer from severe computational problems. The enumeration of all possible chunkings J and E together with word alignment A and chunk alignment A requires a significant amount of computation. Therefore, we have i</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N.M. Laird, and D.B.Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, B(39):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofEMNLP-2002,</booktitle>
<location>Philadelphia, PA,</location>
<contexts>
<context position="20284" citStr="Marcu and Wong (2002)" startWordPosition="3411" endWordPosition="3414">is is directly reflected in parameter Figure 6: Translation examples by word alignment based model and chunk-based model estimation, where chunk3 took 20 days for 40 iterations, which is roughly the same amount of time required for training IBM Model 5 with pegging. The unit of chunk in the statistical machine translation framework has been extensively discussed in the literature. Och et al. (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. Watanabe et al. (2002) used syntax-based phrase alignment to obtain chunks. Marcu and Wong (2002) argued for a different phrase-based translation modeling that directly induces a phrase-by-phrase lexicon model from word-wise data. All of these methods bias the training and/or decoding with phrase-level examples obtained by preprocessing a corpus (Och et al., 1999; Watanabe et al., 2002) or by allowing a lexicon model to hold phrases (Marcu and Wong, 2002). On the other hand, the chunk-based translation model holds the knowledge of how to construct a sequence of chunks from a sequence of words. The former approach is suitable for inputs with less deviation from a training corpus, while the</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proc. ofEMNLP-2002, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proc. of EMNLP/WVLC,</booktitle>
<institution>University of Maryland,</institution>
<location>College Park, MD,</location>
<contexts>
<context position="20064" citStr="Och et al. (1999)" startWordPosition="3376" endWordPosition="3379">tax-based correspondence. However, the structures evolved into complicated modeling: The translation model involves many stages, notably chunking and two kinds of reordering, word-based and chunk-based alignments. This is directly reflected in parameter Figure 6: Translation examples by word alignment based model and chunk-based model estimation, where chunk3 took 20 days for 40 iterations, which is roughly the same amount of time required for training IBM Model 5 with pegging. The unit of chunk in the statistical machine translation framework has been extensively discussed in the literature. Och et al. (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. Watanabe et al. (2002) used syntax-based phrase alignment to obtain chunks. Marcu and Wong (2002) argued for a different phrase-based translation modeling that directly induces a phrase-by-phrase lexicon model from word-wise data. All of these methods bias the training and/or decoding with phrase-level examples obtained by preprocessing a corpus (Och et al., 1999; Watanabe et al., 2002) or by allowing a lexicon model to hold phrases (Marcu and Wong, 2002). On the other han</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillmann, and Hermann Ney. 1999. Improved alignment models for statistical machine translation. In Proc. of EMNLP/WVLC, University of Maryland, College Park, MD, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>311--318</pages>
<contexts>
<context position="18765" citStr="Papineni et al., 2002" startWordPosition="3183" endWordPosition="3187">ease please give me some room service please i ’d like room service for breakfast hello i ’d like to change my reservation for march nineteenth i ’d like to change my reservation for ninety days be march hello hello i ’d like to change my reservation on march nineteenth input: reference: model4: chunk3: input: reference: model4: chunk3: wait a couple of minutes i ’m telephoning now is this the line is busy now a few minutes i ’m on another phone now please wait a couple of minutes BLEU: BLEU score, which computes the ratio of n-gram for the translation results found in reference translations (Papineni et al., 2002). SE: Subjective evaluation ranks ranging from A to D (A:Perfect, B:Fair, C:Acceptable and D:Nonsense), judged by native speakers. Table 2 summarizes the evaluation of Japanese-toEnglish translations, and Figure 6 presents some of the results by model4 and chunk3+. As Table 2 indicates, chunk3 performs better than model4 in terms of the non-subjective evaluations, although it scores almost equally in subjective evaluations. With the help of example-based decoding, chunk3+ was evaluated as the best among the three systems. 5 Discussion The chunk-based translation model was originally inspired b</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of ACL 2002, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiyuki Takezawa</author>
<author>Eiichiro Sumita</author>
<author>Fumiaki Sugaya</author>
<author>Hirofumi Yamamoto</author>
<author>Seiichi Yamamoto</author>
</authors>
<title>Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world.</title>
<date>2002</date>
<booktitle>In Proc. ofLREC 2002,</booktitle>
<pages>147--152</pages>
<location>Las Palmas, Canary Islands, Spain,</location>
<contexts>
<context position="15985" citStr="Takezawa et al., 2002" startWordPosition="2652" endWordPosition="2655">Expression Corpus Japanese English # of sentences 171,894 # of words 1,181,188 1,009,065 vocabulary size 20472 16232 # of singletons 82,06 5,854 3-gram perplexity 23.7 35.8 �+ weight × freq(EAj, Jj) j in which Ptm(J|E) and Plm(E) are translation model and language model probability, respectively1, freq(EAj,Jj) is the frequency for the pair EAj and Jj appearing in the training corpus, and weight is a tuning parameter. 4 Experiments The corpus for this experiment was extracted from the Basic Travel Expression Corpus (BTEC), a collection of conversational travel phrases for Japanese and English (Takezawa et al., 2002) as seen in Table 1. The entire corpus was split into three parts: 152,169 sentences for training, 4,846 sentences for testing, and the remaining 10,148 sentences for parameter tuning, such as the termination criteria for the training iteration and the parameter tuning for decoders. Three translation systems were tested for comparison: model4: Word alignment based translation model, IBM Model 4 with a beam search decoder. chunk3: Chunk-based translation model, limiting the maximum allowed chunk size to 3. model3+: chunk3 with example-based chunk candidate generation. Figure 5 shows some exampl</context>
</contexts>
<marker>Takezawa, Sumita, Sugaya, Yamamoto, Yamamoto, 2002</marker>
<rawString>Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya, Hirofumi Yamamoto, and Seiichi Yamamoto. 2002. Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world. In Proc. ofLREC 2002, pages 147–152, Las Palmas, Canary Islands, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Word re-ordering and dp-based search in statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proc. of the COLING 2000, JulyAugust.</booktitle>
<contexts>
<context position="13978" citStr="Tillmann and Ney, 2000" startWordPosition="2332" endWordPosition="2335">|Ei) i η(c(Ehi)|hi − i, c(Ei)) S(di|c(Ei), c(Ehi)) ι(0�i|c(Ei)) × H H τ(Jj,k|EAj,k) j k does not guarantee finding the global maximum. In order to solve this problem and to make the parameters converge quickly, IBM Model 4 parameters were used as the initial parameters for training. We directly applied the Lexicon Model and Fertility Model to the chunk-based translation model but set other parameters as uniform. 3.4 Decoding The decoding algorithm employed for this chunkbased statistical translation is based on the beam search algorithm for word alignment statistical translation presented in (Tillmann and Ney, 2000), which generates outputs in left-to-right order by consuming input in an arbitrary order. The decoder consists of two stages: 1. Generate possible output chunks for all possible input chunks. 2. Generate hypothesized output by consuming input chunks in arbitrary order and combining possible output chunks in left-to-right order. The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000). In addition, an example-based method is also introduced, which generates candidate chunks by looking up the viterbi chunk</context>
</contexts>
<marker>Tillmann, Ney, 2000</marker>
<rawString>Christoph Tillmann and Hermann Ney. 2000. Word re-ordering and dp-based search in statistical machine translation. In Proc. of the COLING 2000, JulyAugust.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
</authors>
<title>Grammar Inference and Statistical Machine Translation.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Computer Science, Language Technologies Institute, Carnegie Mellon University.</institution>
<contexts>
<context position="21092" citStr="Wang (1998)" startWordPosition="3547" endWordPosition="3548">ase-level examples obtained by preprocessing a corpus (Och et al., 1999; Watanabe et al., 2002) or by allowing a lexicon model to hold phrases (Marcu and Wong, 2002). On the other hand, the chunk-based translation model holds the knowledge of how to construct a sequence of chunks from a sequence of words. The former approach is suitable for inputs with less deviation from a training corpus, while the latter approach will be able to perform well on unseen word sequences, although chunk-based examples are also useful for decoding to overcome the limited context of a n-gram based language model. Wang (1998) presented a different chunk-based method by treating the translation model as a phraseto-string process. Yamada and Knight (2001) further extended the model to a syntax-to-string translation modeling. Both assume that the source part of a translation model is structured either with a sequence of chunks or with a parse tree, while our method directly models a string-to-string procedure. It is clear that the string-to-string modeling with hiden chunk-layers is computationally more expensive than those structure-to-string models. However, the structure-to-string approaches are already biased by </context>
</contexts>
<marker>Wang, 1998</marker>
<rawString>Ye-Yi Wang. 1998. Grammar Inference and Statistical Machine Translation. Ph.D. thesis, School of Computer Science, Language Technologies Institute, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Kenji Imamura</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Statistical machine translation based on hierarchical phrase alignment.</title>
<date>2002</date>
<booktitle>In Proc. of TMI 2002,</booktitle>
<pages>188--198</pages>
<location>Keihanna, Japan,</location>
<contexts>
<context position="20209" citStr="Watanabe et al. (2002)" startWordPosition="3400" endWordPosition="3403">nking and two kinds of reordering, word-based and chunk-based alignments. This is directly reflected in parameter Figure 6: Translation examples by word alignment based model and chunk-based model estimation, where chunk3 took 20 days for 40 iterations, which is roughly the same amount of time required for training IBM Model 5 with pegging. The unit of chunk in the statistical machine translation framework has been extensively discussed in the literature. Och et al. (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. Watanabe et al. (2002) used syntax-based phrase alignment to obtain chunks. Marcu and Wong (2002) argued for a different phrase-based translation modeling that directly induces a phrase-by-phrase lexicon model from word-wise data. All of these methods bias the training and/or decoding with phrase-level examples obtained by preprocessing a corpus (Och et al., 1999; Watanabe et al., 2002) or by allowing a lexicon model to hold phrases (Marcu and Wong, 2002). On the other hand, the chunk-based translation model holds the knowledge of how to construct a sequence of chunks from a sequence of words. The former approach i</context>
</contexts>
<marker>Watanabe, Imamura, Sumita, 2002</marker>
<rawString>Taro Watanabe, Kenji Imamura, and Eiichiro Sumita. 2002. Statistical machine translation based on hierarchical phrase alignment. In Proc. of TMI 2002, pages 188–198, Keihanna, Japan, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. of ACL</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="13128" citStr="Yamada and Knight, 2001" startWordPosition="2197" endWordPosition="2200">J, A, A, E|J, E) = P(J, J, A, A, E|E)/ EJ,A,A,E P(J, J, A, A, E|E) is first estimated for each pair of J and E (E-step), then each model parameters is computed based on the estimated conditional probability (M-step). The above procedure is iterated until the set of parameters converge. However, this naive algorithm will suffer from severe computational problems. The enumeration of all possible chunkings J and E together with word alignment A and chunk alignment A requires a significant amount of computation. Therefore, we have introduced a variation of the Inside-Outside algorithm as seen in (Yamada and Knight, 2001) for Estep computation. The details of the procedure are described in Appendix A. In addition to the computational problem, there exists a local-maximum problem, where the EMAlgorithm converges to a maximum solution but H c(SOi|Ei) i η(c(Ehi)|hi − i, c(Ei)) S(di|c(Ei), c(Ehi)) ι(0�i|c(Ei)) × H H τ(Jj,k|EAj,k) j k does not guarantee finding the global maximum. In order to solve this problem and to make the parameters converge quickly, IBM Model 4 parameters were used as the initial parameters for training. We directly applied the Lexicon Model and Fertility Model to the chunk-based translation </context>
<context position="21222" citStr="Yamada and Knight (2001)" startWordPosition="3564" endWordPosition="3567">con model to hold phrases (Marcu and Wong, 2002). On the other hand, the chunk-based translation model holds the knowledge of how to construct a sequence of chunks from a sequence of words. The former approach is suitable for inputs with less deviation from a training corpus, while the latter approach will be able to perform well on unseen word sequences, although chunk-based examples are also useful for decoding to overcome the limited context of a n-gram based language model. Wang (1998) presented a different chunk-based method by treating the translation model as a phraseto-string process. Yamada and Knight (2001) further extended the model to a syntax-to-string translation modeling. Both assume that the source part of a translation model is structured either with a sequence of chunks or with a parse tree, while our method directly models a string-to-string procedure. It is clear that the string-to-string modeling with hiden chunk-layers is computationally more expensive than those structure-to-string models. However, the structure-to-string approaches are already biased by a monolingual chunking or parsing, which, in turn, might not be able to uncover the bilingual phrasal or syntactical constraints o</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proc. of ACL 2001, Toulouse, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>