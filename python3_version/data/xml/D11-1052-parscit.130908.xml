<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000652">
<title confidence="0.8926665">
Cooooooooooooooollllllllllllll! !!!!!!!!!!!!!
Using Word Lengthening to Detect Sentiment in Microblogs
</title>
<author confidence="0.994039">
Samuel Brody
</author>
<affiliation confidence="0.939116333333333">
School of Communication
and Information
Rutgers University
</affiliation>
<email confidence="0.994929">
sdbrody@gmail.com
</email>
<author confidence="0.988441">
Nicholas Diakopoulos
</author>
<affiliation confidence="0.936121666666667">
School of Communication
and Information
Rutgers University
</affiliation>
<email confidence="0.998065">
diakop@rutgers.edu
</email>
<sectionHeader confidence="0.993879" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999932">
We present an automatic method which lever-
ages word lengthening to adapt a sentiment
lexicon specifically for Twitter and similar so-
cial messaging networks. The contributions of
the paper are as follows. First, we call at-
tention to lengthening as a widespread phe-
nomenon in microblogs and social messag-
ing, and demonstrate the importance of han-
dling it correctly. We then show that lengthen-
ing is strongly associated with subjectivity and
sentiment. Finally, we present an automatic
method which leverages this association to de-
tect domain-specific sentiment- and emotion-
bearing words. We evaluate our method by
comparison to human judgments, and analyze
its strengths and weaknesses. Our results are
of interest to anyone analyzing sentiment in
microblogs and social networks, whether for
research or commercial purposes.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999896357142857">
Recently, there has been a surge of interest in sen-
timent analysis of Twitter messages. Many re-
searchers (e.g., Bollen et al. 2011; Kivran-Swaine
and Naaman 2011) are interested in studying struc-
ture and interactions in social networks, where senti-
ment can play an important role. Others use Twitter
as a barometer for public mood and opinion in di-
verse areas such as entertainment, politics and eco-
nomics. For example, Diakopoulos and Shamma
(2010) use Twitter messages posted in conjunction
with the live presidential debate between Barack
Obama and John McCain to gauge public opinion,
Bollen et al. (2010) measure public mood on Twitter
and use it to predict upcoming stock market fluc-
</bodyText>
<page confidence="0.958874">
562
</page>
<bodyText confidence="0.999936333333333">
tuations, and O’Connor et al. (2010) connect pub-
lic opinion data from polls to sentiment expressed in
Twitter messages along a timeline.
A prerequisite of all such research is an effec-
tive method for measuring the sentiment of a post
or tweet. Due to the extremely informal nature of
the medium, and the length restriction1, the lan-
guage and jargon which is used in Twitter varies sig-
nificantly from that of commonly studied text cor-
pora. In addition, Twitter is a quickly evolving
domain, and new terms are constantly being intro-
duced. These factors pose difficulties to methods
designed for conventional domains, such as news.
One solution is to use human annotation. For exam-
ple, Kivran-Swaine and Naaman (2011) use manual
coding of tweets in several emotion categories (e.g.,
joy, sadness) for their research. Diakopoulos and
Shamma (2010) use crowd sourcing via Amazon’s
Mechanical Turk. Manual encoding usually offers
a deeper understanding and correspondingly higher
accuracy than shallow automatic methods. However,
it is expensive and labor intensive and cannot be ap-
plied in real time. Crowd-sourcing carries additional
caveats of its own, such as issues of annotator exper-
tise and reliability (see Diakopoulos and Shamma
2010).
The automatic approach to sentiment analysis is
commonly used for processing data from social net-
works and microblogs, where there is often a huge
quantity of information and a need for low latency.
Many automatic approaches (including all those
used in the work mentioned above) have at their core
a sentiment lexicon, containing a list of words la-
</bodyText>
<footnote confidence="0.995192">
1Messages in Twitter are limited to 140 characters, for com-
patibility with SMS messaging
</footnote>
<note confidence="0.9609775">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 562–570,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999713642857143">
beled with specific associated emotions (joy, hap-
piness) or a polarity value (positive, neutral, nega-
tive). The overall sentiment of a piece of text is cal-
culated as a function of the labels of the component
words. Because Twitter messages are short, shal-
low approaches are sometimes considered sufficient
(Bermingham and Smeaton, 2010). There are also
approaches that use deeper machine learning tech-
niques to train sentiment classifiers on examples that
have been labeled for sentiment, either manually or
automatically, as described above. Recent examples
of this approach are Barbosa and Feng (2010) and
Pak and Paroubek (2010).
Most established sentiment lexicons (e.g., Wilson
et al. 2005, see Section 5) were created for a gen-
eral domain, and suffer from limited coverage and
inaccuracies when applied to the highly informal do-
main of social networks communication. By cre-
ating a sentiment lexicon which is specifically tai-
lored to the microblogging domain, or adapting an
existing one, we can expect to achieve higher accu-
racy and increased coverage. Recent work in this
area includes Velikovich et al. (2010), who devel-
oped a method for automatically deriving an exten-
sive sentiment lexicon from the web as a whole.
The resulting lexicon has greatly increased cover-
age compared to existing dictionaries and can handle
spelling errors and web-specific jargon. Bollen et al.
(2010) expand an existing well-vetted psychometric
instrument - Profile of Mood States (POMS) (Mc-
Nair et al., 1971) that associates terms with moods
(e.g. calm, happy). The authors use co-occurrence
information from the Google n-gram corpus (Brants
and Franz, 2006) to enlarge the original list of 72
terms to 964. They use this expanded emotion lexi-
con (named GPOS) in conjunction with the lexicon
of Wilson et al. (2005) to estimate public mood from
Twitter posts2.
The method we present in this paper leverages a
phenomenon that is specific to informal social com-
munication to enable the extension of an existing
lexicon in a domain specific manner.
</bodyText>
<footnote confidence="0.730075333333333">
2Although the authors state that all data and methods will
be made available on a public website, it was not present at the
time of the writing of this article.
</footnote>
<sectionHeader confidence="0.974083" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999944772727273">
Prosodic indicators (such as high pitch, prolonged
duration, intensity, vowel quality, etc.) have long
been know (Bolinger, 1965) as ways for a speaker to
emphasize or accent an important word. The ways
in which they are used in speech are the subject of
ongoing linguistic research (see, for example, Cal-
houn 2010). In written text, many of these indica-
tors are lost. However, there exist some orthographic
conventions which are used to mark or substitute
for prosody, including punctuation and typographic
styling (italic, bold, and underlined text). In purely
text-based domains, such as Twitter, styling is not
always available, and is replaced by capitalization
or other conventions (e.g., enclosing the word in as-
terisks). Additionally, the informal nature of the do-
main leads to an orthographic style which is much
closer to the spoken form than in other, more formal,
domains. In this work, we hypothesize that the com-
monly observed phenomenon of lengthening words
by repeating letters is a substitute for prosodic em-
phasis (increased duration or change of pitch). As
such, it can be used as an indicator of important
words and, in particular, ones that bear strong in-
dication of sentiment.
Our experiments are designed to analyze the phe-
nomenon of lengthening and its implications to sen-
timent detection. First, in Experiment I, we show
the pervasiveness of the phenomenon in our dataset,
and measure the potential gains in coverage that can
be achieved by considering lengthening when pro-
cessing Twitter data. Experiment II substantiates
the claim that word lengthening is not arbitrary, and
is used for emphasis of important words, including
those conveying sentiment and emotion. In the first
part of Experiment III we demonstrate the implica-
tions of this connection for the purpose of sentiment
detection using an existing sentiment lexicon. In
the second part, we present an unsupervised method
for using the lengthening phenomenon to expand an
existing sentiment lexicon and tailor it to our do-
main. We evaluate the method through compari-
son to human judgments, analyze our results, and
demonstrate some of the benefits of our automatic
method.
</bodyText>
<page confidence="0.995569">
563
</page>
<listItem confidence="0.945109555555556">
1. For every word in the vocabulary, extract the condensed form, where sequences of a repeated
letter are replaced with a single instance of that letter.
E.g., niiiice → nice, realllly → realy ...
2. Create sets of words sharing the same condensed form.
E.g., {nice, niiice, niccccceee...}, {realy, really, realllly, ...} ...
3. Remove sets which do not contain at least one repeat of length three.
E.g.,{committee, committe, commitee}
4. Find the most frequently occurring form in the group, and mark it as the canonical form.
E.g., {nice, niiice, niccccceee...}, {realy, really, realllly, ...} ...
</listItem>
<figureCaption confidence="0.993746">
Figure 1: Procedure for detecting lengthened words and associating them with a canonical form.
</figureCaption>
<sectionHeader confidence="0.996186" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999903157894737">
Half a million tweets were sampled from the Twit-
ter Streaming API on March 9th 2011. The tweets
were sampled to cover a diverse geographic distri-
bution within the U.S. such that regional variation in
language use should not bias the data. Some tweets
were also sampled from Britain to provide a more
diverse sampling of English. We restricted our sam-
ple to tweets from accounts which indicated their
primary language as English. However, there may
be some foreign language messages in our dataset,
since multi-lingual users may tweet in other lan-
guages even though their account is marked as “En-
glish”.
The tweets were tokenized and converted to
lower-case. Punctuation, as well as links, hash-
tags, and username mentions were removed. The
resulting corpus consists of approximately 6.5 mil-
lion words, with a vocabulary of 22 thousand words
occurring 10 times or more.
</bodyText>
<sectionHeader confidence="0.986374" genericHeader="method">
4 Experiment I - Detection
</sectionHeader>
<bodyText confidence="0.999973731707317">
To detect and analyze lengthened words, we employ
the procedure described in Figure 1. We find sets
of words in our data which share a common form
and differ only in the number of times each letter is
repeated (Steps 1 &amp; 2). In Step 3 we remove sets
where all the different forms are likely to be the re-
sult of misspelling, rather than lengthening. Finally,
in Step 4, we associate all the forms in a single set
with a canonical form, which is the most common
one observed in the data.
The procedure resulted in 4,359 sets of size &gt; 1.
To reduce noise resulting from typos and mis-
spellings, we do not consider words containing non-
alphabetic characters, or sets where the canonical
form is a single character or occurs less than 10
times. This left us with 3,727 sets.
Analysis Table 1 lists the canonical forms of the
20 largest sets in our list (in terms of the number of
variations). Most of the examples are used to ex-
press emotion or emphasis. Onomatopoeic words
expressing emotion (e.g., ow, ugh, yay) are often
lengthened and, for some, the combined frequency
of the different lengthened forms is actually greater
than that of the canonical (single most frequent) one.
Lengthening is a common phenomenon in our
dataset. Out of half-a-million tweets, containing
roughly 6.5 million words, our procedure identifies
108,762 word occurrences which are lengthenings
of a canonical form. These words occur in 87,187
tweets (17.44% or approximately one out of every
six, on average). The wide-spread use of length-
ening is surprising in light of the length restriction
of Twitter. Grinter and Eldridge (2003) point out
several conventions that are used in text messages
specifically to deal with this restriction. The fact that
lengthening is used in spite of the need for brevity
suggests that it conveys important information.
Canonical Assumption We validate the assump-
tion that the most frequent form in the set is the
canonical form by examining sets containing one or
more word forms that were identified in a standard
</bodyText>
<page confidence="0.994344">
564
</page>
<table confidence="0.999833904761905">
Can. Form Card. # Can. # Non-Can.
nice 76 3847 348
ugh 75 1912 1057
lmao 70 10085 3727
lmfao 67 2615 1619
ah 61 767 1603
love 59 16360 359
crazy 59 3530 253
yeah 57 4562 373
sheesh 56 247 131
damn 52 5706 299
shit 51 10332 372
really 51 9142 142
oh 51 7114 1617
yay 45 1370 375
wow 45 3767 223
good 45 21042 3171
ow 44 116 499
mad 44 3627 827
hey 44 4669 445
please 43 4014 157
</table>
<tableCaption confidence="0.933063333333333">
Table 1: The canonical forms of the 20 largest sets (in
terms of cardinality), with the number of occurrences of
the canonical and non-canonical forms.
</tableCaption>
<bodyText confidence="0.9999399">
English dictionary3. This was the case for 2,092 of
the sets (56.13%). Of these, in only 55 (2.63%) the
most frequent form was not recognized by the dic-
tionary. This indicates that the strategy of choosing
the most frequent form as the canonical one is reli-
able and highly accurate (&gt; 97%).
Implications for NLP To examine the effects of
lengthening on analyzing Twitter data, we look at
the difference in coverage of a standard English dic-
tionary when we explicitly handle lengthened words
by mapping them to the canonical form. Cov-
erage with a standard dictionary is important for
many NLP applications, such as information re-
trieval, translation, part-of-speech tagging and pars-
ing. The canonical form for 2,037 word-sets are
identified by our dictionary. We searched for oc-
currences of these words which were lengthened by
two or more characters, meaning they would not
be identified using standard lemmatization methods
or spell-correction techniques that are based on edit
</bodyText>
<footnote confidence="0.82541">
3We use the standard dictionary for U.S. English included in
the Aspell Unix utility.
</footnote>
<bodyText confidence="0.9998655">
distance. We detected 25,101 occurrences of these,
appearing in 22,064 (4.4%) tweets. This implies that
a lengthening-aware stemming method can be used
to increase coverage substantially.
</bodyText>
<sectionHeader confidence="0.848826" genericHeader="method">
5 Experiment II - Relation to Sentiment
</sectionHeader>
<bodyText confidence="0.999949051282051">
At the beginning of Section 2 we presented the hy-
pothesis that lengthening represents a textual substi-
tute for prosodic indicators in speech. As such, it is
not used arbitrarily, but rather applied to subjective
words to strengthen the sentiment or emotion they
convey. The examples presented in Table 1 in the
previous section appear to support this hypothesis.
In this section we wish to provide experimental evi-
dence for our hypothesis, by demonstrating a signif-
icant degree of association between lengthening and
subjectivity.
For this purpose we use an existing sentiment lex-
icon (Wilson et al., 2005), which is commonly used
in the literature (see Section 1) and is at the core
of OpinionFinder4, a popular sentiment analysis tool
designed to determine opinion in a general domain.
The lexicon provides a list of subjective words, each
annotated with its degree of subjectivity (strongly
subjective, weakly subjective), as well as its sen-
timent polarity (positive, negative, or neutral). In
these experiments, we use the presence of a word
(canonical form) in the lexicon as an indicator of
subjectivity. It should be noted that the reverse is
not true, i.e., the fact that a word is absent from the
lexicon does not indicate it is objective.
As a measure of tendency to lengthen a word, we
look at the number of distinct forms of that word ap-
pearing in our dataset (the cardinality of the set to
which it belongs). We group the words according to
this statistic, and compare to the vocabulary of our
dataset (all words appearing in our data ten times
or more, and consisting of two or more alphabetic
characters, see Section 4). Figure 2 shows the per-
centage of subjective words (those in the lexicon) in
each of the groups. As noted previously, this is a
lower bound, since it is possible (in fact, very likely)
that other words in the group are subjective, despite
being absent from the lexicon. The graph shows a
clear trend - the more lengthening forms a word has,
</bodyText>
<footnote confidence="0.9973655">
4http://www.cs.pitt.edu/mpqa/
opinionfinderrelease/
</footnote>
<page confidence="0.99166">
565
</page>
<sectionHeader confidence="0.476745" genericHeader="method">
% Subjective
</sectionHeader>
<bodyText confidence="0.999224545454546">
the more likely it is to be subjective (as measured by
the percentage of words in the lexicon).
The reverse also holds - if a word is used to con-
vey sentiment, it is more likely to be lengthened. We
can verify this by calculating the average number of
distinct forms for words in our data that are sub-
jective and comparing to the rest. This calculation
yields an average of 2.41 forms for words appearing
in our sentiment lexicon (our proxy for subjectiv-
ity), compared to an average of 1.79 for those that
aren’t5. This difference is statistically significant at
p &lt; 0.01%, using a student t-test.
The lexicon we use was designed for a general
domain, and suffers from limited coverage (see be-
low) and inaccuracies (see O’Connor et al. 2010 and
below Section 6.2 for examples), due to the domain
shift. The sentiment lexicon contains 6,878 words,
but only 4,939 occur in our data, and only 2,446 ap-
pear more than 10 times. Of those appearing in our
data, only 485 words (7% of the lexicon vocabulary)
are lengthened (the bar for group 2+ in Figure 2),
but these are extremely salient. They encompass
701,607 instances (79% of total instances of words
from the lexicon), and 339,895 tweets. This pro-
vides further evidence that lengthening is used with
salient sentiment words.
These results also demonstrates the limitations of
using a sentiment lexicon which is not tailored to
the domain. Only a small fraction of the lexicon is
represented in our data, and it is likely that there are
many sentiment words that are commonly used but
are absent from it. We address this issue in the next
section.
</bodyText>
<sectionHeader confidence="0.990848" genericHeader="method">
6 Experiment III - Adapting the Sentiment
Lexicon
</sectionHeader>
<bodyText confidence="0.946328454545455">
The previous experiment showed the connection be-
tween lengthening and sentiment-bearing words. It
also demonstrated some of the shortcomings of a
lexicon which is not specifically tailored to our do-
main. There are two steps we can take to use
the lengthening phenomenon to adapt an existing
sentiment lexicon. The first of these is simply
to take lengthening into account when identifying
sentiment-bearing words in our corpus. The second
5This, too, is a conservative estimate, since the later group
also includes subjective words, as mentioned.
</bodyText>
<figure confidence="0.9854091">
30%
25%
20%
15%
10%
5%
All 2+ 3+ 4+ 5+ 6+ 7+ 8+ 9+ 10+
Number of Variations
All 2+ 3+ 4+ 5+ 6+ 7+ 8+ 9+ 10+
18,817 3,727 2,451 1,540 1,077 778 615 487 406 335
</figure>
<figureCaption confidence="0.6472056">
Figure 2: The percentage of subjective word-sets (those
whose canonical form appears in the lexicon) as a func-
tion of cardinality (number of lengthening variations).
The accompanying table provides the total number of sets
in each cardinality group.
</figureCaption>
<bodyText confidence="0.985944">
is to exploit the connection between lengthening and
sentiment to expand the lexicon itself.
</bodyText>
<subsectionHeader confidence="0.997974">
6.1 Expanding Coverage of Existing Words
</subsectionHeader>
<bodyText confidence="0.9999900625">
We can assess the effect of specifically consider-
ing lengthening in our domain by measuring the
increase of coverage of the existing sentiment lex-
icon. Similarly to Experiment I (Section 4), we
searched for occurrences of words from the lexi-
con which were lengthened by two or more charac-
ters, and would therefore not be detected using edit-
distance. We found 12,105 instances, occurring in
11,439 tweets (2.29% of the total). This increase in
coverage is relatively small6, but comes at almost no
cost, by simply considering lengthening in the anal-
ysis.
A much greater benefit of lengthening, however,
results from using it as an aid in expanding the sen-
timent lexicon and detecting new sentiment-bearing
words. This is the subject of the following section.
</bodyText>
<subsectionHeader confidence="0.998231">
6.2 Expanding the Sentiment Vocabulary
</subsectionHeader>
<bodyText confidence="0.9999536">
In Experiment II (Section 5) we showed that length-
ening is strongly associated with sentiment. There-
fore, words which are lengthened can provide us
with good candidates for inclusion in the lexicon.
We can employ existing sentiment-detection meth-
</bodyText>
<footnote confidence="0.715616">
6Note that almost half of the increase in coverage calculated
in Experiment I (Section 4) comes from subjective words!
0%
</footnote>
<page confidence="0.99638">
566
</page>
<bodyText confidence="0.99990428">
ods to decide which candidates to include, and de-
termine their polarity.
Choosing a Candidate Set The first step in ex-
panding the lexicon is to choose a set of candidate
words for inclusion. For this purpose we start with
words that have 5 or more distinct forms. There
are 1,077 of these, of which only 217 (20.15%)
are currently in our lexicon (see Figure 2). Since
we are looking for commonly lengthened words,
we disregard those where the combined frequency
of the non-canonical forms is less than 1% that of
the canonical one. We also remove stop words,
even though some are often lengthened for emphasis
(e.g., me, and, so), since they are too frequent, and
introduce many spurious edges in our co-occurrence
graph. Finally, we filter words based on weight, as
described below. This leaves us with 720 candidate
words.
Graph Approach We examine two methods for
sentiment detection - that of Brody and Elhadad
(2010) for detecting sentiment in reviews, and that of
Velikovich et al. (2010) for finding sentiment terms
in a giga-scale web corpus. Both of these employ
a graph-based approach, where candidate terms are
nodes, and sentiments is propagated from a set of
seed words of known sentiment polarity. We calcu-
lated the frequency in our corpus of all strongly pos-
itive and strongly negative words in the Wilson et al.
(2005) lexicon, and chose the 100 most frequent in
each category as our seed sets.
Graph Construction Brody and Elhadad (2010)
considered all frequent adjectives as candidates and
weighted the edge between two adjectives by a func-
tion of the number of times they both modified a
single noun. Velikovich et al. (2010) constructed
a graph where the nodes were 20 million candidate
words or phrases, selected using a set of heuristics
including frequency and mutual information of word
boundaries. Context vectors were constructed for
each candidate from all its mentions in a corpus of
4 billion documents, and the edge between two can-
didates was weighted by the cosine similarity be-
tween their context vectors.
Due to the nature of the domain, which is highly
informal and unstructured, accurate parsing is dif-
ficult. Therefore we cannot employ the exact con-
struction method of Brody and Elhadad (2010). On
the other hand, the method of Velikovich et al.
(2010) is based on huge amounts of data, and takes
advantage of the abundance of contextual informa-
tion available in full documents, whereas our do-
main is closer to that of Brody and Elhadad (2010),
who dealt with a small number of candidates and
short documents typical to online reviews. There-
fore, we adapt their construction method. We con-
sider all our candidates words as nodes, along with
the words in our positive and negative seed sets. As a
proxy for syntactic relationship, edges are weighted
as a function of the number of times two words oc-
curred within a three-word window of each other in
our dataset. We remove nodes whose neighboring
edges have a combined weight of less than 20, mean-
ing they participate in relatively few co-occurrence
relations with the other words in the graph.
Algorithm Once the graph is constructed, we can
use either of the propagation algorithms of Brody
and Elhadad (2010) and Velikovich et al. (2010),
which we will denote Reviews and Web, respec-
tively. The Reviews propagation method is based on
Zhu and Ghahramani (2002). The words in the posi-
tive and negative seed groups are assigned a polarity
score of 1 and 0, respectively. All the rest start with
a score of 0.5. Then, an update step is repeated. In
update iteration t, for each word x that is not in the
seed, the following update rule is applied:
</bodyText>
<equation confidence="0.998329333333333">
Ey∈N(x)w(y, x) · pt−1(y)
pt(x) = (1)
Ey∈N(x)w(y, x)
</equation>
<bodyText confidence="0.9993142">
Where pt(x) is the polarity of word x at step t, N(x)
is the set of the neighbors of x, and w(y, x) is the
weight of the edge connecting x and y. Following
Brody and Elhadad (2010), we set this weight to be
1 + log(#co(y, x)), where #co(y, x) is the number
of times y and x co-occurred within a three-word
window. The update step is repeated to convergence.
Velikovich et al. (2010) employed a different
label propagation method, as described in Fig-
ure 3. Rather than relying on diffusion along the
whole graph, this method considers only the sin-
gle strongest path between each candidate and each
seed word. In their paper, the authors claim that
their algorithm is more suitable than that of Zhu and
Ghahramani (2002) to a web-based dataset, which
</bodyText>
<page confidence="0.989461">
567
</page>
<bodyText confidence="0.861469">
Input: G = (V, E), wij E [0, 1]
</bodyText>
<equation confidence="0.540707">
P,N,y E R,T E N
Output: poli E R|V |
</equation>
<bodyText confidence="0.988435">
Initialize: poli, pol+i , pol-i = 0 for all i
pol+i = 1.0 for all vi E P and
pol-i = 1.0 for all vi E N
</bodyText>
<listItem confidence="0.990224153846154">
1: αij = 0 for all i =� j, αii = 1 for all i
2: for vi E P
3: F = {vi}
4: fort : 1...T
5: for (vk, vj) E E such that vk E F
6: αij = max(αij, αik - wk,j)
F=FU{vj}
7: for vj E V
8: pol+j = E vi∈P αij
9: Repeat steps 1-8 using N to compute pol-
10: β = Ei pol+i / Ei pol-i
11: poli = pol+i − βpol-i, for all i
12: if |poli |&lt; y then poli = 0.0 for all i
</listItem>
<figureCaption confidence="0.991833">
Figure 3: Web algorithm from Velikovich et al. (2010).
P and N are the positive and negative seed sets, respec-
tively, wij are the weights, and T and y are parameters9.
</figureCaption>
<bodyText confidence="0.999868375">
contained many dense subgraphs and unreliable as-
sociations based only on co-occurrence statistics.
We ran both algorithms in our experiment7, and
compared the results.
Evaluation We evaluated the output of the algo-
rithms by comparison to human judgments. For
words appearing in the sentiment lexicon, we used
the polarity label provided. For the rest, similarly
to Brody and Elhadad (2010), we asked volunteers
to rate the words on a five-point scale: strongly-
negative, weakly-negative, neutral, weakly-positive,
or strongly-positive. We also provided a N/A option
if the meaning of the word was unknown. Each word
was rated by two volunteers. Words which were la-
beled N/A by one or more annotators were consid-
ered unknown. For the rest, exact inter-rater agree-
</bodyText>
<footnote confidence="0.97429">
7We normalize the weights described above when using the
Web algorithm.
9In Velikovich et al. (2010), the parameters T and -y were
tuned on a held out dataset. Since our graphs are comparatively
small, we do not need to limit the path length T in our search.
We do not use the threshold -y, but rather employ a simple cutoff
of the top 50 words.
</footnote>
<table confidence="0.999349666666667">
Pos. Human Judgment Unk.
Neg. Neu.
Web Pos. 18 2 26 2
Neg. 8 19 17 8
Reviews Pos. 21 6 21 2
Neg. 9 14 11 16
</table>
<tableCaption confidence="0.986144666666667">
Table 2: Evaluation of the top 50 positive and negative
words retrieved by the two algorithms through compari-
son to human judgment.
</tableCaption>
<table confidence="0.999715411764706">
Web
pos. neg.
see shit
win niggas
way dis
gotta gettin
summer smh
lets tight
haha fuckin
birthday fuck
tomorrow sick
ever holy
school smfh
peace outta
soon odee
stuff wack
canes nigga
</table>
<tableCaption confidence="0.956752333333333">
Table 3: Top fifteen negative and positive words for the
algorithms of Brody and Elhadad (2010) (Reviews) and
Velikovich et al. (2010) (Web).
</tableCaption>
<bodyText confidence="0.999262285714286">
ment was 67.6%, but rose to 93% when considering
adjacent ratings as equivalent10. This is compara-
ble with the agreement reported by Brody and El-
hadad (2010). We assigned values 1 (strongly nega-
tive) to 5 (strongly positive) to the ratings, and cal-
culated the average between the two ratings for each
word. Words with an average rating of 3 were con-
sidered neutral, and those with lower and higher rat-
ings were considered negative and positive, respec-
tively.
Results Table 2 shows the distribution of the hu-
man labels among the top 50 most positive and most
negative words as determined by the two algorithms.
Table 3 lists the top 15 of these as examples.
</bodyText>
<figure confidence="0.985825277777778">
10Cohen’s Kappa κ = 0.853
Reviews
pos. neg.
kidding rell
justin whore
win rocks
feel ugg
finale naw
totally yea
awh headache
boys whack
pls yuck
ever shawty
yer yeah
lord sus
mike sleepy
three hunni
agreed sick
</figure>
<page confidence="0.989317">
568
</page>
<bodyText confidence="0.987728854545455">
From Table 2 we can see that both algorithms do
better on positive words (fewer words with reversed
polarity)11, and that the Web algorithm is more accu-
rate than the Reviews method. The difference in per-
formance can be explained by the associations used
by the algorithms. The Web algorithm takes into ac-
count the strongest path to every seed word, while
the Reviews algorithm propagates from the each
seed to its neighbors and then onward. This makes
the Reviews algorithm sensitive to strong associa-
tions between a word and a single seed. Because our
graph is constructed with co-occurrence edges be-
tween words, rather than syntactic relations between
adjectives, noisy edges are introduced, causing mis-
taken associations. The Web algorithm, on the other
hand, finds words that have a strong association with
the positive or negative seed group as a whole, thus
making it more robust. This explains some of the ex-
amples in Table 3. The words yeah and yea, which
often follow the negative seed word hell, are consid-
ered negative by the Reviews algorithm. The word
Justin refers to Justin Beiber, and is closely associ-
ated with the positive seed word love. Although the
Web algorithm is more robust to associations with
a single seed, it still misclassifies the word holy as
negative, presumably because it appears frequently
before several different expletives.
Detailed analysis shows that the numbers reported
in Table 2 are only rough estimates of performance.
For instance, several of the words in the unknown
category were correctly identified by the algorithm.
Examples include sm(f)h, which stands for “shak-
ing my (fucking) head” and expresses disgust or dis-
dain, sus, which is short for suspicious (as in “i hate
susssss ass cars thatfollow me/us when i’m/we walk-
inggg”), and odee, which means overdose and is
usually negative (though it does not always refers
to drugs, and is sometimes used as an intensifier,
e.g., “aint shit on tv odee bored”).
There were also cases were the human labels were
incorrect in the context of our domain. For exam-
ple, the word bull is listed as positive in the sen-
timent lexicon, presumably because of its financial
sense. In our domain it is (usually) short for bull-
shit. The word canes was rated as negative by one of
11This trend is not apparent from the top 15 results presented
in Table 3, but becomes noticeable when considering the larger
group.
the annotators, but in our data it refers to the Miami
Hurricanes, who won a game on the day our dataset
was sampled, and were the subject of many posi-
tive tweets. This example also demonstrates that our
method is capable of detecting terms which are asso-
ciated with sentiment at different time points, some-
thing that is not possible with a fixed lexicon.
</bodyText>
<sectionHeader confidence="0.998388" genericHeader="method">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999144533333334">
In this paper we explored the phenomenon of length-
ening words by repeating a single letter. We showed
that this is a common phenomenon in Twitter, oc-
curring in one of every six tweets, on average, in our
dataset. Correctly detecting these cases is important
for comprehensive coverage. We also demonstrated
that lengthening is not arbitrary, and is often used
with subjective words, presumably to emphasize the
sentiment they convey. This finding leads us to de-
velop an unsupervised method based on lengthening
for detecting new sentiment bearing words that are
not in the existing lexicon, and discovering their po-
larity. In the rapidly-changing domain of microblog-
ging and net-speak, such a method is essential for
up-to-date sentiment detection.
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="discussions">
8 Future Work
</sectionHeader>
<bodyText confidence="0.999952263157895">
This paper examined one aspect of the lengthening
phenomenon. There are other aspects of lengthen-
ing that merit research, such as the connection be-
tween the amount of lengthening and the strength of
emphasis in individual instances of a word. In addi-
tion to sentiment-bearing words, we saw other word
classes that were commonly lengthened, including
intensifiers (e.g., very, so, odee), and named enti-
ties associated with sentiment (e.g., Justin, ’Canes).
These present interesting targets for further study.
Also, in this work we focused on data in English,
and it would be interesting to examine the phe-
nomenon in other languages. Another direction of
research is the connection between lengthening and
other orthographic conventions associated with sen-
timent and emphasis, such as emoticons, punctua-
tion, and capitalization. Finally, we plan to integrate
lengthening and its related phenomena into an accu-
rate, Twitter-specific, sentiment classifier.
</bodyText>
<page confidence="0.997648">
569
</page>
<sectionHeader confidence="0.997371" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999578">
The authors would like to thank Paul Kantor and
Mor Naaman for their support and assistance in this
project. We would also like to thank Mark Steedman
for his help, and the anonymous reviewers for their
comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9988678625">
Barbosa, Luciano and Junlan Feng. 2010. Robust
sentiment detection on twitter from biased and
noisy data. In Chu-Ren Huang and Dan Jurafsky,
editors, COLING (Posters). Chinese Information
Processing Society of China, pages 36–44.
Bermingham, Adam and Alan F. Smeaton. 2010.
Classifying sentiment in microblogs: is brevity an
advantage? In Proceedings of the 19th ACM in-
ternational conference on Information and knowl-
edge management. ACM, New York, NY, USA,
CIKM ’10, pages 1833–1836.
Bolinger, Dwight. 1965. Forms of English: Accent,
Morpheme, Order. Harvard University Press,
Cambridge, Massachusetts, USA.
Bollen, J., H. Mao, and X.-J. Zeng. 2010. Twitter
mood predicts the stock market. ArXiv e-prints .
Bollen, Johan, Bruno Gonalves, Guangchen Ruan,
and Huina Mao. 2011. Happiness is assortative in
online social networks. Artificial Life 0(0):1–15.
Brants, Thorsten and Alex Franz. 2006. Google web
1T 5-gram corpus, version 1. Linguistic Data
Consortium, Catalog Number LDC2006T13.
Brody, Samuel and Noemie Elhadad. 2010. An un-
supervised aspect-sentiment model for online re-
views. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational
Linguistics (NAACL 2010). ACL, Los Angeles,
CA, pages 804–812.
Calhoun, Sasha. 2010. The centrality of metrical
structure in signaling information structure: A
probabilistic perspective. Language 86:1–42.
Diakopoulos, Nicholas A. and David A. Shamma.
2010. Characterizing debate performance via ag-
gregated twitter sentiment. In Proceedings of
the 28th international conference on Human fac-
tors in computing systems. ACM, New York, NY,
USA, CHI ’10, pages 1195–1198.
Grinter, Rebecca and Margery Eldridge. 2003.
Wan2tlk?: everyday text messaging. In Proceed-
ings of the SIGCHI conference on Human fac-
tors in computing systems. ACM, New York, NY,
USA, CHI ’03, pages 441–448.
Kivran-Swaine, Funda and Mor Naaman. 2011.
Network properties and social sharing of emo-
tions in social awareness streams. In Proceed-
ings of the 2011 ACM Conference on Com-
puter Supported Cooperative Work (CSCW 2011).
Hangzhou, China.
McNair, D. M., M. Lorr, and L. F. Droppleman.
1971. Profile of Mood States (POMS). Educa-
tional and Industrial Testing Service.
O’Connor, Brendan, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
International AAAI Conference on Weblogs and
Social Media.
Pak, Alexander and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Proceedings of the Seventh conference
on International Language Resources and Evalu-
ation (LREC’10). ELRA, Valletta, Malta.
Velikovich, Leonid, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The via-
bility of web-derived polarity lexicons. In Hu-
man Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics. ACL,
Stroudsburg, PA, USA, HLT ’10, pages 777–785.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the
conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing. ACL, Stroudsburg, PA, USA, HLT ’05, pages
347–354.
Zhu, X. and Z. Ghahramani. 2002. Learning from
labeled and unlabeled data with label propagation.
Technical report, CMU-CALD-02.
</reference>
<page confidence="0.996946">
570
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.417585">
<title confidence="0.938913">Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! Using Word Lengthening to Detect Sentiment in Microblogs</title>
<author confidence="0.997015">Samuel</author>
<affiliation confidence="0.91045">School of and Rutgers</affiliation>
<email confidence="0.994046">sdbrody@gmail.com</email>
<author confidence="0.989304">Nicholas</author>
<affiliation confidence="0.897548333333333">School of and Rutgers</affiliation>
<email confidence="0.99637">diakop@rutgers.edu</email>
<abstract confidence="0.9933623">We present an automatic method which leverages word lengthening to adapt a sentiment lexicon specifically for Twitter and similar social messaging networks. The contributions of the paper are as follows. First, we call attention to lengthening as a widespread phenomenon in microblogs and social messaging, and demonstrate the importance of handling it correctly. We then show that lengthening is strongly associated with subjectivity and sentiment. Finally, we present an automatic method which leverages this association to detect domain-specific sentimentand emotionbearing words. We evaluate our method by comparison to human judgments, and analyze its strengths and weaknesses. Our results are of interest to anyone analyzing sentiment in microblogs and social networks, whether for research or commercial purposes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In Chu-Ren Huang and</booktitle>
<pages>36--44</pages>
<editor>Dan Jurafsky, editors, COLING (Posters).</editor>
<contexts>
<context position="4307" citStr="Barbosa and Feng (2010)" startWordPosition="655" endWordPosition="658">ational Linguistics beled with specific associated emotions (joy, happiness) or a polarity value (positive, neutral, negative). The overall sentiment of a piece of text is calculated as a function of the labels of the component words. Because Twitter messages are short, shallow approaches are sometimes considered sufficient (Bermingham and Smeaton, 2010). There are also approaches that use deeper machine learning techniques to train sentiment classifiers on examples that have been labeled for sentiment, either manually or automatically, as described above. Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010). Most established sentiment lexicons (e.g., Wilson et al. 2005, see Section 5) were created for a general domain, and suffer from limited coverage and inaccuracies when applied to the highly informal domain of social networks communication. By creating a sentiment lexicon which is specifically tailored to the microblogging domain, or adapting an existing one, we can expect to achieve higher accuracy and increased coverage. Recent work in this area includes Velikovich et al. (2010), who developed a method for automatically deriving an extensive sentiment lexicon fro</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Barbosa, Luciano and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In Chu-Ren Huang and Dan Jurafsky, editors, COLING (Posters). Chinese Information Processing Society of China, pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Bermingham</author>
<author>Alan F Smeaton</author>
</authors>
<title>Classifying sentiment in microblogs: is brevity an advantage?</title>
<date>2010</date>
<journal>CIKM</journal>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management.</booktitle>
<volume>10</volume>
<pages>1833--1836</pages>
<publisher>ACM,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="4040" citStr="Bermingham and Smeaton, 2010" startWordPosition="615" endWordPosition="618"> words la1Messages in Twitter are limited to 140 characters, for compatibility with SMS messaging Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 562–570, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics beled with specific associated emotions (joy, happiness) or a polarity value (positive, neutral, negative). The overall sentiment of a piece of text is calculated as a function of the labels of the component words. Because Twitter messages are short, shallow approaches are sometimes considered sufficient (Bermingham and Smeaton, 2010). There are also approaches that use deeper machine learning techniques to train sentiment classifiers on examples that have been labeled for sentiment, either manually or automatically, as described above. Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010). Most established sentiment lexicons (e.g., Wilson et al. 2005, see Section 5) were created for a general domain, and suffer from limited coverage and inaccuracies when applied to the highly informal domain of social networks communication. By creating a sentiment lexicon which is specifically tailored </context>
</contexts>
<marker>Bermingham, Smeaton, 2010</marker>
<rawString>Bermingham, Adam and Alan F. Smeaton. 2010. Classifying sentiment in microblogs: is brevity an advantage? In Proceedings of the 19th ACM international conference on Information and knowledge management. ACM, New York, NY, USA, CIKM ’10, pages 1833–1836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dwight Bolinger</author>
</authors>
<title>Forms of English: Accent, Morpheme, Order.</title>
<date>1965</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="6043" citStr="Bolinger, 1965" startWordPosition="940" endWordPosition="941">ion lexicon (named GPOS) in conjunction with the lexicon of Wilson et al. (2005) to estimate public mood from Twitter posts2. The method we present in this paper leverages a phenomenon that is specific to informal social communication to enable the extension of an existing lexicon in a domain specific manner. 2Although the authors state that all data and methods will be made available on a public website, it was not present at the time of the writing of this article. 2 Methodology Prosodic indicators (such as high pitch, prolonged duration, intensity, vowel quality, etc.) have long been know (Bolinger, 1965) as ways for a speaker to emphasize or accent an important word. The ways in which they are used in speech are the subject of ongoing linguistic research (see, for example, Calhoun 2010). In written text, many of these indicators are lost. However, there exist some orthographic conventions which are used to mark or substitute for prosody, including punctuation and typographic styling (italic, bold, and underlined text). In purely text-based domains, such as Twitter, styling is not always available, and is replaced by capitalization or other conventions (e.g., enclosing the word in asterisks). </context>
</contexts>
<marker>Bolinger, 1965</marker>
<rawString>Bolinger, Dwight. 1965. Forms of English: Accent, Morpheme, Order. Harvard University Press, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bollen</author>
<author>H Mao</author>
<author>X-J Zeng</author>
</authors>
<title>Twitter mood predicts the stock market. ArXiv e-prints .</title>
<date>2010</date>
<contexts>
<context position="1748" citStr="Bollen et al. (2010)" startWordPosition="254" endWordPosition="257">troduction Recently, there has been a surge of interest in sentiment analysis of Twitter messages. Many researchers (e.g., Bollen et al. 2011; Kivran-Swaine and Naaman 2011) are interested in studying structure and interactions in social networks, where sentiment can play an important role. Others use Twitter as a barometer for public mood and opinion in diverse areas such as entertainment, politics and economics. For example, Diakopoulos and Shamma (2010) use Twitter messages posted in conjunction with the live presidential debate between Barack Obama and John McCain to gauge public opinion, Bollen et al. (2010) measure public mood on Twitter and use it to predict upcoming stock market fluc562 tuations, and O’Connor et al. (2010) connect public opinion data from polls to sentiment expressed in Twitter messages along a timeline. A prerequisite of all such research is an effective method for measuring the sentiment of a post or tweet. Due to the extremely informal nature of the medium, and the length restriction1, the language and jargon which is used in Twitter varies significantly from that of commonly studied text corpora. In addition, Twitter is a quickly evolving domain, and new terms are constant</context>
<context position="5092" citStr="Bollen et al. (2010)" startWordPosition="782" endWordPosition="785">d coverage and inaccuracies when applied to the highly informal domain of social networks communication. By creating a sentiment lexicon which is specifically tailored to the microblogging domain, or adapting an existing one, we can expect to achieve higher accuracy and increased coverage. Recent work in this area includes Velikovich et al. (2010), who developed a method for automatically deriving an extensive sentiment lexicon from the web as a whole. The resulting lexicon has greatly increased coverage compared to existing dictionaries and can handle spelling errors and web-specific jargon. Bollen et al. (2010) expand an existing well-vetted psychometric instrument - Profile of Mood States (POMS) (McNair et al., 1971) that associates terms with moods (e.g. calm, happy). The authors use co-occurrence information from the Google n-gram corpus (Brants and Franz, 2006) to enlarge the original list of 72 terms to 964. They use this expanded emotion lexicon (named GPOS) in conjunction with the lexicon of Wilson et al. (2005) to estimate public mood from Twitter posts2. The method we present in this paper leverages a phenomenon that is specific to informal social communication to enable the extension of an</context>
</contexts>
<marker>Bollen, Mao, Zeng, 2010</marker>
<rawString>Bollen, J., H. Mao, and X.-J. Zeng. 2010. Twitter mood predicts the stock market. ArXiv e-prints .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Bruno Gonalves</author>
<author>Guangchen Ruan</author>
<author>Huina Mao</author>
</authors>
<title>Happiness is assortative in online social networks.</title>
<date>2011</date>
<journal>Artificial Life</journal>
<volume>0</volume>
<issue>0</issue>
<contexts>
<context position="1269" citStr="Bollen et al. 2011" startWordPosition="178" endWordPosition="181">it correctly. We then show that lengthening is strongly associated with subjectivity and sentiment. Finally, we present an automatic method which leverages this association to detect domain-specific sentiment- and emotionbearing words. We evaluate our method by comparison to human judgments, and analyze its strengths and weaknesses. Our results are of interest to anyone analyzing sentiment in microblogs and social networks, whether for research or commercial purposes. 1 Introduction Recently, there has been a surge of interest in sentiment analysis of Twitter messages. Many researchers (e.g., Bollen et al. 2011; Kivran-Swaine and Naaman 2011) are interested in studying structure and interactions in social networks, where sentiment can play an important role. Others use Twitter as a barometer for public mood and opinion in diverse areas such as entertainment, politics and economics. For example, Diakopoulos and Shamma (2010) use Twitter messages posted in conjunction with the live presidential debate between Barack Obama and John McCain to gauge public opinion, Bollen et al. (2010) measure public mood on Twitter and use it to predict upcoming stock market fluc562 tuations, and O’Connor et al. (2010) </context>
</contexts>
<marker>Bollen, Gonalves, Ruan, Mao, 2011</marker>
<rawString>Bollen, Johan, Bruno Gonalves, Guangchen Ruan, and Huina Mao. 2011. Happiness is assortative in online social networks. Artificial Life 0(0):1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Google web 1T 5-gram corpus, version 1. Linguistic Data Consortium, Catalog Number LDC2006T13.</title>
<date>2006</date>
<contexts>
<context position="5351" citStr="Brants and Franz, 2006" startWordPosition="821" endWordPosition="824">r accuracy and increased coverage. Recent work in this area includes Velikovich et al. (2010), who developed a method for automatically deriving an extensive sentiment lexicon from the web as a whole. The resulting lexicon has greatly increased coverage compared to existing dictionaries and can handle spelling errors and web-specific jargon. Bollen et al. (2010) expand an existing well-vetted psychometric instrument - Profile of Mood States (POMS) (McNair et al., 1971) that associates terms with moods (e.g. calm, happy). The authors use co-occurrence information from the Google n-gram corpus (Brants and Franz, 2006) to enlarge the original list of 72 terms to 964. They use this expanded emotion lexicon (named GPOS) in conjunction with the lexicon of Wilson et al. (2005) to estimate public mood from Twitter posts2. The method we present in this paper leverages a phenomenon that is specific to informal social communication to enable the extension of an existing lexicon in a domain specific manner. 2Although the authors state that all data and methods will be made available on a public website, it was not present at the time of the writing of this article. 2 Methodology Prosodic indicators (such as high pit</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Brants, Thorsten and Alex Franz. 2006. Google web 1T 5-gram corpus, version 1. Linguistic Data Consortium, Catalog Number LDC2006T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Noemie Elhadad</author>
</authors>
<title>An unsupervised aspect-sentiment model for online reviews. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2010). ACL,</booktitle>
<pages>804--812</pages>
<location>Los Angeles, CA,</location>
<contexts>
<context position="20398" citStr="Brody and Elhadad (2010)" startWordPosition="3354" endWordPosition="3357">h only 217 (20.15%) are currently in our lexicon (see Figure 2). Since we are looking for commonly lengthened words, we disregard those where the combined frequency of the non-canonical forms is less than 1% that of the canonical one. We also remove stop words, even though some are often lengthened for emphasis (e.g., me, and, so), since they are too frequent, and introduce many spurious edges in our co-occurrence graph. Finally, we filter words based on weight, as described below. This leaves us with 720 candidate words. Graph Approach We examine two methods for sentiment detection - that of Brody and Elhadad (2010) for detecting sentiment in reviews, and that of Velikovich et al. (2010) for finding sentiment terms in a giga-scale web corpus. Both of these employ a graph-based approach, where candidate terms are nodes, and sentiments is propagated from a set of seed words of known sentiment polarity. We calculated the frequency in our corpus of all strongly positive and strongly negative words in the Wilson et al. (2005) lexicon, and chose the 100 most frequent in each category as our seed sets. Graph Construction Brody and Elhadad (2010) considered all frequent adjectives as candidates and weighted the </context>
<context position="21706" citStr="Brody and Elhadad (2010)" startWordPosition="3571" endWordPosition="3574">a single noun. Velikovich et al. (2010) constructed a graph where the nodes were 20 million candidate words or phrases, selected using a set of heuristics including frequency and mutual information of word boundaries. Context vectors were constructed for each candidate from all its mentions in a corpus of 4 billion documents, and the edge between two candidates was weighted by the cosine similarity between their context vectors. Due to the nature of the domain, which is highly informal and unstructured, accurate parsing is difficult. Therefore we cannot employ the exact construction method of Brody and Elhadad (2010). On the other hand, the method of Velikovich et al. (2010) is based on huge amounts of data, and takes advantage of the abundance of contextual information available in full documents, whereas our domain is closer to that of Brody and Elhadad (2010), who dealt with a small number of candidates and short documents typical to online reviews. Therefore, we adapt their construction method. We consider all our candidates words as nodes, along with the words in our positive and negative seed sets. As a proxy for syntactic relationship, edges are weighted as a function of the number of times two wor</context>
<context position="23341" citStr="Brody and Elhadad (2010)" startWordPosition="3865" endWordPosition="3868"> denote Reviews and Web, respectively. The Reviews propagation method is based on Zhu and Ghahramani (2002). The words in the positive and negative seed groups are assigned a polarity score of 1 and 0, respectively. All the rest start with a score of 0.5. Then, an update step is repeated. In update iteration t, for each word x that is not in the seed, the following update rule is applied: Ey∈N(x)w(y, x) · pt−1(y) pt(x) = (1) Ey∈N(x)w(y, x) Where pt(x) is the polarity of word x at step t, N(x) is the set of the neighbors of x, and w(y, x) is the weight of the edge connecting x and y. Following Brody and Elhadad (2010), we set this weight to be 1 + log(#co(y, x)), where #co(y, x) is the number of times y and x co-occurred within a three-word window. The update step is repeated to convergence. Velikovich et al. (2010) employed a different label propagation method, as described in Figure 3. Rather than relying on diffusion along the whole graph, this method considers only the single strongest path between each candidate and each seed word. In their paper, the authors claim that their algorithm is more suitable than that of Zhu and Ghahramani (2002) to a web-based dataset, which 567 Input: G = (V, E), wij E [0</context>
<context position="24994" citStr="Brody and Elhadad (2010)" startWordPosition="4197" endWordPosition="4200">+i − βpol-i, for all i 12: if |poli |&lt; y then poli = 0.0 for all i Figure 3: Web algorithm from Velikovich et al. (2010). P and N are the positive and negative seed sets, respectively, wij are the weights, and T and y are parameters9. contained many dense subgraphs and unreliable associations based only on co-occurrence statistics. We ran both algorithms in our experiment7, and compared the results. Evaluation We evaluated the output of the algorithms by comparison to human judgments. For words appearing in the sentiment lexicon, we used the polarity label provided. For the rest, similarly to Brody and Elhadad (2010), we asked volunteers to rate the words on a five-point scale: stronglynegative, weakly-negative, neutral, weakly-positive, or strongly-positive. We also provided a N/A option if the meaning of the word was unknown. Each word was rated by two volunteers. Words which were labeled N/A by one or more annotators were considered unknown. For the rest, exact inter-rater agree7We normalize the weights described above when using the Web algorithm. 9In Velikovich et al. (2010), the parameters T and -y were tuned on a held out dataset. Since our graphs are comparatively small, we do not need to limit th</context>
<context position="26231" citStr="Brody and Elhadad (2010)" startWordPosition="4421" endWordPosition="4424"> T in our search. We do not use the threshold -y, but rather employ a simple cutoff of the top 50 words. Pos. Human Judgment Unk. Neg. Neu. Web Pos. 18 2 26 2 Neg. 8 19 17 8 Reviews Pos. 21 6 21 2 Neg. 9 14 11 16 Table 2: Evaluation of the top 50 positive and negative words retrieved by the two algorithms through comparison to human judgment. Web pos. neg. see shit win niggas way dis gotta gettin summer smh lets tight haha fuckin birthday fuck tomorrow sick ever holy school smfh peace outta soon odee stuff wack canes nigga Table 3: Top fifteen negative and positive words for the algorithms of Brody and Elhadad (2010) (Reviews) and Velikovich et al. (2010) (Web). ment was 67.6%, but rose to 93% when considering adjacent ratings as equivalent10. This is comparable with the agreement reported by Brody and Elhadad (2010). We assigned values 1 (strongly negative) to 5 (strongly positive) to the ratings, and calculated the average between the two ratings for each word. Words with an average rating of 3 were considered neutral, and those with lower and higher ratings were considered negative and positive, respectively. Results Table 2 shows the distribution of the human labels among the top 50 most positive and </context>
</contexts>
<marker>Brody, Elhadad, 2010</marker>
<rawString>Brody, Samuel and Noemie Elhadad. 2010. An unsupervised aspect-sentiment model for online reviews. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2010). ACL, Los Angeles, CA, pages 804–812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha Calhoun</author>
</authors>
<title>The centrality of metrical structure in signaling information structure: A probabilistic perspective.</title>
<date>2010</date>
<journal>Language</journal>
<pages>86--1</pages>
<contexts>
<context position="6229" citStr="Calhoun 2010" startWordPosition="973" endWordPosition="975">t is specific to informal social communication to enable the extension of an existing lexicon in a domain specific manner. 2Although the authors state that all data and methods will be made available on a public website, it was not present at the time of the writing of this article. 2 Methodology Prosodic indicators (such as high pitch, prolonged duration, intensity, vowel quality, etc.) have long been know (Bolinger, 1965) as ways for a speaker to emphasize or accent an important word. The ways in which they are used in speech are the subject of ongoing linguistic research (see, for example, Calhoun 2010). In written text, many of these indicators are lost. However, there exist some orthographic conventions which are used to mark or substitute for prosody, including punctuation and typographic styling (italic, bold, and underlined text). In purely text-based domains, such as Twitter, styling is not always available, and is replaced by capitalization or other conventions (e.g., enclosing the word in asterisks). Additionally, the informal nature of the domain leads to an orthographic style which is much closer to the spoken form than in other, more formal, domains. In this work, we hypothesize t</context>
</contexts>
<marker>Calhoun, 2010</marker>
<rawString>Calhoun, Sasha. 2010. The centrality of metrical structure in signaling information structure: A probabilistic perspective. Language 86:1–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas A Diakopoulos</author>
<author>David A Shamma</author>
</authors>
<title>Characterizing debate performance via aggregated twitter sentiment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 28th international conference on Human factors in computing systems.</booktitle>
<volume>10</volume>
<pages>1195--1198</pages>
<publisher>ACM,</publisher>
<location>New York, NY, USA, CHI</location>
<contexts>
<context position="1588" citStr="Diakopoulos and Shamma (2010)" startWordPosition="229" endWordPosition="232"> strengths and weaknesses. Our results are of interest to anyone analyzing sentiment in microblogs and social networks, whether for research or commercial purposes. 1 Introduction Recently, there has been a surge of interest in sentiment analysis of Twitter messages. Many researchers (e.g., Bollen et al. 2011; Kivran-Swaine and Naaman 2011) are interested in studying structure and interactions in social networks, where sentiment can play an important role. Others use Twitter as a barometer for public mood and opinion in diverse areas such as entertainment, politics and economics. For example, Diakopoulos and Shamma (2010) use Twitter messages posted in conjunction with the live presidential debate between Barack Obama and John McCain to gauge public opinion, Bollen et al. (2010) measure public mood on Twitter and use it to predict upcoming stock market fluc562 tuations, and O’Connor et al. (2010) connect public opinion data from polls to sentiment expressed in Twitter messages along a timeline. A prerequisite of all such research is an effective method for measuring the sentiment of a post or tweet. Due to the extremely informal nature of the medium, and the length restriction1, the language and jargon which i</context>
<context position="3069" citStr="Diakopoulos and Shamma 2010" startWordPosition="465" endWordPosition="468">ains, such as news. One solution is to use human annotation. For example, Kivran-Swaine and Naaman (2011) use manual coding of tweets in several emotion categories (e.g., joy, sadness) for their research. Diakopoulos and Shamma (2010) use crowd sourcing via Amazon’s Mechanical Turk. Manual encoding usually offers a deeper understanding and correspondingly higher accuracy than shallow automatic methods. However, it is expensive and labor intensive and cannot be applied in real time. Crowd-sourcing carries additional caveats of its own, such as issues of annotator expertise and reliability (see Diakopoulos and Shamma 2010). The automatic approach to sentiment analysis is commonly used for processing data from social networks and microblogs, where there is often a huge quantity of information and a need for low latency. Many automatic approaches (including all those used in the work mentioned above) have at their core a sentiment lexicon, containing a list of words la1Messages in Twitter are limited to 140 characters, for compatibility with SMS messaging Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 562–570, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Associa</context>
</contexts>
<marker>Diakopoulos, Shamma, 2010</marker>
<rawString>Diakopoulos, Nicholas A. and David A. Shamma. 2010. Characterizing debate performance via aggregated twitter sentiment. In Proceedings of the 28th international conference on Human factors in computing systems. ACM, New York, NY, USA, CHI ’10, pages 1195–1198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Grinter</author>
<author>Margery Eldridge</author>
</authors>
<title>Wan2tlk?: everyday text messaging.</title>
<date>2003</date>
<booktitle>In Proceedings of the SIGCHI conference on Human factors in computing systems.</booktitle>
<volume>03</volume>
<pages>441--448</pages>
<publisher>ACM,</publisher>
<location>New York, NY, USA, CHI</location>
<contexts>
<context position="11281" citStr="Grinter and Eldridge (2003)" startWordPosition="1805" endWordPosition="1808">motion (e.g., ow, ugh, yay) are often lengthened and, for some, the combined frequency of the different lengthened forms is actually greater than that of the canonical (single most frequent) one. Lengthening is a common phenomenon in our dataset. Out of half-a-million tweets, containing roughly 6.5 million words, our procedure identifies 108,762 word occurrences which are lengthenings of a canonical form. These words occur in 87,187 tweets (17.44% or approximately one out of every six, on average). The wide-spread use of lengthening is surprising in light of the length restriction of Twitter. Grinter and Eldridge (2003) point out several conventions that are used in text messages specifically to deal with this restriction. The fact that lengthening is used in spite of the need for brevity suggests that it conveys important information. Canonical Assumption We validate the assumption that the most frequent form in the set is the canonical form by examining sets containing one or more word forms that were identified in a standard 564 Can. Form Card. # Can. # Non-Can. nice 76 3847 348 ugh 75 1912 1057 lmao 70 10085 3727 lmfao 67 2615 1619 ah 61 767 1603 love 59 16360 359 crazy 59 3530 253 yeah 57 4562 373 shees</context>
</contexts>
<marker>Grinter, Eldridge, 2003</marker>
<rawString>Grinter, Rebecca and Margery Eldridge. 2003. Wan2tlk?: everyday text messaging. In Proceedings of the SIGCHI conference on Human factors in computing systems. ACM, New York, NY, USA, CHI ’03, pages 441–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Funda Kivran-Swaine</author>
<author>Mor Naaman</author>
</authors>
<title>Network properties and social sharing of emotions in social awareness streams.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 ACM Conference on Computer Supported Cooperative Work (CSCW 2011).</booktitle>
<location>Hangzhou, China.</location>
<contexts>
<context position="1301" citStr="Kivran-Swaine and Naaman 2011" startWordPosition="182" endWordPosition="185">n show that lengthening is strongly associated with subjectivity and sentiment. Finally, we present an automatic method which leverages this association to detect domain-specific sentiment- and emotionbearing words. We evaluate our method by comparison to human judgments, and analyze its strengths and weaknesses. Our results are of interest to anyone analyzing sentiment in microblogs and social networks, whether for research or commercial purposes. 1 Introduction Recently, there has been a surge of interest in sentiment analysis of Twitter messages. Many researchers (e.g., Bollen et al. 2011; Kivran-Swaine and Naaman 2011) are interested in studying structure and interactions in social networks, where sentiment can play an important role. Others use Twitter as a barometer for public mood and opinion in diverse areas such as entertainment, politics and economics. For example, Diakopoulos and Shamma (2010) use Twitter messages posted in conjunction with the live presidential debate between Barack Obama and John McCain to gauge public opinion, Bollen et al. (2010) measure public mood on Twitter and use it to predict upcoming stock market fluc562 tuations, and O’Connor et al. (2010) connect public opinion data from</context>
<context position="2546" citStr="Kivran-Swaine and Naaman (2011)" startWordPosition="388" endWordPosition="391">iment expressed in Twitter messages along a timeline. A prerequisite of all such research is an effective method for measuring the sentiment of a post or tweet. Due to the extremely informal nature of the medium, and the length restriction1, the language and jargon which is used in Twitter varies significantly from that of commonly studied text corpora. In addition, Twitter is a quickly evolving domain, and new terms are constantly being introduced. These factors pose difficulties to methods designed for conventional domains, such as news. One solution is to use human annotation. For example, Kivran-Swaine and Naaman (2011) use manual coding of tweets in several emotion categories (e.g., joy, sadness) for their research. Diakopoulos and Shamma (2010) use crowd sourcing via Amazon’s Mechanical Turk. Manual encoding usually offers a deeper understanding and correspondingly higher accuracy than shallow automatic methods. However, it is expensive and labor intensive and cannot be applied in real time. Crowd-sourcing carries additional caveats of its own, such as issues of annotator expertise and reliability (see Diakopoulos and Shamma 2010). The automatic approach to sentiment analysis is commonly used for processin</context>
</contexts>
<marker>Kivran-Swaine, Naaman, 2011</marker>
<rawString>Kivran-Swaine, Funda and Mor Naaman. 2011. Network properties and social sharing of emotions in social awareness streams. In Proceedings of the 2011 ACM Conference on Computer Supported Cooperative Work (CSCW 2011). Hangzhou, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M McNair</author>
<author>M Lorr</author>
<author>L F Droppleman</author>
</authors>
<date>1971</date>
<booktitle>Profile of Mood States (POMS). Educational and Industrial Testing Service.</booktitle>
<contexts>
<context position="5201" citStr="McNair et al., 1971" startWordPosition="798" endWordPosition="802">reating a sentiment lexicon which is specifically tailored to the microblogging domain, or adapting an existing one, we can expect to achieve higher accuracy and increased coverage. Recent work in this area includes Velikovich et al. (2010), who developed a method for automatically deriving an extensive sentiment lexicon from the web as a whole. The resulting lexicon has greatly increased coverage compared to existing dictionaries and can handle spelling errors and web-specific jargon. Bollen et al. (2010) expand an existing well-vetted psychometric instrument - Profile of Mood States (POMS) (McNair et al., 1971) that associates terms with moods (e.g. calm, happy). The authors use co-occurrence information from the Google n-gram corpus (Brants and Franz, 2006) to enlarge the original list of 72 terms to 964. They use this expanded emotion lexicon (named GPOS) in conjunction with the lexicon of Wilson et al. (2005) to estimate public mood from Twitter posts2. The method we present in this paper leverages a phenomenon that is specific to informal social communication to enable the extension of an existing lexicon in a domain specific manner. 2Although the authors state that all data and methods will be </context>
</contexts>
<marker>McNair, Lorr, Droppleman, 1971</marker>
<rawString>McNair, D. M., M. Lorr, and L. F. Droppleman. 1971. Profile of Mood States (POMS). Educational and Industrial Testing Service.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media.</booktitle>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>O’Connor, Brendan, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010. From tweets to polls: Linking text sentiment to public opinion time series. In Proceedings of the International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10). ELRA,</booktitle>
<location>Valletta,</location>
<contexts>
<context position="4335" citStr="Pak and Paroubek (2010)" startWordPosition="660" endWordPosition="663">th specific associated emotions (joy, happiness) or a polarity value (positive, neutral, negative). The overall sentiment of a piece of text is calculated as a function of the labels of the component words. Because Twitter messages are short, shallow approaches are sometimes considered sufficient (Bermingham and Smeaton, 2010). There are also approaches that use deeper machine learning techniques to train sentiment classifiers on examples that have been labeled for sentiment, either manually or automatically, as described above. Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010). Most established sentiment lexicons (e.g., Wilson et al. 2005, see Section 5) were created for a general domain, and suffer from limited coverage and inaccuracies when applied to the highly informal domain of social networks communication. By creating a sentiment lexicon which is specifically tailored to the microblogging domain, or adapting an existing one, we can expect to achieve higher accuracy and increased coverage. Recent work in this area includes Velikovich et al. (2010), who developed a method for automatically deriving an extensive sentiment lexicon from the web as a whole. The re</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Pak, Alexander and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10). ELRA, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Velikovich</author>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan McDonald</author>
</authors>
<title>The viability of web-derived polarity lexicons.</title>
<date>2010</date>
<journal>HLT</journal>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. ACL,</booktitle>
<volume>10</volume>
<pages>777--785</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="4821" citStr="Velikovich et al. (2010)" startWordPosition="739" endWordPosition="742">er manually or automatically, as described above. Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010). Most established sentiment lexicons (e.g., Wilson et al. 2005, see Section 5) were created for a general domain, and suffer from limited coverage and inaccuracies when applied to the highly informal domain of social networks communication. By creating a sentiment lexicon which is specifically tailored to the microblogging domain, or adapting an existing one, we can expect to achieve higher accuracy and increased coverage. Recent work in this area includes Velikovich et al. (2010), who developed a method for automatically deriving an extensive sentiment lexicon from the web as a whole. The resulting lexicon has greatly increased coverage compared to existing dictionaries and can handle spelling errors and web-specific jargon. Bollen et al. (2010) expand an existing well-vetted psychometric instrument - Profile of Mood States (POMS) (McNair et al., 1971) that associates terms with moods (e.g. calm, happy). The authors use co-occurrence information from the Google n-gram corpus (Brants and Franz, 2006) to enlarge the original list of 72 terms to 964. They use this expand</context>
<context position="20471" citStr="Velikovich et al. (2010)" startWordPosition="3366" endWordPosition="3369"> are looking for commonly lengthened words, we disregard those where the combined frequency of the non-canonical forms is less than 1% that of the canonical one. We also remove stop words, even though some are often lengthened for emphasis (e.g., me, and, so), since they are too frequent, and introduce many spurious edges in our co-occurrence graph. Finally, we filter words based on weight, as described below. This leaves us with 720 candidate words. Graph Approach We examine two methods for sentiment detection - that of Brody and Elhadad (2010) for detecting sentiment in reviews, and that of Velikovich et al. (2010) for finding sentiment terms in a giga-scale web corpus. Both of these employ a graph-based approach, where candidate terms are nodes, and sentiments is propagated from a set of seed words of known sentiment polarity. We calculated the frequency in our corpus of all strongly positive and strongly negative words in the Wilson et al. (2005) lexicon, and chose the 100 most frequent in each category as our seed sets. Graph Construction Brody and Elhadad (2010) considered all frequent adjectives as candidates and weighted the edge between two adjectives by a function of the number of times they bot</context>
<context position="21765" citStr="Velikovich et al. (2010)" startWordPosition="3582" endWordPosition="3585"> where the nodes were 20 million candidate words or phrases, selected using a set of heuristics including frequency and mutual information of word boundaries. Context vectors were constructed for each candidate from all its mentions in a corpus of 4 billion documents, and the edge between two candidates was weighted by the cosine similarity between their context vectors. Due to the nature of the domain, which is highly informal and unstructured, accurate parsing is difficult. Therefore we cannot employ the exact construction method of Brody and Elhadad (2010). On the other hand, the method of Velikovich et al. (2010) is based on huge amounts of data, and takes advantage of the abundance of contextual information available in full documents, whereas our domain is closer to that of Brody and Elhadad (2010), who dealt with a small number of candidates and short documents typical to online reviews. Therefore, we adapt their construction method. We consider all our candidates words as nodes, along with the words in our positive and negative seed sets. As a proxy for syntactic relationship, edges are weighted as a function of the number of times two words occurred within a three-word window of each other in our</context>
<context position="23543" citStr="Velikovich et al. (2010)" startWordPosition="3902" endWordPosition="3905">respectively. All the rest start with a score of 0.5. Then, an update step is repeated. In update iteration t, for each word x that is not in the seed, the following update rule is applied: Ey∈N(x)w(y, x) · pt−1(y) pt(x) = (1) Ey∈N(x)w(y, x) Where pt(x) is the polarity of word x at step t, N(x) is the set of the neighbors of x, and w(y, x) is the weight of the edge connecting x and y. Following Brody and Elhadad (2010), we set this weight to be 1 + log(#co(y, x)), where #co(y, x) is the number of times y and x co-occurred within a three-word window. The update step is repeated to convergence. Velikovich et al. (2010) employed a different label propagation method, as described in Figure 3. Rather than relying on diffusion along the whole graph, this method considers only the single strongest path between each candidate and each seed word. In their paper, the authors claim that their algorithm is more suitable than that of Zhu and Ghahramani (2002) to a web-based dataset, which 567 Input: G = (V, E), wij E [0, 1] P,N,y E R,T E N Output: poli E R|V | Initialize: poli, pol+i , pol-i = 0 for all i pol+i = 1.0 for all vi E P and pol-i = 1.0 for all vi E N 1: αij = 0 for all i =� j, αii = 1 for all i 2: for vi E</context>
<context position="25466" citStr="Velikovich et al. (2010)" startWordPosition="4273" endWordPosition="4276"> human judgments. For words appearing in the sentiment lexicon, we used the polarity label provided. For the rest, similarly to Brody and Elhadad (2010), we asked volunteers to rate the words on a five-point scale: stronglynegative, weakly-negative, neutral, weakly-positive, or strongly-positive. We also provided a N/A option if the meaning of the word was unknown. Each word was rated by two volunteers. Words which were labeled N/A by one or more annotators were considered unknown. For the rest, exact inter-rater agree7We normalize the weights described above when using the Web algorithm. 9In Velikovich et al. (2010), the parameters T and -y were tuned on a held out dataset. Since our graphs are comparatively small, we do not need to limit the path length T in our search. We do not use the threshold -y, but rather employ a simple cutoff of the top 50 words. Pos. Human Judgment Unk. Neg. Neu. Web Pos. 18 2 26 2 Neg. 8 19 17 8 Reviews Pos. 21 6 21 2 Neg. 9 14 11 16 Table 2: Evaluation of the top 50 positive and negative words retrieved by the two algorithms through comparison to human judgment. Web pos. neg. see shit win niggas way dis gotta gettin summer smh lets tight haha fuckin birthday fuck tomorrow si</context>
</contexts>
<marker>Velikovich, Blair-Goldensohn, Hannan, McDonald, 2010</marker>
<rawString>Velikovich, Leonid, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of web-derived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. ACL, Stroudsburg, PA, USA, HLT ’10, pages 777–785.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<journal>HLT</journal>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing. ACL,</booktitle>
<volume>05</volume>
<pages>347--354</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="4398" citStr="Wilson et al. 2005" startWordPosition="669" endWordPosition="672"> (positive, neutral, negative). The overall sentiment of a piece of text is calculated as a function of the labels of the component words. Because Twitter messages are short, shallow approaches are sometimes considered sufficient (Bermingham and Smeaton, 2010). There are also approaches that use deeper machine learning techniques to train sentiment classifiers on examples that have been labeled for sentiment, either manually or automatically, as described above. Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010). Most established sentiment lexicons (e.g., Wilson et al. 2005, see Section 5) were created for a general domain, and suffer from limited coverage and inaccuracies when applied to the highly informal domain of social networks communication. By creating a sentiment lexicon which is specifically tailored to the microblogging domain, or adapting an existing one, we can expect to achieve higher accuracy and increased coverage. Recent work in this area includes Velikovich et al. (2010), who developed a method for automatically deriving an extensive sentiment lexicon from the web as a whole. The resulting lexicon has greatly increased coverage compared to exis</context>
<context position="14131" citStr="Wilson et al., 2005" startWordPosition="2291" endWordPosition="2294"> to Sentiment At the beginning of Section 2 we presented the hypothesis that lengthening represents a textual substitute for prosodic indicators in speech. As such, it is not used arbitrarily, but rather applied to subjective words to strengthen the sentiment or emotion they convey. The examples presented in Table 1 in the previous section appear to support this hypothesis. In this section we wish to provide experimental evidence for our hypothesis, by demonstrating a significant degree of association between lengthening and subjectivity. For this purpose we use an existing sentiment lexicon (Wilson et al., 2005), which is commonly used in the literature (see Section 1) and is at the core of OpinionFinder4, a popular sentiment analysis tool designed to determine opinion in a general domain. The lexicon provides a list of subjective words, each annotated with its degree of subjectivity (strongly subjective, weakly subjective), as well as its sentiment polarity (positive, negative, or neutral). In these experiments, we use the presence of a word (canonical form) in the lexicon as an indicator of subjectivity. It should be noted that the reverse is not true, i.e., the fact that a word is absent from the </context>
<context position="20811" citStr="Wilson et al. (2005)" startWordPosition="3424" endWordPosition="3427"> graph. Finally, we filter words based on weight, as described below. This leaves us with 720 candidate words. Graph Approach We examine two methods for sentiment detection - that of Brody and Elhadad (2010) for detecting sentiment in reviews, and that of Velikovich et al. (2010) for finding sentiment terms in a giga-scale web corpus. Both of these employ a graph-based approach, where candidate terms are nodes, and sentiments is propagated from a set of seed words of known sentiment polarity. We calculated the frequency in our corpus of all strongly positive and strongly negative words in the Wilson et al. (2005) lexicon, and chose the 100 most frequent in each category as our seed sets. Graph Construction Brody and Elhadad (2010) considered all frequent adjectives as candidates and weighted the edge between two adjectives by a function of the number of times they both modified a single noun. Velikovich et al. (2010) constructed a graph where the nodes were 20 million candidate words or phrases, selected using a set of heuristics including frequency and mutual information of word boundaries. Context vectors were constructed for each candidate from all its mentions in a corpus of 4 billion documents, a</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing. ACL, Stroudsburg, PA, USA, HLT ’05, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical report, CMU-CALD-02.</tech>
<contexts>
<context position="22824" citStr="Zhu and Ghahramani (2002)" startWordPosition="3763" endWordPosition="3766">ets. As a proxy for syntactic relationship, edges are weighted as a function of the number of times two words occurred within a three-word window of each other in our dataset. We remove nodes whose neighboring edges have a combined weight of less than 20, meaning they participate in relatively few co-occurrence relations with the other words in the graph. Algorithm Once the graph is constructed, we can use either of the propagation algorithms of Brody and Elhadad (2010) and Velikovich et al. (2010), which we will denote Reviews and Web, respectively. The Reviews propagation method is based on Zhu and Ghahramani (2002). The words in the positive and negative seed groups are assigned a polarity score of 1 and 0, respectively. All the rest start with a score of 0.5. Then, an update step is repeated. In update iteration t, for each word x that is not in the seed, the following update rule is applied: Ey∈N(x)w(y, x) · pt−1(y) pt(x) = (1) Ey∈N(x)w(y, x) Where pt(x) is the polarity of word x at step t, N(x) is the set of the neighbors of x, and w(y, x) is the weight of the edge connecting x and y. Following Brody and Elhadad (2010), we set this weight to be 1 + log(#co(y, x)), where #co(y, x) is the number of tim</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Zhu, X. and Z. Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. Technical report, CMU-CALD-02.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>