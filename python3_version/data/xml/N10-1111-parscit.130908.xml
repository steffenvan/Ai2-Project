<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024545">
<title confidence="0.998202">
Constraint-Driven Rank-Based Learning for Information Extraction
</title>
<author confidence="0.994267">
Sameer Singh Limin Yao Sebastian Riedel Andrew McCallum
</author>
<affiliation confidence="0.998479">
Dept. of Computer Science
University of Massachusetts
</affiliation>
<address confidence="0.795745">
Amherst MA 01003
</address>
<email confidence="0.999649">
{sameer,lmyao,riedel,mccallum}@cs.umass.edu
</email>
<sectionHeader confidence="0.994794" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997806894736842">
Most learning algorithms for undirected
graphical models require complete inference
over at least one instance before parameter up-
dates can be made. SampleRank is a rank-
based learning framework that alleviates this
problem by updating the parameters during in-
ference. Most semi-supervised learning algo-
rithms also perform full inference on at least
one instance before each parameter update.
We extend SampleRank to semi-supervised
learning in order to circumvent this compu-
tational bottleneck. Different approaches to
incorporate unlabeled data and prior knowl-
edge into this framework are explored. When
evaluated on a standard information extraction
dataset, our method significantly outperforms
the supervised method, and matches results of
a competing state-of-the-art semi-supervised
learning approach.
</bodyText>
<sectionHeader confidence="0.99888" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936044444445">
Most supervised learning algorithms for undirected
graphical models require full inference over the
dataset (e.g., gradient descent), small subsets of the
dataset (e.g., stochastic gradient descent), or at least
a single instance (e.g., perceptron, Collins (2002))
before parameter updates are made. Often this is the
main computational bottleneck during training.
SampleRank (Wick et al., 2009) is a rank-based
learning framework that alleviates this problem by
performing parameter updates within inference. Ev-
ery pair of samples generated during inference is
ranked according to the model and the ground truth,
and the parameters are updated when the rankings
disagree. SampleRank has enabled efficient learn-
ing for massive information extraction tasks (Culotta
et al., 2007; Singh et al., 2009).
The problem of requiring a complete inference it-
eration before parameters are updated also exists in
the semi-supervised learning scenario. Here the sit-
uation is often considerably worse since inference
has to be applied to potentially very large unlabeled
datasets. Most semi-supervised learning algorithms
rely on marginals (GE, Mann and McCallum, 2008)
or MAP assignments (CODL, Chang et al., 2007).
Calculating these is computationally inexpensive for
many simple tasks (such as classification and re-
gression). However, marginal and MAP inference
tends to be expensive for complex structured pre-
diction models (such as the joint information extrac-
tion models of Singh et al. (2009)), making semi-
supervised learning intractable.
In this work we employ a fast rank-based learning
algorithm for semi-supervised learning to circum-
vent the inference bottleneck. The ranking function
is extended to capture both the preference expressed
by the labeled data, and the preference of the domain
expert when the labels are not available. This allows
us to perform SampleRank as is, without sacrificing
its scalability, which is crucial for future large scale
applications of semi-supervised learning.
We applied our method to a standard information
extraction dataset used for semi-supervised learning.
Empirically we demonstrate improvements over the
supervised model, and closely match the results of a
competing state-of-the-art semi-supervised learner.
</bodyText>
<sectionHeader confidence="0.912552" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.762825">
Conditional random fields (Lafferty et al., 2001) are
undirected graphical models represented as factor
</bodyText>
<page confidence="0.958301">
729
</page>
<subsubsectionHeader confidence="0.571152">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 729–732,
</subsubsectionHeader>
<subsectionHeader confidence="0.27202">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.99527725">
graphs. A factor graph G = {Ψi} defines a prob-
ability distribution over assignments y to a set of
output variables, conditioned on an observation x.
A factor Ψi computes the inner product between
the vector of sufficient statistics f(xi, yi) and pa-
rameters Θ. Let Z(x) be the data-dependent par-
tition function used for normalization. The proba-
bility distribution defined by the graph is:
</bodyText>
<equation confidence="0.975487">
p(y|x, Θ) =1 ri
Z(x) ΨiEG eΘ&apos;f(xi,yi)
</equation>
<subsectionHeader confidence="0.945048">
2.1 Rank-Based Learning
</subsectionHeader>
<bodyText confidence="0.981647333333333">
SampleRank (Wick et al., 2009) is a rank-based
learning framework for that performs parameter up-
dates within MCMC inference. Every pair of con-
secutive samples in the MCMC chain is ranked ac-
cording to the model and the ground truth, and the
parameters are updated when the rankings disagree.
This allows the learner to acquire more supervision
per sample, and has led to efficient training of mod-
els for which inference is very expensive (Singh
et al., 2009).
SampleRank considers two ranking functions: (1)
the unnormalized conditional probability (model
ranking), and (2) a truth function F(y) (objective
ranking) which is defined as −L(y, yL), the neg-
ative loss between the possible assignment y and
the true assignment yL. The truth function can take
different forms, such as tokenwise accuracy or F1-
measure with respect to some labeled data.
In order to learn the parameters for which model
rankings are consistent with objective rankings,
SampleRank performs the following update for each
consecutive pair of samples ya and yb of the MCMC
chain. Let α be the learning rate, and Δ =
f(xi, yai ) − f(xi, ybi), then Θ is updated as follows:
</bodyText>
<equation confidence="0.9966325">
αΔ if p(y&amp;quot;x) p(y��x)&lt; 1 ∧ F(ya) &gt; F(yb)
−αΔ if p(y��x)
p(y��x) &gt; 1 ∧ F(ya) &lt; F(yb)
0 otherwise.
</equation>
<bodyText confidence="0.999297142857143">
This update is usually fast: in order to calculate
the required model ratio, only factors that touch
changed variables have to be taken into account.
SampleRank has been incorporated into the FAC-
TORIE toolkit for probabilistic programming with
imperatively-defined factor graphs (McCallum et al.,
2009).
</bodyText>
<sectionHeader confidence="0.963302" genericHeader="method">
3 Semi-Supervised Rank-Based Learning
</sectionHeader>
<bodyText confidence="0.999978285714286">
To apply SampleRank to the semi-supervised set-
ting, we need to specify the truth function F over
both labeled and unlabeled data. For labeled data
YL, we can use the true labels. These are not avail-
able for unlabeled data YU, and we present alterna-
tive ways of defining a truth function FU : YU → &lt;
for this case.
</bodyText>
<subsectionHeader confidence="0.999823">
3.1 Self-Training
</subsectionHeader>
<bodyText confidence="0.996768625">
Self-training, which uses predictions as truth, fits di-
rectly into our SampleRank framework. After per-
forming SampleRank on training data (using FL),
MAP inference is performed on the unlabeled data.
The prediction ˆyU is used as the ground truth for
the unlabeled data. Thus the self-training objective
function Fs over the unlabeled data can be defined
as Fs(y) = −L(y, ˆyU).
</bodyText>
<subsectionHeader confidence="0.998703">
3.2 Encoding Constraints
</subsectionHeader>
<bodyText confidence="0.999971642857143">
Constraint-driven semi-supervised learning uses
constraints to incorporate external domain knowl-
edge when labels are missing (Chang et al., 2007;
Mann and McCallum, 2008; Bellare et al., 2009).
Constraints prefer certain label configurations over
others. For example, one constraint may be that oc-
currences of the word “California” are preferred to
have the label “location”.
We can encode constraints directly into the objec-
tive function FU. Let a constraint i be specified as
hpi, cii, where ci(y) denotes whether assignment y
satisfies the constraint i (+1), violates it (−1), or the
constraint does not apply (0), and pi is the constraint
strength. Then the objective function is:
</bodyText>
<equation confidence="0.958353">
Fc(y) = E pici(y)
i
</equation>
<subsectionHeader confidence="0.920283">
3.3 Incorporating Model Predictions
</subsectionHeader>
<bodyText confidence="0.999970666666667">
When the objective function Fc is used, every pre-
diction on unlabeled data is ranked only according to
the constraints, and thus the model is trained to sat-
isfy all the constraints. This is a problem when the
constraints prefer a wrong solution while the model
favors the correct solution, resulting in SampleR-
ank updating the model away from the true solution.
To avoid this, the ranking function needs to balance
preferences of the constraints and the current model.
</bodyText>
<equation confidence="0.735791">
Θ+← {
</equation>
<page confidence="0.932237">
730
</page>
<bodyText confidence="0.9986145">
One option is to incorporate the self-training ob-
jective function .F&apos;s. A new objective function that
combines self-training with constraints can be de-
fined as:
</bodyText>
<equation confidence="0.991141333333333">
.F&apos;sc(y) = .F&apos;s(y) + As.F&apos;c(y)
= −L(y, YU) + As pici(y)
i
</equation>
<bodyText confidence="0.999982625">
This objective function has at least two limita-
tions. First, self-training involves a complete infer-
ence step to obtain yU. Second, the model might
have low confidence in its prediction (this is the case
when the underlying marginals are almost uniform),
but the self-training objective des not take this into
account. Hence, we also propose an objective func-
tion that incorporates the model score directly, i.e.
</bodyText>
<equation confidence="0.984603333333333">
.F&apos;mc(y) = log p(y|x, O) + log Z(x) + Am.F&apos;c(y)
�= O · f(xi, yi) + Am � pici(y)
Api i
</equation>
<bodyText confidence="0.999957666666667">
This objective does not require inference, and also
takes into account model confidence.
In both objective functions .F&apos;sc and .F&apos;mc, A con-
trols the relative contribution of the constraint pref-
erences to the objective function. With higher A,
SampleRank will make updates that never try to vi-
olate constraints, while with low A, SampleRank
trusts the model more. A corresponds to constraint
satisfaction weights p used in (Chang et al., 2007).
</bodyText>
<sectionHeader confidence="0.99993" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.99939448">
Chang et al. propose constraint-driven learn-
ing (CODL, Chang et al., 2007) which can be in-
terpreted as a variation of self-training: Instances
are selected for supervision based not only on the
model’s prediction, but also on their consistency
with a set of user-defined constraints. By directly in-
corporating the model score and the constraints (as
in .F&apos;mc in Section 3.3) we follow the same approach,
but avoid the expensive “Top-K” inference step.
Generalized expectation criterion (GE, Mann and
McCallum, 2008) and Alternating Projections (AP,
Bellare et al., 2009) encode preferences by speci-
fying constraints on feature expectations, which re-
quire expensive inference. Although AP can use on-
line training, it still involves full inference over each
instance. Furthermore, these methods only support
constraints that factorize according to the model.
Li (2009) incorporates prior knowledge into con-
ditional random fields as variables. They require full
inference during learning, restricting the application
to simple models. Furthermore, higher-order con-
straints are specified using large cliques in the graph,
which slow down inference. Our approach directly
incorporates these constraints into the ranking func-
tion, with no impact on inference time.
</bodyText>
<sectionHeader confidence="0.998036" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99990975">
We carried out experiments on the Cora citation
dataset. The task is to segment each citation into
different fields, such as “author” and “title”. We use
300 instances as training data, 100 instances as de-
velopment data, and 100 instances as test data. Some
instances from the training data are selected as la-
beled instances, and the remaining data (including
development) as unlabeled. We use the same token-
label constraints as Chang et al. (2007).
We use the objective functions defined in Sec-
tion 3, specifically self-training (Self:.F&apos;s), direct
constraints (Cons:.F&apos;c), the combination of the two
(Self+Cons:.F&apos;sc), and combination of the model
score and the constraints (Model+Cons:.F&apos;mc). We
set pi = 1.0, a = 1.0, As = 10, and Am = 0.0001.
Average token accuracy for 5 runs is reported and
compared with CODL1 in Table 1. We also report
supervised results from (Chang et al., 2007) and
SampleRank. All of our methods show vast im-
provement over the supervised method for smaller
training sizes, but this difference decreases as the
training size increases. When the complete training
data is used, additional unlabeled data hurts our per-
formance. This is not observed in CODL since they
use more unlabeled data, which may also explain
their slightly higher accuracy. Note that Self+Cons
performs better than Self or Cons individually.
Model+Cons also performs competitively, and
may potentially outperform other methods if a bet-
ter Am is chosen. Note, however, that Am is much
harder to tune than As since Am weighs the contri-
bution of the unnormalized model score, the range
</bodyText>
<footnote confidence="0.935684">
1We report inference without constraints results from
CODL. Their results that incorporated constraints were higher,
but we do not implement this alternative due to the difficulty in
balancing the model score and constraint weights.
</footnote>
<page confidence="0.983272">
731
</page>
<table confidence="0.999737">
Method 5 10 15 20 25 300
Sup. (CODL) 55.1 64.6 68.7 70.1 72.7 86.1
SampleRank 66.5 74.6 75.6 77.6 79.5 90.7
CODL 71 76.7 79.4 79.4 82 88.2
Self 67.6 75.1 75.8 78.6 80.4 88
Cons 67.2 75.3 77.5 78.6 79.4 88.3
Self+Cons 71.3 77 77.5 79.5 81.1 87.4
Model+Cons 69.8 75.4 75.7 79.3 79.3 90.6
</table>
<tableCaption confidence="0.999954">
Table 1: Tokenwise Accuracy: for different methods as we vary the size of the labeled data
</tableCaption>
<bodyText confidence="0.999955666666667">
of which depends on many different factors such as
properties of the data, the learning rate, number of
samples, proposal function, etc. For self+cons (as),
the ranges of the predictions and constraint penalties
are fixed and known, making the task simpler.
Self training takes 90 minutes to run on average,
while Self+Cons and Model+Cons need 100 min-
utes. Since the Cons method skips the inference
step over unlabeled data, it takes only 30 minutes
to run. As the size of the model and unlabeled data
set grows, this saving will become more significant.
Running time of CODL was not reported.
</bodyText>
<sectionHeader confidence="0.999015" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999929928571428">
This work extends the rank-based learning frame-
work to semi-supervised learning. By integrating
the two paradigms, we retain the computational effi-
ciency provided by parameter updates within infer-
ence, while utilizing unlabeled data and prior knowl-
edge. We demonstrate accuracy improvements on a
real-word information extraction dataset.
We believe that the method will be of greater ben-
efit to learning in complex factor graphs such as
joint models over multiple extraction tasks. In future
work we will investigate our approach in such set-
tings. Additionally, various sensitivity, convergence,
and robustness properties of the method need to be
analyzed.
</bodyText>
<sectionHeader confidence="0.998025" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998118">
This work was supported in part by the Center for In-
telligent Information Retrieval, in part by SRI Inter-
national subcontract #27-001338 and ARFL prime
contract #FA8750-09-C-0181, and in part by The
Central Intelligence Agency, the National Secu-
rity Agency and National Science Foundation under
NSF grant #IIS-0326249. Any opinions, findings
and conclusions or recommendations expressed in
this material are the authors’ and do not necessarily
reflect those of the sponsor.
</bodyText>
<sectionHeader confidence="0.999202" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987353125">
Kedar Bellare, Gregory Druck, and Andrew McCallum.
Alternating projections for learning with expectation
constraints. In UAI, 2009.
Mingwei Chang, Lev Ratinov, and Dan Roth. Guiding
semi-supervision with constraint-driven learning. In
ACL, 2007.
Michael Collins. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithm. In ACL, 2002.
Aron Culotta, Michael Wick, and Andrew McCallum.
First-order probabilistic models for coreference reso-
lution. In NAACL/HLT, 2007.
John Lafferty, Andrew McCallum, and Fernando Pereira.
Conditional random fields: probabilistic models for
segmenting and labeling sequence data. In ICML,
2001.
Xiao Li. On the use of virtual evidence in conditional
random fields. In EMNLP, 2009.
Gideon S. Mann and Andrew McCallum. Generalized ex-
pectation criteria for semi-supervised learning of con-
ditional random fields. In ACL, 2008.
Andrew McCallum, Karl Schultz, and Sameer Singh.
FACTORIE: probabilistic programming via impera-
tively defined factor graphs. In NIPS, 2009.
Sameer Singh, Karl Schultz, and Andrew McCallum.
Bi-directional joint inference for entity resolution
and segmentation using imperatively-defined factor
graphs. In ECML/PKDD, 2009.
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. SampleRank: Learning pref-
erences from atomic gradients. In NIPS Workshop on
Advances in Ranking, 2009.
</reference>
<page confidence="0.997143">
732
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.695000">
<title confidence="0.999775">Constraint-Driven Rank-Based Learning for Information Extraction</title>
<author confidence="0.99892">Sameer Singh Limin Yao Sebastian Riedel Andrew</author>
<affiliation confidence="0.9993505">Dept. of Computer University of</affiliation>
<address confidence="0.717132">Amherst MA</address>
<abstract confidence="0.99854295">Most learning algorithms for undirected graphical models require complete inference over at least one instance before parameter updates can be made. SampleRank is a rankbased learning framework that alleviates this problem by updating the parameters during inference. Most semi-supervised learning algorithms also perform full inference on at least one instance before each parameter update. We extend SampleRank to semi-supervised learning in order to circumvent this computational bottleneck. Different approaches to incorporate unlabeled data and prior knowledge into this framework are explored. When evaluated on a standard information extraction dataset, our method significantly outperforms the supervised method, and matches results of a competing state-of-the-art semi-supervised learning approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kedar Bellare</author>
<author>Gregory Druck</author>
<author>Andrew McCallum</author>
</authors>
<title>Alternating projections for learning with expectation constraints.</title>
<date>2009</date>
<booktitle>In UAI,</booktitle>
<contexts>
<context position="6597" citStr="Bellare et al., 2009" startWordPosition="997" endWordPosition="1000">s case. 3.1 Self-Training Self-training, which uses predictions as truth, fits directly into our SampleRank framework. After performing SampleRank on training data (using FL), MAP inference is performed on the unlabeled data. The prediction ˆyU is used as the ground truth for the unlabeled data. Thus the self-training objective function Fs over the unlabeled data can be defined as Fs(y) = −L(y, ˆyU). 3.2 Encoding Constraints Constraint-driven semi-supervised learning uses constraints to incorporate external domain knowledge when labels are missing (Chang et al., 2007; Mann and McCallum, 2008; Bellare et al., 2009). Constraints prefer certain label configurations over others. For example, one constraint may be that occurrences of the word “California” are preferred to have the label “location”. We can encode constraints directly into the objective function FU. Let a constraint i be specified as hpi, cii, where ci(y) denotes whether assignment y satisfies the constraint i (+1), violates it (−1), or the constraint does not apply (0), and pi is the constraint strength. Then the objective function is: Fc(y) = E pici(y) i 3.3 Incorporating Model Predictions When the objective function Fc is used, every predi</context>
<context position="9372" citStr="Bellare et al., 2009" startWordPosition="1454" endWordPosition="1457">hts p used in (Chang et al., 2007). 4 Related Work Chang et al. propose constraint-driven learning (CODL, Chang et al., 2007) which can be interpreted as a variation of self-training: Instances are selected for supervision based not only on the model’s prediction, but also on their consistency with a set of user-defined constraints. By directly incorporating the model score and the constraints (as in .F&apos;mc in Section 3.3) we follow the same approach, but avoid the expensive “Top-K” inference step. Generalized expectation criterion (GE, Mann and McCallum, 2008) and Alternating Projections (AP, Bellare et al., 2009) encode preferences by specifying constraints on feature expectations, which require expensive inference. Although AP can use online training, it still involves full inference over each instance. Furthermore, these methods only support constraints that factorize according to the model. Li (2009) incorporates prior knowledge into conditional random fields as variables. They require full inference during learning, restricting the application to simple models. Furthermore, higher-order constraints are specified using large cliques in the graph, which slow down inference. Our approach directly inc</context>
</contexts>
<marker>Bellare, Druck, McCallum, 2009</marker>
<rawString>Kedar Bellare, Gregory Druck, and Andrew McCallum. Alternating projections for learning with expectation constraints. In UAI, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingwei Chang</author>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Guiding semi-supervision with constraint-driven learning.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<contexts>
<context position="2269" citStr="Chang et al., 2007" startWordPosition="312" endWordPosition="315">the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before parameters are updated also exists in the semi-supervised learning scenario. Here the situation is often considerably worse since inference has to be applied to potentially very large unlabeled datasets. Most semi-supervised learning algorithms rely on marginals (GE, Mann and McCallum, 2008) or MAP assignments (CODL, Chang et al., 2007). Calculating these is computationally inexpensive for many simple tasks (such as classification and regression). However, marginal and MAP inference tends to be expensive for complex structured prediction models (such as the joint information extraction models of Singh et al. (2009)), making semisupervised learning intractable. In this work we employ a fast rank-based learning algorithm for semi-supervised learning to circumvent the inference bottleneck. The ranking function is extended to capture both the preference expressed by the labeled data, and the preference of the domain expert when </context>
<context position="6549" citStr="Chang et al., 2007" startWordPosition="989" endWordPosition="992">defining a truth function FU : YU → &lt; for this case. 3.1 Self-Training Self-training, which uses predictions as truth, fits directly into our SampleRank framework. After performing SampleRank on training data (using FL), MAP inference is performed on the unlabeled data. The prediction ˆyU is used as the ground truth for the unlabeled data. Thus the self-training objective function Fs over the unlabeled data can be defined as Fs(y) = −L(y, ˆyU). 3.2 Encoding Constraints Constraint-driven semi-supervised learning uses constraints to incorporate external domain knowledge when labels are missing (Chang et al., 2007; Mann and McCallum, 2008; Bellare et al., 2009). Constraints prefer certain label configurations over others. For example, one constraint may be that occurrences of the word “California” are preferred to have the label “location”. We can encode constraints directly into the objective function FU. Let a constraint i be specified as hpi, cii, where ci(y) denotes whether assignment y satisfies the constraint i (+1), violates it (−1), or the constraint does not apply (0), and pi is the constraint strength. Then the objective function is: Fc(y) = E pici(y) i 3.3 Incorporating Model Predictions Whe</context>
<context position="8785" citStr="Chang et al., 2007" startWordPosition="1362" endWordPosition="1365">ce, we also propose an objective function that incorporates the model score directly, i.e. .F&apos;mc(y) = log p(y|x, O) + log Z(x) + Am.F&apos;c(y) �= O · f(xi, yi) + Am � pici(y) Api i This objective does not require inference, and also takes into account model confidence. In both objective functions .F&apos;sc and .F&apos;mc, A controls the relative contribution of the constraint preferences to the objective function. With higher A, SampleRank will make updates that never try to violate constraints, while with low A, SampleRank trusts the model more. A corresponds to constraint satisfaction weights p used in (Chang et al., 2007). 4 Related Work Chang et al. propose constraint-driven learning (CODL, Chang et al., 2007) which can be interpreted as a variation of self-training: Instances are selected for supervision based not only on the model’s prediction, but also on their consistency with a set of user-defined constraints. By directly incorporating the model score and the constraints (as in .F&apos;mc in Section 3.3) we follow the same approach, but avoid the expensive “Top-K” inference step. Generalized expectation criterion (GE, Mann and McCallum, 2008) and Alternating Projections (AP, Bellare et al., 2009) encode prefe</context>
<context position="10523" citStr="Chang et al. (2007)" startWordPosition="1629" endWordPosition="1632">s in the graph, which slow down inference. Our approach directly incorporates these constraints into the ranking function, with no impact on inference time. 5 Experiments We carried out experiments on the Cora citation dataset. The task is to segment each citation into different fields, such as “author” and “title”. We use 300 instances as training data, 100 instances as development data, and 100 instances as test data. Some instances from the training data are selected as labeled instances, and the remaining data (including development) as unlabeled. We use the same tokenlabel constraints as Chang et al. (2007). We use the objective functions defined in Section 3, specifically self-training (Self:.F&apos;s), direct constraints (Cons:.F&apos;c), the combination of the two (Self+Cons:.F&apos;sc), and combination of the model score and the constraints (Model+Cons:.F&apos;mc). We set pi = 1.0, a = 1.0, As = 10, and Am = 0.0001. Average token accuracy for 5 runs is reported and compared with CODL1 in Table 1. We also report supervised results from (Chang et al., 2007) and SampleRank. All of our methods show vast improvement over the supervised method for smaller training sizes, but this difference decreases as the training </context>
</contexts>
<marker>Chang, Ratinov, Roth, 2007</marker>
<rawString>Mingwei Chang, Lev Ratinov, and Dan Roth. Guiding semi-supervision with constraint-driven learning. In ACL, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithm.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<contexts>
<context position="1331" citStr="Collins (2002)" startWordPosition="174" endWordPosition="175">putational bottleneck. Different approaches to incorporate unlabeled data and prior knowledge into this framework are explored. When evaluated on a standard information extraction dataset, our method significantly outperforms the supervised method, and matches results of a competing state-of-the-art semi-supervised learning approach. 1 Introduction Most supervised learning algorithms for undirected graphical models require full inference over the dataset (e.g., gradient descent), small subsets of the dataset (e.g., stochastic gradient descent), or at least a single instance (e.g., perceptron, Collins (2002)) before parameter updates are made. Often this is the main computational bottleneck during training. SampleRank (Wick et al., 2009) is a rank-based learning framework that alleviates this problem by performing parameter updates within inference. Every pair of samples generated during inference is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithm. In ACL, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Michael Wick</author>
<author>Andrew McCallum</author>
</authors>
<title>First-order probabilistic models for coreference resolution. In</title>
<date>2007</date>
<booktitle>NAACL/HLT,</booktitle>
<contexts>
<context position="1845" citStr="Culotta et al., 2007" startWordPosition="248" endWordPosition="251">taset (e.g., stochastic gradient descent), or at least a single instance (e.g., perceptron, Collins (2002)) before parameter updates are made. Often this is the main computational bottleneck during training. SampleRank (Wick et al., 2009) is a rank-based learning framework that alleviates this problem by performing parameter updates within inference. Every pair of samples generated during inference is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before parameters are updated also exists in the semi-supervised learning scenario. Here the situation is often considerably worse since inference has to be applied to potentially very large unlabeled datasets. Most semi-supervised learning algorithms rely on marginals (GE, Mann and McCallum, 2008) or MAP assignments (CODL, Chang et al., 2007). Calculating these is computationally inexpensive for many simple tasks (such as classification and regression). However, marginal and MAP inference tends to be expensive for </context>
</contexts>
<marker>Culotta, Wick, McCallum, 2007</marker>
<rawString>Aron Culotta, Michael Wick, and Andrew McCallum. First-order probabilistic models for coreference resolution. In NAACL/HLT, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML,</booktitle>
<contexts>
<context position="3380" citStr="Lafferty et al., 2001" startWordPosition="472" endWordPosition="475">ded to capture both the preference expressed by the labeled data, and the preference of the domain expert when the labels are not available. This allows us to perform SampleRank as is, without sacrificing its scalability, which is crucial for future large scale applications of semi-supervised learning. We applied our method to a standard information extraction dataset used for semi-supervised learning. Empirically we demonstrate improvements over the supervised model, and closely match the results of a competing state-of-the-art semi-supervised learner. 2 Background Conditional random fields (Lafferty et al., 2001) are undirected graphical models represented as factor 729 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 729–732, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics graphs. A factor graph G = {Ψi} defines a probability distribution over assignments y to a set of output variables, conditioned on an observation x. A factor Ψi computes the inner product between the vector of sufficient statistics f(xi, yi) and parameters Θ. Let Z(x) be the data-dependent partition function used for normalization. The proba</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In ICML, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Li</author>
</authors>
<title>On the use of virtual evidence in conditional random fields.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<contexts>
<context position="9668" citStr="Li (2009)" startWordPosition="1499" endWordPosition="1500"> user-defined constraints. By directly incorporating the model score and the constraints (as in .F&apos;mc in Section 3.3) we follow the same approach, but avoid the expensive “Top-K” inference step. Generalized expectation criterion (GE, Mann and McCallum, 2008) and Alternating Projections (AP, Bellare et al., 2009) encode preferences by specifying constraints on feature expectations, which require expensive inference. Although AP can use online training, it still involves full inference over each instance. Furthermore, these methods only support constraints that factorize according to the model. Li (2009) incorporates prior knowledge into conditional random fields as variables. They require full inference during learning, restricting the application to simple models. Furthermore, higher-order constraints are specified using large cliques in the graph, which slow down inference. Our approach directly incorporates these constraints into the ranking function, with no impact on inference time. 5 Experiments We carried out experiments on the Cora citation dataset. The task is to segment each citation into different fields, such as “author” and “title”. We use 300 instances as training data, 100 ins</context>
</contexts>
<marker>Li, 2009</marker>
<rawString>Xiao Li. On the use of virtual evidence in conditional random fields. In EMNLP, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized expectation criteria for semi-supervised learning of conditional random fields.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<contexts>
<context position="2223" citStr="Mann and McCallum, 2008" startWordPosition="304" endWordPosition="307"> generated during inference is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before parameters are updated also exists in the semi-supervised learning scenario. Here the situation is often considerably worse since inference has to be applied to potentially very large unlabeled datasets. Most semi-supervised learning algorithms rely on marginals (GE, Mann and McCallum, 2008) or MAP assignments (CODL, Chang et al., 2007). Calculating these is computationally inexpensive for many simple tasks (such as classification and regression). However, marginal and MAP inference tends to be expensive for complex structured prediction models (such as the joint information extraction models of Singh et al. (2009)), making semisupervised learning intractable. In this work we employ a fast rank-based learning algorithm for semi-supervised learning to circumvent the inference bottleneck. The ranking function is extended to capture both the preference expressed by the labeled data,</context>
<context position="6574" citStr="Mann and McCallum, 2008" startWordPosition="993" endWordPosition="996">ction FU : YU → &lt; for this case. 3.1 Self-Training Self-training, which uses predictions as truth, fits directly into our SampleRank framework. After performing SampleRank on training data (using FL), MAP inference is performed on the unlabeled data. The prediction ˆyU is used as the ground truth for the unlabeled data. Thus the self-training objective function Fs over the unlabeled data can be defined as Fs(y) = −L(y, ˆyU). 3.2 Encoding Constraints Constraint-driven semi-supervised learning uses constraints to incorporate external domain knowledge when labels are missing (Chang et al., 2007; Mann and McCallum, 2008; Bellare et al., 2009). Constraints prefer certain label configurations over others. For example, one constraint may be that occurrences of the word “California” are preferred to have the label “location”. We can encode constraints directly into the objective function FU. Let a constraint i be specified as hpi, cii, where ci(y) denotes whether assignment y satisfies the constraint i (+1), violates it (−1), or the constraint does not apply (0), and pi is the constraint strength. Then the objective function is: Fc(y) = E pici(y) i 3.3 Incorporating Model Predictions When the objective function </context>
<context position="9317" citStr="Mann and McCallum, 2008" startWordPosition="1446" endWordPosition="1449"> model more. A corresponds to constraint satisfaction weights p used in (Chang et al., 2007). 4 Related Work Chang et al. propose constraint-driven learning (CODL, Chang et al., 2007) which can be interpreted as a variation of self-training: Instances are selected for supervision based not only on the model’s prediction, but also on their consistency with a set of user-defined constraints. By directly incorporating the model score and the constraints (as in .F&apos;mc in Section 3.3) we follow the same approach, but avoid the expensive “Top-K” inference step. Generalized expectation criterion (GE, Mann and McCallum, 2008) and Alternating Projections (AP, Bellare et al., 2009) encode preferences by specifying constraints on feature expectations, which require expensive inference. Although AP can use online training, it still involves full inference over each instance. Furthermore, these methods only support constraints that factorize according to the model. Li (2009) incorporates prior knowledge into conditional random fields as variables. They require full inference during learning, restricting the application to simple models. Furthermore, higher-order constraints are specified using large cliques in the grap</context>
</contexts>
<marker>Mann, McCallum, 2008</marker>
<rawString>Gideon S. Mann and Andrew McCallum. Generalized expectation criteria for semi-supervised learning of conditional random fields. In ACL, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Karl Schultz</author>
<author>Sameer Singh</author>
</authors>
<title>FACTORIE: probabilistic programming via imperatively defined factor graphs.</title>
<date>2009</date>
<booktitle>In NIPS,</booktitle>
<contexts>
<context position="5630" citStr="McCallum et al., 2009" startWordPosition="840" endWordPosition="843">with objective rankings, SampleRank performs the following update for each consecutive pair of samples ya and yb of the MCMC chain. Let α be the learning rate, and Δ = f(xi, yai ) − f(xi, ybi), then Θ is updated as follows: αΔ if p(y&amp;quot;x) p(y��x)&lt; 1 ∧ F(ya) &gt; F(yb) −αΔ if p(y��x) p(y��x) &gt; 1 ∧ F(ya) &lt; F(yb) 0 otherwise. This update is usually fast: in order to calculate the required model ratio, only factors that touch changed variables have to be taken into account. SampleRank has been incorporated into the FACTORIE toolkit for probabilistic programming with imperatively-defined factor graphs (McCallum et al., 2009). 3 Semi-Supervised Rank-Based Learning To apply SampleRank to the semi-supervised setting, we need to specify the truth function F over both labeled and unlabeled data. For labeled data YL, we can use the true labels. These are not available for unlabeled data YU, and we present alternative ways of defining a truth function FU : YU → &lt; for this case. 3.1 Self-Training Self-training, which uses predictions as truth, fits directly into our SampleRank framework. After performing SampleRank on training data (using FL), MAP inference is performed on the unlabeled data. The prediction ˆyU is used a</context>
</contexts>
<marker>McCallum, Schultz, Singh, 2009</marker>
<rawString>Andrew McCallum, Karl Schultz, and Sameer Singh. FACTORIE: probabilistic programming via imperatively defined factor graphs. In NIPS, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Karl Schultz</author>
<author>Andrew McCallum</author>
</authors>
<title>Bi-directional joint inference for entity resolution and segmentation using imperatively-defined factor graphs.</title>
<date>2009</date>
<booktitle>In ECML/PKDD,</booktitle>
<contexts>
<context position="1866" citStr="Singh et al., 2009" startWordPosition="252" endWordPosition="255">c gradient descent), or at least a single instance (e.g., perceptron, Collins (2002)) before parameter updates are made. Often this is the main computational bottleneck during training. SampleRank (Wick et al., 2009) is a rank-based learning framework that alleviates this problem by performing parameter updates within inference. Every pair of samples generated during inference is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before parameters are updated also exists in the semi-supervised learning scenario. Here the situation is often considerably worse since inference has to be applied to potentially very large unlabeled datasets. Most semi-supervised learning algorithms rely on marginals (GE, Mann and McCallum, 2008) or MAP assignments (CODL, Chang et al., 2007). Calculating these is computationally inexpensive for many simple tasks (such as classification and regression). However, marginal and MAP inference tends to be expensive for complex structured pr</context>
<context position="4544" citStr="Singh et al., 2009" startWordPosition="660" endWordPosition="663">nt partition function used for normalization. The probability distribution defined by the graph is: p(y|x, Θ) =1 ri Z(x) ΨiEG eΘ&apos;f(xi,yi) 2.1 Rank-Based Learning SampleRank (Wick et al., 2009) is a rank-based learning framework for that performs parameter updates within MCMC inference. Every pair of consecutive samples in the MCMC chain is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. This allows the learner to acquire more supervision per sample, and has led to efficient training of models for which inference is very expensive (Singh et al., 2009). SampleRank considers two ranking functions: (1) the unnormalized conditional probability (model ranking), and (2) a truth function F(y) (objective ranking) which is defined as −L(y, yL), the negative loss between the possible assignment y and the true assignment yL. The truth function can take different forms, such as tokenwise accuracy or F1- measure with respect to some labeled data. In order to learn the parameters for which model rankings are consistent with objective rankings, SampleRank performs the following update for each consecutive pair of samples ya and yb of the MCMC chain. Let </context>
</contexts>
<marker>Singh, Schultz, McCallum, 2009</marker>
<rawString>Sameer Singh, Karl Schultz, and Andrew McCallum. Bi-directional joint inference for entity resolution and segmentation using imperatively-defined factor graphs. In ECML/PKDD, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wick</author>
<author>Khashayar Rohanimanesh</author>
<author>Aron Culotta</author>
<author>Andrew McCallum</author>
</authors>
<title>SampleRank: Learning preferences from atomic gradients.</title>
<date>2009</date>
<booktitle>In NIPS Workshop on Advances in Ranking,</booktitle>
<contexts>
<context position="1463" citStr="Wick et al., 2009" startWordPosition="191" endWordPosition="194">When evaluated on a standard information extraction dataset, our method significantly outperforms the supervised method, and matches results of a competing state-of-the-art semi-supervised learning approach. 1 Introduction Most supervised learning algorithms for undirected graphical models require full inference over the dataset (e.g., gradient descent), small subsets of the dataset (e.g., stochastic gradient descent), or at least a single instance (e.g., perceptron, Collins (2002)) before parameter updates are made. Often this is the main computational bottleneck during training. SampleRank (Wick et al., 2009) is a rank-based learning framework that alleviates this problem by performing parameter updates within inference. Every pair of samples generated during inference is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before parameters are updated also exists in the semi-supervised learning scenario. Here the situation is often considerably worse since in</context>
<context position="4117" citStr="Wick et al., 2009" startWordPosition="587" endWordPosition="590">he North American Chapter of the ACL, pages 729–732, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics graphs. A factor graph G = {Ψi} defines a probability distribution over assignments y to a set of output variables, conditioned on an observation x. A factor Ψi computes the inner product between the vector of sufficient statistics f(xi, yi) and parameters Θ. Let Z(x) be the data-dependent partition function used for normalization. The probability distribution defined by the graph is: p(y|x, Θ) =1 ri Z(x) ΨiEG eΘ&apos;f(xi,yi) 2.1 Rank-Based Learning SampleRank (Wick et al., 2009) is a rank-based learning framework for that performs parameter updates within MCMC inference. Every pair of consecutive samples in the MCMC chain is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. This allows the learner to acquire more supervision per sample, and has led to efficient training of models for which inference is very expensive (Singh et al., 2009). SampleRank considers two ranking functions: (1) the unnormalized conditional probability (model ranking), and (2) a truth function F(y) (objective ranking) which is define</context>
</contexts>
<marker>Wick, Rohanimanesh, Culotta, McCallum, 2009</marker>
<rawString>Michael Wick, Khashayar Rohanimanesh, Aron Culotta, and Andrew McCallum. SampleRank: Learning preferences from atomic gradients. In NIPS Workshop on Advances in Ranking, 2009.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>