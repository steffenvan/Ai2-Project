<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.075878">
<title confidence="0.976143">
Cognates Can Improve Statistical Translation Models
</title>
<author confidence="0.994541">
Grzegorz Kondrak
</author>
<affiliation confidence="0.9978635">
Department of Computing Science
University of Alberta
</affiliation>
<address confidence="0.92592">
221 Athabasca Hall
Edmonton, AB, Canada T6G 2E8
</address>
<email confidence="0.999221">
kondrak@cs.ualberta.edu
</email>
<author confidence="0.992564">
Daniel Marcu and Kevin Knight
</author>
<affiliation confidence="0.797611">
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA, 90292
</affiliation>
<email confidence="0.999207">
marcu,knight@isi.edu
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999726285714286">
We report results of experiments aimed at im-
proving the translation quality by incorporating
the cognate information into translation mod-
els. The results confirm that the cognate iden-
tification approach can improve the quality of
word alignment in bitexts without the need for
extra resources.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980590909091">
In the context of machine translation, the term cognates
denotes words in different languages that are similar
in their orthographic or phonetic form and are possible
translations of each other. The similarity is usually due
either to a genetic relationship (e.g. English night and
German nacht) or borrowing from one language to an-
other (e.g. English sprint and Japanese supurinto). In
a broad sense, cognates include not only genetically re-
lated words and borrowings but also names, numbers, and
punctuation. Practically all bitexts (bilingual parallel cor-
pora) contain some kind of cognates. If the languages are
represented in different scripts, a phonetic transcription
or transliteration of one or both parts of the bitext is a
pre-requisite for identifying cognates.
Cognates have been employed for a number of bitext-
related tasks, including sentence alignment (Simard et
al., 1992), inducing translation lexicons (Mann and Ya-
rowsky, 2001), and improving statistical machine trans-
lation models (Al-Onaizan et al., 1999). Cognates are
particularly useful when machine-readable bilingual dic-
tionaries are not available. Al-Onaizan et al. (1999) ex-
perimented with using bilingual dictionaries and cog-
nates in the training of Czech–English translation mod-
els. They found that appending probable cognates to the
training bitext significantly lowered the perplexity score
on the test bitext (in some cases more than when using a
bilingual dictionary), and observed improvement in word
alignments of test sentences.
In this paper, we investigate the problem of incorpo-
rating the potentially valuable cognate information into
the translation models of Brown et al. (1990), which, in
their original formulation, consider lexical items in ab-
straction of their form. For training of the models, we
use the GIZA program (Al-Onaizan et al., 1999). A list
of likely cognate pairs is extracted from the training cor-
pus on the basis of orthographic similarity, and appended
to the corpus itself. The objective is to reinforce the co-
ocurrence count between cognates in addition to already
existing co-ocurrences. The results of experiments con-
ducted on a variety of bitexts show that cognate iden-
tification can improve word alignments, which leads to
better translation models, and, consequently, translations
of higher quality. The improvement is achieved without
modifying the statistical training algorithm.
</bodyText>
<sectionHeader confidence="0.968231" genericHeader="method">
2 The method
</sectionHeader>
<bodyText confidence="0.999954027777778">
We experimented with three word similarity measu-
res: Simard’s condition, Dice’s coefficient, and LCSR.
Simard et al. (1992) proposed a simple condition for de-
tecting probable cognates in French–English bitexts: two
words are considered cognates if they are at least four
characters long and their first four characters are iden-
tical. Dice’s coefficient is defined as the ratio of the
number of shared character bigrams to the total num-
ber of bigrams in both words. For example, colour and
couleur share three bigrams (co, ou, and ur), so their
Dice’s coefficient is s - 0.55. The Longest Common
Subsequence Ratio (LCSR) of two words is computed
by dividing the length of their longest common subse-
quence by the length of the longer word. For example,
LCSR(colour,couleur) = 7 0.71, as their longest com-
mon subsequence is “c-o-l-u-r”.
In order to identify a set of likely cognates in a tok-
enized and sentence-aligned bitext, each aligned segment
is split into words, and all possible word pairings are
stored in a file. Numbers and punctuation are not con-
sidered, since we feel that they warrant a more specific
approach. After sorting and removing duplicates, the file
represents all possible one-to-one word alignments of the
bitext. Also removed are the pairs that include English
function words, and words shorter than the minimum
length (usually set at four characters). For each word pair,
a similarity measure is computed, and the file is again
sorted, this time by the computed similarity value. If the
measure returns a non-binary similarity value, true cog-
nates are very frequent near the top of the list, and be-
come less frequent towards the bottom. The set of likely
cognates is obtained by selecting all pairs with similarity
above a certain threshold. Typically, lowering the thresh-
old increases recall while decreasing precision of the set.
Finally, one or more copies of the resulting set of likely
cognates are concatenated with the training set.
</bodyText>
<sectionHeader confidence="0.99957" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9999932">
We induced translation models using IBM Model 4
(Brown et al., 1990) with the GIZA toolkit (Al-Onaizan
et al., 1999). The maximum sentence length in the train-
ing data was set at 30 words. The actual translations
were produced with a greedy decoder (Germann et al.,
2001). For the evaluation of translation quality, we used
the BLEU metric (Papineni et al., 2002), which measures
the n-gram overlap between the translated output and one
or more reference translations. In our experiments, we
used only one reference translation.
</bodyText>
<subsectionHeader confidence="0.996293">
3.1 Word alignment quality
</subsectionHeader>
<bodyText confidence="0.9998065">
In order to directly measure the influence of the added
cognate information on the word alignment quality, we
performed a single experiment using a set of 500 man-
ually aligned sentences from Hansards (Och and Ney,
2000). Giza was first trained on 50,000 sentences from
Hansards, and then on the same training set augmented
with a set of cognates. The set consisted of two copies of
a list produced by applying the threshold of 0.58 to LCSR
list. The duplication factor was arbitrarily selected on the
basis of earlier experiments with a different training and
test set taken from Hansards.
The incorporation of the cognate information resulted
in a 10% reduction of the word alignment error rate,
from 17.6% to 15.8%, and a corresponding improvement
in both precision and recall. An examination of ran-
domly selected alignments confirms the observation of
Al-Onaizan et al. (1999) that the use of cognate informa-
tion reduces the tendency of rare words to align to many
co-occurring words.
In another experiment, we concentrated on co-oc-
curring identical words, which are extremely likely to
represent mutual translations. In the baseline model,
links were induced between 93.6% of identical words. In
the cognate-augmented model, the ratio rose to 97.2%.
</bodyText>
<subsectionHeader confidence="0.964944">
3.2 Europarl
</subsectionHeader>
<bodyText confidence="0.980355">
Europarl is a tokenized and sentence-aligned multilingual
corpus extracted from the Proceedings of the European
</bodyText>
<figure confidence="0.9841625">
0 1 2 3 4 5 6
Duplication factor
</figure>
<figureCaption confidence="0.99869">
Figure 1: BLEU scores as a function of the duplication
</figureCaption>
<bodyText confidence="0.992615161290323">
factor for five methods of cognates identification aver-
aged over nine language pairs.
Parliament (Koehn, 2002). The eleven official European
Union languages are represented in the corpus. We con-
sider the variety of languages as important for a valida-
tion of the cognate-based approach as general, rather than
language-specific.
As the training data, we arbitrarily selected a subset of
the corpus that consisted the proceedings from October
1998. By pairing English with the remaining languages,
we obtained nine bitexts1, each comprising about 20,000
aligned sentences (500,000 words). The test data con-
sisted of 1755 unseen sentences varying in length from 5
to 15 words from the 2000 proceedings (Koehn, 2002).
The English language model was trained separately on a
larger set of 700,000 sentences from the 1996 proceed-
ings.
Figure 1 shows the BLEU scores as a function of the
duplication factor for three methods of cognates identi-
fication averaged over nine language pairs. The results
averaged over a number of language pairs are more in-
formative than results obtained on a single language pair,
especially since the BLEU metric is only a rough approx-
imation of the translation quality, and exhibits consider-
able variance. Three different similarity measures were
compared: Simard, DICE with a threshold of 0.39, and
LCSR with a threshold of 0.58. In addition, we experi-
mented with two different methods of extending the train-
ing set with with a list of cognates: one pair as one sen-
tence (Simard), and thirty pairs as one sentence (DICE
and LCSR).2
</bodyText>
<footnote confidence="0.981408">
1Greek was excluded because its non-Latin script requires a
different type of approach to cognate identification.
2In the vast majority of the sentences, the alignment links are
correctly induced between the respective cognates when multi-
</footnote>
<figure confidence="0.598731090909091">
0.208
0.207
0.206
0.205
0.204
0.203
0.202
&amp;quot;Simard&amp;quot;
&amp;quot;DICE&amp;quot;
&amp;quot;LCSR&amp;quot;
BLEU score
</figure>
<table confidence="0.998014857142857">
Threshold Pairs Score
Baseline 0 0.2027
0.99 863 0.2016
0.71 2835 0.2030
0.58 5339 0.2058
0.51 7343 0.2073
0.49 14115 0.2059
</table>
<tableCaption confidence="0.999175">
Table 1: The number of extracted word pairs as a func-
</tableCaption>
<bodyText confidence="0.98986328">
tion of the LCSR threshold, and the corresponding BLEU
scores, averaged over nine Europarl bitexts.
The results show a statistically significant improve-
ment3 in the average BLEU score when the duplication
factor is greater than 1, but no clear trend can be discerned
for larger factors. There does not seem to be much differ-
ence between various methods of cognate identification.
Table 1 shows results of augmenting the training set
with different sets of cognates determined using LCSR.
A threshold of 0.99 implies that only identical word
pairs are admitted as cognates. The words pairs with
LCSR around 0.5 are more likely than not to be unre-
lated. In each case two copies of the cognate list were
used. The somewhat surprising result was that adding
only ”high confidence” cognates is less effective than
adding lots of dubious cognates. In that particular set
of tests, adding only identical word pairs, which almost
always are mutual translations, actually decreased the
BLEU score. Our results are consistent with the results
ofAl-Onaizan et al. (1999), who observed perplexity im-
provement even when “extremely low” thresholds were
used. It seems that the robust statistical training algo-
rithm has the ability of ignoring the unrelated word pairs,
while at the same time utilizing the information provided
by the true cognates.
</bodyText>
<subsectionHeader confidence="0.996499">
3.3 A manual evaluation
</subsectionHeader>
<bodyText confidence="0.993516947368421">
In order to confirm that the higher BLEU scores reflect
higher translation quality, we performed a manual evalua-
tion of a set of a hundred six-token sentences. The models
were induced on a 25,000 sentences portion of Hansards.
The training set was augmented with two copies of a cog-
nate list obtained by thresholding LCSR at 0.56. Results
ple pairs per sentence are added.
3Statistical significance was estimated in the following way.
The variance of the BLEU score was approximated by randomly
picking a sample of translated sentences from the test set. The
size of the test sample was equal to the size of the test set (1755
sentences). The score was computed in this way 200 times for
each language. The mean and the variance of the nine-language
average was computed by randomly picking one of the 200
scores for each language and computing the average. The mean
result produced was 0.2025, which is very close to the baseline
average score of 0.2027. The standard deviation of the average
was estimated to be 0.0018, which implies that averages above
0.2054 are statistically significant at the 0.95 level.
</bodyText>
<table confidence="0.996464666666667">
Evaluation Baseline Cognates
Completely correct 16 21
Syntactically correct 8 7
Semantically correct 14 12
Wrong 62 60
Total 100 100
</table>
<tableCaption confidence="0.968354">
Table 2: A manual evaluation of the translations gener-
ated by the baseline and the cognate-augmented models.
</tableCaption>
<bodyText confidence="0.999790166666667">
of a manual evaluation of the entire set of 100 sentences
are shown in Table 2. Although the overall translation
quality is low due to the small size of the training corpus
and the lack of parameter tuning, the number of com-
pletely acceptable translations is higher when cognates
are added.
</bodyText>
<sectionHeader confidence="0.999704" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999972916666667">
Our experimental results show that the incorporation of
cognate information can improve the quality of word
alignments, which in turn result in better translations, In
our experiments, the improvement, although statistically
significant, is relatively small, which can be attributed to
the relative crudeness of the approach based on append-
ing the cognate pairs directly to the training data. In the
future, we plan to develop a method of incorporating the
cognate information directly into the training algorithm.
We foresee that the performance of such a method will
also depend on using more sophisticated word similarity
measures.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999583318181818">
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty,
D. Melamed, F. Och, D. Purdy, N. Smith, and D. Yarowsky.
1999. Statistical machine translation. Technical report,
Johns Hopkins University.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1990.
The mathematics of statistical machine translation: Parame-
ter estimation. Computational Linguistics, 19(2):263–311.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada.
2001. Fast decoding and optimal decoding for machine
translation. In Proceedings ofACL-01.
P. Koehn. 2002. Europarl: A multilingual corpus for evaluation
of machine translation. In preparation.
G. Mann and D. Yarowsky. Multipath translation lexicon induc-
tion via bridge languages. In Proceedings ofNAACL 2001.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proceedings ofACL-00.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a
method for automatic evaluation of machine translation. In
Proceedings ofACL-02.
M. Simard, G. F. Foster, and P. Isabelle. 1992. Using cognates
to align sentences in bilingual corpora. In Proceedings of
TMI-92.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.561281">
<title confidence="0.996329">Cognates Can Improve Statistical Translation Models</title>
<author confidence="0.941565">Grzegorz</author>
<affiliation confidence="0.99992">Department of Computing University of Alberta</affiliation>
<address confidence="0.9483185">221 Athabasca Edmonton, AB, Canada T6G</address>
<email confidence="0.999276">kondrak@cs.ualberta.edu</email>
<author confidence="0.681243">Marcu</author>
<affiliation confidence="0.9986115">Information Sciences University of Southern</affiliation>
<address confidence="0.994384">4676 Admiralty Way, Suite</address>
<author confidence="0.993456">Marina del Rey</author>
<author confidence="0.993456">CA</author>
<email confidence="0.999139">marcu,knight@isi.edu</email>
<abstract confidence="0.998480625">We report results of experiments aimed at improving the translation quality by incorporating the cognate information into translation models. The results confirm that the cognate identification approach can improve the quality of word alignment in bitexts without the need for extra resources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>J Curin</author>
<author>M Jahr</author>
<author>K Knight</author>
<author>J Lafferty</author>
<author>D Melamed</author>
<author>F Och</author>
<author>D Purdy</author>
<author>N Smith</author>
<author>D Yarowsky</author>
</authors>
<title>Statistical machine translation.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="1712" citStr="Al-Onaizan et al., 1999" startWordPosition="247" endWordPosition="250">cognates include not only genetically related words and borrowings but also names, numbers, and punctuation. Practically all bitexts (bilingual parallel corpora) contain some kind of cognates. If the languages are represented in different scripts, a phonetic transcription or transliteration of one or both parts of the bitext is a pre-requisite for identifying cognates. Cognates have been employed for a number of bitextrelated tasks, including sentence alignment (Simard et al., 1992), inducing translation lexicons (Mann and Yarowsky, 2001), and improving statistical machine translation models (Al-Onaizan et al., 1999). Cognates are particularly useful when machine-readable bilingual dictionaries are not available. Al-Onaizan et al. (1999) experimented with using bilingual dictionaries and cognates in the training of Czech–English translation models. They found that appending probable cognates to the training bitext significantly lowered the perplexity score on the test bitext (in some cases more than when using a bilingual dictionary), and observed improvement in word alignments of test sentences. In this paper, we investigate the problem of incorporating the potentially valuable cognate information into t</context>
<context position="5193" citStr="Al-Onaizan et al., 1999" startWordPosition="802" endWordPosition="805">by the computed similarity value. If the measure returns a non-binary similarity value, true cognates are very frequent near the top of the list, and become less frequent towards the bottom. The set of likely cognates is obtained by selecting all pairs with similarity above a certain threshold. Typically, lowering the threshold increases recall while decreasing precision of the set. Finally, one or more copies of the resulting set of likely cognates are concatenated with the training set. 3 Experiments We induced translation models using IBM Model 4 (Brown et al., 1990) with the GIZA toolkit (Al-Onaizan et al., 1999). The maximum sentence length in the training data was set at 30 words. The actual translations were produced with a greedy decoder (Germann et al., 2001). For the evaluation of translation quality, we used the BLEU metric (Papineni et al., 2002), which measures the n-gram overlap between the translated output and one or more reference translations. In our experiments, we used only one reference translation. 3.1 Word alignment quality In order to directly measure the influence of the added cognate information on the word alignment quality, we performed a single experiment using a set of 500 ma</context>
<context position="6511" citStr="Al-Onaizan et al. (1999)" startWordPosition="1017" endWordPosition="1020">entences from Hansards, and then on the same training set augmented with a set of cognates. The set consisted of two copies of a list produced by applying the threshold of 0.58 to LCSR list. The duplication factor was arbitrarily selected on the basis of earlier experiments with a different training and test set taken from Hansards. The incorporation of the cognate information resulted in a 10% reduction of the word alignment error rate, from 17.6% to 15.8%, and a corresponding improvement in both precision and recall. An examination of randomly selected alignments confirms the observation of Al-Onaizan et al. (1999) that the use of cognate information reduces the tendency of rare words to align to many co-occurring words. In another experiment, we concentrated on co-occurring identical words, which are extremely likely to represent mutual translations. In the baseline model, links were induced between 93.6% of identical words. In the cognate-augmented model, the ratio rose to 97.2%. 3.2 Europarl Europarl is a tokenized and sentence-aligned multilingual corpus extracted from the Proceedings of the European 0 1 2 3 4 5 6 Duplication factor Figure 1: BLEU scores as a function of the duplication factor for f</context>
<context position="10206" citStr="Al-Onaizan et al. (1999)" startWordPosition="1614" endWordPosition="1617">th different sets of cognates determined using LCSR. A threshold of 0.99 implies that only identical word pairs are admitted as cognates. The words pairs with LCSR around 0.5 are more likely than not to be unrelated. In each case two copies of the cognate list were used. The somewhat surprising result was that adding only ”high confidence” cognates is less effective than adding lots of dubious cognates. In that particular set of tests, adding only identical word pairs, which almost always are mutual translations, actually decreased the BLEU score. Our results are consistent with the results ofAl-Onaizan et al. (1999), who observed perplexity improvement even when “extremely low” thresholds were used. It seems that the robust statistical training algorithm has the ability of ignoring the unrelated word pairs, while at the same time utilizing the information provided by the true cognates. 3.3 A manual evaluation In order to confirm that the higher BLEU scores reflect higher translation quality, we performed a manual evaluation of a set of a hundred six-token sentences. The models were induced on a 25,000 sentences portion of Hansards. The training set was augmented with two copies of a cognate list obtained</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty, D. Melamed, F. Och, D. Purdy, N. Smith, and D. Yarowsky. 1999. Statistical machine translation. Technical report, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2356" citStr="Brown et al. (1990)" startWordPosition="342" endWordPosition="345">ly useful when machine-readable bilingual dictionaries are not available. Al-Onaizan et al. (1999) experimented with using bilingual dictionaries and cognates in the training of Czech–English translation models. They found that appending probable cognates to the training bitext significantly lowered the perplexity score on the test bitext (in some cases more than when using a bilingual dictionary), and observed improvement in word alignments of test sentences. In this paper, we investigate the problem of incorporating the potentially valuable cognate information into the translation models of Brown et al. (1990), which, in their original formulation, consider lexical items in abstraction of their form. For training of the models, we use the GIZA program (Al-Onaizan et al., 1999). A list of likely cognate pairs is extracted from the training corpus on the basis of orthographic similarity, and appended to the corpus itself. The objective is to reinforce the coocurrence count between cognates in addition to already existing co-ocurrences. The results of experiments conducted on a variety of bitexts show that cognate identification can improve word alignments, which leads to better translation models, an</context>
<context position="5145" citStr="Brown et al., 1990" startWordPosition="794" endWordPosition="797">d, and the file is again sorted, this time by the computed similarity value. If the measure returns a non-binary similarity value, true cognates are very frequent near the top of the list, and become less frequent towards the bottom. The set of likely cognates is obtained by selecting all pairs with similarity above a certain threshold. Typically, lowering the threshold increases recall while decreasing precision of the set. Finally, one or more copies of the resulting set of likely cognates are concatenated with the training set. 3 Experiments We induced translation models using IBM Model 4 (Brown et al., 1990) with the GIZA toolkit (Al-Onaizan et al., 1999). The maximum sentence length in the training data was set at 30 words. The actual translations were produced with a greedy decoder (Germann et al., 2001). For the evaluation of translation quality, we used the BLEU metric (Papineni et al., 2002), which measures the n-gram overlap between the translated output and one or more reference translations. In our experiments, we used only one reference translation. 3.1 Word alignment quality In order to directly measure the influence of the added cognate information on the word alignment quality, we per</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1990</marker>
<rawString>P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1990. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Germann</author>
<author>M Jahr</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>K Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL-01.</booktitle>
<contexts>
<context position="5347" citStr="Germann et al., 2001" startWordPosition="829" endWordPosition="832">less frequent towards the bottom. The set of likely cognates is obtained by selecting all pairs with similarity above a certain threshold. Typically, lowering the threshold increases recall while decreasing precision of the set. Finally, one or more copies of the resulting set of likely cognates are concatenated with the training set. 3 Experiments We induced translation models using IBM Model 4 (Brown et al., 1990) with the GIZA toolkit (Al-Onaizan et al., 1999). The maximum sentence length in the training data was set at 30 words. The actual translations were produced with a greedy decoder (Germann et al., 2001). For the evaluation of translation quality, we used the BLEU metric (Papineni et al., 2002), which measures the n-gram overlap between the translated output and one or more reference translations. In our experiments, we used only one reference translation. 3.1 Word alignment quality In order to directly measure the influence of the added cognate information on the word alignment quality, we performed a single experiment using a set of 500 manually aligned sentences from Hansards (Och and Ney, 2000). Giza was first trained on 50,000 sentences from Hansards, and then on the same training set au</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proceedings ofACL-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A multilingual corpus for evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In preparation.</booktitle>
<contexts>
<context position="7209" citStr="Koehn, 2002" startWordPosition="1129" endWordPosition="1130">ny co-occurring words. In another experiment, we concentrated on co-occurring identical words, which are extremely likely to represent mutual translations. In the baseline model, links were induced between 93.6% of identical words. In the cognate-augmented model, the ratio rose to 97.2%. 3.2 Europarl Europarl is a tokenized and sentence-aligned multilingual corpus extracted from the Proceedings of the European 0 1 2 3 4 5 6 Duplication factor Figure 1: BLEU scores as a function of the duplication factor for five methods of cognates identification averaged over nine language pairs. Parliament (Koehn, 2002). The eleven official European Union languages are represented in the corpus. We consider the variety of languages as important for a validation of the cognate-based approach as general, rather than language-specific. As the training data, we arbitrarily selected a subset of the corpus that consisted the proceedings from October 1998. By pairing English with the remaining languages, we obtained nine bitexts1, each comprising about 20,000 aligned sentences (500,000 words). The test data consisted of 1755 unseen sentences varying in length from 5 to 15 words from the 2000 proceedings (Koehn, 200</context>
</contexts>
<marker>Koehn, 2002</marker>
<rawString>P. Koehn. 2002. Europarl: A multilingual corpus for evaluation of machine translation. In preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mann</author>
<author>D Yarowsky</author>
</authors>
<title>Multipath translation lexicon induction via bridge languages.</title>
<date>2001</date>
<booktitle>In Proceedings ofNAACL</booktitle>
<contexts>
<context position="1632" citStr="Mann and Yarowsky, 2001" startWordPosition="235" endWordPosition="239">uage to another (e.g. English sprint and Japanese supurinto). In a broad sense, cognates include not only genetically related words and borrowings but also names, numbers, and punctuation. Practically all bitexts (bilingual parallel corpora) contain some kind of cognates. If the languages are represented in different scripts, a phonetic transcription or transliteration of one or both parts of the bitext is a pre-requisite for identifying cognates. Cognates have been employed for a number of bitextrelated tasks, including sentence alignment (Simard et al., 1992), inducing translation lexicons (Mann and Yarowsky, 2001), and improving statistical machine translation models (Al-Onaizan et al., 1999). Cognates are particularly useful when machine-readable bilingual dictionaries are not available. Al-Onaizan et al. (1999) experimented with using bilingual dictionaries and cognates in the training of Czech–English translation models. They found that appending probable cognates to the training bitext significantly lowered the perplexity score on the test bitext (in some cases more than when using a bilingual dictionary), and observed improvement in word alignments of test sentences. In this paper, we investigate </context>
</contexts>
<marker>Mann, Yarowsky, 2001</marker>
<rawString>G. Mann and D. Yarowsky. Multipath translation lexicon induction via bridge languages. In Proceedings ofNAACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL-00.</booktitle>
<contexts>
<context position="5851" citStr="Och and Ney, 2000" startWordPosition="909" endWordPosition="912">ining data was set at 30 words. The actual translations were produced with a greedy decoder (Germann et al., 2001). For the evaluation of translation quality, we used the BLEU metric (Papineni et al., 2002), which measures the n-gram overlap between the translated output and one or more reference translations. In our experiments, we used only one reference translation. 3.1 Word alignment quality In order to directly measure the influence of the added cognate information on the word alignment quality, we performed a single experiment using a set of 500 manually aligned sentences from Hansards (Och and Ney, 2000). Giza was first trained on 50,000 sentences from Hansards, and then on the same training set augmented with a set of cognates. The set consisted of two copies of a list produced by applying the threshold of 0.58 to LCSR list. The duplication factor was arbitrarily selected on the basis of earlier experiments with a different training and test set taken from Hansards. The incorporation of the cognate information resulted in a 10% reduction of the word alignment error rate, from 17.6% to 15.8%, and a corresponding improvement in both precision and recall. An examination of randomly selected ali</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. Improved statistical alignment models. In Proceedings ofACL-00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL-02.</booktitle>
<contexts>
<context position="5439" citStr="Papineni et al., 2002" startWordPosition="844" endWordPosition="847">airs with similarity above a certain threshold. Typically, lowering the threshold increases recall while decreasing precision of the set. Finally, one or more copies of the resulting set of likely cognates are concatenated with the training set. 3 Experiments We induced translation models using IBM Model 4 (Brown et al., 1990) with the GIZA toolkit (Al-Onaizan et al., 1999). The maximum sentence length in the training data was set at 30 words. The actual translations were produced with a greedy decoder (Germann et al., 2001). For the evaluation of translation quality, we used the BLEU metric (Papineni et al., 2002), which measures the n-gram overlap between the translated output and one or more reference translations. In our experiments, we used only one reference translation. 3.1 Word alignment quality In order to directly measure the influence of the added cognate information on the word alignment quality, we performed a single experiment using a set of 500 manually aligned sentences from Hansards (Och and Ney, 2000). Giza was first trained on 50,000 sentences from Hansards, and then on the same training set augmented with a set of cognates. The set consisted of two copies of a list produced by applyi</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings ofACL-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Simard</author>
<author>G F Foster</author>
<author>P Isabelle</author>
</authors>
<title>Using cognates to align sentences in bilingual corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of TMI-92.</booktitle>
<contexts>
<context position="1575" citStr="Simard et al., 1992" startWordPosition="228" endWordPosition="231">sh night and German nacht) or borrowing from one language to another (e.g. English sprint and Japanese supurinto). In a broad sense, cognates include not only genetically related words and borrowings but also names, numbers, and punctuation. Practically all bitexts (bilingual parallel corpora) contain some kind of cognates. If the languages are represented in different scripts, a phonetic transcription or transliteration of one or both parts of the bitext is a pre-requisite for identifying cognates. Cognates have been employed for a number of bitextrelated tasks, including sentence alignment (Simard et al., 1992), inducing translation lexicons (Mann and Yarowsky, 2001), and improving statistical machine translation models (Al-Onaizan et al., 1999). Cognates are particularly useful when machine-readable bilingual dictionaries are not available. Al-Onaizan et al. (1999) experimented with using bilingual dictionaries and cognates in the training of Czech–English translation models. They found that appending probable cognates to the training bitext significantly lowered the perplexity score on the test bitext (in some cases more than when using a bilingual dictionary), and observed improvement in word ali</context>
<context position="3223" citStr="Simard et al. (1992)" startWordPosition="475" endWordPosition="478"> basis of orthographic similarity, and appended to the corpus itself. The objective is to reinforce the coocurrence count between cognates in addition to already existing co-ocurrences. The results of experiments conducted on a variety of bitexts show that cognate identification can improve word alignments, which leads to better translation models, and, consequently, translations of higher quality. The improvement is achieved without modifying the statistical training algorithm. 2 The method We experimented with three word similarity measures: Simard’s condition, Dice’s coefficient, and LCSR. Simard et al. (1992) proposed a simple condition for detecting probable cognates in French–English bitexts: two words are considered cognates if they are at least four characters long and their first four characters are identical. Dice’s coefficient is defined as the ratio of the number of shared character bigrams to the total number of bigrams in both words. For example, colour and couleur share three bigrams (co, ou, and ur), so their Dice’s coefficient is s - 0.55. The Longest Common Subsequence Ratio (LCSR) of two words is computed by dividing the length of their longest common subsequence by the length of th</context>
</contexts>
<marker>Simard, Foster, Isabelle, 1992</marker>
<rawString>M. Simard, G. F. Foster, and P. Isabelle. 1992. Using cognates to align sentences in bilingual corpora. In Proceedings of TMI-92.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>