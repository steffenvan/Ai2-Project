<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.968488">
A PARSING ALGORITHM FOR UNIFICATION GRAMMAR
</title>
<author confidence="0.936316">
Andrew Haas
</author>
<affiliation confidence="0.977566">
Department of Computer Science
State University of New York at Albany
</affiliation>
<address confidence="0.559328">
Albany, New York 12222
</address>
<bodyText confidence="0.996846714285714">
We describe a table-driven parser for unification grammar that combines bottom-up construction of
phrases with top-down filtering. This algorithm works on a class of grammars called depth-bounded
grammars, and it is guaranteed to halt for any input string. Unlike many unification parsers, our
algorithm works directly on a unification grammar—it does not require that we divide the grammar into
a context-free &amp;quot;backbone&amp;quot; and a set of feature agreement constraints. We give a detailed proof of
correctness. For the case of a pure bottom-up parser, our proof does not rely on the details of unification
—it works for any pattern-matching technique that satisfies certain simple conditions.
</bodyText>
<sectionHeader confidence="0.997742" genericHeader="abstract">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999965923076923">
Unrestricted unification grammars have the formal
power of a Turing machine. Thus there is no algorithm
that finds all parses of a given sentence in any unifica-
tion grammar and always halts. Some unification gram-
mar systems just live with this problem. Any general
parsing method for definite clause grammar will enter an
infinite loop in some cases, and it is the task of the
grammar writer to avoid this. Generalized phrase struc-
ture grammar avoids the problem because it has only
the formal power of context-free grammar (Gazdar et al.
1985), but according to Shieber (1985a) this is not
adequate for describing human language.
Lexical functional grammar employs a better solu-
tion. A lexical functional grammar must include a
finitely ambiguous context-free grammar, which we will
call the context-free backbone (Barton 1987). A parser
for lexical functional grammar first builds the finite set
of context-free parses of the input and then eliminates
those that don&apos;t meet the other requirements of the
grammar. This method guarantees that the parser will halt.
This solution may be adequate for lexical functional
grammars, but for other unification grammars finding a
finitely ambiguous context-free backbone is a problem.
In a definite clause grammar, an obvious way to build a
context-free backbone is to keep only the topmost
function letters in each rule. Thus the rule
</bodyText>
<equation confidence="0.977427">
s —&gt; np(P ,N) vp( P ,IV)
becomes
S —&gt; np vp
</equation>
<bodyText confidence="0.999709933333333">
(In this example we use the notation of Pereira and
Warren 1980, except that we do not put square brackets
around terminals, because this conflicts with standard
notation for context-free grammars.) Suppose we use a
simple X-bar theory. Let major-category (Type, Bar-
level) denote a phrase in a major category. A noun
phrase may consist of a single noun, for instance, John.
This suggests a rule like this:
major-category (n,2) —&gt; major-category (n, 1)
In the context-free backbone this becomes
major-category ---&gt; major-category
so the context-free backbone is infinitely ambiguous.
One could devise more elaborate examples, but this one
suffices to make the point: not every natural unification
grammar has an obvious context-free backbone. There-
fore it is useful to have a parser that does not require us
to find a context-free backbone, but works directly on a
unification grammar (Shieber 1985b).
We propose to guarantee that the parsing problem is
solvable by restricting ourselves to depth-bounded
grammars. A unification grammar is depth-bounded if
for every L &gt; 0 there is a D &gt; 0 such that every parse
tree for a sentential form of L symbols has depth less
than D. In other words, the depth of a tree is bounded
by the length of the string it derives. A context-free
grammar is depth-bounded if and only if every string of
symbols is finitely ambiguous. We will generalize the
notion of finite ambiguity to unification grammars and
show that for unification grammars, depth-boundedness
is a stronger property than finite ambiguity.
</bodyText>
<footnote confidence="0.863497">
Copyright 1989 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided
that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To
copy otherwise, or to republish, requires a fee and/or specific permission.
0362-613X/ 89 /010219-232$03.00
</footnote>
<note confidence="0.5059425">
Computational Linguistics, Volume 15, Number 4, December 1989 219
Andrew Haas A Parsing Algorithm for Unification Grammar
</note>
<bodyText confidence="0.999349875">
Depth-bounded unification grammars have more for-
mal power than context-free grammars. As an example
we give a depth-bounded grammar for the language xx,
which is not context-free. Suppose the terminal symbols
are a through z. We introduce function letters a&apos;
through z&apos; to represent the terminals. The rules of the
grammar are as follows, with e denoting the empty
string.
</bodyText>
<equation confidence="0.995066">
s —&gt; x(L)x(L)
x(cons(A,L)) —&gt; pre-terminal(A) x(L)
x(nil) --&gt; e
pre-terminal(a&apos;) —&gt; a
pre-terminal(z&apos;) z
</equation>
<bodyText confidence="0.999958195121951">
The reasoning behind the grammar should be clear—
x(cons(a&apos; ,cons(13&apos; ,ni1))) derives ab, and the first rule
guarantees that every sentence has the form xx. The
grammar is depth-bounded because the depth of a tree is
a linear function of the length of the string it derives. A
similar grammar can derive the crossed serial dependen-
cies of Swiss German, which according to Shieber
(1985a) no context-free grammar can derive. It is clear
where the extra formal power comes from: a context-
free grammar has a finite set of nonterminals, but a
unification grammar can build arbitrarily large nonter-
minal symbols.
It remains to show that there is a parsing algorithm
for depth-bounded unification grammars. We have de-
veloped such an algorithm, based on the context-free
parser of Graham et al. 1980, which is a table-driven
parser. If we generalize the table-building algorithm to a
unification grammar in an obvious way, we get an
algorithm that is guaranteed to halt for all depth-
bounded grammars (not for all unification grammars).
Given that the tables can be built, it is easy to show that
the parser halts on every input. This is not a special
property of our parser—a straightforward bottom-up
parser will also halt on all depth-bounded grammars,
because it builds partial parse trees in order of their
depth. Our contribution is to show that a simple algo-
rithm will verify depth-boundedness when in fact it
holds. If the grammar is not depth-bounded, the table-
building algorithm will enter an infinite loop, and it is up
to the grammar writer to fix this. In practice we have
not found this troublesome, but it is still an unpleasant
property of our method. Section 7 will describe a
possible solution for this problem.
Sections 2 and 3 of this paper define the basic
concepts of our formalism. Section 4 proves the sound-
ness and completeness of our simplest parser, which
is purely bottom-up and excludes rules with empty
right-hand sides. Section 5 admits rules with empty
right sides, and section 6 adds top-down filtering. Sec-
tion 7 discusses the implementation and possible exten-
sions.
</bodyText>
<sectionHeader confidence="0.979762" genericHeader="keywords">
2 BASIC CONCEPTS
</sectionHeader>
<bodyText confidence="0.999150212121212">
The following definitions are from Gallier 1986. Let S be
a finite, nonempty set of sorts. An S-ranked alphabet is
a pair (I,r) consisting of a set I together with a function
✓ S* x S assigning a rank (u,$) to each symbol f in
I. The string u in S* is the arity off and s is the type of
The S-ranked alphabets used in this paper have the
following property. For every sort s E S there is a
c:ountably infinite set Vs of symbols of sort s called
variables. The rank of each variable in Vs is (e,$), where
e is the empty string. Variables are written as strings
beginning with capitals—for instance X, Y, Z. Symbols
that are not variables are called function letters, and
function letters whose arity is e are called constants.
There can be only a finite number of function letters in
any sort.
The set of terms is defined recursively as follows.
For every symbol f of rank (u, ...un, s) and any terms
ti...tn, with each t, of sort u,, f(t, ,...tn) is a term of sort
s. Since every sort in S includes variables, whose arity
is e, it is clear that there are terms of every sort.
A term is called a ground term if it contains no
variables. We make one further requirement on our
ranked alphabets: that every sort contains a ground
term. This can be guaranteed by just requiring at least
one constant of every sort. It is not clear, however, that
this solution is linguistically acceptable—we do not
wish to include constants without linguistic significance
just to make sure that every sort includes a ground term.
Therefore, we give a simple algorithm for checking that
every sort in S includes a ground term.
Let T, be the set of sorts in S that include a constant.
Let Ti ± I be the union of &apos;I&apos;, and the set of all s in S such
that for some function letter f of sort s, the arity off is
</bodyText>
<listItem confidence="0.934463">
• and the sorts ul,...,un are in Ti. Every sort in T,
</listItem>
<bodyText confidence="0.982746818181818">
includes a ground term, and if every sort in T. includes
a ground term then every sort in T, includes a ground
term. Then for all n, every sort in Tn includes a ground
term. The algorithm will compute Tn for successive
values of n until it finds an N such that TN = TN ± (this
N must exist, because S is finite). If TN = S, then every
sort in S includes a ground term, otherwise not.
As an illustration, let S = {phrase, person, numbed.
Let the function letters of I be {np, vp, s, 1st, 2nd, 3rd,
singular, plural}. Let ranks be assigned to the function
letters as follows, omitting the variables.
</bodyText>
<equation confidence="0.99968475">
r(np) = ([person, number], phrase)
r(vp) = ([person, number], phrase)
r(s) = (e, phrase)
r( 1 st) = (e, number)
r(2nd) = (e, number)
r(3rd) = (e, number)
r [(singular) = (e, person)
r(plural) = (e,person)
</equation>
<bodyText confidence="0.9962795">
We have used the notation [a,b,c] for the string of a, b,
and c. Typical terms of this ranked alphabet are np(lst,
</bodyText>
<page confidence="0.910178">
220 Computational Linguistics, Volume 15, Number 4, December 1989
</page>
<note confidence="0.485835">
Andrew Haas A Parsing Algorithm for Unification Grammar
</note>
<bodyText confidence="0.999853018867925">
singular) and vp(2nd, plural). The reader can verify,
using the above algorithm, that every sort includes a
ground term. In this case, Ti = {person, number}, T2 =
{person, number, phrase}, and T3 = T2.
To summarize: we define ranked alphabets in a
standard way, adding the requirements that every sort
includes a countable infinity of variables, a finite num-
ber of function letters, and at least one ground term. We
then define the set of terms in a standard way. All
unification in this paper is unification of terms, as in
Robinson 1965—not graphs or other structures, as in
much recent work (Shieber 1985b).
A unification grammar is a five-tuple G = (S, (/,r) T,
P, Z) where S is a set of sorts, (I,r) an S-ranked
alphabet, T a finite set of terminal symbols, and Z a
function letter of arity e in (2„r). Z is called the start
symbol of the grammar (the standard notation is S not
Z, but by bad luck that conflicts with standard notation
for the set of sorts). P is a finite set of rules; each rule
has the form (A —&gt; a), where A is a term of the ranked
alphabet and a is a sequence of terms of the ranked
alphabet and symbols from T.
We define substitution and substitution instances of
terms in the standard way (Gallier 1986). We also define
instances of rules: if s is a substitution and (A —&gt; B1
...Bn) is a rule, then (s(A)--&gt; s(B1)...s(Bn)) is an instance
of the rule (A Bi...Bn). A ground instance of a term or
rule is an instance that contains no variables.
Here is an example, using the set of sorts S from the
previous example. Let the variables of sort person be P1 ,
P2,... and the variables of sort number be NI ,N2„. etc.
Then the rule (start —&gt; np(Pi , vp(PI , NI) has six
ground instances, since there are three possible substi-
tutions for the variable PI and two possible substitu-
tions for NI .
We come now to the key definition of this paper. Let
G = (S, (14 T, P. Z) be a unification grammar. The
ground grammar for G is the four-tuple (N, T, P&apos;, Z),
where N is the set of all ground terms of (2,4 , T is the
set of terminals of G, P&apos; is the set of all ground instances
of rules in P, and Z is the start symbol of G. If N and P&apos;
are finite, the ground grammar is a context-free gram-
mar. If N or P&apos; is infinite, the ground grammar is not a
context-free grammar, and it may generate a language
that is not context-free. Nonetheless we can define
derivation trees just as in a cfg. Following Hoperoft and
Ullman (1969), we allow derivation trees with nonter-
minals at their leaves. Thus a derivation tree may
represent a partial derivation. We differ from Hoperoft
and Ullman by allowing nonterminals other than the
start symbol to label the root of the tree. A derivation
tree is an A-tree if the non-terminal A labels its root.
The yield of a derivation tree is the string formed by
reading the symbols at its leaves from left to right. As in
a cfg, A a iff there is an A-tree with yield a. The
language generated by a ground grammar is the set of
terminal strings derived from the start symbol. The
language generated by a unification grammar is the
language generated by its ground grammar.
The central idea of this approach is to regard a
unification grammar as an abbreviation for its ground
grammar. Ground grammars are not always cfgs, but
they share many properties of cfgs. Therefore if we
regard unification grammars as abbreviations for ground
grammars, our understanding of cfgs will help us to
understand unification grammars. This is of course
inspired by Robinson&apos;s work on resolution, in which he
showed how to &amp;quot;lift&amp;quot; a proof procedure for proposi-
tional logic up to a proof procedure for general first-
order logic (Robinson 1965).
The case of a finite ground grammar is important,
since it is adequate for describing many syntactic phe-
nomena. A simple condition will guarantee that the
ground grammar is finite. Suppose s1 and s2 are sorts,
and there is a function letter of sort si that has an
argument of sort s2. Then we say that s &gt; s2. Let &gt;* be
the transitive closure of this relation. If &gt;* is irreflex-
ive, and D is the number of sorts, every term of the
ground grammar has depth D. To see this, think of a
ground term as a labeled tree. A path from the root to a
leaf generates a sequence of sorts: the sorts of the
variables and functions letters encountered on that
path. It is a strictly decreasing sequence according to
&gt;*. Therefore, no sort occurs twice; therefore, the
length of the sequence is at most D. Since there are only
a finite number of function letters in the ranked alpha-
bet, each taking a fixed number of arguments, the
number of possible ground terms of depth D is finite.
Then the ground grammar is finite.
A ground grammar G&apos; is depth-bounded if for every
integer n there exists an integer d such that every
derivation tree in G&apos; with a yield of length n has a depth
less than d. In other words, a depth-bounded grammar
cannot build an unbounded amount of tree structure
from a bounded number of symbols. Remember that
these symbols may be either terminals or nonterminals,
because we allow nonterminals at the leaves of a
derivation tree. A unification grammar G is depth-
bounded if its ground grammar is depth-bounded.
We say that a unification grammar is finitely ambig-
uous if its ground grammar is finitely ambiguous. We
can now prove the result claimed above: that a unifica-
tion grammar can be finitely ambiguous but not depth-
bounded. In fact, the following grammar is completely
unambiguous but still not depth-bounded. It has just one
terminal symbol, b, and its start symbol is start.
</bodyText>
<equation confidence="0.9972774">
start —&gt; p(0)
p(IV)—&gt; p(succ(/V))
p(N)—&gt; q(N)
q (succ(N))—&gt; b q(N)
q(0) —&gt; e
</equation>
<bodyText confidence="0.955009866666667">
The function letter &amp;quot;succ&amp;quot; represents the successor
function on the integers, and the terms 0, succ(0),
succ(succ(0))... represent the integers 0, 1, 2... etc. For
convenience, we identify these terms with the integers
Computational Linguistics, Volume 15, Number 4, December 1989 221
Andrew Haas A Parsing Algorithm for Unification Grammar
they represent. A string of N occurrences of b has just
one parse tree. In this tree the start symbol derives p(0),
which derives p(IV) by N applications of the second
rule. p(N) derives q(IV), which derives N occurrences of
b by N applications of the fourth rule and one applica-
tion of the last rule. The reader can verify that this
derivation is the only possible one, so the grammar is
unambiguous. Yet the start symbol derives p(N) by a
tree of depth N, for every N. Thus trees whose frontier
has only one symbol can still be arbitrarily deep. Then
the grammar cannot be depth-bounded.
We have defined the semantics of our grammar
formalism without mentioning unification. This is delib-
erate; for us unification is a computational tool, not a
part of the formalism. It might be better to call the
formalism &amp;quot;substitution grammar,&amp;quot; but the other name
is already established.
Notation: The letters A, B, and C denote symbols of
a ground grammar, including terminals and nontermi-
nals. Lowercase Greek letters denote strings of sym-
bols. a [i k] is the substring of a from space i to space k,
where the space before the first symbol is space zero. e
is always the empty string. We write x U y or U(x,y) for
the union of sets x and y, and also (U i&lt;j&lt;k j(j)) for the
union of the sets f(j) for all j such that i &lt;j &lt; k.
If a is the yield of a tree t, then to every occurrence
of a symbol A in a there corresponds a leaf of t labeled
with A. To every node in t there corresponds an
occurrence of a substring in a—the substring dominated
by that node. Here is a lemma about trees and their
yields that will be useful when we consider top-down
filtering.
Lemma 2.1. Suppose t is a tree with yield apa&apos; and n
is the node of t corresponding to the occurrence of
after a in apa&apos;. Let A be the label of n. If t&apos; is the tree
formed by deleting all descendants of n from t, the yield
of t&apos; is aAa&apos;.
Proof: This is intuitively clear, but the careful reader
may prove it by induction on the depth of t.
</bodyText>
<sectionHeader confidence="0.998851" genericHeader="introduction">
3 OPERATIONS ON SETS OF RULES AND TERMS
</sectionHeader>
<bodyText confidence="0.999333647058824">
The parser must find the set of ground terms that derive
the input string and check whether the start symbol is
one of them. We have taken the rules of a unification
grammar as an abbreviation for the set of all their
ground instances. In the same way, the parser will use
sets of terms and rules containing variables as a repre-
sentation for sets of ground terms and ground rules. In
this section we show how various functions needed for
parsing can be computed using this representation.
A grammatical expression, or g-expression, is either
a term of L, the special symbol nil, or a pair of
g-expressions. The letters u, v, w, x, y, and z denote
g-expressions, and X, Y, and Z denote sets of g-
expressions. We use the usual LISP functions and
predicates to describe g-expressions. [x y] is another
notation for cons (x,y). For any substitution s, s (cons
(x,y)) = cons (s(x),s(y)) and s(Nil) = Nil. A selector is a
function from g-expressions to g-expressions formed by
composition from the functions car, cdr, and identity.
Thus a selector picks out a subexpression from a
g-expression. A constructor is a function that maps two
g-expressions to a g-expression, formed by composition
from the functions cons, car, cdr, nil, (A x y. x), and (A
x y. y). A constructor builds a new g-expression from
parts of two given g-expressions. A g-predicate is a
function from g-expressions to Booleans formed by
composition from the basic functions car, cdr, (A x. x),
consP, and null.
Let ground(X) be the set of ground instances of g-
expressions in X. If f is a selector function, let f(X) be
the set of all fix) such that x E X. If p is a g-predicate,
let separate (p,X) be the set of all x E X such that p(x).
The following lemmas are easily established from the
definition of s(x) for a g-expression x.
</bodyText>
<construct confidence="0.583195111111111">
Lemma 2.2. Iff is a selector function, f(ground(X)) =
ground (NC)).
Lemma 2.3. If p is a g-predicate, separate (p,ground
(X)) = ground(separate (p,X)).
Lemma 2.4. Ground (X U Y) = ground (X) U ground
( Y).
Lemma 2.5. If x is a ground term, x E ground(X) iff x
is an instance of some y E X.
Lemma 2.6. Ground (X) is empty iff X is empty.
</construct>
<bodyText confidence="0.986939451612903">
Proof. A nonempty set of terms must have a non-
empty set of ground instances, because every variable
belongs to a sort and every sort includes at least one
ground term.
These lemmas tell us that if we use sets X and Y of
terms to represent the sets ground(X) and ground( Y) of
ground terms, we can easily construct representations
for .figround(X)), separate(p,ground (X)), and ground
(X) U ground ( Y). Also we can decide whether a given
ground term is contained in ground(X) and whether
ground(X) is empty. All these operations will be needed
in the parser.
The parser requires one more type of operation,
defined as follows.
Definition. Let J., andf2 be selectors and g a construc-
tor, and suppose g(x,y) is well defined whenever f1(x)
and f2(y) are well defined. The symbolic product defined
by J.; , f2, and g is the function
(A X Y { g(x,y) I xEXAyE YAf1(x)=f2(y)
where X and Y range over sets of ground g-expressions.
Note that fl(x) = f2(y) is considered false if either side of
the equation is undefined.
The symbolic product matches every x in X against
every y in Y. If f1(x) equals f2(y), it builds a new
structure from x and y using the function g. As an
example, suppose X and Y are sets of pairs of ground
terms, and we need to find all pairs [A C] such that for
some B, [A B] is in X and [B C] is in Y. We can do this
by finding the symbolic product with f1 = cdr, f2 = car,
and g = (A x y. cons(car(x), cdr(y))). To see that this is
correct, notice that if [A B] is in X and [B C] is in Y, then
</bodyText>
<page confidence="0.925554">
222 Computational Linguistics, Volume 15, Number 4, December 1989
</page>
<subsectionHeader confidence="0.492754">
Andrew Haas A Parsing Algorithm for Unification Grammar
</subsectionHeader>
<bodyText confidence="0.6628976">
fi([A B]) =f2 ([BC]), so the pair g B],[B C]) = [A C]
must be in the answer set.
A second example: we can find the intersection of
two sets of terms by using a symbolic product with fl =
(A x . x), f2 = (A x . x), and g = (A x y. x).
</bodyText>
<equation confidence="0.6831485">
If X is a set of g-expressions and n an integer,
rename(X,n) is an alphabetic variant of X. For all X, Y,
</equation>
<bodyText confidence="0.535873428571429">
m, and n, if m 0 n then rename(X,n) and rename( Y,m)
have no variables in common. The following theorem
tells us that if we use sets of terms X and Y to represent
the sets ground(X) and ground( Y) of ground terms, we
can use unification to compute any symbolic product of
ground(X) and ground( Y). We assume the basic facts
about unification as in Robinson (1965).
</bodyText>
<construct confidence="0.705036">
Theorem 2.1. If h is the symbolic product defined by
f1, f2 and g, and X and Y are sets of g-expressions, then
</construct>
<equation confidence="0.84178375">
h (ground(X),ground(Y)) =
ground({s(g(u,v)) u E rename(X,1) A v E
rename(Y,2)
A s is the m.g.u. of f1(u) and f2(v)}.)
</equation>
<bodyText confidence="0.940539821428571">
Proof. The first step is to show that if Z and W share no
variables
(1) {g(z,w) z E ground(Z) A w E ground(W) A (z)
= f2 (w)} = ground({s(g(u,v)) I uEZAvEWA s is
the m.g.u. of f, (u) and f2(v)
Consider any element of the right side of equation (1). It
must be a ground instance of s(g(u,v)), where u E Z, v
E W, and s is the m.g.u. of fl(u) and f2(v). Any ground
instance of s(g(u,v)) can be written as s&apos;(s(g(u,v))),
where s&apos; is chosen so that s&apos;(s(u)) and s&apos;(s(v)) are ground
terms. Then s&apos;(s(g(u,v))) -=- g(s&apos;(s(u)),s&apos;(s(v))) and
fi(s&apos;(s(u))) = s&apos;(s(fi(u))) = s&apos;(s(f2 (v))) = f2(s&apos;(s(v))).
Therefore s&apos;(s(g(u,v))) belongs to the set on the left side
of equation (1).
Next consider any element of the left side of (1). It
must have the form g(z,w), where z E ground(Z), w E
ground(W), and fl (z) = f2 (w). Then for some u E Z and
v E W, z is a ground instance of u and w is a ground
instance of v. Since u and v share no variables, there is
a substitution s&apos; such that s&apos;(u) = z and s&apos;(v) = w. Then
s&apos;(fi (u)) = A (s&apos;(u)) = f2 (si(v)) = sV2 (0), so there
exists a most general unifier s for J., (u) and f2 (v), and s&apos;
is the composition of s and some substitution s&amp;quot;. Then
g(z,w) = g(s(s(u)) s(s(v))) = s(s(g(u,v))). g(z,w) is a
ground term because z and w are ground terms, so
g(z,w) is a ground instance of s(g(u,v)) and therefore
belongs to the set on the right side of equation (1).
We have proved that if Z and W share no variables,
</bodyText>
<listItem confidence="0.513587">
(2) h(ground(Z),ground(W)) = ground({s( g(u,v)) I u
</listItem>
<bodyText confidence="0.98372378125">
EZAvEWAs is the m.g.u. of fl(u) and f2(v)})
For any X and Y, rename(X,1) and rename( Y,2) share no
variables. Then we can let Z = rename(X,1) and W =
rename( Y,2) in formula (2). Since h(ground(X),
ground(}&apos;)) = h(ground(rename(X,1)), ground(rename
(Y,2))), the theorem follows by transitivity of equality.
This completes the proof.0
As an example, suppose X = {[a(F) b(F)]} and Y =
{[b(G) c(G)]}. Suppose the variables F and G belong to
a sort s that includes just two ground terms, m and n.
We wish to compute the symbolic product of ground(X)
and ground( Y), using fl = cdr, f2 = car, and g = (A x y.
cons(car(x), cdr(y))) (as in our previous example).
ground(X) equals {[a(m) b(m)],[a(n) b(n)ll and
ground( Y) equals {[b(m) c(m)],[b(n) c(n)ll, so the sym-
bolic product is {[a(m) c(m)],[a(n) c(n)}}. We will verify
that the unification method gets the same result. Since X
and Y share no variables, we can skip the renaming
step. Let x = [a(F) b(F)] and y = [b(G) c(G)]. Then J., (x)
= b(F), f2 (y) = b(G), and the most general unifier is the
substitution s that replaces F with G. Then g(x,y) =
[a(F) c(G)] and s(g(x,y)) = [A(G) C(G)]. The set of
ground instances of this g-expression is {[A(m) C(m)],
[A(n)C(n)ll, as desired.
Definition. Let f be a function from sets of g-
expressions to sets of g-expressions, and suppose that
when X C X&apos; and Y C Y&apos;, flX, Y) C f(X&apos;, Y&apos;). Then f is
monotonic.
All symbolic products are monotonic functions, as
the reader can easily show from the definition of sym-
bolic products. Indeed, every function in the parser that
returns a set of g-expressions is monotonic.
</bodyText>
<sectionHeader confidence="0.990097" genericHeader="method">
4 THE PARSER WITHOUT EMPTY SYMBOLS
</sectionHeader>
<bodyText confidence="0.975952578947369">
Our first parser does not allow rules with empty right
sides, since these create complications that obscure the
main ideas. Therefore, throughout this section let G be
a ground grammar in which no rule has an empty side.
When we say that a derives we mean that a derives [3
in G. Thus a e iff a = e.
A dotted rule in G is a rule of G with the right side
divided into two parts by a dot. The symbols to the left
of the dot are said to be before the dot, those to the right
are after the dot. DR is the set of all dotted rules in G.
A dotted rule (A —&gt; a./3) derives a string if a derives that
string. To compute symbolic products on sets of rules or
dotted rules, we must represent them as g-expressions.
We represent the rule (A —&gt; B C) as the list (A B C), and
the dotted rule (A —&gt; B.C) as the pair [(A B C) (C)].
We write A B if A derives B by a tree with more
than one node. The parser relies on a chain table—a
table of all pairs [A B] such that A B. Let Cd be the
set of all [A B] such that A B by a derivation tree of
depth d. Clearly CI is the set of all [A B] such that (A
B) is a rule of G. If SI and S2 are sets of pairs of terms,
define
link(S1,S2) = {[A C] I (3 B. [A B] E Si A [B C] E S2)}
The function link is equal to the symbolic product
defined by f = cdr, f2 = car, and g = (A x y .
cons(car(x), cdr(y))). Therefore we can compute link
(SI, S2) by applying Theorem 2.1. Clearly Cd =
link(Cd,C1). Since the grammar is depth-bounded, there
exists a number D such that every derivation tree whose
yield contains exactly one symbol has depth less than
D. Then CD is empty. The algorithm for building the
chain table is this: compute Cr, for increasing values of
Computational Linguistics, Volume 15, Number 4, December 1989 223
Andrew Haas A Parsing Algorithm for Unification Grammar
n until C„ is empty. Then the union of all the C„&apos;s is the
chain table.
We give an example from a finite ground grammar.
Suppose the rules are
</bodyText>
<equation confidence="0.9574508">
(a —&gt; b)
(b —&gt; c)
(c—&gt; d)
(d—&gt; k f )
(k —&gt; g)
(f --&gt; h)
The terminal symbols are g and h . Then CI = f[a b],
[b c], [c d]}, C2= {[a c],[b d]}, and C3 = {fa C4 is
empty.
Definitions. ChainTable is the set of all [A B] such
</equation>
<bodyText confidence="0.982334363636364">
that A B. If S is a set of dotted pairs of symbols and
S&apos; a set of symbols, ChainUp(S,S&apos;) is the set of symbols
A such that [A B] E S for some B E S&apos;. &amp;quot;ChainUp&amp;quot; is
clearly a symbolic product. If S is a set of symbols,
close(S) is the union of S and ChainUp(ChainTable,S).
By the definition of ChainTable, close(S) is the set of
symbols that derive a symbol of S.
In the example grammar, ChainTable is the union of
C1, C2, and C3—that is, the set {[ a b],[b c],[c d],[a c],
[b d],[a d]}. ChainUp({ a}) = {}, but ChainUp({ d}) =
fa,b,cl. close({ a}) = al, while close({ d}) = a,b,c,d}.
Let a be an input string of length L &gt; 0. For each a[i
k] the parser will construct the set of dotted rules that
derive a[i k]. The start symbol appears on the left side
of one of these rules iff a[i k] is a sentence of G. By
lemma 2.5 this can be tested, so we have a recognizer
for the language generated by G. With a small modifi-
cation the algorithm can find the set of derivation trees
of a. We omit details and speak of the algorithm as a
parser when strictly speaking it is a recognizer only.
The dotted rules that derive a[i k] can be partitioned
into two sets: rules with many symbols before the dot
and rules with exactly one. For each a[i k], the algo-
rithm will carry out three steps. First it collects all
dotted rules that derive a[i k] and have many symbols
before the dot. From this set it constructs the set of all
symbols that derive a[i k], and from these symbols it
constructs the set of all dotted rules that derive a[i k]
with one symbol before the dot. The union of the two
sets of dotted rules is the set of all dotted rules that
derive a[i k]. Note that a dotted rule derives a[i k] with
more than one symbol before the dot iff it can be written
in the form (A--&gt; 013.13&apos;) where 0 a[i j], B a[j k], and
i &lt;j &lt; k (this follows because a nonempty string 0 can
never derive the empty string in G).
If (A B. C) derives a[i j] and B derives aU kl, then
(A --&gt; B C .) derives a[i k]. This observation motivates
the following.
Definition. If S is a set of dotted rules and S&apos; a set of
symbols, AdvanceDot(S,S&apos;) is the set of rules (A —&gt;
aB.f3) such that (A —0 a.Bp) E S and B E S&apos;. Clearly
AdvanceDot is a symbolic product.
For example, AdvanceDot({( d—&gt; k .f)},{af}) equals
{( d —&gt; k f .)}.
Suppose that for each proper substring of a[i k] we
have already found the dotted rules and symbols that
derive that substring. The following lemma tells us that
we can then find the set of dotted rules that derive a[i k]
with many symbols before the dot.
Lemma 3.1. For i &lt;j &lt; k, let S(i,j) be the set of
dotted rules that derive a[i j], and S&apos; (j,k) the set of
symbols that derive aU k]. The set of dotted rules that
derive a[i k] with many symbols before the dot is
i&lt;j&lt; k AdvanceDot(S(i,j),S &apos;(j ,k))
Proof. We have
</bodyText>
<equation confidence="0.814307">
i &lt; j &lt;k AdvanceDot({(B ---&gt; 0.00 E DR I f3 &gt; a[i j]l,
{A I A celi
U
i&lt; j&lt;k {(B ---&gt; 0A-02) E DR I A A aLl kil
</equation>
<bodyText confidence="0.946194842105263">
by definition of AdvanceDot
{(B —&gt; pA./32)ED121(3j.i&lt;j&lt;kAp a[i A
A au j kplE1
As noted above, this is the set of dotted rules that derive
a[i k] with more than one symbol before the dot.
Definition. If S is a set of dotted rules, finished( S) =
{A IA— 0.) E 1.
When the dot reaches the end of the right side of a
rule, the parser has finished building the symbol on the
left side—hence the name finished. For example,
finished({( d—&apos; k f .),(a —&gt; . b)l) is the set { d}.
The next lemma tells us that if we have the set of
dotted rules that derive a[i k] with many symbols before
the clot, we can construct the set of symbols that derive
a[i k].
Lemma 3.2. Suppose length(a) &gt; 1 and S is the set of
dotted rules that derive a with more than one symbol
before the dot. The set of symbols that derive a is
close(finished(S)).
Proof. Suppose first that A E close(finished(S)).
Then for some B, A B, (B—&apos; P.) is a dotted rule, and
a. Then A a. Suppose next that A derives a. We
shovv by induction that if t is a derivation tree in G and
A a by t, then A E close(finished (S)). t contains more
than one node because length(a) &gt; 1, so there is a rule
(A —0 A1 ... An) that admits the root of t. If n &gt; 1, (A —&gt;
Ai...A..) E S and A is in close(finished(S)). If n = 1 then
A1 a and by induction hypothesis AI E close(fin-
ished(S)). Since A AI, A E close(finished(S)).
In our example grammar, the set of dotted rules
deriving a[0 2] = gh with more than one symbol before
the dot is {(d ---&gt; k f.)} finished({( d —&gt; k f.)} ) is { d}, and
close({ d}) = a,b,c,d1. It is easy to check that these are
all the symbols that derive gh.1=1
Definitions. RuleTable is the set of dotted rules (A —&gt;
.a) such that (A —&gt; a) is a rule of G. If S is a set of
symbols, NewRules(S) is AdvanceDot(RuleTable, S).
In our example grammar, NewRules (fkl) = f( d —0 k
</bodyText>
<listItem confidence="0.985908">
• Pl.
</listItem>
<note confidence="0.550497">
Lemma 3.3. If S is the set of symbols that derive a,
</note>
<page confidence="0.905892">
224 Computational Linguistics, Volume 15, Number 4, December 1989
</page>
<note confidence="0.218235">
Andrew Haas A Parsing Algorithm for Unification Grammar
</note>
<bodyText confidence="0.7880331">
the set of dotted rules that derive a with one symbol
before the dot is NewRules(S).
Proof. Expanding the definitions gives Advance
Dot({( A —&gt; .01(A ---&gt; 13)EP}, C I C al) = {(A —&gt;
c.0 (A —&gt; CB&apos;) EPAC al. This is the set of dotted
rules that derive a with one symbol before the dot.
Let terminals(i,k) be the set of terminals that derive
a[i k]; that is, if i + 1 = k then terminals(i,k) = {a[i
and otherwise terminals(i,k) = 0. Let a be a string of
length L &gt; 0. For 0 i &lt; k L, define
</bodyText>
<equation confidence="0.989696375">
dr(i,k) =
if i + 1 = k
then NewRules(close({ a[i i + 1]}))
else (let rules, = &lt; &lt; k AdvanceDot(dr(i,j),
[finished(dr(j,k)) U terminals
,k)])
(let rules2 = NewRules(close(finished(rules I)))
rules, U rules2))
</equation>
<bodyText confidence="0.970251464285714">
Theorem 3.1. For 0 i &lt; k L, dr(i,k) is the set of
dotted rules that derive a[i k].
Proof. By induction on the length of a[i k]. If the
length is 1, then i + 1 = k. The algorithm returns
NewRules(close({a[i i + 1]})). close({ a[i i + l]}) is the
set of symbols that derive a[ ii + 1] (by the definition of
ChainTable), and NewRules(close({a[i i + H})) is the set
of dotted rules that derive a[i i + 1] with one symbol
before the dot (by lemma 3.3). No rule can derive
a[i i + 1] with many symbols before the dot, because
a[i i + 1] has only one symbol. Then NewRules(close
({a[i k]})) is the set of all dotted rules that derive a[i k].
Suppose a[i k] has a length greater than I. If i &lt;j&lt;k,
dr(i,j) contains the dotted rules that derive a[i j] and
dr(j,k) contains the dotted rules that derive a[j k], by
induction hypothesis. Then finished(dr(j,k)) is the set of
nonterminals that derive a[j k], and terminals(j,k) is the
set of terminals that derive a[j k], so the union of these
two sets is the set of all symbols that derive aff k]. By
lemma 3.1, rules, is the set of dotted rules that derive
a[i k] with many symbols before the dot. By lemma 3.2,
close(finished(rules,)) is the set of symbols that derive
a[i k], so by lemma 3.3 rules2 is the set of dotted rules
that derive a[i k] with one symbol before the dot. The
union of rules, and rules2 is the set of dotted rules that
derive a[i k], and this completes the proof.0
Suppose we are parsing the string gh with our exam-
ple grammar. We have
</bodyText>
<equation confidence="0.999008">
dr(0,1) = {(k g .),(d--&gt; k .f)}
dr(1,2) = {(f—&gt; h .)}
dr(0,2) = {(d—. kf .),(c —&gt; d. ),(b—* c . ),(a—. b . )1
</equation>
<sectionHeader confidence="0.986543" genericHeader="method">
5 THE PARSER WITH EMPTY SYMBOLS
</sectionHeader>
<bodyText confidence="0.980499666666667">
Throughout this section, G is an arbitrary depth-
bounded unification grammar, which may contain rules
whose right side is empty. If there are empty rules in the
grammar, the parser will require a table of symbols that
derive the empty string, which we also call the table of
empty symbols. Let Ed be the set of symbols that derive
the empty string by a derivation of depth d, and let E&apos;d
be the set of symbols that derive the empty string by a
derivation of depth d or less. Since the grammar is
depth-bounded, it suffices to construct Ed for succes-
sive values of d until a D &gt; 0 is found such that ED is the
empty set.
E, is the set of symbols that immediately derive the
empty string; that is, the set of all A such that (A ---&gt; e)
is a rule. A E Ed ± iff there is a rule (A —&gt; Bi...Bn) such
that for each i, B. e at depth do and d is the maximum
of the di&apos;s. In other words: A E Ed ± iff there is a rule
(A ---0 aBf3) such that B E Ed and every symbol of a and
</bodyText>
<equation confidence="0.97792">
0 is in E&apos;d.
Let DR be a set of dotted rules and S a set of
symbols. Define
AdvanceDot*(DR,S) =
if DR = 0 then 0
else (DR U AdvanceDot*(AdvanceDot(DR,S),S))
</equation>
<bodyText confidence="0.8903906">
If DR is the set of ground instances of a finite set of rules
with variables, there is a finite bound on the length of
the right sides of rules in DR (because the right side of
a ground instance of a rule r has the same length as the
right side of r). If L is &apos;the length of the right side of the
longest rule in DR, then AdvanceDot*(DR,S) is well
defined because the recursion stops at depth L or
before. Clearly AdvanceDot*(DR,S) is the set of rules
(A —&gt; af3.y) such that (A —&gt; a. py) E DR and every
symbol of p is in S.
</bodyText>
<equation confidence="0.9113418">
Let
SI = AdvanceDot*(RuleTable, E&apos;d)
S2 = AdvanceDot(S1, E&apos;d)
S3 = AdvanceDot*(S2, E&apos;d)
S4 = finished(S3)
</equation>
<bodyText confidence="0.999402333333333">
S, is the set of dotted rules (A ---&gt; a.po) such that every
symbol of a is in E&apos;d. S2 is then the set of dotted rules (A
—&gt; aB.01) such that B E Ed and every symbol of a is in
E&apos;d. Therefore S3 is the set of dotted rules (A —&gt; aBp.p2)
such that B E Ed and every symbol of a and f3 is in E&apos;d.
Finally S4 is the set of symbols A such that for some
rule (A —&gt; «B/3), B E Ed and every symbol of a and 0 is
in E&apos;d. Then S4 is Ed + 1. In this way we can construct Ed
for increasing values of d until the table of empty
symbols is complete.
Here is an example grammar with symbols that
derive the empty string:
</bodyText>
<equation confidence="0.997150833333333">
(a --&gt; e)
(b —&gt; e)
(c --&gt; ab)
(k —&gt; cfcgc)
(f --&gt; r)
(g --&gt; s)
</equation>
<bodyText confidence="0.928234177419355">
The terminal symbols are r and s. In this grammar, E, =
fa,b1, E2 = {c}, and E3 = 0.
Definitions Let EmptyTable be the set of symbols
that derive the empty string. If S is a set of dotted rules,
let SkipEmpty(S) be AdvanceDot*(S, EmptyTable).
Computational Linguistics, Volume 15, Number 4, December 1989 225
Andrew Haas A Parsing Algorithm for Unification Grammar
Note that SkipEmpty(S) is the set of dotted rules (A —&gt;
a/31.132) such that (A —&gt; (LOA) E S and pl e.
SkipEmpty(S) contains every dotted rule that can be
formed from a rule in S by moving the dot past zero or
more symbols that derive the empty string. In the
example grammar EmptyTable = {a, b,c}, so
SkipEmpty({( k —&gt; . cfcgc)}) = {( k —&gt; . cfcgc), (k —&gt; c.
fcgc)}. If the dotted rules in S all derive a, then the
dotted rules in SkipEmpty(S) also derive a.
Let Cd be the set of pairs [A B] such that A B by a
derivation tree in which the unique leaf labelled B is at
depth d (note: this does not imply that the tree is of
depth d). C1 is the set of pairs [A B] such that (A —&gt; aBp)
is a rule and every symbol of a and p derives the empty
string. C1 is easily computed using SkipEmpty. Also
Cd + 1 = link(Cd,Ci), so we can construct the chain table
as before.
In the example grammar there are no A and B such
that A B, but if we added the rule (k—&gt; cfc), we would
have k f. Note that k derives fby a tree of depth 3, but
the path from the root of this tree to the leaf labeled f is
of length one. Therefore the pair [k f] is in C.
The parser of Section 4 relied on the distinction
between dotted rules with one and many symbols before
the dot. If empty symbols are present, we need a
slightly more complex distinction. We say that the
string a derives p using one symbol if there is a
derivation of p from a in which exactly one symbol of a
derives a non-empty string. We say that a derives 0
using many symbols if there is a derivation of 0 from a
in which more than one symbol of a derives a nonempty
string. If a string a derives a string p, then a derives p
using one symbol, or a derives p using many symbols,
or both. In the example grammar, cfc derives r using
one symbol, and cfcg derives rs using many symbols.
We say that a dotted rule derives p using one (or
many) symbols if the string before the dot derives 13
using one (or many) symbols. Note that a dotted rule
derives a[i k] using many symbols iff it can be written as
(A —&gt; PB13&apos; . pi) where pa[ij], B alj k , e, and
I &lt;j &lt; k. This is true because whenever a dotted rule
derives a string using many symbols, there must be a
last symbol before the dot that derives a nonempty
string. Let B be that symbol; it is followed by a p&apos; that
derives the empty string, and preceded by a 0 that must
contain at least one more symbol deriving a non-empty
string.
We prove lemmas analogous to 3.1, 3.2, and 3.3.
Lemma 4.1. For i &lt; j &lt; k let S(i,j) be the set of dotted
rules that derive au j] and Si(j,k) the set of symbols that
derive aU k]. The set of dotted rules that derive a[i k]
using many symbols is
SkipEmpty(i‹.1-/&lt; k AdvanceDot(S(i,j),SV,k)))
Proof. Expanding definitions and using the argument
of lemma 3.3 we have
</bodyText>
<equation confidence="0.856975875">
SkipEmpty(i &lt;Uj &lt; k AdvanceDot({(B —&gt; 0.131) E
DR I 13 a[i {A I A
a[j k]})) =
SkipEmpty (1(B—&gt; AAA) E DR I (3 j. i&lt;j &lt;k AO
a[i j] A A a[j k])})
This in turn is equal to
{(B --&gt; PAO&apos; .133)EDR I (3j.i&lt;j&lt;kApa[ij] A
A a[j k]) A 13&apos; el
</equation>
<bodyText confidence="0.969602826086956">
This is the set of rules that derive a[i k] using many
symbols, as noted above.
If we have a = rs, then the set of dotted rules that
derive a[0 1] is
r .),(k—&gt; cf. . cgc),(k —&gt; cfc . gc)}
The set of symbols that derive a[1 2] is {g,s}. The set of
dotted rules that derive a[0 2] using many symbols is
{(k—&apos; cfcg . c),(k —&gt; cfcgc .)}
Lemma 4.1 tells us that to compute this set we must
apply SkipEmpty to the output of AdvanceDot. If we
failed to apply SkipEmpty we would omit the dotted
rule (k —&gt; cfcgc .) from our answer.
Lemma 4.2. Suppose length(a) &gt; 1 and S is the set of
dotted rules that derive a using many symbols. The set
of symbols that derive a is close(finished(S)).
Proof. By induction as in Lemma 3.2.
Definitions. Let RuleTable&apos; be SkipEmpty({( A —&gt; .a)
(A —&gt; a) E Pl) = {( A —&gt; a.a&apos;) E DR I a e}. If S is a
set of symbols let NewRules&apos; (S) be SkipEmpty(Advance
Dot(RuleTable&apos;,S)).
RuleTable&apos; is like the RuleTable defined in Section 4,
except that we apply SkipEmpty. In the example gram-
mar, this means adding the following dotted rules:
</bodyText>
<equation confidence="0.993456">
(c —&gt; a . b)
( c —&gt; ab .)
(k —&gt; c . fcgc)
NewRules&apos;({ f }) is equal to 1( k —&gt; cf. . cgc),(k —&gt; cfc .
gc)}.
</equation>
<bodyText confidence="0.99958675">
The following lemma tells us that NewRules&apos; will
perform the task that NewRules performed in Section 4.
Lemma 4.3. If S is the set of symbols that derive a,
the set of dotted rules that derive a using one symbol is
</bodyText>
<equation confidence="0.965735333333333">
NewRules&apos;( S).
Proof. Expanding definitions gives
SkipEmpty(AdvanceDotg(A —&gt; 13.01) E DR I el,
IC I C al))
SkipEmptY({(A ---&gt; PC.P2) E DR I /3 &gt; e A C
{(A --&gt; 0c0&apos;.0 3) E DR I peACaAp&apos;
</equation>
<bodyText confidence="0.89643375">
This is the set of dotted rules that derive a using one
symbol, by definition.
Let a be a string of length L. For 0 i &lt; k L,
define
dr(i,k)
if i + 1 = k
then NewRules&apos;(close({ a[i k]}))
else (let rules&apos; =
</bodyText>
<page confidence="0.974277">
226 Computational Linguistics, Volume 15, Number 4, December 1989
</page>
<note confidence="0.149642">
Andrew Haas A Parsing Algorithm for Unification Grammar
</note>
<equation confidence="0.543142571428571">
SkipEmpty(i &lt;Y&lt; k AdvanceDot(dr(i,j),
[finished(dr(j,k)) U terminals
(j,k)]))
(let rules2 = NewRules&apos;(close (finished(rules,)))
rules, U rules2))
Theorem 4.1. dr(i,k) is the set of dotted rules that derive
a[i k].
</equation>
<bodyText confidence="0.884002">
Proof. By induction on the length of a[i k] as in the
proof of theorem 3.1, but with lemmas 4.1, 4.2, and 4.3
replacing 3.1, 3.2, and 3.3, respectively.0
If a = rs we find that
</bodyText>
<equation confidence="0.999564666666667">
dr(0,1) = {(f—. r .),(k--&gt; cf. . cgc),(k ----&gt; cfc . gc)}
dr(1,2) = {(g ---&gt; s.)}
dr(0,2) = {(k—* cfcg . e),(k —&gt; cfcgc .)}
</equation>
<sectionHeader confidence="0.966844" genericHeader="method">
6 THE PARSER WITH TOP-DOWN FILTERING
</sectionHeader>
<bodyText confidence="0.997893857142857">
We have described two parsers that set dr(i,k) to the set
of dotted rules that derive a[i k]. We now consider a
parser that uses top-down filtering to eliminate some
useless rules from dr(i, k). Let us say that A follows p if
the start symbol derives a string beginning with f3A. A
dotted rule (A —&gt; x) follows p if A follows p. The new
algorithm will set dr(i,k) to the set of dotted rules that
derive a[i k] and follow a[0 i].
If A derives a string beginning with B, we say that A
can begin with B. The new algorithm requires a predic-
tion table, which contains all pairs [A B] such that A can
begin with B. Let P, be the set of pairs [A B] such that
(A —&gt; pP0&apos;) is a rule and p &gt; e. Let P„ I be Pn U
Link(Pn, P1).
Lemma 5.1. The set of pairs [A B] such that A can
begin with B is the union of Pn for all n 1.
Proof. By induction on the tree by which A derives a
string beginning with B. Details are left to the reader.0
Our problem is to construct a finite representation for
the prediction table. To see why this is difficult, con-
sider a grammar containing the rule
</bodyText>
<equation confidence="0.9340838">
(f(a,s(X))—&gt; f(a,X) g)
Computing the Ps gives us the following pairs of terms:
[f(a,s(X)) f(a,X)]
[f(a,s(s(Y))) f(a,Y)]
[f(a,s(s(s( Z)))) f(a,Z)]
</equation>
<bodyText confidence="0.982691676923077">
Thus if we try to build the prediction table in the
obvious way, we get an infinite set of pairs of terms.
The key to this problem is to recognize that it is not
necessary or even useful to predict every possible
feature of the next input. It makes sense to predict the
presence of traces, but predicting the subcategorization
frame of a verb will cost more than it saves. To avoid
predicting certain features, we use a weak prediction
table; that is, a set of pairs of symbols that properly
contains the set of all [A B] such that A B. This weak
prediction table is guaranteed to eliminate no more than
what the ideal prediction table eliminates. It may leave
some dotted rules in dr(i,k) that the ideal prediction
table would remove, but it may also cost less to use.
Sato and Tamaki (1984) proposed to analyze the
behavior of Prolog programs, including parsers, by
using something much like a weak prediction table. To
guarantee that the table was finite, they restricted the
depth of terms occurring in the table. Shieber (1985b)
offered a more selective approach—his program pre-
dicts only those features chosen by the user as most
useful for prediction. Pereira and Shieber (1987) discuss
both approaches. We will present a variation of Shie-
ber&apos;s ideas that depends on using a sorted language.
To build a weak prediction table we begin with a set
Q, of terms such that P, C ground(Q1). Define
LP(Q,Q&apos;) = {(s [x z]) I (3 y,y&apos;. [x y] E Q A [y&apos; z] E
Q&apos; A s = m.g.u. of y and y&apos;)}
By Theorem 2.1, ground(LP(Q,Q&apos;)) = Link(ground(Q),
ground(Q&apos;)). Let Q, equal Q, U LP (Q1,Q1). Then by
lemma 2.3 and induction,
Pi c ground(. Q,)
That is, the union of the Qis represents a weak predic-
tion table. Thus we have shown that if a weak prediction
table is adequate, we are free to choose any Q, such that
P, C ground(Q1).
Suppose that QD subsumes LP(QD,Q1). Then
ground(LP(QD,Q1)) C ground(QD) and ground(QD 1)
= ground(QD) U ground(LP(QD, Q1)) = ground(QD).
Since ground(Q, 1) is a function of ground(Q) for all i,
it follows that ground(Q) = ground(QD) for all D, so
ground(QD) = i 1 ground(Q)). That is, QD is a
finite representation of a weak prediction table. Our
problem is to choose Q, so that QD subsumes QD for
some D.
Let s, and s2 be sorts. In section 2 we defined s, &gt; s2
if there is a function letter of sort s, that has an
argument of sort s2. Let &gt;* be the transitive closure of
&gt;; a sort t is cyclic if t &gt;* t , and a term is cyclic if its
sort is cyclic. P, is equal to
{[A B] I (A —&gt; 0.13f3&apos;) E RuleTable&apos;}
so we can build a representation for P1. Let us form Q,
from this representation by replacing all cyclic terms
with new variables. More exactly, we apply the follow-
ing recursive transformation to each term t in the
representation of P1:
transform(fit ...tn)) =
if the sort of f is cyclic
then new-variable()
else fitransform (t1)...transform(tn))
where new-variable is a function that returns a new
variable each time it is called.
Then P, C ground(Q,), and Q, contains no function
letters of cyclic sorts. For example, if the function letter
s belongs to a cyclic sort, we will turn
</bodyText>
<equation confidence="0.92588025">
[ f(a,s(s(X))) f(a,X)]
into
fia,Z).fia,Y)]
If Q, = f[fta,Z),fta,Y)]}, then Q2 = Ilfta,V),f(a,W)], so
</equation>
<bodyText confidence="0.970935174603175">
Q, subsumes Q2, and Q, is already a finite representa-
tion of a weak prediction table. The following lemma
Computational Linguistics, Volume 15, Number 4, December 1989 227
Andrew Haas A Parsing Algorithm for Unification Grammar
shows that in general, the Q1 defined above allows us to
build a finite representation of a weak prediction table.
Lemma 5.2. Let Q1 be a set of pairs of terms that
contains no function letters of cyclic sorts, and let Q, be
as defined above for all i &gt; 1. Then for some D, QD
subsumes LP(QD,Q1).
Proof. Note first that since unification never intro-
duces a function letter that did not occur in the input, Q.
contains no function letters of cyclic sort for any i 1.
Let C be the number of noncyclic sorts in the
language. Then the maximum depth of a term that
contains no function letters of cyclic sorts is C + 1.
Consider a term as a labeled tree, and consider any path
from the root of such a tree to one of its leaves. The path
can contain at most one variable or function letter of
each noncyclic sort, plus one variable of a cyclic sort.
Then its length is at most C + 1.
Consider the set S of all pairs of terms in L that
contain no function letters of cyclic sorts. Let us
partition this set into equivalence classes, counting two
terms equivalent if they are alphabetic variants. We
claim that the number of equivalence classes is finite.
Since there is a finite bound on the depths of terms in S,
and a finite bound on the number of arguments of a
function letter in S, there is a finite bound V on the
number of variables in any term of S. Let v,...vK be a
list of variables containing V variables from each sort.
Then there is a finite number of pairs in S that use only
variables from v,...i/K; let S&apos; be the set of all such pairs.
Now each pair p in S is an alphabetic variant of a pair in
5&apos;, for we can replace the variables of p one-for-one
with variables from v,...vK. Therefore the number of
equivalence classes is no more than the number of
elements in S&apos;. We call this number E. We claim that QD
subsumes LP(QD,Q1) for some D E.
To see this, suppose that a does not subsume
LP(Q1,Q1) for all i &lt; E. If a does not subsume
LP(Q,,Q1), then Q1±1 intersects more equivalence
classes than Q, does. Since Q, intersects at least one
equivalence class, QE intersects all the equivalence
classes. Therefore QE subsumes LP(QE,Q1), which was
to be proved.E]
This lemma tells us that we can build a weak predic-
tion table for any grammar by throwing away all sub-
terms of cyclic sort. In the worst case, such a table
might be too weak to be useful, but our experience
suggests that for natural grammars a prediction table of
this kind is very effective in reducing the size of the
dr(i,k) s. In the following discussion we will assume that
we have a complete prediction table; at the end of this
section we will once again consider weak prediction
tables.
Definitions. If S is a set of symbols, let first(S) = S U
{ B I (3 A E S. [A B] E PredTable 1. If PredTable is
indeed a complete prediction table, first(S) is the set of
symbols B such that some symbol in S can begin with B.
If R is a set of dotted rules let next(R) = {B I (3 A,0,13&apos;
(A ---&gt; P.BP&apos;) E R}.
Consider the following example grammar:
</bodyText>
<figure confidence="0.9571632">
start a
a rg
c --&gt; rh
g s
h s
</figure>
<bodyText confidence="0.990818545454546">
The terminal symbols are r and s. In this grammar
first({ start}) = {start, a,r}, and next({( a —&gt; r. g)}) = { g}.)
The following lemma shows that we can find the set
of symbols that follow a[0 j] if we have a prediction
table and the sets of dotted rules that derive a[ij] for all
I &lt;I.
Lemma 5.3. Let j satisfy 0 j length(a). Suppose
that for 0 5_ i &lt;j, SW is the set of dotted rules that
follow a[0 i] and derive a[i j] (if j = 0 this is vacuous).
Let start be the start symbol of the grammar. Then the
set of symbols that follow a[0 j] is
</bodyText>
<equation confidence="0.832889">
first(if j = 0
{start}
U
&lt; next(S(i))))
_-s j
</equation>
<bodyText confidence="0.997294216216216">
Proof. We show first that every member of the given set
follows a[0 j]. If j = 0, certainly every member of
first({startp follows a[0 01 = e. If j&gt; 0, suppose that C
follows a[0 I], (C ---&gt; f3BP&apos;) is a rule, and a[i j]; then
clearly B follows a[0 j].
Next we show that if A follows a[0 j], A is in the
given set. We prove by induction on d that if start a[0
j]Aa&apos; by a tree t, and the leaf corresponding to the
occurrence of A after a[0 j] is at depth d in t, then A
belongs to the given set. If d = 0, then A = start, and
j = 0. We must prove that start E first(Istartp, which is
obvious.
If d&gt; 0 there are two cases. Suppose first that the
leaf n corresponding to the occurrence of A after a[0 j]
has younger brothers dominating a nonempty string
(younger brothers of n are children of the same father
occurring to the left of n). Then the father of n is
admitted by a rule of the form (C--&gt; pito). c is the label
of the father of n, and f3 consists of the labels of the
younger brothers of n in order. Then 0 a[i j], where
0 i &lt;j. Removing the descendants of n&apos;s father from
t gives a tree t&apos; whose yield is a[0 i]Ca&apos;. Therefore C
follows a[0 i]. We have shown that (C --&gt; pAp&apos;) is a rule,
C follows a[0 i], and p a[i j]. Then (C --&gt; f3.AP&apos;) E
S(i), A E next(S(i)), and A E (U i &lt;j next(S(i))).
Finally suppose that the younger brothers of n dom-
inate the empty string in t. Then if C is the label of n&apos;s
father, C can begin with A. Removing the descendants
of n&apos;s father from t gives a tree t&apos; whose yield begins
with a[0 j]C. Then C belongs to the given set by
induction hypothesis. If C E first(X) and C can begin
with A, then A E first(X). Therefore A belongs to the
given set. This completes the proof.
As an example, let a = rs. Then the set of dotted
rules that derive a[0 1] and follow a[0 0] is {(a ---&gt; r. g)}.
The dotted rule (c --&gt; r. h) derives a[0 1], but it does not
follow a[0 0] because c is not an element of first({start}).
</bodyText>
<page confidence="0.938805">
228 Computational Linguistics, Volume 15, Number 4, December 1989
</page>
<note confidence="0.407168">
Andrew Haas A Parsing Algorithm for Unification Grammar
</note>
<bodyText confidence="0.99786025">
We are finally ready to present the analogs of lemmas
3.1, 3.2, and 3.3 for the parser with prediction. Where
the earlier lemmas mentioned the set of symbols (or
dotted rules) that derive a[i j], these lemmas mention
the set of symbols (or dotted rules) that follow a[0 i] and
derive a[i j].
Lemma 5.4. Let a be a nonempty string. Suppose
that for i &lt;j &lt; k, S(i,j) is the set of dotted rules that
follow a[0 i] and derive a[i j], while S&apos;(j,k) is the set of
symbols that follow a[0 j] and derive a[j k]. The set of
dotted rules that follow a[0 i] and derive a[i k] using
many symbols is
</bodyText>
<subsectionHeader confidence="0.762199">
SkipEmpty(i&lt;Ui&lt;k AdvanceDot(S(i j), S &apos;(j ,k)))
</subsectionHeader>
<bodyText confidence="0.9513825">
Proof. Expanding definitions and using the same argu-
ment as in lemma 3.1, we have
</bodyText>
<equation confidence="0.656981333333333">
SkipEmpty(ik AdvanceDot({(B —&gt; 0.0,) E DR I B
follows a[0 i] A 0 a[i j]}
{A I A follows a[0 j] A A auj k]l) =
SkipEmpty({(B —&gt; 13A.02) E DR I B follows a[0 i]
A (2j. i&lt;j &lt;k A 0 a[i j] A A follows a[0 j] A A
a[j )
</equation>
<bodyText confidence="0.940411333333333">
If B follows a[0 i], (B —&gt; 0A 132) is a rule, and IS &gt; a[i j],
then A follows a[0 j]. Therefore the statement that A
follows a[0 j] is redundant and can be deleted, giving
</bodyText>
<equation confidence="0.95315425">
SkipEmpty({(B —&gt; 13A.02) E DR I B follows a[0 i]
A (3 j. i&lt;j&lt;k A p a[i j] A A a[j k])l)
This in turn is equal to
{(B —&gt; PA/3&apos;433) E DR I B follows a[0 i]
</equation>
<bodyText confidence="0.920734315789474">
A (2j. i&lt;j&lt;k A /3 a[i j] AA a[j k]) A f3&apos; el
This is the set of dotted rules that follow a[0 i] and
derive a[i k] using many symbols.0
Lemma 5.5. Suppose length(a[i j]) &gt; 1, S is the set of
symbols that follow a[0 i], and S&apos; is the set of dotted
rules that follow a[0 i] and derive a[i j] using many
symbols. Then S fl close(finished(S&apos;)) is the set of
symbols that follow a[0 i] and derive a[i j].
Proof. S&apos; is a subset of the set of dotted rules that
derive a[i j], so by lemma 4.2 and monotonicity,
close(finished(S&apos;)) is a subset of the set of symbols that
derive a[i j]. Therefore every symbol in S 1-1 close(fin-
ished(S1))) derives a[ij] and follows a[0 i]. This proves
inclusion in one direction.
For the other direction, suppose A follows a[0 i] and
derives a[i j]. Then by lemma 4.2 there is a dotted rule
(B —&gt; p.) such that f3 &gt; a[i j] using many symbols and
A B. Then B follows a[0 i], so B is in finished(S&apos;),
which means that A is in S fl close(finished(5&apos;)).0
Definition. If S is a set of symbols and R a set of
dotted rules, filter(S,R) is the set of rules in R whose left
sides are in S. In other words, filter(S,R) = {( A —&gt; 0.0&apos;)
ERI AES}.
Lemma 5.6. Suppose S is the set of symbols that
follow a[0 i], and S&apos; is the set of symbols that follow
a[0 i] and derive a[i j]. Then the set of rules that follow
a[0 i] and derive a[i j] using one symbol is
filter(S,NewRules&apos;(S&apos;)).
Proof: S&apos; is a subset of the set of symbols that derive
a[i j]. By lemma 4.3 and monotonicity, we know that
every dotted rule in NewRules&apos;(S&apos;) derives a[i j] using
one symbol. Therefore every dotted rule in filter(S,Ne-
wRules(S&apos;)) follows a[0 i] and derives a[i j] using one
symbol. This proves inclusion in one direction.
For the other direction, consider any dotted rule that
follows a[0 i] and derives a[ij] using one symbol; it can
be written in the form (A —&gt; 0B0&apos;.p1), where s and /3&apos;
derive e, B derives a[i j], and A follows a[0 i]. Since
</bodyText>
<equation confidence="0.9895294">
/3 e, B follows a[0 i]. Therefore B E S&apos; and (A
0B0&apos;.01) is in NewRules&apos;(S&apos;). Since A follows a[0 i], (A
—&gt; .p,) is in filter(S,NewRules&apos;(S&apos;)).
Let a be a string of length L. For ()-i&lt;k5- L, define
pred(j) =
first(ifj = 0
then {Start}
else (U C:11&lt;j next(dr(i,j))))
dr(i,k) =
if i + 1 = k
then filter(pred(i),NewRules&apos;qpred(i) 11 close
({a[i k]})]))
else (let rules, =
SkipEmpty(U AdvanceDot(dr(i,j),
1&lt;j &lt;k [finished(nr(j,k))
U terminals(j,k)])
(let rules2 = filter(pred(i),
NewRules&apos;([pred(i) fl
close(finished(rules,))]))
rules, U rules2))
</equation>
<bodyText confidence="0.906795775510204">
Note that the new version of dr(i,k) is exactly like the
previous version except that we filter the output of close
by intersecting it with pred(i), and we filter the output of
NewRules&apos; by applying the function filter.
Theorem 5.6 For 05-k:sL, pred(k) is the set of sym-
bols that follow a[0 i], and if 0-i&lt; k, dr(i,k) is the set of
dotted rules that follow a[0 i] and derive a[i k].
Proof. This proof is similar to the proof of theorem
3.4, but it is more involved because we must show that
pred(k) has the desired values. Once more we argue by
induction, but this time it is a double induction: an outer
induction on k, and an inner induction on the length of
strings that end at k.
We show by induction on k that pred(k) has the
desired value and for Oi&lt;k, dr(i,k) has the desired
value. If k = 0, lemma 5.3 tells us that pred(0) is the set
of symbols that follow a[0 01, and the second part of the
induction hypothesis is vacuously true.
If k &gt; 0, we first show by induction on the length of
a[i k] that dr(i,k) has the desired value for 0 This
part of the proof is much like the proof of 3.4. If a[i k]
has length 1, then pred(i) is the set of symbols that
follow a[0 i] by the hypothesis of the induction on k.
Then pred(i) n closeQa[i k]l) is the set of symbols that
follow a[0 i] and derive a[i k] , so lemma 5.6 tells us that
filter(pred (i),NewRules&apos;(pred(i) 1-1 close({a[i k]})))
is the set of dotted rules that follow a[0 i] and derive
a[i k].
If length(a[i k]) &gt; 1, consider any j such that i&lt;j&lt;k.
dr(i,j) and dr(j,k) have the desired values by induction
Computational Linguistics, Volume 15, Number 4, December 1989 229
Andrew Haas A Parsing Algorithm for Unification Grammar
hypothesis. Then lemma 5.4 tells us that rules, is the set
of dotted rules that follow a[0 i] and derive a[i k] using
many symbols. pred(i) is the set of symbols that follow
a[0 i] , so pred(i) fl close(finished(rides1)) is, the set of
symbols that follow a[0 i] and derive a[i k], by lemma
5.5. Therefore rules2 is the set of dotted rules that follow
a[0 i] and derive a[i k] using one symbol, by lemma 5.6.
The union of rulesi and rules2 is the set of dotted rules
that follow a[0 i] and derive a[i k], and this completes
the inner induction.
To complete the outer induction, we use lemma 5.3
to show that pred(k) is the set of symbols that follow a[0
k]. This completes the proof.0
Corollary: Start E finished(dr(0,L)) iff a is a sentence
of the language generated by G.
Suppose we are parsing the string rs using the exam-
ple grammar. Then we have
</bodyText>
<equation confidence="0.999936">
pred(0) = {start,a,r}
dr(0,1) = {(a —&gt; r. g)}
pred(1) = {g,s}
dr(1,2) = {(g—&gt; s .)}
dr(0,2) = f(a —&gt; rg .),(start —&gt; a .)}
</equation>
<bodyText confidence="0.999983">
We have proved the correctness of the parser when it
uses an ideal prediction table. We must still consider
what happens when the parser uses a weak prediction
table.
Theorem 5.7. If PredTable is a superset of the set of
all [A B] such that A can begin with B, then start E
finished(dr(0,L)) iff a is a sentence of the language
generated by G.
Proof. Note that the parser with filtering always
builds a smaller dr(i,k) than the parser without filtering.
Since all the operations of the parser are monotonic,
this is an easy induction. So if the parser with filtering
puts the start symbol in dr(0,L), the parser without
filtering will do this also, implying that a is a sentence.
Note also that the parser with filtering produces a larger
dr(i,k) given a larger PredTable (again, this follows
easily because all operations in the parser are monoton-
ic). So if a is a sentence, the parser with the ideal
prediction table includes Start in dr(0,L), and so does
the parser with the weak prediction table.0
</bodyText>
<sectionHeader confidence="0.9218065" genericHeader="method">
7 DISCUSSION AND IMPLEMENTATION NOTES
7.1 RELATED WORK AND POSSIBLE EXTENSIONS
</sectionHeader>
<bodyText confidence="0.999990757142858">
The chief contribution of the present paper is to define
a class of grammars on which bottom-up parsers always
halt, and to give a semi-decision procedure for this
class. This in turn makes it possible to prove a com-
pleteness theorem, which is impossible if one considers
arbitrary unification grammars. One can obtain similar
results for the class of grammars whose context-free
backbone is finitely ambiguous—what Pereira and War-
ren (1983) called the offline-parsable grammars. How-
ever, as Shieber (1985b) observed, this class of gram-
mars excludes many linguistically interesting grammars
that do not use atomic category symbols.
The present parser (as opposed to the table-building
algorithm) is much like those in the literature. Like
nearly all parsers using term unification, it is a special
case of Earley deduction (Pereira and Warren 1985).
The tables are simply collections of theorems proved in
advance and added to the program component of Earley
deduction. Earley deduction is a framework for parsing
rather than a parser. Among implemented parsers, BUP
(Matsumota et al. 1983) is particularly close to the
present work. It is a bottom-up left-corner parser using
term unification. It is written in Prolog and uses back-
tracking, but by recording its results as clauses in the
Prolog database it avoids most backtracking, so that it is
close to a chart parser. It also includes top-down
filtering, although it uses only category symbols in
filtering. The paper includes suggestions for handling
rules with empty right sides as well. The main difference
from the present work is that the authors do not
describe the class of grammars on which their algorithm
halts, and as a result they cannot prove completeness.
The grammar formalism presented here is much
simpler than many formalisms called &amp;quot;unification gram-
mars.&amp;quot; There are no meta-rules, no default values of
features, no general agreement principles (Gazdar et al.
1986). We have found this formalism adequate to de-
scribe a substantial part of English syntax—at least,
substantial by present-day standards. Our grammar
currently contains about 300 syntactic rules, not count-
ing simple rules that introduce single terminals. It
includes a thorough treatment of verb subcategorization
and less thorough treatments of noun and adjective
subcategorization. It covers major construction types:
raising, control, passive, subject-aux inversion, imper-
atives, wh-movement (both questions and relative
clauses), determiners, and comparatives. It assigns
parses to 85% of a corpus of 791 sentences. See Ayuso
et al. 1988 for a description of the grammar.
It is clear that some generalizations are being missed.
For example, to handle passive we enumerate by hand
the rules that other formalisms would derive by meta-
rule. We are certainly missing a generalization here, but
we have found this crude approach quite practical—our
coverage is wide and our grammar is not hard to
maintain. Nevertheless, we would like to add meta-
rules and probably some general feature-passing princi-
ples.. We hope to treat them as abbreviation mecha-
nisms—we would define the semantics of a general
feature-passing principal by showing how a grammar
using that principal can be translated into a grammar
written in our original formalism. We also hope to add
feature disjunction to our grammar (see Kasper 1987;
Kasper and Rounds 1986).
Though our formalism is limited, it has one property
that is theoretically interesting: a sharp separation be-
tween the details of unification and the parsing mecha-
nism. We proved in Section 3 that unification allows us
to compute certain functions and predicates on sets of
grammatical expressions—symbolic products, unions,
</bodyText>
<page confidence="0.902369">
230 Computational Linguistics, Volume 15, Number 4, December 1989
</page>
<note confidence="0.386601">
Andrew Haas A Parsing Algorithm for Unification Grammar
</note>
<bodyText confidence="0.997869733333334">
and so forth. In Section 4 and 5 we assumed that these
functions were available as primitives and used them to
build bottom-up parsers. Nothing in Sections 4 and 5
depends on the details of unification. If we replace
standard unification with another mechanism, we have
only to re-prove the results of Section 3 and the cor-
rectness theorems of Sections 4 and 5 follow at once. To
see that this is not a trivial result, notice that we failed
to maintain this separation in Section 6. To show that
one can build a complete prediction table, we had to
consider the details of unification: we mentioned terms
like &amp;quot;alphabetic variant&amp;quot; and &amp;quot;subsumption.&amp;quot; We have
presented a theory of bottom-up parsing that is general
in the sense that it does not rely on a particular
pattern-matching mechanism—it applies to any mecha-
nism for which the results of Section 3 hold. We claim
that these results should hold for any reasonable
pattern-matching mechanism; the reader must judge this
claim by his or her own intuition.
One drawback of this work is that depth-bounded-
ness is undecidable. To prove this, show that any
Turing machine can be represented as a unification
grammar, and then show that an algorithm that decides
depth-boundedness can also solve the halting problem.
This result raises the question: is there a subset of the
depth-bounded grammars that is strong enough to de-
scribe natural language, and for which membership is
decidable?
Recall the context-free backbone of a grammar,
described in the Introduction. One can form a context-
free backbone for a unification grammar by keeping
only the topmost function letters in each rule. There is
an algorithm to decide whether this backbone is depth-
bounded, and if the backbone is depth-bounded, so is
the original grammar (because the backbone admits
every derivation tree that the original grammar admits).
Unfortunately this class of grammars is too restricted—
it excludes rules like (major-category(n,2) —&gt; major-
category(n,1)), which may well be needed in grammars
for natural language.
Erasing everything but the top function letter of each
term is drastic. Instead, let us form a &amp;quot;backbone&amp;quot; by
applying the transformation of Section 6, which elimi-
nates cyclic function letters. We can call the resulting
grammar the acyclic backbone of the original grammar.
We showed in Section 6 that if we eliminate cyclic
function letters, then the relation of alphabetic variance
will partition the set of all terms into a finite number of
equivalence classes. We used this fact to prove that the
algorithm for building a weak prediction table always
halts. By similar methods we can construct an algorithm
that decides depth-boundedness for grammars without
cyclic function letters. Then the grammars whose acy-
clic backbones are depth-bounded form a decidable
subset of the depth-bounded grammars. One can prove
that this class of grammars generates the same lan-
guages as the off-line parsable grammars. Unlike the
off-line parsable grammars, they do not require atomic
category symbols. A forthcoming paper will discuss
these matters in detail.
</bodyText>
<subsectionHeader confidence="0.985131">
7.2 THE IMPLEMENTATION
</subsectionHeader>
<bodyText confidence="0.999433076923077">
Our implementation is a Common Lisp program on a
Symbolics Lisp Machine. The algorithm as stated is
recursive, but the implementation is a chart parser. It
builds a matrix called &amp;quot;rules&amp;quot; and sets rules[i k] equal to
dr(i,k), considering pairs [1 k] in the same order used for
the induction argument in the proof. It also builds a
matrix &amp;quot;symbols&amp;quot; and sets symbols[i k] to the set of
symbols that derive au k], and a matrix pred with
pred[i] equal to the set of symbols that follow a[0 i].
Currently the standard parser does not incorporate
prediction. We have found that prediction reduces the
size of the chart dramatically, but the cost of prediction
is so great that a purely bottom-up parser runs faster.
</bodyText>
<tableCaption confidence="0.99897">
Table 1. Chart Sizes and Total Time for Parsing with Prediction
</tableCaption>
<table confidence="0.670921133333333">
Sentence No Categories Traces Traces and
Prediction Verb Form
1 524 517 248 150
2 878 867 686 667
3 799 713 500 387
4 936 921 558 467
5 283 279 145 90
6 997 969 524 368
7 531 525 323 247
8 982 950 640 507
9 1519 1503 1007 711
10 930 920 495 400
11 2034 2014 1128 771
total time 917 2201 1538 1085
(in seconds)
</table>
<bodyText confidence="0.989246325842697">
Table 1 presents the results of predicting different
features on a sample of 11 sentences. It describes
parsing without prediction, with prediction of categories
only, with traces and categories, and finally with cate-
gories, traces, and verb form information. In each case
it lists the total number of entries in the matrices
&amp;quot;rules&amp;quot; and &amp;quot;symbols&amp;quot; for every sentence, and the
total time to parse the 11 sentences. The reader should
compare this table with the one in Shieber 1985. Shieber
tried predicting subcategorization information along
with categories. In our grammar there is a separate VP
rule for each subcategorization frame, and this rule
gives the categories of all arguments of the verb.
Shieber eliminated these multiple VP rules by making
the list of arguments a feature of the verb. Therefore by
predicting categories alone, we get the same informa-
tion that Shieber got by predicting subcategorization
information. The table shows that for our grammar,
prediction reduces the chart size drastically, but it is so
costly that a straight bottom-up parser runs faster than
any version of prediction.
The parsing tables for the present grammar are quite
tractable. The largest table is the table of chain rules,
which has 2,270 entries and takes under ten minutes to
build. A prediction table that predicts categories,
traces, and verb forms has 1,510 entries and takes six
minutes to build.
Computational Linguistics, Volume 15, Number 4, December 1989 231
Andrew Haas A Parsing Algorithm for Unification Grammar
In the special case of a context-free grammar, our
parsing program is essentially the same as the parser of
Graham et al. (1980), in particular algorithm 2.2 of that
paper. The only significant differences are that their
chart includes entries for empty substrings, which we
omit, and that we record symbols while they record
only dotted rules. When running on a context-free
grammar, the parser takes time proportional to the cube
of the length n of the input string—because the number
of symbolic products is proportional to n3, and the time
for a symbolic product is independent of the input
string. This result also holds for a grammar without
cyclic function letters. If there are cyclic function
letters, the size of the nonterminals built by the parser
depends on the length of the input, so the time for
unifications and symbolic products is no longer inde-
pendent of the input, and the parsing time is not
bounded by n3.
To save storage we use a simplified version of
structure-sharing (Boyer and Moore 1972). Following
the suggestion of Pereira and Warren (1983), we use
structure-sharing only for dotted rules with symbols
remaining after the dot. When the dot reaches the end of
the right side of a rule, we translate the left side of the
rule back to standard representation. This method guar-
antees that in each resolution only one resolvent is in
structure-sharing representation. Instead of general res-
olution we are doing what the theorem-proving litera-
ture calls input resolution. This allows us to represent a
substitution as a simple association list, using the func-
tion assoc to retrieve the substitutions that have been
made for variables.
Pereira (1985) describes a more sophisticated version
of structure-sharing. This method has two advantages
over our version. First, the time to retrieve a substitu-
tion is 0(log n), where n is the length of the derivation,
compared to 0(n) for Boyer-Moore. Second, only sym-
bols that derive the empty string need to be translated
from structure-sharing form to the standard representa-
tion, and this saves storage. The first advantage may not
be important, for two reasons. By using a single assoc
to retrieve a substitution, we reduce the constant factor
in 0(n). Also by eliminating the structure sharing each
time the dot reaches the end of a rule, we keep our
derivations short—n is no more than the length of the
right side of the longest rule. The second advantage of
Pereira&apos;s method is more important, since our current
parser uses a lot of storage.
The other optimizations are fairly obvious. As usual
we skip the occur check in our unifications (as long as
there are no cyclic sorts, this is guaranteed to be safe).
In each symbolic product, one set is indexed by the
topmost function letter of the term to be matched,
which saves a good number of failed unifications. These
simple techniques gave us adequate performance for
some time, but as the grammar grew the parser slowed
down, and we decided to rewrite the program in C. This
version, running on a Sun 4, is much more efficient. It
parses a corpus of 790 sentences, with an average length
of nine words, in half an hour.
</bodyText>
<sectionHeader confidence="0.989285" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.9973015">
I wish to thank an anonymous referee, whose careful reading and
detailed comments greatly improved this paper. This work was
supported by the Defense Advanced Research Projects Agency under
contract numbers N00014-87-C-0085 and N00014-85-C-0079.
</bodyText>
<sectionHeader confidence="0.998639" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.997206722222222">
Ayuso, Damaris; Chow, Yen-lu; Haas, Andrew; Ingria, Robert;
Roucos, Salim; Scha, Remko; and Stallard, David 1988 Integration
of Speech and Natural Language Interim Report. Report No. 6813,
BBN Laboratories Inc., Cambridge, MA.
Barton, G. Edward; Berwick, Robert C.; and Ristad, Eric S. 1987
Computational Complexity and Natural Language. MIT Press,
Cambridge, MA.
Boyer, Robert and Moore, Jay S. 1972 The Sharing of Structure in
Theorem-Proving Programs. In: Meltzer, Bernard and Michie,
Donald (eds.), Machine Intelligence 7. John Wiley and Sons, New
York, NY: 101-116.
Gather, Jean 1986 Logic for Computer Science. Harper and Row,
New York, NY.
Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey; and Sag, Ivan 1985
Generalized Phrase Structure Grammar. Harvard University
Press, Cambridge, MA.
Graham, Susan L.; Harrison, Michael A.; and Ruzzo, Walter L. 1980
An Improved Context-free Recognizer. ACM Transactions on
Programming Languages and Systems 2(3): 415-462.
Hoperoft, John E. and Ullman, Jeffrey D. 1969 Formal Languages
and Their Relation to Automata. Addison-Wesley Publishing
Company, Reading, MA.
Kasper, Robert 1987 Feature Structures: A Logical Theory with
Application to Language Analysis. Ph.D. Thesis, University of
Michigan, Ann Arbor, MI.
Kasper, Robert and Rounds, William 1986 A Logical Semantics for
Feature Structures. In: Proceedings of the 24th Annual Meeting of
Association for Computational Linguistics. Columbia University,
New York, NY: 257-266.
Matsumoto, Yuji; Tanaka, Hozumi; Hirakawa, Hideki; Miyoshi,
Hideo; and Yasukawa, Hideki 1983 BUP: A Bottom-up Parser
Embedded in Prolog. New Generation Computing, 1(2): 145-158.
Pereira, Fernando 1985 A Structure-Sharing Representation for Uni-
fication-Based Grammar Formalisms. In: Proceedings of the 23rd
Annual Meeting of the Association for Computational Linguistics.
University of Chicago, Chicago, IL: 137-144.
Pereira, Fernando and Sheiber, Stuart 1987 Prolog and Natural-
Language Analysis. Center for the Study of Language and Infor-
mation, Stanford, CA. Distributed by Chicago University Press.
Pereira, Fernando and Warren, David H. D. 1980 Definite Clause
Grammars for Natural Language Analysis—A Survey of the
Formalism and a Comparison with Augmented Transition Net-
works. Artificial Intelligence 13(3): 231-278.
Robinson, John A. 1965 A Machine-Oriented Logic Based on the
Resolution Principle. Journal of the ACM 12(1): 23-41.
Sato, Taisuke and Tamaki, Hisao 1984 Enumeration of Success
Patterns in Logic Programs. Theoretical Computer Science 34:
227-240.
Shieber, Stuart 1985 Evidence against the Context-Freeness of Nat-
ural Language. Linguistics and Philosophy 8(3): 333-343.
Shieber, Stuart 1985 Using Restriction to Extend Parsing Algorithms
for Complex-Feature-Based Formalisms. In: Proceedings of the
23rd Annual Meeting of the Association for Computational Lin-
guistics. University of Chicago, Chicago, IL: 145-152.
</reference>
<page confidence="0.903311">
232 Computational Linguistics, Volume 15, Number 4, December 1989
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912990">
<title confidence="0.998587">ALGORITHM FOR UNIFICATION GRAMMAR</title>
<author confidence="0.999941">Andrew Haas</author>
<affiliation confidence="0.99923">Department of Computer Science State University of New York at</affiliation>
<address confidence="0.924573">Albany, New York 12222</address>
<abstract confidence="0.998555571428571">We describe a table-driven parser for unification grammar that combines bottom-up construction of phrases with top-down filtering. This algorithm works on a class of grammars called depth-bounded grammars, and it is guaranteed to halt for any input string. Unlike many unification parsers, our algorithm works directly on a unification grammar—it does not require that we divide the grammar into a context-free &amp;quot;backbone&amp;quot; and a set of feature agreement constraints. We give a detailed proof of correctness. For the case of a pure bottom-up parser, our proof does not rely on the details of unification —it works for any pattern-matching technique that satisfies certain simple conditions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Damaris Ayuso</author>
<author>Yen-lu Chow</author>
<author>Andrew Haas</author>
<author>Robert Ingria</author>
<author>Salim Roucos</author>
<author>Remko Scha</author>
<author>David Stallard</author>
</authors>
<date>1988</date>
<journal>Integration of Speech and Natural Language Interim</journal>
<tech>Report. Report No. 6813,</tech>
<institution>BBN Laboratories Inc.,</institution>
<location>Cambridge, MA.</location>
<contexts>
<context position="63708" citStr="Ayuso et al. 1988" startWordPosition="12037" endWordPosition="12040">rmalism adequate to describe a substantial part of English syntax—at least, substantial by present-day standards. Our grammar currently contains about 300 syntactic rules, not counting simple rules that introduce single terminals. It includes a thorough treatment of verb subcategorization and less thorough treatments of noun and adjective subcategorization. It covers major construction types: raising, control, passive, subject-aux inversion, imperatives, wh-movement (both questions and relative clauses), determiners, and comparatives. It assigns parses to 85% of a corpus of 791 sentences. See Ayuso et al. 1988 for a description of the grammar. It is clear that some generalizations are being missed. For example, to handle passive we enumerate by hand the rules that other formalisms would derive by metarule. We are certainly missing a generalization here, but we have found this crude approach quite practical—our coverage is wide and our grammar is not hard to maintain. Nevertheless, we would like to add metarules and probably some general feature-passing principles.. We hope to treat them as abbreviation mechanisms—we would define the semantics of a general feature-passing principal by showing how a </context>
</contexts>
<marker>Ayuso, Chow, Haas, Ingria, Roucos, Scha, Stallard, 1988</marker>
<rawString>Ayuso, Damaris; Chow, Yen-lu; Haas, Andrew; Ingria, Robert; Roucos, Salim; Scha, Remko; and Stallard, David 1988 Integration of Speech and Natural Language Interim Report. Report No. 6813, BBN Laboratories Inc., Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Edward Barton</author>
<author>Robert C Berwick</author>
<author>Eric S Ristad</author>
</authors>
<date>1987</date>
<booktitle>Computational Complexity and Natural Language.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Barton, Berwick, Ristad, 1987</marker>
<rawString>Barton, G. Edward; Berwick, Robert C.; and Ristad, Eric S. 1987 Computational Complexity and Natural Language. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Boyer</author>
<author>Jay S Moore</author>
</authors>
<title>The Sharing of Structure in Theorem-Proving Programs.</title>
<date>1972</date>
<booktitle>Machine Intelligence 7.</booktitle>
<pages>101--116</pages>
<editor>In: Meltzer, Bernard and Michie, Donald (eds.),</editor>
<publisher>John Wiley and Sons,</publisher>
<location>New York, NY:</location>
<contexts>
<context position="71690" citStr="Boyer and Moore 1972" startWordPosition="13361" endWordPosition="13364"> takes time proportional to the cube of the length n of the input string—because the number of symbolic products is proportional to n3, and the time for a symbolic product is independent of the input string. This result also holds for a grammar without cyclic function letters. If there are cyclic function letters, the size of the nonterminals built by the parser depends on the length of the input, so the time for unifications and symbolic products is no longer independent of the input, and the parsing time is not bounded by n3. To save storage we use a simplified version of structure-sharing (Boyer and Moore 1972). Following the suggestion of Pereira and Warren (1983), we use structure-sharing only for dotted rules with symbols remaining after the dot. When the dot reaches the end of the right side of a rule, we translate the left side of the rule back to standard representation. This method guarantees that in each resolution only one resolvent is in structure-sharing representation. Instead of general resolution we are doing what the theorem-proving literature calls input resolution. This allows us to represent a substitution as a simple association list, using the function assoc to retrieve the subst</context>
</contexts>
<marker>Boyer, Moore, 1972</marker>
<rawString>Boyer, Robert and Moore, Jay S. 1972 The Sharing of Structure in Theorem-Proving Programs. In: Meltzer, Bernard and Michie, Donald (eds.), Machine Intelligence 7. John Wiley and Sons, New York, NY: 101-116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Gather</author>
</authors>
<date>1986</date>
<institution>Logic for Computer Science. Harper and Row,</institution>
<location>New York, NY.</location>
<marker>Gather, 1986</marker>
<rawString>Gather, Jean 1986 Logic for Computer Science. Harper and Row, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Ewan Klein</author>
<author>Geoffrey Pullum</author>
<author>Ivan Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1397" citStr="Gazdar et al. 1985" startWordPosition="218" endWordPosition="221">atching technique that satisfies certain simple conditions. 1 INTRODUCTION Unrestricted unification grammars have the formal power of a Turing machine. Thus there is no algorithm that finds all parses of a given sentence in any unification grammar and always halts. Some unification grammar systems just live with this problem. Any general parsing method for definite clause grammar will enter an infinite loop in some cases, and it is the task of the grammar writer to avoid this. Generalized phrase structure grammar avoids the problem because it has only the formal power of context-free grammar (Gazdar et al. 1985), but according to Shieber (1985a) this is not adequate for describing human language. Lexical functional grammar employs a better solution. A lexical functional grammar must include a finitely ambiguous context-free grammar, which we will call the context-free backbone (Barton 1987). A parser for lexical functional grammar first builds the finite set of context-free parses of the input and then eliminates those that don&apos;t meet the other requirements of the grammar. This method guarantees that the parser will halt. This solution may be adequate for lexical functional grammars, but for other un</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey; and Sag, Ivan 1985 Generalized Phrase Structure Grammar. Harvard University Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan L Graham</author>
<author>Michael A Harrison</author>
<author>Walter L Ruzzo</author>
</authors>
<title>An Improved Context-free Recognizer.</title>
<date>1980</date>
<journal>ACM Transactions on Programming Languages and Systems</journal>
<volume>2</volume>
<issue>3</issue>
<pages>415--462</pages>
<contexts>
<context position="5605" citStr="Graham et al. 1980" startWordPosition="895" endWordPosition="898">pth-bounded because the depth of a tree is a linear function of the length of the string it derives. A similar grammar can derive the crossed serial dependencies of Swiss German, which according to Shieber (1985a) no context-free grammar can derive. It is clear where the extra formal power comes from: a contextfree grammar has a finite set of nonterminals, but a unification grammar can build arbitrarily large nonterminal symbols. It remains to show that there is a parsing algorithm for depth-bounded unification grammars. We have developed such an algorithm, based on the context-free parser of Graham et al. 1980, which is a table-driven parser. If we generalize the table-building algorithm to a unification grammar in an obvious way, we get an algorithm that is guaranteed to halt for all depthbounded grammars (not for all unification grammars). Given that the tables can be built, it is easy to show that the parser halts on every input. This is not a special property of our parser—a straightforward bottom-up parser will also halt on all depth-bounded grammars, because it builds partial parse trees in order of their depth. Our contribution is to show that a simple algorithm will verify depth-boundedness</context>
<context position="70802" citStr="Graham et al. (1980)" startWordPosition="13211" endWordPosition="13214">y that a straight bottom-up parser runs faster than any version of prediction. The parsing tables for the present grammar are quite tractable. The largest table is the table of chain rules, which has 2,270 entries and takes under ten minutes to build. A prediction table that predicts categories, traces, and verb forms has 1,510 entries and takes six minutes to build. Computational Linguistics, Volume 15, Number 4, December 1989 231 Andrew Haas A Parsing Algorithm for Unification Grammar In the special case of a context-free grammar, our parsing program is essentially the same as the parser of Graham et al. (1980), in particular algorithm 2.2 of that paper. The only significant differences are that their chart includes entries for empty substrings, which we omit, and that we record symbols while they record only dotted rules. When running on a context-free grammar, the parser takes time proportional to the cube of the length n of the input string—because the number of symbolic products is proportional to n3, and the time for a symbolic product is independent of the input string. This result also holds for a grammar without cyclic function letters. If there are cyclic function letters, the size of the n</context>
</contexts>
<marker>Graham, Harrison, Ruzzo, 1980</marker>
<rawString>Graham, Susan L.; Harrison, Michael A.; and Ruzzo, Walter L. 1980 An Improved Context-free Recognizer. ACM Transactions on Programming Languages and Systems 2(3): 415-462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Hoperoft</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Formal Languages and Their Relation to Automata.</title>
<date>1969</date>
<publisher>Addison-Wesley Publishing Company,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="12241" citStr="Hoperoft and Ullman (1969)" startWordPosition="2132" endWordPosition="2135">I . We come now to the key definition of this paper. Let G = (S, (14 T, P. Z) be a unification grammar. The ground grammar for G is the four-tuple (N, T, P&apos;, Z), where N is the set of all ground terms of (2,4 , T is the set of terminals of G, P&apos; is the set of all ground instances of rules in P, and Z is the start symbol of G. If N and P&apos; are finite, the ground grammar is a context-free grammar. If N or P&apos; is infinite, the ground grammar is not a context-free grammar, and it may generate a language that is not context-free. Nonetheless we can define derivation trees just as in a cfg. Following Hoperoft and Ullman (1969), we allow derivation trees with nonterminals at their leaves. Thus a derivation tree may represent a partial derivation. We differ from Hoperoft and Ullman by allowing nonterminals other than the start symbol to label the root of the tree. A derivation tree is an A-tree if the non-terminal A labels its root. The yield of a derivation tree is the string formed by reading the symbols at its leaves from left to right. As in a cfg, A a iff there is an A-tree with yield a. The language generated by a ground grammar is the set of terminal strings derived from the start symbol. The language generate</context>
</contexts>
<marker>Hoperoft, Ullman, 1969</marker>
<rawString>Hoperoft, John E. and Ullman, Jeffrey D. 1969 Formal Languages and Their Relation to Automata. Addison-Wesley Publishing Company, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Kasper</author>
</authors>
<title>Feature Structures: A Logical Theory with Application to Language Analysis.</title>
<date>1987</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Michigan,</institution>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="64476" citStr="Kasper 1987" startWordPosition="12165" endWordPosition="12166"> other formalisms would derive by metarule. We are certainly missing a generalization here, but we have found this crude approach quite practical—our coverage is wide and our grammar is not hard to maintain. Nevertheless, we would like to add metarules and probably some general feature-passing principles.. We hope to treat them as abbreviation mechanisms—we would define the semantics of a general feature-passing principal by showing how a grammar using that principal can be translated into a grammar written in our original formalism. We also hope to add feature disjunction to our grammar (see Kasper 1987; Kasper and Rounds 1986). Though our formalism is limited, it has one property that is theoretically interesting: a sharp separation between the details of unification and the parsing mechanism. We proved in Section 3 that unification allows us to compute certain functions and predicates on sets of grammatical expressions—symbolic products, unions, 230 Computational Linguistics, Volume 15, Number 4, December 1989 Andrew Haas A Parsing Algorithm for Unification Grammar and so forth. In Section 4 and 5 we assumed that these functions were available as primitives and used them to build bottom-up</context>
</contexts>
<marker>Kasper, 1987</marker>
<rawString>Kasper, Robert 1987 Feature Structures: A Logical Theory with Application to Language Analysis. Ph.D. Thesis, University of Michigan, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Kasper</author>
<author>William Rounds</author>
</authors>
<title>A Logical Semantics for Feature Structures. In:</title>
<date>1986</date>
<booktitle>Proceedings of the 24th Annual Meeting of Association for Computational Linguistics.</booktitle>
<pages>257--266</pages>
<institution>Columbia University,</institution>
<location>New York, NY:</location>
<contexts>
<context position="64501" citStr="Kasper and Rounds 1986" startWordPosition="12167" endWordPosition="12170">isms would derive by metarule. We are certainly missing a generalization here, but we have found this crude approach quite practical—our coverage is wide and our grammar is not hard to maintain. Nevertheless, we would like to add metarules and probably some general feature-passing principles.. We hope to treat them as abbreviation mechanisms—we would define the semantics of a general feature-passing principal by showing how a grammar using that principal can be translated into a grammar written in our original formalism. We also hope to add feature disjunction to our grammar (see Kasper 1987; Kasper and Rounds 1986). Though our formalism is limited, it has one property that is theoretically interesting: a sharp separation between the details of unification and the parsing mechanism. We proved in Section 3 that unification allows us to compute certain functions and predicates on sets of grammatical expressions—symbolic products, unions, 230 Computational Linguistics, Volume 15, Number 4, December 1989 Andrew Haas A Parsing Algorithm for Unification Grammar and so forth. In Section 4 and 5 we assumed that these functions were available as primitives and used them to build bottom-up parsers. Nothing in Sect</context>
</contexts>
<marker>Kasper, Rounds, 1986</marker>
<rawString>Kasper, Robert and Rounds, William 1986 A Logical Semantics for Feature Structures. In: Proceedings of the 24th Annual Meeting of Association for Computational Linguistics. Columbia University, New York, NY: 257-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
<author>Hozumi Tanaka</author>
<author>Hideki Hirakawa</author>
<author>Hideo Miyoshi</author>
<author>Yasukawa</author>
</authors>
<title>BUP: A Bottom-up Parser Embedded in Prolog.</title>
<date>1983</date>
<journal>New Generation Computing,</journal>
<volume>1</volume>
<issue>2</issue>
<pages>145--158</pages>
<location>Hideki</location>
<marker>Matsumoto, Tanaka, Hirakawa, Miyoshi, Yasukawa, 1983</marker>
<rawString>Matsumoto, Yuji; Tanaka, Hozumi; Hirakawa, Hideki; Miyoshi, Hideo; and Yasukawa, Hideki 1983 BUP: A Bottom-up Parser Embedded in Prolog. New Generation Computing, 1(2): 145-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
</authors>
<title>A Structure-Sharing Representation for Unification-Based Grammar Formalisms. In:</title>
<date>1985</date>
<booktitle>Proceedings of the 23rd Annual Meeting of the Association</booktitle>
<pages>137--144</pages>
<institution>for Computational Linguistics. University of Chicago,</institution>
<location>Chicago, IL:</location>
<contexts>
<context position="72348" citStr="Pereira (1985)" startWordPosition="13469" endWordPosition="13470">rren (1983), we use structure-sharing only for dotted rules with symbols remaining after the dot. When the dot reaches the end of the right side of a rule, we translate the left side of the rule back to standard representation. This method guarantees that in each resolution only one resolvent is in structure-sharing representation. Instead of general resolution we are doing what the theorem-proving literature calls input resolution. This allows us to represent a substitution as a simple association list, using the function assoc to retrieve the substitutions that have been made for variables. Pereira (1985) describes a more sophisticated version of structure-sharing. This method has two advantages over our version. First, the time to retrieve a substitution is 0(log n), where n is the length of the derivation, compared to 0(n) for Boyer-Moore. Second, only symbols that derive the empty string need to be translated from structure-sharing form to the standard representation, and this saves storage. The first advantage may not be important, for two reasons. By using a single assoc to retrieve a substitution, we reduce the constant factor in 0(n). Also by eliminating the structure sharing each time </context>
</contexts>
<marker>Pereira, 1985</marker>
<rawString>Pereira, Fernando 1985 A Structure-Sharing Representation for Unification-Based Grammar Formalisms. In: Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics. University of Chicago, Chicago, IL: 137-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Stuart Sheiber</author>
</authors>
<title>Prolog and NaturalLanguage Analysis. Center for the Study of Language and Information,</title>
<date>1987</date>
<publisher>Chicago University Press.</publisher>
<location>Stanford, CA.</location>
<note>Distributed by</note>
<marker>Pereira, Sheiber, 1987</marker>
<rawString>Pereira, Fernando and Sheiber, Stuart 1987 Prolog and NaturalLanguage Analysis. Center for the Study of Language and Information, Stanford, CA. Distributed by Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>David H D Warren</author>
</authors>
<title>Definite Clause Grammars for Natural Language Analysis—A Survey of the Formalism and a Comparison with Augmented Transition Networks.</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<volume>13</volume>
<issue>3</issue>
<pages>231--278</pages>
<contexts>
<context position="2339" citStr="Pereira and Warren 1980" startWordPosition="371" endWordPosition="374">rammar first builds the finite set of context-free parses of the input and then eliminates those that don&apos;t meet the other requirements of the grammar. This method guarantees that the parser will halt. This solution may be adequate for lexical functional grammars, but for other unification grammars finding a finitely ambiguous context-free backbone is a problem. In a definite clause grammar, an obvious way to build a context-free backbone is to keep only the topmost function letters in each rule. Thus the rule s —&gt; np(P ,N) vp( P ,IV) becomes S —&gt; np vp (In this example we use the notation of Pereira and Warren 1980, except that we do not put square brackets around terminals, because this conflicts with standard notation for context-free grammars.) Suppose we use a simple X-bar theory. Let major-category (Type, Barlevel) denote a phrase in a major category. A noun phrase may consist of a single noun, for instance, John. This suggests a rule like this: major-category (n,2) —&gt; major-category (n, 1) In the context-free backbone this becomes major-category ---&gt; major-category so the context-free backbone is infinitely ambiguous. One could devise more elaborate examples, but this one suffices to make the poin</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, Fernando and Warren, David H. D. 1980 Definite Clause Grammars for Natural Language Analysis—A Survey of the Formalism and a Comparison with Augmented Transition Networks. Artificial Intelligence 13(3): 231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Robinson</author>
</authors>
<title>A Machine-Oriented Logic Based on the Resolution Principle.</title>
<date>1965</date>
<journal>Journal of the ACM</journal>
<volume>12</volume>
<issue>1</issue>
<pages>23--41</pages>
<contexts>
<context position="10342" citStr="Robinson 1965" startWordPosition="1759" endWordPosition="1760">r 4, December 1989 Andrew Haas A Parsing Algorithm for Unification Grammar singular) and vp(2nd, plural). The reader can verify, using the above algorithm, that every sort includes a ground term. In this case, Ti = {person, number}, T2 = {person, number, phrase}, and T3 = T2. To summarize: we define ranked alphabets in a standard way, adding the requirements that every sort includes a countable infinity of variables, a finite number of function letters, and at least one ground term. We then define the set of terms in a standard way. All unification in this paper is unification of terms, as in Robinson 1965—not graphs or other structures, as in much recent work (Shieber 1985b). A unification grammar is a five-tuple G = (S, (/,r) T, P, Z) where S is a set of sorts, (I,r) an S-ranked alphabet, T a finite set of terminal symbols, and Z a function letter of arity e in (2„r). Z is called the start symbol of the grammar (the standard notation is S not Z, but by bad luck that conflicts with standard notation for the set of sorts). P is a finite set of rules; each rule has the form (A —&gt; a), where A is a term of the ranked alphabet and a is a sequence of terms of the ranked alphabet and symbols from T. </context>
<context position="13467" citStr="Robinson 1965" startWordPosition="2345" endWordPosition="2346">cation grammar is the language generated by its ground grammar. The central idea of this approach is to regard a unification grammar as an abbreviation for its ground grammar. Ground grammars are not always cfgs, but they share many properties of cfgs. Therefore if we regard unification grammars as abbreviations for ground grammars, our understanding of cfgs will help us to understand unification grammars. This is of course inspired by Robinson&apos;s work on resolution, in which he showed how to &amp;quot;lift&amp;quot; a proof procedure for propositional logic up to a proof procedure for general firstorder logic (Robinson 1965). The case of a finite ground grammar is important, since it is adequate for describing many syntactic phenomena. A simple condition will guarantee that the ground grammar is finite. Suppose s1 and s2 are sorts, and there is a function letter of sort si that has an argument of sort s2. Then we say that s &gt; s2. Let &gt;* be the transitive closure of this relation. If &gt;* is irreflexive, and D is the number of sorts, every term of the ground grammar has depth D. To see this, think of a ground term as a labeled tree. A path from the root to a leaf generates a sequence of sorts: the sorts of the varia</context>
<context position="22276" citStr="Robinson (1965)" startWordPosition="3991" endWordPosition="3992">d example: we can find the intersection of two sets of terms by using a symbolic product with fl = (A x . x), f2 = (A x . x), and g = (A x y. x). If X is a set of g-expressions and n an integer, rename(X,n) is an alphabetic variant of X. For all X, Y, m, and n, if m 0 n then rename(X,n) and rename( Y,m) have no variables in common. The following theorem tells us that if we use sets of terms X and Y to represent the sets ground(X) and ground( Y) of ground terms, we can use unification to compute any symbolic product of ground(X) and ground( Y). We assume the basic facts about unification as in Robinson (1965). Theorem 2.1. If h is the symbolic product defined by f1, f2 and g, and X and Y are sets of g-expressions, then h (ground(X),ground(Y)) = ground({s(g(u,v)) u E rename(X,1) A v E rename(Y,2) A s is the m.g.u. of f1(u) and f2(v)}.) Proof. The first step is to show that if Z and W share no variables (1) {g(z,w) z E ground(Z) A w E ground(W) A (z) = f2 (w)} = ground({s(g(u,v)) I uEZAvEWA s is the m.g.u. of f, (u) and f2(v) Consider any element of the right side of equation (1). It must be a ground instance of s(g(u,v)), where u E Z, v E W, and s is the m.g.u. of fl(u) and f2(v). Any ground instan</context>
</contexts>
<marker>Robinson, 1965</marker>
<rawString>Robinson, John A. 1965 A Machine-Oriented Logic Based on the Resolution Principle. Journal of the ACM 12(1): 23-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taisuke Sato</author>
<author>Hisao Tamaki</author>
</authors>
<title>Enumeration of Success Patterns in Logic Programs.</title>
<date>1984</date>
<journal>Theoretical Computer Science</journal>
<volume>34</volume>
<pages>227--240</pages>
<contexts>
<context position="45318" citStr="Sato and Tamaki (1984)" startWordPosition="8594" endWordPosition="8597">y or even useful to predict every possible feature of the next input. It makes sense to predict the presence of traces, but predicting the subcategorization frame of a verb will cost more than it saves. To avoid predicting certain features, we use a weak prediction table; that is, a set of pairs of symbols that properly contains the set of all [A B] such that A B. This weak prediction table is guaranteed to eliminate no more than what the ideal prediction table eliminates. It may leave some dotted rules in dr(i,k) that the ideal prediction table would remove, but it may also cost less to use. Sato and Tamaki (1984) proposed to analyze the behavior of Prolog programs, including parsers, by using something much like a weak prediction table. To guarantee that the table was finite, they restricted the depth of terms occurring in the table. Shieber (1985b) offered a more selective approach—his program predicts only those features chosen by the user as most useful for prediction. Pereira and Shieber (1987) discuss both approaches. We will present a variation of Shieber&apos;s ideas that depends on using a sorted language. To build a weak prediction table we begin with a set Q, of terms such that P, C ground(Q1). D</context>
</contexts>
<marker>Sato, Tamaki, 1984</marker>
<rawString>Sato, Taisuke and Tamaki, Hisao 1984 Enumeration of Success Patterns in Logic Programs. Theoretical Computer Science 34: 227-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>Evidence against the Context-Freeness of Natural Language.</title>
<date>1985</date>
<journal>Linguistics and Philosophy</journal>
<volume>8</volume>
<issue>3</issue>
<pages>333--343</pages>
<contexts>
<context position="1429" citStr="Shieber (1985" startWordPosition="225" endWordPosition="226">in simple conditions. 1 INTRODUCTION Unrestricted unification grammars have the formal power of a Turing machine. Thus there is no algorithm that finds all parses of a given sentence in any unification grammar and always halts. Some unification grammar systems just live with this problem. Any general parsing method for definite clause grammar will enter an infinite loop in some cases, and it is the task of the grammar writer to avoid this. Generalized phrase structure grammar avoids the problem because it has only the formal power of context-free grammar (Gazdar et al. 1985), but according to Shieber (1985a) this is not adequate for describing human language. Lexical functional grammar employs a better solution. A lexical functional grammar must include a finitely ambiguous context-free grammar, which we will call the context-free backbone (Barton 1987). A parser for lexical functional grammar first builds the finite set of context-free parses of the input and then eliminates those that don&apos;t meet the other requirements of the grammar. This method guarantees that the parser will halt. This solution may be adequate for lexical functional grammars, but for other unification grammars finding a fin</context>
<context position="3173" citStr="Shieber 1985" startWordPosition="503" endWordPosition="504">ase in a major category. A noun phrase may consist of a single noun, for instance, John. This suggests a rule like this: major-category (n,2) —&gt; major-category (n, 1) In the context-free backbone this becomes major-category ---&gt; major-category so the context-free backbone is infinitely ambiguous. One could devise more elaborate examples, but this one suffices to make the point: not every natural unification grammar has an obvious context-free backbone. Therefore it is useful to have a parser that does not require us to find a context-free backbone, but works directly on a unification grammar (Shieber 1985b). We propose to guarantee that the parsing problem is solvable by restricting ourselves to depth-bounded grammars. A unification grammar is depth-bounded if for every L &gt; 0 there is a D &gt; 0 such that every parse tree for a sentential form of L symbols has depth less than D. In other words, the depth of a tree is bounded by the length of the string it derives. A context-free grammar is depth-bounded if and only if every string of symbols is finitely ambiguous. We will generalize the notion of finite ambiguity to unification grammars and show that for unification grammars, depth-boundedness is</context>
<context position="5198" citStr="Shieber (1985" startWordPosition="830" endWordPosition="831">tters a&apos; through z&apos; to represent the terminals. The rules of the grammar are as follows, with e denoting the empty string. s —&gt; x(L)x(L) x(cons(A,L)) —&gt; pre-terminal(A) x(L) x(nil) --&gt; e pre-terminal(a&apos;) —&gt; a pre-terminal(z&apos;) z The reasoning behind the grammar should be clear— x(cons(a&apos; ,cons(13&apos; ,ni1))) derives ab, and the first rule guarantees that every sentence has the form xx. The grammar is depth-bounded because the depth of a tree is a linear function of the length of the string it derives. A similar grammar can derive the crossed serial dependencies of Swiss German, which according to Shieber (1985a) no context-free grammar can derive. It is clear where the extra formal power comes from: a contextfree grammar has a finite set of nonterminals, but a unification grammar can build arbitrarily large nonterminal symbols. It remains to show that there is a parsing algorithm for depth-bounded unification grammars. We have developed such an algorithm, based on the context-free parser of Graham et al. 1980, which is a table-driven parser. If we generalize the table-building algorithm to a unification grammar in an obvious way, we get an algorithm that is guaranteed to halt for all depthbounded g</context>
<context position="10411" citStr="Shieber 1985" startWordPosition="1770" endWordPosition="1771">mmar singular) and vp(2nd, plural). The reader can verify, using the above algorithm, that every sort includes a ground term. In this case, Ti = {person, number}, T2 = {person, number, phrase}, and T3 = T2. To summarize: we define ranked alphabets in a standard way, adding the requirements that every sort includes a countable infinity of variables, a finite number of function letters, and at least one ground term. We then define the set of terms in a standard way. All unification in this paper is unification of terms, as in Robinson 1965—not graphs or other structures, as in much recent work (Shieber 1985b). A unification grammar is a five-tuple G = (S, (/,r) T, P, Z) where S is a set of sorts, (I,r) an S-ranked alphabet, T a finite set of terminal symbols, and Z a function letter of arity e in (2„r). Z is called the start symbol of the grammar (the standard notation is S not Z, but by bad luck that conflicts with standard notation for the set of sorts). P is a finite set of rules; each rule has the form (A —&gt; a), where A is a term of the ranked alphabet and a is a sequence of terms of the ranked alphabet and symbols from T. We define substitution and substitution instances of terms in the sta</context>
<context position="45557" citStr="Shieber (1985" startWordPosition="8634" endWordPosition="8635">a weak prediction table; that is, a set of pairs of symbols that properly contains the set of all [A B] such that A B. This weak prediction table is guaranteed to eliminate no more than what the ideal prediction table eliminates. It may leave some dotted rules in dr(i,k) that the ideal prediction table would remove, but it may also cost less to use. Sato and Tamaki (1984) proposed to analyze the behavior of Prolog programs, including parsers, by using something much like a weak prediction table. To guarantee that the table was finite, they restricted the depth of terms occurring in the table. Shieber (1985b) offered a more selective approach—his program predicts only those features chosen by the user as most useful for prediction. Pereira and Shieber (1987) discuss both approaches. We will present a variation of Shieber&apos;s ideas that depends on using a sorted language. To build a weak prediction table we begin with a set Q, of terms such that P, C ground(Q1). Define LP(Q,Q&apos;) = {(s [x z]) I (3 y,y&apos;. [x y] E Q A [y&apos; z] E Q&apos; A s = m.g.u. of y and y&apos;)} By Theorem 2.1, ground(LP(Q,Q&apos;)) = Link(ground(Q), ground(Q&apos;)). Let Q, equal Q, U LP (Q1,Q1). Then by lemma 2.3 and induction, Pi c ground(. Q,) That</context>
<context position="61635" citStr="Shieber (1985" startWordPosition="11719" endWordPosition="11720"> the weak prediction table.0 7 DISCUSSION AND IMPLEMENTATION NOTES 7.1 RELATED WORK AND POSSIBLE EXTENSIONS The chief contribution of the present paper is to define a class of grammars on which bottom-up parsers always halt, and to give a semi-decision procedure for this class. This in turn makes it possible to prove a completeness theorem, which is impossible if one considers arbitrary unification grammars. One can obtain similar results for the class of grammars whose context-free backbone is finitely ambiguous—what Pereira and Warren (1983) called the offline-parsable grammars. However, as Shieber (1985b) observed, this class of grammars excludes many linguistically interesting grammars that do not use atomic category symbols. The present parser (as opposed to the table-building algorithm) is much like those in the literature. Like nearly all parsers using term unification, it is a special case of Earley deduction (Pereira and Warren 1985). The tables are simply collections of theorems proved in advance and added to the program component of Earley deduction. Earley deduction is a framework for parsing rather than a parser. Among implemented parsers, BUP (Matsumota et al. 1983) is particularl</context>
<context position="69627" citStr="Shieber 1985" startWordPosition="13026" endWordPosition="13027">5 323 247 8 982 950 640 507 9 1519 1503 1007 711 10 930 920 495 400 11 2034 2014 1128 771 total time 917 2201 1538 1085 (in seconds) Table 1 presents the results of predicting different features on a sample of 11 sentences. It describes parsing without prediction, with prediction of categories only, with traces and categories, and finally with categories, traces, and verb form information. In each case it lists the total number of entries in the matrices &amp;quot;rules&amp;quot; and &amp;quot;symbols&amp;quot; for every sentence, and the total time to parse the 11 sentences. The reader should compare this table with the one in Shieber 1985. Shieber tried predicting subcategorization information along with categories. In our grammar there is a separate VP rule for each subcategorization frame, and this rule gives the categories of all arguments of the verb. Shieber eliminated these multiple VP rules by making the list of arguments a feature of the verb. Therefore by predicting categories alone, we get the same information that Shieber got by predicting subcategorization information. The table shows that for our grammar, prediction reduces the chart size drastically, but it is so costly that a straight bottom-up parser runs faste</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Shieber, Stuart 1985 Evidence against the Context-Freeness of Natural Language. Linguistics and Philosophy 8(3): 333-343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
</authors>
<title>Using Restriction to Extend Parsing Algorithms for Complex-Feature-Based Formalisms. In:</title>
<date>1985</date>
<booktitle>Proceedings of the 23rd Annual Meeting of the Association</booktitle>
<pages>145--152</pages>
<institution>for Computational Linguistics. University of Chicago,</institution>
<location>Chicago, IL:</location>
<contexts>
<context position="1429" citStr="Shieber (1985" startWordPosition="225" endWordPosition="226">in simple conditions. 1 INTRODUCTION Unrestricted unification grammars have the formal power of a Turing machine. Thus there is no algorithm that finds all parses of a given sentence in any unification grammar and always halts. Some unification grammar systems just live with this problem. Any general parsing method for definite clause grammar will enter an infinite loop in some cases, and it is the task of the grammar writer to avoid this. Generalized phrase structure grammar avoids the problem because it has only the formal power of context-free grammar (Gazdar et al. 1985), but according to Shieber (1985a) this is not adequate for describing human language. Lexical functional grammar employs a better solution. A lexical functional grammar must include a finitely ambiguous context-free grammar, which we will call the context-free backbone (Barton 1987). A parser for lexical functional grammar first builds the finite set of context-free parses of the input and then eliminates those that don&apos;t meet the other requirements of the grammar. This method guarantees that the parser will halt. This solution may be adequate for lexical functional grammars, but for other unification grammars finding a fin</context>
<context position="3173" citStr="Shieber 1985" startWordPosition="503" endWordPosition="504">ase in a major category. A noun phrase may consist of a single noun, for instance, John. This suggests a rule like this: major-category (n,2) —&gt; major-category (n, 1) In the context-free backbone this becomes major-category ---&gt; major-category so the context-free backbone is infinitely ambiguous. One could devise more elaborate examples, but this one suffices to make the point: not every natural unification grammar has an obvious context-free backbone. Therefore it is useful to have a parser that does not require us to find a context-free backbone, but works directly on a unification grammar (Shieber 1985b). We propose to guarantee that the parsing problem is solvable by restricting ourselves to depth-bounded grammars. A unification grammar is depth-bounded if for every L &gt; 0 there is a D &gt; 0 such that every parse tree for a sentential form of L symbols has depth less than D. In other words, the depth of a tree is bounded by the length of the string it derives. A context-free grammar is depth-bounded if and only if every string of symbols is finitely ambiguous. We will generalize the notion of finite ambiguity to unification grammars and show that for unification grammars, depth-boundedness is</context>
<context position="5198" citStr="Shieber (1985" startWordPosition="830" endWordPosition="831">tters a&apos; through z&apos; to represent the terminals. The rules of the grammar are as follows, with e denoting the empty string. s —&gt; x(L)x(L) x(cons(A,L)) —&gt; pre-terminal(A) x(L) x(nil) --&gt; e pre-terminal(a&apos;) —&gt; a pre-terminal(z&apos;) z The reasoning behind the grammar should be clear— x(cons(a&apos; ,cons(13&apos; ,ni1))) derives ab, and the first rule guarantees that every sentence has the form xx. The grammar is depth-bounded because the depth of a tree is a linear function of the length of the string it derives. A similar grammar can derive the crossed serial dependencies of Swiss German, which according to Shieber (1985a) no context-free grammar can derive. It is clear where the extra formal power comes from: a contextfree grammar has a finite set of nonterminals, but a unification grammar can build arbitrarily large nonterminal symbols. It remains to show that there is a parsing algorithm for depth-bounded unification grammars. We have developed such an algorithm, based on the context-free parser of Graham et al. 1980, which is a table-driven parser. If we generalize the table-building algorithm to a unification grammar in an obvious way, we get an algorithm that is guaranteed to halt for all depthbounded g</context>
<context position="10411" citStr="Shieber 1985" startWordPosition="1770" endWordPosition="1771">mmar singular) and vp(2nd, plural). The reader can verify, using the above algorithm, that every sort includes a ground term. In this case, Ti = {person, number}, T2 = {person, number, phrase}, and T3 = T2. To summarize: we define ranked alphabets in a standard way, adding the requirements that every sort includes a countable infinity of variables, a finite number of function letters, and at least one ground term. We then define the set of terms in a standard way. All unification in this paper is unification of terms, as in Robinson 1965—not graphs or other structures, as in much recent work (Shieber 1985b). A unification grammar is a five-tuple G = (S, (/,r) T, P, Z) where S is a set of sorts, (I,r) an S-ranked alphabet, T a finite set of terminal symbols, and Z a function letter of arity e in (2„r). Z is called the start symbol of the grammar (the standard notation is S not Z, but by bad luck that conflicts with standard notation for the set of sorts). P is a finite set of rules; each rule has the form (A —&gt; a), where A is a term of the ranked alphabet and a is a sequence of terms of the ranked alphabet and symbols from T. We define substitution and substitution instances of terms in the sta</context>
<context position="45557" citStr="Shieber (1985" startWordPosition="8634" endWordPosition="8635">a weak prediction table; that is, a set of pairs of symbols that properly contains the set of all [A B] such that A B. This weak prediction table is guaranteed to eliminate no more than what the ideal prediction table eliminates. It may leave some dotted rules in dr(i,k) that the ideal prediction table would remove, but it may also cost less to use. Sato and Tamaki (1984) proposed to analyze the behavior of Prolog programs, including parsers, by using something much like a weak prediction table. To guarantee that the table was finite, they restricted the depth of terms occurring in the table. Shieber (1985b) offered a more selective approach—his program predicts only those features chosen by the user as most useful for prediction. Pereira and Shieber (1987) discuss both approaches. We will present a variation of Shieber&apos;s ideas that depends on using a sorted language. To build a weak prediction table we begin with a set Q, of terms such that P, C ground(Q1). Define LP(Q,Q&apos;) = {(s [x z]) I (3 y,y&apos;. [x y] E Q A [y&apos; z] E Q&apos; A s = m.g.u. of y and y&apos;)} By Theorem 2.1, ground(LP(Q,Q&apos;)) = Link(ground(Q), ground(Q&apos;)). Let Q, equal Q, U LP (Q1,Q1). Then by lemma 2.3 and induction, Pi c ground(. Q,) That</context>
<context position="61635" citStr="Shieber (1985" startWordPosition="11719" endWordPosition="11720"> the weak prediction table.0 7 DISCUSSION AND IMPLEMENTATION NOTES 7.1 RELATED WORK AND POSSIBLE EXTENSIONS The chief contribution of the present paper is to define a class of grammars on which bottom-up parsers always halt, and to give a semi-decision procedure for this class. This in turn makes it possible to prove a completeness theorem, which is impossible if one considers arbitrary unification grammars. One can obtain similar results for the class of grammars whose context-free backbone is finitely ambiguous—what Pereira and Warren (1983) called the offline-parsable grammars. However, as Shieber (1985b) observed, this class of grammars excludes many linguistically interesting grammars that do not use atomic category symbols. The present parser (as opposed to the table-building algorithm) is much like those in the literature. Like nearly all parsers using term unification, it is a special case of Earley deduction (Pereira and Warren 1985). The tables are simply collections of theorems proved in advance and added to the program component of Earley deduction. Earley deduction is a framework for parsing rather than a parser. Among implemented parsers, BUP (Matsumota et al. 1983) is particularl</context>
<context position="69627" citStr="Shieber 1985" startWordPosition="13026" endWordPosition="13027">5 323 247 8 982 950 640 507 9 1519 1503 1007 711 10 930 920 495 400 11 2034 2014 1128 771 total time 917 2201 1538 1085 (in seconds) Table 1 presents the results of predicting different features on a sample of 11 sentences. It describes parsing without prediction, with prediction of categories only, with traces and categories, and finally with categories, traces, and verb form information. In each case it lists the total number of entries in the matrices &amp;quot;rules&amp;quot; and &amp;quot;symbols&amp;quot; for every sentence, and the total time to parse the 11 sentences. The reader should compare this table with the one in Shieber 1985. Shieber tried predicting subcategorization information along with categories. In our grammar there is a separate VP rule for each subcategorization frame, and this rule gives the categories of all arguments of the verb. Shieber eliminated these multiple VP rules by making the list of arguments a feature of the verb. Therefore by predicting categories alone, we get the same information that Shieber got by predicting subcategorization information. The table shows that for our grammar, prediction reduces the chart size drastically, but it is so costly that a straight bottom-up parser runs faste</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Shieber, Stuart 1985 Using Restriction to Extend Parsing Algorithms for Complex-Feature-Based Formalisms. In: Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics. University of Chicago, Chicago, IL: 145-152.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>