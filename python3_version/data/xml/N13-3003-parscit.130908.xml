<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.055572">
<title confidence="0.983478">
TMTprime: A Recommender System for MT and TM Integration
</title>
<author confidence="0.993255">
Aswarth Dara†, Sandipan Dandapat‡∗, Declan Groves† and Josef van Genabith†
</author>
<affiliation confidence="0.994971">
† Centre for Next Generation Localisation, School of Computing
Dublin City University, Dublin, Ireland
‡ Department of Computer Science and Engineering
IIT-Guwahati, Assam, India
</affiliation>
<email confidence="0.985983">
{adara, dgroves, josef}@computing.dcu.ie,sdandapat@iitg.ernet.in
</email>
<sectionHeader confidence="0.993813" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999776047619048">
TMTprime is a recommender system that fa-
cilitates the effective use of both transla-
tion memory (TM) and machine translation
(MT) technology within industrial language
service providers (LSPs) localization work-
flows. LSPs have long used Translation Mem-
ory (TM) technology to assist the translation
process. Recent research shows how MT sys-
tems can be combined with TMs in Computer
Aided Translation (CAT) systems, selecting
either TM or MT output based on sophis-
ticated translation quality estimation without
access to a reference. However, to date there
are no commercially available frameworks for
this. TMTprime takes confidence estimation
out of the lab and provides a commercially vi-
able platform that allows for the seamless inte-
gration of MT with legacy TM systems to pro-
vide the most effective (least effort/cost) trans-
lation options to human translators, based on
the TMTprime confidence score.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.953425285714286">
Within the LSP community there is growing interest
in the use of MT as a means to increase automation
and reduce overall localisation project cost. When
high-quality MT output is available, translators see
significant productivity gains over translation from
scratch, but poor MT quality leads to frustration
and wasted time as suggested translations are dis-
carded in favour of providing a translation from
scratch. We present a commercially-relevant soft-
ware platform providing a translation confidence es-
timation metric and, based on this, a mechanism for
effectively integrating MT with TMs in localisation
workflows. The confidence metric ensures that only
∗Author did this work during his post doctoral research at
CNGL.
those MT outputs that are guaranteed to require less
post-editing effort than the best corresponding TM
match are presented to the post-editor (He et al.,
2010a). The MT is integrated seamlessly, and es-
tablished localisation cost estimation models based
on TM technologies still apply as upper bounds.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999876481481482">
MT confidence estimation and its relation to existing
TM scoring methods, together with how to make the
most effective use of both technologies, is an active
area of research.
(Specia, 2011) and (Specia et al., 2009, 2010) pro-
pose a confidence estimator that relates specifically
to the post-editing effort of translators. This re-
search uses regression on both the automatic scores
assigned to the MT and scores assigned by post-
editors and aims to model post-editors’ judgements
of the translation quality between good and bad, or
among three levels of post-editing effort.
Our work is an extension of (He et al., 2010a,b,c),
and uses outputs and features relevant to the TM
and MT systems. We focus on using system exter-
nal features. This is important for cases where the
internals of the MT system are not available, as in
the use of MT as a service in a localisation work-
flow.1 Furthermore, instead of having to solve a
regression problem, our approach is based on solv-
ing an easier binary prediction problem (using Sup-
port Vector Machines) and can be easily integrated
into TMs. (He et al., 2010b) present a MT/TM seg-
ment recommender, (He et al., 2010c) a MT/TM n-
best list segment re-ranker and (He et al., 2010a) a
MT/TM integration method that can use matching
sub-segments in MT/TM combination. Importantly,
</bodyText>
<footnote confidence="0.976038">
1(Specia et al., 2009) note that using glass-box features
when available, in addition to black-box features, offer only
small gains and also incur significant computational effort.
</footnote>
<page confidence="0.980794">
10
</page>
<bodyText confidence="0.926560866666667">
Proceedings of the NAACL HLT 2013 Demonstration Session, pages 10–13,
Atlanta, Georgia, 10-12 June 2013. c�2013 Association for Computational Linguistics
translators can tune the models for precision without
retraining the models.
Related research by (Simard and Isabelle., 2009)
focuses on combining TM information into an SMT
system for improving the performance of the MT
when a close match already exists within the TM.
(Koehn and Haddow, 2009) presents a post-editing
environment using information from the phrase-
based SMT system Moses.2 (Guerberof, 2009) com-
pares the post-editing effort required for TM and
MT output, respectively. (Tatsumi, 2009) studies the
correlation between automatic evaluation scores and
post-editing effort.
</bodyText>
<sectionHeader confidence="0.821532" genericHeader="method">
3 Translation Recommender
</sectionHeader>
<figureCaption confidence="0.999493">
Figure 1: TMTprime Workflow
</figureCaption>
<bodyText confidence="0.999969">
The workflow of the translation recommender is
shown in Figure 1. We train MT systems using a
significant portion of the training data and use these
models as well as TM outputs to obtain a recommen-
dation development data set. MT systems can be
either in-house, e.g. a Moses-based system, or ex-
ternally available systems, such as Microsoft Bing3
or Google Translate.4 For each sentence in the de-
velopment data set, we have access to the reference
as well as to the outputs for each of the MT and TM
systems. We then select the best MT (or TM) output
as the translation with the lowest TER score with
respect to the reference and label the data accord-
ingly. System-independent features for each trans-
lation output are fed as input to the SVM classi-
fier (Cortes and Vapnik, 1995). The SVM classi-
fier outputs class labels and the class labels are con-
verted into confidence scores using the techniques
given in (Lin et al., 2007). Relying on system inde-
pendent black-box features has allowed us to build
</bodyText>
<footnote confidence="0.999871666666667">
2http://www.statmt.org/moses/
3http://www.bing.com/translator
4http://translate.google.com/
</footnote>
<bodyText confidence="0.994865333333333">
a fully extendable platform that will allow any num-
ber of MT systems (or indeed TM systems) to be
plugged into the recommender with little effort.
</bodyText>
<sectionHeader confidence="0.986606" genericHeader="method">
4 Demo Description
</sectionHeader>
<bodyText confidence="0.999906076923077">
Using the Amazon EC25 deployment as a back-end,
we have developed a front-end GUI for the system
(Figure 2). The interface allows the user to select
which of the available translation systems (whether
they be MT or TM) they wish to use within the rec-
ommender system. The user can input their own
pre-established estimated cost of post-editing, based
on error ranges. Typically the costs for post-editing
those translations which have a lower-error rate (i.e.
fewer errors) is less than the cost for post-editing
translations which have a greater number of errors,
as they are of lower quality. The user is requested to
upload a file for translation to the system.
</bodyText>
<figureCaption confidence="0.998356">
Figure 2: TMTprime GUI
</figureCaption>
<bodyText confidence="0.999968176470588">
Once the user has selected their desired options,
the TMTprime platform provides various analysis
measures based on its recommendation engine, such
as how many segments from the input file are recom-
mended for translation by the various selected trans-
lation engines or TMs available. Based on the input
costs, it provides a visualisation of overall estimated
cost of either using an individual translation system
on its own, or using the recommender selecting the
best performing system on a segment-by-segment
basis. The TMTprime system is an implementa-
tion of a segment-based system selector selecting
the most appropriate available translation/TM sys-
tem for a given input. A snapshot of the results pro-
duced by TMTprime is given in Figure 3: the pie-
chart shows what percentage of segments are rec-
ommended from each of the translation systems; the
</bodyText>
<footnote confidence="0.981886">
5http://aws.amazon.com/ec2/
</footnote>
<page confidence="0.998859">
11
</page>
<bodyText confidence="0.999475941176471">
bar-graph gives an estimated cost of using a single
translation system alone and the estimated cost when
using TMTprime’s combined recommendation. The
estimated cost using TMTprime is lower when com-
pared to using a single MT or TM system alone
(in the worst case, it will be the same as the best-
performing single translation engine or TM system).
This estimated cost includes both the cost for trans-
lation (currently uniform cost for each translation
system) and the cost required for post-editing. For
example, if the MT is an in-house system the cost
of translation will be (close to) zero whereas there is
potentially an additional base cost for using an exter-
nal MT engine. Finally, the interface provides statis-
tics related to various confidence levels for different
translation outputs across the various translation and
TM systems.
</bodyText>
<figureCaption confidence="0.988324">
Figure 3: Results shown by TMTprime system
</figureCaption>
<sectionHeader confidence="0.993377" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.998774">
Evaluation targets two objectives and is described
below.
</bodyText>
<subsectionHeader confidence="0.999617">
5.1 Correlation with Automatic Metrics
</subsectionHeader>
<bodyText confidence="0.999964529411765">
TER and METEOR are widely-used automatic met-
rics (Snover et al., 2006; Denkowski and Lavie,
2011) that calculate the quality of translation out-
put by comparing it against a human translation,
known as the reference translation. Our data sets
for the experiment consist of English-French trans-
lation memories from the IT domain. In all instances
MT was carried out for English-French translations.
As we have access to the reference target language
translations for our test set, we are able to calculate
the TER and METEOR scores for the three trans-
lation outputs (here TM, MaTrEx (Dandapat et al.,
2010) and Microsoft Bing). For each sentence in the
test set, TMTprime recommends a particular transla-
tion output with a certain estimated confidence level
without access to a reference. We measure Pearson’s
correlation coefficient (Hollander and Wolfe, 1999)
between the recommendation scores, TER scores
and METEOR scores (for all system outputs) in or-
der to determine how well the TMTprime prediction
score correlates with the widely used automatic eval-
uation metrics. Results of these experiments are pro-
vided in Table 1 which shows there is a negative cor-
relation between TMTprime scores and TER scores.
This shows that both TMTprime scores and TER
scores are moving in opposite directions, supporting
the claim that the higher the recommendation scores,
the lower the TER scores. As TER is an error score,
the lower the TER score, the higher the quality of
the machine translation output compared to its refer-
ence. On the other hand, TMTprime scores are pos-
itively correlated with METEOR scores which sup-
ports the claim that the higher the recommendation
scores, the higher the METEOR scores.
</bodyText>
<table confidence="0.9363755">
Pearson’s r TER METEOR
TMTprime -0.402 0.447
</table>
<tableCaption confidence="0.999945">
Table 1: Correlation with automatic metrics
</tableCaption>
<bodyText confidence="0.999223666666667">
The evaluation has been performed on a test data
set of 2,500 sentences. Both the correlations are sig-
nificant at the (p&lt;0.01) level.
</bodyText>
<subsectionHeader confidence="0.999952">
5.2 Correlation with Post-Editing time
</subsectionHeader>
<bodyText confidence="0.998791">
This is the most important and crucial metric for the
evaluation. For this experiment we made use of post-
editing data captured during a real-world translation
task, for English-French in the IT domain.
</bodyText>
<table confidence="0.5630395">
Pearson’s r TER METEOR PE Time
TMTprime -0.122 0.129 -0.132
</table>
<tableCaption confidence="0.998595">
Table 2: Correlation with Post-Editing times
</tableCaption>
<bodyText confidence="0.9994655">
For testing, we collect the post-editing times for
MT outputs from two different translators using a
commercial computer-aided translation (CAT tool)
in a real-world production scenario. The data set
consists of 1113 samples and is different from the
one used in the correlation with automatic metrics.
</bodyText>
<page confidence="0.997103">
12
</page>
<bodyText confidence="0.999957105263158">
Post-editing times provide a real measure of the
amount of post-editing effort required to perfect the
output of the MT system. For this experiment, we
took the output of the MT system used in the task to-
gether with the post-editing times and measured the
Pearsons correlation coefficient between the TMT-
prime recommendation scores and the post-editing
(PE) times (only for MT output from a single sys-
tem since this data set does contain PE times for
other translation outputs). In addition, we also re-
peated the previous experiment setup for finding the
correlation between the TMTprime scores and the
automatically-produced TER, METEOR scores for
this data set. The results are given in Table 2.
The results show that the confidence scores do
correlate with automatic evaluation metrics and
post-editing times. Although the correlations do not
seem as strong as before, the results are statistically
significant (p&lt;0.01).
</bodyText>
<sectionHeader confidence="0.994928" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999975">
We present a commercially viable translation recom-
mender system which selects the best output from
multiple TM/MT outputs. We have shown that our
confidence score correlates with automatic metrics
and post-editing times. For future work, we are
looking into extending and evaluating the system for
different language pairs and data sets.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998728">
This work is supported by Science Foundation Ire-
land (Grants SFI11-TIDA-B2040 and 07/CE/I1142) as
part of the Centre for Next Generation Localisation
(www.cngl.ie) at Dublin City University. We would also
like to thank Symantec, Autodesk and Welocalize for
their support and provision of data sets used in our ex-
periments.
</bodyText>
<sectionHeader confidence="0.998612" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99988561971831">
Cortes, Corinna and Vladimir Vapnik. 1995. Support-vector
networks. In Machine Learning. pages 273–297.
Dandapat, Sandipan, Mikel L. Forcada, Declan Groves, Ser-
gio Penkale, John Tinsley, and Andy Way. 2010. OpenMa-
TrEx: A free/open-source marker-driven example-based ma-
chine translation system. In Proceedings of the 7th interna-
tional conference on Advances in natural language process-
ing. Springer-Verlag, Berlin, Heidelberg, IceTAL’10, pages
121–126.
Denkowski, Michael and Alon Lavie. 2011. Meteor 1.3: Auto-
matic metric for reliable optimization and evaluation of ma-
chine translation systems. In Proceedings of the EMNLP
2011 Workshop on Statistical Machine Translation. Edin-
burgh, UK.
Guerberof, Ana. 2009. Productivity and quality in mt post-
editing. In Proceedings of Machine Translation Summit XII
- Workshop: Beyond Translation Memories: New Tools for
Translators. Ottawa, Canada.
He, Yifan, Yanjun Ma, J Roturier, Andy Way, and Josef van
Genabith. 2010a. Improving the post-editing experience us-
ing translation recommendation: A user study. In Proceed-
ings of the Ninth Conference of the Association for Ma-
chine Translation in the Americas. Denver, Colorado, AMTA
2010, pages 247–256.
He, Yifan, Yanjun Ma, Josef van Genabith, and Andy Way.
2010b. Bridging smt and tm with translation recommenda-
tion. In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics. Association for Com-
putational Linguistics, Uppsala, Sweden, ACL 2010, pages
622–630.
He, Yifan, Yanjun Ma, Andy Way, and Josef van Genabith.
2010c. Integrating n-best smt outputs into a tm system. In
Proceedings of the 23rd International Conference on Com-
putational Linguistics: Posters. Association for Computa-
tional Linguistics, Beijing, China, COLING 2010, pages
374–382.
Hollander, Myles and Douglas A. Wolfe. 1999. Nonparametric
Statistical Methods. John Wiley and Sons.
Koehn, Philip and Barry Haddow. 2009. Interactive assis-
tance to human translators using statistical machine trans-
lation methods. In Proceedings of MT Summit XII. Ottawa,
Canada, pages 73–80.
Lin, Hsuan-Tien, Chih-Jen Lin, and Ruby C. Weng. 2007. A
note on platt’s probabilistic outputs for support vector ma-
chines. Machine Learning 68(3):267–276.
Simard, Michael and Pierre Isabelle. 2009. Phrase-based ma-
chine translation in a computer-assisted translation environ-
ment. In Proceedings of Machine Translation Summit XII.
Ottawa, Canada, pages 120–127.
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Linnea Mic-
ciulla, and John Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In Proceedings ofAsso-
ciation for Machine Translation in the Americas. Cambridge,
MA, pages 223–231.
Specia, Lucia. 2011. Exploiting objective annotations for mea-
suring translation post-editing effort. In Proceedings of the
15th Annual Conference of the European Association for
Machine Translation. Leuven, Belgium, EAMT 2011, pages
73–80.
Specia, Lucia, Nicola Cancedda, and Marc Dymetman. 2010. A
dataset for assessing machine translation evaluation metrics.
In Proceedings of LREC 2010. Valletta, Malta.
Specia, Lucia, Marco Turqui, Zhuoran Wang, John Shawe-
Taylor, and Craig Saunders. 2009. Improving the confidence
of machine translation quality estimates. In Proceedings
of Machine Translation Summit XII. Ottawa, Canada, pages
136–143.
Tatsumi, Midori. 2009. Correlation between automatic evalua-
tion scores, post-editing speed and some other factors. In
Proceedings of Machine Translation Summit XII. Ottawa,
Canada, pages 332–339.
</reference>
<page confidence="0.999469">
13
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.431842">
<title confidence="0.999854">TMTprime: A Recommender System for MT and TM Integration</title>
<author confidence="0.739097">Sandipan Declan Josef van_for Next Generation Localisation</author>
<author confidence="0.739097">School of</author>
<affiliation confidence="0.9865685">Dublin City University, Dublin, of Computer Science and</affiliation>
<address confidence="0.942982">IIT-Guwahati, Assam,</address>
<email confidence="0.995859">dgroves,</email>
<abstract confidence="0.998998136363636">TMTprime is a recommender system that facilitates the effective use of both translation memory (TM) and machine translation (MT) technology within industrial language service providers (LSPs) localization workflows. LSPs have long used Translation Memory (TM) technology to assist the translation process. Recent research shows how MT systems can be combined with TMs in Computer Aided Translation (CAT) systems, selecting either TM or MT output based on sophisticated translation quality estimation without access to a reference. However, to date there are no commercially available frameworks for this. TMTprime takes confidence estimation out of the lab and provides a commercially viable platform that allows for the seamless integration of MT with legacy TM systems to provide the most effective (least effort/cost) translation options to human translators, based on the TMTprime confidence score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>In Machine Learning.</booktitle>
<pages>273--297</pages>
<contexts>
<context position="5416" citStr="Cortes and Vapnik, 1995" startWordPosition="847" endWordPosition="850">ls as well as TM outputs to obtain a recommendation development data set. MT systems can be either in-house, e.g. a Moses-based system, or externally available systems, such as Microsoft Bing3 or Google Translate.4 For each sentence in the development data set, we have access to the reference as well as to the outputs for each of the MT and TM systems. We then select the best MT (or TM) output as the translation with the lowest TER score with respect to the reference and label the data accordingly. System-independent features for each translation output are fed as input to the SVM classifier (Cortes and Vapnik, 1995). The SVM classifier outputs class labels and the class labels are converted into confidence scores using the techniques given in (Lin et al., 2007). Relying on system independent black-box features has allowed us to build 2http://www.statmt.org/moses/ 3http://www.bing.com/translator 4http://translate.google.com/ a fully extendable platform that will allow any number of MT systems (or indeed TM systems) to be plugged into the recommender with little effort. 4 Demo Description Using the Amazon EC25 deployment as a back-end, we have developed a front-end GUI for the system (Figure 2). The interf</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Cortes, Corinna and Vladimir Vapnik. 1995. Support-vector networks. In Machine Learning. pages 273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandipan Dandapat</author>
<author>Mikel L Forcada</author>
<author>Declan Groves</author>
<author>Sergio Penkale</author>
<author>John Tinsley</author>
<author>Andy Way</author>
</authors>
<title>OpenMaTrEx: A free/open-source marker-driven example-based machine translation system.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th international conference on Advances in natural language processing.</booktitle>
<pages>121--126</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Heidelberg, IceTAL’10,</location>
<contexts>
<context position="9073" citStr="Dandapat et al., 2010" startWordPosition="1430" endWordPosition="1433">Automatic Metrics TER and METEOR are widely-used automatic metrics (Snover et al., 2006; Denkowski and Lavie, 2011) that calculate the quality of translation output by comparing it against a human translation, known as the reference translation. Our data sets for the experiment consist of English-French translation memories from the IT domain. In all instances MT was carried out for English-French translations. As we have access to the reference target language translations for our test set, we are able to calculate the TER and METEOR scores for the three translation outputs (here TM, MaTrEx (Dandapat et al., 2010) and Microsoft Bing). For each sentence in the test set, TMTprime recommends a particular translation output with a certain estimated confidence level without access to a reference. We measure Pearson’s correlation coefficient (Hollander and Wolfe, 1999) between the recommendation scores, TER scores and METEOR scores (for all system outputs) in order to determine how well the TMTprime prediction score correlates with the widely used automatic evaluation metrics. Results of these experiments are provided in Table 1 which shows there is a negative correlation between TMTprime scores and TER scor</context>
</contexts>
<marker>Dandapat, Forcada, Groves, Penkale, Tinsley, Way, 2010</marker>
<rawString>Dandapat, Sandipan, Mikel L. Forcada, Declan Groves, Sergio Penkale, John Tinsley, and Andy Way. 2010. OpenMaTrEx: A free/open-source marker-driven example-based machine translation system. In Proceedings of the 7th international conference on Advances in natural language processing. Springer-Verlag, Berlin, Heidelberg, IceTAL’10, pages 121–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="8566" citStr="Denkowski and Lavie, 2011" startWordPosition="1347" endWordPosition="1350">ired for post-editing. For example, if the MT is an in-house system the cost of translation will be (close to) zero whereas there is potentially an additional base cost for using an external MT engine. Finally, the interface provides statistics related to various confidence levels for different translation outputs across the various translation and TM systems. Figure 3: Results shown by TMTprime system 5 Experiments and Results Evaluation targets two objectives and is described below. 5.1 Correlation with Automatic Metrics TER and METEOR are widely-used automatic metrics (Snover et al., 2006; Denkowski and Lavie, 2011) that calculate the quality of translation output by comparing it against a human translation, known as the reference translation. Our data sets for the experiment consist of English-French translation memories from the IT domain. In all instances MT was carried out for English-French translations. As we have access to the reference target language translations for our test set, we are able to calculate the TER and METEOR scores for the three translation outputs (here TM, MaTrEx (Dandapat et al., 2010) and Microsoft Bing). For each sentence in the test set, TMTprime recommends a particular tra</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Denkowski, Michael and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation. Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana Guerberof</author>
</authors>
<title>Productivity and quality in mt postediting.</title>
<date>2009</date>
<booktitle>In Proceedings of Machine Translation Summit XII - Workshop: Beyond</booktitle>
<location>Ottawa, Canada.</location>
<contexts>
<context position="4405" citStr="Guerberof, 2009" startWordPosition="679" endWordPosition="680">ins and also incur significant computational effort. 10 Proceedings of the NAACL HLT 2013 Demonstration Session, pages 10–13, Atlanta, Georgia, 10-12 June 2013. c�2013 Association for Computational Linguistics translators can tune the models for precision without retraining the models. Related research by (Simard and Isabelle., 2009) focuses on combining TM information into an SMT system for improving the performance of the MT when a close match already exists within the TM. (Koehn and Haddow, 2009) presents a post-editing environment using information from the phrasebased SMT system Moses.2 (Guerberof, 2009) compares the post-editing effort required for TM and MT output, respectively. (Tatsumi, 2009) studies the correlation between automatic evaluation scores and post-editing effort. 3 Translation Recommender Figure 1: TMTprime Workflow The workflow of the translation recommender is shown in Figure 1. We train MT systems using a significant portion of the training data and use these models as well as TM outputs to obtain a recommendation development data set. MT systems can be either in-house, e.g. a Moses-based system, or externally available systems, such as Microsoft Bing3 or Google Translate.</context>
</contexts>
<marker>Guerberof, 2009</marker>
<rawString>Guerberof, Ana. 2009. Productivity and quality in mt postediting. In Proceedings of Machine Translation Summit XII - Workshop: Beyond Translation Memories: New Tools for Translators. Ottawa, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yifan He</author>
<author>Yanjun Ma</author>
<author>J Roturier</author>
<author>Andy Way</author>
<author>Josef van Genabith</author>
</authors>
<title>Improving the post-editing experience using translation recommendation: A user study.</title>
<date>2010</date>
<booktitle>In Proceedings of the Ninth Conference of the Association for Machine Translation in the Americas.</booktitle>
<pages>247--256</pages>
<location>Denver, Colorado, AMTA</location>
<marker>He, Ma, Roturier, Way, van Genabith, 2010</marker>
<rawString>He, Yifan, Yanjun Ma, J Roturier, Andy Way, and Josef van Genabith. 2010a. Improving the post-editing experience using translation recommendation: A user study. In Proceedings of the Ninth Conference of the Association for Machine Translation in the Americas. Denver, Colorado, AMTA 2010, pages 247–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yifan He</author>
<author>Yanjun Ma</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Bridging smt and tm with translation recommendation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>622--630</pages>
<location>Uppsala, Sweden, ACL</location>
<marker>He, Ma, van Genabith, Way, 2010</marker>
<rawString>He, Yifan, Yanjun Ma, Josef van Genabith, and Andy Way. 2010b. Bridging smt and tm with translation recommendation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Uppsala, Sweden, ACL 2010, pages 622–630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yifan He</author>
<author>Yanjun Ma</author>
<author>Andy Way</author>
<author>Josef van Genabith</author>
</authors>
<title>Integrating n-best smt outputs into a tm system.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics,</booktitle>
<pages>374--382</pages>
<location>Beijing, China, COLING</location>
<marker>He, Ma, Way, van Genabith, 2010</marker>
<rawString>He, Yifan, Yanjun Ma, Andy Way, and Josef van Genabith. 2010c. Integrating n-best smt outputs into a tm system. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics, Beijing, China, COLING 2010, pages 374–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myles Hollander</author>
<author>Douglas A Wolfe</author>
</authors>
<title>Nonparametric Statistical Methods.</title>
<date>1999</date>
<publisher>John Wiley and Sons.</publisher>
<contexts>
<context position="9327" citStr="Hollander and Wolfe, 1999" startWordPosition="1467" endWordPosition="1470">data sets for the experiment consist of English-French translation memories from the IT domain. In all instances MT was carried out for English-French translations. As we have access to the reference target language translations for our test set, we are able to calculate the TER and METEOR scores for the three translation outputs (here TM, MaTrEx (Dandapat et al., 2010) and Microsoft Bing). For each sentence in the test set, TMTprime recommends a particular translation output with a certain estimated confidence level without access to a reference. We measure Pearson’s correlation coefficient (Hollander and Wolfe, 1999) between the recommendation scores, TER scores and METEOR scores (for all system outputs) in order to determine how well the TMTprime prediction score correlates with the widely used automatic evaluation metrics. Results of these experiments are provided in Table 1 which shows there is a negative correlation between TMTprime scores and TER scores. This shows that both TMTprime scores and TER scores are moving in opposite directions, supporting the claim that the higher the recommendation scores, the lower the TER scores. As TER is an error score, the lower the TER score, the higher the quality</context>
</contexts>
<marker>Hollander, Wolfe, 1999</marker>
<rawString>Hollander, Myles and Douglas A. Wolfe. 1999. Nonparametric Statistical Methods. John Wiley and Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Koehn</author>
<author>Barry Haddow</author>
</authors>
<title>Interactive assistance to human translators using statistical machine translation methods.</title>
<date>2009</date>
<booktitle>In Proceedings of MT Summit XII.</booktitle>
<pages>73--80</pages>
<location>Ottawa, Canada,</location>
<contexts>
<context position="4293" citStr="Koehn and Haddow, 2009" startWordPosition="662" endWordPosition="665">et al., 2009) note that using glass-box features when available, in addition to black-box features, offer only small gains and also incur significant computational effort. 10 Proceedings of the NAACL HLT 2013 Demonstration Session, pages 10–13, Atlanta, Georgia, 10-12 June 2013. c�2013 Association for Computational Linguistics translators can tune the models for precision without retraining the models. Related research by (Simard and Isabelle., 2009) focuses on combining TM information into an SMT system for improving the performance of the MT when a close match already exists within the TM. (Koehn and Haddow, 2009) presents a post-editing environment using information from the phrasebased SMT system Moses.2 (Guerberof, 2009) compares the post-editing effort required for TM and MT output, respectively. (Tatsumi, 2009) studies the correlation between automatic evaluation scores and post-editing effort. 3 Translation Recommender Figure 1: TMTprime Workflow The workflow of the translation recommender is shown in Figure 1. We train MT systems using a significant portion of the training data and use these models as well as TM outputs to obtain a recommendation development data set. MT systems can be either in</context>
</contexts>
<marker>Koehn, Haddow, 2009</marker>
<rawString>Koehn, Philip and Barry Haddow. 2009. Interactive assistance to human translators using statistical machine translation methods. In Proceedings of MT Summit XII. Ottawa, Canada, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsuan-Tien Lin</author>
<author>Chih-Jen Lin</author>
<author>Ruby C Weng</author>
</authors>
<title>A note on platt’s probabilistic outputs for support vector machines.</title>
<date>2007</date>
<journal>Machine Learning</journal>
<volume>68</volume>
<issue>3</issue>
<contexts>
<context position="5564" citStr="Lin et al., 2007" startWordPosition="873" endWordPosition="876">able systems, such as Microsoft Bing3 or Google Translate.4 For each sentence in the development data set, we have access to the reference as well as to the outputs for each of the MT and TM systems. We then select the best MT (or TM) output as the translation with the lowest TER score with respect to the reference and label the data accordingly. System-independent features for each translation output are fed as input to the SVM classifier (Cortes and Vapnik, 1995). The SVM classifier outputs class labels and the class labels are converted into confidence scores using the techniques given in (Lin et al., 2007). Relying on system independent black-box features has allowed us to build 2http://www.statmt.org/moses/ 3http://www.bing.com/translator 4http://translate.google.com/ a fully extendable platform that will allow any number of MT systems (or indeed TM systems) to be plugged into the recommender with little effort. 4 Demo Description Using the Amazon EC25 deployment as a back-end, we have developed a front-end GUI for the system (Figure 2). The interface allows the user to select which of the available translation systems (whether they be MT or TM) they wish to use within the recommender system. </context>
</contexts>
<marker>Lin, Lin, Weng, 2007</marker>
<rawString>Lin, Hsuan-Tien, Chih-Jen Lin, and Ruby C. Weng. 2007. A note on platt’s probabilistic outputs for support vector machines. Machine Learning 68(3):267–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Simard</author>
<author>Pierre Isabelle</author>
</authors>
<title>Phrase-based machine translation in a computer-assisted translation environment.</title>
<date>2009</date>
<booktitle>In Proceedings of Machine Translation Summit XII.</booktitle>
<pages>120--127</pages>
<location>Ottawa, Canada,</location>
<marker>Simard, Isabelle, 2009</marker>
<rawString>Simard, Michael and Pierre Isabelle. 2009. Phrase-based machine translation in a computer-assisted translation environment. In Proceedings of Machine Translation Summit XII. Ottawa, Canada, pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation for Machine Translation in the Americas.</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="8538" citStr="Snover et al., 2006" startWordPosition="1343" endWordPosition="1346">em) and the cost required for post-editing. For example, if the MT is an in-house system the cost of translation will be (close to) zero whereas there is potentially an additional base cost for using an external MT engine. Finally, the interface provides statistics related to various confidence levels for different translation outputs across the various translation and TM systems. Figure 3: Results shown by TMTprime system 5 Experiments and Results Evaluation targets two objectives and is described below. 5.1 Correlation with Automatic Metrics TER and METEOR are widely-used automatic metrics (Snover et al., 2006; Denkowski and Lavie, 2011) that calculate the quality of translation output by comparing it against a human translation, known as the reference translation. Our data sets for the experiment consist of English-French translation memories from the IT domain. In all instances MT was carried out for English-French translations. As we have access to the reference target language translations for our test set, we are able to calculate the TER and METEOR scores for the three translation outputs (here TM, MaTrEx (Dandapat et al., 2010) and Microsoft Bing). For each sentence in the test set, TMTprime</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Snover, Matthew, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociation for Machine Translation in the Americas. Cambridge, MA, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
</authors>
<title>Exploiting objective annotations for measuring translation post-editing effort.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Annual Conference of the European Association for Machine Translation.</booktitle>
<pages>73--80</pages>
<location>Leuven, Belgium, EAMT</location>
<contexts>
<context position="2536" citStr="Specia, 2011" startWordPosition="378" endWordPosition="379"> The confidence metric ensures that only ∗Author did this work during his post doctoral research at CNGL. those MT outputs that are guaranteed to require less post-editing effort than the best corresponding TM match are presented to the post-editor (He et al., 2010a). The MT is integrated seamlessly, and established localisation cost estimation models based on TM technologies still apply as upper bounds. 2 Related Work MT confidence estimation and its relation to existing TM scoring methods, together with how to make the most effective use of both technologies, is an active area of research. (Specia, 2011) and (Specia et al., 2009, 2010) propose a confidence estimator that relates specifically to the post-editing effort of translators. This research uses regression on both the automatic scores assigned to the MT and scores assigned by posteditors and aims to model post-editors’ judgements of the translation quality between good and bad, or among three levels of post-editing effort. Our work is an extension of (He et al., 2010a,b,c), and uses outputs and features relevant to the TM and MT systems. We focus on using system external features. This is important for cases where the internals of the </context>
</contexts>
<marker>Specia, 2011</marker>
<rawString>Specia, Lucia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In Proceedings of the 15th Annual Conference of the European Association for Machine Translation. Leuven, Belgium, EAMT 2011, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
</authors>
<title>A dataset for assessing machine translation evaluation metrics.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC 2010.</booktitle>
<location>Valletta,</location>
<marker>Specia, Cancedda, Dymetman, 2010</marker>
<rawString>Specia, Lucia, Nicola Cancedda, and Marc Dymetman. 2010. A dataset for assessing machine translation evaluation metrics. In Proceedings of LREC 2010. Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Marco Turqui</author>
<author>Zhuoran Wang</author>
<author>John ShaweTaylor</author>
<author>Craig Saunders</author>
</authors>
<title>Improving the confidence of machine translation quality estimates.</title>
<date>2009</date>
<booktitle>In Proceedings of Machine Translation Summit XII.</booktitle>
<pages>136--143</pages>
<location>Ottawa, Canada,</location>
<contexts>
<context position="2561" citStr="Specia et al., 2009" startWordPosition="381" endWordPosition="384">ric ensures that only ∗Author did this work during his post doctoral research at CNGL. those MT outputs that are guaranteed to require less post-editing effort than the best corresponding TM match are presented to the post-editor (He et al., 2010a). The MT is integrated seamlessly, and established localisation cost estimation models based on TM technologies still apply as upper bounds. 2 Related Work MT confidence estimation and its relation to existing TM scoring methods, together with how to make the most effective use of both technologies, is an active area of research. (Specia, 2011) and (Specia et al., 2009, 2010) propose a confidence estimator that relates specifically to the post-editing effort of translators. This research uses regression on both the automatic scores assigned to the MT and scores assigned by posteditors and aims to model post-editors’ judgements of the translation quality between good and bad, or among three levels of post-editing effort. Our work is an extension of (He et al., 2010a,b,c), and uses outputs and features relevant to the TM and MT systems. We focus on using system external features. This is important for cases where the internals of the MT system are not availab</context>
</contexts>
<marker>Specia, Turqui, Wang, ShaweTaylor, Saunders, 2009</marker>
<rawString>Specia, Lucia, Marco Turqui, Zhuoran Wang, John ShaweTaylor, and Craig Saunders. 2009. Improving the confidence of machine translation quality estimates. In Proceedings of Machine Translation Summit XII. Ottawa, Canada, pages 136–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Midori Tatsumi</author>
</authors>
<title>Correlation between automatic evaluation scores, post-editing speed and some other factors.</title>
<date>2009</date>
<booktitle>In Proceedings of Machine Translation Summit XII.</booktitle>
<pages>332--339</pages>
<location>Ottawa, Canada,</location>
<contexts>
<context position="4499" citStr="Tatsumi, 2009" startWordPosition="693" endWordPosition="694">tration Session, pages 10–13, Atlanta, Georgia, 10-12 June 2013. c�2013 Association for Computational Linguistics translators can tune the models for precision without retraining the models. Related research by (Simard and Isabelle., 2009) focuses on combining TM information into an SMT system for improving the performance of the MT when a close match already exists within the TM. (Koehn and Haddow, 2009) presents a post-editing environment using information from the phrasebased SMT system Moses.2 (Guerberof, 2009) compares the post-editing effort required for TM and MT output, respectively. (Tatsumi, 2009) studies the correlation between automatic evaluation scores and post-editing effort. 3 Translation Recommender Figure 1: TMTprime Workflow The workflow of the translation recommender is shown in Figure 1. We train MT systems using a significant portion of the training data and use these models as well as TM outputs to obtain a recommendation development data set. MT systems can be either in-house, e.g. a Moses-based system, or externally available systems, such as Microsoft Bing3 or Google Translate.4 For each sentence in the development data set, we have access to the reference as well as to</context>
</contexts>
<marker>Tatsumi, 2009</marker>
<rawString>Tatsumi, Midori. 2009. Correlation between automatic evaluation scores, post-editing speed and some other factors. In Proceedings of Machine Translation Summit XII. Ottawa, Canada, pages 332–339.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>