<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.816965">
American Journal of Computational Linguistics Microfiche 45
</note>
<sectionHeader confidence="0.8344045" genericHeader="abstract">
SYNTAX
IN
AUTOMATIC SPEECH UNDERSTANDLNG
MADELEINE BATES
</sectionHeader>
<subsectionHeader confidence="0.678173">
Boston University
</subsectionHeader>
<note confidence="0.471375">
and
Bolt Beranek and Newman Inc.
</note>
<title confidence="0.645035285714286">
50 Moulton Street
Cambridge, Massachusetts 02138
This research was principally supported by the Advanced Research
Projects Agency of the Department of Defense (ARRA Order No.
2904) and was monitored by ONR under Contract No. N00014-75-c-
0533. Partial support of the author by NSF grant GS-39834 to
Harvard University is gratefully acknowledged.
</title>
<author confidence="0.40316">
Copyright 91976
</author>
<affiliation confidence="0.371742">
Association for Computational Linguistics
</affiliation>
<address confidence="0.260102">
Pare 2
</address>
<tableCaption confidence="0.492598">
Table of Contents Page
</tableCaption>
<table confidence="0.862443833333333">
Section 3
1 Introduction 6
2 The BBN Speech Understandinp System 1r5
3 The Grammar 18
4 Overview of SPARSER 31
PreAtminaries 31
Beginning tn Parse an Island 34
Parsing. Through an Zsland 37
Ending an Island
Endinc&apos; a Theory
Prooessing Multiple Theories
Processing Events
</table>
<page confidence="0.933321">
6
</page>
<sectionHeader confidence="0.670858" genericHeader="method">
5 *ore Details of the Parsing Process 10
</sectionHeader>
<bodyText confidence="0.2519">
Depth vs. Breadth 10
</bodyText>
<subsectionHeader confidence="0.231106">
Scoring Paths 42
Storing. Predictions 46
</subsectionHeader>
<bodyText confidence="0.244377">
Examples and Results
Example 1
Examole 2
Example 3
</bodyText>
<sectionHeader confidence="0.88839775" genericHeader="method">
7 Conclusions 78
Strengths and Weaknesses of SPARSER 78
Prosodies 80
Extensions and Further Research 83
Conclusion 8q
Appendix I MINIGRAMMAR 37
Appendix II The Vocabulary and Syntax Classes 89
Bibliography 93
</sectionHeader>
<page confidence="0.765526">
48
q0
;6
67
</page>
<bodyText confidence="0.886702533333333">
Page 3
Section 1
Introduction
Understanding speech is an extremely complex process which
requires the use of many types of knowledge, one of which is
syntax. This report presents a system called SPARSER which is
designed to provide and use the syntactic knowledge necessary to
support an artificial speech understanding system. (We will
assume for the remainder of this paper that unless explicitly
stated otherwise &amp;quot;speech&amp;quot; means grammatical speech spoken at a
mo4erate rate with natural inflections and pauses, spontaneously
produced but similar to the type of speech produced by reading
text.)
We will make the following assumptions about the
characteristics of speech and a speech processor:
</bodyText>
<listItem confidence="0.982363111111111">
1. There is not enough information in the spech signal to
uniquelSr identify the phonemes or words in a normally spoken
utterance.
2. The acoustic processing component of any artificial speech
understanding system will introduce additional errors and
ambiguity as it attempts to identify the phonemes. or words in the
utterance.
3. As a consequence of 1 and 2, when an utterance is scanned to
try to identify the words, it is reasonable to suppose that a
</listItem>
<bodyText confidence="0.919892727272727">
number of (perhaps overlapping) candidates will be found.
Page 4
This is illustrate in Figure 1.1 by a structure called a
word lattice which shows schematically that many Vords may
initially appear to be present. In this representation, the
numbers along the horizontal Scale are segment boundary points in
the utterance which roughly correspond to points in time This
word lattice was produced by the lexical retrieval component of
the BBN speech understanding system from an utterance which had
been segmented and labeled by hand under conditions designed to
simulate the performance of an automatic segmenter and labeler.
</bodyText>
<figure confidence="0.998178243902439">
0 5 10 15 20 25 30 35
1111 111111 1 1 1 1 1 111111111 1 111 1 1 1 111111
mow
mode
lunar 1 sample
I less had
dash
does
percent
1 s I
ten
been
did
give
we
give
mi
greater
dealing
metal
nickel
any
with
give
did
not
magnetite
lead
been
and
I done
did
done
ore
people Orel_ glass I sample
anyone
and I
greater
dealing
metal
nickel.
</figure>
<figureCaption confidence="0.843413">
Figure 1.1
</figureCaption>
<sectionHeader confidence="0.343683" genericHeader="method">
A Word Lattice
</sectionHeader>
<subsectionHeader confidence="0.677874">
Sentence: Give me all glass samples with magnetite.
</subsectionHeader>
<bodyText confidence="0.9909618">
In the system described here, such a word lattice can be
represented by a collection of word matches, each of which is
composed of a word, the boundary points at the left and right
ends of the portion of the utterance where it appears to match
well, and a score indicating how well it matches&apos; the ideal
phonemic representation of the word.
Page 5
We also make a number of assumptiOns about the nature of the
speech understanding process and the characteristics of a system
to carry out that process:
</bodyText>
<listItem confidence="0.689443">
1. People can understand &apos;a speaker even when the speech .is
</listItem>
<bodyText confidence="0.727988">
fairly ungramfttical, so a syntax-driven system which would
accept only input me4ing rigid syntactic requirements would not
be adequate for natural, converstional speech.
</bodyText>
<listItem confidence="0.934448">
2. Since a number word candidates are likely to be found
throughout the utterlane, it may be fruitful to be able to select
a subset of them on semantic, pragmatic, or prosodic grounds as
well as syntactic, depending on which cues seem most robust.
3. Syntax must interact with semantics in order to cut down the
combinatorial explosion of syntactically correct but meaningless
</listItem>
<bodyText confidence="0.8718108">
subsets of the utterance. Even in the small word lattice of
Figure 1.1 lt can be seen that there are numerous short sequences
which are syntactically but not semantically valid (e.g. &amp;quot;Ten
people are glass samples with magnetite&amp;quot;, &amp;quot;glaes samples give
magnetite&amp;quot;, &amp;quot;lunar samples give magnetite&amp;quot;, &amp;quot;samples give lead&amp;quot;,
&amp;quot;people are percent&amp;quot;, etc.).
4. The input to a speech parser will be similar to the word
lattice described above, thus the parser will have to face not
only the problem that one or more words in its input might be
incorrect, but that gaps may ppear in the input as well.
</bodyText>
<listItem confidence="0.5580622">
5. The parser will have to have the ability to predict words and
syntactic classes which are consistent with partial hypotheses
about the content of the sentence in order to help fill gaps in
the lattice.
6. Because of the combinatorial explosion of syntactic
</listItem>
<subsectionHeader confidence="0.361265">
Page 6
</subsectionHeader>
<bodyText confidence="0.998506714285714">
alternatives which occurs when all syntactic possibilities are
explored for small sections of an utterance, the syntactic
component must limit the number of such alternatives which are
actually generated, or at least factor them or treat them
implicitly rather than explicitly. One way of partially solving
this problem is to order the alternatives in such a way that only
the best alternatives al-se extended.
</bodyText>
<subsectionHeader confidence="0.6815985">
Section 2
The BBN Speech Understanding System
</subsectionHeader>
<bodyText confidence="0.985247428571428">
In the past few years there has been a flurry of activity in
the field of automatic speech understanding, resulting in a
number of different systems. For surveys of a number of these
systems the reader is recommended to Woif [31], Bates[4], and
Hyde [10]. For more specific details on some of the individual
systems, see [1, 2, 7, 8, 16, 19, 20, 21, 22, 28, 29, 33, 353.
Since SPARSER was implemented as part of a speech understanding
system called SPEECHLIS which is under development at Bolt
Beranek and Newman Inc., that system is briefly described here
and is further documented in [3, 4, 5, 6, 15, 23, 24, 26, 33,
35]. SPEECHLIS has used two task domains; that of the LUNAR
text question-answering system [16] which deals with chemical
analyses of Apollo 11 moon rocks and one dealing with travel
budget management.
</bodyText>
<subsectionHeader confidence="0.44799">
Page 7
</subsectionHeader>
<bodyText confidence="0.996204518518519">
The overall design of the systein is illustrated in Figure
2.4. The acoustics component analyzes the acoustic signal to
extract features and segment the utterance into a lattice of
alternative possible sequences of phonemes (Schwartz and Makhoul
[26]), phonological rules augment the output of the acoustic
component to include sequences of phonemes which could have
resulted in the observed phonemes; the lexical retrieval
component retrieves words from the lexicon on the basis of this
information (Rovner, et.al. [24)); the word matcher determines
the degree to which the ideal phonetic spelling of a given word
matches the acoustic analysis at a particular location [24]. All
of these components structure their output in such a way ds to
represent the ambiguity whi.;11 is inherent in their analyses. For
example, they can be used to produce word lattices such as that
which was shown in Figure 1.1.
The syntactic component is SPARSER, the system comprising
the body of this paper (see also Bates [3, 4]). Acceptable
utterances are not restricted to context-free syntax, since the
grammar which SPARSER uses is a molified ATN grammar, capable of
handling a large, natural subset of English. The remaining
sections of this thesis detail the structure and operation of
SPARSER.
The semantic component uses a semantic network to associate
semantically related words and to judge the meaningfulness of a
hypothesized interpretation (See Nash-Webber [15]). This
semantic formalism is very-general although a new network must be
constructed for each new task domain.
</bodyText>
<figure confidence="0.9918468">
Page 8
MATCH
SYNTAX
(SPARSER)
LEXICAL
RETRIEVAL
CONTROL
SEMANTICS
ACOUSTICS
PRAGMATICS,
</figure>
<figureCaption confidence="0.821793">
Figure 2.4
</figureCaption>
<subsectionHeader confidence="0.65044">
Design of BBN SPEECHLIS
</subsectionHeader>
<bodyText confidence="0.939986357142857">
The pragmatics component is cUrrenrn being implemented, but
is projected to contain information about the past dialogue, a
model of the user, and other pragmatic &apos;data (see Bruce [6]).
control component contains an overall strategy for
employing the other components in order to obtain an
interpretation of an utterance (see Rovner, et al [23]). It
decides which component is to be called, what input it is to be
given, and what is to be done with the output. It sets
thresholds on word match quality. It combines the scores
produced by the other components in order to rank competing
hypothesies, and is the primary interface to all other
components.
*Page 9
Section 3
</bodyText>
<subsectionHeader confidence="0.64811">
The Grammar
</subsectionHeader>
<bodyText confidence="0.999981409090909">
We have chosen the Augmented Transition Network formalism
[32] for the grammar which drives SPARSER because it is a
representation which allows merging of common portions of the
analysis, it is amenable to both bottom up and top down parsing
techniques, it fairly clearly separates the use of local
information from infoemation which was obtained from a distant
portion of the utterance and, the author s previous experience
with a large ATN &apos;grammar for parsing text laid the groundwprk for
the development of a similar grammar for speech.
We have tried as much as possible to keep the formalism
which was developed by Woods intact, but some changes have been
necessary or desirable to make the grammar more amenable to the
speech parser. We call the formalism a Modified AgramgnIgd
Transition. Network (MATN), and assert that it has the same power
as the original ATN formalism. The changes, are briefly indicated
here. For a fuller discussion, see Bates [4].
Every arc of an ordinary ATN has a test component, which may
bp any predicate. It is usually a boolean combination of tests
on the current input word (its features, etc.) and the contents
of registers which have been set by actions on previous arcs. In
the MATN formalism, the test component of each arc is, on all but
the PUSH arc, a list of two tests. The first is a test on the
</bodyText>
<subsectionHeader confidence="0.657257">
Page 10
</subsectionHeader>
<bodyText confidence="0.989224962962963">
current word and its features, i.e. a local, context-free test.
The second is a test on the register contents, i.e. a
context-sensitive test. Both tests must succeed for the arc to
be taken.
The reason for splitting up the tests in this way is that
register checking tests cannot be made unless the registers are
set, and in many situations in the speech environment there may
not be enough left context to guarantee that the proper registers
would be set. Thus it is useful to be able to evaluate the
context-free test on an arc at a different time in the parsing
process from the context-sensitive one.
On PUSH arcs, there are three types of tests which are used.
It is useful and efficient to test the next word of input before
actually doing the PUSH, to see, for example, if the next word
can begin a constituent of the type being PUSHed for. This test
is called a look-ahead test, and takes the place of the normal
context-free test in the test component of the arc. There is
also the usual context-sensitive test on registers which were set
before the PUSH arc was encountered. And finally, when the PUSH
arc returns with a constituent, another context-free test may be
done on the structure of the entire constituent. Therefore, the
test component of a PUSH arc is a list of the three tests lust
described.
SENDR s were an efficient mechanism for text parsing because
they allowed tests to be made on a lower level which involved
information obtaine0 somewhere (possibly fAr) to the left in the
input string -- information vihich would normally be inaccessible
</bodyText>
<subsectionHeader confidence="0.741359">
Page 11
</subsectionHeader>
<bodyText confidence="0.967963529411765">
beoause it would be hidden on the stack during the parsing of
sub-constituents.
There are svveral reasons for not allowing this mechanism in
the speech parser Suppose, in the input that looks like &amp;quot;...
the person who. travels ...&amp;quot;, the word &amp;quot;person&amp;quot; is not the word
which was really uttered. If it were allowed to be passed down
it would become an integral part of the analysis at the lower
level, and if another word were to be hypothesized in its place,
the ]ower level the analysis would have to be redone even if none
of the words in the relative clause had been cnanged. Thls is a
process which would be extremely wasteful, especially in the
speech environment where one wants to be able to take as much
advantage as possible of information which was gained at one
point and slightly altered at another. In particular, it is
advantageous to consider as constituents such constructions as
relative clauses so that they can be placed in a
well-formed-substring table for use by other professes.
</bodyText>
<tableCaption confidence="0.7382832">
Another reason is that some types of verifications
(semantic, prosodic, and pragmatic, at least) can be done most
conveniently on portions of an utterance which have been assigned
a syntactic structure, i.e. on constituents. If a portion of an
utterance is parsed (e.g 1 &amp;quot;that I gave you&amp;quot; from the complete
utterance &amp;quot;The book that I gave you&amp;quot;) but doas not form a
comDlete constituent because it is missing a piece of information
from a higher constituent to the left which would have been sent
down had it been available, then these verifications may not be
made until the missing word or words are identified. Yet it may
</tableCaption>
<subsectionHeader confidence="0.468405">
Page 12
</subsectionHeader>
<bodyText confidence="0.9357276875">
be important to build and verify the constituent in order to
predict the missing word to the left. Therefore, it is better to
allow constituents to be built without information which would
normally have been passed down When parsing possibly incorrect
fragments with little or no left context, it is heat to keep
constituents as small and as independent as possible.
The conversion process from an AIIN grammar to a MATN r!rammar
with regard to SENDR s is straightforwardand infolves the use of
a dummy symbol which is used in the construction of the lower
level constituent. When the structure is popped, the PUSH arc
examines it for agreement and may replace the dummy node by the
appropriate item which would have been- sent down. The structure
returned by the PUSH for a relative clause on the fragment &amp;quot;that
I gave you&amp;quot; might look like Figure 3.1 (where the structure is
shown in both the usual tree diagram form and a corresponding
form more amenable to computer output).
</bodyText>
<sectionHeader confidence="0.836734333333333" genericHeader="method">
S REL
SNP PRO I
FEATS NU SG
AUX TNS PAST
VP V GIVE
NP **NP**
PP PREP TO
NP PRO YOU
FEATS NU SG
REL
I
/X\ AUX
PRO FEIATS TNS
I NU PAST
SG
</sectionHeader>
<figureCaption confidence="0.702495">
Figure 3.1
</figureCaption>
<figure confidence="0.775573727272727">
VP
PP
iN•■
GIVE NP PREP NP
/ /
TO PRO FEATS
1 I
**NP** YOU NU
SG
Two Representations of a Parse Tree
Page 13
</figure>
<bodyText confidence="0.995680928571429">
The fourth element of every arc in a MATN is a small integer
which is called the weight of the arc. This weight was
originally conceived of as a rough measure of either (a) how
likely the arc is to be taken when the parser is in that state or
(b) how much information is likely to be gained from taking this
arc, i.e. whether the parse path will block quickly if the arc
is wrong. That these two schemes are not equivalent can be seen
by the following example. In a given state, say just after the
main verb of the sentence has been found, the arc which accepts a
particle may be much less likely than the arc which jumps to
another state to look for complements. However if a particle
which agrees with the verb is found in the input stream at this
point, then the particle arc is more likely to be correct. Since
it is not at all clear how to measure or even intuit how much
information is likely to be gained from taking an arc, it was
decided that the weights would reflect relative likelihoods. The
actual weights winch have been used in the speech grammar reflect
an intuitive, though experienced guess as to how likely the arc
is to be correct if it is taken, assuming the state itself is on
the correct path.
Two grammars which will figure predominantly in the
remainder of this paper have been written in the MATN formalism.
One is an extensive grammar which can handle many questions,
declaratives, noun phrase utterances, imperatives, active and
passive forms, relative clauses (reduced and unreduced),
complements, simple quantifiers, noun-noun modifiers,
verb-particle constructions, numbers, and dates (but not
conjunctions). It began as a modification of the grammar for the
</bodyText>
<subsectionHeader confidence="0.662668">
Page 14
</subsectionHeader>
<bodyText confidence="0.999455222222222">
LUNAR system [36] but has been considerably adapted and expanded.
This grammar is. called SPEECHGRAMMAR, and is listed ini[4].
Exampled are given below which were produced using this grammar.
For some illustrative purposes, SPEECHORAMMAR is too big nnd
complex, so we have produced a AINIGRAMMAR which wilt be usell to
show the basic operation of the speech parser. A detailed
listing is given in Appendix 1, but the diagram in Figure 3.:
probably shows the structure more clearly. The serious render is
encouraged to sketch a copy of this grammar for reference later
</bodyText>
<figure confidence="0.882512333333333">
on.
CAT ADJ CAT N PUSH PP/
CAT ART CAT QUANT
</figure>
<figureCaption confidence="0.6173035">
Figure 3.2
MINIGRAMMAR
</figureCaption>
<bodyText confidence="0.998832">
Since the work reported here was finished, the author has
written another grammar, called SMALLGRAM which uses the MATN
formalism but which embodies a great deal of semantic and
pragmatic information specific to the domain of discourse
currently being used by the BBN speech understanding project.
</bodyText>
<subsectionHeader confidence="0.473924">
Page 15
</subsectionHeader>
<bodyText confidence="0.9999488">
In or for the parser der to move from right to left (to
predict what could precede that first given word), it must be
able to determine for any state which arcs can enter it, and for
any arc which state it comes from. Since the grammar is
organized for normal parsing in just the opposite fashion, i.e.
for any state one can determine what arcs leave it and for any
arc (except POP) one can determine which state it terminates on,
it was necessary to build an index into the grammar. This index
consists of a number of tables c.dntaining pre-computed
informationwhich in effect inverts the grammar.
</bodyText>
<subsectionHeader confidence="0.9271015">
Section It
Overview of SPARSER
</subsectionHeader>
<bodyText confidence="0.978240069767442">
The input to SPARSER is assumed to be a set of words
together with their boundary points (which may or may not be
related to points in time). A word together with its boundaries
is termed a word match. A word match also includes a score which
indicates how well the ideal phonemic representation of the word
matched the acoustic analysis of the utterance (but as we Shall
see the parser has little need of this information). Since the
same word may match at several sets of boundary points or may
match in several ways between the same boundary points, each word
match is also given a unique number to help identify it. Thus
Page 16
the structure for a basic word match is:
(number word leffboundary rightboundary lexicalscore)
e.g. (4 TRAVEL 5 11 94), or (4 TRAVEL 9 11 (q4 110)) where the
score is given as a pair of numbers representing the actual and
maximum scores, or (4 TRAVEL 5 11) where the score is omitted.
How is the input to the parser to be constructed? We assume
that acoustic processing and lexical scanning components can
operate on a digitized waveform to produce a number of word
matches such as prevdously shown in the word lattice of&apos;
1.1. (That this is possible has been demonstrated by
[33]). Allowing the parser to operate unrestricted on the
word lattice would probably not be fruitful because of the
number of locally syntactically correct combinations of
Figure
Woods
entire
large
words,
but one possibility for input to the parser would be to take a
set of the best-matching, non-overlapping word matches in the
lattice, such as those in Figure 4.1.
A set of non-overlapping word matches is a hypothesis about
the content of the utterance. In order to avoid creating large
numbers of such sets which are put together combinatorially with
no basis except local acoustic match, semantic or pragmatic
processes can be used to group word matches based on what is
meaningful or likely to be heard. For example, if a dialogue has
been about various nickel compounds, the combination &amp;quot;nickel
analyses&amp;quot; may be more likely than &amp;quot;chemical analyses&amp;quot; even though
the word match for °chemical&amp;quot; has a higher score than that for
&amp;quot;nickel&amp;quot;. We will not attempt to detail here how this semantic
grouping could. be done and how the sets could be scored, since it
</bodyText>
<figure confidence="0.75802975">
Page 17
has been described elsewhere [15].
DO MANY PEOPLE DONE CHEMICAL ANALYSES ROCK
O2 6 11 14 22 30 35 38
GIVE EIGHTY PEOPLE DONE TEN MODAL DETERMINATION ROCK
O 3 6 11 14 15 1621 26 35 38
WERE ANY PEOPLE METAL SEVEN
O 3 6 11 17 21 27 32
</figure>
<figureCaption confidence="0.897658">
Figure 4.1
</figureCaption>
<subsectionHeader confidence="0.767855">
Sample Word Match Sets
</subsectionHeader>
<bodyText confidence="0.913085263157895">
Using more terminology from the BBN speech system, the word
theory to denotes a set of word matches such as we have just
described together with (possibly empty) slots for information
from each of the possible knowledge sources in the system. From
the point of view, of SPARSER, usually only the word match portion
of a theory is of interest, hence we shall fall into the habit of
using the word &amp;quot;theory&amp;quot; to refer to the word match set it
contains. When speaking of the syntactic component of a theory,
however, we are refering to the information slot for syntax whicn
accompanies each word match set.
Theories have the fallowing characteristics:
1) They contain a set of basic, non.4overlapping word
matches.
2) They tend at first to contain long content words and not
many shOrt function words. This is because long words are more
reliably acoustically verified and content words are easier to
Page 18
relate semantically and pragmatically. Since small words such as
&amp;quot;am&amp;quot;, &amp;quot;do&amp;quot;, &amp;quot;the&amp;quot;, &amp;quot;one&amp;quot;, &amp;quot;have&amp;quot;, &amp;quot;of&amp;quot;, &amp;quot;in&amp;quot;, etc. may be
represented by very little acotstic information, they would tend
to match at many places in the utterance where they do not really
occur. Consequently they are not searched for by the initial
word match scan, nor are they proposed in the semantic stages of
hypothesis formation.
3) They need not (and generally do not) completely span the
utterance, but have numerous gaps of varying sizes (e.g. for the
function words).
4) They tend to contain some sequences of contiguous word
matches. Such a sequence is called an island.
That such a set of theories can be created has been
demonstrated by the BBN SPEECHLIS system. The syntactic
component, SPARSER, is expected to process these theories one at
a time. In certain circumstances which will be detailed later,
the input to SPARSER will be a theory together with one or more
word matches which are to be added in order to create a new
larger theory which is then to be syntactically analyzed.
We will assume that there exists a control component which
presents SPARSER with theories to process and to which SPARSER
can communicate predictions and results.
Preliminaries
Given a theory, what is to be done with it? We begin by
considering a subset of the question: Given an island of word
matches, what is to be done with it? The answer is to create one
Page 19
or more parse paths through the island and to predict what words
or syntactic classes could surround the island. A parse path is
the sequehce of arcs in the grammar which would be used by a
conventional ATN parser to process the words in the island, if
the island were embedded in a complete sentence.
For example, consider the way a parser might process an
island of word matches such as (1 CHEMICAL 14 22)
(2 ANALYSES 22 30) using the MINIGRAMMAR of the previous section.
Beginning in state NP/ of the grammar (omitting for the moment
the problem of how it is known that NP/ is the right place to
begin) the sequence or arcs which would IA taken to parse
&amp;quot;chemical analyses&amp;quot; as a noun phrase is that shown below in
Figure 4.2.
</bodyText>
<table confidence="0.46744675">
CAT ADJ
JUMP JUMP CAT N POP
Figure 4.2
Portion of MINIGRAMMAR needed to parse &amp;quot;chemical analyses&amp;quot;
</table>
<bodyText confidence="0.975984405405405">
Let us define a configaration to be a representation of the
parser being in a given state (say NP/QUANT) at a given point in
the utterance (say 14). We will write configurations as
STATE:POSITION in text (e.g. NP/QUANT:14) and schematically as a
•
box within which are written the state and the position. If a
configuration represents a state which is either the initial
state of the grammar or a state which can be PUSHed to (i.e a
state which can begin the parsing of a constituent), it is called
Page 20
ad initial configuration, and is indicated schematically by a
filled-in semi-circle attached to the left edge of the box. Note
that a configuration NP/QUANT:14 is quite distinct from a
configuration NP/QUANT:22 since they are at different positions
in the input. In SPARSER, each configuration is also assigned a
unique number which is a convenient internal pointer.
The process of traversing an arc of the grammar using a
particular word is represented by a transition from one
configuration to another. A transition can be made only if the
arc type is compatible with the current item of input and if the
context-free test on the arc is satisfied. (The
context-sensitive tests are evaluated later.) A transition
carries with it information about the arc which it represents and
the item of input it uses. The item of input is usually the word
match which the arc uses, but it it NIL in cases such as JUMP
arcs which do not use input, and it is a complete constituent for
PUSH arcs. A unique identifying number and the list of features,
if any, which is associated with the input word or constituent
are also recorded on the transition in SPARSER, but they are not
shown schematically. A transition is represented schematically
by an arrow from one configuration to another with an abbreviated
form of the arc written above the arrow and the item of input
under it.
The syntactic part of any theory which SPARSER processes
contains, among other things, lists of the transitions and
configurations which are created or used by the theory. Thus
wheh we talk about creating a configuration or transition it is
</bodyText>
<subsectionHeader confidence="0.466172">
Page 2.1
</subsectionHeader>
<bodyText confidence="0.937970105263158">
implicitly understood that SPARSER also adds it to the
appropriate list, and when we talk of adding an existing
configuration or transition to a theory we mean adding it to the
appropriate list. Therefore, removing a configuration or
transition from a theory means removing it from the syntactic
part of the thebryt not removing it entirely from SPARSER s data
base.
Like configurations, transitions are unique, so only one
transition is ever constructed from point A to point B for arc X
and input Y. We will frequently speak of creating a transition
or a configuration, but the reader must bear in mind that if such
a configuration or transition already exists, this fact will be
recognized and the pre-existing configuration or transition will
be used. (Timing, measurements indicate that it takes about .052
seconds to create a configuration and only .01 seconds to test if
a particular configuration already exists. For transitions,
creation takes about .54 second&apos;S and recognition .012 seconds.
The seqUence of configurations and transitions which would
parse the above example is displayed in Figure 4.3.
</bodyText>
<figure confidence="0.950034625">
JUMP
NIL
NP/QUANT
14
JUMP
NIL
NP/ADJ
14
CAT N
-CHEMICAL
NP/ADJ
22
CAT N
ANALYSES
NP/ JUMP NP/ART
14 IT17&apos;
</figure>
<figureCaption confidence="0.791256">
Figure 4.3
</figureCaption>
<tableCaption confidence="0.7301295">
Path for parsing &amp;quot;chemical analyses&amp;quot;
A connected sequence of transitions and configurations is
called a oath. If the sequence begins with an initial
configuration and ends with a transition representing a POP arc,
</tableCaption>
<subsectionHeader confidence="0.266364">
Page 22
</subsectionHeader>
<bodyText confidence="0.92452675">
it is a complete path, otherwise it is a partial path. Paths are
assumed to be partial unless otherwise specified.
agglnning to Egrse an Igignd
SPARSER processes an island of words by beginning with the
leftmost word and determining its possible parts of speech. Then
the arcs of the grammar which can process the word are fpund (by
looking in the previonsly constructed grammar index). For each
arc, two configurations are constructed one for the state at the
tail of the arc and one for the state at the head, using the left
and right boundary positions of the word match, respectively, and
a transition for that arc using the current word match is also
built. Schematically, we have for our example a situation which
looks like that of Figure 4.4 (such a display of all or some of
the transitions and configupations which the parser has
constructed is called a map). Notice that a configuration may
have any number of transitions entering or leaving it.
</bodyText>
<figureCaption confidence="0.460523">
Figure 4.4
</figureCaption>
<bodyText confidence="0.652917">
Initial map for parsing &amp;quot;chemical analyses&amp;quot;
</bodyText>
<subsectionHeader confidence="0.258585">
Page 23
</subsectionHeader>
<bodyText confidence="0.542743181818182">
The idea of this process is to begin tO set up paths which
may be used to parse the island. However it is not necessarily
the case that the only donfigurations which could start paths
through the island are those which have just been obtained, since
it may be possible to create transitions which enter them via
JUMP arcs or TST arcs. For each state, the sequence of arcs
which can reach it without using the previous word of input have
been be pre-calculated by the grammar indexing package so the
appropriate configurations and transitions may be constructed.
These transitions are cqlled lead-in transitions. Thus the map
becomes that in Figure 4.5
</bodyText>
<table confidence="0.99160775">
NP/
14
JUMP N P/ART JUMP
NIL 14 NIL
</table>
<tableCaption confidence="0.465161">
Lead-in transitions for parsing &amp;quot;chemical analyses&amp;quot;
</tableCaption>
<bodyText confidence="0.99654925">
Note that any of the configurations (except for NP/ADJ:22
and NP/N:22) could actually be the correct leftmost configuration
for this island, depending upon what the (currently unknown) left
context of the island is.
By looking in the grammar index, SPARSER can determine, for
each configuration which could start the island, just what sort
of left context could be appropriate. For example, the CAT ADJ
arc in MINIGRAMMAR which enters state NP/QUANT implies that an
</bodyText>
<subsectionHeader confidence="0.464082">
Page 24
</subsectionHeader>
<bodyText confidence="0.995812033333333">
adjective could precede the island and, if it did, the transition
which would process it would terminate on configuration
NP/ADJ:14.
Because the initial configuration NP/:14 could start the
island, anything which could precede a noun phrase could occur to
the left; again the grammar index provides the information that
the CAT PREP arc could lead to a configuration which could accept
a noun phrase (via the PUSH NP/ arc), so a preposition could also
prefix the island. If the index functions indicate that a
constituent could be picked up by a PUSH arc which could
terminate on the configuration under consideration, an indication
is made in the WFST so that any time a constituent of the desired
type is built which ends at the proper location, it may be tried
here.
Because of the highly recursive nature of ATN grammars, it
is very likely that as we chain back through the possible
sequences of PUSHes which could lead to the beginning of the
current constituent (or the seouence of POPs which could be
initiated by the completion of the current constituent) a large
number of predictions will be made. Rather than make all these
predictions automatically, before we are even sure that there is
in fact a constituent at the current level, the possible
configurations which could make predictions on other levels are
saved to be activated later if the predictions from the current
set of active configurations are not sufficient.
Page 25
The predictions which are made (not saved) are not acted
upon at this time, but art? kept internally by SPARSER until all
the islands of the theory have been processed. We shall see
belaw what then becomes of the predictions.
</bodyText>
<subsectionHeader confidence="0.853194">
Parsing Through an Island
</subsectionHeader>
<bodyText confidence="0.999950095238095">
Once processing has proceeded this far, we can go back and
consider the set of configurations which represent states the
parser could be in just after processing the first word of the
island. In our examine, these are configuratigns NP/ADJ:22 and
NP/N:22. Configurations such as these which are waiting to be
extended to the right are called active configurations. SPARSER
selects a subset of the set of active cdnfigurations (how this
subset is selected will be discussed in the next section) and for
each configuration tries to extend it by trying to parse the rest
of the Island beginning in that configuration. When the parser
is considering a configuration at some position, the input
pointer is set to the word match of the island, if any, which
begins at the same position in the input.
The grammar associates with the state of the configuration a
list of arcs which may be tested (using the arc type, the context
free test on the arc, and the eurrent input) to determine whether
a transition can be made to extend the path. We will consider
each type of arc in turn, since the effects of taking various
types of arca are different, and explain for each case what
happens if the arc is taken. Whether just one transition, or
several, or all possible transitions are made from an active
</bodyText>
<subsectionHeader confidence="0.478076">
Page 26
</subsectionHeader>
<bodyText confidence="0.992677716981132">
configuration is a matter to be discussed in Section Five.
Some JUMP arcs do not look at the current item, so they may
be taken whether the input pointer is set to a word match or to
NIL. The transition which results from taking an arc of this
type has a null item associated with it, even if there is a word
match in the theory at this point. The positions of the
configurations at each end of the transition are the same; this
corresponds to the fact that an ATN parser would not move the
input pointer as a ponsequence of taking this arc.
Rarely, a JUMP arc may test the current item in some way,
for example, to make a feature check. If there is no word match
for input, an arc of this type cannot be taken. If there is a
word match, it is noted on the trahsition wh oh is created, but
the configurations at each end of the transition have the same
position. (It is then the case that theinext input-using or
input-consuming transition on the path including this transition
must use the same word match.)
These are TST, CAT, and WRD arcs which end in a
(TO nextstate) action. The operation is exactly the same as that
above except that the configuration on which the transition
terminates has the position of the right boundary of the current
word match.
Taking a POP arc results in the creation of a transition
which has a null final configuration and a null item, because POP
arcs are not permited to look at input.
Page 27
When a PUSH arc is encountered, a monitor is placed in the
Well-Formed Substring Table (WFST) at the current position to
await the occurrence of a constituent of the required type. If
one or more such constituents are already in the table, then Cor
each one there are three possibilities: it may be composed of
word matches which are in the current theory, it may be composed
of word matches some of which are not in the current theory but
which could be added without violating the non-overlapping
constraint, or it may be composed of word matches some of which
are Incompatible with the current theory.
In the first case a transition is set up using the
constituent as the current word. The transition terminates on a
configuration whose state is determined from the termination of
the PUSH arc and whose position is that of the right boundary of
the rightmost word match in the constituent.
In the second case, a notice is created and sent to the
control component. A notice is a request that SPARSER be called
to enlarge a theory by adding some new information, in this case,
some additional word matches which form a constituent that the
theory can use. SPARSER does not try to determine when (or even
whether) the theory should be so enlarged. That is an issue for
the main controller to decide (see Rovner, et.al. [23]). We
will discuss below how SPARSER enlarges a theory if called upon
to do so.
In the final case, if there are no usable constituents in
the WFST, a new configuration is set up to start looking for one
and is added to the list of active configurations. Its state is
</bodyText>
<subsectionHeader confidence="0.513695">
Page 28
</subsectionHeader>
<bodyText confidence="0.992416">
the state specified by the PUSH arc and its position is the same
as the current configuration.
There is a considerable amount of processing that can happen
any time one of the transitions lust discussed is made. Whenever
an initial configuration is constructed, this fact is recorded in
the configuration. Whenever a transition is made from such
</bodyText>
<figureCaption confidence="0.884950181818182">
configuration, the information that there is a path from some
Initial configuration is recorded on the subsequent
configuration. Similarly, whenever a POP transition is made, the
configuration it emanates from and all previous configurations on
any path which can terminate with the POP transition are marked
to indicate that they can reach a POP transition. Whenever a
transition is made which completes a path from an initial
configuration to a POP transition, the path is executed, one
transition at a time, and the register setting actions and
context sensitive tests are executed. If a test fails or an arc
aborts, the transitions and configurations of the path are
</figureCaption>
<bodyText confidence="0.982801368421053">
removed from the list of configurations and transitions which are
in the syntactic part of the current theory (unless they are used
by another path in the theory) but not removed from the map. If
the execution is successful, a deep structure tree is produced.
That structure together with its features is given a score, whioh
may include evaluations by other compotents such as semantics and
prosodies, and is entered in the WFST.
It is quite important that sources of knowledge other than
syntax be called upon to verify and to rank syntactic
constituents. This is because there are likely to be many
Page 29
combinations of plausible words from the word lattice which form
syntactically reasonable constituents but which may be ruled out
on other grounds. To allow immediate use of this information
which syntax cannot provide alone, SPARSER has an interface to
the seantic component so that constituents can be verified
directly without going through the control component. It will be
amtrivial modification to insert verification calls to pragmatics
and prosodics when they become available. In the meantime, even
semantic knowledge can be turned off; if the parser gets no
information from the call to semantics, it proceeds without it.
Placement of a constituent in the WFST causes a number of
things to happen. First, any monitors which have been set by the
current theory at that position ese activated. That is, for each
configuration which was waiting for this constituent, a PUSH
transition Is made which uses the constituent as its input item.
If no nonitors have been set which can use this constituent, it
is treated exactly as if it were the first word of an island:
all the PUSH arcs which can use it are found in the grammar index
and appropriate configurations and transitions (including lead-in
transitions, if appropriate) are set up. Next, if there are any
monitors for other theories which can use the constituent,
notices are created and output to Control as was described above
In the section on PUSH transitions.
Figure 4.6 shows SPARSER $ map after our example island has
been completely processed. The parsing results-in the creation
or a CAT N transition to configuration NP/N:30 using the word
wanalyses&amp;quot; The PUSH PP/ arc at state NP/N would oause
</bodyText>
<note confidence="0.173685">
Page 30
</note>
<tableCaption confidence="0.586362875">
configuration PP/:30 to be created. Similarly, PP/:22 would be
created when the configuration NP/N:22 is picked up to be
extended. The POP arc transitions from each of the
configurations for state NP/N result in the formation of complete
paths, resulting in the creation of two noun phrases (&amp;quot;chemical
analyses&amp;quot; and &amp;quot;chemical&amp;quot;). Since there were no monitors for
them, they result in the creation of configuration PP/PREP:14 and
*s subsequent paths.
</tableCaption>
<figure confidence="0.998011045454546">
••■••••=1...■11,
JUMP
NIL
JUMP
TIV
JUMP
7I17.
NP/ADJ
14
NP/ART
14
NP/QUANT
14
CAIN NP/N POP
ANALYSES 30
4
NP/N POP
22
NP/
14
PP/
30
</figure>
<figureCaption confidence="0.903887">
Figure 4.6
</figureCaption>
<subsectionHeader confidence="0.487989">
Map after processing island
</subsectionHeader>
<bodyText confidence="0.570552">
Page 31
</bodyText>
<subsectionHeader confidence="0.341289">
Ending an Island
</subsectionHeader>
<bodyText confidence="0.993100238095238">
It may be the case that no path can be found from one end of
an island to the other. (This would occur when all active
configurations block.) In this case, there is no possible way
that the island could form part of a grammatical string, so
SPARSER can inform the control component that the theory is
wrong.
When an active configuration is picked up to be extended and
there is no word match at that point, the end of the island has
been reached. That does not mean that no more transitions can be
made, since arcs which do not test the input word can be taken as
usual. Arcs which do use input cannot be taken, but they can be
used to predict what sort of input would be acceptable at that
position. For example, a CAT V arc which has a test requiring
the verb to be untensed would allow SPARSER to predict an
untensed verb beginning at the position of the current
configuration. CAT and WRD arcs cause the prediction of
syntactic categories and specific words, respectively, modified
by the context-free test on the arc. TST arcs provide only the
test which must be satisfied, and PUSH arcs cause a monitor to be
set in the WFST as well as a TST monitor for the the look-ahead
test (if any) on the arc.
</bodyText>
<subsectionHeader confidence="0.95817">
Ending a Theory
</subsectionHeader>
<bodyText confidence="0.999562333333333">
When &apos;all the islands of a theory have been processed in the
manner just described, it is time to deal with the gaps between
the islands. As we have seen, arcs in the grammar which can
</bodyText>
<subsectionHeader confidence="0.461807">
Page 32
</subsectionHeader>
<bodyText confidence="0.997027884615385">
enter configurations at the left end of an island or which can
leave configurations at the right end of an island can be used to
make predictions about words that may be adjacent to the island.
The prediction is a list of the arc, the configuration it would
connect to, and an indication of whether the transition caused by
the arc will enter the configuration from the left or leave it to
the right.
If a gap between two islands is small enough that it may
contain just one word, then it is likely that the arc which would
process that word may have caused a prediction from both the left
and right sides of the gap. If this is the case, and if the
predictions intersect in a single possibility, it is highly
probable that the word (or syntactic class) so predicted is
correct. If the predictions do not intersect, parsing is
continued from the active configurations which were not tried
earlier because of their scores and from the configurations which
could begin constituents at the right end of an island. This
continued parsing is an attempt to find a path which results in a
common prediction across the gap. If that too fails, then the
configurations which were saved because they could lead up a
chain of PUSHes or POPs to new configurations are tried. If no
possibilities are left to try and there is still no prediction to
fill the gap, this information is noted, but it does not
definitely mean that the islands are incompatible, since in some
cases the gap could actually be filled by two words instead of
one.
</bodyText>
<subsectionHeader confidence="0.6788">
Page 33
</subsectionHeader>
<bodyText confidence="0.997514333333333">
SPARSER has two kind of predictions - those which seem
highly likely and those which seema less likely. A highly likely
prediction, such as one which is made from both sides of a small
gap, is output in the form of a proo2sall which is a request to
the rest of the system to find a word meeting the requirements of
the proposal. A proposal contains:
</bodyText>
<listItem confidence="0.976736333333333">
1) the item being proposed, which is either a particular word
or list of words (from a WRD arc), or a syntactic olass (from a
CAT arc), dr NIL, &apos;meaning any word (from a TST arc)
2) the left and/or right boundary point(s) of the item
3) a test which the item must satisfy (the context free test
from the arc)
</listItem>
<bodyText confidence="0.932829705882353">
4) the context of the proposal, i.e. the word match(es) on
the left apd/or right side of the item being proposed. (This is
to help the lexical retrieval component take into account
phonological phenomena which may occur across word boundaries.)
All predictions whether or not they are confident enough to
become proposals are output as monitors. A monitor is a
notification to the control component that if a word meeting the
requirements of the monitor is somehow found (perhaps by the
action of a proposal) , it may be added to the theory. Thus a
monitor acts like a demon which sits at a particular point in the
word lattice and watches for the appearance of a word match which
it can use. A monitor contains:
I) the item being monitored for (generally a syntactic
categcry, but may be a word or a test)
2) the left or right boundary position of the item being
monitored far
Page 34
</bodyText>
<listItem confidence="0.995442666666667">
3) a test which the item must satisfy (same as for proposals)
4) the theory which generated the monitor
5) the arc in the grammar which will process the item if
found
6) the configuration frnm which the prediction was made
7) a score, indicating roughly how important the monitor is,
</listItem>
<bodyText confidence="0.9998715">
i.e. how much information is likely to be gained by processing
an event for that monitor.
(Notice that monitors which are sent to the control component are
very much like monitors which are set in the WFST by the
occurrence of PUSH arcs.)
Once the proposals have been made and the monitors have been
setj SPARSER bundles up the information it knows about the
current theory, such as the configurations and transitions in the
theory, any configurations which are still candidates for
expansion, the constituents in the theory, the notices,
proposals, and monitors which have been created, etc. and
associates the bundle with the theory number. This insures that
SPARSER will be able to pickup where it left off if it is later
given the thetry to process further.
</bodyText>
<subsectionHeader confidence="0.975439">
Processing Multiple Theories
</subsectionHeader>
<bodyText confidence="0.9998022">
Thus far we have seen only the operations which SPARSER
performs on a single theory, but we made the assumption that
SPARSER would be given a number of theories to process in
sequence. Let us now examine what will happen when the second
(or nth) theory is processed.
</bodyText>
<equation confidence="0.405359">
Page 35
</equation>
<bodyText confidence="0.956026">
SPARSER will no longer have a blank map and WFST; instead
it will have all the configurations, transitions, and
constituents which have been constructed by all previous
theories. For concreteness, let us imagine that the theory (1
CHEMICAL 14 22) (2 ANALYSES 22 30) has been processed, resulting
in the map shown in Figure 4.6. Now we Bre going to process a
theory containing the island (4 NICKEL 16 22) (2 ANALYSES 22 30),
which results in the map of Figure 4.7 Where the configurations
and transitions added by this theory are shown in dotted lines.
The process begins as usual with the creation of
configuration NP/ADJ:16 and three possible lead-in transitions.
The transitions for the two CAT N arcs, however terminate on
configurations which already existed in the map, so the complete
paths from configuration NP/:16 to configurations NP/N:30 and
NP/N:22 will be discovered and processed, resulting in the
construction of two new noun phrases. Those new constituents
would then result in the creation of configuration PP/PREP:16
and two new transitions. Thus we have constructed only five new
configurations and seven new transitions and have been able to
take advantage of six old configurations and six old transitions.
In this fashion any information which has once been
discovered about a possible parse path is made available to any
other path which can use it. No reoarsing is ever done
SPARSER merely realizes the existence of relevant configurations
and transitions and incorporates them into the current theory.
</bodyText>
<figure confidence="0.98512425">
NP/ADJ
30
NP/ADJ CAT N
22 ANALYSES
0-40
NP/N
30
POP
171.17 JUMP NP/ART JUMP NP/QUANT JUMP
cji 14 NIL ,1 14 &amp;quot;Mr° 14
NIL
---1 r--- -1 r - - -,- -1
-1, /
NP/
A &apos;Jaw I NP/ART &apos;JUMP01 1 NP/QuANT utimp I NP/ADJ r cm- ,.i. ,N _ .
I.__ -61 1-• - - &apos; IZZ -01 1
A
.: 16 I NIL I 16 1 NIL I 16 I NIL I 16 1-- --ticiE
&apos; _,....I. L-- — -I I- .--- J L--J
cAL
Of*
c); c&gt;&apos;
/04
NP/ADJ
14
NP/N POP
22
It PP/ PP/
22 30
Ns&amp;quot;
koko‘ct&apos;L 01•/
./31,„ ■
aD,e0-eps
ty oh,
PP/NP POP
ES) 30
AtAt4&apos;15
1
ri.PP/PREPIr/ PP/NP POP
PUSH HP/_I1-L77:i:7 22
</figure>
<subsectionHeader confidence="0.885563">
Map after processing island for &amp;quot;nickel analyses&amp;quot;
</subsectionHeader>
<bodyText confidence="0.999581636363636">
If the new word (or wolids) in a theory are at the end (or in
the middle) of an island, when SPARSER begins to parse the island
it will discover the existing configurations and transitions from
the previous theory. Whenever a transition which can be used in
the current theory is discovered in the map, it and its
terminating configuration are added to the syntactic part of the
current theory. This is called tracing the transition. In
addltion, all paths beginning with that transition which do not
require the next word of input are also included in the syntactic
part of the theory. This is accomplished by tracing from the
terminating configuration all transitions which use either the
</bodyText>
<subsectionHeader confidence="0.669345">
Page 37
</subsectionHeader>
<bodyText confidence="0.9832889">
same word of input as the previous transition or no input word at
all. (A similar process is used to trace backwards, i.e. right
to left, when necessary.) When a configuration is reache4 which
has no traceable transitions emanating from it, the tracing
process, stops. Since both transitions and configurations are
stored in such a way as to facilitate tracing (for example, each
transition has a code attached to indicate whether or not it
consumes or tests input), this process is considerably faster
than creating that portion of the map in the first place. (To
illustrate this, a theory was processed twice, once with an empty
map and once starting with the map previously created; the time
required for processing the theory fell from 47.5 seconds tel
16.5.)
Configurations which can end traced paths are put on the
active configurations list. If, when one of&apos; them is picked up
for extension, it is discovered that the next word of input was
used on a transition already in the map, the tracing process is
repeated. If the next word of input is new (or at least has not
caused any transitions from the configuration being considered)
then parsing continues in the normal manner.
</bodyText>
<subsectionHeader confidence="0.963493">
Processing Events
</subsectionHeader>
<bodyText confidence="0.999953">
As. was mentioned earlier, SPARSER can be called upon to add
some new word matches to a theory it has previously processed.
In this case, SPARSER is said to process an event. An event may
be thought of rather abstractly as the discovery of a piece of
information that has been syntactically proposed, monitored for,
</bodyText>
<equation confidence="0.255722">
Page 38
</equation>
<reference confidence="0.3548527">
or noticed. Concretely, an event is a piece of data consistiniz
of:
1) the old theory that proposed or set a monitor for the
event
2) something to be added to the theory (a new word match or
constituent)
4) the arc in the grammar which will process the new
information
4) the confipuration ih the old theory which will be at one
end of the transition created by the above arc
</reference>
<subsectionHeader confidence="0.780058">
When SPARSER is given an event, it retrieves from its tables
</subsectionHeader>
<bodyText confidence="0.972345166666667">
the bundle of configurations, transition, etc. in the old
theory. Then using the arc and the new word or constituent in
the event, it creates the appropriate transition(s). Then
processing continues as usual, that is, any complete paths are
noticed and processed, and any new active configurations are
exbended, if possible.
</bodyText>
<subsectionHeader confidence="0.545604">
New predictions may be made as a result of this increased
</subsectionHeader>
<bodyText confidence="0.964826421052632">
information. (A record is kept of previous predictions so none
are remade unless with a more liberal score.) Finally SPARSER
returns the new, larger theory. This new theory may be processed
as part of another event at some later time, thus gradually
reducing the number and size of the gaps in the theory.
If an event results in filling the final gap in a theory,
and if the resultinp complete sequence of words can he parsed,
SPARSER notifies the control component of this fact, since the
entire utterance may have been discovered. Of course, this may
Page 39
not be the correct solution -- it is up to the control component
to look at. the acoustic goodness, semantic meaningfulness,
pragmatic likelihood, etc. of the result as well as the
syntactic structure before declaring the utterance to have been
understood. If for reasons other thin syntactic, the utterance
appears to be bad, the control component of the system could go
on to try to find another, more suitable, possibility.
Section 5
More Details of the Parsing Process
</bodyText>
<subsectionHeader confidence="0.929742">
5.1 DEPTH vs BREADTH
</subsectionHeader>
<bodyText confidence="0.9835928">
The parsing strategy just outlined works bottom up when
beginning to parse an island and when a constituent is created
which was not monitored for by the current theory. It works top
down after an island has been started and to make syntactic
predictions at the ends of islands. Both top down and bottom up
techniques can be either depth or breadth first. Depth first
processing takes at every step the fi!ost piece of information
available and pursues its consequences. Breadth first processing
considers at every step every possible next step of every
alternative and pursues all paths in parallel. Breadth first
processing generally takes much more space than the depth first
process since many paths would have to be remembered at once
Rage 40
instead of having just one stack which could be popped and reused
when necessary.
The breadth first process might save some computation steps
and might produce several ambiguous parsings simultaneously while
tlae depth first process would find one before the others (the
latter is a&apos;small difference, since both processes would have to
be run to exhaustion to insure that all possible parsings had
been found). In parsing speech, some mixture of breadth first
and depth first processing can be extremely useful.
To illustrate an advantage of breadth first processing in
the speech environment, consider what miRht happen if, during the
processing of an island the parser picks up a confiruration to
extend which has several possible arcs emanating from it. If one
arc is chosen and all the others are held as alternatives (i.e.
depth first), but the chosen arc is wrong, all subsequent paths
beginning with that arc would have to block before the
alternatives would be tried. However, if the end of the island
were reached before the success or failure of the first choice
were confirmed, the only way that backup would ever take place
would be to have one or more events add words to the theory so
that the path could be extended until it failed. Since the gap
would be likely to be filled by (incorrect) words predicted by
the erroneous path, or by no words at all if the (incorrect)
predictions were not satisfied, it is not at all clear how the
process would ever know to back up.
Page 41
This problem cannot be eliminated completely without
pursuing all alternatives to their fullest extent (a
combinatorially unacceptable solution) but it can be modified to
a great extent by a judicious combination of depth and breadth
first processing to find the best path, not just the first one,
through the island. This &amp;quot;best path&amp;quot; is not guaranteed to be the
correct one, so it is possible to continue processing by
extending paths with were suspended earlier.
SPARSER handles the problem by assigning a score lo every
configuration which reflects the likelihood of the path which
terminates on that configuration to be correct. The score can
also be thought of as a measure of how good that configuration
looks in relation to others as a candidate for extension. One
question which was previously left unanswered, how a subset of
the active configurations is chosen for extension, can now be
answered: the subset of maximally scoring configurations is
chosen at each step until the maximal score of active
configurations begins to fall. (The score on a configuration and
the score of a path terminatinr on that configuration are the
same thing -- we will use which ever terminology seems most
natural at the time.)
The result of this process is a sort of modified breadth
first approach, where at one step all the alternatives are tried
but at the next step only the best ones are chosen for further
extension. This is similar to the best-first parser described by
Paxton in [18] but it can be applied to the sort of partial paths
which SPARSER generates rather than requiring the perfect
Page 42
information resulting from a strictly left to riFht approach.
The success of this method is directly dependent on the relative
accuracy of the scores which are assigned to the paths.
</bodyText>
<subsectionHeader confidence="0.971423">
5.2 SCORING PATHS
</subsectionHeader>
<bodyText confidence="0.98735205882353">
Several attempts have ben made to develop rigorous systems
for parsing errorful or speech-like input based on probabilities
[1, 14, 27]. These atteMpts have all simplified the problem to
such an extent that it is no lonFer realistic or extendible, e.g.
by assuming the input is a sequence (rather than a lattice) of
probability distributions, by assuming that all the necessary
Information is present in the search space to begin with so the
only problem is to find an optimal path through the space, by
requiring a small vocabulary, and/or by limiting the grammar to
be context free.
The ideal scoring mechanism for SPARSER would be one which
accurately reflected at every step the probability that the path
as correct. Bayes rule could be used, but it would be
necessary to know, at any point in the parsing process, what the
probability is that the next arc under consideration is correct,
given that the entire path up to the current step is correct. In
order to use this application of Bayes rule it would be
necessary to pre-calculate the probabilities for every possible
path and partial path which could be generated a clearly
impossible task since there are an infinite numbex of such paths.
Page 43
Given that we cannot calculate the probabilities we need
exactly, what is the next best option? If we ignore the effect
of the path traversed up to the current point, but can say for
any given state how likely each arc emanating from that state is
to be correct, we would have a model which uses only local
Information rather than one which takes into account accurately
all the Left context which is available.
Since it was not practical to run large amounts of data
through a parser in order to obtaid accurate measurements even
for the limited model, the author relied on considerable
experience with ATN grammars to assign a weight to each arc of
thipgrAmmar representing the intuitive likelihood that the arc
(if it can be taken) is thy correct Qre to choose from that
state. These weights are small integers (0 through 5) -- the
larger the weight the more likely the arc.
The question might arise as to why the score on the word
match used by an arc should not be used to influence the score of
the path using it. SPARSER tries to treat each theory as
independently as possible and to assign scores based only on the
syntactic information which is available. The one exception to
this rule is the semantic information which is used to score
constituents. If lexical word match scores were used, the
control component would not be able to separate the lexical
goodness from the syntactic goodness of the theory and make
judgments as to their relative importance. In a syntax-driven
speech understanding system, however, it would prObably be useful
to combine lexical scores with syntactic information.
Page 44
As was described in the previous section, when SPARSER
begins to parse an island each possible partial path is begun by
creating a configuration at the head of a transition for an arc
which can use the current word. Rather arbitrarily, it was
decided to give this configuration a score of one. This starts
all partial paths out equally, a technique which is not quite
accurate, since some contexts are more likely than others. For
example, the words &amp;quot;to&amp;quot; and &amp;quot;for&amp;quot; are more likely to occur in
prepositional phrases than in sentential complements. If this
simplification appears to harm the overall performance of
SPARSER, it coulo be remedied by giving each state an a priori
score similar to the weights on arcs. Configurations on lead-id
paths are also given a score of one.
After the initial step, whenever a transition (other than a
PUSH or POP) is made, the score of the subsequent configuration
is influenced by the score of the configuration being extended
and the weight on the arc being used. If the score&amp; were actual
probabilities; they would be multiplied; since they are not, it
was arbitrarily decided to add them.
When attempting to create a configuration which already
exists (a situation encountered whenever two or more parse paths
for the same theory merge), the configuration is given the
maximum of the existing score and the score which would have been
assigned had the configuration been created anew.
When a PUSH arc is encountered and a configuration created
to begin the search for the required constituent, the score of
that configuration is set to be the sum of the score of the
Page 45
configuration causing the PUSH, and the value (if any) of the
look-ahead test on the PUSH arc. For example, upon encountering
an arc such as (PUSH NP/ ((NPSTART) T T) ...) the look-ahead
function NPSTART returns a high integer value IT the next word is
a noun and a lower, value if it is a verb (e.g. &amp;quot;accounting
costs&amp;quot;). Of course, if the look-ahead funcCion fails altogether,
the configuration is not set up, although the monitor in the WFST
remains.
When a constituent is completed (or found in the WFST) and a
PUSH transition is about to be made, the score of the
configuration on 4hich the transition terminates is a function of
the score of the confiruration being extended the weirht on the
arc, and the score of tbe constituent itself. The score of tht.
constituent is currently very ad hoc, being a function of the
number of words in the constituent (less a function of the number
of sub-constituents subsumed by this constituent, boosted if the
constituent is a major one) and the score which is determined by
semantic verification. Thus semantically &amp;quot;rood&amp;quot; constituents
will Iloost the scores of the paths which use them more than
semantically &amp;quot;bad&amp;quot; ones.
Due to the level of effort required to gather accurate
statistics on the relative frequencies of arcs, the current
scores are admittedly ad hoc. It is not clear whether different
scoring mechanisms would be better, however it is clear that the
current scoring strategy is better than no scoring at all, as
preliminary measurements indcate that the number of transitions
created (as well as the number of configurations and predicions)
Page 46
is reduced about 25% by the current strategy.
(It is reasonable to ask why semantic scores are used to
influence parse paths, since it was iust argued that lexical
scores should not be used in this way Semantic scores may he
more reliable than lexical ones hecause we are assuming that the
utterande is semantically meanihgful. Under this assumption, a
constituent like &amp;quot;range remainder&amp;quot; as a noun-noun modifier
analogous to &amp;quot;surplus money&amp;quot; should be ruled out as early as
possible. Since such constituents cannot be ruled out on
syntactic grounds alone, since prosodic information (which might
help to rule them out) is not available (see discussion in
Section 7.2), an4 since they would seriously overrun the parser
with a plethora of false paths if they were not rejected, it
seems reasonable to permit semantics to influence the parser.)
</bodyText>
<subsectionHeader confidence="0.764155">
5.3 SCORING PREDICTIONS
</subsectionHeader>
<bodyText confidence="0.979906666666667">
The previous section discussed three ways in which SPARSER
can make predictions about what could fill in gaps between
islands. Monitors wait for the occurrence of a word in the word
lattice (or a constituent in the WFST), proposals request a
search for a particular set of words, and notices indicate the
presence of a usable word in the word lattice (or a constituent
in the WFST). Since the processing of a typical theory is likely
to result in a number of predictions it is necessary eo be able
to order them so that predictions most likely to be correct or
most likely to yield important information will be acted upon
Page 47
first. For example, it is more important to fill a gap between
two islands than to extend a single islabd, since by filling the
gap one can check the consistency of information which was
locally good in each island individually but may not be
consistent when they are joined. Since two words can occur
together in (usually) many contexts but longer sequences are
generally more restrictive, adding a word to a one word island is
likely to be less profitable in terms of the number of possible
paths which are eliminated by the addition than adding a word to
a multi-word island.
It is up to the syntactic component to indicate to the
control component the relative importance attached to each notice
and monitor; the higher the score, the stronger the prediction.
Several factors influence the score attached to predictions.
One is the length of the island to which the prediction is
attached. One word islands, if they are processed at all, yield
very little information anc many predictions, hence the
predictions are not scored high. Proposals are less important if
there is already a noticeable word in the word lattice (since
that word is acoustically better than the word to be proposed,
else it would have been found earlier. However, if a proposal
fills a gap between two islands, it is given a higher score.
Notices are boosted in importance if an entire constituent may be
added and penalized if they will add onto a one word island.
Scores range from 0 to 95 for proposals, 0 to 40 for notices, and
0 to 15 for monitors.
Page 48
These scores appear to work fairly well with the rest of the
BBN SPEECHLIS system, but have been developed by a process of
interaction with the other components (in order to make the
scores of syntactic predictions commensurate with those of
semantic predictions) and may be changed considerably as the
entire system evolves.
Small syntactic classes (e.g. determrners and prepositions)
are proposed in their entirety (that is, their elvments are to be
enumerated and given to the lexical matching component for
verification) if the island which monitored for them is more than
one word long. If a gap between two islands is small enough for
just one word and if a syntactic class has been monitored for
from both sides of the gap, it is proposed ir its entirety also.
</bodyText>
<subsectionHeader confidence="0.674451">
Section 6
Examples and Result
</subsectionHeader>
<bodyText confidence="0.967003117647059">
SPARSER is written in INTERLIgP and runs on a PDP-10 under
the TENEX operating system The program and initial data
structures occupy approximately 90000 words of virtual memory.
(The other components of the BBN speech understanding system
occupy separate forks from the syntactic component.)
Page 49
At the time the &apos;examples in this section were run, the
algorithm controlling the decision-making process in the control
component was undergoing revision and was not solidified into a
function which could operate automatically. Rather, there were a
number of primitive operations such as scanning an utterance (or
some specified portion of it), creating theories, calling SPARSER
with a theory or event, calling for the processing of proposals,
etc., which could be invoked by a human simulator The following
examples were produced in this mode, with the user acting as the
control component in a way which could be modelled by later
implementation.
</bodyText>
<subsectionHeader confidence="0.554658">
Several conventions have been used in tracing the operation
</subsectionHeader>
<bodyText confidence="0.966667352941176">
of SPARSER. Configurations are represented as
NUMBER : STATE : POSITION (SCORE). For example, the
configuration written as 30:NP/HEAD:23(39) is the configuration
for state NP/HEAD at position 23 which has been given the
(unique) number 30 and which currently has a score of 39. The
creation of a transition is indicated by naming the type of arc
causing the transition, the (unique) number of the transition,
and the configurations at each end of the transition. For
example,
CAT N TRANS #9 FROM 14:NP/DET:6(1) TO 15:NP/DET:19(4).
Annotations have been inserted within brackets { }, typeout
in upper case was produced by the program.
Page 50
EXAMPLE 1
This example parallels that given in Section Four. A word
lattice was artificially created which contained only the
following three word matches:
</bodyText>
<table confidence="0.739309666666667">
(1 SUMMER 12 16 100)
(2 WINTER 12 16 100)
(3 TRIP 16 21 100 -S)
</table>
<bodyText confidence="0.99970575">
(In this version or the system, regular inflectional endings are
included in word matches after the element representing the
score, hence the somewhat peculiar word match for the word
&amp;quot;trips&amp;quot;.) Two theories were constructed, one for word matches 2
and 3, the other for 1 and 3. What follows is an annotated (but
otherwise unedited except for considerations of spacing)
transcript of SPARSER processing these two theories in sequence,
using the MINIGRAMMAR of Figure 3.3 and Appendix I.
</bodyText>
<sectionHeader confidence="0.71998" genericHeader="method">
SPARSER PROCESSING THEORY&apos; 1:
0 12 WINTER 16 TRIP -S 21 30
</sectionHeader>
<subsectionHeader confidence="0.872385">
(This is a linear representation of the thegry being
</subsectionHeader>
<bodyText confidence="0.977952">
processed. The endpoints are 0 and 30, but the words
occupy-only the middle part of the utterance.)
</bodyText>
<sectionHeader confidence="0.985558" genericHeader="method">
STARTING AN ISLAND
&amp;quot;WINTER&amp;quot; TRYING CAT N ARC FROM NP/ADJ TO NP/ADJ
</sectionHeader>
<bodyText confidence="0.900734">
(This is the first of two arcs retrieved from the index
</bodyText>
<sectionHeader confidence="0.489207" genericHeader="method">
tables }
CAT N TRANS #1 FROM 1:NP/ADJ:12(1) TO 2:NP/ADJ:16(3)
</sectionHeader>
<bodyText confidence="0.816072333333333">
(The first transition is created, and since there is a
CAT N arc which enters state NP/ADJ, a monitor is set up
to monitor for nouns which end at position 12.}
</bodyText>
<sectionHeader confidence="0.936134" genericHeader="method">
ENDING AT 12:
MONITORING [ N ]
JUMP TRANS #2 FROM 3:NP/QUANT:12(1) TO 1:NP/ADJ:12(1)
</sectionHeader>
<bodyText confidence="0.9367472">
(Now the lead-in transitions are being created, along
with the monitors for syntactic categories which may
precede the newly constructed configurations.
Configurations along the lead-in path are all assigned a
score of 1.}
</bodyText>
<sectionHeader confidence="0.500929" genericHeader="method">
ElEkNoCIPt QUANT ]
</sectionHeader>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.232576">
<note confidence="0.877806">Journal of Computational Linguistics 45</note>
<title confidence="0.859896">SYNTAX IN AUTOMATIC SPEECH UNDERSTANDLNG</title>
<author confidence="0.947218">MADELEINE BATES</author>
<affiliation confidence="0.969178333333333">Boston University and Beranek and Inc.</affiliation>
<address confidence="0.9990345">50 Moulton Street Cambridge, Massachusetts 02138</address>
<note confidence="0.8757627">research principally by the Advanced Projects Agency of the Department of Defense (ARRA Order No. and was monitored by ONR under No. N00014-75-c- Partial support of the author by NSF GS-39834 to University is acknowledged. Copyright 91976 Association for Computational Linguistics Pare 2 Table of Contents Page Section 3</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>or noticed</author>
</authors>
<title>Concretely, an event is a piece of data consistiniz of: 1) the old theory that proposed or set a monitor for the event</title>
<marker>noticed, </marker>
<rawString>or noticed. Concretely, an event is a piece of data consistiniz of: 1) the old theory that proposed or set a monitor for the event</rawString>
</citation>
<citation valid="false">
<title>2) something to be added to the theory (a new word match or constituent) 4) the arc in the grammar which will process the new information 4) the confipuration ih the old theory which will be at one end of the transition created by the above arc</title>
<marker></marker>
<rawString>2) something to be added to the theory (a new word match or constituent) 4) the arc in the grammar which will process the new information 4) the confipuration ih the old theory which will be at one end of the transition created by the above arc</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>