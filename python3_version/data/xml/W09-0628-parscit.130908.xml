<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003452">
<title confidence="0.9985925">
Report on the First NLG Challenge on
Generating Instructions in Virtual Environments (GIVE)
</title>
<author confidence="0.997883">
Donna Byron Alexander Koller Kristina Striegnitz
</author>
<affiliation confidence="0.99962">
Northeastern University Saarland University Union College
</affiliation>
<email confidence="0.971055">
dbyron@ccs.neu.edu koller@mmci.uni-saarland.de striegnk@union.edu
</email>
<author confidence="0.99207">
Justine Cassell Robert Dale Johanna Moore Jon Oberlander
</author>
<affiliation confidence="0.999282">
Northwestern University Macquarie University University of Edinburgh University of Edinburgh
</affiliation>
<email confidence="0.993885">
justine@northwestern.edu Robert.Dale@mq.edu.au J.Moore@ed.ac.uk J.Oberlander@ed.ac.uk
</email>
<sectionHeader confidence="0.997338" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976">
We describe the first installment of the
Challenge on Generating Instructions in
Virtual Environments (GIVE), a new
shared task for the NLG community. We
motivate the design of the challenge, de-
scribe how we carried it out, and discuss
the results of the system evaluation.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999953090909091">
This paper reports on the methodology and results
of the First Challenge on Generating Instructions
in Virtual Environments (GIVE-1), which we ran
from March 2008 to February 2009. GIVE is a
new shared task for the NLG community. It pro-
vides an end-to-end evaluation methodology for
NLG systems that generate instructions which are
meant to help a user solve a treasure-hunt task in a
virtual 3D world. The most innovative aspect from
an NLG evaluation perspective is that the NLG
system and the user are connected over the Inter-
net. This makes it possible to cheaply collect large
amounts of evaluation data.
Five NLG systems were evaluated in GIVE-
1 over a period of three months from November
2008 to February 2009. During this time, we
collected 1143 games that were played by users
from 48 countries. As far as we know, this makes
GIVE-1 the largest evaluation effort in terms of
experimental subjects ever. We have evaluated the
five systems both on objective measures (success
rate, completion time, etc.) and subjective mea-
sures which were collected by asking the users to
fill in a questionnaire.
GIVE-1 was intended as a pilot experiment in
order to establish the validity of the evaluation
methodology and understand the challenges in-
volved in the instruction-giving task. We believe
that we have achieved these purposes. At the same
time, we provide evaluation results for the five
NLG systems which will help their developers im-
prove them for participation in a future challenge,
GIVE-2. GIVE-2 will retain the successful aspects
of GIVE-1, while refining the task to emphasize
aspects that we found to be challenging. We invite
the ENLG community to participate in designing
GIVE-2.
Plan of the paper. The paper is structured as
follows. In Section 2, we will describe and moti-
vate the GIVE Challenge. In Section 3, we will
then describe the evaluation method and infras-
tructure for the challenge. Section 4 reports on
the evaluation results. Finally, we conclude and
discuss future work in Section 5.
</bodyText>
<sectionHeader confidence="0.991684" genericHeader="method">
2 The GIVE Challenge
</sectionHeader>
<bodyText confidence="0.99994416">
In the GIVE scenario, subjects try to solve a trea-
sure hunt in a virtual 3D world that they have not
seen before. The computer has a complete sym-
bolic representation of the virtual world. The chal-
lenge for the NLG system is to generate, in real
time, natural-language instructions that will guide
the users to the successful completion of their task.
Users participating in the GIVE evaluation
start the 3D game from our website at www.
give-challenge.org. They then see a 3D
game window as in Fig. 1, which displays instruc-
tions and allows them to move around in the world
and manipulate objects. The first room is a tuto-
rial room where users learn how to interact with
the system; they then enter one of three evaluation
worlds, where instructions for solving the treasure
hunt are generated by an NLG system. Users can
either finish a game successfully, lose it by trig-
gering an alarm, or cancel the game. This result is
stored in a database for later analysis, along with a
complete log of the game.
Complete maps of the game worlds used in the
evaluation are shown in Figs. 3–5: In these worlds,
players must pick up a trophy, which is in a wall
safe behind a picture. In order to access the tro-
</bodyText>
<note confidence="0.91605">
Proceedings of the 12th European Workshop on Natural Language Generation, pages 165–173,
Athens, Greece, 30 – 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.996518">
165
</page>
<figureCaption confidence="0.980286">
Figure 1: What the user sees when playing with
the GIVE Challenge.
</figureCaption>
<bodyText confidence="0.999922571428572">
phy, they must first push a button to move the pic-
ture to the side, and then push another sequence of
buttons to open the safe. One floor tile is alarmed,
and players lose the game if they step on this tile
without deactivating the alarm first. There are also
a number of distractor buttons which either do
nothing when pressed or set off an alarm. These
distractor buttons are intended to make the game
harder and, more importantly, to require appropri-
ate reference to objects in the game world. Finally,
game worlds contained a number of objects such
as chairs and flowers that did not bear on the task,
but were available for use as landmarks in spatial
descriptions generated by the NLG systems.
</bodyText>
<subsectionHeader confidence="0.997855">
2.1 Why a new NLG evaluation paradigm?
</subsectionHeader>
<bodyText confidence="0.999969428571429">
The GIVE Challenge addresses a need for a new
evaluation paradigm for natural language gener-
ation (NLG). NLG systems are notoriously hard
to evaluate. On the one hand, simply compar-
ing system outputs to a gold standard using auto-
matic comparison algorithms has limited value be-
cause there can be multiple generated outputs that
are equally good. Finding metrics that account
for this variability and produce results consistent
with human judgments and task performance mea-
sures is difficult (Belz and Gatt, 2008; Stent et
al., 2005; Foster, 2008). Human assessments of
system outputs are preferred, but lab-based eval-
uations that allow human subjects to assess each
aspect of the system’s functionality are expensive
and time-consuming, thereby favoring larger labs
with adequate resources to conduct human sub-
jects studies. Human assessment studies are also
difficult to replicate across sites, so system devel-
opers that are geographically separated find it dif-
ficult to compare different approaches to the same
problem, which in turn leads to an overall diffi-
culty in measuring progress in the field.
The GIVE-1 evaluation was conducted via a
client/server architecture which allows any user
with an Internet connection to provide system
evaluation data. Internet-based studies have been
shown to provide generous amounts of data in
other areas of AI (von Ahn and Dabbish, 2004;
Orkin and Roy, 2007). Our implementation allows
smaller teams to develop a system that will partici-
pate in the challenge, without taking on the burden
of running the human evaluation experiment, and
it provides a direct comparison of all participating
systems on the same evaluation data.
</bodyText>
<subsectionHeader confidence="0.999273">
2.2 Why study instruction-giving?
</subsectionHeader>
<bodyText confidence="0.999876823529412">
Next to the Internet-based data collection method,
GIVE also differs from other NLG challenges by
its emphasis on generating instructions in a vir-
tual environment and in real time. This focus on
instruction giving is motivated by a growing in-
terest in dialogue-based agents for situated tasks
such as navigation and 3D animations. Due to its
appeal to younger students, the task can also be
used as a pedagogical exercise to stimulate interest
among secondary-school students in the research
challenges found in NLG or Computational Lin-
guistics more broadly.
Embedding the NLG task in a virtual world en-
courages the participating research teams to con-
sider communication in a situated setting. This
makes the NLG task quite different than in other
NLG challenges. For example, experiments have
shown that human instruction givers make the in-
struction follower move to a different location in
order to use a simpler referring expression (RE)
(Stoia et al., 2006). That is, RE generation be-
comes a very different problem than the classi-
cal non-situated Dale &amp; Reiter style RE genera-
tion, which focuses on generating REs that are sin-
gle noun phrases in the context of an unchanging
world.
On the other hand, because the virtual environ-
ments scenario is so open-ended, it – and specif-
ically the instruction-giving task – can potentially
be of interest to a wide range of NLG researchers.
This is most obvious for research in sentence plan-
ning (GRE, aggregation, lexical choice) and real-
ization (the real-time nature of the task imposes
high demands on the system’s efficiency). But if
</bodyText>
<page confidence="0.993665">
166
</page>
<figure confidence="0.97417">
NLG Server
Game Client
Matchmaker
</figure>
<bodyText confidence="0.999200857142857">
extended to two-way dialog, the task can also in-
volve issues of prosody generation (i.e., research
on text/concept-to-speech generation), discourse
generation, and human-robot interaction. Finally,
the game world can be scaled to focus on specific
issues in NLG, such as the generation of REs or
the generation of navigation instructions.
</bodyText>
<sectionHeader confidence="0.938167" genericHeader="method">
3 Evaluation Method and Logistics
</sectionHeader>
<bodyText confidence="0.999998333333333">
Now we describe the method we applied to obtain
experimental data, and sketch the software infras-
tructure we developed for this purpose.
</bodyText>
<subsectionHeader confidence="0.999934">
3.1 Software architecture
</subsectionHeader>
<bodyText confidence="0.9997212">
A crucial aspect of the GIVE evaluation methodol-
ogy is that it physically separates the user and the
NLG system and connects them over the Internet.
To achieve this, the GIVE software infrastructure
consists of three components (shown in Fig. 2):
</bodyText>
<listItem confidence="0.978407">
1. the client, which displays the 3D world to
users and allows them to interact with it;
2. the NLG servers, which generate the natural-
language instructions; and
3. the Matchmaker, which establishes connec-
tions between clients and NLG servers.
</listItem>
<bodyText confidence="0.99975955">
These three components run on different ma-
chines. The client is downloaded by users from
our website and run on their local machine; each
NLG server is run on a server at the institution
that implemented it; and the Matchmaker runs on
a central server we provide. When a user starts the
client, it connects to the Matchmaker and is ran-
domly assigned an NLG server and a game world.
The client and NLG server then communicate over
the course of one game. At the end of the game,
the client displays a questionnaire to the user, and
the game log and questionnaire data are uploaded
to the Matchmaker and stored in a database. Note
that this division allows the challenge to be con-
ducted without making any assumptions about the
internal structure of an NLG system.
The GIVE software is implemented in Java and
available as an open-source Google Code project.
For more details about the software, see (Koller et
al., 2009).
</bodyText>
<subsectionHeader confidence="0.997717">
3.2 Subjects
</subsectionHeader>
<bodyText confidence="0.9880235">
Participants were recruited using email distribu-
tion lists and press releases posted on the internet.
</bodyText>
<figureCaption confidence="0.998273">
Figure 2: The GIVE architecture.
</figureCaption>
<bodyText confidence="0.999944217391304">
Collecting data from anonymous users over the
Internet presents a variety of issues that a lab-
based experiment does not. An Internet-based
evaluation skews the demographic of the subject
pool toward people who use the Internet, but prob-
ably no more so than if recruiting on a college
campus. More worrisome is that, without a face-
to-face meeting, the researcher has less confidence
in the veracity of self-reported demographic data
collected from the subject. For the purposes of
NLG software, the most important demographic
question is the subject’s fluency in English. Play-
ers of the GIVE 2009 challenge were asked to self-
report their command of English, age, and com-
puter experience. English proficiency did interact
with task completion, which leads us to conclude
that users were honest about their level of English
proficiency. See section 4.4 below for a discus-
sion of this interaction. All-in-all, we feel that the
advantage gained from the large increase in the
size of the subject pool offsets any disadvantage
accrued from the lack of accurate demographic in-
formation.
</bodyText>
<subsectionHeader confidence="0.997225">
3.3 Materials
</subsectionHeader>
<bodyText confidence="0.999984266666667">
Figs. 3–5 show the layout of the three evaluation
worlds. The worlds were intended to provide vary-
ing levels of difficulty for the direction-giving sys-
tems and to focus on different aspects of the prob-
lem. World 1 is very similar to the development
world that the research teams were given to test
their system on. World 2 was intended to focus
on object descriptions - the world has only one
room which is full of objects and buttons, many of
which cannot be distinguished by simple descrip-
tions. World 3, on the other hand, puts more em-
phasis on navigation directions as the world has
many interconnected rooms and hallways.
The difference between the worlds clearly bears
out in the task completion rates reported below.
</bodyText>
<page confidence="0.940298">
167
</page>
<figure confidence="0.999551894736842">
lamp
couch
safe
lamp
alarm
alarm
safe
chair
tutorial room
plant
tutorial room
plant
chair
chair
alarm
plant
lamp
safe
tutorial room
</figure>
<figureCaption confidence="0.998031">
Figure 3: World 1 Figure 4: World 2 Figure 5: World 3
</figureCaption>
<subsectionHeader confidence="0.595072">
3.4 Timeline
</subsectionHeader>
<bodyText confidence="0.999972090909091">
After the GIVE Challenge was publicized in
March 2008, eight research teams signed up for
participation. We distributed an initial version of
the GIVE software and a development world to
these teams. In the end, four teams submitted
NLG systems. These were connected to a cen-
tral Matchmaker instance that ran for about three
months, from 7 November 2008 to 5 February
2009. During this time, we advertised participa-
tion in the GIVE Challenge to the public in order
to obtain experimental subjects.
</bodyText>
<subsectionHeader confidence="0.81095">
3.5 NLG systems
</subsectionHeader>
<bodyText confidence="0.986974">
Five NLG systems were evaluated in GIVE-1:
</bodyText>
<listItem confidence="0.892913444444445">
1. one system from the University of Texas at
Austin (“Austin” in the graphics below);
2. one system from Union College in Schenec-
tady, NY (“Union”);
3. one system from the Universidad Com-
plutense de Madrid (“Madrid”);
4. two systems from the University of Twente:
one serious contribution (“Twente”) and one
more playful one (“Warm-Cold”).
</listItem>
<bodyText confidence="0.9994735">
Of these systems, “Austin” can serve as a base-
line: It computes a plan consisting of the actions
the user should take to achieve the goal, and at
each point in the game, it realizes the first step
in this plan as a single instruction. The “Warm-
Cold” system generates very vague instructions
that only tell the user if they are getting closer
(“warmer”) to their next objective or if they are
moving away from it (“colder”). We included this
system in the evaluation to verify whether the eval-
uation methodology would be able to distinguish
such an obviously suboptimal instruction-giving
strategy from the others.
Detailed descriptions of these systems
as well as each team’s own analysis of
the evaluation results can be found at
http://www.give-challenge.org/
research/give-1.
</bodyText>
<sectionHeader confidence="0.999974" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999974666666667">
We now report on the results of GIVE-1. We start
with some basic demographics; then we discuss
objective and subjective evaluation measures.
Notice that some of our evaluation measures are
in tension with each other: For instance, a system
which gives very low-level instructions (“move
forward”; “ok, now move forward”; “ok, now turn
left”), such as the “Austin” baseline, will lead the
user to completing the task in a minimum number
of steps; but it will require more instructions than
a system that aggregates these. This is intentional,
and emphasizes both the pilot experiment char-
acter of GIVE-1 and our desire to make GIVE a
friendly comparative challenge rather than a com-
petition with a clear winner.
</bodyText>
<subsectionHeader confidence="0.955714">
4.1 Demographics
</subsectionHeader>
<bodyText confidence="0.999898583333333">
Over the course of three months, we collected
1143 valid games. A game counted as valid if the
game client didn’t crash, the game wasn’t marked
as a test game by the developers, and the player
completed the tutorial.
Of these games, 80.1% were played by males
and 9.9% by females; a further 10% didn’t specify
their gender. The players were widely distributed
over countries: 37% connected from an IP address
in the US, 33% from an IP address in Germany,
and 17% from China; Canada, the UK, and Aus-
tria also accounted for more than 2% of the partic-
</bodyText>
<page confidence="0.968034">
168
</page>
<figure confidence="0.998578">
150,0
112,5
75,0
37,5
0
</figure>
<figureCaption confidence="0.999572">
Figure 6: Histogram of the connections per day.
</figureCaption>
<listItem confidence="0.999297222222222">
• task success (Did the player get the trophy?)
• instructions (Number of instructions pro-
duced by the NLG system.*)
• steps (Number of all player actions.*)
• actions (Number of object manipulation
action.*)
• second (Time in seconds.*)
* Measured from the end of the tutorial until the
end of the game.
</listItem>
<figureCaption confidence="0.999357">
Figure 7: Objective measurements
</figureCaption>
<figure confidence="0.985169">
Nov 7 Dec 1 Jan 1 Feb 1
Feb 5
German
press release
US
press release
covered by
Chinese blog
posted to
SIGGEN list
# games per day
</figure>
<bodyText confidence="0.999516363636364">
ipants each, and the remaining 2% of participants
connected from 42 further countries. This imbal-
ance stems from very successful press releases that
were issued in Germany and the US and which
were further picked up by blogs, including one
in China. Nevertheless, over 90% of the partici-
pants who answered this question self-rated their
English proficiency as “good” or better. About
75% of users connected with a client running on
Windows, with the rest split about evenly among
Linux and Mac OS X.
The effect of the press releases is also plainly
visible if we look at the distribution of the valid
games over the days from November 7 to Febru-
ary 5 (Fig. 6). There are huge peaks at the
very beginning of the evaluation period, coincid-
ing with press releases through Saarland Univer-
sity in Germany and Northwestern University in
the US, which were picked up by science and tech-
nology blogs on the Web. The US peak contains
a smaller peak of connections from China, which
were sparked by coverage in a Chinese blog.
</bodyText>
<subsectionHeader confidence="0.985078">
4.2 Objective measures
</subsectionHeader>
<bodyText confidence="0.999954083333333">
We then extracted objective and subjective mea-
surements from the valid games. The objective
measures are summarized in Fig. 7. For each sys-
tem and game world, we measured the percent-
age of games which the users completed success-
fully. Furthermore, we counted the numbers of in-
structions the system sent to the user, measured
the time until task completion, and counted the
number of low-level steps executed by the user
(any key press, to either move or manipulate an
object) as well as the number of task-relevant ac-
tions (such as pushing a button to open a door).
</bodyText>
<figure confidence="0.994929727272727">
40% 71% 35% 73% 18%
task A A
success B B
C
83.2 58.3 121.2 80.3 190.0
A
instructions B B
C
D
103.6 124.3 160.9 117.5 307.4
A A
steps B B
C
D
11.2 8.7 14.3 9.0 14.3
actions B A A
C C
129.3 174.8 207.0 175.2 312.2
A
seconds B B
C
D
</figure>
<figureCaption confidence="0.889259">
Figure 8: Objective measures by system. Task
success is reported as the percentage of suc-
cessfully completed games. The other measures
are reported as the mean number of instruc-
tions/steps/actions/seconds, respectively. Letters
group indistinguishable systems; systems that
don’t share a letter were found to be significantly
different with p &lt; 0.05.
</figureCaption>
<figure confidence="0.9972828">
Twente
Union
Austin
Madrid
Warm-Cold
</figure>
<page confidence="0.995676">
169
</page>
<bodyText confidence="0.999972464285714">
To ensure comparability, we only counted success-
fully completed games for all these measures, and
only started counting when the user left the tutorial
room. Crucially, all objective measures were col-
lected completely unobtrusively, without requiring
any action on the user’s part.
Fig. 8 shows the results of these objective mea-
sures. This figure assigns systems to groups A,
B, etc. for each evaluation measure. Systems in
group A are better than systems in group B, etc.;
if two systems don’t share the same letter, the dif-
ference between these two systems is significant
with p &lt; 0.05. Significance was tested using a
x2-test for task success and ANOVAs for instruc-
tions, steps, actions, and seconds. These were fol-
lowed by post-hoc tests (pairwise x2 and Tukey)
to compare the NLG systems pairwise.
Overall, there is a top group consisting of
the Austin, Madrid, and Union systems: While
Madrid and Union outperform Austin on task suc-
cess (with 70 to 80% of successfully completed
games, depending on the world), Austin signifi-
cantly outperforms all other systems in terms of
task completion time. As expected, the Warm-
Cold system performs significantly worse than all
others in almost all categories. This confirms the
ability of the GIVE evaluation method to distin-
guish between systems of very different qualities.
</bodyText>
<subsectionHeader confidence="0.999499">
4.3 Subjective measures
</subsectionHeader>
<bodyText confidence="0.999788666666667">
The subjective measures, which were obtained by
asking the users to fill in a questionnaire after each
game, are shown in Fig. 9. Most of the questions
were answered on 5-point Likert scales (“overall”
on a 7-point scale); the “informativity” and “tim-
ing” questions had nominal answers. For each
question, the user could choose not to answer.
The results of the subjective measurements are
summarized in Fig. 10, in the same format as
above. We ran x2-tests for the nominal variables
informativity and timing, and ANOVAs for the
scale data. Again, we used post-hoc pairwise x2-
and Tukey-tests to compare the NLG systems to
each other one by one.
Here there are fewer significant differences be-
tween different groups than for the objective mea-
sures: For the “play again” category, there is
no significant difference at all. Nevertheless,
“Austin” is shown to be particularly good at navi-
gation instructions and timing, whereas “Madrid”
outperforms the rest of the field in “informativ-
</bodyText>
<table confidence="0.793638269230769">
7-point scale items:
overall: What is your overall evaluation of the quality of the
direction-giving system? (very bad 1 ... 7 very good)
5-point scale items:
task difficulty: How easy or difficult was the task for you to
solve? (very difficult 1 2 3 4 5 very easy)
goal clarity: How easy was it to understand what you were
supposed to do? (very difficult 1 2 3 4 5 very easy)
play again: Would you want to play this game again? (no
way! 1 2 3 4 5 yes please!)
instruction clarity: How clear were the directions? (totally
unclear 1 2 3 4 5 very clear)
instruction helpfulness: How effective were the directions at
helping you complete the task? (not effective 1 2 3 4 5
very effective)
choice of words: How easy to understand was the system’s
choice of wording in its directions to you? (totally un-
clear 1 2 3 4 5 very clear)
referring expressions: How easy was it to pick out which ob-
ject in the world the system was referring to? (very hard
1 2 3 4 5 very easy)
navigation instructions: How easy was it to navigate to a par-
ticular spot, based on the system’s directions? (very
hard 1 2 3 4 5 very easy)
friendliness: How would you rate the friendliness of the sys-
tem? (very unfriendly 1 2 3 4 5 very friendly)
</table>
<subsectionHeader confidence="0.893578">
Nominal items:
</subsectionHeader>
<bodyText confidence="0.98826">
informativity: Did you feel the amount of information you
were given was: too little /just right / too much
timing: Did the directions come ... too early /just at the right
time / too late
</bodyText>
<figureCaption confidence="0.996062">
Figure 9: Questionnaire items
</figureCaption>
<bodyText confidence="0.999750533333333">
ity”. In the overall subjective evaluation, the ear-
lier top group of Austin, Madrid, and Union is
confirmed, although the difference between Union
and Twente is not significant. However, “Warm-
Cold” again performs significantly worse than all
other systems in most measures. Furthermore, al-
though most systems perform similarly on “infor-
mativity” and “timing” in terms of the number of
users who judged them as “just right”, there are
differences in the tendencies: Twente and Union
tend to be overinformative, whereas Austin and
Warm-Cold tend to be underinformative; Twente
and Union tend to give their instructions too late,
whereas Madrid and Warm-Cold tend to give them
too early.
</bodyText>
<page confidence="0.977554">
170
</page>
<table confidence="0.386255085714286">
Twente
Austin
Madrid
Union
Warm-Cold
task 4.3 4.3 4.0 4.3 3.5
difficulty A A A A B
goal clarity 4.0 3.7 3.9 3.7 3.3
A A A A B
play again 2.8 2.6 2.4 2.9 2.5
A A A A A
instruction 4.0 3.6 3.8 3.6 3.0
clarity A A A B C
B B
instruction 3.8 3.9 3.6 3.7 2.9
helpfulness A A A A B
informativity 46% 68% 51% 56% 51%
B A B B B
overall 4.9 4.9 4.3 4.6 3.6
A A B A C
B
choice of 4.2 3.8 4.1 3.7 3.5
words A B A C C
C B
referring 3.4 3.9 3.7 3.7 3.5
expressions B A A A B
B B
navigation 4.6 4.0 4.0 3.7 3.2
instructions A B B B C
timing 78% 62% 60% 62% 49%
A B B B C
C
friendliness 3.4 3.8 3.1 3.6 3.1
A A B A B
B
</table>
<bodyText confidence="0.730557">
Figure 10: Subjective measures by system. Infor-
mativity and timing are reported as the percentage
of successfully completed games. The other mea-
sures are reported as the mean rating received by
the players. Letters group indistinguishable sys-
tems; systems that don’t share a letter were found
to be significantly different with p &lt; 0.05.
</bodyText>
<subsectionHeader confidence="0.999366">
4.4 Further analysis
</subsectionHeader>
<bodyText confidence="0.999959685714286">
In addition to the differences between NLG sys-
tems, there may be other factors which also influ-
ence the outcome of our objective and subjective
measures. We tested the following five factors:
evaluation world, gender, age, computer expertise,
and English proficiency (as reported by the users
on the questionnaire). We found that there is a sig-
nificant difference in task success rate for different
evaluation worlds and between users with different
levels of English proficiency.
The interaction graphs in Figs. 11 and 12 also
suggest that the NLG systems differ in their ro-
bustness with respect to these factors. x2-tests
that compare the success rate of each system in
the three evaluation worlds show that while the
instructions of Union and Madrid seem to work
equally well in all three worlds, the performance
of the other three systems differs dramatically be-
tween the different worlds. Especially World 2
was challenging for some systems as it required
relational object descriptions, such as the blue but-
ton on the left of another blue button.
The players’ English skills also affected the sys-
tems in different ways. While Austin, Madrid and
Warm Cold don’t manage to lead players with only
basic English skills to success as often as other
players, Union’s and Twente’s success rates do not
depend on the players’ English skills (x2-tests do
not find significant differences in success rate be-
tween players with different levels of English pro-
ficiency for these two systems). However, if we
remove the players with the lowest level of En-
glish proficiency, language skills do not have an
effect on the task success rate anymore for any of
the systems.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999986769230769">
In this document, we have described the first in-
stallment of the GIVE Challenge, our experimen-
tal methodology, and the results. Altogether, we
collected 1143 valid games for five NLG systems
over a period of three months. Given that this was
the first time we organized the challenge, that it
was meant as a pilot experiment from the begin-
ning, and that the number of games was sufficient
to get significant differences between systems on
a number of measures, we feel that GIVE-1 was a
success. We are in the process of preparing sev-
eral diagnostic utilities, such as heat maps and a
tool that lets the system developer replay an indi-
</bodyText>
<page confidence="0.997077">
171
</page>
<figureCaption confidence="0.985819">
Figure 11: Effect of the evaluation worlds on the
success rate of the NLG systems.
Figure 12: Effect of the players’ English skills on
the success rate of the NLG systems.
</figureCaption>
<bodyText confidence="0.999668946428572">
vidual game, which will help the participants gain
further insight into their NLG systems.
Nevertheless, there are a number of improve-
ments we will make to GIVE for future install-
ments. For one thing, the timing of the challenge
was not optimal: A number of colleagues would
have been interested in participating, but the call
for participation came too late for them to acquire
funding or interest students in time for summer
projects or MSc theses. Secondly, although the
software performed very well in handling thou-
sands of user connections, there were still game-
invalidating issues with the 3D graphics and the
networking code that were individually rare, but
probably cost us several hundred games. These
should be fixed for GIVE-2. At the same time,
we are investigating ways in which the networking
and matchmaking core of GIVE can be factored
out into a separate, challenge-independent system
on which other Internet-based challenges can be
built. Among other things, it would be straightfor-
ward to use the GIVE platform to connect two hu-
man users and observe their dialogue while solv-
ing a problem. Judicious variation of parameters
(such as the familiarity of users or the visibility of
an instruction giving avatar) would allow the con-
struction of new dialogue corpora along such lines.
Finally, GIVE-1 focused on the generation of
navigation instructions and referring expressions,
in a relatively simple world, without giving the
user a chance to talk back. The high success rate
of some systems in this challenge suggests that
we need to widen the focus for a future GIVE-
2 – by allowing dialogue, by making the world
more complex (e.g., allowing continuous rather
than discrete movements and turns), by making the
communication multi-modal, etc. Such extensions
would require only rather limited changes to the
GIVE software infrastructure. We plan to come to
a decision about such future directions for GIVE
soon, and are looking forward to many fruitful dis-
cussions about this at ENLG.
Acknowledgments. We are grateful to the par-
ticipants of the 2007 NSF/SIGGEN Workshop on
Shared Tasks and Evaluation in NLG and many
other colleagues for fruitful discussions while we
were designing the GIVE Challenge, and to the
organizers of Generation Challenges 2009 and
ENLG 2009 for their support and the opportunity
to present the results at ENLG. We also thank the
four participating research teams for their contri-
butions and their patience while we were working
out bugs in the GIVE software. The creation of
the GIVE infrastructure was supported in part by
a Small Projects grant from the University of Ed-
inburgh.
</bodyText>
<page confidence="0.996986">
172
</page>
<sectionHeader confidence="0.998341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999817038461539">
A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic eval-
uation measures for referring expression generation.
In Proceedings ofACL-08:HLT, Short Papers, pages
197–200, Columbus, Ohio.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for an
embodied conversational agent. In Proceedings of
INLG 2008, pages 95–103, Salt Fork, OH.
A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore,
J. Oberlander, and K. Striegnitz. 2009. The soft-
ware architecture for the first challenge on generat-
ing instructions in virtual environments. In Proceed-
ings of the EACL-09 Demo Session.
J. Orkin and D. Roy. 2007. The restaurant game:
Learning social behavior and language from thou-
sands of players online. Journal of Game Develop-
ment, 3(1):39–60.
A. Stent, M. Marge, and M. Singhai. 2005. Evaluating
evaluation methods for generation in the presence of
variation. In Proceedings of CICLing 2005.
L. Stoia, D. M. Shockley, D. K. Byron, and E. Fosler-
Lussier. 2006. Noun phrase generation for situated
dialogs. In Proceedings of INLG, Sydney.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proceedings of the ACM
CHI Conference.
</reference>
<page confidence="0.999104">
173
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.910373">
<title confidence="0.9886435">Report on the First NLG Challenge Generating Instructions in Virtual Environments (GIVE)</title>
<author confidence="0.999819">Donna Byron Alexander Koller Kristina Striegnitz</author>
<affiliation confidence="0.999753">Northeastern University Saarland University Union College</affiliation>
<email confidence="0.979863">dbyron@ccs.neu.edukoller@mmci.uni-saarland.destriegnk@union.edu</email>
<author confidence="0.998145">Justine Cassell Robert Dale Johanna Moore Jon Oberlander</author>
<affiliation confidence="0.999767">Northwestern University Macquarie University University of Edinburgh University of Edinburgh</affiliation>
<email confidence="0.957811">justine@northwestern.eduRobert.Dale@mq.edu.auJ.Moore@ed.ac.ukJ.Oberlander@ed.ac.uk</email>
<abstract confidence="0.99908325">We describe the first installment of the Challenge on Generating Instructions in Virtual Environments (GIVE), a new shared task for the NLG community. We motivate the design of the challenge, describe how we carried it out, and discuss the results of the system evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>A Gatt</author>
</authors>
<title>Intrinsic vs. extrinsic evaluation measures for referring expression generation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08:HLT, Short Papers,</booktitle>
<pages>197--200</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="5530" citStr="Belz and Gatt, 2008" startWordPosition="902" endWordPosition="905">e as landmarks in spatial descriptions generated by the NLG systems. 2.1 Why a new NLG evaluation paradigm? The GIVE Challenge addresses a need for a new evaluation paradigm for natural language generation (NLG). NLG systems are notoriously hard to evaluate. On the one hand, simply comparing system outputs to a gold standard using automatic comparison algorithms has limited value because there can be multiple generated outputs that are equally good. Finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult (Belz and Gatt, 2008; Stent et al., 2005; Foster, 2008). Human assessments of system outputs are preferred, but lab-based evaluations that allow human subjects to assess each aspect of the system’s functionality are expensive and time-consuming, thereby favoring larger labs with adequate resources to conduct human subjects studies. Human assessment studies are also difficult to replicate across sites, so system developers that are geographically separated find it difficult to compare different approaches to the same problem, which in turn leads to an overall difficulty in measuring progress in the field. The GIVE</context>
</contexts>
<marker>Belz, Gatt, 2008</marker>
<rawString>A. Belz and A. Gatt. 2008. Intrinsic vs. extrinsic evaluation measures for referring expression generation. In Proceedings ofACL-08:HLT, Short Papers, pages 197–200, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Foster</author>
</authors>
<title>Automated metrics that agree with human judgements on generated output for an embodied conversational agent.</title>
<date>2008</date>
<booktitle>In Proceedings of INLG</booktitle>
<pages>95--103</pages>
<location>Salt Fork, OH.</location>
<contexts>
<context position="5565" citStr="Foster, 2008" startWordPosition="910" endWordPosition="911">nerated by the NLG systems. 2.1 Why a new NLG evaluation paradigm? The GIVE Challenge addresses a need for a new evaluation paradigm for natural language generation (NLG). NLG systems are notoriously hard to evaluate. On the one hand, simply comparing system outputs to a gold standard using automatic comparison algorithms has limited value because there can be multiple generated outputs that are equally good. Finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult (Belz and Gatt, 2008; Stent et al., 2005; Foster, 2008). Human assessments of system outputs are preferred, but lab-based evaluations that allow human subjects to assess each aspect of the system’s functionality are expensive and time-consuming, thereby favoring larger labs with adequate resources to conduct human subjects studies. Human assessment studies are also difficult to replicate across sites, so system developers that are geographically separated find it difficult to compare different approaches to the same problem, which in turn leads to an overall difficulty in measuring progress in the field. The GIVE-1 evaluation was conducted via a c</context>
</contexts>
<marker>Foster, 2008</marker>
<rawString>M. E. Foster. 2008. Automated metrics that agree with human judgements on generated output for an embodied conversational agent. In Proceedings of INLG 2008, pages 95–103, Salt Fork, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Koller</author>
<author>D Byron</author>
<author>J Cassell</author>
<author>R Dale</author>
<author>J Moore</author>
<author>J Oberlander</author>
<author>K Striegnitz</author>
</authors>
<title>The software architecture for the first challenge on generating instructions in virtual environments.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL-09 Demo Session.</booktitle>
<contexts>
<context position="10288" citStr="Koller et al., 2009" startWordPosition="1677" endWordPosition="1680">onnects to the Matchmaker and is randomly assigned an NLG server and a game world. The client and NLG server then communicate over the course of one game. At the end of the game, the client displays a questionnaire to the user, and the game log and questionnaire data are uploaded to the Matchmaker and stored in a database. Note that this division allows the challenge to be conducted without making any assumptions about the internal structure of an NLG system. The GIVE software is implemented in Java and available as an open-source Google Code project. For more details about the software, see (Koller et al., 2009). 3.2 Subjects Participants were recruited using email distribution lists and press releases posted on the internet. Figure 2: The GIVE architecture. Collecting data from anonymous users over the Internet presents a variety of issues that a labbased experiment does not. An Internet-based evaluation skews the demographic of the subject pool toward people who use the Internet, but probably no more so than if recruiting on a college campus. More worrisome is that, without a faceto-face meeting, the researcher has less confidence in the veracity of self-reported demographic data collected from the</context>
</contexts>
<marker>Koller, Byron, Cassell, Dale, Moore, Oberlander, Striegnitz, 2009</marker>
<rawString>A. Koller, D. Byron, J. Cassell, R. Dale, J. Moore, J. Oberlander, and K. Striegnitz. 2009. The software architecture for the first challenge on generating instructions in virtual environments. In Proceedings of the EACL-09 Demo Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Orkin</author>
<author>D Roy</author>
</authors>
<title>The restaurant game: Learning social behavior and language from thousands of players online.</title>
<date>2007</date>
<journal>Journal of Game Development,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="6420" citStr="Orkin and Roy, 2007" startWordPosition="1040" endWordPosition="1043">esources to conduct human subjects studies. Human assessment studies are also difficult to replicate across sites, so system developers that are geographically separated find it difficult to compare different approaches to the same problem, which in turn leads to an overall difficulty in measuring progress in the field. The GIVE-1 evaluation was conducted via a client/server architecture which allows any user with an Internet connection to provide system evaluation data. Internet-based studies have been shown to provide generous amounts of data in other areas of AI (von Ahn and Dabbish, 2004; Orkin and Roy, 2007). Our implementation allows smaller teams to develop a system that will participate in the challenge, without taking on the burden of running the human evaluation experiment, and it provides a direct comparison of all participating systems on the same evaluation data. 2.2 Why study instruction-giving? Next to the Internet-based data collection method, GIVE also differs from other NLG challenges by its emphasis on generating instructions in a virtual environment and in real time. This focus on instruction giving is motivated by a growing interest in dialogue-based agents for situated tasks such</context>
</contexts>
<marker>Orkin, Roy, 2007</marker>
<rawString>J. Orkin and D. Roy. 2007. The restaurant game: Learning social behavior and language from thousands of players online. Journal of Game Development, 3(1):39–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stent</author>
<author>M Marge</author>
<author>M Singhai</author>
</authors>
<title>Evaluating evaluation methods for generation in the presence of variation.</title>
<date>2005</date>
<booktitle>In Proceedings of CICLing</booktitle>
<contexts>
<context position="5550" citStr="Stent et al., 2005" startWordPosition="906" endWordPosition="909">tial descriptions generated by the NLG systems. 2.1 Why a new NLG evaluation paradigm? The GIVE Challenge addresses a need for a new evaluation paradigm for natural language generation (NLG). NLG systems are notoriously hard to evaluate. On the one hand, simply comparing system outputs to a gold standard using automatic comparison algorithms has limited value because there can be multiple generated outputs that are equally good. Finding metrics that account for this variability and produce results consistent with human judgments and task performance measures is difficult (Belz and Gatt, 2008; Stent et al., 2005; Foster, 2008). Human assessments of system outputs are preferred, but lab-based evaluations that allow human subjects to assess each aspect of the system’s functionality are expensive and time-consuming, thereby favoring larger labs with adequate resources to conduct human subjects studies. Human assessment studies are also difficult to replicate across sites, so system developers that are geographically separated find it difficult to compare different approaches to the same problem, which in turn leads to an overall difficulty in measuring progress in the field. The GIVE-1 evaluation was co</context>
</contexts>
<marker>Stent, Marge, Singhai, 2005</marker>
<rawString>A. Stent, M. Marge, and M. Singhai. 2005. Evaluating evaluation methods for generation in the presence of variation. In Proceedings of CICLing 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Stoia</author>
<author>D M Shockley</author>
<author>D K Byron</author>
<author>E FoslerLussier</author>
</authors>
<title>Noun phrase generation for situated dialogs.</title>
<date>2006</date>
<booktitle>In Proceedings of INLG,</booktitle>
<location>Sydney.</location>
<contexts>
<context position="7684" citStr="Stoia et al., 2006" startWordPosition="1241" endWordPosition="1244">appeal to younger students, the task can also be used as a pedagogical exercise to stimulate interest among secondary-school students in the research challenges found in NLG or Computational Linguistics more broadly. Embedding the NLG task in a virtual world encourages the participating research teams to consider communication in a situated setting. This makes the NLG task quite different than in other NLG challenges. For example, experiments have shown that human instruction givers make the instruction follower move to a different location in order to use a simpler referring expression (RE) (Stoia et al., 2006). That is, RE generation becomes a very different problem than the classical non-situated Dale &amp; Reiter style RE generation, which focuses on generating REs that are single noun phrases in the context of an unchanging world. On the other hand, because the virtual environments scenario is so open-ended, it – and specifically the instruction-giving task – can potentially be of interest to a wide range of NLG researchers. This is most obvious for research in sentence planning (GRE, aggregation, lexical choice) and realization (the real-time nature of the task imposes high demands on the system’s </context>
</contexts>
<marker>Stoia, Shockley, Byron, FoslerLussier, 2006</marker>
<rawString>L. Stoia, D. M. Shockley, D. K. Byron, and E. FoslerLussier. 2006. Noun phrase generation for situated dialogs. In Proceedings of INLG, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L von Ahn</author>
<author>L Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACM CHI Conference.</booktitle>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>L. von Ahn and L. Dabbish. 2004. Labeling images with a computer game. In Proceedings of the ACM CHI Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>