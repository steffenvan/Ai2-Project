<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.992755">
Content Models with Attitude
</title>
<author confidence="0.998922">
Christina Sauper, Aria Haghighi, Regina Barzilay
</author>
<affiliation confidence="0.9982995">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.993739">
csauper@csail.mit.edu, me@aria42.com, regina@csail.mit.edu
</email>
<sectionHeader confidence="0.995622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998054375">
We present a probabilistic topic model for
jointly identifying properties and attributes of
social media review snippets. Our model
simultaneously learns a set of properties of
a product and captures aggregate user senti-
ments towards these properties. This approach
directly enables discovery of highly rated or
inconsistent properties of a product. Our
model admits an efficient variational mean-
field inference algorithm which can be paral-
lelized and run on large snippet collections.
We evaluate our model on a large corpus of
snippets from Yelp reviews to assess property
and attribute prediction. We demonstrate that
it outperforms applicable baselines by a con-
siderable margin.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999922066666667">
Online product reviews have become an increasingly
valuable and influential source of information for
consumers. Different reviewers may choose to com-
ment on different properties or aspects of a product;
therefore their reviews focus on different qualities of
the product. Even when they discuss the same prop-
erties, their experiences and, subsequently, evalua-
tions of the product can differ dramatically. Thus,
information in any single review may not provide
a complete and balanced view representative of the
product as a whole. To address this need, online re-
tailers often use simple aggregation mechanisms to
represent the spectrum of user sentiment. For in-
stance, product pages on Amazon prominently dis-
play the distribution of numerical scores across re-
</bodyText>
<figure confidence="0.196956">
Coherent property cluster
Incoherent property cluster
</figure>
<tableCaption confidence="0.868555">
Table 1: Example clusters of restaurant review snippets.
</tableCaption>
<bodyText confidence="0.96602703125">
The first cluster represents a coherent property of the un-
derlying product, namely the cocktail property, and as-
sesses distinctions in user sentiment. The latter cluster
simply shares a common attribute expression and does
not represent snippets discussing the same product prop-
erty. In this work, we aim to produce the first type of
property cluster with correct sentiment labeling.
views, providing access to reviews at different levels
of satisfaction.
The goal of our work is to provide a mechanism
for review content aggregation that goes beyond nu-
merical scores. Specifically, we are interested in
identifying fine-grained product properties across
reviews (e.g., battery life for electronics or pizza for
restaurants) as well as capturing attributes of these
properties, namely aggregate user sentiment.
For this task, we assume as input a set of prod-
uct review snippets (i.e., standalone phrases such as
“battery life is the best I’ve found”) rather than com-
plete reviews. There are many techniques for ex-
tracting this type of snippet in existing work; we use
the Sauper et al. (2010) system.
+ The martinis were very good.
The drinks - both wine and martinis - were tasty.
The wine list was pricey.
-
Their wine selection is horrible.
The sushi is the best I’ve ever had.
Best paella I’d ever had.
The fillet was the best steak we’d ever had.
It’s the best soup I’ve ever had.
+
</bodyText>
<page confidence="0.963286">
350
</page>
<note confidence="0.9793845">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 350–358,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999957517241379">
At first glance, this task can be solved using ex-
isting methods for review analysis. These methods
can effectively extract product properties from indi-
vidual snippets along with their corresponding sen-
timent. While the resulting property-attribute pairs
form a useful abstraction for cross-review analysis,
in practice direct comparison of these pairs is chal-
lenging.
Consider, for instance, the two clusters of restau-
rant review snippets shown in Figure 1. While both
clusters have many words in common among their
members, only the first describes a coherent prop-
erty cluster, namely the cocktail property. The snip-
pets of the latter cluster do not discuss a single prod-
uct property, but instead share similar expressions
of sentiment. To solve this issue, we need a method
which can correctly identify both property and sen-
timent words.
In this work, we propose an approach that jointly
analyzes the whole collection of product review
snippets, induces a set of learned properties, and
models the aggregate user sentiment towards these
properties. We capture this idea using a Bayesian
topic model where a set of properties and corre-
sponding attribute tendencies are represented as hid-
den variables. The model takes product review snip-
pets as input and explains how the observed text
arises from the latent variables, thereby connecting
text fragments with corresponding properties and at-
tributes.
The advantages of this formulation are twofold.
First, this encoding provides a common ground for
comparing and aggregating review content in the
presence of varied lexical realizations. For instance,
this representation allows us to directly compare
how many reviewers liked a given property of a
product. Second, our model yields an efficient
mean-field variational inference procedure which
can be parallelized and run on a large number of re-
view snippets.
We evaluate our approach in the domain of snip-
pets taken from restaurant reviews on Yelp. In this
collection, each restaurant has on average 29.8 snip-
pets representing a wide spectrum of opinions about
a restaurant. The evaluation we present demon-
strates that the model can accurately retrieve clusters
of review fragments that describe the same property,
yielding 20% error reduction over a standalone clus-
tering baseline. We also show that the model can ef-
fectively identify binary snippet attributes with 9.2%
error reduction over applicable baselines, demon-
strating that learning to identify attributes in the con-
text of other product reviews yields significant gains.
Finally, we evaluate our model on its ability to iden-
tify product properties for which there is significant
sentiment disagreement amongst user snippets. This
tests our model’s capacity to jointly identify proper-
ties and assess attributes.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999863457142857">
Our work on review aggregation has connections to
three lines of work in text analysis.
First, our work relates to research on extraction of
product properties with associated sentiment from
review text (Hu and Liu, 2004; Liu et al., 2005a;
Popescu et al., 2005). These methods identify rele-
vant information in a document using a wide range
of methods such as association mining (Hu and Liu,
2004), relaxation labeling (Popescu et al., 2005) and
supervised learning (Kim and Hovy, 2006). While
our method also extracts product properties and sen-
timent, our focus is on multi-review aggregation.
This task introduces new challenges which were
not addressed in prior research that focused on per-
document analysis.
A second related line of research is multi-
document review summarization. Some of
these methods directly apply existing domain-
independent summarization methods (Seki et al.,
2006), while others propose new methods targeted
for opinion text (Liu et al., 2005b; Carenini et al.,
2006; Hu and Liu, 2006; Kim and Zhai, 2009). For
instance, these summaries may present contrastive
view points (Kim and Zhai, 2009) or relay average
sentiment (Carenini et al., 2006). The focus of this
line of work is on how to select suitable sentences,
assuming that relevant review features (such as nu-
merical scores) are given. Since our emphasis is on
multi-review analysis, we believe that the informa-
tion we extract can benefit existing summarization
systems.
Finally, a number of approaches analyze review
documents using probabilistic topic models (Lu and
Zhai, 2008; Titov and McDonald, 2008; Mei et al.,
2007). While some of these methods focus primar-
</bodyText>
<page confidence="0.997689">
351
</page>
<bodyText confidence="0.9999595">
ily on modeling ratable aspects (Titov and McDon-
ald, 2008), others explicitly capture the mixture of
topics and sentiments (Mei et al., 2007). These ap-
proaches are capable of identifying latent topics in
the collection in opinion text (e.g., weblogs) as well
as associated sentiment. While our model captures
similar high-level intuition, it analyzes fine-grained
properties expressed at the snippet level, rather than
document-level sentiment. Delivering analysis at
such a fine granularity requires a new technique.
</bodyText>
<sectionHeader confidence="0.981822" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.980241695652174">
In this section, we discuss the core random variables
and abstractions of our model. We describe the gen-
erative models over these elements in Section 4.
Product: A product represents a reviewable ob-
ject. For the experiments in this paper, we use
restaurants as products.
Snippets: A snippet is a user-generated short se-
quence of tokens describing a product. Input snip-
pets are deterministically taken from the output of
the Sauper et al. (2010) system.
Property: A property corresponds to some fine-
grained aspect of a product. For instance, the snippet
“the pad thai was great” describes the pad thai prop-
erty. We assume that each snippet has a single prop-
erty associated with it. We assume a fixed number
of possible properties K for each product.
For the corpus of restaurant reviews, we assume
that the set of properties are specific to a given prod-
uct, in order to capture fine-grained, relevant proper-
ties for each restaurant. For example, reviews from a
sandwich shop may contrast the club sandwich with
the turkey wrap, while for a more general restau-
rant, the snippets refer to sandwiches in general. For
other domains where the properties are more consis-
tent, it is straightforward to alter our model so that
properties are shared across products.
Attribute: An attribute is a description of a prop-
erty. There are multiple attribute types, which may
correspond to semantic differences. We assume a
fixed, pre-specified number of attributes N. For
example, in the case of product reviews, we select
N = 2 attributes corresponding to positive and neg-
ative sentiment. In the case of information extrac-
tion, it may be beneficial to use numeric and alpha-
betic types.
One of the goals of this work in the review do-
main is to improve sentiment prediction by exploit-
ing correlations within a single property cluster. For
example, if there are already many snippets with the
attribute representing positive sentiment in a given
property cluster, additional snippets are biased to-
wards positive sentiment as well; however, data can
always override this bias.
Snippets themselves are always observed; the
goal of this work is to induce the latent property and
attribute underlying each snippet.
</bodyText>
<sectionHeader confidence="0.992134" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.993944382352941">
Our model generates the words of all snippets for
each product in a collection of products. We use
sz,�,&apos;&apos; to represent the wth word of the jth snippet
of the ith product. We use s to denote the collec-
tion of all snippet words. We also assume a fixed
vocabulary of words V .
We present an overview of our generative model
in Figure 1 and describe each component in turn:
Global Distributions: At the global level, we
draw several unigram distributions: a global back-
ground distribution BB and attribute distributions
BA for each attribute. The background distribution
is meant to encode stop-words and domain white-
noise, e.g., food in the restaurants domain. In this
domain, the positive and negative attribute distribu-
tions encode words with positive and negative senti-
ments (e.g., delicious or terrible).
Each of these distributions are drawn from Dirich-
let priors. The background distribution is drawn
from a symmetric Dirichlet with concentration
AB = 0.2. The positive and negative attribute dis-
tributions are initialized using seed words (V�..da
in Figure 1). These seeds are incorporated into
the attribute priors: a non-seed word gets E hyper-
parameter and a seed word gets E + AA, where
E = 0.25 and AA = 1.0.
Product Level: For the ith product, we draw
property unigram distributions B�,1� , ... , B�,� � for
each of the possible K product properties. The prop-
erty distribution represents product-specific content
distributions over properties discussed in reviews of
the product; for instance in the restaurant domains,
properties may correspond to distinct menu items.
Each B�,�� is drawn from a symmetric Dirichlet prior
</bodyText>
<page confidence="0.991344">
352
</page>
<table confidence="0.739577166666667">
Global Level:
- Draw background distribution θB ∼ DIRICHLET(λBV )
- For each attribute type a,
- Draw attribute distribution θaA ∼ DIRICHLET(EV + λAVseeda)
Product Level:
- For each product i,
- Draw property distributions θk P ∼ DIRICHLET(λP V) for k = 1, ... , K
- Draw property attribute binomial φi,k ∼ BETA(αA, βA) for k = 1, ... , K
- Draw property multinomial ψi ∼ DIRICHLET(λMK)
Snippet Level:
- For each snippet j in ith product,
- Draw snippet property Zi,jP ∼ ψi
</table>
<figure confidence="0.996767448979592">
P
- Draw snippet attribute Zi,j A ∼ φZij
- Draw sequence of word topic indicators Zi,j,w
W ∼ A|Zi,j,w−1
W
- Draw snippet word given property Zi,jP and attribute Zi,jA
θi,Zi,j
P , when Zi,j,w
P W = P
θZi,j
A , when Zi,j,w
A W = A
θB, when Zi,j,w
W = B
�
�� �
���
si,j,w ∼
Background word
distribution
θ
Attribute
Attribute word
distributions
θ A
Product
Z ,θ
ZA, θA
θ
Snippet
Property
multinomial
ψ
Snippet property
−1
Z Z
W −1 w w +1
θA
Z ZA
HMM over snippet words
Property
Property word
distributions
θ
Snippet attribute
Property attribute
binomials
Z +1
φ
</figure>
<figureCaption confidence="0.9862635">
Figure 1: A high-level verbal and graphical description for our model in Section 4. We use DIRICHLET(λV ) to denote
a finite Dirichlet prior where the hyper-parameter counts are a scalar times the unit vector of vocabulary items. For
the global attribute distribution, the prior hyper-parameter counts are a for all vocabulary items and λA for V3eed., the
vector of vocabulary items in the set of seed words for attribute a.
</figureCaption>
<bodyText confidence="0.997483708333333">
with hyper-parameter λP = 0.2.
For each property k = 1, ... , K. φi,k, we draw a
binomial distribution φi,k. This represents the dis-
tribution over positive and negative attributes for
that property; it is drawn from a beta prior using
hyper-parameters αA = 2 and βA = 2. We also
draw a multinomial ψi over K possible properties
from a symmetric Dirichlet distribution with hyper-
parameter λM = 1, 000. This distribution is used to
draw snippet properties.
Snippet Level: For the jth snippet of the ith prod-
uct, a property random variable Zi,jP is drawn ac-
cording to the multinomial ψi. Conditioned on this
choice, we draw an attribute Zi,jA (positive or nega-
tive) from the property attribute distribution φi,Zj,j
P .
Once the property Zi,jP and attribute Zi,jA have
been selected, the tokens of the snippet are gener-
ated using a simple HMM. The latent state underly-
ing a token, Zi,j,w
W , indicates whether the wth word
comes from the property distribution, attribute dis-
tribution, or background distribution; we use P, A,
or B to denote these respective values of Zi,j,w
</bodyText>
<equation confidence="0.610914333333333">
W .
The sequence Zi,j,1
W , ... , Zi,j,m
</equation>
<bodyText confidence="0.92135225">
W is generated us-
ing a first-order Markov model. The full transition
parameter matrix A parametrizes these decisions.
Conditioned on the underlying Zi,j,w
</bodyText>
<equation confidence="0.629393">
W , a word, si,j,w
is drawn from θi,jP , θi,Zi,j
A , or θB for the values P,A,
P
</equation>
<bodyText confidence="0.552671">
or B respectively.
</bodyText>
<sectionHeader confidence="0.999649" genericHeader="method">
5 Inference
</sectionHeader>
<bodyText confidence="0.9997935">
The goal of inference is to predict the snippet prop-
erty and attribute distributions over each snippet
given all the observed snippets P(Zi,jP , Zi,jA |s) for
all products i and snippets j. Ideally, we would like
to marginalize out nuisance random variables and
distributions. Specifically, we approximate the full
</bodyText>
<page confidence="0.996533">
353
</page>
<bodyText confidence="0.941829761904762">
model posterior using variational inference:1
P(0, OP, BB, OA, 0, |S) �
Q(0, OP, BB, OA, 0)
where,i, OP, 0 denote the collection of latent distri-
butions in our model. Here, we assume a full mean-
field factorization of the variational distribution; see
Figure 2 for the decomposition. Each variational
factor q(·) represents an approximation of that vari-
able’s posterior given observed random variables.
The variational distribution Q(·) makes the (incor-
rect) assumption that the posteriors amongst factors
are independent. The goal of variational inference is
to set factors q(·) so that it minimizes the KL diver-
gence to the true model posterior:
KL(P(p, OP, BB, OA, 0, |S)II
Q(?G, OP, BB, OA, 0)
We optimize this objective using coordinate descent
on the q(·) factors. Concretely, we update each fac-
tor by optimizing the above criterion with all other
factors fixed to current values. For instance, the up-
date for the factor q(Zi,j,w
</bodyText>
<equation confidence="0.9308128">
W ) takes the form:
q(Zi,j,w)
W �
&amp;quot;/&apos;
EQ/q(Z�y,-) lg P(0, OP, BB, OA, 0, S)
</equation>
<bodyText confidence="0.999812">
The full factorization of Q(·) and updates for
all random variable factors are given in Figure 2.
Updates of parameter factors are omitted; however
these are derived through simple counts of the ZA,
ZP, and ZW latent variables. For related discussion,
see Blei et al. (2003).
</bodyText>
<sectionHeader confidence="0.999586" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999828">
In this section, we describe in detail our data set and
present three experiments and their results.
Data Set Our data set consists of snippets from
Yelp reviews generated by the system described in
Sauper et al. (2010). This system is trained to ex-
tract snippets containing short descriptions of user
sentiment towards some aspect of a restaurant.2 We
</bodyText>
<footnote confidence="0.995653666666667">
1See Liang and Klein (2007) for an overview of variational tech-
niques.
2For exact training procedures, please reference that paper.
</footnote>
<table confidence="0.75632325">
The [P noodles ] and the [P meat ] were actually [+ pretty good ].
I [+ recommend ] the [P chicken noodle pho ].
The [P noodles ] were [- soggy ].
The [P chicken pho ] was also [+ good ].
The [P spring rolls ] and [P coffee ] were [+ good ] though.
The [P spring roll wrappers ] were a [- little dry tasting ].
My [+ favorites ] were the [P crispy spring rolls ].
The [P Crispy Tuna Spring Rolls ] are [+ fantastic ]!
The [P lobster roll ] my mother ordered was [- dry ] and [- scant ].
The [P portabella mushroom ] is my [+ go-to ] [P sandwich ].
The [P bread ] on the [P sandwich ] was [- stale ].
The slice of [P tomato ] was [- rather measly ].
The [P shumai ] and [P California maki sushi ] were [+ decent ].
The [P spicy tuna roll ] and [P eel roll ] were [+ perfect ].
The [P rolls ] with [P spicy mayo ] were [- not so great ].
I [+ love ] [P Thai rolls ].
</table>
<figureCaption confidence="0.990930833333333">
Figure 3: Example snippets from our data set, grouped
according to property. Property words are labeled P and
colored blue, NEGATIVE attribute words are labeled - and
colored red, and POSITIVE attribute words are labeled +
and colored green. The grouping and labeling are not
given in the data set and must be learned by the model.
</figureCaption>
<bodyText confidence="0.999959565217391">
select only the snippets labeled by that system as ref-
erencing food, and we ignore restaurants with fewer
than 20 snippets. There are 13,879 snippets in to-
tal, taken from 328 restaurants in and around the
Boston/Cambridge area. The average snippet length
is 7.8 words, and there are an average of 42.1 snip-
pets per restaurant, although there is high variance
in number of snippets for each restaurant. Figure 3
shows some example snippets.
For sentiment attribute seed words, we use 42 and
33 words for the positive and negative distributions
respectively. These are hand-selected based on the
restaurant review domain; therefore, they include
domain-specific words such as delicious and gross.
Tasks We perform three experiments to evaluate
our model’s effectiveness. First, a cluster predic-
tion task is designed to test the quality of the learned
property clusters. Second, an attribute analysis task
will evaluate the sentiment analysis portion of the
model. Third, we present a task designed to test
whether the system can correctly identify properties
which have conflicting attributes, which tests both
clustering and sentiment analysis.
</bodyText>
<equation confidence="0.860918">
min
Q(·)
</equation>
<page confidence="0.989936">
354
</page>
<figureCaption confidence="0.957585666666667">
Figure 2: The mean-field variational algorithm used during learning and inference to obtain posterior predictions over
snippet properties and attributes, as described in Section 5. Mean-field inference consists of updating each of the latent
variable factors as well as a straightforward update of latent parameters in round robin fashion.
</figureCaption>
<table confidence="0.7154243">
Mean-field Factorization N ! ⎛ ! ⎛
Q(O, OP, θB, OA, 0) = q(θB) Y Yn YK ⎝Y Y
Snippet Property Indicator a=1 q(θa A) ⎝ q(θi,k P )q(φi,k) q(Zi,j A )q(Zi,j q(Zi,j,w
P ) W )
i k=1 j w
Snippet Attribute Indicator
X X q(Zi,j,w
lg q(Zi,j A = a) = q(Zi,j P = k)Eq(φi,k) lg φi,k(a) + W = A)Eq(θaA) lg θaA(si,j,w)
k w
Word Topic Indicator
</table>
<equation confidence="0.8483916">
lg q(Zw,w = P) a lg P(ZW = P) + X q(Zi,j P = k)Eq(θi,k
k P ) lg θi,j P (si,j,w)
lg q(Ziv,w = A) a lg P(ZW = A) + X q(Zi,jA = a)Eq(θaA) lg θaA(si,j,w)
aE{+,−}
lg q(Zi,j,w
W = B) a lg P(ZW = B) + Eq(θB) lg θB(si,j,w)
X
lg q(Zi,jP = k) a Eq(ψi) lg ψi(p) +
w
q(Zi,j,w
W = P)Eq(θi,k
P ) lg θi,kP (si,j,w) +
N
X q(Zi,jA = a)Eq(φi,k) lg φi,k(a)
a=1
</equation>
<subsectionHeader confidence="0.987112">
6.1 Cluster prediction
</subsectionHeader>
<bodyText confidence="0.999975921052632">
The goal of this task is to evaluate the quality of
property clusters; specifically the Zi,jP variable in
Section 4. In an ideal clustering, the predicted clus-
ters will be cohesive (i.e., all snippets predicted for
a given property are related to each other) and com-
prehensive (i.e., all snippets which are related to a
property are predicted for it). For example, a snip-
pet will be assigned the property pad thai if and only
if that snippet mentions some aspect of the pad thai.
Annotation For this task, we use a set of gold
clusters over 3,250 snippets across 75 restaurants
collected through Mechanical Turk. In each task, a
worker was given a set of 25 snippets from a single
restaurant and asked to cluster them into as many
clusters as they desired, with the option of leaving
any number unclustered. This yields a set of gold
clusters and a set of unclustered snippets. For verifi-
cation purposes, each task was provided to two dif-
ferent workers. The intersection of both workers’
judgments was accepted as the gold standard, so the
model is not evaluated on judgments which disagree.
In total, there were 130 unique tasks, each of which
were provided to two workers, for a total output of
210 generated clusters.
Baseline The baseline for this task is a cluster-
ing algorithm weighted by TF*IDF over the data set
as implemented by the publicly available CLUTO
package.3 This baseline will put a strong connec-
tion between things which are lexically similar. Be-
cause our model only uses property words to tie
together clusters, it may miss correlations between
words which are not correctly identified as property
words. The baseline is allowed 10 property clusters
per restaurant.
We use the MUC cluster evaluation metric for
this task (Vilain et al., 1995). This metric measures
the number of cluster merges and splits required to
recreate the gold clusters given the model’s output.
</bodyText>
<footnote confidence="0.999542666666667">
3Available at http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview
with agglomerative clustering, using the cosine similarity
distance metric.
</footnote>
<page confidence="0.991547">
355
</page>
<table confidence="0.99996">
Precision Recall F1
Baseline 80.2 61.1 69.3
Our model 72.2 79.1 75.5
</table>
<tableCaption confidence="0.997893">
Table 2: Results using the MUC metric on the cluster
</tableCaption>
<figureCaption confidence="0.474737">
prediction task. Note that while the precision of the base-
line is higher, the recall and overall F1 of our model out-
weighs that. While MUC has a deficiency in that putting
everything into a single cluster will artificially inflate the
score, parameters on our model are set so that the model
uses the same number of clusters as the baseline system.
</figureCaption>
<bodyText confidence="0.996772045454546">
Therefore, it can concisely show how accurate our
clusters are as a whole. While it would be possible
to artificially inflate the score by putting everything
into a single cluster, the parameters on our model
and the likelihood objective are such that the model
prefers to use all available clusters, the same number
as the baseline system.
Results Results for our cluster prediction task are
in Table 2. While our system does suffer on preci-
sion in comparison to the baseline system, the recall
gains far outweigh this loss, for a total error reduc-
tion of 20% on the MUC measure.
The most common cause of poor cluster choices
in the baseline system is its inability to distinguish
property words from attribute words. For example,
if many snippets in a given restaurant use the word
delicious, there may end up being a cluster based on
that alone. Because our system is capable of dis-
tinguishing which words are property words (i.e.,
words relevant to clustering), it can choose clusters
which make more sense overall. We show an exam-
ple of this in Table 3.
</bodyText>
<subsectionHeader confidence="0.999989">
6.2 Attribute analysis
</subsectionHeader>
<bodyText confidence="0.999331727272727">
We also evaluate the system’s predictions of snip-
pet attribute using the predicted posterior over the
attribute distribution for the snippet (i.e., ZA ). For
this task, we consider the binary judgment to be sim-
ply the one with higher value in q(ZA) (see Sec-
tion 5). The goal of this task is to evaluate whether
our model correctly distinguishes attribute words.
Annotation For this task, we use a set of 260 to-
tal snippets from the Yelp reviews for 30 restaurants,
evenly split into a training and test sets of 130 snip-
pets each. These snippets are manually labeled POS-
The martini selection looked delicious
The s’mores martini sounded excellent
The martinis were good
The martinis are very good
The mozzarella was very fresh
The fish and various meets were very well made
The best carrot cake I’ve ever eaten
Carrot cake was deliciously moist
The carrot cake was delicious.
It was rich, creamy and delicious.
The pasta Bolognese was rich and robust.
</bodyText>
<tableCaption confidence="0.626878125">
Table 3: Example phrases from clusters in both the base-
line and our model. For each pair of clusters, the dashed
line indicates separation by the baseline model, while the
solid line indicates separation by our model. In the first
example, the baseline mistakenly clusters some snippets
about martinis with those containing the word very. In
the second example, the same occurs with the word deli-
cious.
</tableCaption>
<bodyText confidence="0.99939972">
ITIVE or NEGATIVE. Neutral snippets are ignored
for the purpose of this experiment.
Baseline We use two baselines for this task, one
based on a standard discriminative classifier and one
based on the seed words from our model.
The DISCRIMINATIVE baseline for this task is
a standard maximum entropy discriminative bi-
nary classifier over unigrams. Given enough snip-
pets from enough unrelated properties, the classifier
should be able to identify that words like great in-
dicate positive sentiment and those like bad indi-
cate negative sentiment, while words like chicken
are neutral and have no effect.
The SEED baseline simply counts the number of
words from the positive and negative seed lists used
by the model, Vseed+ and Vseed�. If there are more
words from Vseed+, the snippet is labeled positive,
and if there are more words from Vseed_, the snip-
pet is labeled negative. If there is a tie or there are
no seed words, we split the prediction. Because
the seed word lists are specifically slanted toward
restaurant reviews (i.e., they contain words such as
delicious), this baseline should perform well.
Results For this experiment, we measure the over-
all classification accuracy of each system (see Table
</bodyText>
<page confidence="0.99496">
356
</page>
<table confidence="0.9922115">
Accuracy
DISCRIMINATIVE baseline 75.9
SEED baseline 78.2
Our model 80.2
</table>
<tableCaption confidence="0.568526">
Table 4: Attribute prediction accuracy of the full system
compared to the DISCRIMINATIVE and SEED baselines.
The advantage of our system is its ability to distinguish
property words from attribute words in order to restrict
judgment to only the relevant terms.
The naan was hot and fresh
All the veggies were really fresh and crisp.
Perfect mix of fresh flavors and comfort food
The lo main smelled and tasted rancid
My grilled cheese sandwich was a little gross
Table 5: Examples of sentences correctly labeled by our
system but incorrectly labeled by the DISCRIMINATIVE
baseline; the key sentiment words are highlighted. No-
tice that these words are not the most common sentiment
words; therefore, it is difficult for the classifier to make a
correct generalization. Only two of these words are seed
words for our model (fresh and gross).
</tableCaption>
<bodyText confidence="0.997220857142857">
4). Our system outperforms both supervised base-
lines.
As in the cluster prediction case, the main flaw
with the DISCRIMINATIVE baseline system is its in-
ability to recognize which words are relevant for the
task at hand, in this case the attribute words. By
learning to separate attribute words from the other
words in the snippets, our full system is able to more
accurately judge their sentiment. Examples of these
cases are found in Table 5.
The obvious flaw in the SEED baseline is the in-
ability to pre-specify every possible sentiment word;
our model’s performance indicates that it is learning
something beyond just these basic words.
</bodyText>
<subsectionHeader confidence="0.994634">
6.3 Conflict identification
</subsectionHeader>
<bodyText confidence="0.99977425">
Our final task requires both correct cluster prediction
and correct sentiment judgments. In many domains,
it is interesting to know not only whether a product
is rated highly, but also whether there is conflicting
sentiment or debate. In the case of restaurant re-
views, it is relevant to know whether the dishes are
consistently good or whether there is some variation
in quality.
</bodyText>
<table confidence="0.99946785">
Judgment Attribute / Snippet
P A
Yes Yes - The salsa isn’t great
+ Chips and salsa are sublime
- The grits were good, but not great.
+ Grits were the perfect consistency
- The tom yum kha was bland
+ It’s the best Thai soup I ever had
- The naan is a bit doughy and undercooked
+ The naan was pretty tasty
- My reuben was a little dry.
+ The reuben was a good reuben.
Yes No - Belgian frites are crave-able
+ The frites are very, very good.
No Yes - The blackened chicken was meh
+ Chicken enchiladas are yummy!
- The taste overall was mediocre
+ The oysters are tremendous
No No - The cream cheese wasn’t bad
+ Ice cream was just delicious
</table>
<tableCaption confidence="0.574260666666667">
Table 6: Example property-attribute correctness for the
conflict identification task, over both property and at-
tribute. Property judgment (P) indicates whether the snip-
pets are discussing the same item; attribute judgment (A)
indicates whether there is a correct difference in attribute
(sentiment), regardless of properties.
</tableCaption>
<bodyText confidence="0.999907913043478">
To evaluate this, we examine the output clusters
which contain predictions of both positive and neg-
ative snippets. The goal is to identify whether these
are true conflicts of sentiment or there was a failure
in either property clustering or attribute classifica-
tion.
For this task, the output clusters are manually an-
notated for correctness of both property and attribute
judgments, as in Table 6. As there is no obvious
baseline for this experiment, we treat it simply as an
analysis of errors.
Results For this task, we examine the accuracy of
conflict prediction, both with and without the cor-
rectly identified properties. The results by property-
attribute correctness are shown in Table 7. From
these numbers, we can see that 50% of the clusters
are correct in both property (cohesiveness) and at-
tribute (difference in sentiment) dimensions.
Overall, the properties are correctly identified
(subject of NEG matches the subject of POS) 68%
of the time and a correct difference in attribute is
identified 67% of the time. Of the clusters which
are correct in property, 74% show a correctly labeled
</bodyText>
<page confidence="0.995476">
357
</page>
<table confidence="0.998249">
Judgment
P A # Clusters
Yes Yes 52
Yes No 18
No Yes 17
No No 15
</table>
<tableCaption confidence="0.99792">
Table 7: Results of conflict analysis by correctness of
</tableCaption>
<bodyText confidence="0.9165525">
property label (P) and attribute conflict (A). Examples
of each type of correctness pair are show in in Table 6.
50% of the clusters are correct in both labels, and there
are approximately the same number of errors toward both
property and attribute.
difference in attribute.
</bodyText>
<sectionHeader confidence="0.99514" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999970111111111">
We have presented a probabilistic topic model for
identifying properties and attitudes of product re-
view snippets. The model is relatively simple and
admits an efficient variational mean-field inference
procedure which is parallelized and can be run on
a large number of snippets. We have demonstrated
on multiple evaluation tasks that our model outper-
forms applicable baselines by a considerable mar-
gin.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999977">
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168), NIH (grant 5-
R01-LM009723-02), Nokia, and the DARPA Ma-
chine Reading Program (AFRL prime contract no.
FA8750-09-C-0172). Thanks to Peter Szolovits and
the MIT NLP group for their helpful comments.
Any opinions, findings, conclusions, or recommen-
dations expressed in this paper are those of the au-
thors, and do not necessarily reflect the views of the
funding organizations.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999823711538462">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022.
Giuseppe Carenini, Raymond Ng, and Adam Pauls.
2006. Multi-document summarization of evaluative
text. In Proceedings of EACL, pages 305–312.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of SIGKDD,
pages 168–177.
Minqing Hu and Bing Liu. 2006. Opinion extraction and
summarization on the web. In Proceedings of AAAI.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of COLING/ACL, pages 483–490.
Hyun Duk Kim and ChengXiang Zhai. 2009. Generat-
ing comparative summaries of contradictory opinions
in text. In Proceedings of CIKM, pages 385–394.
P. Liang and D. Klein. 2007. Structured Bayesian non-
parametric models with variational inference (tutorial).
In Proceedings of ACL.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005a.
Opinion observer: Analyzing and comparing opinions
on the web. In Proceedings of WWW, pages 342–351.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005b.
Opinion observer: analyzing and comparing opinions
on the web. In Proceedings of WWW, pages 342–351.
Yue Lu and ChengXiang Zhai. 2008. Opinion integra-
tion through semi-supervised topic modeling. In Pro-
ceedings of WWW, pages 121–130.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Proceed-
ings of WWW, pages 171–180.
Ana-Maria Popescu, Bao Nguyen, and Oren Etzioni.
2005. OPINE: Extracting product features and opin-
ions from reviews. In Proceedings of HLT/EMNLP,
pages 339–346.
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2010. Incorporating content structure into text anal-
ysis applications. In Proceedings of EMNLP, pages
377–387.
Yohei Seki, Koji Eguchi, Noriko K, and Masaki Aono.
2006. Opinion-focused summarization and its analysis
at DUC 2006. In Proceedings of DUC, pages 122–
130.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization.
In Proceedings of ACL, pages 308–316.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC, pages 45–52.
</reference>
<page confidence="0.997928">
358
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.905796">
<title confidence="0.998995">Content Models with Attitude</title>
<author confidence="0.998918">Christina Sauper</author>
<author confidence="0.998918">Aria Haghighi</author>
<author confidence="0.998918">Regina</author>
<affiliation confidence="0.9992885">Computer Science and Artificial Intelligence Massachusetts Institute of Technology</affiliation>
<email confidence="0.981239">csauper@csail.mit.edu,me@aria42.com,regina@csail.mit.edu</email>
<abstract confidence="0.995657705882353">We present a probabilistic topic model for jointly identifying properties and attributes of social media review snippets. Our model simultaneously learns a set of properties of a product and captures aggregate user sentiments towards these properties. This approach directly enables discovery of highly rated or inconsistent properties of a product. Our model admits an efficient variational meanfield inference algorithm which can be parallelized and run on large snippet collections. We evaluate our model on a large corpus of snippets from Yelp reviews to assess property and attribute prediction. We demonstrate that it outperforms applicable baselines by a considerable margin.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="16651" citStr="Blei et al. (2003)" startWordPosition="2707" endWordPosition="2710">0, |S)II Q(?G, OP, BB, OA, 0) We optimize this objective using coordinate descent on the q(·) factors. Concretely, we update each factor by optimizing the above criterion with all other factors fixed to current values. For instance, the update for the factor q(Zi,j,w W ) takes the form: q(Zi,j,w) W � &amp;quot;/&apos; EQ/q(Z�y,-) lg P(0, OP, BB, OA, 0, S) The full factorization of Q(·) and updates for all random variable factors are given in Figure 2. Updates of parameter factors are omitted; however these are derived through simple counts of the ZA, ZP, and ZW latent variables. For related discussion, see Blei et al. (2003). 6 Experiments In this section, we describe in detail our data set and present three experiments and their results. Data Set Our data set consists of snippets from Yelp reviews generated by the system described in Sauper et al. (2010). This system is trained to extract snippets containing short descriptions of user sentiment towards some aspect of a restaurant.2 We 1See Liang and Klein (2007) for an overview of variational techniques. 2For exact training procedures, please reference that paper. The [P noodles ] and the [P meat ] were actually [+ pretty good ]. I [+ recommend ] the [P chicken </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
<author>Adam Pauls</author>
</authors>
<title>Multi-document summarization of evaluative text.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>305--312</pages>
<contexts>
<context position="7171" citStr="Carenini et al., 2006" startWordPosition="1104" endWordPosition="1107"> mining (Hu and Liu, 2004), relaxation labeling (Popescu et al., 2005) and supervised learning (Kim and Hovy, 2006). While our method also extracts product properties and sentiment, our focus is on multi-review aggregation. This task introduces new challenges which were not addressed in prior research that focused on perdocument analysis. A second related line of research is multidocument review summarization. Some of these methods directly apply existing domainindependent summarization methods (Seki et al., 2006), while others propose new methods targeted for opinion text (Liu et al., 2005b; Carenini et al., 2006; Hu and Liu, 2006; Kim and Zhai, 2009). For instance, these summaries may present contrastive view points (Kim and Zhai, 2009) or relay average sentiment (Carenini et al., 2006). The focus of this line of work is on how to select suitable sentences, assuming that relevant review features (such as numerical scores) are given. Since our emphasis is on multi-review analysis, we believe that the information we extract can benefit existing summarization systems. Finally, a number of approaches analyze review documents using probabilistic topic models (Lu and Zhai, 2008; Titov and McDonald, 2008; M</context>
</contexts>
<marker>Carenini, Ng, Pauls, 2006</marker>
<rawString>Giuseppe Carenini, Raymond Ng, and Adam Pauls. 2006. Multi-document summarization of evaluative text. In Proceedings of EACL, pages 305–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGKDD,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="6399" citStr="Hu and Liu, 2004" startWordPosition="983" endWordPosition="986">r applicable baselines, demonstrating that learning to identify attributes in the context of other product reviews yields significant gains. Finally, we evaluate our model on its ability to identify product properties for which there is significant sentiment disagreement amongst user snippets. This tests our model’s capacity to jointly identify properties and assess attributes. 2 Related Work Our work on review aggregation has connections to three lines of work in text analysis. First, our work relates to research on extraction of product properties with associated sentiment from review text (Hu and Liu, 2004; Liu et al., 2005a; Popescu et al., 2005). These methods identify relevant information in a document using a wide range of methods such as association mining (Hu and Liu, 2004), relaxation labeling (Popescu et al., 2005) and supervised learning (Kim and Hovy, 2006). While our method also extracts product properties and sentiment, our focus is on multi-review aggregation. This task introduces new challenges which were not addressed in prior research that focused on perdocument analysis. A second related line of research is multidocument review summarization. Some of these methods directly appl</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of SIGKDD, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Opinion extraction and summarization on the web.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="7189" citStr="Hu and Liu, 2006" startWordPosition="1108" endWordPosition="1111">04), relaxation labeling (Popescu et al., 2005) and supervised learning (Kim and Hovy, 2006). While our method also extracts product properties and sentiment, our focus is on multi-review aggregation. This task introduces new challenges which were not addressed in prior research that focused on perdocument analysis. A second related line of research is multidocument review summarization. Some of these methods directly apply existing domainindependent summarization methods (Seki et al., 2006), while others propose new methods targeted for opinion text (Liu et al., 2005b; Carenini et al., 2006; Hu and Liu, 2006; Kim and Zhai, 2009). For instance, these summaries may present contrastive view points (Kim and Zhai, 2009) or relay average sentiment (Carenini et al., 2006). The focus of this line of work is on how to select suitable sentences, assuming that relevant review features (such as numerical scores) are given. Since our emphasis is on multi-review analysis, we believe that the information we extract can benefit existing summarization systems. Finally, a number of approaches analyze review documents using probabilistic topic models (Lu and Zhai, 2008; Titov and McDonald, 2008; Mei et al., 2007). </context>
</contexts>
<marker>Hu, Liu, 2006</marker>
<rawString>Minqing Hu and Bing Liu. 2006. Opinion extraction and summarization on the web. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic identification of pro and con reasons in online reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>483--490</pages>
<contexts>
<context position="6665" citStr="Kim and Hovy, 2006" startWordPosition="1027" endWordPosition="1030">greement amongst user snippets. This tests our model’s capacity to jointly identify properties and assess attributes. 2 Related Work Our work on review aggregation has connections to three lines of work in text analysis. First, our work relates to research on extraction of product properties with associated sentiment from review text (Hu and Liu, 2004; Liu et al., 2005a; Popescu et al., 2005). These methods identify relevant information in a document using a wide range of methods such as association mining (Hu and Liu, 2004), relaxation labeling (Popescu et al., 2005) and supervised learning (Kim and Hovy, 2006). While our method also extracts product properties and sentiment, our focus is on multi-review aggregation. This task introduces new challenges which were not addressed in prior research that focused on perdocument analysis. A second related line of research is multidocument review summarization. Some of these methods directly apply existing domainindependent summarization methods (Seki et al., 2006), while others propose new methods targeted for opinion text (Liu et al., 2005b; Carenini et al., 2006; Hu and Liu, 2006; Kim and Zhai, 2009). For instance, these summaries may present contrastive</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Automatic identification of pro and con reasons in online reviews. In Proceedings of COLING/ACL, pages 483–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hyun Duk Kim</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Generating comparative summaries of contradictory opinions in text.</title>
<date>2009</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>385--394</pages>
<contexts>
<context position="7210" citStr="Kim and Zhai, 2009" startWordPosition="1112" endWordPosition="1115">beling (Popescu et al., 2005) and supervised learning (Kim and Hovy, 2006). While our method also extracts product properties and sentiment, our focus is on multi-review aggregation. This task introduces new challenges which were not addressed in prior research that focused on perdocument analysis. A second related line of research is multidocument review summarization. Some of these methods directly apply existing domainindependent summarization methods (Seki et al., 2006), while others propose new methods targeted for opinion text (Liu et al., 2005b; Carenini et al., 2006; Hu and Liu, 2006; Kim and Zhai, 2009). For instance, these summaries may present contrastive view points (Kim and Zhai, 2009) or relay average sentiment (Carenini et al., 2006). The focus of this line of work is on how to select suitable sentences, assuming that relevant review features (such as numerical scores) are given. Since our emphasis is on multi-review analysis, we believe that the information we extract can benefit existing summarization systems. Finally, a number of approaches analyze review documents using probabilistic topic models (Lu and Zhai, 2008; Titov and McDonald, 2008; Mei et al., 2007). While some of these m</context>
</contexts>
<marker>Kim, Zhai, 2009</marker>
<rawString>Hyun Duk Kim and ChengXiang Zhai. 2009. Generating comparative summaries of contradictory opinions in text. In Proceedings of CIKM, pages 385–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>Structured Bayesian nonparametric models with variational inference (tutorial).</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="17047" citStr="Liang and Klein (2007)" startWordPosition="2773" endWordPosition="2776">ll random variable factors are given in Figure 2. Updates of parameter factors are omitted; however these are derived through simple counts of the ZA, ZP, and ZW latent variables. For related discussion, see Blei et al. (2003). 6 Experiments In this section, we describe in detail our data set and present three experiments and their results. Data Set Our data set consists of snippets from Yelp reviews generated by the system described in Sauper et al. (2010). This system is trained to extract snippets containing short descriptions of user sentiment towards some aspect of a restaurant.2 We 1See Liang and Klein (2007) for an overview of variational techniques. 2For exact training procedures, please reference that paper. The [P noodles ] and the [P meat ] were actually [+ pretty good ]. I [+ recommend ] the [P chicken noodle pho ]. The [P noodles ] were [- soggy ]. The [P chicken pho ] was also [+ good ]. The [P spring rolls ] and [P coffee ] were [+ good ] though. The [P spring roll wrappers ] were a [- little dry tasting ]. My [+ favorites ] were the [P crispy spring rolls ]. The [P Crispy Tuna Spring Rolls ] are [+ fantastic ]! The [P lobster roll ] my mother ordered was [- dry ] and [- scant ]. The [P p</context>
</contexts>
<marker>Liang, Klein, 2007</marker>
<rawString>P. Liang and D. Klein. 2007. Structured Bayesian nonparametric models with variational inference (tutorial). In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Minqing Hu</author>
<author>Junsheng Cheng</author>
</authors>
<title>Opinion observer: Analyzing and comparing opinions on the web.</title>
<date>2005</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>342--351</pages>
<contexts>
<context position="6417" citStr="Liu et al., 2005" startWordPosition="987" endWordPosition="990">ines, demonstrating that learning to identify attributes in the context of other product reviews yields significant gains. Finally, we evaluate our model on its ability to identify product properties for which there is significant sentiment disagreement amongst user snippets. This tests our model’s capacity to jointly identify properties and assess attributes. 2 Related Work Our work on review aggregation has connections to three lines of work in text analysis. First, our work relates to research on extraction of product properties with associated sentiment from review text (Hu and Liu, 2004; Liu et al., 2005a; Popescu et al., 2005). These methods identify relevant information in a document using a wide range of methods such as association mining (Hu and Liu, 2004), relaxation labeling (Popescu et al., 2005) and supervised learning (Kim and Hovy, 2006). While our method also extracts product properties and sentiment, our focus is on multi-review aggregation. This task introduces new challenges which were not addressed in prior research that focused on perdocument analysis. A second related line of research is multidocument review summarization. Some of these methods directly apply existing domaini</context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>Bing Liu, Minqing Hu, and Junsheng Cheng. 2005a. Opinion observer: Analyzing and comparing opinions on the web. In Proceedings of WWW, pages 342–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Minqing Hu</author>
<author>Junsheng Cheng</author>
</authors>
<title>Opinion observer: analyzing and comparing opinions on the web.</title>
<date>2005</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>342--351</pages>
<contexts>
<context position="6417" citStr="Liu et al., 2005" startWordPosition="987" endWordPosition="990">ines, demonstrating that learning to identify attributes in the context of other product reviews yields significant gains. Finally, we evaluate our model on its ability to identify product properties for which there is significant sentiment disagreement amongst user snippets. This tests our model’s capacity to jointly identify properties and assess attributes. 2 Related Work Our work on review aggregation has connections to three lines of work in text analysis. First, our work relates to research on extraction of product properties with associated sentiment from review text (Hu and Liu, 2004; Liu et al., 2005a; Popescu et al., 2005). These methods identify relevant information in a document using a wide range of methods such as association mining (Hu and Liu, 2004), relaxation labeling (Popescu et al., 2005) and supervised learning (Kim and Hovy, 2006). While our method also extracts product properties and sentiment, our focus is on multi-review aggregation. This task introduces new challenges which were not addressed in prior research that focused on perdocument analysis. A second related line of research is multidocument review summarization. Some of these methods directly apply existing domaini</context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>Bing Liu, Minqing Hu, and Junsheng Cheng. 2005b. Opinion observer: analyzing and comparing opinions on the web. In Proceedings of WWW, pages 342–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Lu</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Opinion integration through semi-supervised topic modeling.</title>
<date>2008</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>121--130</pages>
<contexts>
<context position="7742" citStr="Lu and Zhai, 2008" startWordPosition="1195" endWordPosition="1198">text (Liu et al., 2005b; Carenini et al., 2006; Hu and Liu, 2006; Kim and Zhai, 2009). For instance, these summaries may present contrastive view points (Kim and Zhai, 2009) or relay average sentiment (Carenini et al., 2006). The focus of this line of work is on how to select suitable sentences, assuming that relevant review features (such as numerical scores) are given. Since our emphasis is on multi-review analysis, we believe that the information we extract can benefit existing summarization systems. Finally, a number of approaches analyze review documents using probabilistic topic models (Lu and Zhai, 2008; Titov and McDonald, 2008; Mei et al., 2007). While some of these methods focus primar351 ily on modeling ratable aspects (Titov and McDonald, 2008), others explicitly capture the mixture of topics and sentiments (Mei et al., 2007). These approaches are capable of identifying latent topics in the collection in opinion text (e.g., weblogs) as well as associated sentiment. While our model captures similar high-level intuition, it analyzes fine-grained properties expressed at the snippet level, rather than document-level sentiment. Delivering analysis at such a fine granularity requires a new te</context>
</contexts>
<marker>Lu, Zhai, 2008</marker>
<rawString>Yue Lu and ChengXiang Zhai. 2008. Opinion integration through semi-supervised topic modeling. In Proceedings of WWW, pages 121–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xu Ling</author>
<author>Matthew Wondra</author>
<author>Hang Su</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>171--180</pages>
<contexts>
<context position="7787" citStr="Mei et al., 2007" startWordPosition="1203" endWordPosition="1206">6; Hu and Liu, 2006; Kim and Zhai, 2009). For instance, these summaries may present contrastive view points (Kim and Zhai, 2009) or relay average sentiment (Carenini et al., 2006). The focus of this line of work is on how to select suitable sentences, assuming that relevant review features (such as numerical scores) are given. Since our emphasis is on multi-review analysis, we believe that the information we extract can benefit existing summarization systems. Finally, a number of approaches analyze review documents using probabilistic topic models (Lu and Zhai, 2008; Titov and McDonald, 2008; Mei et al., 2007). While some of these methods focus primar351 ily on modeling ratable aspects (Titov and McDonald, 2008), others explicitly capture the mixture of topics and sentiments (Mei et al., 2007). These approaches are capable of identifying latent topics in the collection in opinion text (e.g., weblogs) as well as associated sentiment. While our model captures similar high-level intuition, it analyzes fine-grained properties expressed at the snippet level, rather than document-level sentiment. Delivering analysis at such a fine granularity requires a new technique. 3 Problem Formulation In this sectio</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of WWW, pages 171–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Bao Nguyen</author>
<author>Oren Etzioni</author>
</authors>
<title>OPINE: Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>339--346</pages>
<contexts>
<context position="6441" citStr="Popescu et al., 2005" startWordPosition="991" endWordPosition="994"> that learning to identify attributes in the context of other product reviews yields significant gains. Finally, we evaluate our model on its ability to identify product properties for which there is significant sentiment disagreement amongst user snippets. This tests our model’s capacity to jointly identify properties and assess attributes. 2 Related Work Our work on review aggregation has connections to three lines of work in text analysis. First, our work relates to research on extraction of product properties with associated sentiment from review text (Hu and Liu, 2004; Liu et al., 2005a; Popescu et al., 2005). These methods identify relevant information in a document using a wide range of methods such as association mining (Hu and Liu, 2004), relaxation labeling (Popescu et al., 2005) and supervised learning (Kim and Hovy, 2006). While our method also extracts product properties and sentiment, our focus is on multi-review aggregation. This task introduces new challenges which were not addressed in prior research that focused on perdocument analysis. A second related line of research is multidocument review summarization. Some of these methods directly apply existing domainindependent summarization</context>
</contexts>
<marker>Popescu, Nguyen, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu, Bao Nguyen, and Oren Etzioni. 2005. OPINE: Extracting product features and opinions from reviews. In Proceedings of HLT/EMNLP, pages 339–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Sauper</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Incorporating content structure into text analysis applications.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>377--387</pages>
<contexts>
<context position="2901" citStr="Sauper et al. (2010)" startWordPosition="430" endWordPosition="433">work is to provide a mechanism for review content aggregation that goes beyond numerical scores. Specifically, we are interested in identifying fine-grained product properties across reviews (e.g., battery life for electronics or pizza for restaurants) as well as capturing attributes of these properties, namely aggregate user sentiment. For this task, we assume as input a set of product review snippets (i.e., standalone phrases such as “battery life is the best I’ve found”) rather than complete reviews. There are many techniques for extracting this type of snippet in existing work; we use the Sauper et al. (2010) system. + The martinis were very good. The drinks - both wine and martinis - were tasty. The wine list was pricey. - Their wine selection is horrible. The sushi is the best I’ve ever had. Best paella I’d ever had. The fillet was the best steak we’d ever had. It’s the best soup I’ve ever had. + 350 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 350–358, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics At first glance, this task can be solved using existing methods for review analysis. These methods can effectiv</context>
<context position="8817" citStr="Sauper et al. (2010)" startWordPosition="1364" endWordPosition="1367">properties expressed at the snippet level, rather than document-level sentiment. Delivering analysis at such a fine granularity requires a new technique. 3 Problem Formulation In this section, we discuss the core random variables and abstractions of our model. We describe the generative models over these elements in Section 4. Product: A product represents a reviewable object. For the experiments in this paper, we use restaurants as products. Snippets: A snippet is a user-generated short sequence of tokens describing a product. Input snippets are deterministically taken from the output of the Sauper et al. (2010) system. Property: A property corresponds to some finegrained aspect of a product. For instance, the snippet “the pad thai was great” describes the pad thai property. We assume that each snippet has a single property associated with it. We assume a fixed number of possible properties K for each product. For the corpus of restaurant reviews, we assume that the set of properties are specific to a given product, in order to capture fine-grained, relevant properties for each restaurant. For example, reviews from a sandwich shop may contrast the club sandwich with the turkey wrap, while for a more </context>
<context position="16886" citStr="Sauper et al. (2010)" startWordPosition="2747" endWordPosition="2750">the update for the factor q(Zi,j,w W ) takes the form: q(Zi,j,w) W � &amp;quot;/&apos; EQ/q(Z�y,-) lg P(0, OP, BB, OA, 0, S) The full factorization of Q(·) and updates for all random variable factors are given in Figure 2. Updates of parameter factors are omitted; however these are derived through simple counts of the ZA, ZP, and ZW latent variables. For related discussion, see Blei et al. (2003). 6 Experiments In this section, we describe in detail our data set and present three experiments and their results. Data Set Our data set consists of snippets from Yelp reviews generated by the system described in Sauper et al. (2010). This system is trained to extract snippets containing short descriptions of user sentiment towards some aspect of a restaurant.2 We 1See Liang and Klein (2007) for an overview of variational techniques. 2For exact training procedures, please reference that paper. The [P noodles ] and the [P meat ] were actually [+ pretty good ]. I [+ recommend ] the [P chicken noodle pho ]. The [P noodles ] were [- soggy ]. The [P chicken pho ] was also [+ good ]. The [P spring rolls ] and [P coffee ] were [+ good ] though. The [P spring roll wrappers ] were a [- little dry tasting ]. My [+ favorites ] were </context>
</contexts>
<marker>Sauper, Haghighi, Barzilay, 2010</marker>
<rawString>Christina Sauper, Aria Haghighi, and Regina Barzilay. 2010. Incorporating content structure into text analysis applications. In Proceedings of EMNLP, pages 377–387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>Koji Eguchi</author>
<author>K Noriko</author>
<author>Masaki Aono</author>
</authors>
<title>Opinion-focused summarization and its analysis at DUC</title>
<date>2006</date>
<booktitle>In Proceedings of DUC,</booktitle>
<pages>122--130</pages>
<contexts>
<context position="7069" citStr="Seki et al., 2006" startWordPosition="1087" endWordPosition="1090">thods identify relevant information in a document using a wide range of methods such as association mining (Hu and Liu, 2004), relaxation labeling (Popescu et al., 2005) and supervised learning (Kim and Hovy, 2006). While our method also extracts product properties and sentiment, our focus is on multi-review aggregation. This task introduces new challenges which were not addressed in prior research that focused on perdocument analysis. A second related line of research is multidocument review summarization. Some of these methods directly apply existing domainindependent summarization methods (Seki et al., 2006), while others propose new methods targeted for opinion text (Liu et al., 2005b; Carenini et al., 2006; Hu and Liu, 2006; Kim and Zhai, 2009). For instance, these summaries may present contrastive view points (Kim and Zhai, 2009) or relay average sentiment (Carenini et al., 2006). The focus of this line of work is on how to select suitable sentences, assuming that relevant review features (such as numerical scores) are given. Since our emphasis is on multi-review analysis, we believe that the information we extract can benefit existing summarization systems. Finally, a number of approaches ana</context>
</contexts>
<marker>Seki, Eguchi, Noriko, Aono, 2006</marker>
<rawString>Yohei Seki, Koji Eguchi, Noriko K, and Masaki Aono. 2006. Opinion-focused summarization and its analysis at DUC 2006. In Proceedings of DUC, pages 122– 130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>A joint model of text and aspect ratings for sentiment summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>308--316</pages>
<contexts>
<context position="7768" citStr="Titov and McDonald, 2008" startWordPosition="1199" endWordPosition="1202">005b; Carenini et al., 2006; Hu and Liu, 2006; Kim and Zhai, 2009). For instance, these summaries may present contrastive view points (Kim and Zhai, 2009) or relay average sentiment (Carenini et al., 2006). The focus of this line of work is on how to select suitable sentences, assuming that relevant review features (such as numerical scores) are given. Since our emphasis is on multi-review analysis, we believe that the information we extract can benefit existing summarization systems. Finally, a number of approaches analyze review documents using probabilistic topic models (Lu and Zhai, 2008; Titov and McDonald, 2008; Mei et al., 2007). While some of these methods focus primar351 ily on modeling ratable aspects (Titov and McDonald, 2008), others explicitly capture the mixture of topics and sentiments (Mei et al., 2007). These approaches are capable of identifying latent topics in the collection in opinion text (e.g., weblogs) as well as associated sentiment. While our model captures similar high-level intuition, it analyzes fine-grained properties expressed at the snippet level, rather than document-level sentiment. Delivering analysis at such a fine granularity requires a new technique. 3 Problem Formula</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. A joint model of text and aspect ratings for sentiment summarization. In Proceedings of ACL, pages 308–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of MUC,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="22302" citStr="Vilain et al., 1995" startWordPosition="3729" endWordPosition="3732">h were provided to two workers, for a total output of 210 generated clusters. Baseline The baseline for this task is a clustering algorithm weighted by TF*IDF over the data set as implemented by the publicly available CLUTO package.3 This baseline will put a strong connection between things which are lexically similar. Because our model only uses property words to tie together clusters, it may miss correlations between words which are not correctly identified as property words. The baseline is allowed 10 property clusters per restaurant. We use the MUC cluster evaluation metric for this task (Vilain et al., 1995). This metric measures the number of cluster merges and splits required to recreate the gold clusters given the model’s output. 3Available at http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview with agglomerative clustering, using the cosine similarity distance metric. 355 Precision Recall F1 Baseline 80.2 61.1 69.3 Our model 72.2 79.1 75.5 Table 2: Results using the MUC metric on the cluster prediction task. Note that while the precision of the baseline is higher, the recall and overall F1 of our model outweighs that. While MUC has a deficiency in that putting everything into a single clust</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of MUC, pages 45–52.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>