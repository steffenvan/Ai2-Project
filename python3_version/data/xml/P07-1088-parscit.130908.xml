<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009229">
<title confidence="0.9971055">
Sparse Information Extraction:
Unsupervised Language Models to the Rescue
</title>
<author confidence="0.998537">
Doug Downey, Stefan Schoenmackers, and Oren Etzioni
</author>
<affiliation confidence="0.995269">
Turing Center, Department of Computer Science and Engineering
University of Washington, Box 352350
</affiliation>
<address confidence="0.621736">
Seattle, WA 98195, USA
</address>
<email confidence="0.997999">
Iddowney,stef,etzionif@cs.washington.edu
</email>
<sectionHeader confidence="0.995625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999838952380952">
Even in a massive corpus such as the Web, a
substantial fraction of extractions appear in-
frequently. This paper shows how to assess
the correctness of sparse extractions by uti-
lizing unsupervised language models. The
REALM system, which combines HMM-
based and n-gram-based language models,
ranks candidate extractions by the likeli-
hood that they are correct. Our experiments
show that REALM reduces extraction error
by 39%, on average, when compared with
previous work.
Because REALM pre-computes language
models based on its corpus and does not re-
quire any hand-tagged seeds, it is far more
scalable than approaches that learn mod-
els for each individual relation from hand-
tagged data. Thus, REALM is ideally suited
for open information extraction where the
relations of interest are not specified in ad-
vance and their number is potentially vast.
</bodyText>
<sectionHeader confidence="0.999333" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901418604651">
Information Extraction (IE) from text is far from in-
fallible. In response, researchers have begun to ex-
ploit the redundancy in massive corpora such as the
Web in order to assess the veracity of extractions
(e.g., (Downey et al., 2005; Etzioni et al., 2005;
Feldman et al., 2006)). In essence, such methods uti-
lize extraction patterns to generate candidate extrac-
tions (e.g., “Istanbul”) and then assess each candi-
date by computing co-occurrence statistics between
the extraction and words or phrases indicative of
class membership (e.g., “cities such as”).
However, Zipf’s Law governs the distribution of
extractions. Thus, even the Web has limited redun-
dancy for less prominent instances of relations. In-
deed, 50% of the extractions in the data sets em-
ployed by (Downey et al., 2005) appeared only
once. As a result, Downey et al.’s model, and re-
lated methods, had no way of assessing which ex-
traction is more likely to be correct for fully half of
the extractions. This problem is particularly acute
when moving beyond unary relations. We refer to
this challenge as the task of assessing sparse extrac-
tions.
This paper introduces the idea that language mod-
eling techniques such as n-gram statistics (Manning
and Sch¨utze, 1999) and HMMs (Rabiner, 1989) can
be used to effectively assess sparse extractions. The
paper introduces the REALM system, and highlights
its unique properties. Notably, REALM does not
require any hand-tagged seeds, which enables it to
scale to Open IE—extraction where the relations of
interest are not specified in advance, and their num-
ber is potentially vast (Banko et al., 2007).
REALM is based on two key hypotheses. The
KnowItAll hypothesis is that extractions that oc-
cur more frequently in distinct sentences in the
corpus are more likely to be correct. For exam-
ple, the hypothesis suggests that the argument pair
(Giuliani, New York) is relatively likely to be
appropriate for the Mayor relation, simply because
this pair is extracted for the Mayor relation rela-
tively frequently. Second, we employ an instance of
the distributional hypothesis (Harris, 1985), which
</bodyText>
<page confidence="0.982081">
696
</page>
<note confidence="0.925767">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 696–703,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.994961867346939">
can be phrased as follows: different instances of More formally, the list of candidate extrac-
the same semantic relation tend to appear in sim- tions for a relation R is denoted as ER =
ilar textual contexts. We assess sparse extractions {(a1, b1), ... , (am, bm)}. An extraction (ai, bi) is
by comparing the contexts in which they appear to an ordered pair of strings. The extraction is correct
those of more common extractions. Sparse extrac- if and only if the relation R holds between the argu-
tions whose contexts are more similar to those of ments named by ai and bi. For example, for R =
common extractions are judged more likely to be Headquartered, apair (ai, bi) is correct iff there
correct based on the conjunction of the KnowItAll exists an organization ai that is in fact headquartered
and the distributional hypotheses. in the location bi.1
The contributions of the paper are as follows: ER is generated by applying an extraction mech-
• The paper introduces the insight that the sub- anism, typically a set of extraction “patterns”, to
field of language modeling provides unsuper- each sentence in a corpus, and recording the results.
vised methods that can be leveraged to assess Thus, many elements of ER are identical extractions
sparse extractions. These methods are more derived from different sentences in the corpus.
scalable than previous assessment techniques, This task definition is notable for the minimal
and require no hand tagging whatsoever. inputs required—IE assessment does not require
• The paper introduces an HMM-based tech- knowing the relation name nor does it require hand-
nique for checking whether two arguments are tagged seed examples of the relation. Thus, an IE
of the proper type for a relation. Assessor is applicable to Open IE.
• The paper introduces a relational n-gram 2.1 System Overview
model for the purpose of determining whether In this section, we describe the REALM system,
a sentence that mentions multiple arguments which utilizes language modeling techniques to per-
actually expresses a particular relationship be- form IE Assessment.
tween them. REALM takes as input a set of extractions ER,
• The paper introduces a novel language- and outputs a ranking of those extractions. The
modeling system called REALM that combines algorithm REALM follows is outlined in Figure 1.
both HMM-based models and relational n- REALM begins by automatically selecting from ER
gram models, and shows that REALM reduces a set of bootstrapped seeds 5R intended to serve as
error by an average of 39% over previous meth- correct examples of the relation R. REALM utilizes
ods, when applied to sparse extraction data. the KnowItAll hypothesis, setting 5R equal to the
The remainder of the paper is organized as fol- h elements in ER extracted most frequently from
lows. Section 2 introduces the IE assessment task, the underlying corpus. This results in a noisy set of
and describes the REALM system in detail. Section seeds, but the methods that use these seeds are noise
3 reports on our experimental results followed by a tolerant.
discussion of related work in Section 4. Finally, we REALM then proceeds to rank the remaining
conclude with a discussion of scalability and with (non-seed) extractions by utilizing two language-
directions for future work. modeling components. An n-gram language model
2 IE Assessment is a probability distribution P(w1, ..., wn) over con-
This section formalizes the IE assessment task and secutive word sequences of length n in a corpus.
describes the REALM system for solving it. An IE Formally, if we assume a seed (s1, s2) is a correct
assessor takes as input a list of candidate extractions extraction of a relation R, the distributional hypoth-
meant to denote instances of a relation, and outputs esis states that the context distribution around the
a ranking of the extractions with the goal that cor- seed extraction, P(w1, ..., wn|wi = s1, wj = s2)
rect extractions rank higher than incorrect ones. A for 1 G i, j G n tends to be “more similar” to
correct extraction is defined to be a true instance of
the relation mentioned in the input text.
697
1For clarity, our discussion focuses on relations between
pairs of arguments. However, the methods we propose can be
extended to relations of any arity.
P(w1, ..., wnjwi = e1, wj = e2) when the extrac-
tion (e1, e2) is correct. Naively comparing context
distributions is problematic, however, because the
arguments to a relation often appear separated by
several intervening words. In our experiments, we
found that when relation arguments appear together
in a sentence, 75% of the time the arguments are
separated by at least three words. This implies that
n must be large, and for sparse argument pairs it is
not possible to estimate such a large language model
accurately, because the number of modeling param-
eters is proportional to the vocabulary size raised to
the nth power. To mitigate sparsity, REALM utilizes
smaller language models in its two components as a
means of “backing-off’ from estimating context dis-
tributions explicitly, as described below.
First, REALM utilizes an HMM to estimate
whether each extraction has arguments of the proper
type for the relation. Each relation R has a set
of types for its arguments. For example, the rela-
tion AuthorOf(a, b) requires that its first ar-
gument be an author, and that its second be some
kind of written work. Knowing whether extracted
arguments are of the proper type for a relation can
be quite informative for assessing extractions. The
challenge is, however, that this type information is
not given to the system since the relations (and the
types of the arguments) are not known in advance.
REALM solves this problem by comparing the dis-
tributions of the seed arguments and extraction ar-
guments. Type checking mitigates data sparsity by
leveraging every occurrence of the individual extrac-
tion arguments in the corpus, rather than only those
cases in which argument pairs occur near each other.
Although argument type checking is invalu-
able for extraction assessment, it is not suf-
ficient for extracting relationships between ar-
guments. For example, an IE system us-
ing only type information might determine that
Intel is a corporation and that Seattle is
a city, and therefore erroneously conclude that
Headquartered(Intel, Seattle) is cor-
rect. Thus, REALM’s second step is to employ an
n-gram-based language model to assess whether the
extracted arguments share the appropriate relation.
Again, this information is not given to the system,
so REALM compares the context distributions of the
extractions to those of the seeds. As described in
</bodyText>
<equation confidence="0.916062888888889">
REALM(Extractions ER = {el, ..., em})
SR = the h most frequent extractions in ER
UR = ER - SR
TypeRankings(UR) +- HMM-T(SR, UR)
RelationRankings(UR) +- REL-GRAMS(SR, UR)
return a ranking of ER with the elements of SR at the
top (ranked by frequency) followed by the elements of
UR = {ul, ..., um−h} ranked in ascending order of
TypeRanking(ui) * RelationRanking(ui).
</equation>
<figureCaption confidence="0.877941">
Figure 1: Pseudocode for REALM at run-time.
</figureCaption>
<bodyText confidence="0.982076826086956">
The language models used by the HMM-T and
REL-GRAMS components are constructed in a pre-
processing step.
Section 2.3, REALM employs a relational n-gram
language model in order to accurately compare con-
text distributions when extractions are sparse.
REALM executes the type checking and relation
assessment components separately; each component
takes the seed and non-seed extractions as arguments
and returns a ranking of the non-seeds. REALM then
combines the two components’ assessments into a
single ranking. Although several such combinations
are possible, REALM simply ranks the extractions in
ascending order of the product of the ranks assigned
by the two components. The following subsections
describe REALM’s two components in detail.
We identify the proper nouns in our corpus us-
ing the LEX method (Downey et al., 2007). In ad-
dition to locating the proper nouns in the corpus,
LEX also concatenates each multi-token proper noun
(e.g.,Los Angeles) together into a single token.
Both of REALM’s components construct language
models from this tokenized corpus.
</bodyText>
<subsectionHeader confidence="0.95198">
2.2 Type Checking with HMM-T
</subsectionHeader>
<bodyText confidence="0.999988272727273">
In this section, we describe our type-checking com-
ponent, which takes the form of a Hidden Markov
Model and is referred to as HMM-T. HMM-T ranks
the set UR of non-seed extractions, with a goal of
ranking those extractions with arguments of proper
type for R above extractions containing type errors.
Formally, let URi denote the set of the ith arguments
of the extractions in UR. Let SRi be defined simi-
larly for the seed set SR.
Our type checking technique exploits the distri-
butional hypothesis—in this case, the intuition that
</bodyText>
<page confidence="0.992307">
698
</page>
<bodyText confidence="0.594172">
Intel , headquartered in Santa+Clara
</bodyText>
<figureCaption confidence="0.886966">
Figure 2: Graphical model employed by HMM-
</figureCaption>
<bodyText confidence="0.9995029375">
T. Shown is the case in which k = 2. Corpus
pre-processing results in the proper noun Santa
Clara being concatenated into a single token.
extraction arguments in URi of the proper type will
likely appear in contexts similar to those in which
the seed arguments SRi appear. In order to iden-
tify terms that are distributionally similar, we train
a probabilistic generative Hidden Markov Model
(HMM), which treats each token in the corpus as
generated by a single hidden state variable. Here, the
hidden states take integral values from 11, ... , T},
and each hidden state variable is itself generated by
some number k of previous hidden states.2 For-
mally, the joint distribution of the corpus, repre-
sented as a vector of tokens w, given a correspond-
ing vector of states t is:
</bodyText>
<equation confidence="0.9879925">
P (w|t) = � P(wi|ti)P(ti|ti−1, ... , ti−k) (1)
i
</equation>
<bodyText confidence="0.954084761904762">
The distributions on the right side of Equation 1
can be learned from a corpus in an unsupervised
manner, such that words which are distributed sim-
ilarly in the corpus tend to be generated by simi-
lar hidden states (Rabiner, 1989). The generative
model is depicted as a Bayesian network in Figure 2.
The figure also illustrates the one way in which our
implementation is distinct from a standard HMM,
namely that proper nouns are detected a priori and
modeled as single tokens (e.g., Santa Clara is
generated by a single hidden state). This allows
the type checker to compare the state distributions
of different proper nouns directly, even when the
proper nouns contain differing numbers of words.
To generate a ranking of UR using the learned
HMM parameters, we rank the arguments ei accord-
ing to how similar their state distributions P(t|ei)
2Our implementation makes the simplifying assumption that
each sentence in the corpus is generated independently.
are to those of the seed arguments.3 Specifically, we
define a function:
</bodyText>
<equation confidence="0.8860175">
KL( Ew&apos;ESRi P(t |w,) , P(t |ei)) (2)
 |SRi |
</equation>
<bodyText confidence="0.999039409090909">
where KL represents KL divergence, and the outer
sum is taken over the arguments ei of the extraction
e. We rank the elements of UR in ascending order of
f(e).
HMM-T has two advantages over a more tradi-
tional type checking approach of simply counting
the number of times in the corpus that each extrac-
tion appears in a context in which a seed also ap-
pears (cf. (Ravichandran et al., 2005)). The first
advantage of HMM-T is efficiency, as the traditional
approach involves a computationally expensive step
of retrieving the potentially large set of contexts in
which the extractions and seeds appear. In our ex-
periments, using HMM-T instead of a context-based
approach results in a 10-50x reduction in the amount
of data that is retrieved to perform type checking.
Secondly, on sparse data HMM-T has the poten-
tial to improve type checking accuracy. For exam-
ple, consider comparing Pickerington, a sparse
candidate argument of the type City, to the seed
argument Chicago, for which the following two
phrases appear in the corpus:
</bodyText>
<listItem confidence="0.9503495">
(i) “Pickerington, Ohio”
(ii) “Chicago, Illinois”
</listItem>
<bodyText confidence="0.999879625">
In these phrases, the textual contexts surrounding
Chicago and Pickerington are not identical,
so to the traditional approach these contexts offer
no evidence that Pickerington and Chicago
are of the same type. For a sparse token like
Pickerington, this is problematic because the
token may never occur in a context that precisely
matches that of a seed. In contrast, in the HMM, the
non-sparse tokens Ohio and Illinois are likely
to have similar state distributions, as they are both
the names of U.S. States. Thus, in the state space
employed by the HMM, the contexts in phrases (i)
and (ii) are in fact quite similar, allowing HMM-
T to detect that Pickerington and Chicago
are likely of the same type. Our experiments quan-
tify the performance improvements that HMM-T of-
</bodyText>
<footnote confidence="0.980841">
3The distribution P(tlei) for any ei can be obtained from
the HMM parameters using Bayes Rule.
</footnote>
<equation confidence="0.634332333333333">
�
f(e) =
eiEe
</equation>
<page confidence="0.978328">
699
</page>
<bodyText confidence="0.999856358024691">
fers over the traditional approach for type checking pair of arguments. Formally, for a given sentence
sparse data. containing arguments e1 and e2 consecutively, we
The time required to learn HMM-T’s parameters define a context of the ordered pair (e1, e2) to be
scales proportional to Tk+1 times the corpus size. any window of n tokens around e1. Let C =
Thus, for tractability, HMM-T uses a relatively small {c1, c2, ..., c,C,} be the set of all contexts of all ar-
state space of T = 20 states and a limited k value gument pairs found in the corpus.4 For a pair of ar-
of 3. While these settings are sufficient for type guments (ej, ek), we model their relationship using
checking (e.g., determining that Santa Clara is a |C |dimensional context vector v(ej�ek), whose i-th
a city) they are too coarse-grained to assess relations dimension corresponds to the number of times con-
between arguments (e.g., determining that Santa text ci occurred with the pair (ej, ek) in the corpus.
Clara is the particular city in which Intel is These context vectors are similar to document vec-
headquartered). We now turn to the REL-GRAMS tors from Information Retrieval (IR), and we lever-
component, which performs the latter task. age IR research to compare them, as described be-
2.3 Relation Assessment with REL-GRAMS low.
REALM’s relation assessment component, called To assess each extraction, we determine how sim-
REL-GRAMS, tests whether the extracted arguments ilar its context vector is to a canonical seed vec-
have a desired relationship, but given REALM’s min- tor (created by summing the context vectors of the
imal input it has no a priori information about the seeds). While there are many potential methods
relationship. REL-GRAMS relies instead on the dis- for determining similarity, in this work we rank ex-
tributional hypothesis to test each extraction. tractions by decreasing values of the BM25 dis-
As argued in Section 2.1, it is intractable to build tance metric. BM25 is a TF-IDF variant intro-
an accurate language model for context distributions duced in TREC-3(Robertson et al., 1992), which
surrounding sparse argument pairs. To overcome outperformed both the standard cosine distance and
this problem, we introduce relational n-gram mod- a smoothed KL divergence on our data.
els. Rather than simply modeling the context distri- 3 Experimental Results
bution around a given argument, a relational n-gram This section describes our experiments on IE assess-
model specifies separate context distributions for an ment for sparse data. We start by describing our
arguments conditioned on each of the other argu- experimental methodology, and then present our re-
ments with which it appears. The relational n-gram sults. The first experiment tests the hypothesis that
model allows us to estimate context distributions for HMM-T outperforms an n-gram-based method on
pairs of arguments, even when the arguments do not the task of type checking. The second experiment
appear together within a fixed window of n words. tests the hypothesis that REALM outperforms multi-
Further, by considering only consecutive argument ple approaches from previous work, and also outper-
pairs, the number of distinct argument pairs in the forms each of its HMM-T and REL-GRAMS compo-
model grows at most linearly with the number of nents taken in isolation.
sentences in the corpus. Thus, the relational n-gram 3.1 Experimental Methodology
model can scale. The corpus used for our experiments consisted of a
Formally, for a pair of arguments (e1, e2), a re- sample of sentences taken from Web pages. From
lational n-gram model estimates the distributions an initial crawl of nine million Web pages, we se-
P(w1, ..., wn|wi = e1, e1 H e2) for each 1 G_ i G_ lected sentences containing relations between proper
n, where the notation e1 H e2 indicates the event nouns. The resulting text corpus consisted of about
that e2 is the next argument to either the right or the
left of e1 in the corpus.
REL-GRAMS begins by building a relational n-
gram model of the arguments in the corpus. For
notational convenience, we represent the model’s
distributions in terms of “context vectors” for each
700
4Pre-computing the set C requires identifying in advance
the potential relation arguments in the corpus. We consider the
proper nouns identified by the LEX method (see Section 2.1) to
be the potential arguments.
three million sentences, and was tokenized as de-
scribed in Section 2. For tractability, before and after
performing tokenization, we replaced each token oc-
curring fewer than five times in the corpus with one
of two “unknown word” markers (one for capital-
ized words, and one for uncapitalized words). This
preprocessing resulted in a corpus containing about
sixty-five million total tokens, and 214,787 unique
tokens.
We evaluated performance on four relations:
Conquered, Founded, Headquartered, and
Merged. These four relations were chosen because
they typically take proper nouns as arguments, and
included a large number of sparse extractions. For
each relation R, the candidate extraction list ER was
obtained using TEXTRUNNER (Banko et al., 2007).
TEXTRUNNER is an IE system that computes an in-
dex of all extracted relationships it recognizes, in the
form of (object, predicate, object) triples. For each
of our target relations, we executed a single query
to the TEXTRUNNER index for extractions whose
predicate contained a phrase indicative of the rela-
tion (e.g., “founded by”, “headquartered in”), and
the results formed our extraction list. For each rela-
tion, the 10 most frequent extractions served as boot-
strapped seeds. All of the non-seed extractions were
sparse (no argument pairs were extracted more than
twice for a given relation). These test sets contained
a total of 361 extractions.
</bodyText>
<subsectionHeader confidence="0.999767">
3.2 Type Checking Experiments
</subsectionHeader>
<bodyText confidence="0.999894470588235">
As discussed in Section 2.2, on sparse data HMM-T
has the potential to outperform type checking meth-
ods that rely on textual similarities of context vec-
tors. To evaluate this claim, we tested the HMM-T
system against an N-GRAMS type checking method
on the task of type-checking the arguments to a re-
lation. The N-GRAMS method compares the context
vectors of extractions in the same way as the REL-
GRAMS method described in Section 2.3, but is not
relational (N-GRAMS considers the distribution of
each extraction argument independently, similar to
HMM-T). We tagged an extraction as type correct iff
both arguments were valid for the relation, ignoring
whether the relation held between the arguments.
The results of our type checking experiments are
shown in Table 1. For all types, HMM-T outper-
forms N-GRAMS, and HMM-T reduces error (mea-
</bodyText>
<table confidence="0.999341833333333">
Type HMM-T N-GRAMS
Conquered 0.917 0.767
Founded 0.827 0.636
Headquartered 0.734 0.589
Merged 0.920 0.854
Average 0.849 0.712
</table>
<tableCaption confidence="0.7320822">
Table 1: Type Checking Performance. Listed is area
under the precision/recall curve. HMM-T outper-
forms N-GRAMS for all relations, and reduces the
error in terms of missing area under the curve by
46% on average.
</tableCaption>
<bodyText confidence="0.99706775">
sured in missing area under the precision/recall
curve) by 46%. The performance difference on each
relation is statistically significant (p &lt; 0.01, two-
sampled t-test), using the methodology for measur-
ing the standard deviation of area under the preci-
sion/recall curve given in (Richardson and Domin-
gos, 2006). N-GRAMS, like REL-GRAMS, employs
the BM-25 metric to measure distributional similar-
ity between extractions and seeds. Replacing BM-
25 with cosine distance cuts HMM-T’s advantage
over N-GRAMS, but HMM-T’s error rate is still 23%
lower on average.
</bodyText>
<subsectionHeader confidence="0.998841">
3.3 Experiments with REALM
</subsectionHeader>
<bodyText confidence="0.999940571428572">
The REALM system combines the type checking
and relation assessment components to assess ex-
tractions. Here, we test the ability of REALM to
improve the ranking of a state of the art IE system,
TEXTRUNNER. For these experiments, we evalu-
ate REALM against the TEXTRUNNER frequency-
based ordering, a pattern-learning approach, and the
HMM-T and REL-GRAMS components taken in iso-
lation. The TEXTRUNNER frequency-based order-
ing ranks extractions in decreasing order of their ex-
traction frequency, and importantly, for our task this
ordering is essentially equivalent to that produced by
the “Urns” (Downey et al., 2005) and Pointwise Mu-
tual Information (Etzioni et al., 2005) approaches
employed in previous work.
The pattern-learning approach, denoted as PL, is
modeled after Snowball (Agichtein, 2006). The al-
gorithm and parameter settings for PL were those
manually tuned for the Headquartered relation
in previous work (Agichtein, 2005). A sensitivity
analysis of these parameters indicated that the re-
</bodyText>
<page confidence="0.993265">
701
</page>
<table confidence="0.998477125">
Conquered Founded Headquartered Merged Average
Avg. Prec. 0.698 0.578 0.400 0.742 0.605
TEXTRUNNER 0.738 0.699 0.710 0.784 0.733
PL 0.885 0.633 0.651 0.852 0.785
PL+ HMM-T 0.883 0.722 0.727 0.900 0.808
HMM-T 0.830 0.776 0.678 0.864 0.787
REL-GRAMS 0.929 (39%) 0.713 0.758 0.886 0.822
REALM 0.907 (19%) 0.781 (27%) 0.810 (35%) 0.908 (38%) 0.851 (39%)
</table>
<tableCaption confidence="0.99674">
Table 2: Performance of REALM for assessment of sparse extractions. Listed is area under the preci-
</tableCaption>
<bodyText confidence="0.950472041666666">
sion/recall curve for each method. In parentheses is the percentage reduction in error over the strongest
baseline method (TEXTRUNNER or PL) for each relation. “Avg. Prec.” denotes the fraction of correct
examples in the test set for each relation. REALM outperforms its REL-GRAMS and HMM-T components
taken in isolation, as well as the TEXTRUNNER and PL systems from previous work.
sults are sensitive to the parameter settings. How-
ever, we found no parameter settings that performed
significantly better, and many settings performed
significantly worse. As such, we believe our re-
sults reasonably reflect the performance of a pattern
learning system on this task. Because PL performs
relation assessment, we also attempted combining
PL with HMM-T in a hybrid method (PL+ HMM-T)
analogous to REALM.
The results of these experiments are shown in Ta-
ble 2. REALM outperforms the TEXTRUNNER and
PL baselines for all relations, and reduces the miss-
ing area under the curve by an average of 39% rel-
ative to the strongest baseline. The performance
differences between REALM and TEXTRUNNER are
statistically significant for all relations, as are differ-
ences between REALM and PL for all relations ex-
cept Conquered (p &lt; 0.01, two-sampled t-test).
The hybrid REALM system also outperforms each
of its components in isolation.
</bodyText>
<sectionHeader confidence="0.999989" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999952785714286">
To our knowledge, REALM is the first system to use
language modeling techniques for IE Assessment.
Redundancy-based approaches to pattern-based
IE assessment (Downey et al., 2005; Etzioni et al.,
2005) require that extractions appear relatively fre-
quently with a limited set of patterns. In contrast,
REALM utilizes all contexts to build a model of ex-
tractions, rather than a limited set of patterns. Our
experiments demonstrate that REALM outperforms
these approaches on sparse data.
Type checking using named-entity taggers has
been previously shown to improve the precision of
pattern-based IE systems (Agichtein, 2005; Feld-
man et al., 2006), but the HMM-T type-checking
component we develop differs from this work in im-
portant ways. Named-entity taggers are limited in
that they typically recognize only small set of types
(e.g., ORGANIZATION, LOCATION, PERSON),
and they require hand-tagged training data for each
type. HMM-T, by contrast, performs type check-
ing for any type. Finally, HMM-T does not require
hand-tagged training data.
Pattern learning is a common technique for ex-
tracting and assessing sparse data (e.g. (Agichtein,
2005; Riloff and Jones, 1999; Pas¸ca et al., 2006)).
Our experiments demonstrate that REALM outper-
forms a pattern learning system closely modeled af-
ter (Agichtein, 2005). REALM is inspired by pat-
tern learning techniques (in particular, both use the
distributional hypothesis to assess sparse data) but
is distinct in important ways. Pattern learning tech-
niques require substantial processing of the corpus
after the relations they assess have been specified.
Because of this, pattern learning systems are un-
suited to Open IE. Unlike these techniques, REALM
pre-computes language models which allow it to as-
sess extractions for arbitrary relations at run-time.
In essence, pattern-learning methods run in time lin-
ear in the number of relations whereas REALM’s run
time is constant in the number of relations. Thus,
REALM scales readily to large numbers of relations
whereas pattern-learning methods do not.
</bodyText>
<page confidence="0.991343">
702
</page>
<bodyText confidence="0.999942">
A second distinction of REALM is that its type
checker, unlike the named entity taggers employed
in pattern learning systems (e.g., Snowball), can be
used to identify arbitrary types. A final distinction is
that the language models REALM employs require
fewer parameters and heuristics than pattern learn-
ing techniques.
Similar distinctions exist between REALM and a
recent system designed to assess sparse extractions
by bootstrapping a classifier for each target relation
(Feldman et al., 2006). As in pattern learning, con-
structing the classifiers requires substantial process-
ing after the target relations have been specified, and
a set of hand-tagged examples per relation, making
it unsuitable for Open IE.
</bodyText>
<sectionHeader confidence="0.999826" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999984684210526">
This paper demonstrated that unsupervised language
models, as embodied in the REALM system, are an
effective means of assessing sparse extractions.
Another attractive feature of REALM is its scal-
ability. Scalability is a particularly important con-
cern for Open Information Extraction, the task of ex-
tracting large numbers of relations that are not spec-
ified in advance. Because HMM-T and REL-GRAMS
both pre-compute language models, REALM can be
queried efficiently to perform IE Assessment. Fur-
ther, the language models are constructed indepen-
dently of the target relations, allowing REALM to
perform IE Assessment even when relations are not
specified in advance.
In future work, we plan to develop a probabilistic
model of the information computed by REALM. We
also plan to evaluate the use of non-local context for
IE Assessment by integrating document-level mod-
eling techniques (e.g., Latent Dirichlet Allocation).
</bodyText>
<sectionHeader confidence="0.997529" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.987474125">
This research was supported in part by NSF grants
IIS-0535284 and IIS-0312988, DARPA contract
NBCHD030010, ONR grant N00014-05-1-0185 as
well as a gift from Google. The first author is sup-
ported by an MSR graduate fellowship sponsored by
Microsoft Live Labs. We thank Michele Banko, Jeff
Bilmes, Katrin Kirchhoff, and Alex Yates for helpful
comments.
</bodyText>
<sectionHeader confidence="0.993641" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999758733333333">
E. Agichtein. 2005. Extracting Relations From Large
Text Collections. Ph.D. thesis, Department of Com-
puter Science, Columbia University.
E. Agichtein. 2006. Confidence estimation methods for
partially supervised relation extraction. In SDM 2006.
M. Banko, M. Cararella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In Procs. of IJCAI 2007.
D. Downey, O. Etzioni, and S. Soderland. 2005. A Prob-
abilistic Model of Redundancy in Information Extrac-
tion. In Procs. of IJCAI 2005.
D. Downey, M. Broadhead, and O. Etzioni. 2007. Locat-
ing complex named entities in web text. In Procs. of
IJCAI 2007.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91–134.
R. Feldman, B. Rosenfeld, S. Soderland, and O. Etzioni.
2006. Self-supervised relation extraction from the
web. In ISMIS, pages 755–764.
Z. Harris. 1985. Distributional structure. In J. J. Katz,
editor, The Philosophy of Linguistics, pages 26–47.
New York: Oxford University Press.
C. D. Manning and H. Sch¨utze. 1999. Foundations of
Statistical Natural Language Processing.
M. Pas¸ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: Fact extrac-
tion in the fast lane. In Procs. of ACL/COLING 2006.
L. R. Rabiner. 1989. A tutorial on hidden markov models
and selected applications in speech recognition. Pro-
ceedings of the IEEE, 77(2):257–286.
D. Ravichandran, P. Pantel, and E. H. Hovy. 2005. Ran-
domized Algorithms and NLP: Using Locality Sensi-
tive Hash Functions for High Speed Noun Clustering.
In Procs. of ACL 2005.
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62(1-2):107–136.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-level Boot-strapping.
In Procs. of AAAI-99, pages 1044–1049.
S. E. Robertson, S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at TREC-3. In
Text REtrieval Conference, pages 21–30.
</reference>
<page confidence="0.998994">
703
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.732688">
<title confidence="0.9992935">Sparse Information Extraction: Unsupervised Language Models to the Rescue</title>
<author confidence="0.999284">Doug Downey</author>
<author confidence="0.999284">Stefan Schoenmackers</author>
<author confidence="0.999284">Oren Etzioni</author>
<affiliation confidence="0.946529">Turing Center, Department of Computer Science and Engineering</affiliation>
<address confidence="0.96916">University of Washington, Box 352350 Seattle, WA 98195, USA</address>
<email confidence="0.992288">Iddowney,stef,etzionif@cs.washington.edu</email>
<abstract confidence="0.992333954545455">Even in a massive corpus such as the Web, a substantial fraction of extractions appear infrequently. This paper shows how to assess correctness of by utilizing unsupervised language models. The which combines HMMand language models, ranks candidate extractions by the likelihood that they are correct. Our experiments that extraction error by 39%, on average, when compared with previous work. language models based on its corpus and does not reseeds, it is far more scalable than approaches that learn models for each individual relation from handdata. Thus, ideally suited for open information extraction where the relations of interest are not specified in advance and their number is potentially vast.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
</authors>
<title>Extracting Relations From Large Text Collections.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Columbia University.</institution>
<contexts>
<context position="24561" citStr="Agichtein, 2005" startWordPosition="3993" endWordPosition="3994">ach, and the HMM-T and REL-GRAMS components taken in isolation. The TEXTRUNNER frequency-based ordering ranks extractions in decreasing order of their extraction frequency, and importantly, for our task this ordering is essentially equivalent to that produced by the “Urns” (Downey et al., 2005) and Pointwise Mutual Information (Etzioni et al., 2005) approaches employed in previous work. The pattern-learning approach, denoted as PL, is modeled after Snowball (Agichtein, 2006). The algorithm and parameter settings for PL were those manually tuned for the Headquartered relation in previous work (Agichtein, 2005). A sensitivity analysis of these parameters indicated that the re701 Conquered Founded Headquartered Merged Average Avg. Prec. 0.698 0.578 0.400 0.742 0.605 TEXTRUNNER 0.738 0.699 0.710 0.784 0.733 PL 0.885 0.633 0.651 0.852 0.785 PL+ HMM-T 0.883 0.722 0.727 0.900 0.808 HMM-T 0.830 0.776 0.678 0.864 0.787 REL-GRAMS 0.929 (39%) 0.713 0.758 0.886 0.822 REALM 0.907 (19%) 0.781 (27%) 0.810 (35%) 0.908 (38%) 0.851 (39%) Table 2: Performance of REALM for assessment of sparse extractions. Listed is area under the precision/recall curve for each method. In parentheses is the percentage reduction in e</context>
<context position="27032" citStr="Agichtein, 2005" startWordPosition="4380" endWordPosition="4381"> knowledge, REALM is the first system to use language modeling techniques for IE Assessment. Redundancy-based approaches to pattern-based IE assessment (Downey et al., 2005; Etzioni et al., 2005) require that extractions appear relatively frequently with a limited set of patterns. In contrast, REALM utilizes all contexts to build a model of extractions, rather than a limited set of patterns. Our experiments demonstrate that REALM outperforms these approaches on sparse data. Type checking using named-entity taggers has been previously shown to improve the precision of pattern-based IE systems (Agichtein, 2005; Feldman et al., 2006), but the HMM-T type-checking component we develop differs from this work in important ways. Named-entity taggers are limited in that they typically recognize only small set of types (e.g., ORGANIZATION, LOCATION, PERSON), and they require hand-tagged training data for each type. HMM-T, by contrast, performs type checking for any type. Finally, HMM-T does not require hand-tagged training data. Pattern learning is a common technique for extracting and assessing sparse data (e.g. (Agichtein, 2005; Riloff and Jones, 1999; Pas¸ca et al., 2006)). Our experiments demonstrate t</context>
</contexts>
<marker>Agichtein, 2005</marker>
<rawString>E. Agichtein. 2005. Extracting Relations From Large Text Collections. Ph.D. thesis, Department of Computer Science, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agichtein</author>
</authors>
<title>Confidence estimation methods for partially supervised relation extraction.</title>
<date>2006</date>
<booktitle>In SDM</booktitle>
<contexts>
<context position="24424" citStr="Agichtein, 2006" startWordPosition="3972" endWordPosition="3973"> IE system, TEXTRUNNER. For these experiments, we evaluate REALM against the TEXTRUNNER frequencybased ordering, a pattern-learning approach, and the HMM-T and REL-GRAMS components taken in isolation. The TEXTRUNNER frequency-based ordering ranks extractions in decreasing order of their extraction frequency, and importantly, for our task this ordering is essentially equivalent to that produced by the “Urns” (Downey et al., 2005) and Pointwise Mutual Information (Etzioni et al., 2005) approaches employed in previous work. The pattern-learning approach, denoted as PL, is modeled after Snowball (Agichtein, 2006). The algorithm and parameter settings for PL were those manually tuned for the Headquartered relation in previous work (Agichtein, 2005). A sensitivity analysis of these parameters indicated that the re701 Conquered Founded Headquartered Merged Average Avg. Prec. 0.698 0.578 0.400 0.742 0.605 TEXTRUNNER 0.738 0.699 0.710 0.784 0.733 PL 0.885 0.633 0.651 0.852 0.785 PL+ HMM-T 0.883 0.722 0.727 0.900 0.808 HMM-T 0.830 0.776 0.678 0.864 0.787 REL-GRAMS 0.929 (39%) 0.713 0.758 0.886 0.822 REALM 0.907 (19%) 0.781 (27%) 0.810 (35%) 0.908 (38%) 0.851 (39%) Table 2: Performance of REALM for assessmen</context>
</contexts>
<marker>Agichtein, 2006</marker>
<rawString>E. Agichtein. 2006. Confidence estimation methods for partially supervised relation extraction. In SDM 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M Cararella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open information extraction from the web. In</title>
<date>2007</date>
<booktitle>Procs. of IJCAI</booktitle>
<contexts>
<context position="2767" citStr="Banko et al., 2007" startWordPosition="430" endWordPosition="433">ularly acute when moving beyond unary relations. We refer to this challenge as the task of assessing sparse extractions. This paper introduces the idea that language modeling techniques such as n-gram statistics (Manning and Sch¨utze, 1999) and HMMs (Rabiner, 1989) can be used to effectively assess sparse extractions. The paper introduces the REALM system, and highlights its unique properties. Notably, REALM does not require any hand-tagged seeds, which enables it to scale to Open IE—extraction where the relations of interest are not specified in advance, and their number is potentially vast (Banko et al., 2007). REALM is based on two key hypotheses. The KnowItAll hypothesis is that extractions that occur more frequently in distinct sentences in the corpus are more likely to be correct. For example, the hypothesis suggests that the argument pair (Giuliani, New York) is relatively likely to be appropriate for the Mayor relation, simply because this pair is extracted for the Mayor relation relatively frequently. Second, we employ an instance of the distributional hypothesis (Harris, 1985), which 696 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 696–703, P</context>
<context position="21187" citStr="Banko et al., 2007" startWordPosition="3459" endWordPosition="3462"> replaced each token occurring fewer than five times in the corpus with one of two “unknown word” markers (one for capitalized words, and one for uncapitalized words). This preprocessing resulted in a corpus containing about sixty-five million total tokens, and 214,787 unique tokens. We evaluated performance on four relations: Conquered, Founded, Headquartered, and Merged. These four relations were chosen because they typically take proper nouns as arguments, and included a large number of sparse extractions. For each relation R, the candidate extraction list ER was obtained using TEXTRUNNER (Banko et al., 2007). TEXTRUNNER is an IE system that computes an index of all extracted relationships it recognizes, in the form of (object, predicate, object) triples. For each of our target relations, we executed a single query to the TEXTRUNNER index for extractions whose predicate contained a phrase indicative of the relation (e.g., “founded by”, “headquartered in”), and the results formed our extraction list. For each relation, the 10 most frequent extractions served as bootstrapped seeds. All of the non-seed extractions were sparse (no argument pairs were extracted more than twice for a given relation). Th</context>
</contexts>
<marker>Banko, Cararella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>M. Banko, M. Cararella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open information extraction from the web. In Procs. of IJCAI 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Downey</author>
<author>O Etzioni</author>
<author>S Soderland</author>
</authors>
<title>A Probabilistic Model of Redundancy in Information Extraction.</title>
<date>2005</date>
<booktitle>In Procs. of IJCAI</booktitle>
<contexts>
<context position="1392" citStr="Downey et al., 2005" startWordPosition="208" endWordPosition="211">e REALM pre-computes language models based on its corpus and does not require any hand-tagged seeds, it is far more scalable than approaches that learn models for each individual relation from handtagged data. Thus, REALM is ideally suited for open information extraction where the relations of interest are not specified in advance and their number is potentially vast. 1 Introduction Information Extraction (IE) from text is far from infallible. In response, researchers have begun to exploit the redundancy in massive corpora such as the Web in order to assess the veracity of extractions (e.g., (Downey et al., 2005; Etzioni et al., 2005; Feldman et al., 2006)). In essence, such methods utilize extraction patterns to generate candidate extractions (e.g., “Istanbul”) and then assess each candidate by computing co-occurrence statistics between the extraction and words or phrases indicative of class membership (e.g., “cities such as”). However, Zipf’s Law governs the distribution of extractions. Thus, even the Web has limited redundancy for less prominent instances of relations. Indeed, 50% of the extractions in the data sets employed by (Downey et al., 2005) appeared only once. As a result, Downey et al.’s</context>
<context position="24240" citStr="Downey et al., 2005" startWordPosition="3944" endWordPosition="3947"> REALM The REALM system combines the type checking and relation assessment components to assess extractions. Here, we test the ability of REALM to improve the ranking of a state of the art IE system, TEXTRUNNER. For these experiments, we evaluate REALM against the TEXTRUNNER frequencybased ordering, a pattern-learning approach, and the HMM-T and REL-GRAMS components taken in isolation. The TEXTRUNNER frequency-based ordering ranks extractions in decreasing order of their extraction frequency, and importantly, for our task this ordering is essentially equivalent to that produced by the “Urns” (Downey et al., 2005) and Pointwise Mutual Information (Etzioni et al., 2005) approaches employed in previous work. The pattern-learning approach, denoted as PL, is modeled after Snowball (Agichtein, 2006). The algorithm and parameter settings for PL were those manually tuned for the Headquartered relation in previous work (Agichtein, 2005). A sensitivity analysis of these parameters indicated that the re701 Conquered Founded Headquartered Merged Average Avg. Prec. 0.698 0.578 0.400 0.742 0.605 TEXTRUNNER 0.738 0.699 0.710 0.784 0.733 PL 0.885 0.633 0.651 0.852 0.785 PL+ HMM-T 0.883 0.722 0.727 0.900 0.808 HMM-T 0</context>
<context position="26589" citStr="Downey et al., 2005" startWordPosition="4311" endWordPosition="4314"> PL baselines for all relations, and reduces the missing area under the curve by an average of 39% relative to the strongest baseline. The performance differences between REALM and TEXTRUNNER are statistically significant for all relations, as are differences between REALM and PL for all relations except Conquered (p &lt; 0.01, two-sampled t-test). The hybrid REALM system also outperforms each of its components in isolation. 4 Related Work To our knowledge, REALM is the first system to use language modeling techniques for IE Assessment. Redundancy-based approaches to pattern-based IE assessment (Downey et al., 2005; Etzioni et al., 2005) require that extractions appear relatively frequently with a limited set of patterns. In contrast, REALM utilizes all contexts to build a model of extractions, rather than a limited set of patterns. Our experiments demonstrate that REALM outperforms these approaches on sparse data. Type checking using named-entity taggers has been previously shown to improve the precision of pattern-based IE systems (Agichtein, 2005; Feldman et al., 2006), but the HMM-T type-checking component we develop differs from this work in important ways. Named-entity taggers are limited in that </context>
</contexts>
<marker>Downey, Etzioni, Soderland, 2005</marker>
<rawString>D. Downey, O. Etzioni, and S. Soderland. 2005. A Probabilistic Model of Redundancy in Information Extraction. In Procs. of IJCAI 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Downey</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Locating complex named entities in web text.</title>
<date>2007</date>
<booktitle>In Procs. of IJCAI</booktitle>
<contexts>
<context position="11324" citStr="Downey et al., 2007" startWordPosition="1826" endWordPosition="1829">tributions when extractions are sparse. REALM executes the type checking and relation assessment components separately; each component takes the seed and non-seed extractions as arguments and returns a ranking of the non-seeds. REALM then combines the two components’ assessments into a single ranking. Although several such combinations are possible, REALM simply ranks the extractions in ascending order of the product of the ranks assigned by the two components. The following subsections describe REALM’s two components in detail. We identify the proper nouns in our corpus using the LEX method (Downey et al., 2007). In addition to locating the proper nouns in the corpus, LEX also concatenates each multi-token proper noun (e.g.,Los Angeles) together into a single token. Both of REALM’s components construct language models from this tokenized corpus. 2.2 Type Checking with HMM-T In this section, we describe our type-checking component, which takes the form of a Hidden Markov Model and is referred to as HMM-T. HMM-T ranks the set UR of non-seed extractions, with a goal of ranking those extractions with arguments of proper type for R above extractions containing type errors. Formally, let URi denote the set</context>
</contexts>
<marker>Downey, Broadhead, Etzioni, 2007</marker>
<rawString>D. Downey, M. Broadhead, and O. Etzioni. 2007. Locating complex named entities in web text. In Procs. of IJCAI 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>S Kok</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="1414" citStr="Etzioni et al., 2005" startWordPosition="212" endWordPosition="215">language models based on its corpus and does not require any hand-tagged seeds, it is far more scalable than approaches that learn models for each individual relation from handtagged data. Thus, REALM is ideally suited for open information extraction where the relations of interest are not specified in advance and their number is potentially vast. 1 Introduction Information Extraction (IE) from text is far from infallible. In response, researchers have begun to exploit the redundancy in massive corpora such as the Web in order to assess the veracity of extractions (e.g., (Downey et al., 2005; Etzioni et al., 2005; Feldman et al., 2006)). In essence, such methods utilize extraction patterns to generate candidate extractions (e.g., “Istanbul”) and then assess each candidate by computing co-occurrence statistics between the extraction and words or phrases indicative of class membership (e.g., “cities such as”). However, Zipf’s Law governs the distribution of extractions. Thus, even the Web has limited redundancy for less prominent instances of relations. Indeed, 50% of the extractions in the data sets employed by (Downey et al., 2005) appeared only once. As a result, Downey et al.’s model, and related me</context>
<context position="24296" citStr="Etzioni et al., 2005" startWordPosition="3953" endWordPosition="3956">relation assessment components to assess extractions. Here, we test the ability of REALM to improve the ranking of a state of the art IE system, TEXTRUNNER. For these experiments, we evaluate REALM against the TEXTRUNNER frequencybased ordering, a pattern-learning approach, and the HMM-T and REL-GRAMS components taken in isolation. The TEXTRUNNER frequency-based ordering ranks extractions in decreasing order of their extraction frequency, and importantly, for our task this ordering is essentially equivalent to that produced by the “Urns” (Downey et al., 2005) and Pointwise Mutual Information (Etzioni et al., 2005) approaches employed in previous work. The pattern-learning approach, denoted as PL, is modeled after Snowball (Agichtein, 2006). The algorithm and parameter settings for PL were those manually tuned for the Headquartered relation in previous work (Agichtein, 2005). A sensitivity analysis of these parameters indicated that the re701 Conquered Founded Headquartered Merged Average Avg. Prec. 0.698 0.578 0.400 0.742 0.605 TEXTRUNNER 0.738 0.699 0.710 0.784 0.733 PL 0.885 0.633 0.651 0.852 0.785 PL+ HMM-T 0.883 0.722 0.727 0.900 0.808 HMM-T 0.830 0.776 0.678 0.864 0.787 REL-GRAMS 0.929 (39%) 0.713</context>
<context position="26612" citStr="Etzioni et al., 2005" startWordPosition="4315" endWordPosition="4318"> relations, and reduces the missing area under the curve by an average of 39% relative to the strongest baseline. The performance differences between REALM and TEXTRUNNER are statistically significant for all relations, as are differences between REALM and PL for all relations except Conquered (p &lt; 0.01, two-sampled t-test). The hybrid REALM system also outperforms each of its components in isolation. 4 Related Work To our knowledge, REALM is the first system to use language modeling techniques for IE Assessment. Redundancy-based approaches to pattern-based IE assessment (Downey et al., 2005; Etzioni et al., 2005) require that extractions appear relatively frequently with a limited set of patterns. In contrast, REALM utilizes all contexts to build a model of extractions, rather than a limited set of patterns. Our experiments demonstrate that REALM outperforms these approaches on sparse data. Type checking using named-entity taggers has been previously shown to improve the precision of pattern-based IE systems (Agichtein, 2005; Feldman et al., 2006), but the HMM-T type-checking component we develop differs from this work in important ways. Named-entity taggers are limited in that they typically recogniz</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Kok, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu, T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Feldman</author>
<author>B Rosenfeld</author>
<author>S Soderland</author>
<author>O Etzioni</author>
</authors>
<title>Self-supervised relation extraction from the web. In</title>
<date>2006</date>
<booktitle>ISMIS,</booktitle>
<pages>755--764</pages>
<contexts>
<context position="1437" citStr="Feldman et al., 2006" startWordPosition="216" endWordPosition="219">on its corpus and does not require any hand-tagged seeds, it is far more scalable than approaches that learn models for each individual relation from handtagged data. Thus, REALM is ideally suited for open information extraction where the relations of interest are not specified in advance and their number is potentially vast. 1 Introduction Information Extraction (IE) from text is far from infallible. In response, researchers have begun to exploit the redundancy in massive corpora such as the Web in order to assess the veracity of extractions (e.g., (Downey et al., 2005; Etzioni et al., 2005; Feldman et al., 2006)). In essence, such methods utilize extraction patterns to generate candidate extractions (e.g., “Istanbul”) and then assess each candidate by computing co-occurrence statistics between the extraction and words or phrases indicative of class membership (e.g., “cities such as”). However, Zipf’s Law governs the distribution of extractions. Thus, even the Web has limited redundancy for less prominent instances of relations. Indeed, 50% of the extractions in the data sets employed by (Downey et al., 2005) appeared only once. As a result, Downey et al.’s model, and related methods, had no way of as</context>
<context position="27055" citStr="Feldman et al., 2006" startWordPosition="4382" endWordPosition="4386"> is the first system to use language modeling techniques for IE Assessment. Redundancy-based approaches to pattern-based IE assessment (Downey et al., 2005; Etzioni et al., 2005) require that extractions appear relatively frequently with a limited set of patterns. In contrast, REALM utilizes all contexts to build a model of extractions, rather than a limited set of patterns. Our experiments demonstrate that REALM outperforms these approaches on sparse data. Type checking using named-entity taggers has been previously shown to improve the precision of pattern-based IE systems (Agichtein, 2005; Feldman et al., 2006), but the HMM-T type-checking component we develop differs from this work in important ways. Named-entity taggers are limited in that they typically recognize only small set of types (e.g., ORGANIZATION, LOCATION, PERSON), and they require hand-tagged training data for each type. HMM-T, by contrast, performs type checking for any type. Finally, HMM-T does not require hand-tagged training data. Pattern learning is a common technique for extracting and assessing sparse data (e.g. (Agichtein, 2005; Riloff and Jones, 1999; Pas¸ca et al., 2006)). Our experiments demonstrate that REALM outperforms a</context>
<context position="28956" citStr="Feldman et al., 2006" startWordPosition="4673" endWordPosition="4676">e number of relations. Thus, REALM scales readily to large numbers of relations whereas pattern-learning methods do not. 702 A second distinction of REALM is that its type checker, unlike the named entity taggers employed in pattern learning systems (e.g., Snowball), can be used to identify arbitrary types. A final distinction is that the language models REALM employs require fewer parameters and heuristics than pattern learning techniques. Similar distinctions exist between REALM and a recent system designed to assess sparse extractions by bootstrapping a classifier for each target relation (Feldman et al., 2006). As in pattern learning, constructing the classifiers requires substantial processing after the target relations have been specified, and a set of hand-tagged examples per relation, making it unsuitable for Open IE. 5 Conclusions This paper demonstrated that unsupervised language models, as embodied in the REALM system, are an effective means of assessing sparse extractions. Another attractive feature of REALM is its scalability. Scalability is a particularly important concern for Open Information Extraction, the task of extracting large numbers of relations that are not specified in advance.</context>
</contexts>
<marker>Feldman, Rosenfeld, Soderland, Etzioni, 2006</marker>
<rawString>R. Feldman, B. Rosenfeld, S. Soderland, and O. Etzioni. 2006. Self-supervised relation extraction from the web. In ISMIS, pages 755–764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Distributional structure. In</title>
<date>1985</date>
<booktitle>The Philosophy of Linguistics,</booktitle>
<pages>26--47</pages>
<editor>J. J. Katz, editor,</editor>
<publisher>University Press.</publisher>
<location>New York: Oxford</location>
<contexts>
<context position="3251" citStr="Harris, 1985" startWordPosition="509" endWordPosition="510">xtraction where the relations of interest are not specified in advance, and their number is potentially vast (Banko et al., 2007). REALM is based on two key hypotheses. The KnowItAll hypothesis is that extractions that occur more frequently in distinct sentences in the corpus are more likely to be correct. For example, the hypothesis suggests that the argument pair (Giuliani, New York) is relatively likely to be appropriate for the Mayor relation, simply because this pair is extracted for the Mayor relation relatively frequently. Second, we employ an instance of the distributional hypothesis (Harris, 1985), which 696 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 696–703, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics can be phrased as follows: different instances of More formally, the list of candidate extracthe same semantic relation tend to appear in sim- tions for a relation R is denoted as ER = ilar textual contexts. We assess sparse extractions {(a1, b1), ... , (am, bm)}. An extraction (ai, bi) is by comparing the contexts in which they appear to an ordered pair of strings. The extraction is correct those </context>
</contexts>
<marker>Harris, 1985</marker>
<rawString>Z. Harris. 1985. Distributional structure. In J. J. Katz, editor, The Philosophy of Linguistics, pages 26–47. New York: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<date>1999</date>
<journal>Foundations of Statistical Natural Language Processing.</journal>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>C. D. Manning and H. Sch¨utze. 1999. Foundations of Statistical Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
<author>D Lin</author>
<author>J Bigham</author>
<author>A Lifchits</author>
<author>A Jain</author>
</authors>
<title>Names and similarities on the web: Fact extraction in the fast lane.</title>
<date>2006</date>
<booktitle>In Procs. of ACL/COLING</booktitle>
<marker>Pas¸ca, Lin, Bigham, Lifchits, Jain, 2006</marker>
<rawString>M. Pas¸ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. 2006. Names and similarities on the web: Fact extraction in the fast lane. In Procs. of ACL/COLING 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="2413" citStr="Rabiner, 1989" startWordPosition="376" endWordPosition="377"> limited redundancy for less prominent instances of relations. Indeed, 50% of the extractions in the data sets employed by (Downey et al., 2005) appeared only once. As a result, Downey et al.’s model, and related methods, had no way of assessing which extraction is more likely to be correct for fully half of the extractions. This problem is particularly acute when moving beyond unary relations. We refer to this challenge as the task of assessing sparse extractions. This paper introduces the idea that language modeling techniques such as n-gram statistics (Manning and Sch¨utze, 1999) and HMMs (Rabiner, 1989) can be used to effectively assess sparse extractions. The paper introduces the REALM system, and highlights its unique properties. Notably, REALM does not require any hand-tagged seeds, which enables it to scale to Open IE—extraction where the relations of interest are not specified in advance, and their number is potentially vast (Banko et al., 2007). REALM is based on two key hypotheses. The KnowItAll hypothesis is that extractions that occur more frequently in distinct sentences in the corpus are more likely to be correct. For example, the hypothesis suggests that the argument pair (Giulia</context>
<context position="13256" citStr="Rabiner, 1989" startWordPosition="2159" endWordPosition="2160"> generated by a single hidden state variable. Here, the hidden states take integral values from 11, ... , T}, and each hidden state variable is itself generated by some number k of previous hidden states.2 Formally, the joint distribution of the corpus, represented as a vector of tokens w, given a corresponding vector of states t is: P (w|t) = � P(wi|ti)P(ti|ti−1, ... , ti−k) (1) i The distributions on the right side of Equation 1 can be learned from a corpus in an unsupervised manner, such that words which are distributed similarly in the corpus tend to be generated by similar hidden states (Rabiner, 1989). The generative model is depicted as a Bayesian network in Figure 2. The figure also illustrates the one way in which our implementation is distinct from a standard HMM, namely that proper nouns are detected a priori and modeled as single tokens (e.g., Santa Clara is generated by a single hidden state). This allows the type checker to compare the state distributions of different proper nouns directly, even when the proper nouns contain differing numbers of words. To generate a ranking of UR using the learned HMM parameters, we rank the arguments ei according to how similar their state distrib</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>P Pantel</author>
<author>E H Hovy</author>
</authors>
<title>Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering.</title>
<date>2005</date>
<booktitle>In Procs. of ACL</booktitle>
<contexts>
<context position="14490" citStr="Ravichandran et al., 2005" startWordPosition="2368" endWordPosition="2371">(t|ei) 2Our implementation makes the simplifying assumption that each sentence in the corpus is generated independently. are to those of the seed arguments.3 Specifically, we define a function: KL( Ew&apos;ESRi P(t |w,) , P(t |ei)) (2) |SRi | where KL represents KL divergence, and the outer sum is taken over the arguments ei of the extraction e. We rank the elements of UR in ascending order of f(e). HMM-T has two advantages over a more traditional type checking approach of simply counting the number of times in the corpus that each extraction appears in a context in which a seed also appears (cf. (Ravichandran et al., 2005)). The first advantage of HMM-T is efficiency, as the traditional approach involves a computationally expensive step of retrieving the potentially large set of contexts in which the extractions and seeds appear. In our experiments, using HMM-T instead of a context-based approach results in a 10-50x reduction in the amount of data that is retrieved to perform type checking. Secondly, on sparse data HMM-T has the potential to improve type checking accuracy. For example, consider comparing Pickerington, a sparse candidate argument of the type City, to the seed argument Chicago, for which the foll</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>D. Ravichandran, P. Pantel, and E. H. Hovy. 2005. Randomized Algorithms and NLP: Using Locality Sensitive Hash Functions for High Speed Noun Clustering. In Procs. of ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>Markov Logic Networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--1</pages>
<contexts>
<context position="23353" citStr="Richardson and Domingos, 2006" startWordPosition="3806" endWordPosition="3810">MS Conquered 0.917 0.767 Founded 0.827 0.636 Headquartered 0.734 0.589 Merged 0.920 0.854 Average 0.849 0.712 Table 1: Type Checking Performance. Listed is area under the precision/recall curve. HMM-T outperforms N-GRAMS for all relations, and reduces the error in terms of missing area under the curve by 46% on average. sured in missing area under the precision/recall curve) by 46%. The performance difference on each relation is statistically significant (p &lt; 0.01, twosampled t-test), using the methodology for measuring the standard deviation of area under the precision/recall curve given in (Richardson and Domingos, 2006). N-GRAMS, like REL-GRAMS, employs the BM-25 metric to measure distributional similarity between extractions and seeds. Replacing BM25 with cosine distance cuts HMM-T’s advantage over N-GRAMS, but HMM-T’s error rate is still 23% lower on average. 3.3 Experiments with REALM The REALM system combines the type checking and relation assessment components to assess extractions. Here, we test the ability of REALM to improve the ranking of a state of the art IE system, TEXTRUNNER. For these experiments, we evaluate REALM against the TEXTRUNNER frequencybased ordering, a pattern-learning approach, and</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>M. Richardson and P. Domingos. 2006. Markov Logic Networks. Machine Learning, 62(1-2):107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-level Boot-strapping.</title>
<date>1999</date>
<booktitle>In Procs. of AAAI-99,</booktitle>
<pages>1044--1049</pages>
<contexts>
<context position="27578" citStr="Riloff and Jones, 1999" startWordPosition="4463" endWordPosition="4466"> shown to improve the precision of pattern-based IE systems (Agichtein, 2005; Feldman et al., 2006), but the HMM-T type-checking component we develop differs from this work in important ways. Named-entity taggers are limited in that they typically recognize only small set of types (e.g., ORGANIZATION, LOCATION, PERSON), and they require hand-tagged training data for each type. HMM-T, by contrast, performs type checking for any type. Finally, HMM-T does not require hand-tagged training data. Pattern learning is a common technique for extracting and assessing sparse data (e.g. (Agichtein, 2005; Riloff and Jones, 1999; Pas¸ca et al., 2006)). Our experiments demonstrate that REALM outperforms a pattern learning system closely modeled after (Agichtein, 2005). REALM is inspired by pattern learning techniques (in particular, both use the distributional hypothesis to assess sparse data) but is distinct in important ways. Pattern learning techniques require substantial processing of the corpus after the relations they assess have been specified. Because of this, pattern learning systems are unsuited to Open IE. Unlike these techniques, REALM pre-computes language models which allow it to assess extractions for a</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning Dictionaries for Information Extraction by Multi-level Boot-strapping. In Procs. of AAAI-99, pages 1044–1049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
<author>M Hancock-Beaulieu</author>
<author>A Gull</author>
<author>M Lau</author>
</authors>
<title>Okapi at TREC-3.</title>
<date>1992</date>
<booktitle>In Text REtrieval Conference,</booktitle>
<pages>21--30</pages>
<contexts>
<context position="18149" citStr="Robertson et al., 1992" startWordPosition="2975" endWordPosition="2978">xt vector is to a canonical seed vechave a desired relationship, but given REALM’s min- tor (created by summing the context vectors of the imal input it has no a priori information about the seeds). While there are many potential methods relationship. REL-GRAMS relies instead on the dis- for determining similarity, in this work we rank extributional hypothesis to test each extraction. tractions by decreasing values of the BM25 disAs argued in Section 2.1, it is intractable to build tance metric. BM25 is a TF-IDF variant introan accurate language model for context distributions duced in TREC-3(Robertson et al., 1992), which surrounding sparse argument pairs. To overcome outperformed both the standard cosine distance and this problem, we introduce relational n-gram mod- a smoothed KL divergence on our data. els. Rather than simply modeling the context distri- 3 Experimental Results bution around a given argument, a relational n-gram This section describes our experiments on IE assessmodel specifies separate context distributions for an ment for sparse data. We start by describing our arguments conditioned on each of the other argu- experimental methodology, and then present our rements with which it appear</context>
</contexts>
<marker>Robertson, Walker, Hancock-Beaulieu, Gull, Lau, 1992</marker>
<rawString>S. E. Robertson, S. Walker, M. Hancock-Beaulieu, A. Gull, and M. Lau. 1992. Okapi at TREC-3. In Text REtrieval Conference, pages 21–30.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>