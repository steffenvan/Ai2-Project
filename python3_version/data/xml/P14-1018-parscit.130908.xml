<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000039">
<title confidence="0.989546">
Inferring User Political Preferences from Streaming Communications
</title>
<author confidence="0.99367">
Svitlana Volkova,&apos; Glen Coppersmith&apos; and Benjamin Van Durme&apos;,&apos;
</author>
<affiliation confidence="0.936812">
&apos;Center for Language and Speech Processing,
&apos;Human Language Technology Center of Excellence,
Johns Hopkins University, Baltimore, MD 21218
</affiliation>
<email confidence="0.995433">
svitlana@jhu.edu, coppersmith@jhu.edu, vandurme@cs.jhu.edu
</email>
<sectionHeader confidence="0.993881" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99969647826087">
Existing models for social media per-
sonal analytics assume access to thou-
sands of messages per user, even though
most users author content only sporadi-
cally over time. Given this sparsity, we:
(i) leverage content from the local neigh-
borhood of a user; (ii) evaluate batch mod-
els as a function of size and the amount
of messages in various types of neighbor-
hoods; and (iii) estimate the amount of
time and tweets required for a dynamic
model to predict user preferences. We
show that even when limited or no self-
authored data is available, language from
friend, retweet and user mention commu-
nications provide sufficient evidence for
prediction. When updating models over
time based on Twitter, we find that polit-
ical preference can be often be predicted
using roughly 100 tweets, depending on
the context of user selection, where this
could mean hours, or weeks, based on the
author’s tweeting frequency.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999488833333333">
Inferring latent user attributes such as gender, age,
and political preferences (Rao et al., 2011; Za-
mal et al., 2012; Cohen and Ruths, 2013) auto-
matically from personal communications and so-
cial media including emails, blog posts or public
discussions has become increasingly popular with
the web getting more social and volume of data
available. Resources like Twitter1 or Facebook2
become extremely valuable for studying the un-
derlying properties of such informal communica-
tions because of its volume, dynamic nature, and
diverse population (Lunden, 2012; Smith, 2013).
</bodyText>
<footnote confidence="0.9997725">
1http://www.demographicspro.com/
2http://www.wolframalpha.com/facebook/
</footnote>
<bodyText confidence="0.999306731707317">
The existing batch models for predicting latent
user attributes rely on thousands of tweets per
author (Rao et al., 2010; Conover et al., 2011;
Pennacchiotti and Popescu, 2011a; Burger et al.,
2011; Zamal et al., 2012; Nguyen et al., 2013).
However, most Twitter users are less prolific than
those examined in these works, and thus do not
produce the thousands of tweets required to obtain
their levels of accuracy e.g., the median number of
tweets produced by a random Twitter user per day
is 10. Moreover, recent changes to Twitter API
querying rates further restrict the speed of access
to this resource, effectively reducing the amount of
data that can be collected in a given time period.
In this paper we analyze and go beyond static
models formulating personal analytics in social
media as a streaming task. We first evaluate batch
models that are cognizant of low-resource predic-
tion setting described above, maximizing the effi-
ciency of content in calculating personal analytics.
To the best of our knowledge, this is the first work
that makes explicit the tradeoff between accuracy
and cost (manifest as calls to the Twitter API),
and optimizes to a different tradeoff than state-of-
the-art approaches, seeking maximal performance
when limited data is available. In addition, we
propose streaming models for personal analytics
that dynamically update user labels based on their
stream of communications which has been ad-
dressed previously by Van Durme (2012b). Such
models better capture the real-time nature of evi-
dence being used in latent author attribute predic-
tions tasks. Our main contributions include:
- develop low-resource and real-time dynamic
approaches for personal analytics using as an
example the prediction of political preference
of Twitter users;
- examine the relative utility of six different
notions of “similarity” between users in an
implicit Twitter social network for personal
analytics;
</bodyText>
<page confidence="0.979798">
186
</page>
<note confidence="0.8310505">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 186–196,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999829666666667">
- experiments are performed across multiple
datasets supporting the prediction of politi-
cal preference in Twitter, to highlight the sig-
nificant differences in performance that arise
from the underlying collection and annota-
tion strategies.
</bodyText>
<sectionHeader confidence="0.938109" genericHeader="introduction">
2 Identifying Twitter Social Graph
</sectionHeader>
<bodyText confidence="0.9996992">
Twitter users interact with one another and en-
gage in direct communication in different ways
e.g., using retweets, user mentions e.g., @youtube
or hashtags e.g., #tcot, in addition to having ex-
plicit connections among themselves such as fol-
lowing, friending. To investigate all types of social
relationships between Twitter users and construct
Twitter social graphs we collect lists of followers
and friends, and extract user mentions, hashtags,
replies and retweets from communications.3
</bodyText>
<subsectionHeader confidence="0.930302">
2.1 Social Graph Definition
</subsectionHeader>
<bodyText confidence="0.999487678571429">
Lets define an attributed, undirected graph G =
(V, E), where V is a set of vertices and E is a set
of edges. Each vertex vi represents someone in
a communication graph i.e., communicant: here
a Twitter user. Each vertex is attributed with a
feature vector f(vi) which encodes communica-
tions e.g., tweets available for a given user. Each
vertex is associated with a latent attribute a(vi),
in our case it is binary a(vi) E {D, R}, where
D stands for Democratic and R for Republican
users. Each edge eij E E represents a connec-
tion between vi and vj, eij = (vi, vj) and defines
different social circles between Twitter users e.g.,
follower (f), friend (b), user mention (m), hash-
tag (h), reply (y) and retweet (w). Thus, E E
V (2) x{f, b, h, m, w, y}. We denote a set of edges
of a given type as 0,.(E) for r E {f, b, h, m, w, y}.
We denote a set of vertices adjacent to vi by so-
cial circle type r as N,.(vi) which is equivalent to
{vj I eij E 0,.(E)}. Following Filippova (2012)
we refer to N,.(vi) as vi’s social circle, otherwise
known as a neighborhood. In most cases, we only
work with a sample of a social circle, denoted by
Nr(vi) where IN,(vi)I = k is its size for vi.
Figure 1 presents an example of a social graph
derived from Twitter. Notably, users from differ-
ent social circles can be shared across the users of
the same or different classes e.g., a user vj can be
</bodyText>
<footnote confidence="0.99927425">
3The code and detailed explanation on how we col-
lected all six types of user neighbors and their com-
munications using Twitter API can be found here:
http://www.cs.jhu.edu/ svitlana/
</footnote>
<figureCaption confidence="0.991765333333333">
Figure 1: An example of a social graph with follower, friend,
@mention, reply, retweet and hashtag social circles for each
user of interest e.g., blue: Democratic, red: Republican.
</figureCaption>
<bodyText confidence="0.81564">
in both follower circle vj E Nf(vi), vi E D and
retweet circle vj E Nw(vk), vk E R.
</bodyText>
<subsectionHeader confidence="0.983581">
2.2 Candidate-Centric Graph
</subsectionHeader>
<bodyText confidence="0.999877764705883">
We construct candidate-centric graph Gcand by
looking into following relationships between the
users and Democratic or Republican candidates
during the 2012 US Presidential election. In the
Fall of 2012, leading up to the elections, we ran-
domly sampled n = 516 Democratic and m =
515 Republican users. We labeled users as Demo-
cratic if they exclusively follow both Democratic
candidates4 – BarackObama and JoeBiden but
do not follow both Republican candidates – Mit-
tRomney and RepPaulRyan and vice versa. We
collectively refer to D and R as our “users of in-
terest” for which we aim to predict political prefer-
ence. For each such user we collect recent tweets
and randomly sample their immediate k = 10
neighbors from follower, friend, user mention, re-
ply, retweet and hashtag social circles.
</bodyText>
<subsectionHeader confidence="0.987473">
2.3 Geo-Centric Graph
</subsectionHeader>
<bodyText confidence="0.9999849">
We construct a geo-centric graph Ggeo by col-
lecting n = 135 Democratic and m = 135 Re-
publican users from the Maryland, Virginia and
Delaware region of the US with self-reported po-
litical preference in their biographies. Similar to
the candidate-centric graph, for each user we col-
lect recent tweets and randomly sample user social
circles in the Fall of 2012. We collect this data to
get a sample of politically less active users com-
pared to the users from candidate-centric graph.
</bodyText>
<subsectionHeader confidence="0.996849">
2.4 ZLR Graph
</subsectionHeader>
<bodyText confidence="0.9931925">
We also consider a GZLR graph constructed from
a dataset previously used for political affiliation
</bodyText>
<footnote confidence="0.883318">
4As of Oct 12, 2012, the number of followers for Obama,
Biden, Romney and Ryan were 2m, 168k, 1.3m and 267k.
</footnote>
<page confidence="0.997683">
187
</page>
<bodyText confidence="0.999895909090909">
classification (Zamal et al., 2012). This dataset
consists of 200 Republican and 200 Democratic
users associated with 925 tweets on average per
user.5 Each user has on average 6155 friends with
642 tweets per friend. Sharing restrictions and rate
limits on Twitter data collection only allowed us to
recreate a semblance of ZLR data6 – 193 Demo-
cratic and 178 Republican users with 1K tweets
per user, and 20 neighbors of four types including
follower, friends, user mention and retweet with
200 tweets per neighbor for each user of interest.
</bodyText>
<sectionHeader confidence="0.981979" genericHeader="method">
3 Batch Models
</sectionHeader>
<bodyText confidence="0.9993338">
Baseline User Model As input we are given a set
of vertices representing users of interest vi E V
along with feature vectors f(vi) derived from con-
tent authored by the user of interest. Each user
is associated with a non-zero number of publicly
posted tweets. Our goal is assign to a category
each user of interest vi based on f(vi). Here we
focus on a binary assignment into the categories
Democratic D or Republican R. The log-linear
model7 for such binary classification is:
</bodyText>
<equation confidence="0.815289666666667">
__ f D (1 + exp[−�θ · f (vi)])−1 &gt; 0.5,
Φvi l R otherwise.
(1)
</equation>
<bodyText confidence="0.9988905">
where features are normalized word ngram counts
extracted from vi’s tweets ft(vi) : D xt(vi) —* R.
The proposed baseline model follows the same
trends as the existing state-of-the-art approaches
for user attribute classification in social media as
described in Section 8. Next we propose to ex-
tend the baseline model by taking advantage of
language in user social circles as describe below.
</bodyText>
<listItem confidence="0.7462105">
Neighbor Model As input we are given user-local
neighborhood Nr(vi), where r is a neighborhood
type. Besides the neighborhood’s type r, each is
characterized by:
• the number of communications per neighbor
ft(Nr), t = 15,10,15, 25, 50,100, 2001;
</listItem>
<footnote confidence="0.987850461538461">
5The original dataset was collected in 2012 and has
been recently released at http://icwsm.cs.mcgill.ca/. Politi-
cal labels are extracted from http://www.wefollow.com as de-
scribed by Pennacchiotti and Popescu (2011b).
6This inability to perfectly replicate prior work based on
Twitter is a recognized problem throughout the community of
computational social science, arising from the data policies of
Twitter itself, it is not specific to this work.
7We use log-linear models over reasonable alternatives
such as perceptron or SVM, following the practice of a wide
range of previous work in related areas (Smith, 2004; Liu et
al., 2005; Poon et al., 2009) including text classification in so-
cial media (Van Durme, 2012b; Yang and Eisenstein, 2013).
</footnote>
<listItem confidence="0.941034333333333">
• the order of the social circle – the num-
ber of neighbors per user of interest |Nr |=
deg(vi), n = 11, 2,5, 101.
</listItem>
<bodyText confidence="0.778062">
Our goal is to classify users of interest using
evidence (e.g., communications) from their local
</bodyText>
<equation confidence="0.8030305">
neighborhood P �ft[Nr(vi)] � f(Nr) as Demo-
n
cratic or Republican. The corresponding log-
linear model is defined as:
_ f D (1 + exp[
−B · f (Nr)])−1 &gt; 0.5,
ΦNr l R otherwise.
(2)
</equation>
<bodyText confidence="0.999734363636364">
To check whether our static models are cog-
nizant of low-resource prediction settings we com-
pare the performance of the user model from Eq.1
and the neighborhood model from Eq.2. Follow-
ing the streaming nature of social media, we see
the scarce available resource as the number of re-
quests allowed per day to the Twitter API. Here
we abstract this to a model assumption where we
receive one tweet tk at a time and aim to maximize
classification performance with as few tweets per
user as possible:8
</bodyText>
<listItem confidence="0.987421">
• for the baseline user model:
</listItem>
<equation confidence="0.7795735">
minimize X tk(vi), (3)
k k
</equation>
<listItem confidence="0.955899">
• for the neighborhood model:
</listItem>
<bodyText confidence="0.699558">
minimize XX tk[Nr(vi)]. (4)
k n k
</bodyText>
<sectionHeader confidence="0.986991" genericHeader="method">
4 Streaming Models
</sectionHeader>
<bodyText confidence="0.9999715">
We rely on straightforward Bayesian rule update
to our batch models in order to simulate a real-
time streaming prediction scenario as a first step
beyond the existing models as shown in Figure 2.
The model makes predictions of a latent user at-
tribute e.g., Republican under a model assumption
of sequentially arriving, independent and identi-
cally distributed observations T = (ti, ... , tk)9.
The model dynamically updates posterior proba-
bility estimates p(a(vi) = R|tk) for a given user
</bodyText>
<footnote confidence="0.999623555555556">
8The separate issue is that many authors simply don’t
tweet very often. For instance, 85.3% of all Twitter
users post less than one update per day as reported at
http://www.sysomos.com/insidetwitter/. Thus, their commu-
nications are scare even if we could get all of them without
rate limiting from Twitter API.
9Given the dynamic character of online discourse it will
clearly be of interest in the future to consider models that go
beyond the iid assumption.
</footnote>
<page confidence="0.991336">
188
</page>
<bodyText confidence="0.4436005">
p(R|t1) p(R|t1, t2) p(R|t1, ... tk) 5 Experimental Setup
0.6 0.7 0.9 We design a set of experiments to analyze static
and dynamic models for political affiliation classi-
fication defined in Sections 3 and 4.
</bodyText>
<equation confidence="0.3801115">
Nr vi Nr vi Nr vi
τ1 τ2 τk Time, τ 5.1 Batch Classification Experiments
</equation>
<figureCaption confidence="0.8152">
Figure 2: Stream-based classification of an attribute a(vi) ∈
{R, D} given a stream of communications t1, t2, ... , tk au-
thored by a user vi or user immediate neighbors from Nr
social circles at time 71, 72, ... , 7k.
</figureCaption>
<bodyText confidence="0.989197">
vi as an additional evidence tk is acquired, as de-
fined in a general form below for any latent at-
tribute a(vi) ∈ A given the tweets T of user vi:
</bodyText>
<equation confidence="0.9986585">
p(a(vi)=x ∈ A  |T) =
p(T  |a(vi) = x) · p(a(vi) = x)
P
y∈A p(T  |a(vi) = y) · p(a(vi) = y)
Q
k p(tk  |a(vi) = x) · p(a(vi) = x)
Hk l
Ey∈A k p(tk  |a(vi) = y) · p(a(vi) = y),
</equation>
<bodyText confidence="0.969236857142857">
(5)
where y is the number of all possible attribute val-
ues, and k is the number of tweets per user.
For example, to predict user political prefer-
ence, we start with a prior P(R) = 0.5, and se-
quentially update the posterior p(R  |T) by accu-
mulating evidence from the likelihood p(tk|R):
</bodyText>
<equation confidence="0.998632333333333">
p(R  |T) =
Q k p(tk|R) · p(R) (6)
Qk P(tk|R) · p(R) + Qk P(tk|D) · p(D).
</equation>
<bodyText confidence="0.999773">
Our goal is to maximize posterior probability
estimates given a stream of communications for
each user in the data over (a) time τ and (b) the
number of tweets T. For that, for each user we
take tweets that arrive continuously over time and
apply two different streaming models:
</bodyText>
<listItem confidence="0.95768">
• User Model with Dynamic Updates: re-
lies exclusively on user tweets t(vi)
</listItem>
<equation confidence="0.9915925">
1 , . . . , t(vi)
k
</equation>
<bodyText confidence="0.999972666666667">
following the order they arrive over time τ,
where for each user vi we dynamically up-
date the posterior p(R  |t(vi)
</bodyText>
<equation confidence="0.9634555">
1 , . . . , t(vi)
k ).
</equation>
<listItem confidence="0.985283">
• User-Neighbor Model with Dynamic Up-
dates: relies on both neighbor Nr commu-
nications including friend, follower, retweet,
user mention and user tweets t(vi)
</listItem>
<equation confidence="0.6835205">
1 , ... , t(Nr)
k
</equation>
<bodyText confidence="0.896255333333333">
following the order they arrive over time τ;
here we dynamically update the posterior
probability p(R  |t(vi)
</bodyText>
<equation confidence="0.8697125">
1 , ... , t(Nr)
k ).
</equation>
<bodyText confidence="0.999883791666667">
We first answer whether communications from
user-local neighborhoods can help predict politi-
cal preference for the user. To explore the con-
tribution of different neighborhood types we learn
static user and neighbor models on Gcand, Ggeo
and GZLR graphs. We also examine the ability of
our static models to predict user political prefer-
ences in low-resource setting e.g., 5 tweets.
The existing models follow a standard setup
when either user or neighbor tweets are available
during train and test. For a static neighbor model
we go beyond that, and train our the model on all
data available per user, but only apply part of the
data at the test time, pushing the boundaries of
how little is truly required for classification. For
example, we only use follower tweets for Gtest,
but we use tweets from all types of neighbors for
Gtrain. Such setup will simulate different real-
world prediction scenarios which have not been
previously explored, to our knowledge e.g., when
a user has a private profile or has not tweeted yet,
and only user neighbor tweets are available.
We experiment with our static neighbor model
defined in Eq.2 with the aim to:
</bodyText>
<listItem confidence="0.996072">
1. evaluate neighborhood size influence, we
change the number of neighbors and try n =
[1, 2, 5,10] neighbor(s) per user;
2. estimate neighbor content influence, we alter-
</listItem>
<bodyText confidence="0.969875166666667">
nate the amount of content per neighbor and
try t = [5,10,15, 25, 50,100, 200] tweets.
We perform 10-fold cross validation10 and run
100 random restarts for every n and t parame-
ter combination. We compare our static neigh-
bor and user models using the cost functions
from Eq.3 and Eq.4. For all experiments we use
LibLinear (Fan et al., 2008), integrated in the
Jerboa toolkit (Van Durme, 2012a). Both mod-
els defined in Eq.1 and Eq.2 are learned using
normalized count-based word ngram features ex-
tracted from either user or neighbor tweets.11
</bodyText>
<footnote confidence="0.9993005">
10For each fold we split the data into 3 parts: 70% train,
10% development and 20% test.
11For brevity we omit reporting results for bigram and tri-
gram features, since unigrams showed superior performance.
</footnote>
<page confidence="0.995848">
189
</page>
<subsectionHeader confidence="0.99832">
5.2 Streaming Classification Experiments
</subsectionHeader>
<bodyText confidence="0.999995666666667">
We evaluate our models with dynamic Bayesian
updates on a continuous stream of communica-
tions over time as shown in Figure 2. Unlike static
model experiments, we are not modeling the in-
fluence of the number of neighbors or the amount
of content per neighbor. Here, we order user and
neighbor communication streams by real world
time of posting and measure changes in posterior
probabilities over time. The main purpose of these
experiments is to quantitatively evaluate (1) the
number of tweets and (2) the amount of real world
time it takes to observe enough evidence on Twit-
ter to make reliable predictions.
We experiment with log-linear models defined
in Eq. 1 and 2 and continuously estimate the poste-
rior probabilities P(R I T) as defined in Eq.6. We
average the posterior probability results over the
users in Gcand, Ggeo and GZLR graphs. We train
streaming models on an attribute balanced subset
of tweets for each user vi excluding vi’s tweets (or
vi’s neighbor tweets for a joint model). This setup
is similar to leave-one-out classification. The clas-
sifier is learned using binary word ngram features
extracted from user or user-neighbor communi-
cations. We prefer binary to normalized count-
based features to overcome sparsity issues caused
by making predictions on each tweet individually.
</bodyText>
<sectionHeader confidence="0.998331" genericHeader="method">
6 Static Classification Results
</sectionHeader>
<subsectionHeader confidence="0.999953">
6.1 Modeling User Content Influence
</subsectionHeader>
<bodyText confidence="0.999757">
We investigate classification decision probabilities
for our static user model Φvi by making predic-
tions on a random set of 5 vs. 100 tweets per user.
To our knowledge only limited work on personal
</bodyText>
<figureCaption confidence="0.9867615">
Figure 3: Classification probabilities for Φv4 estimated over
100 users in G.and tested on 5 (blue) vs. 100 (green) tweets
per user where Republican = 1, Democratic = 0, filled mark-
ers = correctly classified, not filled = misclassified users.
</figureCaption>
<figure confidence="0.969888285714286">
● ● ● ● ● ●
● ● ● ● ● ● ●
● ●● ● ● ● ● ●
● ● ●
5 10 20 50 100 200
log(Tweets Per Neighbor)
(c) G.and: 2 neighbors (d) G.and: 10 neighbors
</figure>
<figureCaption confidence="0.997069">
Figure 4: Modeling the influence of the number of tweets per
neighbor t=[5, .., 200] for G.and and G9eo graphs.
</figureCaption>
<bodyText confidence="0.999952333333333">
analytics (Burger et al., 2011; Van Durme, 2012b)
have performed this straight-forward comparison.
For that purpose, we take a random partition con-
taining 100 users of Gcand graph and perform four
independent classification experiments – two runs
using 5 and two runs using 100 tweets per user.
Figure 3 demonstrates that more tweets during
prediction time lead to higher accuracy by show-
ing that more users with 100 tweets are correctly
classified e.g., filled green markers in the right up-
per quadrant are true Republicans and in the left
lower quadrant are true Democrats. Moreover, a
lot of users with 100 tweets are close to 0.5 deci-
sion probability which suggests that the classifier
is just uncertain rather then being completely off,
e.g., misclassified Republican users with 5 tweets
(not filled blue markers in the right lower quad-
rant) are close to 0. These results follow natu-
rally from the underlying feature representation:
having more tweets per user leads to a lower vari-
ance estimate of a target multinomial distribution.
The more robustly this distribution is estimated
(based on having more tweets) the more confident
we should be in the classifier output.
</bodyText>
<subsectionHeader confidence="0.999667">
6.2 Modeling Neighbor Content Influence
</subsectionHeader>
<bodyText confidence="0.99990675">
Here we discuss the results for our static neighbor-
hood model. We study the influence of the neigh-
borhood type r and size in terms of the number of
neighbors n and tweets t per neighbor.
</bodyText>
<figure confidence="0.999941918181819">
0 20 40 60 80 100
User
Classification decision (probability)
0.0 0.2 0.4 0.6 0.8 1.0
misclassified
correct misclassified
correct
10 50 100 200 400 50 100 250 500 1000 2000
5 10 20 50 100 200
log(Tweets Per Neighbor)
(a) G9eo: 2 neighbors
10 50 100 200 400
5 10 20 50 100 200
log(Tweets Per Neighbor)
(b) G9eo: 10 neighbors
50 100 250 500 1000 2000
Accuracy
0.50 0.55 0.60 0.65 0.70
● ●
●
●
●
● ● ● ●
●
● ●
● ● ● ●
●
●
●
●
●
●
●
●
●
●
●
Friend
Follower
Hashtag
Usermention
Retweet
Reply
User
Accuracy
0.50 0.55 0.60 0.65 0.70
●
●
● ● ● ●
● ● ●
●
●
●
● ●
● ●
● ●
●
●
●
●
●
●
●
●
Friend
Follower
Hashtag
Usermention
●
●
●
Retweet
Reply
User
Friend
Follower
Hashtag
Usermention
Retweet
Reply
User
0.50 0.55 0.60 0.65 0.70 0.75
Accuracy
Accuracy
0.50 0.55 0.60 0.65 0.70 0.75
●
●
●
●
●
●
● ●
●
●
●
Friend
Follower
Hashtag
Usermention
●
●
●
●
●
●
Retweet
Reply
User
5 10 20 50 100 200
log(Tweets Per Neighbor)
</figure>
<page confidence="0.550197">
190
</page>
<figureCaption confidence="0.962675">
Figure 5: Modeling the influence of the number of neighbors
per user n=[1, .., 10] for Gcand and G9eo graphs.
</figureCaption>
<bodyText confidence="0.999983833333333">
In Figure 4 we present accuracy results for
Gcand and Ggeo graphs. Following Eq.3 and 4, we
spent an equal amount of resources to obtain 100
user tweets and 10 tweets from 10 neighbors. We
annotate these ‘points of equal number of commu-
nications’ with a line on top marked with a corre-
sponding number of user tweets.
We show that three of six social circles – friend,
retweet and user-mention yield better accuracy
compared to the user model for all graphs when
t ≥ 250. Thus, for effectively classifying a given
user vi it is better to take 200 tweets each from 10
neighbors rather than 2,000 tweets from the user.
The best accuracy for Gcand is 0.75 for friend,
follower, retweet and user-mention neighborhoods
which is 0.03 higher than the user baseline; for
Ggeo is 0.67 for user-mention and 0.64 for retweet
circles compared to 0.57 for the user model; for
GZLR is 0.863 for retweet and 0.849 for friend
circles which is 0.11 higher that the user baseline.
Finally, similarly to the results for the user model
given in Figure 3, increasing the number of tweets
per neighbor from 5 to 200 leads to a significant
gain in performance for all neighborhood types.
</bodyText>
<subsectionHeader confidence="0.999613">
6.3 Modeling Neighborhood Size
</subsectionHeader>
<bodyText confidence="0.999978642857143">
In Figure 5 we present accuracy results to show
neighborhood size influence on classification per-
formance for Ggeo and Gcand graphs. Our re-
sults demonstrate that even small changes to the
neighborhood size n lead to better performance
which does not support the claims by Zamal et al.
(2012). We demonstrate that increasing the size
of the neighborhood leads to better performance
across six neighborhood types. Friend, user men-
tion and retweet neighborhoods yield the highest
accuracy for all graphs. We observe that when the
number of neighbors is n = 1, the difference in
accuracy across all neighborhood types is less sig-
nificant but for n ≥ 2 it becomes more significant.
</bodyText>
<sectionHeader confidence="0.996923" genericHeader="method">
7 Streaming Classification Results
</sectionHeader>
<subsectionHeader confidence="0.998844">
7.1 Modeling Dynamic Posterior Updates
from a User Stream
</subsectionHeader>
<bodyText confidence="0.999706">
Figures 6a and 6b demonstrate dynamic user
model prediction results averaged over users from
Gcand and GZLR graphs. Each figure outlines
changes in sequential average probability esti-
mates pµ(R  |T) for each individual self-authored
tweet tk as defined in Eq. 6. The average proba-
</bodyText>
<equation confidence="0.848975333333333">
bility estimates pµ(R  |T) are reported for every 5
�
nP(R|tk)
</equation>
<bodyText confidence="0.992714333333333">
tweets in a stream T = (t1, ... tk) as n ,
where n is the total number of users with the same
attribute R or D. We represent pµ(R  |T) as a
box and whisker plot with the median, lower and
upper quantiles to show the variance; the length of
whiskers indicate lower and upper extreme values.
We find similar behavior across all three graphs.
In particular, the posterior estimates converge
faster when predicting Democratic than Republi-
can users but it has been trained on an equal num-
ber of tweets per class. We observe that average
posterior estimates Pµ(R  |T) converge faster to 0
</bodyText>
<figure confidence="0.99992227480916">
5 10
25 50
200 400 1000 2000
1 2 5 10
log(Number of Neighbors)
(a) Gcand: 5 tweets
1 2 5 10
log(Number of Neighbors)
(b) Gcand: 200 tweets
5 10
25 50
200 400 1000 2000
1 2 5 10
log(Number of Neighbors)
(c) G9eo: 5 tweets
1 2 5 10
log(Number of Neighbors)
(d) G9eo: 200 tweets
0.50 0.55 0.60 0.65 0.70 0.75
Accuracy
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Accuracy
●
●
●
●
●
●
Friend
Follower
Hashtag
Friend
Follower
Hashtag
Usermention
Retweet
Reply
Usermention
Retweet
Reply
0.50 0.55 0.60 0.65 0.70 0.75
0.50 0.55 0.60 0.65 0.70
0.50 0.55 0.60 0.65 0.70
●
●
●
●
●
●
●
Accuracy
Accuracy
●
●
●
●
●
●
●
●
● ●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Friend
Follower
Hashtag
Usermention
Retweet
Reply
Friend
Follower
Hashtag
Usermention
Retweet
Reply
0 20 40 60 0 50 100 150
0 20 40 60 0 50 100 150
Tweet Stream (T) Tweet Stream (T)
(a) User Gcand (b) User GZLR
0.5
0.5
0.4
0.4
p(RepublicandT)
p(RepublicandT)
0.3
0.3
0.2
0.2
0.1
0.1
</figure>
<figureCaption confidence="0.992484333333333">
Figure 6: Streaming classification results from user commu-
nications for Gcand and GZLR graphs averaged over every 5
tweets (red - Republican, blue - Democratic).
</figureCaption>
<figure confidence="0.997469416666667">
p(RepublicandT)
0.9
0.8
0.7
0.6
0.5
p(RepublicandT)
0.9
0.8
0.7
0.6
0.5
</figure>
<page confidence="0.786138">
191
</page>
<figureCaption confidence="0.99836425">
Figure 7: Time needed for (a) - (b) dynamic user model and
(c) - (d) joint user-neighbor model to infer political prefer-
ences of Democratic (blue) and Republican (red) users at
75% (dotted line) and 95% (solid line) accuracy levels.
</figureCaption>
<bodyText confidence="0.999274064516129">
(Democratic) than to 1 (Republican) in Figures 6a
and 6b. It suggests that language of Democrats is
more expressive of their political preference than
language of Republicans. For example, frequent
politically influenced terms used widely by Demo-
cratic users include faith4liberty, constitutionally,
pass, vote2012, terroristic.
The variance for average posterior estimates
decreases when the number of tweets increases
for all three datasets. Moreover, we detect that
Pµ(R|T) estimates for users in Gcand converge 2-
3 times faster in terms of number of tweets than
for users in GZLR. The lowest convergence is de-
tected for Ggeo where after tk = 250 tweets the
average posterior estimate Pµ(R  |tk) = 0.904 f
0.044 and Pµ(D  |tk) = 0.861 f 0.008. It means
that users in Gcand are more politically vocal com-
pared to users in GZLR and Ggeo. As a result,
less active users in Ggeo just need more than 250
tweets to converge to a true 0 or 1 class. These re-
sults are coherent with the outcomes for our static
models shown in Figures 4 and 5. These findings
further confirm that differences in performance are
caused by various biases present in the data due to
distinct sampling and annotation approaches.
Figure 7a and 7b illustrate the amount of time
required for the user model to infer political pref-
erences estimated for 1,031 users in Gcand and 371
users in GZLR. The amount of time needed can be
evaluated for different accuracy levels e.g., 0.75
and 0.95. Thus, with 75% accuracy we classify:
</bodyText>
<listItem confidence="0.921654066666667">
• 100 (-20%) Republican users in 3.6 hours
and Democratic users in 2.2 hours for Gcand;
• 100 (-56%) R users in 20 weeks and 100
(-52%) D users in 8.9 weeks for GZLR
which is 800 times longer that for Gcand;
• 100 (-75%) R users in 12 weeks and 80
(-60%) D users in 19 weeks for Ggeo.
Such extreme divergences in the amount of time
required for classification across all graphs should
be of strong interest to researchers concerned with
latent attribute prediction tasks because Twitter
users produce messages with extremely different
frequencies. In our case, users in GZLR tweet ap-
proximately 800 times less frequently than users
in Gcand.
</listItem>
<subsectionHeader confidence="0.9901765">
7.2 Modeling Dynamic Posterior Updates
from a Joint User-Neighbor Stream
</subsectionHeader>
<bodyText confidence="0.9999726">
We estimate dynamic posterior updates from a
joint stream of user and neighbor communications
in Ggeo, Gcand and GZLR graphs. To make a fair
comparison with a streaming user model, we start
with the same user tweet t0(vi). Then instead of
waiting for the next user tweet we rely on any
neighbor tweets that appear until the user produces
the next tweet t1(vi). We rely on communications
from four types of neighbors such as friends, fol-
lowers, retweets and user mentions.
The convergence rate for the average posterior
probability estimates Pµ(R|T) depending on the
number of tweets is similar to the user model re-
sults presented in Figure 6. However, for Ggeo
the variance for Pµ(R|T) is higher for Democratic
users; for GZLR Pµ(R|T) → 1 for Republicans
in less than 110 tweets which is At = 40 tweets
faster than the user model; for Gcand the conver-
gence for both Pµ(R|T) → 1 and Pµ(D|T) → 0
is not significantly different than the user model.
Figures 7c and 7d show the amount of time re-
quired for a joint user-neighbor model to infer po-
litical preferences estimated for users in Gcand and
GZLR. We find that with 75% accuracy we can
classify 100 users for:
</bodyText>
<listItem confidence="0.993159857142857">
• Gcand: Republican users in 23 minutes and
Democratic users in 10 minutes;
• GZLR: R users in 3.2 weeks and D users in
1.1 weeks which is 7 times faster on average
across attributes than for the user model;
• Ggeo: R users in 1.2 weeks and D users in
3.5 weeks which is on average 6 times faster
</listItem>
<bodyText confidence="0.78920925">
across attributes than for the user model.
Similar or better Pµ(R|T) convergence in terms
of the number of tweets and, especially, in the
amount of time needed for user and user-neighbor
</bodyText>
<figure confidence="0.999538454545455">
0 5 10 15 0 20 40 60 80
Time in Weeks Time in Weeks
(a) User Gcand (b) User GZLR
0 1 2 3 4 5 0 10 20 30 40
Time in Weeks Time in Weeks
(c) User-Neigh Gcand (d) User-Neigh GZLR
Users
400
500
300
Users
200
100
0
Users
400
500
300
Users
200
100
0
</figure>
<page confidence="0.993031">
192
</page>
<bodyText confidence="0.99971475">
models further confirms that neighborhood con-
tent is useful for political preference prediction.
Moreover, communications from a joint stream al-
low to make an inference up to 7 times faster.
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="method">
8 Related Work
</sectionHeader>
<bodyText confidence="0.999576714285714">
Supervised Batch Approaches The vast major-
ity of work on predicting latent user attributes in
social media apply supervised static SVM mod-
els for discrete categorical e.g., gender and re-
gression models for continuous attributes e.g., age
with lexical bag-of-word features for classifying
user gender (Garera and Yarowsky, 2009; Rao et
al., 2010; Burger et al., 2011; Van Durme, 2012b),
age (Rao et al., 2010; Nguyen et al., 2011; Nguyen
et al., 2013) or political orientation. We present an
overview of the existing models for political pref-
erence prediction in Table 1.
Bergsma et al. (2012) following up on Rao’s
work (2010) on adding socio-linguistic features
to improve gender, ethnicity and political prefer-
ence prediction show that incorporating stylistic
and syntactic information to the bag-of-word fea-
tures improves gender classification.
Other methods characterize Twitter users by ap-
plying limited amounts of network structure in-
formation in addition to lexical features. Con-
</bodyText>
<table confidence="0.999828620689655">
Approach Users Tweets Features Accur.
Rao et al. 1K 2M ngrams 0.824
(2010) socio-ling 0.634
stacked 0.809
Pennacchiotti 10.3K – ling-all 0.770
and Popescu soc-all 0.863
(2011a) full 0.889
Conover et 1,000 1M full-text 0.792
al. (2011) hashtags 0.908
clusters 0.949
Zamal et al. 400 400K UserOnly 0.890
(2012) 3.85M Nbr 0.920
4.25M User-Nbr11 0.932
Cohen and 397 397K features 0.910
Ruths 1.8K 1.8M from (Za- 0.840
(2013) 262 262K mal et al., 0.680
196 196K 2012) 0.870
This paper Gcand 206K user ngrams 0.720
(batch clas- 1,031 2M neighbor 0.750
sification) G9eo 54K user ngrams 0.570
270 540K neighbor 0.670
GZLR 371K user ngrams 0.886
371 1.5M neighbor 0.920
This paper Gcand 103K user stream 0.995
(dynamic 1,031 130K user-neigh. 0.999
Bayesian G9eo 54K user stream 0.843
update clas- 270 67K user-neigh. 0.882
sification) GZLR 74K user stream 0.892
371 185K user-neigh. 0.999
</table>
<tableCaption confidence="0.99167">
Table 1: Overview of the existing approaches for political
preference classification in Twitter.
</tableCaption>
<bodyText confidence="0.99964334">
nover et al. (2011) rely on identifying strong parti-
san clusters of Democratic and Republican users
in a Twitter network based on retweet and user
mention degree of connectivity, and then combine
this clustering information with the follower and
friend neighborhood size features. Pennacchiotti
et al. (2011a; 2011b) focus on user behavior, net-
work structure and linguistic features. Similar to
our work, they assume that users from a partic-
ular class tend to reply and retweet messages of
the users from the same class. We extend this as-
sumption and study other relationship types e.g.,
friends, user mentions etc. Recent work by Wong
et al. (2013) investigates tweeting and retweet-
ing behavior for political learning during 2012 US
Presidential election. The most similar work to
ours is by Zamal et al. (2012), where the authors
apply features from the tweets authored by a user’s
friend to infer attributes of that user. In this paper,
we study different types of user social circles in
addition to a friend network.
Additionally, using social media for mining po-
litical opinions (O’Connor et al., 2010a; May-
nard and Funk, 2012) or understanding socio-
political trends and voting outcomes (Tumasjan
et al., 2010; Gayo-Avello, 2012; Lampos et al.,
2013) is becoming a common practice. For in-
stance, Lampos et al. (2013) propose a bilinear
user-centric model for predicting voting intentions
in the UK and Australia from social media data.
Other works explore political blogs to predict what
content will get the most comments (Yano et al.,
2013) or analyze communications from Capitol
Hill12 to predict campaign contributors based on
this content (Yano and Smith, 2013).
Unsupervised Batch Approaches Bergsma et
al. (2013) show that large-scale clustering of user
names improves gender, ethnicity and location
classification on Twitter. O’Connor et al. (2010b)
following the work by Eisenstein (2010) propose
a Bayesian generative model to discover demo-
graphic language variations in Twitter. Rao et
al. (2011) suggest a hierarchical Bayesian model
which takes advantage of user name morphology
for predicting user gender and ethnicity. Golbeck
et al. (2010) incorporate Twitter data in a spatial
model of political ideology.
Streaming Approaches Van Durme (2012b)
proposed streaming models to predict user gen-
der in Twitter. Other works suggested to process
</bodyText>
<footnote confidence="0.958838">
12http://www.tweetcongress.org
</footnote>
<page confidence="0.998593">
193
</page>
<bodyText confidence="0.999910153846154">
text streams for a variety of NLP tasks e.g., real-
time opinion mining and sentiment analysis in so-
cial media (Pang and Lee, 2008), named entity
disambiguation (Sarmento et al., 2009), statistical
machine translation (Levenberg et al., 2011), first
story detection (Petrovi´c et al., 2010), and unsu-
pervised dependency parsing (Goyal and Daum´e,
2011). Massive Online Analysis (MOA) toolkit
developed by Bifet et al. (2010) is an alternative to
the Jerboa package used in this work developed
by Van Durme (2012a). MOA has been effec-
tively used to detect sentiment changes in Twitter
streams (Bifet et al., 2011).
</bodyText>
<sectionHeader confidence="0.994561" genericHeader="conclusions">
9 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999642075949367">
In this paper, we extensively examined state-of-
the-art static approaches and proposed novel mod-
els with dynamic Bayesian updates for streaming
personal analytics on Twitter. Because our stream-
ing models rely on communications from Twitter
users and content from various notions of user-
local neighborhood they can be effectively applied
to real-time dynamic data streams. Our results
support several key findings listed below.
Neighborhood content is useful for personal
analytics. Content extracted from various notions
of a user-local neighborhood can be as effective
or more effective for political preference classifi-
cation than user self-authored content. This may
be an effect of ‘sparseness’ of relevant user data,
in that users talk about politics very sporadically
compared to a random sample of their neighbors.
Substantial signal for political preference
prediction is distributed in the neighborhood.
Querying for more neighbors per user is more ben-
eficial than querying for extra content from the
existing neighbors e.g., 5 tweets from 10 neigh-
bors leads to higher accuracy than 25 tweets from
2 neighbors or 50 tweets from 1 neighbor. This
may be also the effect of data heterogeneity in
social media compared to e.g., political debate
text (Thomas et al., 2006). These findings demon-
strate that a substantial signal is distributed over
the neighborhood content.
Neighborhoods constructed from friend,
user mention and retweet relationships are
most effective. Friend, user mention and retweet
neighborhoods show the best accuracy for predict-
ing political preferences of Twitter users. We think
that friend relationships are more effective than
e.g., follower relationships because it is very likely
that users share common interests and preferences
with their friends, e.g. Facebook friends can even
be used to predict a user’s credit score.13 User
mentions and retweets are two primary ways of in-
teraction on Twitter. They both allow to share in-
formation e.g., political news, events with others
and to be involved in direct communication e.g.,
live political discussions, political groups.
Streaming models are more effective than
batch models for personal analytics. The predic-
tions made using dynamic models with Bayesian
updates over user and joint user-neighbor commu-
nication streams demonstrate higher performance
with lower resources spent compared to the batch
models. Depending on user political involvement,
expressiveness and activeness, the perfect predic-
tion (approaching 100% accuracy) can be made
using only 100 - 500 tweets per user.
Generalization of the classifiers for political
preference prediction. This work raises a very
important but under-explored problem of the gen-
eralization of classifiers for personal analytics in
social media, also recently discussed by Cohen
and Ruth (2013). For instance, the existing models
developed for political preference prediction are
all trained on Twitter data but report significantly
different results even for the same baseline mod-
els trained using bag-of-word lexical features as
shown in Table 1. In this work we experiment with
three different datasets. Our results for both static
and dynamic models show that the accuracy in-
deed depends on the way the data was constructed.
Therefore, publicly available datasets need to be
released for a meaningful comparison of the ap-
proaches for personal analytics in social media.
In future work, we plan to incorporate itera-
tive model updates from newly classified com-
munications similar to online perceptron-style up-
dates. In addition, we aim to experiment with
neighborhood-specific classifiers applied towards
the tweets from neighborhood-specific streams
e.g., friend classifier used for friend tweets,
retweet classifier applied to retweet tweets etc.
</bodyText>
<sectionHeader confidence="0.9983" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9963855">
The authors would like to thank the anonymous
reviewers for their helpful comments.
</bodyText>
<footnote confidence="0.9348935">
13http://money.cnn.com/2013/08/26/technology/social/
facebook-credit-score/
</footnote>
<page confidence="0.997459">
194
</page>
<sectionHeader confidence="0.989141" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999835537735849">
Shane Bergsma, Matt Post, and David Yarowsky. 2012.
Stylometric analysis of scientific articles. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (NAACL-
HLT), pages 327–337.
Shane Bergsma, Mark Dredze, Benjamin Van Durme,
Theresa Wilson, and David Yarowsky. 2013.
Broadly improving user classification via
communication-based name and location clus-
tering on Twitter. In Proceedings of the Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT), pages 1010–1019.
Albert Bifet, Geoff Holmes, Bernhard Pfahringer,
Philipp Kranen, Hardy Kremer, Timm Jansen, and
Thomas Seidl. 2010. MOA: Massive online analy-
sis, a framework for stream classification and clus-
tering. Journal of Machine Learning Research,
11:44–50.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavald`a. 2011. Detecting sentiment
change in Twitter streaming data. Journal of Ma-
chine Learning Research, 17:5–11.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
Twitter. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1301–1309.
Raviv Cohen and Derek Ruths. 2013. Classifying Po-
litical Orientation on Twitter: It’s Not Easy! In Pro-
ceedings of the International AAAI Conference on
Weblogs and Social Media (ICWSM), pages 91–99.
Michael D. Conover, Bruno Gonc¸alves, Jacob
Ratkiewicz, Alessandro Flammini, and Filippo
Menczer. 2011. Predicting the political alignment
of Twitter users. In Proceedings of Social Comput-
ing, pages 192–199.
Jacob Eisenstein, Brendan O’Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1277–1287.
Rong En Fan, Kai Wei Chang, Cho Jui Hsieh, Xi-
ang Rui Wang, and Chih Jen Lin. 2008. LIBLIN-
EAR: A library for large linear classification. Jour-
nal of Machine Learning Research, 9:1871–1874.
Katja Filippova. 2012. User demographics and lan-
guage in an implicit social network. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1478–1488.
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 710–718.
Daniel Gayo-Avello. 2012. No, you cannot predict
elections with Twitter. Internet Computing, IEEE,
16(6):91–94.
Jennifer Golbeck, Justin M. Grimes, and Anthony
Rogers. 2010. Twitter use by the u.s. congress.
Journal of the American Society forInformation Sci-
ence and Technology, 61(8):1612–1621.
Amit Goyal and Hal Daum´e, III. 2011. Approxi-
mate scalable bounded space sketch for large data
NLP. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 250–261.
Vasileios Lampos, Daniel Preotiuc-Pietro, and Trevor
Cohn. 2013. A user-centric model of voting inten-
tion from social media. In Proceedings of the Asso-
ciation for Computational Linguistics (ACL), pages
993–1003.
Abby Levenberg, Miles Osborne, and David Matthews.
2011. Multiple-stream language models for statis-
tical machine translation. In Proceedings of the
Sixth Workshop on Statistical Machine Translation
(WMT), pages 177–186.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of the Annual Meeting on Association for Computa-
tional Linguistics (ACL), pages 459–466.
Ingrid Lunden. 2012. Analyst: Twitter
passed 500M users in june 2012, 140m of
them in US; Jakarta ‘biggest tweeting’ city.
http://techcrunch.com/2012/07/30/analyst-twitter-
passed-500m-users-in-june-2012-140m-of-them-in-
us-jakarta-biggest-tweeting-city/.
Diana Maynard and Adam Funk. 2012. Automatic de-
tection of political opinions in tweets. In Proceed-
ings of the 8th International Conference on The Se-
mantic Web (ESWC), pages 88–99.
Felix Ming Fai Wong, Chee Wei Tan, Soumya Sen, and
Mung Chiang. 2013. Quantifying political leaning
from tweets and retweets. In Proceedings of the In-
ternational AAAI Conference on Weblogs and Social
Media (ICWSM).
Dong Nguyen, Noah A. Smith, and Carolyn P. Ros´e.
2011. Author age prediction from text using lin-
ear regression. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities
(LaTeCH), pages 115–123.
</reference>
<page confidence="0.985905">
195
</page>
<reference confidence="0.99943695">
Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and
Theo Meder. 2013. ”How old do you think I am?”
A study of language and age in Twitter. In Proceed-
ings of the AAAI Conference on Weblogs and Social
Media (ICWSM), pages 439–448.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010a.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
International AAAI Conference on Weblogs and
Social Media (ICWSM), pages 122–129.
Brendan O’Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010b. A mixture model of de-
mographic lexical variation. In Proceedings of the
NIPS Workshop on Machine Learning and Social
Computing.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations of Trends in Infor-
mation Retrieval, 2(1-2):1–135, January.
Marco Pennacchiotti and Ana-Maria Popescu. 2011a.
Democrats, republicans and starbucks afficionados:
user classification in twitter. In Proceedings of the
17th ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD), pages 430–438.
Marco Pennacchiotti and Ana Maria Popescu. 2011b.
A machine learning approach to Twitter user clas-
sification. In Proceedings of the International AAAI
Conference on Weblogs and Social Media (ICWSM),
pages 281–288.
Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with appli-
cation to Twitter. In Proceedings of Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL).
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 209–217.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proceedings of the 2nd In-
ternational Workshop on Search and Mining User-
generated Contents (SMUC), pages 37–44.
Delip Rao, Michael Paul, Clay Fink, David Yarowsky,
Timothy Oates, and Glen Coppersmith. 2011. Hier-
archical Bayesian models for latent attribute detec-
tion in social media. In Proceedings of the Inter-
national AAAI Conference on Weblogs and Social
Media (ICWSM).
Lu´ıs Sarmento, Alexander Kehlenbeck, Eug´enio
Oliveira, and Lyle Ungar. 2009. An approach to
web-scale named-entity disambiguation. In Pro-
ceedings of the 6th International Conference on Ma-
chine Learning and Data Mining in Pattern Recog-
nition (MLDM), pages 689–703.
Noah A. Smith. 2004. Log-linear models.
Craig Smith. 2013. May 2013 by the
numbers: 16 amazing Twitter stats.
http://expandedramblings.com/index.php/march-
2013-by-the-numbers-a-few-amazing-twitter-stats/.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 327–335.
A. Tumasjan, T. O. Sprenger, P. G. Sandner, and I. M.
Welpe. 2010. Predicting elections with Twitter:
What 140 characters reveal about political senti-
ment. In Proceedings of the International AAAI
Conference on Weblogs and Social Media, pages
178–185.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical re-
port, Human Language Technology Center of Excel-
lence.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 48–58.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of Conference on Empirical Methods in
Natural Language Processing, pages 61–72.
Tao Yano and Noah A. Smith. 2013. What’s worthy
of comment? content and comment volume in po-
litical blogs. In International AAAI Conference on
Weblogs and Social Media (ICWSM).
Tao Yano, Dani Yogatama, and Noah A. Smith. 2013.
A penny for your tweets: Campaign contributions
and capitol hill microblogs. In Proceedings of the
International AAAI Conference on Weblogs and So-
cial Media (ICWSM).
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media, pages 387–390.
</reference>
<page confidence="0.998957">
196
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.291035">
<title confidence="0.999954">Inferring User Political Preferences from Streaming Communications</title>
<author confidence="0.999694">Benjamin Van</author>
<title confidence="0.3624945">for Language and Speech Language Technology Center of</title>
<author confidence="0.336591">Johns Hopkins University</author>
<author confidence="0.336591">Baltimore</author>
<email confidence="0.997641">svitlana@jhu.edu,coppersmith@jhu.edu,vandurme@cs.jhu.edu</email>
<abstract confidence="0.999815333333333">Existing models for social media personal analytics assume access to thousands of messages per user, even though most users author content only sporadically over time. Given this sparsity, we: (i) leverage content from the local neighborhood of a user; (ii) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods; and (iii) estimate the amount of time and tweets required for a dynamic model to predict user preferences. We show that even when limited or no selfauthored data is available, language from friend, retweet and user mention communications provide sufficient evidence for prediction. When updating models over time based on Twitter, we find that political preference can be often be predicted using roughly 100 tweets, depending on the context of user selection, where this could mean hours, or weeks, based on the author’s tweeting frequency.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Matt Post</author>
<author>David Yarowsky</author>
</authors>
<title>Stylometric analysis of scientific articles.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT),</booktitle>
<pages>327--337</pages>
<contexts>
<context position="30429" citStr="Bergsma et al. (2012)" startWordPosition="5294" endWordPosition="5297">times faster. 8 Related Work Supervised Batch Approaches The vast majority of work on predicting latent user attributes in social media apply supervised static SVM models for discrete categorical e.g., gender and regression models for continuous attributes e.g., age with lexical bag-of-word features for classifying user gender (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b), age (Rao et al., 2010; Nguyen et al., 2011; Nguyen et al., 2013) or political orientation. We present an overview of the existing models for political preference prediction in Table 1. Bergsma et al. (2012) following up on Rao’s work (2010) on adding socio-linguistic features to improve gender, ethnicity and political preference prediction show that incorporating stylistic and syntactic information to the bag-of-word features improves gender classification. Other methods characterize Twitter users by applying limited amounts of network structure information in addition to lexical features. ConApproach Users Tweets Features Accur. Rao et al. 1K 2M ngrams 0.824 (2010) socio-ling 0.634 stacked 0.809 Pennacchiotti 10.3K – ling-all 0.770 and Popescu soc-all 0.863 (2011a) full 0.889 Conover et 1,000 1</context>
</contexts>
<marker>Bergsma, Post, Yarowsky, 2012</marker>
<rawString>Shane Bergsma, Matt Post, and David Yarowsky. 2012. Stylometric analysis of scientific articles. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT), pages 327–337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Mark Dredze</author>
<author>Benjamin Van Durme</author>
<author>Theresa Wilson</author>
<author>David Yarowsky</author>
</authors>
<title>Broadly improving user classification via communication-based name and location clustering on Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>1010--1019</pages>
<marker>Bergsma, Dredze, Van Durme, Wilson, Yarowsky, 2013</marker>
<rawString>Shane Bergsma, Mark Dredze, Benjamin Van Durme, Theresa Wilson, and David Yarowsky. 2013. Broadly improving user classification via communication-based name and location clustering on Twitter. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 1010–1019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bifet</author>
<author>Geoff Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Philipp Kranen</author>
<author>Hardy Kremer</author>
<author>Timm Jansen</author>
<author>Thomas Seidl</author>
</authors>
<title>MOA: Massive online analysis, a framework for stream classification and clustering.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>11--44</pages>
<contexts>
<context position="34618" citStr="Bifet et al. (2010)" startWordPosition="5944" endWordPosition="5947">model of political ideology. Streaming Approaches Van Durme (2012b) proposed streaming models to predict user gender in Twitter. Other works suggested to process 12http://www.tweetcongress.org 193 text streams for a variety of NLP tasks e.g., realtime opinion mining and sentiment analysis in social media (Pang and Lee, 2008), named entity disambiguation (Sarmento et al., 2009), statistical machine translation (Levenberg et al., 2011), first story detection (Petrovi´c et al., 2010), and unsupervised dependency parsing (Goyal and Daum´e, 2011). Massive Online Analysis (MOA) toolkit developed by Bifet et al. (2010) is an alternative to the Jerboa package used in this work developed by Van Durme (2012a). MOA has been effectively used to detect sentiment changes in Twitter streams (Bifet et al., 2011). 9 Conclusions and Future Work In this paper, we extensively examined state-ofthe-art static approaches and proposed novel models with dynamic Bayesian updates for streaming personal analytics on Twitter. Because our streaming models rely on communications from Twitter users and content from various notions of userlocal neighborhood they can be effectively applied to real-time dynamic data streams. Our resul</context>
</contexts>
<marker>Bifet, Holmes, Pfahringer, Kranen, Kremer, Jansen, Seidl, 2010</marker>
<rawString>Albert Bifet, Geoff Holmes, Bernhard Pfahringer, Philipp Kranen, Hardy Kremer, Timm Jansen, and Thomas Seidl. 2010. MOA: Massive online analysis, a framework for stream classification and clustering. Journal of Machine Learning Research, 11:44–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bifet</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Ricard Gavald`a</author>
</authors>
<title>Detecting sentiment change in Twitter streaming data.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>17--5</pages>
<marker>Bifet, Holmes, Pfahringer, Gavald`a, 2011</marker>
<rawString>Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer, and Ricard Gavald`a. 2011. Detecting sentiment change in Twitter streaming data. Journal of Machine Learning Research, 17:5–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Burger</author>
<author>John Henderson</author>
<author>George Kim</author>
<author>Guido Zarrella</author>
</authors>
<title>Discriminating gender on Twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1301--1309</pages>
<contexts>
<context position="2100" citStr="Burger et al., 2011" startWordPosition="308" endWordPosition="311">g posts or public discussions has become increasingly popular with the web getting more social and volume of data available. Resources like Twitter1 or Facebook2 become extremely valuable for studying the underlying properties of such informal communications because of its volume, dynamic nature, and diverse population (Lunden, 2012; Smith, 2013). 1http://www.demographicspro.com/ 2http://www.wolframalpha.com/facebook/ The existing batch models for predicting latent user attributes rely on thousands of tweets per author (Rao et al., 2010; Conover et al., 2011; Pennacchiotti and Popescu, 2011a; Burger et al., 2011; Zamal et al., 2012; Nguyen et al., 2013). However, most Twitter users are less prolific than those examined in these works, and thus do not produce the thousands of tweets required to obtain their levels of accuracy e.g., the median number of tweets produced by a random Twitter user per day is 10. Moreover, recent changes to Twitter API querying rates further restrict the speed of access to this resource, effectively reducing the amount of data that can be collected in a given time period. In this paper we analyze and go beyond static models formulating personal analytics in social media as </context>
<context position="18839" citStr="Burger et al., 2011" startWordPosition="3185" endWordPosition="3188">ns on a random set of 5 vs. 100 tweets per user. To our knowledge only limited work on personal Figure 3: Classification probabilities for Φv4 estimated over 100 users in G.and tested on 5 (blue) vs. 100 (green) tweets per user where Republican = 1, Democratic = 0, filled markers = correctly classified, not filled = misclassified users. ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● 5 10 20 50 100 200 log(Tweets Per Neighbor) (c) G.and: 2 neighbors (d) G.and: 10 neighbors Figure 4: Modeling the influence of the number of tweets per neighbor t=[5, .., 200] for G.and and G9eo graphs. analytics (Burger et al., 2011; Van Durme, 2012b) have performed this straight-forward comparison. For that purpose, we take a random partition containing 100 users of Gcand graph and perform four independent classification experiments – two runs using 5 and two runs using 100 tweets per user. Figure 3 demonstrates that more tweets during prediction time lead to higher accuracy by showing that more users with 100 tweets are correctly classified e.g., filled green markers in the right upper quadrant are true Republicans and in the left lower quadrant are true Democrats. Moreover, a lot of users with 100 tweets are close to </context>
<context position="30202" citStr="Burger et al., 2011" startWordPosition="5255" endWordPosition="5258">200 100 0 Users 400 500 300 Users 200 100 0 192 models further confirms that neighborhood content is useful for political preference prediction. Moreover, communications from a joint stream allow to make an inference up to 7 times faster. 8 Related Work Supervised Batch Approaches The vast majority of work on predicting latent user attributes in social media apply supervised static SVM models for discrete categorical e.g., gender and regression models for continuous attributes e.g., age with lexical bag-of-word features for classifying user gender (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b), age (Rao et al., 2010; Nguyen et al., 2011; Nguyen et al., 2013) or political orientation. We present an overview of the existing models for political preference prediction in Table 1. Bergsma et al. (2012) following up on Rao’s work (2010) on adding socio-linguistic features to improve gender, ethnicity and political preference prediction show that incorporating stylistic and syntactic information to the bag-of-word features improves gender classification. Other methods characterize Twitter users by applying limited amounts of network structure information in addition to </context>
</contexts>
<marker>Burger, Henderson, Kim, Zarrella, 2011</marker>
<rawString>John D. Burger, John Henderson, George Kim, and Guido Zarrella. 2011. Discriminating gender on Twitter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1301–1309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raviv Cohen</author>
<author>Derek Ruths</author>
</authors>
<title>Classifying Political Orientation on Twitter: It’s Not Easy!</title>
<date>2013</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM),</booktitle>
<pages>91--99</pages>
<contexts>
<context position="1399" citStr="Cohen and Ruths, 2013" startWordPosition="211" endWordPosition="214">l to predict user preferences. We show that even when limited or no selfauthored data is available, language from friend, retweet and user mention communications provide sufficient evidence for prediction. When updating models over time based on Twitter, we find that political preference can be often be predicted using roughly 100 tweets, depending on the context of user selection, where this could mean hours, or weeks, based on the author’s tweeting frequency. 1 Introduction Inferring latent user attributes such as gender, age, and political preferences (Rao et al., 2011; Zamal et al., 2012; Cohen and Ruths, 2013) automatically from personal communications and social media including emails, blog posts or public discussions has become increasingly popular with the web getting more social and volume of data available. Resources like Twitter1 or Facebook2 become extremely valuable for studying the underlying properties of such informal communications because of its volume, dynamic nature, and diverse population (Lunden, 2012; Smith, 2013). 1http://www.demographicspro.com/ 2http://www.wolframalpha.com/facebook/ The existing batch models for predicting latent user attributes rely on thousands of tweets per </context>
</contexts>
<marker>Cohen, Ruths, 2013</marker>
<rawString>Raviv Cohen and Derek Ruths. 2013. Classifying Political Orientation on Twitter: It’s Not Easy! In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM), pages 91–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael D Conover</author>
<author>Bruno Gonc¸alves</author>
<author>Jacob Ratkiewicz</author>
<author>Alessandro Flammini</author>
<author>Filippo Menczer</author>
</authors>
<title>Predicting the political alignment of Twitter users.</title>
<date>2011</date>
<booktitle>In Proceedings of Social Computing,</booktitle>
<pages>192--199</pages>
<marker>Conover, Gonc¸alves, Ratkiewicz, Flammini, Menczer, 2011</marker>
<rawString>Michael D. Conover, Bruno Gonc¸alves, Jacob Ratkiewicz, Alessandro Flammini, and Filippo Menczer. 2011. Predicting the political alignment of Twitter users. In Proceedings of Social Computing, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>A latent variable model for geographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1277--1287</pages>
<marker>Eisenstein, O’Connor, Smith, Xing, 2010</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah A. Smith, and Eric P. Xing. 2010. A latent variable model for geographic lexical variation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1277–1287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong En Fan</author>
<author>Kai Wei Chang</author>
<author>Cho Jui Hsieh</author>
<author>Xiang Rui Wang</author>
<author>Chih Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="16297" citStr="Fan et al., 2008" startWordPosition="2753" endWordPosition="2756">ghbor tweets are available. We experiment with our static neighbor model defined in Eq.2 with the aim to: 1. evaluate neighborhood size influence, we change the number of neighbors and try n = [1, 2, 5,10] neighbor(s) per user; 2. estimate neighbor content influence, we alternate the amount of content per neighbor and try t = [5,10,15, 25, 50,100, 200] tweets. We perform 10-fold cross validation10 and run 100 random restarts for every n and t parameter combination. We compare our static neighbor and user models using the cost functions from Eq.3 and Eq.4. For all experiments we use LibLinear (Fan et al., 2008), integrated in the Jerboa toolkit (Van Durme, 2012a). Both models defined in Eq.1 and Eq.2 are learned using normalized count-based word ngram features extracted from either user or neighbor tweets.11 10For each fold we split the data into 3 parts: 70% train, 10% development and 20% test. 11For brevity we omit reporting results for bigram and trigram features, since unigrams showed superior performance. 189 5.2 Streaming Classification Experiments We evaluate our models with dynamic Bayesian updates on a continuous stream of communications over time as shown in Figure 2. Unlike static model e</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong En Fan, Kai Wei Chang, Cho Jui Hsieh, Xiang Rui Wang, and Chih Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
</authors>
<title>User demographics and language in an implicit social network.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>1478--1488</pages>
<contexts>
<context position="5796" citStr="Filippova (2012)" startWordPosition="913" endWordPosition="914">h a latent attribute a(vi), in our case it is binary a(vi) E {D, R}, where D stands for Democratic and R for Republican users. Each edge eij E E represents a connection between vi and vj, eij = (vi, vj) and defines different social circles between Twitter users e.g., follower (f), friend (b), user mention (m), hashtag (h), reply (y) and retweet (w). Thus, E E V (2) x{f, b, h, m, w, y}. We denote a set of edges of a given type as 0,.(E) for r E {f, b, h, m, w, y}. We denote a set of vertices adjacent to vi by social circle type r as N,.(vi) which is equivalent to {vj I eij E 0,.(E)}. Following Filippova (2012) we refer to N,.(vi) as vi’s social circle, otherwise known as a neighborhood. In most cases, we only work with a sample of a social circle, denoted by Nr(vi) where IN,(vi)I = k is its size for vi. Figure 1 presents an example of a social graph derived from Twitter. Notably, users from different social circles can be shared across the users of the same or different classes e.g., a user vj can be 3The code and detailed explanation on how we collected all six types of user neighbors and their communications using Twitter API can be found here: http://www.cs.jhu.edu/ svitlana/ Figure 1: An exampl</context>
</contexts>
<marker>Filippova, 2012</marker>
<rawString>Katja Filippova. 2012. User demographics and language in an implicit social network. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 1478–1488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikesh Garera</author>
<author>David Yarowsky</author>
</authors>
<title>Modeling latent biographic attributes in conversational genres.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>710--718</pages>
<contexts>
<context position="30163" citStr="Garera and Yarowsky, 2009" startWordPosition="5247" endWordPosition="5250"> (d) User-Neigh GZLR Users 400 500 300 Users 200 100 0 Users 400 500 300 Users 200 100 0 192 models further confirms that neighborhood content is useful for political preference prediction. Moreover, communications from a joint stream allow to make an inference up to 7 times faster. 8 Related Work Supervised Batch Approaches The vast majority of work on predicting latent user attributes in social media apply supervised static SVM models for discrete categorical e.g., gender and regression models for continuous attributes e.g., age with lexical bag-of-word features for classifying user gender (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b), age (Rao et al., 2010; Nguyen et al., 2011; Nguyen et al., 2013) or political orientation. We present an overview of the existing models for political preference prediction in Table 1. Bergsma et al. (2012) following up on Rao’s work (2010) on adding socio-linguistic features to improve gender, ethnicity and political preference prediction show that incorporating stylistic and syntactic information to the bag-of-word features improves gender classification. Other methods characterize Twitter users by applying limited amounts of networ</context>
</contexts>
<marker>Garera, Yarowsky, 2009</marker>
<rawString>Nikesh Garera and David Yarowsky. 2009. Modeling latent biographic attributes in conversational genres. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 710–718.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gayo-Avello</author>
</authors>
<title>No, you cannot predict elections with Twitter.</title>
<date>2012</date>
<booktitle>Internet Computing, IEEE,</booktitle>
<pages>16--6</pages>
<contexts>
<context position="33033" citStr="Gayo-Avello, 2012" startWordPosition="5708" endWordPosition="5709">Recent work by Wong et al. (2013) investigates tweeting and retweeting behavior for political learning during 2012 US Presidential election. The most similar work to ours is by Zamal et al. (2012), where the authors apply features from the tweets authored by a user’s friend to infer attributes of that user. In this paper, we study different types of user social circles in addition to a friend network. Additionally, using social media for mining political opinions (O’Connor et al., 2010a; Maynard and Funk, 2012) or understanding sociopolitical trends and voting outcomes (Tumasjan et al., 2010; Gayo-Avello, 2012; Lampos et al., 2013) is becoming a common practice. For instance, Lampos et al. (2013) propose a bilinear user-centric model for predicting voting intentions in the UK and Australia from social media data. Other works explore political blogs to predict what content will get the most comments (Yano et al., 2013) or analyze communications from Capitol Hill12 to predict campaign contributors based on this content (Yano and Smith, 2013). Unsupervised Batch Approaches Bergsma et al. (2013) show that large-scale clustering of user names improves gender, ethnicity and location classification on Twi</context>
</contexts>
<marker>Gayo-Avello, 2012</marker>
<rawString>Daniel Gayo-Avello. 2012. No, you cannot predict elections with Twitter. Internet Computing, IEEE, 16(6):91–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Golbeck</author>
<author>Justin M Grimes</author>
<author>Anthony Rogers</author>
</authors>
<title>Twitter use by the u.s. congress.</title>
<date>2010</date>
<journal>Journal of the American Society forInformation Science and Technology,</journal>
<volume>61</volume>
<issue>8</issue>
<contexts>
<context position="33960" citStr="Golbeck et al. (2010)" startWordPosition="5847" endWordPosition="5850">., 2013) or analyze communications from Capitol Hill12 to predict campaign contributors based on this content (Yano and Smith, 2013). Unsupervised Batch Approaches Bergsma et al. (2013) show that large-scale clustering of user names improves gender, ethnicity and location classification on Twitter. O’Connor et al. (2010b) following the work by Eisenstein (2010) propose a Bayesian generative model to discover demographic language variations in Twitter. Rao et al. (2011) suggest a hierarchical Bayesian model which takes advantage of user name morphology for predicting user gender and ethnicity. Golbeck et al. (2010) incorporate Twitter data in a spatial model of political ideology. Streaming Approaches Van Durme (2012b) proposed streaming models to predict user gender in Twitter. Other works suggested to process 12http://www.tweetcongress.org 193 text streams for a variety of NLP tasks e.g., realtime opinion mining and sentiment analysis in social media (Pang and Lee, 2008), named entity disambiguation (Sarmento et al., 2009), statistical machine translation (Levenberg et al., 2011), first story detection (Petrovi´c et al., 2010), and unsupervised dependency parsing (Goyal and Daum´e, 2011). Massive Onli</context>
</contexts>
<marker>Golbeck, Grimes, Rogers, 2010</marker>
<rawString>Jennifer Golbeck, Justin M. Grimes, and Anthony Rogers. 2010. Twitter use by the u.s. congress. Journal of the American Society forInformation Science and Technology, 61(8):1612–1621.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Goyal</author>
<author>Hal Daum´e</author>
</authors>
<title>Approximate scalable bounded space sketch for large data NLP.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>250--261</pages>
<marker>Goyal, Daum´e, 2011</marker>
<rawString>Amit Goyal and Hal Daum´e, III. 2011. Approximate scalable bounded space sketch for large data NLP. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 250–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Daniel Preotiuc-Pietro</author>
<author>Trevor Cohn</author>
</authors>
<title>A user-centric model of voting intention from social media.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>993--1003</pages>
<contexts>
<context position="33055" citStr="Lampos et al., 2013" startWordPosition="5710" endWordPosition="5713"> et al. (2013) investigates tweeting and retweeting behavior for political learning during 2012 US Presidential election. The most similar work to ours is by Zamal et al. (2012), where the authors apply features from the tweets authored by a user’s friend to infer attributes of that user. In this paper, we study different types of user social circles in addition to a friend network. Additionally, using social media for mining political opinions (O’Connor et al., 2010a; Maynard and Funk, 2012) or understanding sociopolitical trends and voting outcomes (Tumasjan et al., 2010; Gayo-Avello, 2012; Lampos et al., 2013) is becoming a common practice. For instance, Lampos et al. (2013) propose a bilinear user-centric model for predicting voting intentions in the UK and Australia from social media data. Other works explore political blogs to predict what content will get the most comments (Yano et al., 2013) or analyze communications from Capitol Hill12 to predict campaign contributors based on this content (Yano and Smith, 2013). Unsupervised Batch Approaches Bergsma et al. (2013) show that large-scale clustering of user names improves gender, ethnicity and location classification on Twitter. O’Connor et al. </context>
</contexts>
<marker>Lampos, Preotiuc-Pietro, Cohn, 2013</marker>
<rawString>Vasileios Lampos, Daniel Preotiuc-Pietro, and Trevor Cohn. 2013. A user-centric model of voting intention from social media. In Proceedings of the Association for Computational Linguistics (ACL), pages 993–1003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abby Levenberg</author>
<author>Miles Osborne</author>
<author>David Matthews</author>
</authors>
<title>Multiple-stream language models for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>177--186</pages>
<contexts>
<context position="34436" citStr="Levenberg et al., 2011" startWordPosition="5917" endWordPosition="5920">suggest a hierarchical Bayesian model which takes advantage of user name morphology for predicting user gender and ethnicity. Golbeck et al. (2010) incorporate Twitter data in a spatial model of political ideology. Streaming Approaches Van Durme (2012b) proposed streaming models to predict user gender in Twitter. Other works suggested to process 12http://www.tweetcongress.org 193 text streams for a variety of NLP tasks e.g., realtime opinion mining and sentiment analysis in social media (Pang and Lee, 2008), named entity disambiguation (Sarmento et al., 2009), statistical machine translation (Levenberg et al., 2011), first story detection (Petrovi´c et al., 2010), and unsupervised dependency parsing (Goyal and Daum´e, 2011). Massive Online Analysis (MOA) toolkit developed by Bifet et al. (2010) is an alternative to the Jerboa package used in this work developed by Van Durme (2012a). MOA has been effectively used to detect sentiment changes in Twitter streams (Bifet et al., 2011). 9 Conclusions and Future Work In this paper, we extensively examined state-ofthe-art static approaches and proposed novel models with dynamic Bayesian updates for streaming personal analytics on Twitter. Because our streaming mo</context>
</contexts>
<marker>Levenberg, Osborne, Matthews, 2011</marker>
<rawString>Abby Levenberg, Miles Osborne, and David Matthews. 2011. Multiple-stream language models for statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation (WMT), pages 177–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Loglinear models for word alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>459--466</pages>
<contexts>
<context position="10561" citStr="Liu et al., 2005" startWordPosition="1713" endWordPosition="1716">set was collected in 2012 and has been recently released at http://icwsm.cs.mcgill.ca/. Political labels are extracted from http://www.wefollow.com as described by Pennacchiotti and Popescu (2011b). 6This inability to perfectly replicate prior work based on Twitter is a recognized problem throughout the community of computational social science, arising from the data policies of Twitter itself, it is not specific to this work. 7We use log-linear models over reasonable alternatives such as perceptron or SVM, following the practice of a wide range of previous work in related areas (Smith, 2004; Liu et al., 2005; Poon et al., 2009) including text classification in social media (Van Durme, 2012b; Yang and Eisenstein, 2013). • the order of the social circle – the number of neighbors per user of interest |Nr |= deg(vi), n = 11, 2,5, 101. Our goal is to classify users of interest using evidence (e.g., communications) from their local neighborhood P �ft[Nr(vi)] � f(Nr) as Demon cratic or Republican. The corresponding loglinear model is defined as: _ f D (1 + exp[ −B · f (Nr)])−1 &gt; 0.5, ΦNr l R otherwise. (2) To check whether our static models are cognizant of low-resource prediction settings we compare th</context>
</contexts>
<marker>Liu, Liu, Lin, 2005</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2005. Loglinear models for word alignment. In Proceedings of the Annual Meeting on Association for Computational Linguistics (ACL), pages 459–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Lunden</author>
</authors>
<title>Analyst: Twitter passed 500M users in june 2012, 140m of them in US; Jakarta ‘biggest tweeting’</title>
<date>2012</date>
<pages>2012--07</pages>
<contexts>
<context position="1815" citStr="Lunden, 2012" startWordPosition="274" endWordPosition="275">sed on the author’s tweeting frequency. 1 Introduction Inferring latent user attributes such as gender, age, and political preferences (Rao et al., 2011; Zamal et al., 2012; Cohen and Ruths, 2013) automatically from personal communications and social media including emails, blog posts or public discussions has become increasingly popular with the web getting more social and volume of data available. Resources like Twitter1 or Facebook2 become extremely valuable for studying the underlying properties of such informal communications because of its volume, dynamic nature, and diverse population (Lunden, 2012; Smith, 2013). 1http://www.demographicspro.com/ 2http://www.wolframalpha.com/facebook/ The existing batch models for predicting latent user attributes rely on thousands of tweets per author (Rao et al., 2010; Conover et al., 2011; Pennacchiotti and Popescu, 2011a; Burger et al., 2011; Zamal et al., 2012; Nguyen et al., 2013). However, most Twitter users are less prolific than those examined in these works, and thus do not produce the thousands of tweets required to obtain their levels of accuracy e.g., the median number of tweets produced by a random Twitter user per day is 10. Moreover, rece</context>
</contexts>
<marker>Lunden, 2012</marker>
<rawString>Ingrid Lunden. 2012. Analyst: Twitter passed 500M users in june 2012, 140m of them in US; Jakarta ‘biggest tweeting’ city. http://techcrunch.com/2012/07/30/analyst-twitterpassed-500m-users-in-june-2012-140m-of-them-inus-jakarta-biggest-tweeting-city/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Maynard</author>
<author>Adam Funk</author>
</authors>
<title>Automatic detection of political opinions in tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on The Semantic Web (ESWC),</booktitle>
<pages>88--99</pages>
<contexts>
<context position="32932" citStr="Maynard and Funk, 2012" startWordPosition="5691" endWordPosition="5695">same class. We extend this assumption and study other relationship types e.g., friends, user mentions etc. Recent work by Wong et al. (2013) investigates tweeting and retweeting behavior for political learning during 2012 US Presidential election. The most similar work to ours is by Zamal et al. (2012), where the authors apply features from the tweets authored by a user’s friend to infer attributes of that user. In this paper, we study different types of user social circles in addition to a friend network. Additionally, using social media for mining political opinions (O’Connor et al., 2010a; Maynard and Funk, 2012) or understanding sociopolitical trends and voting outcomes (Tumasjan et al., 2010; Gayo-Avello, 2012; Lampos et al., 2013) is becoming a common practice. For instance, Lampos et al. (2013) propose a bilinear user-centric model for predicting voting intentions in the UK and Australia from social media data. Other works explore political blogs to predict what content will get the most comments (Yano et al., 2013) or analyze communications from Capitol Hill12 to predict campaign contributors based on this content (Yano and Smith, 2013). Unsupervised Batch Approaches Bergsma et al. (2013) show th</context>
</contexts>
<marker>Maynard, Funk, 2012</marker>
<rawString>Diana Maynard and Adam Funk. 2012. Automatic detection of political opinions in tweets. In Proceedings of the 8th International Conference on The Semantic Web (ESWC), pages 88–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Ming Fai Wong</author>
<author>Chee Wei Tan</author>
<author>Soumya Sen</author>
<author>Mung Chiang</author>
</authors>
<title>Quantifying political leaning from tweets and retweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="32449" citStr="Wong et al. (2013)" startWordPosition="5612" endWordPosition="5615">ntifying strong partisan clusters of Democratic and Republican users in a Twitter network based on retweet and user mention degree of connectivity, and then combine this clustering information with the follower and friend neighborhood size features. Pennacchiotti et al. (2011a; 2011b) focus on user behavior, network structure and linguistic features. Similar to our work, they assume that users from a particular class tend to reply and retweet messages of the users from the same class. We extend this assumption and study other relationship types e.g., friends, user mentions etc. Recent work by Wong et al. (2013) investigates tweeting and retweeting behavior for political learning during 2012 US Presidential election. The most similar work to ours is by Zamal et al. (2012), where the authors apply features from the tweets authored by a user’s friend to infer attributes of that user. In this paper, we study different types of user social circles in addition to a friend network. Additionally, using social media for mining political opinions (O’Connor et al., 2010a; Maynard and Funk, 2012) or understanding sociopolitical trends and voting outcomes (Tumasjan et al., 2010; Gayo-Avello, 2012; Lampos et al.,</context>
</contexts>
<marker>Wong, Tan, Sen, Chiang, 2013</marker>
<rawString>Felix Ming Fai Wong, Chee Wei Tan, Soumya Sen, and Mung Chiang. 2013. Quantifying political leaning from tweets and retweets. In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Nguyen</author>
<author>Noah A Smith</author>
<author>Carolyn P Ros´e</author>
</authors>
<title>Author age prediction from text using linear regression.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th ACLHLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH),</booktitle>
<pages>115--123</pages>
<marker>Nguyen, Smith, Ros´e, 2011</marker>
<rawString>Dong Nguyen, Noah A. Smith, and Carolyn P. Ros´e. 2011. Author age prediction from text using linear regression. In Proceedings of the 5th ACLHLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH), pages 115–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Nguyen</author>
<author>Rilana Gravel</author>
<author>Dolf Trieschnigg</author>
<author>Theo Meder</author>
</authors>
<title>How old do you think I am?” A study of language and age in Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the AAAI Conference on Weblogs and Social Media (ICWSM),</booktitle>
<pages>439--448</pages>
<contexts>
<context position="2142" citStr="Nguyen et al., 2013" startWordPosition="316" endWordPosition="319">increasingly popular with the web getting more social and volume of data available. Resources like Twitter1 or Facebook2 become extremely valuable for studying the underlying properties of such informal communications because of its volume, dynamic nature, and diverse population (Lunden, 2012; Smith, 2013). 1http://www.demographicspro.com/ 2http://www.wolframalpha.com/facebook/ The existing batch models for predicting latent user attributes rely on thousands of tweets per author (Rao et al., 2010; Conover et al., 2011; Pennacchiotti and Popescu, 2011a; Burger et al., 2011; Zamal et al., 2012; Nguyen et al., 2013). However, most Twitter users are less prolific than those examined in these works, and thus do not produce the thousands of tweets required to obtain their levels of accuracy e.g., the median number of tweets produced by a random Twitter user per day is 10. Moreover, recent changes to Twitter API querying rates further restrict the speed of access to this resource, effectively reducing the amount of data that can be collected in a given time period. In this paper we analyze and go beyond static models formulating personal analytics in social media as a streaming task. We first evaluate batch </context>
<context position="30287" citStr="Nguyen et al., 2013" startWordPosition="5271" endWordPosition="5274">rhood content is useful for political preference prediction. Moreover, communications from a joint stream allow to make an inference up to 7 times faster. 8 Related Work Supervised Batch Approaches The vast majority of work on predicting latent user attributes in social media apply supervised static SVM models for discrete categorical e.g., gender and regression models for continuous attributes e.g., age with lexical bag-of-word features for classifying user gender (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b), age (Rao et al., 2010; Nguyen et al., 2011; Nguyen et al., 2013) or political orientation. We present an overview of the existing models for political preference prediction in Table 1. Bergsma et al. (2012) following up on Rao’s work (2010) on adding socio-linguistic features to improve gender, ethnicity and political preference prediction show that incorporating stylistic and syntactic information to the bag-of-word features improves gender classification. Other methods characterize Twitter users by applying limited amounts of network structure information in addition to lexical features. ConApproach Users Tweets Features Accur. Rao et al. 1K 2M ngrams 0.</context>
</contexts>
<marker>Nguyen, Gravel, Trieschnigg, Meder, 2013</marker>
<rawString>Dong Nguyen, Rilana Gravel, Dolf Trieschnigg, and Theo Meder. 2013. ”How old do you think I am?” A study of language and age in Twitter. In Proceedings of the AAAI Conference on Weblogs and Social Media (ICWSM), pages 439–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM),</booktitle>
<pages>122--129</pages>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010a. From tweets to polls: Linking text sentiment to public opinion time series. In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM), pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Jacob Eisenstein</author>
<author>Eric P Xing</author>
<author>Noah A Smith</author>
</authors>
<title>A mixture model of demographic lexical variation.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS Workshop on Machine Learning and Social Computing.</booktitle>
<marker>O’Connor, Eisenstein, Xing, Smith, 2010</marker>
<rawString>Brendan O’Connor, Jacob Eisenstein, Eric P. Xing, and Noah A. Smith. 2010b. A mixture model of demographic lexical variation. In Proceedings of the NIPS Workshop on Machine Learning and Social Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations of Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="34325" citStr="Pang and Lee, 2008" startWordPosition="5903" endWordPosition="5906">pose a Bayesian generative model to discover demographic language variations in Twitter. Rao et al. (2011) suggest a hierarchical Bayesian model which takes advantage of user name morphology for predicting user gender and ethnicity. Golbeck et al. (2010) incorporate Twitter data in a spatial model of political ideology. Streaming Approaches Van Durme (2012b) proposed streaming models to predict user gender in Twitter. Other works suggested to process 12http://www.tweetcongress.org 193 text streams for a variety of NLP tasks e.g., realtime opinion mining and sentiment analysis in social media (Pang and Lee, 2008), named entity disambiguation (Sarmento et al., 2009), statistical machine translation (Levenberg et al., 2011), first story detection (Petrovi´c et al., 2010), and unsupervised dependency parsing (Goyal and Daum´e, 2011). Massive Online Analysis (MOA) toolkit developed by Bifet et al. (2010) is an alternative to the Jerboa package used in this work developed by Van Durme (2012a). MOA has been effectively used to detect sentiment changes in Twitter streams (Bifet et al., 2011). 9 Conclusions and Future Work In this paper, we extensively examined state-ofthe-art static approaches and proposed n</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations of Trends in Information Retrieval, 2(1-2):1–135, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Ana-Maria Popescu</author>
</authors>
<title>Democrats, republicans and starbucks afficionados: user classification in twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>430--438</pages>
<contexts>
<context position="2078" citStr="Pennacchiotti and Popescu, 2011" startWordPosition="304" endWordPosition="307">social media including emails, blog posts or public discussions has become increasingly popular with the web getting more social and volume of data available. Resources like Twitter1 or Facebook2 become extremely valuable for studying the underlying properties of such informal communications because of its volume, dynamic nature, and diverse population (Lunden, 2012; Smith, 2013). 1http://www.demographicspro.com/ 2http://www.wolframalpha.com/facebook/ The existing batch models for predicting latent user attributes rely on thousands of tweets per author (Rao et al., 2010; Conover et al., 2011; Pennacchiotti and Popescu, 2011a; Burger et al., 2011; Zamal et al., 2012; Nguyen et al., 2013). However, most Twitter users are less prolific than those examined in these works, and thus do not produce the thousands of tweets required to obtain their levels of accuracy e.g., the median number of tweets produced by a random Twitter user per day is 10. Moreover, recent changes to Twitter API querying rates further restrict the speed of access to this resource, effectively reducing the amount of data that can be collected in a given time period. In this paper we analyze and go beyond static models formulating personal analyti</context>
<context position="10140" citStr="Pennacchiotti and Popescu (2011" startWordPosition="1646" endWordPosition="1649">ification in social media as described in Section 8. Next we propose to extend the baseline model by taking advantage of language in user social circles as describe below. Neighbor Model As input we are given user-local neighborhood Nr(vi), where r is a neighborhood type. Besides the neighborhood’s type r, each is characterized by: • the number of communications per neighbor ft(Nr), t = 15,10,15, 25, 50,100, 2001; 5The original dataset was collected in 2012 and has been recently released at http://icwsm.cs.mcgill.ca/. Political labels are extracted from http://www.wefollow.com as described by Pennacchiotti and Popescu (2011b). 6This inability to perfectly replicate prior work based on Twitter is a recognized problem throughout the community of computational social science, arising from the data policies of Twitter itself, it is not specific to this work. 7We use log-linear models over reasonable alternatives such as perceptron or SVM, following the practice of a wide range of previous work in related areas (Smith, 2004; Liu et al., 2005; Poon et al., 2009) including text classification in social media (Van Durme, 2012b; Yang and Eisenstein, 2013). • the order of the social circle – the number of neighbors per us</context>
</contexts>
<marker>Pennacchiotti, Popescu, 2011</marker>
<rawString>Marco Pennacchiotti and Ana-Maria Popescu. 2011a. Democrats, republicans and starbucks afficionados: user classification in twitter. In Proceedings of the 17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 430–438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Ana Maria Popescu</author>
</authors>
<title>A machine learning approach to Twitter user classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM),</booktitle>
<pages>281--288</pages>
<contexts>
<context position="2078" citStr="Pennacchiotti and Popescu, 2011" startWordPosition="304" endWordPosition="307">social media including emails, blog posts or public discussions has become increasingly popular with the web getting more social and volume of data available. Resources like Twitter1 or Facebook2 become extremely valuable for studying the underlying properties of such informal communications because of its volume, dynamic nature, and diverse population (Lunden, 2012; Smith, 2013). 1http://www.demographicspro.com/ 2http://www.wolframalpha.com/facebook/ The existing batch models for predicting latent user attributes rely on thousands of tweets per author (Rao et al., 2010; Conover et al., 2011; Pennacchiotti and Popescu, 2011a; Burger et al., 2011; Zamal et al., 2012; Nguyen et al., 2013). However, most Twitter users are less prolific than those examined in these works, and thus do not produce the thousands of tweets required to obtain their levels of accuracy e.g., the median number of tweets produced by a random Twitter user per day is 10. Moreover, recent changes to Twitter API querying rates further restrict the speed of access to this resource, effectively reducing the amount of data that can be collected in a given time period. In this paper we analyze and go beyond static models formulating personal analyti</context>
<context position="10140" citStr="Pennacchiotti and Popescu (2011" startWordPosition="1646" endWordPosition="1649">ification in social media as described in Section 8. Next we propose to extend the baseline model by taking advantage of language in user social circles as describe below. Neighbor Model As input we are given user-local neighborhood Nr(vi), where r is a neighborhood type. Besides the neighborhood’s type r, each is characterized by: • the number of communications per neighbor ft(Nr), t = 15,10,15, 25, 50,100, 2001; 5The original dataset was collected in 2012 and has been recently released at http://icwsm.cs.mcgill.ca/. Political labels are extracted from http://www.wefollow.com as described by Pennacchiotti and Popescu (2011b). 6This inability to perfectly replicate prior work based on Twitter is a recognized problem throughout the community of computational social science, arising from the data policies of Twitter itself, it is not specific to this work. 7We use log-linear models over reasonable alternatives such as perceptron or SVM, following the practice of a wide range of previous work in related areas (Smith, 2004; Liu et al., 2005; Poon et al., 2009) including text classification in social media (Van Durme, 2012b; Yang and Eisenstein, 2013). • the order of the social circle – the number of neighbors per us</context>
</contexts>
<marker>Pennacchiotti, Popescu, 2011</marker>
<rawString>Marco Pennacchiotti and Ana Maria Popescu. 2011b. A machine learning approach to Twitter user classification. In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM), pages 281–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saˇsa Petrovi´c</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Streaming first story detection with application to Twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<marker>Petrovi´c, Osborne, Lavrenko, 2010</marker>
<rawString>Saˇsa Petrovi´c, Miles Osborne, and Victor Lavrenko. 2010. Streaming first story detection with application to Twitter. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>209--217</pages>
<contexts>
<context position="10581" citStr="Poon et al., 2009" startWordPosition="1717" endWordPosition="1720">in 2012 and has been recently released at http://icwsm.cs.mcgill.ca/. Political labels are extracted from http://www.wefollow.com as described by Pennacchiotti and Popescu (2011b). 6This inability to perfectly replicate prior work based on Twitter is a recognized problem throughout the community of computational social science, arising from the data policies of Twitter itself, it is not specific to this work. 7We use log-linear models over reasonable alternatives such as perceptron or SVM, following the practice of a wide range of previous work in related areas (Smith, 2004; Liu et al., 2005; Poon et al., 2009) including text classification in social media (Van Durme, 2012b; Yang and Eisenstein, 2013). • the order of the social circle – the number of neighbors per user of interest |Nr |= deg(vi), n = 11, 2,5, 101. Our goal is to classify users of interest using evidence (e.g., communications) from their local neighborhood P �ft[Nr(vi)] � f(Nr) as Demon cratic or Republican. The corresponding loglinear model is defined as: _ f D (1 + exp[ −B · f (Nr)])−1 &gt; 0.5, ΦNr l R otherwise. (2) To check whether our static models are cognizant of low-resource prediction settings we compare the performance of the</context>
</contexts>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>Hoifung Poon, Colin Cherry, and Kristina Toutanova. 2009. Unsupervised morphological segmentation with log-linear models. In Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 209–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>David Yarowsky</author>
<author>Abhishek Shreevats</author>
<author>Manaswi Gupta</author>
</authors>
<title>Classifying latent user attributes in Twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2nd International Workshop on Search and Mining Usergenerated Contents (SMUC),</booktitle>
<pages>37--44</pages>
<contexts>
<context position="2023" citStr="Rao et al., 2010" startWordPosition="296" endWordPosition="299">ically from personal communications and social media including emails, blog posts or public discussions has become increasingly popular with the web getting more social and volume of data available. Resources like Twitter1 or Facebook2 become extremely valuable for studying the underlying properties of such informal communications because of its volume, dynamic nature, and diverse population (Lunden, 2012; Smith, 2013). 1http://www.demographicspro.com/ 2http://www.wolframalpha.com/facebook/ The existing batch models for predicting latent user attributes rely on thousands of tweets per author (Rao et al., 2010; Conover et al., 2011; Pennacchiotti and Popescu, 2011a; Burger et al., 2011; Zamal et al., 2012; Nguyen et al., 2013). However, most Twitter users are less prolific than those examined in these works, and thus do not produce the thousands of tweets required to obtain their levels of accuracy e.g., the median number of tweets produced by a random Twitter user per day is 10. Moreover, recent changes to Twitter API querying rates further restrict the speed of access to this resource, effectively reducing the amount of data that can be collected in a given time period. In this paper we analyze a</context>
<context position="30181" citStr="Rao et al., 2010" startWordPosition="5251" endWordPosition="5254">400 500 300 Users 200 100 0 Users 400 500 300 Users 200 100 0 192 models further confirms that neighborhood content is useful for political preference prediction. Moreover, communications from a joint stream allow to make an inference up to 7 times faster. 8 Related Work Supervised Batch Approaches The vast majority of work on predicting latent user attributes in social media apply supervised static SVM models for discrete categorical e.g., gender and regression models for continuous attributes e.g., age with lexical bag-of-word features for classifying user gender (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b), age (Rao et al., 2010; Nguyen et al., 2011; Nguyen et al., 2013) or political orientation. We present an overview of the existing models for political preference prediction in Table 1. Bergsma et al. (2012) following up on Rao’s work (2010) on adding socio-linguistic features to improve gender, ethnicity and political preference prediction show that incorporating stylistic and syntactic information to the bag-of-word features improves gender classification. Other methods characterize Twitter users by applying limited amounts of network structure inform</context>
</contexts>
<marker>Rao, Yarowsky, Shreevats, Gupta, 2010</marker>
<rawString>Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. 2010. Classifying latent user attributes in Twitter. In Proceedings of the 2nd International Workshop on Search and Mining Usergenerated Contents (SMUC), pages 37–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Michael Paul</author>
<author>Clay Fink</author>
<author>David Yarowsky</author>
<author>Timothy Oates</author>
<author>Glen Coppersmith</author>
</authors>
<title>Hierarchical Bayesian models for latent attribute detection in social media.</title>
<date>2011</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="1355" citStr="Rao et al., 2011" startWordPosition="202" endWordPosition="205">and tweets required for a dynamic model to predict user preferences. We show that even when limited or no selfauthored data is available, language from friend, retweet and user mention communications provide sufficient evidence for prediction. When updating models over time based on Twitter, we find that political preference can be often be predicted using roughly 100 tweets, depending on the context of user selection, where this could mean hours, or weeks, based on the author’s tweeting frequency. 1 Introduction Inferring latent user attributes such as gender, age, and political preferences (Rao et al., 2011; Zamal et al., 2012; Cohen and Ruths, 2013) automatically from personal communications and social media including emails, blog posts or public discussions has become increasingly popular with the web getting more social and volume of data available. Resources like Twitter1 or Facebook2 become extremely valuable for studying the underlying properties of such informal communications because of its volume, dynamic nature, and diverse population (Lunden, 2012; Smith, 2013). 1http://www.demographicspro.com/ 2http://www.wolframalpha.com/facebook/ The existing batch models for predicting latent user</context>
<context position="33812" citStr="Rao et al. (2011)" startWordPosition="5825" endWordPosition="5828"> the UK and Australia from social media data. Other works explore political blogs to predict what content will get the most comments (Yano et al., 2013) or analyze communications from Capitol Hill12 to predict campaign contributors based on this content (Yano and Smith, 2013). Unsupervised Batch Approaches Bergsma et al. (2013) show that large-scale clustering of user names improves gender, ethnicity and location classification on Twitter. O’Connor et al. (2010b) following the work by Eisenstein (2010) propose a Bayesian generative model to discover demographic language variations in Twitter. Rao et al. (2011) suggest a hierarchical Bayesian model which takes advantage of user name morphology for predicting user gender and ethnicity. Golbeck et al. (2010) incorporate Twitter data in a spatial model of political ideology. Streaming Approaches Van Durme (2012b) proposed streaming models to predict user gender in Twitter. Other works suggested to process 12http://www.tweetcongress.org 193 text streams for a variety of NLP tasks e.g., realtime opinion mining and sentiment analysis in social media (Pang and Lee, 2008), named entity disambiguation (Sarmento et al., 2009), statistical machine translation </context>
</contexts>
<marker>Rao, Paul, Fink, Yarowsky, Oates, Coppersmith, 2011</marker>
<rawString>Delip Rao, Michael Paul, Clay Fink, David Yarowsky, Timothy Oates, and Glen Coppersmith. 2011. Hierarchical Bayesian models for latent attribute detection in social media. In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu´ıs Sarmento</author>
<author>Alexander Kehlenbeck</author>
<author>Eug´enio Oliveira</author>
<author>Lyle Ungar</author>
</authors>
<title>An approach to web-scale named-entity disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 6th International Conference on Machine Learning and Data Mining in Pattern Recognition (MLDM),</booktitle>
<pages>689--703</pages>
<contexts>
<context position="34378" citStr="Sarmento et al., 2009" startWordPosition="5910" endWordPosition="5913">raphic language variations in Twitter. Rao et al. (2011) suggest a hierarchical Bayesian model which takes advantage of user name morphology for predicting user gender and ethnicity. Golbeck et al. (2010) incorporate Twitter data in a spatial model of political ideology. Streaming Approaches Van Durme (2012b) proposed streaming models to predict user gender in Twitter. Other works suggested to process 12http://www.tweetcongress.org 193 text streams for a variety of NLP tasks e.g., realtime opinion mining and sentiment analysis in social media (Pang and Lee, 2008), named entity disambiguation (Sarmento et al., 2009), statistical machine translation (Levenberg et al., 2011), first story detection (Petrovi´c et al., 2010), and unsupervised dependency parsing (Goyal and Daum´e, 2011). Massive Online Analysis (MOA) toolkit developed by Bifet et al. (2010) is an alternative to the Jerboa package used in this work developed by Van Durme (2012a). MOA has been effectively used to detect sentiment changes in Twitter streams (Bifet et al., 2011). 9 Conclusions and Future Work In this paper, we extensively examined state-ofthe-art static approaches and proposed novel models with dynamic Bayesian updates for streami</context>
</contexts>
<marker>Sarmento, Kehlenbeck, Oliveira, Ungar, 2009</marker>
<rawString>Lu´ıs Sarmento, Alexander Kehlenbeck, Eug´enio Oliveira, and Lyle Ungar. 2009. An approach to web-scale named-entity disambiguation. In Proceedings of the 6th International Conference on Machine Learning and Data Mining in Pattern Recognition (MLDM), pages 689–703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
</authors>
<date>2004</date>
<note>Log-linear models.</note>
<contexts>
<context position="10543" citStr="Smith, 2004" startWordPosition="1711" endWordPosition="1712">original dataset was collected in 2012 and has been recently released at http://icwsm.cs.mcgill.ca/. Political labels are extracted from http://www.wefollow.com as described by Pennacchiotti and Popescu (2011b). 6This inability to perfectly replicate prior work based on Twitter is a recognized problem throughout the community of computational social science, arising from the data policies of Twitter itself, it is not specific to this work. 7We use log-linear models over reasonable alternatives such as perceptron or SVM, following the practice of a wide range of previous work in related areas (Smith, 2004; Liu et al., 2005; Poon et al., 2009) including text classification in social media (Van Durme, 2012b; Yang and Eisenstein, 2013). • the order of the social circle – the number of neighbors per user of interest |Nr |= deg(vi), n = 11, 2,5, 101. Our goal is to classify users of interest using evidence (e.g., communications) from their local neighborhood P �ft[Nr(vi)] � f(Nr) as Demon cratic or Republican. The corresponding loglinear model is defined as: _ f D (1 + exp[ −B · f (Nr)])−1 &gt; 0.5, ΦNr l R otherwise. (2) To check whether our static models are cognizant of low-resource prediction sett</context>
</contexts>
<marker>Smith, 2004</marker>
<rawString>Noah A. Smith. 2004. Log-linear models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craig Smith</author>
</authors>
<title>by the numbers: 16 amazing Twitter stats.</title>
<date>2013</date>
<note>http://expandedramblings.com/index.php/march2013-by-the-numbers-a-few-amazing-twitter-stats/.</note>
<contexts>
<context position="1829" citStr="Smith, 2013" startWordPosition="276" endWordPosition="277">hor’s tweeting frequency. 1 Introduction Inferring latent user attributes such as gender, age, and political preferences (Rao et al., 2011; Zamal et al., 2012; Cohen and Ruths, 2013) automatically from personal communications and social media including emails, blog posts or public discussions has become increasingly popular with the web getting more social and volume of data available. Resources like Twitter1 or Facebook2 become extremely valuable for studying the underlying properties of such informal communications because of its volume, dynamic nature, and diverse population (Lunden, 2012; Smith, 2013). 1http://www.demographicspro.com/ 2http://www.wolframalpha.com/facebook/ The existing batch models for predicting latent user attributes rely on thousands of tweets per author (Rao et al., 2010; Conover et al., 2011; Pennacchiotti and Popescu, 2011a; Burger et al., 2011; Zamal et al., 2012; Nguyen et al., 2013). However, most Twitter users are less prolific than those examined in these works, and thus do not produce the thousands of tweets required to obtain their levels of accuracy e.g., the median number of tweets produced by a random Twitter user per day is 10. Moreover, recent changes to </context>
<context position="33471" citStr="Smith, 2013" startWordPosition="5778" endWordPosition="5779">ining political opinions (O’Connor et al., 2010a; Maynard and Funk, 2012) or understanding sociopolitical trends and voting outcomes (Tumasjan et al., 2010; Gayo-Avello, 2012; Lampos et al., 2013) is becoming a common practice. For instance, Lampos et al. (2013) propose a bilinear user-centric model for predicting voting intentions in the UK and Australia from social media data. Other works explore political blogs to predict what content will get the most comments (Yano et al., 2013) or analyze communications from Capitol Hill12 to predict campaign contributors based on this content (Yano and Smith, 2013). Unsupervised Batch Approaches Bergsma et al. (2013) show that large-scale clustering of user names improves gender, ethnicity and location classification on Twitter. O’Connor et al. (2010b) following the work by Eisenstein (2010) propose a Bayesian generative model to discover demographic language variations in Twitter. Rao et al. (2011) suggest a hierarchical Bayesian model which takes advantage of user name morphology for predicting user gender and ethnicity. Golbeck et al. (2010) incorporate Twitter data in a spatial model of political ideology. Streaming Approaches Van Durme (2012b) prop</context>
</contexts>
<marker>Smith, 2013</marker>
<rawString>Craig Smith. 2013. May 2013 by the numbers: 16 amazing Twitter stats. http://expandedramblings.com/index.php/march2013-by-the-numbers-a-few-amazing-twitter-stats/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: determining support or opposition from congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>327--335</pages>
<contexts>
<context position="36113" citStr="Thomas et al., 2006" startWordPosition="6179" endWordPosition="6182">ent. This may be an effect of ‘sparseness’ of relevant user data, in that users talk about politics very sporadically compared to a random sample of their neighbors. Substantial signal for political preference prediction is distributed in the neighborhood. Querying for more neighbors per user is more beneficial than querying for extra content from the existing neighbors e.g., 5 tweets from 10 neighbors leads to higher accuracy than 25 tweets from 2 neighbors or 50 tweets from 1 neighbor. This may be also the effect of data heterogeneity in social media compared to e.g., political debate text (Thomas et al., 2006). These findings demonstrate that a substantial signal is distributed over the neighborhood content. Neighborhoods constructed from friend, user mention and retweet relationships are most effective. Friend, user mention and retweet neighborhoods show the best accuracy for predicting political preferences of Twitter users. We think that friend relationships are more effective than e.g., follower relationships because it is very likely that users share common interests and preferences with their friends, e.g. Facebook friends can even be used to predict a user’s credit score.13 User mentions and</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: determining support or opposition from congressional floor-debate transcripts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 327–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tumasjan</author>
<author>T O Sprenger</author>
<author>P G Sandner</author>
<author>I M Welpe</author>
</authors>
<title>Predicting elections with Twitter: What 140 characters reveal about political sentiment.</title>
<date>2010</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>178--185</pages>
<contexts>
<context position="33014" citStr="Tumasjan et al., 2010" startWordPosition="5704" endWordPosition="5707">ds, user mentions etc. Recent work by Wong et al. (2013) investigates tweeting and retweeting behavior for political learning during 2012 US Presidential election. The most similar work to ours is by Zamal et al. (2012), where the authors apply features from the tweets authored by a user’s friend to infer attributes of that user. In this paper, we study different types of user social circles in addition to a friend network. Additionally, using social media for mining political opinions (O’Connor et al., 2010a; Maynard and Funk, 2012) or understanding sociopolitical trends and voting outcomes (Tumasjan et al., 2010; Gayo-Avello, 2012; Lampos et al., 2013) is becoming a common practice. For instance, Lampos et al. (2013) propose a bilinear user-centric model for predicting voting intentions in the UK and Australia from social media data. Other works explore political blogs to predict what content will get the most comments (Yano et al., 2013) or analyze communications from Capitol Hill12 to predict campaign contributors based on this content (Yano and Smith, 2013). Unsupervised Batch Approaches Bergsma et al. (2013) show that large-scale clustering of user names improves gender, ethnicity and location cl</context>
</contexts>
<marker>Tumasjan, Sprenger, Sandner, Welpe, 2010</marker>
<rawString>A. Tumasjan, T. O. Sprenger, P. G. Sandner, and I. M. Welpe. 2010. Predicting elections with Twitter: What 140 characters reveal about political sentiment. In Proceedings of the International AAAI Conference on Weblogs and Social Media, pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
</authors>
<title>Jerboa: A toolkit for randomized and streaming algorithms.</title>
<date>2012</date>
<tech>Technical report,</tech>
<institution>Human Language Technology Center of Excellence.</institution>
<marker>Van Durme, 2012</marker>
<rawString>Benjamin Van Durme. 2012a. Jerboa: A toolkit for randomized and streaming algorithms. Technical report, Human Language Technology Center of Excellence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
</authors>
<title>Streaming analysis of discourse participants.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>48--58</pages>
<marker>Van Durme, 2012</marker>
<rawString>Benjamin Van Durme. 2012b. Streaming analysis of discourse participants. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 48–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Yang</author>
<author>Jacob Eisenstein</author>
</authors>
<title>A log-linear model for unsupervised text normalization.</title>
<date>2013</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>61--72</pages>
<contexts>
<context position="10673" citStr="Yang and Eisenstein, 2013" startWordPosition="1731" endWordPosition="1734">ls are extracted from http://www.wefollow.com as described by Pennacchiotti and Popescu (2011b). 6This inability to perfectly replicate prior work based on Twitter is a recognized problem throughout the community of computational social science, arising from the data policies of Twitter itself, it is not specific to this work. 7We use log-linear models over reasonable alternatives such as perceptron or SVM, following the practice of a wide range of previous work in related areas (Smith, 2004; Liu et al., 2005; Poon et al., 2009) including text classification in social media (Van Durme, 2012b; Yang and Eisenstein, 2013). • the order of the social circle – the number of neighbors per user of interest |Nr |= deg(vi), n = 11, 2,5, 101. Our goal is to classify users of interest using evidence (e.g., communications) from their local neighborhood P �ft[Nr(vi)] � f(Nr) as Demon cratic or Republican. The corresponding loglinear model is defined as: _ f D (1 + exp[ −B · f (Nr)])−1 &gt; 0.5, ΦNr l R otherwise. (2) To check whether our static models are cognizant of low-resource prediction settings we compare the performance of the user model from Eq.1 and the neighborhood model from Eq.2. Following the streaming nature o</context>
</contexts>
<marker>Yang, Eisenstein, 2013</marker>
<rawString>Yi Yang and Jacob Eisenstein. 2013. A log-linear model for unsupervised text normalization. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Yano</author>
<author>Noah A Smith</author>
</authors>
<title>What’s worthy of comment? content and comment volume in political blogs.</title>
<date>2013</date>
<booktitle>In International AAAI Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="33471" citStr="Yano and Smith, 2013" startWordPosition="5776" endWordPosition="5779">dia for mining political opinions (O’Connor et al., 2010a; Maynard and Funk, 2012) or understanding sociopolitical trends and voting outcomes (Tumasjan et al., 2010; Gayo-Avello, 2012; Lampos et al., 2013) is becoming a common practice. For instance, Lampos et al. (2013) propose a bilinear user-centric model for predicting voting intentions in the UK and Australia from social media data. Other works explore political blogs to predict what content will get the most comments (Yano et al., 2013) or analyze communications from Capitol Hill12 to predict campaign contributors based on this content (Yano and Smith, 2013). Unsupervised Batch Approaches Bergsma et al. (2013) show that large-scale clustering of user names improves gender, ethnicity and location classification on Twitter. O’Connor et al. (2010b) following the work by Eisenstein (2010) propose a Bayesian generative model to discover demographic language variations in Twitter. Rao et al. (2011) suggest a hierarchical Bayesian model which takes advantage of user name morphology for predicting user gender and ethnicity. Golbeck et al. (2010) incorporate Twitter data in a spatial model of political ideology. Streaming Approaches Van Durme (2012b) prop</context>
</contexts>
<marker>Yano, Smith, 2013</marker>
<rawString>Tao Yano and Noah A. Smith. 2013. What’s worthy of comment? content and comment volume in political blogs. In International AAAI Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Yano</author>
<author>Dani Yogatama</author>
<author>Noah A Smith</author>
</authors>
<title>A penny for your tweets: Campaign contributions and capitol hill microblogs.</title>
<date>2013</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="33347" citStr="Yano et al., 2013" startWordPosition="5758" endWordPosition="5761">his paper, we study different types of user social circles in addition to a friend network. Additionally, using social media for mining political opinions (O’Connor et al., 2010a; Maynard and Funk, 2012) or understanding sociopolitical trends and voting outcomes (Tumasjan et al., 2010; Gayo-Avello, 2012; Lampos et al., 2013) is becoming a common practice. For instance, Lampos et al. (2013) propose a bilinear user-centric model for predicting voting intentions in the UK and Australia from social media data. Other works explore political blogs to predict what content will get the most comments (Yano et al., 2013) or analyze communications from Capitol Hill12 to predict campaign contributors based on this content (Yano and Smith, 2013). Unsupervised Batch Approaches Bergsma et al. (2013) show that large-scale clustering of user names improves gender, ethnicity and location classification on Twitter. O’Connor et al. (2010b) following the work by Eisenstein (2010) propose a Bayesian generative model to discover demographic language variations in Twitter. Rao et al. (2011) suggest a hierarchical Bayesian model which takes advantage of user name morphology for predicting user gender and ethnicity. Golbeck </context>
</contexts>
<marker>Yano, Yogatama, Smith, 2013</marker>
<rawString>Tao Yano, Dani Yogatama, and Noah A. Smith. 2013. A penny for your tweets: Campaign contributions and capitol hill microblogs. In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Faiyaz Al Zamal</author>
<author>Wendy Liu</author>
<author>Derek Ruths</author>
</authors>
<title>Homophily and latent attribute inference: Inferring latent attributes of Twitter users from neighbors.</title>
<date>2012</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>387--390</pages>
<contexts>
<context position="1375" citStr="Zamal et al., 2012" startWordPosition="206" endWordPosition="210">d for a dynamic model to predict user preferences. We show that even when limited or no selfauthored data is available, language from friend, retweet and user mention communications provide sufficient evidence for prediction. When updating models over time based on Twitter, we find that political preference can be often be predicted using roughly 100 tweets, depending on the context of user selection, where this could mean hours, or weeks, based on the author’s tweeting frequency. 1 Introduction Inferring latent user attributes such as gender, age, and political preferences (Rao et al., 2011; Zamal et al., 2012; Cohen and Ruths, 2013) automatically from personal communications and social media including emails, blog posts or public discussions has become increasingly popular with the web getting more social and volume of data available. Resources like Twitter1 or Facebook2 become extremely valuable for studying the underlying properties of such informal communications because of its volume, dynamic nature, and diverse population (Lunden, 2012; Smith, 2013). 1http://www.demographicspro.com/ 2http://www.wolframalpha.com/facebook/ The existing batch models for predicting latent user attributes rely on </context>
<context position="8227" citStr="Zamal et al., 2012" startWordPosition="1327" endWordPosition="1330">nia and Delaware region of the US with self-reported political preference in their biographies. Similar to the candidate-centric graph, for each user we collect recent tweets and randomly sample user social circles in the Fall of 2012. We collect this data to get a sample of politically less active users compared to the users from candidate-centric graph. 2.4 ZLR Graph We also consider a GZLR graph constructed from a dataset previously used for political affiliation 4As of Oct 12, 2012, the number of followers for Obama, Biden, Romney and Ryan were 2m, 168k, 1.3m and 267k. 187 classification (Zamal et al., 2012). This dataset consists of 200 Republican and 200 Democratic users associated with 925 tweets on average per user.5 Each user has on average 6155 friends with 642 tweets per friend. Sharing restrictions and rate limits on Twitter data collection only allowed us to recreate a semblance of ZLR data6 – 193 Democratic and 178 Republican users with 1K tweets per user, and 20 neighbors of four types including follower, friends, user mention and retweet with 200 tweets per neighbor for each user of interest. 3 Batch Models Baseline User Model As input we are given a set of vertices representing users</context>
<context position="22716" citStr="Zamal et al. (2012)" startWordPosition="3897" endWordPosition="3900">is 0.863 for retweet and 0.849 for friend circles which is 0.11 higher that the user baseline. Finally, similarly to the results for the user model given in Figure 3, increasing the number of tweets per neighbor from 5 to 200 leads to a significant gain in performance for all neighborhood types. 6.3 Modeling Neighborhood Size In Figure 5 we present accuracy results to show neighborhood size influence on classification performance for Ggeo and Gcand graphs. Our results demonstrate that even small changes to the neighborhood size n lead to better performance which does not support the claims by Zamal et al. (2012). We demonstrate that increasing the size of the neighborhood leads to better performance across six neighborhood types. Friend, user mention and retweet neighborhoods yield the highest accuracy for all graphs. We observe that when the number of neighbors is n = 1, the difference in accuracy across all neighborhood types is less significant but for n ≥ 2 it becomes more significant. 7 Streaming Classification Results 7.1 Modeling Dynamic Posterior Updates from a User Stream Figures 6a and 6b demonstrate dynamic user model prediction results averaged over users from Gcand and GZLR graphs. Each </context>
<context position="32612" citStr="Zamal et al. (2012)" startWordPosition="5638" endWordPosition="5641">ne this clustering information with the follower and friend neighborhood size features. Pennacchiotti et al. (2011a; 2011b) focus on user behavior, network structure and linguistic features. Similar to our work, they assume that users from a particular class tend to reply and retweet messages of the users from the same class. We extend this assumption and study other relationship types e.g., friends, user mentions etc. Recent work by Wong et al. (2013) investigates tweeting and retweeting behavior for political learning during 2012 US Presidential election. The most similar work to ours is by Zamal et al. (2012), where the authors apply features from the tweets authored by a user’s friend to infer attributes of that user. In this paper, we study different types of user social circles in addition to a friend network. Additionally, using social media for mining political opinions (O’Connor et al., 2010a; Maynard and Funk, 2012) or understanding sociopolitical trends and voting outcomes (Tumasjan et al., 2010; Gayo-Avello, 2012; Lampos et al., 2013) is becoming a common practice. For instance, Lampos et al. (2013) propose a bilinear user-centric model for predicting voting intentions in the UK and Austr</context>
</contexts>
<marker>Zamal, Liu, Ruths, 2012</marker>
<rawString>Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012. Homophily and latent attribute inference: Inferring latent attributes of Twitter users from neighbors. In Proceedings of the International AAAI Conference on Weblogs and Social Media, pages 387–390.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>