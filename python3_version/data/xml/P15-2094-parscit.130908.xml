<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022393">
<title confidence="0.992068">
Improving Pivot Translation by Remembering the Pivot
</title>
<author confidence="0.997432">
Akiva Miura, Graham Neubig, Sakriani Sakti, Tomoki Toda, Satoshi Nakamura
</author>
<affiliation confidence="0.9980605">
Graduate School of Information Science
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.696903">
8916-5 Takayama-cho, Ikoma-shi, Nara, Japan
</address>
<email confidence="0.962779">
{miura.akiba.lr9, neubig, ssakti, tomoki, s-nakamura}@is.naist.jp
</email>
<sectionHeader confidence="0.992098" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999743">
Pivot translation allows for translation of
language pairs with little or no parallel
data by introducing a third language for
which data exists. In particular, the trian-
gulation method, which translates by com-
bining source-pivot and pivot-target trans-
lation models into a source-target model,
is known for its high translation accuracy.
However, in the conventional triangulation
method, information of pivot phrases is
forgotten and not used in the translation
process. In this paper, we propose a novel
approach to remember the pivot phrases in
the triangulation stage, and use a pivot lan-
guage model as an additional information
source at translation time. Experimen-
tal results on the Europarl corpus showed
gains of 0.4-1.2 BLEU points in all tested
combinations of languages1.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999574375">
In statistical machine translation (SMT) (Brown et
al., 1993), it is known that translation with mod-
els trained on larger parallel corpora can achieve
greater accuracy (Dyer et al., 2008). Unfor-
tunately, large bilingual corpora are not readily
available for many language pairs, particularly
those that don’t include English. One effective so-
lution to overcome the scarceness of bilingual data
is to introduce a pivot language for which parallel
data with the source and target languages exists
(de Gispert and Mari˜no, 2006).
Among various methods using pivot languages,
the triangulation method (Cohn and Lapata, 2007;
Utiyama and Isahara, 2007; Zhu et al., 2014),
which translates by combining source-pivot and
pivot-target translation models into a source-target
</bodyText>
<footnote confidence="0.976798">
1Code to replicate the experiments can be found at
https://github.com/akivajp/acl2015
</footnote>
<figure confidence="0.9939485">
(a) Triangulation (de-en-it)
Annaherung approccio
(via: approach)
Annaherung ravvicinamento
(via: approach, approximation)
� � �
(b) Traditional Triangulated Phrases
Annaherung (approccio, approach)
Annaherung (rawicinamento, approach)
Annaherung (rawicinamento, approximation)
� � �
(c) Proposed Triangulated Phrases
</figure>
<figureCaption confidence="0.999072">
Figure 1: An example of (a) triangulation and the
</figureCaption>
<bodyText confidence="0.941542347826087">
resulting phrases in the (b) traditional method of
forgetting pivots and (c) our proposed method of
remembering pivots.
model, has been shown to be one of the most effec-
tive approaches. However, word sense ambiguity
and interlingual differences of word usage cause
difficulty in accurately learning correspondences
between source and target phrases.
Figure 1 (a) shows an example of three words
in German and Italian that each correspond to the
English polysemic word “approach.” In such a
case, finding associated source-target phrase pairs
and estimating translation probabilities properly
becomes a complicated problem. Furthermore, in
the conventional triangulation method, informa-
tion about pivot phrases that behave as bridges be-
tween source and target phrases is lost after learn-
ing phrase pairs, as shown in Figure 1 (b).
To overcome these problems, we propose a
novel triangulation method that remembers the
pivot phrase connecting source and target in the
records of phrase/rule table, and estimates a joint
translation probability from the source to target
</bodyText>
<figure confidence="0.994423714285714">
approccio
accesso
rawicinamento
Annaherung approach
Ansatz approximation
entrance
Zufahrt
</figure>
<page confidence="0.906963">
573
</page>
<bodyText confidence="0.948586481481482">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573–577,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
and pivot simultaneously. We show an example
in Figure 1 (c). The advantage of this approach
is that generally we can obtain rich monolingual
resources in pivot languages such as English, and
SMT can utilize this additional information to im-
prove the translation quality.
To utilize information about the pivot language
at translation time, we train a Multi-Synchronous
Context-free Grammar (MSCFG) (Neubig et al.,
2015), a generalized extension of synchronous
CFGs (SCFGs) (Chiang, 2007), that can gener-
ate strings in multiple languages at the same time.
To create the MSCFG, we triangulate source-pivot
and pivot-target SCFG rule tables not into a single
source-target SCFG, but into a source-target-pivot
MSCFG rule table that remembers the pivot. Dur-
ing decoding, we use language models over both
the target and the pivot to assess the naturalness of
the derivation. We perform experiments on pivot
translation of Europarl proceedings, which show
that our method indeed provide significant gains in
accuracy (of up to 1.2 BLEU points), in all com-
binations of 4 languages with English as a pivot
language.
</bodyText>
<sectionHeader confidence="0.986424" genericHeader="method">
2 Translation Formalisms
</sectionHeader>
<subsectionHeader confidence="0.999239">
2.1 Synchronous Context-free Grammars
</subsectionHeader>
<bodyText confidence="0.999514">
First, we cover SCFGs, which are widely used
in machine translation, particularly hierarchical
phrase-based translation (Hiero; Chiang (2007)).
In SCFGs, the elementary structures are rewrite
rules with aligned pairs of right-hand sides:
</bodyText>
<equation confidence="0.989763">
X → (s, t) (1)
</equation>
<bodyText confidence="0.998639857142857">
where X is the head of the rewrite rule, and s and t
are both strings of terminals and non-terminals in
the source and target side respectively. Each string
in the right side tuple has the same number of in-
dexed non-terminals, and identically indexed non-
terminals correspond to each-other. For example,
a synchronous rule could take the form of:
</bodyText>
<equation confidence="0.916535">
X → ⟨X0 of the X1, X1 的 X0⟩. (2)
</equation>
<bodyText confidence="0.999913909090909">
In the SCFG training method proposed by
Chiang (2007), SCFG rules are extracted based
on parallel sentences and automatically obtained
word alignments. Each extracted rule is scored
with phrase translation probabilities in both direc-
tions 0(slt) and 0(t1s), lexical translation proba-
bilities in both directions 0j,(s|t) and 0j,,(t|s),
a word penalty counting the terminals in t, and a
constant phrase penalty of 1.
At translation time, the decoder searches for
the target sentence that maximizes the derivation
probability, which is defined as the sum of the
scores of the rules used in the derivation, and the
log of the language model probability over the tar-
get strings. When not considering an LM, it is pos-
sible to efficiently find the best translation for an
input sentence using the CKY+ algorithm (Chap-
pelier et al., 1998). When using an LM, the ex-
panded search space is further reduced based on a
limit on expanded edges, or total states per span,
through a procedure such as cube pruning (Chi-
ang, 2007).
</bodyText>
<subsectionHeader confidence="0.991335">
2.2 Multi-Synchronous CFGs
</subsectionHeader>
<bodyText confidence="0.9998186">
MSCFGs (Neubig et al., 2015) are a generalization
of SCFGs that are be able to generate sentences in
multiple target languages simultaneously. The sin-
gle target side string t in the SCFG production rule
is extended to have strings for N target languages:
</bodyText>
<equation confidence="0.952384">
X → (s, t1, ..., tN) . (3)
</equation>
<bodyText confidence="0.999931086956522">
Performing multi-target translation with
MSCFGs is quite similar to translating using
standard SCFGs, with the exception of the ex-
panded state space caused by having one LM
for each target. Neubig et al. (2015) propose a
sequential search method, that ensures diversity in
the primary target search space by first expanding
with only primary target LM, then additionally
expands the states for other LMs, a strategy we
also adopt in this work.
In the standard training method for MSCFGs,
the multi-target rewrite rules are extracted from
multilingual line-aligned corpora by applying an
extended version of the standard SCFG rule ex-
traction method, and scored with features that con-
sider the multiple targets. It should be noted that
this training method requires a large amount of
line-aligned training data including the source and
all target languages. This assumption breaks down
when we have little parallel data, and thereby we
propose a method to generate MSCFG rules by
triangulating 2 SCFG rule tables in the following
section.
</bodyText>
<sectionHeader confidence="0.996287" genericHeader="method">
3 Pivot Translation Methods
</sectionHeader>
<bodyText confidence="0.998455333333333">
Several methods have been proposed for SMT us-
ing pivot languages. These include cascade meth-
ods that consecutively translate from source to
</bodyText>
<page confidence="0.990937">
574
</page>
<bodyText confidence="0.99997975">
pivot then pivot to target (de Gispert and Mari˜no,
2006), synthetic data methods that machine-
translate the training data to generate a pseudo-
parallel corpus (de Gispert and Mari˜no, 2006),
and triangulation methods that obtain a source-
target phrase/rule table by merging source-pivot
and pivot-target table entries with identical pivot
language phrases (Cohn and Lapata, 2007). In par-
ticular, the triangulation method is notable for pro-
ducing higher quality translation results than other
pivot methods (Utiyama and Isahara, 2007), so we
use it as a base for our work.
</bodyText>
<subsectionHeader confidence="0.9962">
3.1 Traditional Triangulation Method
</subsectionHeader>
<bodyText confidence="0.999937666666667">
In the triangulation method by Cohn and Lapata
(2007), we first train source-pivot and pivot-target
rule tables, then create rules:
</bodyText>
<equation confidence="0.991611">
X → (s, t) (4)
</equation>
<bodyText confidence="0.9656991875">
if there exists a pivot phrase p such that the pair
⟨s,p⟩ is in source-pivot table TSP and the pair
(p, t) is in pivot-target table TPT. Source-target
table TST is created by calculation of the trans-
lation probabilities using phrase translation prob-
abilities φ(·) and lexical translation probabilities
φlex(·) for all connected phrases according to the
following equations (Cohn and Lapata, 2007):
pETSP nTPT
The equations (5)-(8) are based on the memo-
ryless channel model, which assumes φ (t|p, s) =
φ (t|p) and φ (s|p, t) = φ (s|p). Unfortunately,
these equations are not accurate due to polysemy
and disconnects in the grammar of the languages.
As a result, pivot translation is significantly more
ambiguous than standard translation.
</bodyText>
<subsectionHeader confidence="0.998253">
3.2 Proposed Triangulation Method
</subsectionHeader>
<bodyText confidence="0.999929571428571">
To help reduce this ambiguity, our proposed tri-
angulation method remembers the corresponding
pivot phrase as additional information to be uti-
lized for disambiguation. Specifically, instead of
marginalizing over the pivot phrase p, we create an
MSCFG rule for the tuple of the connected source-
target-pivot phrases such as:
</bodyText>
<equation confidence="0.981646">
X → (s, t, p) . (9)
</equation>
<bodyText confidence="0.99981025">
The advantage of translation with these rules is
that they allow for incorporation of additional fea-
tures over the pivot sentence such as a strong pivot
LM.
In addition to the equations (5)-(8), we also es-
timate translation probabilities φ(t, p|s), φ(s|p, t)
that consider both target and pivot phrases at the
same time according to:
</bodyText>
<equation confidence="0.9999715">
φ�t,p|s) = φ�t|p) φ(p|s), (10)
φ (s|p, t) = φ (s|p) . (11)
</equation>
<bodyText confidence="0.998321666666667">
Translation probabilities between source and pivot
phrases φ(p |s), φ(s|p), φlex(p|s), φlex(s|p) can
also be used directly from the source-pivot rule ta-
ble. This results in 13 features for each MSCFG
rule: 10 translation probabilities, 2 word penalties
counting the terminals in t and p, and a constant
phrase penalty of 1.
It should be noted that remembering the pivot
results in significantly larger rule tables. To save
computational resources, several pruning methods
are conceivable. Neubig et al. (2015) show that an
effective pruning method in the case of a main tar-
get T1 with the help of target T2 is the T1-pruning
method, namely, using L candidates of t1 with the
highest translation probability φ(t1|s) and select-
ing t2 with highest φ(t1, t2|s) for each t1. We fol-
low this approach, using the L best t, and the cor-
responding 1 best p .
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997895">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999625846153846">
We evaluate the proposed triangulation method
through pivot translation experiments on the Eu-
roparl corpus, which is a multilingual corpus in-
cluding 21 European languages (Koehn, 2005)
widely used in pivot translation work. In our
work, we perform translation among German (de),
Spanish (es), French (fr) and Italian (it), with En-
glish (en) as the pivot language. To prepare the
data for these 5 languages, we first use the Gale-
Church alignment algorithm (Gale and Church,
1993) to retrieve a multilingual line-aligned cor-
pus of about 900k sentences, then hold out 1,500
sentences each for tuning and test. In our basic
</bodyText>
<equation confidence="0.999567666666667">
E
φ (t|s) =
pETSP nTPT
E
φ (s|t) =
pETSP nTPT
E
φlex (t|s) =
pETSP nTPT
E
φlex (s|t) =
φ (t|p) φ (p|s) , (5)
φ (s|p) φ (p|t) , (6)
φlex (t|p) φlex (p|s) , (7)
φlex (s|p) φlex (p|t) . (8)
</equation>
<page confidence="0.996567">
575
</page>
<table confidence="0.999916533333333">
Source Target BLEU Score [%]
Direct Cascade Tri. SCFG Tri. MSCFG Tri. MSCFG Tri. MSCFG
(baseline) -PivotLM +PivotLM 100k +PivotLM 2M
es 27.10 25.05 25.31 25.38 25.52 † 25.75
de fr 25.65 23.86 24.12 24.16 24.25 † 24.58
it 23.04 20.76 21.27 21.42 † 21.65 $ 22.29
de 20.11 18.52 18.77 18.97 19.08 † 19.40
es fr 33.48 27.00 29.54 † 29.87 † 29.91 † 29.95
it 27.82 22.57 25.11 25.01 25.18 $ 25.64
de 19.69 18.01 18.73 18.77 18.87 † 19.19
fr es 34.36 27.26 30.31 30.53 † 30.73 $ 31.00
it 28.48 22.73 25.31 25.50 † 25.72 $ 26.22
de 19.09 14.03 17.35 † 17.99 $ 18.17 $ 18.52
it es 31.99 25.64 28.85 28.83 29.01 † 29.31
fr 31.39 25.87 28.48 28.40 28.63 † 29.02
</table>
<tableCaption confidence="0.987358">
Table 1: Results for each method. Bold indicates the highest BLEU score in pivot translation, and
daggers indicate statistically significant gains over Tri. SCFG (t : p &lt; 0.05, t : p &lt; 0.01)
</tableCaption>
<bodyText confidence="0.998987741935484">
training setup, we use 100k sentences for train-
ing both the TMs and the target LMs. We as-
sume that in many situations, a large amount of
English monolingual data is readily available and
therefore, we train pivot LMs with different data
sizes up to 2M sentences.
As a decoder, we use Travatar (Neubig, 2013),
and train SCFG TMs with its Hiero extraction
code. Translation results are evaluated by BLEU
(Papineni et al., 2002) and we tuned to maxi-
mize BLEU scores using MERT (Och, 2003). For
trained and triangulated TMs, we use T1 rule prun-
ing with a limit of 20 rules per source rule. For
decoding using MSCFG, we adopt the sequential
search method.
We evaluate 6 translation methods:
Direct: Translating with a direct SCFG trained on
the source-target parallel corpus (not using a
pivot language) for comparison.
Cascade: Cascading source-pivot and pivot-
target translation systems.
Tri. SCFG: Triangulating source-pivot and
pivot-target SCFG TMs into a source-target
SCFG TM using the traditional method.
Tri. MSCFG: Triangulating source-pivot and
pivot-target SCFG TMs into a source-
target-pivot MSCFG TM in our approach.
-PivotLM indicates translating without a
pivot LM and +PivotLM 100k/2M indicates
a pivot LM trained using 100k/2M sentences
respectively.
</bodyText>
<subsectionHeader confidence="0.943831">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.995121516129032">
The result of experiments using all combinations
of pivot translation tasks for 4 languages via En-
glish is shown in Table 1. From the results, we can
see that the proposed triangulation method consid-
ering pivot LMs outperforms the traditional trian-
gulation method for all language pairs, and trans-
lation with larger pivot LMs improves the BLEU
scores. For all languages, the pivot-remembering
triangulation method with the pivot LM trained
with 2M sentences achieves the highest score of
the pivot translation methods, with gains of 0.4-
1.2 BLEU points from the baseline method. This
shows that remembering the pivot and using it
to disambiguate results is consistently effective in
improving translation accuracy.
We can also see that the MSCFG triangulated
model without using the pivot LM slightly outper-
forms the standard SCFG triangulation method for
the majority of language pairs. It is conceivable
that the additional scores of translation probabil-
ities with pivot phrases are effective features that
allow for more accurate rule selection.
Finally, we show an example of a translated sen-
tence for which pivot-side ambiguity is resolved in
the proposed triangulation method:
Input (German): ich bedaure , daß es keine
gemeinsame ann¨aherung gegeben hat.
Reference (Italian): sono spiacente del mancato
approccio comune .
Tri. SCFG: mi rammarico per il fatto che non si
ravvicinamento comune . (BLEU+1: 13.84)
</bodyText>
<sectionHeader confidence="0.502374" genericHeader="method">
Tri. MSCFG+PivotLM 2M:
</sectionHeader>
<page confidence="0.995241">
576
</page>
<bodyText confidence="0.999239666666667">
mi dispiace che non esiste un approccio co-
mune. (BLEU+1: 25.10)
i regret that there is no common approach .
(Generated English Sentence)
The derivation uses an MSCFG rule connecting
“approccio” to “approach” in the pivot, and we
can consider that appropriate selection of English
words according to the context contributes to se-
lecting relevant vocabulary in Italian.
</bodyText>
<sectionHeader confidence="0.998865" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999266">
In this paper, we have proposed a method for pivot
translation using triangulation of SCFG rule ta-
bles into an MSCFG rule table that remembers the
pivot, and performing translation with pivot LMs.
In experiments, we found that these models are
effective in the case when a strong pivot LM ex-
ists. In the future, we plan to explore more refined
methods to devising effective intermediate expres-
sions, and improve estimation of probabilities for
triangulated rules.
</bodyText>
<sectionHeader confidence="0.994727" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999929666666667">
The authors thank anonymous reviewers for help-
ful suggestions. This work was supported in part
by the Microsoft CORE project.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999939057692308">
Peter F. Brown, Vincent J.Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19:263–312.
Jean-C´edric Chappelier, Martin Rajman, et al. 1998. A
Generalized CYK Algorithm for Parsing Stochastic
CFG. TAPD, 98(133-137):5.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Trevor Cohn and Mirella Lapata. 2007. Machine
Translation by Triangulation: Making Effective Use
of Multi-Parallel Corpora. In Proc. ACL, pages 728–
735, June.
Adri`a de Gispert and Jos´e B. Mari˜no. 2006. Catalan-
English Statistical Machine Translation without Par-
allel Corpus: Bridging through Spanish. In Proc.
of LREC 5th Workshop on Strategies for developing
machine translation for minority languages.
Christopher Dyer, Aaron Cordova, Alex Mont, and
Jimmy Lin. 2008. Fast, easy, and cheap: construc-
tion of statistical machine translation models with
MapReduce. In Proc. WMT, pages 199–207.
William A Gale and Kenneth W Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational linguistics, 19(1):75–102.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5, pages 79–86.
Graham Neubig, Philip Arthur, and Kevin Duh.
2015. Multi-Target Machine Translation with Multi-
Synchronous Context-free Grammars. In Proc.
NAACL.
Graham Neubig. 2013. Travatar: A Forest-to-String
Machine Translation Engine based on Tree Trans-
ducers. In Proc. ACL Demo Track, pages 91–96.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. ACL,
pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL,
pages 311–318.
Masao Utiyama and Hitoshi Isahara. 2007. A Com-
parison of Pivot Methods for Phrase-Based Statis-
tical Machine Translation. In Proc. NAACL, pages
484–491.
Xiaoning Zhu, Zhongjun He, Hua Wu, Conghui Zhu,
Haifeng Wang, and Tiejun Zhao. 2014. Improving
Pivot-Based Statistical Machine Translation by Piv-
oting the Co-occurrence Count of Phrase Pairs. In
Proc. EMNLP.
</reference>
<page confidence="0.997293">
577
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.599519">
<title confidence="0.996881">Improving Pivot Translation by Remembering the Pivot</title>
<author confidence="0.961638">Akiva Miura</author>
<author confidence="0.961638">Graham Neubig</author>
<author confidence="0.961638">Sakriani Sakti</author>
<author confidence="0.961638">Tomoki Toda</author>
<author confidence="0.961638">Satoshi</author>
<affiliation confidence="0.999598">Graduate School of Information Nara Institute of Science and</affiliation>
<address confidence="0.985294">8916-5 Takayama-cho, Ikoma-shi, Nara,</address>
<email confidence="0.958502">neubig,ssakti,tomoki,</email>
<abstract confidence="0.996247473684211">Pivot translation allows for translation of language pairs with little or no parallel data by introducing a third language for which data exists. In particular, the triangulation method, which translates by combining source-pivot and pivot-target translation models into a source-target model, is known for its high translation accuracy. However, in the conventional triangulation method, information of pivot phrases is forgotten and not used in the translation process. In this paper, we propose a novel to pivot phrases in the triangulation stage, and use a pivot language model as an additional information source at translation time. Experimental results on the Europarl corpus showed gains of 0.4-1.2 BLEU points in all tested</abstract>
<intro confidence="0.674638">of</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<date>1993</date>
<booktitle>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</booktitle>
<pages>19--263</pages>
<contexts>
<context position="1185" citStr="Brown et al., 1993" startWordPosition="167" endWordPosition="170">target translation models into a source-target model, is known for its high translation accuracy. However, in the conventional triangulation method, information of pivot phrases is forgotten and not used in the translation process. In this paper, we propose a novel approach to remember the pivot phrases in the triangulation stage, and use a pivot language model as an additional information source at translation time. Experimental results on the Europarl corpus showed gains of 0.4-1.2 BLEU points in all tested combinations of languages1. 1 Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19:263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-C´edric Chappelier</author>
<author>Martin Rajman</author>
</authors>
<title>A Generalized CYK Algorithm for Parsing Stochastic CFG.</title>
<date>1998</date>
<pages>98--133</pages>
<publisher>TAPD,</publisher>
<marker>Chappelier, Rajman, 1998</marker>
<rawString>Jean-C´edric Chappelier, Martin Rajman, et al. 1998. A Generalized CYK Algorithm for Parsing Stochastic CFG. TAPD, 98(133-137):5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="4271" citStr="Chiang, 2007" startWordPosition="611" endWordPosition="612">Processing (Short Papers), pages 573–577, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this approach is that generally we can obtain rich monolingual resources in pivot languages such as English, and SMT can utilize this additional information to improve the translation quality. To utilize information about the pivot language at translation time, we train a Multi-Synchronous Context-free Grammar (MSCFG) (Neubig et al., 2015), a generalized extension of synchronous CFGs (SCFGs) (Chiang, 2007), that can generate strings in multiple languages at the same time. To create the MSCFG, we triangulate source-pivot and pivot-target SCFG rule tables not into a single source-target SCFG, but into a source-target-pivot MSCFG rule table that remembers the pivot. During decoding, we use language models over both the target and the pivot to assess the naturalness of the derivation. We perform experiments on pivot translation of Europarl proceedings, which show that our method indeed provide significant gains in accuracy (of up to 1.2 BLEU points), in all combinations of 4 languages with English </context>
<context position="5640" citStr="Chiang (2007)" startWordPosition="835" endWordPosition="836">ticularly hierarchical phrase-based translation (Hiero; Chiang (2007)). In SCFGs, the elementary structures are rewrite rules with aligned pairs of right-hand sides: X → (s, t) (1) where X is the head of the rewrite rule, and s and t are both strings of terminals and non-terminals in the source and target side respectively. Each string in the right side tuple has the same number of indexed non-terminals, and identically indexed nonterminals correspond to each-other. For example, a synchronous rule could take the form of: X → ⟨X0 of the X1, X1 的 X0⟩. (2) In the SCFG training method proposed by Chiang (2007), SCFG rules are extracted based on parallel sentences and automatically obtained word alignments. Each extracted rule is scored with phrase translation probabilities in both directions 0(slt) and 0(t1s), lexical translation probabilities in both directions 0j,(s|t) and 0j,,(t|s), a word penalty counting the terminals in t, and a constant phrase penalty of 1. At translation time, the decoder searches for the target sentence that maximizes the derivation probability, which is defined as the sum of the scores of the rules used in the derivation, and the log of the language model probability over</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>728--735</pages>
<contexts>
<context position="1743" citStr="Cohn and Lapata, 2007" startWordPosition="253" endWordPosition="256">n In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1Code to replicate the experiments can be found at https://github.com/akivajp/acl2015 (a) Triangulation (de-en-it) Annaherung approccio (via: approach) Annaherung ravvicinamento (via: approach, approximation) � � � (b) Traditional Triangulated Phrases Annaherung (approccio, approach) Annaherung (rawicinamento, approach) Annaherung (rawicinamento, approximation) � � � (c) Proposed Triangulated Phrases Figure 1: An example of (a) triangulation and th</context>
<context position="8496" citStr="Cohn and Lapata, 2007" startWordPosition="1295" endWordPosition="1298">SCFG rules by triangulating 2 SCFG rule tables in the following section. 3 Pivot Translation Methods Several methods have been proposed for SMT using pivot languages. These include cascade methods that consecutively translate from source to 574 pivot then pivot to target (de Gispert and Mari˜no, 2006), synthetic data methods that machinetranslate the training data to generate a pseudoparallel corpus (de Gispert and Mari˜no, 2006), and triangulation methods that obtain a sourcetarget phrase/rule table by merging source-pivot and pivot-target table entries with identical pivot language phrases (Cohn and Lapata, 2007). In particular, the triangulation method is notable for producing higher quality translation results than other pivot methods (Utiyama and Isahara, 2007), so we use it as a base for our work. 3.1 Traditional Triangulation Method In the triangulation method by Cohn and Lapata (2007), we first train source-pivot and pivot-target rule tables, then create rules: X → (s, t) (4) if there exists a pivot phrase p such that the pair ⟨s,p⟩ is in source-pivot table TSP and the pair (p, t) is in pivot-target table TPT. Source-target table TST is created by calculation of the translation probabilities usi</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora. In Proc. ACL, pages 728– 735, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Jos´e B Mari˜no</author>
</authors>
<title>CatalanEnglish Statistical Machine Translation without Parallel Corpus: Bridging through Spanish.</title>
<date>2006</date>
<booktitle>In Proc. of LREC 5th Workshop on Strategies</booktitle>
<marker>de Gispert, Mari˜no, 2006</marker>
<rawString>Adri`a de Gispert and Jos´e B. Mari˜no. 2006. CatalanEnglish Statistical Machine Translation without Parallel Corpus: Bridging through Spanish. In Proc. of LREC 5th Workshop on Strategies for developing machine translation for minority languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Aaron Cordova</author>
<author>Alex Mont</author>
<author>Jimmy Lin</author>
</authors>
<title>Fast, easy, and cheap: construction of statistical machine translation models with MapReduce. In</title>
<date>2008</date>
<booktitle>Proc. WMT,</booktitle>
<pages>199--207</pages>
<contexts>
<context position="1311" citStr="Dyer et al., 2008" startWordPosition="188" endWordPosition="191">triangulation method, information of pivot phrases is forgotten and not used in the translation process. In this paper, we propose a novel approach to remember the pivot phrases in the triangulation stage, and use a pivot language model as an additional information source at translation time. Experimental results on the Europarl corpus showed gains of 0.4-1.2 BLEU points in all tested combinations of languages1. 1 Introduction In statistical machine translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1Code to replicate t</context>
</contexts>
<marker>Dyer, Cordova, Mont, Lin, 2008</marker>
<rawString>Christopher Dyer, Aaron Cordova, Alex Mont, and Jimmy Lin. 2008. Fast, easy, and cheap: construction of statistical machine translation models with MapReduce. In Proc. WMT, pages 199–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="11743" citStr="Gale and Church, 1993" startWordPosition="1833" endWordPosition="1836">est φ(t1, t2|s) for each t1. We follow this approach, using the L best t, and the corresponding 1 best p . 4 Experiments 4.1 Experimental Setup We evaluate the proposed triangulation method through pivot translation experiments on the Europarl corpus, which is a multilingual corpus including 21 European languages (Koehn, 2005) widely used in pivot translation work. In our work, we perform translation among German (de), Spanish (es), French (fr) and Italian (it), with English (en) as the pivot language. To prepare the data for these 5 languages, we first use the GaleChurch alignment algorithm (Gale and Church, 1993) to retrieve a multilingual line-aligned corpus of about 900k sentences, then hold out 1,500 sentences each for tuning and test. In our basic E φ (t|s) = pETSP nTPT E φ (s|t) = pETSP nTPT E φlex (t|s) = pETSP nTPT E φlex (s|t) = φ (t|p) φ (p|s) , (5) φ (s|p) φ (p|t) , (6) φlex (t|p) φlex (p|s) , (7) φlex (s|p) φlex (p|t) . (8) 575 Source Target BLEU Score [%] Direct Cascade Tri. SCFG Tri. MSCFG Tri. MSCFG Tri. MSCFG (baseline) -PivotLM +PivotLM 100k +PivotLM 2M es 27.10 25.05 25.31 25.38 25.52 † 25.75 de fr 25.65 23.86 24.12 24.16 24.25 † 24.58 it 23.04 20.76 21.27 21.42 † 21.65 $ 22.29 de 20.</context>
</contexts>
<marker>Gale, Church, 1993</marker>
<rawString>William A Gale and Kenneth W Church. 1993. A program for aligning sentences in bilingual corpora. Computational linguistics, 19(1):75–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT summit,</booktitle>
<volume>5</volume>
<pages>79--86</pages>
<contexts>
<context position="11449" citStr="Koehn, 2005" startWordPosition="1785" endWordPosition="1786">runing methods are conceivable. Neubig et al. (2015) show that an effective pruning method in the case of a main target T1 with the help of target T2 is the T1-pruning method, namely, using L candidates of t1 with the highest translation probability φ(t1|s) and selecting t2 with highest φ(t1, t2|s) for each t1. We follow this approach, using the L best t, and the corresponding 1 best p . 4 Experiments 4.1 Experimental Setup We evaluate the proposed triangulation method through pivot translation experiments on the Europarl corpus, which is a multilingual corpus including 21 European languages (Koehn, 2005) widely used in pivot translation work. In our work, we perform translation among German (de), Spanish (es), French (fr) and Italian (it), with English (en) as the pivot language. To prepare the data for these 5 languages, we first use the GaleChurch alignment algorithm (Gale and Church, 1993) to retrieve a multilingual line-aligned corpus of about 900k sentences, then hold out 1,500 sentences each for tuning and test. In our basic E φ (t|s) = pETSP nTPT E φ (s|t) = pETSP nTPT E φlex (t|s) = pETSP nTPT E φlex (s|t) = φ (t|p) φ (p|s) , (5) φ (s|p) φ (p|t) , (6) φlex (t|p) φlex (p|s) , (7) φlex </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit, volume 5, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Philip Arthur</author>
<author>Kevin Duh</author>
</authors>
<title>Multi-Target Machine Translation with MultiSynchronous Context-free Grammars. In</title>
<date>2015</date>
<booktitle>Proc. NAACL.</booktitle>
<contexts>
<context position="4203" citStr="Neubig et al., 2015" startWordPosition="600" endWordPosition="603">Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 573–577, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics and pivot simultaneously. We show an example in Figure 1 (c). The advantage of this approach is that generally we can obtain rich monolingual resources in pivot languages such as English, and SMT can utilize this additional information to improve the translation quality. To utilize information about the pivot language at translation time, we train a Multi-Synchronous Context-free Grammar (MSCFG) (Neubig et al., 2015), a generalized extension of synchronous CFGs (SCFGs) (Chiang, 2007), that can generate strings in multiple languages at the same time. To create the MSCFG, we triangulate source-pivot and pivot-target SCFG rule tables not into a single source-target SCFG, but into a source-target-pivot MSCFG rule table that remembers the pivot. During decoding, we use language models over both the target and the pivot to assess the naturalness of the derivation. We perform experiments on pivot translation of Europarl proceedings, which show that our method indeed provide significant gains in accuracy (of up t</context>
<context position="6656" citStr="Neubig et al., 2015" startWordPosition="1001" endWordPosition="1004">earches for the target sentence that maximizes the derivation probability, which is defined as the sum of the scores of the rules used in the derivation, and the log of the language model probability over the target strings. When not considering an LM, it is possible to efficiently find the best translation for an input sentence using the CKY+ algorithm (Chappelier et al., 1998). When using an LM, the expanded search space is further reduced based on a limit on expanded edges, or total states per span, through a procedure such as cube pruning (Chiang, 2007). 2.2 Multi-Synchronous CFGs MSCFGs (Neubig et al., 2015) are a generalization of SCFGs that are be able to generate sentences in multiple target languages simultaneously. The single target side string t in the SCFG production rule is extended to have strings for N target languages: X → (s, t1, ..., tN) . (3) Performing multi-target translation with MSCFGs is quite similar to translating using standard SCFGs, with the exception of the expanded state space caused by having one LM for each target. Neubig et al. (2015) propose a sequential search method, that ensures diversity in the primary target search space by first expanding with only primary targ</context>
<context position="10889" citStr="Neubig et al. (2015)" startWordPosition="1685" endWordPosition="1688">et and pivot phrases at the same time according to: φ�t,p|s) = φ�t|p) φ(p|s), (10) φ (s|p, t) = φ (s|p) . (11) Translation probabilities between source and pivot phrases φ(p |s), φ(s|p), φlex(p|s), φlex(s|p) can also be used directly from the source-pivot rule table. This results in 13 features for each MSCFG rule: 10 translation probabilities, 2 word penalties counting the terminals in t and p, and a constant phrase penalty of 1. It should be noted that remembering the pivot results in significantly larger rule tables. To save computational resources, several pruning methods are conceivable. Neubig et al. (2015) show that an effective pruning method in the case of a main target T1 with the help of target T2 is the T1-pruning method, namely, using L candidates of t1 with the highest translation probability φ(t1|s) and selecting t2 with highest φ(t1, t2|s) for each t1. We follow this approach, using the L best t, and the corresponding 1 best p . 4 Experiments 4.1 Experimental Setup We evaluate the proposed triangulation method through pivot translation experiments on the Europarl corpus, which is a multilingual corpus including 21 European languages (Koehn, 2005) widely used in pivot translation work. </context>
</contexts>
<marker>Neubig, Arthur, Duh, 2015</marker>
<rawString>Graham Neubig, Philip Arthur, and Kevin Duh. 2015. Multi-Target Machine Translation with MultiSynchronous Context-free Grammars. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
</authors>
<title>Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers.</title>
<date>2013</date>
<booktitle>In Proc. ACL Demo Track,</booktitle>
<pages>91--96</pages>
<contexts>
<context position="13225" citStr="Neubig, 2013" startWordPosition="2123" endWordPosition="2124">17.35 † 17.99 $ 18.17 $ 18.52 it es 31.99 25.64 28.85 28.83 29.01 † 29.31 fr 31.39 25.87 28.48 28.40 28.63 † 29.02 Table 1: Results for each method. Bold indicates the highest BLEU score in pivot translation, and daggers indicate statistically significant gains over Tri. SCFG (t : p &lt; 0.05, t : p &lt; 0.01) training setup, we use 100k sentences for training both the TMs and the target LMs. We assume that in many situations, a large amount of English monolingual data is readily available and therefore, we train pivot LMs with different data sizes up to 2M sentences. As a decoder, we use Travatar (Neubig, 2013), and train SCFG TMs with its Hiero extraction code. Translation results are evaluated by BLEU (Papineni et al., 2002) and we tuned to maximize BLEU scores using MERT (Och, 2003). For trained and triangulated TMs, we use T1 rule pruning with a limit of 20 rules per source rule. For decoding using MSCFG, we adopt the sequential search method. We evaluate 6 translation methods: Direct: Translating with a direct SCFG trained on the source-target parallel corpus (not using a pivot language) for comparison. Cascade: Cascading source-pivot and pivottarget translation systems. Tri. SCFG: Triangulatin</context>
</contexts>
<marker>Neubig, 2013</marker>
<rawString>Graham Neubig. 2013. Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers. In Proc. ACL Demo Track, pages 91–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation. In</title>
<date>2003</date>
<booktitle>Proc. ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="13403" citStr="Och, 2003" startWordPosition="2154" endWordPosition="2155">score in pivot translation, and daggers indicate statistically significant gains over Tri. SCFG (t : p &lt; 0.05, t : p &lt; 0.01) training setup, we use 100k sentences for training both the TMs and the target LMs. We assume that in many situations, a large amount of English monolingual data is readily available and therefore, we train pivot LMs with different data sizes up to 2M sentences. As a decoder, we use Travatar (Neubig, 2013), and train SCFG TMs with its Hiero extraction code. Translation results are evaluated by BLEU (Papineni et al., 2002) and we tuned to maximize BLEU scores using MERT (Och, 2003). For trained and triangulated TMs, we use T1 rule pruning with a limit of 20 rules per source rule. For decoding using MSCFG, we adopt the sequential search method. We evaluate 6 translation methods: Direct: Translating with a direct SCFG trained on the source-target parallel corpus (not using a pivot language) for comparison. Cascade: Cascading source-pivot and pivottarget translation systems. Tri. SCFG: Triangulating source-pivot and pivot-target SCFG TMs into a source-target SCFG TM using the traditional method. Tri. MSCFG: Triangulating source-pivot and pivot-target SCFG TMs into a source</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="13343" citStr="Papineni et al., 2002" startWordPosition="2140" endWordPosition="2143">29.02 Table 1: Results for each method. Bold indicates the highest BLEU score in pivot translation, and daggers indicate statistically significant gains over Tri. SCFG (t : p &lt; 0.05, t : p &lt; 0.01) training setup, we use 100k sentences for training both the TMs and the target LMs. We assume that in many situations, a large amount of English monolingual data is readily available and therefore, we train pivot LMs with different data sizes up to 2M sentences. As a decoder, we use Travatar (Neubig, 2013), and train SCFG TMs with its Hiero extraction code. Translation results are evaluated by BLEU (Papineni et al., 2002) and we tuned to maximize BLEU scores using MERT (Och, 2003). For trained and triangulated TMs, we use T1 rule pruning with a limit of 20 rules per source rule. For decoding using MSCFG, we adopt the sequential search method. We evaluate 6 translation methods: Direct: Translating with a direct SCFG trained on the source-target parallel corpus (not using a pivot language) for comparison. Cascade: Cascading source-pivot and pivottarget translation systems. Tri. SCFG: Triangulating source-pivot and pivot-target SCFG TMs into a source-target SCFG TM using the traditional method. Tri. MSCFG: Triang</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A Comparison of Pivot Methods for Phrase-Based Statistical Machine Translation. In</title>
<date>2007</date>
<booktitle>Proc. NAACL,</booktitle>
<pages>484--491</pages>
<contexts>
<context position="1770" citStr="Utiyama and Isahara, 2007" startWordPosition="257" endWordPosition="260">e translation (SMT) (Brown et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1Code to replicate the experiments can be found at https://github.com/akivajp/acl2015 (a) Triangulation (de-en-it) Annaherung approccio (via: approach) Annaherung ravvicinamento (via: approach, approximation) � � � (b) Traditional Triangulated Phrases Annaherung (approccio, approach) Annaherung (rawicinamento, approach) Annaherung (rawicinamento, approximation) � � � (c) Proposed Triangulated Phrases Figure 1: An example of (a) triangulation and the resulting phrases in the </context>
<context position="8650" citStr="Utiyama and Isahara, 2007" startWordPosition="1318" endWordPosition="1321">pivot languages. These include cascade methods that consecutively translate from source to 574 pivot then pivot to target (de Gispert and Mari˜no, 2006), synthetic data methods that machinetranslate the training data to generate a pseudoparallel corpus (de Gispert and Mari˜no, 2006), and triangulation methods that obtain a sourcetarget phrase/rule table by merging source-pivot and pivot-target table entries with identical pivot language phrases (Cohn and Lapata, 2007). In particular, the triangulation method is notable for producing higher quality translation results than other pivot methods (Utiyama and Isahara, 2007), so we use it as a base for our work. 3.1 Traditional Triangulation Method In the triangulation method by Cohn and Lapata (2007), we first train source-pivot and pivot-target rule tables, then create rules: X → (s, t) (4) if there exists a pivot phrase p such that the pair ⟨s,p⟩ is in source-pivot table TSP and the pair (p, t) is in pivot-target table TPT. Source-target table TST is created by calculation of the translation probabilities using phrase translation probabilities φ(·) and lexical translation probabilities φlex(·) for all connected phrases according to the following equations (Coh</context>
</contexts>
<marker>Utiyama, Isahara, 2007</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2007. A Comparison of Pivot Methods for Phrase-Based Statistical Machine Translation. In Proc. NAACL, pages 484–491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoning Zhu</author>
<author>Zhongjun He</author>
<author>Hua Wu</author>
<author>Conghui Zhu</author>
<author>Haifeng Wang</author>
<author>Tiejun Zhao</author>
</authors>
<title>Improving Pivot-Based Statistical Machine Translation by Pivoting the Co-occurrence Count of Phrase Pairs. In</title>
<date>2014</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="1789" citStr="Zhu et al., 2014" startWordPosition="261" endWordPosition="264">et al., 1993), it is known that translation with models trained on larger parallel corpora can achieve greater accuracy (Dyer et al., 2008). Unfortunately, large bilingual corpora are not readily available for many language pairs, particularly those that don’t include English. One effective solution to overcome the scarceness of bilingual data is to introduce a pivot language for which parallel data with the source and target languages exists (de Gispert and Mari˜no, 2006). Among various methods using pivot languages, the triangulation method (Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Zhu et al., 2014), which translates by combining source-pivot and pivot-target translation models into a source-target 1Code to replicate the experiments can be found at https://github.com/akivajp/acl2015 (a) Triangulation (de-en-it) Annaherung approccio (via: approach) Annaherung ravvicinamento (via: approach, approximation) � � � (b) Traditional Triangulated Phrases Annaherung (approccio, approach) Annaherung (rawicinamento, approach) Annaherung (rawicinamento, approximation) � � � (c) Proposed Triangulated Phrases Figure 1: An example of (a) triangulation and the resulting phrases in the (b) traditional met</context>
</contexts>
<marker>Zhu, He, Wu, Zhu, Wang, Zhao, 2014</marker>
<rawString>Xiaoning Zhu, Zhongjun He, Hua Wu, Conghui Zhu, Haifeng Wang, and Tiejun Zhao. 2014. Improving Pivot-Based Statistical Machine Translation by Pivoting the Co-occurrence Count of Phrase Pairs. In Proc. EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>