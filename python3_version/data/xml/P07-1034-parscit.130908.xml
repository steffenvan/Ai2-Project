<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99453">
Instance Weighting for Domain Adaptation in NLP
</title>
<author confidence="0.999113">
Jing Jiang and ChengXiang Zhai
</author>
<affiliation confidence="0.846165666666667">
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
</affiliation>
<email confidence="0.998685">
{jiang4,czhai}@cs.uiuc.edu
</email>
<sectionHeader confidence="0.995634" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920166666666">
Domain adaptation is an important problem
in natural language processing (NLP) due to
the lack of labeled data in novel domains. In
this paper, we study the domain adaptation
problem from the instance weighting per-
spective. We formally analyze and charac-
terize the domain adaptation problem from
a distributional view, and show that there
are two distinct needs for adaptation, cor-
responding to the different distributions of
instances and classification functions in the
source and the target domains. We then
propose a general instance weighting frame-
work for domain adaptation. Our empir-
ical results on three NLP tasks show that
incorporating and exploiting more informa-
tion from the target domain through instance
weighting is effective.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997688695652174">
Many natural language processing (NLP) problems
such as part-of-speech (POS) tagging, named entity
(NE) recognition, relation extraction, and seman-
tic role labeling, are currently solved by supervised
learning from manually labeled data. A bottleneck
problem with this supervised learning approach is
the lack of annotated data. As a special case, we
often face the situation where we have a sufficient
amount of labeled data in one domain, but have little
or no labeled data in another related domain which
we are interested in. We thus face the domain adap-
tation problem. Following (Blitzer et al., 2006), we
call the first the source domain, and the second the
target domain.
The domain adaptation problem is commonly en-
countered in NLP. For example, in POS tagging, the
source domain may be tagged WSJ articles, and the
target domain may be scientific literature that con-
tains scientific terminology. In NE recognition, the
source domain may be annotated news articles, and
the target domain may be personal blogs. Another
example is personalized spam filtering, where we
may have many labeled spam and ham emails from
publicly available sources, but we need to adapt the
learned spam filter to an individual user’s inbox be-
cause the user has her own, and presumably very dif-
ferent, distribution of emails and notion of spams.
Despite the importance of domain adaptation in
NLP, currently there are no standard methods for
solving this problem. An immediate possible solu-
tion is semi-supervised learning, where we simply
treat the target instances as unlabeled data but do
not distinguish the two domains. However, given
that the source data and the target data are from dif-
ferent distributions, we should expect to do better
by exploiting the domain difference. Recently there
have been some studies addressing domain adapta-
tion from different perspectives (Roark and Bacchi-
ani, 2003; Chelba and Acero, 2004; Florian et al.,
2004; Daum´e III and Marcu, 2006; Blitzer et al.,
2006). However, there have not been many studies
that focus on the difference between the instance dis-
tributions in the two domains. A detailed discussion
on related work is given in Section 5.
In this paper, we study the domain adaptation
problem from the instance weighting perspective.
</bodyText>
<page confidence="0.96739">
264
</page>
<note confidence="0.9256055">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998965">
In general, the domain adaptation problem arises
when the source instances and the target instances
are from two different, but related distributions.
We formally analyze and characterize the domain
adaptation problem from this distributional view.
Such an analysis reveals that there are two distinct
needs for adaptation, corresponding to the differ-
ent distributions of instances and the different clas-
sification functions in the source and the target do-
mains. Based on this analysis, we propose a gen-
eral instance weighting method for domain adapta-
tion, which can be regarded as a generalization of
an existing approach to semi-supervised learning.
The proposed method implements several adapta-
tion heuristics with a unified objective function: (1)
removing misleading training instances in the source
domain; (2) assigning more weights to labeled tar-
get instances than labeled source instances; (3) aug-
menting training instances with target instances with
predicted labels. We evaluated the proposed method
with three adaptation problems in NLP, including
POS tagging, NE type classification, and spam filter-
ing. The results show that regular semi-supervised
and supervised learning methods do not perform as
well as our new method, which explicitly captures
domain difference. Our results also show that in-
corporating and exploiting more information from
the target domain is much more useful for improv-
ing performance than excluding misleading training
examples from the source domain.
The rest of the paper is organized as follows. In
Section 2, we formally analyze the domain adapta-
tion problem and distinguish two types of adapta-
tion. In Section 3, we then propose a general in-
stance weighting framework for domain adaptation.
In Section 4, we present the experiment results. Fi-
nally, we compare our framework with related work
in Section 5 before we conclude in Section 6.
</bodyText>
<sectionHeader confidence="0.980689" genericHeader="method">
2 Domain Adaptation
</sectionHeader>
<bodyText confidence="0.999902608695653">
In this section, we define and analyze domain adap-
tation from a theoretical point of view. We show that
the need for domain adaptation arises from two fac-
tors, and the solutions are different for each factor.
We restrict our attention to those NLP tasks that can
be cast into multiclass classification problems, and
we only consider discriminative models for classifi-
cation. Since both are common practice in NLP, our
analysis is applicable to many NLP tasks.
Let X be a feature space we choose to represent
the observed instances, and let Y be the set of class
labels. In the standard supervised learning setting,
we are given a set of labeled instances {(xi, yi)}� i��,
where xi ∈ X, yi ∈ Y, and (xi, yi) are drawn from
an unknown joint distribution p(x, y). Our goal is to
recover this unknown distribution so that we can pre-
dict unlabeled instances drawn from the same distri-
bution. In discriminative models, we are only con-
cerned with p(y|x). Following the maximum likeli-
hood estimation framework, we start with a parame-
terized model family p(y|x; θ), and then find the best
model parameter θ* that maximizes the expected log
likelihood of the data:
</bodyText>
<equation confidence="0.997277666666667">
f, E
θ* = arg max
θ X yEY
</equation>
<bodyText confidence="0.9669745">
Since we do not know the distribution p(x, y), we
maximize the empirical log likelihood instead:
</bodyText>
<equation confidence="0.77302125">
θ* .. arg max J E ˜p(x, y) log p(y|x; θ)dx
θ X yEY
= arg max 1 N log p(yi|xi; θ).
θ N i=1
</equation>
<bodyText confidence="0.997408333333333">
Note that since we use the empirical distribution
p(x, y) to approximate p(x, y), the estimated θ* is
dependent on p(x, y). In general, as long as we have
sufficient labeled data, this approximation is fine be-
cause the unlabeled instances we want to classify are
from the same p(x, y).
</bodyText>
<subsectionHeader confidence="0.997739">
2.1 Two Factors for Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.99823975">
Let us now turn to the case of domain adaptation
where the unlabeled instances we want to classify
are from a different distribution than the labeled in-
stances. Let ps(x, y) and pt(x, y) be the true un-
derlying distributions for the source and the target
domains, respectively. Our general idea is to use
ps(x, y) to approximate pt(x, y) so that we can ex-
ploit the labeled examples in the source domain.
If we factor p(x, y) into p(x, y) = p(y|x)p(x),
we can see that pt(x, y) can deviate from ps(x, y) in
two different ways, corresponding to two different
kinds of domain adaptation:
</bodyText>
<equation confidence="0.949763">
p(x, y) log p(y|x; θ)dx.
</equation>
<page confidence="0.993309">
265
</page>
<bodyText confidence="0.999604074074074">
Case 1 (Labeling Adaptation): pt(y|x) deviates
from ps(y|x) to a certain extent. In this case, it is
clear that our estimation of ps(y|x) from the labeled
source domain instances will not be a good estima-
tion of pt(y|x), and therefore domain adaptation is
needed. We refer to this kind of adaptation as func-
tion/labeling adaptation.
Case 2 (Instance Adaptation): pt(y|x) is mostly
similar to ps(y|x), but pt(x) deviates from ps(x). In
this case, it may appear that our estimated ps(y|x)
can still be used in the target domain. However, as
we have pointed out, the estimation of ps(y|x) de-
pends on the empirical distribution �ps(x, y), which
deviates from pt(x, y) due to the deviation of ps(x)
from pt(x). In general, the estimation of ps(y|x)
would be more influenced by the instances with high
ps(x, y) (i.e., high J5s(x)). If pt(x) is very differ-
ent from ps(x), then we should expect pt(x, y) to be
very different from ps(x, y), and therefore different
from J5s(x, y). We thus cannot expect the estimated
ps(y|x) to work well on the regions where pt(x, y)
is high, but ps(x, y) is low. Therefore, in this case,
we still need domain adaptation, which we refer to
as instance adaptation.
Because the need for domain adaptation arises
from two different factors, we need different solu-
tions for each factor.
</bodyText>
<subsectionHeader confidence="0.989351">
2.2 Solutions for Labeling Adaptation
</subsectionHeader>
<bodyText confidence="0.999299">
If pt(y|x) deviates from ps(y|x) to some extent, we
have one of the following choices:
</bodyText>
<subsectionHeader confidence="0.991874">
Change of representation:
</subsectionHeader>
<bodyText confidence="0.977553688888889">
It may be the case that if we change the rep-
resentation of the instances, i.e., if we choose a
feature space X0 different from X, we can bridge
the gap between the two distributions ps(y|x) and
pt(y|x). For example, consider domain adaptive
NE recognition where the source domain contains
clean newswire data, while the target domain con-
tains broadcast news data that has been transcribed
by automatic speech recognition and lacks capital-
ization. Suppose we use a naive NE tagger that
only looks at the word itself. If we consider capi-
talization, then the instance Bush is represented dif-
ferently from the instance bush. In the source do-
main, ps(y = Person|x = Bush) is high while
ps(y = Person|x = bush) is low, but in the target
domain, pt(y = Person|x = bush) is high. If we
ignore the capitalization information, then in both
domains p(y = Person|x = bush) will be high pro-
vided that the source domain contains much fewer
instances of bush than Bush.
Adaptation through prior:
When we use a parameterized model p(y|x; θ)
to approximate p(y|x) and estimate θ based on the
source domain data, we can place some prior on the
model parameter θ so that the estimated distribution
p(y|x; B) will be closer to pt(y|x). Consider again
the NE tagging example. If we use capitalization as
a feature, in the source domain where capitalization
information is available, this feature will be given a
large weight in the learned model because it is very
useful. If we place a prior on the weight for this fea-
ture so that a large weight will be penalized, then
we can prevent the learned model from relying too
much on this domain specific feature.
Instance pruning:
If we know the instances x for which pt(y|x) is
different from ps(y|x), we can actively remove these
instances from the training data because they are
“misleading”.
For all the three solutions given above, we need
either some prior knowledge about the target do-
main, or some labeled target domain instances;
from only the unlabeled target domain instances, we
would not know where and why pt(y|x) differs from
ps(y|x).
</bodyText>
<subsectionHeader confidence="0.995609">
2.3 Solutions for Instance Adaptation
</subsectionHeader>
<bodyText confidence="0.999972444444444">
In the case where pt(y|x) is similar to ps(y|x), but
pt(x) deviates from ps(x), we may use the (unla-
beled) target domain instances to bias the estimate
of ps(x) toward a better approximation of pt(x), and
thus achieve domain adaptation. We explain the idea
below.
Our goal is to obtain a good estimate of θt that is
optimized according to the target domain distribu-
tion pt(x, y). The exact objective function is thus
</bodyText>
<equation confidence="0.96799825">
Bt = arg max J
B X yEY pt(x, y) log p(ylx; B)dx
arg max pt(x) E
B fX yEY pt(ylx) logp(ylx; B)dx.
</equation>
<page confidence="0.970065">
266
</page>
<bodyText confidence="0.955967">
Our idea of domain adaptation is to exploit the la-
beled instances in the source domain to help obtain
B� t .
Let Ds = {(xsi, ysi )}Ns
i=1 denote the set of la-
beled instances we have from the source domain.
Assume that we have a (small) set of labeled and
a (large) set of unlabeled instances from the tar-
get domain, denoted by Dt,l = {(xt,l
</bodyText>
<equation confidence="0.8887062">
j , yt,l
j )}Nt,l
j=1 and
Dt,u = {xt,u
k }Nt,u
</equation>
<bodyText confidence="0.908244166666667">
k=1 , respectively. We now show three
ways to approximate the objective function above,
corresponding to using three different sets of in-
stances to approximate the instance space X.
Using Ds:
Using ps(y|x) to approximate pt(y|x), we obtain
</bodyText>
<equation confidence="0.9991885">
Z
pt(x)
θ∗ t ≈ arg max ps(x)
θ X
pt(xs i ) i; θ).
ps(xs i ) log p(ys i |xs
</equation>
<bodyText confidence="0.946372666666667">
Here we use only the labeled instances in Ds but
we adjust the weight of each instance by pt(x) ps(x). The
major difficulty is how to accurately estimate pt(x)
</bodyText>
<equation confidence="0.95495125">
ps(x).
Using Dt,l:
Z X
θ∗ t ≈ arg max ˜pt,l(x)
θ Xy∈Y
log p(yt,l
j |xt,l
j ; θ)
</equation>
<bodyText confidence="0.998918">
Note that this is the standard supervised learning
method using only the small amount of labeled tar-
get instances. The major weakness of this approxi-
mation is that when Nt,l is very small, the estimation
is not accurate.
</bodyText>
<equation confidence="0.964302142857143">
Using Dt,u:
Z X pt(y|x) log p(y|x; θ)dx
θ∗ t ≈ arg max ˜pt,u(x)
θ X y∈Y
pt(y|xt,u
k ) log p(y|xt,u
k ; θ),
</equation>
<bodyText confidence="0.554953285714286">
The challenge here is that pt(y|xt,u
k ; B) is unknown
to us, thus we need to estimate it. One possibility
is to approximate it with a model B� learned from
Ds and Dt,l. For example, we can set pt(y|x, B) =
p(y|x; B). Alternatively, we can also set pt(y|x, B)
to 1 if y = arg maxy0 p(y&apos;|x; B) and 0 otherwise.
</bodyText>
<sectionHeader confidence="0.898059" genericHeader="method">
3 A Framework of Instance Weighting for
Domain Adaptation
</sectionHeader>
<bodyText confidence="0.99543355">
The theoretical analysis we give in Section 2 sug-
gests that one way to solve the domain adaptation
problem is through instance weighting. We propose
a framework that incorporates instance pruning in
Section 2.2 and the three approximations in Sec-
tion 2.3. Before we show the formal framework, we
first introduce some weighting parameters and ex-
plain the intuitions behind these parameters.
First, for each (xsi , ysi ) E Ds, we introduce a pa-
rameter αi to indicate how likely pt(ys i |xsi) is close
to ps(ysi |xsi ). Large αi means the two probabilities
are close, and therefore we can trust the labeled in-
stance (xsi , ysi ) for the purpose of learning a clas-
sifier for the target domain. Small αi means these
two probabilities are very different, and therefore we
should probably discard the instance (xsi , ysi ) in the
learning process.
Second, again for each (xsi, ysi ) E Ds, we intro-
duce another parameter Qi that ideally is equal to
pt(xs i )
ps(xs i ). From the approximation in Section 2.3 that
uses only Ds, it is clear that such a parameter is use-
ful.
Next, for each xt,u
i E Dt,u, and for each possible
label y E Y, we introduce a parameter -yi(y) that
indicates how likely we would like to assign y as a
tentative label to xt,u iand include (xt,u
i , y) as a train-
ing example.
Finally, we introduce three global parameters As,
At,l and At,u that are not instance-specific but are as-
sociated with Ds, Dt,l and Dt,u, respectively. These
three parameters allow us to control the contribution
of each of the three approximation methods in Sec-
tion 2.3 when we linearly combine them together.
We now formally define our instance weighting
framework. Given Ds, Dt,l and Dt,u, to learn a clas-
sifier for the target domain, we find a parameter B
that optimizes the following objective function:
</bodyText>
<equation confidence="0.928596617647059">
ps(x) X ps(y|x) log p(y|x; θ)dx
y∈Y
arg maxZX pt(x)˜ps(x) y∈Y ˜ps(y|x) log p(y|x; θ)dx
θ ps(x)
1
Ns
= arg max
θ
XNg
i=1
˜pt,l(y|x) log p(y|x; θ)dx
1
Nt,lX
j=1
= arg max
θ
Nt,l
= arg max 1 Nt,.0X X
θ Nt,u k=1 y∈Y
267
· XNs
θ� = arg max λs · 1 αiβi log p(ys i |xs i ; θ)
Cs
θ i=1
log p(yt,l
j |xt,l
j ; θ)
γk(y) log p(y|xt,u
k ; θ)
¸+ log p(θ) ,
where Cs = �Ns
i�1 αiβi, Ct,l = Nt,l, Ct,u =
ENt,u EyEY γk(y), and λs + λt,l + λt,u = 1. The
k�1
</equation>
<bodyText confidence="0.9997302">
last term, log p(θ), is the log of a Gaussian prior dis-
tribution of θ, commonly used to regularize the com-
plexity of the model.
In general, we do not know the optimal values of
these parameters for the target domain. Neverthe-
less, the intuitions behind these parameters serve as
guidelines for us to design heuristics to set these pa-
rameters. In the rest of this section, we introduce
several heuristics that we used in our experiments to
set these parameters.
</bodyText>
<subsectionHeader confidence="0.999167">
3.1 Setting α
</subsectionHeader>
<bodyText confidence="0.994276928571429">
Following the intuition that if pt(y|x) differs much
from ps(y|x), then (x, y) should be discarded from
the training set, we use the following heuristic to
set αs. First, with standard supervised learning, we
train a model 0t,l from Dt,l. We consider p(y|x; �θt,l)
to be a crude approximation of pt(y|x). Then, we
classify {xsi jNs i�1using �θt,l. The top k instances
that are incorrectly predicted by �θt,l (ranked by their
prediction confidence) are discarded. In another
word, αsi of the top k instances for which ysi =�
arg maxy p(y|xsi ; �θt,l) are set to 0, and αi of all
the
ces are set to 1.
other source instan
</bodyText>
<subsectionHeader confidence="0.999371">
3.2 Setting
</subsectionHeader>
<bodyText confidence="0.994013222222222">
Accurately setting
involves accurately estimating
and pt(x) from the empirical distributions.
For many NLP classification tasks, we do not have a
good parametric model for p(x). We thus need to re-
sort to non-parametric density estimation methods.
However, for many NLP tasks, x resides in a high
dimensional space, which makes it hard to apply
standard non-parametri
</bodyText>
<equation confidence="0.923509">
β
β
ps(x)
</equation>
<bodyText confidence="0.972436">
c density estimation meth-
268 ods. We have not explored this direction, and in our
experiments, we set
to 1 for all source instan
</bodyText>
<subsectionHeader confidence="0.973527">
3.4 Setting
</subsectionHeader>
<bodyText confidence="0.813137461538461">
and
control the balance among the three
sets of instances. Using standard supervised learn-
ing,
and
are set proportionally to
and
that is, each instance is weighted the same whether
it is in
or in
and
is set to 0. Similarly,
using standard bootstrapping,
</bodyText>
<equation confidence="0.738906333333333">
is set proportion-
ally to
that is, each target instance added to the
</equation>
<bodyText confidence="0.970948461538462">
training set is also weighted the same as a source
instance. In neither case are the target instances em-
phasize more than source instances. However, for
domain adaptation, we want to focus more on the
target domain instances. So intuitively, we want to
make
and
somehow larger relative to
As
we will show in Section 4, this is indeed beneficial.
In general, the framework provides great flexibil-
ity for implementing different adaptation strategies
through these instan
</bodyText>
<figure confidence="0.929463642857143">
λλs,λt,l
λt,u
λs
λt,l
Cs
Ct,l,
Ds
Dt,l,
λt,u
λt,u
Ct,u,
λt,l
λt,u
λs.
</figure>
<bodyText confidence="0.630584">
ce weighting parameters.
</bodyText>
<sectionHeader confidence="0.982808" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.976788">
4.1 Tasks and Data Sets
</subsectionHeader>
<figure confidence="0.838057210526316">
β
ces.
3.3 Setting
Setting
is closely related to some semi-supervised
learning methods. One option is to set
In this case,
no longer a constant
but is a function of
This way of setting
corre-
sponds to the entropy minimization semi-supervised
learning method (Grandvalet and Bengio, 2005).
Another way to set
corresponds to bootstrapping
semi-supervised learning. First, let
be a model
learned from the previous round of training. We then
select the top k instances from
</figure>
<figureCaption confidence="0.228647">
that have the
</figureCaption>
<bodyText confidence="0.898715333333333">
highest prediction confidence. For these instances,
we set
= 1 for y = arg maxy,
and
= 0 for all other y. In another word, we
select the top k confidently predicted instances, and
include these instances together with their predicted
labels in the training set. All other instances in
are not considered. In our experi
</bodyText>
<equation confidence="0.828743941176471">
γ
γ
γk(y)=p(y|xt,u
k ;θ).
γ is
θ.
γ
γ
�θ(n)
Dt,u
γk(y)
p(y�|xt,u
k
(n)),
γk(y)
Dt,u
ments, we only con-
</equation>
<bodyText confidence="0.977346">
sidered this bootstrapping way of setting γ.
We chose three different NLP tasks to evaluate our
instan
ce weighting method for domain adaptation.
The first task is POS tagging, for which we used
</bodyText>
<figure confidence="0.559507">
1
+λt,l · Ct,l
Nt��X
j=1
1
+λt,u · Ct,u
X
y��
Nt��X
k=1
</figure>
<bodyText confidence="0.999333">
6166 WSJ sentences from Sections 00 and 01 of
Penn Treebank as the source domain data, and 2730
PubMed sentences from the Oncology section of the
PennBioIE corpus as the target domain data. The
second task is entity type classification. The setup is
very similar to Daum´e III and Marcu (2006). We
assume that the entity boundaries have been cor-
rectly identified, and we want to classify the types
of the entities. We used ACE 2005 training data
for this task. For the source domain, we used the
newswire collection, which contains 11256 exam-
ples, and for the target domains, we used the we-
blog (WL) collection (5164 examples) and the con-
versational telephone speech (CTS) collection (4868
examples). The third task is personalized spam fil-
tering. We used the ECML/PKDD 2006 discov-
ery challenge data set. The source domain contains
4000 spam and ham emails from publicly available
sources, and the target domains are three individual
users’ inboxes, each containing 2500 emails.
For each task, we consider two experiment set-
tings. In the first setting, we assume there are a small
number of labeled target instances available. For
POS tagging, we used an additional 300 Oncology
sentences as labeled target instances. For NE typ-
ing, we used 500 labeled target instances and 2000
unlabeled target instances for each target domain.
For spam filtering, we used 200 labeled target in-
stances and 1800 unlabeled target instances. In the
second setting, we assume there is no labeled target
instance. We thus used all available target instances
for testing in all three tasks.
We used logistic regression as our model of
p(ylx; 0) because it is a robust learning algorithm
and widely used.
We now describe three sets of experiments, cor-
responding to three heuristic ways of setting a, At,l
and At,,,.
</bodyText>
<subsectionHeader confidence="0.984531">
4.2 Removing “Misleading” Source Domain
Instances
</subsectionHeader>
<bodyText confidence="0.99993704">
In the first set of experiments, we gradually remove
“misleading” labeled instances from the source do-
main, using the small number of labeled target in-
stances we have. We follow the heuristic we de-
scribed in Section 3.1, which sets the a for the top
k misclassified source instances to 0, and the a for
all the other source instances to 1. We also set At,l
and At,l to 0 in order to focus only on the effect of
removing “misleading” instances. We compare with
a baseline method which uses all source instances
with equal weight but no target instances. The re-
sults are shown in Table 1.
From the table, we can see that in most exper-
iments, removing these predicted “misleading” ex-
amples improved the performance over the baseline.
In some experiments (Oncology, CTS, u00, u01), the
largest improvement was achieved when all misclas-
sified source instances were removed. In the case of
weblog NE type classification, however, removing
the source instances hurt the performance. A pos-
sible reason for this is that the set of labeled target
instances we use is a biased sample from the target
domain, and therefore the model trained on these in-
stances is not always a good predictor of “mislead-
ing” source instances.
</bodyText>
<subsectionHeader confidence="0.9679135">
4.3 Adding Labeled Target Domain Instances
with Higher Weights
</subsectionHeader>
<bodyText confidence="0.998668">
The second set of experiments is to add the labeled
target domain instances into the training set. This
corresponds to setting At,l to some non-zero value,
but still keeping At,,, as 0. If we ignore the do-
main difference, then each labeled target instance
is weighted the same as a labeled source instance
(Au,l �s= Cu,l ), which is what happens in regular su-
Cs
pervised learning. However, based on our theoret-
ical analysis, we can expect the labeled target in-
stances to be more representative of the target do-
main than the source instances. We can therefore
assign higher weights for the target instances, by ad-
justing the ratio between At,l and A, In our experi-
ments, we set �t,l
</bodyText>
<equation confidence="0.68866">
�s = aCt,l
</equation>
<bodyText confidence="0.999943230769231">
Cs , where a ranges from 2 to
20. The results are shown in Table 2.
As shown from the table, adding some labeled tar-
get instances can greatly improve the performance
for all tasks. And in almost all cases, weighting the
target instances more than the source instances per-
formed better than weighting them equally.
We also tested another setting where we first
removed the “misleading” source examples as we
showed in Section 4.2, and then added the labeled
target instances. The results are shown in the last
row of Table 2. However, although both removing
“misleading” source instances and adding labeled
</bodyText>
<page confidence="0.993862">
269
</page>
<table confidence="0.999722625">
POS NE Type Spam
k Oncology k CTS k WL k u00 u01 u02
0 0.8630 0 0.7815 0 0.7045 0 0.6306 0.6950 0.7644
4000 0.8675 800 0.8245 600 0.7070 150 0.6417 0.7078 0.7950
8000 0.8709 1600 0.8640 1200 0.6975 300 0.6611 0.7228 0.8222
12000 0.8713 2400 0.8825 1800 0.6830 450 0.7106 0.7806 0.8239
16000 0.8714 3000 0.8825 2400 0.6795 600 0.7911 0.8322 0.8328
all 0.8720 all 0.8830 all 0.6600 all 0.8106 0.8517 0.8067
</table>
<tableCaption confidence="0.999208">
Table 1: Accuracy on the target domain after removing “misleading” source domain instances.
</tableCaption>
<table confidence="0.999978875">
POS NE Type Spam
method Oncology method CTS WL method u00 u01 u02
Ds only 0.8630 Ds only 0.7815 0.7045 Ds only 0.6306 0.6950 0.7644
Ds + Dt,l 0.9349 Ds + Dt,l 0.9340 0.7735 Ds + Dt,l 0.9572 0.9572 0.9461
Ds + 5Dt,l 0.9411 Ds + 2Dt,l 0.9355 0.7810 Ds + 2Dt,l 0.9606 0.9600 0.9533
Ds + 10Dt,l 0.9429 Ds + 5Dt,l 0.9360 0.7820 Ds + 5Dt,l 0.9628 09611 0.9601
Ds + 20Dt,l 0.9443 Ds + 10Dt,l 0.9355 0.7840 Ds + 10Dt,l 0.9639 0.9628 0.9633
Ds + 20Dt,l 0.9422 Ds + 10Dt,l 0.8950 0.6670 Ds + 10Dt,l 0.9717 0.9478 0.9494
</table>
<tableCaption confidence="0.999752">
Table 2: Accuracy on the unlabeled target instances after adding the labeled target instances.
</tableCaption>
<bodyText confidence="0.999910882352941">
target instances work well individually, when com-
bined, the performance in most cases is not as good
as when no source instances are removed. We hy-
pothesize that this is because after we added some
labeled target instances with large weights, we al-
ready gained a good balance between the source data
and the target data. Further removing source in-
stances would push the emphasis more on the set
of labeled target instances, which is only a biased
sample of the whole target domain.
The POS data set and the CTS data set have pre-
viously been used for testing other adaptation meth-
ods (Daum´e III and Marcu, 2006; Blitzer et al.,
2006), though the setup there is different from ours.
Our performance using instance weighting is com-
parable to their best performance (slightly worse for
POS and better for CTS).
</bodyText>
<subsectionHeader confidence="0.999867">
4.4 Bootstrapping with Higher Weights
</subsectionHeader>
<bodyText confidence="0.999983826086957">
In the third set of experiments, we assume that we
do not have any labeled target instances. We tried
two bootstrapping methods. The first is a standard
bootstrapping method, in which we gradually added
the most confidently predicted unlabeled target in-
stances with their predicted labels to the training
set. Since we believe that the target instances should
in general be given more weight because they bet-
ter represent the target domain than the source in-
stances, in the second method, we gave the added
target instances more weight in the objective func-
tion. In particular, we set At,,, = as such that the
total contribution of the added target instances is
equal to that of all the labeled source instances. We
call this second method the balanced bootstrapping
method. Table 3 shows the results.
As we can see, while bootstrapping can generally
improve the performance over the baseline where
no unlabeled data is used, the balanced bootstrap-
ping method performed slightly better than the stan-
dard bootstrapping method. This again shows that
weighting the target instances more is a right direc-
tion to go for domain adaptation.
</bodyText>
<sectionHeader confidence="0.999961" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999913125">
There have been several studies in NLP that address
domain adaptation, and most of them need labeled
data from both the source domain and the target do-
main. Here we highlight a few representative ones.
For generative syntactic parsing, Roark and Bac-
chiani (2003) have used the source domain data
to construct a Dirichlet prior for MAP estimation
of the PCFG for the target domain. Chelba and
Acero (2004) use the parameters of the maximum
entropy model learned from the source domain as
the means of a Gaussian prior when training a new
model on the target data. Florian et al. (2004) first
train a NE tagger on the source domain, and then use
the tagger’s predictions as features for training and
testing on the target domain.
The only work we are aware of that directly mod-
</bodyText>
<page confidence="0.991606">
270
</page>
<table confidence="0.9994564">
POS NE Type Spam
method Oncology CTS WL u00 u01 u02
supervised 0.8630 0.7781 0.7351 0.6476 0.6976 0.8068
standard bootstrap 0.8728 0.8917 0.7498 0.8720 0.9212 0.9760
balanced bootstrap 0.8750 0.8923 0.7523 0.8816 0.9256 0.9772
</table>
<tableCaption confidence="0.998989">
Table 3: Accuracy on the target domain without using labeled target instances. In balanced bootstrapping,
</tableCaption>
<bodyText confidence="0.98011864">
more weights are put on the target instances in the objective function than in standard bootstrapping.
els the different distributions in the source and the
target domains is by Daum´e III and Marcu (2006).
They assume a “truly source domain” distribution,
a “truly target domain” distribution, and a “general
domain” distribution. The source (target) domain
data is generated from a mixture of the “truly source
(target) domain” distribution and the “general do-
main” distribution. In contrast, we do not assume
such a mixture model.
None of the above methods would work if there
were no labeled target instances. Indeed, all the
above methods do not make use of the unlabeled
instances in the target domain. In contrast, our in-
stance weighting framework allows unlabeled target
instances to contribute to the model estimation.
Blitzer et al. (2006) propose a domain adaptation
method that uses the unlabeled target instances to
infer a good feature representation, which can be re-
garded as weighting the features. In contrast, we
weight the instances. The idea of using �����
����� to
weight instances has been studied in statistics (Shi-
modaira, 2000), but has not been applied to NLP
tasks.
</bodyText>
<sectionHeader confidence="0.999165" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999968454545454">
Domain adaptation is a very important problem with
applications to many NLP tasks. In this paper,
we formally analyze the domain adaptation problem
and propose a general instance weighting framework
for domain adaptation. The framework is flexible to
support many different strategies for adaptation. In
particular, it can support adaptation with some target
domain labeled instances as well as that without any
labeled target instances. Experiment results on three
NLP tasks show that while regular semi-supervised
learning methods and supervised learning methods
can be applied to domain adaptation without con-
sidering domain difference, they do not perform as
well as our new method, which explicitly captures
domain difference. Our results also show that incor-
porating and exploiting more information from the
target domain is much more useful than excluding
misleading training examples from the source do-
main. The framework opens up many interesting
future research directions, especially those related to
how to more accurately set/estimate those weighting
parameters.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998084">
This work was in part supported by the National Sci-
ence Foundation under award numbers 0425852 and
0428472. We thank the anonymous reviewers for
their valuable comments.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998167454545455">
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of EMNLP, pages 120–128.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Proc. of EMNLP, pages 285–292.
Hal Daum´e III and Daniel Marcu. 2006. Domain adapta-
tion for statistical classifiers. J. Artificial Intelligence
Res., 26:101–126.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proc. of HLT-NAACL, pages 1–8.
Y. Grandvalet and Y. Bengio. 2005. Semi-supervised
learning by entropy minimization. In NIPS.
Brian Roark and Michiel Bacchiani. 2003. Supervised
and unsupervised PCFG adaptatin to novel domains.
In Proc. of HLT-NAACL, pages 126–133.
Hidetoshi Shimodaira. 2000. Improving predictive in-
ference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning
and Inference, 90:227–244.
</reference>
<page confidence="0.997736">
271
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.798027">
<title confidence="0.999081">Instance Weighting for Domain Adaptation in NLP</title>
<author confidence="0.999947">Jiang Zhai</author>
<affiliation confidence="0.9995085">Department of Computer Science University of Illinois at Urbana-Champaign</affiliation>
<address confidence="0.811719">Urbana, IL 61801, USA</address>
<abstract confidence="0.999116421052632">Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains. In this paper, we study the domain adaptation problem from the instance weighting perspective. We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains. We then propose a general instance weighting framework for domain adaptation. Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>120--128</pages>
<contexts>
<context position="1574" citStr="Blitzer et al., 2006" startWordPosition="236" endWordPosition="239">1 Introduction Many natural language processing (NLP) problems such as part-of-speech (POS) tagging, named entity (NE) recognition, relation extraction, and semantic role labeling, are currently solved by supervised learning from manually labeled data. A bottleneck problem with this supervised learning approach is the lack of annotated data. As a special case, we often face the situation where we have a sufficient amount of labeled data in one domain, but have little or no labeled data in another related domain which we are interested in. We thus face the domain adaptation problem. Following (Blitzer et al., 2006), we call the first the source domain, and the second the target domain. The domain adaptation problem is commonly encountered in NLP. For example, in POS tagging, the source domain may be tagged WSJ articles, and the target domain may be scientific literature that contains scientific terminology. In NE recognition, the source domain may be annotated news articles, and the target domain may be personal blogs. Another example is personalized spam filtering, where we may have many labeled spam and ham emails from publicly available sources, but we need to adapt the learned spam filter to an indi</context>
<context position="2953" citStr="Blitzer et al., 2006" startWordPosition="461" endWordPosition="464">adaptation in NLP, currently there are no standard methods for solving this problem. An immediate possible solution is semi-supervised learning, where we simply treat the target instances as unlabeled data but do not distinguish the two domains. However, given that the source data and the target data are from different distributions, we should expect to do better by exploiting the domain difference. Recently there have been some studies addressing domain adaptation from different perspectives (Roark and Bacchiani, 2003; Chelba and Acero, 2004; Florian et al., 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2006). However, there have not been many studies that focus on the difference between the instance distributions in the two domains. A detailed discussion on related work is given in Section 5. In this paper, we study the domain adaptation problem from the instance weighting perspective. 264 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics In general, the domain adaptation problem arises when the source instances and the target instances are from two different, b</context>
<context position="25467" citStr="Blitzer et al., 2006" startWordPosition="4422" endWordPosition="4425">nces work well individually, when combined, the performance in most cases is not as good as when no source instances are removed. We hypothesize that this is because after we added some labeled target instances with large weights, we already gained a good balance between the source data and the target data. Further removing source instances would push the emphasis more on the set of labeled target instances, which is only a biased sample of the whole target domain. The POS data set and the CTS data set have previously been used for testing other adaptation methods (Daum´e III and Marcu, 2006; Blitzer et al., 2006), though the setup there is different from ours. Our performance using instance weighting is comparable to their best performance (slightly worse for POS and better for CTS). 4.4 Bootstrapping with Higher Weights In the third set of experiments, we assume that we do not have any labeled target instances. We tried two bootstrapping methods. The first is a standard bootstrapping method, in which we gradually added the most confidently predicted unlabeled target instances with their predicted labels to the training set. Since we believe that the target instances should in general be given more we</context>
<context position="28790" citStr="Blitzer et al. (2006)" startWordPosition="4969" endWordPosition="4972">source domain” distribution, a “truly target domain” distribution, and a “general domain” distribution. The source (target) domain data is generated from a mixture of the “truly source (target) domain” distribution and the “general domain” distribution. In contrast, we do not assume such a mixture model. None of the above methods would work if there were no labeled target instances. Indeed, all the above methods do not make use of the unlabeled instances in the target domain. In contrast, our instance weighting framework allows unlabeled target instances to contribute to the model estimation. Blitzer et al. (2006) propose a domain adaptation method that uses the unlabeled target instances to infer a good feature representation, which can be regarded as weighting the features. In contrast, we weight the instances. The idea of using ����� ����� to weight instances has been studied in statistics (Shimodaira, 2000), but has not been applied to NLP tasks. 6 Conclusions and Future Work Domain adaptation is a very important problem with applications to many NLP tasks. In this paper, we formally analyze the domain adaptation problem and propose a general instance weighting framework for domain adaptation. The </context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proc. of EMNLP, pages 120–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of maximum entropy capitalizer: Little data can help a lot.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>285--292</pages>
<contexts>
<context position="2880" citStr="Chelba and Acero, 2004" startWordPosition="448" endWordPosition="451">tribution of emails and notion of spams. Despite the importance of domain adaptation in NLP, currently there are no standard methods for solving this problem. An immediate possible solution is semi-supervised learning, where we simply treat the target instances as unlabeled data but do not distinguish the two domains. However, given that the source data and the target data are from different distributions, we should expect to do better by exploiting the domain difference. Recently there have been some studies addressing domain adaptation from different perspectives (Roark and Bacchiani, 2003; Chelba and Acero, 2004; Florian et al., 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2006). However, there have not been many studies that focus on the difference between the instance distributions in the two domains. A detailed discussion on related work is given in Section 5. In this paper, we study the domain adaptation problem from the instance weighting perspective. 264 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics In general, the domain adaptation problem arises whe</context>
<context position="27233" citStr="Chelba and Acero (2004)" startWordPosition="4715" endWordPosition="4718">balanced bootstrapping method performed slightly better than the standard bootstrapping method. This again shows that weighting the target instances more is a right direction to go for domain adaptation. 5 Related Work There have been several studies in NLP that address domain adaptation, and most of them need labeled data from both the source domain and the target domain. Here we highlight a few representative ones. For generative syntactic parsing, Roark and Bacchiani (2003) have used the source domain data to construct a Dirichlet prior for MAP estimation of the PCFG for the target domain. Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data. Florian et al. (2004) first train a NE tagger on the source domain, and then use the tagger’s predictions as features for training and testing on the target domain. The only work we are aware of that directly mod270 POS NE Type Spam method Oncology CTS WL u00 u01 u02 supervised 0.8630 0.7781 0.7351 0.6476 0.6976 0.8068 standard bootstrap 0.8728 0.8917 0.7498 0.8720 0.9212 0.9760 balanced bootstrap 0.8750 0.8923 0.7523 0.8816 0.9256 0.977</context>
</contexts>
<marker>Chelba, Acero, 2004</marker>
<rawString>Ciprian Chelba and Alex Acero. 2004. Adaptation of maximum entropy capitalizer: Little data can help a lot. In Proc. of EMNLP, pages 285–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>J. Artificial Intelligence Res.,</journal>
<pages>26--101</pages>
<marker>Daum´e, Marcu, 2006</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. J. Artificial Intelligence Res., 26:101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Hassan</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>X Luo</author>
<author>N Nicolov</author>
<author>S Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2902" citStr="Florian et al., 2004" startWordPosition="452" endWordPosition="455">notion of spams. Despite the importance of domain adaptation in NLP, currently there are no standard methods for solving this problem. An immediate possible solution is semi-supervised learning, where we simply treat the target instances as unlabeled data but do not distinguish the two domains. However, given that the source data and the target data are from different distributions, we should expect to do better by exploiting the domain difference. Recently there have been some studies addressing domain adaptation from different perspectives (Roark and Bacchiani, 2003; Chelba and Acero, 2004; Florian et al., 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2006). However, there have not been many studies that focus on the difference between the instance distributions in the two domains. A detailed discussion on related work is given in Section 5. In this paper, we study the domain adaptation problem from the instance weighting perspective. 264 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics In general, the domain adaptation problem arises when the source instances</context>
<context position="27413" citStr="Florian et al. (2004)" startWordPosition="4748" endWordPosition="4751">or domain adaptation. 5 Related Work There have been several studies in NLP that address domain adaptation, and most of them need labeled data from both the source domain and the target domain. Here we highlight a few representative ones. For generative syntactic parsing, Roark and Bacchiani (2003) have used the source domain data to construct a Dirichlet prior for MAP estimation of the PCFG for the target domain. Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data. Florian et al. (2004) first train a NE tagger on the source domain, and then use the tagger’s predictions as features for training and testing on the target domain. The only work we are aware of that directly mod270 POS NE Type Spam method Oncology CTS WL u00 u01 u02 supervised 0.8630 0.7781 0.7351 0.6476 0.6976 0.8068 standard bootstrap 0.8728 0.8917 0.7498 0.8720 0.9212 0.9760 balanced bootstrap 0.8750 0.8923 0.7523 0.8816 0.9256 0.9772 Table 3: Accuracy on the target domain without using labeled target instances. In balanced bootstrapping, more weights are put on the target instances in the objective function t</context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A statistical model for multilingual entity detection and tracking. In Proc. of HLT-NAACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Grandvalet</author>
<author>Y Bengio</author>
</authors>
<title>Semi-supervised learning by entropy minimization.</title>
<date>2005</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="18440" citStr="Grandvalet and Bengio, 2005" startWordPosition="3197" endWordPosition="3200">d somehow larger relative to As we will show in Section 4, this is indeed beneficial. In general, the framework provides great flexibility for implementing different adaptation strategies through these instan λλs,λt,l λt,u λs λt,l Cs Ct,l, Ds Dt,l, λt,u λt,u Ct,u, λt,l λt,u λs. ce weighting parameters. 4 Experiments 4.1 Tasks and Data Sets β ces. 3.3 Setting Setting is closely related to some semi-supervised learning methods. One option is to set In this case, no longer a constant but is a function of This way of setting corresponds to the entropy minimization semi-supervised learning method (Grandvalet and Bengio, 2005). Another way to set corresponds to bootstrapping semi-supervised learning. First, let be a model learned from the previous round of training. We then select the top k instances from that have the highest prediction confidence. For these instances, we set = 1 for y = arg maxy, and = 0 for all other y. In another word, we select the top k confidently predicted instances, and include these instances together with their predicted labels in the training set. All other instances in are not considered. In our experi γ γ γk(y)=p(y|xt,u k ;θ). γ is θ. γ γ �θ(n) Dt,u γk(y) p(y�|xt,u k (n)), γk(y) Dt,u </context>
</contexts>
<marker>Grandvalet, Bengio, 2005</marker>
<rawString>Y. Grandvalet and Y. Bengio. 2005. Semi-supervised learning by entropy minimization. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Michiel Bacchiani</author>
</authors>
<title>Supervised and unsupervised PCFG adaptatin to novel domains.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>126--133</pages>
<contexts>
<context position="2856" citStr="Roark and Bacchiani, 2003" startWordPosition="443" endWordPosition="447">sumably very different, distribution of emails and notion of spams. Despite the importance of domain adaptation in NLP, currently there are no standard methods for solving this problem. An immediate possible solution is semi-supervised learning, where we simply treat the target instances as unlabeled data but do not distinguish the two domains. However, given that the source data and the target data are from different distributions, we should expect to do better by exploiting the domain difference. Recently there have been some studies addressing domain adaptation from different perspectives (Roark and Bacchiani, 2003; Chelba and Acero, 2004; Florian et al., 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2006). However, there have not been many studies that focus on the difference between the instance distributions in the two domains. A detailed discussion on related work is given in Section 5. In this paper, we study the domain adaptation problem from the instance weighting perspective. 264 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics In general, the domain adapt</context>
<context position="27091" citStr="Roark and Bacchiani (2003)" startWordPosition="4689" endWordPosition="4693">ows the results. As we can see, while bootstrapping can generally improve the performance over the baseline where no unlabeled data is used, the balanced bootstrapping method performed slightly better than the standard bootstrapping method. This again shows that weighting the target instances more is a right direction to go for domain adaptation. 5 Related Work There have been several studies in NLP that address domain adaptation, and most of them need labeled data from both the source domain and the target domain. Here we highlight a few representative ones. For generative syntactic parsing, Roark and Bacchiani (2003) have used the source domain data to construct a Dirichlet prior for MAP estimation of the PCFG for the target domain. Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data. Florian et al. (2004) first train a NE tagger on the source domain, and then use the tagger’s predictions as features for training and testing on the target domain. The only work we are aware of that directly mod270 POS NE Type Spam method Oncology CTS WL u00 u01 u02 supervised 0.8630 0.7781 0.7351</context>
</contexts>
<marker>Roark, Bacchiani, 2003</marker>
<rawString>Brian Roark and Michiel Bacchiani. 2003. Supervised and unsupervised PCFG adaptatin to novel domains. In Proc. of HLT-NAACL, pages 126–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetoshi Shimodaira</author>
</authors>
<title>Improving predictive inference under covariate shift by weighting the loglikelihood function.</title>
<date>2000</date>
<journal>Journal of Statistical Planning and Inference,</journal>
<pages>90--227</pages>
<contexts>
<context position="29093" citStr="Shimodaira, 2000" startWordPosition="5019" endWordPosition="5021">one of the above methods would work if there were no labeled target instances. Indeed, all the above methods do not make use of the unlabeled instances in the target domain. In contrast, our instance weighting framework allows unlabeled target instances to contribute to the model estimation. Blitzer et al. (2006) propose a domain adaptation method that uses the unlabeled target instances to infer a good feature representation, which can be regarded as weighting the features. In contrast, we weight the instances. The idea of using ����� ����� to weight instances has been studied in statistics (Shimodaira, 2000), but has not been applied to NLP tasks. 6 Conclusions and Future Work Domain adaptation is a very important problem with applications to many NLP tasks. In this paper, we formally analyze the domain adaptation problem and propose a general instance weighting framework for domain adaptation. The framework is flexible to support many different strategies for adaptation. In particular, it can support adaptation with some target domain labeled instances as well as that without any labeled target instances. Experiment results on three NLP tasks show that while regular semi-supervised learning meth</context>
</contexts>
<marker>Shimodaira, 2000</marker>
<rawString>Hidetoshi Shimodaira. 2000. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inference, 90:227–244.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>