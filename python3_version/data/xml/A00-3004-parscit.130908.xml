<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000050">
<title confidence="0.98182">
A Weighted Robust Parsing Approach to Semantic Annotation
</title>
<author confidence="0.61027">
Hatem Ghorbel and Vincenzo Pallotta
</author>
<affiliation confidence="0.411637">
LITH-MEDIA group
Swiss Federal Institute of Technology
</affiliation>
<address confidence="0.602612">
IN Ecublens, 1015 Lausanne, Switzerland
</address>
<email confidence="0.835352">
Ighorbel,pallottal @di. epfl. ch
</email>
<sectionHeader confidence="0.992032" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981333333333">
This paper proposes a grammar-based approach to
semantic annotation which combines the notions of
robust parsing and fuzzy grammars. We present an
overview of a preliminary research aimed to gener-
alize some results from a recent project on interac-
tion through speech with information systems where
techniques based on the above notions have been
successfully applied. The goal of the article is to
give a development environment to linguists.
</bodyText>
<sectionHeader confidence="0.997908" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925222222222">
In this article we are mainly interested in seman-
tic annotation (Sperberg-McQueen and Burnard,
1994). We are considering the Information Extrac-
tion (I.E.) problem as a semantic annotation prob-
lem: extracting information is finding the relevant
terms that contribute to describe an appropriate se-
mantic structure of the text. Some of the most im-
portant works in I.E. have been dealing with domain
dependent documents like (Moll et al., 1998; Hobbs
et al., 1996). Both systems employ complex analysis
schemas. Assigning semantic field tags is in general
a difficult task. This is due at least to the crucial
need of the domain knowledge and also of the lin-
guistic knowledge. Our approach considers that for
some specific domains a semantic annotation can be
achieved by a light parsing of the text which is based
on the user of certain cue-words as a heuristic for de-
scribing its semantic structure.
</bodyText>
<subsectionHeader confidence="0.969878">
1.1 A case study in query generation
</subsectionHeader>
<bodyText confidence="0.9944351">
The availability of a large collection of annotated
telephone calls for querying the Swiss phone-book
database (i.e the Swiss French PolyPhone corpus) al-
lowed us to experiment our recent findings in robust
text analysis obtained in the context of the Swiss
National Fund research project ROTA (Robust Text
Analysis), and in the recent Swisscom funded project
ISIS (Interaction through Speech with Information
Systems)1 (Chappelier et al., 1999). This database
contains 4293 simulated recordings related to the
</bodyText>
<footnote confidence="0.843074">
1The final report of ISIS project is available at
http://lithwww.epfl.chrpallotta/isis.html
&amp;quot;111&amp;quot; Swisscom service calls. For instance a query
call like:
Bonjour j&apos;aimerais un nurser° de
telephone a Saignelegier c&apos;est
Mottaz m o deux ta z Monique rue du
printemps numero quatre
</footnote>
<bodyText confidence="0.946465375">
would produce the following query frame filling for
the Swiss Phone-book database:
Nom de famille / Firme: MOTTAZ
Prenom / Autres informations: MONIQUE
Rue, numero: rue du PRINTEMPS, 4
NPA, localito: SAIGNELEGIER.
The goal of semantic annotation is to provide a tree
structure which can be superposed over the flat sen-
tence. This structure can be supported by a PRO-
LOG compound term consisting of &amp;quot;tag&amp;quot; functors
and list arguments. Moreover this same structure
can also be supported by the SGML structural rep-
resentation. The translation between the two models
is an intuitive task and an example of such transla-
tion is provided by the two following corresponding
representations for a possible query call schema.
</bodyText>
<figure confidence="0.68989392">
PROLOG:
s([...,announce([...3),
query([...,
nmne(C...,
first_name([...]),
address([...,
street([.. .3), A
city([...3),
...7),
...3)
—3) .
SGML:
&lt;announce&gt;
&lt;/announce&gt;
&lt;query&gt;
&lt;name&gt;
19
&lt;fam...name&gt; &lt;/fam_name&gt;
&lt;first_name&gt; &lt;/f irst_name&gt;
&lt;/name&gt;
&lt;address&gt;
&lt;street&gt; ... &lt;/street&gt;
&lt;city&gt; .. &lt;/city&gt;
&lt;/address&gt;
&lt;/query&gt;
</figure>
<subsubsectionHeader confidence="0.3674">
1.1.1 Processing phases
</subsubsectionHeader>
<bodyText confidence="0.998207837209302">
The processing of the corpus data is performed at
various linguistic levels by modules organized into
a pipeline. Each module assumes as input the out-
put of the preceding module. The main goal of this
architecture is to understand how far it is possible
to go without using any kind of feedback and in-
teractions among different linguistic modules. At
a first stage, morphologic and syntactic processing&apos;
is applied to the output from the speech recognizer
module which usually produces a huge word-graph
hypothesis. Thus the forest of syntactic trees pro-
duced by this phase have been used to achieve two
goals:
1: The n-best analyses are used to disambiguate
speech recognizer hypotheses
2. They served as supplementary input for the ro-
bust semantic analysis that we performed, that
had as goal the production of query frames for
the information system.
Although robustness can be considered as being ap-
plied at either a syntactic or semantic level, we be-
lieve it is generally at the semantic level that it is
most effective. This robust analysis needs a model
of the domain in which the system operates, and
a way of linking this model to the lexicon used by
the other components. The degree of detail required
of the domain model used by the robust analyzer
2Our partner institution ISSCO (Institute Dalle Molle,
University of Geneva) performed this analysis phase using
tools that were developed in the European Linguistics En-
gineering project MULTEXT. For syntactic analysis, ISSCO
developed a Feature Unification Grammar based on a small
sample of the Polyphone data. This grammar was taken by
another of our partners (the Laboratory for Artificial Intel-
ligence of the Swiss Federal Institute of Technology, Lau-
sanne) and converted into a probabilistic context-free gram-
mar, which was then applied to a sample of 500 entries from
the Polyphone data.
depends upon the ultimate task that must be per-
formed — in our case, furnishing a query to an infor-
mation system. The results of the processing phase
of the previous example is represented below as an
SGML annotation:
</bodyText>
<figure confidence="0.997868">
&lt;announce&gt;
Bonjour j&apos;aimerais un
&lt;/announce&gt;
&lt;query&gt; numero de telephone a Saignelegier
&lt;name&gt;
&lt;fmn_name&gt; c&apos;est Mottaz m o deux ta z
&lt;/fam_name&gt;
&lt;first_name&gt; Monique &lt;/first_name&gt;
&lt;/name&gt;
&lt;address&gt;
&lt;street&gt; rue du printemps &lt;/street&gt;
&lt;number&gt; numero quatre &lt;/number&gt;
&lt;city&gt; &lt;/city&gt;
&lt;/address&gt;
&lt;/query&gt;
</figure>
<sectionHeader confidence="0.934175" genericHeader="method">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999947846153846">
In this section we propose the use of a &amp;quot;light-parser&amp;quot;
for doing sentence-level semantic annotation. The
main idea comes from the observation that annota-
tion does not always need to rely on the deep struc-
ture of the sentence (e.g. at morpho-syntactic level).
It is sometimes sufficient to find some cue-words
which allow us to locate the logical sub-structures
of the sentence. If the domain is simple enough, this
task can be easily mechanized. A similar approach,
using finite state parsing technology, has been pro-
posed by Grefenstette in (Grefenstette, 1996) where
the main applications are slanted to the extraction
of syntactic information.
</bodyText>
<subsectionHeader confidence="0.968744">
2.1 Robust Definite Clause Grammars
</subsectionHeader>
<bodyText confidence="0.999850285714286">
LHIP (Left-corner Head-driven Island Parser) (Bal-
urn and Russell, 1994; Lieske and Ballim, 1998) is a
system which performs robust analysis of its input,
using a grammar defined in an extended form of the
PROLOG Definite Clause Grammars (DCGs). The
chief modifications to the standard PROLOG &apos;gram-
mar rule&apos; format are of two types: one or more right-
hand side (RHS) items may be marked as &apos;heads&apos;
(e.g. using a leading &apos;*&apos;), and one or more RHS items
may be marked as &apos;ignorable&apos; (e.g. using a leading
&apos;-&apos;). LHIP employs a different control strategy from
that used by PROLOG DCGs, in order to allow it to
cope with ungrammatical or unforeseen input. The
behavior of LHIP can best be understood in terms
of the complementary notions of span and cover.
A grammar rule is said to produce an island which
spans input terminals ti to ti+,, if the island starts
at the ith terminal, and the i + nth terminal is the
terminal immediately to the right of the last termi-
nal of the island. A rule is said to cover in items
if m terminals are consumed in the span of the rule.
</bodyText>
<page confidence="0.973723">
20
</page>
<bodyText confidence="0.998976">
Thus m &lt; n. If m = n then the rule has completely
covered the span. As implied here, rules need not
cover all of the input in order to succeed.
</bodyText>
<subsectionHeader confidence="0.569793">
2.1.1 Weighted LHIP rules
</subsectionHeader>
<bodyText confidence="0.914221551724138">
The main goal of introducing weights into LHIP
rules is to induce a partial order over the generated
hypotheses. The following schema illustrate how to
build a simple weighted rule in a compositional fash-
ion where the resulting weight is computed from
the sub-constituents using the minimum operator.
Weights are real numbers in the interval [0, 1].
cat(cat(Hyp),Weight) --&gt;
sub_cat1(H1,W1),
sub_catn(Hn,Wn),
{app_list([H1,...,Hn],Hyp),
min_list(EW1,...,Wn],Weight)}.
This strategy is not the only possible since the LHIP
formalism allows a greater flexibility. Without en-
tering into formal details we can observe that if we
strictly follow the above schema and we impose a
cover threshold of 1 we are dealing with fuzzy DCG
grammars (Lee and Zadeh, 1969; Asveld, 1996). We
actually extend this class of grammars with a no-
tion of fuzzy-robustness where weights are used to
compute confidence factors for the membership of is-
lands to categories&apos;. The order of constituents may
play an important role in assigning weights for dif-
ferent rules having the same number and type of
constituents. Each LHIP rule returns a weight to-
gether with a term which will contribute to build
the resulting structure. The confidence factor for a
pre-terminal rule has been assigned statically on the
basis of the rule designer&apos;s domain knowledge.
</bodyText>
<subsectionHeader confidence="0.995032">
2.2 The methodology at work
</subsectionHeader>
<bodyText confidence="0.94865525">
In our case study we try to integrate the above prin-
ciples in order to effectively compute annotation hy-
potheses for the query generation task. This can
be done by building a lattice of annotation hypothe-
ses and possibly selecting the best one. This lattice
is generated by means of a LHIP weighted gram-
mar which is used to extract and assemble what
we called semantic constituents. At the end of this
process we presumably obtain suitable annotations
from which we will able to extract the content of
the query (e.g. name, address, city, etc.). The rules
are designed taking into consideration the following
kind of knowledge:
Domain Knowledge is exploited to provide quan-
titative support (or confidence factor) to our
rules.
3Development of this notion is currently under investiga-
tion and not yet formalized.
Linguistic Knowledge (as for instance previous
POS tagging or syntactic analysis) is used for
determining constraints in order to prune the
hypotheses space.
Lexical knowledge: As pointed out in (Basili
and M.T., 1997), lexical knowledge plays an im-
portant role in Information Extraction since it can
contribute in guiding the analysis process at various
linguistic level. In our case we are concerned with
lexical knowledge when we need to specify lexical
LHIP rules which represent the building blocks of
our parsing system. Semantic markers are domain-
dependent word patterns and must be defined for
a given corpus. They identify cue-words serving
both as separators among logical subparts of the
same sentence and as introducers of semantic con-
stituents. In our specific case they allow us to search
for the content of the query only in interesting parts
of the sentence. One of the most important sep-
arators is the announcement-query separator. The
LHIP clauses defining this separator can be one or
more words covering rule like for instance:
ann_query_separator([X],0.7) #1.0 --&gt;
Oterminal(X),
{X=&apos;teilephoneq.
ann_query_separator([X,Y],1) #1.0 --&gt;
Oterminal(X),
Oterminal(T),
{EX = &apos;numdro&apos;,Y =
As an example of semantic constituents introducers
we propose here the following rule:
street_intro(ET,Prep1,1) #1.0 --&gt;
* street_type(T),
preposition (Prep).
which make use of some word knowledge about street
types coming from an external thesaurus like:
street_type(X) --&gt;
Oterminal(X),
{thesaurus(street,W),member(X,W)}.
It should be noted that we mix weighted and non-
weighted rules, simply because non-weighted rules
are rules with the highest weight 1.
</bodyText>
<subsectionHeader confidence="0.988072">
2.2.1 Generation of hypotheses
</subsectionHeader>
<bodyText confidence="0.999905333333333">
The generation of annotation hypotheses is per-
formed by: composing weighted rules, assembling
constituents and filtering possible hypotheses. In
this case the grammar should provide a means to
provide an empty constituent when all possible hy-
pothesis rules have failed. The highest level con-
stituent is represented by the whole sentence struc-
ture which simply specifies the possible orders of
constituents relative to annotation hypotheses.
</bodyText>
<page confidence="0.981581">
21
</page>
<equation confidence="0.952710363636364">
s(s(CAnn,Query]), W) --&gt;
ann(Ann),
query(Query,W2).
ann(ann(Ann)) --&gt; closK(word(Ann)).
query(query(Q),W) --&gt;
* ann_query_separator(QSep,W1),
target(Target,W2),
address(Addr,W3)
fapp_list(Nsep, (Target]
CAddr31,Q) ,
min_list ( [wi , w2 , w3],W)}.
</equation>
<bodyText confidence="0.9999882">
In the arm rule we have made use of the Kleene
closure operator closK which allow LHIP to sim-
ply formulate regular expressions. In the query rule
we have specified a possible order of constituents in-
terleaved by semantic markers (e.g. separators and
introducers). In this case we did not provide any lin-
guistic constraint (e.g. preferring names belonging
to the minimal common syntactic sub-tree or those
having the longest sequence of proper names belong-
ing to the same sub-tree).
</bodyText>
<sectionHeader confidence="0.989815" genericHeader="conclusions">
3 Conclusions and future works
</sectionHeader>
<bodyText confidence="0.991530447368421">
In this paper we summarized a proposal for a frame-
work for designing grammar-based automated an-
notation applications. Starting with a case study
and following an approach which combines the no-
tions of fuzziness and robustness in sentence parsing,
we showed how to build practical domain-dependent
rules which can be applied whenever it is possible to
superimpose a sentence-level semantic structure to
a text without relying on a previous deep syntacti-
cal analysis. Even if the query generation problem
may not seem a critical application one should bear
in mind that the sentence processing must be done
on-line.
As we have previously seen, the cue-words used as
semantic markers are domain-dependent. Even their
relevance disposal and their weight within the rules
depends on their linguistic usage. Therefore, a com-
plete automatic annotation system based on the ap-
proach proposed in this article seems to be adequate
to give precise results. However, a semi-automatic
system could satisfy our needs. This system should
be based on the following techniques to achieve a
high level of performance:
1. For each annotation, the system offers a list
of propositions based on standard grammars
as well as on external knowledge (ontologies,
knowledge bases ...)
2. According to the grammar initially proposed,
the user may change the annotation accord-
ing to his needs. These modifications are held
within the system to change the grammar rules
as well as their weights. This makes the system
interactive and enhanced by a learning phase.
3. We could imagine that rule design process can
be partially automated and we intend to pursue
some research on developing methods for both
assisted rule design and corpus based rule in-
duction.
</bodyText>
<sectionHeader confidence="0.997691" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994965958333334">
Peter. R.J. Asveld. 1996. Towards robustness in
parsing - fuzzifying context-free language recog-
nition. In J. Dassow, G. Rozemberg, and A. Sa-
lomaa, editors, Developments in Language Theory
II - At the Crossroad of Mathematics, Computer
Science and Biology, pages 443-453. World Scien-
tific, Singapore.
A. Ballim and G. Russell. 1994. LHIP: Extended
DCGs for Configurable Robust Parsing. In Pro-
ceedings of the 15th International Conference on
Computational Linguistics, pages 501 — 507, Ky-
oto, Japan. ACL.
R. Basili and Pazienza M.T. 1997. Lexical ac-
quisitiion and information extraction. In Pazienza
M.T., editor, Information Extraction — A multi-
diciplinary approach to an ermerging information
technology, volume 1299 of LNAI, pages 44-72.
Springer Verlag.
J-C. Chappelier, M. Rajman, P. Bouillon, S. Arm-
strong, V. Pallotta, and A Ballim. 1999. Isis
project: final report. Technical report, Computer
Science Department - Swiss Federal Institute of
Technology, September. .
G. Grefenstette. 1996. Light parsing as finite-state
filtering. In Kornai A., editor, Proceedings of
the ECAI 96 Workshop on Extended Finite State
Models of Language, pages 20-25.
J. Hobbs, D. Appelt, J. Bear, D. Israel,
M. Kameyama, M. Stickel, and M. Tyson. 1996.
Fastus: a cascaded finite-state transducer for ex-
tracting information from natural-language text.
In E. Roche and Y. Schabes, editors, Finite State
Devices for Natural Language Processing. MIT
Press, Cambridge MA.
E.T. Lee and L.A. Zadeh. 1969. Note on fuzzy lan-
guages. Information Science, 1:421-434.
C. Lieske and A. Ballim. 1998. Rethinking nat-
ural language processing with prolog. In Pro-
ceedings of Practical Applications of Prolog and
Practical Applications of Constraint Technology
(PAPPACTS98), London,UK. Practical Applica-
tion Company.
D. Moll, J. Bern, and M. Hess. 1998. A real world
implementation of answer extraction. In Proc. of
the 9th International Conference and Workshop
on Database and Expert Systems. Workshop on
Natural Language and Information Systems, vol-
ume NLIS&apos;98, pages 143-148, Vienna.
</reference>
<page confidence="0.97584">
22
</page>
<reference confidence="0.995769">
C.M. Sperberg-McQueen and L. Burnard, editors.
1994. Guidelines for Electronic Text Encoding and
Interchange, Text Encoding Initiative. Chicago
and Oxford.
</reference>
<page confidence="0.998923">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.518151">
<title confidence="0.999958">A Weighted Robust Parsing Approach to Semantic Annotation</title>
<author confidence="0.98868">Hatem Ghorbel</author>
<author confidence="0.98868">Vincenzo Pallotta</author>
<affiliation confidence="0.9966225">LITH-MEDIA group Swiss Federal Institute of Technology</affiliation>
<address confidence="0.998799">IN Ecublens, 1015 Lausanne, Switzerland</address>
<email confidence="0.533946">Ighorbel,pallottal@di.epfl.ch</email>
<abstract confidence="0.9987217">This paper proposes a grammar-based approach to semantic annotation which combines the notions of robust parsing and fuzzy grammars. We present an overview of a preliminary research aimed to generalize some results from a recent project on interaction through speech with information systems where techniques based on the above notions have been successfully applied. The goal of the article is to give a development environment to linguists.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R J Asveld</author>
</authors>
<title>Towards robustness in parsing - fuzzifying context-free language recognition.</title>
<date>1996</date>
<booktitle>Developments in Language Theory II - At the Crossroad of Mathematics, Computer Science and Biology,</booktitle>
<pages>443--453</pages>
<editor>In J. Dassow, G. Rozemberg, and A. Salomaa, editors,</editor>
<publisher>World Scientific, Singapore.</publisher>
<contexts>
<context position="8545" citStr="Asveld, 1996" startWordPosition="1365" endWordPosition="1366">d a simple weighted rule in a compositional fashion where the resulting weight is computed from the sub-constituents using the minimum operator. Weights are real numbers in the interval [0, 1]. cat(cat(Hyp),Weight) --&gt; sub_cat1(H1,W1), sub_catn(Hn,Wn), {app_list([H1,...,Hn],Hyp), min_list(EW1,...,Wn],Weight)}. This strategy is not the only possible since the LHIP formalism allows a greater flexibility. Without entering into formal details we can observe that if we strictly follow the above schema and we impose a cover threshold of 1 we are dealing with fuzzy DCG grammars (Lee and Zadeh, 1969; Asveld, 1996). We actually extend this class of grammars with a notion of fuzzy-robustness where weights are used to compute confidence factors for the membership of islands to categories&apos;. The order of constituents may play an important role in assigning weights for different rules having the same number and type of constituents. Each LHIP rule returns a weight together with a term which will contribute to build the resulting structure. The confidence factor for a pre-terminal rule has been assigned statically on the basis of the rule designer&apos;s domain knowledge. 2.2 The methodology at work In our case st</context>
</contexts>
<marker>Asveld, 1996</marker>
<rawString>Peter. R.J. Asveld. 1996. Towards robustness in parsing - fuzzifying context-free language recognition. In J. Dassow, G. Rozemberg, and A. Salomaa, editors, Developments in Language Theory II - At the Crossroad of Mathematics, Computer Science and Biology, pages 443-453. World Scientific, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ballim</author>
<author>G Russell</author>
</authors>
<title>LHIP: Extended DCGs for Configurable Robust Parsing.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics, pages 501 — 507,</booktitle>
<publisher>ACL.</publisher>
<location>Kyoto, Japan.</location>
<marker>Ballim, Russell, 1994</marker>
<rawString>A. Ballim and G. Russell. 1994. LHIP: Extended DCGs for Configurable Robust Parsing. In Proceedings of the 15th International Conference on Computational Linguistics, pages 501 — 507, Kyoto, Japan. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>M T Pazienza</author>
</authors>
<title>Lexical acquisitiion and information extraction.</title>
<date>1997</date>
<booktitle>Information Extraction — A multidiciplinary approach to an ermerging information technology, volume 1299 of LNAI,</booktitle>
<pages>44--72</pages>
<editor>In Pazienza M.T., editor,</editor>
<publisher>Springer Verlag.</publisher>
<marker>Basili, Pazienza, 1997</marker>
<rawString>R. Basili and Pazienza M.T. 1997. Lexical acquisitiion and information extraction. In Pazienza M.T., editor, Information Extraction — A multidiciplinary approach to an ermerging information technology, volume 1299 of LNAI, pages 44-72. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-C Chappelier</author>
<author>M Rajman</author>
<author>P Bouillon</author>
<author>S Armstrong</author>
<author>V Pallotta</author>
<author>A Ballim</author>
</authors>
<title>Isis project: final report.</title>
<date>1999</date>
<tech>Technical report,</tech>
<publisher></publisher>
<institution>Computer Science Department - Swiss Federal Institute of Technology,</institution>
<contexts>
<context position="2068" citStr="Chappelier et al., 1999" startWordPosition="320" endWordPosition="323"> achieved by a light parsing of the text which is based on the user of certain cue-words as a heuristic for describing its semantic structure. 1.1 A case study in query generation The availability of a large collection of annotated telephone calls for querying the Swiss phone-book database (i.e the Swiss French PolyPhone corpus) allowed us to experiment our recent findings in robust text analysis obtained in the context of the Swiss National Fund research project ROTA (Robust Text Analysis), and in the recent Swisscom funded project ISIS (Interaction through Speech with Information Systems)1 (Chappelier et al., 1999). This database contains 4293 simulated recordings related to the 1The final report of ISIS project is available at http://lithwww.epfl.chrpallotta/isis.html &amp;quot;111&amp;quot; Swisscom service calls. For instance a query call like: Bonjour j&apos;aimerais un nurser° de telephone a Saignelegier c&apos;est Mottaz m o deux ta z Monique rue du printemps numero quatre would produce the following query frame filling for the Swiss Phone-book database: Nom de famille / Firme: MOTTAZ Prenom / Autres informations: MONIQUE Rue, numero: rue du PRINTEMPS, 4 NPA, localito: SAIGNELEGIER. The goal of semantic annotation is to prov</context>
</contexts>
<marker>Chappelier, Rajman, Bouillon, Armstrong, Pallotta, Ballim, 1999</marker>
<rawString>J-C. Chappelier, M. Rajman, P. Bouillon, S. Armstrong, V. Pallotta, and A Ballim. 1999. Isis project: final report. Technical report, Computer Science Department - Swiss Federal Institute of Technology, September. .</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Light parsing as finite-state filtering.</title>
<date>1996</date>
<booktitle>Proceedings of the ECAI 96 Workshop on Extended Finite State Models of Language,</booktitle>
<pages>20--25</pages>
<editor>In Kornai A., editor,</editor>
<contexts>
<context position="6418" citStr="Grefenstette, 1996" startWordPosition="1002" endWordPosition="1003">number&gt; &lt;city&gt; &lt;/city&gt; &lt;/address&gt; &lt;/query&gt; 2 Methodology In this section we propose the use of a &amp;quot;light-parser&amp;quot; for doing sentence-level semantic annotation. The main idea comes from the observation that annotation does not always need to rely on the deep structure of the sentence (e.g. at morpho-syntactic level). It is sometimes sufficient to find some cue-words which allow us to locate the logical sub-structures of the sentence. If the domain is simple enough, this task can be easily mechanized. A similar approach, using finite state parsing technology, has been proposed by Grefenstette in (Grefenstette, 1996) where the main applications are slanted to the extraction of syntactic information. 2.1 Robust Definite Clause Grammars LHIP (Left-corner Head-driven Island Parser) (Balurn and Russell, 1994; Lieske and Ballim, 1998) is a system which performs robust analysis of its input, using a grammar defined in an extended form of the PROLOG Definite Clause Grammars (DCGs). The chief modifications to the standard PROLOG &apos;grammar rule&apos; format are of two types: one or more righthand side (RHS) items may be marked as &apos;heads&apos; (e.g. using a leading &apos;*&apos;), and one or more RHS items may be marked as &apos;ignorable&apos; </context>
</contexts>
<marker>Grefenstette, 1996</marker>
<rawString>G. Grefenstette. 1996. Light parsing as finite-state filtering. In Kornai A., editor, Proceedings of the ECAI 96 Workshop on Extended Finite State Models of Language, pages 20-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
<author>D Appelt</author>
<author>J Bear</author>
<author>D Israel</author>
<author>M Kameyama</author>
<author>M Stickel</author>
<author>M Tyson</author>
</authors>
<title>Fastus: a cascaded finite-state transducer for extracting information from natural-language text.</title>
<date>1996</date>
<booktitle>Finite State Devices for Natural Language Processing.</booktitle>
<editor>In E. Roche and Y. Schabes, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge MA.</location>
<contexts>
<context position="1149" citStr="Hobbs et al., 1996" startWordPosition="171" endWordPosition="174">echniques based on the above notions have been successfully applied. The goal of the article is to give a development environment to linguists. 1 Introduction In this article we are mainly interested in semantic annotation (Sperberg-McQueen and Burnard, 1994). We are considering the Information Extraction (I.E.) problem as a semantic annotation problem: extracting information is finding the relevant terms that contribute to describe an appropriate semantic structure of the text. Some of the most important works in I.E. have been dealing with domain dependent documents like (Moll et al., 1998; Hobbs et al., 1996). Both systems employ complex analysis schemas. Assigning semantic field tags is in general a difficult task. This is due at least to the crucial need of the domain knowledge and also of the linguistic knowledge. Our approach considers that for some specific domains a semantic annotation can be achieved by a light parsing of the text which is based on the user of certain cue-words as a heuristic for describing its semantic structure. 1.1 A case study in query generation The availability of a large collection of annotated telephone calls for querying the Swiss phone-book database (i.e the Swiss</context>
</contexts>
<marker>Hobbs, Appelt, Bear, Israel, Kameyama, Stickel, Tyson, 1996</marker>
<rawString>J. Hobbs, D. Appelt, J. Bear, D. Israel, M. Kameyama, M. Stickel, and M. Tyson. 1996. Fastus: a cascaded finite-state transducer for extracting information from natural-language text. In E. Roche and Y. Schabes, editors, Finite State Devices for Natural Language Processing. MIT Press, Cambridge MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Lee</author>
<author>L A Zadeh</author>
</authors>
<title>Note on fuzzy languages.</title>
<date>1969</date>
<journal>Information Science,</journal>
<pages>1--421</pages>
<contexts>
<context position="8530" citStr="Lee and Zadeh, 1969" startWordPosition="1361" endWordPosition="1364">llustrate how to build a simple weighted rule in a compositional fashion where the resulting weight is computed from the sub-constituents using the minimum operator. Weights are real numbers in the interval [0, 1]. cat(cat(Hyp),Weight) --&gt; sub_cat1(H1,W1), sub_catn(Hn,Wn), {app_list([H1,...,Hn],Hyp), min_list(EW1,...,Wn],Weight)}. This strategy is not the only possible since the LHIP formalism allows a greater flexibility. Without entering into formal details we can observe that if we strictly follow the above schema and we impose a cover threshold of 1 we are dealing with fuzzy DCG grammars (Lee and Zadeh, 1969; Asveld, 1996). We actually extend this class of grammars with a notion of fuzzy-robustness where weights are used to compute confidence factors for the membership of islands to categories&apos;. The order of constituents may play an important role in assigning weights for different rules having the same number and type of constituents. Each LHIP rule returns a weight together with a term which will contribute to build the resulting structure. The confidence factor for a pre-terminal rule has been assigned statically on the basis of the rule designer&apos;s domain knowledge. 2.2 The methodology at work</context>
</contexts>
<marker>Lee, Zadeh, 1969</marker>
<rawString>E.T. Lee and L.A. Zadeh. 1969. Note on fuzzy languages. Information Science, 1:421-434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lieske</author>
<author>A Ballim</author>
</authors>
<title>Rethinking natural language processing with prolog.</title>
<date>1998</date>
<booktitle>In Proceedings of Practical Applications of Prolog and Practical Applications of Constraint Technology (PAPPACTS98), London,UK. Practical Application Company.</booktitle>
<contexts>
<context position="6635" citStr="Lieske and Ballim, 1998" startWordPosition="1031" endWordPosition="1034">on does not always need to rely on the deep structure of the sentence (e.g. at morpho-syntactic level). It is sometimes sufficient to find some cue-words which allow us to locate the logical sub-structures of the sentence. If the domain is simple enough, this task can be easily mechanized. A similar approach, using finite state parsing technology, has been proposed by Grefenstette in (Grefenstette, 1996) where the main applications are slanted to the extraction of syntactic information. 2.1 Robust Definite Clause Grammars LHIP (Left-corner Head-driven Island Parser) (Balurn and Russell, 1994; Lieske and Ballim, 1998) is a system which performs robust analysis of its input, using a grammar defined in an extended form of the PROLOG Definite Clause Grammars (DCGs). The chief modifications to the standard PROLOG &apos;grammar rule&apos; format are of two types: one or more righthand side (RHS) items may be marked as &apos;heads&apos; (e.g. using a leading &apos;*&apos;), and one or more RHS items may be marked as &apos;ignorable&apos; (e.g. using a leading &apos;-&apos;). LHIP employs a different control strategy from that used by PROLOG DCGs, in order to allow it to cope with ungrammatical or unforeseen input. The behavior of LHIP can best be understood in </context>
</contexts>
<marker>Lieske, Ballim, 1998</marker>
<rawString>C. Lieske and A. Ballim. 1998. Rethinking natural language processing with prolog. In Proceedings of Practical Applications of Prolog and Practical Applications of Constraint Technology (PAPPACTS98), London,UK. Practical Application Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moll</author>
<author>J Bern</author>
<author>M Hess</author>
</authors>
<title>A real world implementation of answer extraction.</title>
<date>1998</date>
<booktitle>In Proc. of the 9th International Conference and Workshop on Database and Expert Systems. Workshop on Natural Language and Information Systems,</booktitle>
<volume>98</volume>
<pages>143--148</pages>
<location>Vienna.</location>
<contexts>
<context position="1128" citStr="Moll et al., 1998" startWordPosition="167" endWordPosition="170">ion systems where techniques based on the above notions have been successfully applied. The goal of the article is to give a development environment to linguists. 1 Introduction In this article we are mainly interested in semantic annotation (Sperberg-McQueen and Burnard, 1994). We are considering the Information Extraction (I.E.) problem as a semantic annotation problem: extracting information is finding the relevant terms that contribute to describe an appropriate semantic structure of the text. Some of the most important works in I.E. have been dealing with domain dependent documents like (Moll et al., 1998; Hobbs et al., 1996). Both systems employ complex analysis schemas. Assigning semantic field tags is in general a difficult task. This is due at least to the crucial need of the domain knowledge and also of the linguistic knowledge. Our approach considers that for some specific domains a semantic annotation can be achieved by a light parsing of the text which is based on the user of certain cue-words as a heuristic for describing its semantic structure. 1.1 A case study in query generation The availability of a large collection of annotated telephone calls for querying the Swiss phone-book da</context>
</contexts>
<marker>Moll, Bern, Hess, 1998</marker>
<rawString>D. Moll, J. Bern, and M. Hess. 1998. A real world implementation of answer extraction. In Proc. of the 9th International Conference and Workshop on Database and Expert Systems. Workshop on Natural Language and Information Systems, volume NLIS&apos;98, pages 143-148, Vienna.</rawString>
</citation>
<citation valid="true">
<date>1994</date>
<booktitle>Guidelines for Electronic Text Encoding and Interchange, Text Encoding Initiative. Chicago and Oxford.</booktitle>
<editor>C.M. Sperberg-McQueen and L. Burnard, editors.</editor>
<marker>1994</marker>
<rawString>C.M. Sperberg-McQueen and L. Burnard, editors. 1994. Guidelines for Electronic Text Encoding and Interchange, Text Encoding Initiative. Chicago and Oxford.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>