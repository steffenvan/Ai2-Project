<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.996565">
Encoding Relation Requirements for Relation Extraction via Joint
Inference
</title>
<author confidence="0.992619">
Liwei Chen1, Yansong Feng∗1, Songfang Huang2, Yong Qin2 and Dongyan Zhao1
</author>
<affiliation confidence="0.86408">
1ICST, Peking University, Beijing, China
2IBM China Research Lab, Beijing, China
</affiliation>
<email confidence="0.9456455">
chenliwei,fengyansong,zhaodongyan@pku.edu.cn
huangsf,qinyong@cn.ibm.com
</email>
<sectionHeader confidence="0.981429" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948047619048">
Most existing relation extraction models
make predictions for each entity pair lo-
cally and individually, while ignoring im-
plicit global clues available in the knowl-
edge base, sometimes leading to conflicts
among local predictions from different en-
tity pairs. In this paper, we propose
a joint inference framework that utilizes
these global clues to resolve disagree-
ments among local predictions. We ex-
ploit two kinds of clues to generate con-
straints which can capture the implicit type
and cardinality requirements of a relation.
Experimental results on three datasets, in
both English and Chinese, show that our
framework outperforms the state-of-the-
art relation extraction models when such
clues are applicable to the datasets. And,
we find that the clues learnt automatically
from existing knowledge bases perform
comparably to those refined by human.
</bodyText>
<sectionHeader confidence="0.995167" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997300172413794">
Identifying predefined kinds of relationship be-
tween pairs of entities is crucial for many knowl-
edge base related applications(Suchanek et al.,
2013). In the literature, relation extraction (RE) is
usually investigated in a classification style, where
relations are simply treated as isolated class labels,
while their definitions or background information
are sometimes ignored. Take the relation Capital
as an example, we can imagine that this relation
will expect a country as its subject and a city as
object, and in most cases, a city can be the capital
of only one country. All these clues are no doubt
helpful, for instance, Yao et al. (2010) explicitly
modeled the expected types of a relation’s argu-
ments with the help of Freebase’s type taxonomy
and obtained promising results for RE.
∗Yansong Feng is the corresponding author.
However, properly capturing and utilizing such
typing clues are not trivial. One of the hurdles here
is the lack of off-the-shelf resources and such clues
often have to be coded by human experts. Many
knowledge bases do not have a well-defined typing
system, let alone fine-grained typing taxonomies
with corresponding type recognizers, which are
crucial to explicitly model the typing requirements
for arguments of a relation, but rather expensive
and time-consuming to collect. Similarly, the car-
dinality requirements of arguments, e.g., a person
can have only one birthdate and a city can only be
labeled as capital of one country, should be con-
sidered as a strong indicator to eliminate wrong
predictions, but has to be coded manually as well.
On the other hand, most previous relation ex-
tractors process each entity pair (we will use en-
tity pair and entity tuple exchangeably in the rest
of the paper) locally and individually, i.e., the ex-
tractor makes decisions solely based on the sen-
tences containing the current entity pair and ig-
nores other related pairs, therefore has difficulties
to capture possible disagreements among different
entity pairs. However, when looking at the output
of a multi-class relation predictor globally, we can
easily find possible incorrect predictions such as a
university locates in two different cities, two dif-
ferent cities have been labeled as capital for one
country, a country locates in a city and so on.
In this paper, we will address how to derive and
exploit two categories of these clues: the expected
types and the cardinality requirements of a rela-
tion’s arguments, in the scenario of relation extrac-
tion. We propose to perform joint inference upon
multiple local predictions by leveraging implicit
clues that are encoded with relation specific re-
quirements and can be learnt from existing knowl-
edge bases. Specifically, the joint inference frame-
work operates on the output of a sentence level re-
lation extractor as input, derives 5 types of con-
straints from an existing KB to implicitly capture
</bodyText>
<page confidence="0.465482">
818
</page>
<note confidence="0.9670035">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 818–827,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999843611111111">
the expected type and cardinality requirements for
a relation’s arguments, and jointly resolve the dis-
agreements among candidate predictions. We for-
malize this procedure as a constrained optimiza-
tion problem, which can be solved by many opti-
mization frameworks. We use integer linear pro-
gramming (ILP) as the solver and evaluate our
framework on English and Chinese datasets. The
experimental results show that our framework per-
forms better than the state-of-the-art approaches
when such clues are applicable to the datasets. We
also show that the automatically learnt clues per-
form comparably to those refined manually.
In the rest of the paper, we first review related
work in Section 2, and in Section 3, we describe
our framework in detail. Experimental setup and
results are discussed in Section 4. We conclude
this paper in Section 5.
</bodyText>
<sectionHeader confidence="0.999288" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999575590909091">
Since traditional supervised relation extraction
methods (Soderland et al., 1995; Zhao and Gr-
ishman, 2005) require manual annotations and are
often domain-specific, nowadays many efforts fo-
cus on semi-supervised or unsupervised methods
(Banko et al., 2007; Fader et al., 2011). Distant
supervision (DS) is a semi-supervised RE frame-
work and has attracted many attentions (Bunescu,
2007; Mintz et al., 2009; Yao et al., 2010; Sur-
deanu et al., 2010; Hoffmann et al., 2011; Sur-
deanu et al., 2012). DS approaches can predict
canonicalized (predefined in KBs) relations for
large amount of data and do not need much hu-
man involvement. Since the automatically gener-
ated training datasets in DS often contain noises,
there are also research efforts focusing on reduc-
ing the noisy labels in the training data (Takamatsu
et al., 2012). To bridge the gaps between the rela-
tions extracted from open information extraction
and the canonicalized relations in KBs, Yao et al.
(2012) and Riedel et al. (2013) propose a universal
schema which is a union of KB schemas and nat-
ural language patterns, making it possible to in-
tegrate the unlimited set of uncanonicalized rela-
tions in open settings with the relations in existing
KBs.
As far as we know, few works have managed
to take the relation specific requirements for ar-
guments into account, and most existing works
make predictions locally and individually. The
MultiR system allows entity tuples to have more
than one relations, but still predicts each entity
tuple locally (Hoffmann et al., 2011). Surdeanu
et al. (2012) propose a two-layer multi-instance
multi-label (MIML) framework to capture the de-
pendencies among relations. The first layer is a
multi-class classifier making local predictions for
single sentences, the output of which are aggre-
gated by the second layer into the entity pair level.
Their approach only captures relation dependen-
cies, while we learn implicit relation backgrounds
from knowledge bases, including argument type
and cardinality requirements. Riedel et al. (2013)
propose to use latent vectors to estimate the pref-
erences between relations and entities. These can
be considered as the latent type information of the
relations’ arguments, which is learnt from various
data sources. In contrast, our approach learn im-
plicit clues from existing KBs, and jointly opti-
mize local predictions among different entity tu-
ples to capture both relation argument type clues
and cardinality clues. Li et al. (2011) and Li et al.
(2013) use co-occurring statistics among relations
or events to jointly improve information extrac-
tion performances in ACE tasks, while we mine
existing KBs to collect global clues to solve lo-
cal conflicts and find the optimal aggregation as-
signments, regarding existing knowledge facts. de
Lacalle and Lapata (2013) encode general domain
knowledge as FOL rules in a topic model while
our instantiated constraints are directly operated in
an ILP model. Zhang et al. (2013) utilize relation
cardinality to create negative samples for distant
supervision while we use both implicit type clues
and relation cardinality expectations to discover
possible inconsistencies among local predictions.
</bodyText>
<sectionHeader confidence="0.994632" genericHeader="method">
3 The Framework
</sectionHeader>
<bodyText confidence="0.999878">
Our framework takes a set of entity pairs and their
supporting sentences as its input. We first train
a preliminary sentence level extractor which can
output confidence scores for its predictions, e.g.,
a maximum entropy or logistic regression model,
and use this local extractor to produce local predic-
tions. In order to implicitly capture the expected
type and cardinality requirements for a relation’s
arguments, we derive two kinds of clues from an
existing KB, which are further utilized to discover
the disagreements among local candidate predic-
tions. Our objective is to maximize the overall
confidence of all the selected predictions.
</bodyText>
<page confidence="0.860651">
819
</page>
<subsectionHeader confidence="0.996794">
3.1 Generating Candidate Relations
</subsectionHeader>
<bodyText confidence="0.99517264516129">
Since we will focus on the open domain relation
extraction, we still follow the distant supervision
paradigm to collect our training data guided by
a KB, and train the local extractor accordingly.
Specifically, we train a sentence level extractor us-
ing the maximum entropy model. Given a sen-
tence containing an entity pair, the model will
output the confidence of this sentence represent-
ing certain relationship (from a predefined relation
set) between the entity pair. Formally R repre-
sents the relation set we are working on, T is the
set of entity tuples that we will predict in the test
set.
Keep in mind that our local extractor is trained
on noisy training data, which, we admit, is not
fully reliable. As we observed in a pilot experi-
ment that there is a good chance that the predic-
tions ranked in the second or third may still be
correct, we select top three predictions as the can-
didate relations for each mention in order to intro-
duce more potentially correct output.
On the other hand, we should discard the pre-
dictions whose confidences are too low to be true,
where we set up a threshold of 0.1. For a tuple t,
we obtain its candidate relation set by combining
the candidate relations of all its mentions, and rep-
resent it as Rt. For a candidate relation r E Rt and
a tuple t, we define Mrt as all t’s mentions whose
candidate relations contain r. Now the confidence
score of a relation r E Rt being assigned to tuple
t can be calculated as:
</bodyText>
<equation confidence="0.942191">
�conf(t, r) = MEscore(m, r) (1)
m∈Mrt
</equation>
<bodyText confidence="0.973323148148148">
where MEscore(m, r) is the confidence of mention
m representing relation r output by our prelimi-
nary extractor.
Traditionally, both lexical features and syntac-
tic features are used in relation extraction. Lexi-
cal features are the word chains between the sub-
jects and objects in the sentences, while syntactic
features are the dependency paths from the sub-
jects to the objects on the dependency graphs of
the supporting sentences. However, lexical fea-
tures are usually too specific to frequently appear
in the test data, while the reliability of syntactic
features depends heavily on the quality of depen-
dency parsing tools. Generally, we expect more
potentially correct relations to be put into the can-
didate relation set for further consideration. So in
Figure 1: The different types of disagreements we
will investigate in the candidate relations. The
clues of detecting these inconsistencies can be
learnt from a knowledge base.
addition to lexical and syntactic features, we also
use n-gram features to train our preliminary rela-
tion extraction model. N-gram features are consid-
ered as more ambiguous compared to traditional
lexical and syntactic features, and may introduce
incorrect predictions, thus improving the recall at
the cost of precision.
</bodyText>
<subsectionHeader confidence="0.998863">
3.2 Disagreements among the Candidates
</subsectionHeader>
<bodyText confidence="0.984065">
The candidate relations we obtained in the pre-
vious subsection inevitably include many incor-
rect predictions. Ideally we should discard those
wrong predictions to produce more accurate re-
sults.
As discussed earlier, we will exploit from the
knowledge base two categories of clues that im-
plicitly capture relations’ backgrounds: their ex-
pected argument types and argument cardinalities,
based on which we can discover two categories
of disagreements among the candidate predictions,
summarized as argument type inconsistencies and
violations of arguments’ uniqueness, which have
been rarely considered before. We will discuss
them in detail, and describe how to learn the clues
from a KB afterwards.
Implicit Argument Types Inconsistencies:
Generally, the argument types of the correct
predictions should be consistent with each other.
Given a relation, its arguments sometimes are
required to be certain types of entities. For
example, in Figure 1, the relation LargestCity
restricts its subject to be either countries or states,
and its object to be cities. If the predictions
among different entity tuples require the same
entity to belong to different types, we call this
</bodyText>
<figure confidence="0.995130833333333">
Germany, Washington D.C.
1-)0
Capital: 0.3
Columbia University, New York
LocationCountry: 0.5 1-)0
LocationCity: 0.3 1
conflict
conflict
LocationCity: 0.05
LocationCity: 0.03
LargestCity: 0.4
Capital: 0.95
USA, Washington D.C.
Capital: 0.5
USA, New York
1-)0
1-)0
1
1
1
conflict
conflict
conflict
conflict
</figure>
<table confidence="0.923885333333333">
New York University, New York
LocationCity: 0.8 1
FoundationPlace: 0.15 1
Richard Fuld,USA
Nationality: 0.7 1
BirthPlace: 0.2 1
</table>
<page confidence="0.62572">
820
</page>
<bodyText confidence="0.997920338461539">
an argument type inconsistency. Take &lt;USA,
New York&gt; and &lt;USA, Washington D.C.&gt; as an
example. In Figure 1, &lt;USA, New York&gt; has
a candidate relation LargestCity which restricts
USA to be either countries or states, while &lt;USA,
Washington D.C.&gt; has a prediction LocationCity
which indicates a disagreement in terms of USA’s
type because the latter prediction expects USA to
be an organization located in a city. This warns
that at least one of the two candidate relations is
incorrect.
The previous scenario shows that the subjects
of two candidate relations may disagree with each
other. From Figure 1, we can observe two more
situations: the first one is that the objects of
the two candidate relations are inconsistent with
each other, for example &lt;New York University,
New York&gt; with the prediction LocationCity and
&lt;Columbia University, New York&gt; with the pre-
diction LocationCountry. The second one is
that the subject of one candidate relation do not
agree with another prediction’s object, for exam-
ple &lt;Richard Fuld, USA&gt; with the prediction Na-
tionality and &lt;USA, New York&gt; with the pre-
diction LocationCity. Although we have not as-
signed explicit types to these entities, we can still
exploit the inconsistencies implicitly with the help
of shared entities. Note that the implicit argument
typing clues here mean whether two relations can
share arguments, but NOT enumate what types ex-
plicitly their arguments should have.
We formalize all the relation pairs that disagree
with each other as follows. These relation pairs
can be divided into three subcategories. We repre-
sent the relation pairs (rz, rj) that are inconsistent
in terms of subjects as C&amp;quot;, the relations pairs that
are inconsistent in terms of objects as C&apos;o, the re-
lation pairs that are inconsistent in terms of one’s
subject and the other one’s object as C&apos;�&apos;.
It is worth mentioning that disagreements in-
side a tuple are also included here. For instance,
an entity tuple &lt;USA, Washington D.C.&gt; in Fig-
ure 1 has two candidate relations, Capital and Lo-
cationCity. These two predictions are inconsistent
with each other with respect to the type of USA.
They implicitly consider USA as “country” and
“organization”, respectively.
Violations of Arguments’ Uniqueness: The
previous categories of disagreements are all based
on the implicit type information of the relations’
arguments, Now we make use of the clues of ar-
gument cardinality requirements. Given a subject,
some relations should have unique objects. For
example, in Figure 1, given USA as the subject of
the relation Capital, we can only accept one pos-
sible object, because there is great chance that a
country only have one capital. On the other hand,
given Washington D.C. as the object of the relation
Capital, we can only accept one subject, since usu-
ally a city can only be the capital of one country
or state. If these are violating in the candidates,
we could know that there may be some incorrect
predictions. We represent the relations expecting
unique objects as Co�, and the relations expecting
unique subjects as C&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.999495">
3.3 Obtaining the Global Clues
</subsectionHeader>
<bodyText confidence="0.999800857142857">
Now, the issue is how to obtain the clues used
in the previous subsection. That is, how we de-
termine which relations expect certain types of
subjects, which relations expect certain types of
objects, etc. These knowledge can be definitely
coded by human, or learnt from a KB.
Most existing knowledge bases represent their
knowledge facts in the form of (&lt;subject, rela-
tion, object&gt;) triple, which can be seen as re-
lational facts between entity tuples. Usually the
triples in a KB are carefully defined by experts. It
is rare to find inconsistencies among the triples in
the knowledge base. The clues are therefore learnt
from KBs, and further refined manually if needed.
Given two relations r1 and r2, we query the KB
for all tuples bearing the relation r1 or r2. We use
Sz and Oz to represent rz’s (i E {1, 2}) subject set
and object set, respectively. We adopt the point-
wise mutual information (PMI) to estimate the de-
pendency between the argument sets of two rela-
tions:
</bodyText>
<equation confidence="0.96169">
PMI(A, B) = log p(A, B) (2)
p(A)p(B)
</equation>
<bodyText confidence="0.999980636363636">
where p(A, B) is number of the entities both in
A and B, p(A) and p(B) are the numbers of
the entities in A and B, respectively. For any
pair of relations from R x R, we calculate four
scores: PMI(S1, S2), PMI(O1, O2), PMI(S1, O2)
and PMI(S2, O1). To make more stable esti-
mations, we set up a threshold for the PMI. If
PMI(S1, S2) is lower than the threshold, we will
consider that r1 and r2 cannot share a subject.
Things are similar for the other three scores. The
threshold is set to -3 in this paper.
</bodyText>
<page confidence="0.619891">
821
</page>
<bodyText confidence="0.999966">
We can also learn the uniqueness of arguments
for relations. For each pre-defined relation in R,
we collect all the triples containing this relation,
and count the portion of the triples which only
have one object for each subject, and the por-
tion of the triples which only have one subject
for each object. The relations whose portions are
higher than the threshold will be considered to
have unique argument values. This threshold is
set to 0.8 in this paper.
</bodyText>
<subsectionHeader confidence="0.906462">
3.4 Integer Linear Program Formulation
</subsectionHeader>
<bodyText confidence="0.999390473684211">
As discussed above, given a set of entity pairs and
their candidate relations output by a preliminary
extractor, our goal is to find an optimal configura-
tion for all those entities pairs jointly, solving the
disagreements among those candidate predictions
and maximizing the overall confidence of the se-
lected predictions. This is an NP-hard optimiza-
tion problem. Many optimization models can be
used to obtain the approximate solutions.
In this paper, we propose to solve the problem
by using an ILP tool, IBM ILOG Cplex1. Firstly,
for each tuple t and one of its candidate relations
r, we define a binary decision variable drt indicat-
ing whether the candidate relation r is selected by
the solver. Our objective is to maximize the total
confidence of all the selected candidates, and the
objective function can be written as:
sharing a subject. These constraints can be repre-
sented as:
</bodyText>
<equation confidence="0.98660325">
drti
ti + drtj
tj &lt; 1 (3)
bti, tj : subj(ti) = subj(tj) n (rti, rtj) E Csr
</equation>
<bodyText confidence="0.999102142857143">
where ti and tj are two tuples in T , subj(ti) is the
subject of ti, rti is a candidate relation of ti, rtj is
a candidate relation of tj.
The object-relation constraints avoid the incon-
sistencies between the predictions of two tuples
sharing an object. Formally we add the following
constraints:
</bodyText>
<equation confidence="0.9881855">
drti
ti + drtj
tj &lt; 1 (4)
bti,tj : obj(ti) = obj(tj) n (rti,rtj) E Cro
</equation>
<bodyText confidence="0.998649666666667">
where ti E T and tj E T are two tuples, obj(ti)
is the object of ti.
The relation-entity-relation constraints ensure
that if an entity works as subject and object in two
tuples ti and tj respectively, their relations agree
with each other. The constraints we add are:
</bodyText>
<equation confidence="0.988168">
drti
ti + drtj
tj &lt; 1 (5)
bti,tj : obj(ti) = subj(tj) n (rti,rtj) E Crer
</equation>
<bodyText confidence="0.998656">
The object uniqueness constraints ensure that
the relations requiring unique objects do not bear
more than one object given a subject.
</bodyText>
<figure confidence="0.952967545454546">
conf(t, r)drt
drt &lt; 1 (6)
max MEscore(m, r)drt
be n r E Cou
�max
tET,rERt
�
+
bt,rERt,mEMrt
�
tETuple(r),subj(t)=e
</figure>
<bodyText confidence="0.9999833125">
where conf(t, r) is the confidence of the tuple t
bearing the candidate relation r. The first compo-
nent is the sum of the original confidence scores of
all the selected candidates, and the second one is
the sum of the maximal mention-level confidence
scores of all the selected candidates. The latter is
designed to encourage the model to select the can-
didates with higher individual mention-level con-
fidence scores.
We add the constraints with respect to the dis-
agreements described in Section 3.2. For the sake
of clarity, we describe the constraints derived from
each scenario of the two categories of disagree-
ments separately.
The subject-relation constraints avoid the dis-
agreements between the predictions of two tuples
</bodyText>
<footnote confidence="0.727348">
1www.cplex.com
</footnote>
<bodyText confidence="0.9996944">
where e is an entity, Tuple(r) are the tuples whose
candidate relations contain r.
The subject uniqueness constraints ensure that
given an object, the relations expecting unique
subjects do not bear more than one subject.
</bodyText>
<equation confidence="0.816888333333333">
� drt &lt; 1 (7)
tETuple(r),obj(t)=e
be n r E Csu
</equation>
<bodyText confidence="0.996446">
By adopting ILP, we can combine the local
information including MaxEnt confidence scores
and the implicit relation backgrounds that are em-
bedded into global consistencies of the entity tu-
ples together. After the optimization problem is
solved, we will obtain a list of selected candidate
relations for each tuple, which will be our final
output.
</bodyText>
<figure confidence="0.663747">
822
4 Experiments tems. We tune the models of MultiR and MIML-
4.1 Datasets RE so that they fit our datasets.
</figure>
<bodyText confidence="0.999958230769231">
We evaluate our approach on three datasets, in-
cluding two English datasets and one Chinese
dataset.
The first English dataset, Riedel’s dataset, is the
one used in (Riedel et al., 2010; Hoffmann et al.,
2011; Surdeanu et al., 2012), with the same split.
It uses Freebase as the knowledge base and New
York Time corpus as the text corpus, including
about 60,000 entity tuples in the training set, and
about 90,000 entity tuples in the testing set.
We generate the second English dataset, DB-
pedia dataset, by mapping the triples in DBpedia
(Bizer et al., 2009) to the sentences in New York
Time corpus. We map 51 different relations to the
corpus and result in about 50,000 entity tuples,
134,000 sentences for training and 30,000 entity
tuples, 53,000 sentences for testing.
For the Chinese dataset, we derive knowledge
facts and construct a Chinese KB from the In-
foboxes of HudongBaike, one of the largest Chi-
nese online encyclopedias. We collect four na-
tional economic newspapers in 2009 as our corpus.
28 different relations are mapped to the corpus and
this results in 60,000 entity tuples, 120,000 sen-
tences for training and 40,000 tuples, 83,000 sen-
tences for testing.
</bodyText>
<subsectionHeader confidence="0.996777">
4.2 Baselines and Competitors
</subsectionHeader>
<bodyText confidence="0.99999745">
The baseline we use in this paper is Mintz++,
which is described in (Surdeanu et al., 2012). It
is a modification of the model proposed by Mintz
et al. (2009). The model predicts for each mention
separately, and allows multi-label outputs for an
entity tuple by OR-ing the outputs of its mentions.
As we described in Section 3.1, originally we
select the top three predicted relations as the can-
didates for each mention. In order to investigate
whether it is necessary to use up to three candi-
dates, we implement two variants of our approach,
which select the top one and top two relations as
candidates for each mention, and represented as
ILP-1cand and ILP-2cand, respectively.
We also use two distant supervision approaches
for the comparison. The first one is MultiR (Hoff-
mann et al., 2011), a novel joint model that can
deal with the relation overlap issue. The second
one, MIML-RE (Surdeanu et al., 2012), is one of
the state-of-the-art MIML relation extraction sys-
</bodyText>
<subsectionHeader confidence="0.999431">
4.3 Overall Performance
</subsectionHeader>
<bodyText confidence="0.999223468085107">
First we compare our framework and its vari-
ants with the baseline and the state-of-the-art RE
models. Following previous works, we use the
Precision-Recall curve as the evaluation criterion
in our experiment. The results are summarized
in Figure 2. For the constraints, we first manu-
ally select an average of 20 relation pairs for each
subcategory of the first kind of clues, and all the
relations with unique argument values in R. We
also show how automatically learnt clues perform
in Section 4.5.
Figure 2 shows that compared with the baseline,
our framework performs consistently better in the
DBpedia dataset and the Chinese dataset. Mintz++
proves to be a strong baseline on both datasets. It
tends to result in a high recall, and its weakness of
low precision is perfectly fixed by the ILP model.
Our ILP model and its variants all outperform
Mintz++ in precision in both datasets, indicating
that our approach helps filter out incorrect predic-
tions from the output of MaxEnt model. Com-
pared with MultiR, our framework obtains better
results in both datasets. Especially in the Chinese
dataset, the improvement in precision reaches as
high as 10-16% at the same recall points. Our
framework performs better compared to MIML-
RE in the English dataset. On the Chinese dataset,
our framework outperforms MIML-RE except in
the low-recall portion (&lt;10%) of the P-R curve.
All these results show that embedding the relation
background information into RE can help elim-
inate the wrong predictions and improve the re-
sults.
However, in the Riedel’s dataset, Mintz++, the
MaxEnt relation extractor, does not perform well,
and our framework cannot improve its perfor-
mance. In order to find out the reasons, we manu-
ally investigate the dataset. The top three relations
of this dataset are /location/location/contains,
/people/person/nationality and
/people/person/place lived. About two-thirds of
the entity tuples belongs to these three relations,
and the outputs of the local extractor usually
bias even more to the large relations. What is
worse, we cannot find any clues from the top
three relations because their arguments’ types are
too general. Things are similar for many other
</bodyText>
<figure confidence="0.987670194444445">
823
0.8
0.6
0.4
0.2
0 0.1 0.2 0.3 0.4 0.5
1
Mintz++
ILP−1cand
ILP−2cand
ILP
precision
0.8
0.6
0.4
0.2
0
0 0.1 0.2 0.3 0.4 0.5 0.6
1
Mintz++
ILP−1cand
ILP−2cand
ILP
precision
0.9
0.8
0.7
0.6
0.5
0.4
0.30 0.1 0.2 0.3 0.4 0.5 0.6
1
Mintz++
ILP−1cand
ILP−2cand
ILP
</figure>
<figureCaption confidence="0.9733975">
Figure 2: Overall performances of our framework and its variants, the baselines and the state-of-the-art
approaches on the three datasets.
</figureCaption>
<figure confidence="0.998279926829268">
recall recall recall
(a) The DBpedia Dataset (b) The Riedel’s Dataset (c) The Chinese Dataset
recall recall recall
(d) The DBpedia Dataset (e) The Riedel’s Dataset (f) The Chinese Dataset
precision
0.8
0.6
0.4
0.2
1
0.1 0.2 0.3 0.4 0.5
Mintz++
MultiR
MIML−RE
ILP
precision
0.8
0.6
0.4
0.2
0
0 0.1 0.2 0.3 0.4 0.5 0.6
1
Mintz++
MultiR
MIML−RE
ILP
precision
0.9
0.8
0.7
0.6
0.5
0.4
0.30 0.1 0.2 0.3 0.4 0.5 0.6
1
Mintz++
MultiR
MIML−RE
ILP
precision
</figure>
<bodyText confidence="0.999687392857143">
relations in this dataset. Although we may find
some clues any way, they are too few to make
any improvement. Hence, our framework does
not perform well due to the poor performance of
MaxEnt extractor and the lack of clues. To solve
this problem, we think of addressing the selection
preferences between relations and entities pro-
posed in (Riedel et al., 2013), which should be
our future work.
We notice that in all three datasets our variant
ILP-1cand is shorter than Mintz++ in recall, in-
dicating we may incorrectly discard some predic-
tions. Compared to ILP-2cand and original ILP,
ILP-1cand leads to slightly lower precision but
much lower recall, showing that selecting more
candidates may help us collect more potentially
correct predictions. Comparing ILP-2cand and
original ILP, the latter hardly makes any improve-
ment in precision, but is slightly longer in re-
call, indicating using three candidates can still col-
lect some more potentially correct predictions, al-
though the number may be limited.
In order to study how our framework improves
the performances on the DBpedia dataset and the
Chinese dataset, we further investigate the num-
ber of incorrect predictions eliminated by ILP and
the number of incorrect predictions corrected by
ILP. We also examine the number of correct pre-
</bodyText>
<tableCaption confidence="0.9377105">
Table 1: Details of the improvements made by ILP
in the DBpedia and Chinese datasets.
</tableCaption>
<table confidence="0.9966645">
Datasets Incorrect Predictions Wrong Predictioins Correct Predictions
Eliminated Corrected Newly Introduced
DBpedia 268 61 1426
Chinese 1506 14 283
</table>
<bodyText confidence="0.998864268292683">
dictions newly introduce by ILP, which were NA
in Mintz++. We summarize the results in Table 1.
The results show that our framework can reduce
the incorrect predictions and introduce more cor-
rect predictions at the same time. We also find
an interesting results: in the DBpedia dataset, ILP
is more likely to introduce correct predictions to
the results, while in the Chinese dataset it tends to
reduce more incorrect predictions, which may be
caused by the differences between performances
of Mintz++ on the two datasets, where it gets a
higher recall on the Chinese dataset.
Following Surdeanu et al. (2012), we also list
the peak F1 score (highest F1 score) for each
model in Table 2. Different from (Surdeanu et al.,
2012), we use all the entity pairs instead of the
ones with more than 10 mentions. We can observe
that our model obtains the best performance in the
DBpedia dataset and the Chinese dataset. In the
DBpedia dataset, it is 3.6% higher than Mintz++,
7.9% higher than MIML-RE and 13.9% higher
than MultiR. In the Chinese dataset, Mintz++,
MultiR and MIML-RE performs similarly in terms
of the highest F1 score, while our model gains
about 8% improvement. In the Riedel’s dataset,
our framework hardly obtains any improvement
compared with Mintz++.
We also investigate the impacts of the con-
straints used in ILP, which are derived based on the
two kinds of clues and can encode relation defini-
tion information into our framework. Experimen-
tal results in Table 2 shows that in the DBpedia
dataset, the highest F1 score increases from 35.2%
to 38.3% with the help of both kinds of clues,
while in the Chinese dataset the improvement is
from 44.4% to 52.8%. In the Riedel’s dataset we
do not see any improvements since there are al-
most no clues. Furthermore, using constraints de-
rived from only one kind of clues can also improve
the performance, but not as well as using both of
them.
</bodyText>
<subsectionHeader confidence="0.9701835">
4.4 Adapting MultiR Sentence Level
Extractor to Our Framework
</subsectionHeader>
<bodyText confidence="0.9369703125">
The preliminary relation extractor of our optimiza-
tion framework is not limited to the MaxEnt ex-
tractor, and can take any sentence level relation
extractor with confidence scores. We also fit Mul-
tiR’s mention level extractor into our framework.
As shown in Figure 3, in the DBpedia dataset
and the Chinese dataset, in most parts of the curve,
ILP optimized MultiR outperforms original Mul-
tiR. We think the reason is that our framework
make use of global clues to discard the incorrect
predictions. The results are not as high as when
we use MaxEnt as the preliminary extractor. We
think one reason is that MultiR does not perform
well in these two datasets. Furthermore, the confi-
dence scores which MultiR outputs are not nor-
malized to the same scale, which brings us dif-
ficulties in setting up a confidence threshold to
select the candidates. As a result, we only use
the top one result as the candidate since including
top two predictions without thresholding the confi-
dences performs bad, indicating that a probabilis-
tic sentence-level extractor is more suitable for our
framework. We also notice that in the Riedel’s
dataset our framework does not improve the per-
formance significantly, and we have discussed the
reasons in Section 4.3.
Figure 4: F1 score v.s. number of relations (used
to introduce the related learnt clues into the ILP
framework) on the DBpedia dataset (a) and the
Chinese dataset (b).
Figure 5: Performances of manually selected clues
and automatically learnt clues on two datasets.
</bodyText>
<subsectionHeader confidence="0.9049185">
4.5 Examining the Automatically Learnt
Clues
</subsectionHeader>
<bodyText confidence="0.9999369375">
Now we evaluate the performance of automati-
cally collected clues used in our model. Since
there are almost no clues in the Riedel’s dataset,
we only investigate the other two datasets. We add
clues according to their related relations’ propor-
tions in the local predictions. For example, Coun-
try and birthPlace take up about 30% in the local
predictions, we thus add clues that are related to
these two relations, and then move on with new
clues related to other relations according to those
relations’ proportions in the local predictions.
As is shown in Figure 4, in both datasets, the
clues related to more local predictions will solve
more inconsistencies, thus are more effective.
Adding the first two relations improves the model
significantly, and as more relations are added, the
</bodyText>
<figure confidence="0.977102818181818">
p
(b)
1
1
Manual
Auto
0.8
Manual
Auto
0.8
0.6
0.4
0.20 0.2 0.4 0.6
recall
(a) The DBpedia Dataset
0.6
0.4
0.20 0.2 0.4 0.6
recall
(b) The Chinese Dataset
s
825
</figure>
<tableCaption confidence="0.997185">
Table 2: Results of the highest F1 score on all three datasets.
</tableCaption>
<table confidence="0.996100777777778">
DBpedia Riedel Chinese
Method P(%) R(%) F1(%) P(%) R(%) F1(%) P(%) R(%) F1(%)
Mintz++ 40.2 30.5 34.7 35.3 23.2 27.9 43.3 45.7 44.4
MultiR 60.4 15.3 24.4 32.3 25.1 28.2 53.5 38.2 44.6
MIML-RE 51.3 21.6 30.4 41.5 19.9 26.9 49.2 41.3 44.9
ILP 37.4 39.2 38.3 35.5 23.2 28.0 52.6 52.9 52.8
ILP-No-Constraint 34.1 36.3 35.2 35.3 23.2 28.0 43.3 45.7 44.4
ILP-Type-Inconsistent 36.3 39.2 37.7 35.5 23.2 28.0 49.5 49.0 49.2
ILP-Cardinality 35.3 37.8 36.5 35.4 23.2 28.0 50.3 48.8 49.6
</table>
<figure confidence="0.953664333333334">
1
ILP Optimized MultiR
Original MultiR
0.9
0.8
0.7
0.60 0.05 0.1 0.15
1
0.9
0.8
0.7
0.6
0.5
0.4
0.30 0.05 0.1 0.15 0.2 0.25 0.3
ILP Optimized MultiR
Original MultiR
0.8
0.7
0.6
0.50 0.1 0.2 0.3 0.4
precision
precision
precision
1
0.9
ILP Optimized MultiR
Original MultiR
recall recall recall
(a) The DBpedia Dataset (b) The Riedel’s Dataset (c) The Chinese Dataset
</figure>
<figureCaption confidence="0.999949">
Figure 3: The results of original MultiR and ILP optimized MultiR on the three datasets.
</figureCaption>
<bodyText confidence="0.9998852">
performances keep increasing until approaching
the still state. It is worth mentioning that when
sufficient learnt clues are added into the model, the
results are comparable to those based on the clues
refined manually, as shown in Figure 5. This indi-
cates that the clues can be collected automatically,
and further used to examine whether predicted re-
lations are consistent with the existing ones in the
KB, which can be considered as a form of quality
control.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999965727272727">
In this paper, we make use of the global clues de-
rived from KB to help resolve the disagreements
among local relation predictions, thus reduce the
incorrect predictions and improve the performance
of relation extraction. Two kinds of clues, includ-
ing implicit argument type information and argu-
ment cardinality information of relations are in-
vestigated. Our framework outperforms the state-
of-the-art models if we can find such clues in the
KB. Furthermore, our framework is scalable for
other local sentence level extractors in addition to
the MaxEnt model. Finally, we show that the clues
can be learnt automatically from the KB, and lead
to comparable performance to manually refined
ones.
For future work, we will investigate other kinds
of clues and attempt a joint optimization frame-
work that could host entity disambiguation, rela-
tion extraction and entity linking together. We
will also adopt selection preference between en-
tities and relations since sometimes we may not
find useful clues.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999976888888889">
We would like to thank Heng Ji, Dong Wang and
Kun Xu for their useful discussions and the anony-
mous reviewers for their helpful comments which
greatly improved the work. This work was sup-
ported by the National High Technology R&amp;D
Program of China (Grant No. 2012AA011101),
National Natural Science Foundation of China
(Grant No. 61272344, 61202233, 61370055) and
the joint project with IBM Research.
</bodyText>
<sectionHeader confidence="0.99646" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999475166666667">
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of IJCAI, IJCAI’07, pages 2670–2676.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S¨oren Auer, Christian Becker, Richard Cyganiak,
</reference>
<page confidence="0.651267">
826
</page>
<reference confidence="0.999874711538462">
and Sebastian Hellmann. 2009. Dbpedia - a crys-
tallization point for the web of data. Web Semant.,
7:154–165, September.
Razvan C. Bunescu. 2007. Learning to extract rela-
tions from the web using minimal supervision. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL07.
Oier Lopez de Lacalle and Mirella Lapata. 2013. Un-
supervised relation extraction with general domain
knowledge. In EMNLP, pages 415–425. ACL.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 1535–1545, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th ACL-HLT - Volume 1, HLT ’11, pages
541–550, Stroudsburg, PA, USA. ACL.
Qi Li, Sam Anzaroot, Wen-Pin Lin, Xiang Li, and
Heng Ji. 2011. Joint inference for cross-document
information extraction. In Proceedings of the 20th
ACM International Conference on Information and
Knowledge Management, CIKM ’11, pages 2225–
2228, New York, NY, USA. ACM.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In ACL, pages 73–82. The Association for
Computer Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th IJCNLP of the AFNLP: Volume 2 -
Volume 2, ACL ’09, pages 1003–1011.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Machine Learning and Knowl-
edge Discovery in Databases, volume 6323 of Lec-
ture Notes in Computer Science, pages 148–163.
Springer Berlin / Heidelberg.
Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and
Andrew McCallum. 2013. Relation extraction with
matrix factorization and universal schemas. In Joint
Human Language Technology Conference/Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT-NAACL
’13), June.
Stephen Soderland, David Fisher, Jonathan Aseltine,
and Wendy Lehnert. 1995. Crystal inducing a con-
ceptual dictionary. In Proceedings of the 14th IJCAI
- Volume 2, IJCAI’95, pages 1314–1319, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Fabian Suchanek, James Fan, Raphael Hoffmann, Se-
bastian Riedel, and Partha Pratim Talukdar. 2013.
Advances in automated knowledge base construc-
tion. In SIGMOD Records journal, March.
Mihai Surdeanu, David McClosky, Julie Tibshirani,
John Bauer, Angel X. Chang, Valentin I. Spitkovsky,
and Christopher D. Manning. 2010. A simple dis-
tant supervision approach for the TAC-KBP slot fill-
ing task. In Proceedings of the Third Text Anal-
ysis Conference (TAC 2010), Gaithersburg, Mary-
land, USA, November.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallap-
ati, and Christopher D. Manning. 2012. Multi-
instance multi-label learning for relation extraction.
In EMNLP-CoNLL, pages 455–465. ACL.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
’12, pages 721–729, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of EMNLP,
EMNLP ’10, pages 1013–1023, Stroudsburg, PA,
USA. ACL.
Limin Yao, Sebastian Riedel, and Andrew McCal-
lum. 2012. Probabilistic databases of universal
schema. In Proceedings of the Joint Workshop on
Automatic Knowledge Base Construction and Web-
scale Knowledge Extraction, AKBC-WEKEX ’12,
pages 116–121, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun
Yan, Zheng Chen, and Zhifang Sui. 2013. Towards
accurate distant supervision for relational facts ex-
traction. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 810–815, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’05, pages 419–426, Stroudsburg, PA, USA.
Association for Computational Linguistics.
</reference>
<page confidence="0.895497">
827
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.467435">
<title confidence="0.991641">Encoding Relation Requirements for Relation Extraction via Joint Inference</title>
<author confidence="0.995257">Yansong Songfang Yong</author>
<affiliation confidence="0.999942">Peking University, Beijing,</affiliation>
<address confidence="0.493608">China Research Lab, Beijing,</address>
<email confidence="0.999734">huangsf,qinyong@cn.ibm.com</email>
<abstract confidence="0.998288227272727">Most existing relation extraction models make predictions for each entity pair locally and individually, while ignoring implicit global clues available in the knowledge base, sometimes leading to conflicts among local predictions from different entity pairs. In this paper, we propose a joint inference framework that utilizes these global clues to resolve disagreements among local predictions. We exploit two kinds of clues to generate constraints which can capture the implicit type and cardinality requirements of a relation. Experimental results on three datasets, in both English and Chinese, show that our framework outperforms the state-of-theart relation extraction models when such clues are applicable to the datasets. And, we find that the clues learnt automatically from existing knowledge bases perform comparably to those refined by human.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Michael J Cafarella</author>
<author>Stephen Soderland</author>
<author>Matt Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Open information extraction from the web.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI, IJCAI’07,</booktitle>
<pages>2670--2676</pages>
<contexts>
<context position="5377" citStr="Banko et al., 2007" startWordPosition="830" endWordPosition="833"> are applicable to the datasets. We also show that the automatically learnt clues perform comparably to those refined manually. In the rest of the paper, we first review related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between t</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open information extraction from the web. In Proceedings of IJCAI, IJCAI’07, pages 2670–2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Bizer</author>
<author>Jens Lehmann</author>
<author>Georgi Kobilarov</author>
<author>S¨oren Auer</author>
<author>Christian Becker</author>
<author>Richard Cyganiak</author>
<author>Sebastian Hellmann</author>
</authors>
<title>Dbpedia - a crystallization point for the web of data. Web Semant.,</title>
<date>2009</date>
<pages>7--154</pages>
<contexts>
<context position="22451" citStr="Bizer et al., 2009" startWordPosition="3665" endWordPosition="3668">ultiR and MIML4.1 Datasets RE so that they fit our datasets. We evaluate our approach on three datasets, including two English datasets and one Chinese dataset. The first English dataset, Riedel’s dataset, is the one used in (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), with the same split. It uses Freebase as the knowledge base and New York Time corpus as the text corpus, including about 60,000 entity tuples in the training set, and about 90,000 entity tuples in the testing set. We generate the second English dataset, DBpedia dataset, by mapping the triples in DBpedia (Bizer et al., 2009) to the sentences in New York Time corpus. We map 51 different relations to the corpus and result in about 50,000 entity tuples, 134,000 sentences for training and 30,000 entity tuples, 53,000 sentences for testing. For the Chinese dataset, we derive knowledge facts and construct a Chinese KB from the Infoboxes of HudongBaike, one of the largest Chinese online encyclopedias. We collect four national economic newspapers in 2009 as our corpus. 28 different relations are mapped to the corpus and this results in 60,000 entity tuples, 120,000 sentences for training and 40,000 tuples, 83,000 sentenc</context>
</contexts>
<marker>Bizer, Lehmann, Kobilarov, Auer, Becker, Cyganiak, Hellmann, 2009</marker>
<rawString>Christian Bizer, Jens Lehmann, Georgi Kobilarov, S¨oren Auer, Christian Becker, Richard Cyganiak, and Sebastian Hellmann. 2009. Dbpedia - a crystallization point for the web of data. Web Semant., 7:154–165, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
</authors>
<title>Learning to extract relations from the web using minimal supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL07.</booktitle>
<contexts>
<context position="5507" citStr="Bunescu, 2007" startWordPosition="852" endWordPosition="853">rest of the paper, we first review related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al</context>
</contexts>
<marker>Bunescu, 2007</marker>
<rawString>Razvan C. Bunescu. 2007. Learning to extract relations from the web using minimal supervision. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oier Lopez de Lacalle</author>
<author>Mirella Lapata</author>
</authors>
<title>Unsupervised relation extraction with general domain knowledge.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>415--425</pages>
<publisher>ACL.</publisher>
<marker>de Lacalle, Lapata, 2013</marker>
<rawString>Oier Lopez de Lacalle and Mirella Lapata. 2013. Unsupervised relation extraction with general domain knowledge. In EMNLP, pages 415–425. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1535--1545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5398" citStr="Fader et al., 2011" startWordPosition="834" endWordPosition="837">he datasets. We also show that the automatically learnt clues perform comparably to those refined manually. In the rest of the paper, we first review related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracte</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1535–1545, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledge-based weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th ACL-HLT - Volume 1, HLT ’11,</booktitle>
<pages>541--550</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5591" citStr="Hoffmann et al., 2011" startWordPosition="867" endWordPosition="870"> 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al. (2013) propose a universal schema which is a union of KB schemas and natural langu</context>
<context position="22100" citStr="Hoffmann et al., 2011" startWordPosition="3604" endWordPosition="3607">l information including MaxEnt confidence scores and the implicit relation backgrounds that are embedded into global consistencies of the entity tuples together. After the optimization problem is solved, we will obtain a list of selected candidate relations for each tuple, which will be our final output. 822 4 Experiments tems. We tune the models of MultiR and MIML4.1 Datasets RE so that they fit our datasets. We evaluate our approach on three datasets, including two English datasets and one Chinese dataset. The first English dataset, Riedel’s dataset, is the one used in (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), with the same split. It uses Freebase as the knowledge base and New York Time corpus as the text corpus, including about 60,000 entity tuples in the training set, and about 90,000 entity tuples in the testing set. We generate the second English dataset, DBpedia dataset, by mapping the triples in DBpedia (Bizer et al., 2009) to the sentences in New York Time corpus. We map 51 different relations to the corpus and result in about 50,000 entity tuples, 134,000 sentences for training and 30,000 entity tuples, 53,000 sentences for testing. For the Chinese dataset, we deriv</context>
<context position="23891" citStr="Hoffmann et al., 2011" startWordPosition="3906" endWordPosition="3910">edicts for each mention separately, and allows multi-label outputs for an entity tuple by OR-ing the outputs of its mentions. As we described in Section 3.1, originally we select the top three predicted relations as the candidates for each mention. In order to investigate whether it is necessary to use up to three candidates, we implement two variants of our approach, which select the top one and top two relations as candidates for each mention, and represented as ILP-1cand and ILP-2cand, respectively. We also use two distant supervision approaches for the comparison. The first one is MultiR (Hoffmann et al., 2011), a novel joint model that can deal with the relation overlap issue. The second one, MIML-RE (Surdeanu et al., 2012), is one of the state-of-the-art MIML relation extraction sys4.3 Overall Performance First we compare our framework and its variants with the baseline and the state-of-the-art RE models. Following previous works, we use the Precision-Recall curve as the evaluation criterion in our experiment. The results are summarized in Figure 2. For the constraints, we first manually select an average of 20 relation pairs for each subcategory of the first kind of clues, and all the relations w</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th ACL-HLT - Volume 1, HLT ’11, pages 541–550, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Sam Anzaroot</author>
<author>Wen-Pin Lin</author>
<author>Xiang Li</author>
<author>Heng Ji</author>
</authors>
<title>Joint inference for cross-document information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM International Conference on Information and Knowledge Management, CIKM ’11,</booktitle>
<pages>2225--2228</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7595" citStr="Li et al. (2011)" startWordPosition="1187" endWordPosition="1190"> only captures relation dependencies, while we learn implicit relation backgrounds from knowledge bases, including argument type and cardinality requirements. Riedel et al. (2013) propose to use latent vectors to estimate the preferences between relations and entities. These can be considered as the latent type information of the relations’ arguments, which is learnt from various data sources. In contrast, our approach learn implicit clues from existing KBs, and jointly optimize local predictions among different entity tuples to capture both relation argument type clues and cardinality clues. Li et al. (2011) and Li et al. (2013) use co-occurring statistics among relations or events to jointly improve information extraction performances in ACE tasks, while we mine existing KBs to collect global clues to solve local conflicts and find the optimal aggregation assignments, regarding existing knowledge facts. de Lacalle and Lapata (2013) encode general domain knowledge as FOL rules in a topic model while our instantiated constraints are directly operated in an ILP model. Zhang et al. (2013) utilize relation cardinality to create negative samples for distant supervision while we use both implicit type </context>
</contexts>
<marker>Li, Anzaroot, Lin, Li, Ji, 2011</marker>
<rawString>Qi Li, Sam Anzaroot, Wen-Pin Lin, Xiang Li, and Heng Ji. 2011. Joint inference for cross-document information extraction. In Proceedings of the 20th ACM International Conference on Information and Knowledge Management, CIKM ’11, pages 2225– 2228, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Li</author>
<author>Heng Ji</author>
<author>Liang Huang</author>
</authors>
<title>Joint event extraction via structured prediction with global features.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>73--82</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="7616" citStr="Li et al. (2013)" startWordPosition="1192" endWordPosition="1195">on dependencies, while we learn implicit relation backgrounds from knowledge bases, including argument type and cardinality requirements. Riedel et al. (2013) propose to use latent vectors to estimate the preferences between relations and entities. These can be considered as the latent type information of the relations’ arguments, which is learnt from various data sources. In contrast, our approach learn implicit clues from existing KBs, and jointly optimize local predictions among different entity tuples to capture both relation argument type clues and cardinality clues. Li et al. (2011) and Li et al. (2013) use co-occurring statistics among relations or events to jointly improve information extraction performances in ACE tasks, while we mine existing KBs to collect global clues to solve local conflicts and find the optimal aggregation assignments, regarding existing knowledge facts. de Lacalle and Lapata (2013) encode general domain knowledge as FOL rules in a topic model while our instantiated constraints are directly operated in an ILP model. Zhang et al. (2013) utilize relation cardinality to create negative samples for distant supervision while we use both implicit type clues and relation ca</context>
</contexts>
<marker>Li, Ji, Huang, 2013</marker>
<rawString>Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In ACL, pages 73–82. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>1003--1011</pages>
<contexts>
<context position="5527" citStr="Mintz et al., 2009" startWordPosition="854" endWordPosition="857">er, we first review related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al. (2013) propose a u</context>
<context position="23255" citStr="Mintz et al. (2009)" startWordPosition="3801" endWordPosition="3804"> 53,000 sentences for testing. For the Chinese dataset, we derive knowledge facts and construct a Chinese KB from the Infoboxes of HudongBaike, one of the largest Chinese online encyclopedias. We collect four national economic newspapers in 2009 as our corpus. 28 different relations are mapped to the corpus and this results in 60,000 entity tuples, 120,000 sentences for training and 40,000 tuples, 83,000 sentences for testing. 4.2 Baselines and Competitors The baseline we use in this paper is Mintz++, which is described in (Surdeanu et al., 2012). It is a modification of the model proposed by Mintz et al. (2009). The model predicts for each mention separately, and allows multi-label outputs for an entity tuple by OR-ing the outputs of its mentions. As we described in Section 3.1, originally we select the top three predicted relations as the candidates for each mention. In order to investigate whether it is necessary to use up to three candidates, we implement two variants of our approach, which select the top one and top two relations as candidates for each mention, and represented as ILP-1cand and ILP-2cand, respectively. We also use two distant supervision approaches for the comparison. The first o</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP: Volume 2 -Volume 2, ACL ’09, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases,</booktitle>
<volume>6323</volume>
<pages>148--163</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="22077" citStr="Riedel et al., 2010" startWordPosition="3600" endWordPosition="3603"> can combine the local information including MaxEnt confidence scores and the implicit relation backgrounds that are embedded into global consistencies of the entity tuples together. After the optimization problem is solved, we will obtain a list of selected candidate relations for each tuple, which will be our final output. 822 4 Experiments tems. We tune the models of MultiR and MIML4.1 Datasets RE so that they fit our datasets. We evaluate our approach on three datasets, including two English datasets and one Chinese dataset. The first English dataset, Riedel’s dataset, is the one used in (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), with the same split. It uses Freebase as the knowledge base and New York Time corpus as the text corpus, including about 60,000 entity tuples in the training set, and about 90,000 entity tuples in the testing set. We generate the second English dataset, DBpedia dataset, by mapping the triples in DBpedia (Bizer et al., 2009) to the sentences in New York Time corpus. We map 51 different relations to the corpus and result in about 50,000 entity tuples, 134,000 sentences for training and 30,000 entity tuples, 53,000 sentences for testing. For the Ch</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Machine Learning and Knowledge Discovery in Databases, volume 6323 of Lecture Notes in Computer Science, pages 148–163. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Benjamin M Marlin</author>
<author>Andrew McCallum</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’13),</booktitle>
<contexts>
<context position="6115" citStr="Riedel et al. (2013)" startWordPosition="954" endWordPosition="957">unescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al. (2013) propose a universal schema which is a union of KB schemas and natural language patterns, making it possible to integrate the unlimited set of uncanonicalized relations in open settings with the relations in existing KBs. As far as we know, few works have managed to take the relation specific requirements for arguments into account, and most existing works make predictions locally and individually. The MultiR system allows entity tuples to have more than one relations, but still predicts each entity tuple locally (Hoffmann et al., 2011). Surdeanu et al. (2012) propose a two-layer multi-instanc</context>
<context position="27480" citStr="Riedel et al., 2013" startWordPosition="4507" endWordPosition="4510">precision 0.8 0.6 0.4 0.2 1 0.1 0.2 0.3 0.4 0.5 Mintz++ MultiR MIML−RE ILP precision 0.8 0.6 0.4 0.2 0 0 0.1 0.2 0.3 0.4 0.5 0.6 1 Mintz++ MultiR MIML−RE ILP precision 0.9 0.8 0.7 0.6 0.5 0.4 0.30 0.1 0.2 0.3 0.4 0.5 0.6 1 Mintz++ MultiR MIML−RE ILP precision relations in this dataset. Although we may find some clues any way, they are too few to make any improvement. Hence, our framework does not perform well due to the poor performance of MaxEnt extractor and the lack of clues. To solve this problem, we think of addressing the selection preferences between relations and entities proposed in (Riedel et al., 2013), which should be our future work. We notice that in all three datasets our variant ILP-1cand is shorter than Mintz++ in recall, indicating we may incorrectly discard some predictions. Compared to ILP-2cand and original ILP, ILP-1cand leads to slightly lower precision but much lower recall, showing that selecting more candidates may help us collect more potentially correct predictions. Comparing ILP-2cand and original ILP, the latter hardly makes any improvement in precision, but is slightly longer in recall, indicating using three candidates can still collect some more potentially correct pre</context>
</contexts>
<marker>Riedel, Yao, Marlin, McCallum, 2013</marker>
<rawString>Sebastian Riedel, Limin Yao, Benjamin M. Marlin, and Andrew McCallum. 2013. Relation extraction with matrix factorization and universal schemas. In Joint Human Language Technology Conference/Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL ’13), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
<author>David Fisher</author>
<author>Jonathan Aseltine</author>
<author>Wendy Lehnert</author>
</authors>
<title>Crystal inducing a conceptual dictionary.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th IJCAI - Volume 2, IJCAI’95,</booktitle>
<pages>1314--1319</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="5202" citStr="Soderland et al., 1995" startWordPosition="804" endWordPosition="807">r and evaluate our framework on English and Chinese datasets. The experimental results show that our framework performs better than the state-of-the-art approaches when such clues are applicable to the datasets. We also show that the automatically learnt clues perform comparably to those refined manually. In the rest of the paper, we first review related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training dataset</context>
</contexts>
<marker>Soderland, Fisher, Aseltine, Lehnert, 1995</marker>
<rawString>Stephen Soderland, David Fisher, Jonathan Aseltine, and Wendy Lehnert. 1995. Crystal inducing a conceptual dictionary. In Proceedings of the 14th IJCAI - Volume 2, IJCAI’95, pages 1314–1319, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Suchanek</author>
<author>James Fan</author>
<author>Raphael Hoffmann</author>
<author>Sebastian Riedel</author>
</authors>
<title>and Partha Pratim Talukdar.</title>
<date>2013</date>
<booktitle>In SIGMOD Records journal,</booktitle>
<contexts>
<context position="1330" citStr="Suchanek et al., 2013" startWordPosition="185" endWordPosition="188">e exploit two kinds of clues to generate constraints which can capture the implicit type and cardinality requirements of a relation. Experimental results on three datasets, in both English and Chinese, show that our framework outperforms the state-of-theart relation extraction models when such clues are applicable to the datasets. And, we find that the clues learnt automatically from existing knowledge bases perform comparably to those refined by human. 1 Introduction Identifying predefined kinds of relationship between pairs of entities is crucial for many knowledge base related applications(Suchanek et al., 2013). In the literature, relation extraction (RE) is usually investigated in a classification style, where relations are simply treated as isolated class labels, while their definitions or background information are sometimes ignored. Take the relation Capital as an example, we can imagine that this relation will expect a country as its subject and a city as object, and in most cases, a city can be the capital of only one country. All these clues are no doubt helpful, for instance, Yao et al. (2010) explicitly modeled the expected types of a relation’s arguments with the help of Freebase’s type ta</context>
</contexts>
<marker>Suchanek, Fan, Hoffmann, Riedel, 2013</marker>
<rawString>Fabian Suchanek, James Fan, Raphael Hoffmann, Sebastian Riedel, and Partha Pratim Talukdar. 2013. Advances in automated knowledge base construction. In SIGMOD Records journal, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>David McClosky</author>
<author>Julie Tibshirani</author>
<author>John Bauer</author>
<author>Angel X Chang</author>
<author>Valentin I Spitkovsky</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple distant supervision approach for the TAC-KBP slot filling task.</title>
<date>2010</date>
<booktitle>In Proceedings of the Third Text Analysis Conference (TAC 2010),</booktitle>
<location>Gaithersburg, Maryland, USA,</location>
<contexts>
<context position="5568" citStr="Surdeanu et al., 2010" startWordPosition="862" endWordPosition="866">ction 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al. (2013) propose a universal schema which is a union of KB sc</context>
</contexts>
<marker>Surdeanu, McClosky, Tibshirani, Bauer, Chang, Spitkovsky, Manning, 2010</marker>
<rawString>Mihai Surdeanu, David McClosky, Julie Tibshirani, John Bauer, Angel X. Chang, Valentin I. Spitkovsky, and Christopher D. Manning. 2010. A simple distant supervision approach for the TAC-KBP slot filling task. In Proceedings of the Third Text Analysis Conference (TAC 2010), Gaithersburg, Maryland, USA, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multiinstance multi-label learning for relation extraction.</title>
<date>2012</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>455--465</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="5615" citStr="Surdeanu et al., 2012" startWordPosition="871" endWordPosition="875">mework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al. (2013) propose a universal schema which is a union of KB schemas and natural language patterns, making it </context>
<context position="22124" citStr="Surdeanu et al., 2012" startWordPosition="3608" endWordPosition="3611"> MaxEnt confidence scores and the implicit relation backgrounds that are embedded into global consistencies of the entity tuples together. After the optimization problem is solved, we will obtain a list of selected candidate relations for each tuple, which will be our final output. 822 4 Experiments tems. We tune the models of MultiR and MIML4.1 Datasets RE so that they fit our datasets. We evaluate our approach on three datasets, including two English datasets and one Chinese dataset. The first English dataset, Riedel’s dataset, is the one used in (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012), with the same split. It uses Freebase as the knowledge base and New York Time corpus as the text corpus, including about 60,000 entity tuples in the training set, and about 90,000 entity tuples in the testing set. We generate the second English dataset, DBpedia dataset, by mapping the triples in DBpedia (Bizer et al., 2009) to the sentences in New York Time corpus. We map 51 different relations to the corpus and result in about 50,000 entity tuples, 134,000 sentences for training and 30,000 entity tuples, 53,000 sentences for testing. For the Chinese dataset, we derive knowledge facts and co</context>
<context position="24007" citStr="Surdeanu et al., 2012" startWordPosition="3927" endWordPosition="3930">mentions. As we described in Section 3.1, originally we select the top three predicted relations as the candidates for each mention. In order to investigate whether it is necessary to use up to three candidates, we implement two variants of our approach, which select the top one and top two relations as candidates for each mention, and represented as ILP-1cand and ILP-2cand, respectively. We also use two distant supervision approaches for the comparison. The first one is MultiR (Hoffmann et al., 2011), a novel joint model that can deal with the relation overlap issue. The second one, MIML-RE (Surdeanu et al., 2012), is one of the state-of-the-art MIML relation extraction sys4.3 Overall Performance First we compare our framework and its variants with the baseline and the state-of-the-art RE models. Following previous works, we use the Precision-Recall curve as the evaluation criterion in our experiment. The results are summarized in Figure 2. For the constraints, we first manually select an average of 20 relation pairs for each subcategory of the first kind of clues, and all the relations with unique argument values in R. We also show how automatically learnt clues perform in Section 4.5. Figure 2 shows </context>
<context position="29255" citStr="Surdeanu et al. (2012)" startWordPosition="4790" endWordPosition="4793"> 1506 14 283 dictions newly introduce by ILP, which were NA in Mintz++. We summarize the results in Table 1. The results show that our framework can reduce the incorrect predictions and introduce more correct predictions at the same time. We also find an interesting results: in the DBpedia dataset, ILP is more likely to introduce correct predictions to the results, while in the Chinese dataset it tends to reduce more incorrect predictions, which may be caused by the differences between performances of Mintz++ on the two datasets, where it gets a higher recall on the Chinese dataset. Following Surdeanu et al. (2012), we also list the peak F1 score (highest F1 score) for each model in Table 2. Different from (Surdeanu et al., 2012), we use all the entity pairs instead of the ones with more than 10 mentions. We can observe that our model obtains the best performance in the DBpedia dataset and the Chinese dataset. In the DBpedia dataset, it is 3.6% higher than Mintz++, 7.9% higher than MIML-RE and 13.9% higher than MultiR. In the Chinese dataset, Mintz++, MultiR and MIML-RE performs similarly in terms of the highest F1 score, while our model gains about 8% improvement. In the Riedel’s dataset, our framework</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning. 2012. Multiinstance multi-label learning for relation extraction. In EMNLP-CoNLL, pages 455–465. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shingo Takamatsu</author>
<author>Issei Sato</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Reducing wrong labels in distant supervision for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>721--729</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5947" citStr="Takamatsu et al., 2012" startWordPosition="926" endWordPosition="929">upervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al. (2013) propose a universal schema which is a union of KB schemas and natural language patterns, making it possible to integrate the unlimited set of uncanonicalized relations in open settings with the relations in existing KBs. As far as we know, few works have managed to take the relation specific requirements for arguments into account, and most existing works make predictions locally and individually. The MultiR system allows entit</context>
</contexts>
<marker>Takamatsu, Sato, Nakagawa, 2012</marker>
<rawString>Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 721–729, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Collective cross-document relation extraction without labelled data.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP, EMNLP ’10,</booktitle>
<pages>1013--1023</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1830" citStr="Yao et al. (2010)" startWordPosition="268" endWordPosition="271">relationship between pairs of entities is crucial for many knowledge base related applications(Suchanek et al., 2013). In the literature, relation extraction (RE) is usually investigated in a classification style, where relations are simply treated as isolated class labels, while their definitions or background information are sometimes ignored. Take the relation Capital as an example, we can imagine that this relation will expect a country as its subject and a city as object, and in most cases, a city can be the capital of only one country. All these clues are no doubt helpful, for instance, Yao et al. (2010) explicitly modeled the expected types of a relation’s arguments with the help of Freebase’s type taxonomy and obtained promising results for RE. ∗Yansong Feng is the corresponding author. However, properly capturing and utilizing such typing clues are not trivial. One of the hurdles here is the lack of off-the-shelf resources and such clues often have to be coded by human experts. Many knowledge bases do not have a well-defined typing system, let alone fine-grained typing taxonomies with corresponding type recognizers, which are crucial to explicitly model the typing requirements for argument</context>
<context position="5545" citStr="Yao et al., 2010" startWordPosition="858" endWordPosition="861">related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al. (2013) propose a universal schema wh</context>
</contexts>
<marker>Yao, Riedel, McCallum, 2010</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2010. Collective cross-document relation extraction without labelled data. In Proceedings of EMNLP, EMNLP ’10, pages 1013–1023, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Probabilistic databases of universal schema.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Webscale Knowledge Extraction, AKBC-WEKEX ’12,</booktitle>
<pages>116--121</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6090" citStr="Yao et al. (2012)" startWordPosition="949" endWordPosition="952">ted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain noises, there are also research efforts focusing on reducing the noisy labels in the training data (Takamatsu et al., 2012). To bridge the gaps between the relations extracted from open information extraction and the canonicalized relations in KBs, Yao et al. (2012) and Riedel et al. (2013) propose a universal schema which is a union of KB schemas and natural language patterns, making it possible to integrate the unlimited set of uncanonicalized relations in open settings with the relations in existing KBs. As far as we know, few works have managed to take the relation specific requirements for arguments into account, and most existing works make predictions locally and individually. The MultiR system allows entity tuples to have more than one relations, but still predicts each entity tuple locally (Hoffmann et al., 2011). Surdeanu et al. (2012) propose </context>
</contexts>
<marker>Yao, Riedel, McCallum, 2012</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2012. Probabilistic databases of universal schema. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Webscale Knowledge Extraction, AKBC-WEKEX ’12, pages 116–121, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xingxing Zhang</author>
<author>Jianwen Zhang</author>
<author>Junyu Zeng</author>
<author>Jun Yan</author>
<author>Zheng Chen</author>
<author>Zhifang Sui</author>
</authors>
<title>Towards accurate distant supervision for relational facts extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>810--815</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="8082" citStr="Zhang et al. (2013)" startWordPosition="1265" endWordPosition="1268">local predictions among different entity tuples to capture both relation argument type clues and cardinality clues. Li et al. (2011) and Li et al. (2013) use co-occurring statistics among relations or events to jointly improve information extraction performances in ACE tasks, while we mine existing KBs to collect global clues to solve local conflicts and find the optimal aggregation assignments, regarding existing knowledge facts. de Lacalle and Lapata (2013) encode general domain knowledge as FOL rules in a topic model while our instantiated constraints are directly operated in an ILP model. Zhang et al. (2013) utilize relation cardinality to create negative samples for distant supervision while we use both implicit type clues and relation cardinality expectations to discover possible inconsistencies among local predictions. 3 The Framework Our framework takes a set of entity pairs and their supporting sentences as its input. We first train a preliminary sentence level extractor which can output confidence scores for its predictions, e.g., a maximum entropy or logistic regression model, and use this local extractor to produce local predictions. In order to implicitly capture the expected type and ca</context>
</contexts>
<marker>Zhang, Zhang, Zeng, Yan, Chen, Sui, 2013</marker>
<rawString>Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun Yan, Zheng Chen, and Zhifang Sui. 2013. Towards accurate distant supervision for relational facts extraction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 810–815, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubin Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>419--426</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5228" citStr="Zhao and Grishman, 2005" startWordPosition="808" endWordPosition="812">work on English and Chinese datasets. The experimental results show that our framework performs better than the state-of-the-art approaches when such clues are applicable to the datasets. We also show that the automatically learnt clues perform comparably to those refined manually. In the rest of the paper, we first review related work in Section 2, and in Section 3, we describe our framework in detail. Experimental setup and results are discussed in Section 4. We conclude this paper in Section 5. 2 Related Work Since traditional supervised relation extraction methods (Soderland et al., 1995; Zhao and Grishman, 2005) require manual annotations and are often domain-specific, nowadays many efforts focus on semi-supervised or unsupervised methods (Banko et al., 2007; Fader et al., 2011). Distant supervision (DS) is a semi-supervised RE framework and has attracted many attentions (Bunescu, 2007; Mintz et al., 2009; Yao et al., 2010; Surdeanu et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). DS approaches can predict canonicalized (predefined in KBs) relations for large amount of data and do not need much human involvement. Since the automatically generated training datasets in DS often contain nois</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Shubin Zhao and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 419–426, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>