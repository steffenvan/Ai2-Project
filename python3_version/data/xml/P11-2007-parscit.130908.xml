<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.049125">
<title confidence="0.998722">
The Arabic Online Commentary Dataset:
an Annotated Dataset of Informal Arabic with High Dialectal Content
</title>
<author confidence="0.933185">
Omar F. Zaidan and Chris Callison-Burch
</author>
<affiliation confidence="0.99272">
Dept. of Computer Science, Johns Hopkins University
</affiliation>
<address confidence="0.801991">
Baltimore, MD 21218, USA
</address>
<email confidence="0.999744">
{ozaidan,ccb}@cs.jhu.edu
</email>
<sectionHeader confidence="0.997401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999813764705883">
The written form of Arabic, Modern Standard
Arabic (MSA), differs quite a bit from the
spoken dialects of Arabic, which are the true
“native” languages of Arabic speakers used in
daily life. However, due to MSA’s prevalence
in written form, almost all Arabic datasets
have predominantly MSA content. We present
the Arabic Online Commentary Dataset, a
52M-word monolingual dataset rich in dialec-
tal content, and we describe our long-term an-
notation effort to identify the dialect level (and
dialect itself) in each sentence of the dataset.
So far, we have labeled 108K sentences, 41%
of which as having dialectal content. We also
present experimental results on the task of au-
tomatic dialect identification, using the col-
lected labels for training and evaluation.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999586875">
The Arabic language is characterized by an interest-
ing linguistic dichotomy, whereby the written form
of the language, Modern Standard Arabic (MSA),
differs in a non-trivial fashion from the various spo-
ken varieties of Arabic. As the variant of choice for
written and official communication, MSA content
significantly dominates dialectal content, and in turn
MSA dominates in datasets available for linguistic
research, especially in textual form.
The abundance of MSA data has greatly aided re-
search on computational methods applied to Arabic,
but only the MSA variant of it. A state-of-the-art
Arabic-to-English machine translation system per-
forms quite well when translating MSA source sen-
tences, but often produces incomprehensible output
when the input is dialectal. For example, most words
</bodyText>
<page confidence="0.992769">
37
</page>
<figureCaption confidence="0.741045333333333">
Figure 1: Two roughly equivalent Arabic sentences, one
in MSA and one in Levantine Arabic, translated by the
same MT system into English. An acceptable translation
would be When will we see this group of criminals un-
dergo trial (or tried)?. The MSA variant is handled well,
while the dialectal variant is mostly transliterated.
</figureCaption>
<bodyText confidence="0.9998539375">
of the dialectal sentence of Figure 1 are transliter-
ated.1 Granted, it is conceivable that processing di-
alectal content is more difficult than MSA, but the
main problem is the lack of dialectal training data.2
In this paper, we present our efforts to create
a dataset of dialectal Arabic, the Arabic Online
Commentary Dataset, by extracting reader com-
mentary from the online versions of three Arabic
newspapers, which have a high degree (about half)
of dialectal content (Levantine, Gulf, and Egyptian).
Furthermore, we describe a long-term crowdsourced
effort to have the sentences labeled by Arabic speak-
ers for the level of dialect in each sentence and the
dialect itself. Finally, we present experimental re-
sults on the task of automatic dialect classification
with systems trained on the collected dialect labels.
</bodyText>
<footnote confidence="0.9996024">
1The high transliteration rate is somewhat alarming, as the
first two words of the sentence are relatively frequent: AymtY
means ‘when’ and rH corresponds to the modal ‘will’.
2It can in fact be argued that MSA is the variant with the
more complex sentence structure and richer morphology.
</footnote>
<note confidence="0.927699">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 37–41,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<figureCaption confidence="0.978675">
Figure 2: One possible breakdown of spoken Arabic into
dialect groups: Maghrebi, Egyptian, Levantine, Gulf, and
Iraqi. Habash (2010) also gives a very similar breakdown.
</figureCaption>
<table confidence="0.999862888888889">
News Source Al-Ghad Al-Riyadh Al-Youm
Al-Sabe’
# articles 6.30K 34.2K 45.7K
# comments 26.6K 805K 565K
# sentences 63.3K 1,686K 1,384K
# words 1.24M 18.8M 32.1M
comments/article 4.23 23.56 12.37
sentences/comment 2.38 2.09 2.45
words/sentence 19.51 11.14 23.22
</table>
<tableCaption confidence="0.988794666666667">
Table 1: A summary of the different components of the
AOC dataset. Overall, 1.4M comments were harvested
from 86.1K articles, corresponding to 52.1M words.
</tableCaption>
<listItem confidence="0.810635">
• The prevalence of dialectal Arabic.
</listItem>
<sectionHeader confidence="0.991057" genericHeader="method">
2 The AOC Dataset
</sectionHeader>
<bodyText confidence="0.997969434782609">
Arabic is the official language in over 20 countries,
spoken by more than 250 million people. The of-
ficial status only refers to a written form of Arabic
known as Modern Standard Arabic (MSA). The spo-
ken dialects of Arabic (Figure 2) differ quite a bit
from MSA and from each other. The dominance of
MSA in available Arabic text makes dialectal Arabic
datasets hard to come by.3
We set out to create a dataset of dialectal Ara-
bic to address this need. The most viable re-
source of dialectal Arabic text is online data, which
is more individual-driven and less institutionalized,
and therefore more likely to contain dialectal con-
tent. Possible sources of dialectal text include we-
blogs, forums, and chat transcripts. However, we-
blogs usually contain relatively little data, and a
writer might use dialect in their writing only occa-
sionaly, forums usually have content that is of little
interest or relevance to actual applications, and chat
transcripts are difficult to obtain and extract.
We instead diverted our attention to online com-
mentary by readers of online content. This source
of data has several advantages:
</bodyText>
<listItem confidence="0.999791">
• A large amount of data, with more data becom-
ing available on a daily basis.
• The data is publicly accessible, exists in a struc-
tured, consistent format, and is easy to extract.
• A high level of topic relevance.
</listItem>
<footnote confidence="0.435001666666667">
3The problem is somewhat mitigated in the speech domain,
since dialectal data exists in the form of phone conversations
and television program recordings.
</footnote>
<bodyText confidence="0.99924975">
The Arabic Online Commentary dataset that we
created was based on reader commentary from the
online versions of three Arabic newspapers: Al-
Ghad from Jordan, Al-Riyadh from Saudi Arabia,
and Al-Youm Al-Sabe’ from Egypt.4 The common
dialects in those countries are Levantine, Gulf, and
Egyptian, respectively.
We crawled webpages corresponding to articles
published during a roughly-6-month period, cover-
ing early April 2010 to early October 2010. This
resulted in crawling about 150K URL’s, 86.1K of
which included reader commentary (Table 1). The
data consists of 1.4M comments, corresponding to
52.1M words.
We also extract the following information for each
comment, whenever available:
</bodyText>
<listItem confidence="0.999788">
• The URL of the relevant newspaper article.
• The date and time of the comment.
• The author ID associated with the comment.5
• The subtitle header.5
• The author’s e-mail address.5
• The author’s geographical location.5
</listItem>
<bodyText confidence="0.990315">
The AOC dataset (and the dialect labels of Sec-
tion 3) is fully documented and publicly available.6
</bodyText>
<footnote confidence="0.998709">
4URL’s: www.alghad.com, www.alriyadh.com, and
www.youm7.com.
5These fields are provided by the author.
6Data URL: http://cs.jhu.edu/˜ozaidan/AOC/.
The release also includes all sentences from articles in the 150K
crawled webpages.
</footnote>
<page confidence="0.998087">
38
</page>
<sectionHeader confidence="0.9709385" genericHeader="method">
3 Augmenting the AOC with Dialect
Labels
</sectionHeader>
<bodyText confidence="0.9723682">
We have started an ongoing effort to have each sen-
tence in the AOC dataset labeled with dialect labels.
For each sentence, we would like to know whether
or not it has dialectal content, how much dialect
there is, and which variant of Arabic it is. Having
those labels would greatly aid researchers interested
in dialect by helping them focus on the sentences
identified as having dialectal content.
N
o
</bodyText>
<subsectionHeader confidence="0.999091">
3.1 Amazon’s Mechanical Turk
</subsectionHeader>
<bodyText confidence="0.99998025">
The dialect labeling task requires knowledge of Ara-
bic at a native level. To gain access to native Arabic
speakers, and a large number of them, we crowd-
sourced the annotation task to Amazon’s Mechani-
cal Turk (MTurk), an online marketplace that allows
“Requesters” to create simple tasks requiring human
knowledge, and have them completed by “Workers”
from all over the world.
</bodyText>
<subsectionHeader confidence="0.997228">
3.2 The Annotation Task
</subsectionHeader>
<bodyText confidence="0.999978214285714">
Of the 3.1M available sentences, we selected a
‘small’ subset of 142,530 sentences to be labeled by
MTurk Workers.7 We kept the annotation instruc-
tions relatively simple, augmenting them with the
map from Figure 2 (with the Arabic names of the
dialects) to illustrate the different dialect classes.
The sentences were randomly grouped into
14,253 sets of 10 sentences each. When a Worker
chooses to perform our task, they are shown the 10
sentences of some random set, on a single HTML
page. For each sentence, they indicate the level of
dialectal Arabic, and which dialect it is (if any). We
offer a reward of $0.05 per screen, and request each
one be completed by three distinct Workers.
</bodyText>
<subsectionHeader confidence="0.994534">
3.3 Quality Control
</subsectionHeader>
<bodyText confidence="0.999872166666667">
To ensure high annotation quality, we insert two ad-
ditional control sentences into each screen, taken
from the article bodies. Such sentences are almost
always in MSA Arabic. Hence, a careless Worker
can be easily identified if they label many control
sentences as having dialect in them.
</bodyText>
<footnote confidence="0.98875025">
7There are far fewer sentences available from Al-Ghad than
the other two sources (fourth line of Table 1). We have taken
this imbalance into accout and heavily oversampled Al-Ghad
sentences when choosing sentences to be labeled.
</footnote>
<table confidence="0.999937333333333">
# MSA # words # dialectal # words
sentences sentences
Al-Ghad 18,947 409K 11,350 240K
Al-Riyadh 31,096 378K 20,741 288K
Al-Youm Al-Sabe’ 13,512 334K 12,527 327K
ALL 63,555 1,121K 44,618 855K
</table>
<tableCaption confidence="0.997455">
Table 2: A breakdown of sentences for which &gt; 2 anno-
tators agreed on whether dialectal content exists or not.
</tableCaption>
<bodyText confidence="0.999931461538461">
Another effective method to judge a Worker’s
quality of work is to examine their label distribution
within each news source. For instance, within the
sentences from Al-Youm Al-Sabe’, most sentences
judged as having dialectal content should be clas-
sified as Egyptian. A similar strong prior exists for
Levantine within Al-Ghad sentences, and for Gulf
within Al-Riyadh sentences.
Using those two criteria, there is a very clear
distinction between Workers who are faithful and
those who are not (mostly spammers), and 13.8%
of assignments are rejected on these grounds and re-
posted to MTurk.
</bodyText>
<subsectionHeader confidence="0.981552">
3.4 Dataset Statistics
</subsectionHeader>
<bodyText confidence="0.999960333333333">
We have been collecting labels from MTurk for a pe-
riod of about four and a half months. In that period,
11,031 HITs were performed to completion (cor-
responding to 110,310 sentences, each labeled by
three distinct annotators). Overall, 455 annotators
took part, 63 of whom judged at least 50 screens.
Our most prolific annotator completed over 6,000
screens, with the top 25 annotators supplying about
80% of the labels, and the top 50 annotators supply-
ing about 90% of the labels.
We consider a sentence to be dialectal if it is la-
beled as such by at least two annotators. Similarly,
a sentence is considered to be MSA if it has at least
two MSA labels. For a small set of sentences (2%),
no such agreement existed, and those sentences were
discarded (they are mostly sentences identified as
being non-Arabic). Table 2 shows a breakdown of
the rest of the sentences.8
</bodyText>
<footnote confidence="0.997583">
8Data URL: http://cs.jhu.edu/˜ozaidan/
RCLMT/.
</footnote>
<page confidence="0.995199">
39
</page>
<table confidence="0.999215285714286">
Classification Task Accuracy (%) Precision (%) Recall (%)
Al-Ghad MSA vs. LEV 79.6 70.6 78.2
Al-Riyadh MSA vs. GLF 75.1 66.9 74.6
Al-Youm Al-Sabe’ MSA vs. EGY 80.9 77.7 84.4
MSA vs. dialect 77.8 71.2 77.6
LEV vs. GLF vs. EGY 83.5 N/A N/A
MSA vs. LEV vs. GLF vs. EGY 69.4 N/A N/A
</table>
<tableCaption confidence="0.9105355">
Table 3: Accuracy, dialect precision, and dialect recall
(10-fold cross validation) for various classification tasks.
</tableCaption>
<sectionHeader confidence="0.99495" genericHeader="method">
4 Automatic Dialect Classification
</sectionHeader>
<bodyText confidence="0.999948903225806">
One can think of dialect classification as a lan-
guage identification task, and techniques for lan-
guage identification can be applied to dialect clas-
sification. We use the collected labels to investigate
how well a machine learner can distinguish dialectal
Arabic from MSA, and how well it can distinguish
between the different Arabic dialects.
We experiment with a language modeling ap-
proach. In a classification task with c classes, we
build c language models, one per class. At test
time, we score a test sentence with all c models,
and choose the class label of the model assigning
the highest score (i.e. lowest perplexity). We use the
SRILM toolkit to build word trigram models, with
modified Kneser-Ney as a smoothing method, and
report the results of 10-fold cross validation.
Table 3 illustrates the performance of this method
under various two-, three-, and four-way scenarios.
We find that it is quite good at distinguishing each
dialect from the corresponding MSA content, and
distinguishing the dialects from each other.
We should note that, in practice, accuracy is prob-
ably not as important of a measure as (dialect) pre-
cision, since we are mainly interested in identifying
dialectal data, and much less so MSA data. To that
end, one can significantly increase the precision rate
(at the expense of recall, naturally) by biasing clas-
sification towards MSA, and choosing the dialectal
label only if the ratio of the two LM scores exceeds
a certain threshold. Figure 3 illustrates this tradeoff
for the classification task over Al-Ghad sentences.
</bodyText>
<figureCaption confidence="0.98202">
Figure 3: Dialect precision vs. recall for the classification
task over Al-Ghad sentences (MSA vs. Levantine). The
square point corresponds to the first line in Table 3.
</figureCaption>
<sectionHeader confidence="0.999954" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99997762962963">
The COLABA project (Diab et al., 2010) is an-
other large effort to create dialectal Arabic resources
(and tools). They too focus on online sources such
as blogs and forums, and use information retrieval
tasks for measuring their ability to properly process
dialectal Arabic content.
The work of Irvine and Klementiev (2010) is sim-
ilar to ours in spirit, as they too use MTurk to find an-
notators with relatively uncommon linguistic skills,
to create translation lexicons between English and
42 rare languages. In the same vein, Zaidan and
Callison-Burch (2011) solicit English translations of
Urdu sentences from non-professional translators,
and show that translation quality can rival that of
professionals, for a fraction of the cost.
Lei and Hansen (2011) build Gaussian mixture
models to identify the same three dialects we con-
sider, and are able to achieve an accuracy rate of
71.7% using about 10 hours of speech data for train-
ing. Biadsy et al. (2009) utilize a much larger dataset
(170 hours of speech data) and take a phone recog-
nition and language modeling approach (Zissman,
1996). In a four-way classification task (with Iraqi
as a fourth dialect), they achieve a 78.5% accuracy
rate. It must be noted that both works use speech
data, and that dialect identification is done on the
speaker level, not the sentence level as we do.
</bodyText>
<figure confidence="0.9980563">
100%
60%
prec=70.6%
60%
rec=78.2%
40%
20%
20% 40% 60% 60% 100%
Recall
Precision
</figure>
<page confidence="0.979495">
40
</page>
<sectionHeader confidence="0.982537" genericHeader="conclusions">
6 Current and Future Work
</sectionHeader>
<bodyText confidence="0.9999795">
We have already utilized the dialect labels to identify
dialectal sentences to be translated into English, in
an effort to create a Dialectal Arabic-to-English par-
allel dataset (also taking a crowdsourcing approach)
to aid machine translation of dialectal Arabic.
Given the recent political unrest in the Middle
East (early 2011), another rich source of dialectal
Arabic are Twitter posts (e.g. with the #Egypt
tag) and discussions on various political Facebook
groups. Here again, given the topic at hand and
the individualistic nature of the posts, they are very
likely to contain a high degree of dialectal data.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999853833333333">
This research was supported by the Human
Language Technology Center of Excellence,
by the DARPA GALE program under Contract
No. HR0011-06-2-0001, and by Raetheon BBN
Technologies. The views and findings are the
authors’ alone.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999653275862069">
Fadi Biadsy, Julia Hirschberg, and Nizar Habash. 2009.
Spoken arabic dialect identification using phonotac-
tic modeling. In Proceedings of the EACL Workshop
on Computational Approaches to Semitic Languages,
pages 53–61.
Mona Diab, Nizar Habash, Owen Rambow, Mohamed
Altantawy, and Yassine Benajiba. 2010. COLABA:
Arabic dialect annotation and processing. In LREC
Workshop on Semitic Language Processing, pages 66–
74.
Nizar Y. Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan &amp; Claypool.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In Proceedings of the NAACL HLT
Workshop on Creating Speech and Language Data
With Amazon’s Mechanical Turk, pages 108–113.
Yun Lei and John H. L. Hansen. 2011. Dialect clas-
sification via text-independent training and testing for
arabic, spanish, and chinese. IEEE Transactions on
Audio, Speech, and Language Processing, 19(1):85–
96.
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings ofACL (this volume).
Marc A. Zissman. 1996. Comparison of four ap-
proaches to automatic language identification of tele-
phone speech. IEEE Transactions on Speech and Au-
dio Processing, 4(1):31–44.
</reference>
<page confidence="0.999446">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.894136">
<title confidence="0.987127">The Arabic Online Commentary an Annotated Dataset of Informal Arabic with High Dialectal Content</title>
<author confidence="0.998882">F Zaidan</author>
<affiliation confidence="0.999212">Dept. of Computer Science, Johns Hopkins</affiliation>
<address confidence="0.998309">Baltimore, MD 21218,</address>
<abstract confidence="0.995302888888889">written form of Arabic, Standard differs quite a bit from the spoken dialects of Arabic, which are the true “native” languages of Arabic speakers used in daily life. However, due to MSA’s prevalence in written form, almost all Arabic datasets have predominantly MSA content. We present Online Commentary a 52M-word monolingual dataset rich in dialectal content, and we describe our long-term annotation effort to identify the dialect level (and dialect itself) in each sentence of the dataset. So far, we have labeled 108K sentences, 41% of which as having dialectal content. We also present experimental results on the task of automatic dialect identification, using the collected labels for training and evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fadi Biadsy</author>
<author>Julia Hirschberg</author>
<author>Nizar Habash</author>
</authors>
<title>Spoken arabic dialect identification using phonotactic modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>53--61</pages>
<contexts>
<context position="13911" citStr="Biadsy et al. (2009)" startWordPosition="2233" endWordPosition="2236">ours in spirit, as they too use MTurk to find annotators with relatively uncommon linguistic skills, to create translation lexicons between English and 42 rare languages. In the same vein, Zaidan and Callison-Burch (2011) solicit English translations of Urdu sentences from non-professional translators, and show that translation quality can rival that of professionals, for a fraction of the cost. Lei and Hansen (2011) build Gaussian mixture models to identify the same three dialects we consider, and are able to achieve an accuracy rate of 71.7% using about 10 hours of speech data for training. Biadsy et al. (2009) utilize a much larger dataset (170 hours of speech data) and take a phone recognition and language modeling approach (Zissman, 1996). In a four-way classification task (with Iraqi as a fourth dialect), they achieve a 78.5% accuracy rate. It must be noted that both works use speech data, and that dialect identification is done on the speaker level, not the sentence level as we do. 100% 60% prec=70.6% 60% rec=78.2% 40% 20% 20% 40% 60% 60% 100% Recall Precision 40 6 Current and Future Work We have already utilized the dialect labels to identify dialectal sentences to be translated into English, </context>
</contexts>
<marker>Biadsy, Hirschberg, Habash, 2009</marker>
<rawString>Fadi Biadsy, Julia Hirschberg, and Nizar Habash. 2009. Spoken arabic dialect identification using phonotactic modeling. In Proceedings of the EACL Workshop on Computational Approaches to Semitic Languages, pages 53–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
<author>Mohamed Altantawy</author>
<author>Yassine Benajiba</author>
</authors>
<title>COLABA: Arabic dialect annotation and processing.</title>
<date>2010</date>
<booktitle>In LREC Workshop on Semitic Language Processing,</booktitle>
<pages>66--74</pages>
<contexts>
<context position="12992" citStr="Diab et al., 2010" startWordPosition="2083" endWordPosition="2086">terested in identifying dialectal data, and much less so MSA data. To that end, one can significantly increase the precision rate (at the expense of recall, naturally) by biasing classification towards MSA, and choosing the dialectal label only if the ratio of the two LM scores exceeds a certain threshold. Figure 3 illustrates this tradeoff for the classification task over Al-Ghad sentences. Figure 3: Dialect precision vs. recall for the classification task over Al-Ghad sentences (MSA vs. Levantine). The square point corresponds to the first line in Table 3. 5 Related Work The COLABA project (Diab et al., 2010) is another large effort to create dialectal Arabic resources (and tools). They too focus on online sources such as blogs and forums, and use information retrieval tasks for measuring their ability to properly process dialectal Arabic content. The work of Irvine and Klementiev (2010) is similar to ours in spirit, as they too use MTurk to find annotators with relatively uncommon linguistic skills, to create translation lexicons between English and 42 rare languages. In the same vein, Zaidan and Callison-Burch (2011) solicit English translations of Urdu sentences from non-professional translator</context>
</contexts>
<marker>Diab, Habash, Rambow, Altantawy, Benajiba, 2010</marker>
<rawString>Mona Diab, Nizar Habash, Owen Rambow, Mohamed Altantawy, and Yassine Benajiba. 2010. COLABA: Arabic dialect annotation and processing. In LREC Workshop on Semitic Language Processing, pages 66– 74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Y Habash</author>
</authors>
<title>Introduction to Arabic Natural Language Processing.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool.</publisher>
<contexts>
<context position="3602" citStr="Habash (2010)" startWordPosition="551" endWordPosition="552">h transliteration rate is somewhat alarming, as the first two words of the sentence are relatively frequent: AymtY means ‘when’ and rH corresponds to the modal ‘will’. 2It can in fact be argued that MSA is the variant with the more complex sentence structure and richer morphology. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 37–41, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Figure 2: One possible breakdown of spoken Arabic into dialect groups: Maghrebi, Egyptian, Levantine, Gulf, and Iraqi. Habash (2010) also gives a very similar breakdown. News Source Al-Ghad Al-Riyadh Al-Youm Al-Sabe’ # articles 6.30K 34.2K 45.7K # comments 26.6K 805K 565K # sentences 63.3K 1,686K 1,384K # words 1.24M 18.8M 32.1M comments/article 4.23 23.56 12.37 sentences/comment 2.38 2.09 2.45 words/sentence 19.51 11.14 23.22 Table 1: A summary of the different components of the AOC dataset. Overall, 1.4M comments were harvested from 86.1K articles, corresponding to 52.1M words. • The prevalence of dialectal Arabic. 2 The AOC Dataset Arabic is the official language in over 20 countries, spoken by more than 250 million peo</context>
</contexts>
<marker>Habash, 2010</marker>
<rawString>Nizar Y. Habash. 2010. Introduction to Arabic Natural Language Processing. Morgan &amp; Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Irvine</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Using Mechanical Turk to annotate lexicons for less commonly used languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk,</booktitle>
<pages>108--113</pages>
<contexts>
<context position="13276" citStr="Irvine and Klementiev (2010)" startWordPosition="2128" endWordPosition="2131">ores exceeds a certain threshold. Figure 3 illustrates this tradeoff for the classification task over Al-Ghad sentences. Figure 3: Dialect precision vs. recall for the classification task over Al-Ghad sentences (MSA vs. Levantine). The square point corresponds to the first line in Table 3. 5 Related Work The COLABA project (Diab et al., 2010) is another large effort to create dialectal Arabic resources (and tools). They too focus on online sources such as blogs and forums, and use information retrieval tasks for measuring their ability to properly process dialectal Arabic content. The work of Irvine and Klementiev (2010) is similar to ours in spirit, as they too use MTurk to find annotators with relatively uncommon linguistic skills, to create translation lexicons between English and 42 rare languages. In the same vein, Zaidan and Callison-Burch (2011) solicit English translations of Urdu sentences from non-professional translators, and show that translation quality can rival that of professionals, for a fraction of the cost. Lei and Hansen (2011) build Gaussian mixture models to identify the same three dialects we consider, and are able to achieve an accuracy rate of 71.7% using about 10 hours of speech data</context>
</contexts>
<marker>Irvine, Klementiev, 2010</marker>
<rawString>Ann Irvine and Alexandre Klementiev. 2010. Using Mechanical Turk to annotate lexicons for less commonly used languages. In Proceedings of the NAACL HLT Workshop on Creating Speech and Language Data With Amazon’s Mechanical Turk, pages 108–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun Lei</author>
<author>John H L Hansen</author>
</authors>
<title>Dialect classification via text-independent training and testing for arabic, spanish, and chinese.</title>
<date>2011</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>96</pages>
<contexts>
<context position="13711" citStr="Lei and Hansen (2011)" startWordPosition="2196" endWordPosition="2199">e sources such as blogs and forums, and use information retrieval tasks for measuring their ability to properly process dialectal Arabic content. The work of Irvine and Klementiev (2010) is similar to ours in spirit, as they too use MTurk to find annotators with relatively uncommon linguistic skills, to create translation lexicons between English and 42 rare languages. In the same vein, Zaidan and Callison-Burch (2011) solicit English translations of Urdu sentences from non-professional translators, and show that translation quality can rival that of professionals, for a fraction of the cost. Lei and Hansen (2011) build Gaussian mixture models to identify the same three dialects we consider, and are able to achieve an accuracy rate of 71.7% using about 10 hours of speech data for training. Biadsy et al. (2009) utilize a much larger dataset (170 hours of speech data) and take a phone recognition and language modeling approach (Zissman, 1996). In a four-way classification task (with Iraqi as a fourth dialect), they achieve a 78.5% accuracy rate. It must be noted that both works use speech data, and that dialect identification is done on the speaker level, not the sentence level as we do. 100% 60% prec=70</context>
</contexts>
<marker>Lei, Hansen, 2011</marker>
<rawString>Yun Lei and John H. L. Hansen. 2011. Dialect classification via text-independent training and testing for arabic, spanish, and chinese. IEEE Transactions on Audio, Speech, and Language Processing, 19(1):85– 96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Crowdsourcing translation: Professional quality from nonprofessionals.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL (this volume).</booktitle>
<contexts>
<context position="13512" citStr="Zaidan and Callison-Burch (2011)" startWordPosition="2167" endWordPosition="2170">The square point corresponds to the first line in Table 3. 5 Related Work The COLABA project (Diab et al., 2010) is another large effort to create dialectal Arabic resources (and tools). They too focus on online sources such as blogs and forums, and use information retrieval tasks for measuring their ability to properly process dialectal Arabic content. The work of Irvine and Klementiev (2010) is similar to ours in spirit, as they too use MTurk to find annotators with relatively uncommon linguistic skills, to create translation lexicons between English and 42 rare languages. In the same vein, Zaidan and Callison-Burch (2011) solicit English translations of Urdu sentences from non-professional translators, and show that translation quality can rival that of professionals, for a fraction of the cost. Lei and Hansen (2011) build Gaussian mixture models to identify the same three dialects we consider, and are able to achieve an accuracy rate of 71.7% using about 10 hours of speech data for training. Biadsy et al. (2009) utilize a much larger dataset (170 hours of speech data) and take a phone recognition and language modeling approach (Zissman, 1996). In a four-way classification task (with Iraqi as a fourth dialect)</context>
</contexts>
<marker>Zaidan, Callison-Burch, 2011</marker>
<rawString>Omar F. Zaidan and Chris Callison-Burch. 2011. Crowdsourcing translation: Professional quality from nonprofessionals. In Proceedings ofACL (this volume).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc A Zissman</author>
</authors>
<title>Comparison of four approaches to automatic language identification of telephone speech.</title>
<date>1996</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="14044" citStr="Zissman, 1996" startWordPosition="2257" endWordPosition="2258"> English and 42 rare languages. In the same vein, Zaidan and Callison-Burch (2011) solicit English translations of Urdu sentences from non-professional translators, and show that translation quality can rival that of professionals, for a fraction of the cost. Lei and Hansen (2011) build Gaussian mixture models to identify the same three dialects we consider, and are able to achieve an accuracy rate of 71.7% using about 10 hours of speech data for training. Biadsy et al. (2009) utilize a much larger dataset (170 hours of speech data) and take a phone recognition and language modeling approach (Zissman, 1996). In a four-way classification task (with Iraqi as a fourth dialect), they achieve a 78.5% accuracy rate. It must be noted that both works use speech data, and that dialect identification is done on the speaker level, not the sentence level as we do. 100% 60% prec=70.6% 60% rec=78.2% 40% 20% 20% 40% 60% 60% 100% Recall Precision 40 6 Current and Future Work We have already utilized the dialect labels to identify dialectal sentences to be translated into English, in an effort to create a Dialectal Arabic-to-English parallel dataset (also taking a crowdsourcing approach) to aid machine translati</context>
</contexts>
<marker>Zissman, 1996</marker>
<rawString>Marc A. Zissman. 1996. Comparison of four approaches to automatic language identification of telephone speech. IEEE Transactions on Speech and Audio Processing, 4(1):31–44.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>