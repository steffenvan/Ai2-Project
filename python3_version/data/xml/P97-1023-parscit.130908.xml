<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000271">
<title confidence="0.99654">
Predicting the Semantic Orientation of Adjectives
</title>
<author confidence="0.992452">
Vasileios Hatzivassiloglou and Kathleen R. McKeown
</author>
<affiliation confidence="0.907141">
Department of Computer Science
450 Computer Science Building
Columbia University
</affiliation>
<address confidence="0.984799">
New York, N.Y. 10027, USA
</address>
<email confidence="0.828307">
{vh, kathy}Ocs . columbia. edu
</email>
<sectionHeader confidence="0.993701" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989263157895">
We identify and validate from a large cor-
pus constraints from conjunctions on the
positive or negative semantic orientation
of the conjoined adjectives. A log-linear
regression model uses these constraints to
predict whether conjoined adjectives are
of same or different orientations, achiev-
ing 82% accuracy in this task when each
conjunction is considered independently.
Combining the constraints across many ad-
jectives, a clustering algorithm separates
the adjectives into groups of different orien-
tations, and finally, adjectives are labeled
positive or negative. Evaluations on real
data and simulation experiments indicate
high levels of performance: classification
precision is more than 90% for adjectives
that occur in a modest number of conjunc-
tions in the corpus.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999878566037736">
The semantic orientation or polarity of a word indi-
cates the direction the word deviates from the norm
for its semantic group or lexical field (Lehrer, 1974).
It also constrains the word&apos;s usage in the language
(Lyons, 1977), due to its evaluative characteristics
(Battistella, 1990). For example, some nearly syn-
onymous words differ in orientation because one im-
plies desirability and the other does not (e.g., sim-
ple versus simplistic). In linguistic constructs such
as conjunctions, which impose constraints on the se-
mantic orientation of their arguments (Anscombre
and Ducrot, 1983; Elhadad and McKeown, 1990),
the choices of arguments and connective are mutu-
ally constrained, as illustrated by:
The tax proposal was
{
simple and well-received
simplistic but well-received
*simplistic and well-received
by the public.
In addition, almost all antonyms have different se-
mantic orientations.&apos; If we know that two words
relate to the same property (for example, members
of the same scalar group such as hot and cold) but
have different orientations, we can usually infer that
they are antonyms. Given that semantically similar
words can be identified automatically on the basis of
distributional properties and linguistic cues (Brown
et al., 1992; Pereira et al., 1993; Hatzivassiloglou and
McKeown, 1993), identifying the semantic orienta-
tion of words would allow a system to further refine
the retrieved semantic similarity relationships, ex-
tracting antonyms.
Unfortunately, dictionaries and similar sources
(theusari, WordNet (Miller et al., 1990)) do not in-
clude semantic orientation information .2 Explicit
links between antonyms and synonyms may also be
lacking, particularly when they depend on the do-
main of discourse; for example, the opposition bear—
bull appears only in stock market reports, where the
two words take specialized meanings.
In this paper, we present and evaluate a method
that automatically retrieves semantic orientation in-
formation using indirect information collected from
a large corpus. Because the method relies on the cor-
pus, it extracts domain-dependent information and
automatically adapts to a new domain when the cor-
pus is changed. Our method achieves high preci-
sion (more than 90%), and, while our focus to date
has been on adjectives, it can be directly applied to
other word classes. Ultimately, our goal is to use this
method in a larger system to automatically identify
antonyms and distinguish near synonyms.
</bodyText>
<sectionHeader confidence="0.957404" genericHeader="introduction">
2 Overview of Our Approach
</sectionHeader>
<bodyText confidence="0.993302166666667">
Our approach relies on an analysis of textual corpora
that correlates linguistic features, or indicators, with
I Exceptions include a small number of terms that are
both negative from a pragmatic viewpoint and yet stand
in an antonyrnic relationship; such terms frequently lex-
icalize two unwanted extremes, e.g., verbose—terse.
</bodyText>
<footnote confidence="0.980638">
2Except implicitly, in the form of definitions and us-
age examples.
</footnote>
<page confidence="0.997273">
174
</page>
<bodyText confidence="0.999910666666667">
semantic orientation. While no direct indicators of
positive or negative semantic orientation have been
proposed3, we demonstrate that conjunctions be-
tween adjectives provide indirect information about
orientation. For most connectives, the conjoined ad-
jectives usually are of the same orientation: compare
fair and legitimate and corrupt and brutal which ac-
tually occur in our corpus, with *fair and brutal and
*corrupt and legitimate (or the other cross-products
of the above conjunctions) which are semantically
anomalous. The situation is reversed for but, which
usually connects two adjectives of different orienta-
tions.
The system identifies and uses this indirect infor-
mation in the following stages:
</bodyText>
<listItem confidence="0.9585651875">
1. All conjunctions of adjectives are extracted
from the corpus along with relevant morpho-
logical relations.
2. A log-linear regression model combines informa-
tion from different conjunctions to determine
if each two conjoined adjectives are of same
or different orientation. The result is a graph
with hypothesized same- or different-orientation
links between adjectives.
3. A clustering algorithm separates the adjectives
into two subsets of different orientation. It
places as many words of same orientation as
possible into the same subset.
4. The average frequencies in each group are com-
pared and the group with the higher frequency
is labeled as positive.
</listItem>
<bodyText confidence="0.999859111111111">
In the following sections, we first present the set
of adjectives used for training and evaluation. We
next validate our hypothesis that conjunctions con-
strain the orientation of conjoined adjectives and
then describe the remaining three steps of the algo-
rithm. After presenting our results and evaluation,
we discuss simulation experiments that show how
our method performs under different conditions of
sparseness of data.
</bodyText>
<sectionHeader confidence="0.99095" genericHeader="method">
3 Data Collection
</sectionHeader>
<bodyText confidence="0.996523666666667">
For our experiments, we use the 21 million word
1987 Wall Street Journal corpus&apos;, automatically an-
notated with part-of-speech tags using the PARTS
tagger (Church, 1988).
In order to verify our hypothesis about the ori-
entations of conjoined adjectives, and also to train
and evaluate our subsequent algorithms, we need a
3Certain words inflected with negative affixes (such
as in- or un-) tend to be mostly negative, but this rule
applies only to a fraction of the negative words. Further-
more, there are words so inflected which have positive
orientation, e.g., independent and unbiased.
</bodyText>
<footnote confidence="0.657189">
4Available form the ACL Data Collection Initiative
as CD ROM 1.
</footnote>
<bodyText confidence="0.973314166666667">
Positive: adequate central clever famous
intelligent remarkable reputed
sensitive slender thriving
Negative: contagious drunken ignorant lanky
listless primitive strident troublesome
unresolved unsuspecting
</bodyText>
<figureCaption confidence="0.986109">
Figure 1: Randomly selected adjectives with positive
and negative orientations.
</figureCaption>
<bodyText confidence="0.999666361702128">
set of adjectives with predetermined orientation la-
bels. We constructed this set by taking all adjectives
appearing in our corpus 20 times or more, then re-
moving adjectives that have no orientation. These
are typically members of groups of complementary,
qualitative terms (Lyons, 1977), e.g., domestic or
medical.
We then assigned an orientation label (either + or
—) to each adjective, using an evaluative approach.
The criterion was whether the use of this adjective
ascribes in general a positive or negative quality to
the modified item, making it better or worse than a
similar unmodified item. We were unable to reach
a unique label out of context for several adjectives
which we removed from consideration; for example,
cheap is positive if it is used as a synonym of in-
expensive, but negative if it implies inferior quality.
The operations of selecting adjectives and assigning
labels were performed before testing our conjunction
hypothesis or implementing any other algorithms, to
avoid any influence on our labels. The final set con-
tained 1,336 adjectives (657 positive and 679 nega-
tive terms). Figure 1 shows randomly selected terms
from this set.
To further validate our set of labeled adjectives,
we subsequently asked four people to independently
label a randomly drawn sample of 500 of these
adjectives. They agreed with us that the posi-
tive/negative concept applies to 89.15% of these ad-
jectives on average. For the adjectives where a pos-
itive or negative label was assigned by both us and
the independent evaluators, the average agreement
on the label was 97.38%. The average inter-reviewer
agreement on labeled adjectives was 96.97%. These
results are extremely significant statistically and
compare favorably with validation studies performed
for other tasks (e.g., sense disambiguation) in the
past. They show that positive and negative orien-
tation are objective properties that can be reliably
determined by humans.
To extract conjunctions between adjectives, we
used a two-level finite-state grammar, which covers
complex modification patterns and noun-adjective
apposition. Running this parser on the 21 mil-
lion word corpus, we collected 13,426 conjunctions
of adjectives, expanding to a total of 15,431 con-
joined adjective pairs. After morphological trans-
</bodyText>
<page confidence="0.989091">
175
</page>
<table confidence="0.896991">
Conjunction category Conjunction % same- % same- P-Value
types orientation orientation (for types)
analyzed (types) (tokens)
All conjunctions 2,748 77.84% 72.39% &lt; 1 • 10-16
All and conjunctions 2,294 81.73% 78.07% &lt; 1 • 10-1b
All or conjunctions 305 77.05% 60.97% &lt; 1 • 10-1b
All but conjunctions 214 30.84% 25.94% 2.09 • 10-8
All attributive and conjunctions 1,077 80.04% 76.82% &lt; 1 • 10-16
All predicative and conjunctions 860 84.77% 84.54% &lt; 1 • 10-16
All appositive and conjunctions 30 70.00% 63.64% 0.04277
</table>
<tableCaption confidence="0.751239">
Table 1 Validation of our conjunction hypothesis. The P-value is the probability that similar or more
extreme results would have been obtained if same- and different-orientation conjunction types were actually
equally distributed.
</tableCaption>
<bodyText confidence="0.970957516129032">
formations, the remaining 15,048 conjunction tokens
involve 9,296 distinct pairs of conjoined adjectives
(types). Each conjunction token is classified by the
parser according to three variables: the conjunction
used (and, or, but, either-or, or neither-nor), the
type of modification (attributive, predicative, appos-
itive, resultative), and the number of the modified
noun (singular or plural).
4 Validation of the Conjunction
Hypothesis
Using the three attributes extracted by the parser,
we constructed a cross-classification of the conjunc-
tions in a three-way table. We counted types and to-
kens of each conjoined pair that had both members
in the set of pre-selected labeled adjectives discussed
above; 2,748 (29.56%) of all conjoined pairs (types)
and 4,024 (26.74%) of all conjunction occurrences
(tokens) met this criterion. We augmented this ta-
ble with marginal totals, arriving at 90 categories,
each of which represents a triplet of attribute values,
possibly with one or more &amp;quot;don&apos;t care&amp;quot; elements.
We then measured the percentage of conjunctions
in each category with adjectives of same or differ-
ent orientations. Under the null hypothesis of same
proportions of adjective pairs (types) of same and
different orientation in a given category, the num-
ber of same- or different-orientation pairs follows a
binomial distribution with p = 0.5 (Conover, 1980).
We show in Table 1 the results for several repre-
sentative categories, and summarize all results be-
low:
</bodyText>
<listItem confidence="0.9996006">
• Our conjunction hypothesis is validated overall
and for almost all individual cases. The results
are extremely significant statistically, except for
a few cases where the sample is small.
• Aside from the use of but with adjectives of
</listItem>
<bodyText confidence="0.899751">
different orientations, there are, rather surpris-
ingly, small differences in the behavior of con-
junctions between linguistic environments (as
represented by the three attributes). There are
a few exceptions, e.g., appositive and conjunc-
tions modifying plural nouns are evenly split
between same and different orientation. But
in these exceptional cases the sample is very
small, and the observed behavior may be due
to chance.
</bodyText>
<listItem confidence="0.935292333333333">
• Further analysis of different-orientation pairs in
conjunctions other than but shows that con-
joined antonyms are far more frequent than ex-
pected by chance, in agreement with (Justeson
and Katz, 1991).
5 Prediction of Link Type
</listItem>
<bodyText confidence="0.999970103448276">
The analysis in the previous section suggests a base-
line method for classifying links between adjectives:
since 77.84% of all links from conjunctions indicate
same orientation, we can achieve this level of perfor-
mance by always guessing that a link is of the same-
orientation type. However, we can improve perfor-
mance by noting that conjunctions using but exhibit
the opposite pattern, usually involving adjectives of
different orientations. Thus, a revised but still sim-
ple rule predicts a different-orientation link if the
two adjectives have been seen in a but conjunction,
and a same-orientation link otherwise, assuming the
two adjectives were seen connected by at least one
conjunction.
Morphological relationships between adjectives al-
so play a role. Adjectives related in form (e.g., ade-
quate-inadequate or thoughtful-thoughtless) almost
always have different semantic orientations. We im-
plemented a morphological analyzer which matches
adjectives related in this manner. This process is
highly accurate, but unfortunately does not apply
to many of the possible pairs: in our set of 1,336
labeled adjectives (891,780 possible pairs), 102 pairs
are morphologically related; among them, 99 are of
different orientation, yielding 97.06% accuracy for
the morphology method. This information is orthog-
onal to that extracted from conjunctions: only 12
of the 102 morphologically related pairs have been
observed in conjunctions in our corpus. Thus, we
</bodyText>
<page confidence="0.98625">
176
</page>
<table confidence="0.999508888888889">
Prediction Morphology Accuracy on reported Accuracy on reported Overall
method used? same-orientation links different-orientation links accuracy
Always predict No 77.84% — 77.84%
same orientation
Yes 78.18% 97.06% 78.86%
But rule No 81.81% 69.16% 80.82%
Yes 82.20% 78.16% 81.75%
Log-linear model No 81.53% 73.70% 80.97%
Yes 82.00% 82.44% 82.05%
</table>
<tableCaption confidence="0.999608">
Table 2: Accuracy of several link prediction models.
</tableCaption>
<bodyText confidence="0.963425277777778">
add to the predictions made from conjunctions the
different-orientation links suggested by morphologi-
cal relationships.
We improve the accuracy of classifying links de-
rived from conjunctions as same or different orienta-
tion with a log-linear regression model (Santner and
Duffy, 1989), exploiting the differences between the
various conjunction categories. This is a generalized
linear model (McCullagh and Nelder, 1989) with a
linear predictor
wTx
where x is the vector of the observed counts in the
various conjunction categories for the particular ad-
jective pair we try to classify and w is a vector of
weights to be learned during training. The response
y is non-linearly related to 77 through the inverse
logit function,
en
</bodyText>
<equation confidence="0.962189">
1 + en
</equation>
<bodyText confidence="0.960663733333333">
Note that y E (0, 1), with each of these endpoints
associated with one of the possible outcomes.
We have 90 possible predictor variables, 42 of
which are linearly independent. Since using all the
42 independent predictors invites overfitting (Duda
and Hart, 1973), we have investigated subsets of the
full log-linear model for our data using the method
of iterative stepwise refinement: starting with an ini-
tial model, variables are added or dropped if their
contribution to the reduction or increase of the resid-
ual deviance compares favorably to the resulting loss
or gain of residual degrees of freedom. This process
led to the selection of nine predictor variables.
We evaluated the three prediction models dis-
cussed above with and without the secondary source
of morphology relations. For the log-linear model,
we repeatedly partitioned our data into equally sized
training and testing sets, estimated the weights on
the training set, and scored the model&apos;s performance
on the testing set, averaging the resulting scores.&apos;
Table 2 shows the results of these analyses. Al-
though the log-linear model offers only a small im-
provement on pair classification than the simpler but
prediction rule, it confers the important advantage
5When morphology is to be used as a supplementary
predictor, we remove the morphologically related pairs
from the training and testing sets.
of rating each prediction between 0 and 1. We make
extensive use of this in the next phase of our algo-
rithm.
</bodyText>
<sectionHeader confidence="0.9971925" genericHeader="method">
6 Finding Groups of Same-Oriented
Adjectives
</sectionHeader>
<bodyText confidence="0.999967114285714">
The third phase of our method assigns the adjectives
into groups, placing adjectives of the same (but un-
known) orientation in the same group. Each pair
of adjectives has an associated dissimilarity value
between 0 and 1; adjectives connected by same-
orientation links have low dissimilarities, and con-
versely, different-orientation links result in high dis-
similarities. Adjective pairs with no connecting links
are assigned the neutral dissimilarity 0.5.
The baseline and but methods make qualitative
distinctions only (i.e., same-orientation, different-
orientation, or unknown); for them, we define dis-
similarity for same-orientation links as one minus
the probability that such a classification link is cor-
rect and dissimilarity for different-orientation links
as the probability that such a classification is cor-
rect. These probabilities are estimated from sep-
arate training data. Note that for these prediction
models, dissimilarities are identical for similarly clas-
sified links.
The log-linear model, on the other hand, offers
an estimate of how good each prediction is, since it
produces a value y between 0 and 1. We construct
the model so that 1 corresponds to same-orientation,
and define dissimilarity as one minus the produced
value.
Same and different-orientation links between ad-
jectives form a graph. To partition the graph nodes
into subsets of the same orientation, we employ an
iterative optimization procedure on each connected
component, based on the exchange method, a non-
hierarchical clustering algorithm (Sp5,th, 1985). We
define an objective function 4) scoring each possible
partition P of the adjectives into two subgroups Ci
and C2 as
</bodyText>
<equation confidence="0.9993306">
(1)(2) = E
I.1
2 ( icd .,yEc,
roy d(x , y))
Y =
</equation>
<page confidence="0.995963">
177
</page>
<table confidence="0.784479">
a Number of Number of Average number Accuracy Ratio of average
adjectives in links in of links for group frequencies
test set (lila&apos;) test set (IL,I) each adjective
2 730 2,568 7.04 78.08% 1.8699
3 516 2,159 8.37 82.56% 1.9235
4 369 1,742 9.44 87.26% 1.3486
5 236 1,238 10.49 92.37% 1.4040
</table>
<tableCaption confidence="0.99991">
Table 3: Evaluation of the adjective classification and labeling methods.
</tableCaption>
<bodyText confidence="0.9801186">
where ICI stands for the cardinality of cluster i, and
d(x, y) is the dissimilarity between adjectives x and
y. We want to select the partition Pmin that min-
imizes (I), subject to the additional constraint that
for each adjective x in a cluster C,
</bodyText>
<equation confidence="0.998386">
E
1 d(x,y) &lt; —1 E d(x y) (1)
—
y E C Y E
x0Y
</equation>
<bodyText confidence="0.999938052631579">
where C is the complement of cluster C, i.e., the
other member of the partition. This constraint,
based on Rousseeuw&apos;s (1987) silhouettes, helps cor-
rect wrong cluster assignments.
To find Pi we first construct a random parti-
tion of the adjectives, then locate the adjective that
will most reduce the objective function if it is moved
from its current cluster. We move this adjective and
proceed with the next iteration until no movements
can improve the objective function. At the final it-
eration, the cluster assignment of any adjective that
violates constraint (1) is changed. This is a steepest-
descent hill-climbing method, and thus is guaran-
teed to converge. However, it will in general find a
local minimum rather than the global one; the prob-
lem is NP-complete (Garey and Johnson, 1979). We
can arbitrarily increase the probability of finding the
globally optimal solution by repeatedly running the
algorithm with different starting partitions.
</bodyText>
<sectionHeader confidence="0.9647395" genericHeader="method">
7 Labeling the Clusters as Positive
or Negative
</sectionHeader>
<bodyText confidence="0.999853095238096">
The clustering algorithm separates each component
of the graph into two groups of adjectives, but does
not actually label the adjectives as positive or neg-
ative. To accomplish that, we use a simple criterion
that applies only to pairs or groups of words of oppo-
site orientation. We have previously shown (Hatzi-
vassiloglou and McKeown, 1995) that in oppositions
of gradable adjectives where one member is semanti-
cally unmarked, the unmarked member is the most
frequent one about 81% of the time. This is relevant
to our task because semantic markedness exhibits
a strong correlation with orientation, the unmarked
member almost always having positive orientation
(Lehrer, 1985; Battistella, 1990).
We compute the average frequency of the words
in each group, expecting the group with higher av-
erage frequency to contain the positive terms. This
aggregation operation increases the precision of the
labeling dramatically since indicators for many pairs
of words are combined, even when some of the words
are incorrectly assigned to their group.
</bodyText>
<sectionHeader confidence="0.999581" genericHeader="method">
8 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.999784081081081">
Since graph connectivity affects performance, we de-
vised a method of selecting test sets that makes this
dependence explicit. Note that the graph density is
largely a function of corpus size, and thus can be
increased by adding more data. Nevertheless, we
report results on sparser test sets to show how our
algorithm scales up.
We separated our sets of adjectives A (containing
1,336 adjectives) and conjunction- and morphology-
based links L (containing 2,838 links) into training
and testing groups by selecting, for several values
of the parameter a, the maximal subset of A, Aa,
which includes an adjective x if and only if there
exist at least a links from L between x and other
elements of Aa. This operation in turn defines a
subset of L, La, which includes all links between
members of Aa. We train our log-linear model on
L— La (excluding links between morphologically re-
lated adjectives), compute predictions and dissimi-
larities for the links in La, and use these to classify
and label the adjectives in Aa. a must be at least
2, since we need to leave some links for training.
Table 3 shows the results of these experiments for
= 2 to 5. Our method produced the correct clas-
sification between 78% of the time on the sparsest
test set up to more than 92% of the time when a
higher number of links was present. Moreover, in all
cases, the ratio of the two group frequencies correctly
identified the positive subgroup. These results are
extremely significant statistically (P-value less than
10&apos;) when compared with the baseline method of
randomly assigning orientations to adjectives, or the
baseline method of always predicting the most fre-
quent (for types) category (50.82% of the adjectives
in our collection are classified as negative). Figure 2
shows some of the adjectives in set A4 and their clas-
sifications.
</bodyText>
<page confidence="0.995683">
178
</page>
<bodyText confidence="0.948757428571429">
Classified as positive:
bold decisive disturbing generous good
honest important large mature patient
peaceful positive proud sound
stimulating straightforward strange
talented vigorous witty
Classified as negative:
</bodyText>
<figureCaption confidence="0.791137875">
ambiguous cautious cynical evasive
harmful hypocritical inefficient insecure
irrational irresponsible minor outspoken
pleasant reckless risky selfish tedious
unsupported vulnerable wasteful
Figure 2: Sample retrieved classifications of adjec-
tives from set A4. Correctly matched adjectives are
shown in bold.
</figureCaption>
<sectionHeader confidence="0.7410075" genericHeader="method">
9 Graph Connectivity and
Performance
</sectionHeader>
<bodyText confidence="0.999992333333333">
A strong point of our method is that decisions on
individual words are aggregated to provide decisions
on how to group words into a class and whether to
label the class as positive or negative. Thus, the
overall result can be much more accurate than the
individual indicators. To verify this, we ran a series
of simulation experiments. Each experiment mea-
sures how our algorithm performs for a given level
of precision P for identifying links and a given av-
erage number of links k for each word. The goal is
to show that even when P is low, given enough data
(i.e., high k), we can achieve high performance for
the grouping. .
As we noted earlier, the corpus data is eventually
represented in our system as a graph, with the nodes
corresponding to adjectives and the links to predic-
tions about whether the two connected adjectives
have the same or different orientation. Thus the pa-
rameter P in the simulation experiments measures
how well we are able to predict each link indepen-
dently of the others, and the parameter k measures
the number of distinct adjectives each adjective ap-
pears with in conjunctions. P therefore directly rep-
resents the precision of the link classification algo-
rithm, while k indirectly represents the corpus size.
To measure the effect of P and k (which are re-
flected in the graph topology), we need to carry out a
series of experiments where we systematically vary
their values. For example, as k (or the amount of
data) increases for a given level of precision P for in-
dividual links, we want to measure how this affects
overall accuracy of the resulting groups of nodes.
Thus, we need to construct a series of data sets,
or graphs, which represent different scenarios cor-
responding to a given combination of values of P
and k. To do this, we construct a random graph
by randomly assigning 50 nodes to the two possible
orientations. Because we don&apos;t have frequency and
morphology information on these abstract nodes, we
cannot predict whether two nodes are of the same
or different orientation. Rather, we randomly as-
sign links between nodes so that, on average, each
node participates in k links and 100 x P% of all
links connect nodes of the same orientation. Then
we consider these links as identified by the link pre-
diction algorithm as connecting two nodes with the
same orientation (so that 100 x P% of these pre-
dictions will be correct). This is equivalent to the
baseline link classification method, and provides a
lower bound on the performance of the algorithm
actually used in our system (Section 5).
Because of the lack of actual measurements such
as frequency on these abstract nodes, we also de-
couple the partitioning and labeling components of
our system and score the partition found under the
best matching conditions for the actual labels. Thus
the simulation measures only how well the system
separates positive from negative adjectives, not how
well it determines which is which. However, in all
the experiments performed on real corpus data (Sec-
tion 8), the system correctly found the labels of the
groups; any misclassifications came from misplacing
an adjective in the wrong group. The whole proce-
dure of constructing the random graph and finding
and scoring the groups is repeated 200 times for any
given combination of P and k, and the results are
averaged, thus avoiding accidentally evaluating our
system on a graph that is not truly representative of
graphs with the given P and k.
We observe (Figure 3) that even for relatively low
P, our ability to correctly classify the nodes ap-
proaches very high levels with a modest number of
links. For P =-- 0.8, we need only about 7 links
per adjective for classification performance over 90%
and only 12 links per adjective for performance over
99%.6 The difference between low and high values
of P is in the rate at which increasing data increases
overall precision. These results are somewhat more
optimistic than those obtained with real data (Sec-
tion 8), a difference which is probably due to the uni-
form distributional assumptions in the simulation.
Nevertheless, we expect the trends to be similar to
the ones shown in Figure 3 and the results of Table 3
on real data support this expectation.
</bodyText>
<sectionHeader confidence="0.984912" genericHeader="conclusions">
10 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999944666666667">
We have proposed and verified from corpus data con-
straints on the semantic orientations of conjoined ad-
jectives. We used these constraints to automatically
construct a log-linear regression model, which, com-
bined with supplementary morphology rules, pre-
dicts whether two conjoined adjectives are of same
</bodyText>
<footnote confidence="0.993577">
612 links per adjective for a set of n adjectives requires
6n conjunctions between the n adjectives in the corpus.
</footnote>
<page confidence="0.993936">
179
</page>
<figure confidence="0.999141666666667">
0 1 2 3 4 5 6 7 910 12 14 16 18 20 25 30 32.77
Average neighbors per node
(a) P =- 0.75
012345678910 12 14 18 18 20
AIWine neighbors per node
(c) P = 0.85
</figure>
<figureCaption confidence="0.992848">
Figure 3: Simulation results obtained on 50 nodes.
(average) maximum possible value of k for this P, an
classifier.
</figureCaption>
<figure confidence="0.992298666666667">
0 1 2 3 4 5 6 7 8 910 12 14 16 18 20 2527.13
Average neighbors per node
(d) P = 0.9
</figure>
<bodyText confidence="0.7080675">
In each figure, the last x coordinate indicates the
d the dotted line shows the performance of a random
</bodyText>
<figure confidence="0.989782142857143">
95
90
85
1 so
8,75
19
fc 70
65
910 12 14 16 18 20 25 30.76
Average neighbors per node
(b) P = 0.8
95
90
as
so
75
1 70
es
60
55
SO
</figure>
<bodyText confidence="0.999765068965517">
or different orientation with 82% accuracy. We then
classified several sets of adjectives according to the
links inferred in this way and labeled them as posi-
tive or negative, obtaining 92% accuracy on the clas-
sification task for reasonably dense graphs and 100%
accuracy on the labeling task. Simulation experi-
ments establish that very high levels of performance
can be obtained with a modest number of links per
word, even when the links themselves are not always
correctly classified.
As part of our clustering algorithm&apos;s output, a
&amp;quot;goodness-of-fit&amp;quot; measure for each word is com-
puted, based on Rousseeuw&apos;s (1987) silhouettes.
This measure ranks the words according to how well
they fit in their group, and can thus be used as
a quantitative measure of orientation, refining the
binary positive—negative distinction. By restricting
the labeling decisions to words with high values of
this measure we can also increase the precision of
our system, at the cost of sacrificing some coverage.
We are currently combining the output of this sys-
tem with a semantic group finding system so that we
can automatically identify antonyms from the cor-
pus, without access to any semantic descriptions.
The learned semantic categorization of the adjec-
tives can also be used in the reverse direction, to
help in interpreting the conjunctions they partici-
pate. We will also extend our analyses to nouns and
verbs.
</bodyText>
<sectionHeader confidence="0.998497" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9863408">
This work was supported in part by the Office
of Naval Research under grant N00014-95-1-0745,
jointly by the Office of Naval Research and the
Advanced Research Projects Agency under grant
N00014-89-J-1782, by the National Science Founda-
</bodyText>
<page confidence="0.988101">
180
</page>
<bodyText confidence="0.9998141">
tion under grant GER-90-24069, and by the New
York State Center for Advanced Technology un-
der contracts NYSSTF-CAT(95)-013 and NYSSTF-
CAT(96)-013. We thank Ken Church and the
AT&amp;T Bell Laboratories for making the PARTS
part-of-speech tagger available to us. We also thank
Dragomir Radev, Eric Siegel, and Gregory Sean
McKinley who provided models for the categoriza-
tion of the adjectives in our training and testing sets
as positive and negative.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999734773333333">
Jean-Claude Anscombre and Oswald Ducrot. 1983.
L&apos; Argumentation dans la Langue. Philosophie et
Langage. Pierre Mardaga, Brussels, Belgium.
Edwin L. Battistella. 1990. Markedness: The Eval-
uative Superstructure of Language. State Univer-
sity of New York Press, Albany, New York.
Peter F. Brown, Vincent J. della Pietra, Peter V.
de Souza, Jennifer C. Lai, and Robert L. Mercer.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467-479.
Kenneth W. Church. 1988. A stochastic parts
program and noun phrase parser for unrestricted
text. In Proceedings of the Second Conference on
Applied Natural Language Processing (ANLP-88),
pages 136-143, Austin, Texas, February. Associa-
tion for Computational Linguistics.
W. J. Conover. 1980. Practical Nonparametric
Statistics. Wiley, New York, 2nd edition.
Richard 0. Duda and Peter E. Hart. 1973. Pattern
Classification and Scene Analysis. Wiley, New
York.
Michael Elhadad and Kathleen It. McKeown. 1990.
A procedure for generating connectives. In Pro-
ceedings of COLING, Helsinki, Finland, July.
Michael R. Garey and David S. Johnson. 1979.
Computers and Intractability: A Guide to the
Theory of NP-Completeness. W. H. Freeman, San
Francisco, California.
Vasileios Hatzivassiloglou and Kathleen Ft. McKe-
own. 1993. Towards the automatic identification
of adjectival scales: Clustering adjectives accord-
ing to meaning. In Proceedings of the 31st Annual
Meeting of the ACL, pages 172-182, Columbus,
Ohio, June. Association for Computational Lin-
guistics.
Vasileios Hatzivassiloglou and Kathleen R. McKe-
own. 1995. A quantitative evaluation of linguis-
tic tests for the automatic prediction of semantic
markedness. In Proceedings of the 33rd Annual
Meeting of the ACL, pages 197-204, Boston, Mas-
sachusetts, June. Association for Computational
Linguistics.
John S. Justeson and Slava M. Katz. 1991. Co-
occurrences of antonymous adjectives and their
contexts. Computational Linguistics, 17(1):1-19.
Adrienne Lehrer. 1974. Semantic Fields and Lexical
Structure. North Holland, Amsterdam and New
York.
Adrienne Lehrer. 1985. Markedness and antonymy.
Journal of Linguistics, 31(3):397-429, September.
John Lyons. 1977. Semantics, volume 1. Cambridge
University Press, Cambridge, England.
Peter McCullagh and John A. Nelder. 1989. Gen-
eralized Linear Models. Chapman and Hall, Lon-
don, 2nd edition.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine J. Miller.
1990. Introduction to WordNet: An on-line lexi-
cal database. International Journal of Lexicogra-
phy (special issue), 3(4):235-312.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words.
In Proceedings of the 31st Annual Meeting of the
ACL, pages 183-190, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Peter J. Rousseeuw. 1987. Silhouettes: A graphical
aid to the interpretation and validation of cluster
analysis. Journal of Computational and Applied
Mathematics, 20:53-65.
Thomas J. Santner and Diane E. Duffy. 1989. The
Statistical Analysis of Discrete Data. Springer-
Verlag, New York.
Helmuth Spath. 1985. Cluster Dissection and Anal-
ysis: Theory, FORTRAN Programs, Examples.
Ellis Horwood, Chichester, West Sussex, England.
</reference>
<page confidence="0.998376">
181
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.809765">
<title confidence="0.9999">Predicting the Semantic Orientation of Adjectives</title>
<author confidence="0.999962">Vasileios Hatzivassiloglou</author>
<author confidence="0.999962">Kathleen R McKeown</author>
<affiliation confidence="0.999886">Department of Computer Science</affiliation>
<address confidence="0.878753">450 Computer Science Building</address>
<affiliation confidence="0.996428">Columbia University</affiliation>
<address confidence="0.999886">New York, N.Y. 10027, USA</address>
<email confidence="0.967419">vhOcs.columbia.edu</email>
<email confidence="0.967419">kathyOcs.columbia.edu</email>
<abstract confidence="0.997659">We identify and validate from a large corpus constraints from conjunctions on the positive or negative semantic orientation of the conjoined adjectives. A log-linear regression model uses these constraints to predict whether conjoined adjectives are of same or different orientations, achieving 82% accuracy in this task when each conjunction is considered independently. Combining the constraints across many adjectives, a clustering algorithm separates the adjectives into groups of different orientations, and finally, adjectives are labeled positive or negative. Evaluations on real data and simulation experiments indicate high levels of performance: classification precision is more than 90% for adjectives that occur in a modest number of conjunctions in the corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jean-Claude Anscombre</author>
<author>Oswald Ducrot</author>
</authors>
<title>L&apos; Argumentation dans la Langue. Philosophie et Langage. Pierre Mardaga,</title>
<date>1983</date>
<location>Brussels, Belgium.</location>
<contexts>
<context position="1622" citStr="Anscombre and Ducrot, 1983" startWordPosition="235" endWordPosition="238">njunctions in the corpus. 1 Introduction The semantic orientation or polarity of a word indicates the direction the word deviates from the norm for its semantic group or lexical field (Lehrer, 1974). It also constrains the word&apos;s usage in the language (Lyons, 1977), due to its evaluative characteristics (Battistella, 1990). For example, some nearly synonymous words differ in orientation because one implies desirability and the other does not (e.g., simple versus simplistic). In linguistic constructs such as conjunctions, which impose constraints on the semantic orientation of their arguments (Anscombre and Ducrot, 1983; Elhadad and McKeown, 1990), the choices of arguments and connective are mutually constrained, as illustrated by: The tax proposal was { simple and well-received simplistic but well-received *simplistic and well-received by the public. In addition, almost all antonyms have different semantic orientations.&apos; If we know that two words relate to the same property (for example, members of the same scalar group such as hot and cold) but have different orientations, we can usually infer that they are antonyms. Given that semantically similar words can be identified automatically on the basis of dist</context>
</contexts>
<marker>Anscombre, Ducrot, 1983</marker>
<rawString>Jean-Claude Anscombre and Oswald Ducrot. 1983. L&apos; Argumentation dans la Langue. Philosophie et Langage. Pierre Mardaga, Brussels, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edwin L Battistella</author>
</authors>
<title>Markedness: The Evaluative Superstructure of Language.</title>
<date>1990</date>
<publisher>State University of New York Press,</publisher>
<location>Albany, New York.</location>
<contexts>
<context position="1320" citStr="Battistella, 1990" startWordPosition="191" endWordPosition="192"> the adjectives into groups of different orientations, and finally, adjectives are labeled positive or negative. Evaluations on real data and simulation experiments indicate high levels of performance: classification precision is more than 90% for adjectives that occur in a modest number of conjunctions in the corpus. 1 Introduction The semantic orientation or polarity of a word indicates the direction the word deviates from the norm for its semantic group or lexical field (Lehrer, 1974). It also constrains the word&apos;s usage in the language (Lyons, 1977), due to its evaluative characteristics (Battistella, 1990). For example, some nearly synonymous words differ in orientation because one implies desirability and the other does not (e.g., simple versus simplistic). In linguistic constructs such as conjunctions, which impose constraints on the semantic orientation of their arguments (Anscombre and Ducrot, 1983; Elhadad and McKeown, 1990), the choices of arguments and connective are mutually constrained, as illustrated by: The tax proposal was { simple and well-received simplistic but well-received *simplistic and well-received by the public. In addition, almost all antonyms have different semantic orie</context>
<context position="20238" citStr="Battistella, 1990" startWordPosition="3131" endWordPosition="3132">s of adjectives, but does not actually label the adjectives as positive or negative. To accomplish that, we use a simple criterion that applies only to pairs or groups of words of opposite orientation. We have previously shown (Hatzivassiloglou and McKeown, 1995) that in oppositions of gradable adjectives where one member is semantically unmarked, the unmarked member is the most frequent one about 81% of the time. This is relevant to our task because semantic markedness exhibits a strong correlation with orientation, the unmarked member almost always having positive orientation (Lehrer, 1985; Battistella, 1990). We compute the average frequency of the words in each group, expecting the group with higher average frequency to contain the positive terms. This aggregation operation increases the precision of the labeling dramatically since indicators for many pairs of words are combined, even when some of the words are incorrectly assigned to their group. 8 Results and Evaluation Since graph connectivity affects performance, we devised a method of selecting test sets that makes this dependence explicit. Note that the graph density is largely a function of corpus size, and thus can be increased by adding</context>
</contexts>
<marker>Battistella, 1990</marker>
<rawString>Edwin L. Battistella. 1990. Markedness: The Evaluative Superstructure of Language. State University of New York Press, Albany, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J della Pietra</author>
<author>Peter V de Souza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<marker>Brown, Pietra, de Souza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. della Pietra, Peter V. de Souza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing (ANLP-88),</booktitle>
<pages>136--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Austin, Texas,</location>
<contexts>
<context position="5897" citStr="Church, 1988" startWordPosition="887" endWordPosition="888">. In the following sections, we first present the set of adjectives used for training and evaluation. We next validate our hypothesis that conjunctions constrain the orientation of conjoined adjectives and then describe the remaining three steps of the algorithm. After presenting our results and evaluation, we discuss simulation experiments that show how our method performs under different conditions of sparseness of data. 3 Data Collection For our experiments, we use the 21 million word 1987 Wall Street Journal corpus&apos;, automatically annotated with part-of-speech tags using the PARTS tagger (Church, 1988). In order to verify our hypothesis about the orientations of conjoined adjectives, and also to train and evaluate our subsequent algorithms, we need a 3Certain words inflected with negative affixes (such as in- or un-) tend to be mostly negative, but this rule applies only to a fraction of the negative words. Furthermore, there are words so inflected which have positive orientation, e.g., independent and unbiased. 4Available form the ACL Data Collection Initiative as CD ROM 1. Positive: adequate central clever famous intelligent remarkable reputed sensitive slender thriving Negative: contagio</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Kenneth W. Church. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing (ANLP-88), pages 136-143, Austin, Texas, February. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J Conover</author>
</authors>
<title>Practical Nonparametric Statistics.</title>
<date>1980</date>
<publisher>Wiley,</publisher>
<location>New York,</location>
<note>2nd edition.</note>
<contexts>
<context position="11060" citStr="Conover, 1980" startWordPosition="1675" endWordPosition="1676">) and 4,024 (26.74%) of all conjunction occurrences (tokens) met this criterion. We augmented this table with marginal totals, arriving at 90 categories, each of which represents a triplet of attribute values, possibly with one or more &amp;quot;don&apos;t care&amp;quot; elements. We then measured the percentage of conjunctions in each category with adjectives of same or different orientations. Under the null hypothesis of same proportions of adjective pairs (types) of same and different orientation in a given category, the number of same- or different-orientation pairs follows a binomial distribution with p = 0.5 (Conover, 1980). We show in Table 1 the results for several representative categories, and summarize all results below: • Our conjunction hypothesis is validated overall and for almost all individual cases. The results are extremely significant statistically, except for a few cases where the sample is small. • Aside from the use of but with adjectives of different orientations, there are, rather surprisingly, small differences in the behavior of conjunctions between linguistic environments (as represented by the three attributes). There are a few exceptions, e.g., appositive and conjunctions modifying plural</context>
</contexts>
<marker>Conover, 1980</marker>
<rawString>W. J. Conover. 1980. Practical Nonparametric Statistics. Wiley, New York, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duda</author>
<author>Peter E Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="14912" citStr="Duda and Hart, 1973" startWordPosition="2266" endWordPosition="2269">linear model (McCullagh and Nelder, 1989) with a linear predictor wTx where x is the vector of the observed counts in the various conjunction categories for the particular adjective pair we try to classify and w is a vector of weights to be learned during training. The response y is non-linearly related to 77 through the inverse logit function, en 1 + en Note that y E (0, 1), with each of these endpoints associated with one of the possible outcomes. We have 90 possible predictor variables, 42 of which are linearly independent. Since using all the 42 independent predictors invites overfitting (Duda and Hart, 1973), we have investigated subsets of the full log-linear model for our data using the method of iterative stepwise refinement: starting with an initial model, variables are added or dropped if their contribution to the reduction or increase of the residual deviance compares favorably to the resulting loss or gain of residual degrees of freedom. This process led to the selection of nine predictor variables. We evaluated the three prediction models discussed above with and without the secondary source of morphology relations. For the log-linear model, we repeatedly partitioned our data into equally</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>Richard 0. Duda and Peter E. Hart. 1973. Pattern Classification and Scene Analysis. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>McKeown</author>
</authors>
<title>A procedure for generating connectives.</title>
<date>1990</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Helsinki, Finland,</location>
<contexts>
<context position="1650" citStr="McKeown, 1990" startWordPosition="241" endWordPosition="242"> The semantic orientation or polarity of a word indicates the direction the word deviates from the norm for its semantic group or lexical field (Lehrer, 1974). It also constrains the word&apos;s usage in the language (Lyons, 1977), due to its evaluative characteristics (Battistella, 1990). For example, some nearly synonymous words differ in orientation because one implies desirability and the other does not (e.g., simple versus simplistic). In linguistic constructs such as conjunctions, which impose constraints on the semantic orientation of their arguments (Anscombre and Ducrot, 1983; Elhadad and McKeown, 1990), the choices of arguments and connective are mutually constrained, as illustrated by: The tax proposal was { simple and well-received simplistic but well-received *simplistic and well-received by the public. In addition, almost all antonyms have different semantic orientations.&apos; If we know that two words relate to the same property (for example, members of the same scalar group such as hot and cold) but have different orientations, we can usually infer that they are antonyms. Given that semantically similar words can be identified automatically on the basis of distributional properties and li</context>
</contexts>
<marker>McKeown, 1990</marker>
<rawString>Michael Elhadad and Kathleen It. McKeown. 1990. A procedure for generating connectives. In Proceedings of COLING, Helsinki, Finland, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Garey</author>
<author>David S Johnson</author>
</authors>
<title>Computers and Intractability: A Guide to the Theory of NP-Completeness.</title>
<date>1979</date>
<location>San Francisco, California.</location>
<contexts>
<context position="19336" citStr="Garey and Johnson, 1979" startWordPosition="2991" endWordPosition="2994">ssignments. To find Pi we first construct a random partition of the adjectives, then locate the adjective that will most reduce the objective function if it is moved from its current cluster. We move this adjective and proceed with the next iteration until no movements can improve the objective function. At the final iteration, the cluster assignment of any adjective that violates constraint (1) is changed. This is a steepestdescent hill-climbing method, and thus is guaranteed to converge. However, it will in general find a local minimum rather than the global one; the problem is NP-complete (Garey and Johnson, 1979). We can arbitrarily increase the probability of finding the globally optimal solution by repeatedly running the algorithm with different starting partitions. 7 Labeling the Clusters as Positive or Negative The clustering algorithm separates each component of the graph into two groups of adjectives, but does not actually label the adjectives as positive or negative. To accomplish that, we use a simple criterion that applies only to pairs or groups of words of opposite orientation. We have previously shown (Hatzivassiloglou and McKeown, 1995) that in oppositions of gradable adjectives where one</context>
</contexts>
<marker>Garey, Johnson, 1979</marker>
<rawString>Michael R. Garey and David S. Johnson. 1979. Computers and Intractability: A Guide to the Theory of NP-Completeness. W. H. Freeman, San Francisco, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>McKeown</author>
</authors>
<title>Towards the automatic identification of adjectival scales: Clustering adjectives according to meaning.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the ACL,</booktitle>
<pages>172--182</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2342" citStr="McKeown, 1993" startWordPosition="347" endWordPosition="348">ated by: The tax proposal was { simple and well-received simplistic but well-received *simplistic and well-received by the public. In addition, almost all antonyms have different semantic orientations.&apos; If we know that two words relate to the same property (for example, members of the same scalar group such as hot and cold) but have different orientations, we can usually infer that they are antonyms. Given that semantically similar words can be identified automatically on the basis of distributional properties and linguistic cues (Brown et al., 1992; Pereira et al., 1993; Hatzivassiloglou and McKeown, 1993), identifying the semantic orientation of words would allow a system to further refine the retrieved semantic similarity relationships, extracting antonyms. Unfortunately, dictionaries and similar sources (theusari, WordNet (Miller et al., 1990)) do not include semantic orientation information .2 Explicit links between antonyms and synonyms may also be lacking, particularly when they depend on the domain of discourse; for example, the opposition bear— bull appears only in stock market reports, where the two words take specialized meanings. In this paper, we present and evaluate a method that a</context>
</contexts>
<marker>McKeown, 1993</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen Ft. McKeown. 1993. Towards the automatic identification of adjectival scales: Clustering adjectives according to meaning. In Proceedings of the 31st Annual Meeting of the ACL, pages 172-182, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>A quantitative evaluation of linguistic tests for the automatic prediction of semantic markedness.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the ACL,</booktitle>
<pages>197--204</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="19883" citStr="Hatzivassiloglou and McKeown, 1995" startWordPosition="3075" endWordPosition="3079">minimum rather than the global one; the problem is NP-complete (Garey and Johnson, 1979). We can arbitrarily increase the probability of finding the globally optimal solution by repeatedly running the algorithm with different starting partitions. 7 Labeling the Clusters as Positive or Negative The clustering algorithm separates each component of the graph into two groups of adjectives, but does not actually label the adjectives as positive or negative. To accomplish that, we use a simple criterion that applies only to pairs or groups of words of opposite orientation. We have previously shown (Hatzivassiloglou and McKeown, 1995) that in oppositions of gradable adjectives where one member is semantically unmarked, the unmarked member is the most frequent one about 81% of the time. This is relevant to our task because semantic markedness exhibits a strong correlation with orientation, the unmarked member almost always having positive orientation (Lehrer, 1985; Battistella, 1990). We compute the average frequency of the words in each group, expecting the group with higher average frequency to contain the positive terms. This aggregation operation increases the precision of the labeling dramatically since indicators for </context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1995</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1995. A quantitative evaluation of linguistic tests for the automatic prediction of semantic markedness. In Proceedings of the 33rd Annual Meeting of the ACL, pages 197-204, Boston, Massachusetts, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Justeson</author>
<author>Slava M Katz</author>
</authors>
<title>Cooccurrences of antonymous adjectives and their contexts.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<pages>17--1</pages>
<contexts>
<context position="12030" citStr="Justeson and Katz, 1991" startWordPosition="1826" endWordPosition="1829">erent orientations, there are, rather surprisingly, small differences in the behavior of conjunctions between linguistic environments (as represented by the three attributes). There are a few exceptions, e.g., appositive and conjunctions modifying plural nouns are evenly split between same and different orientation. But in these exceptional cases the sample is very small, and the observed behavior may be due to chance. • Further analysis of different-orientation pairs in conjunctions other than but shows that conjoined antonyms are far more frequent than expected by chance, in agreement with (Justeson and Katz, 1991). 5 Prediction of Link Type The analysis in the previous section suggests a baseline method for classifying links between adjectives: since 77.84% of all links from conjunctions indicate same orientation, we can achieve this level of performance by always guessing that a link is of the sameorientation type. However, we can improve performance by noting that conjunctions using but exhibit the opposite pattern, usually involving adjectives of different orientations. Thus, a revised but still simple rule predicts a different-orientation link if the two adjectives have been seen in a but conjuncti</context>
</contexts>
<marker>Justeson, Katz, 1991</marker>
<rawString>John S. Justeson and Slava M. Katz. 1991. Cooccurrences of antonymous adjectives and their contexts. Computational Linguistics, 17(1):1-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrienne Lehrer</author>
</authors>
<title>Semantic Fields and Lexical Structure. North Holland,</title>
<date>1974</date>
<location>Amsterdam and New York.</location>
<contexts>
<context position="1194" citStr="Lehrer, 1974" startWordPosition="173" endWordPosition="174">njunction is considered independently. Combining the constraints across many adjectives, a clustering algorithm separates the adjectives into groups of different orientations, and finally, adjectives are labeled positive or negative. Evaluations on real data and simulation experiments indicate high levels of performance: classification precision is more than 90% for adjectives that occur in a modest number of conjunctions in the corpus. 1 Introduction The semantic orientation or polarity of a word indicates the direction the word deviates from the norm for its semantic group or lexical field (Lehrer, 1974). It also constrains the word&apos;s usage in the language (Lyons, 1977), due to its evaluative characteristics (Battistella, 1990). For example, some nearly synonymous words differ in orientation because one implies desirability and the other does not (e.g., simple versus simplistic). In linguistic constructs such as conjunctions, which impose constraints on the semantic orientation of their arguments (Anscombre and Ducrot, 1983; Elhadad and McKeown, 1990), the choices of arguments and connective are mutually constrained, as illustrated by: The tax proposal was { simple and well-received simplisti</context>
</contexts>
<marker>Lehrer, 1974</marker>
<rawString>Adrienne Lehrer. 1974. Semantic Fields and Lexical Structure. North Holland, Amsterdam and New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrienne Lehrer</author>
</authors>
<title>Markedness and antonymy.</title>
<date>1985</date>
<journal>Journal of Linguistics,</journal>
<pages>31--3</pages>
<contexts>
<context position="20218" citStr="Lehrer, 1985" startWordPosition="3129" endWordPosition="3130">into two groups of adjectives, but does not actually label the adjectives as positive or negative. To accomplish that, we use a simple criterion that applies only to pairs or groups of words of opposite orientation. We have previously shown (Hatzivassiloglou and McKeown, 1995) that in oppositions of gradable adjectives where one member is semantically unmarked, the unmarked member is the most frequent one about 81% of the time. This is relevant to our task because semantic markedness exhibits a strong correlation with orientation, the unmarked member almost always having positive orientation (Lehrer, 1985; Battistella, 1990). We compute the average frequency of the words in each group, expecting the group with higher average frequency to contain the positive terms. This aggregation operation increases the precision of the labeling dramatically since indicators for many pairs of words are combined, even when some of the words are incorrectly assigned to their group. 8 Results and Evaluation Since graph connectivity affects performance, we devised a method of selecting test sets that makes this dependence explicit. Note that the graph density is largely a function of corpus size, and thus can be</context>
</contexts>
<marker>Lehrer, 1985</marker>
<rawString>Adrienne Lehrer. 1985. Markedness and antonymy. Journal of Linguistics, 31(3):397-429, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lyons</author>
</authors>
<date>1977</date>
<journal>Semantics,</journal>
<volume>1</volume>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="1261" citStr="Lyons, 1977" startWordPosition="184" endWordPosition="185">oss many adjectives, a clustering algorithm separates the adjectives into groups of different orientations, and finally, adjectives are labeled positive or negative. Evaluations on real data and simulation experiments indicate high levels of performance: classification precision is more than 90% for adjectives that occur in a modest number of conjunctions in the corpus. 1 Introduction The semantic orientation or polarity of a word indicates the direction the word deviates from the norm for its semantic group or lexical field (Lehrer, 1974). It also constrains the word&apos;s usage in the language (Lyons, 1977), due to its evaluative characteristics (Battistella, 1990). For example, some nearly synonymous words differ in orientation because one implies desirability and the other does not (e.g., simple versus simplistic). In linguistic constructs such as conjunctions, which impose constraints on the semantic orientation of their arguments (Anscombre and Ducrot, 1983; Elhadad and McKeown, 1990), the choices of arguments and connective are mutually constrained, as illustrated by: The tax proposal was { simple and well-received simplistic but well-received *simplistic and well-received by the public. In</context>
<context position="6953" citStr="Lyons, 1977" startWordPosition="1042" endWordPosition="1043">ata Collection Initiative as CD ROM 1. Positive: adequate central clever famous intelligent remarkable reputed sensitive slender thriving Negative: contagious drunken ignorant lanky listless primitive strident troublesome unresolved unsuspecting Figure 1: Randomly selected adjectives with positive and negative orientations. set of adjectives with predetermined orientation labels. We constructed this set by taking all adjectives appearing in our corpus 20 times or more, then removing adjectives that have no orientation. These are typically members of groups of complementary, qualitative terms (Lyons, 1977), e.g., domestic or medical. We then assigned an orientation label (either + or —) to each adjective, using an evaluative approach. The criterion was whether the use of this adjective ascribes in general a positive or negative quality to the modified item, making it better or worse than a similar unmodified item. We were unable to reach a unique label out of context for several adjectives which we removed from consideration; for example, cheap is positive if it is used as a synonym of inexpensive, but negative if it implies inferior quality. The operations of selecting adjectives and assigning</context>
</contexts>
<marker>Lyons, 1977</marker>
<rawString>John Lyons. 1977. Semantics, volume 1. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter McCullagh</author>
<author>John A Nelder</author>
</authors>
<title>Generalized Linear Models.</title>
<date>1989</date>
<publisher>Chapman and Hall,</publisher>
<location>London,</location>
<note>2nd edition.</note>
<contexts>
<context position="14333" citStr="McCullagh and Nelder, 1989" startWordPosition="2165" endWordPosition="2168">tion Yes 78.18% 97.06% 78.86% But rule No 81.81% 69.16% 80.82% Yes 82.20% 78.16% 81.75% Log-linear model No 81.53% 73.70% 80.97% Yes 82.00% 82.44% 82.05% Table 2: Accuracy of several link prediction models. add to the predictions made from conjunctions the different-orientation links suggested by morphological relationships. We improve the accuracy of classifying links derived from conjunctions as same or different orientation with a log-linear regression model (Santner and Duffy, 1989), exploiting the differences between the various conjunction categories. This is a generalized linear model (McCullagh and Nelder, 1989) with a linear predictor wTx where x is the vector of the observed counts in the various conjunction categories for the particular adjective pair we try to classify and w is a vector of weights to be learned during training. The response y is non-linearly related to 77 through the inverse logit function, en 1 + en Note that y E (0, 1), with each of these endpoints associated with one of the possible outcomes. We have 90 possible predictor variables, 42 of which are linearly independent. Since using all the 42 independent predictors invites overfitting (Duda and Hart, 1973), we have investigate</context>
</contexts>
<marker>McCullagh, Nelder, 1989</marker>
<rawString>Peter McCullagh and John A. Nelder. 1989. Generalized Linear Models. Chapman and Hall, London, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography (special issue),</journal>
<pages>3--4</pages>
<contexts>
<context position="2587" citStr="Miller et al., 1990" startWordPosition="378" endWordPosition="381">the same property (for example, members of the same scalar group such as hot and cold) but have different orientations, we can usually infer that they are antonyms. Given that semantically similar words can be identified automatically on the basis of distributional properties and linguistic cues (Brown et al., 1992; Pereira et al., 1993; Hatzivassiloglou and McKeown, 1993), identifying the semantic orientation of words would allow a system to further refine the retrieved semantic similarity relationships, extracting antonyms. Unfortunately, dictionaries and similar sources (theusari, WordNet (Miller et al., 1990)) do not include semantic orientation information .2 Explicit links between antonyms and synonyms may also be lacking, particularly when they depend on the domain of discourse; for example, the opposition bear— bull appears only in stock market reports, where the two words take specialized meanings. In this paper, we present and evaluate a method that automatically retrieves semantic orientation information using indirect information collected from a large corpus. Because the method relies on the corpus, it extracts domain-dependent information and automatically adapts to a new domain when the</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1990. Introduction to WordNet: An on-line lexical database. International Journal of Lexicography (special issue), 3(4):235-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the ACL,</booktitle>
<pages>183--190</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2305" citStr="Pereira et al., 1993" startWordPosition="341" endWordPosition="344">ective are mutually constrained, as illustrated by: The tax proposal was { simple and well-received simplistic but well-received *simplistic and well-received by the public. In addition, almost all antonyms have different semantic orientations.&apos; If we know that two words relate to the same property (for example, members of the same scalar group such as hot and cold) but have different orientations, we can usually infer that they are antonyms. Given that semantically similar words can be identified automatically on the basis of distributional properties and linguistic cues (Brown et al., 1992; Pereira et al., 1993; Hatzivassiloglou and McKeown, 1993), identifying the semantic orientation of words would allow a system to further refine the retrieved semantic similarity relationships, extracting antonyms. Unfortunately, dictionaries and similar sources (theusari, WordNet (Miller et al., 1990)) do not include semantic orientation information .2 Explicit links between antonyms and synonyms may also be lacking, particularly when they depend on the domain of discourse; for example, the opposition bear— bull appears only in stock market reports, where the two words take specialized meanings. In this paper, we</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proceedings of the 31st Annual Meeting of the ACL, pages 183-190, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter J Rousseeuw</author>
</authors>
<title>Silhouettes: A graphical aid to the interpretation and validation of cluster analysis.</title>
<date>1987</date>
<journal>Journal of Computational and Applied Mathematics,</journal>
<pages>20--53</pages>
<marker>Rousseeuw, 1987</marker>
<rawString>Peter J. Rousseeuw. 1987. Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20:53-65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas J Santner</author>
<author>Diane E Duffy</author>
</authors>
<title>The Statistical Analysis of Discrete Data.</title>
<date>1989</date>
<publisher>SpringerVerlag,</publisher>
<location>New York.</location>
<contexts>
<context position="14197" citStr="Santner and Duffy, 1989" startWordPosition="2147" endWordPosition="2150">orted Overall method used? same-orientation links different-orientation links accuracy Always predict No 77.84% — 77.84% same orientation Yes 78.18% 97.06% 78.86% But rule No 81.81% 69.16% 80.82% Yes 82.20% 78.16% 81.75% Log-linear model No 81.53% 73.70% 80.97% Yes 82.00% 82.44% 82.05% Table 2: Accuracy of several link prediction models. add to the predictions made from conjunctions the different-orientation links suggested by morphological relationships. We improve the accuracy of classifying links derived from conjunctions as same or different orientation with a log-linear regression model (Santner and Duffy, 1989), exploiting the differences between the various conjunction categories. This is a generalized linear model (McCullagh and Nelder, 1989) with a linear predictor wTx where x is the vector of the observed counts in the various conjunction categories for the particular adjective pair we try to classify and w is a vector of weights to be learned during training. The response y is non-linearly related to 77 through the inverse logit function, en 1 + en Note that y E (0, 1), with each of these endpoints associated with one of the possible outcomes. We have 90 possible predictor variables, 42 of whic</context>
</contexts>
<marker>Santner, Duffy, 1989</marker>
<rawString>Thomas J. Santner and Diane E. Duffy. 1989. The Statistical Analysis of Discrete Data. SpringerVerlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmuth Spath</author>
</authors>
<title>Cluster Dissection and Analysis: Theory, FORTRAN Programs, Examples. Ellis Horwood,</title>
<date>1985</date>
<location>Chichester, West Sussex, England.</location>
<marker>Spath, 1985</marker>
<rawString>Helmuth Spath. 1985. Cluster Dissection and Analysis: Theory, FORTRAN Programs, Examples. Ellis Horwood, Chichester, West Sussex, England.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>