<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001246">
<title confidence="0.99698">
Loss-Sensitive Discriminative Training of Machine Transliteration Models
</title>
<author confidence="0.991564">
Kedar Bellare
</author>
<affiliation confidence="0.900082666666667">
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003, USA
</affiliation>
<email confidence="0.992116">
kedarb@cs.umass.edu
</email>
<author confidence="0.978201">
Koby Crammer
</author>
<affiliation confidence="0.883164">
Department of Computer Science
University of Pennsylvania
Philadelphia, PA 19104, USA
</affiliation>
<email confidence="0.990656">
crammer@cis.upenn.edu
</email>
<author confidence="0.94694">
Dayne Freitag
</author>
<affiliation confidence="0.796986">
SRI International
</affiliation>
<address confidence="0.970743">
San Diego, CA 92130, USA
</address>
<email confidence="0.998888">
dayne.freitag@sri.com
</email>
<sectionHeader confidence="0.995601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995847571428572">
In machine transliteration we transcribe a
name across languages while maintaining its
phonetic information. In this paper, we
present a novel sequence transduction algo-
rithm for the problem of machine transliter-
ation. Our model is discriminatively trained
by the MIRA algorithm, which improves the
traditional Perceptron training in three ways:
(1) It allows us to consider k-best translitera-
tions instead of the best one. (2) It is trained
based on the ranking of these transliterations
according to user-specified loss function (Lev-
enshtein edit distance). (3) It enables the user
to tune a built-in parameter to cope with noisy
non-separable data during training. On an
Arabic-English name transliteration task, our
model achieves a relative error reduction of
2.2% over a perceptron-based model with sim-
ilar features, and an error reduction of 7.2%
over a statistical machine translation model
with more complex features.
</bodyText>
<sectionHeader confidence="0.984993" genericHeader="categories and subject descriptors">
1 Introduction and Related Work
</sectionHeader>
<bodyText confidence="0.9999073">
Proper names and other technical terms are fre-
quently encountered in natural language text. Both
machine translation (Knight and Graehl, 1997) and
cross-language information retrieval (Jeong et al.,
1999; Virga and Khudanpur, 2003; Abdul-Jaleel and
Larkey, 2003) can benefit by explicitly translating
such words from one language into another. This
approach is decidedly better than treating them uni-
formly as out-of-vocabulary tokens. The goal of ma-
chine transliteration is to translate words between
</bodyText>
<page confidence="0.995612">
61
</page>
<bodyText confidence="0.978101571428571">
alphabets of different languages such that they are
phonetically equivalent.
Given a source language sequence f =
f1f2 ... f,,t from an alphabet .F, we want to produce
a target language sequence e = e1e2 ... e,,, in the al-
phabet £ such that it maximizes some score function
s(e, f),
</bodyText>
<equation confidence="0.995426">
e = arg max
e′
</equation>
<bodyText confidence="0.995700230769231">
Virga and Khudanpur (2003) model this scoring
function using a separate translation and language
model, that is, s(e,f) = Pr(f|e)Pr(e). In con-
strast, Al-Onaizan and Knight (2002) directly model
the translation probability Pr(e|f) using a log-linear
combination of several individually trained phrase
and character-based models. Others have treated
transliteration as a phrase-based transduction (Sherif
and Kondrak, 2007). All these approaches are adap-
tations of statistical models for machine transla-
tion (Brown et al., 1994). In general, the parame-
ters of the scoring function in such approaches are
trained generatively and do not utilize complex fea-
tures of the input sequence pairs.
Recently, there has been interest in applying
discriminatively-trained sequence alignment mod-
els to many real-world problems. McCallum et al.
(2005) train a conditional random field model to
discriminate between matching and non-matching
string pairs treating alignments as latent. Learning
accurate alignments in this model requires finding
“close” non-match pairs which can be a challenge.
A similar conditional latent-variable model has been
applied to the task of lemmatization and genera-
tion of morphological forms (Dreyer et al., 2008).
s(e′, f).
</bodyText>
<note confidence="0.6764505">
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 61–65,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999605975">
Zelenko and Aone (2006) model transliteration as
a structured prediction problem where the letter ei
is predicted using local and global features derived
from e1e2 ... ei−1 and f. Bergsma and Kondrak
(2007) address cognate identification by training a
SVM classification model using phrase-based fea-
tures obtained from a Levenshtein alignment. Both
these models do not learn alignments that is needed
to obtain high performance on transliteration tasks.
Freitag and Khadivi (2007) describe a discrimina-
tively trained sequence alignment model based on
averaged perceptron, which is closely related to the
method proposed in this paper.
Our approach improves over previous directions
in two ways. First, our system produces better k-best
transliterations than related approaches by training
on multiple hypotheses ranked according to a user-
specified loss function (Levenshtein edit distance).
Hence, our method achieves a 19.2% error reduction
in 5-best performance over a baseline only trained
with 1-best transliterations. This is especially help-
ful when machine transliteration is part of a larger
machine translation or information retrieval pipeline
since additional sentence context can be used to
choose the best among top-K transliterations. Sec-
ond, our training procedure accounts for noise and
non-separability in the data. Therefore, our translit-
eration system would work well in cases where per-
son names were misspelled or in cases in which a
single name had many reasonable translations in the
foreign language.
The training algorithm we propose in this pa-
per is based on the K-best MIRA algorithm which
has been used earlier in structured prediction prob-
lems (McDonald et al., 2005a; McDonald et al.,
2005b). Our results demonstrate a significant im-
provement in accuracy of 7.2% over a statistical
machine translation (SMT) system (Zens et al.,
2005) and of 2.2% over a perceptron-based edit
model (Freitag and Khadivi, 2007).
</bodyText>
<sectionHeader confidence="0.978189" genericHeader="method">
2 Sequence Alignment Model
</sectionHeader>
<bodyText confidence="0.999846866666667">
Let e = e1e2 ... en and f = f1f2 ... fm be se-
quences from the target alphabet E and source al-
phabet F respectively. Let a = a1a2 ... al be a se-
quence of alignment operations needed to convert f
into e. Each alignment operation either appends a
letter to the end of the source sequence, the target
sequence or both sequences. Hence, it is a member
of the cross-product ak ∈ E∪{ǫ}×F∪{ǫ}\{(ǫ, ǫ)},
where ǫ is the null character symbol. Let ak1 =
a1a2 ... ak denote the sequence of first k alignment
operations. Similarly ek1 and fk1 are prefixes of e and
f of length k.
We define the scoring function between a word
and its transliteration to be the a maximum over all
possible alignment sequences a,
</bodyText>
<equation confidence="0.941656">
s(e,f) = max
a
</equation>
<bodyText confidence="0.960881090909091">
where the score of a specific alignment a between
two words is given by a linear relation,
s(a, e, f) = w · Φ(a, e, f),
for a parameter vector w and a feature vec-
tor Φ(a, e, f). Furthermore, let Φ(a, e, f) =
Elk=1 φ(ak, e, i, f, j) be the sum of feature vec-
tors associated with individual alignment operations.
Here i, j are positions in sequences e, f after per-
forming operations ak1. For fixed sequences e and f
the function s(e, f) can be efficiently computed us-
ing a dynamic programming algorithm,
</bodyText>
<equation confidence="0.999736">
s(ei 1,fj 1) =
s(ei−1 1 ,fj1) + w · φ(hei,ǫi, e, i, f, j)
s(ei1, fj−1
1 ) + w · φ(hǫ, fji, e, i, f, j)
s(ei−1 1 ,fj−1 1 ) + w · φ(hei,fji, e, i, f, j).
(1)
</equation>
<bodyText confidence="0.9439355625">
Given a source sequence f computing the best scor-
ing target sequence e = arg maxe′ s(e′, f) among
all possible sequences E∗ requires a beam search
procedure (Freitag and Khadivi, 2007). This pro-
cedure can also be used to produce K-best target
sequences {e′1, e′2, ... , e′K} such that s(e′1, f) ≥
s(e′2,f) ≥ ... ≥ s(e′ K, f).
In this paper, we employ the same features as
those used by Freitag and Khadivi (2007). All lo-
cal feature functions φ(ak, e, i, f, j) are conjunc-
tions of the alignment operation ak and forward or
backward-looking character m-grams in sequences
e and f at positions i and j respectively. For
the source sequence f both forward and backward-
looking m-gram features are included. We restrict
the m-gram features in our target sequence e to only
</bodyText>
<equation confidence="0.458125">
s(a, e, f) ,
I
max
</equation>
<page confidence="0.975078">
62
</page>
<bodyText confidence="0.999636714285714">
be backward-looking since we do not have access to
forward-looking m-grams during beam-search. An
order M model is one that uses m-gram features
where m = 0,1, ... M.
Our training algorithm takes as input a data set
D of source-target transliteration pairs and outputs
a parameter vector u. The algorithm pseudo-code
appears in Fig. (1). In the algorithm, the function
L(e′, e) defines a loss incurred by predicting e′ in-
stead of e. In most structured prediction problems,
the targets are of equal length and in such cases the
Hamming loss function can be used. However, in
our case the targets may differ in terms of length and
thus we use the Levenshtein edit distance (Leven-
shtein, 1966) with unit costs for insertions, deletions
and substitutions. Since the targets are both in the
same alphabet E this loss function is well-defined.
The user also supplies three paramters: (1) T - the
number of training iterations (2) K - the number
of best target hypotheses used (3) C - a complex-
ity parameter. A low C is useful if the data is non-
separable and noisy.
The final parameter vector u returned by the al-
gorithm is the average of the intermediate parameter
vectors produced during training. We find that av-
eraging helps to improve performance. At test time,
we use the beam search procedure to produce K-
best hypotheses using the parameter vector u.
</bodyText>
<sectionHeader confidence="0.998472" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999131">
We apply our model to the real-world Arabic-
English name transliteration task on a data set of
10,084 Arabic names from the LDC. The data set
consists of Arabic names in an ASCII-based alpha-
bet and its English rendering. Table 1 shows a
few examples of Arabic-English pairs in our data
set. We use the same training/development/testing
(8084/1000/1000) set as the one used in a previ-
ous benchmark study (Freitag and Khadivi, 2007).
The development and testing data were obtained
by randomly removing entries from the training
data. The absence of short vowels (e.g. “a” in
hNB”I, nab’ii), doubled consonants (e.g. “ww”
in hFWAL, fawwali) and other diacritics in Arabic
make the transliteration a hard problem. Therefore,
it is hard to achieve perfect accuracy on this data set.
For training, we set K = 20 best hypotheses and
</bodyText>
<figure confidence="0.894709428571429">
Input parameters
Training Data D
Complexity parameter C &gt; 0
Number of epochs T
Initialize w0 = 0 (zero vector) ; τ = 0 ; u = 0
Repeat T times:
For Each (e, f) ∈ D :
</figure>
<listItem confidence="0.99333775">
1. a = arg maxˆa wτ · Φ(ˆa, e, f) (Find best scoring
alignment between e and f using dynamic program-
ming)
2. Generate a list of K-best target hypotheses
{e′1, e′2, ... , e′K} given the current parameters wτ.
Let the corresponding alignments for the targets be
{a′1,a′2,...,a′K}.
3. Set wτ+1 to be the solution of :
</listItem>
<equation confidence="0.95277625">
minw12||w−wτ||2+CEKk=1 ξk
subject to (fork = 1 ... K) :
w · (Φ(a,e,f) − Φ(a′k,e′k,f)) ≥ L(e,e′k) − ξk
ξk ≥ 0
</equation>
<figure confidence="0.461078">
4. u ← u + wτ+1
5. τ ← τ + 1
Output Scoring function s(a, e, f) = u · Φ(a, e, f)
</figure>
<figureCaption confidence="0.9423115">
Figure 1: The k-best MIRA algorithm for discriminative
learning of transliterations.
</figureCaption>
<table confidence="0.9956578">
Arabic English
NB”I nab’i
HNBLI hanbali
FRIFI furayfi
MLKIAN malikian
BI;ANT bizant
FWAL fawwal
OALDAWI khalidawi
BUWUI battuti
H;? hazzah
</table>
<tableCaption confidence="0.9894545">
Table 1: Examples of Arabic names in the ASCII alpha-
bet and their English transliterations.
</tableCaption>
<bodyText confidence="0.999779571428571">
C = 1.0 and run the algorithm for T = 10 epochs.
To evaluate our algorithm, we generate 1-best (or 5-
best) hypotheses using the beam search procedure
and measure accuracy as the percentage of instances
in which the target sequence e is one of the 1-best
(or 5-best) targets. The input features are based on
character m-grams for m = 1, 2, 3. Unlike previ-
</bodyText>
<page confidence="0.998846">
63
</page>
<bodyText confidence="0.9981058">
ous generative transliteration models, no additional
language model feature is used.
We compare our model against a state-of-the-art
statistical machine translation (SMT) system (Zens
et al., 2005) and an averaged perceptron edit
model (PTEM) with identical features (Freitag and
Khadivi, 2007). The SMT system directly models
the posterior probability Pr(e|f) using a log-linear
combination of several sub-models: a character-
based phrase translation model, a character-based
lexicon model, a character penalty and a phrase
penalty. In the PTEM model, the update rule only
considers the best target sequence and modifies the
parameters wτ+1 = wτ + Φ(a, e, f) − Φ(a′, e′, f)
if the score s(e′, f) ≥ s(e, f).
</bodyText>
<table confidence="0.99883025">
Model (train+dev) 1-best 5-best
SMT 0.528 0.824
PTEM 0.552 0.803
MIRA 0.562 0.841
</table>
<tableCaption confidence="0.9843856">
Table 2: The 1-best and 5-best accuracy of differ-
ent models on the Arabic-English transliteration task.
At 95% confidence level, MIRA/PTEM outperform the
SMT model in 1-best accuracy and MIRA outperforms
PTEM/SMT in 5-best accuracy.
</tableCaption>
<bodyText confidence="0.986964904761905">
Table 2 shows the 1-best and 5-best accuracy of
each model trained on the combined train+dev data
set. All the models are evaluated on the same test
set. Both MIRA and PTEM algorithms outperform
the SMT model in terms of 1-best accuracy. The
differences in accuracy are significant at 95% con-
fidence level, using the bootstrapping method for
hypothesis testing. The difference in 1-best per-
formance of MIRA and PTEM is not significant.
At 5-best, the MIRA model outperforms both SMT
and PTEM model. We conjecture that using the
problem-specific Levenshtein loss function helps fil-
ter bad target sequences from the K-best outputs
during training.
In a second experiment we studied the effect
of changing C on the performance of the algo-
rithm. We ran the algorithm with the above set-
tings, except varying the value of the complexity
parameter to one of 7 values in the range C =
0.00001, 0.0001, ... , 0.1, 1.0, training only using
the train set, and evaluating the resulting model on
</bodyText>
<table confidence="0.999610375">
Model (train) 1-best 5-best
C = 1.0 0.545* 0.832
C = 0.5 0.548* 0.83
C = 0.2 0.549* 0.834
C = 0.01 0.545 0.852*
C = 0.001 0.518 0.843
C = 0.0001 0.482 0.798
C = 0.00001 0.476 0.798
</table>
<tableCaption confidence="0.5504726">
Table 3: The effect of varying model parameter C on 1,5-
best accuracy on the test set. All the models are trained
with Levenshtein loss and 20-best targets. The super-
script * indicates the models that achieved the greatest
performance on the dev set for a particular column.
</tableCaption>
<bodyText confidence="0.999982882352941">
the test set. The results are summarized in Table 3.
The entry marked with a star * indicates the model
that achieved the best performance on the dev set for
a particular choice of evaluation measure (1-best or
5-best). We find that changing C does have an effect
on model performance. As the value of C decreases,
the performance at lower ranks improves: C = 0.01
is good for 5-best accuracy and C = 0.001 for 20-
best accuracy (not in table). As C is further reduced,
a greater number of iterations are needed to con-
verge. In our model, where the alignments are not
observed but inferred during training, we find that
making small incremental updates makes our algo-
rithm more robust. Indeed, setting C = 0.01 and
training on the train+dev set improves 5-best per-
formance of our model from 0.841 to 0.861. Hence,
the choice of C is important.
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999972153846154">
We have shown a significant improvement in accu-
racy over state-of-the-art transliteration models by
taking into consideration the ranking of multiple
hypotheses (top-K) by Levenshtein distance, and
making the training algorithm robust to noisy non-
separable data. Our model does consistently well
at high (K = 1) and low ranks (K = 5), and can
therefore be used in isolation or in a pipelined sys-
tem (e.g. machine translation or cross-language in-
formation retrieval) to achieve better performance.
In a pipeline system, more features of names around
proper nouns and previous mentions of the name can
be used to improve scoring of K-best outputs.
</bodyText>
<page confidence="0.997931">
64
</page>
<bodyText confidence="0.999796222222222">
In our experiments, the Levenshtein loss function
uses only unit costs for edit operations and is not
specifically tuned towards our application. In fu-
ture work, we may imagine penalizing insertions
and deletions higher than substitutions and other
non-uniform schemes for better transliteration per-
formance. Our K-best framework can also be easily
extended to cases where one name has multiple for-
eign translations that are equally likely.
</bodyText>
<sectionHeader confidence="0.998422" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999454944444444">
Nasreen Abdul-Jaleel and Leah S. Larkey. 2003. Statis-
tical transliteration for English-Arabic cross language
information retrieval. In CIKM ’03, pages 139–146,
New York, NY, USA. ACM.
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in arabic text. In Proceed-
ings of the ACL-02 Workshop on Computational Ap-
proaches to Semitic Languages, pages 1–13.
Shane Bergsma and Greg Kondrak. 2007. Alignment-
based discriminative string similarity. In ACL, pages
656–663, June.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions with
finite-state methods. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1080–1089, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Dayne Freitag and Shahram Khadivi. 2007. A sequence
alignment model based on the averaged perceptron. In
EMNLP-CoNLL, pages 238–247.
K.S. Jeong, S. H. Myaeng, J.S. Lee, and K.-S.
Choi. 1999. Automatic identification and back-
transliteration of foreign words for information re-
trieval. Information Processing and Management,
35:523–540.
Kevin Knight and Jonathan Graehl. 1997. Machine
transliteration. In Philip R. Cohen and Wolfgang
Wahlster, editors, Proceedings of the Thirty-Fifth An-
nual Meeting of the Association for Computational
Linguistics and Eighth Conference of the European
Chapter ofthe Association for Computational Linguis-
tics, pages 128–135, Somerset, New Jersey. Associa-
tion for Computational Linguistics.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707–710.
Andrew McCallum, Kedar Bellare, and Fernando Pereira.
2005. A conditional random field for discriminatively-
trained finite-state string edit distance. In UAI.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Flexible text segmentation with structured
multilabel classification. In HLT-EMNLP, pages 987–
994, Vancouver, BC, Canada, October. Association for
Computational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005b. Online large-margin training of dependency
parsers. In ACL, pages 91–98, Ann Arbor, Michigan,
June.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In ACL, pages 944–951, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proceedings of the ACL 2003 workshop
on Multilingual and Mixed-language Named Entity
Recognition, pages 57–64, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In EMNLP, pages
612–617, Sydney, Australia, July. Association for
Computational Linguistics.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
Phrase-based Statistical Machine Translation System.
In Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
</reference>
<page confidence="0.999612">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.674848">
<title confidence="0.998938">Loss-Sensitive Discriminative Training of Machine Transliteration Models</title>
<author confidence="0.895586">Kedar</author>
<affiliation confidence="0.99996">Department of Computer University of Massachusetts</affiliation>
<address confidence="0.999554">Amherst, MA 01003,</address>
<email confidence="0.999845">kedarb@cs.umass.edu</email>
<author confidence="0.884517">Koby</author>
<affiliation confidence="0.9998775">Department of Computer University of</affiliation>
<address confidence="0.931872">Philadelphia, PA 19104,</address>
<email confidence="0.999431">crammer@cis.upenn.edu</email>
<author confidence="0.929336">Dayne</author>
<affiliation confidence="0.992886">SRI</affiliation>
<address confidence="0.99656">San Diego, CA 92130,</address>
<email confidence="0.999839">dayne.freitag@sri.com</email>
<abstract confidence="0.999709090909091">In machine transliteration we transcribe a name across languages while maintaining its phonetic information. In this paper, we present a novel sequence transduction algorithm for the problem of machine transliteration. Our model is discriminatively trained by the MIRA algorithm, which improves the traditional Perceptron training in three ways: (1) It allows us to consider k-best transliterations instead of the best one. (2) It is trained based on the ranking of these transliterations according to user-specified loss function (Levenshtein edit distance). (3) It enables the user to tune a built-in parameter to cope with noisy non-separable data during training. On an Arabic-English name transliteration task, our model achieves a relative error reduction of 2.2% over a perceptron-based model with similar features, and an error reduction of 7.2% over a statistical machine translation model with more complex features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nasreen Abdul-Jaleel</author>
<author>Leah S Larkey</author>
</authors>
<title>Statistical transliteration for English-Arabic cross language information retrieval.</title>
<date>2003</date>
<booktitle>In CIKM ’03,</booktitle>
<pages>139--146</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1627" citStr="Abdul-Jaleel and Larkey, 2003" startWordPosition="226" endWordPosition="229"> a built-in parameter to cope with noisy non-separable data during training. On an Arabic-English name transliteration task, our model achieves a relative error reduction of 2.2% over a perceptron-based model with similar features, and an error reduction of 7.2% over a statistical machine translation model with more complex features. 1 Introduction and Related Work Proper names and other technical terms are frequently encountered in natural language text. Both machine translation (Knight and Graehl, 1997) and cross-language information retrieval (Jeong et al., 1999; Virga and Khudanpur, 2003; Abdul-Jaleel and Larkey, 2003) can benefit by explicitly translating such words from one language into another. This approach is decidedly better than treating them uniformly as out-of-vocabulary tokens. The goal of machine transliteration is to translate words between 61 alphabets of different languages such that they are phonetically equivalent. Given a source language sequence f = f1f2 ... f,,t from an alphabet .F, we want to produce a target language sequence e = e1e2 ... e,,, in the alphabet £ such that it maximizes some score function s(e, f), e = arg max e′ Virga and Khudanpur (2003) model this scoring function usin</context>
</contexts>
<marker>Abdul-Jaleel, Larkey, 2003</marker>
<rawString>Nasreen Abdul-Jaleel and Leah S. Larkey. 2003. Statistical transliteration for English-Arabic cross language information retrieval. In CIKM ’03, pages 139–146, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kevin Knight</author>
</authors>
<title>Machine transliteration of names in arabic text.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>1--13</pages>
<contexts>
<context position="2346" citStr="Al-Onaizan and Knight (2002)" startWordPosition="346" endWordPosition="349">oach is decidedly better than treating them uniformly as out-of-vocabulary tokens. The goal of machine transliteration is to translate words between 61 alphabets of different languages such that they are phonetically equivalent. Given a source language sequence f = f1f2 ... f,,t from an alphabet .F, we want to produce a target language sequence e = e1e2 ... e,,, in the alphabet £ such that it maximizes some score function s(e, f), e = arg max e′ Virga and Khudanpur (2003) model this scoring function using a separate translation and language model, that is, s(e,f) = Pr(f|e)Pr(e). In constrast, Al-Onaizan and Knight (2002) directly model the translation probability Pr(e|f) using a log-linear combination of several individually trained phrase and character-based models. Others have treated transliteration as a phrase-based transduction (Sherif and Kondrak, 2007). All these approaches are adaptations of statistical models for machine translation (Brown et al., 1994). In general, the parameters of the scoring function in such approaches are trained generatively and do not utilize complex features of the input sequence pairs. Recently, there has been interest in applying discriminatively-trained sequence alignment </context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Yaser Al-Onaizan and Kevin Knight. 2002. Machine transliteration of names in arabic text. In Proceedings of the ACL-02 Workshop on Computational Approaches to Semitic Languages, pages 1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Greg Kondrak</author>
</authors>
<title>Alignmentbased discriminative string similarity.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>656--663</pages>
<contexts>
<context position="3787" citStr="Bergsma and Kondrak (2007)" startWordPosition="554" endWordPosition="557">ments in this model requires finding “close” non-match pairs which can be a challenge. A similar conditional latent-variable model has been applied to the task of lemmatization and generation of morphological forms (Dreyer et al., 2008). s(e′, f). Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 61–65, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Zelenko and Aone (2006) model transliteration as a structured prediction problem where the letter ei is predicted using local and global features derived from e1e2 ... ei−1 and f. Bergsma and Kondrak (2007) address cognate identification by training a SVM classification model using phrase-based features obtained from a Levenshtein alignment. Both these models do not learn alignments that is needed to obtain high performance on transliteration tasks. Freitag and Khadivi (2007) describe a discriminatively trained sequence alignment model based on averaged perceptron, which is closely related to the method proposed in this paper. Our approach improves over previous directions in two ways. First, our system produces better k-best transliterations than related approaches by training on multiple hypot</context>
</contexts>
<marker>Bergsma, Kondrak, 2007</marker>
<rawString>Shane Bergsma and Greg Kondrak. 2007. Alignmentbased discriminative string similarity. In ACL, pages 656–663, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2694" citStr="Brown et al., 1994" startWordPosition="393" endWordPosition="396">.. e,,, in the alphabet £ such that it maximizes some score function s(e, f), e = arg max e′ Virga and Khudanpur (2003) model this scoring function using a separate translation and language model, that is, s(e,f) = Pr(f|e)Pr(e). In constrast, Al-Onaizan and Knight (2002) directly model the translation probability Pr(e|f) using a log-linear combination of several individually trained phrase and character-based models. Others have treated transliteration as a phrase-based transduction (Sherif and Kondrak, 2007). All these approaches are adaptations of statistical models for machine translation (Brown et al., 1994). In general, the parameters of the scoring function in such approaches are trained generatively and do not utilize complex features of the input sequence pairs. Recently, there has been interest in applying discriminatively-trained sequence alignment models to many real-world problems. McCallum et al. (2005) train a conditional random field model to discriminate between matching and non-matching string pairs treating alignments as latent. Learning accurate alignments in this model requires finding “close” non-match pairs which can be a challenge. A similar conditional latent-variable model ha</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1994</marker>
<rawString>Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1994. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Latent-variable modeling of string transductions with finite-state methods.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1080--1089</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="3397" citStr="Dreyer et al., 2008" startWordPosition="497" endWordPosition="500"> generatively and do not utilize complex features of the input sequence pairs. Recently, there has been interest in applying discriminatively-trained sequence alignment models to many real-world problems. McCallum et al. (2005) train a conditional random field model to discriminate between matching and non-matching string pairs treating alignments as latent. Learning accurate alignments in this model requires finding “close” non-match pairs which can be a challenge. A similar conditional latent-variable model has been applied to the task of lemmatization and generation of morphological forms (Dreyer et al., 2008). s(e′, f). Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 61–65, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Zelenko and Aone (2006) model transliteration as a structured prediction problem where the letter ei is predicted using local and global features derived from e1e2 ... ei−1 and f. Bergsma and Kondrak (2007) address cognate identification by training a SVM classification model using phrase-based features obtained from a Levenshtein alignment. Both these models do not learn alignments that is needed to obtain high </context>
</contexts>
<marker>Dreyer, Smith, Eisner, 2008</marker>
<rawString>Markus Dreyer, Jason Smith, and Jason Eisner. 2008. Latent-variable modeling of string transductions with finite-state methods. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1080–1089, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
<author>Shahram Khadivi</author>
</authors>
<title>A sequence alignment model based on the averaged perceptron. In EMNLP-CoNLL,</title>
<date>2007</date>
<pages>238--247</pages>
<contexts>
<context position="4061" citStr="Freitag and Khadivi (2007)" startWordPosition="593" endWordPosition="596">LT Student Research Workshop and Doctoral Consortium, pages 61–65, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Zelenko and Aone (2006) model transliteration as a structured prediction problem where the letter ei is predicted using local and global features derived from e1e2 ... ei−1 and f. Bergsma and Kondrak (2007) address cognate identification by training a SVM classification model using phrase-based features obtained from a Levenshtein alignment. Both these models do not learn alignments that is needed to obtain high performance on transliteration tasks. Freitag and Khadivi (2007) describe a discriminatively trained sequence alignment model based on averaged perceptron, which is closely related to the method proposed in this paper. Our approach improves over previous directions in two ways. First, our system produces better k-best transliterations than related approaches by training on multiple hypotheses ranked according to a userspecified loss function (Levenshtein edit distance). Hence, our method achieves a 19.2% error reduction in 5-best performance over a baseline only trained with 1-best transliterations. This is especially helpful when machine transliteration i</context>
<context position="5518" citStr="Freitag and Khadivi, 2007" startWordPosition="816" endWordPosition="819">ability in the data. Therefore, our transliteration system would work well in cases where person names were misspelled or in cases in which a single name had many reasonable translations in the foreign language. The training algorithm we propose in this paper is based on the K-best MIRA algorithm which has been used earlier in structured prediction problems (McDonald et al., 2005a; McDonald et al., 2005b). Our results demonstrate a significant improvement in accuracy of 7.2% over a statistical machine translation (SMT) system (Zens et al., 2005) and of 2.2% over a perceptron-based edit model (Freitag and Khadivi, 2007). 2 Sequence Alignment Model Let e = e1e2 ... en and f = f1f2 ... fm be sequences from the target alphabet E and source alphabet F respectively. Let a = a1a2 ... al be a sequence of alignment operations needed to convert f into e. Each alignment operation either appends a letter to the end of the source sequence, the target sequence or both sequences. Hence, it is a member of the cross-product ak ∈ E∪{ǫ}×F∪{ǫ}\{(ǫ, ǫ)}, where ǫ is the null character symbol. Let ak1 = a1a2 ... ak denote the sequence of first k alignment operations. Similarly ek1 and fk1 are prefixes of e and f of length k. We d</context>
<context position="7102" citStr="Freitag and Khadivi, 2007" startWordPosition="1128" endWordPosition="1131">ak, e, i, f, j) be the sum of feature vectors associated with individual alignment operations. Here i, j are positions in sequences e, f after performing operations ak1. For fixed sequences e and f the function s(e, f) can be efficiently computed using a dynamic programming algorithm, s(ei 1,fj 1) = s(ei−1 1 ,fj1) + w · φ(hei,ǫi, e, i, f, j) s(ei1, fj−1 1 ) + w · φ(hǫ, fji, e, i, f, j) s(ei−1 1 ,fj−1 1 ) + w · φ(hei,fji, e, i, f, j). (1) Given a source sequence f computing the best scoring target sequence e = arg maxe′ s(e′, f) among all possible sequences E∗ requires a beam search procedure (Freitag and Khadivi, 2007). This procedure can also be used to produce K-best target sequences {e′1, e′2, ... , e′K} such that s(e′1, f) ≥ s(e′2,f) ≥ ... ≥ s(e′ K, f). In this paper, we employ the same features as those used by Freitag and Khadivi (2007). All local feature functions φ(ak, e, i, f, j) are conjunctions of the alignment operation ak and forward or backward-looking character m-grams in sequences e and f at positions i and j respectively. For the source sequence f both forward and backwardlooking m-gram features are included. We restrict the m-gram features in our target sequence e to only s(a, e, f) , I ma</context>
<context position="9510" citStr="Freitag and Khadivi, 2007" startWordPosition="1552" endWordPosition="1555">uring training. We find that averaging helps to improve performance. At test time, we use the beam search procedure to produce Kbest hypotheses using the parameter vector u. 3 Experimental Results We apply our model to the real-world ArabicEnglish name transliteration task on a data set of 10,084 Arabic names from the LDC. The data set consists of Arabic names in an ASCII-based alphabet and its English rendering. Table 1 shows a few examples of Arabic-English pairs in our data set. We use the same training/development/testing (8084/1000/1000) set as the one used in a previous benchmark study (Freitag and Khadivi, 2007). The development and testing data were obtained by randomly removing entries from the training data. The absence of short vowels (e.g. “a” in hNB”I, nab’ii), doubled consonants (e.g. “ww” in hFWAL, fawwali) and other diacritics in Arabic make the transliteration a hard problem. Therefore, it is hard to achieve perfect accuracy on this data set. For training, we set K = 20 best hypotheses and Input parameters Training Data D Complexity parameter C &gt; 0 Number of epochs T Initialize w0 = 0 (zero vector) ; τ = 0 ; u = 0 Repeat T times: For Each (e, f) ∈ D : 1. a = arg maxˆa wτ · Φ(ˆa, e, f) (Find</context>
<context position="11544" citStr="Freitag and Khadivi, 2007" startWordPosition="1913" endWordPosition="1916">thm for T = 10 epochs. To evaluate our algorithm, we generate 1-best (or 5- best) hypotheses using the beam search procedure and measure accuracy as the percentage of instances in which the target sequence e is one of the 1-best (or 5-best) targets. The input features are based on character m-grams for m = 1, 2, 3. Unlike previ63 ous generative transliteration models, no additional language model feature is used. We compare our model against a state-of-the-art statistical machine translation (SMT) system (Zens et al., 2005) and an averaged perceptron edit model (PTEM) with identical features (Freitag and Khadivi, 2007). The SMT system directly models the posterior probability Pr(e|f) using a log-linear combination of several sub-models: a characterbased phrase translation model, a character-based lexicon model, a character penalty and a phrase penalty. In the PTEM model, the update rule only considers the best target sequence and modifies the parameters wτ+1 = wτ + Φ(a, e, f) − Φ(a′, e′, f) if the score s(e′, f) ≥ s(e, f). Model (train+dev) 1-best 5-best SMT 0.528 0.824 PTEM 0.552 0.803 MIRA 0.562 0.841 Table 2: The 1-best and 5-best accuracy of different models on the Arabic-English transliteration task. A</context>
</contexts>
<marker>Freitag, Khadivi, 2007</marker>
<rawString>Dayne Freitag and Shahram Khadivi. 2007. A sequence alignment model based on the averaged perceptron. In EMNLP-CoNLL, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Jeong</author>
<author>S H Myaeng</author>
<author>J S Lee</author>
<author>K-S Choi</author>
</authors>
<title>Automatic identification and backtransliteration of foreign words for information retrieval.</title>
<date>1999</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>35--523</pages>
<contexts>
<context position="1568" citStr="Jeong et al., 1999" startWordPosition="218" endWordPosition="221">edit distance). (3) It enables the user to tune a built-in parameter to cope with noisy non-separable data during training. On an Arabic-English name transliteration task, our model achieves a relative error reduction of 2.2% over a perceptron-based model with similar features, and an error reduction of 7.2% over a statistical machine translation model with more complex features. 1 Introduction and Related Work Proper names and other technical terms are frequently encountered in natural language text. Both machine translation (Knight and Graehl, 1997) and cross-language information retrieval (Jeong et al., 1999; Virga and Khudanpur, 2003; Abdul-Jaleel and Larkey, 2003) can benefit by explicitly translating such words from one language into another. This approach is decidedly better than treating them uniformly as out-of-vocabulary tokens. The goal of machine transliteration is to translate words between 61 alphabets of different languages such that they are phonetically equivalent. Given a source language sequence f = f1f2 ... f,,t from an alphabet .F, we want to produce a target language sequence e = e1e2 ... e,,, in the alphabet £ such that it maximizes some score function s(e, f), e = arg max e′ </context>
</contexts>
<marker>Jeong, Myaeng, Lee, Choi, 1999</marker>
<rawString>K.S. Jeong, S. H. Myaeng, J.S. Lee, and K.-S. Choi. 1999. Automatic identification and backtransliteration of foreign words for information retrieval. Information Processing and Management, 35:523–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<title>Machine transliteration.</title>
<date>1997</date>
<booktitle>Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter ofthe Association for Computational Linguistics,</booktitle>
<pages>128--135</pages>
<editor>In Philip R. Cohen and Wolfgang Wahlster, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="1507" citStr="Knight and Graehl, 1997" startWordPosition="210" endWordPosition="213">literations according to user-specified loss function (Levenshtein edit distance). (3) It enables the user to tune a built-in parameter to cope with noisy non-separable data during training. On an Arabic-English name transliteration task, our model achieves a relative error reduction of 2.2% over a perceptron-based model with similar features, and an error reduction of 7.2% over a statistical machine translation model with more complex features. 1 Introduction and Related Work Proper names and other technical terms are frequently encountered in natural language text. Both machine translation (Knight and Graehl, 1997) and cross-language information retrieval (Jeong et al., 1999; Virga and Khudanpur, 2003; Abdul-Jaleel and Larkey, 2003) can benefit by explicitly translating such words from one language into another. This approach is decidedly better than treating them uniformly as out-of-vocabulary tokens. The goal of machine transliteration is to translate words between 61 alphabets of different languages such that they are phonetically equivalent. Given a source language sequence f = f1f2 ... f,,t from an alphabet .F, we want to produce a target language sequence e = e1e2 ... e,,, in the alphabet £ such t</context>
</contexts>
<marker>Knight, Graehl, 1997</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1997. Machine transliteration. In Philip R. Cohen and Wolfgang Wahlster, editors, Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter ofthe Association for Computational Linguistics, pages 128–135, Somerset, New Jersey. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="8397" citStr="Levenshtein, 1966" startWordPosition="1361" endWordPosition="1363">ms during beam-search. An order M model is one that uses m-gram features where m = 0,1, ... M. Our training algorithm takes as input a data set D of source-target transliteration pairs and outputs a parameter vector u. The algorithm pseudo-code appears in Fig. (1). In the algorithm, the function L(e′, e) defines a loss incurred by predicting e′ instead of e. In most structured prediction problems, the targets are of equal length and in such cases the Hamming loss function can be used. However, in our case the targets may differ in terms of length and thus we use the Levenshtein edit distance (Levenshtein, 1966) with unit costs for insertions, deletions and substitutions. Since the targets are both in the same alphabet E this loss function is well-defined. The user also supplies three paramters: (1) T - the number of training iterations (2) K - the number of best target hypotheses used (3) C - a complexity parameter. A low C is useful if the data is nonseparable and noisy. The final parameter vector u returned by the algorithm is the average of the intermediate parameter vectors produced during training. We find that averaging helps to improve performance. At test time, we use the beam search procedu</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Kedar Bellare</author>
<author>Fernando Pereira</author>
</authors>
<title>A conditional random field for discriminativelytrained finite-state string edit distance.</title>
<date>2005</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="3004" citStr="McCallum et al. (2005)" startWordPosition="440" endWordPosition="443">ability Pr(e|f) using a log-linear combination of several individually trained phrase and character-based models. Others have treated transliteration as a phrase-based transduction (Sherif and Kondrak, 2007). All these approaches are adaptations of statistical models for machine translation (Brown et al., 1994). In general, the parameters of the scoring function in such approaches are trained generatively and do not utilize complex features of the input sequence pairs. Recently, there has been interest in applying discriminatively-trained sequence alignment models to many real-world problems. McCallum et al. (2005) train a conditional random field model to discriminate between matching and non-matching string pairs treating alignments as latent. Learning accurate alignments in this model requires finding “close” non-match pairs which can be a challenge. A similar conditional latent-variable model has been applied to the task of lemmatization and generation of morphological forms (Dreyer et al., 2008). s(e′, f). Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 61–65, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Zelenko and Aone (2006)</context>
</contexts>
<marker>McCallum, Bellare, Pereira, 2005</marker>
<rawString>Andrew McCallum, Kedar Bellare, and Fernando Pereira. 2005. A conditional random field for discriminativelytrained finite-state string edit distance. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Flexible text segmentation with structured multilabel classification.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP,</booktitle>
<pages>987--994</pages>
<location>Vancouver, BC, Canada,</location>
<contexts>
<context position="5274" citStr="McDonald et al., 2005" startWordPosition="778" endWordPosition="781">ration is part of a larger machine translation or information retrieval pipeline since additional sentence context can be used to choose the best among top-K transliterations. Second, our training procedure accounts for noise and non-separability in the data. Therefore, our transliteration system would work well in cases where person names were misspelled or in cases in which a single name had many reasonable translations in the foreign language. The training algorithm we propose in this paper is based on the K-best MIRA algorithm which has been used earlier in structured prediction problems (McDonald et al., 2005a; McDonald et al., 2005b). Our results demonstrate a significant improvement in accuracy of 7.2% over a statistical machine translation (SMT) system (Zens et al., 2005) and of 2.2% over a perceptron-based edit model (Freitag and Khadivi, 2007). 2 Sequence Alignment Model Let e = e1e2 ... en and f = f1f2 ... fm be sequences from the target alphabet E and source alphabet F respectively. Let a = a1a2 ... al be a sequence of alignment operations needed to convert f into e. Each alignment operation either appends a letter to the end of the source sequence, the target sequence or both sequences. He</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Flexible text segmentation with structured multilabel classification. In HLT-EMNLP, pages 987– 994, Vancouver, BC, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>91--98</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="5274" citStr="McDonald et al., 2005" startWordPosition="778" endWordPosition="781">ration is part of a larger machine translation or information retrieval pipeline since additional sentence context can be used to choose the best among top-K transliterations. Second, our training procedure accounts for noise and non-separability in the data. Therefore, our transliteration system would work well in cases where person names were misspelled or in cases in which a single name had many reasonable translations in the foreign language. The training algorithm we propose in this paper is based on the K-best MIRA algorithm which has been used earlier in structured prediction problems (McDonald et al., 2005a; McDonald et al., 2005b). Our results demonstrate a significant improvement in accuracy of 7.2% over a statistical machine translation (SMT) system (Zens et al., 2005) and of 2.2% over a perceptron-based edit model (Freitag and Khadivi, 2007). 2 Sequence Alignment Model Let e = e1e2 ... en and f = f1f2 ... fm be sequences from the target alphabet E and source alphabet F respectively. Let a = a1a2 ... al be a sequence of alignment operations needed to convert f into e. Each alignment operation either appends a letter to the end of the source sequence, the target sequence or both sequences. He</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005b. Online large-margin training of dependency parsers. In ACL, pages 91–98, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tarek Sherif</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Substringbased transliteration.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>944--951</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2589" citStr="Sherif and Kondrak, 2007" startWordPosition="376" endWordPosition="379">nguage sequence f = f1f2 ... f,,t from an alphabet .F, we want to produce a target language sequence e = e1e2 ... e,,, in the alphabet £ such that it maximizes some score function s(e, f), e = arg max e′ Virga and Khudanpur (2003) model this scoring function using a separate translation and language model, that is, s(e,f) = Pr(f|e)Pr(e). In constrast, Al-Onaizan and Knight (2002) directly model the translation probability Pr(e|f) using a log-linear combination of several individually trained phrase and character-based models. Others have treated transliteration as a phrase-based transduction (Sherif and Kondrak, 2007). All these approaches are adaptations of statistical models for machine translation (Brown et al., 1994). In general, the parameters of the scoring function in such approaches are trained generatively and do not utilize complex features of the input sequence pairs. Recently, there has been interest in applying discriminatively-trained sequence alignment models to many real-world problems. McCallum et al. (2005) train a conditional random field model to discriminate between matching and non-matching string pairs treating alignments as latent. Learning accurate alignments in this model requires</context>
</contexts>
<marker>Sherif, Kondrak, 2007</marker>
<rawString>Tarek Sherif and Grzegorz Kondrak. 2007. Substringbased transliteration. In ACL, pages 944–951, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Virga</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Transliteration of proper names in cross-lingual information retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 workshop on Multilingual and Mixed-language Named Entity Recognition,</booktitle>
<pages>57--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1595" citStr="Virga and Khudanpur, 2003" startWordPosition="222" endWordPosition="225">It enables the user to tune a built-in parameter to cope with noisy non-separable data during training. On an Arabic-English name transliteration task, our model achieves a relative error reduction of 2.2% over a perceptron-based model with similar features, and an error reduction of 7.2% over a statistical machine translation model with more complex features. 1 Introduction and Related Work Proper names and other technical terms are frequently encountered in natural language text. Both machine translation (Knight and Graehl, 1997) and cross-language information retrieval (Jeong et al., 1999; Virga and Khudanpur, 2003; Abdul-Jaleel and Larkey, 2003) can benefit by explicitly translating such words from one language into another. This approach is decidedly better than treating them uniformly as out-of-vocabulary tokens. The goal of machine transliteration is to translate words between 61 alphabets of different languages such that they are phonetically equivalent. Given a source language sequence f = f1f2 ... f,,t from an alphabet .F, we want to produce a target language sequence e = e1e2 ... e,,, in the alphabet £ such that it maximizes some score function s(e, f), e = arg max e′ Virga and Khudanpur (2003) </context>
</contexts>
<marker>Virga, Khudanpur, 2003</marker>
<rawString>Paola Virga and Sanjeev Khudanpur. 2003. Transliteration of proper names in cross-lingual information retrieval. In Proceedings of the ACL 2003 workshop on Multilingual and Mixed-language Named Entity Recognition, pages 57–64, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
</authors>
<title>Discriminative methods for transliteration.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<pages>612--617</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="3604" citStr="Zelenko and Aone (2006)" startWordPosition="525" endWordPosition="528"> McCallum et al. (2005) train a conditional random field model to discriminate between matching and non-matching string pairs treating alignments as latent. Learning accurate alignments in this model requires finding “close” non-match pairs which can be a challenge. A similar conditional latent-variable model has been applied to the task of lemmatization and generation of morphological forms (Dreyer et al., 2008). s(e′, f). Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 61–65, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics Zelenko and Aone (2006) model transliteration as a structured prediction problem where the letter ei is predicted using local and global features derived from e1e2 ... ei−1 and f. Bergsma and Kondrak (2007) address cognate identification by training a SVM classification model using phrase-based features obtained from a Levenshtein alignment. Both these models do not learn alignments that is needed to obtain high performance on transliteration tasks. Freitag and Khadivi (2007) describe a discriminatively trained sequence alignment model based on averaged perceptron, which is closely related to the method proposed in </context>
</contexts>
<marker>Zelenko, Aone, 2006</marker>
<rawString>Dmitry Zelenko and Chinatsu Aone. 2006. Discriminative methods for transliteration. In EMNLP, pages 612–617, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>O Bender</author>
<author>S Hasan</author>
<author>S Khadivi</author>
<author>E Matusov</author>
<author>J Xu</author>
<author>Y Zhang</author>
<author>H Ney</author>
</authors>
<title>The RWTH Phrase-based Statistical Machine Translation System.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="5443" citStr="Zens et al., 2005" startWordPosition="804" endWordPosition="807">ns. Second, our training procedure accounts for noise and non-separability in the data. Therefore, our transliteration system would work well in cases where person names were misspelled or in cases in which a single name had many reasonable translations in the foreign language. The training algorithm we propose in this paper is based on the K-best MIRA algorithm which has been used earlier in structured prediction problems (McDonald et al., 2005a; McDonald et al., 2005b). Our results demonstrate a significant improvement in accuracy of 7.2% over a statistical machine translation (SMT) system (Zens et al., 2005) and of 2.2% over a perceptron-based edit model (Freitag and Khadivi, 2007). 2 Sequence Alignment Model Let e = e1e2 ... en and f = f1f2 ... fm be sequences from the target alphabet E and source alphabet F respectively. Let a = a1a2 ... al be a sequence of alignment operations needed to convert f into e. Each alignment operation either appends a letter to the end of the source sequence, the target sequence or both sequences. Hence, it is a member of the cross-product ak ∈ E∪{ǫ}×F∪{ǫ}\{(ǫ, ǫ)}, where ǫ is the null character symbol. Let ak1 = a1a2 ... ak denote the sequence of first k alignment </context>
<context position="11447" citStr="Zens et al., 2005" startWordPosition="1899" endWordPosition="1902">ames in the ASCII alphabet and their English transliterations. C = 1.0 and run the algorithm for T = 10 epochs. To evaluate our algorithm, we generate 1-best (or 5- best) hypotheses using the beam search procedure and measure accuracy as the percentage of instances in which the target sequence e is one of the 1-best (or 5-best) targets. The input features are based on character m-grams for m = 1, 2, 3. Unlike previ63 ous generative transliteration models, no additional language model feature is used. We compare our model against a state-of-the-art statistical machine translation (SMT) system (Zens et al., 2005) and an averaged perceptron edit model (PTEM) with identical features (Freitag and Khadivi, 2007). The SMT system directly models the posterior probability Pr(e|f) using a log-linear combination of several sub-models: a characterbased phrase translation model, a character-based lexicon model, a character penalty and a phrase penalty. In the PTEM model, the update rule only considers the best target sequence and modifies the parameters wτ+1 = wτ + Φ(a, e, f) − Φ(a′, e′, f) if the score s(e′, f) ≥ s(e, f). Model (train+dev) 1-best 5-best SMT 0.528 0.824 PTEM 0.552 0.803 MIRA 0.562 0.841 Table 2:</context>
</contexts>
<marker>Zens, Bender, Hasan, Khadivi, Matusov, Xu, Zhang, Ney, 2005</marker>
<rawString>R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov, J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH Phrase-based Statistical Machine Translation System. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), Pittsburgh, PA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>