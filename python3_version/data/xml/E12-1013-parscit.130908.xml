<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.928923">
Computing Lattice BLEU Oracle Scores for Machine Translation
</title>
<note confidence="0.787275333333333">
Artem Sokolov Guillaume Wisniewski Franc¸ois Yvon
LIMSI-CNRS &amp; Univ. Paris Sud
BP-133, 91 403 Orsay, France
</note>
<email confidence="0.957692">
{firstname.lastname}@limsi.fr
</email>
<sectionHeader confidence="0.982227" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999810681818182">
The search space of Phrase-Based Statisti-
cal Machine Translation (PBSMT) systems
can be represented under the form of a di-
rected acyclic graph (lattice). The quality
of this search space can thus be evaluated
by computing the best achievable hypoth-
esis in the lattice, the so-called oracle hy-
pothesis. For common SMT metrics, this
problem is however NP-hard and can only
be solved using heuristics. In this work,
we present two new methods for efficiently
computing BLEU oracles on lattices: the
first one is based on a linear approximation
of the corpus BLEU score and is solved us-
ing the FST formalism; the second one re-
lies on integer linear programming formu-
lation and is solved directly and using the
Lagrangian relaxation framework. These
new decoders are positively evaluated and
compared with several alternatives from the
literature for three language pairs, using lat-
tices produced by two PBSMT systems.
</bodyText>
<sectionHeader confidence="0.992532" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999567">
The search space of Phrase-Based Statistical Ma-
chine Translation (PBSMT) systems has the form
of a very large directed acyclic graph. In several
softwares, an approximation of this search space
can be outputted, either as a n-best list contain-
ing the n top hypotheses found by the decoder, or
as a phrase or word graph (lattice) which com-
pactly encodes those hypotheses that have sur-
vived search space pruning. Lattices usually con-
tain much more hypotheses than n-best lists and
better approximate the search space.
Exploring the PBSMT search space is one of
the few means to perform diagnostic analysis and
to better understand the behavior of the system
(Turchi et al., 2008; Auli et al., 2009). Useful
diagnostics are, for instance, provided by look-
ing at the best (oracle) hypotheses contained in
the search space, i.e, those hypotheses that have
the highest quality score with respect to one or
several references. Such oracle hypotheses can
be used for failure analysis and to better under-
stand the bottlenecks of existing translation sys-
tems (Wisniewski et al., 2010). Indeed, the in-
ability to faithfully reproduce reference transla-
tions can have many causes, such as scantiness
of the translation table, insufficient expressiveness
of reordering models, inadequate scoring func-
tion, non-literal references, over-pruned lattices,
etc. Oracle decoding has several other applica-
tions: for instance, in (Liang et al., 2006; Chi-
ang et al., 2008) it is used as a work-around to
the problem of non-reachability of the reference
in discriminative training of MT systems. Lattice
reranking (Li and Khudanpur, 2009), a promising
way to improve MT systems, also relies on oracle
decoding to build the training data for a reranking
algorithm.
For sentence level metrics, finding oracle hy-
potheses in n-best lists is a simple issue; how-
ever, solving this problem on lattices proves much
more challenging, due to the number of embed-
ded hypotheses, which prevents the use of brute-
force approaches. When using BLEU, or rather
sentence-level approximations thereof, the prob-
lem is in fact known to be NP-hard (Leusch et
al., 2008). This complexity stems from the fact
that the contribution of a given edge to the total
modified n-gram precision can not be computed
without looking at all other edges on the path.
Similar (or worse) complexity result are expected
</bodyText>
<note confidence="0.790046">
120
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 120–129,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999242735294118">
for other metrics such as METEOR (Banerjee and
Lavie, 2005) or TER (Snover et al., 2006). The
exact computation of oracles under corpus level
metrics, such as BLEU, poses supplementary com-
binatorial problems that will not be addressed in
this work.
In this paper, we present two original methods
for finding approximate oracle hypotheses on lat-
tices. The first one is based on a linear approxima-
tion of the corpus BLEU, that was originally de-
signed for efficient Minimum Bayesian Risk de-
coding on lattices (Tromble et al., 2008). The sec-
ond one, based on Integer Linear Programming, is
an extension to lattices of a recent work on failure
analysis for phrase-based decoders (Wisniewski
et al., 2010). In this framework, we study two
decoding strategies: one based on a generic ILP
solver, and one, based on Lagrangian relaxation.
Our contribution is also experimental as we
compare the quality of the BLEU approxima-
tions and the time performance of these new ap-
proaches with several existing methods, for differ-
ent language pairs and using the lattice generation
capacities of two publicly-available state-of-the-
art phrase-based decoders: Moses1 and N-code2.
The rest of this paper is organized as follows.
In Section 2, we formally define the oracle decod-
ing task and recall the formalism of finite state
automata on semirings. We then describe (Sec-
tion 3) two existing approaches for solving this
task, before detailing our new proposals in sec-
tions 4 and 5. We then report evaluations of the
existing and new oracles on machine translation
tasks.
</bodyText>
<sectionHeader confidence="0.993829" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.99233">
2.1 Oracle Decoding Task
</subsectionHeader>
<bodyText confidence="0.999865333333333">
We assume that a phrase-based decoder is able
to produce, for each source sentence f, a lattice
Lf = (Q, Ξ), with # {Q} vertices (states) and
# {Ξ} edges. Each edge carries a source phrase
fi, an associated output phrase ei as well as a fea-
ture vector ¯hi, the components of which encode
various compatibility measures between fi and ei.
We further assume that Lf is a word lattice,
meaning that each ei carries a single word3 and
</bodyText>
<footnote confidence="0.66861775">
1http://www.statmt.org/moses/
2http://ncode.limsi.fr/
3Converting a phrase lattice to a word lattice is a simple
matter of redistributing a compound input or output over a
</footnote>
<bodyText confidence="0.983538291666667">
that it contains a unique initial state q0 and a
unique final state qF. Let Πf denote the set of all
paths from q0 to qF in Lf. Each path ,7r E Πf cor-
responds to a possible translation e,r. The job of
a (conventional) decoder is to find the best path(s)
in Lf using scores that combine the edges’ fea-
ture vectors with the parameters A¯ learned during
tuning.
In oracle decoding, the decoder’s job is quite
different, as we assume that at least a reference
rf is provided to evaluate the quality of each indi-
vidual hypothesis. The decoder therefore aims at
finding the path 7r* that generates the hypothesis
that best matches rf. For this task, only the output
labels ei will matter, the other informations can be
left aside.4
Oracle decoding assumes the definition of a
measure of the similarity between a reference
and a hypothesis. In this paper we will con-
sider sentence-level approximations of the popu-
lar BLEU score (Papineni et al., 2002). BLEU is
formally defined for two parallel corpora, £ =
{ej} j�1 and R = {rj} j�1, each containing J
sentences as:
</bodyText>
<equation confidence="0.989946666666667">
� n
n-BLEU(£, R) = BP ri pm
m�1
</equation>
<bodyText confidence="0.871638375">
where BP = min(1, e1−e1(1z.)/e1(6)) is the
brevity penalty and pm = cm(£, R)/cm(£) are
clipped or modified m-gram precisions: cm(£) is
the total number of word m-grams in £; cm(£, R)
accumulates over sentences the number of m-
grams in ej that also belong to rj. These counts
are clipped, meaning that a m-gram that appears
k times in £ and l times in R, with k &gt; l, is only
counted l times. As it is well known, BLEU per-
forms a compromise between precision, which is
directly appears in Equation (1), and recall, which
is indirectly taken into account via the brevity
penalty. In most cases, Equation (1) is computed
with n = 4 and we use BLEU as a synonym for
4-BLEU.
BLEU is defined for a pair of corpora, but, as an
oracle decoder is working at the sentence-level, it
should rely on an approximation of BLEU that can
linear chain of arcs.
4The algorithms described below can be straightfor-
wardly generalized to compute oracle hypotheses under
combined metrics mixing model scores and quality measures
(Chiang et al., 2008), by weighting each edge with its model
score and by using these weights down the pipe.
</bodyText>
<equation confidence="0.812036">
1/n
, (1)
121
</equation>
<bodyText confidence="0.9985724">
evaluate the similarity between a single hypoth-
esis and its reference. This approximation intro-
duces a discrepancy as gathering sentences with
the highest (local) approximation may not result
in the highest possible (corpus-level) BLEU score.
Let BLEU&apos; be such a sentence-level approximation
of BLEU. Then lattice oracle decoding is the task
of finding an optimal path π*(f) among all paths
IIf for a given f, and amounts to the following
optimization problem:
</bodyText>
<equation confidence="0.975513">
π*(f) = argmax BLEU&apos;(e,, rf). (2)
7rEIIf
</equation>
<subsectionHeader confidence="0.999353">
2.2 Compromises of Oracle Decoding
</subsectionHeader>
<bodyText confidence="0.999993428571428">
As proved by Leusch et al. (2008), even with
brevity penalty dropped, the problem of deciding
whether a confusion network contains a hypoth-
esis with clipped uni- and bigram precisions all
equal to 1.0 is NP-complete (and so is the asso-
ciated optimization problem of oracle decoding
for 2-BLEU). The case of more general word and
phrase lattices and 4-BLEU score is consequently
also NP-complete. This complexity stems from
chaining up of local unigram decisions that, due
to the clipping constraints, have non-local effect
on the bigram precision scores. It is consequently
necessary to keep a possibly exponential num-
ber of non-recombinable hypotheses (character-
ized by counts for each n-gram in the reference)
until very late states in the lattice.
These complexity results imply that any oracle
decoder has to waive either the form of the objec-
tive function, replacing BLEU with better-behaved
scoring functions, or the exactness of the solu-
tion, relying on approximate heuristic search al-
gorithms.
In Table 1, we summarize different compro-
mises that the existing (section 3), as well as
our novel (sections 4 and 5) oracle decoders,
have to make. The “target” and “target level”
columns specify the targeted score. None of
the decoders optimizes it directly: their objec-
tive function is rather the approximation of BLEU
given in the “target replacement” column. Col-
umn “search” details the accuracy of the target re-
placement optimization. Finally, columns “clip-
ping” and “brevity” indicate whether the corre-
sponding properties of BLEU score are considered
in the target substitute and in the search algorithm.
</bodyText>
<subsectionHeader confidence="0.999024">
2.3 Finite State Acceptors
</subsectionHeader>
<bodyText confidence="0.9860375">
The implementations of the oracles described in
the first part of this work (sections 3 and 4) use the
common formalism of finite state acceptors (FSA)
over different semirings and are implemented us-
ing the generic OpenFST toolbox (Allauzen et al.,
2007).
A (®, ®)-semiring K over a set K is a system
(K, ®, ®, 0, 1), where (K, ®, 0) is a commutative
monoid with identity element 0, and (K, ®,1) is
a monoid with identity element 1. ® distributes
over®, so that a®(b®c) = (a®b)®(a®c)
and (b ® c) ® a = (b ® a) ® (c ® a) and element
0 annihilates K (a ® 0 = 0 ® a = 0).
Let A = (E, Q, I, F, E) be a weighted finite-
state acceptor with labels in E and weights in K,
meaning that the transitions (q, Q, q&apos;) in A carry a
weight w E K. Formally, E is a mapping from
(Q x E x Q) into K; likewise, initial I and fi-
nal weight F functions are mappings from Q into
K. We borrow the notations of Mohri (2009):
if � = (q, a, q&apos;) is a transition in domain(E),
p(�) = q (resp. n(�) = q&apos;) denotes its origin
(resp. destination) state, w(�) = Q its label and
E(�) its weight. These notations extend to paths:
if π is a path in A, p(π) (resp. n(π)) is its initial
(resp. ending) state and w(π) is the label along
the path. A finite state transducer (FST) is an FSA
with output alphabet, so that each transition car-
ries a pair of input/output symbols.
As discussed in Sections 3 and 4, several oracle
decoding algorithms can be expressed as shortest-
path problems, provided a suitable definition of
the underlying acceptor and associated semiring.
In particular, quantities such as:
</bodyText>
<equation confidence="0.9979685">
� E(π), (3)
7rEII(A)
</equation>
<bodyText confidence="0.89736">
where the total weight of a successful path π =
</bodyText>
<equation confidence="0.939061">
�1 ... �l in A is computed as:
E(π) =I(p(�1)) ® �l E(�i)] ® F(n(�l))
i=1
</equation>
<bodyText confidence="0.999874714285714">
can be efficiently found by generic shortest dis-
tance algorithms over acyclic graphs (Mohri,
2002). For FSA-based implementations over
semirings where ® = max, the optimization
problem (2) is thus reduced to Equation (3), while
the oracle-specific details can be incorporated into
in the definition of ®.
</bodyText>
<page confidence="0.650053">
122
</page>
<table confidence="0.998648">
oracle target target level target replacement search clipping brevity
existing LM-2g/4g 2/4-BLEU sentence P2(e; r) or P4(e; r) exact no no
PB 4-BLEU sentence partial log BLEU (4) appr. no no
PBE 4-BLEU sentence partial log BLEU (4) appr. no yes
this paper LB-2g/4g 2/4-BLEU corpus linear appr. lin BLEU (5) exact no yes
SP 1-BLEU sentence unigram count exact no yes
ILP 2-BLEU sentence uni/bi-gram counts (7) appr. yes yes
RLX 2-BLEU sentence uni/bi-gram counts (8) exact yes yes
</table>
<tableCaption confidence="0.999766">
Table 1: Recapitulative overview of oracle decoders.
</tableCaption>
<sectionHeader confidence="0.966096" genericHeader="method">
3 Existing Algorithms
</sectionHeader>
<bodyText confidence="0.999969142857143">
In this section, we describe our reimplementation
of two approximate search algorithms that have
been proposed in the literature to solve the oracle
decoding problem for BLEU. In addition to their
approximate nature, none of them accounts for the
fact that the count of each matching word has to
be clipped.
</bodyText>
<subsectionHeader confidence="0.986141">
3.1 Language Model Oracle (LM)
</subsectionHeader>
<bodyText confidence="0.99825595">
The simplest approach we consider is introduced
in (Li and Khudanpur, 2009), where oracle decod-
ing is reduced to the problem of finding the most
likely hypothesis under a n-gram language model
trained with the sole reference translation.
Let us suppose we have a n-gram language
model that gives a probability P(en|e1 ... en−1)
of word en given the n − 1 previous words.
The probability of a hypothesis e is then
Pn(e|r) = Hi=1 P(ei+n|ei ... ei+n−1). The lan-
guage model can conveniently be represented as a
FSA ALM, with each arc carrying a negative log-
probability weight and with additional p-type fail-
ure transitions to accommodate for back-off arcs.
If we train, for each source sentence f, a sepa-
rate language model ALM(rf) using only the ref-
erence rf, oracle decoding amounts to finding a
shortest (most probable) path in the weighted FSA
resulting from the composition L o ALM(rf) over
the (min, +)-semiring:
</bodyText>
<equation confidence="0.991669">
7riM(f) = ShortestPath(L o ALM(rf)).
</equation>
<bodyText confidence="0.999738666666667">
This approach replaces the optimization of n-
BLEU with a search for the most probable path
under a simplistic n-gram language model. One
may expect the most probable path to select fre-
quent n-gram from the reference, thus augment-
ing n-BLEU.
</bodyText>
<subsectionHeader confidence="0.997908">
3.2 Partial BLEU Oracle (PB)
</subsectionHeader>
<bodyText confidence="0.999981666666667">
Another approach is put forward in (Dreyer et
al., 2007) and used in (Li and Khudanpur, 2009):
oracle translations are shortest paths in a lattice
L, where the weight of each path -7r is the sen-
tence level log BLEU(π) score of the correspond-
ing complete or partial hypothesis:
</bodyText>
<equation confidence="0.96032">
1 � log BLEU(π) = 4 log pm. (4)
m=1...4
</equation>
<bodyText confidence="0.981774689655172">
Here, the brevity penalty is ignored and n-
gram precisions are offset to avoid null counts:
pm = (cm(e, r) + 0.1)/(cm(e,) + 0.1).
This approach has been reimplemented using
the FST formalism by defining a suitable semir-
ing. Let each weight of the semiring keep a set
of tuples accumulated up to the current state of
the lattice. Each tuple contains three words of re-
cent history, a partial hypothesis as well as current
values of the length of the partial hypothesis, n-
gram counts (4 numbers) and the sentence-level
log BLEU score defined by Equation (4). In the
beginning each arc is initialized with a singleton
set containing one tuple with a single word as the
partial hypothesis. For the semiring operations we
define one common ®-operation and two versions
of the ®-operation:
• L1 ®PB L2 – appends a word on the edge of
L2 to L1’s hypotheses, shifts their recent histories
and updates n-gram counts, lengths, and current
score; • L1 ®PB L2 – merges all sets from L1
and L2 and recombinates those having the same
recent history; • L1 ®PB` L2 – merges all sets
from L1 and L2 and recombinates those having
the same recent history and the same hypothesis
length.
If several hypotheses have the same recent
history (and length in the case of ®PB`), re-
combination removes all of them, but the one
</bodyText>
<figure confidence="0.992579965517241">
123
1:1/0
0:0/0
q0
(a) 01
1:j0
1:111/0
(b) 02
(c) 03
0:00/0 1:11/0
1
1:101/0
0:j0 10
11
1:j0
0:110/0
1:01/0
0:ε/0
0 0:10/0 1
0:100/0
q0
1:011/0
0:j0
0:000/0
0:j0
q0
1:ε/0
0 1:j0 01 0:010/0 00
1:001/0
</figure>
<figureCaption confidence="0.987346">
Figure 1: Examples of the A, automata for E = {0, 1} and n = 1... 3. Initial and final states are marked,
respectively, with bold and with double borders. Note that arcs between final states are weighted with 0, while in
reality they will have this weight only if the corresponding n-gram does not appear in the reference.
</figureCaption>
<bodyText confidence="0.9999662">
with the largest current BLEU score. Optimal
path is then found by launching the generic
ShortestDistance(L) algorithm over one of
the semirings above.
The (⊕PB`, ⊗PB)-semiring, in which the
equal length requirement also implies equal
brevity penalties, is more conservative in recom-
bining hypotheses and should achieve final BLEU
that is least as good as that obtained with the
(⊕PB, ⊗PB)-semiring5.
</bodyText>
<sectionHeader confidence="0.988329" genericHeader="method">
4 Linear BLEU Oracle (LB)
</sectionHeader>
<bodyText confidence="0.999825666666667">
In this section, we propose a new oracle based on
the linear approximation of the corpus BLEU in-
troduced in (Tromble et al., 2008). While this ap-
proximation was earlier used for Minimum Bayes
Risk decoding in lattices (Tromble et al., 2008;
Blackwood et al., 2010), we show here how it can
also be used to approximately compute an oracle
translation.
Given five real parameters O0...4 and a word vo-
cabulary E, Tromble et al. (2008) showed that one
can approximate the corpus-BLEU with its first-
order (linear) Taylor expansion:
</bodyText>
<equation confidence="0.982876">
4
linBLEU(7r) = O0 |e,r|+ � �On cu(e,,)Ju(r),
n=1 u∈Σn
(5)
</equation>
<bodyText confidence="0.91662925">
where cu(e) is the number of times the n-gram
u appears in e, and Ju(r) is an indicator variable
testing the presence of u in r.
To exploit this approximation for oracle decod-
ing, we construct four weighted FSTs An con-
taining a (final) state for each possible (n − 1)-
5See, however, experiments in Section 6.
gram, and all weighted transitions of the kind
</bodyText>
<equation confidence="0.784703">
(Qn−1
1 , Qn : Qn1 /On × Jσn1 (r), Qn2 ), where Qs are
</equation>
<bodyText confidence="0.9877755">
in E, input word sequence Qn−1 1and output se-
quence Qn2 , are, respectively, the maximal prefix
and suffix of an n-gram Qn 1
.
In supplement, we add auxiliary states corre-
sponding to m-grams (m &lt; n − 1), whose func-
tional purpose is to help reach one of the main
(n − 1)-gram states. There are |Σ|n�1−1
</bodyText>
<equation confidence="0.9047572">
|Σ|−1 , n &gt; 1,
such supplementary states and their transitions are
(Qk1, Qk+1 : Qk+1
1 /0, Qk+1
1 ), k = 1... n−2. Apart
</equation>
<bodyText confidence="0.999502">
from these auxiliary states, the rest of the graph
(i.e., all final states) reproduces the structure of
the well-known de Bruijn graph B(E, n) (see Fig-
ure 1).
To actually compute the best hypothesis, we
first weight all arcs in the input FSA L with O0 to
obtain A0. This makes each word’s weight equal
in a hypothesis path, and the total weight of the
path in A0 is proportional to the number of words
in it. Then, by sequentially composing A0 with
other Ans, we discount arcs whose output n-gram
corresponds to a matching n-gram. The amount
of discount is regulated by the ratio between On’s
for n &gt; 0.
With all operations performed over the
(min, +)-semiring, the oracle translation is then
given by:
</bodyText>
<equation confidence="0.987187">
7r∗LB = ShortestPath(A0◦A1◦A2◦A3◦A4).
</equation>
<bodyText confidence="0.999184">
We set parameters On as in (Tromble et al.,
2008): O0 = 1, roughly corresponding to the
brevity penalty (each word in a hypothesis adds
up equally to the final path length) and On =
</bodyText>
<figure confidence="0.509193">
−(4p · rn−1)−1, which are increasing discounts
124
</figure>
<figureCaption confidence="0.9803455">
Figure 2: Performance of the LB-4g oracle for differ-
ent combinations of p and r on WMT11 de2en task.
</figureCaption>
<bodyText confidence="0.999913714285714">
for matching n-grams. The values of p and r were
found by grid search with a 0.05 step value. A
typical result of the grid evaluation of the LB or-
acle for German to English WMT’11 task is dis-
played on Figure 2. The optimal values for the
other pairs of languages were roughly in the same
ballpark, with p Pz� 0.3 and r Pz� 0.2.
</bodyText>
<sectionHeader confidence="0.92301" genericHeader="method">
5 Oracles with n-gram Clipping
</sectionHeader>
<bodyText confidence="0.999970153846154">
In this section, we describe two new oracle de-
coders that take n-gram clipping into account.
These oracles leverage on the well-known fact
that the shortest path problem, at the heart of
all the oracles described so far, can be reduced
straightforwardly to an Integer Linear Program-
ming (ILP) problem (Wolsey, 1998). Once oracle
decoding is formulated as an ILP problem, it is
relatively easy to introduce additional constraints,
for instance to enforce n-gram clipping. We will
first describe the optimization problem of oracle
decoding and then present several ways to effi-
ciently solve it.
</bodyText>
<subsectionHeader confidence="0.975716">
5.1 Problem Description
</subsectionHeader>
<bodyText confidence="0.999977052631579">
Throughout this section, abusing the notations,
we will also think of an edge ξi as a binary vari-
able describing whether the edge is “selected” or
not. The set {0,1}#{Ξ} of all possible edge as-
signments will be denoted by P. Note that Π, the
set of all paths in the lattice is a subset of P: by
enforcing some constraints on an assignment � in
P, it can be guaranteed that it will represent a path
in the lattice. For the sake of presentation, we as-
sume that each edge ξi generates a single word
w(ξi) and we focus first on finding the optimal
hypothesis with respect to the sentence approxi-
mation of the 1-BLEU score.
As 1-BLEU is decomposable, it is possible to
define, for every edge ξi, an associated reward, θi
that describes the edge’s local contribution to the
hypothesis score. For instance, for the sentence
approximation of the 1-BLEU score, the rewards
are defined as:
</bodyText>
<equation confidence="0.9796954">
�
θi
Θ1 if w(ξi) is in the reference,
=
−Θ2 otherwise,
</equation>
<bodyText confidence="0.999662333333333">
where Θ1 and Θ2 are two positive constants cho-
sen to maximize the corpus BLEU score6. Con-
stant Θ1 (resp. Θ2) is a reward (resp. a penalty)
for generating a word in the reference (resp. not in
the reference). The score of an assignment � E P
is then defined as: score(ξ) = �#{Ξ}
</bodyText>
<equation confidence="0.792608">
i=1 ξi · θi. This
</equation>
<bodyText confidence="0.9760535">
score can be seen as a compromise between the
number of common words in the hypothesis and
the reference (accounting for recall) and the num-
ber of words of the hypothesis that do not appear
in the reference (accounting for precision).
As explained in Section 2.3, finding the or-
acle hypothesis amounts to solving the shortest
distance (or path) problem (3), which can be re-
formulated by a constrained optimization prob-
lem (Wolsey, 1998):
</bodyText>
<equation confidence="0.9994624">
ξi · θi (6)
�s.t. ξ = 1, � ξ = 1
ξ∈Ξ−(qF ) ξ∈Ξ+(q0)
� ξ − � ξ = 0, q E 2\{q0, qF}
ξ∈Ξ+(q) ξ∈Ξ−(q)
</equation>
<bodyText confidence="0.999985909090909">
where q0 (resp. qF) is the initial (resp. final) state
of the lattice and Ξ−(q) (resp. Ξ+(q)) denotes the
set of incoming (resp. outgoing) edges of state q.
These path constraints ensure that the solution of
the problem is a valid path in the lattice.
The optimization problem in Equation (6) can
be further extended to take clipping into account.
Let us introduce, for each word w, a variable γw
that denotes the number of times w appears in the
hypothesis clipped to the number of times, it ap-
pears in the reference. Formally, γw is defined by:
</bodyText>
<figure confidence="0.855667507936508">
γw = min
6We tried several combinations of 81 and 82 and kept
the one that had the highest corpus 4-BLEU score.
0.6
0.8
1
0.6
0.4
0.8
0.2
1
p
0
36
34
32
BLEU 30
28
26
24
22
0.2
0.4
36
34
32
30
28
26
24
22
0
r
#{Ξ} �
i=1
arg max
S∈P
{ � ξ, cw(r) }
ξ∈Ω(w)
125
where Ω (w) is the subset of edges generating w, 5.2 Shortest Path Oracle (SP)
and PξEΩ(w) ξ is the number of occurrences of algori
w in the solution and cw(r) is the number of oc- Bellman
currences of w in the reference r. Using the γ thm.
variables, we define a “clipped” approximation of
1-BLEU:
⎛ ⎞
#&apos;Ξ&apos; XX
γw − Θ2 · ⎝ ξi − γw ⎠
i=1 w
XΘ1 ·
w
and
corresponds to
the number of words in the hypothesis that do not
appear in the reference or that are surplus to the
clipped count.
Finally, the clipped lattice oracle is defined by
the foll
Pwγw,
P#&apos;Ξ&apos;i=1ξi−Pwγw
owing optimization problem:
γw−Θ2 ·
</figure>
<bodyText confidence="0.917945142857143">
ficiently using the standard
laxing the clipping constraints: starting from an
unconstrained problem, the counts clipping is en-
forced by incrementally strengthening the weight
of paths satisfying the constraints.
The oracle decoding problem with clipping
constraints amounts to solving:
</bodyText>
<figure confidence="0.907051487804878">
Xs.t.γw≥0,γw≤cw(r),γw≤ξEΩ(w) ξ arg min − #&apos;Ξ&apos; X ξi · θi (8)
X ξEΠ i=1
X ξ = 1 s.t. As a trivial special class of the above formula- ξ ≤ cw(r),w ∈ r
ξEΞ−(qF ) tion, we also define a Shortest Path Oracle (SP)
that solves the optimization problem in (6). As
no clipping constraints apply, it can be solved ef-
X
ξEΩ(w)
X X ξ = 0,q ∈ Q \ {q0,qF}
ξEΞ+(q) ξ −ξEΞ−(q)
domain (the arg min runs over
and not over P).
To solve this optimization problem we consider its
dual form and use Lagrangian relaxation to deal
with clipping constraints.
=
be positive Lagrange mul-
tipliers, one for each different word of the refer-
ence, then the Lagrangian
Π
λ
{λw}wEr
of the problem (8) is:
L(λ, ξ) = − #&apos;Ξ&apos; X
i=1
The dual objective is
=
an
L(λ)
minξL(λ,ξ)
d the dual problem is: maxλ,λ}0 L(λ). To
solve the latter, we first need to work out the dual
objective:
ξ* min L(λ, ξ)
ξEΠ
= arg
= arg min
ξEΠ
Indeed, the clipped number of words in the hy-
pothesis that appear in the reference is given by
arg max
ξEP,γw
+
X
(Θ1
Θ2) ·w
#&apos;Ξ&apos; X
i=1
ξ = 1,
ξEΞ+(q0)
ξi
(7)
where the first three sets of constraints are the lin-
5.3 Oracle Decoding through Lagrangian
Relaxation (RLX)
In this section, we introduce another method to
solve problem (7) without relying on an exter-
nal ILP solver. Following (Rush et al., 2010;
Chang and Collins, 2011), we propose an original
method for oracle decoding based on Lagrangian
relaxation. This method relies on the idea of re-
where, by abusing the notations, r also denotes
the set of words in the reference. For sake of clar-
ity, the path constraints are incorporated into the
Let
X
ξiθi+
wEr
w
w
r)
(
E
(
)
λ
Xξ−c
(
ξ
Ω
w
�
</figure>
<equation confidence="0.658086">
ξi �λw(ξ�) − θi
#&apos;Ξ&apos; X
i=1
</equation>
<bodyText confidence="0.966943666666667">
earization of the definition of
made possible
by the positivity of
and
and the last three
sets of constraints are the path constraints.
In our implementation we generalized this op-
timization problem to bigram lattices, in which
each edge is labeled by the bigram it generates.
Such bigram FSAs can be produced by compos-
ing the word lattice with
from Section 4. In
this case, the reward of an edge will be defined as
a combination of the (clipped) number of unigram
matches and bigram matches, and solving the op-
timization problem yields a 2-BLEU optimal hy-
pothesis. The approach can be further generalized
to higher-order BLEU or other metrics, as long as
the reward of an edge can be computed locally.
The constrained optimization problem (7) can
γw,
</bodyText>
<equation confidence="0.757760666666667">
Θ1
Θ2,
Δ2
</equation>
<bodyText confidence="0.549765">
be solved efficiently using off-the-shelf ILP
solvers7.
our experiments we used Gurobi (Optimization,
2010) a commercial ILP solver that offers fr
</bodyText>
<equation confidence="0.28607675">
7In
ee academic li-
cense.
126
</equation>
<bodyText confidence="0.97409784">
where we assume that Aw(ξ,) is 0 when word
w(�i) is not in the reference. In the same way
as in Section 5.2, the solution of this problem can
be efficiently retrieved with a shortest path algo-
rithm.
It is possible to optimize L(λ) by noticing that
it is a concave function. It can be shown (Chang
and Collins, 2011) that, at convergence, the clip-
ping constraints will be enforced in the optimal
solution. In this work, we chose to use a simple
gradient descent to solve the dual problem. A sub-
gradient of the dual objective is:
aL(λ)
aAw
Each component of the gradient corresponds to
the difference between the number of times the
word w appears in the hypothesis and the num-
ber of times it appears in the reference. The algo-
rithm below sums up the optimization of task (8).
In the algorithm a(t) corresponds to the step size
at the tth iteration. In our experiments we used a
constant step size of 0.1. Compared to the usual
gradient descent algorithm, there is an additional
projection step of λ on the positive orthant, which
enforces the constraint λ r 0.
</bodyText>
<equation confidence="0.9865224">
bw, A(�)
w 0
for t = 1 — *T do
)
ξ�(t) = arg min� Ei �i · (Aw(ξ,) � �i
</equation>
<bodyText confidence="0.8551595">
if all clipping constraints are enforced
then optimal solution found
else for w E r do
nw n. of occurrences of w i (t)
</bodyText>
<equation confidence="0.982842333333333">
n �*
A(t)
w Aw(t) + a(t) · (nw — cw(r))
A(t)
w max(0, A(t)
w )
</equation>
<sectionHeader confidence="0.983897" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.826234">
For the proposed new oracles and the existing ap-
proaches, we compare the quality of oracle trans-
lations and the average time per sentence needed
to compute them8 on several datasets for 3 lan-
guage pairs, using lattices generated by two open-
source decoders: N-code and Moses9 (Figures 3
8Experiments were run in parallel on a server with 64G
of RAM and 2 Xeon CPUs with 4 cores at 2.3 GHz.
9As the ILP (and RLX) oracle were implemented in
Python, we pruned Moses lattices to accelerate task prepa-
ration for it.
</bodyText>
<table confidence="0.9994938">
test decoder fr2en de2en en2de
N-code 27.88 22.05 15.83
Moses 27.68 21.85 15.89
oracle N-code 36.36 29.22 21.18
Moses 35.25 29.13 22.03
</table>
<tableCaption confidence="0.987057">
Table 2: Test BLEU scores and oracle scores on
100-best lists for the evaluated systems.
</tableCaption>
<bodyText confidence="0.999838368421053">
and 4). Systems were trained on the data provided
for the WMT’11 Evaluation task10, tuned on the
WMT’09 test data and evaluated on WMT’10 test
set11 to produce lattices. The BLEU test scores
and oracle scores on 100-best lists with the ap-
proximation (4) for N-code and Moses are given
in Table 2. It is not until considering 10,000-best
lists that n-best oracles achieve performance com-
parable to the (mediocre) SP oracle.
To make a fair comparison with the ILP and
RLX oracles which optimize 2-BLEU, we in-
cluded 2-BLEU versions of the LB and LM ora-
cles, identified below with the “-2g” suffix. The
two versions of the PB oracle are respectively
denoted as PB and PB E, by the type of the ®-
operation they consider (Section 3.2). Parame-
ters p and r for the LB-4g oracle for N-code were
found with grid search and reused for Moses:
p = 0.25, r = 0.15 (fr2en); p = 0.175, r = 0.575
(en2de) and p = 0.35, r = 0.425 (de2en). Cor-
respondingly, for the LB-2g oracle: p = 0.3, r =
0.15; p = 0.3, r = 0.175 and p = 0.575, r = 0.1.
The proposed LB, ILP and RLX oracles were
the best performing oracles, with the ILP and
RLX oracles being considerably faster, suffering
only a negligible decrease in BLEU, compared to
the 4-BLEU-optimized LB oracle. We stopped
RLX oracle after 20 iterations, as letting it con-
verge had a small negative effect (—1 point of the
corpus BLEU), because of the sentence/corpus dis-
crepancy ushered by the BLEU score approxima-
tion.
Experiments showed consistently inferior per-
formance of the LM-oracle resulting from the op-
timization of the sentence probability rather than
BLEU. The PB oracle often performed compara-
bly to our new oracles, however, with sporadic
resource-consumption bursts, that are difficult to
</bodyText>
<figure confidence="0.978745666666667">
10http://www.statmt.org/wmt2011
11All BLEU scores are reported using the multi-bleu.pl
script.
�= S — cw(r).
ξEQ(w)nξ*
127
</figure>
<figureCaption confidence="0.940871">
Figure 3: Oracles performance for N-code lattices.
</figureCaption>
<figure confidence="0.993331524475525">
30
48.22
47.82
48.12
35.49
46.48
47.71
46.76
BLEU
avg. time
BLEU
avg. time
1.5
35.09
34.85
34.79
25
34.76
34.70
25.34
1
24.66
41.23
24.75
24.85
avg. time, e
BLEU
BLEU
38.91
38.75
30.78
20
0.5
29.53
29.53
0
15
BLEU
avg. time
24.73
24.78
22.19
20.78
20.74
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g
(c) en2de
6
5
4
avg. time, e
2
1
0
1
avg. time, s
0.5
0
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g
(a) fr2en
35
30
25
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g
(b) de2en
BLEU
3
BLEU
50
45
40
35
30
25
50
45
40
35
30
25
3 avg. time, s BLEU 35
2 30
1 25
0
9
8
7
6
5
4
3
2
1
0
avg. time, e
avg. time, e
BLEU
30
25
15
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g
(a) fr2en
2
1
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g
(b) de2en
20
RLX ILP LB-4g LB-2g PB PBl SP LM-4g LM-2g
(c) en2de
43.82
44.08
44.44
43.82
43.42
43.20
41.03
BLEU
avg. time
36.34
36.25
36.43
36.91
37.73
36.52
36.75
36.62
BLEU 4
avg. time
3
30.52
29.51
29.45
0
28.68
28.64
29.94
28.94
28.76
28.65
BLEU
avg. time
26.48
21.29
21.23
</figure>
<figureCaption confidence="0.999898">
Figure 4: Oracles performance for Moses lattices pruned with parameter -b 0.5.
</figureCaption>
<bodyText confidence="0.9999786">
avoid without more cursory hypotheses recom-
bination strategies and the induced effect on the
translations quality. The length-aware PB E oracle
has unexpectedly poorer scores compared to its
length-agnostic PB counterpart, while it should,
at least, stay even, as it takes the brevity penalty
into account. We attribute this fact to the com-
plex effect of clipping coupled with the lack of
control of the process of selecting one hypothe-
sis among several having the same BLEU score,
length and recent history. Anyhow, BLEU scores
of both of PB oracles are only marginally differ-
ent, so the PB E’s conservative policy of pruning
and, consequently, much heavier memory con-
sumption makes it an unwanted choice.
</bodyText>
<sectionHeader confidence="0.997872" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999976457142857">
We proposed two methods for finding oracle
translations in lattices, based, respectively, on a
linear approximation to the corpus-level BLEU
and on integer linear programming techniques.
We also proposed a variant of the latter approach
based on Lagrangian relaxation that does not rely
on a third-party ILP solver. All these oracles have
superior performance to existing approaches, in
terms of the quality of the found translations, re-
source consumption and, for the LB-2g oracles,
in terms of speed. It is thus possible to use bet-
ter approximations of BLEU than was previously
done, taking the corpus-based nature of BLEU, or
clipping constrainst into account, delivering better
oracles without compromising speed.
Using 2-BLEU and 4-BLEU oracles yields com-
parable performance, which confirms the intuition
that hypotheses sharing many 2-grams, would
likely have many common 3- and 4-grams as well.
Taking into consideration the exceptional speed of
the LB-2g oracle, in practice one can safely opti-
mize for 2-BLEU instead of 4-BLEU, saving large
amounts of time for oracle decoding on long sen-
tences.
Overall, these experiments accentuate the
acuteness of scoring problems that plague modern
decoders: very good hypotheses exist for most in-
put sentences, but are poorly evaluated by a linear
combination of standard features functions. Even
though the tuning procedure can be held respon-
sible for part of the problem, the comparison be-
tween lattice and n-best oracles shows that the
beam search leaves good hypotheses out of the n-
best list until very high value of n, that are never
used in practice.
</bodyText>
<sectionHeader confidence="0.997138" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995359">
This work has been partially funded by OSEO un-
der the Quaero program.
</bodyText>
<page confidence="0.870389">
128
</page>
<sectionHeader confidence="0.991318" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999947789473684">
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proc. of the Int. Conf. on Imple-
mentation and Application of Automata, pages 11–
23.
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of translation
model search spaces. In Proc. of WMT, pages 224–
232, Athens, Greece.
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation with
improved correlation with human judgments. In
Proc. of the ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation,
pages 65–72, Ann Arbor, MI, USA.
Graeme Blackwood, Adri`a de Gispert, and William
Byrne. 2010. Efficient path counting transducers
for minimum bayes-risk decoding of statistical ma-
chine translation lattices. In Proc. of the ACL 2010
Conference Short Papers, pages 27–32, Strouds-
burg, PA, USA.
Yin-Wen Chang and Michael Collins. 2011. Exact de-
coding of phrase-based translation models through
lagrangian relaxation. In Proc. of the 2011 Conf. on
EMNLP, pages 26–37, Edinburgh, UK.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic
and structural translation features. In Proc. of the
2008 Conf. on EMNLP, pages 224–233, Honolulu,
Hawaii.
Markus Dreyer, Keith B. Hall, and Sanjeev P. Khu-
danpur. 2007. Comparing reordering constraints
for SMT using efficient BLEU oracle computation.
In Proc. of the Workshop on Syntax and Structure
in Statistical Translation, pages 103–110, Morris-
town, NJ, USA.
Gregor Leusch, Evgeny Matusov, and Hermann Ney.
2008. Complexity of finding the BLEU-optimal hy-
pothesis in a confusion network. In Proc. of the
2008 Conf. on EMNLP, pages 839–847, Honolulu,
Hawaii.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proc. of Human Language Technolo-
gies: The 2009 Annual Conf. of the North Ameri-
can Chapter of the ACL, Companion Volume: Short
Papers, pages 9–12, Morristown, NJ, USA.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrim-
inative approach to machine translation. In Proc.
of the 21st Int. Conf. on Computational Linguistics
and the 44th annual meeting of the ACL, pages 761–
768, Morristown, NJ, USA.
Mehryar Mohri. 2002. Semiring frameworks and al-
gorithms for shortest-distance problems. J. Autom.
Lang. Comb., 7:321–350.
Mehryar Mohri. 2009. Weighted automata algo-
rithms. In Manfred Droste, Werner Kuich, and
Heiko Vogler, editors, Handbook of Weighted Au-
tomata, chapter 6, pages 213–254.
Gurobi Optimization. 2010. Gurobi optimizer, April.
Version 3.0.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc. of
the Annual Meeting of the ACL, pages 311–318.
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural
language processing. In Proc. of the 2010 Conf. on
EMNLP, pages 1–11, Stroudsburg, PA, USA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human anno-
tation. In Proc. of the Conf. of the Association for
Machine Translation in the America (AMTA), pages
223–231.
Roy W. Tromble, Shankar Kumar, Franz Och, and
Wolfgang Macherey. 2008. Lattice minimum
bayes-risk decoding for statistical machine transla-
tion. In Proc. of the Conf. on EMNLP, pages 620–
629, Stroudsburg, PA, USA.
Marco Turchi, Tijl De Bie, and Nello Cristianini.
2008. Learning performance of a machine trans-
lation system: a statistical and computational anal-
ysis. In Proc. of WMT, pages 35–43, Columbus,
Ohio.
Guillaume Wisniewski, Alexandre Allauzen, and
Franc¸ois Yvon. 2010. Assessing phrase-based
translation models with oracle decoding. In Proc.
of the 2010 Conf. on EMNLP, pages 933–943,
Stroudsburg, PA, USA.
L. Wolsey. 1998. Integer Programming. John Wiley
&amp; Sons, Inc.
</reference>
<page confidence="0.936332">
129
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.973969">
<title confidence="0.999606">Computing Lattice BLEU Oracle Scores for Machine Translation</title>
<author confidence="0.998905">Sokolov Guillaume Wisniewski Yvon</author>
<affiliation confidence="0.99901">LIMSI-CNRS &amp; Univ. Paris</affiliation>
<address confidence="0.995158">BP-133, 91 403 Orsay, France</address>
<abstract confidence="0.999147869565217">The search space of Phrase-Based Statistical Machine Translation (PBSMT) systems can be represented under the form of a directed acyclic graph (lattice). The quality of this search space can thus be evaluated by computing the best achievable hypothin the lattice, the so-called hypothesis. For common SMT metrics, this problem is however NP-hard and can only be solved using heuristics. In this work, we present two new methods for efficiently on lattices: the first one is based on a linear approximation the corpus and is solved using the FST formalism; the second one relies on integer linear programming formulation and is solved directly and using the Lagrangian relaxation framework. These new decoders are positively evaluated and compared with several alternatives from the literature for three language pairs, using lattices produced by two PBSMT systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
<author>Wojciech Skut</author>
<author>Mehryar Mohri</author>
</authors>
<title>OpenFst: A general and efficient weighted finite-state transducer library.</title>
<date>2007</date>
<booktitle>In Proc. of the Int. Conf. on Implementation and Application of Automata,</booktitle>
<pages>11--23</pages>
<contexts>
<context position="10549" citStr="Allauzen et al., 2007" startWordPosition="1739" endWordPosition="1742">ctive function is rather the approximation of BLEU given in the “target replacement” column. Column “search” details the accuracy of the target replacement optimization. Finally, columns “clipping” and “brevity” indicate whether the corresponding properties of BLEU score are considered in the target substitute and in the search algorithm. 2.3 Finite State Acceptors The implementations of the oracles described in the first part of this work (sections 3 and 4) use the common formalism of finite state acceptors (FSA) over different semirings and are implemented using the generic OpenFST toolbox (Allauzen et al., 2007). A (®, ®)-semiring K over a set K is a system (K, ®, ®, 0, 1), where (K, ®, 0) is a commutative monoid with identity element 0, and (K, ®,1) is a monoid with identity element 1. ® distributes over®, so that a®(b®c) = (a®b)®(a®c) and (b ® c) ® a = (b ® a) ® (c ® a) and element 0 annihilates K (a ® 0 = 0 ® a = 0). Let A = (E, Q, I, F, E) be a weighted finitestate acceptor with labels in E and weights in K, meaning that the transitions (q, Q, q&apos;) in A carry a weight w E K. Formally, E is a mapping from (Q x E x Q) into K; likewise, initial I and final weight F functions are mappings from Q into </context>
</contexts>
<marker>Allauzen, Riley, Schalkwyk, Skut, Mohri, 2007</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer library. In Proc. of the Int. Conf. on Implementation and Application of Automata, pages 11– 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>A systematic analysis of translation model search spaces.</title>
<date>2009</date>
<booktitle>In Proc. of WMT,</booktitle>
<pages>224--232</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="1833" citStr="Auli et al., 2009" startWordPosition="290" endWordPosition="293">ms has the form of a very large directed acyclic graph. In several softwares, an approximation of this search space can be outputted, either as a n-best list containing the n top hypotheses found by the decoder, or as a phrase or word graph (lattice) which compactly encodes those hypotheses that have survived search space pruning. Lattices usually contain much more hypotheses than n-best lists and better approximate the search space. Exploring the PBSMT search space is one of the few means to perform diagnostic analysis and to better understand the behavior of the system (Turchi et al., 2008; Auli et al., 2009). Useful diagnostics are, for instance, provided by looking at the best (oracle) hypotheses contained in the search space, i.e, those hypotheses that have the highest quality score with respect to one or several references. Such oracle hypotheses can be used for failure analysis and to better understand the bottlenecks of existing translation systems (Wisniewski et al., 2010). Indeed, the inability to faithfully reproduce reference translations can have many causes, such as scantiness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-l</context>
</contexts>
<marker>Auli, Lopez, Hoang, Koehn, 2009</marker>
<rawString>Michael Auli, Adam Lopez, Hieu Hoang, and Philipp Koehn. 2009. A systematic analysis of translation model search spaces. In Proc. of WMT, pages 224– 232, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation,</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, MI, USA.</location>
<contexts>
<context position="3764" citStr="Banerjee and Lavie, 2005" startWordPosition="594" endWordPosition="597">BLEU, or rather sentence-level approximations thereof, the problem is in fact known to be NP-hard (Leusch et al., 2008). This complexity stems from the fact that the contribution of a given edge to the total modified n-gram precision can not be computed without looking at all other edges on the path. Similar (or worse) complexity result are expected 120 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 120–129, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics for other metrics such as METEOR (Banerjee and Lavie, 2005) or TER (Snover et al., 2006). The exact computation of oracles under corpus level metrics, such as BLEU, poses supplementary combinatorial problems that will not be addressed in this work. In this paper, we present two original methods for finding approximate oracle hypotheses on lattices. The first one is based on a linear approximation of the corpus BLEU, that was originally designed for efficient Minimum Bayesian Risk decoding on lattices (Tromble et al., 2008). The second one, based on Integer Linear Programming, is an extension to lattices of a recent work on failure analysis for phrase-</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation, pages 65–72, Ann Arbor, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Blackwood</author>
<author>Adri`a de Gispert</author>
<author>William Byrne</author>
</authors>
<title>Efficient path counting transducers for minimum bayes-risk decoding of statistical machine translation lattices.</title>
<date>2010</date>
<booktitle>In Proc. of the ACL 2010 Conference Short Papers,</booktitle>
<pages>27--32</pages>
<location>Stroudsburg, PA, USA.</location>
<marker>Blackwood, de Gispert, Byrne, 2010</marker>
<rawString>Graeme Blackwood, Adri`a de Gispert, and William Byrne. 2010. Efficient path counting transducers for minimum bayes-risk decoding of statistical machine translation lattices. In Proc. of the ACL 2010 Conference Short Papers, pages 27–32, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yin-Wen Chang</author>
<author>Michael Collins</author>
</authors>
<title>Exact decoding of phrase-based translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In Proc. of the 2011 Conf. on EMNLP,</booktitle>
<pages>26--37</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="25229" citStr="Chang and Collins, 2011" startWordPosition="4423" endWordPosition="4426">X i=1 The dual objective is = an L(λ) minξL(λ,ξ) d the dual problem is: maxλ,λ}0 L(λ). To solve the latter, we first need to work out the dual objective: ξ* min L(λ, ξ) ξEΠ = arg = arg min ξEΠ Indeed, the clipped number of words in the hypothesis that appear in the reference is given by arg max ξEP,γw + X (Θ1 Θ2) ·w #&apos;Ξ&apos; X i=1 ξ = 1, ξEΞ+(q0) ξi (7) where the first three sets of constraints are the lin5.3 Oracle Decoding through Lagrangian Relaxation (RLX) In this section, we introduce another method to solve problem (7) without relying on an external ILP solver. Following (Rush et al., 2010; Chang and Collins, 2011), we propose an original method for oracle decoding based on Lagrangian relaxation. This method relies on the idea of rewhere, by abusing the notations, r also denotes the set of words in the reference. For sake of clarity, the path constraints are incorporated into the Let X ξiθi+ wEr w w r) ( E ( ) λ Xξ−c ( ξ Ω w � ξi �λw(ξ�) − θi #&apos;Ξ&apos; X i=1 earization of the definition of made possible by the positivity of and and the last three sets of constraints are the path constraints. In our implementation we generalized this optimization problem to bigram lattices, in which each edge is labeled by th</context>
<context position="26828" citStr="Chang and Collins, 2011" startWordPosition="4714" endWordPosition="4717"> other metrics, as long as the reward of an edge can be computed locally. The constrained optimization problem (7) can γw, Θ1 Θ2, Δ2 be solved efficiently using off-the-shelf ILP solvers7. our experiments we used Gurobi (Optimization, 2010) a commercial ILP solver that offers fr 7In ee academic license. 126 where we assume that Aw(ξ,) is 0 when word w(�i) is not in the reference. In the same way as in Section 5.2, the solution of this problem can be efficiently retrieved with a shortest path algorithm. It is possible to optimize L(λ) by noticing that it is a concave function. It can be shown (Chang and Collins, 2011) that, at convergence, the clipping constraints will be enforced in the optimal solution. In this work, we chose to use a simple gradient descent to solve the dual problem. A subgradient of the dual objective is: aL(λ) aAw Each component of the gradient corresponds to the difference between the number of times the word w appears in the hypothesis and the number of times it appears in the reference. The algorithm below sums up the optimization of task (8). In the algorithm a(t) corresponds to the step size at the tth iteration. In our experiments we used a constant step size of 0.1. Compared to</context>
</contexts>
<marker>Chang, Collins, 2011</marker>
<rawString>Yin-Wen Chang and Michael Collins. 2011. Exact decoding of phrase-based translation models through lagrangian relaxation. In Proc. of the 2011 Conf. on EMNLP, pages 26–37, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proc. of the 2008 Conf. on EMNLP,</booktitle>
<pages>224--233</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="2585" citStr="Chiang et al., 2008" startWordPosition="403" endWordPosition="407">e hypotheses that have the highest quality score with respect to one or several references. Such oracle hypotheses can be used for failure analysis and to better understand the bottlenecks of existing translation systems (Wisniewski et al., 2010). Indeed, the inability to faithfully reproduce reference translations can have many causes, such as scantiness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices, etc. Oracle decoding has several other applications: for instance, in (Liang et al., 2006; Chiang et al., 2008) it is used as a work-around to the problem of non-reachability of the reference in discriminative training of MT systems. Lattice reranking (Li and Khudanpur, 2009), a promising way to improve MT systems, also relies on oracle decoding to build the training data for a reranking algorithm. For sentence level metrics, finding oracle hypotheses in n-best lists is a simple issue; however, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of bruteforce approaches. When using BLEU, or rather sentence-level approximations </context>
<context position="8015" citStr="Chiang et al., 2008" startWordPosition="1332" endWordPosition="1335">known, BLEU performs a compromise between precision, which is directly appears in Equation (1), and recall, which is indirectly taken into account via the brevity penalty. In most cases, Equation (1) is computed with n = 4 and we use BLEU as a synonym for 4-BLEU. BLEU is defined for a pair of corpora, but, as an oracle decoder is working at the sentence-level, it should rely on an approximation of BLEU that can linear chain of arcs. 4The algorithms described below can be straightforwardly generalized to compute oracle hypotheses under combined metrics mixing model scores and quality measures (Chiang et al., 2008), by weighting each edge with its model score and by using these weights down the pipe. 1/n , (1) 121 evaluate the similarity between a single hypothesis and its reference. This approximation introduces a discrepancy as gathering sentences with the highest (local) approximation may not result in the highest possible (corpus-level) BLEU score. Let BLEU&apos; be such a sentence-level approximation of BLEU. Then lattice oracle decoding is the task of finding an optimal path π*(f) among all paths IIf for a given f, and amounts to the following optimization problem: π*(f) = argmax BLEU&apos;(e,, rf). (2) 7rE</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proc. of the 2008 Conf. on EMNLP, pages 224–233, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Keith B Hall</author>
<author>Sanjeev P Khudanpur</author>
</authors>
<title>Comparing reordering constraints for SMT using efficient BLEU oracle computation.</title>
<date>2007</date>
<booktitle>In Proc. of the Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>103--110</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="14480" citStr="Dreyer et al., 2007" startWordPosition="2444" endWordPosition="2447">e train, for each source sentence f, a separate language model ALM(rf) using only the reference rf, oracle decoding amounts to finding a shortest (most probable) path in the weighted FSA resulting from the composition L o ALM(rf) over the (min, +)-semiring: 7riM(f) = ShortestPath(L o ALM(rf)). This approach replaces the optimization of nBLEU with a search for the most probable path under a simplistic n-gram language model. One may expect the most probable path to select frequent n-gram from the reference, thus augmenting n-BLEU. 3.2 Partial BLEU Oracle (PB) Another approach is put forward in (Dreyer et al., 2007) and used in (Li and Khudanpur, 2009): oracle translations are shortest paths in a lattice L, where the weight of each path -7r is the sentence level log BLEU(π) score of the corresponding complete or partial hypothesis: 1 � log BLEU(π) = 4 log pm. (4) m=1...4 Here, the brevity penalty is ignored and ngram precisions are offset to avoid null counts: pm = (cm(e, r) + 0.1)/(cm(e,) + 0.1). This approach has been reimplemented using the FST formalism by defining a suitable semiring. Let each weight of the semiring keep a set of tuples accumulated up to the current state of the lattice. Each tuple </context>
</contexts>
<marker>Dreyer, Hall, Khudanpur, 2007</marker>
<rawString>Markus Dreyer, Keith B. Hall, and Sanjeev P. Khudanpur. 2007. Comparing reordering constraints for SMT using efficient BLEU oracle computation. In Proc. of the Workshop on Syntax and Structure in Statistical Translation, pages 103–110, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Evgeny Matusov</author>
<author>Hermann Ney</author>
</authors>
<title>Complexity of finding the BLEU-optimal hypothesis in a confusion network.</title>
<date>2008</date>
<booktitle>In Proc. of the 2008 Conf. on EMNLP,</booktitle>
<pages>839--847</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="3258" citStr="Leusch et al., 2008" startWordPosition="515" endWordPosition="518">achability of the reference in discriminative training of MT systems. Lattice reranking (Li and Khudanpur, 2009), a promising way to improve MT systems, also relies on oracle decoding to build the training data for a reranking algorithm. For sentence level metrics, finding oracle hypotheses in n-best lists is a simple issue; however, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of bruteforce approaches. When using BLEU, or rather sentence-level approximations thereof, the problem is in fact known to be NP-hard (Leusch et al., 2008). This complexity stems from the fact that the contribution of a given edge to the total modified n-gram precision can not be computed without looking at all other edges on the path. Similar (or worse) complexity result are expected 120 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 120–129, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics for other metrics such as METEOR (Banerjee and Lavie, 2005) or TER (Snover et al., 2006). The exact computation of oracles under corpus level metrics, su</context>
<context position="8687" citStr="Leusch et al. (2008)" startWordPosition="1443" endWordPosition="1446"> using these weights down the pipe. 1/n , (1) 121 evaluate the similarity between a single hypothesis and its reference. This approximation introduces a discrepancy as gathering sentences with the highest (local) approximation may not result in the highest possible (corpus-level) BLEU score. Let BLEU&apos; be such a sentence-level approximation of BLEU. Then lattice oracle decoding is the task of finding an optimal path π*(f) among all paths IIf for a given f, and amounts to the following optimization problem: π*(f) = argmax BLEU&apos;(e,, rf). (2) 7rEIIf 2.2 Compromises of Oracle Decoding As proved by Leusch et al. (2008), even with brevity penalty dropped, the problem of deciding whether a confusion network contains a hypothesis with clipped uni- and bigram precisions all equal to 1.0 is NP-complete (and so is the associated optimization problem of oracle decoding for 2-BLEU). The case of more general word and phrase lattices and 4-BLEU score is consequently also NP-complete. This complexity stems from chaining up of local unigram decisions that, due to the clipping constraints, have non-local effect on the bigram precision scores. It is consequently necessary to keep a possibly exponential number of non-reco</context>
</contexts>
<marker>Leusch, Matusov, Ney, 2008</marker>
<rawString>Gregor Leusch, Evgeny Matusov, and Hermann Ney. 2008. Complexity of finding the BLEU-optimal hypothesis in a confusion network. In Proc. of the 2008 Conf. on EMNLP, pages 839–847, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Efficient extraction of oracle-best translations from hypergraphs.</title>
<date>2009</date>
<booktitle>In Proc. of Human Language Technologies: The 2009 Annual Conf. of the North American Chapter of the ACL, Companion Volume: Short Papers,</booktitle>
<pages>9--12</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2750" citStr="Li and Khudanpur, 2009" startWordPosition="430" endWordPosition="433">understand the bottlenecks of existing translation systems (Wisniewski et al., 2010). Indeed, the inability to faithfully reproduce reference translations can have many causes, such as scantiness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices, etc. Oracle decoding has several other applications: for instance, in (Liang et al., 2006; Chiang et al., 2008) it is used as a work-around to the problem of non-reachability of the reference in discriminative training of MT systems. Lattice reranking (Li and Khudanpur, 2009), a promising way to improve MT systems, also relies on oracle decoding to build the training data for a reranking algorithm. For sentence level metrics, finding oracle hypotheses in n-best lists is a simple issue; however, solving this problem on lattices proves much more challenging, due to the number of embedded hypotheses, which prevents the use of bruteforce approaches. When using BLEU, or rather sentence-level approximations thereof, the problem is in fact known to be NP-hard (Leusch et al., 2008). This complexity stems from the fact that the contribution of a given edge to the total mod</context>
<context position="13278" citStr="Li and Khudanpur, 2009" startWordPosition="2239" endWordPosition="2242"> count exact no yes ILP 2-BLEU sentence uni/bi-gram counts (7) appr. yes yes RLX 2-BLEU sentence uni/bi-gram counts (8) exact yes yes Table 1: Recapitulative overview of oracle decoders. 3 Existing Algorithms In this section, we describe our reimplementation of two approximate search algorithms that have been proposed in the literature to solve the oracle decoding problem for BLEU. In addition to their approximate nature, none of them accounts for the fact that the count of each matching word has to be clipped. 3.1 Language Model Oracle (LM) The simplest approach we consider is introduced in (Li and Khudanpur, 2009), where oracle decoding is reduced to the problem of finding the most likely hypothesis under a n-gram language model trained with the sole reference translation. Let us suppose we have a n-gram language model that gives a probability P(en|e1 ... en−1) of word en given the n − 1 previous words. The probability of a hypothesis e is then Pn(e|r) = Hi=1 P(ei+n|ei ... ei+n−1). The language model can conveniently be represented as a FSA ALM, with each arc carrying a negative logprobability weight and with additional p-type failure transitions to accommodate for back-off arcs. If we train, for each </context>
<context position="14517" citStr="Li and Khudanpur, 2009" startWordPosition="2451" endWordPosition="2454">f, a separate language model ALM(rf) using only the reference rf, oracle decoding amounts to finding a shortest (most probable) path in the weighted FSA resulting from the composition L o ALM(rf) over the (min, +)-semiring: 7riM(f) = ShortestPath(L o ALM(rf)). This approach replaces the optimization of nBLEU with a search for the most probable path under a simplistic n-gram language model. One may expect the most probable path to select frequent n-gram from the reference, thus augmenting n-BLEU. 3.2 Partial BLEU Oracle (PB) Another approach is put forward in (Dreyer et al., 2007) and used in (Li and Khudanpur, 2009): oracle translations are shortest paths in a lattice L, where the weight of each path -7r is the sentence level log BLEU(π) score of the corresponding complete or partial hypothesis: 1 � log BLEU(π) = 4 log pm. (4) m=1...4 Here, the brevity penalty is ignored and ngram precisions are offset to avoid null counts: pm = (cm(e, r) + 0.1)/(cm(e,) + 0.1). This approach has been reimplemented using the FST formalism by defining a suitable semiring. Let each weight of the semiring keep a set of tuples accumulated up to the current state of the lattice. Each tuple contains three words of recent histor</context>
</contexts>
<marker>Li, Khudanpur, 2009</marker>
<rawString>Zhifei Li and Sanjeev Khudanpur. 2009. Efficient extraction of oracle-best translations from hypergraphs. In Proc. of Human Language Technologies: The 2009 Annual Conf. of the North American Chapter of the ACL, Companion Volume: Short Papers, pages 9–12, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st Int. Conf. on Computational Linguistics and the 44th annual meeting of the ACL,</booktitle>
<pages>761--768</pages>
<location>Morristown, NJ, USA.</location>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In Proc. of the 21st Int. Conf. on Computational Linguistics and the 44th annual meeting of the ACL, pages 761– 768, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Semiring frameworks and algorithms for shortest-distance problems.</title>
<date>2002</date>
<journal>J. Autom. Lang. Comb.,</journal>
<pages>7--321</pages>
<contexts>
<context position="12098" citStr="Mohri, 2002" startWordPosition="2050" endWordPosition="2051">ong the path. A finite state transducer (FST) is an FSA with output alphabet, so that each transition carries a pair of input/output symbols. As discussed in Sections 3 and 4, several oracle decoding algorithms can be expressed as shortestpath problems, provided a suitable definition of the underlying acceptor and associated semiring. In particular, quantities such as: � E(π), (3) 7rEII(A) where the total weight of a successful path π = �1 ... �l in A is computed as: E(π) =I(p(�1)) ® �l E(�i)] ® F(n(�l)) i=1 can be efficiently found by generic shortest distance algorithms over acyclic graphs (Mohri, 2002). For FSA-based implementations over semirings where ® = max, the optimization problem (2) is thus reduced to Equation (3), while the oracle-specific details can be incorporated into in the definition of ®. 122 oracle target target level target replacement search clipping brevity existing LM-2g/4g 2/4-BLEU sentence P2(e; r) or P4(e; r) exact no no PB 4-BLEU sentence partial log BLEU (4) appr. no no PBE 4-BLEU sentence partial log BLEU (4) appr. no yes this paper LB-2g/4g 2/4-BLEU corpus linear appr. lin BLEU (5) exact no yes SP 1-BLEU sentence unigram count exact no yes ILP 2-BLEU sentence uni</context>
</contexts>
<marker>Mohri, 2002</marker>
<rawString>Mehryar Mohri. 2002. Semiring frameworks and algorithms for shortest-distance problems. J. Autom. Lang. Comb., 7:321–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Weighted automata algorithms.</title>
<date>2009</date>
<booktitle>Handbook of Weighted Automata, chapter 6,</booktitle>
<pages>213--254</pages>
<editor>In Manfred Droste, Werner Kuich, and Heiko Vogler, editors,</editor>
<contexts>
<context position="11191" citStr="Mohri (2009)" startWordPosition="1887" endWordPosition="1888">et K is a system (K, ®, ®, 0, 1), where (K, ®, 0) is a commutative monoid with identity element 0, and (K, ®,1) is a monoid with identity element 1. ® distributes over®, so that a®(b®c) = (a®b)®(a®c) and (b ® c) ® a = (b ® a) ® (c ® a) and element 0 annihilates K (a ® 0 = 0 ® a = 0). Let A = (E, Q, I, F, E) be a weighted finitestate acceptor with labels in E and weights in K, meaning that the transitions (q, Q, q&apos;) in A carry a weight w E K. Formally, E is a mapping from (Q x E x Q) into K; likewise, initial I and final weight F functions are mappings from Q into K. We borrow the notations of Mohri (2009): if � = (q, a, q&apos;) is a transition in domain(E), p(�) = q (resp. n(�) = q&apos;) denotes its origin (resp. destination) state, w(�) = Q its label and E(�) its weight. These notations extend to paths: if π is a path in A, p(π) (resp. n(π)) is its initial (resp. ending) state and w(π) is the label along the path. A finite state transducer (FST) is an FSA with output alphabet, so that each transition carries a pair of input/output symbols. As discussed in Sections 3 and 4, several oracle decoding algorithms can be expressed as shortestpath problems, provided a suitable definition of the underlying ac</context>
</contexts>
<marker>Mohri, 2009</marker>
<rawString>Mehryar Mohri. 2009. Weighted automata algorithms. In Manfred Droste, Werner Kuich, and Heiko Vogler, editors, Handbook of Weighted Automata, chapter 6, pages 213–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gurobi Optimization</author>
</authors>
<date>2010</date>
<journal>Gurobi optimizer, April. Version</journal>
<volume>3</volume>
<contexts>
<context position="26444" citStr="Optimization, 2010" startWordPosition="4643" endWordPosition="4644">y the bigram it generates. Such bigram FSAs can be produced by composing the word lattice with from Section 4. In this case, the reward of an edge will be defined as a combination of the (clipped) number of unigram matches and bigram matches, and solving the optimization problem yields a 2-BLEU optimal hypothesis. The approach can be further generalized to higher-order BLEU or other metrics, as long as the reward of an edge can be computed locally. The constrained optimization problem (7) can γw, Θ1 Θ2, Δ2 be solved efficiently using off-the-shelf ILP solvers7. our experiments we used Gurobi (Optimization, 2010) a commercial ILP solver that offers fr 7In ee academic license. 126 where we assume that Aw(ξ,) is 0 when word w(�i) is not in the reference. In the same way as in Section 5.2, the solution of this problem can be efficiently retrieved with a shortest path algorithm. It is possible to optimize L(λ) by noticing that it is a concave function. It can be shown (Chang and Collins, 2011) that, at convergence, the clipping constraints will be enforced in the optimal solution. In this work, we chose to use a simple gradient descent to solve the dual problem. A subgradient of the dual objective is: aL(</context>
</contexts>
<marker>Optimization, 2010</marker>
<rawString>Gurobi Optimization. 2010. Gurobi optimizer, April. Version 3.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Annual Meeting of the ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="6843" citStr="Papineni et al., 2002" startWordPosition="1117" endWordPosition="1120">parameters A¯ learned during tuning. In oracle decoding, the decoder’s job is quite different, as we assume that at least a reference rf is provided to evaluate the quality of each individual hypothesis. The decoder therefore aims at finding the path 7r* that generates the hypothesis that best matches rf. For this task, only the output labels ei will matter, the other informations can be left aside.4 Oracle decoding assumes the definition of a measure of the similarity between a reference and a hypothesis. In this paper we will consider sentence-level approximations of the popular BLEU score (Papineni et al., 2002). BLEU is formally defined for two parallel corpora, £ = {ej} j�1 and R = {rj} j�1, each containing J sentences as: � n n-BLEU(£, R) = BP ri pm m�1 where BP = min(1, e1−e1(1z.)/e1(6)) is the brevity penalty and pm = cm(£, R)/cm(£) are clipped or modified m-gram precisions: cm(£) is the total number of word m-grams in £; cm(£, R) accumulates over sentences the number of mgrams in ej that also belong to rj. These counts are clipped, meaning that a m-gram that appears k times in £ and l times in R, with k &gt; l, is only counted l times. As it is well known, BLEU performs a compromise between precis</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of the Annual Meeting of the ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proc. of the 2010 Conf. on EMNLP,</booktitle>
<pages>1--11</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="25203" citStr="Rush et al., 2010" startWordPosition="4419" endWordPosition="4422">: L(λ, ξ) = − #&apos;Ξ&apos; X i=1 The dual objective is = an L(λ) minξL(λ,ξ) d the dual problem is: maxλ,λ}0 L(λ). To solve the latter, we first need to work out the dual objective: ξ* min L(λ, ξ) ξEΠ = arg = arg min ξEΠ Indeed, the clipped number of words in the hypothesis that appear in the reference is given by arg max ξEP,γw + X (Θ1 Θ2) ·w #&apos;Ξ&apos; X i=1 ξ = 1, ξEΞ+(q0) ξi (7) where the first three sets of constraints are the lin5.3 Oracle Decoding through Lagrangian Relaxation (RLX) In this section, we introduce another method to solve problem (7) without relying on an external ILP solver. Following (Rush et al., 2010; Chang and Collins, 2011), we propose an original method for oracle decoding based on Lagrangian relaxation. This method relies on the idea of rewhere, by abusing the notations, r also denotes the set of words in the reference. For sake of clarity, the path constraints are incorporated into the Let X ξiθi+ wEr w w r) ( E ( ) λ Xξ−c ( ξ Ω w � ξi �λw(ξ�) − θi #&apos;Ξ&apos; X i=1 earization of the definition of made possible by the positivity of and and the last three sets of constraints are the path constraints. In our implementation we generalized this optimization problem to bigram lattices, in which </context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proc. of the 2010 Conf. on EMNLP, pages 1–11, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. of the Conf. of the Association for Machine Translation in the America (AMTA),</booktitle>
<pages>223--231</pages>
<contexts>
<context position="3793" citStr="Snover et al., 2006" startWordPosition="600" endWordPosition="603">roximations thereof, the problem is in fact known to be NP-hard (Leusch et al., 2008). This complexity stems from the fact that the contribution of a given edge to the total modified n-gram precision can not be computed without looking at all other edges on the path. Similar (or worse) complexity result are expected 120 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 120–129, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics for other metrics such as METEOR (Banerjee and Lavie, 2005) or TER (Snover et al., 2006). The exact computation of oracles under corpus level metrics, such as BLEU, poses supplementary combinatorial problems that will not be addressed in this work. In this paper, we present two original methods for finding approximate oracle hypotheses on lattices. The first one is based on a linear approximation of the corpus BLEU, that was originally designed for efficient Minimum Bayesian Risk decoding on lattices (Tromble et al., 2008). The second one, based on Integer Linear Programming, is an extension to lattices of a recent work on failure analysis for phrase-based decoders (Wisniewski et</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. of the Conf. of the Association for Machine Translation in the America (AMTA), pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy W Tromble</author>
<author>Shankar Kumar</author>
<author>Franz Och</author>
<author>Wolfgang Macherey</author>
</authors>
<title>Lattice minimum bayes-risk decoding for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of the Conf. on EMNLP,</booktitle>
<pages>620--629</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4233" citStr="Tromble et al., 2008" startWordPosition="673" endWordPosition="676">0–129, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics for other metrics such as METEOR (Banerjee and Lavie, 2005) or TER (Snover et al., 2006). The exact computation of oracles under corpus level metrics, such as BLEU, poses supplementary combinatorial problems that will not be addressed in this work. In this paper, we present two original methods for finding approximate oracle hypotheses on lattices. The first one is based on a linear approximation of the corpus BLEU, that was originally designed for efficient Minimum Bayesian Risk decoding on lattices (Tromble et al., 2008). The second one, based on Integer Linear Programming, is an extension to lattices of a recent work on failure analysis for phrase-based decoders (Wisniewski et al., 2010). In this framework, we study two decoding strategies: one based on a generic ILP solver, and one, based on Lagrangian relaxation. Our contribution is also experimental as we compare the quality of the BLEU approximations and the time performance of these new approaches with several existing methods, for different language pairs and using the lattice generation capacities of two publicly-available state-of-theart phrase-based</context>
<context position="17119" citStr="Tromble et al., 2008" startWordPosition="2914" endWordPosition="2917">y if the corresponding n-gram does not appear in the reference. with the largest current BLEU score. Optimal path is then found by launching the generic ShortestDistance(L) algorithm over one of the semirings above. The (⊕PB`, ⊗PB)-semiring, in which the equal length requirement also implies equal brevity penalties, is more conservative in recombining hypotheses and should achieve final BLEU that is least as good as that obtained with the (⊕PB, ⊗PB)-semiring5. 4 Linear BLEU Oracle (LB) In this section, we propose a new oracle based on the linear approximation of the corpus BLEU introduced in (Tromble et al., 2008). While this approximation was earlier used for Minimum Bayes Risk decoding in lattices (Tromble et al., 2008; Blackwood et al., 2010), we show here how it can also be used to approximately compute an oracle translation. Given five real parameters O0...4 and a word vocabulary E, Tromble et al. (2008) showed that one can approximate the corpus-BLEU with its firstorder (linear) Taylor expansion: 4 linBLEU(7r) = O0 |e,r|+ � �On cu(e,,)Ju(r), n=1 u∈Σn (5) where cu(e) is the number of times the n-gram u appears in e, and Ju(r) is an indicator variable testing the presence of u in r. To exploit this</context>
<context position="19200" citStr="Tromble et al., 2008" startWordPosition="3295" endWordPosition="3298">y compute the best hypothesis, we first weight all arcs in the input FSA L with O0 to obtain A0. This makes each word’s weight equal in a hypothesis path, and the total weight of the path in A0 is proportional to the number of words in it. Then, by sequentially composing A0 with other Ans, we discount arcs whose output n-gram corresponds to a matching n-gram. The amount of discount is regulated by the ratio between On’s for n &gt; 0. With all operations performed over the (min, +)-semiring, the oracle translation is then given by: 7r∗LB = ShortestPath(A0◦A1◦A2◦A3◦A4). We set parameters On as in (Tromble et al., 2008): O0 = 1, roughly corresponding to the brevity penalty (each word in a hypothesis adds up equally to the final path length) and On = −(4p · rn−1)−1, which are increasing discounts 124 Figure 2: Performance of the LB-4g oracle for different combinations of p and r on WMT11 de2en task. for matching n-grams. The values of p and r were found by grid search with a 0.05 step value. A typical result of the grid evaluation of the LB oracle for German to English WMT’11 task is displayed on Figure 2. The optimal values for the other pairs of languages were roughly in the same ballpark, with p Pz� 0.3 an</context>
</contexts>
<marker>Tromble, Kumar, Och, Macherey, 2008</marker>
<rawString>Roy W. Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice minimum bayes-risk decoding for statistical machine translation. In Proc. of the Conf. on EMNLP, pages 620– 629, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Turchi</author>
<author>Tijl De Bie</author>
<author>Nello Cristianini</author>
</authors>
<title>Learning performance of a machine translation system: a statistical and computational analysis.</title>
<date>2008</date>
<booktitle>In Proc. of WMT,</booktitle>
<pages>35--43</pages>
<location>Columbus, Ohio.</location>
<marker>Turchi, De Bie, Cristianini, 2008</marker>
<rawString>Marco Turchi, Tijl De Bie, and Nello Cristianini. 2008. Learning performance of a machine translation system: a statistical and computational analysis. In Proc. of WMT, pages 35–43, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillaume Wisniewski</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Assessing phrase-based translation models with oracle decoding.</title>
<date>2010</date>
<booktitle>In Proc. of the 2010 Conf. on EMNLP,</booktitle>
<pages>933--943</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2211" citStr="Wisniewski et al., 2010" startWordPosition="350" endWordPosition="353">theses than n-best lists and better approximate the search space. Exploring the PBSMT search space is one of the few means to perform diagnostic analysis and to better understand the behavior of the system (Turchi et al., 2008; Auli et al., 2009). Useful diagnostics are, for instance, provided by looking at the best (oracle) hypotheses contained in the search space, i.e, those hypotheses that have the highest quality score with respect to one or several references. Such oracle hypotheses can be used for failure analysis and to better understand the bottlenecks of existing translation systems (Wisniewski et al., 2010). Indeed, the inability to faithfully reproduce reference translations can have many causes, such as scantiness of the translation table, insufficient expressiveness of reordering models, inadequate scoring function, non-literal references, over-pruned lattices, etc. Oracle decoding has several other applications: for instance, in (Liang et al., 2006; Chiang et al., 2008) it is used as a work-around to the problem of non-reachability of the reference in discriminative training of MT systems. Lattice reranking (Li and Khudanpur, 2009), a promising way to improve MT systems, also relies on oracl</context>
<context position="4404" citStr="Wisniewski et al., 2010" startWordPosition="701" endWordPosition="704">et al., 2006). The exact computation of oracles under corpus level metrics, such as BLEU, poses supplementary combinatorial problems that will not be addressed in this work. In this paper, we present two original methods for finding approximate oracle hypotheses on lattices. The first one is based on a linear approximation of the corpus BLEU, that was originally designed for efficient Minimum Bayesian Risk decoding on lattices (Tromble et al., 2008). The second one, based on Integer Linear Programming, is an extension to lattices of a recent work on failure analysis for phrase-based decoders (Wisniewski et al., 2010). In this framework, we study two decoding strategies: one based on a generic ILP solver, and one, based on Lagrangian relaxation. Our contribution is also experimental as we compare the quality of the BLEU approximations and the time performance of these new approaches with several existing methods, for different language pairs and using the lattice generation capacities of two publicly-available state-of-theart phrase-based decoders: Moses1 and N-code2. The rest of this paper is organized as follows. In Section 2, we formally define the oracle decoding task and recall the formalism of finite</context>
</contexts>
<marker>Wisniewski, Allauzen, Yvon, 2010</marker>
<rawString>Guillaume Wisniewski, Alexandre Allauzen, and Franc¸ois Yvon. 2010. Assessing phrase-based translation models with oracle decoding. In Proc. of the 2010 Conf. on EMNLP, pages 933–943, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wolsey</author>
</authors>
<title>Integer Programming.</title>
<date>1998</date>
<publisher>John Wiley &amp; Sons, Inc.</publisher>
<contexts>
<context position="20159" citStr="Wolsey, 1998" startWordPosition="3472" endWordPosition="3473">d search with a 0.05 step value. A typical result of the grid evaluation of the LB oracle for German to English WMT’11 task is displayed on Figure 2. The optimal values for the other pairs of languages were roughly in the same ballpark, with p Pz� 0.3 and r Pz� 0.2. 5 Oracles with n-gram Clipping In this section, we describe two new oracle decoders that take n-gram clipping into account. These oracles leverage on the well-known fact that the shortest path problem, at the heart of all the oracles described so far, can be reduced straightforwardly to an Integer Linear Programming (ILP) problem (Wolsey, 1998). Once oracle decoding is formulated as an ILP problem, it is relatively easy to introduce additional constraints, for instance to enforce n-gram clipping. We will first describe the optimization problem of oracle decoding and then present several ways to efficiently solve it. 5.1 Problem Description Throughout this section, abusing the notations, we will also think of an edge ξi as a binary variable describing whether the edge is “selected” or not. The set {0,1}#{Ξ} of all possible edge assignments will be denoted by P. Note that Π, the set of all paths in the lattice is a subset of P: by enf</context>
<context position="22128" citStr="Wolsey, 1998" startWordPosition="3822" endWordPosition="3823">d (resp. a penalty) for generating a word in the reference (resp. not in the reference). The score of an assignment � E P is then defined as: score(ξ) = �#{Ξ} i=1 ξi · θi. This score can be seen as a compromise between the number of common words in the hypothesis and the reference (accounting for recall) and the number of words of the hypothesis that do not appear in the reference (accounting for precision). As explained in Section 2.3, finding the oracle hypothesis amounts to solving the shortest distance (or path) problem (3), which can be reformulated by a constrained optimization problem (Wolsey, 1998): ξi · θi (6) �s.t. ξ = 1, � ξ = 1 ξ∈Ξ−(qF ) ξ∈Ξ+(q0) � ξ − � ξ = 0, q E 2\{q0, qF} ξ∈Ξ+(q) ξ∈Ξ−(q) where q0 (resp. qF) is the initial (resp. final) state of the lattice and Ξ−(q) (resp. Ξ+(q)) denotes the set of incoming (resp. outgoing) edges of state q. These path constraints ensure that the solution of the problem is a valid path in the lattice. The optimization problem in Equation (6) can be further extended to take clipping into account. Let us introduce, for each word w, a variable γw that denotes the number of times w appears in the hypothesis clipped to the number of times, it appears</context>
</contexts>
<marker>Wolsey, 1998</marker>
<rawString>L. Wolsey. 1998. Integer Programming. John Wiley &amp; Sons, Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>