<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9975925">
Modelling Early Language Acquisition Skills:
Towards a General Statistical Learning Mechanism
</title>
<author confidence="0.913921">
Guillaume Aimetti
</author>
<affiliation confidence="0.914119">
University of Sheffield
</affiliation>
<address confidence="0.58273">
Sheffield, UK
</address>
<email confidence="0.995945">
g.aimetti@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.998583" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99983125">
This paper reports the on-going research of a
thesis project investigating a computational
model of early language acquisition. The
model discovers word-like units from cross-
modal input data and builds continuously
evolving internal representations within a cog-
nitive model of memory. Current cognitive
theories suggest that young infants employ
general statistical mechanisms that exploit the
statistical regularities within their environment
to acquire language skills. The discovery of
lexical units is modelled on this behaviour as
the system detects repeating patterns from the
speech signal and associates them to discrete
abstract semantic tags. In its current state, the
algorithm is a novel approach for segmenting
speech directly from the acoustic signal in an
unsupervised manner, therefore liberating it
from a pre-defined lexicon. By the end of the
project, it is planned to have an architecture
that is capable of acquiring language and
communicative skills in an online manner, and
carry out robust speech recognition. Prelimi-
nary results already show that this method is
capable of segmenting and building accurate
internal representations of important lexical
units as ‘emergent’ properties from cross-
modal data.
</bodyText>
<sectionHeader confidence="0.999838" genericHeader="keywords">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.838286">
Conventional Automatic Speech Recognition
</subsectionHeader>
<bodyText confidence="0.99965425">
(ASR) systems can achieve very accurate recog-
nition results, particularly when used in their op-
timal acoustic environment on examples within
their stored vocabularies. However, when taken
out of their comfort zone accuracy significantly
deteriorates and does not come anywhere near
human speech processing abilities for even the
simplest of tasks. This project investigates novel
computational language acquisition techniques
that attempt to model current cognitive theories
in order to achieve a more robust speech recogni-
tion system.
Current cognitive theories suggest that our
surrounding environment is rich enough to ac-
quire language through the use of simple statisti-
cal processes, which can be applied to all our
senses. The system under development aims to
help clarify this theory, implementing a compu-
tational model that is general across multiple
modalities and has not been pre-defined with any
linguistic knowledge.
In its current form, the system is able to detect
words directly from the acoustic signal and in-
crementally build internal representations within
a memory architecture that is motivated by cog-
nitive plausibility. The algorithm proposed can
be split into two main processes, automatic seg-
mentation and word discovery. Automatically
segmenting speech directly from the acoustic
signal is made possible through the use of dy-
namic programming (DP); we call this method
acoustic DP-ngram’s. The second stage, key
word discovery (KWD), enables the model to
hypothesise and build internal representations of
word classes that associates the discovered lexi-
cal units with discrete abstract semantic tags.
Cross-modal input is fed to the system through
the interaction of a carer module as an ‘audio’
and ‘visual’ stream. The audio stream consists of
an acoustic signal representing an utterance,
while the visual stream is a discrete abstract se-
mantic tag referencing the presence of a key
word within the utterance.
Initial test results show that there is significant
potential with the current algorithm, as it seg-
ments in an unsupervised manner and does not
rely on a predefined lexicon or acoustic phone
models that constrain current ASR methods.
</bodyText>
<note confidence="0.9599965">
Proceedings of the EACL 2009 Student Research Workshop, pages 1–9,
Athens, Greece, 2 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.973481">
1
</page>
<bodyText confidence="0.908639375">
The rest of this paper is organized as follows.
Section 2 reviews current developmental theories
and computational models of early language ac-
quisition. In section 3, we present the current
implementation of the system. Preliminary ex-
periments and results are described in sections 4
and 5 respectively. Conclusions and further work
are discussed in sections 6 and 7 respectively.
</bodyText>
<sectionHeader confidence="0.998867" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.998857">
2.1 Current Developmental Theories
</subsectionHeader>
<bodyText confidence="0.978932111111111">
The ‘nature’ vs. ‘nurture’ debate has been fought
out for many years now; are we born with innate
language learning capabilities, or do we solely
use the input from the environment to find struc-
ture in language?
Nativists believe that infants have an innate
capability for acquiring language. It is their view
that an infant can acquire linguistic structure
with little input and that it plays a minor role in
the speed and sequence with which they learn
language. Noam Chomsky is one of the most
cited language acquisition nativists, claiming
children can acquire language “On relatively
slight exposure and without specific training”
(Chomsky, 1975, p.4).
On the other hand, non-nativists argue that the
input contains much more structural information
and is not as full of errors as suggested by nativ-
</bodyText>
<note confidence="0.957735666666667">
ists (Eimas et al., 1971; Best et al., 1988; Jusc-
zyk et al., 1993; Saffran et al., 1996;
Christiansen et al., 1998; Saffran et al., 1999;
Saffran et al., 2000; Kirkham et al., 2002;
Anderson et al., 2003; Seidenberg et al., 2002;
Kuhl, 2004; Hannon and Trehub, 2005).
</note>
<bodyText confidence="0.991648318181818">
Experiments by Saffran et al. (1996, 1999)
show that 8-month old infants use the statistical
information in speech as an aid for word segmen-
tation with only two minutes of familiarisation.
Inspired by these results, Kirkham et al.
(2002) suggest that the same statistical processes
are also present in the visual domain. Kirkham et
al. (2002) carried out experiments showing that
preverbal infants are able to learn patterns of vis-
ual stimuli with very short exposure.
Other theories hypothesise that statistical and
grammatical processes are both used when learn-
ing language (Seidenberg et al., 2002; Kuhl,
2004). The hypothesis is that newborns begin life
using statistical processes for simpler problems,
such as learning the sounds of their native lan-
guage and building a lexicon, whereas grammar
is learnt via non-statistical methods later on. Sei-
denberg et al. (2002) believe that learning
grammar begins when statistical learning ends.
This has proven to be a very difficult boundary
to detect.
</bodyText>
<subsectionHeader confidence="0.999821">
2.2 Current Computational Models
</subsectionHeader>
<bodyText confidence="0.976432181818182">
There has been a lot of interest in trying to seg-
ment speech in an unsupervised manner, there-
fore liberating it from the required expert knowl-
edge needed to predefine the lexical units for
conventional ASR systems. This has led speech
recognition researchers to delve into the cogni-
tive sciences to try and gain an insight into how
humans achieve this without much difficulty and
model it.
Brent (1999) states that for a computational
algorithm to be cognitively plausible it must:
</bodyText>
<listItem confidence="0.999783">
• Start with no prior knowledge of general
language structure.
• Learn in a completely unsupervised
manner.
• Segment incrementally.
</listItem>
<bodyText confidence="0.99750468">
An automatic segmentation method similar to
that of the acoustic DP-ngram method is segmen-
tal DTW. Park &amp; Glass (2008) have adapted dy-
namic time warping (DTW) to find matching
acoustic patterns between two utterances. The
discovered units are then clustered, using an ad-
jacency graph method, to describe the topic of
the speech data.
Statistical Word Discovery (SWD) (ten Bosch
and Cranen, 2007) and the Cross-channel Early
Lexical Learning (CELL) model (Roy and Pent-
land, 2002), also similar methods to the one de-
scribed in this paper, discover word-like units
and then updating internal representations
through clustering processes. The downfall of the
CELL approach is that it assumes speech is ob-
served as an array of phone probabilities.
A more radical approach is Non-negative ma-
trix factorization (NMF) (Stouten et al., 2008).
NMF detects words from ‘raw’ cross-modal in-
put without any kind of segmentation during the
whole process, coding recurrent speech frag-
ments into to ‘word-like’ entities. However, the
factorisation process removes all temporal in-
formation.
</bodyText>
<sectionHeader confidence="0.96878" genericHeader="method">
3 The Proposed System
</sectionHeader>
<subsectionHeader confidence="0.92542">
3.1 ACORNS
</subsectionHeader>
<bodyText confidence="0.970524333333333">
The computational model reported in this paper
is being developed as part of a European project
called ACORNS (Acquisition of Communication
</bodyText>
<page confidence="0.988566">
2
</page>
<bodyText confidence="0.93855836">
and Recognition Skills). The ACORNS project
intends to design an artificial agent (Little
Acorns) that is capable of acquiring human ver-
bal communication skills. The main objective is
to develop an end-to-end system that is biologi-
cally plausible; restricting the computational and
mathematical methods to those that model be-
havioural data of human speech perception and
production within five main areas:
Front-end Processing: Research and devel-
opment of new feature representations guided by
phonetic and psycho-linguistic experiments.
Pattern Discovery: Little Acorns (LA) will
start life without any prior knowledge of basic
speech units, discovering them from patterns
within the continuous input.
Memory Organisation and Access: A mem-
ory architecture that approaches cognitive plau-
sibility is employed to store discovered units.
Information Discovery and Integration: Ef-
ficient and effective techniques for retrieving the
patterns stored in memory are being developed.
Interaction and Communication: LA is
given an innate need to grow his vocabulary and
communicate with the environment.
</bodyText>
<subsectionHeader confidence="0.999456">
3.2 The Computational Model
</subsectionHeader>
<bodyText confidence="0.999155888888889">
There are two key processes to the language ac-
quisition model described in this paper; auto-
matic segmentation and word discovery. The
automatic segmentation stage allows the system
to build a library of similar repeating speech
fragments directly from the acoustic signal. The
second stage associates these fragments with the
observed semantic tags to create distinct key
word classes.
</bodyText>
<subsectionHeader confidence="0.961886">
Automatic Segmentation
</subsectionHeader>
<bodyText confidence="0.997493303030303">
The acoustic DP-ngram algorithm reported in
this section is a modification of the preceding
DP-ngram algorithm (Sankoff and Kruskal,
1983; Nowell and Moore, 1995). The original
DP-ngram model was developed by Sankoff and
Kruskal (1983) to find two similar portions of
gene sequences. Nowell and Moore (1995) then
modified this model to find repeated patterns
within a single phone transcription sequence
through self-similarity. Expanding on these
methods, the author has developed a variant that
is able to segment speech, directly from the
acoustic signal; automatically segmenting impor-
tant lexical fragments by discovering ‘similar’
repeating patterns. Speech is never the same
twice and therefore impossible to find exact
repetitions of importance (e.g. phones, words or
sentences).
The use of DP allows this algorithm to ac-
commodate temporal distortion through dynamic
time warping (DTW). The algorithm finds partial
matches, portions that are similar but not neces-
sarily identical, taking into account noise, speed
and different pronunciations of the speech.
Traditional template based speech recognition
algorithms using DP would compare two se-
quences, the input speech vectors and a word
template, penalising insertions, deletions and
substitutions with negative scores. Instead, this
algorithm uses quality scores, positive and nega-
tive, to reward matches and prevent anything
else; resulting in longer, more meaningful sub-
sequences.
</bodyText>
<figureCaption confidence="0.999619">
Figure 1: Acoustic DP-ngram Processes.
</figureCaption>
<bodyText confidence="0.833382875">
Figure 1 displays the simplified architecture of
the acoustic DP-ngram algorithm. There are four
main stages to the process:
Stage 1: The ACORNS MFCC front-end is
used to parameterise the raw speech signal of the
two utterances being fed to the system. The de-
fault settings have been used to output a series of
37-element feature vectors. The front-end is
based on Mel-Frequency Coefficients (MFCC),
which reflects the frequency sensitivity of the
auditory system, to give 12 MFCC coefficients.
A measure of the raw energy is added along with
12 differential (Δ) and 12 2nd differential (ΔΔ)
coefficients. The front-end also allows the option
for cepstral mean normalisation (CMN) and cep-
stral mean and variance normalisation (CMVN).
</bodyText>
<figureCaption confidence="0.3863665">
Stage 2: A local-match distance matrix is then
calculated by measuring the cosine distance be-
</figureCaption>
<figure confidence="0.999280454545454">
Pre-Processing
DP-ngram
Algorithm
Calculate Quality Scores
Discovered Lexical Units
Create Distance Matrix
Find Local Alignments
Get Feature Vectors
Speech
Utterance 1 (Ui)
Utterance 2 (Uj)
</figure>
<page confidence="0.977737">
3
</page>
<bodyText confidence="0.9871965">
tween each pair of frames (v1,v2) from the two
sequences, which is defined by:
</bodyText>
<equation confidence="0.8711495">
( 1, 2) ( 1 . 2) / ( 1 . 2)
T v v
T
d v v = v v (1)
</equation>
<bodyText confidence="0.99609125">
Stage 3: The distance matrix is then used to cal-
culate accumulative quality scores for successive
frame steps. The recurrence defined in equation
(2) is used to find all quality scores qi,j .
In order to maximize on quality, substitution
scores must be positive and both insertion and
deletion scores must be negative as initialised in
equation (3).
</bodyText>
<equation confidence="0.911027777777778">
( s d q
. . )
a b
, i j i j
− −
1, 1 − −
1, 1
i j
0,
</equation>
<bodyText confidence="0.913422">
where,
</bodyText>
<subsectionHeader confidence="0.986997">
1.1 (Insertion score)
1.1 (Deletion score)
</subsectionHeader>
<bodyText confidence="0.996571428571429">
= + 1.1 (Substitution score)
frame-frame distance
Accumulative quality score
The recurrence in equation (2) stops past dissimi-
larities causing global effects by setting all nega-
tive scores to zero, starting a fresh new homolo-
gous relationship between local alignments.
</bodyText>
<figure confidence="0.986844714285714">
Quality Score Matrix with Local Alignment
Utterance 1 10 15
20 10
30 5
40 0
50
60
70
80
90
100
110
10 20 30 40 50 60 70 80
Utterance 2
</figure>
<figureCaption confidence="0.998665333333333">
Figure 2: Quality score matrix calculated from two
different utterances. The plot also displays the optimal
local alignment.
</figureCaption>
<bodyText confidence="0.991229166666667">
Figure 2 shows the plot of the quality scores cal-
culated from two different utterances. The
shaded areas show repeating structure; longer
and more accurate fragments attain greater qual-
ity scores, indicated by the darker areas within
the plot.
Applying a substitution score of 1 will cause
the accumulative quality score to grow as a linear
function. The current settings defined by equa-
tion (3) use a substitution score greater than 1,
thus allowing local accumulative quality scores
to grow exponentially, giving longer alignments
more importance.
By setting insertion and deletion scores to val-
ues less than -1, the model will find closer
matching acoustic repetitions; whereas a value
greater than -1 and less than 0 allows the model
to find repeated patterns that are longer and less
accurate, therefore allowing control over the tol-
erance for temporal distortion.
Stage 4: The final stage is to discover local
alignments from within the quality score matrix.
Backtracking pointers ( bt) are maintained at
each step of the recursion:
</bodyText>
<equation confidence="0.9815532">
(i−1, j), (Insertion)
( , 1), (Deletion)
i j −
(i −1, j −1), (Substitution)
(0,0) (Initial pointer)
</equation>
<bodyText confidence="0.999893933333333">
When the quality scores have been calculated
through equation (2), it is possible to backtrack
from the highest score to obtain the local align-
ments in order of importance with equation (4).
A threshold is set so that only local alignments
above a desired quality score are to be retrieved.
Figure 2 presents the optimal local alignment
that was discovered by the acoustic DP-ngram
algorithm for the utterances “Ewan is shy” and
“Ewan sits on the couch”.
The discovered repeated pattern (the dark line
in figure 2) is [y uw ah n]. Start and stop times
are collected which allows the model to retrieve
the local alignment from the original audio signal
in full fidelity when required.
</bodyText>
<subsectionHeader confidence="0.928423">
Key Word Discovery
</subsectionHeader>
<bodyText confidence="0.999993">
The milestone set for all systems developed
within the ACORNS project is for LA to learn 10
key words. To carry out this task, the DP-ngram
algorithm has been modified with the addition of
a key word discovery (KWD) method that con-
tinues the theme of a general statistical learning
mechanism. The acoustic DP-ngram algorithm
exploits the co-occurrence of similar acoustic
patterns within different utterances; whereas, the
</bodyText>
<equation confidence="0.876719">
( s d . q )
a φ . − 1
, i j
− i j
1, − 1,
i
( s d
. − 1 . q )
φ , b i j
, 1
− i j
, 1
−
j
qi−1,j +
qi,j−1 +
,
, (2)
</equation>
<figure confidence="0.994385916666666">
qi j = max i
, �
�
��
qi−1,j−1 +
,
,
φ
sai
,bj
sφ
sa b
i j
,
di j
,
q i j
,
(3) bt i j �
, � �
= �
�
��
(4)
</figure>
<page confidence="0.966825">
4
</page>
<bodyText confidence="0.991032291666667">
KWD method exploits the co-occurrence of the
associated discrete abstract semantic tags. This
allows the system to associate cross-modal re-
peating patterns and build internal representa-
tions of the key words.
KWD is a simple approach that creates a class
for each key word (semantic tag) observed, in
which all discovered exemplar units representing
each key word are stored. With this list of epi-
sodic segments we can perform a clustering
process to derive an ideal representation of each
key word.
For a single iteration of the DP-ngram algo-
rithm, the current utterance (Uttcur) is compared
with another utterance in memory ( Uttn ) . KWD
hypothesises whether the segments found within
the two utterances are potential key words, by
simply comparing the associated semantic tags.
There are three possible paths for a single itera-
tion:
1: If the tag of Uttcur has never been seen then
create a new key word class and store the whole
utterance as an exemplar of it. Do not carry out
the acoustic DP-ngram process and proceed to
the next utterance in memory (Uttn+1) .
2: If both utterances share the same tag then
proceed with the acoustic DP-ngram process and
append discovered local alignments to the key
word class representing that tag. Proceed to the
next utterance in memory (Uttn+1) .
3: If both utterances contain different tags then
do not carry out acoustic DP-ngram’s and pro-
ceed to the next utterance in memory (Uttn+1) .
By creating an exemplar list for each key word
class we are able to carry out a clustering process
that allows us to create a model of the ideal rep-
resentation. Currently, the clustering process im-
plemented simply calculates the ‘centroid’ ex-
emplar, finding the local alignment with the
shortest distance from all the other local align-
ments within the same class. The ‘centroid’ is
updated every time a new local alignment is
added, therefore the system is creating internal
representations that are continuously evolving
and becoming more accurate with experience.
For recognition tasks the system can be set to
use either the ‘centroid’ exemplar or all the
stored local alignments for each key word class.
</bodyText>
<subsectionHeader confidence="0.685731">
LA Architecture
</subsectionHeader>
<bodyText confidence="0.998212125">
The algorithm runs within a memory structure
(fig. 3) developed with inspiration from current
cognitive theories of memory (Jones et al.,
2006). The memory architecture works as fol-
lows:
Carer: The carer interacts with LA to con-
tinuously feed the system with cross-modal input
(acoustic &amp; semantic).
</bodyText>
<figureCaption confidence="0.993576">
Figure 3: Little Acorns’ memory architecture.
</figureCaption>
<bodyText confidence="0.99826125">
Perception: The stimulus is processed by the
‘perception’ module, converting the acoustic sig-
nal into a representation similar to the human
auditory system.
</bodyText>
<subsectionHeader confidence="0.423343">
Short Term Memory (STM): The output of
</subsectionHeader>
<bodyText confidence="0.961563296296296">
the ‘perception’ module is stored in a limited
STM which acts as a circular buffer to store n
past utterances. The n past utterances are com-
pared with the current input to discover repeated
patterns in an incremental fashion. As a batch
process LA can only run on a limited number of
utterances as the search space is unbound. As an
incremental process, LA could potentially handle
an infinite number of utterances, thus making it a
more cognitively plausible system.
Long Term Memory (LTM): The ever in-
creasing lists of discovered units for each key
word representation are stored in LTM. Cluster-
ing processes can then be applied to build and
update internal representations. The representa-
tions stored within LTM are only pointers to
where the segment lies within the very long term
memory.
Very Long Term Memory: The very long
term memory is used to store every observed ut-
terance. It is important to note that unless there is
a pointer for a segment of speech within LTM
then the data cannot be retrieved. But, future
work may be carried out to incorporate addi-
tional ‘sleeping’ processes on the data stored in
VLTM to re-organise internal representations or
carry out additional analysis.
</bodyText>
<sectionHeader confidence="0.99982" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999561333333333">
Accuracy of experiments within the ACORNS
project is based on LA’s response to its carer.
The correct response is for LA to predict the key
</bodyText>
<figure confidence="0.99916275">
CARER
Multi-Modal
Sensory Data
Response from LA
LA
Perception
Front-end
processing
Retrieval:
Memory Access
DP-ngram - Pattern Discovery
STM/Working Memory
Episodic Buffer
LTM
Internal Represen-
tations
VLTM
Episodic Memory
of all past events
KWD
Information
Discovery
and Organi-
sation
</figure>
<page confidence="0.945186">
5
</page>
<bodyText confidence="0.982029243243243">
word tag associated with the current incoming
utterance while only observing the speech signal.
LA re-uses the acoustic DP-ngram algorithm to
solve this task in a similar manner to traditional
DP template based speech recognition. The rec-
ognition process is carried out by comparing ex-
emplars, of discovered key words, against the
current incoming utterance and calculating a
quality distance (as described in stage 3 of sec-
tion 3.2). Thus, the exemplar producing the high-
est quality score, by finding the longest align-
ment, is taken to be the match, with which we
can predict its associated visual tag.
A number of different experiments have been
carried out:
E1 - Optimal STM Window: This experi-
ment finds the optimal utterance window length
for the system as an incremental process. Vary-
ing values of the utterance window length (from
1 to 100) were used to obtain key word recogni-
tion accuracy results across the same data set.
E2 - Batch vs. Incremental: The optimal
window length chosen for the incremental im-
plementation is compared against the batch im-
plementation of the algorithm.
E3 - Centroid vs. Exemplars: The KWD
process stores a list of exemplars representing
each key word class. For the recognition task we
can either use all the exemplars in each key word
list or a single ‘centroid’ exemplar that best
represents the list. This experiment will compare
these two methods for representing internal rep-
resentations of the key words.
E4 – Speaker Dependency: The algorithm is
tested on its ability to handle the variation in
speech from different speakers with different
feature vectors.
</bodyText>
<equation confidence="0.99756425">
V1 = HTK MFCC&apos;s (no norm)
V2 = ACORNS MFCC&apos;s (no norm)
V3 = ACORNS MFCC&apos;s (Cepstral Mean Norm)
V4 = ACORNS MFCC&apos;s (Cepstral Mean and Variance Norm)
</equation>
<bodyText confidence="0.9993612">
Using normalisation methods will reduce the
information within the feature vectors, removing
some of the speaker variation. Therefore, key
word detection should be more accurate for a
data set of multiple speakers with normalisation.
</bodyText>
<subsectionHeader confidence="0.999791">
4.1 Test Data
</subsectionHeader>
<bodyText confidence="0.998911458333334">
The ACORNS English corpus is used for the
above experiments. Sentences were created by
combining a carrier sentence with a keyword. A
total of 10 different carrier sentences, such as
“Do you see the X”, “Where is the X”, etc., where
X is a keyword, were combined with one of ten
different keywords, such as “Bottle”, “Ball”, etc.
This created 100 unique sentences which were
repeated 10 times and recorded with 4 different
speakers (2 male and 2 female) to produce 4000
utterances.
In addition to the acoustic data, each utterance
is associated with an abstract semantic tag. As an
example, the utterance “What matches this
shoe” will contain the tag referring to “shoe”.
The tag does not give any location or phonetic
information about the key word within the utter-
ance.
E1 and E2 use a sub-set of 100 different utter-
ances from a single speaker. E3 is carried out on
a sub-set of 200 utterances from a single speaker
and the database used for E4 is a sub-set of 200
utterances from all four speakers (2 male and 2
female) presented in a random order.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999907153846154">
E1: LA was tested on 100 utterances with vary-
ing utterance window lengths. The plot in figure
4 shows the total key word detection accuracy
for each window length used. The x-axis displays
the utterance window lengths (1–100) and the y-
axis displays the total accuracy.
The results are as expected. Longer window
lengths achieve more accurate results. This is
because longer window lengths produce a larger
search space and therefore have more chance of
capturing repeating events. Shorter window
lengths are still able to build internal representa-
tions, but over a longer period.
</bodyText>
<subsectionHeader confidence="0.4906945">
Word Detection Accuracy for varying window
lengths (1-100) over 100 utterances
</subsectionHeader>
<figure confidence="0.628545">
0 10 20 30 40 50 60 70 80 90 100
Utterance Window Length
</figure>
<figureCaption confidence="0.8311935">
Figure 4: Single speaker key word accuracy using
varying utterance window lengths of 1-100.
</figureCaption>
<bodyText confidence="0.999153333333333">
Accuracy results reach a maximum with an ut-
terance window length of 21 and then stabilize at
around 58% (±1%). From this we can conclude
</bodyText>
<figure confidence="0.997945545454546">
Accuracy (%) 100
90
80
70
60
50
40
30
20
10
0
</figure>
<page confidence="0.990096">
6
</page>
<bodyText confidence="0.999546666666667">
that 21 is the minimum window length needed to
build accurate internal representations of the
words within the test set, and will be used for all
subsequent experiments.
E2: The plot in figure 4 displays the total key
word detection accuracy for the different utter-
ance window lengths and does not show the
gradual word acquisition process. Figure 5 com-
pares the word detection accuracy of the system
(y-axis) as a function of the number of utterances
observed (x-axis). Accuracy is recorded as the
percentage of correct replies for the last ten ob-
servations. The long discontinuous line in the
plot shows the word detections accuracy for ran-
domly guessing the key word.
</bodyText>
<subsectionHeader confidence="0.8480205">
Word Detection Accuracy
Incremental vs. Batch Process
</subsectionHeader>
<figure confidence="0.974817722222222">
Y
h
C
h
h
100
90
80
70
60
50
40
30
20
10
0
0 10 20 30 40 50 60 70 80 90 100
Utterances Observed
</figure>
<figureCaption confidence="0.862932666666667">
Figure 5: Word detection accuracy LA running as a
batch and incremental process. Results are plotted as a
function of the past 10 utterances observed.
</figureCaption>
<bodyText confidence="0.99741328125">
It can be seen from the plot in figure 5 that the
system begins life with no word representations.
At the beginning, the system hypothesises new
word units from which it can begin to bootstrap
its internal representations.
As an incremental process, with the optimal
window length, the system is able to capture
enough repeating patterns and even begins to
outperform the batch process after 90 utterances.
This is due to additional alignments discovered
by the batch process that are temporarily distort-
ing a word representation, but the batch process
would ‘catch up’ in time.
Another important result to take into account
is that only comparing the current incoming ut-
terance with the last observed utterance is
enough to build word representations. Although
this is very efficient, the problem is that there is a
greater possibility that some words will never be
discovered if they are not present in adjacent ut-
terances within the data set.
E3: Currently the recognition process uses all the
discovered exemplars within each key word
class. This process causes the computational
complexity to increase exponentially. It is also
not suitable for an incremental process with the
potential of running on an infinite data set.
To tackle this problem, recognition was car-
ried out using the ‘centroid’ exemplar of each
key word class. Figure 6 shows the word detec-
tion accuracy as a function of utterances ob-
served for both methods.
</bodyText>
<subsectionHeader confidence="0.729143">
Word Detection Accuracy
Centroid vs. Complete Exemplar List
</subsectionHeader>
<figure confidence="0.997039176470588">
100
All Exemplars
Centroid
Random
80
70
60
50
40
4 5 6 7 8 9 0
0 0 0 0 0 0 0
30
20
10
0
0 20 40 60 80 100 120 140 160 180 200
Utterances Observed
</figure>
<figureCaption confidence="0.997019">
Figure 6: Word detection accuracy using centroids
and complete exemplar list for recognition.
</figureCaption>
<bodyText confidence="0.999987923076923">
The results show that the ‘centroid’ method is
quickly outperformed and that the word detection
accuracy difference increases with experience.
After 120 utterances performance seems to
gradually decline. This is because the ‘centroid’
method cannot handle the variation in the acous-
tic speech data. Using all the discovered units for
recognition allows the system to reach an accu-
racy of 90% at around 140 utterances, where it
then seems to stabilise at around 88%.
E4: The addition of multiple speakers will add
greater variation to the acoustic signal, distorting
patterns of the same underlying unit. Over the
200 utterances observed, word detection accu-
racy of the internal representations increases, but
at a much slower rate than the single speaker ex-
periments (fig. 7).
The assumption that using normalisation meth-
ods would achieve greater word detection accu-
racy, by reducing speaker variation, does not
hold true. On reflection this comes as no sur-
prise, as the system collects exemplar units with
a larger relative fidelity for each speaker.
This raises an important issue; the optimal ut-
terance window length for the algorithm as an
incremental process was calculated for a single
</bodyText>
<figure confidence="0.96300225">
t1ttWin = 1
t1ttWin = 21
Batch
Random
Accuracy (%)
n n
Accuracy (%)
90
</figure>
<page confidence="0.997422">
7
</page>
<bodyText confidence="0.999975">
speaker, therefore, increasing the search space
will allow the model to find more repeating pat-
terns from the same speaker. Following this
logic, it could be hypothesised that the optimal
search space should be four times the size used
for one speaker and that it will take four times as
many observations to achieve the same accuracy.
</bodyText>
<subsectionHeader confidence="0.9186185">
Word Detection Accuracy
Speaker-Dependency
</subsectionHeader>
<figureCaption confidence="0.964789">
Figure 7: Total accuracy using different feature vec-
tors after 200 observed utterances.
</figureCaption>
<sectionHeader confidence="0.998726" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999916307692307">
Preliminary results indicate that the environment
is rich enough for word acquisition tasks. The
pattern discovery and word learning algorithm
implemented within the LA memory architecture
has proven to be a successful approach for build-
ing stable internal representations of word-like
units. The model approaches cognitive plausibil-
ity by employing statistical processes that are
general across multiple modalities. The incre-
mental approach also shows that the model is
still able to learn correct word representations
with a very limited working memory model.
Additionally to the acquisition of words and
word-like units, the system is able to use the dis-
covered tokens for speech recognition. An im-
portant property of this method, that differenti-
ates it from conventional ASR systems, is that it
does not rely on a pre-defined vocabulary, there-
fore reducing language-dependency and out-of-
dictionary errors.
Another advantage of this system, compared
to systems such as NMF, is that it is able to give
temporal information of the whereabouts of im-
portant repeating structure which can be used to
code the acoustic signal as a lossless compres-
sion method.
</bodyText>
<sectionHeader confidence="0.995278" genericHeader="discussions">
7 Discussion &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999956976744186">
A key question driving this research is whether
modelling human language acquisition can help
create a more robust speech recognition system.
Therefore further development of the proposed
architecture will continue to be limited to cogni-
tively plausible approaches and should exhibit
similar developmental properties as early human
language learners. In its current state, the system
is fully operational and intends to be used as a
platform for further development and experi-
ments.
The experimental results are promising. How-
ever, it is clear to see that the model suffers from
speaker-dependency issues. The problem can be
split into two areas, front-end processing of the
incoming acoustic signal and the representation
of discovered lexical units in memory.
Development is being carried out on various
clustering techniques that build constantly evolv-
ing internal representations of internal lexical
classes in an attempt to model speech variation.
Additionally, a secondary update process, im-
plemented as a re-occurring ‘sleeping phase’ is
being investigated. This phase is going to allow
the memory organisation to re-structure itself by
looking at events over a longer history, which
could be carried out as a batch process.
The processing of prosodic cues, such as
speech rhythm and pitch intonation, will be in-
corporated within the algorithm to increase the
key word detection accuracy and further exploit
the richness of the learners surrounding envi-
ronment. Adults, when speaking to infants, will
highlight words of importance through infant
directed speech (IDS). During IDS adults place
more pitch variance on words that they want the
infant to attend to.
Further experiments have been planned to see
if the model exhibits similar patterns of learning
behaviour as young multiple language learners.
Experiments will be carried out with the multiple
languages available in the ACORNS database
(English, Finnish and Dutch).
</bodyText>
<sectionHeader confidence="0.983251" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.974056666666666">
This research was funded by the European
Commission, under contract number FP6-
034362, in the ACORNS project (www.acorns-
project.org). The author would also like to thank
Prof. Roger K. Moore for helping to shape this
work.
</bodyText>
<figure confidence="0.958530862068965">
V1 V2 V3 V4 V5
Accuracy (%)
100
90
80
70
60
50
40
30
20
10
0
Random
HTK – no norm
ACORNS –no norm
ACORNS – cmn
ACORNS – cmvn
8
M. R. Brent. 1999. Speech Segmentation and Word
Discovery: A Computational Perspective. Trends
in Cognitive Sciences, 3(8):294-301.
N. Chomsky. 1975. Reflections on Language. New
York: Pantheon Books.
References
A. Park and J. R. Glass. 2008. Unsupervised Pattern
Discovery in Speech. Transactions on Audio,
Speech and Language Processing, 16(1):186-
197.
</figure>
<reference confidence="0.999854484848485">
C. T. Best, G. W. McRoberts and N. M. Sithole. 1988.
Examination of the perceptual re-organization for
speech contrasts: Zulu click discrimination by Eng-
lish-speaking adults and infants. Journal of Ex-
perimental Psychology: Human Perception
and Performance, 14:345-360.
D. M. Jones, R. W. Hughes and W. J. Macken. 2006.
Perceptual Organization Masquerading as Phono-
logical Storage: Further Support for a Perceptual-
Gestural View of Short-Term Memory. Journal
of Memory and Language, 54:265-328.
D. Roy and A. Pentland. 2002. Learning Words from
Sights and Sounds: A Computational Model. Cog-
nitive Science, 26(1):113-146.
D. Sankoff and Kruskal J. B. 1983. Time Warps,
String Edits, and Macromolecules: The The-
ory and Practice of Sequence Comparison.
Addison-Wesley Publishing Company, Inc.
E. E. Hannon and S. E. Trehub. 2005. Turning in to
Musical Rhythms: Infants Learn More readily than
Adults. PNAS, 102(35):12639-12643.
J. L. Anderson, J. L. Morgan and K. S. White. 2003.
A Statistical Basis for Speech Sound Discrimina-
tion. Language and Speech, 46(43):155-182.
J. R. Saffran, R. N. Aslin and E. L. Newport. 1996.
Statistical Learning by 8-Month-Old Infants. SCI-
ENCE, 274:1926-1928.
J. R. Saffran, E. K. Johnson, R. N. Aslin and E. L.
Newport. 1999. Statistical Learning of Tone Se-
quences by Human Infants and Adults. Cognition,
70(1):27-52.
J. R. Saffran, A. Senghashas and J. C. Trueswell.
2000. The Acquisition of Language by Children.
PNAS, 98(23):12874-12875.
L. ten Bosch and B. Cranen. 2007. A Computational
Model for Unsupervised Word Discovery. IN-
TERSPEECH 2007, 1481-1484.
M. H. Christiansen, J. Allen and M. Seidenberg. 1998.
Learning to Segment Speech using Multiple Cues.
Language and Cognitive Processes, 13:221-
268.
M. S. Seidenberg, M. C. MacDonald and J. R. Saf-
fran. 2002. Does Grammar Start Where Statistics
Stop?. SCIENCE, 298:552-554.
N. Z. Kirkham, A. J. Slemmer and S. P. Johnson.
2002. Visual Statistical Learning in Infancy: Evi-
dence for a Domain General Learning Mechanism.
Cognition, 83:B35-B42.
P. D. Eimas, E. R. Siqueland, P. Jusczyk and J. Vigo-
rito. 1971. Speech Perception in Infants. Science,
171(3968):303-606.
P. K. Kuhl. 2004. Early Language Acquisition: Crack-
ing the Speech Code. Nature, 5:831-843.
P. Nowell and R. K. Moore. 1995. The Application of
Dynamic Programming Techniques to Non-Word
Based Topic Spotting. EuroSpeech ’95, 1355-
1358.
P. W. Jusczyk, A. D. Friederici, J. Wessels, V. Y.
Svenkerud and A. M. Jusczyk. 1993. Infants’ Sen-
sitivity to the Sound Patterns of Native Language
Words. Journal of Memory &amp; Language,
32:402-420.
V. Stouten, K. Demuynck and H. Van hamme. 2008.
Discovering Phone Patterns in Spoken Utterances
by Non-negative Matrix Factorisation. IEEE Sig-
nal Processing Letters, 131-134.
</reference>
<page confidence="0.997097">
9
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912874">
<title confidence="0.9997745">Modelling Early Language Acquisition Skills: Towards a General Statistical Learning Mechanism</title>
<author confidence="0.999833">Guillaume Aimetti</author>
<affiliation confidence="0.999887">University of Sheffield</affiliation>
<address confidence="0.986968">Sheffield, UK</address>
<email confidence="0.998435">g.aimetti@dcs.shef.ac.uk</email>
<abstract confidence="0.997458517241379">This paper reports the on-going research of a thesis project investigating a computational model of early language acquisition. The model discovers word-like units from crossmodal input data and builds continuously evolving internal representations within a cognitive model of memory. Current cognitive theories suggest that young infants employ general statistical mechanisms that exploit the statistical regularities within their environment to acquire language skills. The discovery of lexical units is modelled on this behaviour as the system detects repeating patterns from the speech signal and associates them to discrete abstract semantic tags. In its current state, the algorithm is a novel approach for segmenting speech directly from the acoustic signal in an unsupervised manner, therefore liberating it from a pre-defined lexicon. By the end of the project, it is planned to have an architecture that is capable of acquiring language and communicative skills in an online manner, and carry out robust speech recognition. Preliminary results already show that this method is capable of segmenting and building accurate internal representations of important lexical units as ‘emergent’ properties from crossmodal data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C T Best</author>
<author>G W McRoberts</author>
<author>N M Sithole</author>
</authors>
<title>Examination of the perceptual re-organization for speech contrasts: Zulu click discrimination by English-speaking adults and infants. Journal of Experimental Psychology: Human Perception and Performance,</title>
<date>1988</date>
<pages>14--345</pages>
<contexts>
<context position="5067" citStr="Best et al., 1988" startWordPosition="762" endWordPosition="765">that infants have an innate capability for acquiring language. It is their view that an infant can acquire linguistic structure with little input and that it plays a minor role in the speed and sequence with which they learn language. Noam Chomsky is one of the most cited language acquisition nativists, claiming children can acquire language “On relatively slight exposure and without specific training” (Chomsky, 1975, p.4). On the other hand, non-nativists argue that the input contains much more structural information and is not as full of errors as suggested by nativists (Eimas et al., 1971; Best et al., 1988; Jusczyk et al., 1993; Saffran et al., 1996; Christiansen et al., 1998; Saffran et al., 1999; Saffran et al., 2000; Kirkham et al., 2002; Anderson et al., 2003; Seidenberg et al., 2002; Kuhl, 2004; Hannon and Trehub, 2005). Experiments by Saffran et al. (1996, 1999) show that 8-month old infants use the statistical information in speech as an aid for word segmentation with only two minutes of familiarisation. Inspired by these results, Kirkham et al. (2002) suggest that the same statistical processes are also present in the visual domain. Kirkham et al. (2002) carried out experiments showing </context>
</contexts>
<marker>Best, McRoberts, Sithole, 1988</marker>
<rawString>C. T. Best, G. W. McRoberts and N. M. Sithole. 1988. Examination of the perceptual re-organization for speech contrasts: Zulu click discrimination by English-speaking adults and infants. Journal of Experimental Psychology: Human Perception and Performance, 14:345-360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Jones</author>
<author>R W Hughes</author>
<author>W J Macken</author>
</authors>
<title>Perceptual Organization Masquerading as Phonological Storage: Further Support for a PerceptualGestural View of Short-Term Memory.</title>
<date>2006</date>
<journal>Journal of Memory and Language,</journal>
<pages>54--265</pages>
<contexts>
<context position="18128" citStr="Jones et al., 2006" startWordPosition="2922" endWordPosition="2925">, finding the local alignment with the shortest distance from all the other local alignments within the same class. The ‘centroid’ is updated every time a new local alignment is added, therefore the system is creating internal representations that are continuously evolving and becoming more accurate with experience. For recognition tasks the system can be set to use either the ‘centroid’ exemplar or all the stored local alignments for each key word class. LA Architecture The algorithm runs within a memory structure (fig. 3) developed with inspiration from current cognitive theories of memory (Jones et al., 2006). The memory architecture works as follows: Carer: The carer interacts with LA to continuously feed the system with cross-modal input (acoustic &amp; semantic). Figure 3: Little Acorns’ memory architecture. Perception: The stimulus is processed by the ‘perception’ module, converting the acoustic signal into a representation similar to the human auditory system. Short Term Memory (STM): The output of the ‘perception’ module is stored in a limited STM which acts as a circular buffer to store n past utterances. The n past utterances are compared with the current input to discover repeated patterns in</context>
</contexts>
<marker>Jones, Hughes, Macken, 2006</marker>
<rawString>D. M. Jones, R. W. Hughes and W. J. Macken. 2006. Perceptual Organization Masquerading as Phonological Storage: Further Support for a PerceptualGestural View of Short-Term Memory. Journal of Memory and Language, 54:265-328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roy</author>
<author>A Pentland</author>
</authors>
<title>Learning Words from Sights and Sounds: A Computational Model.</title>
<date>2002</date>
<journal>Cognitive Science,</journal>
<pages>26--1</pages>
<contexts>
<context position="7420" citStr="Roy and Pentland, 2002" startWordPosition="1140" endWordPosition="1144">le it must: • Start with no prior knowledge of general language structure. • Learn in a completely unsupervised manner. • Segment incrementally. An automatic segmentation method similar to that of the acoustic DP-ngram method is segmental DTW. Park &amp; Glass (2008) have adapted dynamic time warping (DTW) to find matching acoustic patterns between two utterances. The discovered units are then clustered, using an adjacency graph method, to describe the topic of the speech data. Statistical Word Discovery (SWD) (ten Bosch and Cranen, 2007) and the Cross-channel Early Lexical Learning (CELL) model (Roy and Pentland, 2002), also similar methods to the one described in this paper, discover word-like units and then updating internal representations through clustering processes. The downfall of the CELL approach is that it assumes speech is observed as an array of phone probabilities. A more radical approach is Non-negative matrix factorization (NMF) (Stouten et al., 2008). NMF detects words from ‘raw’ cross-modal input without any kind of segmentation during the whole process, coding recurrent speech fragments into to ‘word-like’ entities. However, the factorisation process removes all temporal information. 3 The</context>
</contexts>
<marker>Roy, Pentland, 2002</marker>
<rawString>D. Roy and A. Pentland. 2002. Learning Words from Sights and Sounds: A Computational Model. Cognitive Science, 26(1):113-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sankoff</author>
<author>J B Kruskal</author>
</authors>
<title>Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison.</title>
<date>1983</date>
<publisher>Addison-Wesley Publishing Company, Inc.</publisher>
<contexts>
<context position="9858" citStr="Sankoff and Kruskal, 1983" startWordPosition="1502" endWordPosition="1505">s vocabulary and communicate with the environment. 3.2 The Computational Model There are two key processes to the language acquisition model described in this paper; automatic segmentation and word discovery. The automatic segmentation stage allows the system to build a library of similar repeating speech fragments directly from the acoustic signal. The second stage associates these fragments with the observed semantic tags to create distinct key word classes. Automatic Segmentation The acoustic DP-ngram algorithm reported in this section is a modification of the preceding DP-ngram algorithm (Sankoff and Kruskal, 1983; Nowell and Moore, 1995). The original DP-ngram model was developed by Sankoff and Kruskal (1983) to find two similar portions of gene sequences. Nowell and Moore (1995) then modified this model to find repeated patterns within a single phone transcription sequence through self-similarity. Expanding on these methods, the author has developed a variant that is able to segment speech, directly from the acoustic signal; automatically segmenting important lexical fragments by discovering ‘similar’ repeating patterns. Speech is never the same twice and therefore impossible to find exact repetition</context>
</contexts>
<marker>Sankoff, Kruskal, 1983</marker>
<rawString>D. Sankoff and Kruskal J. B. 1983. Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison. Addison-Wesley Publishing Company, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E E Hannon</author>
<author>S E Trehub</author>
</authors>
<title>Turning in to Musical Rhythms: Infants Learn More readily than Adults.</title>
<date>2005</date>
<pages>102--35</pages>
<publisher>PNAS,</publisher>
<contexts>
<context position="5290" citStr="Hannon and Trehub, 2005" startWordPosition="801" endWordPosition="804">ey learn language. Noam Chomsky is one of the most cited language acquisition nativists, claiming children can acquire language “On relatively slight exposure and without specific training” (Chomsky, 1975, p.4). On the other hand, non-nativists argue that the input contains much more structural information and is not as full of errors as suggested by nativists (Eimas et al., 1971; Best et al., 1988; Jusczyk et al., 1993; Saffran et al., 1996; Christiansen et al., 1998; Saffran et al., 1999; Saffran et al., 2000; Kirkham et al., 2002; Anderson et al., 2003; Seidenberg et al., 2002; Kuhl, 2004; Hannon and Trehub, 2005). Experiments by Saffran et al. (1996, 1999) show that 8-month old infants use the statistical information in speech as an aid for word segmentation with only two minutes of familiarisation. Inspired by these results, Kirkham et al. (2002) suggest that the same statistical processes are also present in the visual domain. Kirkham et al. (2002) carried out experiments showing that preverbal infants are able to learn patterns of visual stimuli with very short exposure. Other theories hypothesise that statistical and grammatical processes are both used when learning language (Seidenberg et al., 20</context>
</contexts>
<marker>Hannon, Trehub, 2005</marker>
<rawString>E. E. Hannon and S. E. Trehub. 2005. Turning in to Musical Rhythms: Infants Learn More readily than Adults. PNAS, 102(35):12639-12643.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Anderson</author>
<author>J L Morgan</author>
<author>K S White</author>
</authors>
<title>A Statistical Basis for Speech Sound Discrimination. Language and Speech,</title>
<date>2003</date>
<pages>46--43</pages>
<contexts>
<context position="5227" citStr="Anderson et al., 2003" startWordPosition="791" endWordPosition="794">t plays a minor role in the speed and sequence with which they learn language. Noam Chomsky is one of the most cited language acquisition nativists, claiming children can acquire language “On relatively slight exposure and without specific training” (Chomsky, 1975, p.4). On the other hand, non-nativists argue that the input contains much more structural information and is not as full of errors as suggested by nativists (Eimas et al., 1971; Best et al., 1988; Jusczyk et al., 1993; Saffran et al., 1996; Christiansen et al., 1998; Saffran et al., 1999; Saffran et al., 2000; Kirkham et al., 2002; Anderson et al., 2003; Seidenberg et al., 2002; Kuhl, 2004; Hannon and Trehub, 2005). Experiments by Saffran et al. (1996, 1999) show that 8-month old infants use the statistical information in speech as an aid for word segmentation with only two minutes of familiarisation. Inspired by these results, Kirkham et al. (2002) suggest that the same statistical processes are also present in the visual domain. Kirkham et al. (2002) carried out experiments showing that preverbal infants are able to learn patterns of visual stimuli with very short exposure. Other theories hypothesise that statistical and grammatical proces</context>
</contexts>
<marker>Anderson, Morgan, White, 2003</marker>
<rawString>J. L. Anderson, J. L. Morgan and K. S. White. 2003. A Statistical Basis for Speech Sound Discrimination. Language and Speech, 46(43):155-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Saffran</author>
<author>R N Aslin</author>
<author>E L Newport</author>
</authors>
<title>Statistical Learning by 8-Month-Old Infants.</title>
<date>1996</date>
<pages>274--1926</pages>
<publisher>SCIENCE,</publisher>
<contexts>
<context position="5111" citStr="Saffran et al., 1996" startWordPosition="771" endWordPosition="774">r acquiring language. It is their view that an infant can acquire linguistic structure with little input and that it plays a minor role in the speed and sequence with which they learn language. Noam Chomsky is one of the most cited language acquisition nativists, claiming children can acquire language “On relatively slight exposure and without specific training” (Chomsky, 1975, p.4). On the other hand, non-nativists argue that the input contains much more structural information and is not as full of errors as suggested by nativists (Eimas et al., 1971; Best et al., 1988; Jusczyk et al., 1993; Saffran et al., 1996; Christiansen et al., 1998; Saffran et al., 1999; Saffran et al., 2000; Kirkham et al., 2002; Anderson et al., 2003; Seidenberg et al., 2002; Kuhl, 2004; Hannon and Trehub, 2005). Experiments by Saffran et al. (1996, 1999) show that 8-month old infants use the statistical information in speech as an aid for word segmentation with only two minutes of familiarisation. Inspired by these results, Kirkham et al. (2002) suggest that the same statistical processes are also present in the visual domain. Kirkham et al. (2002) carried out experiments showing that preverbal infants are able to learn pat</context>
</contexts>
<marker>Saffran, Aslin, Newport, 1996</marker>
<rawString>J. R. Saffran, R. N. Aslin and E. L. Newport. 1996. Statistical Learning by 8-Month-Old Infants. SCIENCE, 274:1926-1928.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Saffran</author>
<author>E K Johnson</author>
<author>R N Aslin</author>
<author>E L Newport</author>
</authors>
<title>Statistical Learning of Tone Sequences by Human Infants and Adults.</title>
<date>1999</date>
<journal>Cognition,</journal>
<pages>70--1</pages>
<contexts>
<context position="5160" citStr="Saffran et al., 1999" startWordPosition="779" endWordPosition="782">fant can acquire linguistic structure with little input and that it plays a minor role in the speed and sequence with which they learn language. Noam Chomsky is one of the most cited language acquisition nativists, claiming children can acquire language “On relatively slight exposure and without specific training” (Chomsky, 1975, p.4). On the other hand, non-nativists argue that the input contains much more structural information and is not as full of errors as suggested by nativists (Eimas et al., 1971; Best et al., 1988; Jusczyk et al., 1993; Saffran et al., 1996; Christiansen et al., 1998; Saffran et al., 1999; Saffran et al., 2000; Kirkham et al., 2002; Anderson et al., 2003; Seidenberg et al., 2002; Kuhl, 2004; Hannon and Trehub, 2005). Experiments by Saffran et al. (1996, 1999) show that 8-month old infants use the statistical information in speech as an aid for word segmentation with only two minutes of familiarisation. Inspired by these results, Kirkham et al. (2002) suggest that the same statistical processes are also present in the visual domain. Kirkham et al. (2002) carried out experiments showing that preverbal infants are able to learn patterns of visual stimuli with very short exposure.</context>
</contexts>
<marker>Saffran, Johnson, Aslin, Newport, 1999</marker>
<rawString>J. R. Saffran, E. K. Johnson, R. N. Aslin and E. L. Newport. 1999. Statistical Learning of Tone Sequences by Human Infants and Adults. Cognition, 70(1):27-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Saffran</author>
<author>A Senghashas</author>
<author>J C Trueswell</author>
</authors>
<title>The Acquisition of Language by Children.</title>
<date>2000</date>
<pages>98--23</pages>
<publisher>PNAS,</publisher>
<contexts>
<context position="5182" citStr="Saffran et al., 2000" startWordPosition="783" endWordPosition="786">istic structure with little input and that it plays a minor role in the speed and sequence with which they learn language. Noam Chomsky is one of the most cited language acquisition nativists, claiming children can acquire language “On relatively slight exposure and without specific training” (Chomsky, 1975, p.4). On the other hand, non-nativists argue that the input contains much more structural information and is not as full of errors as suggested by nativists (Eimas et al., 1971; Best et al., 1988; Jusczyk et al., 1993; Saffran et al., 1996; Christiansen et al., 1998; Saffran et al., 1999; Saffran et al., 2000; Kirkham et al., 2002; Anderson et al., 2003; Seidenberg et al., 2002; Kuhl, 2004; Hannon and Trehub, 2005). Experiments by Saffran et al. (1996, 1999) show that 8-month old infants use the statistical information in speech as an aid for word segmentation with only two minutes of familiarisation. Inspired by these results, Kirkham et al. (2002) suggest that the same statistical processes are also present in the visual domain. Kirkham et al. (2002) carried out experiments showing that preverbal infants are able to learn patterns of visual stimuli with very short exposure. Other theories hypoth</context>
</contexts>
<marker>Saffran, Senghashas, Trueswell, 2000</marker>
<rawString>J. R. Saffran, A. Senghashas and J. C. Trueswell. 2000. The Acquisition of Language by Children. PNAS, 98(23):12874-12875.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L ten Bosch</author>
<author>B Cranen</author>
</authors>
<title>A Computational Model for Unsupervised Word Discovery. INTERSPEECH</title>
<date>2007</date>
<pages>1481--1484</pages>
<contexts>
<context position="7337" citStr="Bosch and Cranen, 2007" startWordPosition="1128" endWordPosition="1131">t. Brent (1999) states that for a computational algorithm to be cognitively plausible it must: • Start with no prior knowledge of general language structure. • Learn in a completely unsupervised manner. • Segment incrementally. An automatic segmentation method similar to that of the acoustic DP-ngram method is segmental DTW. Park &amp; Glass (2008) have adapted dynamic time warping (DTW) to find matching acoustic patterns between two utterances. The discovered units are then clustered, using an adjacency graph method, to describe the topic of the speech data. Statistical Word Discovery (SWD) (ten Bosch and Cranen, 2007) and the Cross-channel Early Lexical Learning (CELL) model (Roy and Pentland, 2002), also similar methods to the one described in this paper, discover word-like units and then updating internal representations through clustering processes. The downfall of the CELL approach is that it assumes speech is observed as an array of phone probabilities. A more radical approach is Non-negative matrix factorization (NMF) (Stouten et al., 2008). NMF detects words from ‘raw’ cross-modal input without any kind of segmentation during the whole process, coding recurrent speech fragments into to ‘word-like’ e</context>
</contexts>
<marker>Bosch, Cranen, 2007</marker>
<rawString>L. ten Bosch and B. Cranen. 2007. A Computational Model for Unsupervised Word Discovery. INTERSPEECH 2007, 1481-1484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H Christiansen</author>
<author>J Allen</author>
<author>M Seidenberg</author>
</authors>
<title>Learning to Segment Speech using Multiple Cues. Language and Cognitive Processes,</title>
<date>1998</date>
<pages>13--221</pages>
<contexts>
<context position="5138" citStr="Christiansen et al., 1998" startWordPosition="775" endWordPosition="778">It is their view that an infant can acquire linguistic structure with little input and that it plays a minor role in the speed and sequence with which they learn language. Noam Chomsky is one of the most cited language acquisition nativists, claiming children can acquire language “On relatively slight exposure and without specific training” (Chomsky, 1975, p.4). On the other hand, non-nativists argue that the input contains much more structural information and is not as full of errors as suggested by nativists (Eimas et al., 1971; Best et al., 1988; Jusczyk et al., 1993; Saffran et al., 1996; Christiansen et al., 1998; Saffran et al., 1999; Saffran et al., 2000; Kirkham et al., 2002; Anderson et al., 2003; Seidenberg et al., 2002; Kuhl, 2004; Hannon and Trehub, 2005). Experiments by Saffran et al. (1996, 1999) show that 8-month old infants use the statistical information in speech as an aid for word segmentation with only two minutes of familiarisation. Inspired by these results, Kirkham et al. (2002) suggest that the same statistical processes are also present in the visual domain. Kirkham et al. (2002) carried out experiments showing that preverbal infants are able to learn patterns of visual stimuli wit</context>
</contexts>
<marker>Christiansen, Allen, Seidenberg, 1998</marker>
<rawString>M. H. Christiansen, J. Allen and M. Seidenberg. 1998. Learning to Segment Speech using Multiple Cues. Language and Cognitive Processes, 13:221-268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Seidenberg</author>
<author>M C MacDonald</author>
<author>J R Saffran</author>
</authors>
<title>Does Grammar Start Where Statistics Stop?.</title>
<date>2002</date>
<pages>298--552</pages>
<publisher>SCIENCE,</publisher>
<contexts>
<context position="5252" citStr="Seidenberg et al., 2002" startWordPosition="795" endWordPosition="798"> the speed and sequence with which they learn language. Noam Chomsky is one of the most cited language acquisition nativists, claiming children can acquire language “On relatively slight exposure and without specific training” (Chomsky, 1975, p.4). On the other hand, non-nativists argue that the input contains much more structural information and is not as full of errors as suggested by nativists (Eimas et al., 1971; Best et al., 1988; Jusczyk et al., 1993; Saffran et al., 1996; Christiansen et al., 1998; Saffran et al., 1999; Saffran et al., 2000; Kirkham et al., 2002; Anderson et al., 2003; Seidenberg et al., 2002; Kuhl, 2004; Hannon and Trehub, 2005). Experiments by Saffran et al. (1996, 1999) show that 8-month old infants use the statistical information in speech as an aid for word segmentation with only two minutes of familiarisation. Inspired by these results, Kirkham et al. (2002) suggest that the same statistical processes are also present in the visual domain. Kirkham et al. (2002) carried out experiments showing that preverbal infants are able to learn patterns of visual stimuli with very short exposure. Other theories hypothesise that statistical and grammatical processes are both used when le</context>
</contexts>
<marker>Seidenberg, MacDonald, Saffran, 2002</marker>
<rawString>M. S. Seidenberg, M. C. MacDonald and J. R. Saffran. 2002. Does Grammar Start Where Statistics Stop?. SCIENCE, 298:552-554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Z Kirkham</author>
<author>A J Slemmer</author>
<author>S P Johnson</author>
</authors>
<title>Visual Statistical Learning in Infancy: Evidence for a Domain General Learning Mechanism.</title>
<date>2002</date>
<tech>Cognition, 83:B35-B42.</tech>
<contexts>
<context position="5204" citStr="Kirkham et al., 2002" startWordPosition="787" endWordPosition="790">ittle input and that it plays a minor role in the speed and sequence with which they learn language. Noam Chomsky is one of the most cited language acquisition nativists, claiming children can acquire language “On relatively slight exposure and without specific training” (Chomsky, 1975, p.4). On the other hand, non-nativists argue that the input contains much more structural information and is not as full of errors as suggested by nativists (Eimas et al., 1971; Best et al., 1988; Jusczyk et al., 1993; Saffran et al., 1996; Christiansen et al., 1998; Saffran et al., 1999; Saffran et al., 2000; Kirkham et al., 2002; Anderson et al., 2003; Seidenberg et al., 2002; Kuhl, 2004; Hannon and Trehub, 2005). Experiments by Saffran et al. (1996, 1999) show that 8-month old infants use the statistical information in speech as an aid for word segmentation with only two minutes of familiarisation. Inspired by these results, Kirkham et al. (2002) suggest that the same statistical processes are also present in the visual domain. Kirkham et al. (2002) carried out experiments showing that preverbal infants are able to learn patterns of visual stimuli with very short exposure. Other theories hypothesise that statistical</context>
</contexts>
<marker>Kirkham, Slemmer, Johnson, 2002</marker>
<rawString>N. Z. Kirkham, A. J. Slemmer and S. P. Johnson. 2002. Visual Statistical Learning in Infancy: Evidence for a Domain General Learning Mechanism. Cognition, 83:B35-B42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Eimas</author>
<author>E R Siqueland</author>
<author>P Jusczyk</author>
<author>J Vigorito</author>
</authors>
<title>Speech Perception in Infants.</title>
<date>1971</date>
<journal>Science,</journal>
<pages>171--3968</pages>
<contexts>
<context position="5048" citStr="Eimas et al., 1971" startWordPosition="758" endWordPosition="761">? Nativists believe that infants have an innate capability for acquiring language. It is their view that an infant can acquire linguistic structure with little input and that it plays a minor role in the speed and sequence with which they learn language. Noam Chomsky is one of the most cited language acquisition nativists, claiming children can acquire language “On relatively slight exposure and without specific training” (Chomsky, 1975, p.4). On the other hand, non-nativists argue that the input contains much more structural information and is not as full of errors as suggested by nativists (Eimas et al., 1971; Best et al., 1988; Jusczyk et al., 1993; Saffran et al., 1996; Christiansen et al., 1998; Saffran et al., 1999; Saffran et al., 2000; Kirkham et al., 2002; Anderson et al., 2003; Seidenberg et al., 2002; Kuhl, 2004; Hannon and Trehub, 2005). Experiments by Saffran et al. (1996, 1999) show that 8-month old infants use the statistical information in speech as an aid for word segmentation with only two minutes of familiarisation. Inspired by these results, Kirkham et al. (2002) suggest that the same statistical processes are also present in the visual domain. Kirkham et al. (2002) carried out e</context>
</contexts>
<marker>Eimas, Siqueland, Jusczyk, Vigorito, 1971</marker>
<rawString>P. D. Eimas, E. R. Siqueland, P. Jusczyk and J. Vigorito. 1971. Speech Perception in Infants. Science, 171(3968):303-606.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P K Kuhl</author>
</authors>
<title>Early Language Acquisition: Cracking the Speech Code.</title>
<date>2004</date>
<journal>Nature,</journal>
<pages>5--831</pages>
<contexts>
<context position="5264" citStr="Kuhl, 2004" startWordPosition="799" endWordPosition="800">ith which they learn language. Noam Chomsky is one of the most cited language acquisition nativists, claiming children can acquire language “On relatively slight exposure and without specific training” (Chomsky, 1975, p.4). On the other hand, non-nativists argue that the input contains much more structural information and is not as full of errors as suggested by nativists (Eimas et al., 1971; Best et al., 1988; Jusczyk et al., 1993; Saffran et al., 1996; Christiansen et al., 1998; Saffran et al., 1999; Saffran et al., 2000; Kirkham et al., 2002; Anderson et al., 2003; Seidenberg et al., 2002; Kuhl, 2004; Hannon and Trehub, 2005). Experiments by Saffran et al. (1996, 1999) show that 8-month old infants use the statistical information in speech as an aid for word segmentation with only two minutes of familiarisation. Inspired by these results, Kirkham et al. (2002) suggest that the same statistical processes are also present in the visual domain. Kirkham et al. (2002) carried out experiments showing that preverbal infants are able to learn patterns of visual stimuli with very short exposure. Other theories hypothesise that statistical and grammatical processes are both used when learning langu</context>
</contexts>
<marker>Kuhl, 2004</marker>
<rawString>P. K. Kuhl. 2004. Early Language Acquisition: Cracking the Speech Code. Nature, 5:831-843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nowell</author>
<author>R K Moore</author>
</authors>
<title>The Application of Dynamic Programming Techniques to Non-Word Based Topic Spotting.</title>
<date>1995</date>
<journal>EuroSpeech</journal>
<volume>95</volume>
<pages>1355--1358</pages>
<contexts>
<context position="9883" citStr="Nowell and Moore, 1995" startWordPosition="1506" endWordPosition="1509">e with the environment. 3.2 The Computational Model There are two key processes to the language acquisition model described in this paper; automatic segmentation and word discovery. The automatic segmentation stage allows the system to build a library of similar repeating speech fragments directly from the acoustic signal. The second stage associates these fragments with the observed semantic tags to create distinct key word classes. Automatic Segmentation The acoustic DP-ngram algorithm reported in this section is a modification of the preceding DP-ngram algorithm (Sankoff and Kruskal, 1983; Nowell and Moore, 1995). The original DP-ngram model was developed by Sankoff and Kruskal (1983) to find two similar portions of gene sequences. Nowell and Moore (1995) then modified this model to find repeated patterns within a single phone transcription sequence through self-similarity. Expanding on these methods, the author has developed a variant that is able to segment speech, directly from the acoustic signal; automatically segmenting important lexical fragments by discovering ‘similar’ repeating patterns. Speech is never the same twice and therefore impossible to find exact repetitions of importance (e.g. pho</context>
</contexts>
<marker>Nowell, Moore, 1995</marker>
<rawString>P. Nowell and R. K. Moore. 1995. The Application of Dynamic Programming Techniques to Non-Word Based Topic Spotting. EuroSpeech ’95, 1355-1358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Jusczyk</author>
<author>A D Friederici</author>
<author>J Wessels</author>
<author>V Y Svenkerud</author>
<author>A M Jusczyk</author>
</authors>
<title>Infants’ Sensitivity to the Sound Patterns of Native Language Words.</title>
<date>1993</date>
<journal>Journal of Memory &amp; Language,</journal>
<pages>32--402</pages>
<contexts>
<context position="5089" citStr="Jusczyk et al., 1993" startWordPosition="766" endWordPosition="770">n innate capability for acquiring language. It is their view that an infant can acquire linguistic structure with little input and that it plays a minor role in the speed and sequence with which they learn language. Noam Chomsky is one of the most cited language acquisition nativists, claiming children can acquire language “On relatively slight exposure and without specific training” (Chomsky, 1975, p.4). On the other hand, non-nativists argue that the input contains much more structural information and is not as full of errors as suggested by nativists (Eimas et al., 1971; Best et al., 1988; Jusczyk et al., 1993; Saffran et al., 1996; Christiansen et al., 1998; Saffran et al., 1999; Saffran et al., 2000; Kirkham et al., 2002; Anderson et al., 2003; Seidenberg et al., 2002; Kuhl, 2004; Hannon and Trehub, 2005). Experiments by Saffran et al. (1996, 1999) show that 8-month old infants use the statistical information in speech as an aid for word segmentation with only two minutes of familiarisation. Inspired by these results, Kirkham et al. (2002) suggest that the same statistical processes are also present in the visual domain. Kirkham et al. (2002) carried out experiments showing that preverbal infants</context>
</contexts>
<marker>Jusczyk, Friederici, Wessels, Svenkerud, Jusczyk, 1993</marker>
<rawString>P. W. Jusczyk, A. D. Friederici, J. Wessels, V. Y. Svenkerud and A. M. Jusczyk. 1993. Infants’ Sensitivity to the Sound Patterns of Native Language Words. Journal of Memory &amp; Language, 32:402-420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stouten</author>
<author>K Demuynck</author>
<author>H Van hamme</author>
</authors>
<title>Discovering Phone Patterns in Spoken Utterances by Non-negative Matrix Factorisation.</title>
<date>2008</date>
<journal>IEEE Signal Processing Letters,</journal>
<pages>131--134</pages>
<marker>Stouten, Demuynck, Van hamme, 2008</marker>
<rawString>V. Stouten, K. Demuynck and H. Van hamme. 2008. Discovering Phone Patterns in Spoken Utterances by Non-negative Matrix Factorisation. IEEE Signal Processing Letters, 131-134.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>