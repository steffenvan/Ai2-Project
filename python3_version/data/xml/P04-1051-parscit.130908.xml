<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000146">
<title confidence="0.974097">
Computing Locally Coherent Discourses
</title>
<author confidence="0.953961">
Alexander Koller
</author>
<affiliation confidence="0.9761985">
Dept. of Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.548592">
Saarbr¨ucken, Germany
</address>
<email confidence="0.96198">
koller@coli.uni-sb.de
</email>
<note confidence="0.6324075">
Ernst Althaus
LORIA
</note>
<address confidence="0.4711225">
Universit´e Henri Poincar´e
Vandœuvre-l`es-Nancy, France
</address>
<email confidence="0.986363">
althaus@loria.fr
</email>
<author confidence="0.983254">
Nikiforos Karamanis
</author>
<affiliation confidence="0.857891666666667">
School of Informatics
University of Edinburgh
Edinburgh, UK
</affiliation>
<email confidence="0.996175">
N.Karamanis@sms.ed.ac.uk
</email>
<sectionHeader confidence="0.993841" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909833333333">
We present the first algorithm that computes opti-
mal orderings of sentences into a locally coherent
discourse. The algorithm runs very efficiently on a
variety of coherence measures from the literature.
We also show that the discourse ordering problem
is NP-complete and cannot be approximated.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959533333334">
One central problem in discourse generation and
summarisation is to structure the discourse in a
way that maximises coherence. Coherence is the
property of a good human-authored text that makes
it easier to read and understand than a randomly-
ordered collection of sentences.
Several papers in the recent literature (Mellish et
al., 1998; Barzilay et al., 2002; Karamanis and Ma-
nurung, 2002; Lapata, 2003; Karamanis et al., 2004)
have focused on defining local coherence, which
evaluates the quality of sentence-to-sentence transi-
tions. This is in contrast to theories of global coher-
ence, which can consider relations between larger
chunks of the discourse and e.g. structures them into
a tree (Mann and Thompson, 1988; Marcu, 1997;
Webber et al., 1999). Measures of local coherence
specify which ordering of the sentences makes for
the most coherent discourse, and can be based e.g.
on Centering Theory (Walker et al., 1998; Brennan
et al., 1987; Kibble and Power, 2000; Karamanis
and Manurung, 2002) or on statistical models (Lap-
ata, 2003).
But while formal models of local coherence have
made substantial progress over the past few years,
the question of how to efficiently compute an order-
ing of the sentences in a discourse that maximises
local coherence is still largely unsolved. The fun-
damental problem is that any of the factorial num-
ber of permutations of the sentences could be the
optimal discourse, which makes for a formidable
search space for nontrivial discourses. Mellish et
al. (1998) and Karamanis and Manurung (2002)
present algorithms based on genetic programming,
and Lapata (2003) uses a graph-based heuristic al-
gorithm, but none of them can give any guarantees
about the quality of the computed ordering.
This paper presents the first algorithm that com-
putes optimal locally coherent discourses, and es-
tablishes the complexity of the discourse ordering
problem. We first prove that the discourse order-
ing problem for local coherence measures is equiva-
lent to the Travelling Salesman Problem (TSP). This
means that discourse ordering is NP-complete, i.e.
there are probably no polynomial algorithms for it.
Worse, our result implies that the problem is not
even approximable; any polynomial algorithm will
compute arbitrarily bad solutions on unfortunate in-
puts. Note that all approximation algorithms for the
TSP assume that the underlying cost function is a
metric, which is not the case for the coherence mea-
sures we consider.
Despite this negative result, we show that by ap-
plying modern algorithms for TSP, the discourse or-
dering problem can be solved efficiently enough for
practical applications. We define a branch-and-cut
algorithm based on linear programming, and evalu-
ate it on discourse ordering problems based on the
GNOME corpus (Karamanis, 2003) and the BLLIP
corpus (Lapata, 2003). If the local coherence mea-
sure depends only on the adjacent pairs of sentences
in the discourse, we can order discourses of up to 50
sentences in under a second. If it is allowed to de-
pend on the left-hand context of the sentence pair,
computation is often still efficient, but can become
expensive.
The structure of the paper is as follows. We will
first formally define the discourse ordering problem
and relate our definition to the literature on local co-
herence measures in Section 2. Then we will prove
the equivalence of discourse ordering and TSP (Sec-
tion 3), and present algorithms for solving it in Sec-
tion 4. Section 5 evaluates our algorithms on exam-
ples from the literature. We compare our approach
to various others in Section 6, and then conclude in
Section 7.
</bodyText>
<sectionHeader confidence="0.927635" genericHeader="introduction">
2 The Discourse Ordering Problem
</sectionHeader>
<bodyText confidence="0.9999865">
We will first give a formal definition of the prob-
lem of computing locally coherent discourses, and
demonstrate how some local coherence measures
from the literature fit into this framework.
</bodyText>
<subsectionHeader confidence="0.969611">
2.1 Definitions
</subsectionHeader>
<bodyText confidence="0.888573884615385">
We assume that a discourse is made up of discourse
units (depending on the underlying theory, these
could be utterances, sentences, clauses, etc.), which
must be ordered to achieve maximum local coher-
ence. We call the problem of computing the optimal
ordering the discourse ordering problem.
We formalise the problem by assigning a cost to
each unit-to-unit transition, and a cost for the dis-
course to start with a certain unit. Transition costs
may depend on the local context, i.e. a fixed num-
ber of discourse units to the left may influence the
cost of a transition. The optimal ordering is the one
which minimises the sum of the costs.
Definition 1. A d-place transition cost function for
a set U of discourse units is a function cT : Ud →
R. Intuitively, cT(un|u1, ... , ud−1) is the cost of
the transition (ud−1, ud) given that the immediately
preceding units were u1, ... , ud−2.
A d-place initial cost function for U is a function
cI : Ud → R. Intuitively, cI(u1, ... , ud) is the
cost for the fact that the discourse starts with the
sequence u1, ... , ud.
The d-place discourse ordering problem is de-
fined as follows: Given a set U = {u1,... , un},
a d-place transition cost function cT and a (d − 1)-
place initial cost function cI, compute a permutation
</bodyText>
<equation confidence="0.86159625">
π of {1, ... , n} such that
cI(uw(1),... , uw(d−1))
cT (uw(i+d−1)|uw(i), ... , uw(i+d−2))
is minimal.
</equation>
<bodyText confidence="0.999879">
The notation for the cost functions is suggestive:
The transition cost function has the character of a
conditional probability, which specifies that the cost
of continuing the discourse with the unit ud depends
on the local context u1, ... , ud−1. This local con-
text is not available for the first d − 1 units of the
discourse, which is why their costs are summarily
covered by the initial function.
</bodyText>
<subsectionHeader confidence="0.992868">
2.2 Centering-Based Cost Functions
</subsectionHeader>
<bodyText confidence="0.999957589285715">
One popular class of coherence measures is based
on Centering Theory (CT, (Walker et al., 1998)). We
will briefly sketch its basic notions and then show
how some CT-based coherence measures can be cast
into our framework.
The standard formulation of CT e.g. in (Walker et
al., 1998), calls the discourse units utterances, and
assigns to each utterance ui in the discourse a list
Cf(ui) of forward-looking centres. The members
of Cf(ui) correspond to the referents of the NPs
in ui and are ranked in order of prominence, the
first element being the preferred centre Cp(ui). The
backward-looking centre Cb(ui) of ui is defined as
the highest ranked element of Cf(ui) which also ap-
pears in Cf(ui−1), and serves as the link between
the two subsequent utterances ui−1 and ui. Each
utterance has at most one Cb. If ui and ui−1 have
no forward-looking centres in common, or if ui is
the first utterance in the discourse, then ui does not
have a Cb at all.
Based on these concepts, CT classifies the tran-
sitions between subsequent utterances into differ-
ent types. Table 1 shows the most common clas-
sification into the four types CONTINUE, RETAIN,
SMOOTH-SHIFT, and ROUGH-SHIFT, which are pre-
dicted to be less and less coherent in this order
(Brennan et al., 1987). Kibble and Power (2000)
define three further classes of transitions: COHER-
ENCE and SALIENCE, which are both defined in Ta-
ble 1 as well, and NOCB, the class of transitions
for which Cb(ui) is undefined. Finally, a transition
is considered to satisfy the CHEAPNESS constraint
(Strube and Hahn, 1999) if Cb(ui) = Cp(ui−1).
Table 2 summarises some cost functions from the
literature, in the reconstruction of Karamanis et al.
(2004). Each line shows the name of the coherence
measure, the arity d from Definition 1, and the ini-
tial and transition cost functions. To fit the defini-
tions in one line, we use terms of the form fk, which
abbreviate applications of f to the last k arguments
of the cost functions, i.e. f(ud−k+1, . . . , ud).
The most basic coherence measure, M.NOCB
(Karamanis and Manurung, 2002), simply assigns
to each NOCB transition the cost 1 and to every other
transition the cost 0. The definition of cT(u2|u1),
which decodes to nocb(u1, u2), only looks at the
two units in the transition, and no further context.
The initial costs for this coherence measure are al-
ways zero.
The measure M.KP (Kibble and Power, 2000)
sums the value of nocb and the values of three func-
tions which evaluate to 0 if the transition is cheap,
salient, or coherent, and 1 otherwise. This is an in-
stance of the 3-place discourse ordering problem be-
cause COHERENCE depends on Cb(ui−1), which it-
self depends on Cf(ui−2); hence nocoh must take
</bodyText>
<equation confidence="0.9022115">
n−d+1
�
i=1
+
</equation>
<table confidence="0.90312">
COHERENCE: COHERENCE∗:
Cb(ui) = Cb(ui−1) Cb(ui) =6 Cb(ui−1)
SALIENCE: Cb(ui) = Cp(ui) CONTINUE SMOOTH-SHIFT
SALIENCE∗: Cb(ui) =6 Cp(ui) RETAIN ROUGH-SHIFT
</table>
<tableCaption confidence="0.991202">
Table 1: COHERENCE, SALIENCE and the table of standard transitions
</tableCaption>
<table confidence="0.9910788">
d initial cost cI(u1, . . . , ud−1) transition cost cT(ud|u1, . . . , ud−1)
M.NOCB 2 0 nocb2
M.KP 3 nocb2 + nocheap2 + nosal2 nocb2 + nocheap2 + nosal2 + nocoh3
M.BFP 3 (1 − nosal2, nosal2, 0, 0) (cont3, ret3, ss3, rs3)
M.LAPATA 2 −log P(u1) − log P(u2|u1)
</table>
<tableCaption confidence="0.99976">
Table 2: Some cost functions from the literature.
</tableCaption>
<bodyText confidence="0.987348285714286">
three arguments.
Finally, the measure M.BFP (Brennan et al.,
1987) uses a lexicographic ordering on 4-tuples
which indicate whether the transition is a CON-
TINUE, RETAIN, SMOOTH-SHIFT, or ROUGH-
SHIFT. cT and all four functions it is computed from
take three arguments because the classification de-
pends on COHERENCE. As the first transition in the
discourse is coherent by default (it has no Cb), we
can compute cI by distinguishing RETAIN and CON-
TINUE via SALIENCE. The tuple-valued cost func-
tions can be converted to real-valued functions by
choosing a sufficiently large number M and using
the value M3 · cont + M2 · ret + M · ss + rs.
</bodyText>
<subsectionHeader confidence="0.995093">
2.3 Probability-Based Cost Functions
</subsectionHeader>
<bodyText confidence="0.999898578947369">
A fundamentally different approach to measure dis-
course coherence was proposed by Lapata (2003).
It uses a statistical bigram model that assigns each
pair ui, uk of utterances a probability P(uk|ui) of
appearing in subsequent positions, and each utter-
ance a probability P(ui) of appearing in the initial
position of the discourse. The probabilities are es-
timated on the grounds of syntactic features of the
discourse units. The probability of the entire dis-
course u1 ... un is the product P(u1) · P(u2|u1) ·
... · P(un|un−1).
We can transform Lapata’s model straightfor-
wardly into our cost function framework, as shown
under M.LAPATA in Table 2. The discourse that
minimizes the sum of the negative logarithms will
also maximise the product of the probabilities. We
have d = 2 because it is a bigram model in which
the transition probability does not depend on the
previous discourse units.
</bodyText>
<sectionHeader confidence="0.9755105" genericHeader="method">
3 Equivalence of Discourse Ordering and
TSP
</sectionHeader>
<bodyText confidence="0.9965074">
Now we show that discourse ordering and the travel-
ling salesman problem are equivalent. In order to do
this, we first redefine discourse ordering as a graph
problem.
d-place discourse ordering problem (dPDOP):
Given a directed graph G = (V, E), a node
s ∈ V and a function c : V d → IR,, compute a
simple directed path P = (s = v0, v1, ... , vn)
from s through all vertices in V which min-
imises En−d+1 i=0 c(vi, vi+1, . . . , vi+d−1). We
write instances of dPDOP as (V, E, s, c).
The nodes v1, ... , vn correspond to the discourse
units. The cost function c encodes both the initial
and the transition cost functions from Section 2 by
returning the initial cost if its first argument is the
(new) start node s.
Now let’s define the version of the travelling
salesman problem we will use below.
Generalised asymmetric TSP (GATSP): Given a
directed graph G = (V, E), edge weights c :
E → IR,, and a partition (V1, ... , Vk) of the
nodes V , compute the shortest directed cycle
that visits exactly one node of each Vi. We
call such a cycle a tour and write instances of
GATSP as ((V1, ... , Vk), E, c).
The usual definition of the TSP, in which every
node must be visited exactly once, is the special
case of GATSP where each Vi contains exactly one
node. We call this case asymmetric travelling sales-
man problem, ATSP.
</bodyText>
<sectionHeader confidence="0.532139" genericHeader="method">
ATSP 2PDOP
</sectionHeader>
<figureCaption confidence="0.999918">
Figure 1: Reduction of ATSP to 2PDOP
</figureCaption>
<bodyText confidence="0.998608333333333">
We will show that ATSP can be reduced to
2PDOP, and that any dPDOP can be reduced to
GATSP.
</bodyText>
<subsectionHeader confidence="0.999325">
3.1 Reduction of ATSP to 2PDOP
</subsectionHeader>
<bodyText confidence="0.999217264705882">
First, we introduce the reduction of ATSP to
2PDOP, which establishes NP-completeness of
dPDOP for all d &gt; 1. The reduction is approxi-
mation preserving, i.e. if we can find a solution of
2PDOP that is worse than the optimum only by a
factor of E (an E-approximation), it translates to a
solution of ATSP that is also an E-approximation.
Since it is known that there can be no polynomial al-
gorithms that compute E-approximations for general
ATSP, for any E (Cormen et al., 1990), this means
that dPDOP cannot be approximated either (unless
P=NP): Any polynomial algorithm for dPDOP will
compute arbitrarily bad solutions on certain inputs.
The reduction works as follows. Let G =
((V1, ... , Vk), E, c) be an instance of ATSP, and
V = V1 U ... U Vk. We choose an arbitrary node
v E V and split it into two nodes vs and vt. We as-
sign all edges with source node v to vs and all edges
with target node v to vt (compare Figure 1). Finally
we make vs the source node of our 2PDOP instance
G0.
For every tour in G, we have a path in G0 starting
at vs visiting all other nodes (and ending in vt) with
the same cost by replacing the edge (v, u) out of
v by (vs, u) and the edge (w, v) into v by (w, vt).
Conversely, for every path starting at vs visiting all
nodes, we have an ATSP tour of the same cost, since
all such paths will end in vt (as vt has no outgoing
edges).
An example is shown in Fig. 1. The ATSP in-
stance on the left has the tour (1, 3, 2, 1), indicated
by the solid edges. The node 1 is split into the two
nodes 1s and 1t, and the tour translates to the path
(1s, 3, 2, 1t) in the 2PDOP instance.
</bodyText>
<subsectionHeader confidence="0.999338">
3.2 Reduction of dPDOP to GATSP
</subsectionHeader>
<bodyText confidence="0.967654">
Conversely, we can encode an instance G =
(V, E, s, c) of dPDOP as an instance G0 =
</bodyText>
<footnote confidence="0.455772">
3PDOP GATSP
</footnote>
<figureCaption confidence="0.996636">
Figure 2: Reduction of dPDOP to GATSP. Edges to
the source node [s, s] are not drawn.
</figureCaption>
<bodyText confidence="0.998083113636364">
((V0u)u∈V, E0, c0) of GATSP, in such a way that the
optimal solutions correspond. The cost of traversing
an edge in dPDOP depends on the previous d − 1
nodes; we compress these costs into ordinary costs
of single edges in the reduction to GATSP.
The GATSP instance has a node [u1, ... , ud−1]
for every d − 1-tuple of nodes of V . It has an edge
from [u1, ... , ud−1] to [u2, ... , ud−1, ud] iff there
is an edge from ud−1 to ud in G, and it has an edge
from each node into [s, ... , s]. The idea is to en-
code a path P = (s = u0, u1, ... , un) in Gas
a tour TP in G0 that successively visits the nodes
[uz−d+1, ... uz], i = 0,... n, where we assume that
uj = s for all j G 0 (compare Figure 2).
The cost of TP can be made equal to the cost of P
by making the cost of the edge from [u1, ... , ud−1]
to [u2, ... , ud] equal to c(u1, ... ud). (We set c0(e)
to 0 for all edges e between nodes with first compo-
nent s and for the edges e with target node [sd−1].)
Finally, we define Vu0 to be the set of all nodes in G0
with last component u. It is not hard to see that for
any simple path of length n in G, we find a tour TP
in G0 with the same cost. Conversely, we can find
for every tour in G0 a simple path of length n in G
with the same cost.
Note that the encoding G0 will contain many un-
necessary nodes and edges. For instance, all nodes
that have no incoming edges can never be used in a
tour, and can be deleted. We can safely delete such
unnecessary nodes in a post-processing step.
An example is shown in Fig. 2. The 3PDOP
instance on the left has a path (s, 3, 1, 2), which
translates to the path ([s, s], [s, 3], [3, 1], [1, 2]) in
the GATSP instance shown on the right. This path
can be completed by a tour by adding the edge
([1, 2], [s, s]), of cost 0. The tour indeed visits each
Vu0 (i.e., each column) exactly once. Nodes with last
component s which are not [s, s] are unreachable
and are not shown.
For the special case of d = 2, the GATSP is sim-
ply an ordinary ATSP. The graphs of both problems
look identical in this case, except that the GATSP
instance has edges of cost 0 from any node to the
source [s].
</bodyText>
<sectionHeader confidence="0.976374" genericHeader="method">
4 Computing Optimal Orderings
</sectionHeader>
<bodyText confidence="0.999984883116883">
The equivalence of dPDOP and GATSP implies that
we can now bring algorithms from the vast litera-
ture on TSP to bear on the discourse ordering prob-
lem. One straightforward method is to reduce the
GATSP further to ATSP (Noon and Bean, 1993);
for the case d = 2, nothing has to be done. Then
one can solve the reduced ATSP instance; see (Fis-
chetti et al., 2001; Fischetti et al., 2002) for a recent
survey of exact methods.
We choose the alternative of developing a new
algorithm for solving GATSP directly, which uses
standard techniques from combinatorial optimisa-
tion, gives us a better handle on optimising the al-
gorithm for our problem instances, and runs more
efficiently in practice. Our algorithm translates
the GATSP instance into an integer linear pro-
gram (ILP) and uses the branch-and-cut method
(Nemhauser and Wolsey, 1988) to solve it. Integer
linear programs consist of a set of linear equations
and inequalities, and are solved by integer variable
assignments which maximise or minimise a goal
function while satisfying the other conditions.
Let G = (V, E) be a directed graph and S C_ V .
We define δ+(S) = {(u, v) E E  |u E S and v E�
S} and δ−(S) = {(u, v) E E  |u E/ S and v E S},
i.e. δ+(S) and δ−(S) are the sets of all incoming
and outgoing edges of S, respectively. We assume
that the graph G has no edges within one partition
Vu, since such edges cannot be used by any solution.
With this assumption, GATSP can be phrased as an
ILP as follows (this formulation is similar to the one
proposed by Laporte et al. (1987)):
We have a binary variable xe for each edge e of
the graph. The intention is that xe has value 1 if
e is used in the tour, and 0 otherwise. Thus the
cost of the tour can be written as PeEE cexe. The
three conditions enforce the variable assignment to
encode a valid GATSP tour. (1) ensures that all inte-
ger solutions encode a set of cycles. (2) guarantees
that every partition Vi is visited by exactly one cy-
cle. The inequalities (3) say that every subset of the
partitions has an outgoing edge; this makes sure a
solution encodes one cycle, rather than a set of mul-
tiple cycles.
To solve such an ILP using the branch-and-cut
method, we drop the integrality constraints (i.e. we
replace xe E {0, 1} by 0 &lt; xe &lt; 1) and solve
the corresponding linear programming (LP) relax-
ation. If the solution of the LP is integral, we found
the optimal solution. Otherwise we pick a variable
with a fractional value and split the problem into
two subproblems by setting the variable to 0 and 1,
respectively. We solve the subproblems recursively
and disregard a subproblem if its LP bound is worse
than the best known solution.
Since our ILP contains an exponential number of
inequalities of type (3), solving the complete LPs
directly would be too expensive. Instead, we start
with a small subset of these inequalities, and test
(efficiently) whether a solution of the smaller LP
violates an inequality which is not in the current
LP. If so, we add the inequality to the LP, resolve
it, and iterate. Otherwise we found the solution of
the LP with the exponential number of inequalities.
The inequalities we add by need are called cutting
planes; algorithms that find violated cutting planes
are called separation algorithms.
To keep the size of the branch-and-cut tree small,
our algorithm employs some heuristics to find fur-
ther upper bounds. In addition, we improve lower
bound from the LP relaxations by adding further in-
equalities to the LP that are valid for all integral so-
lutions, but can be violated for optimal solutions of
the LP. One major challenge here was to find separa-
tion algorithms for these inequalities. We cannot go
into these details for lack of space, but will discuss
them in a separate paper.
</bodyText>
<sectionHeader confidence="0.996628" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.997291285714286">
We implemented the algorithm and ran it on some
examples to evaluate its practical efficiency. The
runtimes are shown in Tables 3 and 4 for an imple-
mentation using a branch-and-cut ILP solver which
is free for all academic purposes (ILP-FS) and a
commercial branch-and-cut ILP solver (ILP-CS).
Our implementations are based on LEDA 4.4.1
</bodyText>
<equation confidence="0.932961416666667">
Xmin cexe
eEE
Xs.t. Xxe =
eEd+(v) eEd−(v)
xe b v E V (1)
xe E {0, 1}
X
eEd−(Vi)
X
eEd+(Ui∈IVi)
xe = 1 1 &lt; i &lt; n (2)
xe &gt; 1 I C {1, ... , n} (3)
</equation>
<table confidence="0.999856625">
Instance Size ILP-FS ILP-CS
lapata-10 13 0.05 0.05
coffers1 M.NOCB 10 0.04 0.02
cabinet1 M.NOCB 15 0.07 0.01
random (avg) 20 0.09 0.07
random (avg) 40 0.28 0.17
random (avg) 60 1.39 0.40
random (avg) 100 6.17 1.97
</table>
<tableCaption confidence="0.999788">
Table 3: Some runtimes for d = 2 (in seconds).
</tableCaption>
<bodyText confidence="0.993744511627907">
(www.algorithmic-solutions.com) for
the data structures and the graph algorithms and
on SCIL 0.8 (www.mpi-sb.mpg.de/SCIL)
for implementing the ILP-based branch-and-cut
algorithm. SCIL can be used with different
branch-and-cut core codes. We used CPLEX
9.0 (www.ilog.com) as commercial core and
SCIP 0.68 (www.zib.de/Optimization/
Software/SCIP/) based on SOPLEX 1.2.2a
(www.zib.de/Optimization/Software/
Soplex/) as the free implementation. Note that
all our implementations are still preliminary. The
software is publicly available (www.mpi-sb.
mpg.de/˜althaus/PDOP.html).
We evaluate the implementations on three classes
of inputs. First, we use two discourses from the
GNOME corpus, taken from (Karamanis, 2003), to-
gether with the centering-based cost functions from
Section 2: coffers1, containing 10 discourse units,
and cabinet1, containing 15 discourse units. Sec-
ond, we use twelve discourses from the BLLIP
corpus taken from (Lapata, 2003), together with
M.LAPATA. These discourses are 4 to 13 discourse
units long; the table only shows the instance with
the highest running time. Finally, we generate ran-
dom instances of 2PDOP of size 20–100, and of
3PDOP of size 10, 15, and 20. A random instance is
the complete graph, where c(ui, ... , ud) is chosen
uniformly at random from {0, ... , 999}.
The results for the 2-place instances are shown
in Table 3, and the results for the 3-place instances
are shown in Table 4. The numbers are runtimes in
seconds on a Pentium 4 (Xeon) processor with 3.06
GHz. Note that a hypothetical baseline implementa-
tion which naively generates and evaluates all per-
mutations would run over 77 years for a discourse
of length 20, even on a highly optimistic platform
that evaluates one billion permutations per second.
For d = 2, all real-life instances and all random
instances of size up to 50 can be solved in less than
one second, with either implementation. The prob-
lem becomes more challenging for d = 3. Here the
algorithm quickly establishes good LP bounds for
</bodyText>
<table confidence="0.99981575">
Instance Size ILP-FS ILP-CS
coffers1 M.KP 10 0.05 0.05
coffers1 M.BFP 10 0.08 0.06
cabinet1 M.KP 15 0.40 1.12
cabinet1 M.BFP 15 0.39 0.28
random (avg) 10 1.00 0.42
random (avg) 15 35.1 5.79
random (avg) 20 - 115.8
</table>
<tableCaption confidence="0.999684">
Table 4: Some runtimes for d = 3 (in seconds).
</tableCaption>
<bodyText confidence="0.999909333333334">
the real-life instances, and thus the branch-and-cut
trees remain small. The LP bounds for the random
instances are worse, in particular when the number
of units gets larger. In this case, the further opti-
misations in the commercial software make a big
difference in the size of the branch-and-cut tree and
thus in the solution time.
An example output for cabinet1 with M.NOCB
is shown in Fig. 3; we have modified referring ex-
pressions to make the text more readable, and have
marked discourse unit boundaries with “/” and ex-
pressions that establish local coherence with square
brackets. This is one of many possible optimal so-
lutions, which have cost 2 because of the two NOCB
transitions at the very start of the discourse. Details
on the comparison of different centering-based co-
herence measures are discussed by Karamanis et al.
(2004).
</bodyText>
<sectionHeader confidence="0.987727" genericHeader="method">
6 Comparison to Other Approaches
</sectionHeader>
<bodyText confidence="0.999909486486486">
There are two approaches in the literature that are
similar enough to ours that a closer comparison is
in order.
The first is a family of algorithms for discourse
ordering based on genetic programming (Mellish et
al., 1998; Karamanis and Manurung, 2002). This is
a very flexible and powerful approach, which can be
applied to measures of local coherence that do not
seem to fit in our framework trivially. For exam-
ple, the measure from (Mellish et al., 1998) looks at
the entire discourse up to the current transition for
some of their cost factors. However, our algorithm
is several orders of magnitude faster where a direct
comparison is possible (Manurung, p.c.), and it is
guaranteed to find an optimal ordering. The non-
approximability result for TSP means that a genetic
(or any other) algorithm which is restricted to poly-
nomial runtime could theoretically deliver arbitrar-
ily bad solutions.
Second, the discourse ordering problem we have
discussed in this paper looks very similar to the Ma-
jority Ordering problem that arises in the context
of multi-document summarisation (Barzilay et al.,
Both cabinets probably entered England in the early nineteenth century / after the French Revolution caused
the dispersal of so many French collections. / The pair to [this monumental cabinet] still exists in Scotland.
/ The fleurs-de-lis on the top two drawers indicate that [the cabinet] was made for the French King Louis
XIV. / [It] may have served as a royal gift, / as [it] does not appear in inventories of [his] possessions. /
Another medallion inside shows [him] a few years later. / The bronze medallion above [the central door]
was cast from a medal struck in 1661 which shows [the king] at the age of twenty-one. / A panel of marquetry
showing the cockerel of [France] standing triumphant over both the eagle of the Holy Roman Empire and the
lion of Spain and the Spanish Netherlands decorates [the central door]. / In [the Dutch Wars] of 1672 - 1678,
[France] fought simultaneously against the Dutch, Spanish, and Imperial armies, defeating them all. / [The
cabinet] celebrates the Treaty of Nijmegen, which concluded [the war]. / The Sun King’s portrait appears
twice on [this work]. / Two large figures from Greek mythology, Hercules and Hippolyta, Queen of the
Amazons, representatives of strength and bravery in war appear to support [the cabinet]. / The decoration on
[the cabinet] refers to [Louis XIV’s] military victories. / On the drawer above the door, gilt-bronze military
trophies flank a medallion portrait of [the king].
</bodyText>
<figureCaption confidence="0.9953">
Figure 3: An example output based on M.NOCB.
</figureCaption>
<bodyText confidence="0.999946846153846">
2002). The difference between the two problems is
that Barzilay et al. minimise the sum of all costs
CZE for any pair i, j of discourse units with i &lt; j,
whereas we only sum over the CZE for i = j − 1.
This makes their problem amenable to the approxi-
mation algorithm by Cohen et al. (1999), which al-
lows them to compute a solution that is at least half
as good as the optimum, in polynomial time; i.e.
this problem is strictly easier than TSP or discourse
ordering. However, a Majority Ordering algorithm
is not guaranteed to compute good solutions to the
discourse ordering problem, as Lapata (2003) as-
sumes.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99997823255814">
We have shown that the problem of ordering clauses
into a discourse that maximises local coherence is
equivalent to the travelling salesman problem: Even
the two-place discourse ordering problem can en-
code ATSP. This means that the problem is NP-
complete and doesn’t even admit polynomial ap-
proximation algorithms (unless P=NP).
On the other hand, we have shown how to encode
the discourse ordering problems of arbitrary arity
d into GATSP. We have demonstrated that mod-
ern branch-and-cut algorithms for GATSP can eas-
ily solve practical discourse ordering problems if
d = 2, and are still usable for many instances with
d = 3. As far as we are aware, this is the first al-
gorithm for discourse ordering that can make any
guarantees about the solution it computes.
Our efficient implementation can benefit genera-
tion and summarisation research in at least two re-
spects. First, we show that computing locally co-
herent orderings of clauses is feasible in practice,
as such coherence measures will probably be ap-
plied on sentences within the same paragraph, i.e.
on problem instances of limited size. Second, our
system should be a useful experimentation tool in
developing new measures of local coherence.
We have focused on local coherence in this paper,
but it seems clear that notions of global coherence,
which go beyond the level of sentence-to-sentence
transitions, capture important aspects of coherence
that a purely local model cannot. However, our al-
gorithm can still be useful as a subroutine in a more
complex system that deals with global coherence
(Marcu, 1997; Mellish et al., 1998). Whether our
methods can be directly applied to the tree struc-
tures that come up in theories of global coherence is
an interesting question for future research.
Acknowledgments. We would like to thank
Mirella Lapata for providing the experimental data
and Andrea Lodi for providing an efficiency base-
line by running his ATSP solver on our inputs. We
are grateful to Malte Gabsdil, Ruli Manurung, Chris
Mellish, Kristina Striegnitz, and our reviewers for
helpful comments and discussions.
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996186643835617">
R. Barzilay, N. Elhadad, and K. R. McKeown.
2002. Inferring strategies for sentence ordering
in multidocument news summarization. Journal
ofArtificial Intelligence Research, 17:35–55.
S. Brennan, M. Walker Friedman, and C. Pollard.
1987. A centering approach to pronouns. In
Proc. 25th ACL, pages 155–162, Stanford.
W. Cohen, R. Schapire, and Y. Singer. 1999. Learn-
ing to order things. Journal of Artificial Intelli-
gence Research, 10:243–270.
T. H. Cormen, C. E. Leiserson, and R. L. Rivest.
1990. Introduction to Algorithms. MIT Press,
Cambridge.
M. Fischetti, A. Lodi, and P. Toth. 2001. Solv-
ing real-world ATSP instances by branch-and-
cut. Combinatorial Optimization.
M. Fischetti, A. Lodi, and P. Toth. 2002. Exact
methods for the asymmmetric traveling salesman
problem. In G. Gutin and A. Punnen, editors, The
Traveling Salesman Problem and its Variations.
Kluwer.
N. Karamanis and H. M. Manurung. 2002.
Stochastic text structuring using the principle of
continuity. In Proceedings of INLG-02, pages
81–88, New York.
N. Karamanis, M. Poesio, C. Mellish, and J. Ober-
lander. 2004. Evaluating centering-based met-
rics of coherence for text structuring using a re-
liably annotated corpus. In Proceedings of the
42nd ACL, Barcelona.
N. Karamanis. 2003. Entity Coherence for De-
scriptive Text Structuring. Ph.D. thesis, Division
of Informatics, University of Edinburgh.
R. Kibble and R. Power. 2000. An integrated
framework for text planning and pronominalisa-
tion. In Proc. INLG 2000, pages 77–84, Mitzpe
Ramon.
M. Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proc. 41st
ACL, pages 545–552, Sapporo, Japan.
G. Laporte, H. Mercure, and Y. Nobert. 1987. Gen-
eralized travelling salesman problem through n
sets of nodes: the asymmetrical case. Discrete
Applied Mathematics, 18:185–197.
W. Mann and S. Thompson. 1988. Rhetorical struc-
ture theory: A theory of text organization. Text,
8(3):243–281.
D. Marcu. 1997. From local to global coherence:
A bottom-up approach to text planning. In Pro-
ceedings of the 14th AAAI, pages 629–635.
C. Mellish, A. Knott, J. Oberlander, and
M. O’Donnell. 1998. Experiments using
stochastic search for text planning. In Proc. 9th
INLG, pages 98–107, Niagara-on-the-Lake.
G.L. Nemhauser and L.A. Wolsey. 1988. Integer
and Combinatorial Optimization. John Wiley &amp;
Sons.
C.E. Noon and J.C. Bean. 1993. An efficient trans-
formation of the generalized traveling salesman
problem. Information Systems and Operational
Research, 31(1).
M. Strube and U. Hahn. 1999. Functional center-
ing: Grounding referential coherence in informa-
tion structure. Computational Linguistics, 25(3).
M. Walker, A. Joshi, and E. Prince. 1998. Center-
ing in naturally occuring discourse: An overview.
In M. Walker, A. Joshi, and E. Prince, edi-
tors, Centering Theory in Discourse, pages 1–30.
Clarendon Press, Oxford.
B. Webber, A. Knott, M. Stone, and A. Joshi. 1999.
What are little trees made of: A structural and
presuppositional account using Lexicalized TAG.
In Proc. 36th ACL, pages 151–156, College Park.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.365963">
<title confidence="0.999964">Computing Locally Coherent Discourses</title>
<author confidence="0.999949">Alexander Koller</author>
<affiliation confidence="0.999978">Dept. of Computational Linguistics Saarland University</affiliation>
<address confidence="0.974808">Saarbr¨ucken, Germany</address>
<email confidence="0.998707">koller@coli.uni-sb.de</email>
<author confidence="0.998809">Ernst Althaus</author>
<affiliation confidence="0.7236365">LORIA Universit´e Henri Poincar´e</affiliation>
<address confidence="0.716769">Vandœuvre-l`es-Nancy, France</address>
<email confidence="0.986197">althaus@loria.fr</email>
<author confidence="0.798591">Nikiforos Karamanis</author>
<affiliation confidence="0.999817">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.962405">Edinburgh, UK</address>
<email confidence="0.997241">N.Karamanis@sms.ed.ac.uk</email>
<abstract confidence="0.994332285714286">We present the first algorithm that computes optimal orderings of sentences into a locally coherent discourse. The algorithm runs very efficiently on a variety of coherence measures from the literature. We also show that the discourse ordering problem is NP-complete and cannot be approximated.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>N Elhadad</author>
<author>K R McKeown</author>
</authors>
<title>Inferring strategies for sentence ordering in multidocument news summarization.</title>
<date>2002</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>17--35</pages>
<contexts>
<context position="1031" citStr="Barzilay et al., 2002" startWordPosition="136" endWordPosition="139">imal orderings of sentences into a locally coherent discourse. The algorithm runs very efficiently on a variety of coherence measures from the literature. We also show that the discourse ordering problem is NP-complete and cannot be approximated. 1 Introduction One central problem in discourse generation and summarisation is to structure the discourse in a way that maximises coherence. Coherence is the property of a good human-authored text that makes it easier to read and understand than a randomlyordered collection of sentences. Several papers in the recent literature (Mellish et al., 1998; Barzilay et al., 2002; Karamanis and Manurung, 2002; Lapata, 2003; Karamanis et al., 2004) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions. This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree (Mann and Thompson, 1988; Marcu, 1997; Webber et al., 1999). Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory (Walker et al., 1998; Brennan et al., 1987; Kibble and </context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>R. Barzilay, N. Elhadad, and K. R. McKeown. 2002. Inferring strategies for sentence ordering in multidocument news summarization. Journal ofArtificial Intelligence Research, 17:35–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brennan</author>
<author>M Walker Friedman</author>
<author>C Pollard</author>
</authors>
<title>A centering approach to pronouns.</title>
<date>1987</date>
<booktitle>In Proc. 25th ACL,</booktitle>
<pages>155--162</pages>
<location>Stanford.</location>
<contexts>
<context position="1618" citStr="Brennan et al., 1987" startWordPosition="230" endWordPosition="233"> al., 1998; Barzilay et al., 2002; Karamanis and Manurung, 2002; Lapata, 2003; Karamanis et al., 2004) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions. This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree (Mann and Thompson, 1988; Marcu, 1997; Webber et al., 1999). Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory (Walker et al., 1998; Brennan et al., 1987; Kibble and Power, 2000; Karamanis and Manurung, 2002) or on statistical models (Lapata, 2003). But while formal models of local coherence have made substantial progress over the past few years, the question of how to efficiently compute an ordering of the sentences in a discourse that maximises local coherence is still largely unsolved. The fundamental problem is that any of the factorial number of permutations of the sentences could be the optimal discourse, which makes for a formidable search space for nontrivial discourses. Mellish et al. (1998) and Karamanis and Manurung (2002) present a</context>
<context position="7565" citStr="Brennan et al., 1987" startWordPosition="1238" endWordPosition="1241">ghest ranked element of Cf(ui) which also appears in Cf(ui−1), and serves as the link between the two subsequent utterances ui−1 and ui. Each utterance has at most one Cb. If ui and ui−1 have no forward-looking centres in common, or if ui is the first utterance in the discourse, then ui does not have a Cb at all. Based on these concepts, CT classifies the transitions between subsequent utterances into different types. Table 1 shows the most common classification into the four types CONTINUE, RETAIN, SMOOTH-SHIFT, and ROUGH-SHIFT, which are predicted to be less and less coherent in this order (Brennan et al., 1987). Kibble and Power (2000) define three further classes of transitions: COHERENCE and SALIENCE, which are both defined in Table 1 as well, and NOCB, the class of transitions for which Cb(ui) is undefined. Finally, a transition is considered to satisfy the CHEAPNESS constraint (Strube and Hahn, 1999) if Cb(ui) = Cp(ui−1). Table 2 summarises some cost functions from the literature, in the reconstruction of Karamanis et al. (2004). Each line shows the name of the coherence measure, the arity d from Definition 1, and the initial and transition cost functions. To fit the definitions in one line, we </context>
<context position="9611" citStr="Brennan et al., 1987" startWordPosition="1594" endWordPosition="1597">h must take n−d+1 � i=1 + COHERENCE: COHERENCE∗: Cb(ui) = Cb(ui−1) Cb(ui) =6 Cb(ui−1) SALIENCE: Cb(ui) = Cp(ui) CONTINUE SMOOTH-SHIFT SALIENCE∗: Cb(ui) =6 Cp(ui) RETAIN ROUGH-SHIFT Table 1: COHERENCE, SALIENCE and the table of standard transitions d initial cost cI(u1, . . . , ud−1) transition cost cT(ud|u1, . . . , ud−1) M.NOCB 2 0 nocb2 M.KP 3 nocb2 + nocheap2 + nosal2 nocb2 + nocheap2 + nosal2 + nocoh3 M.BFP 3 (1 − nosal2, nosal2, 0, 0) (cont3, ret3, ss3, rs3) M.LAPATA 2 −log P(u1) − log P(u2|u1) Table 2: Some cost functions from the literature. three arguments. Finally, the measure M.BFP (Brennan et al., 1987) uses a lexicographic ordering on 4-tuples which indicate whether the transition is a CONTINUE, RETAIN, SMOOTH-SHIFT, or ROUGHSHIFT. cT and all four functions it is computed from take three arguments because the classification depends on COHERENCE. As the first transition in the discourse is coherent by default (it has no Cb), we can compute cI by distinguishing RETAIN and CONTINUE via SALIENCE. The tuple-valued cost functions can be converted to real-valued functions by choosing a sufficiently large number M and using the value M3 · cont + M2 · ret + M · ss + rs. 2.3 Probability-Based Cost Fu</context>
</contexts>
<marker>Brennan, Friedman, Pollard, 1987</marker>
<rawString>S. Brennan, M. Walker Friedman, and C. Pollard. 1987. A centering approach to pronouns. In Proc. 25th ACL, pages 155–162, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
<author>R Schapire</author>
<author>Y Singer</author>
</authors>
<title>Learning to order things.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>10--243</pages>
<contexts>
<context position="27134" citStr="Cohen et al. (1999)" startWordPosition="4780" endWordPosition="4783"> Queen of the Amazons, representatives of strength and bravery in war appear to support [the cabinet]. / The decoration on [the cabinet] refers to [Louis XIV’s] military victories. / On the drawer above the door, gilt-bronze military trophies flank a medallion portrait of [the king]. Figure 3: An example output based on M.NOCB. 2002). The difference between the two problems is that Barzilay et al. minimise the sum of all costs CZE for any pair i, j of discourse units with i &lt; j, whereas we only sum over the CZE for i = j − 1. This makes their problem amenable to the approximation algorithm by Cohen et al. (1999), which allows them to compute a solution that is at least half as good as the optimum, in polynomial time; i.e. this problem is strictly easier than TSP or discourse ordering. However, a Majority Ordering algorithm is not guaranteed to compute good solutions to the discourse ordering problem, as Lapata (2003) assumes. 7 Conclusion We have shown that the problem of ordering clauses into a discourse that maximises local coherence is equivalent to the travelling salesman problem: Even the two-place discourse ordering problem can encode ATSP. This means that the problem is NPcomplete and doesn’t </context>
</contexts>
<marker>Cohen, Schapire, Singer, 1999</marker>
<rawString>W. Cohen, R. Schapire, and Y. Singer. 1999. Learning to order things. Journal of Artificial Intelligence Research, 10:243–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Cormen</author>
<author>C E Leiserson</author>
<author>R L Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="13119" citStr="Cormen et al., 1990" startWordPosition="2228" endWordPosition="2231">on of ATSP to 2PDOP We will show that ATSP can be reduced to 2PDOP, and that any dPDOP can be reduced to GATSP. 3.1 Reduction of ATSP to 2PDOP First, we introduce the reduction of ATSP to 2PDOP, which establishes NP-completeness of dPDOP for all d &gt; 1. The reduction is approximation preserving, i.e. if we can find a solution of 2PDOP that is worse than the optimum only by a factor of E (an E-approximation), it translates to a solution of ATSP that is also an E-approximation. Since it is known that there can be no polynomial algorithms that compute E-approximations for general ATSP, for any E (Cormen et al., 1990), this means that dPDOP cannot be approximated either (unless P=NP): Any polynomial algorithm for dPDOP will compute arbitrarily bad solutions on certain inputs. The reduction works as follows. Let G = ((V1, ... , Vk), E, c) be an instance of ATSP, and V = V1 U ... U Vk. We choose an arbitrary node v E V and split it into two nodes vs and vt. We assign all edges with source node v to vs and all edges with target node v to vt (compare Figure 1). Finally we make vs the source node of our 2PDOP instance G0. For every tour in G, we have a path in G0 starting at vs visiting all other nodes (and end</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>T. H. Cormen, C. E. Leiserson, and R. L. Rivest. 1990. Introduction to Algorithms. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fischetti</author>
<author>A Lodi</author>
<author>P Toth</author>
</authors>
<title>Solving real-world ATSP instances by branch-andcut. Combinatorial Optimization.</title>
<date>2001</date>
<contexts>
<context position="16971" citStr="Fischetti et al., 2001" startWordPosition="3033" endWordPosition="3037">hable and are not shown. For the special case of d = 2, the GATSP is simply an ordinary ATSP. The graphs of both problems look identical in this case, except that the GATSP instance has edges of cost 0 from any node to the source [s]. 4 Computing Optimal Orderings The equivalence of dPDOP and GATSP implies that we can now bring algorithms from the vast literature on TSP to bear on the discourse ordering problem. One straightforward method is to reduce the GATSP further to ATSP (Noon and Bean, 1993); for the case d = 2, nothing has to be done. Then one can solve the reduced ATSP instance; see (Fischetti et al., 2001; Fischetti et al., 2002) for a recent survey of exact methods. We choose the alternative of developing a new algorithm for solving GATSP directly, which uses standard techniques from combinatorial optimisation, gives us a better handle on optimising the algorithm for our problem instances, and runs more efficiently in practice. Our algorithm translates the GATSP instance into an integer linear program (ILP) and uses the branch-and-cut method (Nemhauser and Wolsey, 1988) to solve it. Integer linear programs consist of a set of linear equations and inequalities, and are solved by integer variab</context>
</contexts>
<marker>Fischetti, Lodi, Toth, 2001</marker>
<rawString>M. Fischetti, A. Lodi, and P. Toth. 2001. Solving real-world ATSP instances by branch-andcut. Combinatorial Optimization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fischetti</author>
<author>A Lodi</author>
<author>P Toth</author>
</authors>
<title>Exact methods for the asymmmetric traveling salesman problem.</title>
<date>2002</date>
<editor>In G. Gutin and A. Punnen, editors,</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="16996" citStr="Fischetti et al., 2002" startWordPosition="3038" endWordPosition="3041"> For the special case of d = 2, the GATSP is simply an ordinary ATSP. The graphs of both problems look identical in this case, except that the GATSP instance has edges of cost 0 from any node to the source [s]. 4 Computing Optimal Orderings The equivalence of dPDOP and GATSP implies that we can now bring algorithms from the vast literature on TSP to bear on the discourse ordering problem. One straightforward method is to reduce the GATSP further to ATSP (Noon and Bean, 1993); for the case d = 2, nothing has to be done. Then one can solve the reduced ATSP instance; see (Fischetti et al., 2001; Fischetti et al., 2002) for a recent survey of exact methods. We choose the alternative of developing a new algorithm for solving GATSP directly, which uses standard techniques from combinatorial optimisation, gives us a better handle on optimising the algorithm for our problem instances, and runs more efficiently in practice. Our algorithm translates the GATSP instance into an integer linear program (ILP) and uses the branch-and-cut method (Nemhauser and Wolsey, 1988) to solve it. Integer linear programs consist of a set of linear equations and inequalities, and are solved by integer variable assignments which maxi</context>
</contexts>
<marker>Fischetti, Lodi, Toth, 2002</marker>
<rawString>M. Fischetti, A. Lodi, and P. Toth. 2002. Exact methods for the asymmmetric traveling salesman problem. In G. Gutin and A. Punnen, editors, The Traveling Salesman Problem and its Variations. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Karamanis</author>
<author>H M Manurung</author>
</authors>
<title>Stochastic text structuring using the principle of continuity.</title>
<date>2002</date>
<booktitle>In Proceedings of INLG-02,</booktitle>
<pages>81--88</pages>
<location>New York.</location>
<contexts>
<context position="1061" citStr="Karamanis and Manurung, 2002" startWordPosition="140" endWordPosition="144">nces into a locally coherent discourse. The algorithm runs very efficiently on a variety of coherence measures from the literature. We also show that the discourse ordering problem is NP-complete and cannot be approximated. 1 Introduction One central problem in discourse generation and summarisation is to structure the discourse in a way that maximises coherence. Coherence is the property of a good human-authored text that makes it easier to read and understand than a randomlyordered collection of sentences. Several papers in the recent literature (Mellish et al., 1998; Barzilay et al., 2002; Karamanis and Manurung, 2002; Lapata, 2003; Karamanis et al., 2004) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions. This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree (Mann and Thompson, 1988; Marcu, 1997; Webber et al., 1999). Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory (Walker et al., 1998; Brennan et al., 1987; Kibble and Power, 2000; Karamanis and Man</context>
<context position="8372" citStr="Karamanis and Manurung, 2002" startWordPosition="1377" endWordPosition="1380">hich Cb(ui) is undefined. Finally, a transition is considered to satisfy the CHEAPNESS constraint (Strube and Hahn, 1999) if Cb(ui) = Cp(ui−1). Table 2 summarises some cost functions from the literature, in the reconstruction of Karamanis et al. (2004). Each line shows the name of the coherence measure, the arity d from Definition 1, and the initial and transition cost functions. To fit the definitions in one line, we use terms of the form fk, which abbreviate applications of f to the last k arguments of the cost functions, i.e. f(ud−k+1, . . . , ud). The most basic coherence measure, M.NOCB (Karamanis and Manurung, 2002), simply assigns to each NOCB transition the cost 1 and to every other transition the cost 0. The definition of cT(u2|u1), which decodes to nocb(u1, u2), only looks at the two units in the transition, and no further context. The initial costs for this coherence measure are always zero. The measure M.KP (Kibble and Power, 2000) sums the value of nocb and the values of three functions which evaluate to 0 if the transition is cheap, salient, or coherent, and 1 otherwise. This is an instance of the 3-place discourse ordering problem because COHERENCE depends on Cb(ui−1), which itself depends on Cf</context>
<context position="24507" citStr="Karamanis and Manurung, 2002" startWordPosition="4333" endWordPosition="4336">boundaries with “/” and expressions that establish local coherence with square brackets. This is one of many possible optimal solutions, which have cost 2 because of the two NOCB transitions at the very start of the discourse. Details on the comparison of different centering-based coherence measures are discussed by Karamanis et al. (2004). 6 Comparison to Other Approaches There are two approaches in the literature that are similar enough to ours that a closer comparison is in order. The first is a family of algorithms for discourse ordering based on genetic programming (Mellish et al., 1998; Karamanis and Manurung, 2002). This is a very flexible and powerful approach, which can be applied to measures of local coherence that do not seem to fit in our framework trivially. For example, the measure from (Mellish et al., 1998) looks at the entire discourse up to the current transition for some of their cost factors. However, our algorithm is several orders of magnitude faster where a direct comparison is possible (Manurung, p.c.), and it is guaranteed to find an optimal ordering. The nonapproximability result for TSP means that a genetic (or any other) algorithm which is restricted to polynomial runtime could theo</context>
</contexts>
<marker>Karamanis, Manurung, 2002</marker>
<rawString>N. Karamanis and H. M. Manurung. 2002. Stochastic text structuring using the principle of continuity. In Proceedings of INLG-02, pages 81–88, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Karamanis</author>
<author>M Poesio</author>
<author>C Mellish</author>
<author>J Oberlander</author>
</authors>
<title>Evaluating centering-based metrics of coherence for text structuring using a reliably annotated corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd ACL,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="1100" citStr="Karamanis et al., 2004" startWordPosition="147" endWordPosition="150">algorithm runs very efficiently on a variety of coherence measures from the literature. We also show that the discourse ordering problem is NP-complete and cannot be approximated. 1 Introduction One central problem in discourse generation and summarisation is to structure the discourse in a way that maximises coherence. Coherence is the property of a good human-authored text that makes it easier to read and understand than a randomlyordered collection of sentences. Several papers in the recent literature (Mellish et al., 1998; Barzilay et al., 2002; Karamanis and Manurung, 2002; Lapata, 2003; Karamanis et al., 2004) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions. This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree (Mann and Thompson, 1988; Marcu, 1997; Webber et al., 1999). Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory (Walker et al., 1998; Brennan et al., 1987; Kibble and Power, 2000; Karamanis and Manurung, 2002) or on statistical models (</context>
<context position="7995" citStr="Karamanis et al. (2004)" startWordPosition="1308" endWordPosition="1311">1 shows the most common classification into the four types CONTINUE, RETAIN, SMOOTH-SHIFT, and ROUGH-SHIFT, which are predicted to be less and less coherent in this order (Brennan et al., 1987). Kibble and Power (2000) define three further classes of transitions: COHERENCE and SALIENCE, which are both defined in Table 1 as well, and NOCB, the class of transitions for which Cb(ui) is undefined. Finally, a transition is considered to satisfy the CHEAPNESS constraint (Strube and Hahn, 1999) if Cb(ui) = Cp(ui−1). Table 2 summarises some cost functions from the literature, in the reconstruction of Karamanis et al. (2004). Each line shows the name of the coherence measure, the arity d from Definition 1, and the initial and transition cost functions. To fit the definitions in one line, we use terms of the form fk, which abbreviate applications of f to the last k arguments of the cost functions, i.e. f(ud−k+1, . . . , ud). The most basic coherence measure, M.NOCB (Karamanis and Manurung, 2002), simply assigns to each NOCB transition the cost 1 and to every other transition the cost 0. The definition of cT(u2|u1), which decodes to nocb(u1, u2), only looks at the two units in the transition, and no further context</context>
<context position="24219" citStr="Karamanis et al. (2004)" startWordPosition="4286" endWordPosition="4289"> the commercial software make a big difference in the size of the branch-and-cut tree and thus in the solution time. An example output for cabinet1 with M.NOCB is shown in Fig. 3; we have modified referring expressions to make the text more readable, and have marked discourse unit boundaries with “/” and expressions that establish local coherence with square brackets. This is one of many possible optimal solutions, which have cost 2 because of the two NOCB transitions at the very start of the discourse. Details on the comparison of different centering-based coherence measures are discussed by Karamanis et al. (2004). 6 Comparison to Other Approaches There are two approaches in the literature that are similar enough to ours that a closer comparison is in order. The first is a family of algorithms for discourse ordering based on genetic programming (Mellish et al., 1998; Karamanis and Manurung, 2002). This is a very flexible and powerful approach, which can be applied to measures of local coherence that do not seem to fit in our framework trivially. For example, the measure from (Mellish et al., 1998) looks at the entire discourse up to the current transition for some of their cost factors. However, our al</context>
</contexts>
<marker>Karamanis, Poesio, Mellish, Oberlander, 2004</marker>
<rawString>N. Karamanis, M. Poesio, C. Mellish, and J. Oberlander. 2004. Evaluating centering-based metrics of coherence for text structuring using a reliably annotated corpus. In Proceedings of the 42nd ACL, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Karamanis</author>
</authors>
<title>Entity Coherence for Descriptive Text Structuring.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Division of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="3455" citStr="Karamanis, 2003" startWordPosition="522" endWordPosition="523">oblem is not even approximable; any polynomial algorithm will compute arbitrarily bad solutions on unfortunate inputs. Note that all approximation algorithms for the TSP assume that the underlying cost function is a metric, which is not the case for the coherence measures we consider. Despite this negative result, we show that by applying modern algorithms for TSP, the discourse ordering problem can be solved efficiently enough for practical applications. We define a branch-and-cut algorithm based on linear programming, and evaluate it on discourse ordering problems based on the GNOME corpus (Karamanis, 2003) and the BLLIP corpus (Lapata, 2003). If the local coherence measure depends only on the adjacent pairs of sentences in the discourse, we can order discourses of up to 50 sentences in under a second. If it is allowed to depend on the left-hand context of the sentence pair, computation is often still efficient, but can become expensive. The structure of the paper is as follows. We will first formally define the discourse ordering problem and relate our definition to the literature on local coherence measures in Section 2. Then we will prove the equivalence of discourse ordering and TSP (Section</context>
<context position="21825" citStr="Karamanis, 2003" startWordPosition="3875" endWordPosition="3876">www.mpi-sb.mpg.de/SCIL) for implementing the ILP-based branch-and-cut algorithm. SCIL can be used with different branch-and-cut core codes. We used CPLEX 9.0 (www.ilog.com) as commercial core and SCIP 0.68 (www.zib.de/Optimization/ Software/SCIP/) based on SOPLEX 1.2.2a (www.zib.de/Optimization/Software/ Soplex/) as the free implementation. Note that all our implementations are still preliminary. The software is publicly available (www.mpi-sb. mpg.de/˜althaus/PDOP.html). We evaluate the implementations on three classes of inputs. First, we use two discourses from the GNOME corpus, taken from (Karamanis, 2003), together with the centering-based cost functions from Section 2: coffers1, containing 10 discourse units, and cabinet1, containing 15 discourse units. Second, we use twelve discourses from the BLLIP corpus taken from (Lapata, 2003), together with M.LAPATA. These discourses are 4 to 13 discourse units long; the table only shows the instance with the highest running time. Finally, we generate random instances of 2PDOP of size 20–100, and of 3PDOP of size 10, 15, and 20. A random instance is the complete graph, where c(ui, ... , ud) is chosen uniformly at random from {0, ... , 999}. The results</context>
</contexts>
<marker>Karamanis, 2003</marker>
<rawString>N. Karamanis. 2003. Entity Coherence for Descriptive Text Structuring. Ph.D. thesis, Division of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kibble</author>
<author>R Power</author>
</authors>
<title>An integrated framework for text planning and pronominalisation.</title>
<date>2000</date>
<booktitle>In Proc. INLG</booktitle>
<pages>77--84</pages>
<location>Mitzpe Ramon.</location>
<contexts>
<context position="1642" citStr="Kibble and Power, 2000" startWordPosition="234" endWordPosition="237">t al., 2002; Karamanis and Manurung, 2002; Lapata, 2003; Karamanis et al., 2004) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions. This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree (Mann and Thompson, 1988; Marcu, 1997; Webber et al., 1999). Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory (Walker et al., 1998; Brennan et al., 1987; Kibble and Power, 2000; Karamanis and Manurung, 2002) or on statistical models (Lapata, 2003). But while formal models of local coherence have made substantial progress over the past few years, the question of how to efficiently compute an ordering of the sentences in a discourse that maximises local coherence is still largely unsolved. The fundamental problem is that any of the factorial number of permutations of the sentences could be the optimal discourse, which makes for a formidable search space for nontrivial discourses. Mellish et al. (1998) and Karamanis and Manurung (2002) present algorithms based on genet</context>
<context position="7590" citStr="Kibble and Power (2000)" startWordPosition="1242" endWordPosition="1245"> Cf(ui) which also appears in Cf(ui−1), and serves as the link between the two subsequent utterances ui−1 and ui. Each utterance has at most one Cb. If ui and ui−1 have no forward-looking centres in common, or if ui is the first utterance in the discourse, then ui does not have a Cb at all. Based on these concepts, CT classifies the transitions between subsequent utterances into different types. Table 1 shows the most common classification into the four types CONTINUE, RETAIN, SMOOTH-SHIFT, and ROUGH-SHIFT, which are predicted to be less and less coherent in this order (Brennan et al., 1987). Kibble and Power (2000) define three further classes of transitions: COHERENCE and SALIENCE, which are both defined in Table 1 as well, and NOCB, the class of transitions for which Cb(ui) is undefined. Finally, a transition is considered to satisfy the CHEAPNESS constraint (Strube and Hahn, 1999) if Cb(ui) = Cp(ui−1). Table 2 summarises some cost functions from the literature, in the reconstruction of Karamanis et al. (2004). Each line shows the name of the coherence measure, the arity d from Definition 1, and the initial and transition cost functions. To fit the definitions in one line, we use terms of the form fk,</context>
</contexts>
<marker>Kibble, Power, 2000</marker>
<rawString>R. Kibble and R. Power. 2000. An integrated framework for text planning and pronominalisation. In Proc. INLG 2000, pages 77–84, Mitzpe Ramon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In Proc. 41st ACL,</booktitle>
<pages>545--552</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1075" citStr="Lapata, 2003" startWordPosition="145" endWordPosition="146">iscourse. The algorithm runs very efficiently on a variety of coherence measures from the literature. We also show that the discourse ordering problem is NP-complete and cannot be approximated. 1 Introduction One central problem in discourse generation and summarisation is to structure the discourse in a way that maximises coherence. Coherence is the property of a good human-authored text that makes it easier to read and understand than a randomlyordered collection of sentences. Several papers in the recent literature (Mellish et al., 1998; Barzilay et al., 2002; Karamanis and Manurung, 2002; Lapata, 2003; Karamanis et al., 2004) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions. This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree (Mann and Thompson, 1988; Marcu, 1997; Webber et al., 1999). Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory (Walker et al., 1998; Brennan et al., 1987; Kibble and Power, 2000; Karamanis and Manurung, 2002) o</context>
<context position="3491" citStr="Lapata, 2003" startWordPosition="528" endWordPosition="529">ynomial algorithm will compute arbitrarily bad solutions on unfortunate inputs. Note that all approximation algorithms for the TSP assume that the underlying cost function is a metric, which is not the case for the coherence measures we consider. Despite this negative result, we show that by applying modern algorithms for TSP, the discourse ordering problem can be solved efficiently enough for practical applications. We define a branch-and-cut algorithm based on linear programming, and evaluate it on discourse ordering problems based on the GNOME corpus (Karamanis, 2003) and the BLLIP corpus (Lapata, 2003). If the local coherence measure depends only on the adjacent pairs of sentences in the discourse, we can order discourses of up to 50 sentences in under a second. If it is allowed to depend on the left-hand context of the sentence pair, computation is often still efficient, but can become expensive. The structure of the paper is as follows. We will first formally define the discourse ordering problem and relate our definition to the literature on local coherence measures in Section 2. Then we will prove the equivalence of discourse ordering and TSP (Section 3), and present algorithms for solv</context>
<context position="10314" citStr="Lapata (2003)" startWordPosition="1715" endWordPosition="1716">INUE, RETAIN, SMOOTH-SHIFT, or ROUGHSHIFT. cT and all four functions it is computed from take three arguments because the classification depends on COHERENCE. As the first transition in the discourse is coherent by default (it has no Cb), we can compute cI by distinguishing RETAIN and CONTINUE via SALIENCE. The tuple-valued cost functions can be converted to real-valued functions by choosing a sufficiently large number M and using the value M3 · cont + M2 · ret + M · ss + rs. 2.3 Probability-Based Cost Functions A fundamentally different approach to measure discourse coherence was proposed by Lapata (2003). It uses a statistical bigram model that assigns each pair ui, uk of utterances a probability P(uk|ui) of appearing in subsequent positions, and each utterance a probability P(ui) of appearing in the initial position of the discourse. The probabilities are estimated on the grounds of syntactic features of the discourse units. The probability of the entire discourse u1 ... un is the product P(u1) · P(u2|u1) · ... · P(un|un−1). We can transform Lapata’s model straightforwardly into our cost function framework, as shown under M.LAPATA in Table 2. The discourse that minimizes the sum of the negat</context>
<context position="22058" citStr="Lapata, 2003" startWordPosition="3910" endWordPosition="3911">ware/SCIP/) based on SOPLEX 1.2.2a (www.zib.de/Optimization/Software/ Soplex/) as the free implementation. Note that all our implementations are still preliminary. The software is publicly available (www.mpi-sb. mpg.de/˜althaus/PDOP.html). We evaluate the implementations on three classes of inputs. First, we use two discourses from the GNOME corpus, taken from (Karamanis, 2003), together with the centering-based cost functions from Section 2: coffers1, containing 10 discourse units, and cabinet1, containing 15 discourse units. Second, we use twelve discourses from the BLLIP corpus taken from (Lapata, 2003), together with M.LAPATA. These discourses are 4 to 13 discourse units long; the table only shows the instance with the highest running time. Finally, we generate random instances of 2PDOP of size 20–100, and of 3PDOP of size 10, 15, and 20. A random instance is the complete graph, where c(ui, ... , ud) is chosen uniformly at random from {0, ... , 999}. The results for the 2-place instances are shown in Table 3, and the results for the 3-place instances are shown in Table 4. The numbers are runtimes in seconds on a Pentium 4 (Xeon) processor with 3.06 GHz. Note that a hypothetical baseline imp</context>
<context position="27445" citStr="Lapata (2003)" startWordPosition="4834" endWordPosition="4835">d on M.NOCB. 2002). The difference between the two problems is that Barzilay et al. minimise the sum of all costs CZE for any pair i, j of discourse units with i &lt; j, whereas we only sum over the CZE for i = j − 1. This makes their problem amenable to the approximation algorithm by Cohen et al. (1999), which allows them to compute a solution that is at least half as good as the optimum, in polynomial time; i.e. this problem is strictly easier than TSP or discourse ordering. However, a Majority Ordering algorithm is not guaranteed to compute good solutions to the discourse ordering problem, as Lapata (2003) assumes. 7 Conclusion We have shown that the problem of ordering clauses into a discourse that maximises local coherence is equivalent to the travelling salesman problem: Even the two-place discourse ordering problem can encode ATSP. This means that the problem is NPcomplete and doesn’t even admit polynomial approximation algorithms (unless P=NP). On the other hand, we have shown how to encode the discourse ordering problems of arbitrary arity d into GATSP. We have demonstrated that modern branch-and-cut algorithms for GATSP can easily solve practical discourse ordering problems if d = 2, and</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>M. Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proc. 41st ACL, pages 545–552, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Laporte</author>
<author>H Mercure</author>
<author>Y Nobert</author>
</authors>
<title>Generalized travelling salesman problem through n sets of nodes: the asymmetrical case.</title>
<date>1987</date>
<journal>Discrete Applied Mathematics,</journal>
<pages>18--185</pages>
<contexts>
<context position="18149" citStr="Laporte et al. (1987)" startWordPosition="3248" endWordPosition="3251">qualities, and are solved by integer variable assignments which maximise or minimise a goal function while satisfying the other conditions. Let G = (V, E) be a directed graph and S C_ V . We define δ+(S) = {(u, v) E E |u E S and v E� S} and δ−(S) = {(u, v) E E |u E/ S and v E S}, i.e. δ+(S) and δ−(S) are the sets of all incoming and outgoing edges of S, respectively. We assume that the graph G has no edges within one partition Vu, since such edges cannot be used by any solution. With this assumption, GATSP can be phrased as an ILP as follows (this formulation is similar to the one proposed by Laporte et al. (1987)): We have a binary variable xe for each edge e of the graph. The intention is that xe has value 1 if e is used in the tour, and 0 otherwise. Thus the cost of the tour can be written as PeEE cexe. The three conditions enforce the variable assignment to encode a valid GATSP tour. (1) ensures that all integer solutions encode a set of cycles. (2) guarantees that every partition Vi is visited by exactly one cycle. The inequalities (3) say that every subset of the partitions has an outgoing edge; this makes sure a solution encodes one cycle, rather than a set of multiple cycles. To solve such an I</context>
</contexts>
<marker>Laporte, Mercure, Nobert, 1987</marker>
<rawString>G. Laporte, H. Mercure, and Y. Nobert. 1987. Generalized travelling salesman problem through n sets of nodes: the asymmetrical case. Discrete Applied Mathematics, 18:185–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Mann</author>
<author>S Thompson</author>
</authors>
<title>Rhetorical structure theory: A theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="1390" citStr="Mann and Thompson, 1988" startWordPosition="192" endWordPosition="195">a way that maximises coherence. Coherence is the property of a good human-authored text that makes it easier to read and understand than a randomlyordered collection of sentences. Several papers in the recent literature (Mellish et al., 1998; Barzilay et al., 2002; Karamanis and Manurung, 2002; Lapata, 2003; Karamanis et al., 2004) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions. This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree (Mann and Thompson, 1988; Marcu, 1997; Webber et al., 1999). Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory (Walker et al., 1998; Brennan et al., 1987; Kibble and Power, 2000; Karamanis and Manurung, 2002) or on statistical models (Lapata, 2003). But while formal models of local coherence have made substantial progress over the past few years, the question of how to efficiently compute an ordering of the sentences in a discourse that maximises local coherence is still largely unsolved. The fundamental problem is that</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>W. Mann and S. Thompson. 1988. Rhetorical structure theory: A theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>From local to global coherence: A bottom-up approach to text planning.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th AAAI,</booktitle>
<pages>629--635</pages>
<contexts>
<context position="1403" citStr="Marcu, 1997" startWordPosition="196" endWordPosition="197">rence. Coherence is the property of a good human-authored text that makes it easier to read and understand than a randomlyordered collection of sentences. Several papers in the recent literature (Mellish et al., 1998; Barzilay et al., 2002; Karamanis and Manurung, 2002; Lapata, 2003; Karamanis et al., 2004) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions. This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree (Mann and Thompson, 1988; Marcu, 1997; Webber et al., 1999). Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory (Walker et al., 1998; Brennan et al., 1987; Kibble and Power, 2000; Karamanis and Manurung, 2002) or on statistical models (Lapata, 2003). But while formal models of local coherence have made substantial progress over the past few years, the question of how to efficiently compute an ordering of the sentences in a discourse that maximises local coherence is still largely unsolved. The fundamental problem is that any of the f</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>D. Marcu. 1997. From local to global coherence: A bottom-up approach to text planning. In Proceedings of the 14th AAAI, pages 629–635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mellish</author>
<author>A Knott</author>
<author>J Oberlander</author>
<author>M O’Donnell</author>
</authors>
<title>Experiments using stochastic search for text planning.</title>
<date>1998</date>
<booktitle>In Proc. 9th INLG,</booktitle>
<pages>98--107</pages>
<marker>Mellish, Knott, Oberlander, O’Donnell, 1998</marker>
<rawString>C. Mellish, A. Knott, J. Oberlander, and M. O’Donnell. 1998. Experiments using stochastic search for text planning. In Proc. 9th INLG, pages 98–107, Niagara-on-the-Lake.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G L Nemhauser</author>
<author>L A Wolsey</author>
</authors>
<title>Integer and Combinatorial Optimization.</title>
<date>1988</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="17446" citStr="Nemhauser and Wolsey, 1988" startWordPosition="3108" endWordPosition="3111">her to ATSP (Noon and Bean, 1993); for the case d = 2, nothing has to be done. Then one can solve the reduced ATSP instance; see (Fischetti et al., 2001; Fischetti et al., 2002) for a recent survey of exact methods. We choose the alternative of developing a new algorithm for solving GATSP directly, which uses standard techniques from combinatorial optimisation, gives us a better handle on optimising the algorithm for our problem instances, and runs more efficiently in practice. Our algorithm translates the GATSP instance into an integer linear program (ILP) and uses the branch-and-cut method (Nemhauser and Wolsey, 1988) to solve it. Integer linear programs consist of a set of linear equations and inequalities, and are solved by integer variable assignments which maximise or minimise a goal function while satisfying the other conditions. Let G = (V, E) be a directed graph and S C_ V . We define δ+(S) = {(u, v) E E |u E S and v E� S} and δ−(S) = {(u, v) E E |u E/ S and v E S}, i.e. δ+(S) and δ−(S) are the sets of all incoming and outgoing edges of S, respectively. We assume that the graph G has no edges within one partition Vu, since such edges cannot be used by any solution. With this assumption, GATSP can be</context>
</contexts>
<marker>Nemhauser, Wolsey, 1988</marker>
<rawString>G.L. Nemhauser and L.A. Wolsey. 1988. Integer and Combinatorial Optimization. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Noon</author>
<author>J C Bean</author>
</authors>
<title>An efficient transformation of the generalized traveling salesman problem.</title>
<date>1993</date>
<booktitle>Information Systems and Operational Research,</booktitle>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="16852" citStr="Noon and Bean, 1993" startWordPosition="3009" endWordPosition="3012"> indeed visits each Vu0 (i.e., each column) exactly once. Nodes with last component s which are not [s, s] are unreachable and are not shown. For the special case of d = 2, the GATSP is simply an ordinary ATSP. The graphs of both problems look identical in this case, except that the GATSP instance has edges of cost 0 from any node to the source [s]. 4 Computing Optimal Orderings The equivalence of dPDOP and GATSP implies that we can now bring algorithms from the vast literature on TSP to bear on the discourse ordering problem. One straightforward method is to reduce the GATSP further to ATSP (Noon and Bean, 1993); for the case d = 2, nothing has to be done. Then one can solve the reduced ATSP instance; see (Fischetti et al., 2001; Fischetti et al., 2002) for a recent survey of exact methods. We choose the alternative of developing a new algorithm for solving GATSP directly, which uses standard techniques from combinatorial optimisation, gives us a better handle on optimising the algorithm for our problem instances, and runs more efficiently in practice. Our algorithm translates the GATSP instance into an integer linear program (ILP) and uses the branch-and-cut method (Nemhauser and Wolsey, 1988) to so</context>
</contexts>
<marker>Noon, Bean, 1993</marker>
<rawString>C.E. Noon and J.C. Bean. 1993. An efficient transformation of the generalized traveling salesman problem. Information Systems and Operational Research, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Strube</author>
<author>U Hahn</author>
</authors>
<title>Functional centering: Grounding referential coherence in information structure.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="7864" citStr="Strube and Hahn, 1999" startWordPosition="1287" endWordPosition="1290">ave a Cb at all. Based on these concepts, CT classifies the transitions between subsequent utterances into different types. Table 1 shows the most common classification into the four types CONTINUE, RETAIN, SMOOTH-SHIFT, and ROUGH-SHIFT, which are predicted to be less and less coherent in this order (Brennan et al., 1987). Kibble and Power (2000) define three further classes of transitions: COHERENCE and SALIENCE, which are both defined in Table 1 as well, and NOCB, the class of transitions for which Cb(ui) is undefined. Finally, a transition is considered to satisfy the CHEAPNESS constraint (Strube and Hahn, 1999) if Cb(ui) = Cp(ui−1). Table 2 summarises some cost functions from the literature, in the reconstruction of Karamanis et al. (2004). Each line shows the name of the coherence measure, the arity d from Definition 1, and the initial and transition cost functions. To fit the definitions in one line, we use terms of the form fk, which abbreviate applications of f to the last k arguments of the cost functions, i.e. f(ud−k+1, . . . , ud). The most basic coherence measure, M.NOCB (Karamanis and Manurung, 2002), simply assigns to each NOCB transition the cost 1 and to every other transition the cost 0</context>
</contexts>
<marker>Strube, Hahn, 1999</marker>
<rawString>M. Strube and U. Hahn. 1999. Functional centering: Grounding referential coherence in information structure. Computational Linguistics, 25(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>A Joshi</author>
<author>E Prince</author>
</authors>
<title>Centering in naturally occuring discourse: An overview.</title>
<date>1998</date>
<booktitle>Centering Theory in Discourse,</booktitle>
<pages>1--30</pages>
<editor>In M. Walker, A. Joshi, and E. Prince, editors,</editor>
<publisher>Clarendon Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="1596" citStr="Walker et al., 1998" startWordPosition="226" endWordPosition="229">iterature (Mellish et al., 1998; Barzilay et al., 2002; Karamanis and Manurung, 2002; Lapata, 2003; Karamanis et al., 2004) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions. This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree (Mann and Thompson, 1988; Marcu, 1997; Webber et al., 1999). Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory (Walker et al., 1998; Brennan et al., 1987; Kibble and Power, 2000; Karamanis and Manurung, 2002) or on statistical models (Lapata, 2003). But while formal models of local coherence have made substantial progress over the past few years, the question of how to efficiently compute an ordering of the sentences in a discourse that maximises local coherence is still largely unsolved. The fundamental problem is that any of the factorial number of permutations of the sentences could be the optimal discourse, which makes for a formidable search space for nontrivial discourses. Mellish et al. (1998) and Karamanis and Man</context>
<context position="6405" citStr="Walker et al., 1998" startWordPosition="1038" endWordPosition="1041"> , n} such that cI(uw(1),... , uw(d−1)) cT (uw(i+d−1)|uw(i), ... , uw(i+d−2)) is minimal. The notation for the cost functions is suggestive: The transition cost function has the character of a conditional probability, which specifies that the cost of continuing the discourse with the unit ud depends on the local context u1, ... , ud−1. This local context is not available for the first d − 1 units of the discourse, which is why their costs are summarily covered by the initial function. 2.2 Centering-Based Cost Functions One popular class of coherence measures is based on Centering Theory (CT, (Walker et al., 1998)). We will briefly sketch its basic notions and then show how some CT-based coherence measures can be cast into our framework. The standard formulation of CT e.g. in (Walker et al., 1998), calls the discourse units utterances, and assigns to each utterance ui in the discourse a list Cf(ui) of forward-looking centres. The members of Cf(ui) correspond to the referents of the NPs in ui and are ranked in order of prominence, the first element being the preferred centre Cp(ui). The backward-looking centre Cb(ui) of ui is defined as the highest ranked element of Cf(ui) which also appears in Cf(ui−1)</context>
</contexts>
<marker>Walker, Joshi, Prince, 1998</marker>
<rawString>M. Walker, A. Joshi, and E. Prince. 1998. Centering in naturally occuring discourse: An overview. In M. Walker, A. Joshi, and E. Prince, editors, Centering Theory in Discourse, pages 1–30. Clarendon Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Webber</author>
<author>A Knott</author>
<author>M Stone</author>
<author>A Joshi</author>
</authors>
<title>What are little trees made of: A structural and presuppositional account using Lexicalized TAG.</title>
<date>1999</date>
<booktitle>In Proc. 36th ACL,</booktitle>
<pages>151--156</pages>
<location>College Park.</location>
<contexts>
<context position="1425" citStr="Webber et al., 1999" startWordPosition="198" endWordPosition="201">nce is the property of a good human-authored text that makes it easier to read and understand than a randomlyordered collection of sentences. Several papers in the recent literature (Mellish et al., 1998; Barzilay et al., 2002; Karamanis and Manurung, 2002; Lapata, 2003; Karamanis et al., 2004) have focused on defining local coherence, which evaluates the quality of sentence-to-sentence transitions. This is in contrast to theories of global coherence, which can consider relations between larger chunks of the discourse and e.g. structures them into a tree (Mann and Thompson, 1988; Marcu, 1997; Webber et al., 1999). Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse, and can be based e.g. on Centering Theory (Walker et al., 1998; Brennan et al., 1987; Kibble and Power, 2000; Karamanis and Manurung, 2002) or on statistical models (Lapata, 2003). But while formal models of local coherence have made substantial progress over the past few years, the question of how to efficiently compute an ordering of the sentences in a discourse that maximises local coherence is still largely unsolved. The fundamental problem is that any of the factorial number of per</context>
</contexts>
<marker>Webber, Knott, Stone, Joshi, 1999</marker>
<rawString>B. Webber, A. Knott, M. Stone, and A. Joshi. 1999. What are little trees made of: A structural and presuppositional account using Lexicalized TAG. In Proc. 36th ACL, pages 151–156, College Park.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>