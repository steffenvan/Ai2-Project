<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.796751">
Learning Bigrams from Unigrams
</title>
<author confidence="0.881973">
Xiaojin Zhu† and Andrew B. Goldberg† and Michael Rabbat$ and Robert Nowak§
</author>
<affiliation confidence="0.955611">
†Department of Computer Sciences, University of Wisconsin-Madison
$Department of Electrical and Computer Engineering, McGill University
§Department of Electrical and Computer Engineering, University of Wisconsin-Madison
</affiliation>
<email confidence="0.985047">
{jerryzhu, goldberg}@cs.wisc.edu, michael.rabbat@mcgill.ca, nowak@ece.wisc.edu
</email>
<sectionHeader confidence="0.996649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999767523809524">
Traditional wisdom holds that once docu-
ments are turned into bag-of-words (unigram
count) vectors, word orders are completely
lost. We introduce an approach that, perhaps
surprisingly, is able to learn a bigram lan-
guage model from a set of bag-of-words docu-
ments. At its heart, our approach is an EM al-
gorithm that seeks a model which maximizes
the regularized marginal likelihood of the bag-
of-words documents. In experiments on seven
corpora, we observed that our learned bigram
language models: i) achieve better test set per-
plexity than unigram models trained on the
same bag-of-words documents, and are not far
behind “oracle bigram models” trained on the
corresponding ordered documents; ii) assign
higher probabilities to sensible bigram word
pairs; iii) improve the accuracy of ordered-
document recovery from a bag-of-words. Our
approach opens the door to novel phenomena,
for example, privacy leakage from index files.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998321282051282">
A bag-of-words (BOW) is a basic document repre-
sentation in natural language processing. In this pa-
per, we consider a BOW in its simplest form, i.e.,
a unigram count vector or word histogram over the
vocabulary. When performing the counting, word
order is ignored. For example, the phrases “really
neat” and “neat really” contribute equally to a BOW.
Obviously, once a set of documents is turned into
a set of BOWs, the word order information within
them is completely lost—or is it?
In this paper, we show that one can in fact partly
recover the order information. Specifically, given a
set of documents in unigram-count BOW representa-
tion, one can recover a non-trivial bigram language
model (LM)1, which has part of the power of a bi-
gram LM trained on ordered documents. At first
glance this seems impossible: How can one learn
bigram information from unigram counts? However,
we will demonstrate that multiple BOW documents
enable us to recover some higher-order information.
Our results have implications in a wide range of
natural language problems, in particular document
privacy. With the wide adoption of natural language
applications like desktop search engines, software
programs are increasingly indexing computer users’
personal files for fast processing. Most index files
include some variant of the BOW. As we demon-
strate in this paper, if a malicious party gains access
to BOW index files, it can recover more than just
unigram frequencies: (i) the malicious party can re-
cover a higher-order LM; (ii) with the LM it may at-
tempt to recover the original ordered document from
a BOW by finding the most-likely word permuta-
tion2. Future research will quantify the extent to
which such a privacy breach is possible in theory,
and will find solutions to prevent it.
There is a vast literature on language modeling;
see, e.g., (Rosenfeld, 2000; Chen and Goodman,
1999; Brants et al., 2007; Roark et al., 2007). How-
</bodyText>
<footnote confidence="0.8589406">
1A trivial bigram LM is a unigram LM which ignores his-
tory: P(vlu) = P(v).
2It is possible to use a generic higher-order LM, e.g., a tri-
gram LM trained on standard English corpora, for this purpose.
However, incorporating a user-specific LM helps.
</footnote>
<page confidence="0.941645">
656
</page>
<note confidence="0.721319">
Proceedings of ACL-08: HLT, pages 656–664,
</note>
<page confidence="0.538631">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999669833333333">
ever, to the best of our knowledge, none addresses
this reverse direction of learning higher-order LMs
from lower-order data. This work is inspired by re-
cent advances in inferring network structure from
co-occurrence data, for example, for computer net-
works and biological pathways (Rabbat et al., 2007).
</bodyText>
<sectionHeader confidence="0.902528" genericHeader="method">
2 Problem Formulation and Identifiability
</sectionHeader>
<bodyText confidence="0.984512317073171">
We assume that a vocabulary of size W is given.
For notational convenience, we include in the vo-
cabulary a special “begin-of-document” symbol (d)
which appears only at the beginning of each docu-
ment. The training corpus consists of a collection of
n BOW documents {x1, ... , xn}. Each BOW xi is
a vector (xi1, ... , xiW) where xiu is the number of
times word u occurs in document i. Our goal is to
learn a bigram LM θ, represented as a W xW transi-
tion matrix with θu„ = P(v|u), from the BOW cor-
pus. Note P(v|(d)) corresponds to the initial state
probability for word v, and P((d)|u) = 0, bu.
It is worth noting that traditionally one needs or-
dered documents to learn a bigram LM. A natural
question that arises in our problem is whether or not
a bigram LM can be recovered from the BOW cor-
pus with any guarantee. Let X denote the space
of all possible BOWs. As a toy example, consider
W = 3 with the vocabulary {(d), A, B}. Assuming
all documents have equal length |x |= 4 (including
(d)), then X = {((d):1, A:3, B:0), ((d):1, A:2, B:1),
((d):1, A:1, B:2), ((d):1, A:0, B:3)}. Our training
BOW corpus, when sufficiently large, provides the
marginal distribution p(x) for x E X. Can we re-
cover a bigram LM from p(x)?
To answer this question, we first need to introduce
a generative model for the BOWs. We assume that
the BOW corpus is generated from a bigram LM θ
in two steps: (i) An ordered document is generated
from the bigram LM θ; (ii) The document’s unigram
counts are collected to produce the BOW x. There-
fore, the probability of a BOW x being generated
by θ can be computed by marginalizing over unique
orderings z of x:
where σ(x) is the set of unique orderings, and |x |is
the document length. For example, if x =((d):1,
A:2, B:1) then σ(x) = {z1, z2, z3} with z1 =
“(d) A A B”, z2 = “(d) A B A”, z3 = “(d) B A A”.
Bigram LM recovery then amounts to finding a θ
that satisfies the system of marginal-matching equa-
tions
</bodyText>
<equation confidence="0.997486">
P(x|θ) = p(x) , bx E X. (1)
</equation>
<bodyText confidence="0.999290857142857">
As a concrete example where one can exactly re-
cover a bigram LM from BOWs, consider our toy
example again. We know there are only three free
variables in our 3x3 bigram LM θ: r = θ(d)A, p =
θAA, q = θBB, since the rest are determined by
normalization. Suppose the documents are gener-
ated from a bigram LM with true parameters r =
0.25,p = 0.9, q = 0.5. If our BOW corpus is very
large, we will observe that 20.25% of the BOWs are
((d):1, A:3, B:0), 37.25% are ((d):1, A:2, B:1), and
18.75% are ((d):1, A:0, B:3). These numbers are
computed using the definition of P(x|θ). We solve
the reverse problem of finding r, p, q from the sys-
tem of equations (1), now explicitly written as
</bodyText>
<equation confidence="0.9993875">
rp2 = 0.2025
rp(1 − p) + r(1 − p)(1 − q)
+(1 − r)(1 − q)p = 0.3725
(1 − r)q2 = 0.1875.
</equation>
<bodyText confidence="0.997704590909091">
The above system has only one valid solution,
which is the correct set of bigram LM parameters
(r, p, q) = (0.25, 0.9, 0.5).
However, if the true parameters were (r, p, q) =
(0.1, 0.2, 0.3) with proportions of BOWs being
0.4%, 19.8%, 8.1%, respectively, it is easy to ver-
ify that the system would have multiple valid solu-
tions: (0.1, 0.2, 0.3), (0.8819, 0.0673, 0.8283), and
(0.1180, 0.1841, 0.3030). In general, if p(x) is
known from the training BOW corpus, when can
we guarantee to uniquely recover the bigram LM
θ? This is the question of identifiability, which
means the transition matrix θ satisfying (1) exists
and is unique. Identifiability is related to finding
unique solutions of a system of polynomial equa-
tions since (1) is such a system in the elements of θ.
The details are beyond the scope of this paper, but
applying the technique in (Basu and Boston, 2000),
it is possible to show that for W = 3 (including (d))
we need longer documents (|x |&gt; 5) to ensure iden-
tifiability. The identifiability of more general cases
is still an open research question.
</bodyText>
<equation confidence="0.9925085">
X X
P(x|θ) = P(z|θ) =
zEQ(x)
|x|
Y
zEQ(x) j=2
θzj−1,zj,



</equation>
<page confidence="0.997395">
657
</page>
<sectionHeader confidence="0.938454" genericHeader="method">
3 Bigram Recovery Algorithm
</sectionHeader>
<bodyText confidence="0.999433625">
In practice, the documents are not truly generated
from a bigram LM, and the BOW corpus may be
small. We therefore seek a maximum likelihood es-
timate of θ or a regularized version of it. Equiva-
lently, we no longer require equality in (1), but in-
stead find θ that makes the distribution P(x|θ) as
close to p(x) as possible. We formalize this notion
below.
</bodyText>
<subsectionHeader confidence="0.99923">
3.1 The Objective Function
</subsectionHeader>
<bodyText confidence="0.972449052631579">
Given a BOW corpus {x1, ... , xn}, its nor-
malized log likelihood under θ is ℓ(θ) ≡
c EZ 1 log P(xi|θ), where C = Eni= 1(|xi |− 1)
is the corpus length excluding hdi’s. The idea is to
find θ that maximizes ℓ(θ). This also brings P(x|θ)
closest to p(x) in the KL-divergence sense. How-
ever, to prevent overfitting, we regularize the prob-
lem so that θ prefers to be close to a “prior” bi-
gram LM φ. The prior φ is also estimated from the
BOW corpus, and is discussed in Section 3.4. We
define the regularizer to be an asymmetric dissimi-
larity D(φ, θ) between the prior φ and the learned
model θ. The dissimilarity is 0 if θ = φ, and
increases as they diverge. Specifically, the KL-
divergence between two word distributions condi-
tioned on the same history u is KL(φu·kθu·) =
EWv=1 φuv log φuv
θuv . We define D(φ, θ) to be
the average KL-divergence over all histories:
</bodyText>
<equation confidence="0.890687333333333">
EW
D(φ, θ) ≡ 1 u=1 KL(φu·kθu·), which is con-
W
</equation>
<bodyText confidence="0.998569666666667">
vex in θ (Cover and Thomas, 1991). We will use
the following derivative later: ∂D(φ, θ)/∂θuv =
−φuv/(Wθuv).
We are now ready to define the regularized op-
timization problem for recovering a bigram LM θ
from the BOW corpus:
</bodyText>
<equation confidence="0.991246333333333">
max ℓ(θ) − λD(φ, θ)
θ
subject to θ1 = 1, θ ≥ 0. (2)
</equation>
<bodyText confidence="0.984639142857143">
The weight λ controls the strength of the prior. The
constraints ensure that θ is a valid bigram matrix,
where 1 is an all-one vector, and the non-negativity
constraint is element-wise. Equivalently, (2) can be
viewed as the maximum a posteriori (MAP) estimate
of θ, with independent Dirichlet priors for each row
of θ: p(θu·) = Dir(θu·|αu·) and hyperparameters
</bodyText>
<equation confidence="0.669414">
λC
αuv = W φuv + 1.
</equation>
<bodyText confidence="0.99875575">
The summation over hidden ordered documents
z in P(x|θ) couples the variables and makes (2) a
non-concave problem. We optimize θ using an EM
algorithm.
</bodyText>
<subsectionHeader confidence="0.999563">
3.2 The EM Algorithm
</subsectionHeader>
<bodyText confidence="0.99920575">
We derive the EM algorithm for the optimization
problem (2). Let O(θ) ≡ ℓ(θ) − λD(φ, θ) be the
objective function. Let θ(t−1) be the bigram LM at
iteration t − 1. We can lower-bound O as follows:
</bodyText>
<equation confidence="0.994381375">
O(θ)
= 1 n log zE� i ) P(z|θ(t−1), x) P(z|θ)
C i=1 P(z|θ(t−1), x)
−λD(φ, θ)
≥ 1 n � P(z|θ(t−1), x) log P(z|θ)
C i=1 zEσ(xi) P(z|θ(t−1),x)
−λD(φ, θ)
≡ L(θ, θ(t−1)).
</equation>
<bodyText confidence="0.975787095238095">
We used Jensen’s inequality above since log()
is concave. The lower bound L involves
P(z|θ(t−1), x), the probability of hidden orderings
of the BOW under the previous iteration’s model.
In the E-step of EM we compute P(z|θ(t−1), x),
which will be discussed in Section 3.3. One
can verify that L(θ,θ(t−1)) is concave in θ, un-
like the original objective O(θ). In addition, the
lower bound “touches” the objective at θ(t−1), i.e.,
L(θ(t−1), θ(t−1)) = O(θ(t−1)).
The EM algorithm iteratively maximizes the
lower bound, which is now a concave optimization
problem: maxθ L(θ, θ(t−1)), subject to θ1 = 1.
The non-negativity constraints turn out to be auto-
matically satisfied. Introducing Lagrange multipli-
ers βu for each history u = 1... W, we form the
Lagrangian A:
�θuv − 1 .
Taking the partial derivative with respect to θuv and
setting it to zero: ∂A/∂θuv = 0, we arrive at the
following update:
</bodyText>
<equation confidence="0.97789725">
λC
P(z|θ(t−1), x)cuv(z) + W φuv.
(3)
W W
A ≡ L(θ, θ(t−1)) − βu
u=1 v=1
θuv ∝ n �
i=1 zEσ(xi)
</equation>
<page confidence="0.981635">
658
</page>
<bodyText confidence="0.971711">
Input: BOW documents {x1, ... , x,,,}, a prior bi-
gram LM φ, weight A.
</bodyText>
<listItem confidence="0.971427333333333">
1. t = 1. Initialize θ(°) = φ.
2. Repeat until the objective O(θ) converges:
(a) (E-step) Compute P(zJθ(t−1), x) for z E
a(xi), i = 1, ... , n.
(b) (M-step) Compute θ(t) using (3). Let t =
t + 1.
</listItem>
<tableCaption confidence="0.8483365">
Output: The recovered bigram LM θ.
Table 1: The EM algorithm
</tableCaption>
<bodyText confidence="0.999778076923077">
The normalization is over v = 1... W. We use
cuv(z) to denote the number of times the bigram
“uv” appears in the ordered document z. This is the
M-step of EM. Intuitively, the first term counts how
often the bigram “uv” occurs, weighing each order-
ing by its probability under the previous model; the
second term pulls the parameter towards the prior.
If the weight of the prior A —* oc, we would have
0uv = ouv. The update is related to the MAP esti-
mate for a multinomial distribution with a Dirichlet
prior, where we use the expected counts.
We initialize the EM algorithm with θ(0) = φ.
The EM algorithm is summarized in Table 1.
</bodyText>
<subsectionHeader confidence="0.98417">
3.3 Approximate E-step
</subsectionHeader>
<bodyText confidence="0.999976">
The E-step needs to compute the expected bigram
counts of the form
</bodyText>
<equation confidence="0.995071">
X P(zJθ,x)cuv(z). (4)
zEσ(x)
</equation>
<bodyText confidence="0.999648764705882">
However, this poses a computational problem. The
summation is over unique ordered documents. The
number of unique ordered documents can be on the
order of JxJ!, i.e., all permutations of the BOW. For a
short document of length 15, this number is already
1012. Clearly, brute-force enumeration is only fea-
sible for very short documents. Approximation is
necessary to handle longer ones.
A simple Monte Carlo approximation to (4)
would involve sampling ordered documents
z1, z2, ... , zL according to zi ti P(zJθ, x), and
replacing (4) with PL i=1 cuv(zi)/L. This estimate
is unbiased, and the variance decreases linearly
with the number of samples, L. However, sampling
directly from P is difficult.
Instead, we sample ordered documents zi ti
R(ziJθ, x) from a distribution R which is easy
to generate, and construct an approximation us-
ing importance sampling (see, e.g., (Liu, 2001)).
With each sample, zi, we associate a weight
wi a P(ziJ0,x)/R(ziJ0,x). The importance
sampling approximation to (4) is then given by
(PL i=1 wicuv(zi))/(PL i=1 wi). Re-weighting the
samples in this fashion accounts for the fact that we
are using a sampling distribution R which is differ-
ent the target distribution P, and guarantees that our
approximation is asymptotically unbiased.
The quality of an importance sampling approxi-
mation is closely related to how closely R resembles
P; the more similar they are, the better the approxi-
mation, in general. Given a BOW x and our current
bigram model estimate, θ, we generate one sample
(an ordered document zi) by sequentially drawing
words from the bag, with probabilities proportional
to θ, but properly normalized to form a distribution
based on which words remain in the bag. For exam-
ple, suppose x = ((d):1, A:2, B:1, C:1). Then we
set zi1 = (d), and sample zi2 = A with probabil-
ity 20(d)A/(20(d)A + 0(d)B + 0(d)C). Similarly,
if zi(j_1) = u and if v is in the original BOW that
hasn’t been sampled yet, then we set the next word in
the ordered document zij equal to v with probability
proportional to cv0uv, where cv is the count of v in
the remaining BOW. For this scheme, one can ver-
ify (Rabbat et al., 2007) that the importance weight
corresponding to a sampled ordered document zi =
(zi1, ... , zi|x|) is given by wi = Qtx|2 PIx|t 0zt_1zz.
In our implementation, the number of importance
samples used for a document x is 10JxJ2 if the length
of the document JxJ &gt; 8; otherwise we enumerate
a(x) without importance sampling.
</bodyText>
<subsectionHeader confidence="0.936634">
3.4 Prior Bigram LM φ
</subsectionHeader>
<bodyText confidence="0.9986964">
The quality of the EM solution θ can depend on the
prior bigram LM φ. To assess bigram recoverabil-
ity from a BOW corpus alone, we consider only pri-
ors estimated from the corpus itself3. Like θ, φ is a
W xW transition matrix with ouv = P (vJu). When
</bodyText>
<footnote confidence="0.763673">
3Priors based on general English text or domain-specific
knowledge could be used in specific applications.
</footnote>
<page confidence="0.997294">
659
</page>
<bodyText confidence="0.959254285714286">
appropriate, we set the initial probability 0(d)v pro-
portional to the number of times word v appears in
the BOW corpus. We consider three prior models:
Prior 1: Unigram φunigram. The most naive
φ is a unigram LM which ignores word history.
The probability for word v is estimated from the
BOW corpus frequency of v, with add-1 smoothing:
</bodyText>
<table confidence="0.854566">
00unigram a 1 + En
uv i=1 xiv. We should point out
that the unigram prior is an asymmetric bigram, i.e.,
unigram =� 0unigram
uv vu .
Prior 2: Frequency of Document Co-
occurrence (FDC) φfdc. Let 6(u, v|x) = 1 if
</table>
<bodyText confidence="0.710170571428571">
words u =� v co-occur (regardless of their counts)
in BOW x, and 0 otherwise. In the case u = v,
6(u, u|x) = 1 only if u appears at least twice in
x. Let cfdc
uv = Eni=1 6(u, v|xi) be the number of
BOWs in which u, v co-occur. The FDC prior is
φfdc
</bodyText>
<subsectionHeader confidence="0.775727">
uv a cfdc
</subsectionHeader>
<bodyText confidence="0.998974142857143">
uv + 1. The co-occurrence counts cfdc
are symmetric, but φfdc is asymmetric because
of normalization. FDC captures some notion of
potential transitions from u to v. FDC is in spirit
similar to Kneser-Ney smoothing (Kneser and Ney,
1995) and other methods that accumulate indicators
of document membership.
</bodyText>
<equation confidence="0.8203605">
Prior 3: Permutation-Based (Perm)φperm.
Re-
</equation>
<bodyText confidence="0.962167090909091">
call that cuv(z) is the number of times the bigram
“uv” appears in an ordered document z. We define
cperm
uv = Eni=1 Ez∈σ(x,)[cuv(z)], where the expecta-
tion is with respect to all unique orderings of each
BOW. We make the zero-knowledge assumption of
uniform probability over these orderings, rather than
P(z|θ) as in the EM algorithm described above. EM
will refine these estimates, though, so this is a natu-
ral starting point. Space precludes a full discussion,
but it can be proven that cperm
</bodyText>
<equation confidence="0.375773333333333">
uv = En i=1 xiuxiv/|xi|
if u =� v, and cperm
uu = En i=1 xiu(xiu − 1)/|xi|. Fi-
nally, 0perm
uv a cperm
uv + 1.
</equation>
<subsectionHeader confidence="0.887272">
3.5 Decoding Ordered Documents from BOWs
</subsectionHeader>
<bodyText confidence="0.995156176470588">
Given a BOW x and a bigram LM θ, we for-
mulate document recovery as the problem z∗ =
argmaxz∈σ(x)P(z|θ). In fact, we can generate
the top N candidate ordered documents in terms
of P(z|θ). We use A∗ search to construct such
an N-best list (Russell and Norvig, 2003). Each
state is an ordered, partial document. Its succes-
sor states append one more unused word in x to
the partial document. The actual cost g from the
start (empty document) to a state is the log proba-
bility of the partial document under bigram θ. We
design a heuristic cost h from the state to the goal
(complete document) that is admissible: the idea is
to over-use the best bigram history for the remain-
ing words in x. Let the partial document end with
word we. Let the count vector for the remaining
BOW be (c1, ... , cW). One admissible heuristic
is h = log HWu=1 P(u|bh(u); θ)cu, where the “best
history” for word type u is bh(u) = argmaxvθvu,
and v ranges over the word types with non-zero
counts in (c1, ... , cW), plus we. It is easy to see that
h is an upper bound on the bigram log probability
that the remaining words in x can achieve.
We use a memory-bounded A∗ search similar
to (Russell, 1992), because long BOWs would oth-
erwise quickly exhaust memory. When the priority
queue grows larger than the bound, the worst states
(in terms of g + h) in the queue are purged. This
necessitates a double-ended priority queue that can
pop either the maximum or minimum item. We use
an efficient implementation with Splay trees (Chong
and Sahni, 2000). We continue running A∗ after
popping the goal state from its priority queue. Re-
peating this N times gives the N-best list.
</bodyText>
<sectionHeader confidence="0.999788" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99991">
We show experimentally that the proposed algo-
rithm is indeed able to recover reasonable bigram
LMs from BOW corpora. We observe:
</bodyText>
<listItem confidence="0.858291923076923">
1. Good test set perplexity: Using test (held-
out) set perplexity (PP) as an objective measure of
LM quality, we demonstrate that our recovered bi-
gram LMs are much better than naive unigram LMs
trained on the same BOW corpus. Furthermore, they
are not far behind the “oracle” bigram LMs trained
on ordered documents that correspond to the BOWs.
2. Sensible bigram pairs: We inspect the recov-
ered bigram LMs and find that they assign higher
probabilities to sensible bigram pairs (e.g., “i mean”,
“oh boy”, “that’s funny”), and lower probabilities to
nonsense pairs (e.g., “i yep”, “you let’s”, “right lot”).
3. Document recovery from BOW: With the bi-
</listItem>
<bodyText confidence="0.806853">
gram LMs, we show improved accuracy in recover-
ing ordered documents from BOWs.
We describe these experiments in detail below.
</bodyText>
<page confidence="0.993924">
660
</page>
<table confidence="0.9990485">
Corpus IVI # Docs # Tokens IxI
SV10 10 6775 7792 1.2
SV25 25 9778 13324 1.4
SV50 50 12442 20914 1.7
SV100 100 14602 28611 2.0
SV250 250 18933 51950 2.7
SV500 500 23669 89413 3.8
SumTime 882 3341 68815 20.6
</table>
<tableCaption confidence="0.9991">
Table 2: Corpora statistics: vocabulary size, document
count, total token count, and mean document length.
</tableCaption>
<subsectionHeader confidence="0.995295">
4.1 Corpora and Protocols
</subsectionHeader>
<bodyText confidence="0.999980787878788">
We note that although in principle our algorithm
works on large corpora, the current implementa-
tion does not scale well (Table 3 last column). We
therefore experimented on seven corpora with rel-
atively small vocabulary sizes, and with short doc-
uments (mostly one sentence per document). Ta-
ble 2 lists statistics describing the corpora. The first
six contain text transcripts of conversational tele-
phone speech from the small vocabulary “SVitch-
board 1” data set. King et al. constructed each cor-
pus from the full Switchboard corpus, with the re-
striction that the sentences use only words in the cor-
responding vocabulary (King et al., 2005). We re-
fer to these corpora as SV10, SV25, SV50, SV100,
SV250, and SV500. The seventh corpus comes from
the SumTime-Meteo data set (Sripada et al., 2003),
which contains real weather forecasts for offshore
oil rigs in the North Sea. For the SumTime cor-
pus, we performed sentence segmentation to pro-
duce documents, removed punctuation, and replaced
numeric digits with a special token.
For each of the seven corpora, we perform 5-fold
cross validation. We use four folds other than the
k-th fold as the training set to train (recover) bigram
LMs, and the k-th fold as the test set for evaluation.
This is repeated for k = 1... 5, and we report the
average cross validation results. We distinguish the
original ordered documents (training set z1, ... zn,
test set zn+1, ... , zm) and the corresponding BOWs
(training set x1 ... xn, test set xn+1 ... xm). In all
experiments, we simply set the weight A = 1 in (2).
Given a training set and a test set, we perform the
following steps:
</bodyText>
<listItem confidence="0.715836818181818">
1. Build prior LMs φX from the training BOW
corpus x1,... xn, for X = unigram, fdc, perm.
2. Recover the bigram LMs θX with the EM al-
gorithm in Table 1, from the training BOW corpus
x1,... xn and using the prior from step 1.
3. Compute the MAP bigram LM from the or-
dered training documents z1, ... zn. We call this the
“oracle” bigram LM because it uses order informa-
tion (not available to our algorithm), and we use it
as a lower-bound on perplexity.
4. Test all LMs on zn+1, ... , zm by perplexity.
</listItem>
<subsectionHeader confidence="0.989687">
4.2 Good Test Set Perplexity
</subsectionHeader>
<bodyText confidence="0.999707205882353">
Table 3 reports the 5-fold cross validation mean-test-
set-PP values for all corpora, and the run time per
EM iteration. Because of the long running time, we
adopt the rule-of-thumb stopping criterion of “two
EM iterations”. First, we observe that all bigram
LMs perform better than unigram LMs φunigram
even though they are trained on the same BOW cor-
pus. Second, all recovered bigram LMs θX im-
proved upon their corresponding baselines φX. The
difference across every row is statistically significant
according to a two-tailed paired t-test with p &lt; 0.05.
The differences among PP(θX) for the same corpus
are also significant (except between θunigram and
θperm for SV500). Finally, we observe that θperm
tends to be best for the smaller vocabulary corpora,
whereas θfdc dominates as the vocabulary grows.
To see how much better we could do if we had or-
dered training documents z1, ... , zn, we present the
mean-test-set-PP of “oracle” bigram LMs in Table 4.
We used three smoothing methods to obtain oracle
LMs: absolute discounting using a constant of 0.5
(we experimented with other values, but 0.5 worked
best), Good-Turing, and interpolated Witten-Bell as
implemented in the SRILM toolkit (Stolcke, 2002).
We see that our recovered LMs (trained on un-
ordered BOW documents), especially for small vo-
cabulary corpora, are close to the oracles (trained on
ordered documents). For the larger datasets, the re-
covery task is more difficult, and the gap between
the oracle LMs and the θ LMs widens. Note that the
oracle LMs do much better than the recovered LMs
on the SumTime corpus; we suspect the difference is
due to the larger vocabulary and significantly higher
average sentence length (see Table 2).
</bodyText>
<subsectionHeader confidence="0.997755">
4.3 Sensible Bigram Pairs
</subsectionHeader>
<bodyText confidence="0.996649">
The next set of experiments compares the recov-
ered bigram LMs to their corresponding prior LMs
</bodyText>
<page confidence="0.994592">
661
</page>
<table confidence="0.999943913043479">
Corpus X PP(φX) PP(θX) Time/
Iter
unigram 7.48 6.95 &lt; is
SV10 fdc 6.52 6.47 &lt; is
perm 6.50 6.45 &lt; is
unigram 16.4 12.8 0.1s
SV25 fdc 12.3 11.8 0.1s
perm 12.2 11.7 0.1s
unigram 29.1 19.7 2s
SV50 fdc 19.6 17.8 4s
perm 19.5 17.7 5s
unigram 45.4 27.8 7s
SV100 fdc 29.5 25.3 11s
perm 30.0 25.6 11s
unigram 91.8 51.2 5m
SV250 fdc 60.0 47.3 8m
perm 65.4 49.7 8m
unigram 149.1 87.2 3h
SV500 fdc 104.8 80.1 3h
perm 123.9 87.4 3h
unigram 129.7 81.8 4h
SumTime fdc 103.2 77.7 4h
perm 187.9 85.4 3h
</table>
<tableCaption confidence="0.8941185">
Table 3: Mean test set perplexities of prior LMs and bi-
gram LMs recovered after 2 EM iterations.
</tableCaption>
<bodyText confidence="0.999044115384615">
in terms of how they assign probabilities to word
pairs. One naturally expects probabilities for fre-
quently occurring bigrams to increase, while rare
or nonsensical bigrams’ probabilities should de-
crease. For a prior-bigram pair (φ, θ), we evaluate
the change in probabilities by computing the ratio
θhw . For a given history h, we
φhw
sort words w by this ratio rather than by actual bi-
gram probability because the bigrams with the high-
est and lowest probabilities tend to stay the same,
while the changes accounting for differences in PP
scores are more noticeable by considering the ratio.
Due to space limitation, we present one specific
result (FDC prior, fold 1) for the SV500 corpus in
Table 5. Other results are similar. The table lists
a few most frequent unigrams as history words h
(left), and the words w with the smallest (center)
and largest (right) ρhw ratio. Overall we see that our
EM algorithm is forcing meaningless bigrams (e.g.,
“i goodness”, “oh thing”) to have lower probabil-
ities, while assigning higher probabilities to sensi-
ble bigram pairs (e.g., “really good”, “that’s funny”).
Note that the reverse of some common expressions
(e.g., “right that’s”) also rise in probability, suggest-
ing the algorithm detects that the two words are of-
</bodyText>
<table confidence="0.997093">
Corpus Good- Witten- θ*
Absolute Turing Bell
Discount
SV10 6.27 6.28 6.27 6.45
SV25 10.5 10.6 10.5 11.7
SV50 14.8 14.9 14.8 17.7
SV100 20.0 20.1 20.0 25.3
SV250 33.7 33.7 33.8 47.3
SV500 50.9 50.9 51.3 80.1
SumTime 10.8 10.5 10.6 77.7
</table>
<tableCaption confidence="0.987416">
Table 4: Mean test set perplexities for oracle bigram LMs
trained on z1, ... , z&amp;quot;,, and tested on z,,+1, ... , z&amp;quot;,,,. For
reference, the rightmost column lists the best result using
a recovered bigram LM (θp&amp;quot;&apos;&apos; for the first three corpora,
θfdo for the latter four).
</tableCaption>
<bodyText confidence="0.976256">
ten adjacent, but lacks sufficient information to nail
down the exact order.
</bodyText>
<subsectionHeader confidence="0.997416">
4.4 Document Recovery from BOW
</subsectionHeader>
<bodyText confidence="0.999209535714286">
We now play the role of the malicious party men-
tioned in the introduction. We show that, com-
pared to their corresponding prior LMs, our recov-
ered bigram LMs are better able to reconstruct or-
dered documents out of test BOWs xn+1, ... , xm.
We perform document recovery using 1-best A* de-
coding. We use “document accuracy” and “n-gram
accuracy” (for n = 2,3) as our evaluation criteria.
We define document accuracy (Accdoc) as the frac-
tion of documents4 for which the decoded document
matches the true ordered document exactly. Simi-
larly, n-gram accuracy (Accn) measures the fraction
of all n-grams in test documents (with n or more
words) that are recovered correctly.
For this evaluation, we compare models built for
the SV500 corpus. Table 6 presents 5-fold cross val-
idation average test-set accuracies. For each accu-
racy measure, we compare the prior LM with the
recovered bigram LM. It is interesting to note that
the FDC and Perm priors reconstruct documents sur-
prisingly well, but we can always improve them by
running our EM algorithm. The accuracies obtained
by θ are statistically significantly better (via two-
tailed paired t-tests with p &lt; 0.05) than their cor-
responding priors φ in all cases except Accdoc for
θperm versus φperm. Furthermore, θfdc and θperm
are significantly better than all other models in terms
of all three reconstruction accuracy measures.
</bodyText>
<footnote confidence="0.707284">
4We omit single-word documents from these computations.
</footnote>
<equation confidence="0.7520545">
ρhw = P(w|h,θ) =
P(w |h,φ)
</equation>
<page confidence="0.889284">
662
</page>
<table confidence="0.839514857142857">
h w (smallest phw) w (largest phw)
i yep, bye-bye, ah, goodness, ahead mean, guess, think, bet, agree
you let’s, us, fact, such, deal thank, bet, know, can, do
right as, lot, going, years, were that’s, all, right, now, you’re
oh thing, here, could, were, doing boy, really, absolutely, gosh, great
that’s talking, home, haven’t, than, care funny, wonderful, true, interesting, amazing
really now, more, yep, work, you’re sad, neat, not, good, it’s
</table>
<tableCaption confidence="0.999338">
Table 5: The recovered bigram LM θfd, decreases nonsense bigram probabilities (center column) and increases
sensible ones (right column) compared to the prior φfd, on the SV500 corpus.
</tableCaption>
<bodyText confidence="0.996257125">
φperm reconstructions of test BOWs θperm reconstructions of test BOWs
just it’s it’s it’s just going it’s just it’s just it’s going
it’s probably out there else something it’s probably something else out there
the the have but it doesn’t but it doesn’t have the the
you to talking nice was it yes yes it was nice talking to you
that’s well that’s what i’m saying well that’s that’s what i’m saying
a little more here home take a little more take home here
and they can very be nice too and they can be very nice too
i think well that’s great i’m well i think that’s great i’m
but was he because only always but only because he was always
that’s think i don’t i no no i don’t i think that’s
that in and it it’s interesting and it it’s interesting that in
that’s right that’s right that’s difficult right that’s that’s right that’s difficult
so just not quite a year so just not a quite year
well it is a big dog well it is big a dog
so do you have a car so you do have a car
</bodyText>
<tableCaption confidence="0.981562">
Table 7: Subset of SV500 documents that only φp&amp;quot;&apos; or θp&amp;quot;&apos; (but not both) reconstructs correctly. The correct
reconstructions are in bold.
</tableCaption>
<table confidence="0.9994974">
Accdoc Acc2 Acca
X φX θX φX θX φX θX
unigram 11.1 26.8 17.7 32.8 2.7 11.8
fdc 30.2 31.0 33.0 35.1 11.4 13.3
perm 30.9 31.5 32.7 34.8 11.5 13.1
</table>
<tableCaption confidence="0.89289775">
Table 6: Percentage of correctly reconstructed docu-
ments, 2-grams and 3-grams from test BOWs in SV500,
5-fold cross validation. The same trends continue for 4-
grams and 5-grams (not shown).
</tableCaption>
<bodyText confidence="0.999978363636364">
We conclude our experiments with a closer look
at some BOWs for which φ and θ reconstruct dif-
ferently. As a representative example, we compare
θperm to φperm on one test set of the SV500 cor-
pus. There are 92 documents that are correctly re-
constructed by θperm but not by φperm. In con-
trast, only 65 documents are accurately reordered by
φperm but not by θperm. Table 7 presents a subset
of these documents with six or more words. Over-
all, we conclude that the recovered bigram LMs do
a better job at reconstructing BOW documents.
</bodyText>
<sectionHeader confidence="0.999388" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999985777777778">
We presented an algorithm that learns bigram lan-
guage models from BOWs. We plan to: i) inves-
tigate ways to speed up our algorithm; ii) extend
it to trigram and higher-order models; iii) handle
the mixture of BOW documents and some ordered
documents (or phrases) when available; iv) adapt a
general English LM to a special domain using only
BOWs from that domain; and v) explore novel ap-
plications of our algorithm.
</bodyText>
<sectionHeader confidence="0.999333" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.989683428571429">
We thank Ben Liblit for tips on doubled-ended
priority queues, and the anonymous reviewers for
valuable comments. This work is supported in
part by the Wisconsin Alumni Research Founda-
tion, NSF CCF-0353079 and CCF-0728767, and the
Natural Sciences and Engineering Research Council
(NSERC) of Canada.
</bodyText>
<page confidence="0.998749">
663
</page>
<sectionHeader confidence="0.998342" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999868352941177">
Samit Basu and Nigel Boston. 2000. Identifiability of
polynomial systems. Technical report, University of
Illinois at Urbana-Champaign.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language models
in machine translation. In Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
Stanley F. Chen and Joshua T. Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
13(4):359–393.
Kyun-Rak Chong and Sartaj Sahni. 2000.
Correspondence-based data structures for double-
ended priority queues. The ACM Journal of
Experimental Algorithmics, 5(2).
Thomas M. Cover and Joy A. Thomas. 1991. Elements
of Information Theory. John Wiley &amp; Sons, Inc.
Simon King, Chris Bartels, and Jeff Bilmes. 2005.
SVitchboard 1: Small vocabulary tasks from Switch-
board 1. In Interspeech 2005, Lisbon, Portugal.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for M-gram language modeling. In
ICASSP.
Jun S. Liu. 2001. Monte Carlo Strategies in Scientific
Computing. Springer.
Michael Rabbat, M´ario Figueiredo, and Robert Nowak.
2007. Inferring network structure from co-
occurrences. In Advances in Neural Information Pro-
cessing Systems (NIPS) 20.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech and Language, 21(2):373–392.
Ronald Rosenfeld. 2000. Two decades of statistical lan-
guage modeling: Where do we go from here? Pro-
ceedings of the IEEE, 88(8).
Stuart Russell and Peter Norvig. 2003. Artificial Intel-
ligence: A Modern Approach. Prentice-Hall, Engle-
wood Cliffs, NJ, second edition.
Stuart Russell. 1992. Efficient memory-bounded search
methods. In The 10th European Conference on Artifi-
cial Intelligence.
Somayajulu G. Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003. Exploiting a parallel TEXT-DATA corpus.
In Proceedings of Corpus Linguistics, pages 734–743,
Lancaster, U.K.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
Denver, Colorado.
</reference>
<page confidence="0.998129">
664
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.880812">
<title confidence="0.999989">Learning Bigrams from Unigrams</title>
<author confidence="0.999979">Andrew B Michael Robert</author>
<affiliation confidence="0.968156666666667">of Computer Sciences, University of Wisconsin-Madison of Electrical and Computer Engineering, McGill University of Electrical and Computer Engineering, University of Wisconsin-Madison</affiliation>
<email confidence="0.998465">michael.rabbat@mcgill.ca,nowak@ece.wisc.edu</email>
<abstract confidence="0.998347727272727">Traditional wisdom holds that once documents are turned into bag-of-words (unigram count) vectors, word orders are completely lost. We introduce an approach that, perhaps surprisingly, is able to learn a bigram language model from a set of bag-of-words documents. At its heart, our approach is an EM algorithm that seeks a model which maximizes the regularized marginal likelihood of the bagof-words documents. In experiments on seven corpora, we observed that our learned bigram language models: i) achieve better test set perplexity than unigram models trained on the same bag-of-words documents, and are not far behind “oracle bigram models” trained on the corresponding ordered documents; ii) assign higher probabilities to sensible bigram word pairs; iii) improve the accuracy of ordereddocument recovery from a bag-of-words. Our approach opens the door to novel phenomena, for example, privacy leakage from index files.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Samit Basu</author>
<author>Nigel Boston</author>
</authors>
<title>Identifiability of polynomial systems.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>University of Illinois at Urbana-Champaign.</institution>
<contexts>
<context position="7598" citStr="Basu and Boston, 2000" startWordPosition="1305" endWordPosition="1308">, it is easy to verify that the system would have multiple valid solutions: (0.1, 0.2, 0.3), (0.8819, 0.0673, 0.8283), and (0.1180, 0.1841, 0.3030). In general, if p(x) is known from the training BOW corpus, when can we guarantee to uniquely recover the bigram LM θ? This is the question of identifiability, which means the transition matrix θ satisfying (1) exists and is unique. Identifiability is related to finding unique solutions of a system of polynomial equations since (1) is such a system in the elements of θ. The details are beyond the scope of this paper, but applying the technique in (Basu and Boston, 2000), it is possible to show that for W = 3 (including (d)) we need longer documents (|x |&gt; 5) to ensure identifiability. The identifiability of more general cases is still an open research question. X X P(x|θ) = P(z|θ) = zEQ(x) |x| Y zEQ(x) j=2 θzj−1,zj,    657 3 Bigram Recovery Algorithm In practice, the documents are not truly generated from a bigram LM, and the BOW corpus may be small. We therefore seek a maximum likelihood estimate of θ or a regularized version of it. Equivalently, we no longer require equality in (1), but instead find θ that makes the distribution P(x|θ) as close to p</context>
</contexts>
<marker>Basu, Boston, 2000</marker>
<rawString>Samit Basu and Nigel Boston. 2000. Identifiability of polynomial systems. Technical report, University of Illinois at Urbana-Champaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL).</booktitle>
<contexts>
<context position="3251" citStr="Brants et al., 2007" startWordPosition="501" endWordPosition="504">x files include some variant of the BOW. As we demonstrate in this paper, if a malicious party gains access to BOW index files, it can recover more than just unigram frequencies: (i) the malicious party can recover a higher-order LM; (ii) with the LM it may attempt to recover the original ordered document from a BOW by finding the most-likely word permutation2. Future research will quantify the extent to which such a privacy breach is possible in theory, and will find solutions to prevent it. There is a vast literature on language modeling; see, e.g., (Rosenfeld, 2000; Chen and Goodman, 1999; Brants et al., 2007; Roark et al., 2007). How1A trivial bigram LM is a unigram LM which ignores history: P(vlu) = P(v). 2It is possible to use a generic higher-order LM, e.g., a trigram LM trained on standard English corpora, for this purpose. However, incorporating a user-specific LM helps. 656 Proceedings of ACL-08: HLT, pages 656–664, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics ever, to the best of our knowledge, none addresses this reverse direction of learning higher-order LMs from lower-order data. This work is inspired by recent advances in inferring network structure </context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua T Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech and Language,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="3230" citStr="Chen and Goodman, 1999" startWordPosition="497" endWordPosition="500">st processing. Most index files include some variant of the BOW. As we demonstrate in this paper, if a malicious party gains access to BOW index files, it can recover more than just unigram frequencies: (i) the malicious party can recover a higher-order LM; (ii) with the LM it may attempt to recover the original ordered document from a BOW by finding the most-likely word permutation2. Future research will quantify the extent to which such a privacy breach is possible in theory, and will find solutions to prevent it. There is a vast literature on language modeling; see, e.g., (Rosenfeld, 2000; Chen and Goodman, 1999; Brants et al., 2007; Roark et al., 2007). How1A trivial bigram LM is a unigram LM which ignores history: P(vlu) = P(v). 2It is possible to use a generic higher-order LM, e.g., a trigram LM trained on standard English corpora, for this purpose. However, incorporating a user-specific LM helps. 656 Proceedings of ACL-08: HLT, pages 656–664, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics ever, to the best of our knowledge, none addresses this reverse direction of learning higher-order LMs from lower-order data. This work is inspired by recent advances in inferri</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua T. Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 13(4):359–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyun-Rak Chong</author>
<author>Sartaj Sahni</author>
</authors>
<title>Correspondence-based data structures for doubleended priority queues.</title>
<date>2000</date>
<journal>The ACM Journal of Experimental Algorithmics,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="18607" citStr="Chong and Sahni, 2000" startWordPosition="3312" endWordPosition="3315">u) = argmaxvθvu, and v ranges over the word types with non-zero counts in (c1, ... , cW), plus we. It is easy to see that h is an upper bound on the bigram log probability that the remaining words in x can achieve. We use a memory-bounded A∗ search similar to (Russell, 1992), because long BOWs would otherwise quickly exhaust memory. When the priority queue grows larger than the bound, the worst states (in terms of g + h) in the queue are purged. This necessitates a double-ended priority queue that can pop either the maximum or minimum item. We use an efficient implementation with Splay trees (Chong and Sahni, 2000). We continue running A∗ after popping the goal state from its priority queue. Repeating this N times gives the N-best list. 4 Experiments We show experimentally that the proposed algorithm is indeed able to recover reasonable bigram LMs from BOW corpora. We observe: 1. Good test set perplexity: Using test (heldout) set perplexity (PP) as an objective measure of LM quality, we demonstrate that our recovered bigram LMs are much better than naive unigram LMs trained on the same BOW corpus. Furthermore, they are not far behind the “oracle” bigram LMs trained on ordered documents that correspond t</context>
</contexts>
<marker>Chong, Sahni, 2000</marker>
<rawString>Kyun-Rak Chong and Sartaj Sahni. 2000. Correspondence-based data structures for doubleended priority queues. The ACM Journal of Experimental Algorithmics, 5(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons, Inc.</publisher>
<contexts>
<context position="9215" citStr="Cover and Thomas, 1991" startWordPosition="1613" endWordPosition="1616">ularize the problem so that θ prefers to be close to a “prior” bigram LM φ. The prior φ is also estimated from the BOW corpus, and is discussed in Section 3.4. We define the regularizer to be an asymmetric dissimilarity D(φ, θ) between the prior φ and the learned model θ. The dissimilarity is 0 if θ = φ, and increases as they diverge. Specifically, the KLdivergence between two word distributions conditioned on the same history u is KL(φu·kθu·) = EWv=1 φuv log φuv θuv . We define D(φ, θ) to be the average KL-divergence over all histories: EW D(φ, θ) ≡ 1 u=1 KL(φu·kθu·), which is conW vex in θ (Cover and Thomas, 1991). We will use the following derivative later: ∂D(φ, θ)/∂θuv = −φuv/(Wθuv). We are now ready to define the regularized optimization problem for recovering a bigram LM θ from the BOW corpus: max ℓ(θ) − λD(φ, θ) θ subject to θ1 = 1, θ ≥ 0. (2) The weight λ controls the strength of the prior. The constraints ensure that θ is a valid bigram matrix, where 1 is an all-one vector, and the non-negativity constraint is element-wise. Equivalently, (2) can be viewed as the maximum a posteriori (MAP) estimate of θ, with independent Dirichlet priors for each row of θ: p(θu·) = Dir(θu·|αu·) and hyperparamete</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Thomas M. Cover and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley &amp; Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon King</author>
<author>Chris Bartels</author>
<author>Jeff Bilmes</author>
</authors>
<title>SVitchboard 1: Small vocabulary tasks from Switchboard 1. In Interspeech</title>
<date>2005</date>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="20631" citStr="King et al., 2005" startWordPosition="3653" endWordPosition="3656">h in principle our algorithm works on large corpora, the current implementation does not scale well (Table 3 last column). We therefore experimented on seven corpora with relatively small vocabulary sizes, and with short documents (mostly one sentence per document). Table 2 lists statistics describing the corpora. The first six contain text transcripts of conversational telephone speech from the small vocabulary “SVitchboard 1” data set. King et al. constructed each corpus from the full Switchboard corpus, with the restriction that the sentences use only words in the corresponding vocabulary (King et al., 2005). We refer to these corpora as SV10, SV25, SV50, SV100, SV250, and SV500. The seventh corpus comes from the SumTime-Meteo data set (Sripada et al., 2003), which contains real weather forecasts for offshore oil rigs in the North Sea. For the SumTime corpus, we performed sentence segmentation to produce documents, removed punctuation, and replaced numeric digits with a special token. For each of the seven corpora, we perform 5-fold cross validation. We use four folds other than the k-th fold as the training set to train (recover) bigram LMs, and the k-th fold as the test set for evaluation. This</context>
</contexts>
<marker>King, Bartels, Bilmes, 2005</marker>
<rawString>Simon King, Chris Bartels, and Jeff Bilmes. 2005. SVitchboard 1: Small vocabulary tasks from Switchboard 1. In Interspeech 2005, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for M-gram language modeling.</title>
<date>1995</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="16329" citStr="Kneser and Ney, 1995" startWordPosition="2899" endWordPosition="2902">metric bigram, i.e., unigram =� 0unigram uv vu . Prior 2: Frequency of Document Cooccurrence (FDC) φfdc. Let 6(u, v|x) = 1 if words u =� v co-occur (regardless of their counts) in BOW x, and 0 otherwise. In the case u = v, 6(u, u|x) = 1 only if u appears at least twice in x. Let cfdc uv = Eni=1 6(u, v|xi) be the number of BOWs in which u, v co-occur. The FDC prior is φfdc uv a cfdc uv + 1. The co-occurrence counts cfdc are symmetric, but φfdc is asymmetric because of normalization. FDC captures some notion of potential transitions from u to v. FDC is in spirit similar to Kneser-Ney smoothing (Kneser and Ney, 1995) and other methods that accumulate indicators of document membership. Prior 3: Permutation-Based (Perm)φperm. Recall that cuv(z) is the number of times the bigram “uv” appears in an ordered document z. We define cperm uv = Eni=1 Ez∈σ(x,)[cuv(z)], where the expectation is with respect to all unique orderings of each BOW. We make the zero-knowledge assumption of uniform probability over these orderings, rather than P(z|θ) as in the EM algorithm described above. EM will refine these estimates, though, so this is a natural starting point. Space precludes a full discussion, but it can be proven tha</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for M-gram language modeling. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun S Liu</author>
</authors>
<title>Monte Carlo Strategies in Scientific Computing.</title>
<date>2001</date>
<publisher>Springer.</publisher>
<contexts>
<context position="13321" citStr="Liu, 2001" startWordPosition="2353" endWordPosition="2354">te-force enumeration is only feasible for very short documents. Approximation is necessary to handle longer ones. A simple Monte Carlo approximation to (4) would involve sampling ordered documents z1, z2, ... , zL according to zi ti P(zJθ, x), and replacing (4) with PL i=1 cuv(zi)/L. This estimate is unbiased, and the variance decreases linearly with the number of samples, L. However, sampling directly from P is difficult. Instead, we sample ordered documents zi ti R(ziJθ, x) from a distribution R which is easy to generate, and construct an approximation using importance sampling (see, e.g., (Liu, 2001)). With each sample, zi, we associate a weight wi a P(ziJ0,x)/R(ziJ0,x). The importance sampling approximation to (4) is then given by (PL i=1 wicuv(zi))/(PL i=1 wi). Re-weighting the samples in this fashion accounts for the fact that we are using a sampling distribution R which is different the target distribution P, and guarantees that our approximation is asymptotically unbiased. The quality of an importance sampling approximation is closely related to how closely R resembles P; the more similar they are, the better the approximation, in general. Given a BOW x and our current bigram model e</context>
</contexts>
<marker>Liu, 2001</marker>
<rawString>Jun S. Liu. 2001. Monte Carlo Strategies in Scientific Computing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Rabbat</author>
<author>M´ario Figueiredo</author>
<author>Robert Nowak</author>
</authors>
<title>Inferring network structure from cooccurrences.</title>
<date>2007</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS)</booktitle>
<pages>20</pages>
<contexts>
<context position="3956" citStr="Rabbat et al., 2007" startWordPosition="613" endWordPosition="616"> P(vlu) = P(v). 2It is possible to use a generic higher-order LM, e.g., a trigram LM trained on standard English corpora, for this purpose. However, incorporating a user-specific LM helps. 656 Proceedings of ACL-08: HLT, pages 656–664, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics ever, to the best of our knowledge, none addresses this reverse direction of learning higher-order LMs from lower-order data. This work is inspired by recent advances in inferring network structure from co-occurrence data, for example, for computer networks and biological pathways (Rabbat et al., 2007). 2 Problem Formulation and Identifiability We assume that a vocabulary of size W is given. For notational convenience, we include in the vocabulary a special “begin-of-document” symbol (d) which appears only at the beginning of each document. The training corpus consists of a collection of n BOW documents {x1, ... , xn}. Each BOW xi is a vector (xi1, ... , xiW) where xiu is the number of times word u occurs in document i. Our goal is to learn a bigram LM θ, represented as a W xW transition matrix with θu„ = P(v|u), from the BOW corpus. Note P(v|(d)) corresponds to the initial state probabilit</context>
<context position="14586" citStr="Rabbat et al., 2007" startWordPosition="2574" endWordPosition="2577">d document zi) by sequentially drawing words from the bag, with probabilities proportional to θ, but properly normalized to form a distribution based on which words remain in the bag. For example, suppose x = ((d):1, A:2, B:1, C:1). Then we set zi1 = (d), and sample zi2 = A with probability 20(d)A/(20(d)A + 0(d)B + 0(d)C). Similarly, if zi(j_1) = u and if v is in the original BOW that hasn’t been sampled yet, then we set the next word in the ordered document zij equal to v with probability proportional to cv0uv, where cv is the count of v in the remaining BOW. For this scheme, one can verify (Rabbat et al., 2007) that the importance weight corresponding to a sampled ordered document zi = (zi1, ... , zi|x|) is given by wi = Qtx|2 PIx|t 0zt_1zz. In our implementation, the number of importance samples used for a document x is 10JxJ2 if the length of the document JxJ &gt; 8; otherwise we enumerate a(x) without importance sampling. 3.4 Prior Bigram LM φ The quality of the EM solution θ can depend on the prior bigram LM φ. To assess bigram recoverability from a BOW corpus alone, we consider only priors estimated from the corpus itself3. Like θ, φ is a W xW transition matrix with ouv = P (vJu). When 3Priors bas</context>
</contexts>
<marker>Rabbat, Figueiredo, Nowak, 2007</marker>
<rawString>Michael Rabbat, M´ario Figueiredo, and Robert Nowak. 2007. Inferring network structure from cooccurrences. In Advances in Neural Information Processing Systems (NIPS) 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
</authors>
<title>Discriminative n-gram language modeling.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="3272" citStr="Roark et al., 2007" startWordPosition="505" endWordPosition="508">variant of the BOW. As we demonstrate in this paper, if a malicious party gains access to BOW index files, it can recover more than just unigram frequencies: (i) the malicious party can recover a higher-order LM; (ii) with the LM it may attempt to recover the original ordered document from a BOW by finding the most-likely word permutation2. Future research will quantify the extent to which such a privacy breach is possible in theory, and will find solutions to prevent it. There is a vast literature on language modeling; see, e.g., (Rosenfeld, 2000; Chen and Goodman, 1999; Brants et al., 2007; Roark et al., 2007). How1A trivial bigram LM is a unigram LM which ignores history: P(vlu) = P(v). 2It is possible to use a generic higher-order LM, e.g., a trigram LM trained on standard English corpora, for this purpose. However, incorporating a user-specific LM helps. 656 Proceedings of ACL-08: HLT, pages 656–664, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics ever, to the best of our knowledge, none addresses this reverse direction of learning higher-order LMs from lower-order data. This work is inspired by recent advances in inferring network structure from co-occurrence da</context>
</contexts>
<marker>Roark, Saraclar, Collins, 2007</marker>
<rawString>Brian Roark, Murat Saraclar, and Michael Collins. 2007. Discriminative n-gram language modeling. Computer Speech and Language, 21(2):373–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Two decades of statistical language modeling: Where do we go from here?</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>88</volume>
<issue>8</issue>
<contexts>
<context position="3206" citStr="Rosenfeld, 2000" startWordPosition="495" endWordPosition="496">onal files for fast processing. Most index files include some variant of the BOW. As we demonstrate in this paper, if a malicious party gains access to BOW index files, it can recover more than just unigram frequencies: (i) the malicious party can recover a higher-order LM; (ii) with the LM it may attempt to recover the original ordered document from a BOW by finding the most-likely word permutation2. Future research will quantify the extent to which such a privacy breach is possible in theory, and will find solutions to prevent it. There is a vast literature on language modeling; see, e.g., (Rosenfeld, 2000; Chen and Goodman, 1999; Brants et al., 2007; Roark et al., 2007). How1A trivial bigram LM is a unigram LM which ignores history: P(vlu) = P(v). 2It is possible to use a generic higher-order LM, e.g., a trigram LM trained on standard English corpora, for this purpose. However, incorporating a user-specific LM helps. 656 Proceedings of ACL-08: HLT, pages 656–664, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics ever, to the best of our knowledge, none addresses this reverse direction of learning higher-order LMs from lower-order data. This work is inspired by re</context>
</contexts>
<marker>Rosenfeld, 2000</marker>
<rawString>Ronald Rosenfeld. 2000. Two decades of statistical language modeling: Where do we go from here? Proceedings of the IEEE, 88(8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Russell</author>
<author>Peter Norvig</author>
</authors>
<title>Artificial Intelligence: A Modern Approach.</title>
<date>2003</date>
<publisher>Prentice-Hall,</publisher>
<location>Englewood Cliffs, NJ,</location>
<note>second edition.</note>
<contexts>
<context position="17351" citStr="Russell and Norvig, 2003" startWordPosition="3080" endWordPosition="3083">erings, rather than P(z|θ) as in the EM algorithm described above. EM will refine these estimates, though, so this is a natural starting point. Space precludes a full discussion, but it can be proven that cperm uv = En i=1 xiuxiv/|xi| if u =� v, and cperm uu = En i=1 xiu(xiu − 1)/|xi|. Finally, 0perm uv a cperm uv + 1. 3.5 Decoding Ordered Documents from BOWs Given a BOW x and a bigram LM θ, we formulate document recovery as the problem z∗ = argmaxz∈σ(x)P(z|θ). In fact, we can generate the top N candidate ordered documents in terms of P(z|θ). We use A∗ search to construct such an N-best list (Russell and Norvig, 2003). Each state is an ordered, partial document. Its successor states append one more unused word in x to the partial document. The actual cost g from the start (empty document) to a state is the log probability of the partial document under bigram θ. We design a heuristic cost h from the state to the goal (complete document) that is admissible: the idea is to over-use the best bigram history for the remaining words in x. Let the partial document end with word we. Let the count vector for the remaining BOW be (c1, ... , cW). One admissible heuristic is h = log HWu=1 P(u|bh(u); θ)cu, where the “be</context>
</contexts>
<marker>Russell, Norvig, 2003</marker>
<rawString>Stuart Russell and Peter Norvig. 2003. Artificial Intelligence: A Modern Approach. Prentice-Hall, Englewood Cliffs, NJ, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Russell</author>
</authors>
<title>Efficient memory-bounded search methods.</title>
<date>1992</date>
<booktitle>In The 10th European Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="18260" citStr="Russell, 1992" startWordPosition="3255" endWordPosition="3256"> the goal (complete document) that is admissible: the idea is to over-use the best bigram history for the remaining words in x. Let the partial document end with word we. Let the count vector for the remaining BOW be (c1, ... , cW). One admissible heuristic is h = log HWu=1 P(u|bh(u); θ)cu, where the “best history” for word type u is bh(u) = argmaxvθvu, and v ranges over the word types with non-zero counts in (c1, ... , cW), plus we. It is easy to see that h is an upper bound on the bigram log probability that the remaining words in x can achieve. We use a memory-bounded A∗ search similar to (Russell, 1992), because long BOWs would otherwise quickly exhaust memory. When the priority queue grows larger than the bound, the worst states (in terms of g + h) in the queue are purged. This necessitates a double-ended priority queue that can pop either the maximum or minimum item. We use an efficient implementation with Splay trees (Chong and Sahni, 2000). We continue running A∗ after popping the goal state from its priority queue. Repeating this N times gives the N-best list. 4 Experiments We show experimentally that the proposed algorithm is indeed able to recover reasonable bigram LMs from BOW corpor</context>
</contexts>
<marker>Russell, 1992</marker>
<rawString>Stuart Russell. 1992. Efficient memory-bounded search methods. In The 10th European Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Somayajulu G Sripada</author>
<author>Ehud Reiter</author>
<author>Jim Hunter</author>
<author>Jin Yu</author>
</authors>
<title>Exploiting a parallel TEXT-DATA corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of Corpus Linguistics,</booktitle>
<pages>734--743</pages>
<location>Lancaster, U.K.</location>
<contexts>
<context position="20784" citStr="Sripada et al., 2003" startWordPosition="3680" endWordPosition="3683">n seven corpora with relatively small vocabulary sizes, and with short documents (mostly one sentence per document). Table 2 lists statistics describing the corpora. The first six contain text transcripts of conversational telephone speech from the small vocabulary “SVitchboard 1” data set. King et al. constructed each corpus from the full Switchboard corpus, with the restriction that the sentences use only words in the corresponding vocabulary (King et al., 2005). We refer to these corpora as SV10, SV25, SV50, SV100, SV250, and SV500. The seventh corpus comes from the SumTime-Meteo data set (Sripada et al., 2003), which contains real weather forecasts for offshore oil rigs in the North Sea. For the SumTime corpus, we performed sentence segmentation to produce documents, removed punctuation, and replaced numeric digits with a special token. For each of the seven corpora, we perform 5-fold cross validation. We use four folds other than the k-th fold as the training set to train (recover) bigram LMs, and the k-th fold as the test set for evaluation. This is repeated for k = 1... 5, and we report the average cross validation results. We distinguish the original ordered documents (training set z1, ... zn, </context>
</contexts>
<marker>Sripada, Reiter, Hunter, Yu, 2003</marker>
<rawString>Somayajulu G. Sripada, Ehud Reiter, Jim Hunter, and Jin Yu. 2003. Exploiting a parallel TEXT-DATA corpus. In Proceedings of Corpus Linguistics, pages 734–743, Lancaster, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing,</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="23349" citStr="Stolcke, 2002" startWordPosition="4131" endWordPosition="4132">ame corpus are also significant (except between θunigram and θperm for SV500). Finally, we observe that θperm tends to be best for the smaller vocabulary corpora, whereas θfdc dominates as the vocabulary grows. To see how much better we could do if we had ordered training documents z1, ... , zn, we present the mean-test-set-PP of “oracle” bigram LMs in Table 4. We used three smoothing methods to obtain oracle LMs: absolute discounting using a constant of 0.5 (we experimented with other values, but 0.5 worked best), Good-Turing, and interpolated Witten-Bell as implemented in the SRILM toolkit (Stolcke, 2002). We see that our recovered LMs (trained on unordered BOW documents), especially for small vocabulary corpora, are close to the oracles (trained on ordered documents). For the larger datasets, the recovery task is more difficult, and the gap between the oracle LMs and the θ LMs widens. Note that the oracle LMs do much better than the recovered LMs on the SumTime corpus; we suspect the difference is due to the larger vocabulary and significantly higher average sentence length (see Table 2). 4.3 Sensible Bigram Pairs The next set of experiments compares the recovered bigram LMs to their correspo</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing, Denver, Colorado.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>