<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9478225">
Source-side Preordering for Translation using Logistic Regression and
Depth-first Branch-and-Bound Search∗
</title>
<author confidence="0.761427">
Laura Jehl* Adri`a de Gispert$ Mark Hopkins$ William Byrne$
</author>
<affiliation confidence="0.773047">
*Dept. of Computational Linguistics, Heidelberg University. 69120 Heidelberg, Germany
</affiliation>
<email confidence="0.510682">
jehl@cl.uni-heidelberg.de
</email>
<note confidence="0.36767">
$ SDL Research. East Road, Cambridge CB1 1BH, U.K.
</note>
<email confidence="0.99679">
{agispert,mhopkins,bbyrne}@sdl.com
</email>
<sectionHeader confidence="0.994749" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999833380952381">
We present a simple preordering approach
for machine translation based on a feature-
rich logistic regression model to predict
whether two children of the same node
in the source-side parse tree should be
swapped or not. Given the pair-wise chil-
dren regression scores we conduct an effi-
cient depth-first branch-and-bound search
through the space of possible children per-
mutations, avoiding using a cascade of
classifiers or limiting the list of possi-
ble ordering outcomes. We report exper-
iments in translating English to Japanese
and Korean, demonstrating superior per-
formance as (a) the number of crossing
links drops by more than 10% absolute
with respect to other state-of-the-art pre-
ordering approaches, (b) BLEU scores im-
prove on 2.2 points over the baseline with
lexicalised reordering model, and (c) de-
coding can be carried out 80 times faster.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992574982142857">
Source-side preordering for translation is the task
of rearranging the order of a given source sen-
tence so that it best resembles the order of the tar-
get sentence. It is a divide-and-conquer strategy
aiming to decouple long-range word movement
from the core translation task. The main advan-
tage is that translation becomes computationally
cheaper as less word movement needs to be con-
sidered, which results in faster and better transla-
tions, if preordering is done well and efficiently.
Preordering also can facilitate better estimation
of alignment and translation models as the paral-
lel data becomes more monotonically-aligned, and
∗This work was done during an internship of the first au-
thor at SDL Research, Cambridge.
translation gains can be obtained for various sys-
tem architectures, e.g. phrase-based, hierarchical
phrase-based, etc.
For these reasons, preordering has a clear re-
search and commercial interest, as reflected by the
extensive previous work on the subject (see Sec-
tion 2). From these approaches, we are particu-
larly interested in those that (i) involve little or no
human intervention, (ii) require limited computa-
tional resources at runtime, and (iii) make use of
available linguistic analysis tools.
In this paper we propose a novel preordering
approach based on a logistic regression model
trained to predict whether to swap nodes in
the source-side dependency tree. For each pair
of sibling nodes in the tree, the model uses a
feature-rich representation that includes lexical
cues to make relative reordering predictions be-
tween them. Given these predictions, we conduct
a depth-first branch-and-bound search through
the space of possible permutations of all sibling
nodes, using the regression scores to guide the
search. This approach has multiple advantages.
First, the search for permutations is efficient and
does not require specific heuristics or hard limits
for nodes with many children. Second, the inclu-
sion of the regression prediction directly into the
search allows for finer-grained global decisions as
the predictions that the model is more confident
about are preferred. Finally, the use of a single
regression model to handle any number of child
nodes avoids incurring sparsity issues, while al-
lowing the integration of a vast number of features
into the preordering model.
We empirically contrast our proposed method
against another preordering approach based on
automatically-extracted rules when translating En-
glish into Japanese and Korean. We demonstrate
a significant reduction in number of crossing links
of more than 10% absolute, as well as translation
gains of over 2.2 BLEU points over the baseline.
</bodyText>
<page confidence="0.978075">
239
</page>
<note confidence="0.992926">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239–248,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996485">
We also show it outperforms a multi-class classifi-
cation approach and analyse why this is the case.
</bodyText>
<sectionHeader confidence="0.999085" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999964148148149">
One useful way to organize previous preordering
techniques is by how they incorporate linguistic
knowledge.
On one end of the spectrum we find those ap-
proaches that rely on syntactic parsers and hu-
man knowledge, typically encoded via a set of
hand-crafted rules for parse tree rewriting or trans-
formation. Examples of these can be found
for French-English (Xia and McCord, 2004),
German-English (Collins et al., 2005), Chinese-
English (Wang et al., 2007), English-Arabic (Badr
et al., 2009), English-Hindi (Ramanathan et al.,
2009), English-Korean (Hong et al., 2009), and
English-Japanese (Lee et al., 2010; Isozaki et
al., 2010). A generic set of rules for transform-
ing SVO to SOV languages has also been de-
scribed (Xu et al., 2009). The main advantage of
these approaches is that a relatively small set of
good rules can yield significant improvements in
translation. The common criticism they receive is
that they are language-specific.
On the other end of the spectrum, there are pre-
ordering models that rely neither on human knowl-
edge nor on syntactic analysis, but only on word
alignments. One such approach is to form a cas-
cade of two translation systems, where the first
one translates the source to its preordered ver-
sion (Costa-juss`a and Fonollosa, 2006). Alterna-
tively, one can define models that assign a cost to
the relative position of each pair of words in the
sentence, and search for the sequence that opti-
mizes the global score as a linear ordering prob-
lem (Tromble and Eisner, 2009) or as a travel-
ing salesman problem (Visweswariah et al., 2011).
Yet another line of work attempts to automatically
induce a parse tree and a preordering model from
word alignments (DeNero and Uszkoreit, 2011;
Neubig et al., 2012). These approaches are at-
tractive due to their minimal reliance on linguistic
knowledge. However, their findings reveal that the
best performance is obtained when using human-
aligned data which is expensive to create.
Somewhere in the middle of the spectrum are
works that rely on automatic source-language syn-
tactic parses, but no direct human intervention.
Preordering rules can be automatically extracted
from word alignments and constituent trees (Li
et al., 2007; Habash, 2007; Visweswariah et
al., 2010), dependency trees (Genzel, 2010) or
predicate-argument structures (Wu et al., 2011),
or simply part-of-speech sequences (Crego and
Mari˜no, 2006; Rottmann and Vogel, 2007). Rules
are assigned a cost based on Maximum En-
tropy (Li et al., 2007) or Maximum Likelihood es-
timation (Visweswariah et al., 2010), or directly
on their ability to make the training corpus more
monotonic (Genzel, 2010). The latter performs
very well in practice but comes at the cost of a
brute-force extraction heuristic that cannot incor-
porate lexical information. Recently, other ap-
proaches treat ordering the children of a node as
a learning to rank (Yang et al., 2012) or discrimi-
native multi-classification task (Lerner and Petrov,
2013). These are appealing for their use of finer-
grained lexical information, but they struggle to
adequately handle nodes with multiple children.
Our approach is closely related to this latter
work, as we are interested in feature-rich discrim-
inative approaches that automatically learn pre-
ordering rules from source-side dependency trees.
Similarly to Yang et al. (2012) we train a large
discriminative linear model, but rather than model
each child’s position in an ordered list of children,
we model a more natural pair-wise swap / no-swap
preference (like Tromble and Eisner (2009) did at
the word level). We then incorporate this model
into a global, efficient branch-and-bound search
through the space of permutations. In this way, we
avoid an error-prone cascade of classifiers or any
limit on the possible ordering outcomes (Lerner
and Petrov, 2013).
</bodyText>
<subsectionHeader confidence="0.5229605">
3 Preordering using logistic regression
and branch-and-bound search
</subsectionHeader>
<bodyText confidence="0.999728818181818">
Like Genzel (2010), our method starts with depen-
dency parses of source sentences (which we con-
vert to shallow constituent trees; see Figure 1 for
an example), and reorders the source text by per-
muting sibling nodes in the parse tree. For each
non-terminal node, we first apply a logistic regres-
sion model which predicts, for each pair of child
nodes, the probability that they should be swapped
or kept in their original order. We then apply
a depth-first branch-and-bound search to find the
global optimal reordering of children.
</bodyText>
<page confidence="0.985729">
240
</page>
<figure confidence="0.757699">
VB
</figure>
<figureCaption confidence="0.970575666666667">
Figure 1: Shallow constituent tree generated from
the dependency tree. Non-terminal nodes inherit
the tag from the head.
</figureCaption>
<subsectionHeader confidence="0.99959">
3.1 Logistic regression
</subsectionHeader>
<bodyText confidence="0.995704888888889">
We build a regression model that assigns a prob-
ability of swapping any two sibling nodes, a and
b, in the source-side dependency tree. The proba-
bility of swapping them is denoted p(a, b) and the
probability of keeping them in their original order
is 1 − p(a, b). We use LIBLINEAR (Fan et al.,
2008) for training an L1-regularised logistic re-
gression model based on positively and negatively
labelled samples.
</bodyText>
<subsectionHeader confidence="0.976269">
3.1.1 Training data
</subsectionHeader>
<bodyText confidence="0.977419461538461">
We generate training examples for the logistic re-
gression from word-aligned parallel data which is
annotated with source-side dependency trees. For
each non-terminal node, we extract all possible
pairs of child nodes. For each pair, we obtain a
binary label y ∈ {−1, 1} by calculating whether
swapping the two nodes would reduce the number
of crossing alignment links. The crossing score of
having two nodes a and b in the given order is
cs(a, b) := |{(i, j) ∈ Aa × Ab : i &gt; j}|
where Aa and Ab are the target-side positions to
which the words spanned by a and b are aligned.
The label is then given as
</bodyText>
<equation confidence="0.842557666666667">
� 1 , cs(a, b) &gt; cs(b, a)
y(a, b) =
−1 , cs(b, a) &gt; cs(a, b)
</equation>
<bodyText confidence="0.977260153846154">
Instances for which cs(a, b) = cs(b, a) are not
included in the training data. This usually happens
if either Aa or Ab is empty, and in this case the
alignments provide no indication of which order
is better. We also discard any samples from nodes
that have more than 16 children, as these are rare
cases that often result from parsing errors.
Figure 2: Branch-and-bound search: Partial search
space of permutations for a dependency tree node
with four children. The gray node marks a goal
node. For the root node of the tree in Figure 1, the
permutation corresponding to this path (1,4,3,2)
would produce “he the smell stand could”.
</bodyText>
<subsubsectionHeader confidence="0.359072">
3.1.2 Features
</subsubsectionHeader>
<bodyText confidence="0.99415216">
Using a machine learning setup allows us to in-
corporate fine-grained information in the form of
features. We use the following features to charac-
terise pairs of nodes:
l The dependency labels of each node
t The part-of-speech tags of each node.
hw The head words and classes of each node.
lm, rm The left-most and right-most words and classes
of a node.
dst The distances between each node and the head.
gap If there is a gap between nodes, the left-most
and right-most words and classes in the gap.
In order to keep the size of our feature space
manageable, we only consider features which oc-
cur at least 5 times1. For the lexical features, we
use the top 100 vocabulary items from our training
data, and 51 clusters generated by mkcls (Och,
1999). Similarly to previous work (Genzel, 2010;
Yang et al., 2012), we also explore feature con-
junctions. For the tag and label classes, we gen-
erate all possible combinations up to a given size.
For the lexical and distance features, we explicitly
specify conjunctions with the tag and label fea-
tures. Results for various feature configurations
are discussed in Section 4.3.1.
</bodyText>
<subsectionHeader confidence="0.999711">
3.2 Search
</subsectionHeader>
<bodyText confidence="0.9966435">
For each non-terminal node in the source-side de-
pendency tree, we search for the best possible
</bodyText>
<footnote confidence="0.448167">
1Additional feature selection is achieved through L1-
regularisation.
</footnote>
<figure confidence="0.979897347826087">
could
MD
2
he
NN
1
nsubj aux HEAD dobj
the smell
DT NN
stand
VB
3
NN
4
det HEAD
E
1
2 3 4
2 3
1 ...
2
. . .
2
</figure>
<page confidence="0.993225">
241
</page>
<bodyText confidence="0.99770775">
permutation of its children. We define the score
of a permutation 7r as the product of the proba-
bilities of its node pair orientations (swapped or
unswapped):
</bodyText>
<equation confidence="0.96400725">
H
1&lt;i&lt;j&lt;k|7r[i]&gt;7r[j]
11 ·
1&lt;i&lt;j&lt;k|7r[i]&lt;7r[j]
</equation>
<bodyText confidence="0.999815090909091">
Here, we represent a permutation 7r of k nodes
as a k-length sequence containing each integer in
{1, ..., k} exactly once. Define a partial permu-
tation of k nodes as a k&apos; &lt; k length sequence
containing each integer in {1, ..., k} at most once.
We can construct a search space over partial per-
mutations in the natural way (see Figure 2). The
root node represents the empty sequence c and has
score 1. Then, given a search node representing
a k&apos;-length partial permutation 7r&apos;, its successor
nodes are obtained by extending it by one element:
</bodyText>
<equation confidence="0.9989974">
score(7r&apos; · (i)) = score(7r&apos;)
11 · p(i, j)
jEV |i&gt;j
11 · 1 − p(i, j)
jEV |i&lt;j
</equation>
<bodyText confidence="0.999991304347826">
where V = {1, ..., k}\(7r&apos; · (i)) is the set of source
child positions that have not yet been visited. Ob-
serve that the nodes at search depth k correspond
exactly to the set of complete permutations. To
search this space, we employ depth-first branch-
and-bound (Balas and Toth, 1983) as our search
algorithm. The idea of branch-and-bound is to
remember the best scoring goal node found thus
far, abandoning any partial paths that cannot lead
to a better scoring goal node. Algorithm 1 gives
pseudocode for the algorithm2. If the initial bound
(bound0) is set to 0, the search is guaranteed to
find the optimal solution. By raising the bound,
which acts as an under-estimate of the best scor-
ing permutation, search can be faster but possibly
fail to find any solution. All our experiments were
done with bound0 = 0, i.e. exact search, but we
discuss search time in detail and pruning alterna-
tives in Section 4.3.2.
Since we use a logistic regression model and in-
corporate its predictions directly as swap probabil-
ities, our search prefers those permutations with
swaps which the model is more confident about.
</bodyText>
<footnote confidence="0.987809">
2See (Poole and Mackworth, 2010) for more details and a
worked example.
</footnote>
<table confidence="0.881212545454546">
Algorithm 1 Depth-first branch-and-bound
Require: k: maximum sequence length, E: empty sequence,
boundo: initial bound
procedure BNBSEARCH(E, boundo, k)
best path ← L
bound ← boundo
SEARCH((E))
return best path
end procedure
procedure SEARCH(ir0)
if score(ir0) &gt; bound then
if |ir0 |= k then
best path ← (ir0)
bound ← score(ir0)
return
else
for each i E {1, ..., k}\ir0 do
SEARCH(ir0 · (i))
end for
end if
end if
end procedure
</table>
<sectionHeader confidence="0.99398" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.968681">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999990448275862">
We report translation results in English-to-
Japanese/Korean. Our corpora are comprised of
generic parallel data extracted from the web, with
some documents extracted manually and some au-
tomatically crawled. Both have about 6M sentence
pairs and roughly 100M words per language.
The dev and test sets are also generic. Source
sentences were extracted from the web and one
target reference was produced by a bilingual
speaker. These sentences were chosen to evenly
represent 10 domains, including world news,
chat/SMS, health, sport, science, business, and
others. The dev/test sets contain 602/903 sen-
tences and 14K/20K words each. We do English
part-of-speech tagging using SVMTool (Gim´enez
and M`arquez, 2004) and dependency parsing us-
ing MaltParser (Nivre et al., 2007).
For translation experiments, we use a phrase-
based decoder that incorporates a set of standard
features and a hierarchical reordering model (Gal-
ley and Manning, 2008) with weights tuned us-
ing MERT to optimize the character-based BLEU
score on the dev set. The Japanese and Korean lan-
guage models are 5-grams estimated on &gt; 350M
words of generic web text.
For training the logistic regression model, we
automatically align the parallel training data and
intersect the source-to-target and target-to-source
alignments. We reserve a random 5K-sentence
</bodyText>
<equation confidence="0.996285333333333">
score(7r) =
p(i, j)
1 − p(i, j)
</equation>
<page confidence="0.982282">
242
</page>
<table confidence="0.98456625">
approach EJ cs (%) EK cs (%)
rule-based (Genzel, 2010) 61.9 64.2
multi-class 65.2 -
df-bnb 51.4 51.8
</table>
<tableCaption confidence="0.996397">
Table 1: Percentage of the original crossing score
</tableCaption>
<bodyText confidence="0.956266454545455">
on the heldout set, obtained after applying each
preordering approach in English-Japanese (EJ,
left) and Korean (EK, right). Lower is better.
subset for intrinsic evaluation of preordering, and
use the remainder for model parameter estimation.
We evaluate our preordering approach with lo-
gistic regression and depth-first branch-and-bound
search (in short, ‘df-bnb’) both in terms of reorder-
ing via crossing score reduction on the heldout set,
and in terms of translation quality as measured by
character-based BLEU on the test set.
</bodyText>
<subsectionHeader confidence="0.994542">
4.2 Preordering baselines
</subsectionHeader>
<bodyText confidence="0.998807666666667">
We contrast our work against two data-driven pre-
ordering approaches. First, we implemented the
rule-based approach of Genzel (2010) and opti-
mised its multiple parameters for our task. We
report only the best results achieved, which corre-
spond to using —100K training sentences for rule
extraction, applying a sliding window width of 3
children, and creating rule sequences of —60 rules.
This approach cannot incorporate lexical features
as that would make the brute-force rule extraction
algorithm unmanageable.
We also implemented a multi-class classifica-
tion setup where we directly predict complete per-
mutations of children nodes using multi-class clas-
sification (Lerner and Petrov, 2013). While this
is straightforward for small numbers of children,
it leads to a very large number of possible per-
mutations for larger sets of children nodes, mak-
ing classification too difficult. While Lerner and
Petrov (2013) use a cascade of classifiers and im-
pose a hard limit on the possible reordering out-
comes to solve this, we follow Genzel’s heuristic:
rather than looking at the complete set of children,
we apply a sliding window of size 3 starting from
the left, and make classification/reordering deci-
sions for each window separately. Since the win-
dows overlap, decisions made for the first window
affect the order of nodes in the second window,
etc. We address this by soliciting decisions from
the classifier on the fly as we preorder. One lim-
</bodyText>
<figureCaption confidence="0.950903">
Figure 3: Crossing scores and classification accu-
racy improve with training data size.
</figureCaption>
<bodyText confidence="0.99991">
itation of this approach is that it is able to move
children only within the window. We try to rem-
edy this by applying the method iteratively, each
time re-training the classifier on the preordered
data from the previous run.
</bodyText>
<subsectionHeader confidence="0.999711">
4.3 Crossing score
</subsectionHeader>
<bodyText confidence="0.956902884615385">
We now report contrastive results in the intrin-
sic preordering task, as measured by the num-
ber of crossing links (Genzel, 2010; Yang et al.,
2012) on the 5K held-out set. Without preorder-
ing, there is an average of 22.2 crossing links in
English-Japanese and 20.2 in English-Korean. Ta-
ble 1 shows what percentage of these links re-
main after applying each preordering approach to
the data. We find that the ‘df-bnb’ method out-
performs the other approaches in both language
pairs, achieving more than 10 additional percent-
age points reduction over the rule-based approach.
Interestingly, the multi-class approach is not able
to match the rule-based approach despite using ad-
ditional lexical cues. We hypothesise that this is
due to the sliding window heuristic, which causes
a mismatch in train-test conditions: while samples
are not independent of each other at test time due
to window overlaps, they are considered to be so
when training the classifier.
4.3.1 Impact of training size and feature
configuration
We now report the effects of feature configura-
tion and training data size for the English-Japanese
case. We assess our ‘df-bnb’ approach in terms of
the classification accuracy of the trained logistic
</bodyText>
<page confidence="0.997712">
243
</page>
<table confidence="0.999074">
features used acc (%) cs (%)
l,t,hw,lm,rm,dst,gap 82.43 51.3
l,t,hw,lm,rm,dst 82.44 51.4
l,t,hw,lm,rm 82.32 53.1
l,t,hw 82.02 55
l,t 81.07 58.4
</table>
<tableCaption confidence="0.856014">
Table 2: Ablation tests showing crossing scores
and classification accuracy as features are re-
moved. All models were trained on 8M samples.
</tableCaption>
<bodyText confidence="0.99204346875">
regression model (using it to predict ±1 labels in
the held-out set) and by the percentage of crossing
alignment links reduced by preordering.
Figure 3 shows the performance of the logistic
regression model over different training set sizes,
extracted from the training corpus as described in
Section 3. We observe a constant increase in pre-
diction accuracy, mirrored by a steady decrease in
crossing score. However, gains are less for more
than 8M training examples. Note that a small vari-
ation in accuracy can produce a large variation in
crossing score if two nodes are swapped which
have a large number of crossing alignments.
Table 2 shows an ablation test for various fea-
ture configurations. We start with all features, in-
cluding head word and class (hw), left-most and
right-most word in each node’s span (lm, rm), each
node’s distance to the head (dst), and left-most
and right-most word of the gap between nodes
(gap). We then proceed by removing features to
end with only label and tag features (l,t), as in
Genzel (2010). For each configuration, we gener-
ated all tag- and label- combinations of size 2. We
then specified combinations between tag and label
and all other features. For the lexical features we
always used conjunctions of the word itself, and its
class. Class information is included for all words,
not just those in the top 100 vocabulary. Table 2
shows that lexical and distance feature groups con-
tribute to prediction accuracy and crossing score,
except for the gap features, which we omit from
further experiments.
</bodyText>
<subsectionHeader confidence="0.845221">
4.3.2 Run time
</subsectionHeader>
<bodyText confidence="0.973587025641026">
We now demonstrate the efficiency of branch-and-
bound search for the problem of finding the opti-
mum permutation of n children at runtime. Even
though in the worst case the search could ex-
plore all n! permutations, making it prohibitive for
Figure 4: Average number of nodes explored in
branch-and-bound search by number of children.
nodes with many children, in practice this does
not happen. Many low-scoring paths are discarded
early by branch-and-bound search so that the opti-
mal solution can be found quickly. The top curve
in Figure 4 shows the average number of nodes
explored in searches run on our validation set (5K
sentences) as a function of the number of children.
All instances are far from the worst case3.
In our experiments, the time needed to conduct
exact search (boundo = 0) was not a problem ex-
cept for a few bad cases (nodes with more than 16
children), which we simply chose not to preorder;
in our data, 90% of the nodes have less than 6 chil-
dren, while only 0.9% have 10 children or more, so
this omission does not affect performance notice-
ably. We verified this on our held-out set, by car-
rying out exhaustive searches. We found that not
preordering nodes with 16 children did not worsen
the crossing score. In fact, setting a harsher limit
of 10 nodes would still produce a crossing score
of 51.9%, compared to the best score of 51.4%.
There are various ways to speed up the search,
if needed. First, one could impose a hard limit
on the number of explored nodes4. As shown
in Figure 4, a limit of 4K would still allow ex-
act search on average for permutations of up to
11 children, while stopping search early for more
children. We tested this for limits of 1K/4K nodes
and obtained crossing scores of 51.9/51.5%. Al-
ternatively, one could define a higher initial bound;
since the score of a path is a product of proba-
bilities, one would select a threshold probability
</bodyText>
<footnote confidence="0.9868865">
3Note that 12!≈479M nodes, whereas our search finds the
optimal permutation path after exploring &lt;10K nodes.
4As long as the limit exceeds the permutation length, a
solution will always be found as search is depth-first.
</footnote>
<page confidence="0.989924">
244
</page>
<table confidence="0.999661">
d approach −LRM Δ +LRM Δ
10 baseline 25.39 - 26.62 -
rule-based 25.93 +0.54 27.65 +1.03
multi-class 25.60 +0.21 26.10 −0.52
df-bnb 26.73 +1.34 28.09 +1.47
4 baseline 25.07 - 25.92 -
rule-based 26.35 +1.28 27.54 +1.62
multi-class 25.37 +0.30 26.31 +0.39
df-bnb 26.98 +1.91 28.13 +2.21
</table>
<tableCaption confidence="0.99911">
Table 3: English-Japanese BLEU scores with var-
</tableCaption>
<bodyText confidence="0.993499066666667">
ious preordering approaches (and improvement
over baseline) under two distortion limits d. Re-
sults reported both excluding and including lexi-
calised reordering model features (LRM).
p and calculate a bound depending on the size n
of the permutation as boundo = p
ples of this would be the lower curves of Figure 4.
The curve labels show the crossing score produced
with each threshold, and in parenthesis the per-
centage of searches that fail to find a solution with
a better score than boundo, in which case children
are left in their original order. As shown, this strat-
egy proves less effective than simply limiting the
number of explored nodes, because the more fre-
quent cases with less children remain unaffected.
</bodyText>
<subsectionHeader confidence="0.999476">
4.4 Translation performance
</subsectionHeader>
<bodyText confidence="0.99901875">
Table 3 reports English-Japanese translation re-
sults for two different values of the distortion limit
d, i.e. the maximum number of source words that
the decoder is allowed to jump during search. We
draw the following conclusions. Firstly, all the
preordering approaches outperform the baseline
and the BLEU score gain they provide increases as
the distortion limit decreases. This is further anal-
ysed in Figure 5, where we report BLEU as a func-
tion of the distortion limit in decoding for both
English-Japanese and English-Korean. This re-
veals the power of preordering as a targeted strat-
egy to obtain high performance at fast decoding
times, since d can be drastically reduced with-
out performance degradation which leads to huge
decoding speed-ups; this is consistent with the
observations in (Xu et al., 2009; Genzel, 2010;
Visweswariah et al., 2011). We also find that with
preordering it is possible to apply harsher pruning
conditions in decoding while still maintaining the
</bodyText>
<figureCaption confidence="0.919493">
Figure 5: BLEU scores as a function of distor-
tion limit in decoder (+LRM case). Top: English-
Japanese. Bottom: English-Korean.
</figureCaption>
<bodyText confidence="0.999969592592592">
exact same performance, achieving further speed-
ups. With preordering, our system is able to de-
code 80 times faster while producing translation
output of the same quality.
Secondly, we observe that the preordering
gains, which are correlated with the crossing score
reductions of Table 1, are largely orthogonal to
the gains obtained when incorporating a lexi-
calised reordering model (LRM). In fact, preorder-
ing gains are slightly larger with LRM, suggest-
ing that this reordering model can be better esti-
mated with preordered text. This echoes the notion
that reordering models are particularly sensitive
to alignment noise (DeNero and Uszkoreit, 2011;
Neubig et al., 2012; Visweswariah et al., 2013),
and that a ‘more monotonic’ training corpus leads
to better translation models.
Finally, ‘df-bnb’ outperforms all other preorder-
ing approaches, and achieves an extra 0.5–0.8
BLEU over the rule-based one even at zero distor-
tion limit. This is consistent with the substantial
crossing score reductions reported in Section 4.3.
We argue that these improvements are due to
the usage of lexical features to facilitate finer-
grained ordering decisions, and to our better
search through the children permutation space
which is not restricted by sliding windows, does
</bodyText>
<equation confidence="0.947081666666667">
n·(n−1)
2
. Exam-
</equation>
<page confidence="0.993976">
245
</page>
<table confidence="0.557425304347826">
reference [1J --M—/]Barlow [2-19R+]the smell [3M]endure [4&apos;i�K�-(R]could [50#f]hoped [60]
source [1Barlow] [5hoped] he [4could] [3stand] [2the smell] [6.]
preordered [1Barlow] he [2the smell] [3stand] [4could] [5hoped] [6.]
Example 1
Example 3
reference [1 _1{{7.a,.u�u}}.440)]my own [21417 ]experience [3+:1 L�&amp;]in , [4M— J\JL J �]Rosa Parks [5 L� 7]called [6AA.]black
[7kll l�]woman, [S K Q] [ ti� fJ&apos; C(ti� fJ&apos; C ] [ J �O)] [ Kp ]
��II�� .00w day 9 so )mehow {� 10 bus of 11 qN IC back seat in
[12�L=6]sit H 7 + [13 aO ` 6 ]told being [14 �- ( +]of [15 7 /v;T / 1]was fed up with o
source [3In] [1my own] [2experience] , a [6black] [7woman] [5named] [4Rosa Parks] [14was just tired] [Sone day]
[14of] [13being told] [12to sit] [11in the back] [10of the bus] .
rule-based [1my own] [2experience] [3In] [14was just tired] [13being told] [10the bus of] [11the back in] [12sit to] [14of]
[Sone day] , [6a black] [7woman] [4Rosa Parks] [5named] .
df-bnb [1my own] [2experience] [3In] , [5named] [6a black] [7woman] [4Rosa Parks] [10the bus of] [11the back in]
[12sit to] [13told being] [14of] [Sone day] [14was just tired] .
Example 2
reference [1tf!/]we1[21#b&apos;J]quite [394le�Y]Xi’an [4Ifi�]like [5+]to [6*J&gt;l,f]come have o
source [1we] [6have come] [5to] [2quite] [4like] [1xi’an] .
rule-based [1we] [2quite] [4like] [3xi’an] [5to] [6come have] .
df-bnb [1we] have [2quite] [3xi’an] [4like] [5to] [6come] .
baseline tf!/Rb&apos;*J94le(n4&apos;I .
rule-based tf!/b,*J.H7+94le+3K&gt;10
df-bnb tf!/b *J94le�Ifi�+*K.
</table>
<tableCaption confidence="0.997212">
Table 4: Examples from our test data illustrating the differences between the preordering approaches.
</tableCaption>
<bodyText confidence="0.993783">
not depend heavily on getting the right decision
in a multi-class scenario, and which incorporates
regression to carry out a score-driven search.
</bodyText>
<subsectionHeader confidence="0.980377">
4.5 Analysis
</subsectionHeader>
<bodyText confidence="0.999958371428572">
Table 4 gives three English-Japanese examples
to illustrate the different preordering approaches.
The first, very short, example is preordered cor-
rectly by the rule-based and the df-bnb approach,
as the order of the brackets matches the order of
the Japanese reference.
For longer sentences we see more differences
between approaches, as illustrated by Example 2.
In this case, both approaches succeed at moving
prepositions to the back of the phrase (“my expe-
rience in”, “the bus of”). However, while the df-
bnb approach correctly moves the predicate of the
second clause (“was just tired”) to the back, the
rule-based approach incorrectly moves the subject
(“a black woman named Rosa Parks”) to this posi-
tion - possibly because of the verb “named” which
occurs in the phrase. This could be an indication
that the df-bnb is better suited for more compli-
cated constructions. With the exception of phrases
4 and 8, all other phrases are in the correct order
in the df-bnb reordering. None of the approaches
manage to reorder “a black woman named Rosa
Parks” to the correct order.
Example 3 shows that the translations into
Japanese also reflect preordering quality. The
original source results in “like” being translated
as the main verb (which is incorrectly interpreted
as “to be like, to be equal to”). The rule-based
version correctly moves “have come” to the end,
but fails to swap “xi’an” and “like”, resulting in
“come” being interpreted as a full verb, rather than
an auxiliary. Only the df-bnb version achieves al-
most perfect reordering, resulting in the correct
word choice of *K (to get to, to become) for
“have come to”.5
</bodyText>
<sectionHeader confidence="0.998924" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9428405625">
We have presented a novel preordering approach
that estimates a preference for swapping or not
swapping pairs of children nodes in the source-
side dependency tree by training a feature-rich
logistic regression model. Given the pair-wise
scores, we efficiently search through the space
of possible children permutations using depth-first
branch-and-bound search. The approach is able
to incorporate large numbers of features includ-
ing lexical cues, is efficient at runtime even with
a large number of children, and proves superior to
other state-of-the-art preordering approaches both
in terms of crossing score and translation perfor-
mance.
5This translation is still not perfect, since it uses the wrong
level of politeness, an important distinction in Japanese.
</bodyText>
<page confidence="0.997376">
246
</page>
<sectionHeader confidence="0.983119" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999822403846154">
Ibrahim Badr, Rabih Zbib, and James Glass. 2009.
Syntactic Phrase Reordering for English-to-Arabic
Statistical Machine Translation. In Proceedings of
EACL, pages 86–93, Athens, Greece.
Egon Balas and Paolo Toth. 1983. Branch and
Bound Methods for the Traveling Salesman Prob-
lem. Carnegie-Mellon Univ. Pittsburgh PA Manage-
ment Sciences Research Group.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings ofACL, pages 531–540,
Ann Arbor, Michigan.
Marta R. Costa-juss`a and Jos´e A. R. Fonollosa. 2006.
Statistical Machine Reordering. In Proceedings of
EMNLP, pages 70–76, Sydney, Australia.
Josep M. Crego and Jos´e B. Mari˜no. 2006. Integra-
tion of POStag-based Source Reordering into SMT
Decoding by an Extended Search Graph. In Pro-
ceedings of AMTA, pages 29–36, Cambridge, Mas-
sachusetts.
John DeNero and Jakob Uszkoreit. 2011. Inducing
Sentence Structure from Parallel Corpora for Re-
ordering. In Proceedings of EMNLP, pages 193–
203, Edinburgh, Scotland, UK.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871–1874.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of EMNLP, pages 847–
855, Honolulu, Hawaii.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of COLING, pages 376–384,
Beijing, China.
Jes´us Gim´enez and Llu´ıs M`arquez. 2004. SVMTool:
A general POS tagger generator based on Support
Vector Machines. In Proceedings of LREC, Lisbon,
Portugal.
Nizar Habash. 2007. Syntactic Preprocessing for Sta-
tistical Machine Translation. In Proceedings of MT-
Summit, pages 215–222, Copenhagen, Denmark.
Gumwon Hong, Seung-Wook Lee, and Hae-Chang
Rim. 2009. Bridging Morpho-Syntactic Gap be-
tween Source and Target Sentences for English-
Korean Statistical Machine Translation. In Proceed-
ings of ACL-IJCNLP, pages 233–236, Suntec, Sin-
gapore.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A Simple Re-
ordering Rule for SOV Languages. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 244–251, Up-
psala, Sweden.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.
2010. Constituent Reordering and Syntax Models
for English-to-Japanese Statistical Machine Trans-
lation. In Proceedings of COLING, pages 626–634,
Beijing, China.
Uri Lerner and Slav Petrov. 2013. Source-Side Clas-
sifier Preordering for Machine Translation. In Pro-
ceedings of EMNLP, Seattle, USA.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A Probabilistic
Approach to Syntax-based Reordering for Statistical
Machine Translation. In Proceedings of ACL, pages
720–727, Prague, Czech Republic.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a Discriminative Parser to Optimize
Machine Translation Reordering. In Proceedings of
EMNLP-CoNLL, pages 843–853, Jeju Island, Korea.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95–135.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
EACL, pages 71–76, Bergen, Norway.
David L. Poole and Alan K. Mackworth. 2010. Ar-
tificial Intelligence: Foundations of Computational
Agents. Cambridge University Press. Full text on-
line at http://artint.info.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and Morphology: Addressing the crux
of the fluency problem in English-Hindi SMT. In
Proceedings of ACL-IJCNLP, pages 800–808, Sun-
tec, Singapore.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
TMI, pages 171–180, Sk¨ovde, Sweden.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of EMNLP, pages 1007–1016, Singapore.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with auto-
matically derived rules for improved statistical ma-
chine translation. In Proceedings of COLING, pages
1119–1127, Beijing, China.
</reference>
<page confidence="0.96772">
247
</page>
<reference confidence="0.999197606060606">
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for
improved machine translation. In Proceedings of
EMNLP, pages 486–496, Edinburgh, United King-
dom.
Karthik Visweswariah, Mitesh M. Khapra, and Anan-
thakrishnan Ramanathan. 2013. Cut the noise: Mu-
tually reinforcing reordering and alignments for im-
proved machine translation. In Proceedings of ACL,
pages 1275–1284, Sofia, Bulgaria.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese Syntactic Reordering for Statistical
Machine Translation. In Proceedings of EMNLP-
CoNLL, pages 737–745, Prague, Czech Republic.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
Pre-ordering Rules from Predicate-Argument Struc-
tures. In Proceedings of IJCNLP, pages 29–37, Chi-
ang Mai, Thailand.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of COLING,
Geneva, Switzerland.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a Dependency Parser to Improve
SMT for Subject-Object-Verb Languages. In Pro-
ceedings of HTL-NAACL, pages 245–253, Boulder,
Colorado.
Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.
2012. A ranking-based approach to word reordering
for statistical machine translation. In Proceedings of
ACL, pages 912–920, Jeju Island, Korea.
</reference>
<page confidence="0.996999">
248
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.528275">
<title confidence="0.995074">Source-side Preordering for Translation using Logistic Regression</title>
<author confidence="0.77828">Branch-and-Bound</author>
<email confidence="0.793744">de</email>
<note confidence="0.832532">Dept. of Computational Linguistics, Heidelberg University. 69120 Heidelberg, Research. East Road, Cambridge CB1 1BH,</note>
<abstract confidence="0.996421">We present a simple preordering approach for machine translation based on a featurerich logistic regression model to predict whether two children of the same node in the source-side parse tree should be swapped or not. Given the pair-wise children regression scores we conduct an efficient depth-first branch-and-bound search through the space of possible children permutations, avoiding using a cascade of classifiers or limiting the list of possible ordering outcomes. We report experiments in translating English to Japanese and Korean, demonstrating superior performance as (a) the number of crossing links drops by more than 10% absolute with respect to other state-of-the-art preordering approaches, (b) BLEU scores improve on 2.2 points over the baseline with lexicalised reordering model, and (c) decoding can be carried out 80 times faster.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ibrahim Badr</author>
<author>Rabih Zbib</author>
<author>James Glass</author>
</authors>
<title>Syntactic Phrase Reordering for English-to-Arabic Statistical Machine Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>86--93</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="4713" citStr="Badr et al., 2009" startWordPosition="712" endWordPosition="715">l Linguistics We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word align</context>
</contexts>
<marker>Badr, Zbib, Glass, 2009</marker>
<rawString>Ibrahim Badr, Rabih Zbib, and James Glass. 2009. Syntactic Phrase Reordering for English-to-Arabic Statistical Machine Translation. In Proceedings of EACL, pages 86–93, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egon Balas</author>
<author>Paolo Toth</author>
</authors>
<title>Branch and Bound Methods for the Traveling Salesman Problem. Carnegie-Mellon Univ.</title>
<date>1983</date>
<institution>PA Management Sciences Research Group.</institution>
<location>Pittsburgh</location>
<contexts>
<context position="13046" citStr="Balas and Toth, 1983" startWordPosition="2138" endWordPosition="2141"> partial permutations in the natural way (see Figure 2). The root node represents the empty sequence c and has score 1. Then, given a search node representing a k&apos;-length partial permutation 7r&apos;, its successor nodes are obtained by extending it by one element: score(7r&apos; · (i)) = score(7r&apos;) 11 · p(i, j) jEV |i&gt;j 11 · 1 − p(i, j) jEV |i&lt;j where V = {1, ..., k}\(7r&apos; · (i)) is the set of source child positions that have not yet been visited. Observe that the nodes at search depth k correspond exactly to the set of complete permutations. To search this space, we employ depth-first branchand-bound (Balas and Toth, 1983) as our search algorithm. The idea of branch-and-bound is to remember the best scoring goal node found thus far, abandoning any partial paths that cannot lead to a better scoring goal node. Algorithm 1 gives pseudocode for the algorithm2. If the initial bound (bound0) is set to 0, the search is guaranteed to find the optimal solution. By raising the bound, which acts as an under-estimate of the best scoring permutation, search can be faster but possibly fail to find any solution. All our experiments were done with bound0 = 0, i.e. exact search, but we discuss search time in detail and pruning </context>
</contexts>
<marker>Balas, Toth, 1983</marker>
<rawString>Egon Balas and Paolo Toth. 1983. Branch and Bound Methods for the Traveling Salesman Problem. Carnegie-Mellon Univ. Pittsburgh PA Management Sciences Research Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause Restructuring for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>531--540</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="4641" citStr="Collins et al., 2005" startWordPosition="701" endWordPosition="704">, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neit</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause Restructuring for Statistical Machine Translation. In Proceedings ofACL, pages 531–540, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta R Costa-juss`a</author>
<author>Jos´e A R Fonollosa</author>
</authors>
<title>Statistical Machine Reordering.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>70--76</pages>
<location>Sydney, Australia.</location>
<marker>Costa-juss`a, Fonollosa, 2006</marker>
<rawString>Marta R. Costa-juss`a and Jos´e A. R. Fonollosa. 2006. Statistical Machine Reordering. In Proceedings of EMNLP, pages 70–76, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Jos´e B Mari˜no</author>
</authors>
<title>Integration of POStag-based Source Reordering into SMT Decoding by an Extended Search Graph.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<pages>29--36</pages>
<location>Cambridge, Massachusetts.</location>
<marker>Crego, Mari˜no, 2006</marker>
<rawString>Josep M. Crego and Jos´e B. Mari˜no. 2006. Integration of POStag-based Source Reordering into SMT Decoding by an Extended Search Graph. In Proceedings of AMTA, pages 29–36, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Inducing Sentence Structure from Parallel Corpora for Reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>193--203</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="5934" citStr="DeNero and Uszkoreit, 2011" startWordPosition="918" endWordPosition="921">alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source to its preordered version (Costa-juss`a and Fonollosa, 2006). Alternatively, one can define models that assign a cost to the relative position of each pair of words in the sentence, and search for the sequence that optimizes the global score as a linear ordering problem (Tromble and Eisner, 2009) or as a traveling salesman problem (Visweswariah et al., 2011). Yet another line of work attempts to automatically induce a parse tree and a preordering model from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering rules can be automatically extracted from word alignments and constituent trees (Li et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu</context>
<context position="26248" citStr="DeNero and Uszkoreit, 2011" startWordPosition="4305" endWordPosition="4308">achieving further speedups. With preordering, our system is able to decode 80 times faster while producing translation output of the same quality. Secondly, we observe that the preordering gains, which are correlated with the crossing score reductions of Table 1, are largely orthogonal to the gains obtained when incorporating a lexicalised reordering model (LRM). In fact, preordering gains are slightly larger with LRM, suggesting that this reordering model can be better estimated with preordered text. This echoes the notion that reordering models are particularly sensitive to alignment noise (DeNero and Uszkoreit, 2011; Neubig et al., 2012; Visweswariah et al., 2013), and that a ‘more monotonic’ training corpus leads to better translation models. Finally, ‘df-bnb’ outperforms all other preordering approaches, and achieves an extra 0.5–0.8 BLEU over the rule-based one even at zero distortion limit. This is consistent with the substantial crossing score reductions reported in Section 4.3. We argue that these improvements are due to the usage of lexical features to facilitate finergrained ordering decisions, and to our better search through the children permutation space which is not restricted by sliding wind</context>
</contexts>
<marker>DeNero, Uszkoreit, 2011</marker>
<rawString>John DeNero and Jakob Uszkoreit. 2011. Inducing Sentence Structure from Parallel Corpora for Reordering. In Proceedings of EMNLP, pages 193– 203, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="9084" citStr="Fan et al., 2008" startWordPosition="1420" endWordPosition="1423">ty that they should be swapped or kept in their original order. We then apply a depth-first branch-and-bound search to find the global optimal reordering of children. 240 VB Figure 1: Shallow constituent tree generated from the dependency tree. Non-terminal nodes inherit the tag from the head. 3.1 Logistic regression We build a regression model that assigns a probability of swapping any two sibling nodes, a and b, in the source-side dependency tree. The probability of swapping them is denoted p(a, b) and the probability of keeping them in their original order is 1 − p(a, b). We use LIBLINEAR (Fan et al., 2008) for training an L1-regularised logistic regression model based on positively and negatively labelled samples. 3.1.1 Training data We generate training examples for the logistic regression from word-aligned parallel data which is annotated with source-side dependency trees. For each non-terminal node, we extract all possible pairs of child nodes. For each pair, we obtain a binary label y ∈ {−1, 1} by calculating whether swapping the two nodes would reduce the number of crossing alignment links. The crossing score of having two nodes a and b in the given order is cs(a, b) := |{(i, j) ∈ Aa × Ab </context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A Simple and Effective Hierarchical Phrase Reordering Model.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>847--855</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="15333" citStr="Galley and Manning, 2008" startWordPosition="2508" endWordPosition="2512">ic. Source sentences were extracted from the web and one target reference was produced by a bilingual speaker. These sentences were chosen to evenly represent 10 domains, including world news, chat/SMS, health, sport, science, business, and others. The dev/test sets contain 602/903 sentences and 14K/20K words each. We do English part-of-speech tagging using SVMTool (Gim´enez and M`arquez, 2004) and dependency parsing using MaltParser (Nivre et al., 2007). For translation experiments, we use a phrasebased decoder that incorporates a set of standard features and a hierarchical reordering model (Galley and Manning, 2008) with weights tuned using MERT to optimize the character-based BLEU score on the dev set. The Japanese and Korean language models are 5-grams estimated on &gt; 350M words of generic web text. For training the logistic regression model, we automatically align the parallel training data and intersect the source-to-target and target-to-source alignments. We reserve a random 5K-sentence score(7r) = p(i, j) 1 − p(i, j) 242 approach EJ cs (%) EK cs (%) rule-based (Genzel, 2010) 61.9 64.2 multi-class 65.2 - df-bnb 51.4 51.8 Table 1: Percentage of the original crossing score on the heldout set, obtained </context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Model. In Proceedings of EMNLP, pages 847– 855, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
</authors>
<title>Automatically learning sourceside reordering rules for large scale machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>376--384</pages>
<location>Beijing, China.</location>
<contexts>
<context position="6497" citStr="Genzel, 2010" startWordPosition="1005" endWordPosition="1006">l from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering rules can be automatically extracted from word alignments and constituent trees (Li et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang </context>
<context position="8121" citStr="Genzel (2010)" startWordPosition="1257" endWordPosition="1258"> trees. Similarly to Yang et al. (2012) we train a large discriminative linear model, but rather than model each child’s position in an ordered list of children, we model a more natural pair-wise swap / no-swap preference (like Tromble and Eisner (2009) did at the word level). We then incorporate this model into a global, efficient branch-and-bound search through the space of permutations. In this way, we avoid an error-prone cascade of classifiers or any limit on the possible ordering outcomes (Lerner and Petrov, 2013). 3 Preordering using logistic regression and branch-and-bound search Like Genzel (2010), our method starts with dependency parses of source sentences (which we convert to shallow constituent trees; see Figure 1 for an example), and reorders the source text by permuting sibling nodes in the parse tree. For each non-terminal node, we first apply a logistic regression model which predicts, for each pair of child nodes, the probability that they should be swapped or kept in their original order. We then apply a depth-first branch-and-bound search to find the global optimal reordering of children. 240 VB Figure 1: Shallow constituent tree generated from the dependency tree. Non-termi</context>
<context position="11318" citStr="Genzel, 2010" startWordPosition="1824" endWordPosition="1825">ndency labels of each node t The part-of-speech tags of each node. hw The head words and classes of each node. lm, rm The left-most and right-most words and classes of a node. dst The distances between each node and the head. gap If there is a gap between nodes, the left-most and right-most words and classes in the gap. In order to keep the size of our feature space manageable, we only consider features which occur at least 5 times1. For the lexical features, we use the top 100 vocabulary items from our training data, and 51 clusters generated by mkcls (Och, 1999). Similarly to previous work (Genzel, 2010; Yang et al., 2012), we also explore feature conjunctions. For the tag and label classes, we generate all possible combinations up to a given size. For the lexical and distance features, we explicitly specify conjunctions with the tag and label features. Results for various feature configurations are discussed in Section 4.3.1. 3.2 Search For each non-terminal node in the source-side dependency tree, we search for the best possible 1Additional feature selection is achieved through L1- regularisation. could MD 2 he NN 1 nsubj aux HEAD dobj the smell DT NN stand VB 3 NN 4 det HEAD E 1 2 3 4 2 3</context>
<context position="15806" citStr="Genzel, 2010" startWordPosition="2589" endWordPosition="2590">ts, we use a phrasebased decoder that incorporates a set of standard features and a hierarchical reordering model (Galley and Manning, 2008) with weights tuned using MERT to optimize the character-based BLEU score on the dev set. The Japanese and Korean language models are 5-grams estimated on &gt; 350M words of generic web text. For training the logistic regression model, we automatically align the parallel training data and intersect the source-to-target and target-to-source alignments. We reserve a random 5K-sentence score(7r) = p(i, j) 1 − p(i, j) 242 approach EJ cs (%) EK cs (%) rule-based (Genzel, 2010) 61.9 64.2 multi-class 65.2 - df-bnb 51.4 51.8 Table 1: Percentage of the original crossing score on the heldout set, obtained after applying each preordering approach in English-Japanese (EJ, left) and Korean (EK, right). Lower is better. subset for intrinsic evaluation of preordering, and use the remainder for model parameter estimation. We evaluate our preordering approach with logistic regression and depth-first branch-and-bound search (in short, ‘df-bnb’) both in terms of reordering via crossing score reduction on the heldout set, and in terms of translation quality as measured by charact</context>
<context position="18365" citStr="Genzel, 2010" startWordPosition="2997" endWordPosition="2998">window affect the order of nodes in the second window, etc. We address this by soliciting decisions from the classifier on the fly as we preorder. One limFigure 3: Crossing scores and classification accuracy improve with training data size. itation of this approach is that it is able to move children only within the window. We try to remedy this by applying the method iteratively, each time re-training the classifier on the preordered data from the previous run. 4.3 Crossing score We now report contrastive results in the intrinsic preordering task, as measured by the number of crossing links (Genzel, 2010; Yang et al., 2012) on the 5K held-out set. Without preordering, there is an average of 22.2 crossing links in English-Japanese and 20.2 in English-Korean. Table 1 shows what percentage of these links remain after applying each preordering approach to the data. We find that the ‘df-bnb’ method outperforms the other approaches in both language pairs, achieving more than 10 additional percentage points reduction over the rule-based approach. Interestingly, the multi-class approach is not able to match the rule-based approach despite using additional lexical cues. We hypothesise that this is due</context>
<context position="20769" citStr="Genzel (2010)" startWordPosition="3389" endWordPosition="3390">ains are less for more than 8M training examples. Note that a small variation in accuracy can produce a large variation in crossing score if two nodes are swapped which have a large number of crossing alignments. Table 2 shows an ablation test for various feature configurations. We start with all features, including head word and class (hw), left-most and right-most word in each node’s span (lm, rm), each node’s distance to the head (dst), and left-most and right-most word of the gap between nodes (gap). We then proceed by removing features to end with only label and tag features (l,t), as in Genzel (2010). For each configuration, we generated all tag- and label- combinations of size 2. We then specified combinations between tag and label and all other features. For the lexical features we always used conjunctions of the word itself, and its class. Class information is included for all words, not just those in the top 100 vocabulary. Table 2 shows that lexical and distance feature groups contribute to prediction accuracy and crossing score, except for the gap features, which we omit from further experiments. 4.3.2 Run time We now demonstrate the efficiency of branch-andbound search for the prob</context>
<context position="25316" citStr="Genzel, 2010" startWordPosition="4162" endWordPosition="4163">wing conclusions. Firstly, all the preordering approaches outperform the baseline and the BLEU score gain they provide increases as the distortion limit decreases. This is further analysed in Figure 5, where we report BLEU as a function of the distortion limit in decoding for both English-Japanese and English-Korean. This reveals the power of preordering as a targeted strategy to obtain high performance at fast decoding times, since d can be drastically reduced without performance degradation which leads to huge decoding speed-ups; this is consistent with the observations in (Xu et al., 2009; Genzel, 2010; Visweswariah et al., 2011). We also find that with preordering it is possible to apply harsher pruning conditions in decoding while still maintaining the Figure 5: BLEU scores as a function of distortion limit in decoder (+LRM case). Top: EnglishJapanese. Bottom: English-Korean. exact same performance, achieving further speedups. With preordering, our system is able to decode 80 times faster while producing translation output of the same quality. Secondly, we observe that the preordering gains, which are correlated with the crossing score reductions of Table 1, are largely orthogonal to the </context>
</contexts>
<marker>Genzel, 2010</marker>
<rawString>Dmitriy Genzel. 2010. Automatically learning sourceside reordering rules for large scale machine translation. In Proceedings of COLING, pages 376–384, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>SVMTool: A general POS tagger generator based on Support Vector Machines.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Gim´enez, M`arquez, 2004</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2004. SVMTool: A general POS tagger generator based on Support Vector Machines. In Proceedings of LREC, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Syntactic Preprocessing for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of MTSummit,</booktitle>
<pages>215--222</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="6436" citStr="Habash, 2007" startWordPosition="997" endWordPosition="998"> to automatically induce a parse tree and a preordering model from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering rules can be automatically extracted from word alignments and constituent trees (Li et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat</context>
</contexts>
<marker>Habash, 2007</marker>
<rawString>Nizar Habash. 2007. Syntactic Preprocessing for Statistical Machine Translation. In Proceedings of MTSummit, pages 215–222, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gumwon Hong</author>
<author>Seung-Wook Lee</author>
<author>Hae-Chang Rim</author>
</authors>
<title>Bridging Morpho-Syntactic Gap between Source and Target Sentences for EnglishKorean Statistical Machine Translation.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP,</booktitle>
<pages>233--236</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="4790" citStr="Hong et al., 2009" startWordPosition="722" endWordPosition="725">ch and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, whe</context>
</contexts>
<marker>Hong, Lee, Rim, 2009</marker>
<rawString>Gumwon Hong, Seung-Wook Lee, and Hae-Chang Rim. 2009. Bridging Morpho-Syntactic Gap between Source and Target Sentences for EnglishKorean Statistical Machine Translation. In Proceedings of ACL-IJCNLP, pages 233–236, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Kevin Duh</author>
</authors>
<title>Head Finalization: A Simple Reordering Rule for SOV Languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>244--251</pages>
<location>Uppsala,</location>
<contexts>
<context position="4853" citStr="Isozaki et al., 2010" startWordPosition="732" endWordPosition="735">ul way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source to its preordered versio</context>
</contexts>
<marker>Isozaki, Sudoh, Tsukada, Duh, 2010</marker>
<rawString>Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and Kevin Duh. 2010. Head Finalization: A Simple Reordering Rule for SOV Languages. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 244–251, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
<author>Bing Zhao</author>
<author>Xiaoqian Luo</author>
</authors>
<title>Constituent Reordering and Syntax Models for English-to-Japanese Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>626--634</pages>
<location>Beijing, China.</location>
<contexts>
<context position="4830" citStr="Lee et al., 2010" startWordPosition="728" endWordPosition="731">ated work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source t</context>
</contexts>
<marker>Lee, Zhao, Luo, 2010</marker>
<rawString>Young-Suk Lee, Bing Zhao, and Xiaoqian Luo. 2010. Constituent Reordering and Syntax Models for English-to-Japanese Statistical Machine Translation. In Proceedings of COLING, pages 626–634, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uri Lerner</author>
<author>Slav Petrov</author>
</authors>
<title>Source-Side Classifier Preordering for Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Seattle, USA.</location>
<contexts>
<context position="7180" citStr="Lerner and Petrov, 2013" startWordPosition="1111" endWordPosition="1114">ply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classification task (Lerner and Petrov, 2013). These are appealing for their use of finergrained lexical information, but they struggle to adequately handle nodes with multiple children. Our approach is closely related to this latter work, as we are interested in feature-rich discriminative approaches that automatically learn preordering rules from source-side dependency trees. Similarly to Yang et al. (2012) we train a large discriminative linear model, but rather than model each child’s position in an ordered list of children, we model a more natural pair-wise swap / no-swap preference (like Tromble and Eisner (2009) did at the word le</context>
<context position="17154" citStr="Lerner and Petrov, 2013" startWordPosition="2789" endWordPosition="2792">. First, we implemented the rule-based approach of Genzel (2010) and optimised its multiple parameters for our task. We report only the best results achieved, which correspond to using —100K training sentences for rule extraction, applying a sliding window width of 3 children, and creating rule sequences of —60 rules. This approach cannot incorporate lexical features as that would make the brute-force rule extraction algorithm unmanageable. We also implemented a multi-class classification setup where we directly predict complete permutations of children nodes using multi-class classification (Lerner and Petrov, 2013). While this is straightforward for small numbers of children, it leads to a very large number of possible permutations for larger sets of children nodes, making classification too difficult. While Lerner and Petrov (2013) use a cascade of classifiers and impose a hard limit on the possible reordering outcomes to solve this, we follow Genzel’s heuristic: rather than looking at the complete set of children, we apply a sliding window of size 3 starting from the left, and make classification/reordering decisions for each window separately. Since the windows overlap, decisions made for the first w</context>
</contexts>
<marker>Lerner, Petrov, 2013</marker>
<rawString>Uri Lerner and Slav Petrov. 2013. Source-Side Classifier Preordering for Machine Translation. In Proceedings of EMNLP, Seattle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Ho Li</author>
<author>Minghui Li</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Yi Guan</author>
</authors>
<title>A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>720--727</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6422" citStr="Li et al., 2007" startWordPosition="993" endWordPosition="996"> of work attempts to automatically induce a parse tree and a preordering model from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering rules can be automatically extracted from word alignments and constituent trees (Li et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other ap</context>
</contexts>
<marker>Li, Li, Zhang, Li, Zhou, Guan, 2007</marker>
<rawString>Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou, and Yi Guan. 2007. A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation. In Proceedings of ACL, pages 720–727, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Shinsuke Mori</author>
</authors>
<title>Inducing a Discriminative Parser to Optimize Machine Translation Reordering.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>843--853</pages>
<location>Jeju Island,</location>
<contexts>
<context position="5956" citStr="Neubig et al., 2012" startWordPosition="922" endWordPosition="925">h is to form a cascade of two translation systems, where the first one translates the source to its preordered version (Costa-juss`a and Fonollosa, 2006). Alternatively, one can define models that assign a cost to the relative position of each pair of words in the sentence, and search for the sequence that optimizes the global score as a linear ordering problem (Tromble and Eisner, 2009) or as a traveling salesman problem (Visweswariah et al., 2011). Yet another line of work attempts to automatically induce a parse tree and a preordering model from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering rules can be automatically extracted from word alignments and constituent trees (Li et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or sim</context>
<context position="26269" citStr="Neubig et al., 2012" startWordPosition="4309" endWordPosition="4312">With preordering, our system is able to decode 80 times faster while producing translation output of the same quality. Secondly, we observe that the preordering gains, which are correlated with the crossing score reductions of Table 1, are largely orthogonal to the gains obtained when incorporating a lexicalised reordering model (LRM). In fact, preordering gains are slightly larger with LRM, suggesting that this reordering model can be better estimated with preordered text. This echoes the notion that reordering models are particularly sensitive to alignment noise (DeNero and Uszkoreit, 2011; Neubig et al., 2012; Visweswariah et al., 2013), and that a ‘more monotonic’ training corpus leads to better translation models. Finally, ‘df-bnb’ outperforms all other preordering approaches, and achieves an extra 0.5–0.8 BLEU over the rule-based one even at zero distortion limit. This is consistent with the substantial crossing score reductions reported in Section 4.3. We argue that these improvements are due to the usage of lexical features to facilitate finergrained ordering decisions, and to our better search through the children permutation space which is not restricted by sliding windows, does n·(n−1) 2 .</context>
</contexts>
<marker>Neubig, Watanabe, Mori, 2012</marker>
<rawString>Graham Neubig, Taro Watanabe, and Shinsuke Mori. 2012. Inducing a Discriminative Parser to Optimize Machine Translation Reordering. In Proceedings of EMNLP-CoNLL, pages 843–853, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="15166" citStr="Nivre et al., 2007" startWordPosition="2483" endWordPosition="2486">ts extracted manually and some automatically crawled. Both have about 6M sentence pairs and roughly 100M words per language. The dev and test sets are also generic. Source sentences were extracted from the web and one target reference was produced by a bilingual speaker. These sentences were chosen to evenly represent 10 domains, including world news, chat/SMS, health, sport, science, business, and others. The dev/test sets contain 602/903 sentences and 14K/20K words each. We do English part-of-speech tagging using SVMTool (Gim´enez and M`arquez, 2004) and dependency parsing using MaltParser (Nivre et al., 2007). For translation experiments, we use a phrasebased decoder that incorporates a set of standard features and a hierarchical reordering model (Galley and Manning, 2008) with weights tuned using MERT to optimize the character-based BLEU score on the dev set. The Japanese and Korean language models are 5-grams estimated on &gt; 350M words of generic web text. For training the logistic regression model, we automatically align the parallel training data and intersect the source-to-target and target-to-source alignments. We reserve a random 5K-sentence score(7r) = p(i, j) 1 − p(i, j) 242 approach EJ cs</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>71--76</pages>
<location>Bergen,</location>
<contexts>
<context position="11276" citStr="Och, 1999" startWordPosition="1818" endWordPosition="1819"> characterise pairs of nodes: l The dependency labels of each node t The part-of-speech tags of each node. hw The head words and classes of each node. lm, rm The left-most and right-most words and classes of a node. dst The distances between each node and the head. gap If there is a gap between nodes, the left-most and right-most words and classes in the gap. In order to keep the size of our feature space manageable, we only consider features which occur at least 5 times1. For the lexical features, we use the top 100 vocabulary items from our training data, and 51 clusters generated by mkcls (Och, 1999). Similarly to previous work (Genzel, 2010; Yang et al., 2012), we also explore feature conjunctions. For the tag and label classes, we generate all possible combinations up to a given size. For the lexical and distance features, we explicitly specify conjunctions with the tag and label features. Results for various feature configurations are discussed in Section 4.3.1. 3.2 Search For each non-terminal node in the source-side dependency tree, we search for the best possible 1Additional feature selection is achieved through L1- regularisation. could MD 2 he NN 1 nsubj aux HEAD dobj the smell DT</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz Josef Och. 1999. An efficient method for determining bilingual word classes. In Proceedings of EACL, pages 71–76, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Poole</author>
<author>Alan K Mackworth</author>
</authors>
<title>Artificial Intelligence: Foundations of Computational Agents.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<note>Full text online at http://artint.info.</note>
<contexts>
<context position="13904" citStr="Poole and Mackworth, 2010" startWordPosition="2283" endWordPosition="2286">ithm2. If the initial bound (bound0) is set to 0, the search is guaranteed to find the optimal solution. By raising the bound, which acts as an under-estimate of the best scoring permutation, search can be faster but possibly fail to find any solution. All our experiments were done with bound0 = 0, i.e. exact search, but we discuss search time in detail and pruning alternatives in Section 4.3.2. Since we use a logistic regression model and incorporate its predictions directly as swap probabilities, our search prefers those permutations with swaps which the model is more confident about. 2See (Poole and Mackworth, 2010) for more details and a worked example. Algorithm 1 Depth-first branch-and-bound Require: k: maximum sequence length, E: empty sequence, boundo: initial bound procedure BNBSEARCH(E, boundo, k) best path ← L bound ← boundo SEARCH((E)) return best path end procedure procedure SEARCH(ir0) if score(ir0) &gt; bound then if |ir0 |= k then best path ← (ir0) bound ← score(ir0) return else for each i E {1, ..., k}\ir0 do SEARCH(ir0 · (i)) end for end if end if end procedure 4 Experiments 4.1 Setup We report translation results in English-toJapanese/Korean. Our corpora are comprised of generic parallel dat</context>
</contexts>
<marker>Poole, Mackworth, 2010</marker>
<rawString>David L. Poole and Alan K. Mackworth. 2010. Artificial Intelligence: Foundations of Computational Agents. Cambridge University Press. Full text online at http://artint.info.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananthakrishnan Ramanathan</author>
<author>Hansraj Choudhary</author>
<author>Avishek Ghosh</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Case markers and Morphology: Addressing the crux of the fluency problem in English-Hindi SMT.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP,</booktitle>
<pages>800--808</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="4754" citStr="Ramanathan et al., 2009" startWordPosition="717" endWordPosition="720">rforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cas</context>
</contexts>
<marker>Ramanathan, Choudhary, Ghosh, Bhattacharyya, 2009</marker>
<rawString>Ananthakrishnan Ramanathan, Hansraj Choudhary, Avishek Ghosh, and Pushpak Bhattacharyya. 2009. Case markers and Morphology: Addressing the crux of the fluency problem in English-Hindi SMT. In Proceedings of ACL-IJCNLP, pages 800–808, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kay Rottmann</author>
<author>Stephan Vogel</author>
</authors>
<title>Word Reordering in Statistical Machine Translation with a POS-Based Distortion Model.</title>
<date>2007</date>
<booktitle>In Proceedings of TMI,</booktitle>
<pages>171--180</pages>
<location>Sk¨ovde,</location>
<contexts>
<context position="6636" citStr="Rottmann and Vogel, 2007" startWordPosition="1022" endWordPosition="1025">reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering rules can be automatically extracted from word alignments and constituent trees (Li et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classification task (Lerner and Petrov, 2013). These are appealing for their use of finergrained lexi</context>
</contexts>
<marker>Rottmann, Vogel, 2007</marker>
<rawString>Kay Rottmann and Stephan Vogel. 2007. Word Reordering in Statistical Machine Translation with a POS-Based Distortion Model. In Proceedings of TMI, pages 171–180, Sk¨ovde, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>Learning linear ordering problems for better translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1007--1016</pages>
<contexts>
<context position="5726" citStr="Tromble and Eisner, 2009" startWordPosition="884" endWordPosition="887">mon criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source to its preordered version (Costa-juss`a and Fonollosa, 2006). Alternatively, one can define models that assign a cost to the relative position of each pair of words in the sentence, and search for the sequence that optimizes the global score as a linear ordering problem (Tromble and Eisner, 2009) or as a traveling salesman problem (Visweswariah et al., 2011). Yet another line of work attempts to automatically induce a parse tree and a preordering model from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering </context>
<context position="7761" citStr="Tromble and Eisner (2009)" startWordPosition="1201" endWordPosition="1204">lassification task (Lerner and Petrov, 2013). These are appealing for their use of finergrained lexical information, but they struggle to adequately handle nodes with multiple children. Our approach is closely related to this latter work, as we are interested in feature-rich discriminative approaches that automatically learn preordering rules from source-side dependency trees. Similarly to Yang et al. (2012) we train a large discriminative linear model, but rather than model each child’s position in an ordered list of children, we model a more natural pair-wise swap / no-swap preference (like Tromble and Eisner (2009) did at the word level). We then incorporate this model into a global, efficient branch-and-bound search through the space of permutations. In this way, we avoid an error-prone cascade of classifiers or any limit on the possible ordering outcomes (Lerner and Petrov, 2013). 3 Preordering using logistic regression and branch-and-bound search Like Genzel (2010), our method starts with dependency parses of source sentences (which we convert to shallow constituent trees; see Figure 1 for an example), and reorders the source text by permuting sibling nodes in the parse tree. For each non-terminal no</context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>Roy Tromble and Jason Eisner. 2009. Learning linear ordering problems for better translation. In Proceedings of EMNLP, pages 1007–1016, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
<author>Jiri Navratil</author>
<author>Jeffrey Sorensen</author>
<author>Vijil Chenthamarakshan</author>
<author>Nandakishore Kambhatla</author>
</authors>
<title>Syntax based reordering with automatically derived rules for improved statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1119--1127</pages>
<location>Beijing, China.</location>
<contexts>
<context position="6464" citStr="Visweswariah et al., 2010" startWordPosition="999" endWordPosition="1002">lly induce a parse tree and a preordering model from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering rules can be automatically extracted from word alignments and constituent trees (Li et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a </context>
</contexts>
<marker>Visweswariah, Navratil, Sorensen, Chenthamarakshan, Kambhatla, 2010</marker>
<rawString>Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, Vijil Chenthamarakshan, and Nandakishore Kambhatla. 2010. Syntax based reordering with automatically derived rules for improved statistical machine translation. In Proceedings of COLING, pages 1119–1127, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
<author>Rajakrishnan Rajkumar</author>
<author>Ankur Gandhe</author>
<author>Ananthakrishnan Ramanathan</author>
<author>Jiri Navratil</author>
</authors>
<title>A word reordering model for improved machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>486--496</pages>
<location>Edinburgh, United Kingdom.</location>
<contexts>
<context position="5789" citStr="Visweswariah et al., 2011" startWordPosition="895" endWordPosition="898"> On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source to its preordered version (Costa-juss`a and Fonollosa, 2006). Alternatively, one can define models that assign a cost to the relative position of each pair of words in the sentence, and search for the sequence that optimizes the global score as a linear ordering problem (Tromble and Eisner, 2009) or as a traveling salesman problem (Visweswariah et al., 2011). Yet another line of work attempts to automatically induce a parse tree and a preordering model from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering rules can be automatically extracted from word alignments and c</context>
<context position="25344" citStr="Visweswariah et al., 2011" startWordPosition="4164" endWordPosition="4167">ns. Firstly, all the preordering approaches outperform the baseline and the BLEU score gain they provide increases as the distortion limit decreases. This is further analysed in Figure 5, where we report BLEU as a function of the distortion limit in decoding for both English-Japanese and English-Korean. This reveals the power of preordering as a targeted strategy to obtain high performance at fast decoding times, since d can be drastically reduced without performance degradation which leads to huge decoding speed-ups; this is consistent with the observations in (Xu et al., 2009; Genzel, 2010; Visweswariah et al., 2011). We also find that with preordering it is possible to apply harsher pruning conditions in decoding while still maintaining the Figure 5: BLEU scores as a function of distortion limit in decoder (+LRM case). Top: EnglishJapanese. Bottom: English-Korean. exact same performance, achieving further speedups. With preordering, our system is able to decode 80 times faster while producing translation output of the same quality. Secondly, we observe that the preordering gains, which are correlated with the crossing score reductions of Table 1, are largely orthogonal to the gains obtained when incorpor</context>
</contexts>
<marker>Visweswariah, Rajkumar, Gandhe, Ramanathan, Navratil, 2011</marker>
<rawString>Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur Gandhe, Ananthakrishnan Ramanathan, and Jiri Navratil. 2011. A word reordering model for improved machine translation. In Proceedings of EMNLP, pages 486–496, Edinburgh, United Kingdom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
<author>Mitesh M Khapra</author>
<author>Ananthakrishnan Ramanathan</author>
</authors>
<title>Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1275--1284</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="26297" citStr="Visweswariah et al., 2013" startWordPosition="4313" endWordPosition="4316"> system is able to decode 80 times faster while producing translation output of the same quality. Secondly, we observe that the preordering gains, which are correlated with the crossing score reductions of Table 1, are largely orthogonal to the gains obtained when incorporating a lexicalised reordering model (LRM). In fact, preordering gains are slightly larger with LRM, suggesting that this reordering model can be better estimated with preordered text. This echoes the notion that reordering models are particularly sensitive to alignment noise (DeNero and Uszkoreit, 2011; Neubig et al., 2012; Visweswariah et al., 2013), and that a ‘more monotonic’ training corpus leads to better translation models. Finally, ‘df-bnb’ outperforms all other preordering approaches, and achieves an extra 0.5–0.8 BLEU over the rule-based one even at zero distortion limit. This is consistent with the substantial crossing score reductions reported in Section 4.3. We argue that these improvements are due to the usage of lexical features to facilitate finergrained ordering decisions, and to our better search through the children permutation space which is not restricted by sliding windows, does n·(n−1) 2 . Exam245 reference [1J --M—/</context>
</contexts>
<marker>Visweswariah, Khapra, Ramanathan, 2013</marker>
<rawString>Karthik Visweswariah, Mitesh M. Khapra, and Ananthakrishnan Ramanathan. 2013. Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation. In Proceedings of ACL, pages 1275–1284, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese Syntactic Reordering for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL,</booktitle>
<pages>737--745</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4677" citStr="Wang et al., 2007" startWordPosition="707" endWordPosition="710"> c�2014 Association for Computational Linguistics We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntac</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chinese Syntactic Reordering for Statistical Machine Translation. In Proceedings of EMNLPCoNLL, pages 737–745, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianchao Wu</author>
<author>Katsuhito Sudoh</author>
<author>Kevin Duh</author>
<author>Hajime Tsukada</author>
<author>Masaaki Nagata</author>
</authors>
<title>Extracting Pre-ordering Rules from Predicate-Argument Structures.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>29--37</pages>
<location>Chiang Mai, Thailand.</location>
<contexts>
<context position="6548" citStr="Wu et al., 2011" startWordPosition="1010" endWordPosition="1013">11; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering rules can be automatically extracted from word alignments and constituent trees (Li et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classificatio</context>
</contexts>
<marker>Wu, Sudoh, Duh, Tsukada, Nagata, 2011</marker>
<rawString>Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime Tsukada, and Masaaki Nagata. 2011. Extracting Pre-ordering Rules from Predicate-Argument Structures. In Proceedings of IJCNLP, pages 29–37, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="4602" citStr="Xia and McCord, 2004" startWordPosition="696" endWordPosition="699">omputational Linguistics, pages 239–248, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, ther</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In Proceedings of COLING, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Jaeho Kang</author>
<author>Michael Ringgaard</author>
<author>Franz Och</author>
</authors>
<title>Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages.</title>
<date>2009</date>
<booktitle>In Proceedings of HTL-NAACL,</booktitle>
<pages>245--253</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="4957" citStr="Xu et al., 2009" startWordPosition="753" endWordPosition="756">d of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source to its preordered version (Costa-juss`a and Fonollosa, 2006). Alternatively, one can define models that assign a cost to the rel</context>
<context position="25302" citStr="Xu et al., 2009" startWordPosition="4158" endWordPosition="4161">We draw the following conclusions. Firstly, all the preordering approaches outperform the baseline and the BLEU score gain they provide increases as the distortion limit decreases. This is further analysed in Figure 5, where we report BLEU as a function of the distortion limit in decoding for both English-Japanese and English-Korean. This reveals the power of preordering as a targeted strategy to obtain high performance at fast decoding times, since d can be drastically reduced without performance degradation which leads to huge decoding speed-ups; this is consistent with the observations in (Xu et al., 2009; Genzel, 2010; Visweswariah et al., 2011). We also find that with preordering it is possible to apply harsher pruning conditions in decoding while still maintaining the Figure 5: BLEU scores as a function of distortion limit in decoder (+LRM case). Top: EnglishJapanese. Bottom: English-Korean. exact same performance, achieving further speedups. With preordering, our system is able to decode 80 times faster while producing translation output of the same quality. Secondly, we observe that the preordering gains, which are correlated with the crossing score reductions of Table 1, are largely orth</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a Dependency Parser to Improve SMT for Subject-Object-Verb Languages. In Proceedings of HTL-NAACL, pages 245–253, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Yang</author>
<author>Mu Li</author>
<author>Dongdong Zhang</author>
<author>Nenghai Yu</author>
</authors>
<title>A ranking-based approach to word reordering for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>912--920</pages>
<location>Jeju Island,</location>
<contexts>
<context position="7110" citStr="Yang et al., 2012" startWordPosition="1102" endWordPosition="1105">2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classification task (Lerner and Petrov, 2013). These are appealing for their use of finergrained lexical information, but they struggle to adequately handle nodes with multiple children. Our approach is closely related to this latter work, as we are interested in feature-rich discriminative approaches that automatically learn preordering rules from source-side dependency trees. Similarly to Yang et al. (2012) we train a large discriminative linear model, but rather than model each child’s position in an ordered list of children, we model a more natural pair-wise swap /</context>
<context position="11338" citStr="Yang et al., 2012" startWordPosition="1826" endWordPosition="1829">of each node t The part-of-speech tags of each node. hw The head words and classes of each node. lm, rm The left-most and right-most words and classes of a node. dst The distances between each node and the head. gap If there is a gap between nodes, the left-most and right-most words and classes in the gap. In order to keep the size of our feature space manageable, we only consider features which occur at least 5 times1. For the lexical features, we use the top 100 vocabulary items from our training data, and 51 clusters generated by mkcls (Och, 1999). Similarly to previous work (Genzel, 2010; Yang et al., 2012), we also explore feature conjunctions. For the tag and label classes, we generate all possible combinations up to a given size. For the lexical and distance features, we explicitly specify conjunctions with the tag and label features. Results for various feature configurations are discussed in Section 4.3.1. 3.2 Search For each non-terminal node in the source-side dependency tree, we search for the best possible 1Additional feature selection is achieved through L1- regularisation. could MD 2 he NN 1 nsubj aux HEAD dobj the smell DT NN stand VB 3 NN 4 det HEAD E 1 2 3 4 2 3 1 ... 2 . . . 2 241</context>
<context position="18385" citStr="Yang et al., 2012" startWordPosition="2999" endWordPosition="3002">the order of nodes in the second window, etc. We address this by soliciting decisions from the classifier on the fly as we preorder. One limFigure 3: Crossing scores and classification accuracy improve with training data size. itation of this approach is that it is able to move children only within the window. We try to remedy this by applying the method iteratively, each time re-training the classifier on the preordered data from the previous run. 4.3 Crossing score We now report contrastive results in the intrinsic preordering task, as measured by the number of crossing links (Genzel, 2010; Yang et al., 2012) on the 5K held-out set. Without preordering, there is an average of 22.2 crossing links in English-Japanese and 20.2 in English-Korean. Table 1 shows what percentage of these links remain after applying each preordering approach to the data. We find that the ‘df-bnb’ method outperforms the other approaches in both language pairs, achieving more than 10 additional percentage points reduction over the rule-based approach. Interestingly, the multi-class approach is not able to match the rule-based approach despite using additional lexical cues. We hypothesise that this is due to the sliding wind</context>
</contexts>
<marker>Yang, Li, Zhang, Yu, 2012</marker>
<rawString>Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu. 2012. A ranking-based approach to word reordering for statistical machine translation. In Proceedings of ACL, pages 912–920, Jeju Island, Korea.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>