<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000526">
<title confidence="0.994973">
Chinese Term Extraction Using Different Types of Relevance
</title>
<author confidence="0.999791">
Yuhang Yang1, Tiejun Zhao1, Qin Lu2, Dequan Zheng1 and Hao Yu1
</author>
<affiliation confidence="0.9977795">
1School of Computer Science and Technology,
Harbin Institute of Technology, Harbin 150001, China
</affiliation>
<email confidence="0.984434">
{yhyang,tjzhao,dqzheng,yu}@mtlab.hit.edu.cn
</email>
<affiliation confidence="0.8325995">
2Department of Computing,
The Hong Kong Polytechnic University, Hong Kong, China
</affiliation>
<email confidence="0.995871">
csluqin@comp.polyu.edu.hk
</email>
<sectionHeader confidence="0.995597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976466666667">
This paper presents a new term extraction ap-
proach using relevance between term candi-
dates calculated by a link analysis based
method. Different types of relevance are used
separately or jointly for term verification. The
proposed approach requires no prior domain
knowledge and no adaptation for new domains.
Consequently, the method can be used in any
domain corpus and it is especially useful for
resource-limited domains. Evaluations con-
ducted on two different domains for Chinese
term extraction show significant improve-
ments over existing techniques and also verify
the efficiency and relative domain independent
nature of the approach.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940357142857">
Terms are the lexical units to represent the most
fundamental knowledge of a domain. Term ex-
traction is an essential task in domain knowledge
acquisition which can be used for lexicon update,
domain ontology construction, etc. Term extrac-
tion involves two steps. The first step extracts
candidates by unithood calculation to qualify a
string as a valid term. The second step verifies
them through termhood measures (Kageura and
Umino, 1996) to validate their domain specificity.
Many previous studies are conducted on term
candidate extraction. Other tasks such as named
entity recognition, meaningful word extraction
and unknown word detection, use techniques
similar to that for term candidate extraction. But,
their focuses are not on domain specificity. This
study focuses on the verification of candidates by
termhood calculation.
Relevance between term candidates and docu-
ments is the most popular feature used for term
verification such as TF-IDF (Salton and McGill,
1983; Frank, 1999) and Inter-Domain Entropy
(Chang, 2005), which are all based on the hy-
pothesis that “if a candidate occurs frequently in
a few documents of a domain, it is likely a term”.
Limited distribution information of term candi-
dates in different documents often limits the abil-
ity of such algorithms to distinguish terms from
non-terms. There are also attempts to use prior
domain specific knowledge and annotated cor-
pora for term verification. TV_ConSem (Ji and
Lu, 2007) calculates the percentage of context
words in a domain lexicon using both frequency
information and semantic information. However,
this technique requires a domain lexicon whose
size and quality have great impact on the per-
formance of the algorithm. Some supervised
learning approaches have been applied to pro-
tein/gene name recognition (Zhou et al., 2005)
and Chinese new word identification (Li et al.,
2004) using SVM classifiers (Vapnik, 1995)
which also require large domain corpora and an-
notations. The latest work by Yang (2008) ap-
plied the relevance between term candidates and
sentences by using the link analysis approach
based on the HITS algorithm to achieve better
performance.
In this work, a new feature on the relevance
between different term candidates is integrated
with other features to validate their domain
specificity. The relevance between candidate
terms may be useful to identify domain specific
terms based on two assumptions. First, terms are
more likely to occur with other terms in order to
express domain information. Second, term can-
didates extracted from domain corpora are likely
</bodyText>
<page confidence="0.990716">
213
</page>
<note confidence="0.925916">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 213–216,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999827666666667">
to be domain specific. Previous work by (e.g. Ji
and Lu, 2007) uses similar information by com-
paring the context to an existing large domain
lexicon. In this study, the relevance between
term candidates are iteratively calculated by
graphs using link analysis algorithm to avoid the
dependency on prior domain knowledge.
The rest of the paper is organized as follows.
Section 2 describes the proposed algorithms.
Section 3 explains the experiments and the per-
formance evaluation. Section 4 concludes and
presents the future plans.
</bodyText>
<sectionHeader confidence="0.934545" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.992481625">
This study assumes the availability of term can-
didates since the focus is on term verification by
termhood calculation. Three types of relevance
are first calculated including (1) the term candi-
date relevance, CC; (2) the candidate to sentence
relevance, CS; and the candidates to document
relevance, CD. Terms are then verified by using
different types of relevance.
</bodyText>
<subsectionHeader confidence="0.936687">
2.1 Relevance between Term Candidates
</subsectionHeader>
<bodyText confidence="0.999983222222222">
Based on the assumptions that term candidates
are likely to be used together in order to repre-
sent a particular domain concept, relevance of
term candidates can be represented by graphs in
a domain corpus. In this study, CC is defined as
their co-occurrence in the same sentence of the
domain corpus. For each document, a graph of
term candidates is first constructed. In the graph,
a node is a term candidate. If two term candi-
dates TC1 and TC2 occur in the same sentence,
two directional links between TC1 to TC2 are
given to indicate their mutually related. Candi-
dates with overlapped substrings are not removed
which means long terms can be linked to their
components if the components are also candi-
dates.
After graph construction, the term candidate
relevance, CC, is then iteratively calculated using
the PageRank algorithm (Page et al. 1998) origi-
nally proposed for information retrieval. PageR-
ank assumes that the more a node is connected to
other nodes, it is more likely to be a salient node.
The algorithm assigns the significance score to
each node according to the number of nodes link-
ing to it as well as the significance of the nodes.
The PageRank calculation PR of a node A is
shown as follows:
</bodyText>
<equation confidence="0.962308066666667">
PR(A) = (1 − d) + d( PR(B1) + PR(B2) +...+
C B
( ) C B
( )
1 2
PR(Bt )
C B
( )
t
)
(1)
B1,B2,...,
Bi;
TCi
mentally.
</equation>
<bodyText confidence="0.999826375">
where
Bt are all nodes linked to node A;
C(Bi) is the number of outgoing links from node
d is the factor to avoid loop trap in the
graphic structure. d is set to 0.85 as suggested in
(Page et al., 1998). Initially, all PR weights are
set to 1. The weight score of each node are ob-
tained by (1), iteratively. The significance of
each term candidate in the domain specific cor-
pus is then derived based on the significance of
other candidates it co-occurred with. The CC
weight of term candidate
is given by its PR
value after k iterations, a parameter to be deter-
mined experi
to as CS, is calculated using the
</bodyText>
<figure confidence="0.627727">
(Term
Verification
</figure>
<figureCaption confidence="0.86116625">
algorithm proposed in
(Yang et al., 2008) based on Hyperlink-Induced
Topic Search (HITS) algorithm (Kleinberg,
1997). In
a good hub in the domain
corpus is a sentence that contains many good
authorities; a good authority is a term candidate
that is contained in many good hubs.
</figureCaption>
<bodyText confidence="0.816417909090909">
In
a node p can either be a sentence
or a term candidate. If a term candidate TC is
contained in a sentence Sen of the domain corpus,
there is a directional link from Sen to TC.
then makes use of the relationship be-
tween candidates and sentences via an iterative
process to update CS weight for each TC.
Let
denote the
authority vector and
w(p2)H,...,
denote the hub vector.
and VH are initialized
to (1, 1,..., 1). Given weights
and VH with a
directional link
w(q)A and w(p)H are up-
dated by using the I operation(an in-pointer to a
node) and the O operation(an out-pointer to a
node) shown as follows. The CS weight of term
can
</bodyText>
<equation confidence="0.987415904761905">
TV_HITS
– HITS)
TV_HITS,
TV_HITS,
TV_HITS
VA(w(p1)A,w(p2)A,...,w(pn)A)
VH(w(p1)H,
w(pn)H)
VA
VA
p→q,
didate TCi is given by its w(q)A value after
iteration.
I operation: w(q)A = ∑ w(p)
H
p q E
→ ∈
O operation: w(p)H =
∑∈ w(q)A (3)
p q E
→
</equation>
<subsectionHeader confidence="0.453789">
2.3 Relevance between Term Candidates
and Documents
</subsectionHeader>
<page confidence="0.970037">
214
</page>
<subsectionHeader confidence="0.847367">
2.2 Relevance between Term Candidates
and Sentences
</subsectionHeader>
<bodyText confidence="0.901512666666667">
A domain specific term is more likely to be con-
tained in domain relevant sentences. Relevance
between term candidate and sentences, referred
The relevance between term can
didates and
documents is used in many term extraction algo-
(2)
rithms. The relevance is measured by the TF-IDF
value according to the following equations:
</bodyText>
<equation confidence="0.997913714285714">
TFIDF(TCi) = TF(TCi) ⋅ IDF(TCi) (4)
D
IDF TC = (5)
( ) log( )
i
DF(TC)
i
</equation>
<bodyText confidence="0.999979285714286">
where TF(TCi) is the number of times term can-
didate TCi occurs in the domain corpus, DF(TCi)
is the number of documents in which TCi occurs
at least once, |D |is the total number of docu-
ments in the corpus, IDF(TCi) is the inverse
document frequency which can be calculated
from the document frequency.
</bodyText>
<subsectionHeader confidence="0.999401">
2.4 Combination of Relevance
</subsectionHeader>
<bodyText confidence="0.9999695">
To evaluate the effective of the different types of
relevance, they are combined in different ways in
the evaluation. Term candidates are then ranked
according to the corresponding termhood values
Th(TC) and the top ranked candidates are con-
sidered terms.
For each document Dj in the domain corpus
where a term candidate TCi occurs, there is CCij
weight and a CSij weight. When features CC and
CS are used separately, termhood ThCC(TCi) and
ThCS(TCi) are calculated by averaging CCij and
CSij, respectively. Termhood of different combi-
nations are given in formula (6) to (9). R(TCi)
denotes the ranking position of TCi.
</bodyText>
<equation confidence="0.9944534">
1 1
ThCC+CS+CD (TCi) = +
RCC+CD(TC)
1 1
RCS+CD(TCi
</equation>
<sectionHeader confidence="0.99104" genericHeader="method">
3 Performance Evaluation
</sectionHeader>
<subsectionHeader confidence="0.994384">
3.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999927419354839">
To evaluate the performance of the proposed
relevance measures for Chinese in different do-
mains, experiments are conducted on two sepa-
rate domain corpora CorpusIT and CorpusLegal.,
respectively. CorpusIT includes academic papers
of 6.64M in size from Chinese IT journals be-
tween 1998 and 2000. CorpusLegal includes the
complete set of official Chinese constitutional
law articles and Economics/Finance law articles
of 1.04M in size (http://www.law-lib.com/).
For comparison to previous work, all term
candidates are extracted from the same domain
corpora using the delimiter based algorithm
TCE_DI (Term Candidate Extraction – Delimiter
Identification) which is efficient according to
(Yang et al., 2008). In TCE_DI, term delimiters
are identified first. Words between delimiters are
then taken as term candidates.
The performances are evaluated in terms of
precision (P), recall (R) and F-value (F). Since
the corpora are relatively large, sampling is used
for evaluation based on fixed interval of 1 in
each 10 ranked results. The verification of all the
sampled data is carried out manually by two ex-
perts independently. To evaluate the recall, a set
of correct terms which are manually verified
from the extracted terms by different methods is
constructed as the standard answer. The answer
set is certainly not complete. But it is useful as a
performance indication for comparison since it is
fair to all algorithms.
</bodyText>
<subsectionHeader confidence="0.99908">
3.2 Evaluation on Term Extraction
</subsectionHeader>
<bodyText confidence="0.999821761904762">
For comparison, three reference algorithms are
used in the evaluation. The first algorithm is
TV_LinkA which takes CS and CD into consid-
eration and performs well (Yang et al., 2008).
The second one is a supervised learning ap-
proach based on a SVM classifier, SVMlight
(Joachims, 1999). Internal and external features
are used by SVMlight. The third algorithm is the
popular used TF-IDF algorithm. All the refer-
ence algorithms require no training except
SVMlight. Two training sets containing thousands
of positive and negative examples from IT do-
main and legal domain are constructed for the
SVM classifier. The training and testing sets are
not overlapped.
Table 1 and Table 2 show the performance of
the proposed algorithms using different features
for IT domain and legal domain, respectively.
The algorithm using CD alone is the same as the
TF-IDF algorithm. The algorithm using CS and
CD is the TV_LinkA algorithm.
</bodyText>
<table confidence="0.999748181818182">
Algorithms Precision Recall F-value
(%) (%) (%)
SVM 63.6 49.5 55.6
CC 47.1 36.5 41.2
CS 65.6 51 57.4
CD(TF-IDF) 64.8 50.4 56.7
CC+CS 80.4 62.5 70.3
CC+CD 49 38.1 42.9
CS+CD 75.4 58.6 66
(TV_LinkA)
CC+CS+CD 82.8 64.4 72.4
</table>
<tableCaption confidence="0.999953">
Table 1. Performance on IT Domain
</tableCaption>
<figure confidence="0.658801793103448">
Th
+
D i (TC )
CC
C
D
= (∑ CC
) log( )
ij
DFC
j
D
= (∑ CS
) log( )
Th
+
D i (TC )
CS
C
ij
DFCj
ThCC+CS(TCi )
RCC
) RCS
(TC
i
)
(TC
)
</figure>
<page confidence="0.993185">
215
</page>
<table confidence="0.999899454545455">
Algorithms Precision Recall F-value
(%) (%) (%)
SVM 60.1 54.2 57.3
CC 45.2 40.3 42.6
CS 70.5 40.1 51.1
CD(TF-IDF) 59.4 52.9 56
CC+CS 64.2 49.9 56.1
CC+CD 48.4 43.1 45.6
CS+CD 67.4 60.1 63.5
(TV_LinkA)
CC+CS+CD 70.2 62.6 66.2
</table>
<tableCaption confidence="0.999963">
Table 2. Performance on Legal Domain
</tableCaption>
<bodyText confidence="0.96378524">
Table 1 and Table 2 show that the proposed
algorithms achieve similar performance on both
domains. The proposed algorithm using all three
features (CC+CS+CD) performs the best. The
results confirm that the proposed approach are
quite stable across domains and the relevance
between candidates are efficient for improving
performance of term extraction in different do-
mains. The algorithm using CC only does not
achieve good performance. Neither does CC+CS.
The main reason is that the term candidates used
in the experiments are extracted using the
TCE_DI algorithm which can extract candidates
with low statistical significance. TCE_DI pro-
vides a better compromise between recall and
precision. CC alone is vulnerable to noisy candi-
dates since it relies on the relevance between
candidates themselves. However, as an addi-
tional feature to the combined use of CS and CD
(TV_LinkA), improvement of over 10% on F-
value is obtained for the IT domain, and 5% for
the legal domain. This is because the noise data
are eliminated by CS and CD, and CC help to
identify additional terms that may not be statisti-
cally significant.
</bodyText>
<sectionHeader confidence="0.996724" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999739666666667">
In conclusion, this paper exploits the relevance
between term candidates as an additional feature
for term extraction approach. The proposed ap-
proach requires no prior domain knowledge and
no adaptation for new domains. Experiments for
term extraction are conducted on IT domain and
legal domain, respectively. Evaluations indicate
that the proposed algorithm using different types
of relevance achieves the best performance in
both domains without training.
In this work, only co-occurrence in a sentence
is used as the relevance between term candidates.
Other features such as syntactic relations can
also be exploited. The performance may be fur-
ther improved by using more efficient combina-
tion strategies. It would also be interesting to
apply this approach to other languages such as
English.
Acknowledgement: The project is partially sup-
ported by the Hong Kong Polytechnic University
(PolyU CRG G-U297)
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999835886363636">
Chang Jing-Shin. 2005. Domain Specific Word Ex-
traction from Hierarchical Web Documents: A
First Step toward Building Lexicon Trees from
Web Corpora. In Proc of the 4th SIGHAN Work-
shop on Chinese Language Learning: 64-71.
Eibe Frank, Gordon. W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. Do-
main-specific Keyphrase Extraction. In Proc.of
16th Int. Joint Conf. on AI, IJCAI-99: 668-673.
Joachims T. 2000. Estimating the Generalization Per-
formance of a SVM Efficiently. In Proc. of the Int
Conf. on Machine Learning, Morgan Kaufman,
2000.
Kageura K., and B. Umino. 1996. Methods of auto-
matic term recognition: a review. Term 3(2):259-
289.
Kleinberg J. 1997. Authoritative sources in a hyper-
linked environment. In Proc. of the 9th ACM-SIAM
Symposium on Discrete Algorithms: 668-677. New
Orleans, America, January 1997.
Ji Luning, and Qin Lu. 2007. Chinese Term Extrac-
tion Using Window-Based Contextual Information.
In Proc. of CICLing 2007, LNCS 4394: 62 – 74.
Li Hongqiao, Chang-Ning Huang, Jianfeng Gao, and
Xiaozhong Fan. The Use of SVM for Chinese New
Word Identification. In Proc. of the 1st Int.Joint
Conf. on NLP (IJCNLP2004): 723-732. Hainan Is-
land, China, March 2004.
Salton, G., and McGill, M.J. (1983). Introduction to
Modern Information Retrieval. McGraw-Hill.
S. Brin, L. Page. The anatomy of a large-scale hyper-
textual web search engine. The 7th Int. World Wide
Web Conf, Brisbane, Australia, April 1998, 107-
117.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, 1995.
Yang Yuhang, Qin Lu, Tiejun Zhao. (2008). Chinese
Term Extraction Using Minimal Resources. The
22nd Int. Conf. on Computational Linguistics (Col-
ing 2008). Manchester, Aug., 2008, 1033-1040.
Zhou GD, Shen D, Zhang J, Su J, and Tan SH. 2005.
Recognition of Protein/Gene Names from Text us-
ing an Ensemble of Classifiers. BMC Bioinformat-
ics 2005, 6(Suppl 1):S7.
</reference>
<page confidence="0.99914">
216
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.738144">
<title confidence="0.997888">Chinese Term Extraction Using Different Types of Relevance</title>
<author confidence="0.99775">Tiejun Qin Dequan</author>
<author confidence="0.99775">Hao</author>
<affiliation confidence="0.997971">of Computer Science and Technology,</affiliation>
<address confidence="0.923685">Harbin Institute of Technology, Harbin 150001, China</address>
<email confidence="0.900495">yhyang@mtlab.hit.edu.cn</email>
<email confidence="0.900495">tjzhao@mtlab.hit.edu.cn</email>
<email confidence="0.900495">dqzheng@mtlab.hit.edu.cn</email>
<email confidence="0.900495">yu@mtlab.hit.edu.cn</email>
<affiliation confidence="0.952802">of Computing,</affiliation>
<address confidence="0.909174">The Hong Kong Polytechnic University, Hong Kong, China</address>
<email confidence="0.995181">csluqin@comp.polyu.edu.hk</email>
<abstract confidence="0.9983211875">This paper presents a new term extraction approach using relevance between term candidates calculated by a link analysis based method. Different types of relevance are used separately or jointly for term verification. The proposed approach requires no prior domain knowledge and no adaptation for new domains. Consequently, the method can be used in any domain corpus and it is especially useful for resource-limited domains. Evaluations conducted on two different domains for Chinese term extraction show significant improvements over existing techniques and also verify the efficiency and relative domain independent nature of the approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chang Jing-Shin</author>
</authors>
<title>Domain Specific Word Extraction from Hierarchical Web Documents: A First Step toward Building Lexicon Trees from Web Corpora.</title>
<date>2005</date>
<booktitle>In Proc of the 4th SIGHAN Workshop on Chinese Language Learning:</booktitle>
<pages>64--71</pages>
<marker>Jing-Shin, 2005</marker>
<rawString>Chang Jing-Shin. 2005. Domain Specific Word Extraction from Hierarchical Web Documents: A First Step toward Building Lexicon Trees from Web Corpora. In Proc of the 4th SIGHAN Workshop on Chinese Language Learning: 64-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Paynter</author>
<author>Ian H Witten</author>
<author>Carl Gutwin</author>
<author>Craig G Nevill-Manning</author>
</authors>
<title>Domain-specific Keyphrase Extraction.</title>
<date>1999</date>
<booktitle>In Proc.of 16th Int. Joint Conf. on AI, IJCAI-99:</booktitle>
<pages>668--673</pages>
<marker>Paynter, Witten, Gutwin, Nevill-Manning, 1999</marker>
<rawString>Eibe Frank, Gordon. W. Paynter, Ian H. Witten, Carl Gutwin, and Craig G. Nevill-Manning. 1999. Domain-specific Keyphrase Extraction. In Proc.of 16th Int. Joint Conf. on AI, IJCAI-99: 668-673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Estimating the Generalization Performance of a SVM Efficiently.</title>
<date>2000</date>
<booktitle>In Proc. of the Int Conf. on Machine Learning,</booktitle>
<location>Morgan Kaufman,</location>
<marker>Joachims, 2000</marker>
<rawString>Joachims T. 2000. Estimating the Generalization Performance of a SVM Efficiently. In Proc. of the Int Conf. on Machine Learning, Morgan Kaufman, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kageura</author>
<author>B Umino</author>
</authors>
<title>Methods of automatic term recognition: a review.</title>
<date>1996</date>
<journal>Term</journal>
<pages>3--2</pages>
<contexts>
<context position="1477" citStr="Kageura and Umino, 1996" startWordPosition="211" endWordPosition="214">or Chinese term extraction show significant improvements over existing techniques and also verify the efficiency and relative domain independent nature of the approach. 1 Introduction Terms are the lexical units to represent the most fundamental knowledge of a domain. Term extraction is an essential task in domain knowledge acquisition which can be used for lexicon update, domain ontology construction, etc. Term extraction involves two steps. The first step extracts candidates by unithood calculation to qualify a string as a valid term. The second step verifies them through termhood measures (Kageura and Umino, 1996) to validate their domain specificity. Many previous studies are conducted on term candidate extraction. Other tasks such as named entity recognition, meaningful word extraction and unknown word detection, use techniques similar to that for term candidate extraction. But, their focuses are not on domain specificity. This study focuses on the verification of candidates by termhood calculation. Relevance between term candidates and documents is the most popular feature used for term verification such as TF-IDF (Salton and McGill, 1983; Frank, 1999) and Inter-Domain Entropy (Chang, 2005), which a</context>
</contexts>
<marker>Kageura, Umino, 1996</marker>
<rawString>Kageura K., and B. Umino. 1996. Methods of automatic term recognition: a review. Term 3(2):259-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1997</date>
<booktitle>In Proc. of the 9th ACM-SIAM Symposium on Discrete Algorithms:</booktitle>
<pages>668--677</pages>
<location>New Orleans, America,</location>
<contexts>
<context position="6763" citStr="Kleinberg, 1997" startWordPosition="1091" endWordPosition="1092">the graphic structure. d is set to 0.85 as suggested in (Page et al., 1998). Initially, all PR weights are set to 1. The weight score of each node are obtained by (1), iteratively. The significance of each term candidate in the domain specific corpus is then derived based on the significance of other candidates it co-occurred with. The CC weight of term candidate is given by its PR value after k iterations, a parameter to be determined experi to as CS, is calculated using the (Term Verification algorithm proposed in (Yang et al., 2008) based on Hyperlink-Induced Topic Search (HITS) algorithm (Kleinberg, 1997). In a good hub in the domain corpus is a sentence that contains many good authorities; a good authority is a term candidate that is contained in many good hubs. In a node p can either be a sentence or a term candidate. If a term candidate TC is contained in a sentence Sen of the domain corpus, there is a directional link from Sen to TC. then makes use of the relationship between candidates and sentences via an iterative process to update CS weight for each TC. Let denote the authority vector and w(p2)H,..., denote the hub vector. and VH are initialized to (1, 1,..., 1). Given weights and VH w</context>
</contexts>
<marker>Kleinberg, 1997</marker>
<rawString>Kleinberg J. 1997. Authoritative sources in a hyperlinked environment. In Proc. of the 9th ACM-SIAM Symposium on Discrete Algorithms: 668-677. New Orleans, America, January 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji Luning</author>
<author>Qin Lu</author>
</authors>
<title>Chinese Term Extraction Using Window-Based Contextual Information.</title>
<date>2007</date>
<booktitle>In Proc. of CICLing 2007, LNCS 4394: 62 – 74.</booktitle>
<marker>Luning, Lu, 2007</marker>
<rawString>Ji Luning, and Qin Lu. 2007. Chinese Term Extraction Using Window-Based Contextual Information. In Proc. of CICLing 2007, LNCS 4394: 62 – 74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Hongqiao</author>
</authors>
<title>Chang-Ning Huang, Jianfeng Gao, and Xiaozhong Fan. The Use of SVM for Chinese New Word Identification.</title>
<date>2004</date>
<booktitle>In Proc. of the 1st Int.Joint Conf. on NLP (IJCNLP2004):</booktitle>
<pages>723--732</pages>
<location>Hainan Island, China,</location>
<marker>Hongqiao, 2004</marker>
<rawString>Li Hongqiao, Chang-Ning Huang, Jianfeng Gao, and Xiaozhong Fan. The Use of SVM for Chinese New Word Identification. In Proc. of the 1st Int.Joint Conf. on NLP (IJCNLP2004): 723-732. Hainan Island, China, March 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="2015" citStr="Salton and McGill, 1983" startWordPosition="290" endWordPosition="293">rm. The second step verifies them through termhood measures (Kageura and Umino, 1996) to validate their domain specificity. Many previous studies are conducted on term candidate extraction. Other tasks such as named entity recognition, meaningful word extraction and unknown word detection, use techniques similar to that for term candidate extraction. But, their focuses are not on domain specificity. This study focuses on the verification of candidates by termhood calculation. Relevance between term candidates and documents is the most popular feature used for term verification such as TF-IDF (Salton and McGill, 1983; Frank, 1999) and Inter-Domain Entropy (Chang, 2005), which are all based on the hypothesis that “if a candidate occurs frequently in a few documents of a domain, it is likely a term”. Limited distribution information of term candidates in different documents often limits the ability of such algorithms to distinguish terms from non-terms. There are also attempts to use prior domain specific knowledge and annotated corpora for term verification. TV_ConSem (Ji and Lu, 2007) calculates the percentage of context words in a domain lexicon using both frequency information and semantic information. </context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, G., and McGill, M.J. (1983). Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine. The 7th Int. World Wide Web Conf,</title>
<date>1998</date>
<pages>107--117</pages>
<location>Brisbane, Australia,</location>
<marker>Brin, Page, 1998</marker>
<rawString>S. Brin, L. Page. The anatomy of a large-scale hypertextual web search engine. The 7th Int. World Wide Web Conf, Brisbane, Australia, April 1998, 107-117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer,</publisher>
<contexts>
<context position="2940" citStr="Vapnik, 1995" startWordPosition="438" endWordPosition="439">inguish terms from non-terms. There are also attempts to use prior domain specific knowledge and annotated corpora for term verification. TV_ConSem (Ji and Lu, 2007) calculates the percentage of context words in a domain lexicon using both frequency information and semantic information. However, this technique requires a domain lexicon whose size and quality have great impact on the performance of the algorithm. Some supervised learning approaches have been applied to protein/gene name recognition (Zhou et al., 2005) and Chinese new word identification (Li et al., 2004) using SVM classifiers (Vapnik, 1995) which also require large domain corpora and annotations. The latest work by Yang (2008) applied the relevance between term candidates and sentences by using the link analysis approach based on the HITS algorithm to achieve better performance. In this work, a new feature on the relevance between different term candidates is integrated with other features to validate their domain specificity. The relevance between candidate terms may be useful to identify domain specific terms based on two assumptions. First, terms are more likely to occur with other terms in order to express domain information</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Yuhang</author>
<author>Qin Lu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Chinese Term Extraction Using Minimal Resources.</title>
<date>2008</date>
<booktitle>The 22nd Int. Conf. on Computational Linguistics (Coling</booktitle>
<pages>1033--1040</pages>
<location>Manchester,</location>
<marker>Yuhang, Lu, Zhao, 2008</marker>
<rawString>Yang Yuhang, Qin Lu, Tiejun Zhao. (2008). Chinese Term Extraction Using Minimal Resources. The 22nd Int. Conf. on Computational Linguistics (Coling 2008). Manchester, Aug., 2008, 1033-1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GD</author>
<author>D Shen</author>
<author>J Zhang</author>
<author>J Su</author>
<author>Tan SH</author>
</authors>
<title>Recognition of Protein/Gene Names from Text using an Ensemble of Classifiers.</title>
<date>2005</date>
<journal>BMC Bioinformatics</journal>
<pages>6--1</pages>
<marker>GD, Shen, Zhang, Su, SH, 2005</marker>
<rawString>Zhou GD, Shen D, Zhang J, Su J, and Tan SH. 2005. Recognition of Protein/Gene Names from Text using an Ensemble of Classifiers. BMC Bioinformatics 2005, 6(Suppl 1):S7.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>