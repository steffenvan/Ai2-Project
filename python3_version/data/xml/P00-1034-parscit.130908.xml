<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<note confidence="0.638941">
Part-of-Speech Tagging Based on Hidden Markov Model
Assuming Joint Independence
</note>
<author confidence="0.633824">
Sang-Zoo Lee and Jun-ichi Tsujii
</author>
<affiliation confidence="0.994908">
Department of Information Science
University of Tokyo
</affiliation>
<address confidence="0.893328">
Hongo 7-3-1, Bunkyo-ku
Tokyo 113-0033, Japan
</address>
<email confidence="0.9911">
flee,tsujiil@is.s.u-tokyo.acjp
</email>
<author confidence="0.910568">
Hae-Chang Rim
</author>
<affiliation confidence="0.9987015">
Department of Computer Science
Korea University
</affiliation>
<address confidence="0.946691">
1 5-Ga Anam-Dong, Seongbuk-Gu
Seoul 136-701, Korea
</address>
<email confidence="0.998239">
rim@nlp.korea.ac.kr
</email>
<sectionHeader confidence="0.98844" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999755208333333">
In this paper we present part-of-
speech taggers based on hidden
Markov models, which adopt a less
strict Markov assumption to con-
sider rich contexts. In models whose
parameters are very specific like
lexicalized ones, sparse-data prob-
lem is very serious and also condi-
tional probabilities tend to be es-
timated unreliably. To overcome
data-sparseness, a simplified version
of the well-known back-off smooth-
ing method is used. To mitigate
unreliable estimation problem, our
models assume joint independence
instead of conditional independence
because joint probabilities have the
same degree of estimation reliabil-
ity. In experiments for the Brown
corpus, models with rich contexts
achieve relatively high accuracy and
some models assuming joint inde-
pendence show better results than
the corresponding HMMs.
</bodyText>
<sectionHeader confidence="0.997239" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940857142857">
Part-of-speech (POS) tagging can be defined
as a process in which a proper POS tag
is assigned to each word in texts and so
it can be viewed as a classification prob-
lem (Mitchell, 1997). Over a decade, many
works for POS tagging have used a wide
range of machine learning techniques such
as a hidden Markov model (HMM) (Char-
niak et al., 1993), a maximum entropy
model (Ratnaparkhi, 1996), transformation
rules (Brill, 1994), a decision tree (Lee et
al., 1999), relaxation labeling (Padr6, 1996),
Bayesian inference (Samuelsson, 1993), dis-
criminative learning (Lin, 1992), a neural net-
work (Schmid, 1994), and so on.
In this paper we propose hidden Markov
models for part-of-speech tagging, which
adopt a less strict Markov assumption(Cinlar,
1975) to consider rich contexts. Because such
models have a large number of parameters,
they must suffer from sparse-data problem
unless they have an enough volume of train-
ing corpus. Moreover, because such models
assume conditional independence, the prob-
ability estimates of their parameters may
have statistically different reliability that de-
pends on the number of samples of condi-
tional terms. To overcome the first problem,
a simplified version of the well-known back-off
smoothing method is used. To mitigate unre-
liable estimation problem, our models assume
joint independence between random variables
instead of conditional independence because
joint probabilities have the same degree of es-
timation reliability.
</bodyText>
<sectionHeader confidence="0.474898" genericHeader="method">
2 HMM-based POS tagging
</sectionHeader>
<bodyText confidence="0.9998408">
Figure 1 shows a lattice structure of an En-
glish sentence, &amp;quot;Flies like a flower.&amp;quot;, where
each node has a word and its POS tag and
where the sequence connected by bold lines
indicates the most likely sequence.
</bodyText>
<subsectionHeader confidence="0.904344">
2.1 Standard model
</subsectionHeader>
<bodyText confidence="0.99872825">
We basically follow the notation of (Char-
niak et al., 1993) to describe Bayesian mod-
els for POS tagging. In this paper, we as-
sume that {wl, w2, ... , /e} is a set of
</bodyText>
<equation confidence="0.82322875">
$/$
Flies/NNS Flies/VBZ
like/CS like/IN like/JJ like/VB
a/AT a/IN a/NN
flower/NN flower/VB
Ii
•i•
sis
</equation>
<figureCaption confidence="0.9996">
Figure 1: A lattice of &amp;quot;Flies like a flower
</figureCaption>
<bodyText confidence="0.988345588235294">
words, ft&apos;, t2, , CI is a set of POS
tags, a sequence of random variables Wl,„ =
W2 . . . Wn is a sentence of n words,
and a sequence of random variables T1, =
T2 . . . Tn is a sequence of n POS tags. Be-
cause each of random variables W can take as
its value any of the words in the vocabulary,
we denote the value of W, by w, and a par-
ticular sequence of values for Wzo (i &lt; j) by
w3. In a similar way, we denote the value of
T, by t, and a particular sequence of values for
(i &lt; j) by 4,3. For generality, terms w,,3
and 4,3 (i &gt; ,j) are defined as being empty.
The purpose of Bayesian models for POS
tagging is to find the most likely sequence of
POS tags for a given sequence of words, as
follows:
</bodyText>
<equation confidence="0.977689333333333">
argmax = ti,nI W,, = wi,n) (1)
- argmax Pr(ti.,,,,I wi,n) (2)
Pr(tl,n, 201,n,)
- argmax
Pr(wi,n)
- argmax wi,n)
</equation>
<bodyText confidence="0.998343333333333">
Eqn. 1 becomes Eqn. 2 because reference to
the random variables themselves can be omit-
ted. Eqn. 2 is then transformed into Eqn. 3
since Pr(wi,n) is constant for all 4,n.
Then, the probability Pr(4 /Dim) is bro-
ken down into Eqn. 4 by using the chain rule.
</bodyText>
<equation confidence="0.9040322">
Pr(tl,n, Wl,n) = H ( x pr(wi (4)
Pr(ti ti,i-i,wi,i-i)
However, it is either implausible or impossible
to compute Pr(t, I and Pr(w, I
t,,, w,,_,) in Eqn. 4.
</equation>
<bodyText confidence="0.99658">
The standard HMM simplifies them by
making the following two strict Markov as-
sumption (conditional independence), Eqn. 5
and Eqn. 6, to get a more tractable form,
Eqn. 7.
</bodyText>
<table confidence="0.4754468">
Pr(ti Pr(t I ti_x,i-i)
Pr(wi I ti,i,wi,i-i) Pr(wi I ti)
wi,n) H
Pr(ti I ti_K,i-i)
x Pr(wi I ti)
</table>
<bodyText confidence="0.999608777777778">
The standard HMM assumes that the prob-
ability of a current tag t, conditionally de-
pends on only the previous K tags 4_1(,,_1
and that the probability of a current word
w, conditionally depends on only the current
tag&apos;. In the standard model (K=1), for ex-
ample, the probability of a node &amp;quot;a/AT&amp;quot; of
the most likely sequence in Figure 1 is calcu-
lated as follows:
</bodyText>
<equation confidence="0.844018">
Pr(AT I NNS,VB)
x Pr(a I AT)
</equation>
<bodyText confidence="0.9998725">
Generally, the standard HMM has a limita-
tion that it can not solve complicated ambi-
guities because it does not consider rich con-
texts. To overcome this limitation, the stan-
dard HMM should be extended so that it can
consult rich information in contexts.
</bodyText>
<subsectionHeader confidence="0.948473">
2.2 Extended models
</subsectionHeader>
<bodyText confidence="0.9905465">
An extended HMM, A(T(K, (L,I)), for
POS tagging is defined by making the follow-
ing two less strict Markov assumption, Eqn. 8
and Eqn. 9, as follows:
</bodyText>
<equation confidence="0.99389">
Pr(ti tl,i-1, Wl,i-1) C-Z1 Pr(ti ti-K,i-1, (8)
Pr(wi Pr(wi I ti-L,i,wi-i,i-i) (9)
A(T(K,J), W(L,n) p Pr(ti,n,w,n)
Pr(ti I ti-K,i- 1, Wi-J,i-1)
(10)
x Pr(wi I ti-L,i, Wi-/,i-1) )
</equation>
<bodyText confidence="0.768107">
In a model A(T(K, (L,I)), the probability
of the current tag t, conditionally depends on
&apos;Usually, K is determined as 1 (bigram as in (Char-
niak et al., 1993)) or 2 (trigram as in (Merialdo,
1991)).
</bodyText>
<equation confidence="0.910057">
(3)
i=1
</equation>
<bodyText confidence="0.999284692307692">
both the previous K tags t,_K,,_1 and the pre-
vious J words and the probability of
the current word w, conditionally depends on
the current tag and the previous L tags t,_L,,
and the previous / words w,_/,,_1. In exper-
iments, we set K as 1 or 2, J as 0 or K, L as
1 or 2, and / as 0 or L. If J and / are zero,
the above models are non-lexicalized models.
Otherwise, they are lexicalized models.
In an extended model A(T(252), W(252)), for
example, the probability of a node &amp;quot;a/AT&amp;quot; of
the most likely sequence in Figure 1 is calcu-
lated as follows:
</bodyText>
<equation confidence="0.7818575">
Pr(AT I NNS,VB,Flies,like)
x Pr(a I AT, NNS,V B, Flies, like)
</equation>
<sectionHeader confidence="0.508605" genericHeader="method">
3 Parameter estimation
</sectionHeader>
<bodyText confidence="0.999812714285714">
Because the extended models have a large
number of parameters, they must suffer from
both sparse-data problem and unreliable es-
timation problem. The models adopt a sim-
plified back-off smoothing technique as a so-
lution to the first problem, and joint indepen-
dence assumption as a solution to the second.
</bodyText>
<subsectionHeader confidence="0.997611">
3.1 Simplified back-off smoothing
</subsectionHeader>
<bodyText confidence="0.999532428571429">
In supervised learning, the simpliest pa-
rameter estimation is the maximum like-
lihood(ML) estimation(Duda et al., 1973)
which maximizes the probability of a train-
ing set. The ML estimate of tag (K+1)-gram
probability, Prmi(t% I 4-K 5%-i) , is calculated
as follows:
</bodyText>
<equation confidence="0.9968315">
Pr (t, I t,_K,,_1) = (11)
ML Fq(4_1(1)
</equation>
<bodyText confidence="0.985279142857143">
where the function Fq(x) returns the fre-
quency of x in the training set. When using
the ML estimation, data sparseness is even
more serious in the extended models than in
the standard models because the former has
even more parameters than the latter.
(Chen, 1996), where various smoothing
techniques was tested for a language model by
using the perplexity measure, reported that a
back-off smoothing(Katz, 1987) performs bet-
ter on a small traning set than other meth-
ods. In the back-off smoothing, the smoothed
probability of tag (K+1)-gram PrsBo(t, I
t,_1(5%-1) is calculated as follows:
</bodyText>
<equation confidence="0.9762955">
Pr (t I ti—K,i-1) =
SBO
{dr PrML(t I ti-K,i-1) if r &gt; 0
a(ti_K,i-1) Pr sBo(t I ti-K+1,i-1)if r = 0
where r = Fq(ti—K,i), r* = (r+ 1) nr+1
) nr
T * (r+1)xn,±1
dr = r
1 (r+1) xn,±1.
ro.
</equation>
<bodyText confidence="0.997072333333333">
In the equation above, nr denotes the num-
ber of (K+1)-gram whose frequency is r,
and the coefficient dr is called the discount
ratio, which reflects the Good-Turing es-
timate(Good, 1953)2. Eqn. 12 says that
PrsBo(t I t,_K,,_1) is under-estimated by
dr than its maximum likelihood estimate, if
r &gt; 0, or is backed off by its smoothing term
in proportion to the
</bodyText>
<equation confidence="0.763099">
PrsBo(t% t%—i-c+15%—i)
</equation>
<bodyText confidence="0.999293833333333">
value of the function a(t,_K,,_1) of its condi-
tional term 4_1(5,-1, if r = 0.
However, because Eqn. 12 requires compli-
cated computation in a(t,_K,,_1), we simplify
it to get a function of the frequency of a con-
ditional term, as follows:
</bodyText>
<equation confidence="0.940674333333333">
a(Fq(ti_K,i-1) = f) =
x E[Fq(ti_K,i-1) = f]
ET=0 = f]
where
= 1 PrSBO(tilti-K,i-1)
Eti K,i5r&gt;0 PrmL(tilti_K,i_i)
E[Fq(ti_K,i-1) = f] =
Pr (tilti—K+1,i-1)
SBO
</equation>
<bodyText confidence="0.999689555555556">
In Eqn. 13, the range of f is bucketed into 7
regions such as f = 0, 1, 2, 3, 4, 5 and f &gt; 6
since it is also difficult to compute this equa-
tion for all possible values of f.
Using the formalism of our simplified back-
off smoothing, each of probabilities whose
ML estimate is zero is backed off by its
corresponding smoothing term. In experi-
ments, the smoothing terms of PrsBo(t,
</bodyText>
<equation confidence="0.94170655">
2111 (Katz, 1987) dr = 1 if r &gt; 5.
711
(13)
,r=0,Fq(t-K,- 1)=f
(12)
ti_K5i_1,Wi-J5i-1) are determined as follows: lows:
Pr sB0(t I
i t -K+15i-1, )
Wi-J+15i-1 ifK&gt;1_,J&gt;1
Pr S BO(ti ti-K5i-1) if K&gt;l,J=1
Pr s Bo(ti ti-K+15i-1) if K&gt; 1,J= 0
Pr AD(ti) if K= 1,J=0
Also, the smoothing terms of PrsBo(w,
t,_L,„ w,_/,,_1) are determined as follows:
Pr SBO(Wi I 5 )
ti-L+1
Wi-I+15i-1 if L&gt;1,I&gt;1
PrSBO(Wi ti-L5i) if L&gt;1,I=1
PrsBo(w I ti—L+1,i) if L&gt;1,I=0
PrAD (wi) if L=0,1= 0
</equation>
<bodyText confidence="0.9995112">
In the equations above, the unigram prob-
abilities are calculated by using an additive
smoothing with 6 = 10-2 which is chosen
through experiments. The equation for the
additive smoothing (Chen, 1996) is as follows:
</bodyText>
<equation confidence="0.916562666666667">
Fq(t,—K,i) ± 6
Pr (t I ti—K,i-1) = Eti (Fq(ti_K,i) + 6)
AD
</equation>
<subsectionHeader confidence="0.884334">
3.2 Joint independence
</subsectionHeader>
<bodyText confidence="0.999983285714286">
The parameters of an HMM may have differ-
ent degree of statistical reliability because pa-
rameter reliability depends on the frequency
of conditional term. For example, let a corpus
consist of 1 million words and let the follow-
ing parameters be extracted from the corpus
by using the maximum likelihood estimation.
</bodyText>
<equation confidence="0.997643">
Pr(a) = 0.01 Pr(d I a) = 0.1
Pr(b) = 0.001 Pr(d = 0.1
Pr(c) = 0.0001 Pr(d = 0.1
</equation>
<bodyText confidence="0.999889818181818">
In this case, three conditional probabilities,
Pr(d I a), Pr(d I b), and Pr(d I c) are all
0.1 but Pr(d I a) is statistically more reliable
than others because its sample size (10,000
words = 1 million x Pr(a)) is bigger than oth-
ers. Actually, this phenomenon is very serious
in extended models, even though parameters
of the models are seen in the training corpus.
To consider such statistical reliability of a
probability estimate, we introduce the con-
cept of weighting Markov assumption, as fol-
</bodyText>
<equation confidence="0.990915333333333">
Pr(t, I ti,—i,wi,-1)
Pr(t, I ti_K5,_1, (14)
x
Pr(wi I ti,, w1,_1)
Pr(wilti—L,i,wi—i,i-1) (15)
x
</equation>
<bodyText confidence="0.999777">
If the probability function, Pr, is used as
the weight function, W, the equations above
become equations assuming joint indepen-
dence between random variables as follows:
</bodyText>
<equation confidence="0.99364">
Pr (ti t1,_1, w1,_1) Rt
Pr(ti,ti-K5i-1,Wi-J5i-1)
PO% I ti,, w1,%-1)
Pr(w%, tt-L,t,
</equation>
<bodyText confidence="0.99985724">
The equations above assume that the prob-
ability of the current tag t, jointly depends
on both the previous K tags t,_1(5,_1 and the
previous J words and that the prob-
ability of the current word w, jointly depends
on the current tag and the previous L tags
t,_L,, and the previous I words w,_/,,_1. If a
Bayesian model assumes joint independence,
we call it a joint independence model (JIM).
Actually, using the probability function as
the weight function is mathematically incor-
rect and implausible. For example, while the
sum of probabilities of all sentences with the
same length becomes 1.0 in an HMM, it be-
comes naturally less than 1.0 in a JIM. There-
fore, JIMs should not be used in calculating
the probability of a sentence. However, if we
want to find the most likely sequence for each
sentence and the joint probability of each pa-
rameter is regarded as a score, JIMs have no
problem.
By replacing corresponding parameters, an
extended HMM can be transformed into the
corresponding JIM, which is defined as fol-
lows:
</bodyText>
<equation confidence="0.9973676">
4)(T(K, ,), (L,,)) H Pr(ti,n,
i=1
H
n Pr(ti, ti-K5i-1,Wi-J5i-1)
( x Pr(w,, ti-L,i, Wi-/,i-1) (18)
</equation>
<bodyText confidence="0.9990825">
In an extended JIM, d)(T(252), (2,2)), for
example, the probability of a node &amp;quot;a/AT&amp;quot; of
the most likely sequence in Figure 1 is calcu-
lated as follows:
</bodyText>
<equation confidence="0.8309985">
Pr(AT,NNS,VB,Flies,like)
x Pr(a, AT, NN S,V B, Flies, like)
</equation>
<bodyText confidence="0.999624666666667">
The parameters of a JIM are estimated by
using the parameters of the corresponding
HMM as follows:
</bodyText>
<equation confidence="0.9990035">
Pr S BO(ti,ti-K,i-1, Wi-J,i-1) =
Pr sBo(t I ti-K,i-1,Wi-J,i-1)
X Pr AD(ti-K,i-1,Wi-J,i-1)
Pr SBO(Wi, ti-L,i, =
Pr sBo(w I ti-L,i, wi-/,i-1)
x PrAD(t,_L,,, w,_/,,_1)
AD (Fq(t,_K,,) + 6)
Pr (t,_K,,) = Fq(t,_K,,) + 6
</equation>
<sectionHeader confidence="0.910153" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99997388">
For experiments, we used the Brown corpus
which consists of 1,113,180 words and 53,885
sentences and is tagged with 82 POS tags3.
It was segmented into two parts, the training
set of 90% and the test set of 10%, in the
way that each sentence in the test set was
extracted from every 10 sentence. In the same
way, we made 10-fold data set for 10-fold cross
validation.
In order to assign all possible tags to each
word, we made two assumption: closed vo-
cabulary assumption and open vocabulary as-
sumption. For closed vocabulary assump-
tion, we looked up a dictionary tailored to
the Brown corpus. In this case, the aver-
age number of tags per word became 1.64.
For open vocabulary assumption, we looked
up a dictionary tailored only to a training
set in order to assign possible tags to fre-
quent words whose frequency is greater than
5. In case of rare words, tags in the dictionary
were assigned and then 6 tags with highest
score were assigned by using a naive Bayesian
classifier(Mitchell, 1997) considering charac-
ter features as follows:
</bodyText>
<equation confidence="0.578932">
Pr(ti, wi) = Pr(t) x Pr(w I ti)
</equation>
<footnote confidence="0.680071">
3Note that some sentences, which have composite
tags(such as &amp;quot;HV±TO&amp;quot; in &amp;quot;hafta&amp;quot;), &amp;quot;ILLEGAL&amp;quot; tag,
or &amp;quot;NIL&amp;quot; tag, were removed from the Brown corpus
and tags with &amp;quot;*&amp;quot; (not) such as &amp;quot;BEZ*&amp;quot; were replaced
by corresponding tags without &amp;quot;*&amp;quot; such as &amp;quot;BEZ&amp;quot;.
</footnote>
<equation confidence="0.9430245">
Pr(t) x H Pr(fl ti)
j=1
</equation>
<bodyText confidence="0.995543906976744">
where A/ indicates j-th character features of
w, and F(=12) is the number of character fea-
ture types including prefixes (whose length
is 1 through 4), suffixes (whose length is 1
through 4), if w, contains numbers, if w, con-
tains an initial uppercase letter, if w, contains
any non-initial uppercase letter, if w, contains
hyphens. In this case, the average number of
tags per word became 2.00 and the rate of
words that have the correct tag among all as-
signed tags became 99.85%.
Figure 2 illustrates graphs showing the av-
erage accuracy rates of HMMs and JIMs un-
der the closed vocabulary assumption. Here,
labels in the x-axis specify models in the
way that KL5/ denotes A(T(K5j), W(41)) or
d)(T
(K5j), W(L5.0). The models are arranged
by the ascending order of theoretical num-
ber of parameters. The first two models are
standard models and the others are extended
models. The average accuracy rates beyond
the range of each graph are just below the
figure.
In this figure, we can observe that the sim-
plified back-off smoothing technique mitigates
sparse-data problems in both HMMs and
JIMs. As expected, JIMs achieves higher ac-
curacy than the corresponding HMMs in some
extended models consulting rich contexts.
It is statistically significant with confidence
99that the model, d)(T(252),
— (151)) (98.05%),
is better than any other models including
the standard bigram HMM, A(T(150), (0,0))
(97.27%) and the best HMM, A(T(151), (1,0))
(97.93%).
Figure 3 depicts graphs indicating the av-
erage accuracy rates of HMMs and JIMs un-
der the open vocabulary assumption. Unlike
Figure 2, the model, A(T(250) , W(150)), achieves
the best accuracy rate (96.86%) with confi-
dence 99%.
</bodyText>
<sectionHeader confidence="0.997381" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.997014">
We have presented the extended HMMs for
English POS tagging, which can consider rich
</bodyText>
<figure confidence="0.767792">
10 2,0 1,0 2,0 1,1 1,1 1,0 2,0 1,1 1,0 2,0 1,1 2,2 2,2 2,2 2,2 1,0 2,0 1,1 2,2
00 0,0 1,0 1,0 0,0 1,0 2,0 2,0 2,0 1,1 1,1 1,1 0,0 1,0 2,0 1,1 2,2 2,2 2,2 22
94.90 95.18 95.50 95.49 95.67 95.50 96.83
</figure>
<figureCaption confidence="0.911803">
Figure 2: Results under the closed vocabulary assumption
</figureCaption>
<figure confidence="0.995880846153846">
98.1
98.0
x.
97.8
97.6
97.4
HMM .x. -
JIM -El-
97.2 -
1 1 1 1 1 1 1 1 1 1
1
97.0
_x
</figure>
<bodyText confidence="0.999453552631579">
information in contexts. In the models, a sim-
plified version of back-off smoothing is used to
mitigate data sparseness problem. The mod-
els assume joint independence between ran-
dom variables in order to make the parameter
estimation more reliable.
From the experiments, we have observed
that extended models achieved even better
results than the standard models in case of
both HMMs and JIMs, that the simplified
back-off smoothing technique mitigated data
sparseness quite effectively, and that some ex-
tended JIMs outperformed the corresponding
HMMs. Under the closed vocabulary assump-
tion, the best JIM outperformed the best
HMM. On the contrary, under the open vo-
cabulary assumption, the best HMM outper-
formed the best JIM. Intuitively speaking, it
is empirically proven that the joint indepen-
dence assumption is more effective than the
Markov assumption in some models that con-
sult specific features such as lexicalized ones.
Generally, the uniform extension of mod-
els requires rapid increase of parameters, and
hence suffers from large storage and sparse
data. Recently in many areas where HMMs
are used, many efforts to extend models non-
uniformly have been made, sometimes result-
ing in noticeable improvement. For this rea-
son, we are trying to transform our uniform
models into non-uniform models, which may
be more effective in terms of both space com-
plexity and reliable estimation of paremeters,
without loss of accuracy.
And also, we are trying to apply our models
to different areas such as information extrac-
tion in the bio-molecular domain, noun phrase
chuncking, and so on.
</bodyText>
<sectionHeader confidence="0.997114" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997642875">
E. Brill. 1994. Some Advances in Transformation-
Based Part of Speech Tagging. In Proc.
of the 12th Nat&apos;l Conf. on Artificial
Intelligence (AAAI-9j), 722-727.
E. Charniak, C. Hendrickson, N. Jacobson, and
M. Perkowitz. 1993. Equations for Part-of-
Speech Tagging. In Proc. of the 11th Nat&apos;l
Conf. on Artificial Intelligence(AAAI-93), 784-
789.
S. F. Chen. 1996. Building Probabilistic Models
for Natural Language. Doctoral Dissertation,
Harvard University, USA.
E. Cinlar. 1975. Introduction to Stochastic Pro-
cesses. Prentice-Hall, New Jersey.
R. 0. Duda and R. E. Hart. 1973. Pattern Clas-
sification and Scene Analysis. John Wiley.
</reference>
<figure confidence="0.996947727272727">
HMM .x. -
JIM -El—
97.0
96.8
96.6
96.4
96.2
96.0
10 2,0 1,0 2,0 1,1 1,1 1,0 2,0 1,1 1,0 2,0 1,1 2,2 2,2 2,2 2,2 1,0 2,0 1,1 2,2
00 0,0 1,0 1,0 0,0 1,0 2,0 2,0 2,0 1,1 1,1 1,1 0,0 1,0 2,0 1,1 2,2 2,2 2,2 22
93.13 93.28 93.72 93.53 95.84 93.68 93.3 95.4
</figure>
<figureCaption confidence="0.997664">
Figure 3: Results under the open vocabulary assumption
</figureCaption>
<reference confidence="0.998973775510204">
W. N. Francis and H. Kueera. 1982. Frequency
Analysis of English Usage: Lexicon and Gram-
mar. Houghton Mifflin Company, Boston, Mas-
sachusetts.
I. J. Good. 1953. &amp;quot;The Population Frequencies of
Species and the Estimation of Population Pa-
rameters,&amp;quot; In Biometrika, 40(3-4):237-264.
S. M. Katz. 1987. Estimation of Probabilities
from Sparse Data for the Language Model Com-
ponent of a Speech Recognizer. In IEEE Trans-
actions on Acoustics, Speech and Signal Pro-
cessing(ASSP), 35(3) :400-401.
S.-Z. Lee, J.-D. Kim, W.-H. Ryu, and H-
C. Rim. 1999. A Part-of-Speech Tagging
Model Using Lexical Rules Based on Corpus
Statistics. In Proc. of the International Con-
ference on Computer Processing of Oriental
Languages (ICCP OL - 99), 385-390.
S.-Z. Lee. 1999. New Statistical Models for Au-
tomatic POS Tagging. Doctoral Dissertation,
Korea University, Korea.
Y.-C. Lin, T.-H. Chiang, and K.-Y. Su.
1992. Discrimination Oriented Probabilis-
tic Tagging. In Proc. of the 5th Interna-
tional Conference: Research on Computational
Linguistics (ROCLING- V), 85-96.
B. Merialdo. 1991. Tagging Text with a Prob-
abilistic Model. In Proc. of the International
Conference on Acoustic, Speech and Signal
Processing (ICA SSP-91), 809-812.
T. M. Mitchell. 1997. Machine Learning. New
York: McGraw-Hill.
L. Padr6. 1996. POS Tagging Using Re-
laxation Labeling. Research Report LSI-96-
10-R. Department de Llenguatgatges i Sis-
temes Informatics, Universitat Politecnica de
Catalunya.
A. Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-of-Speech Tagging. In Proc.
of the Empirical Methods in Natural Language
Processing Conference(EMNLP-96), 133-142.
C. Samuelsson. 1993. Morphological Tagging
Based Entirely on Bayesian Inference. In Proc.
of the 9&apos; Nordic Conference on Computational
Linguistics, 225-238.
H. Schmid. 1994. Part-of-Speech Tagging
with Neural Networks. In Proc. of the
International Conference on Computational
Linguistics (COLING-94), 172-176.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.507958">
<title confidence="0.8974955">Part-of-Speech Tagging Based on Hidden Markov Model Assuming Joint Independence</title>
<author confidence="0.999761">Sang-Zoo Lee</author>
<author confidence="0.999761">Jun-ichi Tsujii</author>
<affiliation confidence="0.9999635">Department of Information Science University of Tokyo</affiliation>
<address confidence="0.8838735">Hongo 7-3-1, Bunkyo-ku Tokyo 113-0033, Japan</address>
<email confidence="0.982487">flee,tsujiil@is.s.u-tokyo.acjp</email>
<author confidence="0.993507">Hae-Chang Rim</author>
<affiliation confidence="0.999809">Department of Computer Science Korea University</affiliation>
<address confidence="0.8447505">1 5-Ga Anam-Dong, Seongbuk-Gu Seoul 136-701, Korea</address>
<email confidence="0.97351">rim@nlp.korea.ac.kr</email>
<abstract confidence="0.99905564">In this paper we present part-ofspeech taggers based on hidden Markov models, which adopt a less strict Markov assumption to consider rich contexts. In models whose parameters are very specific like lexicalized ones, sparse-data problem is very serious and also conditional probabilities tend to be estimated unreliably. To overcome data-sparseness, a simplified version of the well-known back-off smoothing method is used. To mitigate unreliable estimation problem, our models assume joint independence instead of conditional independence because joint probabilities have the same degree of estimation reliability. In experiments for the Brown corpus, models with rich contexts achieve relatively high accuracy and some models assuming joint independence show better results than the corresponding HMMs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Some Advances in TransformationBased Part of Speech Tagging.</title>
<date>1994</date>
<booktitle>In Proc. of the 12th Nat&apos;l Conf. on Artificial Intelligence (AAAI-9j),</booktitle>
<pages>722--727</pages>
<contexts>
<context position="1628" citStr="Brill, 1994" startWordPosition="240" endWordPosition="241">for the Brown corpus, models with rich contexts achieve relatively high accuracy and some models assuming joint independence show better results than the corresponding HMMs. 1 Introduction Part-of-speech (POS) tagging can be defined as a process in which a proper POS tag is assigned to each word in texts and so it can be viewed as a classification problem (Mitchell, 1997). Over a decade, many works for POS tagging have used a wide range of machine learning techniques such as a hidden Markov model (HMM) (Charniak et al., 1993), a maximum entropy model (Ratnaparkhi, 1996), transformation rules (Brill, 1994), a decision tree (Lee et al., 1999), relaxation labeling (Padr6, 1996), Bayesian inference (Samuelsson, 1993), discriminative learning (Lin, 1992), a neural network (Schmid, 1994), and so on. In this paper we propose hidden Markov models for part-of-speech tagging, which adopt a less strict Markov assumption(Cinlar, 1975) to consider rich contexts. Because such models have a large number of parameters, they must suffer from sparse-data problem unless they have an enough volume of training corpus. Moreover, because such models assume conditional independence, the probability estimates of their</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>E. Brill. 1994. Some Advances in TransformationBased Part of Speech Tagging. In Proc. of the 12th Nat&apos;l Conf. on Artificial Intelligence (AAAI-9j), 722-727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>C Hendrickson</author>
<author>N Jacobson</author>
<author>M Perkowitz</author>
</authors>
<title>Equations for Part-ofSpeech Tagging.</title>
<date>1993</date>
<booktitle>In Proc. of the 11th Nat&apos;l Conf. on Artificial Intelligence(AAAI-93),</booktitle>
<pages>784--789</pages>
<contexts>
<context position="1547" citStr="Charniak et al., 1993" startWordPosition="227" endWordPosition="231">because joint probabilities have the same degree of estimation reliability. In experiments for the Brown corpus, models with rich contexts achieve relatively high accuracy and some models assuming joint independence show better results than the corresponding HMMs. 1 Introduction Part-of-speech (POS) tagging can be defined as a process in which a proper POS tag is assigned to each word in texts and so it can be viewed as a classification problem (Mitchell, 1997). Over a decade, many works for POS tagging have used a wide range of machine learning techniques such as a hidden Markov model (HMM) (Charniak et al., 1993), a maximum entropy model (Ratnaparkhi, 1996), transformation rules (Brill, 1994), a decision tree (Lee et al., 1999), relaxation labeling (Padr6, 1996), Bayesian inference (Samuelsson, 1993), discriminative learning (Lin, 1992), a neural network (Schmid, 1994), and so on. In this paper we propose hidden Markov models for part-of-speech tagging, which adopt a less strict Markov assumption(Cinlar, 1975) to consider rich contexts. Because such models have a large number of parameters, they must suffer from sparse-data problem unless they have an enough volume of training corpus. Moreover, becaus</context>
<context position="2978" citStr="Charniak et al., 1993" startWordPosition="446" endWordPosition="450"> the first problem, a simplified version of the well-known back-off smoothing method is used. To mitigate unreliable estimation problem, our models assume joint independence between random variables instead of conditional independence because joint probabilities have the same degree of estimation reliability. 2 HMM-based POS tagging Figure 1 shows a lattice structure of an English sentence, &amp;quot;Flies like a flower.&amp;quot;, where each node has a word and its POS tag and where the sequence connected by bold lines indicates the most likely sequence. 2.1 Standard model We basically follow the notation of (Charniak et al., 1993) to describe Bayesian models for POS tagging. In this paper, we assume that {wl, w2, ... , /e} is a set of $/$ Flies/NNS Flies/VBZ like/CS like/IN like/JJ like/VB a/AT a/IN a/NN flower/NN flower/VB Ii •i• sis Figure 1: A lattice of &amp;quot;Flies like a flower words, ft&apos;, t2, , CI is a set of POS tags, a sequence of random variables Wl,„ = W2 . . . Wn is a sentence of n words, and a sequence of random variables T1, = T2 . . . Tn is a sequence of n POS tags. Because each of random variables W can take as its value any of the words in the vocabulary, we denote the value of W, by w, and a particular sequ</context>
<context position="5868" citStr="Charniak et al., 1993" startWordPosition="997" endWordPosition="1001">To overcome this limitation, the standard HMM should be extended so that it can consult rich information in contexts. 2.2 Extended models An extended HMM, A(T(K, (L,I)), for POS tagging is defined by making the following two less strict Markov assumption, Eqn. 8 and Eqn. 9, as follows: Pr(ti tl,i-1, Wl,i-1) C-Z1 Pr(ti ti-K,i-1, (8) Pr(wi Pr(wi I ti-L,i,wi-i,i-i) (9) A(T(K,J), W(L,n) p Pr(ti,n,w,n) Pr(ti I ti-K,i- 1, Wi-J,i-1) (10) x Pr(wi I ti-L,i, Wi-/,i-1) ) In a model A(T(K, (L,I)), the probability of the current tag t, conditionally depends on &apos;Usually, K is determined as 1 (bigram as in (Charniak et al., 1993)) or 2 (trigram as in (Merialdo, 1991)). (3) i=1 both the previous K tags t,_K,,_1 and the previous J words and the probability of the current word w, conditionally depends on the current tag and the previous L tags t,_L,, and the previous / words w,_/,,_1. In experiments, we set K as 1 or 2, J as 0 or K, L as 1 or 2, and / as 0 or L. If J and / are zero, the above models are non-lexicalized models. Otherwise, they are lexicalized models. In an extended model A(T(252), W(252)), for example, the probability of a node &amp;quot;a/AT&amp;quot; of the most likely sequence in Figure 1 is calculated as follows: Pr(AT</context>
</contexts>
<marker>Charniak, Hendrickson, Jacobson, Perkowitz, 1993</marker>
<rawString>E. Charniak, C. Hendrickson, N. Jacobson, and M. Perkowitz. 1993. Equations for Part-ofSpeech Tagging. In Proc. of the 11th Nat&apos;l Conf. on Artificial Intelligence(AAAI-93), 784-789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
</authors>
<title>Building Probabilistic Models for Natural Language. Doctoral Dissertation,</title>
<date>1996</date>
<location>Harvard University, USA.</location>
<contexts>
<context position="7453" citStr="Chen, 1996" startWordPosition="1281" endWordPosition="1282"> second. 3.1 Simplified back-off smoothing In supervised learning, the simpliest parameter estimation is the maximum likelihood(ML) estimation(Duda et al., 1973) which maximizes the probability of a training set. The ML estimate of tag (K+1)-gram probability, Prmi(t% I 4-K 5%-i) , is calculated as follows: Pr (t, I t,_K,,_1) = (11) ML Fq(4_1(1) where the function Fq(x) returns the frequency of x in the training set. When using the ML estimation, data sparseness is even more serious in the extended models than in the standard models because the former has even more parameters than the latter. (Chen, 1996), where various smoothing techniques was tested for a language model by using the perplexity measure, reported that a back-off smoothing(Katz, 1987) performs better on a small traning set than other methods. In the back-off smoothing, the smoothed probability of tag (K+1)-gram PrsBo(t, I t,_1(5%-1) is calculated as follows: Pr (t I ti—K,i-1) = SBO {dr PrML(t I ti-K,i-1) if r &gt; 0 a(ti_K,i-1) Pr sBo(t I ti-K+1,i-1)if r = 0 where r = Fq(ti—K,i), r* = (r+ 1) nr+1 ) nr T * (r+1)xn,±1 dr = r 1 (r+1) xn,±1. ro. In the equation above, nr denotes the number of (K+1)-gram whose frequency is r, and the c</context>
<context position="9785" citStr="Chen, 1996" startWordPosition="1697" endWordPosition="1698">,- 1)=f (12) ti_K5i_1,Wi-J5i-1) are determined as follows: lows: Pr sB0(t I i t -K+15i-1, ) Wi-J+15i-1 ifK&gt;1_,J&gt;1 Pr S BO(ti ti-K5i-1) if K&gt;l,J=1 Pr s Bo(ti ti-K+15i-1) if K&gt; 1,J= 0 Pr AD(ti) if K= 1,J=0 Also, the smoothing terms of PrsBo(w, t,_L,„ w,_/,,_1) are determined as follows: Pr SBO(Wi I 5 ) ti-L+1 Wi-I+15i-1 if L&gt;1,I&gt;1 PrSBO(Wi ti-L5i) if L&gt;1,I=1 PrsBo(w I ti—L+1,i) if L&gt;1,I=0 PrAD (wi) if L=0,1= 0 In the equations above, the unigram probabilities are calculated by using an additive smoothing with 6 = 10-2 which is chosen through experiments. The equation for the additive smoothing (Chen, 1996) is as follows: Fq(t,—K,i) ± 6 Pr (t I ti—K,i-1) = Eti (Fq(ti_K,i) + 6) AD 3.2 Joint independence The parameters of an HMM may have different degree of statistical reliability because parameter reliability depends on the frequency of conditional term. For example, let a corpus consist of 1 million words and let the following parameters be extracted from the corpus by using the maximum likelihood estimation. Pr(a) = 0.01 Pr(d I a) = 0.1 Pr(b) = 0.001 Pr(d = 0.1 Pr(c) = 0.0001 Pr(d = 0.1 In this case, three conditional probabilities, Pr(d I a), Pr(d I b), and Pr(d I c) are all 0.1 but Pr(d I a) </context>
</contexts>
<marker>Chen, 1996</marker>
<rawString>S. F. Chen. 1996. Building Probabilistic Models for Natural Language. Doctoral Dissertation, Harvard University, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Cinlar</author>
</authors>
<title>Introduction to Stochastic Processes.</title>
<date>1975</date>
<publisher>Prentice-Hall,</publisher>
<location>New Jersey.</location>
<contexts>
<context position="1952" citStr="Cinlar, 1975" startWordPosition="288" endWordPosition="289">viewed as a classification problem (Mitchell, 1997). Over a decade, many works for POS tagging have used a wide range of machine learning techniques such as a hidden Markov model (HMM) (Charniak et al., 1993), a maximum entropy model (Ratnaparkhi, 1996), transformation rules (Brill, 1994), a decision tree (Lee et al., 1999), relaxation labeling (Padr6, 1996), Bayesian inference (Samuelsson, 1993), discriminative learning (Lin, 1992), a neural network (Schmid, 1994), and so on. In this paper we propose hidden Markov models for part-of-speech tagging, which adopt a less strict Markov assumption(Cinlar, 1975) to consider rich contexts. Because such models have a large number of parameters, they must suffer from sparse-data problem unless they have an enough volume of training corpus. Moreover, because such models assume conditional independence, the probability estimates of their parameters may have statistically different reliability that depends on the number of samples of conditional terms. To overcome the first problem, a simplified version of the well-known back-off smoothing method is used. To mitigate unreliable estimation problem, our models assume joint independence between random variabl</context>
</contexts>
<marker>Cinlar, 1975</marker>
<rawString>E. Cinlar. 1975. Introduction to Stochastic Processes. Prentice-Hall, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duda</author>
<author>R E Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>John Wiley.</publisher>
<marker>Duda, Hart, 1973</marker>
<rawString>R. 0. Duda and R. E. Hart. 1973. Pattern Classification and Scene Analysis. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H Kueera</author>
</authors>
<title>Frequency Analysis of English Usage: Lexicon and Grammar.</title>
<date>1982</date>
<publisher>Houghton Mifflin Company,</publisher>
<location>Boston, Massachusetts.</location>
<marker>Francis, Kueera, 1982</marker>
<rawString>W. N. Francis and H. Kueera. 1982. Frequency Analysis of English Usage: Lexicon and Grammar. Houghton Mifflin Company, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I J Good</author>
</authors>
<title>The Population Frequencies of Species and the Estimation of Population Parameters,&amp;quot;</title>
<date>1953</date>
<journal>In Biometrika,</journal>
<pages>40--3</pages>
<contexts>
<context position="8148" citStr="Good, 1953" startWordPosition="1404" endWordPosition="1406">rplexity measure, reported that a back-off smoothing(Katz, 1987) performs better on a small traning set than other methods. In the back-off smoothing, the smoothed probability of tag (K+1)-gram PrsBo(t, I t,_1(5%-1) is calculated as follows: Pr (t I ti—K,i-1) = SBO {dr PrML(t I ti-K,i-1) if r &gt; 0 a(ti_K,i-1) Pr sBo(t I ti-K+1,i-1)if r = 0 where r = Fq(ti—K,i), r* = (r+ 1) nr+1 ) nr T * (r+1)xn,±1 dr = r 1 (r+1) xn,±1. ro. In the equation above, nr denotes the number of (K+1)-gram whose frequency is r, and the coefficient dr is called the discount ratio, which reflects the Good-Turing estimate(Good, 1953)2. Eqn. 12 says that PrsBo(t I t,_K,,_1) is under-estimated by dr than its maximum likelihood estimate, if r &gt; 0, or is backed off by its smoothing term in proportion to the PrsBo(t% t%—i-c+15%—i) value of the function a(t,_K,,_1) of its conditional term 4_1(5,-1, if r = 0. However, because Eqn. 12 requires complicated computation in a(t,_K,,_1), we simplify it to get a function of the frequency of a conditional term, as follows: a(Fq(ti_K,i-1) = f) = x E[Fq(ti_K,i-1) = f] ET=0 = f] where = 1 PrSBO(tilti-K,i-1) Eti K,i5r&gt;0 PrmL(tilti_K,i_i) E[Fq(ti_K,i-1) = f] = Pr (tilti—K+1,i-1) SBO In Eqn. </context>
</contexts>
<marker>Good, 1953</marker>
<rawString>I. J. Good. 1953. &amp;quot;The Population Frequencies of Species and the Estimation of Population Parameters,&amp;quot; In Biometrika, 40(3-4):237-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer. In</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing(ASSP),</journal>
<volume>35</volume>
<issue>3</issue>
<pages>400--401</pages>
<contexts>
<context position="7601" citStr="Katz, 1987" startWordPosition="1302" endWordPosition="1303">et al., 1973) which maximizes the probability of a training set. The ML estimate of tag (K+1)-gram probability, Prmi(t% I 4-K 5%-i) , is calculated as follows: Pr (t, I t,_K,,_1) = (11) ML Fq(4_1(1) where the function Fq(x) returns the frequency of x in the training set. When using the ML estimation, data sparseness is even more serious in the extended models than in the standard models because the former has even more parameters than the latter. (Chen, 1996), where various smoothing techniques was tested for a language model by using the perplexity measure, reported that a back-off smoothing(Katz, 1987) performs better on a small traning set than other methods. In the back-off smoothing, the smoothed probability of tag (K+1)-gram PrsBo(t, I t,_1(5%-1) is calculated as follows: Pr (t I ti—K,i-1) = SBO {dr PrML(t I ti-K,i-1) if r &gt; 0 a(ti_K,i-1) Pr sBo(t I ti-K+1,i-1)if r = 0 where r = Fq(ti—K,i), r* = (r+ 1) nr+1 ) nr T * (r+1)xn,±1 dr = r 1 (r+1) xn,±1. ro. In the equation above, nr denotes the number of (K+1)-gram whose frequency is r, and the coefficient dr is called the discount ratio, which reflects the Good-Turing estimate(Good, 1953)2. Eqn. 12 says that PrsBo(t I t,_K,,_1) is under-est</context>
<context position="9136" citStr="Katz, 1987" startWordPosition="1584" endWordPosition="1585">tion of the frequency of a conditional term, as follows: a(Fq(ti_K,i-1) = f) = x E[Fq(ti_K,i-1) = f] ET=0 = f] where = 1 PrSBO(tilti-K,i-1) Eti K,i5r&gt;0 PrmL(tilti_K,i_i) E[Fq(ti_K,i-1) = f] = Pr (tilti—K+1,i-1) SBO In Eqn. 13, the range of f is bucketed into 7 regions such as f = 0, 1, 2, 3, 4, 5 and f &gt; 6 since it is also difficult to compute this equation for all possible values of f. Using the formalism of our simplified backoff smoothing, each of probabilities whose ML estimate is zero is backed off by its corresponding smoothing term. In experiments, the smoothing terms of PrsBo(t, 2111 (Katz, 1987) dr = 1 if r &gt; 5. 711 (13) ,r=0,Fq(t-K,- 1)=f (12) ti_K5i_1,Wi-J5i-1) are determined as follows: lows: Pr sB0(t I i t -K+15i-1, ) Wi-J+15i-1 ifK&gt;1_,J&gt;1 Pr S BO(ti ti-K5i-1) if K&gt;l,J=1 Pr s Bo(ti ti-K+15i-1) if K&gt; 1,J= 0 Pr AD(ti) if K= 1,J=0 Also, the smoothing terms of PrsBo(w, t,_L,„ w,_/,,_1) are determined as follows: Pr SBO(Wi I 5 ) ti-L+1 Wi-I+15i-1 if L&gt;1,I&gt;1 PrSBO(Wi ti-L5i) if L&gt;1,I=1 PrsBo(w I ti—L+1,i) if L&gt;1,I=0 PrAD (wi) if L=0,1= 0 In the equations above, the unigram probabilities are calculated by using an additive smoothing with 6 = 10-2 which is chosen through experiments. The</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>S. M. Katz. 1987. Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer. In IEEE Transactions on Acoustics, Speech and Signal Processing(ASSP), 35(3) :400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rim</author>
</authors>
<title>A Part-of-Speech Tagging Model Using Lexical Rules Based on Corpus Statistics.</title>
<date>1999</date>
<booktitle>In Proc. of the International Conference on Computer Processing of Oriental Languages (ICCP OL -</booktitle>
<volume>99</volume>
<pages>385--390</pages>
<marker>Rim, 1999</marker>
<rawString>S.-Z. Lee, J.-D. Kim, W.-H. Ryu, and HC. Rim. 1999. A Part-of-Speech Tagging Model Using Lexical Rules Based on Corpus Statistics. In Proc. of the International Conference on Computer Processing of Oriental Languages (ICCP OL - 99), 385-390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-Z Lee</author>
</authors>
<title>New Statistical Models for Automatic POS Tagging. Doctoral Dissertation,</title>
<date>1999</date>
<institution>Korea University,</institution>
<marker>Lee, 1999</marker>
<rawString>S.-Z. Lee. 1999. New Statistical Models for Automatic POS Tagging. Doctoral Dissertation, Korea University, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-C Lin</author>
<author>T-H Chiang</author>
<author>K-Y Su</author>
</authors>
<title>Discrimination Oriented Probabilistic Tagging.</title>
<date>1992</date>
<booktitle>In Proc. of the 5th International Conference: Research on Computational Linguistics (ROCLING- V),</booktitle>
<pages>85--96</pages>
<marker>Lin, Chiang, Su, 1992</marker>
<rawString>Y.-C. Lin, T.-H. Chiang, and K.-Y. Su. 1992. Discrimination Oriented Probabilistic Tagging. In Proc. of the 5th International Conference: Research on Computational Linguistics (ROCLING- V), 85-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging Text with a Probabilistic Model.</title>
<date>1991</date>
<booktitle>In Proc. of the International Conference on Acoustic, Speech and Signal Processing (ICA SSP-91),</booktitle>
<pages>809--812</pages>
<contexts>
<context position="5906" citStr="Merialdo, 1991" startWordPosition="1007" endWordPosition="1008"> should be extended so that it can consult rich information in contexts. 2.2 Extended models An extended HMM, A(T(K, (L,I)), for POS tagging is defined by making the following two less strict Markov assumption, Eqn. 8 and Eqn. 9, as follows: Pr(ti tl,i-1, Wl,i-1) C-Z1 Pr(ti ti-K,i-1, (8) Pr(wi Pr(wi I ti-L,i,wi-i,i-i) (9) A(T(K,J), W(L,n) p Pr(ti,n,w,n) Pr(ti I ti-K,i- 1, Wi-J,i-1) (10) x Pr(wi I ti-L,i, Wi-/,i-1) ) In a model A(T(K, (L,I)), the probability of the current tag t, conditionally depends on &apos;Usually, K is determined as 1 (bigram as in (Charniak et al., 1993)) or 2 (trigram as in (Merialdo, 1991)). (3) i=1 both the previous K tags t,_K,,_1 and the previous J words and the probability of the current word w, conditionally depends on the current tag and the previous L tags t,_L,, and the previous / words w,_/,,_1. In experiments, we set K as 1 or 2, J as 0 or K, L as 1 or 2, and / as 0 or L. If J and / are zero, the above models are non-lexicalized models. Otherwise, they are lexicalized models. In an extended model A(T(252), W(252)), for example, the probability of a node &amp;quot;a/AT&amp;quot; of the most likely sequence in Figure 1 is calculated as follows: Pr(AT I NNS,VB,Flies,like) x Pr(a I AT, NNS</context>
</contexts>
<marker>Merialdo, 1991</marker>
<rawString>B. Merialdo. 1991. Tagging Text with a Probabilistic Model. In Proc. of the International Conference on Acoustic, Speech and Signal Processing (ICA SSP-91), 809-812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Mitchell</author>
</authors>
<date>1997</date>
<booktitle>Machine Learning.</booktitle>
<publisher>McGraw-Hill.</publisher>
<location>New York:</location>
<contexts>
<context position="1390" citStr="Mitchell, 1997" startWordPosition="201" endWordPosition="202">ack-off smoothing method is used. To mitigate unreliable estimation problem, our models assume joint independence instead of conditional independence because joint probabilities have the same degree of estimation reliability. In experiments for the Brown corpus, models with rich contexts achieve relatively high accuracy and some models assuming joint independence show better results than the corresponding HMMs. 1 Introduction Part-of-speech (POS) tagging can be defined as a process in which a proper POS tag is assigned to each word in texts and so it can be viewed as a classification problem (Mitchell, 1997). Over a decade, many works for POS tagging have used a wide range of machine learning techniques such as a hidden Markov model (HMM) (Charniak et al., 1993), a maximum entropy model (Ratnaparkhi, 1996), transformation rules (Brill, 1994), a decision tree (Lee et al., 1999), relaxation labeling (Padr6, 1996), Bayesian inference (Samuelsson, 1993), discriminative learning (Lin, 1992), a neural network (Schmid, 1994), and so on. In this paper we propose hidden Markov models for part-of-speech tagging, which adopt a less strict Markov assumption(Cinlar, 1975) to consider rich contexts. Because su</context>
<context position="13775" citStr="Mitchell, 1997" startWordPosition="2386" endWordPosition="2387"> assign all possible tags to each word, we made two assumption: closed vocabulary assumption and open vocabulary assumption. For closed vocabulary assumption, we looked up a dictionary tailored to the Brown corpus. In this case, the average number of tags per word became 1.64. For open vocabulary assumption, we looked up a dictionary tailored only to a training set in order to assign possible tags to frequent words whose frequency is greater than 5. In case of rare words, tags in the dictionary were assigned and then 6 tags with highest score were assigned by using a naive Bayesian classifier(Mitchell, 1997) considering character features as follows: Pr(ti, wi) = Pr(t) x Pr(w I ti) 3Note that some sentences, which have composite tags(such as &amp;quot;HV±TO&amp;quot; in &amp;quot;hafta&amp;quot;), &amp;quot;ILLEGAL&amp;quot; tag, or &amp;quot;NIL&amp;quot; tag, were removed from the Brown corpus and tags with &amp;quot;*&amp;quot; (not) such as &amp;quot;BEZ*&amp;quot; were replaced by corresponding tags without &amp;quot;*&amp;quot; such as &amp;quot;BEZ&amp;quot;. Pr(t) x H Pr(fl ti) j=1 where A/ indicates j-th character features of w, and F(=12) is the number of character feature types including prefixes (whose length is 1 through 4), suffixes (whose length is 1 through 4), if w, contains numbers, if w, contains an initial uppercase l</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>T. M. Mitchell. 1997. Machine Learning. New York: McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Padr6</author>
</authors>
<title>POS Tagging Using Relaxation Labeling.</title>
<date>1996</date>
<tech>Research Report LSI-96-10-R.</tech>
<institution>Department de Llenguatgatges i Sistemes Informatics, Universitat Politecnica de Catalunya.</institution>
<marker>Padr6, 1996</marker>
<rawString>L. Padr6. 1996. POS Tagging Using Relaxation Labeling. Research Report LSI-96-10-R. Department de Llenguatgatges i Sistemes Informatics, Universitat Politecnica de Catalunya.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Proc. of the Empirical Methods in Natural Language Processing Conference(EMNLP-96),</booktitle>
<pages>133--142</pages>
<contexts>
<context position="1592" citStr="Ratnaparkhi, 1996" startWordPosition="236" endWordPosition="237">of estimation reliability. In experiments for the Brown corpus, models with rich contexts achieve relatively high accuracy and some models assuming joint independence show better results than the corresponding HMMs. 1 Introduction Part-of-speech (POS) tagging can be defined as a process in which a proper POS tag is assigned to each word in texts and so it can be viewed as a classification problem (Mitchell, 1997). Over a decade, many works for POS tagging have used a wide range of machine learning techniques such as a hidden Markov model (HMM) (Charniak et al., 1993), a maximum entropy model (Ratnaparkhi, 1996), transformation rules (Brill, 1994), a decision tree (Lee et al., 1999), relaxation labeling (Padr6, 1996), Bayesian inference (Samuelsson, 1993), discriminative learning (Lin, 1992), a neural network (Schmid, 1994), and so on. In this paper we propose hidden Markov models for part-of-speech tagging, which adopt a less strict Markov assumption(Cinlar, 1975) to consider rich contexts. Because such models have a large number of parameters, they must suffer from sparse-data problem unless they have an enough volume of training corpus. Moreover, because such models assume conditional independence</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A Maximum Entropy Model for Part-of-Speech Tagging. In Proc. of the Empirical Methods in Natural Language Processing Conference(EMNLP-96), 133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelsson</author>
</authors>
<title>Morphological Tagging Based Entirely on Bayesian Inference.</title>
<date>1993</date>
<booktitle>In Proc. of the 9&apos; Nordic Conference on Computational Linguistics,</booktitle>
<pages>225--238</pages>
<contexts>
<context position="1738" citStr="Samuelsson, 1993" startWordPosition="255" endWordPosition="256"> joint independence show better results than the corresponding HMMs. 1 Introduction Part-of-speech (POS) tagging can be defined as a process in which a proper POS tag is assigned to each word in texts and so it can be viewed as a classification problem (Mitchell, 1997). Over a decade, many works for POS tagging have used a wide range of machine learning techniques such as a hidden Markov model (HMM) (Charniak et al., 1993), a maximum entropy model (Ratnaparkhi, 1996), transformation rules (Brill, 1994), a decision tree (Lee et al., 1999), relaxation labeling (Padr6, 1996), Bayesian inference (Samuelsson, 1993), discriminative learning (Lin, 1992), a neural network (Schmid, 1994), and so on. In this paper we propose hidden Markov models for part-of-speech tagging, which adopt a less strict Markov assumption(Cinlar, 1975) to consider rich contexts. Because such models have a large number of parameters, they must suffer from sparse-data problem unless they have an enough volume of training corpus. Moreover, because such models assume conditional independence, the probability estimates of their parameters may have statistically different reliability that depends on the number of samples of conditional </context>
</contexts>
<marker>Samuelsson, 1993</marker>
<rawString>C. Samuelsson. 1993. Morphological Tagging Based Entirely on Bayesian Inference. In Proc. of the 9&apos; Nordic Conference on Computational Linguistics, 225-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Part-of-Speech Tagging with Neural Networks.</title>
<date>1994</date>
<booktitle>In Proc. of the International Conference on Computational Linguistics (COLING-94),</booktitle>
<pages>172--176</pages>
<contexts>
<context position="1808" citStr="Schmid, 1994" startWordPosition="266" endWordPosition="267">roduction Part-of-speech (POS) tagging can be defined as a process in which a proper POS tag is assigned to each word in texts and so it can be viewed as a classification problem (Mitchell, 1997). Over a decade, many works for POS tagging have used a wide range of machine learning techniques such as a hidden Markov model (HMM) (Charniak et al., 1993), a maximum entropy model (Ratnaparkhi, 1996), transformation rules (Brill, 1994), a decision tree (Lee et al., 1999), relaxation labeling (Padr6, 1996), Bayesian inference (Samuelsson, 1993), discriminative learning (Lin, 1992), a neural network (Schmid, 1994), and so on. In this paper we propose hidden Markov models for part-of-speech tagging, which adopt a less strict Markov assumption(Cinlar, 1975) to consider rich contexts. Because such models have a large number of parameters, they must suffer from sparse-data problem unless they have an enough volume of training corpus. Moreover, because such models assume conditional independence, the probability estimates of their parameters may have statistically different reliability that depends on the number of samples of conditional terms. To overcome the first problem, a simplified version of the well</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H. Schmid. 1994. Part-of-Speech Tagging with Neural Networks. In Proc. of the International Conference on Computational Linguistics (COLING-94), 172-176.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>