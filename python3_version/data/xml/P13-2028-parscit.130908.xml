<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000029">
<title confidence="0.994021">
Post-Retrieval Clustering Using Third-Order Similarity Measures
</title>
<author confidence="0.963855">
Jos´e G. Moreno
</author>
<affiliation confidence="0.965728">
Normandie University
</affiliation>
<address confidence="0.637172">
UNICAEN, GREYC CNRS
F-14032 Caen, France
</address>
<email confidence="0.996229">
jose.moreno@unicaen.fr
</email>
<author confidence="0.969278">
Ga¨el Dias
</author>
<affiliation confidence="0.972576">
Normandie University
</affiliation>
<address confidence="0.639994">
UNICAEN, GREYC CNRS
F-14032 Caen, France
</address>
<email confidence="0.995978">
gael.dias@unicaen.fr
</email>
<author confidence="0.980098">
Guillaume Cleuziou
</author>
<affiliation confidence="0.6715735">
University of Orl´eans
LIFO
</affiliation>
<address confidence="0.494398">
F-45067 Orl´eans, France
</address>
<email confidence="0.725269">
cleuziou@univ-orleans.fr
</email>
<sectionHeader confidence="0.99441" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999337636363636">
Post-retrieval clustering is the task of clus-
tering Web search results. Within this
context, we propose a new methodology
that adapts the classical K-means algo-
rithm to a third-order similarity measure
initially developed for NLP tasks. Results
obtained with the definition of a new stop-
ping criterion over the ODP-239 and the
MORESQUE gold standard datasets evi-
dence that our proposal outperforms all re-
ported text-based approaches.
</bodyText>
<sectionHeader confidence="0.999396" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999816765625">
Post-retrieval clustering (PRC), also known as
search results clustering or ephemeral clustering,
is the task of clustering Web search results. For
a given query, the retrieved Web snippets are au-
tomatically clustered and presented to the user
with meaningful labels in order to minimize the
information search process. This technique can
be particularly useful for polysemous queries but
it is hard to implement efficiently and effectively
(Carpineto et al., 2009). Indeed, as opposed to
classical text clustering, PRC must deal with small
collections of short text fragments (Web snippets)
and be processed in run-time.
As a consequence, most of the successful
methodologies follow a monothetic approach (Za-
mir and Etzioni, 1998; Ferragina and Gulli, 2008;
Carpineto and Romano, 2010; Navigli and Crisa-
fulli, 2010; Scaiella et al., 2012). The underlying
idea is to discover the most discriminant topical
words in the collection and group together Web
snippets containing these relevant terms. On the
other hand, the polythetic approach which main
idea is to represent Web snippets as word feature
vectors has received less attention, the only rele-
vant work being (Osinski and Weiss, 2005). The
main reasons for this situation are that (1) word
feature vectors are hard to define in small collec-
tions of short text fragments (Timonen, 2013), (2)
existing second-order similarity measures such as
the cosine are unadapted to capture the seman-
tic similarity between small texts, (3) Latent Se-
mantic Analysis has evidenced inconclusive re-
sults (Osinski and Weiss, 2005) and (4) the la-
beling process is a surprisingly hard extra task
(Carpineto et al., 2009).
This paper is motivated by the fact that the poly-
thetic approach should lead to improved results if
correctly applied to small collections of short text
fragments. For that purpose, we propose a new
methodology that adapts the classical K-means
algorithm to a third-order similarity measure ini-
tially developed for Topic Segmentation (Dias et
al., 2007). Moreover, the adapted K-means algo-
rithm allows to label each cluster directly from its
centroids thus avoiding the abovementioned extra
task. Finally, the evolution of the objective func-
tion of the adapted K-means is modeled to auto-
matically define the “best” number of clusters.
Finally, we propose different experiments over
the ODP-239 (Carpineto and Romano, 2010)
and MORESQUE (Navigli and Crisafulli, 2010)
datasets against the most competitive text-based
PRC algorithms: STC (Zamir and Etzioni, 1998),
LINGO (Osinski and Weiss, 2005), OPTIMSRC
(Carpineto and Romano, 2010) and the classical
bisecting incremental K-means (which may be
seen as a baseline for the polythetic paradigm)1.
A new evaluation measure called the b-cubed F-
measure (Fbs) and defined in (Amig´o et al., 2009)
is then calculated to evaluate both cluster homo-
geneity and completeness. Results evidence that
our proposal outperforms all state-of-the-art ap-
proaches with a maximum Fbs = 0.452 for ODP-
239 and Fbs = 0.490 for MORESQUE.
</bodyText>
<footnote confidence="0.990389">
1The TOPICAL algorithm proposed by (Scaiella et
al., 2012) is a knowledge-driven methodology based on
Wikipedia.
</footnote>
<page confidence="0.960608">
153
</page>
<note confidence="0.553925">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 153–158,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.657761" genericHeader="method">
2 Polythetic Post-Retrieval Clustering
</sectionHeader>
<equation confidence="0.9936025">
K
X
k=1
QS3s =
X
xiEπk
</equation>
<bodyText confidence="0.992093428571429">
The K-means is a geometric clustering algorithm
(Lloyd, 1982). Given a set of n data points, the
algorithm uses a local search approach to partition
the points into K clusters. A set of K initial clus-
ter centers is chosen. Each point is then assigned
to the center closest to it and the centers are recom-
puted as centers of mass of their assigned points.
The process is repeated until convergence. To as-
sure convergence, an objective function Q is de-
fined which decreases at each processing step. The
classical objective function is defined in Equation
(1) where Irk is a cluster labeled k, xi E Irk is
an object in the cluster, mπk is the centroid of the
cluster Irk and E(.,.) is the Euclidean distance.
</bodyText>
<equation confidence="0.903913">
E(xi, mπk)2. (1)
</equation>
<bodyText confidence="0.999778769230769">
Within the context of PRC, the K-means algo-
rithm needs to be adapted to integrate third-order
similarity measures (Mihalcea et al., 2006; Dias
et al., 2007). Third-order similarity measures,
also called weighted second-order similarity mea-
sures, do not rely on exact matches of word fea-
tures as classical second-order similarity measures
(e.g. the cosine metric), but rather evaluate simi-
larity based on related matches. In this paper, we
propose to use the third-order similarity measure
called InfoSimba introduced in (Dias et al., 2007)
for Topic Segmentation and implement its simpli-
fied version S3s in Equation 2.
</bodyText>
<equation confidence="0.80115">
S3 s(Xi,Xj) = p2 1
</equation>
<bodyText confidence="0.9999109">
Given two Web snippets Xi and Xj, their sim-
ilarity is evaluated by the similarity of its con-
stituents based on any symmetric similarity mea-
sure S(.,.) where Wik (resp. Wjl) corresponds to
the word at the kth (resp. lth) position in the vector
Xi (resp. Xj) and Xik (resp. Xjl) is the weight of
word Wik (resp. Wjl) in the set of retrieved Web
snippets. A direct consequence of the change in
similarity measure is the definition of a new ob-
jective function QS3 to ensure convergence. This
</bodyText>
<subsectionHeader confidence="0.692345">
s
</subsectionHeader>
<bodyText confidence="0.8780145">
function is defined in Equation (3) and must be
maximized2.
</bodyText>
<footnote confidence="0.7939705">
2A maximization process can easily be transformed into a
minimization one
</footnote>
<equation confidence="0.737415">
S3 s(xi, mπk). (3)
</equation>
<bodyText confidence="0.96562">
A cluster centroid mπk is defined by a vector of
p words (wπk
1 , ... , wπk
p ). As a consequence, each
cluster centroid must be instantiated in such a way
that QS3 increases at each step of the clustering
</bodyText>
<subsectionHeader confidence="0.727041">
s
</subsectionHeader>
<bodyText confidence="0.9998408">
process. The choice of the best p words repre-
senting each cluster is a way of assuring conver-
gence. For that purpose, we define a procedure
which consists in selecting the best p words from
the global vocabulary V in such a way that QS3
</bodyText>
<subsectionHeader confidence="0.730833">
s
</subsectionHeader>
<bodyText confidence="0.999969">
increases. The global vocabulary is the set of all
words which appear in any context vector.
So, for each word w E V and any symmet-
ric similarity measure S(.,.), its interestingness
Ak(w) is computed as regards to cluster Irk. This
operation is defined in Equation (4) where si E Irk
is any Web snippet from cluster Irk. Finally, the p
words with higher Ak(w) are selected to construct
the cluster centroid. In such a way, we can easily
prove that QS3 is maximized. Note that a word
</bodyText>
<equation confidence="0.745444">
s
which is not part of cluster Irk may be part of the
centroid mπk.
λk (w) =1
p s,Ek
</equation>
<bodyText confidence="0.999991684210526">
Finally, we propose to rely on a modified ver-
sion of the K-means algorithm called Global K-
means (Likasa et al., 2003), which has proved to
lead to improved results. To solve a clustering
problem with M clusters, all intermediate prob-
lems with 1, 2,..., M − 1 clusters are sequentially
solved. The underlying idea is that an optimal so-
lution for a clustering problem with M clusters
can be obtained using a series of local searches us-
ing the K-means algorithm. At each local search,
the M − 1 cluster centers are always initially
placed at their optimal positions corresponding to
the clustering problem with M − 1 clusters. The
remaining Mth cluster center is initially placed at
several positions within the data space. In addi-
tion to effectiveness, the method is deterministic
and does not depend on any initial conditions or
empirically adjustable parameters. Moreover, its
adaptation to PRC is straightforward.
</bodyText>
<sectionHeader confidence="0.968474" genericHeader="method">
3 Stopping Criterion
</sectionHeader>
<bodyText confidence="0.991857">
Once clustering has been processed, selecting the
best number of clusters still remains to be decided.
</bodyText>
<equation confidence="0.998841">
K
X
k=1
Q =
X
xiEπk
Xik * Xjl * S(Wik, Wjl). (2)
X p
l=1
p
X
k=1
X S(wiq, w). (4)
wi qEsi
</equation>
<page confidence="0.997434">
154
</page>
<bodyText confidence="0.998767777777778">
For that purpose, numerous procedures have been
proposed (Milligan and Cooper, 1985). However,
none of the listed methods were effective or adapt-
able to our specific problem. So, we proposed
a procedure based on the definition of a ratio-
nal function which models the quality criterion
QS3 s . To better understand the behaviour of QS3 s
at each step of the adapted GK-means algorithm,
we present its values for K = 10 in Figure 1.
</bodyText>
<figureCaption confidence="0.998962">
Figure 1: QS3 and its modelisation.
</figureCaption>
<equation confidence="0.601853333333333">
s
QS3 can be modelled as in Equation (5) which
s
converges to a limit α when K increases and starts
from Q1 s at K = 1). The underlying
S3 s (i.e. QS3
</equation>
<bodyText confidence="0.99844025">
idea is that the best number of clusters is given by
the 0 value which maximizes the difference with
the average 0mean. So, α, 0 and -y need to be
expressed independently of unknown variables.
</bodyText>
<equation confidence="0.9350495">
∀K, f(K) = α − γ. (5)
Kβ
</equation>
<bodyText confidence="0.9770749">
As α can theoretically or operationally be de-
fined and it can easily be proved that -y = α − Q S13,
0 needs to be defined based on -y or α. This can
also be easily proved and the given result is ex-
pressed in Equation (6).
snippet individually. Finally, the best number of
clusters is defined as in Algorithm (1) and each
one receives its label based on the p words with
greater interestingness of its centroid M1k.
Algorithm 1 The best K selection procedure.
</bodyText>
<listItem confidence="0.99921075">
1. Calculate 0K for each K
2. Evaluate the mean of all 0K i.e. 0mean
3. Select 0K which maximizes 0K l— 0mean
4. Return K as the best number of partitions
</listItem>
<bodyText confidence="0.999664785714286">
This situation is illustrated in Figure (1) where
the red line corresponds to the rational functional
for 0mean and the blue line models the best 0
value (i.e. the one which maximizes the difference
with 0mean). In this case, the best number would
correspond to 06 and as a consequence, the best
number of clusters would be 6. In order to illus-
trate the soundness of the procedure, we present
the different values for 0 at each K iteration and
the differences between consecutive values of 0 at
each iteration in Figure 2. We clearly see that the
highest inclination of the curve is between clus-
ter 5 and 6 which also corresponds to the highest
difference between two consecutive values of 0.
</bodyText>
<figureCaption confidence="0.872238">
Figure 2: Values of 0 (on the left) and differences
between consecutive values of 0 (on the right).
</figureCaption>
<equation confidence="0.9764116">
)
.log(K)
(6)
β =
log(α − Q1S3s) − log(α − QKS3s
</equation>
<sectionHeader confidence="0.644251" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9999567">
Now, the value of α which best approximates
the limit of the rational function must be defined.
For that purpose, we computed its maximum theo-
retical and experimental values as well as its ap-
proximated maximum experimental value based
on the S2-Aitken (Aitken, 1926) procedure to ac-
celerate convergence as explained in (Kuroda et
al., 2008). Best results were obtained with the
maximum experimental value which is defined as
building the cluster centroid M1k for each Web
Evaluating PRC systems is a difficult task as stated
in (Carpineto et al., 2009). Indeed, a successful
PRC system must evidence high quality level clus-
tering. Ideally, each query subtopic should be rep-
resented by a unique cluster containing all the rel-
evant Web pages inside. However, this task is far
from being achievable. As such, this constraint
is reformulated as follows: the task of PRC sys-
tems is to provide complete topical cluster cov-
erage of a given query, while avoiding excessive
</bodyText>
<page confidence="0.998441">
155
</page>
<table confidence="0.9997956">
Fb3 K Stop Criterion
2 3 4 5 6 7 8 9 10 Fb3 Avg. K
2 0.387 0.396 0.398 0.396 0.391 0.386 0.382 0.378 0.374 0.395 4.799
3 0.400 0.411 0.412 0.409 0.406 0.400 0.397 0.391 0.388 0.411 4.690
SCP p 4 0.405 0.416 0.423 0.425 0.423 0.420 0.416 0.414 0.411 0.441 4.766
5 0.408 0.422 0.431 0.431 0.429 0.429 0.423 0.422 0.421 0.452 4.778
2 0.391 0.399 0.397 0.393 0.388 0.383 0.377 0.373 0.366 0.393 4.778
3 0.408 0.418 0.422 0.418 0.414 0.410 0.405 0.398 0.392 0.416 4.879
PMI p 4 0.420 0.434 0.439 0.439 0.435 0.430 0.425 0.420 0.412 0.436 4.874
5 0.423 0.444 0.451 0.451 0.451 0.445 0.441 0.434 0.429 0.450 4.778
</table>
<tableCaption confidence="0.805435">
Table 1: Fb3 for SCP and PMI for the global search and the stopping criterion for the ODP-239 dataset.
</tableCaption>
<table confidence="0.999163">
Adapated GK-means STC LINGO BIK OPTIMSRC
SCP PMI
ODP-239 p p
2 3 4 5 2 3 4 5
F1 0.312 0.341 0.352 0.366 0.332 0.358 0.378 0.390 0.324 0.273 0.200 0.313
F2 0.363 0.393 0.404 0.416 0.363 0.395 0.421 0.435 0.319 0.167 0.173 0.341
F5 0.411 0.441 0.453 0.462 0.390 0.430 0.459 0.476 0.322 0.153 0.165 0.380
Fb3 0.395 0.411 0.441 0.452 0.393 0.416 0.436 0,450 0.403 0.346 0.307 N/A
MORESQUE F1 0.627 0.649 0.665 0.664 0.615 0.551 0.543 0.571 0.455 0.326 0.317 N/A
F2 0.685 0.733 0.767 0.770 0.644 0.548 0.521 0.551 0.392 0.260 0.269 N/A
F5 0.747 0.817 0.865 0.872 0.679 0.563 0.519 0.553 0.370 0.237 0.255 N/A
Fb3 0.482 0.482 0.473 0.464 0.490 0.465 0.462 0.485 0.460 0.399 0.315 N/A
</table>
<tableCaption confidence="0.999572">
Table 2: PRC comparative results for Fβ and Fb3 over the ODP-239 and MORESQUE datasets.
</tableCaption>
<bodyText confidence="0.999553666666667">
redundancy of the subtopics in the result list of
clusters. So, in order to evaluate our methodol-
ogy, we propose two different evaluations. First,
we want to evidence the quality of the stopping
criterion when compared to an exhaustive search
over all tunable parameters. Second, we propose a
comparative evaluation with existing state-of-the-
art algorithms over gold standard datasets and re-
cent clustering evaluation metrics.
</bodyText>
<subsectionHeader confidence="0.992437">
4.1 Text Processing
</subsectionHeader>
<bodyText confidence="0.999963529411765">
Before the clustering process takes place, Web
snippets are represented as word feature vectors.
In order to define the set of word features, the
Web service proposed in (Machado et al., 2009) is
used3. In particular, it assigns a relevance score to
any token present in the set of retrieved Web snip-
pets based on the analysis of left and right token
contexts. A specific threshold is then applied to
withdraw irrelevant tokens and the remaining ones
form the vocabulary V . Then, each Web snippet is
represented by the set of its p most relevant to-
kens in the sense of the W(.) value proposed in
(Machado et al., 2009). Note that within the pro-
posed Web service, multiword units are also iden-
tified. They are exclusively composed of relevant
individual tokens and their weight is given by the
arithmetic mean of their constituents scores.
</bodyText>
<footnote confidence="0.523186">
3Access to this Web service is available upon request.
</footnote>
<subsectionHeader confidence="0.864598">
4.2 Intrinsic Evaluation
</subsectionHeader>
<bodyText confidence="0.99995695">
The first set of experiments focuses on understand-
ing the behaviour of our methodology within a
greedy search strategy for different tunable param-
eters defined as a tuple &lt; p, K, S(Wik, Wjl) &gt;.
In particular, p is the size of the word feature vec-
tors representing both Web snippets and centroids
(p = 2..5), K is the number of clusters to be
found (K = 2..10) and S(Wik, Wjl) is the col-
location measure integrated in the InfoSimba sim-
ilarity measure. In these experiments, two asso-
ciation measures which are known to have dif-
ferent behaviours (Pecina and Schlesinger, 2006)
are tested. We implement the Symmetric Condi-
tional Probability (Silva et al., 1999) in Equation
(7) which tends to give more credits to frequent as-
sociations and the Pointwise Mutual Information
(Church and Hanks, 1990) in Equation (8) which
over-estimates infrequent associations. Then, best
&lt; p, K, S(Wik, Wjl) &gt; configurations are com-
pared to our stopping criterion.
</bodyText>
<equation confidence="0.9990858">
P(Wik, Wjl)2
SCP(Wik, Wjl) = (7)
P(Wik) × P(Wjl).
P(Wik, Wjl)
PMI(Wik, Wjl) = log2 P(Wik) × P(Wjl). (8)
</equation>
<bodyText confidence="0.93210525">
In order to perform this task, we evaluate per-
formance based on the Fb3 measure defined in
(Amig´o et al., 2009) over the ODP-239 gold stan-
dard dataset proposed in (Carpineto and Romano,
</bodyText>
<page confidence="0.997386">
156
</page>
<bodyText confidence="0.988647277777778">
2010). In particular, (Amig´o et al., 2009) indi-
cate that common metrics such as the Fβ-measure
are good to assign higher scores to clusters with
high homogeneity, but fail to evaluate cluster com-
pleteness. First results are provided in Table 1 and
evidence that the best configurations for different
&lt; p, K, S(Wik, Wjl) &gt; tuples are obtained for
high values of p, K ranging from 4 to 6 clusters
and PMI steadily improving over SCP. How-
ever, such a fuzzy configuration is not satisfac-
tory. As such, we proposed a new stopping cri-
terion which evidences coherent results as it (1)
does not depend on the used association measure
(FSCP bs= 0.452 and FP MI
bs = 0.450), (2) discov-
ers similar numbers of clusters independently of
the length of the p-context vector and (3) increases
performance with high values of p.
</bodyText>
<subsectionHeader confidence="0.998285">
4.3 Comparative Evaluation
</subsectionHeader>
<bodyText confidence="0.999976101694915">
The second evaluation aims to compare our
methodology to current state-of-the-art text-based
PRC algorithms. We propose comparative exper-
iments over two gold standard datasets (ODP-239
(Carpineto and Romano, 2010) and MORESQUE
(Di Marco and Navigli, 2013)) for STC (Za-
mir and Etzioni, 1998), LINGO (Osinski and
Weiss, 2005), OPTIMSRC (Carpineto and Ro-
mano, 2010) and the Bisecting Incremental K-
means (BIK) which may be seen as a baseline for
the polythetic paradigm. A brief description of
each PRC algorithm is given as follows.
STC: (Zamir and Etzioni, 1998) defined the
Suffix Tree Clustering algorithm which is still a
difficult standard to beat in the field. In partic-
ular, they propose a monothetic clustering tech-
nique which merges base clusters with high string
overlap. Indeed, instead of using the classical Vec-
tor Space Model (VSM) representation, they pro-
pose to represent Web snippets as compact tries.
LINGO: (Osinski and Weiss, 2005) proposed a
polythetic solution called LINGO which takes into
account the string representation proposed by (Za-
mir and Etzioni, 1998). They first extract frequent
phrases based on suffix-arrays. Then, they reduce
the term-document matrix (defined as a VSM) us-
ing Single Value Decomposition to discover latent
structures. Finally, they match group descriptions
with the extracted topics and assign relevant doc-
uments to them.
OPTIMSRC: (Carpineto and Romano, 2010)
showed that the characteristics of the outputs re-
turned by PRC algorithms suggest the adoption of
a meta clustering approach. As such, they intro-
duce a novel criterion to measure the concordance
of two partitions of objects into different clusters
based on the information content associated to the
series of decisions made by the partitions on single
pairs of objects. Then, the meta clustering phase
is casted to an optimization problem of the concor-
dance between the clustering combination and the
given set of clusterings.
With respect to implementation, we used the
Carrot2 APIs4 which are freely available for STC,
LINGO and the classical BIK. It is worth notic-
ing that all implementations in Carrot2 are tuned
to extract exactly 10 clusters. For OPTIMSRC,
we reproduced the results presented in the paper
of (Carpineto and Romano, 2010) as no imple-
mentation is freely available. The results are il-
lustrated in Table 2 including both Fβ-measure
and Fbs. They evidence clear improvements of
our methodology when compared to state-of-the-
art text-based PRC algorithms, over both datasets
and all evaluation metrics. But more important,
even when the p-context vector is small (p = 3),
the adapted GK-means outperforms all other ex-
isting text-based PRC which is particularly impor-
tant as they need to perform in real-time.
</bodyText>
<sectionHeader confidence="0.999652" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9998146">
In this paper, we proposed a new PRC ap-
proach which (1) is based on the adaptation of
the K-means algorithm to third-order similar-
ity measures and (2) proposes a coherent stop-
ping criterion. Results evidenced clear improve-
ments over the evaluated state-of-the-art text-
based approaches for two gold standard datasets.
Moreover, our best F1-measure over ODP-239
(0.390) approximates the highest ever-reached F1-
measure (0.413) by the TOPICAL knowledge-
driven algorithm proposed in (Scaiella et al.,
2012)5. These results are promising and in future
works, we propose to define new knowledge-based
third-order similarity measures based on studies in
entity-linking (Ferragina and Scaiella, 2010).
</bodyText>
<footnote confidence="0.9940874">
4http://search.carrot2.org/stable/search [Last access:
15/05/2013].
5Notice that the authors only propose the Fl-measure al-
though different results can be obtained for different Fβ-
measures and Fbs as evidenced in Table 2.
</footnote>
<page confidence="0.995378">
157
</page>
<sectionHeader confidence="0.996281" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999797804347826">
A.C. Aitken. 1926. On bernoulli’s numerical solution
of algebraic equations. Research Society Edinburgh,
46:289–305.
E. Amig´o, J. Gonzalo, J. Artiles, and F. Verdejo. 2009.
A comparison of extrinsic clustering evaluation met-
rics based on formal constraints. Information Re-
trieval, 12(4):461–486.
C. Carpineto and G. Romano. 2010. Optimal meta
search results clustering. In 33rd International ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 170–177.
C. Carpineto, S. Osinski, G. Romano, and D. Weiss.
2009. A survey of web clustering engines. ACM
Computer Survey, 41(3):1–38.
K. Church and P. Hanks. 1990. Word association
norms mutual information and lexicography. Com-
putational Linguistics, 16(1):23–29.
A. Di Marco and R. Navigli. 2013. Clustering and
diversifying web search results with graph-based
word sense induction. Computational Linguistics,
39(4):1–43.
G. Dias, E. Alves, and J.G.P. Lopes. 2007. Topic
segmentation algorithms for text summarization and
passage retrieval: An exhaustive evaluation. In Pro-
ceedings of 22nd Conference on Artificial Intelli-
gence (AAAI), pages 1334–1339.
P. Ferragina and A. Gulli. 2008. A personalized search
engine based on web-snippet hierarchical clustering.
Software: Practice and Experience, 38(2):189–225.
P. Ferragina and U. Scaiella. 2010. Tagme: On-the-
fly annotation of short text fragments (by wikipedia
entities). In Proceedings of the 19th ACM Inter-
national Conference on Information and Knowledge
Management (CIKM), pages 1625–1628.
M. Kuroda, M. Sakakihara, and Z. Geng. 2008. Ac-
celeration of the em and ecm algorithms using the
aitken δ&apos; method for log-linear models with par-
tially classified data. Statistics &amp; Probability Let-
ters, 78(15):2332–2338.
A. Likasa, Vlassis. N., and J. Verbeek. 2003.
The global k-means clustering algorithm. Pattern
Recognition, 36:451–461.
S.P. Lloyd. 1982. Least squares quantization in
pcm. IEEE Transactions on Information Theory,
28(2):129–137.
D. Machado, T. Barbosa, S. Pais, B. Martins, and
G. Dias. 2009. Universal mobile information re-
trieval. In Proceedings of the 5th International Con-
ference on Universal Access in Human-Computer
Interaction (HCI), pages 345–354.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of
text semantic similarity. In Proceedings of the
21st National Conference on Artificial Intelligence
(AAAI), pages 775–780.
G.W. Milligan and M.C. Cooper. 1985. An exami-
nation of procedures for determining the number of
clusters in a data set. Psychometrika, 50(2):159–
179.
R. Navigli and G. Crisafulli. 2010. Inducing word
senses to improve web search result clustering.
In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 116–126.
S. Osinski and D. Weiss. 2005. A concept-driven algo-
rithm for clustering search results. IEEE Intelligent
Systems, 20(3):48–54.
P. Pecina and P. Schlesinger. 2006. Combining as-
sociation measures for collocation extraction. In
Proceedings of the Joint Conference of the Inter-
national Committee on Computational Linguistics
and the Association for Computational Linguistics
(COLING/ACL), pages 651–658.
U. Scaiella, P. Ferragina, A. Marino, and M. Ciaramita.
2012. Topical clustering of search results. In Pro-
ceedings of the 5th ACM International Conference
on Web Search and Data Mining (WSDM), pages
223–232.
J. Silva, G. Dias, S. Guillor´e, and J.G.P. Lopes. 1999.
Using localmaxs algorithm for the extraction of con-
tiguous and non-contiguous multiword lexical units.
In Proceedings of 9th Portuguese Conference in Ar-
tificial Intelligence (EPIA), pages 113–132.
M. Timonen. 2013. Term Weighting in Short Docu-
ments for Document Categorization, Keyword Ex-
traction and Query Expansion. Ph.D. thesis, Uni-
versity of Helsinki, Finland.
O. Zamir and O. Etzioni. 1998. Web document clus-
tering: A feasibility demonstration. In 21st Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR),
pages 46–54.
</reference>
<page confidence="0.997057">
158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.153307">
<title confidence="0.999362">Post-Retrieval Clustering Using Third-Order Similarity Measures</title>
<author confidence="0.994991">G Jos´e</author>
<affiliation confidence="0.770811">Normandie UNICAEN, GREYC</affiliation>
<address confidence="0.996979">F-14032 Caen, France</address>
<email confidence="0.995579">jose.moreno@unicaen.fr</email>
<author confidence="0.80386">Ga¨el</author>
<affiliation confidence="0.6622755">Normandie UNICAEN, GREYC</affiliation>
<address confidence="0.994228">F-14032 Caen, France</address>
<email confidence="0.997004">gael.dias@unicaen.fr</email>
<author confidence="0.929261">Guillaume</author>
<affiliation confidence="0.999833">University of</affiliation>
<address confidence="0.991179">F-45067 Orl´eans, France</address>
<email confidence="0.997018">cleuziou@univ-orleans.fr</email>
<abstract confidence="0.99457725">Post-retrieval clustering is the task of clustering Web search results. Within this context, we propose a new methodology adapts the classical algorithm to a third-order similarity measure initially developed for NLP tasks. Results obtained with the definition of a new stopping criterion over the ODP-239 and the MORESQUE gold standard datasets evidence that our proposal outperforms all reported text-based approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A C Aitken</author>
</authors>
<title>On bernoulli’s numerical solution of algebraic equations.</title>
<date>1926</date>
<journal>Research Society Edinburgh,</journal>
<pages>46--289</pages>
<contexts>
<context position="10923" citStr="Aitken, 1926" startWordPosition="1849" endWordPosition="1850">n in Figure 2. We clearly see that the highest inclination of the curve is between cluster 5 and 6 which also corresponds to the highest difference between two consecutive values of 0. Figure 2: Values of 0 (on the left) and differences between consecutive values of 0 (on the right). ) .log(K) (6) β = log(α − Q1S3s) − log(α − QKS3s 4 Evaluation Now, the value of α which best approximates the limit of the rational function must be defined. For that purpose, we computed its maximum theoretical and experimental values as well as its approximated maximum experimental value based on the S2-Aitken (Aitken, 1926) procedure to accelerate convergence as explained in (Kuroda et al., 2008). Best results were obtained with the maximum experimental value which is defined as building the cluster centroid M1k for each Web Evaluating PRC systems is a difficult task as stated in (Carpineto et al., 2009). Indeed, a successful PRC system must evidence high quality level clustering. Ideally, each query subtopic should be represented by a unique cluster containing all the relevant Web pages inside. However, this task is far from being achievable. As such, this constraint is reformulated as follows: the task of PRC </context>
</contexts>
<marker>Aitken, 1926</marker>
<rawString>A.C. Aitken. 1926. On bernoulli’s numerical solution of algebraic equations. Research Society Edinburgh, 46:289–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Amig´o</author>
<author>J Gonzalo</author>
<author>J Artiles</author>
<author>F Verdejo</author>
</authors>
<title>A comparison of extrinsic clustering evaluation metrics based on formal constraints.</title>
<date>2009</date>
<journal>Information Retrieval,</journal>
<volume>12</volume>
<issue>4</issue>
<marker>Amig´o, Gonzalo, Artiles, Verdejo, 2009</marker>
<rawString>E. Amig´o, J. Gonzalo, J. Artiles, and F. Verdejo. 2009. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information Retrieval, 12(4):461–486.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Carpineto</author>
<author>G Romano</author>
</authors>
<title>Optimal meta search results clustering.</title>
<date>2010</date>
<booktitle>In 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>170--177</pages>
<contexts>
<context position="1599" citStr="Carpineto and Romano, 2010" startWordPosition="224" endWordPosition="227">ry, the retrieved Web snippets are automatically clustered and presented to the user with meaningful labels in order to minimize the information search process. This technique can be particularly useful for polysemous queries but it is hard to implement efficiently and effectively (Carpineto et al., 2009). Indeed, as opposed to classical text clustering, PRC must deal with small collections of short text fragments (Web snippets) and be processed in run-time. As a consequence, most of the successful methodologies follow a monothetic approach (Zamir and Etzioni, 1998; Ferragina and Gulli, 2008; Carpineto and Romano, 2010; Navigli and Crisafulli, 2010; Scaiella et al., 2012). The underlying idea is to discover the most discriminant topical words in the collection and group together Web snippets containing these relevant terms. On the other hand, the polythetic approach which main idea is to represent Web snippets as word feature vectors has received less attention, the only relevant work being (Osinski and Weiss, 2005). The main reasons for this situation are that (1) word feature vectors are hard to define in small collections of short text fragments (Timonen, 2013), (2) existing second-order similarity measu</context>
<context position="3193" citStr="Carpineto and Romano, 2010" startWordPosition="477" endWordPosition="480">s if correctly applied to small collections of short text fragments. For that purpose, we propose a new methodology that adapts the classical K-means algorithm to a third-order similarity measure initially developed for Topic Segmentation (Dias et al., 2007). Moreover, the adapted K-means algorithm allows to label each cluster directly from its centroids thus avoiding the abovementioned extra task. Finally, the evolution of the objective function of the adapted K-means is modeled to automatically define the “best” number of clusters. Finally, we propose different experiments over the ODP-239 (Carpineto and Romano, 2010) and MORESQUE (Navigli and Crisafulli, 2010) datasets against the most competitive text-based PRC algorithms: STC (Zamir and Etzioni, 1998), LINGO (Osinski and Weiss, 2005), OPTIMSRC (Carpineto and Romano, 2010) and the classical bisecting incremental K-means (which may be seen as a baseline for the polythetic paradigm)1. A new evaluation measure called the b-cubed Fmeasure (Fbs) and defined in (Amig´o et al., 2009) is then calculated to evaluate both cluster homogeneity and completeness. Results evidence that our proposal outperforms all state-of-the-art approaches with a maximum Fbs = 0.452 </context>
<context position="16761" citStr="Carpineto and Romano, 2010" startWordPosition="2851" endWordPosition="2854">r SCP. However, such a fuzzy configuration is not satisfactory. As such, we proposed a new stopping criterion which evidences coherent results as it (1) does not depend on the used association measure (FSCP bs= 0.452 and FP MI bs = 0.450), (2) discovers similar numbers of clusters independently of the length of the p-context vector and (3) increases performance with high values of p. 4.3 Comparative Evaluation The second evaluation aims to compare our methodology to current state-of-the-art text-based PRC algorithms. We propose comparative experiments over two gold standard datasets (ODP-239 (Carpineto and Romano, 2010) and MORESQUE (Di Marco and Navigli, 2013)) for STC (Zamir and Etzioni, 1998), LINGO (Osinski and Weiss, 2005), OPTIMSRC (Carpineto and Romano, 2010) and the Bisecting Incremental Kmeans (BIK) which may be seen as a baseline for the polythetic paradigm. A brief description of each PRC algorithm is given as follows. STC: (Zamir and Etzioni, 1998) defined the Suffix Tree Clustering algorithm which is still a difficult standard to beat in the field. In particular, they propose a monothetic clustering technique which merges base clusters with high string overlap. Indeed, instead of using the class</context>
<context position="18803" citStr="Carpineto and Romano, 2010" startWordPosition="3177" endWordPosition="3180">objects into different clusters based on the information content associated to the series of decisions made by the partitions on single pairs of objects. Then, the meta clustering phase is casted to an optimization problem of the concordance between the clustering combination and the given set of clusterings. With respect to implementation, we used the Carrot2 APIs4 which are freely available for STC, LINGO and the classical BIK. It is worth noticing that all implementations in Carrot2 are tuned to extract exactly 10 clusters. For OPTIMSRC, we reproduced the results presented in the paper of (Carpineto and Romano, 2010) as no implementation is freely available. The results are illustrated in Table 2 including both Fβ-measure and Fbs. They evidence clear improvements of our methodology when compared to state-of-theart text-based PRC algorithms, over both datasets and all evaluation metrics. But more important, even when the p-context vector is small (p = 3), the adapted GK-means outperforms all other existing text-based PRC which is particularly important as they need to perform in real-time. 5 Conclusions In this paper, we proposed a new PRC approach which (1) is based on the adaptation of the K-means algori</context>
</contexts>
<marker>Carpineto, Romano, 2010</marker>
<rawString>C. Carpineto and G. Romano. 2010. Optimal meta search results clustering. In 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 170–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Carpineto</author>
<author>S Osinski</author>
<author>G Romano</author>
<author>D Weiss</author>
</authors>
<title>A survey of web clustering engines.</title>
<date>2009</date>
<journal>ACM Computer Survey,</journal>
<volume>41</volume>
<issue>3</issue>
<contexts>
<context position="1279" citStr="Carpineto et al., 2009" startWordPosition="175" endWordPosition="178"> criterion over the ODP-239 and the MORESQUE gold standard datasets evidence that our proposal outperforms all reported text-based approaches. 1 Introduction Post-retrieval clustering (PRC), also known as search results clustering or ephemeral clustering, is the task of clustering Web search results. For a given query, the retrieved Web snippets are automatically clustered and presented to the user with meaningful labels in order to minimize the information search process. This technique can be particularly useful for polysemous queries but it is hard to implement efficiently and effectively (Carpineto et al., 2009). Indeed, as opposed to classical text clustering, PRC must deal with small collections of short text fragments (Web snippets) and be processed in run-time. As a consequence, most of the successful methodologies follow a monothetic approach (Zamir and Etzioni, 1998; Ferragina and Gulli, 2008; Carpineto and Romano, 2010; Navigli and Crisafulli, 2010; Scaiella et al., 2012). The underlying idea is to discover the most discriminant topical words in the collection and group together Web snippets containing these relevant terms. On the other hand, the polythetic approach which main idea is to repre</context>
<context position="11209" citStr="Carpineto et al., 2009" startWordPosition="1894" endWordPosition="1897"> right). ) .log(K) (6) β = log(α − Q1S3s) − log(α − QKS3s 4 Evaluation Now, the value of α which best approximates the limit of the rational function must be defined. For that purpose, we computed its maximum theoretical and experimental values as well as its approximated maximum experimental value based on the S2-Aitken (Aitken, 1926) procedure to accelerate convergence as explained in (Kuroda et al., 2008). Best results were obtained with the maximum experimental value which is defined as building the cluster centroid M1k for each Web Evaluating PRC systems is a difficult task as stated in (Carpineto et al., 2009). Indeed, a successful PRC system must evidence high quality level clustering. Ideally, each query subtopic should be represented by a unique cluster containing all the relevant Web pages inside. However, this task is far from being achievable. As such, this constraint is reformulated as follows: the task of PRC systems is to provide complete topical cluster coverage of a given query, while avoiding excessive 155 Fb3 K Stop Criterion 2 3 4 5 6 7 8 9 10 Fb3 Avg. K 2 0.387 0.396 0.398 0.396 0.391 0.386 0.382 0.378 0.374 0.395 4.799 3 0.400 0.411 0.412 0.409 0.406 0.400 0.397 0.391 0.388 0.411 4.</context>
</contexts>
<marker>Carpineto, Osinski, Romano, Weiss, 2009</marker>
<rawString>C. Carpineto, S. Osinski, G. Romano, and D. Weiss. 2009. A survey of web clustering engines. ACM Computer Survey, 41(3):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms mutual information and lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="15262" citStr="Church and Hanks, 1990" startWordPosition="2600" endWordPosition="2603"> &lt; p, K, S(Wik, Wjl) &gt;. In particular, p is the size of the word feature vectors representing both Web snippets and centroids (p = 2..5), K is the number of clusters to be found (K = 2..10) and S(Wik, Wjl) is the collocation measure integrated in the InfoSimba similarity measure. In these experiments, two association measures which are known to have different behaviours (Pecina and Schlesinger, 2006) are tested. We implement the Symmetric Conditional Probability (Silva et al., 1999) in Equation (7) which tends to give more credits to frequent associations and the Pointwise Mutual Information (Church and Hanks, 1990) in Equation (8) which over-estimates infrequent associations. Then, best &lt; p, K, S(Wik, Wjl) &gt; configurations are compared to our stopping criterion. P(Wik, Wjl)2 SCP(Wik, Wjl) = (7) P(Wik) × P(Wjl). P(Wik, Wjl) PMI(Wik, Wjl) = log2 P(Wik) × P(Wjl). (8) In order to perform this task, we evaluate performance based on the Fb3 measure defined in (Amig´o et al., 2009) over the ODP-239 gold standard dataset proposed in (Carpineto and Romano, 156 2010). In particular, (Amig´o et al., 2009) indicate that common metrics such as the Fβ-measure are good to assign higher scores to clusters with high hom</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>K. Church and P. Hanks. 1990. Word association norms mutual information and lexicography. Computational Linguistics, 16(1):23–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Di Marco</author>
<author>R Navigli</author>
</authors>
<title>Clustering and diversifying web search results with graph-based word sense induction.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<marker>Di Marco, Navigli, 2013</marker>
<rawString>A. Di Marco and R. Navigli. 2013. Clustering and diversifying web search results with graph-based word sense induction. Computational Linguistics, 39(4):1–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Dias</author>
<author>E Alves</author>
<author>J G P Lopes</author>
</authors>
<title>Topic segmentation algorithms for text summarization and passage retrieval: An exhaustive evaluation.</title>
<date>2007</date>
<booktitle>In Proceedings of 22nd Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>1334--1339</pages>
<contexts>
<context position="2824" citStr="Dias et al., 2007" startWordPosition="421" endWordPosition="424">h as the cosine are unadapted to capture the semantic similarity between small texts, (3) Latent Semantic Analysis has evidenced inconclusive results (Osinski and Weiss, 2005) and (4) the labeling process is a surprisingly hard extra task (Carpineto et al., 2009). This paper is motivated by the fact that the polythetic approach should lead to improved results if correctly applied to small collections of short text fragments. For that purpose, we propose a new methodology that adapts the classical K-means algorithm to a third-order similarity measure initially developed for Topic Segmentation (Dias et al., 2007). Moreover, the adapted K-means algorithm allows to label each cluster directly from its centroids thus avoiding the abovementioned extra task. Finally, the evolution of the objective function of the adapted K-means is modeled to automatically define the “best” number of clusters. Finally, we propose different experiments over the ODP-239 (Carpineto and Romano, 2010) and MORESQUE (Navigli and Crisafulli, 2010) datasets against the most competitive text-based PRC algorithms: STC (Zamir and Etzioni, 1998), LINGO (Osinski and Weiss, 2005), OPTIMSRC (Carpineto and Romano, 2010) and the classical b</context>
<context position="5079" citStr="Dias et al., 2007" startWordPosition="784" endWordPosition="787">ter closest to it and the centers are recomputed as centers of mass of their assigned points. The process is repeated until convergence. To assure convergence, an objective function Q is defined which decreases at each processing step. The classical objective function is defined in Equation (1) where Irk is a cluster labeled k, xi E Irk is an object in the cluster, mπk is the centroid of the cluster Irk and E(.,.) is the Euclidean distance. E(xi, mπk)2. (1) Within the context of PRC, the K-means algorithm needs to be adapted to integrate third-order similarity measures (Mihalcea et al., 2006; Dias et al., 2007). Third-order similarity measures, also called weighted second-order similarity measures, do not rely on exact matches of word features as classical second-order similarity measures (e.g. the cosine metric), but rather evaluate similarity based on related matches. In this paper, we propose to use the third-order similarity measure called InfoSimba introduced in (Dias et al., 2007) for Topic Segmentation and implement its simplified version S3s in Equation 2. S3 s(Xi,Xj) = p2 1 Given two Web snippets Xi and Xj, their similarity is evaluated by the similarity of its constituents based on any sym</context>
</contexts>
<marker>Dias, Alves, Lopes, 2007</marker>
<rawString>G. Dias, E. Alves, and J.G.P. Lopes. 2007. Topic segmentation algorithms for text summarization and passage retrieval: An exhaustive evaluation. In Proceedings of 22nd Conference on Artificial Intelligence (AAAI), pages 1334–1339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ferragina</author>
<author>A Gulli</author>
</authors>
<title>A personalized search engine based on web-snippet hierarchical clustering.</title>
<date>2008</date>
<journal>Software: Practice and Experience,</journal>
<volume>38</volume>
<issue>2</issue>
<contexts>
<context position="1571" citStr="Ferragina and Gulli, 2008" startWordPosition="220" endWordPosition="223">ch results. For a given query, the retrieved Web snippets are automatically clustered and presented to the user with meaningful labels in order to minimize the information search process. This technique can be particularly useful for polysemous queries but it is hard to implement efficiently and effectively (Carpineto et al., 2009). Indeed, as opposed to classical text clustering, PRC must deal with small collections of short text fragments (Web snippets) and be processed in run-time. As a consequence, most of the successful methodologies follow a monothetic approach (Zamir and Etzioni, 1998; Ferragina and Gulli, 2008; Carpineto and Romano, 2010; Navigli and Crisafulli, 2010; Scaiella et al., 2012). The underlying idea is to discover the most discriminant topical words in the collection and group together Web snippets containing these relevant terms. On the other hand, the polythetic approach which main idea is to represent Web snippets as word feature vectors has received less attention, the only relevant work being (Osinski and Weiss, 2005). The main reasons for this situation are that (1) word feature vectors are hard to define in small collections of short text fragments (Timonen, 2013), (2) existing s</context>
</contexts>
<marker>Ferragina, Gulli, 2008</marker>
<rawString>P. Ferragina and A. Gulli. 2008. A personalized search engine based on web-snippet hierarchical clustering. Software: Practice and Experience, 38(2):189–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ferragina</author>
<author>U Scaiella</author>
</authors>
<title>Tagme: On-thefly annotation of short text fragments (by wikipedia entities).</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>1625--1628</pages>
<marker>Ferragina, Scaiella, 2010</marker>
<rawString>P. Ferragina and U. Scaiella. 2010. Tagme: On-thefly annotation of short text fragments (by wikipedia entities). In Proceedings of the 19th ACM International Conference on Information and Knowledge Management (CIKM), pages 1625–1628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kuroda</author>
<author>M Sakakihara</author>
<author>Z Geng</author>
</authors>
<title>Acceleration of the em and ecm algorithms using the aitken δ&apos; method for log-linear models with partially classified data.</title>
<date>2008</date>
<journal>Statistics &amp; Probability Letters,</journal>
<volume>78</volume>
<issue>15</issue>
<contexts>
<context position="10997" citStr="Kuroda et al., 2008" startWordPosition="1859" endWordPosition="1862">urve is between cluster 5 and 6 which also corresponds to the highest difference between two consecutive values of 0. Figure 2: Values of 0 (on the left) and differences between consecutive values of 0 (on the right). ) .log(K) (6) β = log(α − Q1S3s) − log(α − QKS3s 4 Evaluation Now, the value of α which best approximates the limit of the rational function must be defined. For that purpose, we computed its maximum theoretical and experimental values as well as its approximated maximum experimental value based on the S2-Aitken (Aitken, 1926) procedure to accelerate convergence as explained in (Kuroda et al., 2008). Best results were obtained with the maximum experimental value which is defined as building the cluster centroid M1k for each Web Evaluating PRC systems is a difficult task as stated in (Carpineto et al., 2009). Indeed, a successful PRC system must evidence high quality level clustering. Ideally, each query subtopic should be represented by a unique cluster containing all the relevant Web pages inside. However, this task is far from being achievable. As such, this constraint is reformulated as follows: the task of PRC systems is to provide complete topical cluster coverage of a given query, </context>
</contexts>
<marker>Kuroda, Sakakihara, Geng, 2008</marker>
<rawString>M. Kuroda, M. Sakakihara, and Z. Geng. 2008. Acceleration of the em and ecm algorithms using the aitken δ&apos; method for log-linear models with partially classified data. Statistics &amp; Probability Letters, 78(15):2332–2338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N</author>
<author>J Verbeek</author>
</authors>
<title>The global k-means clustering algorithm. Pattern Recognition,</title>
<date>2003</date>
<pages>36--451</pages>
<marker>N, Verbeek, 2003</marker>
<rawString>A. Likasa, Vlassis. N., and J. Verbeek. 2003. The global k-means clustering algorithm. Pattern Recognition, 36:451–461.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Lloyd</author>
</authors>
<title>Least squares quantization in pcm.</title>
<date>1982</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="4261" citStr="Lloyd, 1982" startWordPosition="639" endWordPosition="640">cluster homogeneity and completeness. Results evidence that our proposal outperforms all state-of-the-art approaches with a maximum Fbs = 0.452 for ODP239 and Fbs = 0.490 for MORESQUE. 1The TOPICAL algorithm proposed by (Scaiella et al., 2012) is a knowledge-driven methodology based on Wikipedia. 153 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 153–158, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Polythetic Post-Retrieval Clustering K X k=1 QS3s = X xiEπk The K-means is a geometric clustering algorithm (Lloyd, 1982). Given a set of n data points, the algorithm uses a local search approach to partition the points into K clusters. A set of K initial cluster centers is chosen. Each point is then assigned to the center closest to it and the centers are recomputed as centers of mass of their assigned points. The process is repeated until convergence. To assure convergence, an objective function Q is defined which decreases at each processing step. The classical objective function is defined in Equation (1) where Irk is a cluster labeled k, xi E Irk is an object in the cluster, mπk is the centroid of the clust</context>
</contexts>
<marker>Lloyd, 1982</marker>
<rawString>S.P. Lloyd. 1982. Least squares quantization in pcm. IEEE Transactions on Information Theory, 28(2):129–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Machado</author>
<author>T Barbosa</author>
<author>S Pais</author>
<author>B Martins</author>
<author>G Dias</author>
</authors>
<title>Universal mobile information retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the 5th International Conference on Universal Access in Human-Computer Interaction (HCI),</booktitle>
<pages>345--354</pages>
<contexts>
<context position="13741" citStr="Machado et al., 2009" startWordPosition="2338" endWordPosition="2341">subtopics in the result list of clusters. So, in order to evaluate our methodology, we propose two different evaluations. First, we want to evidence the quality of the stopping criterion when compared to an exhaustive search over all tunable parameters. Second, we propose a comparative evaluation with existing state-of-theart algorithms over gold standard datasets and recent clustering evaluation metrics. 4.1 Text Processing Before the clustering process takes place, Web snippets are represented as word feature vectors. In order to define the set of word features, the Web service proposed in (Machado et al., 2009) is used3. In particular, it assigns a relevance score to any token present in the set of retrieved Web snippets based on the analysis of left and right token contexts. A specific threshold is then applied to withdraw irrelevant tokens and the remaining ones form the vocabulary V . Then, each Web snippet is represented by the set of its p most relevant tokens in the sense of the W(.) value proposed in (Machado et al., 2009). Note that within the proposed Web service, multiword units are also identified. They are exclusively composed of relevant individual tokens and their weight is given by th</context>
</contexts>
<marker>Machado, Barbosa, Pais, Martins, Dias, 2009</marker>
<rawString>D. Machado, T. Barbosa, S. Pais, B. Martins, and G. Dias. 2009. Universal mobile information retrieval. In Proceedings of the 5th International Conference on Universal Access in Human-Computer Interaction (HCI), pages 345–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Corley</author>
<author>C Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>775--780</pages>
<contexts>
<context position="5059" citStr="Mihalcea et al., 2006" startWordPosition="780" endWordPosition="783">hen assigned to the center closest to it and the centers are recomputed as centers of mass of their assigned points. The process is repeated until convergence. To assure convergence, an objective function Q is defined which decreases at each processing step. The classical objective function is defined in Equation (1) where Irk is a cluster labeled k, xi E Irk is an object in the cluster, mπk is the centroid of the cluster Irk and E(.,.) is the Euclidean distance. E(xi, mπk)2. (1) Within the context of PRC, the K-means algorithm needs to be adapted to integrate third-order similarity measures (Mihalcea et al., 2006; Dias et al., 2007). Third-order similarity measures, also called weighted second-order similarity measures, do not rely on exact matches of word features as classical second-order similarity measures (e.g. the cosine metric), but rather evaluate similarity based on related matches. In this paper, we propose to use the third-order similarity measure called InfoSimba introduced in (Dias et al., 2007) for Topic Segmentation and implement its simplified version S3s in Equation 2. S3 s(Xi,Xj) = p2 1 Given two Web snippets Xi and Xj, their similarity is evaluated by the similarity of its constitue</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>R. Mihalcea, C. Corley, and C. Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI), pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G W Milligan</author>
<author>M C Cooper</author>
</authors>
<title>An examination of procedures for determining the number of clusters in a data set.</title>
<date>1985</date>
<journal>Psychometrika,</journal>
<volume>50</volume>
<issue>2</issue>
<pages>179</pages>
<contexts>
<context position="8436" citStr="Milligan and Cooper, 1985" startWordPosition="1382" endWordPosition="1385">ing problem with M − 1 clusters. The remaining Mth cluster center is initially placed at several positions within the data space. In addition to effectiveness, the method is deterministic and does not depend on any initial conditions or empirically adjustable parameters. Moreover, its adaptation to PRC is straightforward. 3 Stopping Criterion Once clustering has been processed, selecting the best number of clusters still remains to be decided. K X k=1 Q = X xiEπk Xik * Xjl * S(Wik, Wjl). (2) X p l=1 p X k=1 X S(wiq, w). (4) wi qEsi 154 For that purpose, numerous procedures have been proposed (Milligan and Cooper, 1985). However, none of the listed methods were effective or adaptable to our specific problem. So, we proposed a procedure based on the definition of a rational function which models the quality criterion QS3 s . To better understand the behaviour of QS3 s at each step of the adapted GK-means algorithm, we present its values for K = 10 in Figure 1. Figure 1: QS3 and its modelisation. s QS3 can be modelled as in Equation (5) which s converges to a limit α when K increases and starts from Q1 s at K = 1). The underlying S3 s (i.e. QS3 idea is that the best number of clusters is given by the 0 value w</context>
</contexts>
<marker>Milligan, Cooper, 1985</marker>
<rawString>G.W. Milligan and M.C. Cooper. 1985. An examination of procedures for determining the number of clusters in a data set. Psychometrika, 50(2):159– 179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>G Crisafulli</author>
</authors>
<title>Inducing word senses to improve web search result clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>116--126</pages>
<contexts>
<context position="1629" citStr="Navigli and Crisafulli, 2010" startWordPosition="228" endWordPosition="232">ts are automatically clustered and presented to the user with meaningful labels in order to minimize the information search process. This technique can be particularly useful for polysemous queries but it is hard to implement efficiently and effectively (Carpineto et al., 2009). Indeed, as opposed to classical text clustering, PRC must deal with small collections of short text fragments (Web snippets) and be processed in run-time. As a consequence, most of the successful methodologies follow a monothetic approach (Zamir and Etzioni, 1998; Ferragina and Gulli, 2008; Carpineto and Romano, 2010; Navigli and Crisafulli, 2010; Scaiella et al., 2012). The underlying idea is to discover the most discriminant topical words in the collection and group together Web snippets containing these relevant terms. On the other hand, the polythetic approach which main idea is to represent Web snippets as word feature vectors has received less attention, the only relevant work being (Osinski and Weiss, 2005). The main reasons for this situation are that (1) word feature vectors are hard to define in small collections of short text fragments (Timonen, 2013), (2) existing second-order similarity measures such as the cosine are una</context>
<context position="3237" citStr="Navigli and Crisafulli, 2010" startWordPosition="483" endWordPosition="486">s of short text fragments. For that purpose, we propose a new methodology that adapts the classical K-means algorithm to a third-order similarity measure initially developed for Topic Segmentation (Dias et al., 2007). Moreover, the adapted K-means algorithm allows to label each cluster directly from its centroids thus avoiding the abovementioned extra task. Finally, the evolution of the objective function of the adapted K-means is modeled to automatically define the “best” number of clusters. Finally, we propose different experiments over the ODP-239 (Carpineto and Romano, 2010) and MORESQUE (Navigli and Crisafulli, 2010) datasets against the most competitive text-based PRC algorithms: STC (Zamir and Etzioni, 1998), LINGO (Osinski and Weiss, 2005), OPTIMSRC (Carpineto and Romano, 2010) and the classical bisecting incremental K-means (which may be seen as a baseline for the polythetic paradigm)1. A new evaluation measure called the b-cubed Fmeasure (Fbs) and defined in (Amig´o et al., 2009) is then calculated to evaluate both cluster homogeneity and completeness. Results evidence that our proposal outperforms all state-of-the-art approaches with a maximum Fbs = 0.452 for ODP239 and Fbs = 0.490 for MORESQUE. 1Th</context>
</contexts>
<marker>Navigli, Crisafulli, 2010</marker>
<rawString>R. Navigli and G. Crisafulli. 2010. Inducing word senses to improve web search result clustering. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 116–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Osinski</author>
<author>D Weiss</author>
</authors>
<title>A concept-driven algorithm for clustering search results.</title>
<date>2005</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="2004" citStr="Osinski and Weiss, 2005" startWordPosition="289" endWordPosition="292"> fragments (Web snippets) and be processed in run-time. As a consequence, most of the successful methodologies follow a monothetic approach (Zamir and Etzioni, 1998; Ferragina and Gulli, 2008; Carpineto and Romano, 2010; Navigli and Crisafulli, 2010; Scaiella et al., 2012). The underlying idea is to discover the most discriminant topical words in the collection and group together Web snippets containing these relevant terms. On the other hand, the polythetic approach which main idea is to represent Web snippets as word feature vectors has received less attention, the only relevant work being (Osinski and Weiss, 2005). The main reasons for this situation are that (1) word feature vectors are hard to define in small collections of short text fragments (Timonen, 2013), (2) existing second-order similarity measures such as the cosine are unadapted to capture the semantic similarity between small texts, (3) Latent Semantic Analysis has evidenced inconclusive results (Osinski and Weiss, 2005) and (4) the labeling process is a surprisingly hard extra task (Carpineto et al., 2009). This paper is motivated by the fact that the polythetic approach should lead to improved results if correctly applied to small collec</context>
<context position="3365" citStr="Osinski and Weiss, 2005" startWordPosition="501" endWordPosition="504">r similarity measure initially developed for Topic Segmentation (Dias et al., 2007). Moreover, the adapted K-means algorithm allows to label each cluster directly from its centroids thus avoiding the abovementioned extra task. Finally, the evolution of the objective function of the adapted K-means is modeled to automatically define the “best” number of clusters. Finally, we propose different experiments over the ODP-239 (Carpineto and Romano, 2010) and MORESQUE (Navigli and Crisafulli, 2010) datasets against the most competitive text-based PRC algorithms: STC (Zamir and Etzioni, 1998), LINGO (Osinski and Weiss, 2005), OPTIMSRC (Carpineto and Romano, 2010) and the classical bisecting incremental K-means (which may be seen as a baseline for the polythetic paradigm)1. A new evaluation measure called the b-cubed Fmeasure (Fbs) and defined in (Amig´o et al., 2009) is then calculated to evaluate both cluster homogeneity and completeness. Results evidence that our proposal outperforms all state-of-the-art approaches with a maximum Fbs = 0.452 for ODP239 and Fbs = 0.490 for MORESQUE. 1The TOPICAL algorithm proposed by (Scaiella et al., 2012) is a knowledge-driven methodology based on Wikipedia. 153 Proceedings of</context>
<context position="16871" citStr="Osinski and Weiss, 2005" startWordPosition="2870" endWordPosition="2873">ch evidences coherent results as it (1) does not depend on the used association measure (FSCP bs= 0.452 and FP MI bs = 0.450), (2) discovers similar numbers of clusters independently of the length of the p-context vector and (3) increases performance with high values of p. 4.3 Comparative Evaluation The second evaluation aims to compare our methodology to current state-of-the-art text-based PRC algorithms. We propose comparative experiments over two gold standard datasets (ODP-239 (Carpineto and Romano, 2010) and MORESQUE (Di Marco and Navigli, 2013)) for STC (Zamir and Etzioni, 1998), LINGO (Osinski and Weiss, 2005), OPTIMSRC (Carpineto and Romano, 2010) and the Bisecting Incremental Kmeans (BIK) which may be seen as a baseline for the polythetic paradigm. A brief description of each PRC algorithm is given as follows. STC: (Zamir and Etzioni, 1998) defined the Suffix Tree Clustering algorithm which is still a difficult standard to beat in the field. In particular, they propose a monothetic clustering technique which merges base clusters with high string overlap. Indeed, instead of using the classical Vector Space Model (VSM) representation, they propose to represent Web snippets as compact tries. LINGO: </context>
</contexts>
<marker>Osinski, Weiss, 2005</marker>
<rawString>S. Osinski and D. Weiss. 2005. A concept-driven algorithm for clustering search results. IEEE Intelligent Systems, 20(3):48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pecina</author>
<author>P Schlesinger</author>
</authors>
<title>Combining association measures for collocation extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL),</booktitle>
<pages>651--658</pages>
<contexts>
<context position="15042" citStr="Pecina and Schlesinger, 2006" startWordPosition="2565" endWordPosition="2568">ice is available upon request. 4.2 Intrinsic Evaluation The first set of experiments focuses on understanding the behaviour of our methodology within a greedy search strategy for different tunable parameters defined as a tuple &lt; p, K, S(Wik, Wjl) &gt;. In particular, p is the size of the word feature vectors representing both Web snippets and centroids (p = 2..5), K is the number of clusters to be found (K = 2..10) and S(Wik, Wjl) is the collocation measure integrated in the InfoSimba similarity measure. In these experiments, two association measures which are known to have different behaviours (Pecina and Schlesinger, 2006) are tested. We implement the Symmetric Conditional Probability (Silva et al., 1999) in Equation (7) which tends to give more credits to frequent associations and the Pointwise Mutual Information (Church and Hanks, 1990) in Equation (8) which over-estimates infrequent associations. Then, best &lt; p, K, S(Wik, Wjl) &gt; configurations are compared to our stopping criterion. P(Wik, Wjl)2 SCP(Wik, Wjl) = (7) P(Wik) × P(Wjl). P(Wik, Wjl) PMI(Wik, Wjl) = log2 P(Wik) × P(Wjl). (8) In order to perform this task, we evaluate performance based on the Fb3 measure defined in (Amig´o et al., 2009) over the ODP</context>
</contexts>
<marker>Pecina, Schlesinger, 2006</marker>
<rawString>P. Pecina and P. Schlesinger. 2006. Combining association measures for collocation extraction. In Proceedings of the Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL), pages 651–658.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Scaiella</author>
<author>P Ferragina</author>
<author>A Marino</author>
<author>M Ciaramita</author>
</authors>
<title>Topical clustering of search results.</title>
<date>2012</date>
<booktitle>In Proceedings of the 5th ACM International Conference on Web Search and Data Mining (WSDM),</booktitle>
<pages>223--232</pages>
<contexts>
<context position="1653" citStr="Scaiella et al., 2012" startWordPosition="233" endWordPosition="236"> and presented to the user with meaningful labels in order to minimize the information search process. This technique can be particularly useful for polysemous queries but it is hard to implement efficiently and effectively (Carpineto et al., 2009). Indeed, as opposed to classical text clustering, PRC must deal with small collections of short text fragments (Web snippets) and be processed in run-time. As a consequence, most of the successful methodologies follow a monothetic approach (Zamir and Etzioni, 1998; Ferragina and Gulli, 2008; Carpineto and Romano, 2010; Navigli and Crisafulli, 2010; Scaiella et al., 2012). The underlying idea is to discover the most discriminant topical words in the collection and group together Web snippets containing these relevant terms. On the other hand, the polythetic approach which main idea is to represent Web snippets as word feature vectors has received less attention, the only relevant work being (Osinski and Weiss, 2005). The main reasons for this situation are that (1) word feature vectors are hard to define in small collections of short text fragments (Timonen, 2013), (2) existing second-order similarity measures such as the cosine are unadapted to capture the se</context>
<context position="3892" citStr="Scaiella et al., 2012" startWordPosition="585" endWordPosition="588">itive text-based PRC algorithms: STC (Zamir and Etzioni, 1998), LINGO (Osinski and Weiss, 2005), OPTIMSRC (Carpineto and Romano, 2010) and the classical bisecting incremental K-means (which may be seen as a baseline for the polythetic paradigm)1. A new evaluation measure called the b-cubed Fmeasure (Fbs) and defined in (Amig´o et al., 2009) is then calculated to evaluate both cluster homogeneity and completeness. Results evidence that our proposal outperforms all state-of-the-art approaches with a maximum Fbs = 0.452 for ODP239 and Fbs = 0.490 for MORESQUE. 1The TOPICAL algorithm proposed by (Scaiella et al., 2012) is a knowledge-driven methodology based on Wikipedia. 153 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 153–158, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Polythetic Post-Retrieval Clustering K X k=1 QS3s = X xiEπk The K-means is a geometric clustering algorithm (Lloyd, 1982). Given a set of n data points, the algorithm uses a local search approach to partition the points into K clusters. A set of K initial cluster centers is chosen. Each point is then assigned to the center closest to it and the cente</context>
</contexts>
<marker>Scaiella, Ferragina, Marino, Ciaramita, 2012</marker>
<rawString>U. Scaiella, P. Ferragina, A. Marino, and M. Ciaramita. 2012. Topical clustering of search results. In Proceedings of the 5th ACM International Conference on Web Search and Data Mining (WSDM), pages 223–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Silva</author>
<author>G Dias</author>
<author>S Guillor´e</author>
<author>J G P Lopes</author>
</authors>
<title>Using localmaxs algorithm for the extraction of contiguous and non-contiguous multiword lexical units.</title>
<date>1999</date>
<booktitle>In Proceedings of 9th Portuguese Conference in Artificial Intelligence (EPIA),</booktitle>
<pages>113--132</pages>
<marker>Silva, Dias, Guillor´e, Lopes, 1999</marker>
<rawString>J. Silva, G. Dias, S. Guillor´e, and J.G.P. Lopes. 1999. Using localmaxs algorithm for the extraction of contiguous and non-contiguous multiword lexical units. In Proceedings of 9th Portuguese Conference in Artificial Intelligence (EPIA), pages 113–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Timonen</author>
</authors>
<title>Term Weighting in Short Documents for Document Categorization, Keyword Extraction and Query Expansion.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Helsinki,</institution>
<contexts>
<context position="2155" citStr="Timonen, 2013" startWordPosition="317" endWordPosition="318">998; Ferragina and Gulli, 2008; Carpineto and Romano, 2010; Navigli and Crisafulli, 2010; Scaiella et al., 2012). The underlying idea is to discover the most discriminant topical words in the collection and group together Web snippets containing these relevant terms. On the other hand, the polythetic approach which main idea is to represent Web snippets as word feature vectors has received less attention, the only relevant work being (Osinski and Weiss, 2005). The main reasons for this situation are that (1) word feature vectors are hard to define in small collections of short text fragments (Timonen, 2013), (2) existing second-order similarity measures such as the cosine are unadapted to capture the semantic similarity between small texts, (3) Latent Semantic Analysis has evidenced inconclusive results (Osinski and Weiss, 2005) and (4) the labeling process is a surprisingly hard extra task (Carpineto et al., 2009). This paper is motivated by the fact that the polythetic approach should lead to improved results if correctly applied to small collections of short text fragments. For that purpose, we propose a new methodology that adapts the classical K-means algorithm to a third-order similarity m</context>
</contexts>
<marker>Timonen, 2013</marker>
<rawString>M. Timonen. 2013. Term Weighting in Short Documents for Document Categorization, Keyword Extraction and Query Expansion. Ph.D. thesis, University of Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Zamir</author>
<author>O Etzioni</author>
</authors>
<title>Web document clustering: A feasibility demonstration.</title>
<date>1998</date>
<booktitle>In 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>46--54</pages>
<contexts>
<context position="1544" citStr="Zamir and Etzioni, 1998" startWordPosition="215" endWordPosition="219">sk of clustering Web search results. For a given query, the retrieved Web snippets are automatically clustered and presented to the user with meaningful labels in order to minimize the information search process. This technique can be particularly useful for polysemous queries but it is hard to implement efficiently and effectively (Carpineto et al., 2009). Indeed, as opposed to classical text clustering, PRC must deal with small collections of short text fragments (Web snippets) and be processed in run-time. As a consequence, most of the successful methodologies follow a monothetic approach (Zamir and Etzioni, 1998; Ferragina and Gulli, 2008; Carpineto and Romano, 2010; Navigli and Crisafulli, 2010; Scaiella et al., 2012). The underlying idea is to discover the most discriminant topical words in the collection and group together Web snippets containing these relevant terms. On the other hand, the polythetic approach which main idea is to represent Web snippets as word feature vectors has received less attention, the only relevant work being (Osinski and Weiss, 2005). The main reasons for this situation are that (1) word feature vectors are hard to define in small collections of short text fragments (Tim</context>
<context position="3332" citStr="Zamir and Etzioni, 1998" startWordPosition="496" endWordPosition="499">K-means algorithm to a third-order similarity measure initially developed for Topic Segmentation (Dias et al., 2007). Moreover, the adapted K-means algorithm allows to label each cluster directly from its centroids thus avoiding the abovementioned extra task. Finally, the evolution of the objective function of the adapted K-means is modeled to automatically define the “best” number of clusters. Finally, we propose different experiments over the ODP-239 (Carpineto and Romano, 2010) and MORESQUE (Navigli and Crisafulli, 2010) datasets against the most competitive text-based PRC algorithms: STC (Zamir and Etzioni, 1998), LINGO (Osinski and Weiss, 2005), OPTIMSRC (Carpineto and Romano, 2010) and the classical bisecting incremental K-means (which may be seen as a baseline for the polythetic paradigm)1. A new evaluation measure called the b-cubed Fmeasure (Fbs) and defined in (Amig´o et al., 2009) is then calculated to evaluate both cluster homogeneity and completeness. Results evidence that our proposal outperforms all state-of-the-art approaches with a maximum Fbs = 0.452 for ODP239 and Fbs = 0.490 for MORESQUE. 1The TOPICAL algorithm proposed by (Scaiella et al., 2012) is a knowledge-driven methodology based</context>
<context position="16838" citStr="Zamir and Etzioni, 1998" startWordPosition="2864" endWordPosition="2868">osed a new stopping criterion which evidences coherent results as it (1) does not depend on the used association measure (FSCP bs= 0.452 and FP MI bs = 0.450), (2) discovers similar numbers of clusters independently of the length of the p-context vector and (3) increases performance with high values of p. 4.3 Comparative Evaluation The second evaluation aims to compare our methodology to current state-of-the-art text-based PRC algorithms. We propose comparative experiments over two gold standard datasets (ODP-239 (Carpineto and Romano, 2010) and MORESQUE (Di Marco and Navigli, 2013)) for STC (Zamir and Etzioni, 1998), LINGO (Osinski and Weiss, 2005), OPTIMSRC (Carpineto and Romano, 2010) and the Bisecting Incremental Kmeans (BIK) which may be seen as a baseline for the polythetic paradigm. A brief description of each PRC algorithm is given as follows. STC: (Zamir and Etzioni, 1998) defined the Suffix Tree Clustering algorithm which is still a difficult standard to beat in the field. In particular, they propose a monothetic clustering technique which merges base clusters with high string overlap. Indeed, instead of using the classical Vector Space Model (VSM) representation, they propose to represent Web s</context>
</contexts>
<marker>Zamir, Etzioni, 1998</marker>
<rawString>O. Zamir and O. Etzioni. 1998. Web document clustering: A feasibility demonstration. In 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 46–54.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>