<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015398">
<title confidence="0.9965515">
Automatic Scoring of Children&apos;s Read-Aloud Text Passages and
Word Lists
</title>
<author confidence="0.896928">
Klaus Zechner and John Sabatini and Lei Chen
</author>
<affiliation confidence="0.747494">
Educational Testing Service
Rosedale Road
</affiliation>
<address confidence="0.620053">
Princeton, NJ 08541, USA
</address>
<email confidence="0.997509">
{kzechner,jsabatini,lchen}@ets.org
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99953047368421">
Assessment of reading proficiency is typically
done by asking subjects to read a text passage
silently and then answer questions related to
the text. An alternate approach, measuring
reading-aloud proficiency, has been shown to
correlate well with the aforementioned com-
mon method and is used as a paradigm in this
paper.
We describe a system that is able to automati-
cally score two types of children’s read speech
samples (text passages and word lists), using
automatic speech recognition and the target
criterion “correctly read words per minute”.
Its performance is dependent on the data type
(passages vs. word lists) as well as on the rela-
tive difficulty of passages or words for indi-
vidual readers. Pearson correlations with
human assigned scores are around 0.86 for
passages and around 0.80 for word lists.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998915">
It has long been noted that a substantial number of
U.S. students in the 10-14 years age group have
deficiencies in their reading competence (National
Center of Educational Statistics, 2006). With the
enactment of the No Child Left Behind Act (2002),
interest and focus on objectively assessing and im-
proving this unsatisfactory situation has come to
the forefront.
</bodyText>
<page confidence="0.968821">
10
</page>
<bodyText confidence="0.999956121212121">
While assessment of reading is usually done post-
hoc with measures of reading comprehension, di-
rect reading assessment is also often performed
using a different method, oral (read-aloud) reading.
In this paradigm, students read texts aloud and
their proficiency in terms of speed, fluency, pro-
nunciation, intonation etc. can be monitored di-
rectly while reading is in progress. In the reading
research literature, oral reading has been one of the
best diagnostic and predictive measures of founda-
tional reading weaknesses and of overall reading
ability (e.g., Deno et al., 2001; Wayman et al.,
2007). An association between low reading com-
prehension and slow, inaccurate reading rate has
been confirmed repeatedly in middle school popu-
lations (e.g., Deno &amp; Marsten, 2006). Correlations
consistently fall in the 0.65-0.7 range for predict-
ing untimed passage reading comprehension test
outcomes (Wayman et al., 2007).
In this paper, we investigate the feasibility of
large-scale, automatic assessment of read-aloud
speech of middle school students with a reasonable
degree of accuracy (these students typically attend
grades 6-8 and their age is in the 10-14 years
range). If possible, this would improve the utility
of oral reading as a large-scale, school-based as-
sessment technique, making it more efficient by
saving costs and time of human annotations and
grading of reading errors.
The most widely used measure of oral reading pro-
ficiency is “correctly read words per minute”
(cwpm) (Wayman et al., 2007). To obtain this
measure, students’ read speech samples are first
</bodyText>
<note confidence="0.98823">
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 10–18,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999791979591837">
recorded, then the reading time is determined, and
finally a human rater has to listen to the recording
and note all reading errors and sum them up. Read-
ing errors are categorized into word substitutions,
deletions etc.
We have several sets of digitally recorded read-
aloud samples from middle school students avail-
able which were not collected for use with auto-
matic speech recognition (ASR) but which were
scored by hand.
Our approach here is to pass the children’s speech
samples through an automatic speech recognizer
and then to align its output word hypotheses with
the original text that was read by the student. From
this alignment and from the reading time, an esti-
mate for the above mentioned measure of cwpm
can then be computed. If the automatically com-
puted cwpm measures are close enough to those
obtained by human hand-scoring, this process may
be employed in real world settings eventually to
save much time and money.
Recognizing children’s speech, however, has been
shown to be substantially harder than adult speech
(Lee et al., 1999; Li and Russell, 2002), which is
partly due to children’s higher degree of variability
in different dimensions of language such as pro-
nunciation or grammar. In our data, there was also
a substantial number of non-native speakers of
English, presenting additional challenges. We used
targeted training and adaptation of our ASR sys-
tems to achieve reasonable word accuracies. While
for text passages, the word accuracy on unseen
speakers was about 72%, it was only about 50%
for word lists, which was due in part to a higher
percentage of non-native speakers in this data set,
to the fact that various sources of noise often pre-
vented the recognizer from correctly locating the
spoken words in the signal, and also due to our
choice of a uniform language model since conven-
tional n-gram models did not work on this data
with many silences and noises between words.
The remainder of this paper is organized as fol-
lows: in Section 2 we review related work, fol-
lowed by a description of our data in Section 3.
Section 4 provides a brief description of our speech
recognizer as well as the experimental setup. Sec-
tion 5 provides the results of our experiments, fol-
lowed by a discussion in Section 6 and conclusions
and future work in Section 7.
</bodyText>
<sectionHeader confidence="0.998671" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999902860465116">
Following the seminal paper about the LISTEN
project (Mostow et al. 1994), a number of studies
have been conducted on using automatic speech
recognition technology to score children’s read
speech.
Similar to automated assessment of adults’ speech
(Neumeyer, Franco et al. 2000; Witt, 1999), the
likelihood computed in the Hidden Markov Model
(HMM) decoding and some measurements of flu-
ency, e.g., speaking rate, are widely used as fea-
tures for predicting children’s speaking
proficiency. Children’s speech is different than
adults’. For example, children’s speech exhibits
higher fundamental frequencies (F0) than adults on
average. Also, children’s more limited knowledge
of vocabulary and grammar results in more errors
when reading printed text. Therefore, to achieve
high-quality recognition on children’s speech,
modifications have to be made on recognizers that
otherwise work well for adults.
In the LISTEN project (Mostow et al., 1994), the
basic technology is to use speech recognition to
classify each word of text as correctly read or not.
Such a classification task is hard in that the chil-
dren’s speaking deviations from the text may in-
clude arbitrary words and non-words. In a study,
they modeled variations by the modification of the
lexicon and the language model of the Sphinx1
speech recognizer.
Recently, the Technology Based Assessment of
Language and Literacy project (TBALL, (Alwan,
2007)) has been attempting to assess and evaluate
the language and literacy skills of young children
automatically. In the TBALL project, a variety of
tests including word verification, syllable blending,
letter naming, and reading comprehension, are
jointly used. Word verification is an assessment
that measures the child’s pronunciation of read-
aloud target words. A traditional pronunciation
verification method based on log-likelihoods from
HMM models is used initially (Tepperman et al.,
2006). Then an improvement based on a Bayesian
network classifier (Tepperman et al., 2007) is em-
</bodyText>
<footnote confidence="0.981376">
1 See http://cmusphinx.sourceforge.net/html/cmusphinx.php
</footnote>
<page confidence="0.999552">
11
</page>
<bodyText confidence="0.9994255625">
ployed to handle complicated errors such as pro-
nunciation variations and other reading mistakes.
mark-up any insertions, substitutions or deletions
by the student.
Many other approaches have been developed to
further improve recognition performance on chil-
dren’s speech. For example, one highly accurate
recognizer of children’s speech has been developed
by Hagen et al. (2007). Vocal tract length normali-
zation (VTLN) has been utilized to cope with the
children‘s different acoustic properties. Some spe-
cial processing techniques, e.g., using a general
garbage model to model all miscues in speaking,
have been devised to improve the language model
used in the recognition of children’s speech (Li et
al., 2007).
</bodyText>
<sectionHeader confidence="0.998181" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.98456844">
For both system training and evaluation, we use a
data set containing 3 passages read by the same
265 speakers (Set1) and a fourth passage (a longer
version of Passage 1), read by a different set of 55
speakers (Set2). Further, we have word lists read
by about 500 different speakers (Set3). All speak-
ers from Set12 and most (84%) from the third set
were U. S. middle school students in grades 6-8
(age 10-14). A smaller number of older students in
grades 10-12 (age 15-18) was also included in the
third set (16%).3 4
In terms of native language, about 15% of Set1 and
about 76% of Set35 are non-native speakers of
English or list a language different from English as
their preferred language.
Table 1 provides the details of these data sets. In
the word lists data set, there are 178 different word
lists containing 212 different word types in total
(some word lists were read by several different
students).
All data was manually transcribed using a spread-
sheet where each word is presented in one line and
the annotator, who listens to the audio file, has to
2 For Set1, we have demographics for 254 of 265 speakers
(both for grade level and native language).
</bodyText>
<footnote confidence="0.901448">
3 Grade demographics are available for 477 speakers of Set3.
4 We do not have demographic data for the small Set2 (55
speakers).
5 This set (Set 3) has information on native language for 165
speakers.
</footnote>
<table confidence="0.999747909090909">
Name Recordings Length in
words
Passage 1 265 158
(“Bed”, Set1-A)
Passage 2 265 74
(“Girls”, Set1-B)
Passage 3 265 100
(“Keen”, Set1-C)
Passage 4 55 197
(“Bed*”) (Set2)
Word lists (Set3) 590 62 (average)
</table>
<tableCaption confidence="0.999821">
Table 1. Text passages and word lists data sets.
</tableCaption>
<bodyText confidence="0.99989075">
For ASR system training only, we additionally
used parts of the OGI (Oregon Graduate Institute)
and CMU (Carnegie Mellon University) Kids data
sets as well (CSLU, 2008; LDC, 1997).
</bodyText>
<sectionHeader confidence="0.800951" genericHeader="method">
4 ASR system and experiments
</sectionHeader>
<bodyText confidence="0.999957269230769">
The ASR system’s acoustic model (AM) was
trained using portions of the OGI and CMU Kids’
corpora as well as a randomly selected sub-set of
our own passage and word list data sets described
in the previous section. About 90% of each data set
(Set1, Set2, Set3) was used for that purpose. Since
the size of our own data set was too small for AM
training, we had to augment it with the two men-
tioned corpora (OGI, CMU Kids), although they
were not a perfect match in age range and accent.
All recordings were first converted and down-
sampled to 11 kHz, mono, 16 bit resolution, PCM
format. There was no speaker overlap between
training and test sets.
For the language model (LM), two different mod-
els were created: for passages, we built an interpo-
lated trigram LM where 90% of the weight is
assigned to a LM trained only on the 4 passages
from the training set (Set1, Set2) and 10% to a ge-
neric LM using the Linguistic Data Consortium
(LDC) Broadcast News corpus (LDC, 1997). The
dictionary contains all words from the transcribed
passages in the training set, augmented with the
1,000 most frequent words from the Broadcast
News corpus. That way, the LM is not too restric-
tive and allows the recognizer to hypothesize some
</bodyText>
<page confidence="0.993066">
12
</page>
<bodyText confidence="0.999942409090909">
reading mistakes not already encountered in the
human transcriptions of the training set.
time and therefore can be considered to be ac-
counted for already in the formula.
For the word lists, a trigram LM was found to be
not working well since the words were spoken in
isolation with sometimes significant pauses in be-
tween and automatic removal of these silences
proved too hard given other confounding factors
such as microphone, speaker, or background noise.
Therefore it was decided to implement a grammar
LM for the word list decoder where all possible
words are present in a network that allows them to
occur at any time and in any sequence, allowing
for silence and/or noises in between words. This
model with uniform priors, however, has the dis-
advantage of not including any words not present
in the word list training set, such as common mis-
pronunciations and is therefore more restrictive
than the LM for text passages.
One could make the argument of using forced
alignment instead of a statistical LM to determine
reading errors. In fact, this approach is typically
used when assessing the pronunciation of read
speech. However, in our case, the interest is more
in determining how many words were read cor-
rectly in the sequence of the text (and how fast
they were read) as opposed to details in pronuncia-
tion. Further, even if we had confidence scores
attached to words in forced alignment, deciding on
which of the words obtained low confidence due to
poor pronunciation or due to substitution would
not be an easy decision. Finally, word deletions
and insertions, if too frequent, might prevent the
forced alignment algorithm from terminating.
After training was complete, we tested the recog-
nizer on the held-out passage and word list data.
After recognizing, we computed our target meas-
ure of “correct words per minute” (cwpm) accord-
ing to the following formula (W= all words in a
text, S= substitutions, D= deletions, T= reading
time in minutes), performing a string alignment
between the recognizer hypothesis and the passage
or word list to be read:
</bodyText>
<equation confidence="0.981717">
W S D
− −
T
</equation>
<bodyText confidence="0.999575588235294">
The reason that insertions are not considered here
is that they contribute to an increase in reading
Next, we performed an experiment that looks at
whether automatic scoring of read-aloud speech
allows for accurate predictions of student place-
ments in broad cohorts of reading proficiency.
We then also look more closely at typical errors
made by human readers and the speech recognizer.
All these experiments are described and discussed
in the following section.
Table 2 describes the set-up of the experiments.
Note that Passage4 (Set2) was included only in the
training but not in the evaluation set since this set
was very small. As mentioned in the previous sec-
tion, most speakers from the passage sets read
more than one passage and a few speakers from the
word lists set read more than one word list.
</bodyText>
<table confidence="0.999581333333333">
Data set Recordings Speakers Language
model
type
Passages1- 101 37 Trigram
3
Word lists 42 38 Grammar
</table>
<tableCaption confidence="0.999717">
Table 2. Experiment set-up (evaluation sets).
</tableCaption>
<sectionHeader confidence="0.999879" genericHeader="method">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.998034">
5.1 Overall results
</subsectionHeader>
<bodyText confidence="0.999905555555556">
Table 3 depicts the results of our evaluation run
with the ASR system described above. Word accu-
racy is measured against the transcribed speaker
reference (not against the true text that was read).
Word accuracy is computed according to Equation
(2), giving equal weight to reference and ASR hy-
pothesis (c=correct, s=substitutions, d=deletions,
i=insertions). This way, the formula is unbiased
with respect to insertions or deletions:
</bodyText>
<figure confidence="0.753565666666667">
wacc = 0.5x 100.0 x c + c
(2) (c+s+d c+s+i
(1) cwpm =
</figure>
<page confidence="0.987732">
13
</page>
<table confidence="0.99993875">
Data set Recordings Speakers Average word Minimum word Maximum word
Accuracy over all accuracy on a accuracy on a speech
speech sample speech sample sample
All Passages 101 37 72.2 20.4 93.8
(1-3)
Passage1 28 28 70.8 20.4 83.6
(“Bed”)
Passage2 36 36 64.1 25.4 85.7
(“Girls”)
Passage3 37 37 77.7 27.4 93.8
(“Keen”)
Word lists 42 38 49.6 10.8 78.9
</table>
<tableCaption confidence="0.999823">
Table 3. ASR experiment results (word accuracies in percent)
</tableCaption>
<bodyText confidence="0.999824333333334">
The typical run-time on a 3.2GHz Pentium proces-
sor was less than 30 seconds for a recording (faster
than real time).
We next compute cwpm measures for both human
annotations (transcripts, “gold standard”) and ma-
chine (ASR) hypotheses
Human annotators went over each read passage
and word list and marked all reading errors of the
speakers (here, only deletions and substitutions are
relevant). The reading time is computed directly
from the speech sample, so machine and human
cwpm scores only differ in error counts of dele-
tions and substitutions. Currently we only have one
human annotation available per speech sample, but
we aim to obtain a second annotation for the pur-
pose of determining inter-annotator agreement.
Table 4 presents the overall results of comparing
machine and human cwpm scoring. We performed
both Pearson correlation as well as Spearman rank
correlation. While the former provides a more ge-
neric measure of cwpm correlation, the latter fo-
cuses more on the question of the relative
performance of different speakers compared to
their peers which is usually the more interesting
question in practical applications of reading as-
sessment. Note that unlike for Table 3, the ASR
hypotheses are now aligned with the text to be read
since in a real-world application, no human tran-
scriptions would be available.
We can see that despite the less than perfect recog-
nition rate of the ASR system which causes a much
lower average estimate for cwpm or cw (for word-
lists), both Pearson and Spearman correlation coef-
ficients are quite high, all above 0.7 for Spearman
rank correlation and equal to 0.8 or higher for the
Pearson product moment correlation. This is en-
couraging as it indicates that while current ASR
technology is not yet able to exactly transcribe
children’s read speech, it is
</bodyText>
<table confidence="0.992129642857143">
Data set Gold ASR- Pearson Spearman
cwpm based r corre- rank cor-
cwpm lation relation
All Pas- 152.0 109.8 0.86 NA
sages
(1-3)
Passage1 174.3 123.5 0.87 0.72
(Bed)
Passage2 133.1 86.5 0.86 0.73
(Girls)
Passage3 153.4 122.2 0.86 0.77
(Keen)
Word 48.0 29.4 0.80 0.81
lists*
</table>
<tableCaption confidence="0.9637225">
Table 4. CWPM results for passages and word
lists. All correlations are significant at p&lt;0.01.
</tableCaption>
<bodyText confidence="0.987037833333333">
*For word lists, we use “cw” (correct words, nu-
merator of Equation (1)) as the measure, since stu-
dents were not told to be rewarded for faster
reading time here.
possible to use its output to compute reasonable
read-aloud performance measures such as cwpm
</bodyText>
<page confidence="0.997963">
14
</page>
<bodyText confidence="0.95780075">
which can help to quickly and automatically assess
reading proficiencies of students.
Table 5 provides general statistics on these two
alignments.
</bodyText>
<subsectionHeader confidence="0.994203">
5.2 Cohort assignment experiment
</subsectionHeader>
<bodyText confidence="0.973918696969697">
To follow up on the encouraging results with basic
and rank correlation, we conducted an experiment
to explore the question of practical importance
whether the automatic system can assign students
to reading proficiency cohorts automatically.
For better comparison, we selected those 27 stu-
dents from 37 total who read all 3 passages (Set 1)
and grouped them into three cohorts of 9 students
each, based on their human generated cwpm score
for all passages combined: (a) proficient
(cwpm&gt;190), (b) intermediate (135&lt;cwpm&lt;190),
and (c) low proficient (cwpm&lt;135).
We then had the automatic system predict each
student’s cohort based on the cwpm computed
from ASR. Since ASR-based cwpm values are co-
nsistently lower than human annotator based cwpm
values, the automatic cohort assignment is not
based on the cwpm values but rather on their rank-
ing.
The outcome of this experiment is very encourag-
ing in that there were no cohort prediction errors
by the automatic system. While the precise ranking
differs, the system is very well able to predict
overall cohort placement of students based on
cwpm.
5.3 Overall comparison of students’ reading er-
rors and ASR recognition errors
To look into more detail of what types of reading
errors children make and to what extent they are
reflected by the ASR system output, we used the
sclite-tool by the National Institute for Standards
and Technology (NIST, 2008) and performed two
alignments on the evaluation set:
</bodyText>
<listItem confidence="0.9944405">
1. TRANS-TRUE: Alignment between human
transcription and true passage or word list text to
be read: this alignment informs us about the kinds
of reading errors made by the students.
2. HYPO-TRANS: Alignment between the ASR
hypotheses and the human transcriptions; this
alignment informs us of ASR errors. (Note that this
is different from the experiments reported in Table
4 above where we aligned the ASR hypotheses
with the true reference texts to compute cwpm.)
</listItem>
<table confidence="0.999099">
Data set Alignment SUB DEL INS
Passages TRANS- 2.0% 6.1% 1.8%
1-3 TRUE
Pas- HYPO- 18.7% 9.6% 8.1%
sages1-3 TRANS
Word TRANS- 5.6% 6.2% 0.6%
lists TRUE
Word HYPO- 42.0% 8.9% 6.4%
lists TRANS
</table>
<tableCaption confidence="0.895902">
Table 5. Word error statistics on TRANS-TRUE
</tableCaption>
<bodyText confidence="0.952862">
and HYPO-TRANS alignments for both evaluation
data sets.
From Table 5 we can see that while for students,
deletions occur more frequently than substitutions
and, in particular, insertions, the ASR system, due
to its imperfect recognition, generates mostly sub-
stitutions, in particular for the word lists where the
word accuracy is only around 50%.
Further, we observe that the students’ average
reading word error rate (only taking into account
substitutions and deletions as we did above for the
cwpm and cw measures) lies around 8% for pas-
sages and 12% for wordlists (all measured on the
held-out evaluation data).
</bodyText>
<subsectionHeader confidence="0.995863">
5.4 Specific examples
</subsectionHeader>
<bodyText confidence="0.9999300625">
Next, we look at some examples of frequent confu-
sion pairs for those 4 combinations of data sets and
alignments. Table 6 lists the top 5 most frequent
confusion pairs (i.e., substitutions).
For passages, all of the most frequent reading er-
rors by students are morphological variants of the
target words, whereas this is only true for some of
the ASR errors, while other ASR errors can be far
off the target words. For word lists, student errors
are sometimes just orthographically related to the
target word (e.g., “liner” instead of “linear”), and
sometimes of different part-of-speech (e.g.,
“equally” instead of “equality”). ASR errors are
typically related to the target word by some pho-
netic similarity (e.g., “example” instead of “sim-
ple”).
</bodyText>
<page confidence="0.997902">
15
</page>
<sectionHeader confidence="0.998269" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999075333333333">
Finally, we look at a comparison between errors
made by the students and the fraction of those cor-
rectly identified by the ASR system in the recogni-
tion hypotheses. Table 7 provides the statistics on
these matched errors for text passages and word
lists.
</bodyText>
<table confidence="0.999052962962963">
Data Align- Refer- Spoken/ Count
set ment ence recog-
nized
Pas- TRANS asks ask 6
sages -TRUE savings saving 5
1-3 projects project 4
teacher’s teacher 4
time times 4
Pas- HYPO- storm storms 11
sages TRANS lee’s be 6
1-3 lee’s we 6
observer and 6
thousand the 6
Word TRANS nature Natural 6
lists -TRUE over- overslept 5
sleep
equality equally 4
linear liner 4
ware- ware- 3
housed house
Word HYPO- plan planned 8
lists TRANS see season 6
simple example 6
unoffi- competi- 5
cial tion
loud through- 4
out
</table>
<bodyText confidence="0.982768238095238">
Table 6. Top 5 most frequent confusion pairs for
passages and word list evaluation sets in two dif-
ferent alignments. For passages, substitutions
among closed class words such as determiners or
prepositions are omitted.
Table 7 shows that while for text passages, almost
half of the relevant errors (substitutions and dele-
tions) were correctly identified by the recognizer,
for word lists, this percentage is substantially
smaller.
The goal of this paper is to evaluate the possibility
of creating a system for automatic oral reading as-
sessment for middle school children, based on text
passages and word lists.
We decided to use the common reading profi-
ciency measure of “correct words per minute”
which enables us to align ASR word hypotheses
with the correct texts, estimate cwpm based on this
alignment and the reading time, and then compare
the automatically estimated cwpm with human an-
notations of the same texts.
</bodyText>
<table confidence="0.9994661">
Data set / error type Percentage of correctly
identified errors
Passages 1-3 – SUB 20.6
Passages 1-3 – DEL 56.4
Passages 1-3 – 47.7
SUB+DEL
Word lists – SUB 2.7
Word lists – DEL 29.4
Word lists – 16.8
SUB+DEL
</table>
<tableCaption confidence="0.912416">
Table 7. Statistics on matched errors: percentage of
</tableCaption>
<bodyText confidence="0.999280083333333">
students’ reading errors (substitutions and dele-
tions) that were also correctly identified by the
ASR system.
We built a recognizer with an acoustic model
based on CMU and OGI kids’ corpora as well as
about 90% of our own text passages and word list
data (Sets 1-3). For the in-context reading (text
passages) we trained a trigram model focused
mostly on transcriptions of the passages. For the
out-of-context isolated word reading, we used a
grammar language model where every possible
word of the word lists in the training set can follow
any other word at any time, with silence and/or
noise between words. (While this was not our pre-
ferred choice, standard n-gram language models
performed very poorly given the difficulty of re-
moving inter-word silences or noise automati-
cally.)
Given how hard ASR for children’s speech is and
given our small matched data sets, the word accu-
racy of 72% for text passages was not unreason-
able and was acceptable, particularly in a first
development cycle. The word accuracy of only
about 50% for word lists, however, is more prob-
</bodyText>
<page confidence="0.990158">
16
</page>
<bodyText confidence="0.999985472222222">
lematic and we conjecture that the two main rea-
sons for the worse performance were (a) the ab-
sence of time stamps for the location of words
which made it sometimes hard for the recognizer to
locate the correct segment in the signal for word
decoding (given noises in between), and (b) the
sometimes poor recording conditions where vol-
umes were set too high or too low, too much back-
ground or speaker noise was present etc. Further,
the high relative number of non-native speakers in
that data set may also have contributed to the lower
word accuracy of the word lists.
While the current data collection had not been
done with speech recognition in mind, in future
data collection efforts, we will make sure that the
sound quality of recordings is better monitored,
with some initial calibration, and that we store time
stamps when words are presented on the screen to
facilitate the recognition task and to allow the rec-
ognizer to expect one particular word at one par-
ticular point in time.
Despite imperfect word accuracies, however, for
both passages and word lists we found encourag-
ingly high correlations between human and auto-
matic cwpm measures (cw measures for word
lists). Obviously, the absolute values of cwpm dif-
fer greatly as the ASR system generates many
more errors on average than the readers, but both
Pearson correlation as well as Spearman rank cor-
relation measures are all above 0.7. This means
that if we would use our automatic scoring results
to rank students’ reading proficiency, the ranking
order would be overall quite similar to an order
produced by human annotators. This observation
about the rank, rather than the absolute value of
cwpm, is important in so far as it is often the case
that educators are interested in separating “co-
horts” of readers with similar proficiency and in
particular to identify the lowest performing cohort
for additional reading practice and tutoring.
An experiment testing the ability of the system to
place students into three reading proficiency co-
horts based on cwpm was very encouraging in that
all 27 students of the test set were placed in the
correct cohort by the system.
When we compare frequent student errors with
those made by the machine (Table 6), we see that
often times, students just substitute slight morpho-
logical variants (e.g., “ask” for “asks”), whereas in
the ASR system, errors are typically more complex
than just simple substitutions of morphological
variants. However, in the case of word lists, we do
find substitutions with related phonological content
in the ASR output (e.g., “example” for “simple”).
Finally, we observed that, only for the text pas-
sages, the ASR system could correctly identify a
substantial percentage of readers’ substitutions and
deletions (about 48%, see Table 7). This is also
encouraging as it is a first step towards meaningful
feedback in a potential interactive setting. How-
ever, we here only look at recall – because of the
much larger number of ASR substitutions, preci-
sion is much lower and therefore the risk of over-
correction (false alarms) is still quite high.
Despite all of the current shortcomings, we feel
that we were able to demonstrate a “proof-of-
concept” with our initial system in that we can use
our trained ASR system to make reliable estimates
on students’ reading proficiency as measured with
“correct words per minute”, where correlations
between human and machine scores are in the
0.80-0.86 range for text passages and word lists.
</bodyText>
<sectionHeader confidence="0.991214" genericHeader="conclusions">
7 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999960086956522">
This paper demonstrates the feasibility of building
an automatic scoring system for middle school stu-
dents’ reading proficiency, using a targeted trained
speech recognition system and the widely used
measure of “correctly read words per minute”
(cwpm).
The speech recognizer was trained both on external
data (OGI and CMU kids’ corpora) and internal
data (text passages and word lists), yielding two
different modes for text passages (trigram language
model) and word lists (grammar language model).
Automatically estimated cwpm measures agreed
closely with human cwpm measures, achieving 0.8
and higher correlation with Pearson and 0.7 and
higher correlation with Spearman rank correlation
measures.
Future work includes an improved set-up for re-
cordings such as initial calibration and on-line
sound quality monitoring, adding time stamps to
recordings of word lists, adding more data for
training/adaptation of the ASR system, and explor-
ing other features (such as fluency features) and
their potential role in cwpm prediction.
</bodyText>
<page confidence="0.998688">
17
</page>
<sectionHeader confidence="0.997376" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999922666666667">
The authors would like to acknowledge the contri-
butions of Kathy Sheehan, Tenaha O’Reilly and
Kelly Bruce to this work. We further are grateful
for the useful feedback and suggestions from our
colleagues at ETS and the anonymous reviewers
that greatly helped improve our paper.
</bodyText>
<sectionHeader confidence="0.999171" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999860236842105">
Alwan, A. (2007). A System for Technology Based
Assessment of Language and Literacy in Young
Children: the Role of Multiple Information
Sources. Proceedings of MMSP, Greece.
Center for Spoken Language Understanding
(CSLU), 2008. Kids’ Speech Corpus,
http://www.cslu.ogi.edu/corpora/kids/.LDC, BN.
Deno, S. L., Fuchs, L. S., Marston, D., &amp; Shin, J.
(2001). Using curriculum-based measurements
to establish growth standards for students with
learning disabilities. School Psychology Re-
view, 30(4), 507-524.
Deno, S. L. and D. Marsten (2006). Curriculum-
based measurement of oral reading: An indicator
of growth in fluency. What Research Has to Say
about Fluency Instruction. S. J. Samuels and A.
E. Farstrup. Newark, DE, International Reading
Association: 179-203.
Hagen, A., B. Pellom, &amp; R. Cole. (2007). &amp;quot;Highly
accurate children’s speech recognition for inter-
active reading tutors using subword units.&amp;quot;
Speech Communication 49(6): 861-873.
Lee, S., A. Potamianos, &amp; S. Narayanan. (1999).
&amp;quot;Acoustics of children&apos;s speech: developmental
changes of temporal and spectral parameters.&amp;quot;
Journal of Acoustics Society of American
(JASA) 105: 1455-1468.
Li, X., Y. C. Ju, L. Deng &amp; A. Acero. (2007). Effi-
cient and Robust Language Modeling in an
Automatic Children&apos;s Reading Tutor System.
Proc. IEEE International Conference on Acous-
tics, Speech and Signal Processing ICASSP
2007.
Li, Q. and M. Russell (2002). An analysis of the
causes of increased error rates in children&apos;s
speech recognition. ICSLP. Denver, CO.
Linguistic Data Consortium (LDC), 1997. 1996
English Broadcast News Speech (HUB4),
LDC97S44.
Linguistic Data Consortium (LDC), 1997. The
CMU Kids Corpus, LDC97S63.
Mostow, J., S. F. Roth, G. Hauptmann &amp; M. Kane.
(1994). A prototype reading coach that listens.
AAAI &apos;94: Proceedings of the twelfth national
conference on Artificial intelligence, Menlo
Park, CA, USA, American Association for Arti-
ficial Intelligence.
National Center of Educational Statistics. (2006).
National Assessment of Educational Progress.
Washington DC: U.S. Government Printing Of-
fice.
National Institute for Standards and Technology
(NIST), 2008. Sclite software package.
http://www.nist.gov/speech/tools/
Neumeyer, L., H. Franco, V. Digalakis &amp; M.
Weintraub. (2000). &amp;quot;Automatic Scoring of Pro-
nunciation Quality.&amp;quot; Speech Communication 6.
No Child Left Behind Act of 2001, Pub. L. No.
107-110, 115 Stat. 1425 (2002).
Tepperman, J., J. Silva, A. Kazemzadeh, H. You,
S. Lee, A. Alwan &amp; S. Narayanan. (2006). Pro-
nunciation verification of children&apos;s speech for
automatic literacy assessment. INTERSPEECH-
2006. Pittsburg, PA.
Tepperman, J., M. Black, P. Price, S. Lee, A. Ka-
zemzadeh, M. Gerosa, M. Heritage, A. Alwan &amp;
S. Narayanan.(2007). A bayesian network clas-
sifier for word-level reading assessment. Pro-
ceedings of ICSLP, Antwerp, Belgium.
Wayman, M. M., Wallace, T., Wiley, H. I., Ticha,
R., &amp; Espin, C. A. (2007). Literature synthesis
on curriculum-based measurement in reading.
The Journal of Special Education, 41(2), 85-120.
Witt, S. M. (1999). Use of Speech Recognition in
Computer-assisted Language Learning, Univer-
sity of Cambridge.
</reference>
<page confidence="0.999294">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.655177">
<title confidence="0.997981">Automatic Scoring of Children&apos;s Read-Aloud Text Passages and Word Lists</title>
<author confidence="0.999254">Zechner Sabatini</author>
<affiliation confidence="0.98485">Educational Testing</affiliation>
<address confidence="0.8552005">Rosedale Princeton, NJ 08541,</address>
<email confidence="0.996338">kzechner@ets.org</email>
<email confidence="0.996338">jsabatini@ets.org</email>
<email confidence="0.996338">lchen@ets.org</email>
<abstract confidence="0.99659285">Assessment of reading proficiency is typically done by asking subjects to read a text passage silently and then answer questions related to the text. An alternate approach, measuring reading-aloud proficiency, has been shown to correlate well with the aforementioned common method and is used as a paradigm in this paper. We describe a system that is able to automatically score two types of children’s read speech samples (text passages and word lists), using automatic speech recognition and the target criterion “correctly read words per minute”. Its performance is dependent on the data type (passages vs. word lists) as well as on the relative difficulty of passages or words for individual readers. Pearson correlations with human assigned scores are around 0.86 for passages and around 0.80 for word lists.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Alwan</author>
</authors>
<title>A System for Technology Based Assessment of Language and Literacy in Young Children: the Role of Multiple Information Sources.</title>
<date>2007</date>
<booktitle>Proceedings of MMSP,</booktitle>
<contexts>
<context position="6908" citStr="Alwan, 2007" startWordPosition="1099" endWordPosition="1100"> children’s speech, modifications have to be made on recognizers that otherwise work well for adults. In the LISTEN project (Mostow et al., 1994), the basic technology is to use speech recognition to classify each word of text as correctly read or not. Such a classification task is hard in that the children’s speaking deviations from the text may include arbitrary words and non-words. In a study, they modeled variations by the modification of the lexicon and the language model of the Sphinx1 speech recognizer. Recently, the Technology Based Assessment of Language and Literacy project (TBALL, (Alwan, 2007)) has been attempting to assess and evaluate the language and literacy skills of young children automatically. In the TBALL project, a variety of tests including word verification, syllable blending, letter naming, and reading comprehension, are jointly used. Word verification is an assessment that measures the child’s pronunciation of readaloud target words. A traditional pronunciation verification method based on log-likelihoods from HMM models is used initially (Tepperman et al., 2006). Then an improvement based on a Bayesian network classifier (Tepperman et al., 2007) is em1 See http://cmu</context>
</contexts>
<marker>Alwan, 2007</marker>
<rawString>Alwan, A. (2007). A System for Technology Based Assessment of Language and Literacy in Young Children: the Role of Multiple Information Sources. Proceedings of MMSP, Greece.</rawString>
</citation>
<citation valid="true">
<title>Spoken Language Understanding (CSLU),</title>
<date>2008</date>
<journal>Kids’ Speech Corpus, http://www.cslu.ogi.edu/corpora/kids/.LDC, BN.</journal>
<institution>Center for</institution>
<marker>2008</marker>
<rawString>Center for Spoken Language Understanding (CSLU), 2008. Kids’ Speech Corpus, http://www.cslu.ogi.edu/corpora/kids/.LDC, BN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Deno</author>
<author>L S Fuchs</author>
<author>D Marston</author>
<author>J Shin</author>
</authors>
<title>Using curriculum-based measurements to establish growth standards for students with learning disabilities.</title>
<date>2001</date>
<journal>School Psychology Review,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>507--524</pages>
<contexts>
<context position="2001" citStr="Deno et al., 2001" startWordPosition="308" endWordPosition="311">y situation has come to the forefront. 10 While assessment of reading is usually done posthoc with measures of reading comprehension, direct reading assessment is also often performed using a different method, oral (read-aloud) reading. In this paradigm, students read texts aloud and their proficiency in terms of speed, fluency, pronunciation, intonation etc. can be monitored directly while reading is in progress. In the reading research literature, oral reading has been one of the best diagnostic and predictive measures of foundational reading weaknesses and of overall reading ability (e.g., Deno et al., 2001; Wayman et al., 2007). An association between low reading comprehension and slow, inaccurate reading rate has been confirmed repeatedly in middle school populations (e.g., Deno &amp; Marsten, 2006). Correlations consistently fall in the 0.65-0.7 range for predicting untimed passage reading comprehension test outcomes (Wayman et al., 2007). In this paper, we investigate the feasibility of large-scale, automatic assessment of read-aloud speech of middle school students with a reasonable degree of accuracy (these students typically attend grades 6-8 and their age is in the 10-14 years range). If pos</context>
</contexts>
<marker>Deno, Fuchs, Marston, Shin, 2001</marker>
<rawString>Deno, S. L., Fuchs, L. S., Marston, D., &amp; Shin, J. (2001). Using curriculum-based measurements to establish growth standards for students with learning disabilities. School Psychology Review, 30(4), 507-524.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Deno</author>
<author>D Marsten</author>
</authors>
<title>Curriculumbased measurement of oral reading: An indicator of growth in fluency. What Research Has to Say about Fluency Instruction.</title>
<date>2006</date>
<pages>179--203</pages>
<contexts>
<context position="2195" citStr="Deno &amp; Marsten, 2006" startWordPosition="338" endWordPosition="341">a different method, oral (read-aloud) reading. In this paradigm, students read texts aloud and their proficiency in terms of speed, fluency, pronunciation, intonation etc. can be monitored directly while reading is in progress. In the reading research literature, oral reading has been one of the best diagnostic and predictive measures of foundational reading weaknesses and of overall reading ability (e.g., Deno et al., 2001; Wayman et al., 2007). An association between low reading comprehension and slow, inaccurate reading rate has been confirmed repeatedly in middle school populations (e.g., Deno &amp; Marsten, 2006). Correlations consistently fall in the 0.65-0.7 range for predicting untimed passage reading comprehension test outcomes (Wayman et al., 2007). In this paper, we investigate the feasibility of large-scale, automatic assessment of read-aloud speech of middle school students with a reasonable degree of accuracy (these students typically attend grades 6-8 and their age is in the 10-14 years range). If possible, this would improve the utility of oral reading as a large-scale, school-based assessment technique, making it more efficient by saving costs and time of human annotations and grading of r</context>
</contexts>
<marker>Deno, Marsten, 2006</marker>
<rawString>Deno, S. L. and D. Marsten (2006). Curriculumbased measurement of oral reading: An indicator of growth in fluency. What Research Has to Say about Fluency Instruction. S. J. Samuels and A. E. Farstrup. Newark, DE, International Reading Association: 179-203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hagen</author>
<author>B Pellom</author>
<author>R Cole</author>
</authors>
<title>Highly accurate children’s speech recognition for interactive reading tutors using subword units.&amp;quot;</title>
<date>2007</date>
<journal>Speech Communication</journal>
<volume>49</volume>
<issue>6</issue>
<pages>861--873</pages>
<contexts>
<context position="7930" citStr="Hagen et al. (2007)" startWordPosition="1241" endWordPosition="1244">method based on log-likelihoods from HMM models is used initially (Tepperman et al., 2006). Then an improvement based on a Bayesian network classifier (Tepperman et al., 2007) is em1 See http://cmusphinx.sourceforge.net/html/cmusphinx.php 11 ployed to handle complicated errors such as pronunciation variations and other reading mistakes. mark-up any insertions, substitutions or deletions by the student. Many other approaches have been developed to further improve recognition performance on children’s speech. For example, one highly accurate recognizer of children’s speech has been developed by Hagen et al. (2007). Vocal tract length normalization (VTLN) has been utilized to cope with the children‘s different acoustic properties. Some special processing techniques, e.g., using a general garbage model to model all miscues in speaking, have been devised to improve the language model used in the recognition of children’s speech (Li et al., 2007). 3 Data For both system training and evaluation, we use a data set containing 3 passages read by the same 265 speakers (Set1) and a fourth passage (a longer version of Passage 1), read by a different set of 55 speakers (Set2). Further, we have word lists read by a</context>
</contexts>
<marker>Hagen, Pellom, Cole, 2007</marker>
<rawString>Hagen, A., B. Pellom, &amp; R. Cole. (2007). &amp;quot;Highly accurate children’s speech recognition for interactive reading tutors using subword units.&amp;quot; Speech Communication 49(6): 861-873.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lee</author>
<author>A Potamianos</author>
<author>S Narayanan</author>
</authors>
<title>Acoustics of children&apos;s speech: developmental changes of temporal and spectral parameters.&amp;quot;</title>
<date>1999</date>
<journal>Journal of Acoustics Society of American (JASA)</journal>
<volume>105</volume>
<pages>1455--1468</pages>
<contexts>
<context position="4243" citStr="Lee et al., 1999" startWordPosition="664" endWordPosition="667"> here is to pass the children’s speech samples through an automatic speech recognizer and then to align its output word hypotheses with the original text that was read by the student. From this alignment and from the reading time, an estimate for the above mentioned measure of cwpm can then be computed. If the automatically computed cwpm measures are close enough to those obtained by human hand-scoring, this process may be employed in real world settings eventually to save much time and money. Recognizing children’s speech, however, has been shown to be substantially harder than adult speech (Lee et al., 1999; Li and Russell, 2002), which is partly due to children’s higher degree of variability in different dimensions of language such as pronunciation or grammar. In our data, there was also a substantial number of non-native speakers of English, presenting additional challenges. We used targeted training and adaptation of our ASR systems to achieve reasonable word accuracies. While for text passages, the word accuracy on unseen speakers was about 72%, it was only about 50% for word lists, which was due in part to a higher percentage of non-native speakers in this data set, to the fact that various</context>
</contexts>
<marker>Lee, Potamianos, Narayanan, 1999</marker>
<rawString>Lee, S., A. Potamianos, &amp; S. Narayanan. (1999). &amp;quot;Acoustics of children&apos;s speech: developmental changes of temporal and spectral parameters.&amp;quot; Journal of Acoustics Society of American (JASA) 105: 1455-1468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>Y C Ju</author>
<author>L Deng</author>
<author>A Acero</author>
</authors>
<title>Efficient and Robust Language Modeling in an Automatic Children&apos;s Reading Tutor System.</title>
<date>2007</date>
<booktitle>Proc. IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP</booktitle>
<contexts>
<context position="8265" citStr="Li et al., 2007" startWordPosition="1294" endWordPosition="1297"> mark-up any insertions, substitutions or deletions by the student. Many other approaches have been developed to further improve recognition performance on children’s speech. For example, one highly accurate recognizer of children’s speech has been developed by Hagen et al. (2007). Vocal tract length normalization (VTLN) has been utilized to cope with the children‘s different acoustic properties. Some special processing techniques, e.g., using a general garbage model to model all miscues in speaking, have been devised to improve the language model used in the recognition of children’s speech (Li et al., 2007). 3 Data For both system training and evaluation, we use a data set containing 3 passages read by the same 265 speakers (Set1) and a fourth passage (a longer version of Passage 1), read by a different set of 55 speakers (Set2). Further, we have word lists read by about 500 different speakers (Set3). All speakers from Set12 and most (84%) from the third set were U. S. middle school students in grades 6-8 (age 10-14). A smaller number of older students in grades 10-12 (age 15-18) was also included in the third set (16%).3 4 In terms of native language, about 15% of Set1 and about 76% of Set35 ar</context>
</contexts>
<marker>Li, Ju, Deng, Acero, 2007</marker>
<rawString>Li, X., Y. C. Ju, L. Deng &amp; A. Acero. (2007). Efficient and Robust Language Modeling in an Automatic Children&apos;s Reading Tutor System. Proc. IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Li</author>
<author>M Russell</author>
</authors>
<title>An analysis of the causes of increased error rates in children&apos;s speech recognition.</title>
<date>2002</date>
<publisher>ICSLP. Denver, CO.</publisher>
<contexts>
<context position="4266" citStr="Li and Russell, 2002" startWordPosition="668" endWordPosition="671">he children’s speech samples through an automatic speech recognizer and then to align its output word hypotheses with the original text that was read by the student. From this alignment and from the reading time, an estimate for the above mentioned measure of cwpm can then be computed. If the automatically computed cwpm measures are close enough to those obtained by human hand-scoring, this process may be employed in real world settings eventually to save much time and money. Recognizing children’s speech, however, has been shown to be substantially harder than adult speech (Lee et al., 1999; Li and Russell, 2002), which is partly due to children’s higher degree of variability in different dimensions of language such as pronunciation or grammar. In our data, there was also a substantial number of non-native speakers of English, presenting additional challenges. We used targeted training and adaptation of our ASR systems to achieve reasonable word accuracies. While for text passages, the word accuracy on unseen speakers was about 72%, it was only about 50% for word lists, which was due in part to a higher percentage of non-native speakers in this data set, to the fact that various sources of noise often</context>
</contexts>
<marker>Li, Russell, 2002</marker>
<rawString>Li, Q. and M. Russell (2002). An analysis of the causes of increased error rates in children&apos;s speech recognition. ICSLP. Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<date>1997</date>
<journal>English Broadcast News Speech</journal>
<volume>4</volume>
<pages>97--44</pages>
<marker>Consortium, 1997</marker>
<rawString>Linguistic Data Consortium (LDC), 1997. 1996 English Broadcast News Speech (HUB4), LDC97S44.</rawString>
</citation>
<citation valid="true">
<title>Linguistic Data Consortium (LDC),</title>
<date>1997</date>
<booktitle>The CMU Kids Corpus, LDC97S63.</booktitle>
<marker>1997</marker>
<rawString>Linguistic Data Consortium (LDC), 1997. The CMU Kids Corpus, LDC97S63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mostow</author>
<author>S F Roth</author>
<author>G Hauptmann</author>
<author>M Kane</author>
</authors>
<title>A prototype reading coach that listens.</title>
<date>1994</date>
<journal>American Association for Artificial Intelligence.</journal>
<booktitle>AAAI &apos;94: Proceedings of the twelfth national conference on Artificial intelligence,</booktitle>
<location>Menlo Park, CA, USA,</location>
<contexts>
<context position="5571" citStr="Mostow et al. 1994" startWordPosition="894" endWordPosition="897">also due to our choice of a uniform language model since conventional n-gram models did not work on this data with many silences and noises between words. The remainder of this paper is organized as follows: in Section 2 we review related work, followed by a description of our data in Section 3. Section 4 provides a brief description of our speech recognizer as well as the experimental setup. Section 5 provides the results of our experiments, followed by a discussion in Section 6 and conclusions and future work in Section 7. 2 Related work Following the seminal paper about the LISTEN project (Mostow et al. 1994), a number of studies have been conducted on using automatic speech recognition technology to score children’s read speech. Similar to automated assessment of adults’ speech (Neumeyer, Franco et al. 2000; Witt, 1999), the likelihood computed in the Hidden Markov Model (HMM) decoding and some measurements of fluency, e.g., speaking rate, are widely used as features for predicting children’s speaking proficiency. Children’s speech is different than adults’. For example, children’s speech exhibits higher fundamental frequencies (F0) than adults on average. Also, children’s more limited knowledge </context>
</contexts>
<marker>Mostow, Roth, Hauptmann, Kane, 1994</marker>
<rawString>Mostow, J., S. F. Roth, G. Hauptmann &amp; M. Kane. (1994). A prototype reading coach that listens. AAAI &apos;94: Proceedings of the twelfth national conference on Artificial intelligence, Menlo Park, CA, USA, American Association for Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<date>2006</date>
<booktitle>National Assessment of Educational Progress. Washington DC: U.S. Government Printing Office.</booktitle>
<institution>National Center of Educational Statistics.</institution>
<marker>2006</marker>
<rawString>National Center of Educational Statistics. (2006). National Assessment of Educational Progress. Washington DC: U.S. Government Printing Office.</rawString>
</citation>
<citation valid="false">
<date>2008</date>
<institution>National Institute for Standards and Technology</institution>
<note>Sclite software package. http://www.nist.gov/speech/tools/</note>
<marker>2008</marker>
<rawString>National Institute for Standards and Technology (NIST), 2008. Sclite software package. http://www.nist.gov/speech/tools/</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Neumeyer</author>
<author>H Franco</author>
<author>V Digalakis</author>
<author>M Weintraub</author>
</authors>
<title>Automatic Scoring of Pronunciation Quality.&amp;quot;</title>
<date>2000</date>
<journal>Speech Communication</journal>
<volume>6</volume>
<marker>Neumeyer, Franco, Digalakis, Weintraub, 2000</marker>
<rawString>Neumeyer, L., H. Franco, V. Digalakis &amp; M. Weintraub. (2000). &amp;quot;Automatic Scoring of Pronunciation Quality.&amp;quot; Speech Communication 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>No Child</author>
</authors>
<title>Left Behind Act of</title>
<date>2001</date>
<journal>Pub. L.</journal>
<volume>107</volume>
<pages>115--1425</pages>
<marker>Child, 2001</marker>
<rawString>No Child Left Behind Act of 2001, Pub. L. No. 107-110, 115 Stat. 1425 (2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tepperman</author>
<author>J Silva</author>
<author>A Kazemzadeh</author>
<author>H You</author>
<author>S Lee</author>
<author>A Alwan</author>
<author>S Narayanan</author>
</authors>
<title>Pronunciation verification of children&apos;s speech for automatic literacy assessment.</title>
<date>2006</date>
<booktitle>INTERSPEECH2006.</booktitle>
<location>Pittsburg, PA.</location>
<contexts>
<context position="7401" citStr="Tepperman et al., 2006" startWordPosition="1167" endWordPosition="1170">of the Sphinx1 speech recognizer. Recently, the Technology Based Assessment of Language and Literacy project (TBALL, (Alwan, 2007)) has been attempting to assess and evaluate the language and literacy skills of young children automatically. In the TBALL project, a variety of tests including word verification, syllable blending, letter naming, and reading comprehension, are jointly used. Word verification is an assessment that measures the child’s pronunciation of readaloud target words. A traditional pronunciation verification method based on log-likelihoods from HMM models is used initially (Tepperman et al., 2006). Then an improvement based on a Bayesian network classifier (Tepperman et al., 2007) is em1 See http://cmusphinx.sourceforge.net/html/cmusphinx.php 11 ployed to handle complicated errors such as pronunciation variations and other reading mistakes. mark-up any insertions, substitutions or deletions by the student. Many other approaches have been developed to further improve recognition performance on children’s speech. For example, one highly accurate recognizer of children’s speech has been developed by Hagen et al. (2007). Vocal tract length normalization (VTLN) has been utilized to cope wit</context>
</contexts>
<marker>Tepperman, Silva, Kazemzadeh, You, Lee, Alwan, Narayanan, 2006</marker>
<rawString>Tepperman, J., J. Silva, A. Kazemzadeh, H. You, S. Lee, A. Alwan &amp; S. Narayanan. (2006). Pronunciation verification of children&apos;s speech for automatic literacy assessment. INTERSPEECH2006. Pittsburg, PA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Tepperman</author>
<author>M Black</author>
<author>P Price</author>
<author>S Lee</author>
<author>A Kazemzadeh</author>
<author>M Gerosa</author>
<author>M Heritage</author>
<author>A Alwan</author>
<author>S Narayanan</author>
</authors>
<title>A bayesian network classifier for word-level reading assessment.</title>
<booktitle>Proceedings of ICSLP,</booktitle>
<location>Antwerp, Belgium.</location>
<marker>Tepperman, Black, Price, Lee, Kazemzadeh, Gerosa, Heritage, Alwan, Narayanan, </marker>
<rawString>Tepperman, J., M. Black, P. Price, S. Lee, A. Kazemzadeh, M. Gerosa, M. Heritage, A. Alwan &amp; S. Narayanan.(2007). A bayesian network classifier for word-level reading assessment. Proceedings of ICSLP, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Wayman</author>
<author>T Wallace</author>
<author>H I Wiley</author>
<author>R Ticha</author>
<author>C A Espin</author>
</authors>
<title>Literature synthesis on curriculum-based measurement in reading.</title>
<date>2007</date>
<journal>The Journal of Special Education,</journal>
<volume>41</volume>
<issue>2</issue>
<pages>85--120</pages>
<contexts>
<context position="2023" citStr="Wayman et al., 2007" startWordPosition="312" endWordPosition="315">e to the forefront. 10 While assessment of reading is usually done posthoc with measures of reading comprehension, direct reading assessment is also often performed using a different method, oral (read-aloud) reading. In this paradigm, students read texts aloud and their proficiency in terms of speed, fluency, pronunciation, intonation etc. can be monitored directly while reading is in progress. In the reading research literature, oral reading has been one of the best diagnostic and predictive measures of foundational reading weaknesses and of overall reading ability (e.g., Deno et al., 2001; Wayman et al., 2007). An association between low reading comprehension and slow, inaccurate reading rate has been confirmed repeatedly in middle school populations (e.g., Deno &amp; Marsten, 2006). Correlations consistently fall in the 0.65-0.7 range for predicting untimed passage reading comprehension test outcomes (Wayman et al., 2007). In this paper, we investigate the feasibility of large-scale, automatic assessment of read-aloud speech of middle school students with a reasonable degree of accuracy (these students typically attend grades 6-8 and their age is in the 10-14 years range). If possible, this would impr</context>
</contexts>
<marker>Wayman, Wallace, Wiley, Ticha, Espin, 2007</marker>
<rawString>Wayman, M. M., Wallace, T., Wiley, H. I., Ticha, R., &amp; Espin, C. A. (2007). Literature synthesis on curriculum-based measurement in reading. The Journal of Special Education, 41(2), 85-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Witt</author>
</authors>
<title>Use of Speech Recognition in Computer-assisted Language Learning,</title>
<date>1999</date>
<institution>University of Cambridge.</institution>
<contexts>
<context position="5787" citStr="Witt, 1999" startWordPosition="928" endWordPosition="929">review related work, followed by a description of our data in Section 3. Section 4 provides a brief description of our speech recognizer as well as the experimental setup. Section 5 provides the results of our experiments, followed by a discussion in Section 6 and conclusions and future work in Section 7. 2 Related work Following the seminal paper about the LISTEN project (Mostow et al. 1994), a number of studies have been conducted on using automatic speech recognition technology to score children’s read speech. Similar to automated assessment of adults’ speech (Neumeyer, Franco et al. 2000; Witt, 1999), the likelihood computed in the Hidden Markov Model (HMM) decoding and some measurements of fluency, e.g., speaking rate, are widely used as features for predicting children’s speaking proficiency. Children’s speech is different than adults’. For example, children’s speech exhibits higher fundamental frequencies (F0) than adults on average. Also, children’s more limited knowledge of vocabulary and grammar results in more errors when reading printed text. Therefore, to achieve high-quality recognition on children’s speech, modifications have to be made on recognizers that otherwise work well f</context>
</contexts>
<marker>Witt, 1999</marker>
<rawString>Witt, S. M. (1999). Use of Speech Recognition in Computer-assisted Language Learning, University of Cambridge.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>