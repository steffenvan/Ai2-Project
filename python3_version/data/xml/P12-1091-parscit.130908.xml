<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001401">
<title confidence="0.999191">
Modeling Sentences in the Latent Space
</title>
<author confidence="0.999402">
Weiwei Guo Mona Diab
</author>
<affiliation confidence="0.997649">
Department of Computer Science, Center for Computational Learning Systems,
Columbia University, Columbia University,
</affiliation>
<email confidence="0.998967">
weiwei@cs.columbia.edu mdiab@ccls.columbia.edu
</email>
<sectionHeader confidence="0.993748" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998167476190476">
Sentence Similarity is the process of comput-
ing a similarity score between two sentences.
Previous sentence similarity work finds that
latent semantics approaches to the problem do
not perform well due to insufficient informa-
tion in single sentences. In this paper, we
show that by carefully handling words that
are not in the sentences (missing words), we
can train a reliable latent variable model on
sentences. In the process, we propose a new
evaluation framework for sentence similarity:
Concept Definition Retrieval. The new frame-
work allows for large scale tuning and test-
ing of Sentence Similarity models. Experi-
ments on the new task and previous data sets
show significant improvement of our model
over baselines and other traditional latent vari-
able models. Our results indicate comparable
and even better performance than current state
of the art systems addressing the problem of
sentence similarity.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999148591836735">
Identifying the degree of semantic similarity [SS]
between two sentences is at the core of many NLP
applications that focus on sentence level semantics
such as Machine Translation (Kauchak and Barzi-
lay, 2006), Summarization (Zhou et al., 2006), Text
Coherence Detection (Lapata and Barzilay, 2005),
etc.To date, almost all Sentence Similarity [SS] ap-
proaches work in the high-dimensional word space
and rely mainly on word similarity. There are two
main (not unrelated) disadvantages to word similar-
ity based approaches: 1. lexical ambiguity as the
pairwise word similarity ignores the semantic inter-
action between the word and its sentential context;
2. word co-occurrence information is not sufficiently
exploited.
Latent variable models, such as Latent Semantic
Analysis [LSA] (Landauer et al., 1998), Probabilis-
tic Latent Semantic Analysis [PLSA] (Hofmann,
1999), Latent Dirichlet Allocation [LDA] (Blei et
al., 2003) can solve the two issues naturally by mod-
eling the semantics of words and sentences simulta-
neously in the low-dimensional latent space. How-
ever, attempts at addressing SS using LSA perform
significantly below high dimensional word similar-
ity based models (Mihalcea et al., 2006; O’Shea et
al., 2008).
We believe that the latent semantics approaches
applied to date to the SS problem have not yielded
positive results due to the deficient modeling of the
sparsity in the semantic space. SS operates in a very
limited contextual setting where the sentences are
typically very short to derive robust latent semantics.
Apart from the SS setting, robust modeling of the
latent semantics of short sentences/texts is becom-
ing a pressing need due to the pervasive presence of
more bursty data sets such as Twitter feeds and SMS
where short contexts are an inherent characteristic of
the data.
In this paper, we propose to model the missing
words (words that are not in the sentence), a fea-
ture that is typically overlooked in the text model-
ing literature, to address the sparseness issue for the
SS task. We define the missing words of a sentence
as the whole vocabulary in a corpus minus the ob-
served words in the sentence. Our intuition is since
observed words in a sentence are too few to tell us
what the sentence is about, missing words can be
used to tell us what the sentence is not about. We
assume that the semantic space of both the observed
</bodyText>
<page confidence="0.976283">
864
</page>
<note confidence="0.9857575">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 864–872,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999786541666667">
and missing words make up the complete semantics
profile of a sentence.
After analyzing the way traditional latent variable
models (LSA, PLSA/LDA) handle missing words,
we decide to model sentences using a weighted ma-
trix factorization approach (Srebro and Jaakkola,
2003), which allows us to treat observed words and
missing words differently. We handle missing words
using a weighting scheme that distinguishes missing
words from observed words yielding robust latent
vectors for sentences.
Since we use a feature that is already implied by
the text itself, our approach is very general (similar
to LSA/LDA) in that it can be applied to any format
of short texts. In contrast, existing work on model-
ing short texts focuses on exploiting additional data,
e.g., Ramage et al. (2010) model tweets using their
metadata (author, hashtag, etc.).
Moreover in this paper, we introduce a new eval-
uation framework for SS: Concept Definition Re-
trieval (CDR). Compared to existing data sets, the
CDR data set allows for large scale tuning and test-
ing of SS modules without further human annota-
tion.
</bodyText>
<subsectionHeader confidence="0.8520665">
2 Limitations of Topic Models and LSA
for Modeling Sentences
</subsectionHeader>
<bodyText confidence="0.998743775">
Usually latent variable models aim to find a latent
semantic profile for a sentence that is most relevant
to the observed words. By explicitly modeling miss-
ing words, we set another criterion to the latent se-
mantics profile: it should not be related to the miss-
ing words in the sentence. However, missing words
are not as informative as observed words, hence the
need for a model that does a good job of modeling
missing words at the right level of emphasis/impact
is central to completing the semantic picture for a
sentence.
LSA and PLSA/LDA work on a word-sentence
co-occurrence matrix. Given a corpus, the row en-
tries of the matrix are the unique M words in the
corpus, and the N columns are the sentence ids. The
yielded M x N co-occurrence matrix X comprises
the TF-IDF values in each Xij cell, namely that TF-
IDF value of word wi in sentence sj. For ease of
exposition, we will illustrate the problem using a
special case of the SS framework where the sen-
tences are concept definitions in a dictionary such
as WordNet (Fellbaum, 1998) (WN). Therefore, the
sentence corresponding to the concept definition of
bank#n#1 is a sparse vector in X containing the
following observed words where Xij =� 0:
the 0.1, financial 5.5, institution 4, that 0.2,
accept 2.1, deposit 3, and 0.1, channel 6, the 0.1,
money 5, into 0.3, lend 3.5, activity 3
All the other words (girl, car,..., check, loan, busi-
ness,...) in matrix X that do not occur in the concept
definition are considered missing words for the con-
cept entry bank#n#1, thereby their Xij = 0 .
Topic models (PLSA/LDA) do not explicitly
model missing words. PLSA assumes each docu-
ment has a distribution over K topics P(zk|dj), and
each topic has a distribution over all vocabularies
P(wi|zk). Therefore, PLSA finds a topic distribu-
tion for each concept definition that maximizes the
log likelihood of the corpus X (LDA has a similar
form):
</bodyText>
<equation confidence="0.999044">
P(zk|dj)P(wi|zk) (1)
</equation>
<bodyText confidence="0.988423461538461">
In this formulation, missing words do not contribute
to the estimation of sentence semantics, i.e., exclud-
ing missing words (Xij = 0) in equation 1 does not
make a difference.
However, empirical results show that given a
small number of observed words, usually topic mod-
els can only find one topic (most evident topic)
for a sentence, e.g., the concept definitions of
bank#n#1 and stock#n#1 are assigned the fi-
nancial topic only without any further discernabil-
ity. This results in many sentences are assigned ex-
actly the same semantics profile as long as they are
pertaining/mentioned within the same domain/topic.
The reason is topic models try to learn a 100-
dimension latent vector (assume dimension K =
100) from very few data points (10 observed words
on average). It would be desirable if topic models
can exploit missing words (a lot more data than ob-
served words) to render more nuanced latent seman-
tics, so that pairs of sentences in the same domain
can be differentiable.
On the other hand, LSA explicitly models missing
words but not at the right level of emphasis. LSA
finds another matrix X� (latent vectors) with rank K
to approximate X using Singular Vector Decompo-
sition (X ,: X = UKEKVK� ), such that the Frobe-
</bodyText>
<equation confidence="0.970246142857143">
�
Xij log
k
�
i
�
j
</equation>
<page confidence="0.997424">
865
</page>
<table confidence="0.99502925">
financial sport institution Ro Rm Ro − Rm Ro − 0.01Rm
v1 1 0 0 20 600 -580 14
v2 0.6 0 0.1 18 300 -282 15
v3 0.2 0.3 0.2 5 100 -95 4
</table>
<tableCaption confidence="0.999933">
Table 1: Three possible latent vectors hypotheses for the definition of bank#n#1
</tableCaption>
<bodyText confidence="0.8243015">
nius norm of difference between the two matrices is
minimized:
</bodyText>
<equation confidence="0.9927835">
v u u X �
X
t)2 �Xij − Xij(2)
i j
</equation>
<bodyText confidence="0.9999126">
In effect, LSA allows missing and observed words
to equally impact the objective function. Given the
inherent short length of the sentences, LSA (equa-
tion 2) allows for much more potential influence
from missing words rather than observed words
(99.9% cells are 0 in X). Hence the contribution
of the observed words is significantly diminished.
Moreover, the true semantics of the concept defini-
tions is actually related to some missing words, but
such true semantics will not be favored by the objec-
tive function, since equation 2 allows for too strong
an impact by Xij = 0 for any missing word. There-
fore the LSA model, in the context of short texts,
is allowing missing words to have a significant “un-
controlled” impact on the model.
</bodyText>
<subsectionHeader confidence="0.998668">
2.1 An Example
</subsectionHeader>
<bodyText confidence="0.999939407407407">
The three latent semantics profiles in table 1 il-
lustrate our analysis for topic models and LSA. As-
sume there are three dimensions: financial, sports,
institution. We use Rvo to denote the sum of related-
ness between latent vector v and all observed words;
similarly, Rvm is the sum of relatedness between the
vector v and all missing words. The first latent vec-
tor (generated by topic models) is chosen by maxi-
mizing Robs = 600. It suggests bank#n#1 is only
related to the financial dimension. The second la-
tent vector (found by LSA) has the maximum value
of Robs − Rmiss = 95, but obviously the latent vec-
tor is not related to bank#n#1 at all. This is be-
cause LSA treats observed words and missing words
equally the same, and due to the large number of
missing words, the information of observed words
is lost: Robs − Rmiss ≈ −Rmiss. The third vector is
the ideal semantics profile, since it is also related to
the institution dimension. It has a slightly smaller
Robs in comparison to the first vector, yet it has a
substantially smaller Rmiss.
In order to favor the ideal vector over other vec-
tors, we simply need to adjust the objective func-
tion by assigning a smaller weight to Rmiss such as:
Robs − 0.01 × Rmiss. Accordingly, we use weighted
matrix factorization (Srebro and Jaakkola, 2003) to
model missing words.
</bodyText>
<sectionHeader confidence="0.998076" genericHeader="method">
3 The Proposed Approach
</sectionHeader>
<subsectionHeader confidence="0.999658">
3.1 Weighted Matrix Factorization
</subsectionHeader>
<bodyText confidence="0.99966425">
The weighted matrix factorization [WMF] ap-
proach is very similar to SVD, except that it allows
for direct control on each matrix cell Xij. The model
factorizes the original matrix X into two matrices
such that X ≈ P&gt;Q, where P is a K × M matrix,
and Q is a K × N matrix (figure 1).
The model parameters (vectors in P and Q) are
optimized by minimizing the objective function:
</bodyText>
<subsectionHeader confidence="0.701704">
Wij (P·,i · Q·,j − Xij)2 + λ||P||22 + λ||Q||22 (3)
</subsectionHeader>
<bodyText confidence="0.977176625">
where A is a free regularization factor, and the
weight matrix W defines a weight for each cell in
X.
Accordingly, P·,i is a K-dimension latent seman-
tics vector profile for word wi; similarly, Q·,j is the
K-dimension vector profile that represents the sen-
tence sj. Operations on these K-dimensional vec-
tors have very intuitive semantic meanings:
</bodyText>
<listItem confidence="0.9241346">
(1) the inner product of P·,i and Q·,j is used to ap-
proximate semantic relatedness of word wi and sen-
tence sj: P·,i · Q·,j ≈ Xij, as the shaded parts in
Figure 1;
(2) equation 3 explicitly requires a sentence should
not be related to its missing words by forcing P·,i ·
Q·,j = 0 for missing words Xij = 0.
(3) we can compute the similarity of two sentences
sj and sj0 using the cosine similarity between Q·,j,
Q·,j0.
</listItem>
<bodyText confidence="0.9959378">
The latent vectors in P and Q are first randomly
initialized, then can be computed iteratively by the
following equations (derivation is omitted due to
limited space, which can be found in (Srebro and
Jaakkola, 2003)):
</bodyText>
<equation confidence="0.985182777777778">
� �−1
P·,i = Q W� (i)Q&gt; + λI Q W�(i)X&gt; i,·
� �−1
Q·,j = PW� (j)P&gt; + λI PW�(i)X·,j
X
i
X
j
(4)
</equation>
<page confidence="0.992518">
866
</page>
<figureCaption confidence="0.999873">
Figure 1: Matrix Factorization
</figureCaption>
<bodyText confidence="0.99920025">
where W�(i) = diag(W�,i) is an M x M diagonal
matrix containing ith row of weight matrix W. Sim-
ilarly, W�(j) = diag(W�,j) is an N x N diagonal
matrix containing jth column of W.
</bodyText>
<subsectionHeader confidence="0.999924">
3.2 Modeling Missing Words
</subsectionHeader>
<bodyText confidence="0.9999918">
It is straightforward to implement the idea in Sec-
tion 2.1 (choosing a latent vector that maximizes
Robs − 0.01 x Rmiss) in the WMF framework, by
assigning a small weight for all the missing words
and minimizing equation 3:
</bodyText>
<equation confidence="0.978341">
� Wz 1, if Xzj 0 ( )
&apos;� = wm, if Xzj = 0 5
</equation>
<bodyText confidence="0.999567464285714">
We refer to our model as Weighted Textual Matrix
Factorization [WTMF]. 1
This solution is quite elegant: 1. it explicitly tells
the model that in general all missing words should
not be related to the sentence; 2. meanwhile latent
semantics are mainly generalized based on observed
words, and the model is not penalized too much
(wm is very small) when it is very confident that
the sentence is highly related to a small subset of
missing words based on their latent semantics pro-
files (bank#n#1 definition sentence is related to its
missing words check loan).
We adopt the same approach (assigning a small
weight for some cells in WMF) proposed for rec-
ommender systems [RS] (Steck, 2010). In RS, an
incomplete rating matrix R is proposed, where rows
are users and columns are items. Typically, a user
rates only some of the items, hence, the RS system
needs to predict the missing ratings. Steck (2010)
guesses a value for all the missing cells, and sets a
small weight for those cells.
Compared to (Steck, 2010), we are facing a differ-
ent problem and targeting a different goal. We have
a full matrix X where missing words have a 0 value,
while the missing ratings in RS are unavailable – the
values are unknown, hence R is not complete. In the
RS setting, they are interested in predicting individ-
ual ratings, while we are interested in the sentence
</bodyText>
<footnote confidence="0.8342775">
1An efficient way to compute equation 4 is proposed in
(Steck, 2010).
</footnote>
<bodyText confidence="0.999099">
semantics. More importantly, they do not have the
sparsity issue (each user has rated over 100 items in
the movie lens data2) and robust predictions can be
made based on the observed ratings alone.
</bodyText>
<sectionHeader confidence="0.995546" genericHeader="method">
4 Evaluation for SS
</sectionHeader>
<bodyText confidence="0.999656323529412">
We need to show the impact of our proposed model
WTMF on the SS task. However we are faced with
a problem, the lack of a suitable large evaluation set
from which we can derive robust observations. The
two data sets we know of for SS are: 1. human-rated
sentence pair similarity data set (Li et al., 2006)
[LI06]; 2. the Microsoft Research Paraphrase Cor-
pus (Dolan et al., 2004) [MSR04]. The LI06 data
set consists of 65 pairs of noun definitions selected
from the Collin Cobuild Dictionary. A subset of 30
pairs is further selected by LI06 to render the sim-
ilarity scores evenly distributed. While this is the
ideal data set for SS, the small size makes it impos-
sible for tuning SS algorithms or deriving significant
performance conclusions.
On the other hand, the MSR04 data set comprises
a much larger set of sentence pairs: 4,076 training
and 1,725 test pairs. The ratings on the pairs are
binary labels: similar/not similar. This is not a prob-
lem per se, however the issue is that it is very strict
in its assignment of a positive label, for example
the following sentence pair as cited in (Islam and
Inkpen, 2008) is rated not semantically similar:
Ballmer has been vocal in the past warning that
Linux is a threat to Microsoft.
In the memo, Ballmer reiterated the open-source
threat to Microsoft.
We believe that the ratings on a data set for SS
should accommodate variable degrees of similarity
with various ratings, however such a large scale set
does not exist yet. Therefore for purposes of evaluat-
ing our proposed approach we devise a new frame-
work inspired by the LI06 data set in that it com-
prises concept definitions but on a large scale.
</bodyText>
<subsectionHeader confidence="0.966095">
4.1 Concept Definition Retrieval
</subsectionHeader>
<bodyText confidence="0.99989475">
We define a new framework for evaluating SS and
project it as a Concept Definition Retrieval (CDR)
task where the data points are dictionary definitions.
The intuition is that two definitions in different dic-
</bodyText>
<footnote confidence="0.9937765">
2http://www.grouplens.org/node/73, with 1M data set being
the most widely used.
</footnote>
<page confidence="0.994017">
867
</page>
<bodyText confidence="0.99982555882353">
tionaries referring to the same concept should be as-
signed large similarity. In this setting, we design the
CDR task in a search engine style. The SS algorithm
has access to all the definitions in WordNet (WN).
Given an OntoNotes (ON) definition (Hovy et al.,
2006), the SS algorithm should rank the equivalent
WN definition as high as possible based on sentence
similarity.
The manual mapping already exists for ON to
WN. One ON definition can be mapped to sev-
eral WN definitions. After preprocessing we obtain
13669 ON definitions mapped to 19655 WN defini-
tions. The data set has the advantage of being very
large and it doesn’t require further human scrutiny.
After the SS model learns the co-occurrence of
words from WN definitions, in the testing phase,
given an ON definition d, the SS algorithm needs to
identify the equivalent WN definitions by comput-
ing the similarity values between all WN definitions
and the ON definition d, then sorting the values in
decreasing order. Clearly, it is very difficult to rank
the one correct definition as highest out of all WN
definitions (110,000 in total), hence we use ATOPd,
area under the TOPKd(k) recall curve for an ON
definition d, to measure the performance. Basically,
it is the ranking of the correct WN definition among
all WN definitions. The higher a model is able to
rank the correct WN definition, the better its perfor-
mance.
Let Nd be the number of aligned WN definitions
for the ON definition d, and Nd be the number of
aligned WN definitions in the top-k list. Then with
a normalized k E [0,1], TOPKd(k) and ATOPd is
defined as:
</bodyText>
<equation confidence="0.998691">
TOPKd(k) = Nd /Nd
ATOPd = fo TOPKd(k)dk (6)
</equation>
<bodyText confidence="0.9999665">
ATOPd computes the normalized rank (in the range
of [0, 1]) of aligned WN definitions among all WN
definitions, with value 0.5 being the random case,
and 1 being ranked as most similar.
</bodyText>
<sectionHeader confidence="0.99809" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999937675675676">
We evaluate WTMF on three data sets: 1. CDR
data set using ATOP metric; 2. Human-rated Sen-
tence Similarity data set [LI06] using Pearson and
Spearman Correlation; 3. MSR Paraphrase corpus
[MSR04] using accuracy.
The performance of WTMF on CDR is com-
pared with (a) an Information Retrieval model (IR)
that is based on surface word matching, (b) an n-
gram model (N-gram) that captures phrase overlaps
by returning the number of overlapping ngrams as
the similarity score of two sentences, (c) LSA that
uses svds() function in Matlab, and (d) LDA that
uses Gibbs Sampling for inference (Griffiths and
Steyvers, 2004). WTMF is also compared with all
existing reported SS results on LI06 and MSR04
data sets, as well as LDA that is trained on the
same data as WTMF. The similarity of two sentences
is computed by cosine similarity (except N-gram).
More details on each task will be explained in the
subsections.
To eliminate randomness in statistical models
(WTMF and LDA), all the reported results are aver-
aged over 10 runs. We run 20 iterations for WTMF.
And we run 5000 iterations for LDA; each LDA
model is averaged over the last 10 Gibbs Sampling
iterations to get more robust predictions.
The latent vector of a sentence is computed by:
(1) using equation 4 in WTMF, or (2) summing
up the latent vectors of all the constituent words
weighted by Xij in LSA and LDA, similar to the
work reported in (Mihalcea et al., 2006). For LDA
the latent vector of a word is computed by P(z|w).
It is worth noting that we could directly use the es-
timated topic distribution Bj to represent a sentence,
however, as discussed the topic distribution has only
non-zero values on one or two topics, leading to a
low ATOP value around 0.8.
</bodyText>
<subsectionHeader confidence="0.981729">
5.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999911636363636">
The corpus we use comprises three dictionaries
WN, ON, Wiktionary [Wik],3 Brown corpus. For
all dictionaries, we only keep the definitions without
examples, and discard the mapping between sense
ids and definitions. All definitions are simply treated
as individual documents. We crawl Wik and remove
the entries that are not tagged as noun, verb, adjec-
tive, or adverb, resulting in 220, 000 entries. For the
Brown corpus, each sentence is treated as a docu-
ment in order to create more coherent co-occurrence
values. All data is tokenized, pos-tagged4, and lem-
</bodyText>
<footnote confidence="0.999956">
3http://en.wiktionary.org/wiki/Wiktionary:Main Page
4http://nlp.stanford.edu/software/tagger.shtml
</footnote>
<page confidence="0.991755">
868
</page>
<note confidence="0.301459">
Models Parameters Dev Test
</note>
<listItem confidence="0.658463666666667">
1. IR - 0.8578 0.8515
2. N-gram - 0.8238 0.8171
3. LSA - 0.8218 0.8143
4a. LDA α = 0.1, (3 = 0.01 0.9466 f 0.0020 0.9427 f 0.0006
4b. LDA α = 0.05, C3 = 0.05 0.9506 f 0.0017 0.9470 f 0.0005
5. WTMF wm = 1, A = 0 0.8273 f 0.0028 0.8273 f 0.0014
6. WTMF wm = 0, A = 20 0.8745 f 0.0058 0.8645 f 0.0031
7a. WTMF wm = 0.01, A = 20 0.9555 f 0.0015 0.9511 f 0.0003
7b. WTMF wm = 0.0005, A = 20 0.9610 f 0.0011 0.9558 f 0.0004
</listItem>
<tableCaption confidence="0.99445">
Table 2: ATOP Values of Models (K = 100 for LSA/LDA/WTMF)
</tableCaption>
<bodyText confidence="0.993188125">
matized5. The importance of words in a sentence is
estimated by the TF-IDF schema.
All the latent variable models (LSA, LDA,
WTMF) are built on the same set of cor-
pus: WN+Wik+Brown (393, 666 sentences and
4, 262, 026 words). Words that appear only once are
removed. The test data is never used during training
phrase.
</bodyText>
<subsectionHeader confidence="0.989313">
5.2 Concept Definition Retrieval
</subsectionHeader>
<bodyText confidence="0.99998312">
Among the 13669 ON definitions, 1000 defini-
tions are randomly selected as a development set
(dev) for picking best parameters in the models, and
the rest is used as a test set (test). The performance
of each model is evaluated by the average ATOPd
value over the 12669 definitions (test). We use the
subscript set in ATOPset to denote the average of
ATOPd of a set of ON definitions, where d E {set}.
If all the words in an ON definition are not covered
in the training data (WN+Wik+Br), then ATOPd for
this instance is set to 0.5.
To compute ATOPd for an ON definition effi-
ciently, we use the rank of the aligned WN definition
among a random sample (size=1000) of WN defini-
tions, to approximate its rank among all WN defini-
tions. In practice, the difference between using 1000
samples and all data is tiny for ATOPtest (±0.0001),
due to the large number of data points in CDR.
We mainly compare the performance of IR, N-
gram, LSA, LDA, and WTMF models. Generally
results are reported based on the last iteration. How-
ever, we observe that for model 6 in table 2, the best
performance occurs at the first few iterations. Hence
for that model we use the ATOPde, to indicate when
to stop.
</bodyText>
<footnote confidence="0.925975">
5http://wn-similarity.sourceforge.net, WordNet::QueryData
</footnote>
<sectionHeader confidence="0.834191" genericHeader="evaluation">
5.2.1 Results
</sectionHeader>
<bodyText confidence="0.999928594594595">
Table 2 summarizes the ATOP values on the dev
and test sets. All parameters are tuned based on the
dev set. In LDA, we choose an optimal combination
of α and Q from {0.01, 0.05, 0.1, 0.5}.In WTMF, we
choose the best parameters of weight wm for miss-
ing words and A for regularization. We fix the di-
mension K = 100. Later in section 5.2.2, we will
see that a larger value of K can further improve the
performance.
WTMF that models missing words using a small
weight (model 7b with wm = 0.0005) outperforms
the second best model LDA by a large margin. This
is because LDA only uses 10 observed words to infer
a 100 dimension vector for a sentence, while WTMF
takes advantage of much more missing words to
learn more robust latent semantics vectors.
The IR model that works in word space achieves
better ATOP scores than N-gram, although the idea
of N-gram is commonly used in detecting para-
phrases as well as machine translation. Applying
TF-IDF for N-gram is better, but still the ATOPtest is
not higher: 0.8467. The reason is words are enough
to capture semantics for SS, while n-grams/phrases
are used for a more fine-grained level of semantics.
We also present model 5 and 6 (both are WTMF),
to show the impact of: 1. modeling missing words
with equal weights as observed words (wm = 1)
(LSA manner), and 2. not modeling missing words
at all (wm = 0) (LDA manner) in the context of
WTMF model. As expected, both model 5 and
model 6 generate much worse results.
Both LDA and model 6 ignore missing words,
with better ATOPtest scores achieved by LDA. This
may be due to the different inference algorithms.
Model 5 and LSA are comparable, where missing
words are used with a large weight. Both of them
yield low results. This confirms our assumption
</bodyText>
<page confidence="0.993803">
869
</page>
<table confidence="0.788100928571429">
WiMF
0.955
0.945
0.94
0.0001 0.0005 0.001 0.005 0.01 0.05
ATOP
0.95
wM 30 pairs 35 pairs
r p r p
0.0005 0.8247 0.8440 0.4200 0.6006
0.001 0.8470 0.8636 0.4308 0.5985
0.005 0.8876 0.8966 0.4638 0.5809
0.01 0.8984 0.9091 0.4564 0.5450
0.05 0.8804 0.8812 0.4087 0.4766
</table>
<tableCaption confidence="0.998284">
Table 3: Different wm of WTMF on LI06 (K = 100)
</tableCaption>
<figure confidence="0.692847">
Wm
</figure>
<figureCaption confidence="0.963132">
Figure 2: missing words weight wm in WTMF
</figureCaption>
<figure confidence="0.794259">
K
</figure>
<figureCaption confidence="0.998023">
Figure 3: dimension K in WTMF and LDA
</figureCaption>
<bodyText confidence="0.998224">
that allowing for equal impact of both observed and
missing words is not the correct characterization of
the semantic space.
</bodyText>
<subsectionHeader confidence="0.958529">
5.2.2 Analysis
</subsectionHeader>
<bodyText confidence="0.999884625">
In these latent variable models, there are several
essential parameters: weight of missing words wm,
and dimension K. Figure 2 and 3 analyze the impact
of these parameters on ATOPtest.
Figure 2 shows the influence of wm on ATOPtest
values. The peak ATOPtest is around wm = 0.0005,
while other values of wm (except wm = 0.05) also
yield high ATOP values (better than LDA).
We also measure the influence of the dimension
K = {50, 75,100,125,150} on LDA and WTMF
in Figure 3, where parameters for WTMF are wm =
0.0005, A = 20, and for LDA are α = 0.05, Q =
0.05. We can see WTMF consistently outperforms
LDA by an ATOP value of 0.01 in each dimension.
Although a larger K yields a better result, we still
use a 100 due to computational complexity.
</bodyText>
<subsectionHeader confidence="0.998209">
5.3 LI06: Human-rated Sentence Similarity
</subsectionHeader>
<bodyText confidence="0.999891375">
We also assess WTMF and LDA model on LI06
data set. We still use K = 100. As we can see
in Figure 2, choosing the appropriate parameter wm
could boost the performance significantly. Since we
do not have any tuning data for this task, we present
Pearson’s correlation r for different values of wm in
Table 3. In addition, to demonstrate that wm does
not overfit the 30 data points, we also evaluate on
the other 35 pairs in LI06. Same as in (Tsatsaronis
et al., 2010), we also include Spearman’s rank order
correlation p, which is correlation of ranks of simi-
larity values . Note that r and p are much lower for
35 pairs set, since most of the sentence pairs have
a very low similarity (the average similarity value
is 0.065 in 35 pairs set and 0.367 in 30 pairs set)
and SS models need to identify the tiny difference
among them, thereby rendering this set much harder
to predict.
Using wm = 0.01 gives the best results on 30
pairs while on 35 pairs the peak values of r and p
happens when wm = 0.005. In general, the cor-
relations in 30 pairs and in 35 pairs are consistent,
which indicates wm = 0.01 or wm = 0.005 does
not overfit the 30 pairs set.
Compared to CDR, LI06 data set has a strong
preference for a larger wm. This could be caused by
different goals of the two tasks: CDR is evaluated
by the rank of the most similar ones among all can-
didates, while the LI06 data set treats similar pairs
and dissimilar pairs as equally important. Using a
smaller wm means the similarity score is computed
mainly from semantics of the observed words. This
benefits CDR, since it gives more accurate similarity
scores for those similar pairs, but not so accurate for
dissimilar pairs. In fact, from Figure 2 and Table 2
we see that wm = 0.01 also produces a very high
ATOPtest value in CDR.
Table 4 shows the results of all current SS models
with respect to the LI06 data set (30 pairs set). We
cite their best performance for all reported results.
Once the correct wm = 0.01 is chosen, WTMF
results in the best Pearson’s r and best Spearman’s
p (wm = 0.005 yields the second best r and p).
Same as in CDR task, WTMF outperforms LDA by
a large margin in both r and p. It indicates that the
latent vectors induced by WTMF are able to not only
identify same/similar sentences, but also identify the
“correct” degree of dissimilar sentences.
</bodyText>
<figure confidence="0.99535675">
0.955
ATOP
0.95
0.945
0.94
50 100 150
WTMF
LDA
</figure>
<page confidence="0.978455">
870
</page>
<note confidence="0.994191111111111">
Model r p
STASIS (Li et al., 2006) 0.8162 0.8126
(Liu et al., 2007) 0.841 0.8538
(Feng et al., 2008) 0.756 0.608
STS (Islam and Inkpen, 2008) 0.853 0.838
LSA (O’Shea et al., 2008) 0.8384 0.8714
Omiotis (Tsatsaronis et al., 2010) 0.856 0.8905
WSD-STS (Ho et al., 2010) 0.864 0.8341
SPD-STS (Ho et al., 2010) 0.895 0.9034
</note>
<table confidence="0.679541333333333">
LDA (α = 0.05, 0 = 0.05) 0.8422 0.8663
WTMF (w. = 0.005, A = 20) 0.8876 0.8966
WTMF (w. = 0.01, A = 20) 0.8984 0.9091
</table>
<tableCaption confidence="0.973725">
Table 4: Pearson’s correlation r and Spearman’s corre-
lation p on LI06 30 pairs
</tableCaption>
<table confidence="0.99934625">
Model Accuracy
Random 51.3
LSA (Mihalcea et al., 2006) 68.4
full model (Mihalcea et al., 2006) 70.3
STS (Islam and Inkpen, 2008) 72.6
Omiotis (Tsatsaronis et al., 2010) 69.97
LDA (α = 0.05, 0 = 0.05) 68.6
WTMF (w. = 0.01, A = 20) 71.51
</table>
<tableCaption confidence="0.99971">
Table 5: Performance on MSR04 test set
</tableCaption>
<subsectionHeader confidence="0.794147">
5.4 MSR04: MSR Paraphrase Corpus
</subsectionHeader>
<bodyText confidence="0.9997705">
Finally, we briefly discuss results of applying
WTMF on MSR04 data. We use the same pa-
rameter setting used for the LI06 evaluation set-
ting since both sets are human-rated sentence pairs
(A = 20,w.. = 0.01, K = 100). We use the train-
ing set of MSR04 data to select a threshold of sen-
tence similarity for the binary label. Table 5 sum-
marizes the accuracy of other SS models noted in
the literature and evaluated on MSR04 test set.
Compared to previous SS work and LDA, WTMF
has the second best accuracy. It suggests that WTMF
is quite competitive in the paraphrase recognition
task.
It is worth noting that the best system on MSR04,
STS (Islam and Inkpen, 2008), has much lower cor-
relations on LI06 data set. The second best system
among previous work on LI06 uses Spearman cor-
relation, Omiotis (Tsatsaronis et al., 2010), and it
yields a much worse accuracy on MSR04. The other
works do not evaluate on both data sets.
</bodyText>
<sectionHeader confidence="0.999982" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999943896551724">
Almost all current SS methods work in the high-
dimensional word space, and rely heavily on
word/sense similarity measures, which is knowledge
based (Li et al., 2006; Feng et al., 2008; Ho et al.,
2010; Tsatsaronis et al., 2010), corpus-based (Islam
and Inkpen, 2008) or hybrid (Mihalcea et al., 2006).
Almost all of them are evaluated on LI06 data set. It
is interesting to see that most works find word sim-
ilarity measures, especially knowledge based ones,
to be the most effective component, while other fea-
tures do not work well (such as word order or syn-
tactic information). Mihalcea et al. (2006) use LSA
as a baseline, and O’Shea et al. (2008) train LSA
on regular length documents. Both results are con-
siderably lower than word similarity based methods.
Hence, our work is the first to successfully approach
SS in the latent space.
Although there has been work modeling latent se-
mantics for short texts (tweets) in LDA, the focus
has been on exploiting additional features in Twit-
ter, hence restricted to Twitter data. Ramage et al.
(2010) use tweet metadata (author, hashtag) as some
supervised information to model tweets. Jin et al.
(2011) use long similar documents (the article that
is referred by a url in tweets) to help understand the
tweet. In contrast, our approach relies solely on the
information in the texts by modeling local missing
words, and does not need any additional data, which
renders our approach much more widely applicable.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999982666666667">
We explicitly model missing words to alleviate the
sparsity problem in modeling short texts. We also
propose a new evaluation framework for sentence
similarity that allows large scale tuning and test-
ing. Experiment results on three data sets show that
our model WTMF significantly outperforms existing
methods. For future work, we would like to compare
the text modeling performance of WTMF with LSA
and LDA on regular length documents.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999886909090909">
We would like to thank the anonymous reviewers for
their valuable comments and suggestions to improve
the quality of the paper.
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
</bodyText>
<page confidence="0.997202">
871
</page>
<sectionHeader confidence="0.995879" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999506034090909">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3.
William Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Jin Feng, Yi-Ming Zhou, and Trevor Martin. 2008. Sen-
tence similarity based on relevance. In Proceedings of
IPMU.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101.
Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Ab-
dul Kadir, and Shyamala C. Doraisamy. 2010. Word
sense disambiguation-based sentence similarity. In
Proceedings of the 23rd International Conference on
Computational Linguistics.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the ACL.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data, 2.
Ou Jin, Nathan N. Liu, Kai Zhao, Yong Yu, and Qiang
Yang. 2011. Transferring topical knowledge from
auxiliary long texts for short text clustering. In Pro-
ceedings of the 20th ACM international conference on
Information and knowledge management.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In Proceedings of the
Human Language Technology Conference of the North
American Chapter of the ACL.
Thomas K Landauer, Peter W. Foltz, and Darrell Laham.
1998. An introduction to latent semantic analysis.
Discourse Processes, 25.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In Proceedings of the 19th International Joint
Conference on Artificial Intelligence.
Yuhua Li, Davi d McLean, Zuhair A. Bandar, James D. O
Shea, and Keeley Crockett. 2006. Sentence similar-
ity based on semantic nets and corpus statistics. IEEE
Transaction on Knowledge and Data Engineering, 18.
Xiao-Ying Liu, Yi-Ming Zhou, and Ruo-Shi Zheng.
2007. Sentence similarity based on dynamic time
warping. In The International Conference on Seman-
tic Computing.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the 21st
National Conference on Articial Intelligence.
James O’Shea, Zuhair Bandar, Keeley Crockett, and
David McLean. 2008. A comparative study of two
short text semantic similarity measures. In Proceed-
ings of the Agent and Multi-Agent Systems: Technolo-
gies and Applications, Second KES International Sym-
posium (KES-AMSTA).
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing microblogs with topic models. In Pro-
ceedings of the Fourth International AAAI Conference
on Weblogs and Social Media.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted
low-rank approximations. In Proceedings of the Twen-
tieth International Conference on Machine Learning.
Harald Steck. 2010. Training and testing of recom-
mender systems on data missing not at random. In
Proceedings of the 16th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing.
George Tsatsaronis, Iraklis Varlamis, and Michalis Vazir-
giannis. 2010. Text relatedness based on a word the-
saurus. Journal of Articial Intelligence Research, 37.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of Human Language Tech-nology Conference of the
North American Chapter of the ACL,.
</reference>
<page confidence="0.998073">
872
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.993017">
<title confidence="0.99983">Modeling Sentences in the Latent Space</title>
<author confidence="0.999848">Weiwei Guo Mona Diab</author>
<affiliation confidence="0.9999855">Department of Computer Science, Center for Computational Learning Systems, Columbia University, Columbia University,</affiliation>
<email confidence="0.999878">weiwei@cs.columbia.edumdiab@ccls.columbia.edu</email>
<abstract confidence="0.9997">Sentence Similarity is the process of computing a similarity score between two sentences. Previous sentence similarity work finds that latent semantics approaches to the problem do not perform well due to insufficient information in single sentences. In this paper, we show that by carefully handling words that the sentences (missing words), we can train a reliable latent variable model on sentences. In the process, we propose a new evaluation framework for sentence similarity: Concept Definition Retrieval. The new framework allows for large scale tuning and testing of Sentence Similarity models. Experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models. Our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="2082" citStr="Blei et al., 2003" startWordPosition="303" endWordPosition="306">To date, almost all Sentence Similarity [SS] approaches work in the high-dimensional word space and rely mainly on word similarity. There are two main (not unrelated) disadvantages to word similarity based approaches: 1. lexical ambiguity as the pairwise word similarity ignores the semantic interaction between the word and its sentential context; 2. word co-occurrence information is not sufficiently exploited. Latent variable models, such as Latent Semantic Analysis [LSA] (Landauer et al., 1998), Probabilistic Latent Semantic Analysis [PLSA] (Hofmann, 1999), Latent Dirichlet Allocation [LDA] (Blei et al., 2003) can solve the two issues naturally by modeling the semantics of words and sentences simultaneously in the low-dimensional latent space. However, attempts at addressing SS using LSA perform significantly below high dimensional word similarity based models (Mihalcea et al., 2006; O’Shea et al., 2008). We believe that the latent semantics approaches applied to date to the SS problem have not yielded positive results due to the deficient modeling of the sparsity in the semantic space. SS operates in a very limited contextual setting where the sentences are typically very short to derive robust la</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="14512" citStr="Dolan et al., 2004" startWordPosition="2502" endWordPosition="2505">posed in (Steck, 2010). semantics. More importantly, they do not have the sparsity issue (each user has rated over 100 items in the movie lens data2) and robust predictions can be made based on the observed ratings alone. 4 Evaluation for SS We need to show the impact of our proposed model WTMF on the SS task. However we are faced with a problem, the lack of a suitable large evaluation set from which we can derive robust observations. The two data sets we know of for SS are: 1. human-rated sentence pair similarity data set (Li et al., 2006) [LI06]; 2. the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) [MSR04]. The LI06 data set consists of 65 pairs of noun definitions selected from the Collin Cobuild Dictionary. A subset of 30 pairs is further selected by LI06 to render the similarity scores evenly distributed. While this is the ideal data set for SS, the small size makes it impossible for tuning SS algorithms or deriving significant performance conclusions. On the other hand, the MSR04 data set comprises a much larger set of sentence pairs: 4,076 training and 1,725 test pairs. The ratings on the pairs are binary labels: similar/not similar. This is not a problem per se, however the issue </context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>William Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5915" citStr="Fellbaum, 1998" startWordPosition="955" endWordPosition="956">s at the right level of emphasis/impact is central to completing the semantic picture for a sentence. LSA and PLSA/LDA work on a word-sentence co-occurrence matrix. Given a corpus, the row entries of the matrix are the unique M words in the corpus, and the N columns are the sentence ids. The yielded M x N co-occurrence matrix X comprises the TF-IDF values in each Xij cell, namely that TFIDF value of word wi in sentence sj. For ease of exposition, we will illustrate the problem using a special case of the SS framework where the sentences are concept definitions in a dictionary such as WordNet (Fellbaum, 1998) (WN). Therefore, the sentence corresponding to the concept definition of bank#n#1 is a sparse vector in X containing the following observed words where Xij =� 0: the 0.1, financial 5.5, institution 4, that 0.2, accept 2.1, deposit 3, and 0.1, channel 6, the 0.1, money 5, into 0.3, lend 3.5, activity 3 All the other words (girl, car,..., check, loan, business,...) in matrix X that do not occur in the concept definition are considered missing words for the concept entry bank#n#1, thereby their Xij = 0 . Topic models (PLSA/LDA) do not explicitly model missing words. PLSA assumes each document ha</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Feng</author>
<author>Yi-Ming Zhou</author>
<author>Trevor Martin</author>
</authors>
<title>Sentence similarity based on relevance.</title>
<date>2008</date>
<booktitle>In Proceedings of IPMU.</booktitle>
<contexts>
<context position="28030" citStr="Feng et al., 2008" startWordPosition="4913" endWordPosition="4916">set (30 pairs set). We cite their best performance for all reported results. Once the correct wm = 0.01 is chosen, WTMF results in the best Pearson’s r and best Spearman’s p (wm = 0.005 yields the second best r and p). Same as in CDR task, WTMF outperforms LDA by a large margin in both r and p. It indicates that the latent vectors induced by WTMF are able to not only identify same/similar sentences, but also identify the “correct” degree of dissimilar sentences. 0.955 ATOP 0.95 0.945 0.94 50 100 150 WTMF LDA 870 Model r p STASIS (Li et al., 2006) 0.8162 0.8126 (Liu et al., 2007) 0.841 0.8538 (Feng et al., 2008) 0.756 0.608 STS (Islam and Inkpen, 2008) 0.853 0.838 LSA (O’Shea et al., 2008) 0.8384 0.8714 Omiotis (Tsatsaronis et al., 2010) 0.856 0.8905 WSD-STS (Ho et al., 2010) 0.864 0.8341 SPD-STS (Ho et al., 2010) 0.895 0.9034 LDA (α = 0.05, 0 = 0.05) 0.8422 0.8663 WTMF (w. = 0.005, A = 20) 0.8876 0.8966 WTMF (w. = 0.01, A = 20) 0.8984 0.9091 Table 4: Pearson’s correlation r and Spearman’s correlation p on LI06 30 pairs Model Accuracy Random 51.3 LSA (Mihalcea et al., 2006) 68.4 full model (Mihalcea et al., 2006) 70.3 STS (Islam and Inkpen, 2008) 72.6 Omiotis (Tsatsaronis et al., 2010) 69.97 LDA (α =</context>
<context position="29870" citStr="Feng et al., 2008" startWordPosition="5249" endWordPosition="5252">cy. It suggests that WTMF is quite competitive in the paraphrase recognition task. It is worth noting that the best system on MSR04, STS (Islam and Inkpen, 2008), has much lower correlations on LI06 data set. The second best system among previous work on LI06 uses Spearman correlation, Omiotis (Tsatsaronis et al., 2010), and it yields a much worse accuracy on MSR04. The other works do not evaluate on both data sets. 6 Related Work Almost all current SS methods work in the highdimensional word space, and rely heavily on word/sense similarity measures, which is knowledge based (Li et al., 2006; Feng et al., 2008; Ho et al., 2010; Tsatsaronis et al., 2010), corpus-based (Islam and Inkpen, 2008) or hybrid (Mihalcea et al., 2006). Almost all of them are evaluated on LI06 data set. It is interesting to see that most works find word similarity measures, especially knowledge based ones, to be the most effective component, while other features do not work well (such as word order or syntactic information). Mihalcea et al. (2006) use LSA as a baseline, and O’Shea et al. (2008) train LSA on regular length documents. Both results are considerably lower than word similarity based methods. Hence, our work is the</context>
</contexts>
<marker>Feng, Zhou, Martin, 2008</marker>
<rawString>Jin Feng, Yi-Ming Zhou, and Trevor Martin. 2008. Sentence similarity based on relevance. In Proceedings of IPMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>101</pages>
<contexts>
<context position="18574" citStr="Griffiths and Steyvers, 2004" startWordPosition="3199" endWordPosition="3202">ents and Results We evaluate WTMF on three data sets: 1. CDR data set using ATOP metric; 2. Human-rated Sentence Similarity data set [LI06] using Pearson and Spearman Correlation; 3. MSR Paraphrase corpus [MSR04] using accuracy. The performance of WTMF on CDR is compared with (a) an Information Retrieval model (IR) that is based on surface word matching, (b) an ngram model (N-gram) that captures phrase overlaps by returning the number of overlapping ngrams as the similarity score of two sentences, (c) LSA that uses svds() function in Matlab, and (d) LDA that uses Gibbs Sampling for inference (Griffiths and Steyvers, 2004). WTMF is also compared with all existing reported SS results on LI06 and MSR04 data sets, as well as LDA that is trained on the same data as WTMF. The similarity of two sentences is computed by cosine similarity (except N-gram). More details on each task will be explained in the subsections. To eliminate randomness in statistical models (WTMF and LDA), all the reported results are averaged over 10 runs. We run 20 iterations for WTMF. And we run 5000 iterations for LDA; each LDA model is averaged over the last 10 Gibbs Sampling iterations to get more robust predictions. The latent vector of a </context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chukfong Ho</author>
</authors>
<title>Masrah Azrifah Azmi Murad, Rabiah Abdul Kadir, and Shyamala</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics.</booktitle>
<marker>Ho, 2010</marker>
<rawString>Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Abdul Kadir, and Shyamala C. Doraisamy. 2010. Word sense disambiguation-based sentence similarity. In Proceedings of the 23rd International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<contexts>
<context position="2027" citStr="Hofmann, 1999" startWordPosition="297" endWordPosition="298">herence Detection (Lapata and Barzilay, 2005), etc.To date, almost all Sentence Similarity [SS] approaches work in the high-dimensional word space and rely mainly on word similarity. There are two main (not unrelated) disadvantages to word similarity based approaches: 1. lexical ambiguity as the pairwise word similarity ignores the semantic interaction between the word and its sentential context; 2. word co-occurrence information is not sufficiently exploited. Latent variable models, such as Latent Semantic Analysis [LSA] (Landauer et al., 1998), Probabilistic Latent Semantic Analysis [PLSA] (Hofmann, 1999), Latent Dirichlet Allocation [LDA] (Blei et al., 2003) can solve the two issues naturally by modeling the semantics of words and sentences simultaneously in the low-dimensional latent space. However, attempts at addressing SS using LSA perform significantly below high dimensional word similarity based models (Mihalcea et al., 2006; O’Shea et al., 2008). We believe that the latent semantics approaches applied to date to the SS problem have not yielded positive results due to the deficient modeling of the sparsity in the semantic space. SS operates in a very limited contextual setting where the</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL.</booktitle>
<contexts>
<context position="16378" citStr="Hovy et al., 2006" startWordPosition="2820" endWordPosition="2823">tions but on a large scale. 4.1 Concept Definition Retrieval We define a new framework for evaluating SS and project it as a Concept Definition Retrieval (CDR) task where the data points are dictionary definitions. The intuition is that two definitions in different dic2http://www.grouplens.org/node/73, with 1M data set being the most widely used. 867 tionaries referring to the same concept should be assigned large similarity. In this setting, we design the CDR task in a search engine style. The SS algorithm has access to all the definitions in WordNet (WN). Given an OntoNotes (ON) definition (Hovy et al., 2006), the SS algorithm should rank the equivalent WN definition as high as possible based on sentence similarity. The manual mapping already exists for ON to WN. One ON definition can be mapped to several WN definitions. After preprocessing we obtain 13669 ON definitions mapped to 19655 WN definitions. The data set has the advantage of being very large and it doesn’t require further human scrutiny. After the SS model learns the co-occurrence of words from WN definitions, in the testing phase, given an ON definition d, the SS algorithm needs to identify the equivalent WN definitions by computing th</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic text similarity using corpus-based word similarity and string similarity.</title>
<date>2008</date>
<journal>ACM Transactions on Knowledge Discovery from Data,</journal>
<volume>2</volume>
<contexts>
<context position="15253" citStr="Islam and Inkpen, 2008" startWordPosition="2632" endWordPosition="2635">bset of 30 pairs is further selected by LI06 to render the similarity scores evenly distributed. While this is the ideal data set for SS, the small size makes it impossible for tuning SS algorithms or deriving significant performance conclusions. On the other hand, the MSR04 data set comprises a much larger set of sentence pairs: 4,076 training and 1,725 test pairs. The ratings on the pairs are binary labels: similar/not similar. This is not a problem per se, however the issue is that it is very strict in its assignment of a positive label, for example the following sentence pair as cited in (Islam and Inkpen, 2008) is rated not semantically similar: Ballmer has been vocal in the past warning that Linux is a threat to Microsoft. In the memo, Ballmer reiterated the open-source threat to Microsoft. We believe that the ratings on a data set for SS should accommodate variable degrees of similarity with various ratings, however such a large scale set does not exist yet. Therefore for purposes of evaluating our proposed approach we devise a new framework inspired by the LI06 data set in that it comprises concept definitions but on a large scale. 4.1 Concept Definition Retrieval We define a new framework for ev</context>
<context position="28071" citStr="Islam and Inkpen, 2008" startWordPosition="4920" endWordPosition="4923">st performance for all reported results. Once the correct wm = 0.01 is chosen, WTMF results in the best Pearson’s r and best Spearman’s p (wm = 0.005 yields the second best r and p). Same as in CDR task, WTMF outperforms LDA by a large margin in both r and p. It indicates that the latent vectors induced by WTMF are able to not only identify same/similar sentences, but also identify the “correct” degree of dissimilar sentences. 0.955 ATOP 0.95 0.945 0.94 50 100 150 WTMF LDA 870 Model r p STASIS (Li et al., 2006) 0.8162 0.8126 (Liu et al., 2007) 0.841 0.8538 (Feng et al., 2008) 0.756 0.608 STS (Islam and Inkpen, 2008) 0.853 0.838 LSA (O’Shea et al., 2008) 0.8384 0.8714 Omiotis (Tsatsaronis et al., 2010) 0.856 0.8905 WSD-STS (Ho et al., 2010) 0.864 0.8341 SPD-STS (Ho et al., 2010) 0.895 0.9034 LDA (α = 0.05, 0 = 0.05) 0.8422 0.8663 WTMF (w. = 0.005, A = 20) 0.8876 0.8966 WTMF (w. = 0.01, A = 20) 0.8984 0.9091 Table 4: Pearson’s correlation r and Spearman’s correlation p on LI06 30 pairs Model Accuracy Random 51.3 LSA (Mihalcea et al., 2006) 68.4 full model (Mihalcea et al., 2006) 70.3 STS (Islam and Inkpen, 2008) 72.6 Omiotis (Tsatsaronis et al., 2010) 69.97 LDA (α = 0.05, 0 = 0.05) 68.6 WTMF (w. = 0.01, A </context>
<context position="29414" citStr="Islam and Inkpen, 2008" startWordPosition="5169" endWordPosition="5172">of applying WTMF on MSR04 data. We use the same parameter setting used for the LI06 evaluation setting since both sets are human-rated sentence pairs (A = 20,w.. = 0.01, K = 100). We use the training set of MSR04 data to select a threshold of sentence similarity for the binary label. Table 5 summarizes the accuracy of other SS models noted in the literature and evaluated on MSR04 test set. Compared to previous SS work and LDA, WTMF has the second best accuracy. It suggests that WTMF is quite competitive in the paraphrase recognition task. It is worth noting that the best system on MSR04, STS (Islam and Inkpen, 2008), has much lower correlations on LI06 data set. The second best system among previous work on LI06 uses Spearman correlation, Omiotis (Tsatsaronis et al., 2010), and it yields a much worse accuracy on MSR04. The other works do not evaluate on both data sets. 6 Related Work Almost all current SS methods work in the highdimensional word space, and rely heavily on word/sense similarity measures, which is knowledge based (Li et al., 2006; Feng et al., 2008; Ho et al., 2010; Tsatsaronis et al., 2010), corpus-based (Islam and Inkpen, 2008) or hybrid (Mihalcea et al., 2006). Almost all of them are ev</context>
</contexts>
<marker>Islam, Inkpen, 2008</marker>
<rawString>Aminul Islam and Diana Inkpen. 2008. Semantic text similarity using corpus-based word similarity and string similarity. ACM Transactions on Knowledge Discovery from Data, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ou Jin</author>
<author>Nathan N Liu</author>
<author>Kai Zhao</author>
<author>Yong Yu</author>
<author>Qiang Yang</author>
</authors>
<title>Transferring topical knowledge from auxiliary long texts for short text clustering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th ACM international conference on Information and knowledge management.</booktitle>
<contexts>
<context position="30836" citStr="Jin et al. (2011)" startWordPosition="5413" endWordPosition="5416">(such as word order or syntactic information). Mihalcea et al. (2006) use LSA as a baseline, and O’Shea et al. (2008) train LSA on regular length documents. Both results are considerably lower than word similarity based methods. Hence, our work is the first to successfully approach SS in the latent space. Although there has been work modeling latent semantics for short texts (tweets) in LDA, the focus has been on exploiting additional features in Twitter, hence restricted to Twitter data. Ramage et al. (2010) use tweet metadata (author, hashtag) as some supervised information to model tweets. Jin et al. (2011) use long similar documents (the article that is referred by a url in tweets) to help understand the tweet. In contrast, our approach relies solely on the information in the texts by modeling local missing words, and does not need any additional data, which renders our approach much more widely applicable. 7 Conclusions We explicitly model missing words to alleviate the sparsity problem in modeling short texts. We also propose a new evaluation framework for sentence similarity that allows large scale tuning and testing. Experiment results on three data sets show that our model WTMF significant</context>
</contexts>
<marker>Jin, Liu, Zhao, Yu, Yang, 2011</marker>
<rawString>Ou Jin, Nathan N. Liu, Kai Zhao, Yong Yu, and Qiang Yang. 2011. Transferring topical knowledge from auxiliary long texts for short text clustering. In Proceedings of the 20th ACM international conference on Information and knowledge management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL.</booktitle>
<contexts>
<context position="1369" citStr="Kauchak and Barzilay, 2006" startWordPosition="199" endWordPosition="203">on Retrieval. The new framework allows for large scale tuning and testing of Sentence Similarity models. Experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models. Our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is at the core of many NLP applications that focus on sentence level semantics such as Machine Translation (Kauchak and Barzilay, 2006), Summarization (Zhou et al., 2006), Text Coherence Detection (Lapata and Barzilay, 2005), etc.To date, almost all Sentence Similarity [SS] approaches work in the high-dimensional word space and rely mainly on word similarity. There are two main (not unrelated) disadvantages to word similarity based approaches: 1. lexical ambiguity as the pairwise word similarity ignores the semantic interaction between the word and its sentential context; 2. word co-occurrence information is not sufficiently exploited. Latent variable models, such as Latent Semantic Analysis [LSA] (Landauer et al., 1998), Pro</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<volume>25</volume>
<contexts>
<context position="1964" citStr="Landauer et al., 1998" startWordPosition="287" endWordPosition="290">Kauchak and Barzilay, 2006), Summarization (Zhou et al., 2006), Text Coherence Detection (Lapata and Barzilay, 2005), etc.To date, almost all Sentence Similarity [SS] approaches work in the high-dimensional word space and rely mainly on word similarity. There are two main (not unrelated) disadvantages to word similarity based approaches: 1. lexical ambiguity as the pairwise word similarity ignores the semantic interaction between the word and its sentential context; 2. word co-occurrence information is not sufficiently exploited. Latent variable models, such as Latent Semantic Analysis [LSA] (Landauer et al., 1998), Probabilistic Latent Semantic Analysis [PLSA] (Hofmann, 1999), Latent Dirichlet Allocation [LDA] (Blei et al., 2003) can solve the two issues naturally by modeling the semantics of words and sentences simultaneously in the low-dimensional latent space. However, attempts at addressing SS using LSA perform significantly below high dimensional word similarity based models (Mihalcea et al., 2006; O’Shea et al., 2008). We believe that the latent semantics approaches applied to date to the SS problem have not yielded positive results due to the deficient modeling of the sparsity in the semantic sp</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K Landauer, Peter W. Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis. Discourse Processes, 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatic evaluation of text coherence: Models and representations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1458" citStr="Lapata and Barzilay, 2005" startWordPosition="212" endWordPosition="215">larity models. Experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models. Our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is at the core of many NLP applications that focus on sentence level semantics such as Machine Translation (Kauchak and Barzilay, 2006), Summarization (Zhou et al., 2006), Text Coherence Detection (Lapata and Barzilay, 2005), etc.To date, almost all Sentence Similarity [SS] approaches work in the high-dimensional word space and rely mainly on word similarity. There are two main (not unrelated) disadvantages to word similarity based approaches: 1. lexical ambiguity as the pairwise word similarity ignores the semantic interaction between the word and its sentential context; 2. word co-occurrence information is not sufficiently exploited. Latent variable models, such as Latent Semantic Analysis [LSA] (Landauer et al., 1998), Probabilistic Latent Semantic Analysis [PLSA] (Hofmann, 1999), Latent Dirichlet Allocation [</context>
</contexts>
<marker>Lapata, Barzilay, 2005</marker>
<rawString>Mirella Lapata and Regina Barzilay. 2005. Automatic evaluation of text coherence: Models and representations. In Proceedings of the 19th International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>Davi d McLean</author>
<author>Zuhair A Bandar</author>
<author>James D O Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence similarity based on semantic nets and corpus statistics.</title>
<date>2006</date>
<journal>IEEE Transaction on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<contexts>
<context position="14439" citStr="Li et al., 2006" startWordPosition="2490" endWordPosition="2493">erested in the sentence 1An efficient way to compute equation 4 is proposed in (Steck, 2010). semantics. More importantly, they do not have the sparsity issue (each user has rated over 100 items in the movie lens data2) and robust predictions can be made based on the observed ratings alone. 4 Evaluation for SS We need to show the impact of our proposed model WTMF on the SS task. However we are faced with a problem, the lack of a suitable large evaluation set from which we can derive robust observations. The two data sets we know of for SS are: 1. human-rated sentence pair similarity data set (Li et al., 2006) [LI06]; 2. the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) [MSR04]. The LI06 data set consists of 65 pairs of noun definitions selected from the Collin Cobuild Dictionary. A subset of 30 pairs is further selected by LI06 to render the similarity scores evenly distributed. While this is the ideal data set for SS, the small size makes it impossible for tuning SS algorithms or deriving significant performance conclusions. On the other hand, the MSR04 data set comprises a much larger set of sentence pairs: 4,076 training and 1,725 test pairs. The ratings on the pairs are binary labe</context>
<context position="27964" citStr="Li et al., 2006" startWordPosition="4901" endWordPosition="4904"> results of all current SS models with respect to the LI06 data set (30 pairs set). We cite their best performance for all reported results. Once the correct wm = 0.01 is chosen, WTMF results in the best Pearson’s r and best Spearman’s p (wm = 0.005 yields the second best r and p). Same as in CDR task, WTMF outperforms LDA by a large margin in both r and p. It indicates that the latent vectors induced by WTMF are able to not only identify same/similar sentences, but also identify the “correct” degree of dissimilar sentences. 0.955 ATOP 0.95 0.945 0.94 50 100 150 WTMF LDA 870 Model r p STASIS (Li et al., 2006) 0.8162 0.8126 (Liu et al., 2007) 0.841 0.8538 (Feng et al., 2008) 0.756 0.608 STS (Islam and Inkpen, 2008) 0.853 0.838 LSA (O’Shea et al., 2008) 0.8384 0.8714 Omiotis (Tsatsaronis et al., 2010) 0.856 0.8905 WSD-STS (Ho et al., 2010) 0.864 0.8341 SPD-STS (Ho et al., 2010) 0.895 0.9034 LDA (α = 0.05, 0 = 0.05) 0.8422 0.8663 WTMF (w. = 0.005, A = 20) 0.8876 0.8966 WTMF (w. = 0.01, A = 20) 0.8984 0.9091 Table 4: Pearson’s correlation r and Spearman’s correlation p on LI06 30 pairs Model Accuracy Random 51.3 LSA (Mihalcea et al., 2006) 68.4 full model (Mihalcea et al., 2006) 70.3 STS (Islam and In</context>
<context position="29851" citStr="Li et al., 2006" startWordPosition="5245" endWordPosition="5248">econd best accuracy. It suggests that WTMF is quite competitive in the paraphrase recognition task. It is worth noting that the best system on MSR04, STS (Islam and Inkpen, 2008), has much lower correlations on LI06 data set. The second best system among previous work on LI06 uses Spearman correlation, Omiotis (Tsatsaronis et al., 2010), and it yields a much worse accuracy on MSR04. The other works do not evaluate on both data sets. 6 Related Work Almost all current SS methods work in the highdimensional word space, and rely heavily on word/sense similarity measures, which is knowledge based (Li et al., 2006; Feng et al., 2008; Ho et al., 2010; Tsatsaronis et al., 2010), corpus-based (Islam and Inkpen, 2008) or hybrid (Mihalcea et al., 2006). Almost all of them are evaluated on LI06 data set. It is interesting to see that most works find word similarity measures, especially knowledge based ones, to be the most effective component, while other features do not work well (such as word order or syntactic information). Mihalcea et al. (2006) use LSA as a baseline, and O’Shea et al. (2008) train LSA on regular length documents. Both results are considerably lower than word similarity based methods. Hen</context>
</contexts>
<marker>Li, McLean, Bandar, Shea, Crockett, 2006</marker>
<rawString>Yuhua Li, Davi d McLean, Zuhair A. Bandar, James D. O Shea, and Keeley Crockett. 2006. Sentence similarity based on semantic nets and corpus statistics. IEEE Transaction on Knowledge and Data Engineering, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao-Ying Liu</author>
<author>Yi-Ming Zhou</author>
<author>Ruo-Shi Zheng</author>
</authors>
<title>Sentence similarity based on dynamic time warping.</title>
<date>2007</date>
<booktitle>In The International Conference on Semantic Computing.</booktitle>
<contexts>
<context position="27997" citStr="Liu et al., 2007" startWordPosition="4907" endWordPosition="4910">s with respect to the LI06 data set (30 pairs set). We cite their best performance for all reported results. Once the correct wm = 0.01 is chosen, WTMF results in the best Pearson’s r and best Spearman’s p (wm = 0.005 yields the second best r and p). Same as in CDR task, WTMF outperforms LDA by a large margin in both r and p. It indicates that the latent vectors induced by WTMF are able to not only identify same/similar sentences, but also identify the “correct” degree of dissimilar sentences. 0.955 ATOP 0.95 0.945 0.94 50 100 150 WTMF LDA 870 Model r p STASIS (Li et al., 2006) 0.8162 0.8126 (Liu et al., 2007) 0.841 0.8538 (Feng et al., 2008) 0.756 0.608 STS (Islam and Inkpen, 2008) 0.853 0.838 LSA (O’Shea et al., 2008) 0.8384 0.8714 Omiotis (Tsatsaronis et al., 2010) 0.856 0.8905 WSD-STS (Ho et al., 2010) 0.864 0.8341 SPD-STS (Ho et al., 2010) 0.895 0.9034 LDA (α = 0.05, 0 = 0.05) 0.8422 0.8663 WTMF (w. = 0.005, A = 20) 0.8876 0.8966 WTMF (w. = 0.01, A = 20) 0.8984 0.9091 Table 4: Pearson’s correlation r and Spearman’s correlation p on LI06 30 pairs Model Accuracy Random 51.3 LSA (Mihalcea et al., 2006) 68.4 full model (Mihalcea et al., 2006) 70.3 STS (Islam and Inkpen, 2008) 72.6 Omiotis (Tsatsar</context>
</contexts>
<marker>Liu, Zhou, Zheng, 2007</marker>
<rawString>Xiao-Ying Liu, Yi-Ming Zhou, and Ruo-Shi Zheng. 2007. Sentence similarity based on dynamic time warping. In The International Conference on Semantic Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Articial Intelligence.</booktitle>
<contexts>
<context position="2360" citStr="Mihalcea et al., 2006" startWordPosition="347" endWordPosition="350">res the semantic interaction between the word and its sentential context; 2. word co-occurrence information is not sufficiently exploited. Latent variable models, such as Latent Semantic Analysis [LSA] (Landauer et al., 1998), Probabilistic Latent Semantic Analysis [PLSA] (Hofmann, 1999), Latent Dirichlet Allocation [LDA] (Blei et al., 2003) can solve the two issues naturally by modeling the semantics of words and sentences simultaneously in the low-dimensional latent space. However, attempts at addressing SS using LSA perform significantly below high dimensional word similarity based models (Mihalcea et al., 2006; O’Shea et al., 2008). We believe that the latent semantics approaches applied to date to the SS problem have not yielded positive results due to the deficient modeling of the sparsity in the semantic space. SS operates in a very limited contextual setting where the sentences are typically very short to derive robust latent semantics. Apart from the SS setting, robust modeling of the latent semantics of short sentences/texts is becoming a pressing need due to the pervasive presence of more bursty data sets such as Twitter feeds and SMS where short contexts are an inherent characteristic of th</context>
<context position="19382" citStr="Mihalcea et al., 2006" startWordPosition="3344" endWordPosition="3347">puted by cosine similarity (except N-gram). More details on each task will be explained in the subsections. To eliminate randomness in statistical models (WTMF and LDA), all the reported results are averaged over 10 runs. We run 20 iterations for WTMF. And we run 5000 iterations for LDA; each LDA model is averaged over the last 10 Gibbs Sampling iterations to get more robust predictions. The latent vector of a sentence is computed by: (1) using equation 4 in WTMF, or (2) summing up the latent vectors of all the constituent words weighted by Xij in LSA and LDA, similar to the work reported in (Mihalcea et al., 2006). For LDA the latent vector of a word is computed by P(z|w). It is worth noting that we could directly use the estimated topic distribution Bj to represent a sentence, however, as discussed the topic distribution has only non-zero values on one or two topics, leading to a low ATOP value around 0.8. 5.1 Corpus The corpus we use comprises three dictionaries WN, ON, Wiktionary [Wik],3 Brown corpus. For all dictionaries, we only keep the definitions without examples, and discard the mapping between sense ids and definitions. All definitions are simply treated as individual documents. We crawl Wik </context>
<context position="28501" citStr="Mihalcea et al., 2006" startWordPosition="5000" endWordPosition="5003">0.955 ATOP 0.95 0.945 0.94 50 100 150 WTMF LDA 870 Model r p STASIS (Li et al., 2006) 0.8162 0.8126 (Liu et al., 2007) 0.841 0.8538 (Feng et al., 2008) 0.756 0.608 STS (Islam and Inkpen, 2008) 0.853 0.838 LSA (O’Shea et al., 2008) 0.8384 0.8714 Omiotis (Tsatsaronis et al., 2010) 0.856 0.8905 WSD-STS (Ho et al., 2010) 0.864 0.8341 SPD-STS (Ho et al., 2010) 0.895 0.9034 LDA (α = 0.05, 0 = 0.05) 0.8422 0.8663 WTMF (w. = 0.005, A = 20) 0.8876 0.8966 WTMF (w. = 0.01, A = 20) 0.8984 0.9091 Table 4: Pearson’s correlation r and Spearman’s correlation p on LI06 30 pairs Model Accuracy Random 51.3 LSA (Mihalcea et al., 2006) 68.4 full model (Mihalcea et al., 2006) 70.3 STS (Islam and Inkpen, 2008) 72.6 Omiotis (Tsatsaronis et al., 2010) 69.97 LDA (α = 0.05, 0 = 0.05) 68.6 WTMF (w. = 0.01, A = 20) 71.51 Table 5: Performance on MSR04 test set 5.4 MSR04: MSR Paraphrase Corpus Finally, we briefly discuss results of applying WTMF on MSR04 data. We use the same parameter setting used for the LI06 evaluation setting since both sets are human-rated sentence pairs (A = 20,w.. = 0.01, K = 100). We use the training set of MSR04 data to select a threshold of sentence similarity for the binary label. Table 5 summarizes the ac</context>
<context position="29987" citStr="Mihalcea et al., 2006" startWordPosition="5268" endWordPosition="5271">est system on MSR04, STS (Islam and Inkpen, 2008), has much lower correlations on LI06 data set. The second best system among previous work on LI06 uses Spearman correlation, Omiotis (Tsatsaronis et al., 2010), and it yields a much worse accuracy on MSR04. The other works do not evaluate on both data sets. 6 Related Work Almost all current SS methods work in the highdimensional word space, and rely heavily on word/sense similarity measures, which is knowledge based (Li et al., 2006; Feng et al., 2008; Ho et al., 2010; Tsatsaronis et al., 2010), corpus-based (Islam and Inkpen, 2008) or hybrid (Mihalcea et al., 2006). Almost all of them are evaluated on LI06 data set. It is interesting to see that most works find word similarity measures, especially knowledge based ones, to be the most effective component, while other features do not work well (such as word order or syntactic information). Mihalcea et al. (2006) use LSA as a baseline, and O’Shea et al. (2008) train LSA on regular length documents. Both results are considerably lower than word similarity based methods. Hence, our work is the first to successfully approach SS in the latent space. Although there has been work modeling latent semantics for sh</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the 21st National Conference on Articial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James O’Shea</author>
<author>Zuhair Bandar</author>
<author>Keeley Crockett</author>
<author>David McLean</author>
</authors>
<title>A comparative study of two short text semantic similarity measures.</title>
<date>2008</date>
<booktitle>In Proceedings of the Agent and Multi-Agent Systems: Technologies and Applications, Second KES International Symposium (KES-AMSTA).</booktitle>
<marker>O’Shea, Bandar, Crockett, McLean, 2008</marker>
<rawString>James O’Shea, Zuhair Bandar, Keeley Crockett, and David McLean. 2008. A comparative study of two short text semantic similarity measures. In Proceedings of the Agent and Multi-Agent Systems: Technologies and Applications, Second KES International Symposium (KES-AMSTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>Susan Dumais</author>
<author>Dan Liebling</author>
</authors>
<title>Characterizing microblogs with topic models.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="4507" citStr="Ramage et al. (2010)" startWordPosition="706" endWordPosition="709">words, we decide to model sentences using a weighted matrix factorization approach (Srebro and Jaakkola, 2003), which allows us to treat observed words and missing words differently. We handle missing words using a weighting scheme that distinguishes missing words from observed words yielding robust latent vectors for sentences. Since we use a feature that is already implied by the text itself, our approach is very general (similar to LSA/LDA) in that it can be applied to any format of short texts. In contrast, existing work on modeling short texts focuses on exploiting additional data, e.g., Ramage et al. (2010) model tweets using their metadata (author, hashtag, etc.). Moreover in this paper, we introduce a new evaluation framework for SS: Concept Definition Retrieval (CDR). Compared to existing data sets, the CDR data set allows for large scale tuning and testing of SS modules without further human annotation. 2 Limitations of Topic Models and LSA for Modeling Sentences Usually latent variable models aim to find a latent semantic profile for a sentence that is most relevant to the observed words. By explicitly modeling missing words, we set another criterion to the latent semantics profile: it shou</context>
<context position="30733" citStr="Ramage et al. (2010)" startWordPosition="5397" endWordPosition="5400">specially knowledge based ones, to be the most effective component, while other features do not work well (such as word order or syntactic information). Mihalcea et al. (2006) use LSA as a baseline, and O’Shea et al. (2008) train LSA on regular length documents. Both results are considerably lower than word similarity based methods. Hence, our work is the first to successfully approach SS in the latent space. Although there has been work modeling latent semantics for short texts (tweets) in LDA, the focus has been on exploiting additional features in Twitter, hence restricted to Twitter data. Ramage et al. (2010) use tweet metadata (author, hashtag) as some supervised information to model tweets. Jin et al. (2011) use long similar documents (the article that is referred by a url in tweets) to help understand the tweet. In contrast, our approach relies solely on the information in the texts by modeling local missing words, and does not need any additional data, which renders our approach much more widely applicable. 7 Conclusions We explicitly model missing words to alleviate the sparsity problem in modeling short texts. We also propose a new evaluation framework for sentence similarity that allows lar</context>
</contexts>
<marker>Ramage, Dumais, Liebling, 2010</marker>
<rawString>Daniel Ramage, Susan Dumais, and Dan Liebling. 2010. Characterizing microblogs with topic models. In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Srebro</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Weighted low-rank approximations.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twentieth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="3997" citStr="Srebro and Jaakkola, 2003" startWordPosition="622" endWordPosition="625"> us what the sentence is about, missing words can be used to tell us what the sentence is not about. We assume that the semantic space of both the observed 864 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 864–872, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics and missing words make up the complete semantics profile of a sentence. After analyzing the way traditional latent variable models (LSA, PLSA/LDA) handle missing words, we decide to model sentences using a weighted matrix factorization approach (Srebro and Jaakkola, 2003), which allows us to treat observed words and missing words differently. We handle missing words using a weighting scheme that distinguishes missing words from observed words yielding robust latent vectors for sentences. Since we use a feature that is already implied by the text itself, our approach is very general (similar to LSA/LDA) in that it can be applied to any format of short texts. In contrast, existing work on modeling short texts focuses on exploiting additional data, e.g., Ramage et al. (2010) model tweets using their metadata (author, hashtag, etc.). Moreover in this paper, we int</context>
<context position="10401" citStr="Srebro and Jaakkola, 2003" startWordPosition="1750" endWordPosition="1753">s observed words and missing words equally the same, and due to the large number of missing words, the information of observed words is lost: Robs − Rmiss ≈ −Rmiss. The third vector is the ideal semantics profile, since it is also related to the institution dimension. It has a slightly smaller Robs in comparison to the first vector, yet it has a substantially smaller Rmiss. In order to favor the ideal vector over other vectors, we simply need to adjust the objective function by assigning a smaller weight to Rmiss such as: Robs − 0.01 × Rmiss. Accordingly, we use weighted matrix factorization (Srebro and Jaakkola, 2003) to model missing words. 3 The Proposed Approach 3.1 Weighted Matrix Factorization The weighted matrix factorization [WMF] approach is very similar to SVD, except that it allows for direct control on each matrix cell Xij. The model factorizes the original matrix X into two matrices such that X ≈ P&gt;Q, where P is a K × M matrix, and Q is a K × N matrix (figure 1). The model parameters (vectors in P and Q) are optimized by minimizing the objective function: Wij (P·,i · Q·,j − Xij)2 + λ||P||22 + λ||Q||22 (3) where A is a free regularization factor, and the weight matrix W defines a weight for each</context>
<context position="11890" citStr="Srebro and Jaakkola, 2003" startWordPosition="2019" endWordPosition="2022"> inner product of P·,i and Q·,j is used to approximate semantic relatedness of word wi and sentence sj: P·,i · Q·,j ≈ Xij, as the shaded parts in Figure 1; (2) equation 3 explicitly requires a sentence should not be related to its missing words by forcing P·,i · Q·,j = 0 for missing words Xij = 0. (3) we can compute the similarity of two sentences sj and sj0 using the cosine similarity between Q·,j, Q·,j0. The latent vectors in P and Q are first randomly initialized, then can be computed iteratively by the following equations (derivation is omitted due to limited space, which can be found in (Srebro and Jaakkola, 2003)): � �−1 P·,i = Q W� (i)Q&gt; + λI Q W�(i)X&gt; i,· � �−1 Q·,j = PW� (j)P&gt; + λI PW�(i)X·,j X i X j (4) 866 Figure 1: Matrix Factorization where W�(i) = diag(W�,i) is an M x M diagonal matrix containing ith row of weight matrix W. Similarly, W�(j) = diag(W�,j) is an N x N diagonal matrix containing jth column of W. 3.2 Modeling Missing Words It is straightforward to implement the idea in Section 2.1 (choosing a latent vector that maximizes Robs − 0.01 x Rmiss) in the WMF framework, by assigning a small weight for all the missing words and minimizing equation 3: � Wz 1, if Xzj 0 ( ) &apos;� = wm, if Xzj = </context>
</contexts>
<marker>Srebro, Jaakkola, 2003</marker>
<rawString>Nathan Srebro and Tommi Jaakkola. 2003. Weighted low-rank approximations. In Proceedings of the Twentieth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Steck</author>
</authors>
<title>Training and testing of recommender systems on data missing not at random.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="13182" citStr="Steck, 2010" startWordPosition="2264" endWordPosition="2265">s solution is quite elegant: 1. it explicitly tells the model that in general all missing words should not be related to the sentence; 2. meanwhile latent semantics are mainly generalized based on observed words, and the model is not penalized too much (wm is very small) when it is very confident that the sentence is highly related to a small subset of missing words based on their latent semantics profiles (bank#n#1 definition sentence is related to its missing words check loan). We adopt the same approach (assigning a small weight for some cells in WMF) proposed for recommender systems [RS] (Steck, 2010). In RS, an incomplete rating matrix R is proposed, where rows are users and columns are items. Typically, a user rates only some of the items, hence, the RS system needs to predict the missing ratings. Steck (2010) guesses a value for all the missing cells, and sets a small weight for those cells. Compared to (Steck, 2010), we are facing a different problem and targeting a different goal. We have a full matrix X where missing words have a 0 value, while the missing ratings in RS are unavailable – the values are unknown, hence R is not complete. In the RS setting, they are interested in predic</context>
</contexts>
<marker>Steck, 2010</marker>
<rawString>Harald Steck. 2010. Training and testing of recommender systems on data missing not at random. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Tsatsaronis</author>
</authors>
<title>Iraklis Varlamis, and Michalis Vazirgiannis.</title>
<date>2010</date>
<journal>Journal of Articial Intelligence Research,</journal>
<volume>37</volume>
<marker>Tsatsaronis, 2010</marker>
<rawString>George Tsatsaronis, Iraklis Varlamis, and Michalis Vazirgiannis. 2010. Text relatedness based on a word thesaurus. Journal of Articial Intelligence Research, 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Chin-Yew Lin</author>
<author>Dragos Stefan Munteanu</author>
<author>Eduard Hovy</author>
</authors>
<title>Paraeval: Using paraphrases to evaluate summaries automatically.</title>
<date>2006</date>
<booktitle>In Proceedings of Human Language Tech-nology Conference of the North American Chapter of the ACL,.</booktitle>
<contexts>
<context position="1404" citStr="Zhou et al., 2006" startWordPosition="205" endWordPosition="208">arge scale tuning and testing of Sentence Similarity models. Experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models. Our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is at the core of many NLP applications that focus on sentence level semantics such as Machine Translation (Kauchak and Barzilay, 2006), Summarization (Zhou et al., 2006), Text Coherence Detection (Lapata and Barzilay, 2005), etc.To date, almost all Sentence Similarity [SS] approaches work in the high-dimensional word space and rely mainly on word similarity. There are two main (not unrelated) disadvantages to word similarity based approaches: 1. lexical ambiguity as the pairwise word similarity ignores the semantic interaction between the word and its sentential context; 2. word co-occurrence information is not sufficiently exploited. Latent variable models, such as Latent Semantic Analysis [LSA] (Landauer et al., 1998), Probabilistic Latent Semantic Analysis</context>
</contexts>
<marker>Zhou, Lin, Munteanu, Hovy, 2006</marker>
<rawString>Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy. 2006. Paraeval: Using paraphrases to evaluate summaries automatically. In Proceedings of Human Language Tech-nology Conference of the North American Chapter of the ACL,.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>