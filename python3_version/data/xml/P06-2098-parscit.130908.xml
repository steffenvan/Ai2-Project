<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.996013">
Exact Decoding for Jointly Labeling and Chunking Sequences
</title>
<author confidence="0.994994">
Nobuyuki Shimizu
</author>
<affiliation confidence="0.999117">
Department of Computer Science
State University of New York at Albany
</affiliation>
<address confidence="0.772157">
Albany, NY 12222, USA
</address>
<email confidence="0.995431">
nobuyuki@shimizu.name
</email>
<author confidence="0.998177">
Andrew Haas
</author>
<affiliation confidence="0.9989685">
Department of Computer Science
State University of New York at Albany
</affiliation>
<address confidence="0.846381">
Albany, NY 12222 USA
</address>
<email confidence="0.998956">
haas@cs.albany.edu
</email>
<sectionHeader confidence="0.995637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999819647058824">
There are two decoding algorithms essen-
tial to the area of natural language pro-
cessing. One is the Viterbi algorithm
for linear-chain models, such as HMMs
or CRFs. The other is the CKY algo-
rithm for probabilistic context free gram-
mars. However, tasks such as noun phrase
chunking and relation extraction seem to
fall between the two, neither of them be-
ing the best fit. Ideally we would like to
model entities and relations, with two lay-
ers of labels. We present a tractable algo-
rithm for exact inference over two layers
of labels and chunks with time complexity
O(n2), and provide empirical results com-
paring our model with linear-chain mod-
els.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999863409090909">
The Viterbi algorithm and the CKY algorithms are
two decoding algorithms essential to the area of nat-
ural language processing. The former models a lin-
ear chain of labels such as part of speech tags, and
the latter models a parse tree. Both are used to ex-
tract the best prediction from the model (Manning
and Schutze, 1999).
However, some tasks seem to fall between the
two, having more than one layer but flatter than the
trees created by parsers. For example, in relation
extraction, we have entities in one layer and rela-
tions between entities as another layer. Another task
is shallow parsing. We may want to model part-of-
speech tags and noun/verb chunks at the same time,
since performing simultaneous labeling may result
in increased joint accuracy by sharing information
between the two layers of labels.
To apply the Viterbi decoder to such tasks, we
need two models, one for each layer. We must feed
the output of one layer to the next layer. In such an
approach, errors in earlier processing nearly always
accumulate and produce erroneous results at the end.
If we use CKY, we usually end up flattening the out-
put tree to obtain the desired output. This seems like
a round-about way of modeling two layers.
There are previous attempts at modeling two
layer labeling. Dynamic Conditional Random Fields
(DCRFs) by (McCallum et al, 2003; Sutton et al,
2004) is one such attempt, however, exact inference
is in general intractable for these models and the
authors were forced to settle for approximate infer-
ence.
Our contribution is a novel model for two layer
labeling, for which exact decoding is tractable. Our
experiments show that our use of label-chunk struc-
tures results in significantly better performance over
cascaded CRFs, and that the model is a promising
alternative to DCRFs.
The paper is organaized a follows: In Section 2
and 3, we describe the model and present the de-
coding algorithm. Section 4 describes the learning
methods applicable to our model and the baseline
models. In Section 5 and 6, we describe the experi-
ments and the results.
</bodyText>
<page confidence="0.978363">
763
</page>
<note confidence="0.9350295">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 763–770,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<table confidence="0.986998538461538">
Token POS NP
U.K. JADJ B
base NOUN I
rates NOUN I
are VERB O
at OTHER O
their OTHER B
highest JADJ I
level NOUN I
in OTHER O
eight OTHER B
years NOUN I
. OTHER O
</table>
<tableCaption confidence="0.999969">
Table 1: Example with POS and NP tags
</tableCaption>
<sectionHeader confidence="0.966056" genericHeader="method">
2 Model for Joint Labeling and Chunking
</sectionHeader>
<bodyText confidence="0.999574766666667">
Consider the task of finding noun chunks. The noun
chunk extends from the beginning of a noun phrase
to the head noun, excluding postmodifiers (which
are difficult to attach correctly). Table 1 shows a
sentence labeled with POS tags and segmented into
noun chunks. B marks the first word of a noun
chunk, I the other words in a noun chunk, and O
the words that are not in a noun chunk. Note that
we collapsed the 45 different POS labels into 5 la-
bels, following (McCallum et al, 2003). All differ-
ent types of adjectives are labeled as JADJ.
Each word carries two tags. Given the first layer,
our aim is to present a model that can predict the
second and third layers of tags at the same time.
Assume we have n training samples, I(xi, yi)}ni�1,
where xi is a sequence of input tokens and yi is a
label-chunk structure for xi. In this example, the
first column contains the tokens xi and the second
and third columns together represent the label-chunk
structures yi. We will present an efficient exact de-
coding for this structure.
The label-chunk structure, shown in Table 2, is a
representation of the two layers of tags. The tuples
in Table 2 are called parts. If the token at index r
carries a POS tag P and a chunk tag C, the first layer
includes part (C, P, r). This part is called a node.
If the tokens at index r − 1 and r are in the same
chunk, and C is the label of that chunk, the first layer
also includes part (C, P0, P, r−1, r) (where P0 and
P are the POS tags of the tokens at r − 1 and r
</bodyText>
<equation confidence="0.983030458333333">
Token First Layer (POS) Second Layer (NP)
U.K. (I, JADJ, 0)
(I,JADJ,NOUN, 0,1)
base (I, NOUN, 1)
(I,NOUN,NOUN,1, 2)
rates (I, NOUN, 2) (I, 0, 2)
(I, O, 2, 3)
are (O, VERB, 3)
(O, VERB, OTHER, 3, 4)
at (O, OTHER, 4) (O, 3, 4)
(O, I, 4, 5)
their (I, OTHER, 5)
(I, OTHER, JADJ, 5, 6)
highest (I, JADJ, 6)
(I, JADJ, NOUN, 6,7)
level (I, NOUN, 7) (I, 5, 7)
(I, O, 7, 8)
in (O, OTHER, 8) (O, 8, 8)
(O, I, 8, 9)
eight (I, OTHER, 9)
(I, OTHER, NOUN, 9,10)
years (I, NOUN, 10) (I, 9, 10)
(I, O, 10, 11)
. (O, OTHER, 11) (O, 11, 11)
</equation>
<tableCaption confidence="0.836965">
Table 2: Example Parts
</tableCaption>
<bodyText confidence="0.99991375">
respectively). This part is called a transition. If a
chunk tagged C extends from the token at q to the
token at r inclusive, the second layer includes part
(C, q, r). This part is a chunk node. And if the token
at q −1 is the last token in a chunk tagged C0, while
the token at q is the first token of a chunk tagged C,
the second layer includes part (C0, C, q−1, q). This
part is a chunk transition.
In this paper we use the common method of fac-
toring the score of the label-chunk structure as the
sum of the scores of all the parts. Each part in a
label-chunk structure can be lexicalized, and gives
rise to several features. For each feature, we have a
corresponding weight. If we sum up the weights for
these features, we have the score for the part, and if
we sum up the scores of the parts, we have the score
for the label-chunk structure.
Suppose we would like to score a pair (xi, yi) in
the training set, and it happens to be the one shown
in Table 2. To begin, let’s say we would like to find
the features for the part (I, NOUN, 7) of POS node
type (1st Layer). This is the NOUN tag on the sev-
enth token “level” in Table 2. By default, the POS
node type generates the following binary feature.
</bodyText>
<listItem confidence="0.8775605">
• Is there a token labeled with “NOUN” in a
chunk labeled with “I”?
</listItem>
<page confidence="0.989344">
764
</page>
<bodyText confidence="0.864057125">
Now, to have more features, we can lexicalize POS
node type. Suppose we use xr to lexicalize POS
node (C, P, r), then we have the following binary
feature, as it is (I, NOUN, 7) and xi7 = “level”.
• Is there a token “level” labeled with “NOUN”
in a chunk labeled with “I”?
We can also use xr_1 and xr to lexicalize the parts
of POS node type.
</bodyText>
<listItem confidence="0.836485333333333">
• Is there a token “level” labeled with “NOUN”
in a chunk labeled with “I” that’s preceded by
“highest”?
</listItem>
<bodyText confidence="0.999939210526316">
This way, we have a complete specification of the
feature set given the part type, lexicalization for each
part type and the training set. Let us define f a
boolean feature vector function such that each di-
mension of f(xi, yi) contains 1 if the pair (xi, yi)
has the feature, 0 otherwise. Now define a real-
valued weight vector w with the same dimension
as f. To represent the score of the pair (xi, yi), we
write s(xi, yi) = wTf(xi, yi) We could also have
wTf(xi, {pJ) where p just a single part, in which
case we just write s(p).
Assuming an appropriate feature representation
as well as a weight vector w, we would like to
find the highest scoring label-chunk structure y =
argmaxy′(wTf(x, y′)) given an input sentence x.
In the upcoming section, we present a decoding
algorithm for the label-chunk structures, and later
we give a method for learning the weight vector used
in the decoding.
</bodyText>
<sectionHeader confidence="0.994288" genericHeader="method">
3 Decoding
</sectionHeader>
<bodyText confidence="0.999047487804878">
The decoding algorithm is shown in Figure 1. The
idea is to use two tables for dynamic programming:
label table and chunk table.
Suppose we are examining the current position
r, and would like to consider extending the chunk
[q, r −1] to [q, r]. We need to know the chunk tag C
for [q, r − 1] and the last POS tag P0 at index r − 1.
The array entry label table[q][r − 1] keeps track of
this information.
Then we examine how the current chunk is con-
nected with the previous chunk. The array entry
chunk table[q][C0] keeps track of the score of the
best label-chunk structure from 0 up to the index q
that has the ending chunk tag C0. Now checking
the chunk transition from C0 to C at the index q is
simple, and we can record the score of this chunk to
chunk table[r][C], so that the next chunk starting at
r can use this information.
In short, we are executing two Viterbi algorithms
on the first and second layer at the same time. One
extends [q, r − 1] to [q, r], considering the node in-
dexed by r (first layer). The other extends [0, q] to
[0, r], considering the node indexed by [q, r] (sec-
ond layer). The dynamic programming table for the
first layer is kept in the label table (r − 1 and P0
are used in the Viterbi algorithm for this layer) and
that for the second layer in the chunk table (q and
C0 used). The algorithm returns the best score of
the label-chunk structure.
To recover the structure, we simply need to main-
tain back pointers to the items that gave rise to the
each item in the dynamic programming table. This
is just like maintaining back pointers in the Viterbi
algorithm for sequences, or the CKY algorithm for
parsing.
The pseudo-code shows that the run-time com-
plexity of the decoding algorithm is O(n2) unlike
that of CFG parsing, O(n3). Thus the algorithm per-
forms better on long sentences. On the other hand,
the constant is c2p2 where c is the number of chunk
tags and p is the number of POS tags.
</bodyText>
<sectionHeader confidence="0.998497" genericHeader="method">
4 Learning
</sectionHeader>
<subsectionHeader confidence="0.999113">
4.1 Voted Perceptron
</subsectionHeader>
<bodyText confidence="0.9939999375">
In the CKY and Viterbi decoders, we use the
forward-backward or inside-outside algorithm to
find the marginal probabilities. Since we don’t yet
have the inference algorithm to find the marginal
probabilities of the parts of a label-chunk structure,
we use an online learning algorithm to train the
model. Despite this restriction, the voted percep-
tron is known for its performance (Sha and Pereira,
2003).
The voted perceptron we use is the adaptation of
(Freund and Schapire, 1999) to the structured set-
ting. Algorithm 4.1 shows the pseudo code for the
training, and the function update(wk, xi, yi, y′) re-
turns wk − f(xi, y′) + f(xi, yi) .
Given a training set {(xiyi)JZ1 and the epoch
number T, Algorithm 4.1 will return a list of
</bodyText>
<page confidence="0.973994">
765
</page>
<construct confidence="0.650181">
Algorithm 3.1: DECODE(the scoring function s(p))
</construct>
<equation confidence="0.9853835">
score := 0;
for q := index start to index end
for length := 1 to index end − q
r := q + length;
for each Chunk Tag C
for each Chunk Tag C0
for each POS Tag P
for each POS Tag P0
score := 0;
if (length &gt; 1)
</equation>
<bodyText confidence="0.411648">
#Add the score of the transition from r-2 to r-1. (1st Layer, POS)
</bodyText>
<figure confidence="0.718225333333333">
score := score + s((C, P0, P, r − 2, r − 1)) + label table[q][r − 1][C][P0];
#Add the score of the node at r-1. (1st Layer, POS)
score := score + s((C, P, r − 1));
if (score &gt;= label table[q][r][C][P])
label table[q][r][C][P] := score;
#Add the score of the chunk node at [q,r-1]. (2nd Layer, NP)
score := score + s((C, q, r − 1));
if (index start &lt; q)
#Add the score of the chunk transition from q-1 to q. (2nd Layer, NP)
score := score + s((C0, C, q − 1, q)) + chunk table[q][C0];
if (score &gt;= chunk table[r][C])
chunk table[r][C] := score;
</figure>
<tableCaption confidence="0.6377875">
end for
end for
end for
end for
end for
end for
</tableCaption>
<equation confidence="0.903163857142857">
score := 0;
for each C in chunk tags
if (chunk table[index end][C] &gt;= score)
score := chunk table[index end][C];
last symbol := C;
end for
return (score)
</equation>
<bodyText confidence="0.9587145">
Note: Since the scoring function s(p) is defined as w⊤f(xi, {p}), the input sequence xi and the weight
vector w are also the inputs to the algorithm.
</bodyText>
<figureCaption confidence="0.998107">
Figure 1: Decoding Algorithm
</figureCaption>
<page confidence="0.970562">
766
</page>
<bodyText confidence="0.818734">
weighted perceptrons {(w1, c1), ..(wk, ck)1. The fi-
nal model V uses the weight vector
</bodyText>
<equation confidence="0.998811470588235">
w = Pk j�1(cjwj)
Tn
(Collins, 2002).
Algorithm 4.1: TRAIN(T, {(xi, yi)}n i=1)
k := 0;
w1 := 0;
c1 := 0;
fort := 1 to T
for i := 1 to n
y′ := argmaxy(wk f(y, xi))
if (y′ = yi)
ck := ck + 1;
else
wk+1 := update(wk, xi, yi, y′);
ck+1 := 1;
k := k + 1;
ck := ck + 1;
</equation>
<tableCaption confidence="0.5157095">
end for
end for
</tableCaption>
<construct confidence="0.855771666666667">
return ({(w1, c1), --(wk, ck)})
Algorithm 4.2: UPDATE1(wk, xi, yi, y′)
return (wk − f(xi, y′) + f(xi, yi))
Algorithm 4.3: UPDATE2(wk, xi, yi, y′)
6 = max(0, min (�:(V&apos;llfi(yi)-fi(y′)11.2,y′) 1))
return (wk − 6f(xi, y′) + 6f(xi, yz))
</construct>
<subsectionHeader confidence="0.810901">
4.2 Max Margin
4.2.1 Sequential Minimum Optimization
</subsectionHeader>
<bodyText confidence="0.999">
A max margin method minimizes the regularized
empirical risk function with the hard (penalized)
margin
</bodyText>
<equation confidence="0.8485905">
(s(xi, yi)−max(s(xi, y)−li(y)))
y
</equation>
<bodyText confidence="0.99946425">
li finds the loss for y with respect to yi, and it is as-
sumed that the function is decomposable just as y is
decomposable to the parts. This equation is equiva-
lent to
</bodyText>
<equation confidence="0.6647355">
minx, 1211w112 + C Piξi
vi, y, s(xi, yi) + ξi ? s(xi, y) − li(y)
</equation>
<bodyText confidence="0.943464">
After taking the Lagrange dual formation, we have
</bodyText>
<equation confidence="0.962157142857143">
�i(y)(f(xi, yi) − f(xi, y))k2 + X ai(y)li(y)
i,y
such that X ai(y) = C
y
and
Xw = ai(y)(f(xi, yi) − f(xi, y)) (1)
i,y
</equation>
<bodyText confidence="0.99988875">
This quadratic program can be optimized by bi-
coordinate descent, known as Sequential Minimum
Optimization. Given an example i and two label-
chunk structures y′ and y′′,
</bodyText>
<equation confidence="0.999513166666667">
li(y′) − li(y′′) − (s(xi, y′′) − s(xi, y′))
d = (2)
11fi(y′′) − fi(y′)112
δ = max(−αi(y′), min(d, αi(y′′))
The updated values are : αi(y′) := αi(y′) + δ and
αi(y′′) := αi(y′′) − δ.
</equation>
<bodyText confidence="0.999821">
Using the equation (1), any increase in α can be
translated to w. For a naive SMO, this update is
executed for each training sample i, for all pairs of
possible parses y′ and y′′ for xi. See (Taskar and
Klein, 2005; Zhang, 2001; Jaakkola et al, 2000).
Here is where we differ from (Taskar et al, 2004).
We choose y′′ to be the correct parse yi, and y′
to be the best runner-up. After setting the ini-
tial weights using yi, we also set αi(yi) = 1 and
αi(y′) = 0. Although these alphas are not correct,
as optimization nears the end, the margin is wider;
αi(yi) and αi(y′) gets closer to 1 and 0 respec-
tively. Given this approximation, we can compute δ.
Then, the function update(wk, xi, yi, y′) will return
wk −δf(xi, y′)+δf(xi, yi) and we have reduced the
SMO to the perceptron weight update.
</bodyText>
<subsubsectionHeader confidence="0.620458">
4.2.2 Margin Infused Relaxed Algorithm
</subsubsectionHeader>
<bodyText confidence="0.9996647">
We can think of maximizing the margin in terms
of extending the Margin Infused Relaxed Algorithm
(MIRA) (Crammer and Singer, 2003; Crammer et
al, 2003) to learning with structured outputs. (Mc-
Donald et al, 2005) presents this approach for de-
pendency parsing.
In particuler, Single-best MIRA (McDonald et
al, 2005) uses only the single margin constraint for
the runner up y′ with the highest score. The result-
ing online update would be wk+1 with the following
</bodyText>
<equation confidence="0.960490214285714">
1 X
211w112−
i
min
W
X
1�k
i,y
−
max
α&gt;0
767
condition: minkWk+1 − Wkk such that s(xi, yi) −
s(xi, y′) ≥ li(y′) where y′ = argmaxys(xi, y).
</equation>
<bodyText confidence="0.990798">
Incidentally, the equation (2) for d above when
ai(yi) = 1 and ai(y′) = 0 solves this minimization
problem as well, and the weight update is the same
as the SMO case.
</bodyText>
<subsectionHeader confidence="0.833405">
4.2.3 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.9997745">
Instead of minimizing the regularized empirical
risk function with the hard (penalized) margin, con-
ditional random fields try to minimize the same with
the negative log loss:
</bodyText>
<equation confidence="0.972004">
1��kWk2 −
i
</equation>
<bodyText confidence="0.999897444444445">
Usually, CRFs use marginal probabilities of parts to
do the optimization. Since we have not yet come
up with the algorithm to compute marginals for a
label-chunk structure, the training methods for CRFs
is not applicable to our purpose. However, on se-
quence labeling tasks CRFs have shown very good
performance (Lafferty et al, 2001; Sha and Pereira,
2003), and we will use them for the baseline com-
parison.
</bodyText>
<sectionHeader confidence="0.999792" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996384">
5.1 Task: Base Noun Phrase Chunking
</subsectionHeader>
<bodyText confidence="0.9999747">
The data for the training and evaluation comes from
the CoNLL 2000 shared task (Tjong Kim Sang and
Buchholz, 2000), which is a portion of the Wall
Street Journal.
We consider each sentence to be a training in-
stance xi, with single words as tokens.
The shared task data have a standard training set
of 8936 sentences and a test set of 2012 sentences.
For the training, we used the first 447 sentences from
the standard training set, and our evaluation was
done on the standard test set of the 2012 sentences.
Let us define the set D to be the first 447 samples
from the standard training set.
There are 45 different POS labels, and the three
NP labels: begin-phrase, inside-phrase, and other.
(Ramshaw and Marcus, 1995) To reduce the infer-
ence time, following (McCallum et al, 2003), we
collapsed the 45 different POS labels contained in
the original data. The rules for collapsing the POS
labels are listed in the Table 3.
</bodyText>
<subsectionHeader confidence="0.635571">
Original Collapsed
</subsectionHeader>
<bodyText confidence="0.9464966">
all different types of nouns NOUN
all different types of verbs VERB
all different types of adjectives JADJ
all different types of adverbs RBP
the remaining POS labels OTHER
</bodyText>
<tableCaption confidence="0.934577">
Table 3: Rules for collapsing POS tags
</tableCaption>
<table confidence="0.999914846153846">
Token POS Collapsed Chunk NP
U.K. JJ JADJ B-NP B
base NN NOUN I-NP I
rates NNS NOUN I-NP I
are VBP VERB B-VP O
at IN OTHER B-PP O
their PRP$ OTHER B-NP B
highest JJS JADJ I-NP I
level NN NOUN I-NP I
in IN OTHER B-PP O
eight CD OTHER B-NP B
years NNS NOUN I-NP I
. . OTHER O O
</table>
<tableCaption confidence="0.993392">
Table 4: Example with POS and NP labels, before
</tableCaption>
<bodyText confidence="0.985657888888889">
and after collapsing the labels.
We present two experiments: one comparing
our label-chunk model with a cascaded linear-chain
model and a simple linear-chain model, and one
comparing different learning algorithms.
The cascaded linear-chain model uses one linear-
chain model to predict POS tags, and another linear-
chain model to predict NP labels, using the POS tags
predicted by the first model as a feature.
More specifically, we trained a POS-tagger using
the training set D. We then used the learned model
and replaced the POS labels of the test set with the
labels predicted by the learned model. The linear-
chain NP chunker was again trained on D and eval-
uated on this new test set with POS supplied by the
earlier processing. Note that the new test set has ex-
actly the same word tokens and noun chunks as the
original test set.
</bodyText>
<subsectionHeader confidence="0.990692">
5.2 Systems
5.2.1 POS Tagger and NP Chunker
</subsectionHeader>
<bodyText confidence="0.999980142857143">
There are three versions of POS taggers and NP
chunkers: CRF, VP, MMVP. For CRF, L-BFGS,
a quasi-Newton optimization method was used for
the training, and the implementation we used is
CRF++ (Kudo, 2005). VP uses voted perceptron,
and MMVP uses max margin update for the voted
perceptron. For the voted perceptron, we used aver-
</bodyText>
<equation confidence="0.58894825">
min
W
(s(xi, yi) − log( � s(xi, y)))
y
</equation>
<page confidence="0.737148">
768
</page>
<figure confidence="0.746767166666667">
if xq matches then tq is
[A-Z][a-z]+ CAPITAL
[A-Z] CAP ONE
[A-Z]+ CAP ALL
[A-Z]+[a-z]+[A-Z]+[a-z] CAP MIX
.*[0-9].* NUMBER
</figure>
<tableCaption confidence="0.758962">
Table 5: Rules to create tQ for each token xQ
</tableCaption>
<figure confidence="0.926354166666667">
First Layer (POS)
Node (C, P, r) Trans. (C, P0, P, r − 1, r)
xr−1 xr−1
xr xr
xr+1
tr
Second Layer (NP)
Node (C, q, r) Trans. (C0, C, q − 1, q)
xq xq−1
xq−1 xq
xr
xr+1
</figure>
<tableCaption confidence="0.94719">
Table 6: Lexicalized Features for Joint Models
</tableCaption>
<bodyText confidence="0.999712666666667">
aging of the weights suggested by (Collins, 2002).
The features are exactly the same for all three sys-
tems.
</bodyText>
<subsubsectionHeader confidence="0.552527">
5.2.2 Cascaded Models
</subsubsectionHeader>
<bodyText confidence="0.9980844">
For each CRF, VP, MMVP, the output of a POS
tagger was used as a feature for the NP chunker.
The feeds always consist of a POS tagger and NP
chunker of the same kind, thus we have CRF+CRF,
VP+VP, and MMVP+MMVP.
</bodyText>
<subsubsectionHeader confidence="0.56416">
5.2.3 Joint Models
</subsubsectionHeader>
<bodyText confidence="0.9999744">
Since CRF requires the computation of marginals
for each part, we were not able to use the learning
method. VP and MMVP were used to train the label-
chunk structures with the features explained in the
following section.
</bodyText>
<subsectionHeader confidence="0.831992">
5.3 Features
</subsectionHeader>
<bodyText confidence="0.9999206">
First, as a preprocessing step, for each word token
xQ, feature tQ was created with the rule in Table 5,
and included in the input files. This feature is in-
cluded in x along with the word tokens. The feature
tells us whether the token is capitalized, and whether
digits occur in the token. No outside resources such
as a list of names or a gazetteer were used.
Table 6 shows the lexicalized features for the joint
labeling and chunking. For the first iteration of train-
ing, the weights for the lexicalized features were not
</bodyText>
<table confidence="0.998690571428572">
POS tagging POS NP F1
CRF 91.56% N/A N/A
VP 90.55% N/A N/A
MMVP 90.02% N/A N/A
NP chunking POS NP F1
CRF given 94.44% 87.52%
VP given 94.28% 86.96%
MMVP given 94.17% 86.79%
Both POS &amp; NP POS NP F1
CRF + CRF above 90.16% 79.08%
VP + VP above 89.21% 76.26%
MMVP + MMVP above 88.95% 75.28%
VP Joint 88.42% 90.60% 79.69%
MMVP Joint 88.69% 90.84% 80.34%
</table>
<tableCaption confidence="0.999566">
Table 7: Performance
</tableCaption>
<bodyText confidence="0.999357">
updated. The intention is to have more weights on
the unlexicalized features, so that when lexical fea-
ture is not found, unlexicalized features could pro-
vide useful information and avoid overfitting, much
as back-off probabilities do.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="method">
6 Result
</sectionHeader>
<bodyText confidence="0.999990952380952">
We evaluated the performance of the systems using
three measures: POS accuracy, NP accuracy, and F1
measure on NP. These figures show how errors ac-
cumulate as the systems are chained together. For
the statistical significance testing, we have used pair-
samples t test, and for the joint labeling and chunk-
ing task, everything was found to be statistically sig-
nificant except for CRF + CRF vs VP Joint.
One can see that the systems with joint label-
ing and chunking models perform much better than
the cascaded models. Surprisingly, the perceptron
update motivated by the max margin principle per-
formed significantly worse than the simple percep-
tron update for linear-chain models but performed
better on joint labeling and chunking.
Although joint labeling and chunking model takes
longer time per sample because of the time complex-
ity of decoding, the number of iteration needed to
achieve the best result is very low compared to other
systems. The CPU time required to run 10 iterations
of MMVP is 112 minutes.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999785">
We have presented the decoding algorithm for label-
chunk structure and showed its effectiveness in find-
ing two layers of information, POS tags and NP
chunks. This algorithm has a place between the
</bodyText>
<page confidence="0.992978">
769
</page>
<table confidence="0.997821818181818">
POS tagging Iterations
VP 30
MMVP 40
CRF 126
NP chunking Iterations
VP 70
MMVP 50
CRF 101
Both POS &amp; NP Iterations
VP 10
MMVP 10
</table>
<tableCaption confidence="0.999884">
Table 8: Iterations needed for the result
</tableCaption>
<bodyText confidence="0.998870272727273">
Viterbi algorithm for linear-chain models and the
CKY algorithm for parsing, and the time complex-
ity is O(n2). The use of our label-chunk structure
significantly boosted the performance over cascaded
CRFs despite the online learning algorithms used to
train the system, and shows itself as a promising al-
ternative to cascaded models, and possibly dynamic
conditional random fields for modeling two layers of
tags. Further work includes applying the algorithm
to relation extraction, and devising an effective algo-
rithm to find the marginal probabilities of parts.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997433125">
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. of Empirical Methods
in Natural Language Processing (EMNLP)
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. Journal of
Machine Learning Research
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2003. Online passive aggressive algorithms. In Ad-
vances in Neural Information Processing Systems 15
K. Crammer, R. McDonald, and F. Pereira. 2004. New
large margin algorithms for structured prediction. In
Learning with Structured Outputs Workshop (NIPS)
Y. Freund and R. Schapire 1999. Large Margin Classi-
fication using the Perceptron Algorithm. In Machine
Learning, 37(3):277-296.
T.S. Jaakkola, M. Diekhans, and D. Haussler. 2000. A
discriminative framework for detecting remote protein
homologies. Journal of Computational Biology
T. Kudo 2005. CRF++: Yet Another CRF toolkit. Avail-
able at http://chasen.org/˜taku/software/CRF++/
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of the
18th International Conference on Machine Learning
(ICML)
F. Peng and A. McCallum. 2004. Accurate Informa-
tion Extraction from Research Papers using Condi-
tional Random Fields. In Proc. of the Human Lan-
guage Technology Conf. (HLT)
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. of the Human Language
Technology Conf. (HLT)
C. Manning and H. Schutze. 1999. Foundations of Sta-
tistical Natural Language Processing MIT Press.
A. McCallum, K. Rohanimanesh and C. Sutton. 2003.
Dynamic Conditional Random Fields for Jointly La-
beling Multiple Sequences. In Proc. of Workshop on
Syntax, Semantics, Statistics. (NIPS)
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of the 43rd Annual Meeting of the ACL
L. Ramshaw and M. Marcus. 1995. Text chunking us-
ing transformation-based learning. In Proc. of Third
Workshop on Very Large Corpora. ACL
C. Sutton, K. Rohanimanesh and A. McCallum. 2004.
Dynamic Conditional Random Fields: Factorized
Probabilistic Models for Labeling and Segmenting Se-
quence Data. In Proc. of the 21st International Con-
ference on Machine Learning (ICML)
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning 2004. Max Margin Parsing. In Proc. of
Empirical Methods in Natural Language Processing
(EMNLP)
B. Taskar and D. Klein. 2005. Max-Margin Methods for
NLP: Estimation, Structure, and Applications Avail-
able at http://www.cs.berkeley.edu/˜taskar/pubs/max-
margin-acl05-tutorial.pdf
E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In
Proc. of the 4th Conf. on Computational Natural Lan-
guage Learning (CoNLL)
T. Zhang. 2001. Regularized winnow methods. In Ad-
vances in Neural Information Processing Systems 13
</reference>
<page confidence="0.997022">
770
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.702477">
<title confidence="0.999781">Exact Decoding for Jointly Labeling and Chunking Sequences</title>
<author confidence="0.998948">Nobuyuki Shimizu</author>
<affiliation confidence="0.999726">Department of Computer Science State University of New York at Albany</affiliation>
<address confidence="0.999851">Albany, NY 12222, USA</address>
<email confidence="0.951017">nobuyuki@shimizu.name</email>
<author confidence="0.999717">Andrew Haas</author>
<affiliation confidence="0.999748">Department of Computer Science State University of New York at Albany</affiliation>
<address confidence="0.99404">Albany, NY 12222 USA</address>
<email confidence="0.999782">haas@cs.albany.edu</email>
<abstract confidence="0.985658555555556">There are two decoding algorithms essential to the area of natural language processing. One is the Viterbi algorithm for linear-chain models, such as HMMs or CRFs. The other is the CKY algorithm for probabilistic context free grammars. However, tasks such as noun phrase chunking and relation extraction seem to fall between the two, neither of them being the best fit. Ideally we would like to model entities and relations, with two layers of labels. We present a tractable algorithm for exact inference over two layers of labels and chunks with time complexity and provide empirical results comparing our model with linear-chain models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of Empirical Methods in Natural Language Processing (EMNLP)</booktitle>
<contexts>
<context position="12183" citStr="Collins, 2002" startWordPosition="2273" endWordPosition="2274">re + s((C0, C, q − 1, q)) + chunk table[q][C0]; if (score &gt;= chunk table[r][C]) chunk table[r][C] := score; end for end for end for end for end for end for score := 0; for each C in chunk tags if (chunk table[index end][C] &gt;= score) score := chunk table[index end][C]; last symbol := C; end for return (score) Note: Since the scoring function s(p) is defined as w⊤f(xi, {p}), the input sequence xi and the weight vector w are also the inputs to the algorithm. Figure 1: Decoding Algorithm 766 weighted perceptrons {(w1, c1), ..(wk, ck)1. The final model V uses the weight vector w = Pk j�1(cjwj) Tn (Collins, 2002). Algorithm 4.1: TRAIN(T, {(xi, yi)}n i=1) k := 0; w1 := 0; c1 := 0; fort := 1 to T for i := 1 to n y′ := argmaxy(wk f(y, xi)) if (y′ = yi) ck := ck + 1; else wk+1 := update(wk, xi, yi, y′); ck+1 := 1; k := k + 1; ck := ck + 1; end for end for return ({(w1, c1), --(wk, ck)}) Algorithm 4.2: UPDATE1(wk, xi, yi, y′) return (wk − f(xi, y′) + f(xi, yi)) Algorithm 4.3: UPDATE2(wk, xi, yi, y′) 6 = max(0, min (�:(V&apos;llfi(yi)-fi(y′)11.2,y′) 1)) return (wk − 6f(xi, y′) + 6f(xi, yz)) 4.2 Max Margin 4.2.1 Sequential Minimum Optimization A max margin method minimizes the regularized empirical risk function </context>
<context position="19054" citStr="Collins, 2002" startWordPosition="3532" endWordPosition="3533">voted perceptron, and MMVP uses max margin update for the voted perceptron. For the voted perceptron, we used avermin W (s(xi, yi) − log( � s(xi, y))) y 768 if xq matches then tq is [A-Z][a-z]+ CAPITAL [A-Z] CAP ONE [A-Z]+ CAP ALL [A-Z]+[a-z]+[A-Z]+[a-z] CAP MIX .*[0, 1, 2, 3, 4, 5, 6, 7, 8, 9].* NUMBER Table 5: Rules to create tQ for each token xQ First Layer (POS) Node (C, P, r) Trans. (C, P0, P, r − 1, r) xr−1 xr−1 xr xr xr+1 tr Second Layer (NP) Node (C, q, r) Trans. (C0, C, q − 1, q) xq xq−1 xq−1 xq xr xr+1 Table 6: Lexicalized Features for Joint Models aging of the weights suggested by (Collins, 2002). The features are exactly the same for all three systems. 5.2.2 Cascaded Models For each CRF, VP, MMVP, the output of a POS tagger was used as a feature for the NP chunker. The feeds always consist of a POS tagger and NP chunker of the same kind, thus we have CRF+CRF, VP+VP, and MMVP+MMVP. 5.2.3 Joint Models Since CRF requires the computation of marginals for each part, we were not able to use the learning method. VP and MMVP were used to train the labelchunk structures with the features explained in the following section. 5.3 Features First, as a preprocessing step, for each word token xQ, f</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. of Empirical Methods in Natural Language Processing (EMNLP)</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<contexts>
<context position="14559" citStr="Crammer and Singer, 2003" startWordPosition="2722" endWordPosition="2725">arse yi, and y′ to be the best runner-up. After setting the initial weights using yi, we also set αi(yi) = 1 and αi(y′) = 0. Although these alphas are not correct, as optimization nears the end, the margin is wider; αi(yi) and αi(y′) gets closer to 1 and 0 respectively. Given this approximation, we can compute δ. Then, the function update(wk, xi, yi, y′) will return wk −δf(xi, y′)+δf(xi, yi) and we have reduced the SMO to the perceptron weight update. 4.2.2 Margin Infused Relaxed Algorithm We can think of maximizing the margin in terms of extending the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al, 2003) to learning with structured outputs. (McDonald et al, 2005) presents this approach for dependency parsing. In particuler, Single-best MIRA (McDonald et al, 2005) uses only the single margin constraint for the runner up y′ with the highest score. The resulting online update would be wk+1 with the following 1 X 211w112− i min W X 1�k i,y − max α&gt;0 767 condition: minkWk+1 − Wkk such that s(xi, yi) − s(xi, y′) ≥ li(y′) where y′ = argmaxys(xi, y). Incidentally, the equation (2) for d above when ai(yi) = 1 and ai(y′) = 0 solves this minimization problem as well, and the weight</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive aggressive algorithms.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15</booktitle>
<contexts>
<context position="14581" citStr="Crammer et al, 2003" startWordPosition="2726" endWordPosition="2729">best runner-up. After setting the initial weights using yi, we also set αi(yi) = 1 and αi(y′) = 0. Although these alphas are not correct, as optimization nears the end, the margin is wider; αi(yi) and αi(y′) gets closer to 1 and 0 respectively. Given this approximation, we can compute δ. Then, the function update(wk, xi, yi, y′) will return wk −δf(xi, y′)+δf(xi, yi) and we have reduced the SMO to the perceptron weight update. 4.2.2 Margin Infused Relaxed Algorithm We can think of maximizing the margin in terms of extending the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al, 2003) to learning with structured outputs. (McDonald et al, 2005) presents this approach for dependency parsing. In particuler, Single-best MIRA (McDonald et al, 2005) uses only the single margin constraint for the runner up y′ with the highest score. The resulting online update would be wk+1 with the following 1 X 211w112− i min W X 1�k i,y − max α&gt;0 767 condition: minkWk+1 − Wkk such that s(xi, yi) − s(xi, y′) ≥ li(y′) where y′ = argmaxys(xi, y). Incidentally, the equation (2) for d above when ai(yi) = 1 and ai(y′) = 0 solves this minimization problem as well, and the weight update is the same as</context>
</contexts>
<marker>Crammer, Dekel, Shalev-Shwartz, Singer, 2003</marker>
<rawString>K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer. 2003. Online passive aggressive algorithms. In Advances in Neural Information Processing Systems 15</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>New large margin algorithms for structured prediction.</title>
<date>2004</date>
<booktitle>In Learning with Structured Outputs Workshop (NIPS)</booktitle>
<marker>Crammer, McDonald, Pereira, 2004</marker>
<rawString>K. Crammer, R. McDonald, and F. Pereira. 2004. New large margin algorithms for structured prediction. In Learning with Structured Outputs Workshop (NIPS)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>Large Margin Classification using the Perceptron Algorithm.</title>
<date>1999</date>
<booktitle>In Machine Learning,</booktitle>
<pages>37--3</pages>
<contexts>
<context position="10557" citStr="Freund and Schapire, 1999" startWordPosition="1948" endWordPosition="1951">s. On the other hand, the constant is c2p2 where c is the number of chunk tags and p is the number of POS tags. 4 Learning 4.1 Voted Perceptron In the CKY and Viterbi decoders, we use the forward-backward or inside-outside algorithm to find the marginal probabilities. Since we don’t yet have the inference algorithm to find the marginal probabilities of the parts of a label-chunk structure, we use an online learning algorithm to train the model. Despite this restriction, the voted perceptron is known for its performance (Sha and Pereira, 2003). The voted perceptron we use is the adaptation of (Freund and Schapire, 1999) to the structured setting. Algorithm 4.1 shows the pseudo code for the training, and the function update(wk, xi, yi, y′) returns wk − f(xi, y′) + f(xi, yi) . Given a training set {(xiyi)JZ1 and the epoch number T, Algorithm 4.1 will return a list of 765 Algorithm 3.1: DECODE(the scoring function s(p)) score := 0; for q := index start to index end for length := 1 to index end − q r := q + length; for each Chunk Tag C for each Chunk Tag C0 for each POS Tag P for each POS Tag P0 score := 0; if (length &gt; 1) #Add the score of the transition from r-2 to r-1. (1st Layer, POS) score := score + s((C, </context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. Schapire 1999. Large Margin Classification using the Perceptron Algorithm. In Machine Learning, 37(3):277-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T S Jaakkola</author>
<author>M Diekhans</author>
<author>D Haussler</author>
</authors>
<title>A discriminative framework for detecting remote protein homologies.</title>
<date>2000</date>
<journal>Journal of Computational Biology T. Kudo</journal>
<contexts>
<context position="13849" citStr="Jaakkola et al, 2000" startWordPosition="2595" endWordPosition="2598">) − f(xi, y)) (1) i,y This quadratic program can be optimized by bicoordinate descent, known as Sequential Minimum Optimization. Given an example i and two labelchunk structures y′ and y′′, li(y′) − li(y′′) − (s(xi, y′′) − s(xi, y′)) d = (2) 11fi(y′′) − fi(y′)112 δ = max(−αi(y′), min(d, αi(y′′)) The updated values are : αi(y′) := αi(y′) + δ and αi(y′′) := αi(y′′) − δ. Using the equation (1), any increase in α can be translated to w. For a naive SMO, this update is executed for each training sample i, for all pairs of possible parses y′ and y′′ for xi. See (Taskar and Klein, 2005; Zhang, 2001; Jaakkola et al, 2000). Here is where we differ from (Taskar et al, 2004). We choose y′′ to be the correct parse yi, and y′ to be the best runner-up. After setting the initial weights using yi, we also set αi(yi) = 1 and αi(y′) = 0. Although these alphas are not correct, as optimization nears the end, the margin is wider; αi(yi) and αi(y′) gets closer to 1 and 0 respectively. Given this approximation, we can compute δ. Then, the function update(wk, xi, yi, y′) will return wk −δf(xi, y′)+δf(xi, yi) and we have reduced the SMO to the perceptron weight update. 4.2.2 Margin Infused Relaxed Algorithm We can think of max</context>
</contexts>
<marker>Jaakkola, Diekhans, Haussler, 2000</marker>
<rawString>T.S. Jaakkola, M. Diekhans, and D. Haussler. 2000. A discriminative framework for detecting remote protein homologies. Journal of Computational Biology T. Kudo 2005. CRF++: Yet Another CRF toolkit. Available at http://chasen.org/˜taku/software/CRF++/</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proc. of the 18th International Conference on Machine Learning (ICML)</booktitle>
<contexts>
<context position="15747" citStr="Lafferty et al, 2001" startWordPosition="2929" endWordPosition="2932"> problem as well, and the weight update is the same as the SMO case. 4.2.3 Conditional Random Fields Instead of minimizing the regularized empirical risk function with the hard (penalized) margin, conditional random fields try to minimize the same with the negative log loss: 1��kWk2 − i Usually, CRFs use marginal probabilities of parts to do the optimization. Since we have not yet come up with the algorithm to compute marginals for a label-chunk structure, the training methods for CRFs is not applicable to our purpose. However, on sequence labeling tasks CRFs have shown very good performance (Lafferty et al, 2001; Sha and Pereira, 2003), and we will use them for the baseline comparison. 5 Experiments 5.1 Task: Base Noun Phrase Chunking The data for the training and evaluation comes from the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is a portion of the Wall Street Journal. We consider each sentence to be a training instance xi, with single words as tokens. The shared task data have a standard training set of 8936 sentences and a test set of 2012 sentences. For the training, we used the first 447 sentences from the standard training set, and our evaluation was done on the standar</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proc. of the 18th International Conference on Machine Learning (ICML)</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>A McCallum</author>
</authors>
<title>Accurate Information Extraction from Research Papers using Conditional Random Fields.</title>
<date>2004</date>
<booktitle>In Proc. of the Human Language Technology Conf.</booktitle>
<publisher>(HLT)</publisher>
<marker>Peng, McCallum, 2004</marker>
<rawString>F. Peng and A. McCallum. 2004. Accurate Information Extraction from Research Papers using Conditional Random Fields. In Proc. of the Human Language Technology Conf. (HLT)</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. of the Human Language Technology Conf.</booktitle>
<publisher>(HLT)</publisher>
<contexts>
<context position="10479" citStr="Sha and Pereira, 2003" startWordPosition="1935" endWordPosition="1938">of CFG parsing, O(n3). Thus the algorithm performs better on long sentences. On the other hand, the constant is c2p2 where c is the number of chunk tags and p is the number of POS tags. 4 Learning 4.1 Voted Perceptron In the CKY and Viterbi decoders, we use the forward-backward or inside-outside algorithm to find the marginal probabilities. Since we don’t yet have the inference algorithm to find the marginal probabilities of the parts of a label-chunk structure, we use an online learning algorithm to train the model. Despite this restriction, the voted perceptron is known for its performance (Sha and Pereira, 2003). The voted perceptron we use is the adaptation of (Freund and Schapire, 1999) to the structured setting. Algorithm 4.1 shows the pseudo code for the training, and the function update(wk, xi, yi, y′) returns wk − f(xi, y′) + f(xi, yi) . Given a training set {(xiyi)JZ1 and the epoch number T, Algorithm 4.1 will return a list of 765 Algorithm 3.1: DECODE(the scoring function s(p)) score := 0; for q := index start to index end for length := 1 to index end − q r := q + length; for each Chunk Tag C for each Chunk Tag C0 for each POS Tag P for each POS Tag P0 score := 0; if (length &gt; 1) #Add the sco</context>
<context position="15771" citStr="Sha and Pereira, 2003" startWordPosition="2933" endWordPosition="2936">the weight update is the same as the SMO case. 4.2.3 Conditional Random Fields Instead of minimizing the regularized empirical risk function with the hard (penalized) margin, conditional random fields try to minimize the same with the negative log loss: 1��kWk2 − i Usually, CRFs use marginal probabilities of parts to do the optimization. Since we have not yet come up with the algorithm to compute marginals for a label-chunk structure, the training methods for CRFs is not applicable to our purpose. However, on sequence labeling tasks CRFs have shown very good performance (Lafferty et al, 2001; Sha and Pereira, 2003), and we will use them for the baseline comparison. 5 Experiments 5.1 Task: Base Noun Phrase Chunking The data for the training and evaluation comes from the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is a portion of the Wall Street Journal. We consider each sentence to be a training instance xi, with single words as tokens. The shared task data have a standard training set of 8936 sentences and a test set of 2012 sentences. For the training, we used the first 447 sentences from the standard training set, and our evaluation was done on the standard test set of the 2012 s</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. In Proc. of the Human Language Technology Conf. (HLT)</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schutze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1304" citStr="Manning and Schutze, 1999" startWordPosition="213" endWordPosition="216">he best fit. Ideally we would like to model entities and relations, with two layers of labels. We present a tractable algorithm for exact inference over two layers of labels and chunks with time complexity O(n2), and provide empirical results comparing our model with linear-chain models. 1 Introduction The Viterbi algorithm and the CKY algorithms are two decoding algorithms essential to the area of natural language processing. The former models a linear chain of labels such as part of speech tags, and the latter models a parse tree. Both are used to extract the best prediction from the model (Manning and Schutze, 1999). However, some tasks seem to fall between the two, having more than one layer but flatter than the trees created by parsers. For example, in relation extraction, we have entities in one layer and relations between entities as another layer. Another task is shallow parsing. We may want to model part-ofspeech tags and noun/verb chunks at the same time, since performing simultaneous labeling may result in increased joint accuracy by sharing information between the two layers of labels. To apply the Viterbi decoder to such tasks, we need two models, one for each layer. We must feed the output of </context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>C. Manning and H. Schutze. 1999. Foundations of Statistical Natural Language Processing MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Rohanimanesh</author>
<author>C Sutton</author>
</authors>
<title>Dynamic Conditional Random Fields for Jointly Labeling Multiple Sequences.</title>
<date>2003</date>
<booktitle>In Proc. of Workshop on Syntax, Semantics,</booktitle>
<location>Statistics. (NIPS)</location>
<contexts>
<context position="2324" citStr="McCallum et al, 2003" startWordPosition="386" endWordPosition="389">ult in increased joint accuracy by sharing information between the two layers of labels. To apply the Viterbi decoder to such tasks, we need two models, one for each layer. We must feed the output of one layer to the next layer. In such an approach, errors in earlier processing nearly always accumulate and produce erroneous results at the end. If we use CKY, we usually end up flattening the output tree to obtain the desired output. This seems like a round-about way of modeling two layers. There are previous attempts at modeling two layer labeling. Dynamic Conditional Random Fields (DCRFs) by (McCallum et al, 2003; Sutton et al, 2004) is one such attempt, however, exact inference is in general intractable for these models and the authors were forced to settle for approximate inference. Our contribution is a novel model for two layer labeling, for which exact decoding is tractable. Our experiments show that our use of label-chunk structures results in significantly better performance over cascaded CRFs, and that the model is a promising alternative to DCRFs. The paper is organaized a follows: In Section 2 and 3, we describe the model and present the decoding algorithm. Section 4 describes the learning m</context>
<context position="3924" citStr="McCallum et al, 2003" startWordPosition="671" endWordPosition="674">vel NOUN I in OTHER O eight OTHER B years NOUN I . OTHER O Table 1: Example with POS and NP tags 2 Model for Joint Labeling and Chunking Consider the task of finding noun chunks. The noun chunk extends from the beginning of a noun phrase to the head noun, excluding postmodifiers (which are difficult to attach correctly). Table 1 shows a sentence labeled with POS tags and segmented into noun chunks. B marks the first word of a noun chunk, I the other words in a noun chunk, and O the words that are not in a noun chunk. Note that we collapsed the 45 different POS labels into 5 labels, following (McCallum et al, 2003). All different types of adjectives are labeled as JADJ. Each word carries two tags. Given the first layer, our aim is to present a model that can predict the second and third layers of tags at the same time. Assume we have n training samples, I(xi, yi)}ni�1, where xi is a sequence of input tokens and yi is a label-chunk structure for xi. In this example, the first column contains the tokens xi and the second and third columns together represent the label-chunk structures yi. We will present an efficient exact decoding for this structure. The label-chunk structure, shown in Table 2, is a repre</context>
<context position="16654" citStr="McCallum et al, 2003" startWordPosition="3090" endWordPosition="3093">l. We consider each sentence to be a training instance xi, with single words as tokens. The shared task data have a standard training set of 8936 sentences and a test set of 2012 sentences. For the training, we used the first 447 sentences from the standard training set, and our evaluation was done on the standard test set of the 2012 sentences. Let us define the set D to be the first 447 samples from the standard training set. There are 45 different POS labels, and the three NP labels: begin-phrase, inside-phrase, and other. (Ramshaw and Marcus, 1995) To reduce the inference time, following (McCallum et al, 2003), we collapsed the 45 different POS labels contained in the original data. The rules for collapsing the POS labels are listed in the Table 3. Original Collapsed all different types of nouns NOUN all different types of verbs VERB all different types of adjectives JADJ all different types of adverbs RBP the remaining POS labels OTHER Table 3: Rules for collapsing POS tags Token POS Collapsed Chunk NP U.K. JJ JADJ B-NP B base NN NOUN I-NP I rates NNS NOUN I-NP I are VBP VERB B-VP O at IN OTHER B-PP O their PRP$ OTHER B-NP B highest JJS JADJ I-NP I level NN NOUN I-NP I in IN OTHER B-PP O eight CD </context>
</contexts>
<marker>McCallum, Rohanimanesh, Sutton, 2003</marker>
<rawString>A. McCallum, K. Rohanimanesh and C. Sutton. 2003. Dynamic Conditional Random Fields for Jointly Labeling Multiple Sequences. In Proc. of Workshop on Syntax, Semantics, Statistics. (NIPS)</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the ACL</booktitle>
<contexts>
<context position="14641" citStr="McDonald et al, 2005" startWordPosition="2735" endWordPosition="2739"> we also set αi(yi) = 1 and αi(y′) = 0. Although these alphas are not correct, as optimization nears the end, the margin is wider; αi(yi) and αi(y′) gets closer to 1 and 0 respectively. Given this approximation, we can compute δ. Then, the function update(wk, xi, yi, y′) will return wk −δf(xi, y′)+δf(xi, yi) and we have reduced the SMO to the perceptron weight update. 4.2.2 Margin Infused Relaxed Algorithm We can think of maximizing the margin in terms of extending the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al, 2003) to learning with structured outputs. (McDonald et al, 2005) presents this approach for dependency parsing. In particuler, Single-best MIRA (McDonald et al, 2005) uses only the single margin constraint for the runner up y′ with the highest score. The resulting online update would be wk+1 with the following 1 X 211w112− i min W X 1�k i,y − max α&gt;0 767 condition: minkWk+1 − Wkk such that s(xi, yi) − s(xi, y′) ≥ li(y′) where y′ = argmaxys(xi, y). Incidentally, the equation (2) for d above when ai(yi) = 1 and ai(y′) = 0 solves this minimization problem as well, and the weight update is the same as the SMO case. 4.2.3 Conditional Random Fields Instead of mi</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proc. of the 43rd Annual Meeting of the ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proc. of Third Workshop on Very Large Corpora. ACL</booktitle>
<contexts>
<context position="16591" citStr="Ramshaw and Marcus, 1995" startWordPosition="3079" endWordPosition="3082">g and Buchholz, 2000), which is a portion of the Wall Street Journal. We consider each sentence to be a training instance xi, with single words as tokens. The shared task data have a standard training set of 8936 sentences and a test set of 2012 sentences. For the training, we used the first 447 sentences from the standard training set, and our evaluation was done on the standard test set of the 2012 sentences. Let us define the set D to be the first 447 samples from the standard training set. There are 45 different POS labels, and the three NP labels: begin-phrase, inside-phrase, and other. (Ramshaw and Marcus, 1995) To reduce the inference time, following (McCallum et al, 2003), we collapsed the 45 different POS labels contained in the original data. The rules for collapsing the POS labels are listed in the Table 3. Original Collapsed all different types of nouns NOUN all different types of verbs VERB all different types of adjectives JADJ all different types of adverbs RBP the remaining POS labels OTHER Table 3: Rules for collapsing POS tags Token POS Collapsed Chunk NP U.K. JJ JADJ B-NP B base NN NOUN I-NP I rates NNS NOUN I-NP I are VBP VERB B-VP O at IN OTHER B-PP O their PRP$ OTHER B-NP B highest JJ</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. Ramshaw and M. Marcus. 1995. Text chunking using transformation-based learning. In Proc. of Third Workshop on Very Large Corpora. ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>K Rohanimanesh</author>
<author>A McCallum</author>
</authors>
<title>Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data.</title>
<date>2004</date>
<booktitle>In Proc. of the 21st International Conference on Machine Learning (ICML)</booktitle>
<contexts>
<context position="2345" citStr="Sutton et al, 2004" startWordPosition="390" endWordPosition="393"> accuracy by sharing information between the two layers of labels. To apply the Viterbi decoder to such tasks, we need two models, one for each layer. We must feed the output of one layer to the next layer. In such an approach, errors in earlier processing nearly always accumulate and produce erroneous results at the end. If we use CKY, we usually end up flattening the output tree to obtain the desired output. This seems like a round-about way of modeling two layers. There are previous attempts at modeling two layer labeling. Dynamic Conditional Random Fields (DCRFs) by (McCallum et al, 2003; Sutton et al, 2004) is one such attempt, however, exact inference is in general intractable for these models and the authors were forced to settle for approximate inference. Our contribution is a novel model for two layer labeling, for which exact decoding is tractable. Our experiments show that our use of label-chunk structures results in significantly better performance over cascaded CRFs, and that the model is a promising alternative to DCRFs. The paper is organaized a follows: In Section 2 and 3, we describe the model and present the decoding algorithm. Section 4 describes the learning methods applicable to </context>
</contexts>
<marker>Sutton, Rohanimanesh, McCallum, 2004</marker>
<rawString>C. Sutton, K. Rohanimanesh and A. McCallum. 2004. Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data. In Proc. of the 21st International Conference on Machine Learning (ICML)</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C Manning</author>
</authors>
<title>Max Margin Parsing.</title>
<date>2004</date>
<booktitle>In Proc. of Empirical Methods in Natural Language Processing (EMNLP)</booktitle>
<contexts>
<context position="13900" citStr="Taskar et al, 2004" startWordPosition="2605" endWordPosition="2608">ptimized by bicoordinate descent, known as Sequential Minimum Optimization. Given an example i and two labelchunk structures y′ and y′′, li(y′) − li(y′′) − (s(xi, y′′) − s(xi, y′)) d = (2) 11fi(y′′) − fi(y′)112 δ = max(−αi(y′), min(d, αi(y′′)) The updated values are : αi(y′) := αi(y′) + δ and αi(y′′) := αi(y′′) − δ. Using the equation (1), any increase in α can be translated to w. For a naive SMO, this update is executed for each training sample i, for all pairs of possible parses y′ and y′′ for xi. See (Taskar and Klein, 2005; Zhang, 2001; Jaakkola et al, 2000). Here is where we differ from (Taskar et al, 2004). We choose y′′ to be the correct parse yi, and y′ to be the best runner-up. After setting the initial weights using yi, we also set αi(yi) = 1 and αi(y′) = 0. Although these alphas are not correct, as optimization nears the end, the margin is wider; αi(yi) and αi(y′) gets closer to 1 and 0 respectively. Given this approximation, we can compute δ. Then, the function update(wk, xi, yi, y′) will return wk −δf(xi, y′)+δf(xi, yi) and we have reduced the SMO to the perceptron weight update. 4.2.2 Margin Infused Relaxed Algorithm We can think of maximizing the margin in terms of extending the Margin</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning 2004. Max Margin Parsing. In Proc. of Empirical Methods in Natural Language Processing (EMNLP)</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<date>2005</date>
<booktitle>Max-Margin Methods for NLP: Estimation, Structure, and Applications Available at http://www.cs.berkeley.edu/˜taskar/pubs/maxmargin-acl05-tutorial.pdf</booktitle>
<contexts>
<context position="13813" citStr="Taskar and Klein, 2005" startWordPosition="2589" endWordPosition="2592">X ai(y) = C y and Xw = ai(y)(f(xi, yi) − f(xi, y)) (1) i,y This quadratic program can be optimized by bicoordinate descent, known as Sequential Minimum Optimization. Given an example i and two labelchunk structures y′ and y′′, li(y′) − li(y′′) − (s(xi, y′′) − s(xi, y′)) d = (2) 11fi(y′′) − fi(y′)112 δ = max(−αi(y′), min(d, αi(y′′)) The updated values are : αi(y′) := αi(y′) + δ and αi(y′′) := αi(y′′) − δ. Using the equation (1), any increase in α can be translated to w. For a naive SMO, this update is executed for each training sample i, for all pairs of possible parses y′ and y′′ for xi. See (Taskar and Klein, 2005; Zhang, 2001; Jaakkola et al, 2000). Here is where we differ from (Taskar et al, 2004). We choose y′′ to be the correct parse yi, and y′ to be the best runner-up. After setting the initial weights using yi, we also set αi(yi) = 1 and αi(y′) = 0. Although these alphas are not correct, as optimization nears the end, the margin is wider; αi(yi) and αi(y′) gets closer to 1 and 0 respectively. Given this approximation, we can compute δ. Then, the function update(wk, xi, yi, y′) will return wk −δf(xi, y′)+δf(xi, yi) and we have reduced the SMO to the perceptron weight update. 4.2.2 Margin Infused R</context>
</contexts>
<marker>Taskar, Klein, 2005</marker>
<rawString>B. Taskar and D. Klein. 2005. Max-Margin Methods for NLP: Estimation, Structure, and Applications Available at http://www.cs.berkeley.edu/˜taskar/pubs/maxmargin-acl05-tutorial.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proc. of the 4th Conf. on Computational Natural Language Learning (CoNLL)</booktitle>
<contexts>
<context position="15987" citStr="Sang and Buchholz, 2000" startWordPosition="2971" endWordPosition="2974">ze the same with the negative log loss: 1��kWk2 − i Usually, CRFs use marginal probabilities of parts to do the optimization. Since we have not yet come up with the algorithm to compute marginals for a label-chunk structure, the training methods for CRFs is not applicable to our purpose. However, on sequence labeling tasks CRFs have shown very good performance (Lafferty et al, 2001; Sha and Pereira, 2003), and we will use them for the baseline comparison. 5 Experiments 5.1 Task: Base Noun Phrase Chunking The data for the training and evaluation comes from the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is a portion of the Wall Street Journal. We consider each sentence to be a training instance xi, with single words as tokens. The shared task data have a standard training set of 8936 sentences and a test set of 2012 sentences. For the training, we used the first 447 sentences from the standard training set, and our evaluation was done on the standard test set of the 2012 sentences. Let us define the set D to be the first 447 samples from the standard training set. There are 45 different POS labels, and the three NP labels: begin-phrase, inside-phrase, and other. (Ramshaw and Marcus, 1</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proc. of the 4th Conf. on Computational Natural Language Learning (CoNLL)</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
</authors>
<title>Regularized winnow methods.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems 13</booktitle>
<contexts>
<context position="13826" citStr="Zhang, 2001" startWordPosition="2593" endWordPosition="2594">i(y)(f(xi, yi) − f(xi, y)) (1) i,y This quadratic program can be optimized by bicoordinate descent, known as Sequential Minimum Optimization. Given an example i and two labelchunk structures y′ and y′′, li(y′) − li(y′′) − (s(xi, y′′) − s(xi, y′)) d = (2) 11fi(y′′) − fi(y′)112 δ = max(−αi(y′), min(d, αi(y′′)) The updated values are : αi(y′) := αi(y′) + δ and αi(y′′) := αi(y′′) − δ. Using the equation (1), any increase in α can be translated to w. For a naive SMO, this update is executed for each training sample i, for all pairs of possible parses y′ and y′′ for xi. See (Taskar and Klein, 2005; Zhang, 2001; Jaakkola et al, 2000). Here is where we differ from (Taskar et al, 2004). We choose y′′ to be the correct parse yi, and y′ to be the best runner-up. After setting the initial weights using yi, we also set αi(yi) = 1 and αi(y′) = 0. Although these alphas are not correct, as optimization nears the end, the margin is wider; αi(yi) and αi(y′) gets closer to 1 and 0 respectively. Given this approximation, we can compute δ. Then, the function update(wk, xi, yi, y′) will return wk −δf(xi, y′)+δf(xi, yi) and we have reduced the SMO to the perceptron weight update. 4.2.2 Margin Infused Relaxed Algori</context>
</contexts>
<marker>Zhang, 2001</marker>
<rawString>T. Zhang. 2001. Regularized winnow methods. In Advances in Neural Information Processing Systems 13</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>