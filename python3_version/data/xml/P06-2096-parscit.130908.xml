<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000646">
<title confidence="0.9987215">
Adding Syntax to Dynamic Programming for Aligning Comparable Texts
for the Generation of Paraphrases
</title>
<author confidence="0.998177">
Siwei Shen&apos;, Dragomir R. Radev&apos;,&apos;, Agam Patel&apos;, G¨unes¸ Erkan&apos;
</author>
<affiliation confidence="0.998666">
Department of Electrical Engineering and Computer Science
School of Information
University of Michigan
</affiliation>
<address confidence="0.985814">
Ann Arbor, MI 48109
</address>
<email confidence="0.99584">
{shens, radev, agamrp, gerkan}@umich.edu
</email>
<sectionHeader confidence="0.993726" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999823576923077">
Multiple sequence alignment techniques
have recently gained popularity in the Nat-
ural Language community, especially for
tasks such as machine translation, text
generation, and paraphrase identification.
Prior work falls into two categories, de-
pending on the type of input used: (a)
parallel corpora (e.g., multiple translations
of the same text) or (b) comparable texts
(non-parallel but on the same topic). So
far, only techniques based on parallel texts
have successfully used syntactic informa-
tion to guide alignments. In this paper,
we describe an algorithm for incorporat-
ing syntactic features in the alignment pro-
cess for non-parallel texts with the goal of
generating novel paraphrases of existing
texts. Our method uses dynamic program-
ming with alignment decision based on
the local syntactic similarity between two
sentences. Our results show that syntac-
tic alignment outrivals syntax-free meth-
ods by 20% in both grammaticality and fi-
delity when computed over the novel sen-
tences generated by alignment-induced fi-
nite state automata.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998241075471698">
In real life, we often encounter comparable texts
such as news on the same events reported by dif-
ferent sources and papers on the same topic au-
thored by different people. It is useful to recog-
nize if one text cites another in cases like news
sharing among media agencies or citations in aca-
demic work. Applications of such recognition in-
clude machine translation, text generation, para-
phrase identification, and question answering, all
of which have recently drawn the attention of a
number of researchers in natural language pro-
cessing community.
Multiple sequence alignment (MSA) is the ba-
sis for accomplishing these tasks. Previous work
aligns a group of sentences into a compact word
lattice (Barzilay and Lee, 2003), a finite state au-
tomaton representation that can be used to iden-
tify commonality or variability among compara-
ble texts and generate paraphrases. Nevertheless,
this approach has a drawback of over-generating
ungrammatical sentences due to its “almost-free”
alignment. Pang et al. provide a remedy to this
problem by performing alignment on the Charniak
parse trees of the clustered sentences (Pang et al.,
2003). Although it is so far the most similar work
to ours, Pang’s solution assumes the input sen-
tences to be semantically equivalent. Two other
important references for string-based alignments
algorithms, mostly with applications in Biology,
are (Gusfield, 1997) and (Durbin et al., 1998).
In our approach, we work on comparable texts
(not necessarily equivalent in their semantic mean-
ings) as Barzilay and Lee did. However, we use lo-
cal syntactic similarity (as opposed to lexical simi-
larity) in doing the alignment on the raw sentences
instead of on their parse trees. Because of the se-
mantic discrepancies among the inputs, applying
syntactic features in the alignment has a larger im-
pact on the grammaticality and fidelity of the gen-
erated unseen sentences. While previous work po-
sitions the primary focus on the quality of para-
phrases and/or translations, we are more interested
in the relation between the use of syntactic fea-
tures and the correctness of the sentences being
generated, including those that are not paraphrases
of the original input. Figure 1 illustrates the dif-
ference between alignment based solely on lexi-
cal similarity and alignment with consideration of
syntactic features.
Ignoring syntax, the word “Milan” in both sen-
tences is aligned. But it would unfortunately gen-
erate an ungrammatical sentence “I went to Mi-
lan is beautiful”. Aligning according to syntac-
</bodyText>
<page confidence="0.958227">
747
</page>
<note confidence="0.9143035">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 747–754,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<figure confidence="0.99860334375">
Start
I
Start
I
Milan
went
I
Milan
went
I
Milan
went
to
to
went
is
Milan
to
is
Milan
beautiful
to
is is
beautiful
Milan
beautiful beautiful Accept
Accept
Accept
Milan
Accept
Accept
Accept
</figure>
<figureCaption confidence="0.979449">
Figure 1: Alignment on lexical similarity and alignment with syntactic features of the sentences “Milan
is beautiful” and “I went to Milan”.
</figureCaption>
<bodyText confidence="0.999589272727273">
tic features, on the other hand, would avoid this
improper alignment by detecting that the syntactic
feature values of the two “Milan” differ too much.
We shall explain syntactic features and their us-
ages later. In this small example, our syntax-based
alignment will align nothing (the bottom FSA in
Figure 1) since “Milan” is the only lexically com-
mon word in both sentences. For much larger
clusters in our experiments, we are able to pro-
duce a significant number of novel sentences from
our alignment with such tightened syntactic con-
ditions. Figure 2 shows one of the actual clusters
used in our work that has 18 unique sentences.
Two of the many automatically generated gram-
matical sentences are also shown.
Another piece of related work, (Quirk et al.,
2004), starts off with parallel inputs and uses
monolingual Statistical Machine Translation tech-
niques to align them and generate novel sentences.
In our work, the input text does not need to be
nearly as parallel.
The main contribution of this paper is a syntax-
based alignment technique for generating novel
paraphrases of sentences that describe a par-
ticular fact. Such techniques can be poten-
tially useful in multi-document summarizers such
as Newsblaster (http://newsblaster.cs.
columbia.edu) and NewsInEssence (http:
//www.newsinessence.com). Such sys-
tems are notorious for mostly reusing text from
existing news stories. We believe that allowing
them to use novel formulations of known facts will
make these systems much more successful.
</bodyText>
<sectionHeader confidence="0.999592" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999674676470588">
Our work is closest in spirit to the two papers that
inspired us (Barzilay and Lee, 2003) and (Pang
et al., 2003). Both of these papers describe how
multiple sequence alignment can be used for ex-
tracting paraphrases from clustered texts. Pang et
al. use as their input the multiple human English
translations of Chinese documents provided by the
LDC as part of the NIST machine translation eval-
uation. Their approach is to merge multiple parse
trees into a single finite state automaton in which
identical input subconstituents are merged while
alternatives are converted to parallel paths in the
output FSA. Barzilay and Lee, on the other hand,
make use of classic techniques in biological se-
quence analysis to identify paraphrases from com-
parable texts (news from different sources on the
same event).
In summary, Pang et al. use syntactic align-
ment of parallel texts while Barzilay and Lee
use comparable (not parallel) input but ignore
syntax. Our work differs from the two in that
we apply syntactic information on aligning com-
parable texts and that the syntactic clues we
use are drawn from Chunklink ilk.uvt.nl/
˜sabine/homepage/software.html out-
put, which is further analysis from the syntactic
parse trees.
Another related paper using multiple sequence
alignment for text generation was (Barzilay and
Lee, 2002). In that work, the authors were able
to automatically acquire different lexicalizations
of the same concept from “multiple-parallel cor-
pora”. We also draw some ideas from the Fitch-
Margoliash method for building evolutionary trees
</bodyText>
<page confidence="0.970964">
748
</page>
<listItem confidence="0.997582421052632">
1. A police official said it was a Piper tourist plane and that the crash had set the top floors on fire.
2. According to ABCNEWS aviation expert John Nance, Piper planes have no history of mechanical troubles or
other problems that would lead a pilot to lose control.
3. April 18, 2002 8212; A small Piper aircraft crashes into the 417-foot-tall Pirelli skyscraper in Milan,
setting the top floors of the 32-story building on fire.
4. Authorities said the pilot of a small Piper plane called in a problem with the landing gear to the Milan’s
Linate airport at 5:54 p.m., the smaller airport that has a landing strip for private planes.
5. Initial reports described the plane as a Piper, but did not note the specific model.
6. Italian rescue officials reported that at least two people were killed after the Piper aircraft struck the
32-story Pirelli building, which is in the heart of the city s financial district.
7. MILAN, Italy AP A small piper plane with only the pilot on board crashed Thursday into a 30-story landmark
skyscraper, killing at least two people and injuring at least 30.
8. Police officer Celerissimo De Simone said the pilot of the Piper Air Commander plane had sent out a
distress call at 5:50 p.m. just before the crash near Milan’s main train station.
9. Police officer Celerissimo De Simone said the pilot of the Piper aircraft had sent out a distress call at
5:50 p.m. 11:50 a.m.
10. Police officer Celerissimo De Simone said the pilot of the Piper aircraft had sent out a distress
call at 5:50 p.m. just before the crash near Milan’s main train station.
11. Police officer Celerissimo De Simone said the pilot of the Piper aircraft sent out a distress call at
5:50 p.m. just before the crash near Milan’s main train station.
12. Police officer Celerissimo De Simone told The AP the pilot of the Piper aircraft had sent out a distress
call at 5:50 p.m. just before crashing.
13. Police say the aircraft was a Piper tourism plane with only the pilot on board.
14. Police say the plane was an Air Commando 8212; a small plane similar to a Piper.
15. Rescue officials said that at least three people were killed, including the pilot, while dozens were
injured after the Piper aircraft struck the Pirelli high-rise in the heart of the city s financial
district.
16. The crash by the Piper tourist plane into the 26th floor occurred at 5:50 p.m. 1450 GMT on Thursday, said
journalist Desideria Cavina.
17. The pilot of the Piper aircraft, en route from Switzerland, sent out a distress call at 5:54 p.m. just
before the crash, said police officer Celerissimo De Simone.
18. There were conflicting reports as to whether it was a terrorist attack or an accident after the pilot of
the Piper tourist plane reported that he had lost control.
1. Police officer Celerissimo De Simone said the pilot of the Piper aircraft, en route from Switzerland, sent
out a distress call at 5:54 p.m. just before the crash near Milan’s main train station.
2. Italian rescue officials reported that at least three people were killed, including the pilot, while
dozens were injured after the Piper aircraft struck the 32-story Pirelli building, which is in the heart
of the city s financial district.
</listItem>
<figureCaption confidence="0.993054">
Figure 2: A comparable cluster of size 18 and 2 novel sentences produced by syntax-based alignment.
</figureCaption>
<bodyText confidence="0.9993565">
described in (Fitch and Margoliash, 1967). That
method and related techniques in Bioinformatics
such as (Felsenstein, 1995) also make use of a sim-
ilarity matrix for aligning a number of sequences.
</bodyText>
<sectionHeader confidence="0.984429" genericHeader="method">
3 Alignment Algorithms
</sectionHeader>
<bodyText confidence="0.999964">
Our alignment algorithm can be described as mod-
ifying Levenshtein Edit Distance by assigning dif-
ferent scores to lexically matched words according
to their syntactic similarity. And the decision of
whether to align a pair of words is based on such
syntax scores.
</bodyText>
<subsectionHeader confidence="0.999242">
3.1 Modified Levenshtein Edit Distance
</subsectionHeader>
<bodyText confidence="0.999002866666667">
The Levenshtein Edit Distance (LED) is a mea-
sure of similarity between two strings named after
the Russian scientist Vladimir Levenshtein, who
devised the algorithm in 1965. It is the num-
ber of substitutions, deletions or insertions (hence
“edits”) needed to transform one string into the
other. We extend LED to sentence level by count-
ing the substitutions, deletions and insertions of
words necessary to transform a sentence into the
other. We abbreviate this sentence-level edit dis-
tance as MLED. Similar to LED, MLED compu-
tation produces an M+1 by N+1 distance matrix,
D, given two input sentences of length M and N
respectively. This matrix is constructed through
dynamic programming as shown in Figure 3.
</bodyText>
<figureCaption confidence="0.791443">
Figure 3: Dynamic programming in computing
MLED of two sentences of length M and N.
</figureCaption>
<bodyText confidence="0.999044777777778">
“match” is 2 if the ith word in Sentence 1 and
the jth word in Sentence 2 syntactically match,
and is -1 otherwise. “gap” represents the score
for inserting a gap rather than aligning, and is set
to -1. The matching conditions of two words are
far more complicated than lexical equality. Rather,
we judge whether two lexically equal words match
based on a predefined set of syntactic features.
The output matrix is used to guide the align-
ment. Starting from the bottom right entry of the
matrix, we go to the matrix entry from which the
value of the current cell is derived in the recursion
of the dynamic programming. Call the current en-
tryD[i][j]. If it gets its value fromD[i—1][j—1],
theithword in Sentence 1 and thejthword in Sen-
tence 2 are either aligned or both aligned to a gap
depending on whether they syntactically match; if
the value ofD[i][j]is derived fromD[i][j—1]+
</bodyText>
<equation confidence="0.906644">
{ 0 ifj=0
0 ifi=0
D[i — 1][j — 1] +match,
max D[i — 1][j] +gap,
D[i][j — 1] +gap
D[i][j]=
!otherwise
</equation>
<page confidence="0.976226">
749
</page>
<bodyText confidence="0.999530666666667">
“gap”, the ith word in Sentence 1 is aligned to a
gap inserted into Sentence 2 (the jth word in Sen-
tence 2 is not consumed); otherwise, the jth word
in Sentence 2 is aligned to a gap inserted into Sen-
tence 1.
Now that we know how to align two sentences,
aligning a cluster of sentences is done progres-
sively. We start with the overall most similar pair
and then respect the initial ordering of the cluster,
aligning remaining sentences sequentially. Each
sentence is aligned against its best match in the
pool of already-aligned ones. This approach is
a hybrid of the Feng-Doolittle’s Algorithm (Feng
and Doolittle, 1987) and a variant described in
(Fitch and Margoliash, 1967).
</bodyText>
<subsectionHeader confidence="0.977425">
3.2 Syntax-based Alignment
</subsectionHeader>
<bodyText confidence="0.999961166666667">
As remarked earlier, our alignment scheme judges
whether two words match according to their
syntactic similarity on top of lexical equality.
The syntactic features are obtained from run-
ning Chunklink (Buchholz, 2000) on the Charniak
parses of the clustered sentences.
</bodyText>
<subsectionHeader confidence="0.613585">
3.2.1 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.996742185185185">
Among all the information Chunklink provides,
we use in particular the part-of-speech tags, the
Chunk tags, and the syntactic dependence traces.
The Chunk tag shows the constituent of a word
and its relative position in that constituent. It can
take one of the three values,
•“O” meaning that the word is outside of any
chunk;
•“I-XP” meaning that this word is inside an
XP chunk where X = N, V, P, ADV, ...;
•“B-XP” meaning that the word is at the be-
ginning of an XP chunk.
From now on, we shall refer to the Chunk
tag of a word as its IOB value (IOB was named
by Tjong Kim Sang and Jorn Veeenstra (Tjong
Kim Sang and Veenstra, 1999) after Ratnaparkhi
(Ratnaparkhi, 1998)). For example, in the sen-
tence “I visited Milan Theater”, the IOB value for
“I” is B-NP since it marks the beginning of a noun-
phrase (NP). On the other hand, “Theater” has an
IOB value of I-NP because it is inside a noun-
phrase (Milan Theater) and is not at the beginning
of that constituent. Finally, the syntactic depen-
dence trace of a word is the path of IOB values
from the root of the tree to the word itself. The
last element in the trace is hence the IOB of the
word itself.
</bodyText>
<subsubsectionHeader confidence="0.622897">
3.2.2 The Algorithm
</subsubsectionHeader>
<bodyText confidence="0.999975581395349">
Lexically matched words but with different
POS are considered not syntactically matched
(e.g., race VB vs. race NN). Hence, our focus
is really on pairs of lexically matched words with
the same POS. We first compare their IOB values.
Two IOB values are exactly matched only if they
are identical (same constituent and same position);
they are partially matched if they share a common
constituent but have different position (e.g., B-PP
vs. I-PP); and they are unmatched otherwise. For
a pair of words with exactly matched IOB values,
we assign 1 as their IOB-score; for those with par-
tially matched IOB values, 0; and -1 for those with
unmatched IOB values. The numeric values of the
score are from experimental experience.
The next step is to compare syntactic depen-
dence traces of the two words. We start with the
second last element in the traces and go backward
because the last one is already taken care of by the
previous step. We also discard the front element of
both traces since it is “I-S” for all words. The cor-
responding elements in the two traces are checked
by the IOB-comparison described above and the
scores accumulated. The process terminates as
soon as one of the two traces is exhausted. Last,
we adjust down the cumulative score by the length
difference between the two traces. Such final score
is named the trace-score of the two words.
We declare “unmatched” if the sum of the IOB-
score and the trace-score falls below 0. Otherwise,
we perform one last measurement – the relative
position of the two words in their respective sen-
tences. The relative position is defined to be the
word’s absolute position divided by the length of
the sentence it appears in (e.g. the 4th word of a
20-word sentence has a relative position of 0.2).
If the difference between two relative positions
is larger than 0.4 (empirically chosen before run-
ning the experiments), we consider the two words
“unmatched”. Otherwise, they are syntactically
matched.
The pseudo-code of checking syntactic match is
shown in Figure 4.
</bodyText>
<page confidence="0.992254">
750
</page>
<sectionHeader confidence="0.964662" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.691474">
4.1 Experimental Setup
4.1.1 Data
Algorithm Check Syntactic Match of Two Words
For a pair of wordsW1,W2
</subsectionHeader>
<bodyText confidence="0.4954515">
ifW16=W2orpos(W1)6=pos(W2)then
return “unmatched”
</bodyText>
<equation confidence="0.98485984">
endif
score := 0
iob1 := iob(W1)
iob2 := iob(W2)
score += compare iobs(iob1,iob2)
trace1 := trace(W1)
trace2 := trace(W2)
score += compare traces (trace 1, trace2)
if score &lt; 0 then
return “unmatched”
endif
relpos1 := pos(W1)/lengthOf (S1)
relpos2 := pos(W2)/lengthOf (S2)
if jrelpos1 — relpos2j&gt; 0.4 then
return “unmatched”
endif
return “matched”
Function compare iobs(iob1, iob2)
ifiob1=iob2then
return 1
endif
if substring(iob1,1) = substring(iob2,1) then
return 0
endif
return —1
</equation>
<bodyText confidence="0.5648925">
Function compare traces (trace 1, trace2)
Remove first and last elements from both traces
</bodyText>
<equation confidence="0.9523507">
score := 0
i := lengthO f (trace1) — 1
j := lengthO f (trace2) — 1
while i &gt; 0 and j &gt; 0 do
next := compare iobs(trace1[i],trace2[j])
score += next * 0.5
endwhile
score — =jlengthO f (trace1) —
lengthOf(trace2)j * 0.5
return score
</equation>
<figureCaption confidence="0.9801505">
Figure 4: Algorithm for checking the syntactic
match between two words.
</figureCaption>
<bodyText confidence="0.999743086956522">
The data we use in our experiment come from
a number of sentence clusters on a variety of top-
ics, but all related to the Milan plane crash event.
This cluster was collected manually from the Web
of five different news agencies (ABC, CNN, Fox,
MSNBC, and USAToday). It concerns the April
2002 crash of a small plane into a building in Mi-
lan, Italy and contains a total of 56 documents
published over a period of 1.5 days. To divide this
corpus into representative smaller clusters, we had
a colleague thoroughly read all 56 documents in
the cluster and then create a list of important facts
surrounding the story. We then picked key terms
related to these facts, such as names (Fasulo - the
pilot) and locations (Locarno - the city from which
the plane had departed). Finally, we automatically
clustered sentences based on the presence of these
key terms, resulting in 21 clusters of topically re-
lated (comparable) sentences. The 21 clusters are
grouped into three categories: 7 in training set, 3
in dev-testing set, and the remaining 11 in testing
set. Table 1 shows the name and size of each clus-
ter.
</bodyText>
<table confidence="0.9970588">
Cluster Number of Sentences
Training clusters
ambulance 10
belie 14
built 6
malpensa 4
piper 18
president 17
route 11
Dev-test clusters
hospital 17
rescue 12
witness 6
Test clusters
accident 30
cause 18
fasulo 33
floor 79
government 22
injur 43
linate 21
rockwell 9
spokes 18
suicide 22
terror 62
</table>
<tableCaption confidence="0.999946">
Table 1: Experimental clusters.
</tableCaption>
<page confidence="0.99682">
751
</page>
<subsectionHeader confidence="0.45551">
4.1.2 Different Versions of Alignment
</subsectionHeader>
<bodyText confidence="0.998706571428571">
To test the usefulness of our work, we ran 5 dif-
ferent alignments on the clusters. The first three
represent different levels of baseline performance
(without syntax consideration) whereas the last
two fully employ the syntactic features but treat
stop words differently. Table 2 describes the 5 ver-
sions of alignment.
</bodyText>
<table confidence="0.974733833333333">
Run Description
V1 Lexical alignment on everything possible
V2 Lexical alignment on everything but commas
V3 Lexical alignment on everything but commas and stop words
V4 Syntactic alignment on everything but commas and stop words
V5 Syntactic alignment on everything but commas
</table>
<tableCaption confidence="0.97641">
Table 2: Alignment techniques used in the experi-
ments.
</tableCaption>
<table confidence="0.9997025">
Alignment Grammaticality Fidelity
V1 2.89 2.98
V2 3.00 2.95
V3 3.15 3.22
V4 3.68 3.59
V5 3.47 3.30
</table>
<tableCaption confidence="0.997999">
Table 3: Evaluation results on training and dev-
</tableCaption>
<bodyText confidence="0.984716">
testing clusters. For the results on the test clusters,
see Table 6
The motivation of trying such variations is as
follows. Stop words often cause invalid alignment
because of their high frequencies, and so do punc-
tuations. Aligning on commas, in particular, is
likely to produce long sentences that contain mul-
tiple sentence segments ungrammatically patched
together.
</bodyText>
<subsectionHeader confidence="0.515401">
4.1.3 Training and Testing
</subsectionHeader>
<bodyText confidence="0.999952086956522">
In order to get the best possible performance
of the syntactic alignment versions, we use clus-
ters in the training and dev-test sets to tune up
the parameter values in our algorithm for check-
ing syntactic match. The parameters in our algo-
rithm are not independent. We pay special atten-
tion to the threshold of relative position difference,
the discount factor of the trace length difference
penalty, and the scores for exactly matched and
partially matched IOB values. We try different pa-
rameter settings on the training clusters, and apply
the top ranking combinations (according to human
judgments described later) on clusters in the dev-
testing set. The values presented in this paper are
the manually selected ones that yield the best per-
formance on the training and dev-testing sets.
Experimenting on the testing data, we have
two hypotheses to verify: 1) the 2 syntactic ver-
sions outperform the 3 baseline versions by both
grammaticality and fidelity (discussed later) of the
novel sentences produced by alignment; and 2)
disallowing alignment on stop words and commas
enhances the performance.
</bodyText>
<subsectionHeader confidence="0.995328">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999893470588235">
For each cluster, we ran the 5 alignment versions
and produce 5 FSA’s. From each FSA (corre-
sponding to a cluster A and alignment version i),
100 sentences are randomly generated. We re-
moved those that appear in the original cluster.
The remaining ones are hence novel sentences,
among which we randomly chose 10 to test the
performance of alignment version i on cluster A.
In the human evaluation, each sentence received
two scores – grammaticality and fidelity. These
two properties are independent since a sentence
could possibly score high on fidelity even if it is
not fully grammatical. Four different scores are
possible for both criteria: (4) perfect (fully gram-
matical or faithful); (3) good (occasional errors or
quite faithful); (2) bad (many grammar errors or
unfaithful pieces); and (1) nonsense.
</bodyText>
<subsubsectionHeader confidence="0.835625">
4.2.1 Results from the Training Phase
</subsubsectionHeader>
<bodyText confidence="0.999989727272727">
Four judges help our evaluation in the training
phase. They are provided with the original clusters
during the evaluation process, yet they are given
the sentences in shuffled order so that they have
no knowledge about from which alignment ver-
sion each sentence is generated. Table 3 shows
the averages of their evaluation on the 10 clusters
in training and dev-testing set. Each cell corre-
sponds to 400 data points as we presented 10 sen-
tences per cluster per alignment version to each of
the 4 judges (10 x 10 x 4 = 400).
</bodyText>
<subsubsectionHeader confidence="0.910423">
4.2.2 Results from the Testing Phase
</subsubsectionHeader>
<bodyText confidence="0.999923153846154">
After we have optimized the parameter config-
uration for our syntactic alignment in the training
phase, we ask another 6 human judges to evaluate
our work on the testing data. These 6 judges come
from diverse background including Information,
Computer Science, Linguistics, and Bioinformat-
ics. We distribute the 11 testing clusters among
them so that each cluster gets evaluated by at least
3 judges. The workload for each judge is 6 clus-
ters x 5 versions/cluster x 10 sentences/cluster-
version = 300 sentences. Similar to the training
phase, they receive the sentences in shuffled or-
der without knowing the correspondence between
</bodyText>
<page confidence="0.99429">
752
</page>
<bodyText confidence="0.999397857142857">
sentences and alignment versions. Detailed aver-
age statistics are shown in Table 4 and Table 5 for
grammaticality and fidelity, respectively. Each cell
is the average over 30 - 40 data points, and notice
the last row is not the mean of the other rows since
the number of sentences evaluated for each cluster
varies.
</bodyText>
<table confidence="0.999754846153846">
Cluster V1 V2 V3 V4 V5
rockwell 2.27 2.93 3.00 3.60 3.03
cause 2.77 2.83 3.07 3.10 2.93
spokes 2.87 3.07 3.57 3.83 3.50
linate 2.93 3.14 3.26 3.64 3.77
government 2.75 2.83 3.27 3.80 3.20
suicide 2.19 2.51 3.29 3.57 3.11
accident 2.92 3.27 3.54 3.72 3.56
fasulo 2.52 2.52 3.15 3.54 3.32
injur 2.29 2.92 3.03 3.62 3.29
terror 3.04 3.11 3.61 3.23 3.63
floor 2.47 2.77 3.40 3.47 3.27
Overall 2.74 2.75 3.12 3.74 3.29
</table>
<tableCaption confidence="0.9832545">
Table 4: Average grammaticality scores on testing
clusters.
</tableCaption>
<table confidence="0.999955076923077">
Cluster V1 V2 V3 V4 V5
rockwell 2.25 2.75 3.20 3.80 2.70
cause 2.42 3.04 2.92 3.48 3.17
spokes 2.65 2.50 3.20 3.00 3.05
linate 3.15 3.27 3.15 3.36 3.42
government 2.85 3.24 3.14 3.81 3.20
suicide 2.38 2.69 2.93 3.68 3.23
accident 3.14 3.42 3.56 3.91 3.57
fasulo 2.30 2.48 3.14 3.50 3.48
injur 2.56 2.28 2.29 3.18 3.22
terror 2.65 2.48 3.68 3.47 3.20
floor 2.80 2.90 3.10 3.70 3.30
Overall 2.67 2.69 3.07 3.77 3.23
</table>
<tableCaption confidence="0.999165">
Table 5: Average fidelity scores on testing clusters.
</tableCaption>
<figureCaption confidence="0.993036">
Figure 5: Performance of 5 alignment versions by
grammaticality.
Figure 6: Performance of 5 alignment versions by
fidelity.
</figureCaption>
<subsectionHeader confidence="0.992708">
4.3 Result Analysis
</subsectionHeader>
<bodyText confidence="0.999983714285714">
The results support both our hypotheses. For Hy-
pothesis I, we see that the performance of the
two syntactic alignments was higher than the non-
syntactic versions. In particular, Version 4 outper-
forms the the best baseline version by 19.9% on
grammaticality and by 22.8% on fidelity. Our sec-
ond hypothesis is also verified – disallowing align-
ment on stop words and commas yields better re-
sults. This is reflected by the fact that Version 4
beats Version 5, and Version 3 wins over the other
two baseline versions by both criteria.
At the level of individual clusters, the syntactic
versions are also found to outrival the syntax-blind
baselines. Applying at-test on the score sets for
the 5 versions, we can reject the null hypothesis
with 99.5% confidence to ensure that the syntactic
alignment performs better. Similarly, for hypoth-
esis II, the same is true for the versions with and
without stop word alignment. Figures 5 and 6 pro-
vide a graphical view of how each alignment ver-
sion performs on the testing clusters. The clusters
along the x-axis are listed in the order of increas-
ing size.
We have also done an analysis on interjudge
agreement in the evaluation. The judges are in-
structed about the evaluation scheme individually,
and do their work independently. We do not en-
force them to be mutually consistent, as long as
they are self-consistent. However, Table 6 shows
the mean and standard deviation of human judg-
ments (grammaticality and fidelity) on each ver-
sion. The small deviation values indicate a fairly
high agreement.
Finally, because human evaluation is expensive,
we additionally tried to use a language-model ap-
</bodyText>
<figure confidence="0.99464396875">
4.00
3.80
3.60
3.40
3.20
3.00
2.80
2.60
2.40
2.20
2.00
V 1
V 2
V 3
V 4
V 5
4.00
3.80
3.60
3.40
3.20
3.00
2.80
2.60
2.40
2.20
2.00
V 1
V 2
V 3
V 4
V 5
</figure>
<page confidence="0.995161">
753
</page>
<table confidence="0.997657333333333">
Alignment Gr. Mean Gr. StdDev Fi. Mean Fi. StdDev
V1 2.74 0.11 2.67 0.43
V2 2.75 0.08 2.69 0.30
V3 3.12 0.07 3.07 0.27
V4 3.74 0.08 3.77 0.16
V5 3.29 0.16 3.23 0.33
</table>
<tableCaption confidence="0.796134">
Table 6: Mean and standard deviation of human
judgments.
</tableCaption>
<bodyText confidence="0.9932781">
proach in the training phase for automatic eval-
uation of grammaticality. We have used BLEU
scores(Papineni et al., 2001), but have observed
that they are not consistent with those of human
judges. In particular, BLEU assigns too high
scores to segmented sentences that are otherwise
grammatical. It has been noted in the literature
that metrics like BLEU that are solely based on
N-grams might not be suitable for checking gram-
maticality.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999805625">
In this paper, we presented a paraphrase genera-
tion method based on multiple sequence alignment
which combines traditional dynamic program-
ming techniques with linguistically motivated syn-
tactic information. We apply our work on compa-
rable texts for which syntax has not been success-
fully explored in alignment by previous work. We
showed that using syntactic features improves the
quality of the alignment-induced finite state au-
tomaton when it is used for generating novel sen-
tences. The strongest syntax guided alignment sig-
nificantly outperformed all other versions in both
grammaticality and fidelity of the novel sentences.
In this paper we showed the effectiveness of us-
ing syntax in the alignment of structurally diverse
comparable texts as needed for text generation.
</bodyText>
<sectionHeader confidence="0.999278" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999867106382979">
Regina Barzilay and Lillian Lee. 2002. Bootstrapping
Lexical Choice via Multiple-Sequence Alignment.
In Proceedings of EMNLP 2002, Philadelphia.
Regina Barzilay and Lillian Lee. 2003. Learning
to Paraphrase: An Unsupervised Approach Using
Multiple-Sequence Alignment. In Proceedings of
NAACL-HLT03, Edmonton.
Sabine Buchholz. 2000. Readme
for perl script chunklink.pl.
http://ilk.uvt.nl/ sabine/chunklink/README.html.
Richard Durbin, Sean R. Eddy, Anders Krogh, and
Graeme Mitchison. 1998. Biological Sequence
Analysis. Probabilistic Models of Proteins and Nu-
cleic Acids. Cambridge University Press.
Joseph Felsenstein. 1995. PHYLIP:
Phylogeny Inference Package.
http://evolution.genetics.washington.edu/phylip.html.
DF. Feng and Russell F. Doolittle. 1987. Progres-
sive sequence alignment as a prerequisite to correct
phylogenetic trees. Journal ofMolecular Evolution,
25(4).
Walter M. Fitch and Emanuel Margoliash. 1967.
Construction of Phylogenetic Trees. Science,
155(3760):279–284, January.
Dan Gusfield, 1997. Algorithms On Strings: A Dual
View from Computer Science and Computational
Molecular Biology. Cambridge University Press.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based Alignment of Multiple Translations:
Extracting Paraphrases and Generating New Sen-
tences. In Proceedings of HLT/NAACL 2003, Ed-
monton, Canada.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A Method for Automatic
Evaluation of Machine Translation. Research Re-
port RC22176, IBM.
Chris Quirk, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 142–
149, Barcelona, Spain, July. Association for Com-
putational Linguistics.
A Ratnaparkhi. 1998. Maximum Entropy Models for
Natural Language Ambiguity Resolution. Phd. The-
sis, University of Pennsylvania.
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Rep-
resenting text chunks. In EACL, pages 173–179.
</reference>
<page confidence="0.998882">
754
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.929984">
<title confidence="0.997966">Adding Syntax to Dynamic Programming for Aligning Comparable Texts for the Generation of Paraphrases</title>
<author confidence="0.997628">Dragomir R Agam</author>
<affiliation confidence="0.999835">Department of Electrical Engineering and Computer Science School of Information University of Michigan</affiliation>
<address confidence="0.999848">Ann Arbor, MI 48109</address>
<email confidence="0.974783">radev,agamrp,</email>
<abstract confidence="0.99827137037037">Multiple sequence alignment techniques have recently gained popularity in the Natural Language community, especially for tasks such as machine translation, text generation, and paraphrase identification. Prior work falls into two categories, depending on the type of input used: (a) parallel corpora (e.g., multiple translations of the same text) or (b) comparable texts (non-parallel but on the same topic). So far, only techniques based on parallel texts have successfully used syntactic information to guide alignments. In this paper, we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts. Our method uses dynamic programming with alignment decision based on the local syntactic similarity between two sentences. Our results show that syntactic alignment outrivals syntax-free methods by 20% in both grammaticality and fidelity when computed over the novel sentences generated by alignment-induced finite state automata.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Bootstrapping Lexical Choice via Multiple-Sequence Alignment.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP 2002,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="7233" citStr="Barzilay and Lee, 2002" startWordPosition="1136" endWordPosition="1139">ce analysis to identify paraphrases from comparable texts (news from different sources on the same event). In summary, Pang et al. use syntactic alignment of parallel texts while Barzilay and Lee use comparable (not parallel) input but ignore syntax. Our work differs from the two in that we apply syntactic information on aligning comparable texts and that the syntactic clues we use are drawn from Chunklink ilk.uvt.nl/ ˜sabine/homepage/software.html output, which is further analysis from the syntactic parse trees. Another related paper using multiple sequence alignment for text generation was (Barzilay and Lee, 2002). In that work, the authors were able to automatically acquire different lexicalizations of the same concept from “multiple-parallel corpora”. We also draw some ideas from the FitchMargoliash method for building evolutionary trees 748 1. A police official said it was a Piper tourist plane and that the crash had set the top floors on fire. 2. According to ABCNEWS aviation expert John Nance, Piper planes have no history of mechanical troubles or other problems that would lead a pilot to lose control. 3. April 18, 2002 8212; A small Piper aircraft crashes into the 417-foot-tall Pirelli skyscraper</context>
</contexts>
<marker>Barzilay, Lee, 2002</marker>
<rawString>Regina Barzilay and Lillian Lee. 2002. Bootstrapping Lexical Choice via Multiple-Sequence Alignment. In Proceedings of EMNLP 2002, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-HLT03,</booktitle>
<location>Edmonton.</location>
<contexts>
<context position="2116" citStr="Barzilay and Lee, 2003" startWordPosition="320" endWordPosition="323">ferent sources and papers on the same topic authored by different people. It is useful to recognize if one text cites another in cases like news sharing among media agencies or citations in academic work. Applications of such recognition include machine translation, text generation, paraphrase identification, and question answering, all of which have recently drawn the attention of a number of researchers in natural language processing community. Multiple sequence alignment (MSA) is the basis for accomplishing these tasks. Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases. Nevertheless, this approach has a drawback of over-generating ungrammatical sentences due to its “almost-free” alignment. Pang et al. provide a remedy to this problem by performing alignment on the Charniak parse trees of the clustered sentences (Pang et al., 2003). Although it is so far the most similar work to ours, Pang’s solution assumes the input sentences to be semantically equivalent. Two other important references for string-based alignments</context>
<context position="6002" citStr="Barzilay and Lee, 2003" startWordPosition="940" endWordPosition="943">ontribution of this paper is a syntaxbased alignment technique for generating novel paraphrases of sentences that describe a particular fact. Such techniques can be potentially useful in multi-document summarizers such as Newsblaster (http://newsblaster.cs. columbia.edu) and NewsInEssence (http: //www.newsinessence.com). Such systems are notorious for mostly reusing text from existing news stories. We believe that allowing them to use novel formulations of known facts will make these systems much more successful. 2 Related work Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003) and (Pang et al., 2003). Both of these papers describe how multiple sequence alignment can be used for extracting paraphrases from clustered texts. Pang et al. use as their input the multiple human English translations of Chinese documents provided by the LDC as part of the NIST machine translation evaluation. Their approach is to merge multiple parse trees into a single finite state automaton in which identical input subconstituents are merged while alternatives are converted to parallel paths in the output FSA. Barzilay and Lee, on the other hand, make use of classic techniques in biologica</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment. In Proceedings of NAACL-HLT03, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
</authors>
<title>Readme for perl script chunklink.pl.</title>
<date>2000</date>
<note>http://ilk.uvt.nl/ sabine/chunklink/README.html.</note>
<contexts>
<context position="13996" citStr="Buchholz, 2000" startWordPosition="2292" endWordPosition="2293">. We start with the overall most similar pair and then respect the initial ordering of the cluster, aligning remaining sentences sequentially. Each sentence is aligned against its best match in the pool of already-aligned ones. This approach is a hybrid of the Feng-Doolittle’s Algorithm (Feng and Doolittle, 1987) and a variant described in (Fitch and Margoliash, 1967). 3.2 Syntax-based Alignment As remarked earlier, our alignment scheme judges whether two words match according to their syntactic similarity on top of lexical equality. The syntactic features are obtained from running Chunklink (Buchholz, 2000) on the Charniak parses of the clustered sentences. 3.2.1 Syntactic Features Among all the information Chunklink provides, we use in particular the part-of-speech tags, the Chunk tags, and the syntactic dependence traces. The Chunk tag shows the constituent of a word and its relative position in that constituent. It can take one of the three values, •“O” meaning that the word is outside of any chunk; •“I-XP” meaning that this word is inside an XP chunk where X = N, V, P, ADV, ...; •“B-XP” meaning that the word is at the beginning of an XP chunk. From now on, we shall refer to the Chunk tag of </context>
</contexts>
<marker>Buchholz, 2000</marker>
<rawString>Sabine Buchholz. 2000. Readme for perl script chunklink.pl. http://ilk.uvt.nl/ sabine/chunklink/README.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Durbin</author>
<author>Sean R Eddy</author>
<author>Anders Krogh</author>
<author>Graeme Mitchison</author>
</authors>
<title>Biological Sequence Analysis. Probabilistic Models of Proteins and Nucleic Acids.</title>
<date>1998</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2812" citStr="Durbin et al., 1998" startWordPosition="426" endWordPosition="429">nality or variability among comparable texts and generate paraphrases. Nevertheless, this approach has a drawback of over-generating ungrammatical sentences due to its “almost-free” alignment. Pang et al. provide a remedy to this problem by performing alignment on the Charniak parse trees of the clustered sentences (Pang et al., 2003). Although it is so far the most similar work to ours, Pang’s solution assumes the input sentences to be semantically equivalent. Two other important references for string-based alignments algorithms, mostly with applications in Biology, are (Gusfield, 1997) and (Durbin et al., 1998). In our approach, we work on comparable texts (not necessarily equivalent in their semantic meanings) as Barzilay and Lee did. However, we use local syntactic similarity (as opposed to lexical similarity) in doing the alignment on the raw sentences instead of on their parse trees. Because of the semantic discrepancies among the inputs, applying syntactic features in the alignment has a larger impact on the grammaticality and fidelity of the generated unseen sentences. While previous work positions the primary focus on the quality of paraphrases and/or translations, we are more interested in t</context>
</contexts>
<marker>Durbin, Eddy, Krogh, Mitchison, 1998</marker>
<rawString>Richard Durbin, Sean R. Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis. Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Felsenstein</author>
</authors>
<date>1995</date>
<note>PHYLIP: Phylogeny Inference Package. http://evolution.genetics.washington.edu/phylip.html.</note>
<contexts>
<context position="10894" citStr="Felsenstein, 1995" startWordPosition="1767" endWordPosition="1768">he pilot of the Piper aircraft, en route from Switzerland, sent out a distress call at 5:54 p.m. just before the crash near Milan’s main train station. 2. Italian rescue officials reported that at least three people were killed, including the pilot, while dozens were injured after the Piper aircraft struck the 32-story Pirelli building, which is in the heart of the city s financial district. Figure 2: A comparable cluster of size 18 and 2 novel sentences produced by syntax-based alignment. described in (Fitch and Margoliash, 1967). That method and related techniques in Bioinformatics such as (Felsenstein, 1995) also make use of a similarity matrix for aligning a number of sequences. 3 Alignment Algorithms Our alignment algorithm can be described as modifying Levenshtein Edit Distance by assigning different scores to lexically matched words according to their syntactic similarity. And the decision of whether to align a pair of words is based on such syntax scores. 3.1 Modified Levenshtein Edit Distance The Levenshtein Edit Distance (LED) is a measure of similarity between two strings named after the Russian scientist Vladimir Levenshtein, who devised the algorithm in 1965. It is the number of substit</context>
</contexts>
<marker>Felsenstein, 1995</marker>
<rawString>Joseph Felsenstein. 1995. PHYLIP: Phylogeny Inference Package. http://evolution.genetics.washington.edu/phylip.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng</author>
<author>Russell F Doolittle</author>
</authors>
<title>Progressive sequence alignment as a prerequisite to correct phylogenetic trees.</title>
<date>1987</date>
<journal>Journal ofMolecular Evolution,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="13695" citStr="Feng and Doolittle, 1987" startWordPosition="2246" endWordPosition="2249">749 “gap”, the ith word in Sentence 1 is aligned to a gap inserted into Sentence 2 (the jth word in Sentence 2 is not consumed); otherwise, the jth word in Sentence 2 is aligned to a gap inserted into Sentence 1. Now that we know how to align two sentences, aligning a cluster of sentences is done progressively. We start with the overall most similar pair and then respect the initial ordering of the cluster, aligning remaining sentences sequentially. Each sentence is aligned against its best match in the pool of already-aligned ones. This approach is a hybrid of the Feng-Doolittle’s Algorithm (Feng and Doolittle, 1987) and a variant described in (Fitch and Margoliash, 1967). 3.2 Syntax-based Alignment As remarked earlier, our alignment scheme judges whether two words match according to their syntactic similarity on top of lexical equality. The syntactic features are obtained from running Chunklink (Buchholz, 2000) on the Charniak parses of the clustered sentences. 3.2.1 Syntactic Features Among all the information Chunklink provides, we use in particular the part-of-speech tags, the Chunk tags, and the syntactic dependence traces. The Chunk tag shows the constituent of a word and its relative position in th</context>
</contexts>
<marker>Feng, Doolittle, 1987</marker>
<rawString>DF. Feng and Russell F. Doolittle. 1987. Progressive sequence alignment as a prerequisite to correct phylogenetic trees. Journal ofMolecular Evolution, 25(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter M Fitch</author>
<author>Emanuel Margoliash</author>
</authors>
<date>1967</date>
<journal>Construction of Phylogenetic Trees. Science,</journal>
<volume>155</volume>
<issue>3760</issue>
<contexts>
<context position="10812" citStr="Fitch and Margoliash, 1967" startWordPosition="1754" endWordPosition="1757">ist plane reported that he had lost control. 1. Police officer Celerissimo De Simone said the pilot of the Piper aircraft, en route from Switzerland, sent out a distress call at 5:54 p.m. just before the crash near Milan’s main train station. 2. Italian rescue officials reported that at least three people were killed, including the pilot, while dozens were injured after the Piper aircraft struck the 32-story Pirelli building, which is in the heart of the city s financial district. Figure 2: A comparable cluster of size 18 and 2 novel sentences produced by syntax-based alignment. described in (Fitch and Margoliash, 1967). That method and related techniques in Bioinformatics such as (Felsenstein, 1995) also make use of a similarity matrix for aligning a number of sequences. 3 Alignment Algorithms Our alignment algorithm can be described as modifying Levenshtein Edit Distance by assigning different scores to lexically matched words according to their syntactic similarity. And the decision of whether to align a pair of words is based on such syntax scores. 3.1 Modified Levenshtein Edit Distance The Levenshtein Edit Distance (LED) is a measure of similarity between two strings named after the Russian scientist Vl</context>
<context position="13751" citStr="Fitch and Margoliash, 1967" startWordPosition="2255" endWordPosition="2258">gap inserted into Sentence 2 (the jth word in Sentence 2 is not consumed); otherwise, the jth word in Sentence 2 is aligned to a gap inserted into Sentence 1. Now that we know how to align two sentences, aligning a cluster of sentences is done progressively. We start with the overall most similar pair and then respect the initial ordering of the cluster, aligning remaining sentences sequentially. Each sentence is aligned against its best match in the pool of already-aligned ones. This approach is a hybrid of the Feng-Doolittle’s Algorithm (Feng and Doolittle, 1987) and a variant described in (Fitch and Margoliash, 1967). 3.2 Syntax-based Alignment As remarked earlier, our alignment scheme judges whether two words match according to their syntactic similarity on top of lexical equality. The syntactic features are obtained from running Chunklink (Buchholz, 2000) on the Charniak parses of the clustered sentences. 3.2.1 Syntactic Features Among all the information Chunklink provides, we use in particular the part-of-speech tags, the Chunk tags, and the syntactic dependence traces. The Chunk tag shows the constituent of a word and its relative position in that constituent. It can take one of the three values, •“O</context>
</contexts>
<marker>Fitch, Margoliash, 1967</marker>
<rawString>Walter M. Fitch and Emanuel Margoliash. 1967. Construction of Phylogenetic Trees. Science, 155(3760):279–284, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms On Strings: A Dual View from Computer Science and Computational Molecular Biology.</title>
<date>1997</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2786" citStr="Gusfield, 1997" startWordPosition="423" endWordPosition="424">sed to identify commonality or variability among comparable texts and generate paraphrases. Nevertheless, this approach has a drawback of over-generating ungrammatical sentences due to its “almost-free” alignment. Pang et al. provide a remedy to this problem by performing alignment on the Charniak parse trees of the clustered sentences (Pang et al., 2003). Although it is so far the most similar work to ours, Pang’s solution assumes the input sentences to be semantically equivalent. Two other important references for string-based alignments algorithms, mostly with applications in Biology, are (Gusfield, 1997) and (Durbin et al., 1998). In our approach, we work on comparable texts (not necessarily equivalent in their semantic meanings) as Barzilay and Lee did. However, we use local syntactic similarity (as opposed to lexical similarity) in doing the alignment on the raw sentences instead of on their parse trees. Because of the semantic discrepancies among the inputs, applying syntactic features in the alignment has a larger impact on the grammaticality and fidelity of the generated unseen sentences. While previous work positions the primary focus on the quality of paraphrases and/or translations, w</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield, 1997. Algorithms On Strings: A Dual View from Computer Science and Computational Molecular Biology. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based Alignment of Multiple Translations: Extracting Paraphrases and Generating New Sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL 2003,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2528" citStr="Pang et al., 2003" startWordPosition="383" endWordPosition="386">al language processing community. Multiple sequence alignment (MSA) is the basis for accomplishing these tasks. Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases. Nevertheless, this approach has a drawback of over-generating ungrammatical sentences due to its “almost-free” alignment. Pang et al. provide a remedy to this problem by performing alignment on the Charniak parse trees of the clustered sentences (Pang et al., 2003). Although it is so far the most similar work to ours, Pang’s solution assumes the input sentences to be semantically equivalent. Two other important references for string-based alignments algorithms, mostly with applications in Biology, are (Gusfield, 1997) and (Durbin et al., 1998). In our approach, we work on comparable texts (not necessarily equivalent in their semantic meanings) as Barzilay and Lee did. However, we use local syntactic similarity (as opposed to lexical similarity) in doing the alignment on the raw sentences instead of on their parse trees. Because of the semantic discrepan</context>
<context position="6026" citStr="Pang et al., 2003" startWordPosition="945" endWordPosition="948">a syntaxbased alignment technique for generating novel paraphrases of sentences that describe a particular fact. Such techniques can be potentially useful in multi-document summarizers such as Newsblaster (http://newsblaster.cs. columbia.edu) and NewsInEssence (http: //www.newsinessence.com). Such systems are notorious for mostly reusing text from existing news stories. We believe that allowing them to use novel formulations of known facts will make these systems much more successful. 2 Related work Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003) and (Pang et al., 2003). Both of these papers describe how multiple sequence alignment can be used for extracting paraphrases from clustered texts. Pang et al. use as their input the multiple human English translations of Chinese documents provided by the LDC as part of the NIST machine translation evaluation. Their approach is to merge multiple parse trees into a single finite state automaton in which identical input subconstituents are merged while alternatives are converted to parallel paths in the output FSA. Barzilay and Lee, on the other hand, make use of classic techniques in biological sequence analysis to i</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based Alignment of Multiple Translations: Extracting Paraphrases and Generating New Sentences. In Proceedings of HLT/NAACL 2003, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<tech>Research Report RC22176, IBM.</tech>
<contexts>
<context position="27681" citStr="Papineni et al., 2001" startWordPosition="4629" endWordPosition="4632">cate a fairly high agreement. Finally, because human evaluation is expensive, we additionally tried to use a language-model ap4.00 3.80 3.60 3.40 3.20 3.00 2.80 2.60 2.40 2.20 2.00 V 1 V 2 V 3 V 4 V 5 4.00 3.80 3.60 3.40 3.20 3.00 2.80 2.60 2.40 2.20 2.00 V 1 V 2 V 3 V 4 V 5 753 Alignment Gr. Mean Gr. StdDev Fi. Mean Fi. StdDev V1 2.74 0.11 2.67 0.43 V2 2.75 0.08 2.69 0.30 V3 3.12 0.07 3.07 0.27 V4 3.74 0.08 3.77 0.16 V5 3.29 0.16 3.23 0.33 Table 6: Mean and standard deviation of human judgments. proach in the training phase for automatic evaluation of grammaticality. We have used BLEU scores(Papineni et al., 2001), but have observed that they are not consistent with those of human judges. In particular, BLEU assigns too high scores to segmented sentences that are otherwise grammatical. It has been noted in the literature that metrics like BLEU that are solely based on N-grams might not be suitable for checking grammaticality. 5 Conclusion In this paper, we presented a paraphrase generation method based on multiple sequence alignment which combines traditional dynamic programming techniques with linguistically motivated syntactic information. We apply our work on comparable texts for which syntax has no</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. BLEU: A Method for Automatic Evaluation of Machine Translation. Research Report RC22176, IBM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Monolingual machine translation for paraphrase generation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>142--149</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="5159" citStr="Quirk et al., 2004" startWordPosition="811" endWordPosition="814"> much. We shall explain syntactic features and their usages later. In this small example, our syntax-based alignment will align nothing (the bottom FSA in Figure 1) since “Milan” is the only lexically common word in both sentences. For much larger clusters in our experiments, we are able to produce a significant number of novel sentences from our alignment with such tightened syntactic conditions. Figure 2 shows one of the actual clusters used in our work that has 18 unique sentences. Two of the many automatically generated grammatical sentences are also shown. Another piece of related work, (Quirk et al., 2004), starts off with parallel inputs and uses monolingual Statistical Machine Translation techniques to align them and generate novel sentences. In our work, the input text does not need to be nearly as parallel. The main contribution of this paper is a syntaxbased alignment technique for generating novel paraphrases of sentences that describe a particular fact. Such techniques can be potentially useful in multi-document summarizers such as Newsblaster (http://newsblaster.cs. columbia.edu) and NewsInEssence (http: //www.newsinessence.com). Such systems are notorious for mostly reusing text from e</context>
</contexts>
<marker>Quirk, Brockett, Dolan, 2004</marker>
<rawString>Chris Quirk, Chris Brockett, and William Dolan. 2004. Monolingual machine translation for paraphrase generation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 142– 149, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>Phd. Thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="14745" citStr="Ratnaparkhi, 1998" startWordPosition="2427" endWordPosition="2428">n particular the part-of-speech tags, the Chunk tags, and the syntactic dependence traces. The Chunk tag shows the constituent of a word and its relative position in that constituent. It can take one of the three values, •“O” meaning that the word is outside of any chunk; •“I-XP” meaning that this word is inside an XP chunk where X = N, V, P, ADV, ...; •“B-XP” meaning that the word is at the beginning of an XP chunk. From now on, we shall refer to the Chunk tag of a word as its IOB value (IOB was named by Tjong Kim Sang and Jorn Veeenstra (Tjong Kim Sang and Veenstra, 1999) after Ratnaparkhi (Ratnaparkhi, 1998)). For example, in the sentence “I visited Milan Theater”, the IOB value for “I” is B-NP since it marks the beginning of a nounphrase (NP). On the other hand, “Theater” has an IOB value of I-NP because it is inside a nounphrase (Milan Theater) and is not at the beginning of that constituent. Finally, the syntactic dependence trace of a word is the path of IOB values from the root of the tree to the word itself. The last element in the trace is hence the IOB of the word itself. 3.2.2 The Algorithm Lexically matched words but with different POS are considered not syntactically matched (e.g., rac</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>A Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Phd. Thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Jorn Veenstra</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In EACL,</booktitle>
<pages>173--179</pages>
<contexts>
<context position="14707" citStr="Sang and Veenstra, 1999" startWordPosition="2421" endWordPosition="2424">the information Chunklink provides, we use in particular the part-of-speech tags, the Chunk tags, and the syntactic dependence traces. The Chunk tag shows the constituent of a word and its relative position in that constituent. It can take one of the three values, •“O” meaning that the word is outside of any chunk; •“I-XP” meaning that this word is inside an XP chunk where X = N, V, P, ADV, ...; •“B-XP” meaning that the word is at the beginning of an XP chunk. From now on, we shall refer to the Chunk tag of a word as its IOB value (IOB was named by Tjong Kim Sang and Jorn Veeenstra (Tjong Kim Sang and Veenstra, 1999) after Ratnaparkhi (Ratnaparkhi, 1998)). For example, in the sentence “I visited Milan Theater”, the IOB value for “I” is B-NP since it marks the beginning of a nounphrase (NP). On the other hand, “Theater” has an IOB value of I-NP because it is inside a nounphrase (Milan Theater) and is not at the beginning of that constituent. Finally, the syntactic dependence trace of a word is the path of IOB values from the root of the tree to the word itself. The last element in the trace is hence the IOB of the word itself. 3.2.2 The Algorithm Lexically matched words but with different POS are considere</context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Representing text chunks. In EACL, pages 173–179.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>