<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000078">
<title confidence="0.9996025">
Metadata-Aware Measures for Answer Summarization
in Community Question Answering
</title>
<author confidence="0.997424">
Mattia Tomasoni * Minlie Huang
</author>
<affiliation confidence="0.998254">
Dept. of Information Technology Dept. Computer Science and Technology
Uppsala University, Uppsala, Sweden Tsinghua University, Beijing 100084, China
</affiliation>
<email confidence="0.988959">
mattia.tomasoni.8371@student.uu.se aihuang@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.997245" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999924">
This paper presents a framework for au-
tomatically processing information com-
ing from community Question Answering
(cQA) portals with the purpose of gen-
erating a trustful, complete, relevant and
succinct summary in response to a ques-
tion. We exploit the metadata intrinsically
present in User Generated Content (UGC)
to bias automatic multi-document summa-
rization techniques toward high quality in-
formation. We adopt a representation of
concepts alternative to n-grams and pro-
pose two concept-scoring functions based
on semantic overlap. Experimental re-
sults on data drawn from Yahoo! An-
swers demonstrate the effectiveness of our
method in terms of ROUGE scores. We
show that the information contained in the
best answers voted by users of cQA por-
tals can be successfully complemented by
our method.
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991057366666666">
Community Question Answering (cQA) portals
are an example of Social Media where the infor-
mation need of a user is expressed in the form of a
question for which a best answer is picked among
the ones generated by other users. cQA websites
are becoming an increasingly popular complement
to search engines: overnight, a user can expect a
human-crafted, natural language answer tailored
to her specific needs. We have to be aware, though,
that User Generated Content (UGC) is often re-
dundant, noisy and untrustworthy (Jeon et al.,
The research was conducted while the first author was
visiting Tsinghua University.
2006; Wang et al., 2009b; Suryanto et al., 2009).
Interestingly, a great amount of information is em-
bedded in the metadata generated as a byprod-
uct of users’ action and interaction on Social Me-
dia. Much valuable information is contained in an-
swers other than the chosen best one (Liu et al.,
2008). Our work aims to show that such informa-
tion can be successfully extracted and made avail-
able by exploiting metadata to distill cQA content.
To this end, we casted the problem to an instance
of the query-biased multi-document summariza-
tion task, where the question was seen as a query
and the available answers as documents to be sum-
marized. We mapped each characteristic that an
ideal answer should present to a measurable prop-
erty that we wished the final summary could ex-
hibit:
</bodyText>
<listItem confidence="0.999541833333333">
• Quality to assess trustfulness in the source,
• Coverage to ensure completeness of the in-
formation presented,
• Relevance to keep focused on the user’s in-
formation need and
• Novelty to avoid redundancy.
</listItem>
<bodyText confidence="0.998489692307692">
Quality of the information was assessed via Ma-
chine Learning (ML) techniques under best an-
swer supervision in a vector space consisting of
linguistic and statistical features about the answers
and their authors. Coverage was estimated by se-
mantic comparison with the knowledge space of a
corpus of answers to similar questions which had
been retrieved through the Yahoo! Answers API 1.
Relevance was computed as information overlap
between an answer and its question, while Novelty
was calculated as inverse overlap with all other
answers to the same question. A score was as-
signed to each concept in an answer according to
</bodyText>
<footnote confidence="0.970248">
1http://developer.yahoo.com/answers
</footnote>
<page confidence="0.858862">
760
</page>
<note confidence="0.9472385">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 760–769,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99994296">
the above properties. A score-maximizing sum-
mary under a maximum coverage model was then
computed by solving an associated Integer Linear
Programming problem (Gillick and Favre, 2009;
McDonald, 2007). We chose to express concepts
in the form of Basic Elements (BE), a semantic
unit developed at ISI2 and modeled semantic over-
lap as intersection in the equivalence classes of
two concepts (formal definitions will be given in
section 2.3).
The objective of our work was to present what
we believe is a valuable conceptual framework;
more advance machine learning and summariza-
tion techniques would most likely improve the per-
formances.
The remaining of this paper is organized as fol-
lows. In the next section Quality, Coverage, Rel-
evance and Novelty measures are presented; we
explain how they were calculated and combined
to generate a final summary of all answers to a
question. Experiments are illustrated in Section
3, where we give evidence of the effectiveness of
our method. We list related work in Section 5, dis-
cuss possible alternative approaches in Section 4
and provide our conclusions in Section 6.
</bodyText>
<sectionHeader confidence="0.961976" genericHeader="method">
2 The summarization framework
</sectionHeader>
<subsectionHeader confidence="0.986161">
2.1 Quality as a ranking problem
</subsectionHeader>
<bodyText confidence="0.999252789473684">
Quality assessing of information available on So-
cial Media had been studied before mainly as a
binary classification problem with the objective of
detecting low quality content. We, on the other
hand, treated it as a ranking problem and made
use of quality estimates with the novel intent of
successfully combining information from sources
with different levels of trustfulness and writing
ability. This is crucial when manipulating UGC,
which is known to be subject to particularly great
variance in credibility (Jeon et al., 2006; Wang
et al., 2009b; Suryanto et al., 2009) and may be
poorly written.
An answer a was given along with information
about the user u that authored it, the set TAq (To-
tal Answers) of all answers to the same question q
and the set TA&apos;s of all answers by the same user.
Making use of results available in the literature
(Agichtein et al., 2008) 3, we designed a Quality
</bodyText>
<footnote confidence="0.99003975">
2Information Sciences Institute, University of Southern
California, http://www.isi.edu
3A long list of features is proposed; training a classifier
on all of them would no doubt increase the performances.
</footnote>
<bodyText confidence="0.976448">
feature space to capture the following syntactic,
behavioral and statistical properties:
</bodyText>
<listItem confidence="0.999869714285714">
• ϑ, length of answer a
• ς, number of non-stopwords in a with a cor-
pus frequency larger than n (set to 5 in our
experiments)
• $, points awarded to user u according to the
Yahoo! Answers’ points system
• %, ratio of best answers posted by user u
</listItem>
<bodyText confidence="0.903224666666667">
The features mentioned above determined a space
Ψ; An answer a, in such feature space, assumed
the vectorial form:
</bodyText>
<equation confidence="0.969448">
Ψa = ( ϑ, ς, $, % )
</equation>
<bodyText confidence="0.993329">
Following the intuition that chosen best answers
(a*) carry high quality information, we used su-
pervised ML techniques to predict the probability
of a to have been selected as a best answer a*. We
trained a Linear Regression classifier to learn the
weight vector W = (w1, w2, w3, w4) that would
combine the above feature. Supervision was given
in the form of a training set TrQ of labeled pairs
defined as:
</bodyText>
<equation confidence="0.901739">
TrQ = I( Ψa, isbesta )I
</equation>
<bodyText confidence="0.999928857142857">
isbesta was a boolean label indicating whether a
was an a* answer; the training set size was de-
termined experimentally and will be discussed in
Section 3.2. Although the value of isbesta was
known for all answers, the output of the classifier
offered us a real-valued prediction that could be
interpreted as a quality score Q(Ψa):
</bodyText>
<equation confidence="0.999415666666667">
Q(Ψa) Pz� P( isbesta = 1 a, u, TA&apos;s, )
� P( isbesta = 1 I Ψa )
= WT · Ψa (1)
</equation>
<bodyText confidence="0.999488615384616">
The Quality measure for an answer a was approx-
imated by the probability of such answer to be a
best answer (isbesta = 1) with respect to its au-
thor u and the sets TA&apos;s and TAq. It was calcu-
lated as dot product between the learned weight
vector W and the feature vector for answer Ψa.
Our decision to proceed in an unsupervised di-
rection came from the consideration that any use
of external human annotation would have made it
impracticable to build an actual system on larger
scale. An alternative, completely unsupervised ap-
proach to quality detection that has not undergone
experimental analysis is discussed in Section 4.
</bodyText>
<page confidence="0.994176">
761
</page>
<subsectionHeader confidence="0.998114">
2.2 Bag-of-BEs and semantic overlap
</subsectionHeader>
<bodyText confidence="0.999946133333333">
The properties that remain to be discussed, namely
Coverage, Relevance and Novelty, are measures
of semantic overlap between concepts; a concept
is the smallest unit of meaning in a portion of
written text. To represent sentences and answers
we adopted an alternative approach to classical n-
grams that could be defined bag-of-BEs. a BE
is “a head|modifier|relation triple representation
of a document developed at ISI” (Zhou et al.,
2006). BEs are a strong theoretical instrument to
tackle the ambiguity inherent in natural language
that find successful practical applications in real-
world query-based summarization systems. Dif-
ferent from n-grams, they are variant in length and
depend on parsing techniques, named entity de-
tection, part-of-speech tagging and resolution of
syntactic forms such as hyponyms, pronouns, per-
tainyms, abbreviation and synonyms. To each BE
is associated a class of semantically equivalent
BEs as result of what is called a transformation
of the original BE; the mentioned class uniquely
defines the concept. What seemed to us most re-
markable is that this makes the concept context-
dependent. A sentence is defined as a set of con-
cepts and an answer is defined as the union be-
tween the sets that represent its sentences.
The rest of this section gives formal definition
of our model of concept representation and seman-
tic overlap. From a set-theoretical point of view,
each concepts c was uniquely associated with a set
</bodyText>
<equation confidence="0.7614195">
Ec = {c1, c2 ... cm} such that:
bi,j (ci ^L c) ∧ (ci -�c) ∧ (ci -�cj)
</equation>
<bodyText confidence="0.996213153846154">
In our model, the “-” relation indicated syntac-
tic equivalence (exact pattern matching), while the
“NL” relation represented semantic equivalence
under the convention of some language L (two
concepts having the same meaning). Ec was de-
fined as the set of semantically equivalent concepts
to c, called its equivalence class; each concept ci
in Ec carried the same meaning (Pz�L) of concept c
without being syntactically identical (-); further-
more, no two concepts i and j in the same equiva-
lence class were identical.
“Climbing a tree to escape a black bear is pointless be-
cause they can climb very well.”
</bodyText>
<equation confidence="0.990700428571429">
BE = they|climb
E` = {climb|bears, bear|go up, climbing|animals,
climber|instincts, trees|go up, claws|climb...I
Given two concepts c and k:
�
c - k or
Ec n Ek =� 0
</equation>
<bodyText confidence="0.999977222222222">
We defined semantic overlap as occurring between
c and k if they were syntactically identical or if
their equivalence classes Ec and Ek had at least
one element in common. In fact, given the above
definition of equivalence class and the transitivity
of “-” relation, we have that if the equivalence
classes of two concepts are not disjoint, then they
must bare the same meaning under the convention
of some language L; in that case we said that c
semantically overlapped k. It is worth noting that
relation “m” is symmetric, transitive and reflexive;
as a consequence all concepts with the same mean-
ing are part of a same equivalence class. BE and
equivalence class extraction were performed by
modifying the behavior of the BEwT-E-0.3 frame-
work 4. The framework itself is responsible for
the operative definition of the “Pz�L” relation and
the creation of the equivalence classes.
</bodyText>
<subsectionHeader confidence="0.999026">
2.3 Coverage via concept importance
</subsectionHeader>
<bodyText confidence="0.999932523809524">
In the scenario we proposed, the user’s informa-
tion need is addressed in the form of a unique,
summarized answer; information that is left out of
the final summary will simply be unavailable. This
raises the concern of completeness: besides ensur-
ing that the information provided could be trusted,
we wanted to guarantee that the posed question
was being answered thoroughly. We adopted the
general definition of Coverage as the portion of
relevant information about a certain subject that
is contained in a document (Swaminathan et al.,
2009). We proceeded by treating each answer
to a question q as a separate document and we
retrieved through the Yahoo! Answers API a set
TKq (Total Knowledge) of 50 answers 5 to ques-
tions similar to q: the knowledge space of TKq
was chosen to approximate the entire knowledge
space related to the queried question q. We cal-
culated Coverage as a function of the portion of
answers in TKq that presented semantic overlap
with a.
</bodyText>
<footnote confidence="0.826621428571429">
4The authors can be contacted regarding the possibil-
ity of sharing the code of the modified version. Orig-
inal version available from http://www.isi.edu/
publications/licensed-sw/BE/index.html.
5such limit was imposed by the current version of the API.
Experiments with a greater corpus should be carried out in the
future.
</footnote>
<equation confidence="0.81114925">
c m k
762
C(a, q) = γ(ci) · tf(ci, a) (2)
caEa
</equation>
<bodyText confidence="0.999795666666667">
The Coverage measure for an answer a was cal-
culated as the sum of term frequency tf(ci, a) for
concepts in the answer itself, weighted by a con-
cept importance function, γ(ci), for concepts in
the total knowledge space TKq. γ(c) was defined
as follows:
</bodyText>
<equation confidence="0.99965">
γ(c) = |TKq,c ||T Kq|
|TKq |· log2 |T Kq,c |(3)
</equation>
<bodyText confidence="0.999627363636364">
where TKq,c = {d ∈ TKq : ∃k ∈ d, k ./ c}
The function γ(c) of concept c was calculated as
a function of the cardinality of set TKq and set
TKq,c, which was the subset of all those answers
d that contained at least one concept k which pre-
sented semantical overlap with c itself. A similar
idea of knowledge space coverage is addressed by
Swaminathan et al. (2009), from which formulas
(2) and (3) were derived.
A sensible alternative would be to estimate Cov-
erage at the sentence level.
</bodyText>
<subsectionHeader confidence="0.989419">
2.4 Relevance and Novelty via ./ relation
</subsectionHeader>
<bodyText confidence="0.999991375">
To this point, we have addressed matters of trust-
fulness and completeness. Another widely shared
concern for Information Retrieval systems is Rel-
evance to the query. We calculated relevance by
computing the semantic overlap between concepts
in the answers and the question. Intuitively, we re-
ward concepts that express meaning that could be
found in the question to be answered.
</bodyText>
<equation confidence="0.7985145">
R(c, q) = |qc |(4)
|q|
</equation>
<bodyText confidence="0.998657666666667">
where qc = {k ∈ q : k ./ c}
The Relevance measure R(c, q) of a concept c
with respect to a question q was calculated as the
ratio of the cardinality of set qc (containing all
concepts in q that semantically overlapped with c)
normalized by the total number of concepts in q.
Another property we found desirable, was to
minimize redundancy of information in the final
summary. Since all elements in TAq (the set of
all answers to q) would be used for the final sum-
mary, we positively rewarded concepts that were
expressing novel meanings.
</bodyText>
<equation confidence="0.9941455">
N(c, q) = 1 − |T Aq,c |(5)
|T Aq|
</equation>
<bodyText confidence="0.999688571428571">
where TAq,c = {d ∈ TAq : ∃k ∈ d, k ./ c}
The Novelty measure N(c, q) of a concept c with
respect to a question q was calculated as the ratio
of the cardinality of set TAq,c over the cardinality
of set TAq; TAq,c was the subset of all those an-
swers d in TAq that contained at least one concept
k which presented semantical overlap with c.
</bodyText>
<subsectionHeader confidence="0.989948">
2.5 The concept scoring functions
</subsectionHeader>
<bodyText confidence="0.999963166666667">
We have now determined how to calculate the
scores for each property in formulas (1), (2), (4)
and (5); under the assumption that the Quality and
Coverage of a concept are the same of its answer,
every concept c part of an answer a to some ques-
tion q, could be assigned a score vector as follows:
</bodyText>
<equation confidence="0.771385">
Φc = ( Q(Ψa), C(a, q), R(c, q), N(c, q) )
</equation>
<bodyText confidence="0.999946941176471">
What we needed at this point was a function S
of the above vector which would assign a higher
score to concepts most worthy of being included
in the final summary. Our intuition was that since
Quality, Coverage, Novelty and Relevance were
all virtues properties, S needed to be monoton-
ically increasing with respect to all its dimen-
sions. We designed two such functions. Func-
tion (6), which multiplied the scores, was based
on the probabilistic interpretation of each score as
an independent event. Further empirical consid-
erations, brought us to later introduce a logarith-
mic component that would discourage inclusion of
sentences shorter then a threshold t (a reasonable
choice for this parameter is a value around 20).
The score for concept c appearing in sentence sc
was calculated as:
</bodyText>
<equation confidence="0.9624045">
(Φc) · logt (length (sc)) (6)
i
</equation>
<bodyText confidence="0.997869142857143">
A second approach that made use of human
annotation to learn a vector of weights V =
(v1, v2, v3, v4) that linearly combined the scores
was investigated. Analogously to what had been
done with scoring function (6), the Φ space was
augmented with a dimension representing the
length of the answer.
</bodyText>
<equation confidence="0.972897">
(Φc · vi) + length(sc) · v5 (7)
i
</equation>
<bodyText confidence="0.9999358">
In order to learn the weight vector V that would
combine the above scores, we asked three human
annotators to generate question-biased extractive
summaries based on all answers available for a
certain question. We trained a Linear Regression
</bodyText>
<equation confidence="0.988151333333333">
4
Sn(c) = �
i=1
4
SF-(c) = �
i=1
</equation>
<page confidence="0.979019">
763
</page>
<figure confidence="0.6913306">
classifier with a set TrS of labeled pairs defined maximize: S(ci) · xi (9)
as:
i
TrS = {( (V, length(s&apos;)), include&apos; )} Esubject to: length(j) · sj &lt; lengthM
j
</figure>
<bodyText confidence="0.999915176470588">
include&apos; was a boolean label that indicated
whether s&apos;, the sentence containing c, had
been included in the human-generated summary;
length(s&apos;) indicated the length of sentence s&apos;.
Questions and relative answers for the generation
of human summaries were taken from the “filtered
dataset” described in Section 3.1.
The concept score for the same BE in two sep-
arate answers is very likely to be different be-
cause it belongs to answers with their own Quality
and Coverage values: this only makes the scoring
function context-dependent and does not interfere
with the calculation the Coverage, Relevance and
Novelty measures, which are based on information
overlap and will regard two BEs with overlapping
equivalence classes as being the same, regardless
of their score being different.
</bodyText>
<subsectionHeader confidence="0.99709">
2.6 Quality constrained summarization
</subsectionHeader>
<bodyText confidence="0.996747583333333">
The previous sections showed how we quantita-
tively determined which concepts were more wor-
thy of becoming part of the final machine sum-
mary M. The final step was to generate the sum-
mary itself by automatically selecting sentences
under a length constraint. Choosing this constraint
carefully demonstrated to be of crucial importance
during the experimental phase. We again opted
for a metadata-driven approach and designed the
length constraint as a function of the lengths of
all answers to q (TAq) weighted by the respective
Quality measures:
</bodyText>
<equation confidence="0.9755345">
lengthM = E length(a) · Q(Xpa) (8)
aETAq
</equation>
<bodyText confidence="0.999525">
The intuition was that the longer and the more
trustworthy answers to a question were, the more
space was reasonable to allocate for information
in the final, machine summarized answer M.
M was generated so as to maximize the scores
of the concepts it included. This was done under a
maximum coverage model by solving the follow-
ing Integer Linear Programming problem:
</bodyText>
<equation confidence="0.986520333333333">
E yj · occij �! xi Vi (10)
j
occij, xi, yj E {0, 1} Vi,j
occij = 1 if ci E sj, Vi, j
xi = 1 if ci E M, Vi
yj = 1 if sj E M, Vj
</equation>
<bodyText confidence="0.999970095238095">
In the above program, M is the set of selected sen-
tences: M = {sj : yj = 1, Vj}. The integer
variables xi and yj were equals to one if the corre-
sponding concept ci and sentence sj were included
in M. Similarly occij was equal to one if concept
ci was contained in sentence sj. We maximized
the sum of scores S(ci) (for S equals to Sr, or SF,)
for each concept ci in the final summary M. We
did so under the constraint that the total length of
all sentences sj included in M must be less than
the total expected length of the summary itself. In
addition, we imposed a consistency constraint: if
a concept ci was included in M, then at least one
sentence sj that contained the concept must also
be selected (constraint (10)). The described opti-
mization problem was solved using lp solve 6.
We conclude with an empirical side note: since
solving the above can be computationally very de-
manding for large number of concepts, we found
performance-wise very fruitful to skim about one
fourth of the concepts with lowest scores.
</bodyText>
<sectionHeader confidence="0.999961" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999572">
3.1 Datasets and filters
</subsectionHeader>
<bodyText confidence="0.999847666666667">
The initial dataset was composed of 216,563 ques-
tions and 1,982,006 answers written by 171,676
user in 100 categories from the Yahoo! Answers
portal7. We will refer to this dataset as the “un-
filtered version”. The metadata described in sec-
tion 2.1 was extracted and normalized; quality
experiments (Section 3.2) were then conducted.
The unfiltered version was later reduced to 89,814
question-answer pairs that showed statistical and
linguistic properties which made them particularly
adequate for our purpose. In particular, trivial, fac-
toid and encyclopedia-answerable questions were
</bodyText>
<footnote confidence="0.9987495">
6the version used was lp solve 5.5, available at http:
//lpsolve.sourceforge.net/5.5
7The reader is encouraged to contact the authors regarding
the availability of data and filters described in this Section.
</footnote>
<page confidence="0.99362">
764
</page>
<bodyText confidence="0.99920225">
removed by applying a series of patterns for the
identification of complex questions. The work by
Liu et al. (2008) indicates some categories of ques-
tions that are particularly suitable for summariza-
tion, but due to the lack of high-performing ques-
tion classifiers we resorted to human-crafted ques-
tion patterns. Some pattern examples are the fol-
lowing:
</bodyText>
<listItem confidence="0.999939">
• {Why,What is the reason} [...]
• How {to,do,does,did} [...]
• How {is,are,were,was,will} [...]
• How {could,can,would,should} [...]
</listItem>
<bodyText confidence="0.988504285714286">
We also removed questions that showed statistical
values outside of convenient ranges: the number of
answers, length of the longest answer and length
of the sum of all answers (both absolute and nor-
malized) were taken in consideration. In particular
we discarded questions with the following charac-
teristics:
</bodyText>
<listItem confidence="0.987268571428571">
• there were less than three answers 8
• the longest answer was over 400 words
(likely a copy-and-paste)
• the sum of the length of all answers outside
of the (100, 1000) words interval
• the average length of answers was outside of
the (50, 300) words interval
</listItem>
<bodyText confidence="0.999981181818182">
At this point a second version of the dataset
was created to evaluate the summarization perfor-
mance under scoring function (6) and (7); it was
generated by manually selecting questions that
arouse subjective, human interest from the pre-
vious 89,814 question-answer pairs. The dataset
size was thus reduced to 358 answers to 100 ques-
tions that were manually summarized (refer to
Section 3.3). From now on we will refer to this
second version of the dataset as the “filtered ver-
sion”.
</bodyText>
<subsectionHeader confidence="0.999812">
3.2 Quality assessing
</subsectionHeader>
<bodyText confidence="0.999895333333333">
In Section 2.1 we claimed to be able to identify
high quality content. To demonstrate it, we con-
ducted a set of experiments on the original unfil-
tered dataset to establish whether the feature space
IF was powerful enough to capture the quality of
answers; our specific objective was to estimate the
</bodyText>
<figureCaption confidence="0.764970833333333">
8Being too easy to summarize or not requiring any sum-
marization at all, those questions wouldn’t constitute an valu-
able test of the system’s ability to extract information.
Figure 1: Precision values (Y-axis) in detecting best an-
swers a* with increasing training set size (X-axis) for a Lin-
ear Regression classifier on the unfiltered dataset.
</figureCaption>
<bodyText confidence="0.999952333333333">
amount of training examples needed to success-
fully train a classifier for the quality assessing task.
The Linear Regression9 method was chosen to de-
termine the probability Q(Ipa) of a to be a best an-
swer to q; as explained in Section 2.1, those prob-
abilities were interpreted as quality estimates. The
evaluation of the classifier’s output was based on
the observation that given the set of all answers
TAq relative to q and the best answer a*, a suc-
cessfully trained classifier should be able to rank
a* ahead of all other answers to the same question.
More precisely, we defined Precision as follows:
</bodyText>
<equation confidence="0.543014">
|{q E TrQ : ba E TAq, Q(Ipa*) &gt; Q(Ipa)}|
|TrQ|
</equation>
<bodyText confidence="0.999826235294118">
where the numerator was the number of questions
for which the classifier was able to correctly rank
a* by giving it the highest quality estimate in TAq
and the denominator was the total number of ex-
amples in the training set TrQ. Figure 1 shows the
precision values (Y-axis) in identifying best an-
swers as the size of TrQ increases (X-axis). The
experiment started from a training set of size 100
and was repeated adding 300 examples at a time
until precision started decreasing. With each in-
crease in training set size, the experiment was re-
peated ten times and average precision values were
calculated. In all runs, training examples were
picked randomly from the unfiltered dataset de-
scribed in Section 3.1; for details on TrQ see Sec-
tion 2.1. A training set of 12,000 examples was
chosen for the summarization experiments.
</bodyText>
<footnote confidence="0.989274">
9Performed with Weka 3.7.0 available at http://www.
cs.waikato.ac.nz/˜ml/weka
</footnote>
<page confidence="0.991144">
765
</page>
<table confidence="0.9979343">
System a* (baseline) SF SH
ROUGE-1 R 51.7% 67.3% 67.4%
ROUGE-1 P 62.2% 54.0% 71.2%
ROUGE-1 F 52.9% 59.3% 66.1%
ROUGE-2 R 40.5% 52.2% 58.8%
ROUGE-2 P 49.0% 41.4% 63.1%
ROUGE-2 F 41.6% 45.9% 57.9%
ROUGE-L R 50.3% 65.1% 66.3%
ROUGE-L P 60.5% 52.3% 70.7%
ROUGE-L F 51.5% 57.3% 65.1%
</table>
<tableCaption confidence="0.8269245">
Table 1: Summarization Evaluation on filtered dataset (re-
fer to Section 3.1 for details). ROUGE-L, ROUGE-1 and
ROUGE-2 are presented; for each, Recall (R), Precision (P)
and F-1 score (F) are given.
</tableCaption>
<subsectionHeader confidence="0.999674">
3.3 Evaluating answer summaries
</subsectionHeader>
<bodyText confidence="0.999265875">
The objective of our work was to summarize an-
swers from cQA portals. Two systems were de-
signed: Table 1 shows the performances using
function SΣ (see equation (7)), and function SΠ
(see equation (6)). The chosen best answer a*
was used as a baseline. We calculated ROUGE-1
and ROUGE-2 scores10 against human annotation
on the filtered version of the dataset presented in
Section 3.1. The filtered dataset consisted of 358
answers to 100 questions. For each questions q,
three annotators were asked to produce an extrac-
tive summary of the information contained in T Aq
by selecting sentences subject to a fixed length
limit of 250 words. The annotation resulted in 300
summaries (larger-scale annotation is still ongo-
ing). For the SΣ system, 200 of the 300 generated
summaries were used for training and the remain-
ing were used for testing (see the definition of TrS
Section 2.5). Cross-validation was conducted. For
the SΠ system, which required no training, all of
the 300 summaries were used as the test set.
SΣ outperformed the baseline in Recall (R) but
not in Precision (P); nevertheless, the combined F-
1 score (F) was sensibly higher (around 5 points
percentile). On the other hand, our SΠ system
showed very consistent improvements of an order
of 10 to 15 points percentile over the baseline on
all measures; we would like to draw attention on
the fact that even if Precision scores are higher,
it is on Recall scores that greater improvements
were achieved. This, together with the results ob-
tained by SΣ, suggest performances could benefit
</bodyText>
<footnote confidence="0.9931725">
10Available at http://berouge.com/default.
aspx
</footnote>
<figureCaption confidence="0.9794938">
Figure 2: Increase in ROUGE-L, ROUGE-1 and ROUGE-
2 performances of the Sn system as more measures are taken
in consideration in the scoring function, starting from Rele-
vance alone (R) to the complete system (RQNC). F-1 scores
are given.
</figureCaption>
<bodyText confidence="0.99992648">
from the enforcement of a more stringent length
constraint than the one proposed in (8). Further
potential improvements on SΣ could be obtained
by choosing a classifier able to learn a more ex-
pressive underlying function.
In order to determine what influence the single
measures had on the overall performance, we con-
ducted a final experiment on the filtered dataset to
evaluate (the SΠ scoring function was used). The
evaluation was conducted in terms of F-1 scores of
ROUGE-L, ROUGE-1 and ROUGE-2. First only
Relevance was tested (R) and subsequently Qual-
ity was added (RQ); then, in turn, Coverage (RQC)
and Novelty (RQN); Finally the complete system
taking all measures in consideration (RQNC). Re-
sults are shown in Figure 2. In general perfor-
mances increase smoothly with the exception of
ROUGE-2 score, which seems to be particularly
sensitive to Novelty: no matter what combination
of measures is used (R alone, RQ, RQC), changes
in ROUGE-2 score remain under one point per-
centile. Once Novelty is added, performances rise
abruptly to the system’s highest. A summary ex-
ample, along with the question and the best an-
swer, is presented in Table 2.
</bodyText>
<sectionHeader confidence="0.997858" genericHeader="method">
4 Discussion and Future Directions
</sectionHeader>
<bodyText confidence="0.9999056">
We conclude by discussing a few alternatives to
the approaches we presented. The lengthen con-
straint for the final summary (Section 2.6), could
have been determined by making use of external
knowledge such as TKq: since TKq represents
</bodyText>
<page confidence="0.990394">
766
</page>
<note confidence="0.588703">
HOW TO PROTECT YOURSELF FROM A BEAR?
http://answers.yahoo.com/question/index?qid=
20060818062414AA7VldB
***BEST ANSWER***
</note>
<bodyText confidence="0.980250176470588">
Great question. I have done alot of trekking through California, Montana
and Wyoming and have met Black bears (which are quite dinky and placid
but can go nuts if they have babies), and have been half an hour away from
(allegedly) the mother of all grizzley s whilst on a trail through Glacier
National park - so some other trekkerers told me... What the park wardens
say is SING, SHOUT, MAKE NOISE...do it loudly, let them know you
are there..they will get out of the way, it is a surprised bear wot will go
mental and rip your little legs off..No fun permission: anything that will
confuse them and stop them in their tracks...I have been told be an native
american buddy that to keep a bottle of perfume in your pocket...throw it at
the ground near your feet and make the place stink: they have good noses,
them bears, and a mega concentrated dose of Britney Spears Obsessive
Compulsive is gonna give em something to think about...Have you got a
rape alarm? Def take that...you only need to distract them for a second
then they will lose interest..Stick to the trails is the most important thing,
and talk to everyone you see when trekking: make sure others know where
you are.
</bodyText>
<tableCaption confidence="0.719424931034483">
***SUMMARIZED ANSWER***
[...] In addition if the bear actually approaches you or charges you.. still
stand your ground. Many times they will not actually come in contact
with you, they will charge, almost touch you than run away. [...] The
actions you should take are different based on the type of bear. for ex-
ample adult Grizzlies can t climb trees, but Black bears can even when
adults. They can not climb in general as thier claws are longer and not
semi-retractable like a Black bears claws. [...] I truly disagree with the
whole play dead approach because both Grizzlies and Black bears are
oppurtunistic animals and will feed on carrion as well as kill and eat an-
imals. Although Black bears are much more scavenger like and tend not
to kill to eat as much as they just look around for scraps. Grizzlies on the
other hand are very accomplished hunters and will take down large prey
animals when they want. [...] I have lived in the wilderness of Northern
Canada for many years and I can honestly say that Black bears are not at
all likely to attack you in most cases they run away as soon as they see or
smell a human, the only places where Black bears are agressive is in parks
with visitors that feed them, everywhere else the bears know that usually
humans shoot them and so fear us. [...]
Table 2: A summarized answer composed of five different
portions of text generated with the Sr&apos; scoring function; the
chosen best answer is presented for comparison. The rich-
ness of the content and the good level of readability make
it a successful instance of metadata-aware summarization of
information in cQA systems. Less satisfying examples in-
clude summaries to questions that require a specific order of
sentences or a compromise between strongly discordant opin-
ions; in those cases, the summarized answer might lack logi-
cal consistency.
</tableCaption>
<bodyText confidence="0.999882926829268">
the total knowledge available about q, a coverage
estimate of the final answers against it would have
been ideal. Unfortunately the lack of metadata
about those answers prevented us from proceeding
in that direction. This consideration suggests the
idea of building TKq using similar answers in the
dataset itself, for which metadata is indeed avail-
able. Furthermore, similar questions in the dataset
could have been used to augment the set of an-
swers used to generate the final summary with an-
swers coming from similar questions. Wang et al.
(2009a) presents a method to retrieve similar ques-
tions that could be worth taking in consideration
for the task. We suggest that the retrieval method
could be made Quality-aware. A Quality feature
space for questions is presented by Agichtein et
al. (2008) and could be used to rank the quality of
questions in a way similar to how we ranked the
quality of answers.
The Quality assessing component itself could
be built as a module that can be adjusted to the
kind of Social Media in use; the creation of cus-
tomized Quality feature spaces would make it
possible to handle different sources of UGC (fo-
rums, collaborative authoring websites such as
Wikipedia, blogs etc.). A great obstacle is the lack
of systematically available high quality training
examples: a tentative solution could be to make
use of clustering algorithms in the feature space;
high and low quality clusters could then be labeled
by comparison with examples of virtuous behav-
ior (such as Wikipedia’s Featured Articles). The
quality of a document could then be estimated as a
function of distance from the centroid of the clus-
ter it belongs to. More careful estimates could take
the position of other clusters and the concentration
of nearby documents in consideration.
Finally, in addition to the chosen best answer, a
DUC-styled query-focused multi-document sum-
mary could be used as a baseline against which
the performances of the system can be checked.
</bodyText>
<sectionHeader confidence="0.999984" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99985252173913">
A work with a similar objective to our own is
that of Liu et al. (2008), where standard multi-
document summarization techniques are em-
ployed along with taxonomic information about
questions. Our approach differs in two fundamen-
tal aspects: it took in consideration the peculiari-
ties of the data in input by exploiting the nature of
UGC and available metadata; additionally, along
with relevance, we addressed challenges that are
specific to Question Answering, such as Cover-
age and Novelty. For an investigation of Coverage
in the context of Search Engines, refer to Swami-
nathan et al. (2009).
At the core of our work laid information trust-
fulness, summarization techniques and alternative
concept representation. A general approach to
the broad problem of evaluating information cred-
ibility on the Internet is presented by Akamine
et al. (2009) with a system that makes use of
semantic-aware Natural Language Preprocessing
techniques. With analogous goals, but a focus
on UGC, are the papers of Stvilia et al. (2005),
Mcguinness et al. (2006), Hu et al. (2007) and
</bodyText>
<page confidence="0.986236">
767
</page>
<bodyText confidence="0.99982584375">
Zeng et al. (2006), which present a thorough inves-
tigation of Quality and trust in Wikipedia. In the
cQA domain, Jeon et al. (2006) presents a frame-
work to use Maximum Entropy for answer quality
estimation through non-textual features; with the
same purpose, more recent methods based on the
expertise of answerers are proposed by Suryanto
et al. (2009), while Wang et al. (2009b) introduce
the idea of ranking answers taking their relation to
questions in consideration. The paper that we re-
gard as most authoritative on the matter is the work
by Agichtein et al. (2008) which inspired us in the
design of the Quality feature space presented in
Section 2.1.
Our approach merged trustfulness estimation
and summarization techniques: we adapted the au-
tomatic concept-level model presented by Gillick
and Favre (2009) to our needs; related work in
multi-document summarization has been carried
out by Wang et al. (2008) and McDonald (2007).
A relevant selection of approaches that instead
make use of ML techniques for query-biased sum-
marization is the following: Wang et al. (2007),
Metzler and Kanungo (2008) and Li et al. (2009).
An aspect worth investigating is the use of par-
tially labeled or totally unlabeled data for sum-
marization in the work of Wong et al. (2008) and
Amini and Gallinari (2002).
Our final contribution was to explore the use of
Basic Elements document representation instead
of the widely used n-gram paradigm: in this re-
gard, we suggest the paper by Zhou et al. (2006).
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999945888888889">
We presented a framework to generate trust-
ful, complete, relevant and succinct answers to
questions posted by users in cQA portals. We
made use of intrinsically available metadata along
with concept-level multi-document summariza-
tion techniques. Furthermore, we proposed an
original use for the BE representation of concepts
and tested two concept-scoring functions to com-
bine Quality, Coverage, Relevance and Novelty
measures. Evaluation results on human annotated
data showed that our summarized answers consti-
tute a solid complement to best answers voted by
the cQA users.
We are in the process of building a system that
performs on-line summarization of large sets of
questions and answers from Yahoo! Answers.
Larger-scale evaluation of results against other
state-of-the-art summarization systems is ongoing.
</bodyText>
<sectionHeader confidence="0.996115" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999927272727273">
This work was partly supported by the Chi-
nese Natural Science Foundation under grant No.
60803075, and was carried out with the aid of
a grant from the International Development Re-
search Center, Ottawa, Canada. We would like to
thank Prof. Xiaoyan Zhu, Mr. Yang Tang and Mr.
Guillermo Rodriguez for the valuable discussions
and comments and for their support. We would
also like to thank Dr. Chin-yew Lin and Dr. Eu-
gene Agichtein from Emory University for sharing
their data.
</bodyText>
<sectionHeader confidence="0.999406" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995449263157895">
Eugene Agichtein, Carlos Castillo, Debora Donato,
Aristides Gionis, and Gilad Mishne. 2008. Find-
ing high-quality content in social media. In Marc
Najork, Andrei Z. Broder, and Soumen Chakrabarti,
editors, Proceedings of the International Conference
on Web Search and Web Data Mining, WSDM 2008,
Palo Alto, California, USA, February 11-12, 2008,
pages 183–194. ACM.
Susumu Akamine, Daisuke Kawahara, Yoshikiyo
Kato, Tetsuji Nakagawa, Kentaro Inui, Sadao Kuro-
hashi, and Yutaka Kidawara. 2009. Wisdom: a web
information credibility analysis system. In ACL-
IJCNLP ’09: Proceedings of the ACL-IJCNLP 2009
Software Demonstrations, pages 1–4, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Massih-Reza Amini and Patrick Gallinari. 2002. The
use of unlabeled data to improve supervised learning
for text summarization. In SIGIR ’02: Proceedings
of the 25th annual international ACM SIGIR con-
ference on Research and development in informa-
tion retrieval, pages 105–112, New York, NY, USA.
ACM.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In ILP ’09: Proceedings
of the Workshop on Integer Linear Programming for
Natural Langauge Processing, pages 10–18, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Meiqun Hu, Ee-Peng Lim, Aixin Sun, Hady Wirawan
Lauw, and Ba-Quy Vuong. 2007. Measuring arti-
cle quality in wikipedia: models and evaluation. In
CIKM ’07: Proceedings of the sixteenth ACM con-
ference on Conference on information and knowl-
edge management, pages 243–252, New York, NY,
USA. ACM.
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon
Park. 2006. A framework to predict the quality of
</reference>
<page confidence="0.969871">
768
</page>
<reference confidence="0.999401463917525">
answers with non-textual features. In SIGIR ’06:
Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 228–235, New York,
NY, USA. ACM.
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha,
and Yong Yu. 2009. Enhancing diversity, cover-
age and balance for summarization through struc-
ture learning. In WWW ’09: Proceedings of the 18th
international conference on World wide web, pages
71–80, New York, NY, USA. ACM.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin,
Dingyi Han, and Yong Yu. 2008. Understand-
ing and summarizing answers in community-based
question answering services. In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 497–504, Manch-
ester, UK, August. Coling 2008 Organizing Com-
mittee.
Ryan T. McDonald. 2007. A study of global infer-
ence algorithms in multi-document summarization.
In Giambattista Amati, Claudio Carpineto, and Gio-
vanni Romano, editors, ECIR, volume 4425 of Lec-
ture Notes in Computer Science, pages 557–564.
Springer.
Deborah L. Mcguinness, Honglei Zeng, Paulo Pin-
heiro Da Silva, Li Ding, Dhyanesh Narayanan, and
Mayukh Bhaowal. 2006. Investigation into trust for
collaborative information repositories: A wikipedia
case study. In In Proceedings of the Workshop on
Models of Trust for the Web, pages 3–131.
Donald Metzler and Tapas Kanungo. 2008. Ma-
chine learned sentence selection strategies for query-
biased summarization. In Proceedings of SIGIR
Learning to Rank Workshop.
Besiki Stvilia, Michael B. Twidale, Linda C. Smith,
and Les Gasser. 2005. Assessing information qual-
ity of a community-based encyclopedia. In Proceed-
ings of the International Conference on Information
Quality.
Maggy Anastasia Suryanto, Ee Peng Lim, Aixin Sun,
and Roger H. L. Chiang. 2009. Quality-aware col-
laborative question answering: methods and evalu-
ation. In WSDM ’09: Proceedings of the Second
ACM International Conference on Web Search and
Data Mining, pages 142–151, New York, NY, USA.
ACM.
Ashwin Swaminathan, Cherian V. Mathew, and Darko
Kirovski. 2009. Essential pages. In WI-IAT ’09:
Proceedings of the 2009 IEEE/WIC/ACM Interna-
tional Joint Conference on Web Intelligence and In-
telligent Agent Technology, pages 173–182, Wash-
ington, DC, USA. IEEE Computer Society.
Changhu Wang, Feng Jing, Lei Zhang, and Hong-
Jiang Zhang. 2007. Learning query-biased web
page summarization. In CIKM ’07: Proceedings of
the sixteenth ACM conference on Conference on in-
formation and knowledge management, pages 555–
562, New York, NY, USA. ACM.
Dingding Wang, Tao Li, Shenghuo Zhu, and Chris
Ding. 2008. Multi-document summarization via
sentence-level semantic analysis and symmetric ma-
trix factorization. In SIGIR ’08: Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 307–314, New York, NY, USA. ACM.
Kai Wang, Zhaoyan Ming, and Tat-Seng Chua. 2009a.
A syntactic tree matching approach to finding sim-
ilar questions in community-based qa services. In
SIGIR ’09: Proceedings of the 32nd international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 187–194, New
York, NY, USA. ACM.
Xin-Jing Wang, Xudong Tu, Dan Feng, and Lei Zhang.
2009b. Ranking community answers by modeling
question-answer relationships via analogical reason-
ing. In SIGIR ’09: Proceedings of the 32nd interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 179–186,
New York, NY, USA. ACM.
Kam-Fai Wong, Mingli Wu, and Wenjie Li. 2008. Ex-
tractive summarization using supervised and semi-
supervised learning. In COLING ’08: Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pages 985–992, Morristown, NJ,
USA. Association for Computational Linguistics.
Honglei Zeng, Maher A. Alhossaini, Li Ding, Richard
Fikes, and Deborah L. McGuinness. 2006. Com-
puting trust from revision history. In PST ’06: Pro-
ceedings of the 2006 International Conference on
Privacy, Security and Trust, pages 1–1, New York,
NY, USA. ACM.
Liang Zhou, Chin Y. Lin, and Eduard Hovy. 2006.
Summarizing answers for complicated questions. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC),
Genoa, Italy.
</reference>
<page confidence="0.9983">
769
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.194539">
<title confidence="0.999609">Metadata-Aware Measures for Answer Summarization in Community Question Answering</title>
<author confidence="0.999601">Tomasoni Huang</author>
<affiliation confidence="0.999842">Dept. of Information Technology Dept. Computer Science and Technology</affiliation>
<address confidence="0.432604">Uppsala University, Uppsala, Sweden Tsinghua University, Beijing 100084, China</address>
<email confidence="0.38969">mattia.tomasoni.8371@student.uu.seaihuang@tsinghua.edu.cn</email>
<abstract confidence="0.998938363636364">This paper presents a framework for automatically processing information coming from community Question Answering (cQA) portals with the purpose of generating a trustful, complete, relevant and succinct summary in response to a question. We exploit the metadata intrinsically present in User Generated Content (UGC) to bias automatic multi-document summarization techniques toward high quality information. We adopt a representation of concepts alternative to n-grams and propose two concept-scoring functions based on semantic overlap. Experimental results on data drawn from Yahoo! Answers demonstrate the effectiveness of our method in terms of ROUGE scores. We show that the information contained in the best answers voted by users of cQA portals can be successfully complemented by our method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Carlos Castillo</author>
<author>Debora Donato</author>
</authors>
<title>Aristides Gionis, and Gilad Mishne.</title>
<date>2008</date>
<booktitle>Proceedings of the International Conference on Web Search and Web Data Mining, WSDM 2008,</booktitle>
<pages>183--194</pages>
<editor>In Marc Najork, Andrei Z. Broder, and Soumen Chakrabarti, editors,</editor>
<publisher>ACM.</publisher>
<location>Palo Alto, California, USA,</location>
<contexts>
<context position="5638" citStr="Agichtein et al., 2008" startWordPosition="895" endWordPosition="898">uality estimates with the novel intent of successfully combining information from sources with different levels of trustfulness and writing ability. This is crucial when manipulating UGC, which is known to be subject to particularly great variance in credibility (Jeon et al., 2006; Wang et al., 2009b; Suryanto et al., 2009) and may be poorly written. An answer a was given along with information about the user u that authored it, the set TAq (Total Answers) of all answers to the same question q and the set TA&apos;s of all answers by the same user. Making use of results available in the literature (Agichtein et al., 2008) 3, we designed a Quality 2Information Sciences Institute, University of Southern California, http://www.isi.edu 3A long list of features is proposed; training a classifier on all of them would no doubt increase the performances. feature space to capture the following syntactic, behavioral and statistical properties: • ϑ, length of answer a • ς, number of non-stopwords in a with a corpus frequency larger than n (set to 5 in our experiments) • $, points awarded to user u according to the Yahoo! Answers’ points system • %, ratio of best answers posted by user u The features mentioned above deter</context>
<context position="31732" citStr="Agichtein et al. (2008)" startWordPosition="5369" endWordPosition="5372">d us from proceeding in that direction. This consideration suggests the idea of building TKq using similar answers in the dataset itself, for which metadata is indeed available. Furthermore, similar questions in the dataset could have been used to augment the set of answers used to generate the final summary with answers coming from similar questions. Wang et al. (2009a) presents a method to retrieve similar questions that could be worth taking in consideration for the task. We suggest that the retrieval method could be made Quality-aware. A Quality feature space for questions is presented by Agichtein et al. (2008) and could be used to rank the quality of questions in a way similar to how we ranked the quality of answers. The Quality assessing component itself could be built as a module that can be adjusted to the kind of Social Media in use; the creation of customized Quality feature spaces would make it possible to handle different sources of UGC (forums, collaborative authoring websites such as Wikipedia, blogs etc.). A great obstacle is the lack of systematically available high quality training examples: a tentative solution could be to make use of clustering algorithms in the feature space; high an</context>
<context position="34558" citStr="Agichtein et al. (2008)" startWordPosition="5837" endWordPosition="5840"> Mcguinness et al. (2006), Hu et al. (2007) and 767 Zeng et al. (2006), which present a thorough investigation of Quality and trust in Wikipedia. In the cQA domain, Jeon et al. (2006) presents a framework to use Maximum Entropy for answer quality estimation through non-textual features; with the same purpose, more recent methods based on the expertise of answerers are proposed by Suryanto et al. (2009), while Wang et al. (2009b) introduce the idea of ranking answers taking their relation to questions in consideration. The paper that we regard as most authoritative on the matter is the work by Agichtein et al. (2008) which inspired us in the design of the Quality feature space presented in Section 2.1. Our approach merged trustfulness estimation and summarization techniques: we adapted the automatic concept-level model presented by Gillick and Favre (2009) to our needs; related work in multi-document summarization has been carried out by Wang et al. (2008) and McDonald (2007). A relevant selection of approaches that instead make use of ML techniques for query-biased summarization is the following: Wang et al. (2007), Metzler and Kanungo (2008) and Li et al. (2009). An aspect worth investigating is the use</context>
</contexts>
<marker>Agichtein, Castillo, Donato, 2008</marker>
<rawString>Eugene Agichtein, Carlos Castillo, Debora Donato, Aristides Gionis, and Gilad Mishne. 2008. Finding high-quality content in social media. In Marc Najork, Andrei Z. Broder, and Soumen Chakrabarti, editors, Proceedings of the International Conference on Web Search and Web Data Mining, WSDM 2008, Palo Alto, California, USA, February 11-12, 2008, pages 183–194. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susumu Akamine</author>
<author>Daisuke Kawahara</author>
<author>Yoshikiyo Kato</author>
<author>Tetsuji Nakagawa</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
<author>Yutaka Kidawara</author>
</authors>
<title>Wisdom: a web information credibility analysis system.</title>
<date>2009</date>
<booktitle>In ACLIJCNLP ’09: Proceedings of the ACL-IJCNLP 2009 Software Demonstrations,</booktitle>
<pages>1--4</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="33762" citStr="Akamine et al. (2009)" startWordPosition="5702" endWordPosition="5705">mental aspects: it took in consideration the peculiarities of the data in input by exploiting the nature of UGC and available metadata; additionally, along with relevance, we addressed challenges that are specific to Question Answering, such as Coverage and Novelty. For an investigation of Coverage in the context of Search Engines, refer to Swaminathan et al. (2009). At the core of our work laid information trustfulness, summarization techniques and alternative concept representation. A general approach to the broad problem of evaluating information credibility on the Internet is presented by Akamine et al. (2009) with a system that makes use of semantic-aware Natural Language Preprocessing techniques. With analogous goals, but a focus on UGC, are the papers of Stvilia et al. (2005), Mcguinness et al. (2006), Hu et al. (2007) and 767 Zeng et al. (2006), which present a thorough investigation of Quality and trust in Wikipedia. In the cQA domain, Jeon et al. (2006) presents a framework to use Maximum Entropy for answer quality estimation through non-textual features; with the same purpose, more recent methods based on the expertise of answerers are proposed by Suryanto et al. (2009), while Wang et al. (2</context>
</contexts>
<marker>Akamine, Kawahara, Kato, Nakagawa, Inui, Kurohashi, Kidawara, 2009</marker>
<rawString>Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato, Tetsuji Nakagawa, Kentaro Inui, Sadao Kurohashi, and Yutaka Kidawara. 2009. Wisdom: a web information credibility analysis system. In ACLIJCNLP ’09: Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 1–4, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massih-Reza Amini</author>
<author>Patrick Gallinari</author>
</authors>
<title>The use of unlabeled data to improve supervised learning for text summarization.</title>
<date>2002</date>
<booktitle>In SIGIR ’02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>105--112</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="35288" citStr="Amini and Gallinari (2002)" startWordPosition="5956" endWordPosition="5959">ged trustfulness estimation and summarization techniques: we adapted the automatic concept-level model presented by Gillick and Favre (2009) to our needs; related work in multi-document summarization has been carried out by Wang et al. (2008) and McDonald (2007). A relevant selection of approaches that instead make use of ML techniques for query-biased summarization is the following: Wang et al. (2007), Metzler and Kanungo (2008) and Li et al. (2009). An aspect worth investigating is the use of partially labeled or totally unlabeled data for summarization in the work of Wong et al. (2008) and Amini and Gallinari (2002). Our final contribution was to explore the use of Basic Elements document representation instead of the widely used n-gram paradigm: in this regard, we suggest the paper by Zhou et al. (2006). 6 Conclusions We presented a framework to generate trustful, complete, relevant and succinct answers to questions posted by users in cQA portals. We made use of intrinsically available metadata along with concept-level multi-document summarization techniques. Furthermore, we proposed an original use for the BE representation of concepts and tested two concept-scoring functions to combine Quality, Covera</context>
</contexts>
<marker>Amini, Gallinari, 2002</marker>
<rawString>Massih-Reza Amini and Patrick Gallinari. 2002. The use of unlabeled data to improve supervised learning for text summarization. In SIGIR ’02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 105–112, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gillick</author>
<author>Benoit Favre</author>
</authors>
<title>A scalable global model for summarization.</title>
<date>2009</date>
<booktitle>In ILP ’09: Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing,</booktitle>
<pages>10--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3773" citStr="Gillick and Favre, 2009" startWordPosition="583" endWordPosition="586">s information overlap between an answer and its question, while Novelty was calculated as inverse overlap with all other answers to the same question. A score was assigned to each concept in an answer according to 1http://developer.yahoo.com/answers 760 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 760–769, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics the above properties. A score-maximizing summary under a maximum coverage model was then computed by solving an associated Integer Linear Programming problem (Gillick and Favre, 2009; McDonald, 2007). We chose to express concepts in the form of Basic Elements (BE), a semantic unit developed at ISI2 and modeled semantic overlap as intersection in the equivalence classes of two concepts (formal definitions will be given in section 2.3). The objective of our work was to present what we believe is a valuable conceptual framework; more advance machine learning and summarization techniques would most likely improve the performances. The remaining of this paper is organized as follows. In the next section Quality, Coverage, Relevance and Novelty measures are presented; we explai</context>
<context position="34802" citStr="Gillick and Favre (2009)" startWordPosition="5873" endWordPosition="5876">ity estimation through non-textual features; with the same purpose, more recent methods based on the expertise of answerers are proposed by Suryanto et al. (2009), while Wang et al. (2009b) introduce the idea of ranking answers taking their relation to questions in consideration. The paper that we regard as most authoritative on the matter is the work by Agichtein et al. (2008) which inspired us in the design of the Quality feature space presented in Section 2.1. Our approach merged trustfulness estimation and summarization techniques: we adapted the automatic concept-level model presented by Gillick and Favre (2009) to our needs; related work in multi-document summarization has been carried out by Wang et al. (2008) and McDonald (2007). A relevant selection of approaches that instead make use of ML techniques for query-biased summarization is the following: Wang et al. (2007), Metzler and Kanungo (2008) and Li et al. (2009). An aspect worth investigating is the use of partially labeled or totally unlabeled data for summarization in the work of Wong et al. (2008) and Amini and Gallinari (2002). Our final contribution was to explore the use of Basic Elements document representation instead of the widely us</context>
</contexts>
<marker>Gillick, Favre, 2009</marker>
<rawString>Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In ILP ’09: Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, pages 10–18, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meiqun Hu</author>
<author>Ee-Peng Lim</author>
<author>Aixin Sun</author>
<author>Hady Wirawan Lauw</author>
<author>Ba-Quy Vuong</author>
</authors>
<title>Measuring article quality in wikipedia: models and evaluation.</title>
<date>2007</date>
<booktitle>In CIKM ’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>243--252</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="33978" citStr="Hu et al. (2007)" startWordPosition="5738" endWordPosition="5741">tion Answering, such as Coverage and Novelty. For an investigation of Coverage in the context of Search Engines, refer to Swaminathan et al. (2009). At the core of our work laid information trustfulness, summarization techniques and alternative concept representation. A general approach to the broad problem of evaluating information credibility on the Internet is presented by Akamine et al. (2009) with a system that makes use of semantic-aware Natural Language Preprocessing techniques. With analogous goals, but a focus on UGC, are the papers of Stvilia et al. (2005), Mcguinness et al. (2006), Hu et al. (2007) and 767 Zeng et al. (2006), which present a thorough investigation of Quality and trust in Wikipedia. In the cQA domain, Jeon et al. (2006) presents a framework to use Maximum Entropy for answer quality estimation through non-textual features; with the same purpose, more recent methods based on the expertise of answerers are proposed by Suryanto et al. (2009), while Wang et al. (2009b) introduce the idea of ranking answers taking their relation to questions in consideration. The paper that we regard as most authoritative on the matter is the work by Agichtein et al. (2008) which inspired us i</context>
</contexts>
<marker>Hu, Lim, Sun, Lauw, Vuong, 2007</marker>
<rawString>Meiqun Hu, Ee-Peng Lim, Aixin Sun, Hady Wirawan Lauw, and Ba-Quy Vuong. 2007. Measuring article quality in wikipedia: models and evaluation. In CIKM ’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 243–252, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
<author>Soyeon Park</author>
</authors>
<title>A framework to predict the quality of answers with non-textual features.</title>
<date>2006</date>
<booktitle>In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>228--235</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5296" citStr="Jeon et al., 2006" startWordPosition="830" endWordPosition="833">clusions in Section 6. 2 The summarization framework 2.1 Quality as a ranking problem Quality assessing of information available on Social Media had been studied before mainly as a binary classification problem with the objective of detecting low quality content. We, on the other hand, treated it as a ranking problem and made use of quality estimates with the novel intent of successfully combining information from sources with different levels of trustfulness and writing ability. This is crucial when manipulating UGC, which is known to be subject to particularly great variance in credibility (Jeon et al., 2006; Wang et al., 2009b; Suryanto et al., 2009) and may be poorly written. An answer a was given along with information about the user u that authored it, the set TAq (Total Answers) of all answers to the same question q and the set TA&apos;s of all answers by the same user. Making use of results available in the literature (Agichtein et al., 2008) 3, we designed a Quality 2Information Sciences Institute, University of Southern California, http://www.isi.edu 3A long list of features is proposed; training a classifier on all of them would no doubt increase the performances. feature space to capture the</context>
<context position="34118" citStr="Jeon et al. (2006)" startWordPosition="5764" endWordPosition="5767">l. (2009). At the core of our work laid information trustfulness, summarization techniques and alternative concept representation. A general approach to the broad problem of evaluating information credibility on the Internet is presented by Akamine et al. (2009) with a system that makes use of semantic-aware Natural Language Preprocessing techniques. With analogous goals, but a focus on UGC, are the papers of Stvilia et al. (2005), Mcguinness et al. (2006), Hu et al. (2007) and 767 Zeng et al. (2006), which present a thorough investigation of Quality and trust in Wikipedia. In the cQA domain, Jeon et al. (2006) presents a framework to use Maximum Entropy for answer quality estimation through non-textual features; with the same purpose, more recent methods based on the expertise of answerers are proposed by Suryanto et al. (2009), while Wang et al. (2009b) introduce the idea of ranking answers taking their relation to questions in consideration. The paper that we regard as most authoritative on the matter is the work by Agichtein et al. (2008) which inspired us in the design of the Quality feature space presented in Section 2.1. Our approach merged trustfulness estimation and summarization techniques</context>
</contexts>
<marker>Jeon, Croft, Lee, Park, 2006</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon Park. 2006. A framework to predict the quality of answers with non-textual features. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 228–235, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangda Li</author>
<author>Ke Zhou</author>
<author>Gui-Rong Xue</author>
<author>Hongyuan Zha</author>
<author>Yong Yu</author>
</authors>
<title>Enhancing diversity, coverage and balance for summarization through structure learning.</title>
<date>2009</date>
<booktitle>In WWW ’09: Proceedings of the 18th international conference on World wide web,</booktitle>
<pages>71--80</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="35116" citStr="Li et al. (2009)" startWordPosition="5925" endWordPosition="5928">ative on the matter is the work by Agichtein et al. (2008) which inspired us in the design of the Quality feature space presented in Section 2.1. Our approach merged trustfulness estimation and summarization techniques: we adapted the automatic concept-level model presented by Gillick and Favre (2009) to our needs; related work in multi-document summarization has been carried out by Wang et al. (2008) and McDonald (2007). A relevant selection of approaches that instead make use of ML techniques for query-biased summarization is the following: Wang et al. (2007), Metzler and Kanungo (2008) and Li et al. (2009). An aspect worth investigating is the use of partially labeled or totally unlabeled data for summarization in the work of Wong et al. (2008) and Amini and Gallinari (2002). Our final contribution was to explore the use of Basic Elements document representation instead of the widely used n-gram paradigm: in this regard, we suggest the paper by Zhou et al. (2006). 6 Conclusions We presented a framework to generate trustful, complete, relevant and succinct answers to questions posted by users in cQA portals. We made use of intrinsically available metadata along with concept-level multi-document </context>
</contexts>
<marker>Li, Zhou, Xue, Zha, Yu, 2009</marker>
<rawString>Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha, and Yong Yu. 2009. Enhancing diversity, coverage and balance for summarization through structure learning. In WWW ’09: Proceedings of the 18th international conference on World wide web, pages 71–80, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanjie Liu</author>
<author>Shasha Li</author>
<author>Yunbo Cao</author>
<author>Chin-Yew Lin</author>
<author>Dingyi Han</author>
<author>Yong Yu</author>
</authors>
<title>Understanding and summarizing answers in community-based question answering services.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>497--504</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2052" citStr="Liu et al., 2008" startWordPosition="310" endWordPosition="313">rch engines: overnight, a user can expect a human-crafted, natural language answer tailored to her specific needs. We have to be aware, though, that User Generated Content (UGC) is often redundant, noisy and untrustworthy (Jeon et al., The research was conducted while the first author was visiting Tsinghua University. 2006; Wang et al., 2009b; Suryanto et al., 2009). Interestingly, a great amount of information is embedded in the metadata generated as a byproduct of users’ action and interaction on Social Media. Much valuable information is contained in answers other than the chosen best one (Liu et al., 2008). Our work aims to show that such information can be successfully extracted and made available by exploiting metadata to distill cQA content. To this end, we casted the problem to an instance of the query-biased multi-document summarization task, where the question was seen as a query and the available answers as documents to be summarized. We mapped each characteristic that an ideal answer should present to a measurable property that we wished the final summary could exhibit: • Quality to assess trustfulness in the source, • Coverage to ensure completeness of the information presented, • Rele</context>
<context position="20366" citStr="Liu et al. (2008)" startWordPosition="3456" endWordPosition="3459">s (Section 3.2) were then conducted. The unfiltered version was later reduced to 89,814 question-answer pairs that showed statistical and linguistic properties which made them particularly adequate for our purpose. In particular, trivial, factoid and encyclopedia-answerable questions were 6the version used was lp solve 5.5, available at http: //lpsolve.sourceforge.net/5.5 7The reader is encouraged to contact the authors regarding the availability of data and filters described in this Section. 764 removed by applying a series of patterns for the identification of complex questions. The work by Liu et al. (2008) indicates some categories of questions that are particularly suitable for summarization, but due to the lack of high-performing question classifiers we resorted to human-crafted question patterns. Some pattern examples are the following: • {Why,What is the reason} [...] • How {to,do,does,did} [...] • How {is,are,were,was,will} [...] • How {could,can,would,should} [...] We also removed questions that showed statistical values outside of convenient ranges: the number of answers, length of the longest answer and length of the sum of all answers (both absolute and normalized) were taken in consid</context>
<context position="32989" citStr="Liu et al. (2008)" startWordPosition="5583" endWordPosition="5586">eled by comparison with examples of virtuous behavior (such as Wikipedia’s Featured Articles). The quality of a document could then be estimated as a function of distance from the centroid of the cluster it belongs to. More careful estimates could take the position of other clusters and the concentration of nearby documents in consideration. Finally, in addition to the chosen best answer, a DUC-styled query-focused multi-document summary could be used as a baseline against which the performances of the system can be checked. 5 Related Work A work with a similar objective to our own is that of Liu et al. (2008), where standard multidocument summarization techniques are employed along with taxonomic information about questions. Our approach differs in two fundamental aspects: it took in consideration the peculiarities of the data in input by exploiting the nature of UGC and available metadata; additionally, along with relevance, we addressed challenges that are specific to Question Answering, such as Coverage and Novelty. For an investigation of Coverage in the context of Search Engines, refer to Swaminathan et al. (2009). At the core of our work laid information trustfulness, summarization technique</context>
</contexts>
<marker>Liu, Li, Cao, Lin, Han, Yu, 2008</marker>
<rawString>Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin, Dingyi Han, and Yong Yu. 2008. Understanding and summarizing answers in community-based question answering services. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 497–504, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>4425</volume>
<pages>557--564</pages>
<editor>In Giambattista Amati, Claudio Carpineto, and Giovanni Romano, editors, ECIR,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="3790" citStr="McDonald, 2007" startWordPosition="587" endWordPosition="588">ween an answer and its question, while Novelty was calculated as inverse overlap with all other answers to the same question. A score was assigned to each concept in an answer according to 1http://developer.yahoo.com/answers 760 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 760–769, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics the above properties. A score-maximizing summary under a maximum coverage model was then computed by solving an associated Integer Linear Programming problem (Gillick and Favre, 2009; McDonald, 2007). We chose to express concepts in the form of Basic Elements (BE), a semantic unit developed at ISI2 and modeled semantic overlap as intersection in the equivalence classes of two concepts (formal definitions will be given in section 2.3). The objective of our work was to present what we believe is a valuable conceptual framework; more advance machine learning and summarization techniques would most likely improve the performances. The remaining of this paper is organized as follows. In the next section Quality, Coverage, Relevance and Novelty measures are presented; we explain how they were c</context>
<context position="34924" citStr="McDonald (2007)" startWordPosition="5895" endWordPosition="5896">posed by Suryanto et al. (2009), while Wang et al. (2009b) introduce the idea of ranking answers taking their relation to questions in consideration. The paper that we regard as most authoritative on the matter is the work by Agichtein et al. (2008) which inspired us in the design of the Quality feature space presented in Section 2.1. Our approach merged trustfulness estimation and summarization techniques: we adapted the automatic concept-level model presented by Gillick and Favre (2009) to our needs; related work in multi-document summarization has been carried out by Wang et al. (2008) and McDonald (2007). A relevant selection of approaches that instead make use of ML techniques for query-biased summarization is the following: Wang et al. (2007), Metzler and Kanungo (2008) and Li et al. (2009). An aspect worth investigating is the use of partially labeled or totally unlabeled data for summarization in the work of Wong et al. (2008) and Amini and Gallinari (2002). Our final contribution was to explore the use of Basic Elements document representation instead of the widely used n-gram paradigm: in this regard, we suggest the paper by Zhou et al. (2006). 6 Conclusions We presented a framework to </context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>Ryan T. McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Giambattista Amati, Claudio Carpineto, and Giovanni Romano, editors, ECIR, volume 4425 of Lecture Notes in Computer Science, pages 557–564. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah L Mcguinness</author>
<author>Honglei Zeng</author>
<author>Paulo Pinheiro Da Silva</author>
<author>Li Ding</author>
<author>Dhyanesh Narayanan</author>
<author>Mayukh Bhaowal</author>
</authors>
<title>Investigation into trust for collaborative information repositories: A wikipedia case study.</title>
<date>2006</date>
<booktitle>In In Proceedings of the Workshop on Models of Trust for the Web,</booktitle>
<pages>3--131</pages>
<contexts>
<context position="33960" citStr="Mcguinness et al. (2006)" startWordPosition="5734" endWordPosition="5737"> that are specific to Question Answering, such as Coverage and Novelty. For an investigation of Coverage in the context of Search Engines, refer to Swaminathan et al. (2009). At the core of our work laid information trustfulness, summarization techniques and alternative concept representation. A general approach to the broad problem of evaluating information credibility on the Internet is presented by Akamine et al. (2009) with a system that makes use of semantic-aware Natural Language Preprocessing techniques. With analogous goals, but a focus on UGC, are the papers of Stvilia et al. (2005), Mcguinness et al. (2006), Hu et al. (2007) and 767 Zeng et al. (2006), which present a thorough investigation of Quality and trust in Wikipedia. In the cQA domain, Jeon et al. (2006) presents a framework to use Maximum Entropy for answer quality estimation through non-textual features; with the same purpose, more recent methods based on the expertise of answerers are proposed by Suryanto et al. (2009), while Wang et al. (2009b) introduce the idea of ranking answers taking their relation to questions in consideration. The paper that we regard as most authoritative on the matter is the work by Agichtein et al. (2008) w</context>
</contexts>
<marker>Mcguinness, Zeng, Silva, Ding, Narayanan, Bhaowal, 2006</marker>
<rawString>Deborah L. Mcguinness, Honglei Zeng, Paulo Pinheiro Da Silva, Li Ding, Dhyanesh Narayanan, and Mayukh Bhaowal. 2006. Investigation into trust for collaborative information repositories: A wikipedia case study. In In Proceedings of the Workshop on Models of Trust for the Web, pages 3–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>Tapas Kanungo</author>
</authors>
<title>Machine learned sentence selection strategies for querybiased summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR Learning to Rank Workshop.</booktitle>
<contexts>
<context position="35095" citStr="Metzler and Kanungo (2008)" startWordPosition="5920" endWordPosition="5923">that we regard as most authoritative on the matter is the work by Agichtein et al. (2008) which inspired us in the design of the Quality feature space presented in Section 2.1. Our approach merged trustfulness estimation and summarization techniques: we adapted the automatic concept-level model presented by Gillick and Favre (2009) to our needs; related work in multi-document summarization has been carried out by Wang et al. (2008) and McDonald (2007). A relevant selection of approaches that instead make use of ML techniques for query-biased summarization is the following: Wang et al. (2007), Metzler and Kanungo (2008) and Li et al. (2009). An aspect worth investigating is the use of partially labeled or totally unlabeled data for summarization in the work of Wong et al. (2008) and Amini and Gallinari (2002). Our final contribution was to explore the use of Basic Elements document representation instead of the widely used n-gram paradigm: in this regard, we suggest the paper by Zhou et al. (2006). 6 Conclusions We presented a framework to generate trustful, complete, relevant and succinct answers to questions posted by users in cQA portals. We made use of intrinsically available metadata along with concept-</context>
</contexts>
<marker>Metzler, Kanungo, 2008</marker>
<rawString>Donald Metzler and Tapas Kanungo. 2008. Machine learned sentence selection strategies for querybiased summarization. In Proceedings of SIGIR Learning to Rank Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Besiki Stvilia</author>
<author>Michael B Twidale</author>
<author>Linda C Smith</author>
<author>Les Gasser</author>
</authors>
<title>Assessing information quality of a community-based encyclopedia.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Information Quality.</booktitle>
<contexts>
<context position="33934" citStr="Stvilia et al. (2005)" startWordPosition="5730" endWordPosition="5733">we addressed challenges that are specific to Question Answering, such as Coverage and Novelty. For an investigation of Coverage in the context of Search Engines, refer to Swaminathan et al. (2009). At the core of our work laid information trustfulness, summarization techniques and alternative concept representation. A general approach to the broad problem of evaluating information credibility on the Internet is presented by Akamine et al. (2009) with a system that makes use of semantic-aware Natural Language Preprocessing techniques. With analogous goals, but a focus on UGC, are the papers of Stvilia et al. (2005), Mcguinness et al. (2006), Hu et al. (2007) and 767 Zeng et al. (2006), which present a thorough investigation of Quality and trust in Wikipedia. In the cQA domain, Jeon et al. (2006) presents a framework to use Maximum Entropy for answer quality estimation through non-textual features; with the same purpose, more recent methods based on the expertise of answerers are proposed by Suryanto et al. (2009), while Wang et al. (2009b) introduce the idea of ranking answers taking their relation to questions in consideration. The paper that we regard as most authoritative on the matter is the work by</context>
</contexts>
<marker>Stvilia, Twidale, Smith, Gasser, 2005</marker>
<rawString>Besiki Stvilia, Michael B. Twidale, Linda C. Smith, and Les Gasser. 2005. Assessing information quality of a community-based encyclopedia. In Proceedings of the International Conference on Information Quality.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maggy Anastasia Suryanto</author>
<author>Ee Peng Lim</author>
<author>Aixin Sun</author>
<author>Roger H L Chiang</author>
</authors>
<title>Quality-aware collaborative question answering: methods and evaluation.</title>
<date>2009</date>
<booktitle>In WSDM ’09: Proceedings of the Second ACM International Conference on Web Search and Data Mining,</booktitle>
<pages>142--151</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1803" citStr="Suryanto et al., 2009" startWordPosition="266" endWordPosition="269">tals are an example of Social Media where the information need of a user is expressed in the form of a question for which a best answer is picked among the ones generated by other users. cQA websites are becoming an increasingly popular complement to search engines: overnight, a user can expect a human-crafted, natural language answer tailored to her specific needs. We have to be aware, though, that User Generated Content (UGC) is often redundant, noisy and untrustworthy (Jeon et al., The research was conducted while the first author was visiting Tsinghua University. 2006; Wang et al., 2009b; Suryanto et al., 2009). Interestingly, a great amount of information is embedded in the metadata generated as a byproduct of users’ action and interaction on Social Media. Much valuable information is contained in answers other than the chosen best one (Liu et al., 2008). Our work aims to show that such information can be successfully extracted and made available by exploiting metadata to distill cQA content. To this end, we casted the problem to an instance of the query-biased multi-document summarization task, where the question was seen as a query and the available answers as documents to be summarized. We mappe</context>
<context position="5340" citStr="Suryanto et al., 2009" startWordPosition="838" endWordPosition="841">ion framework 2.1 Quality as a ranking problem Quality assessing of information available on Social Media had been studied before mainly as a binary classification problem with the objective of detecting low quality content. We, on the other hand, treated it as a ranking problem and made use of quality estimates with the novel intent of successfully combining information from sources with different levels of trustfulness and writing ability. This is crucial when manipulating UGC, which is known to be subject to particularly great variance in credibility (Jeon et al., 2006; Wang et al., 2009b; Suryanto et al., 2009) and may be poorly written. An answer a was given along with information about the user u that authored it, the set TAq (Total Answers) of all answers to the same question q and the set TA&apos;s of all answers by the same user. Making use of results available in the literature (Agichtein et al., 2008) 3, we designed a Quality 2Information Sciences Institute, University of Southern California, http://www.isi.edu 3A long list of features is proposed; training a classifier on all of them would no doubt increase the performances. feature space to capture the following syntactic, behavioral and statist</context>
<context position="34340" citStr="Suryanto et al. (2009)" startWordPosition="5799" endWordPosition="5802">ternet is presented by Akamine et al. (2009) with a system that makes use of semantic-aware Natural Language Preprocessing techniques. With analogous goals, but a focus on UGC, are the papers of Stvilia et al. (2005), Mcguinness et al. (2006), Hu et al. (2007) and 767 Zeng et al. (2006), which present a thorough investigation of Quality and trust in Wikipedia. In the cQA domain, Jeon et al. (2006) presents a framework to use Maximum Entropy for answer quality estimation through non-textual features; with the same purpose, more recent methods based on the expertise of answerers are proposed by Suryanto et al. (2009), while Wang et al. (2009b) introduce the idea of ranking answers taking their relation to questions in consideration. The paper that we regard as most authoritative on the matter is the work by Agichtein et al. (2008) which inspired us in the design of the Quality feature space presented in Section 2.1. Our approach merged trustfulness estimation and summarization techniques: we adapted the automatic concept-level model presented by Gillick and Favre (2009) to our needs; related work in multi-document summarization has been carried out by Wang et al. (2008) and McDonald (2007). A relevant sel</context>
</contexts>
<marker>Suryanto, Lim, Sun, Chiang, 2009</marker>
<rawString>Maggy Anastasia Suryanto, Ee Peng Lim, Aixin Sun, and Roger H. L. Chiang. 2009. Quality-aware collaborative question answering: methods and evaluation. In WSDM ’09: Proceedings of the Second ACM International Conference on Web Search and Data Mining, pages 142–151, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashwin Swaminathan</author>
<author>Cherian V Mathew</author>
<author>Darko Kirovski</author>
</authors>
<title>Essential pages.</title>
<date>2009</date>
<booktitle>In WI-IAT ’09: Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology,</booktitle>
<pages>173--182</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="11591" citStr="Swaminathan et al., 2009" startWordPosition="1911" endWordPosition="1914"> and the creation of the equivalence classes. 2.3 Coverage via concept importance In the scenario we proposed, the user’s information need is addressed in the form of a unique, summarized answer; information that is left out of the final summary will simply be unavailable. This raises the concern of completeness: besides ensuring that the information provided could be trusted, we wanted to guarantee that the posed question was being answered thoroughly. We adopted the general definition of Coverage as the portion of relevant information about a certain subject that is contained in a document (Swaminathan et al., 2009). We proceeded by treating each answer to a question q as a separate document and we retrieved through the Yahoo! Answers API a set TKq (Total Knowledge) of 50 answers 5 to questions similar to q: the knowledge space of TKq was chosen to approximate the entire knowledge space related to the queried question q. We calculated Coverage as a function of the portion of answers in TKq that presented semantic overlap with a. 4The authors can be contacted regarding the possibility of sharing the code of the modified version. Original version available from http://www.isi.edu/ publications/licensed-sw/</context>
<context position="13045" citStr="Swaminathan et al. (2009)" startWordPosition="2174" endWordPosition="2177">was calculated as the sum of term frequency tf(ci, a) for concepts in the answer itself, weighted by a concept importance function, γ(ci), for concepts in the total knowledge space TKq. γ(c) was defined as follows: γ(c) = |TKq,c ||T Kq| |TKq |· log2 |T Kq,c |(3) where TKq,c = {d ∈ TKq : ∃k ∈ d, k ./ c} The function γ(c) of concept c was calculated as a function of the cardinality of set TKq and set TKq,c, which was the subset of all those answers d that contained at least one concept k which presented semantical overlap with c itself. A similar idea of knowledge space coverage is addressed by Swaminathan et al. (2009), from which formulas (2) and (3) were derived. A sensible alternative would be to estimate Coverage at the sentence level. 2.4 Relevance and Novelty via ./ relation To this point, we have addressed matters of trustfulness and completeness. Another widely shared concern for Information Retrieval systems is Relevance to the query. We calculated relevance by computing the semantic overlap between concepts in the answers and the question. Intuitively, we reward concepts that express meaning that could be found in the question to be answered. R(c, q) = |qc |(4) |q| where qc = {k ∈ q : k ./ c} The </context>
<context position="33509" citStr="Swaminathan et al. (2009)" startWordPosition="5663" endWordPosition="5667">em can be checked. 5 Related Work A work with a similar objective to our own is that of Liu et al. (2008), where standard multidocument summarization techniques are employed along with taxonomic information about questions. Our approach differs in two fundamental aspects: it took in consideration the peculiarities of the data in input by exploiting the nature of UGC and available metadata; additionally, along with relevance, we addressed challenges that are specific to Question Answering, such as Coverage and Novelty. For an investigation of Coverage in the context of Search Engines, refer to Swaminathan et al. (2009). At the core of our work laid information trustfulness, summarization techniques and alternative concept representation. A general approach to the broad problem of evaluating information credibility on the Internet is presented by Akamine et al. (2009) with a system that makes use of semantic-aware Natural Language Preprocessing techniques. With analogous goals, but a focus on UGC, are the papers of Stvilia et al. (2005), Mcguinness et al. (2006), Hu et al. (2007) and 767 Zeng et al. (2006), which present a thorough investigation of Quality and trust in Wikipedia. In the cQA domain, Jeon et a</context>
</contexts>
<marker>Swaminathan, Mathew, Kirovski, 2009</marker>
<rawString>Ashwin Swaminathan, Cherian V. Mathew, and Darko Kirovski. 2009. Essential pages. In WI-IAT ’09: Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, pages 173–182, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Changhu Wang</author>
<author>Feng Jing</author>
<author>Lei Zhang</author>
<author>HongJiang Zhang</author>
</authors>
<title>Learning query-biased web page summarization.</title>
<date>2007</date>
<booktitle>In CIKM ’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,</booktitle>
<pages>555--562</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="35067" citStr="Wang et al. (2007)" startWordPosition="5916" endWordPosition="5919">deration. The paper that we regard as most authoritative on the matter is the work by Agichtein et al. (2008) which inspired us in the design of the Quality feature space presented in Section 2.1. Our approach merged trustfulness estimation and summarization techniques: we adapted the automatic concept-level model presented by Gillick and Favre (2009) to our needs; related work in multi-document summarization has been carried out by Wang et al. (2008) and McDonald (2007). A relevant selection of approaches that instead make use of ML techniques for query-biased summarization is the following: Wang et al. (2007), Metzler and Kanungo (2008) and Li et al. (2009). An aspect worth investigating is the use of partially labeled or totally unlabeled data for summarization in the work of Wong et al. (2008) and Amini and Gallinari (2002). Our final contribution was to explore the use of Basic Elements document representation instead of the widely used n-gram paradigm: in this regard, we suggest the paper by Zhou et al. (2006). 6 Conclusions We presented a framework to generate trustful, complete, relevant and succinct answers to questions posted by users in cQA portals. We made use of intrinsically available </context>
</contexts>
<marker>Wang, Jing, Zhang, Zhang, 2007</marker>
<rawString>Changhu Wang, Feng Jing, Lei Zhang, and HongJiang Zhang. 2007. Learning query-biased web page summarization. In CIKM ’07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 555– 562, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dingding Wang</author>
<author>Tao Li</author>
<author>Shenghuo Zhu</author>
<author>Chris Ding</author>
</authors>
<title>Multi-document summarization via sentence-level semantic analysis and symmetric matrix factorization.</title>
<date>2008</date>
<booktitle>In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>307--314</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="34904" citStr="Wang et al. (2008)" startWordPosition="5890" endWordPosition="5893">se of answerers are proposed by Suryanto et al. (2009), while Wang et al. (2009b) introduce the idea of ranking answers taking their relation to questions in consideration. The paper that we regard as most authoritative on the matter is the work by Agichtein et al. (2008) which inspired us in the design of the Quality feature space presented in Section 2.1. Our approach merged trustfulness estimation and summarization techniques: we adapted the automatic concept-level model presented by Gillick and Favre (2009) to our needs; related work in multi-document summarization has been carried out by Wang et al. (2008) and McDonald (2007). A relevant selection of approaches that instead make use of ML techniques for query-biased summarization is the following: Wang et al. (2007), Metzler and Kanungo (2008) and Li et al. (2009). An aspect worth investigating is the use of partially labeled or totally unlabeled data for summarization in the work of Wong et al. (2008) and Amini and Gallinari (2002). Our final contribution was to explore the use of Basic Elements document representation instead of the widely used n-gram paradigm: in this regard, we suggest the paper by Zhou et al. (2006). 6 Conclusions We prese</context>
</contexts>
<marker>Wang, Li, Zhu, Ding, 2008</marker>
<rawString>Dingding Wang, Tao Li, Shenghuo Zhu, and Chris Ding. 2008. Multi-document summarization via sentence-level semantic analysis and symmetric matrix factorization. In SIGIR ’08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 307–314, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Wang</author>
<author>Zhaoyan Ming</author>
<author>Tat-Seng Chua</author>
</authors>
<title>A syntactic tree matching approach to finding similar questions in community-based qa services.</title>
<date>2009</date>
<booktitle>In SIGIR ’09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>187--194</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1778" citStr="Wang et al., 2009" startWordPosition="262" endWordPosition="265"> Answering (cQA) portals are an example of Social Media where the information need of a user is expressed in the form of a question for which a best answer is picked among the ones generated by other users. cQA websites are becoming an increasingly popular complement to search engines: overnight, a user can expect a human-crafted, natural language answer tailored to her specific needs. We have to be aware, though, that User Generated Content (UGC) is often redundant, noisy and untrustworthy (Jeon et al., The research was conducted while the first author was visiting Tsinghua University. 2006; Wang et al., 2009b; Suryanto et al., 2009). Interestingly, a great amount of information is embedded in the metadata generated as a byproduct of users’ action and interaction on Social Media. Much valuable information is contained in answers other than the chosen best one (Liu et al., 2008). Our work aims to show that such information can be successfully extracted and made available by exploiting metadata to distill cQA content. To this end, we casted the problem to an instance of the query-biased multi-document summarization task, where the question was seen as a query and the available answers as documents t</context>
<context position="5315" citStr="Wang et al., 2009" startWordPosition="834" endWordPosition="837"> 6. 2 The summarization framework 2.1 Quality as a ranking problem Quality assessing of information available on Social Media had been studied before mainly as a binary classification problem with the objective of detecting low quality content. We, on the other hand, treated it as a ranking problem and made use of quality estimates with the novel intent of successfully combining information from sources with different levels of trustfulness and writing ability. This is crucial when manipulating UGC, which is known to be subject to particularly great variance in credibility (Jeon et al., 2006; Wang et al., 2009b; Suryanto et al., 2009) and may be poorly written. An answer a was given along with information about the user u that authored it, the set TAq (Total Answers) of all answers to the same question q and the set TA&apos;s of all answers by the same user. Making use of results available in the literature (Agichtein et al., 2008) 3, we designed a Quality 2Information Sciences Institute, University of Southern California, http://www.isi.edu 3A long list of features is proposed; training a classifier on all of them would no doubt increase the performances. feature space to capture the following syntacti</context>
<context position="31480" citStr="Wang et al. (2009" startWordPosition="5328" endWordPosition="5331">n those cases, the summarized answer might lack logical consistency. the total knowledge available about q, a coverage estimate of the final answers against it would have been ideal. Unfortunately the lack of metadata about those answers prevented us from proceeding in that direction. This consideration suggests the idea of building TKq using similar answers in the dataset itself, for which metadata is indeed available. Furthermore, similar questions in the dataset could have been used to augment the set of answers used to generate the final summary with answers coming from similar questions. Wang et al. (2009a) presents a method to retrieve similar questions that could be worth taking in consideration for the task. We suggest that the retrieval method could be made Quality-aware. A Quality feature space for questions is presented by Agichtein et al. (2008) and could be used to rank the quality of questions in a way similar to how we ranked the quality of answers. The Quality assessing component itself could be built as a module that can be adjusted to the kind of Social Media in use; the creation of customized Quality feature spaces would make it possible to handle different sources of UGC (forums</context>
<context position="34365" citStr="Wang et al. (2009" startWordPosition="5804" endWordPosition="5807"> et al. (2009) with a system that makes use of semantic-aware Natural Language Preprocessing techniques. With analogous goals, but a focus on UGC, are the papers of Stvilia et al. (2005), Mcguinness et al. (2006), Hu et al. (2007) and 767 Zeng et al. (2006), which present a thorough investigation of Quality and trust in Wikipedia. In the cQA domain, Jeon et al. (2006) presents a framework to use Maximum Entropy for answer quality estimation through non-textual features; with the same purpose, more recent methods based on the expertise of answerers are proposed by Suryanto et al. (2009), while Wang et al. (2009b) introduce the idea of ranking answers taking their relation to questions in consideration. The paper that we regard as most authoritative on the matter is the work by Agichtein et al. (2008) which inspired us in the design of the Quality feature space presented in Section 2.1. Our approach merged trustfulness estimation and summarization techniques: we adapted the automatic concept-level model presented by Gillick and Favre (2009) to our needs; related work in multi-document summarization has been carried out by Wang et al. (2008) and McDonald (2007). A relevant selection of approaches that</context>
</contexts>
<marker>Wang, Ming, Chua, 2009</marker>
<rawString>Kai Wang, Zhaoyan Ming, and Tat-Seng Chua. 2009a. A syntactic tree matching approach to finding similar questions in community-based qa services. In SIGIR ’09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 187–194, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin-Jing Wang</author>
<author>Xudong Tu</author>
<author>Dan Feng</author>
<author>Lei Zhang</author>
</authors>
<title>Ranking community answers by modeling question-answer relationships via analogical reasoning.</title>
<date>2009</date>
<booktitle>In SIGIR ’09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>179--186</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1778" citStr="Wang et al., 2009" startWordPosition="262" endWordPosition="265"> Answering (cQA) portals are an example of Social Media where the information need of a user is expressed in the form of a question for which a best answer is picked among the ones generated by other users. cQA websites are becoming an increasingly popular complement to search engines: overnight, a user can expect a human-crafted, natural language answer tailored to her specific needs. We have to be aware, though, that User Generated Content (UGC) is often redundant, noisy and untrustworthy (Jeon et al., The research was conducted while the first author was visiting Tsinghua University. 2006; Wang et al., 2009b; Suryanto et al., 2009). Interestingly, a great amount of information is embedded in the metadata generated as a byproduct of users’ action and interaction on Social Media. Much valuable information is contained in answers other than the chosen best one (Liu et al., 2008). Our work aims to show that such information can be successfully extracted and made available by exploiting metadata to distill cQA content. To this end, we casted the problem to an instance of the query-biased multi-document summarization task, where the question was seen as a query and the available answers as documents t</context>
<context position="5315" citStr="Wang et al., 2009" startWordPosition="834" endWordPosition="837"> 6. 2 The summarization framework 2.1 Quality as a ranking problem Quality assessing of information available on Social Media had been studied before mainly as a binary classification problem with the objective of detecting low quality content. We, on the other hand, treated it as a ranking problem and made use of quality estimates with the novel intent of successfully combining information from sources with different levels of trustfulness and writing ability. This is crucial when manipulating UGC, which is known to be subject to particularly great variance in credibility (Jeon et al., 2006; Wang et al., 2009b; Suryanto et al., 2009) and may be poorly written. An answer a was given along with information about the user u that authored it, the set TAq (Total Answers) of all answers to the same question q and the set TA&apos;s of all answers by the same user. Making use of results available in the literature (Agichtein et al., 2008) 3, we designed a Quality 2Information Sciences Institute, University of Southern California, http://www.isi.edu 3A long list of features is proposed; training a classifier on all of them would no doubt increase the performances. feature space to capture the following syntacti</context>
<context position="31480" citStr="Wang et al. (2009" startWordPosition="5328" endWordPosition="5331">n those cases, the summarized answer might lack logical consistency. the total knowledge available about q, a coverage estimate of the final answers against it would have been ideal. Unfortunately the lack of metadata about those answers prevented us from proceeding in that direction. This consideration suggests the idea of building TKq using similar answers in the dataset itself, for which metadata is indeed available. Furthermore, similar questions in the dataset could have been used to augment the set of answers used to generate the final summary with answers coming from similar questions. Wang et al. (2009a) presents a method to retrieve similar questions that could be worth taking in consideration for the task. We suggest that the retrieval method could be made Quality-aware. A Quality feature space for questions is presented by Agichtein et al. (2008) and could be used to rank the quality of questions in a way similar to how we ranked the quality of answers. The Quality assessing component itself could be built as a module that can be adjusted to the kind of Social Media in use; the creation of customized Quality feature spaces would make it possible to handle different sources of UGC (forums</context>
<context position="34365" citStr="Wang et al. (2009" startWordPosition="5804" endWordPosition="5807"> et al. (2009) with a system that makes use of semantic-aware Natural Language Preprocessing techniques. With analogous goals, but a focus on UGC, are the papers of Stvilia et al. (2005), Mcguinness et al. (2006), Hu et al. (2007) and 767 Zeng et al. (2006), which present a thorough investigation of Quality and trust in Wikipedia. In the cQA domain, Jeon et al. (2006) presents a framework to use Maximum Entropy for answer quality estimation through non-textual features; with the same purpose, more recent methods based on the expertise of answerers are proposed by Suryanto et al. (2009), while Wang et al. (2009b) introduce the idea of ranking answers taking their relation to questions in consideration. The paper that we regard as most authoritative on the matter is the work by Agichtein et al. (2008) which inspired us in the design of the Quality feature space presented in Section 2.1. Our approach merged trustfulness estimation and summarization techniques: we adapted the automatic concept-level model presented by Gillick and Favre (2009) to our needs; related work in multi-document summarization has been carried out by Wang et al. (2008) and McDonald (2007). A relevant selection of approaches that</context>
</contexts>
<marker>Wang, Tu, Feng, Zhang, 2009</marker>
<rawString>Xin-Jing Wang, Xudong Tu, Dan Feng, and Lei Zhang. 2009b. Ranking community answers by modeling question-answer relationships via analogical reasoning. In SIGIR ’09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 179–186, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kam-Fai Wong</author>
<author>Mingli Wu</author>
<author>Wenjie Li</author>
</authors>
<title>Extractive summarization using supervised and semisupervised learning.</title>
<date>2008</date>
<booktitle>In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>985--992</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="35257" citStr="Wong et al. (2008)" startWordPosition="5951" endWordPosition="5954">n 2.1. Our approach merged trustfulness estimation and summarization techniques: we adapted the automatic concept-level model presented by Gillick and Favre (2009) to our needs; related work in multi-document summarization has been carried out by Wang et al. (2008) and McDonald (2007). A relevant selection of approaches that instead make use of ML techniques for query-biased summarization is the following: Wang et al. (2007), Metzler and Kanungo (2008) and Li et al. (2009). An aspect worth investigating is the use of partially labeled or totally unlabeled data for summarization in the work of Wong et al. (2008) and Amini and Gallinari (2002). Our final contribution was to explore the use of Basic Elements document representation instead of the widely used n-gram paradigm: in this regard, we suggest the paper by Zhou et al. (2006). 6 Conclusions We presented a framework to generate trustful, complete, relevant and succinct answers to questions posted by users in cQA portals. We made use of intrinsically available metadata along with concept-level multi-document summarization techniques. Furthermore, we proposed an original use for the BE representation of concepts and tested two concept-scoring funct</context>
</contexts>
<marker>Wong, Wu, Li, 2008</marker>
<rawString>Kam-Fai Wong, Mingli Wu, and Wenjie Li. 2008. Extractive summarization using supervised and semisupervised learning. In COLING ’08: Proceedings of the 22nd International Conference on Computational Linguistics, pages 985–992, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglei Zeng</author>
<author>Maher A Alhossaini</author>
<author>Li Ding</author>
<author>Richard Fikes</author>
<author>Deborah L McGuinness</author>
</authors>
<title>Computing trust from revision history.</title>
<date>2006</date>
<booktitle>In PST ’06: Proceedings of the 2006 International Conference on Privacy, Security and Trust,</booktitle>
<pages>1--1</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="34005" citStr="Zeng et al. (2006)" startWordPosition="5744" endWordPosition="5747">overage and Novelty. For an investigation of Coverage in the context of Search Engines, refer to Swaminathan et al. (2009). At the core of our work laid information trustfulness, summarization techniques and alternative concept representation. A general approach to the broad problem of evaluating information credibility on the Internet is presented by Akamine et al. (2009) with a system that makes use of semantic-aware Natural Language Preprocessing techniques. With analogous goals, but a focus on UGC, are the papers of Stvilia et al. (2005), Mcguinness et al. (2006), Hu et al. (2007) and 767 Zeng et al. (2006), which present a thorough investigation of Quality and trust in Wikipedia. In the cQA domain, Jeon et al. (2006) presents a framework to use Maximum Entropy for answer quality estimation through non-textual features; with the same purpose, more recent methods based on the expertise of answerers are proposed by Suryanto et al. (2009), while Wang et al. (2009b) introduce the idea of ranking answers taking their relation to questions in consideration. The paper that we regard as most authoritative on the matter is the work by Agichtein et al. (2008) which inspired us in the design of the Quality</context>
</contexts>
<marker>Zeng, Alhossaini, Ding, Fikes, McGuinness, 2006</marker>
<rawString>Honglei Zeng, Maher A. Alhossaini, Li Ding, Richard Fikes, and Deborah L. McGuinness. 2006. Computing trust from revision history. In PST ’06: Proceedings of the 2006 International Conference on Privacy, Security and Trust, pages 1–1, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Chin Y Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Summarizing answers for complicated questions.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="8281" citStr="Zhou et al., 2006" startWordPosition="1359" endWordPosition="1362"> alternative, completely unsupervised approach to quality detection that has not undergone experimental analysis is discussed in Section 4. 761 2.2 Bag-of-BEs and semantic overlap The properties that remain to be discussed, namely Coverage, Relevance and Novelty, are measures of semantic overlap between concepts; a concept is the smallest unit of meaning in a portion of written text. To represent sentences and answers we adopted an alternative approach to classical ngrams that could be defined bag-of-BEs. a BE is “a head|modifier|relation triple representation of a document developed at ISI” (Zhou et al., 2006). BEs are a strong theoretical instrument to tackle the ambiguity inherent in natural language that find successful practical applications in realworld query-based summarization systems. Different from n-grams, they are variant in length and depend on parsing techniques, named entity detection, part-of-speech tagging and resolution of syntactic forms such as hyponyms, pronouns, pertainyms, abbreviation and synonyms. To each BE is associated a class of semantically equivalent BEs as result of what is called a transformation of the original BE; the mentioned class uniquely defines the concept. W</context>
<context position="35480" citStr="Zhou et al. (2006)" startWordPosition="5989" endWordPosition="5992">has been carried out by Wang et al. (2008) and McDonald (2007). A relevant selection of approaches that instead make use of ML techniques for query-biased summarization is the following: Wang et al. (2007), Metzler and Kanungo (2008) and Li et al. (2009). An aspect worth investigating is the use of partially labeled or totally unlabeled data for summarization in the work of Wong et al. (2008) and Amini and Gallinari (2002). Our final contribution was to explore the use of Basic Elements document representation instead of the widely used n-gram paradigm: in this regard, we suggest the paper by Zhou et al. (2006). 6 Conclusions We presented a framework to generate trustful, complete, relevant and succinct answers to questions posted by users in cQA portals. We made use of intrinsically available metadata along with concept-level multi-document summarization techniques. Furthermore, we proposed an original use for the BE representation of concepts and tested two concept-scoring functions to combine Quality, Coverage, Relevance and Novelty measures. Evaluation results on human annotated data showed that our summarized answers constitute a solid complement to best answers voted by the cQA users. We are i</context>
</contexts>
<marker>Zhou, Lin, Hovy, 2006</marker>
<rawString>Liang Zhou, Chin Y. Lin, and Eduard Hovy. 2006. Summarizing answers for complicated questions. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC), Genoa, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>