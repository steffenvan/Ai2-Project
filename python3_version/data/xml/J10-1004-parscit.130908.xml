<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993679">
The Noisy Channel Model for Unsupervised
Word Sense Disambiguation
</title>
<author confidence="0.996094">
Deniz Yuret*
</author>
<affiliation confidence="0.954132">
Koc¸ University
</affiliation>
<author confidence="0.756045">
Mehmet Ali Yatbaz
</author>
<affiliation confidence="0.652619">
Koc¸ University
</affiliation>
<bodyText confidence="0.997630615384615">
We introduce a generative probabilistic model, the noisy channel model, for unsupervised word
sense disambiguation. In our model, each context C is modeled as a distinct channel through
which the speaker intends to transmit a particular meaning S using a possibly ambiguous word
W. To reconstruct the intended meaning the hearer uses the distribution of possible meanings
in the given context P(S|C) and possible words that can express each meaning P(W|S). We
assume P(W|S) is independent of the context and estimate it using WordNet sense frequencies.
The main problem of unsupervised WSD is estimating context-dependent P(S|C) without access
to any sense-tagged text. We show one way to solve this problem using a statistical language
model based on large amounts of untagged text. Our model uses coarse-grained semantic classes
for S internally and we explore the effect of using different levels of granularity on WSD per-
formance. The system outputs fine-grained senses for evaluation, and its performance on noun
disambiguation is better than most previously reported unsupervised systems and close to the
best supervised systems.
</bodyText>
<sectionHeader confidence="0.985219" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999734769230769">
Word sense disambiguation (WSD) is the task of identifying the correct sense of an
ambiguous word in a given context. An accurate WSD system would benefit appli-
cations such as machine translation and information retrieval. The most successful
WSD systems to date are based on supervised learning and trained on sense-tagged
corpora. In this article we present an unsupervised WSD algorithm that can leverage
untagged text and can perform at the level of the best supervised systems for the all-
nouns disambiguation task.
The main drawback of the supervised approach is the difficulty of acquiring
considerable amounts of training data, also known as the knowledge acquisition
bottleneck. Yarowsky and Florian (2002) report that each successive doubling of the
training data for WSD only leads to a 3–4% error reduction within their experimental
range. Banko and Brill (2001) experiment with the problem of selection among
confusable words and show that the learning curves do not converge even after
</bodyText>
<note confidence="0.81646">
* Koc¸ University, Department of Computer Engineering, 34450 Sarıyer, ˙Istanbul, Turkey.
</note>
<email confidence="0.959186">
E-mail: dyuret@ku.edu.tr, myatbaz@ku.edu.tr.
</email>
<note confidence="0.90469">
Submission received: 7 October 2008; revised submission received: 17 April 2009; accepted for publication:
12 September 2009.
© 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 1
</note>
<bodyText confidence="0.9998238">
a billion words of training data. They suggest unsupervised, semi-supervised, or
active learning to take advantage of large data sets when labeling is expensive. Yuret
(2004) observes that in a supervised naive Bayes WSD system trained on SemCor,
approximately half of the test instances do not contain any of the contextual features
(e.g., neighboring content words or local collocation patterns) observed in the training
data. SemCor is the largest publicly available corpus of sense-tagged text, and has only
about a quarter million sense-tagged words. In contrast, our unsupervised system uses
the Web1T data set (Brants and Franz 2006) for unlabeled examples, which contains
counts from a 1012 word corpus derived from publicly-available Web pages.
A note on the term “unsupervised” may be appropriate here. In the WSD literature
“unsupervised” is typically used to describe systems that do not directly use sense-
tagged corpora for training. However, many of these unsupervised systems, including
ours, use sense ordering or sense frequencies from WordNet (Fellbaum 1998) or other
dictionaries. Thus it might be more appropriate to call them weakly supervised or
semi-supervised. More specifically, context–sense pairs or context–word–sense triples
are not observed in the training data, but context-word frequencies (from untagged
text) and word-sense frequencies (from dictionaries or other sources) are used in model
building. One of the main problems we explore in this study is the estimation of context-
dependent sense probabilities when no context–sense pairs have been observed in the
training data.
The first contribution of this article is a probabilistic generative model for word
sense disambiguation that seamlessly integrates unlabeled text data into the model
building process. Our approach is based on the noisy channel model (Shannon 1948),
which has been an essential ingredient in fields such as speech recognition and machine
translation. In this study we demonstrate that the noisy channel model can also be the
key component for unsupervised word sense disambiguation, provided we can solve
the context-dependent sense distribution problem. In Section 2.1 we show one way
to estimate the context-dependent sense distribution without using any sense-tagged
data. Section 2.2 outlines the complete unsupervised WSD algorithm using this model.
We estimate the distribution of coarse-grained semantic classes rather than fine-grained
senses. The solution uses the two distributions for which we do have data: the distribu-
tion of words used to express a given sense, and the distribution of words that appear
in a given context. The first can be estimated using WordNet sense frequencies, and the
second can be estimated using an n-gram language model as described in Section 2.3.
The second contribution of this article is an exploration of semantic classes at differ-
ent levels of granularity for word sense disambiguation. Using fine-grained senses for
model building is inefficient both computationally and from a learning perspective. The
noisy channel model can take advantage of the close distribution of similar senses if they
are grouped into semantic classes. We take semantic classes to be groups of WordNet
synsets defined using the hypernym hierarchy. In each experiment we designate a
number of synsets high in the WordNet hypernym hierarchy as “head synsets” and
use their descendants to partition the senses into separate semantic classes. In Section 3
we present performance bounds for such class-based WSD and describe our method of
exploring the different levels of granularity.
In Section 4 we report on our actual experiments and compare our results with
the best supervised and unsupervised systems from SensEval-2 (Cotton et al. 2001),
SensEval-3 (Mihalcea and Edmonds 2004), and SemEval-2007 (Agirre, M`arquez, and
Wicentowski 2007). Section 5 discusses these results and the idiosyncrasies of the
data sets, baselines, and evaluation metrics used. Section 6 presents related work, and
Section 7 summarizes our contributions.
</bodyText>
<page confidence="0.991998">
112
</page>
<note confidence="0.818374">
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
</note>
<sectionHeader confidence="0.614785" genericHeader="keywords">
2. The Noisy Channel Model for WSD
2.1 Model
</sectionHeader>
<bodyText confidence="0.999932923076923">
The noisy channel model has been the foundation of standard models in speech recog-
nition (Bahl, Jelinek, and Mercer 1983) and machine translation (Brown et al. 1990).
In this article we explore its application to WSD. The noisy channel model can be
used whenever a signal received does not uniquely identify the message being sent.
Bayes’ Law is used to interpret the ambiguous signal and identify the most probable
intended message. In WSD, we model each context as a distinct channel where the
intended message is a word sense (or semantic class) S, and the signal received is an
ambiguous word W. In this section we will describe how to model a given context C as
a noisy channel, and in particular how to estimate the context-specific sense distribution
without using any sense-tagged data.
Equation (1) expresses the probability of a sense S of word W in a given context C.
This is the well-known Bayes’ formula with an extra P(.|C) in each term indicating the
dependence on the context.
</bodyText>
<equation confidence="0.999136">
P(W|S,C)P(S|C)
P(S|W,C) = (1)
P(W|C)
</equation>
<bodyText confidence="0.9992036">
To perform WSD we need to find the sense S that maximizes the probability P(S|W, C).
This is equivalent to the maximization of the product P(W|S,C)P(S|C) because the
denominator P(W|C) does not depend on S. To perform the maximization, the two
distributions P(W|S, C) and P(S|C) need to be estimated for each context C.
The main challenge is to estimate P(S|C), the distribution of word senses that can
be expressed in the given context. In unsupervised WSD we do not have access to any
sense-tagged data, thus we do not know what senses are likely to be expressed in any
given context. Therefore it is not possible to estimate P(S|C) directly.
What we do have is the word frequencies for each sense P(W|S), and the word
frequencies for the given context P(W|C). We use the WordNet sense frequencies to
estimate P(W|S) and a statistical language model to estimate P(W|C) as detailed in
Section 2.3. We make the independence assumption P(W|S, C) = P(W|S), that is, the
distribution of words used to express a particular sense is the same for all contexts.
Finally, the relationship between the three distributions, P(S|C), P(W|S, C), and P(W|C)
is given by the total probability theorem:
</bodyText>
<equation confidence="0.99421">
P(W|C) = � P(S|C)P(W|S, C) (2)
S
</equation>
<bodyText confidence="0.7981705">
We can solve for P(S|C) using linear algebra. Let WS be a matrix, s and w two vectors
such that:
</bodyText>
<equation confidence="0.999789333333333">
WSij= P(W = i|S = j)
sj = P(S = j|C = k)
wi = P(W = i|C = k) (3)
</equation>
<page confidence="0.993083">
113
</page>
<note confidence="0.266481">
Computational Linguistics Volume 36, Number 1
</note>
<bodyText confidence="0.75611775">
Using this new form, we can see that Equation (2) is equivalent to the linear equation
w = WS x s and s can be solved using a linear solver. Typically WS is a tall matrix and
the system has no exact solutions. We use the Moore–Penrose pseudoinverse WS+ to
compute an approximate solution:
</bodyText>
<equation confidence="0.984902">
s = WS+ x w (4)
</equation>
<bodyText confidence="0.98771325">
Appendix A discusses possible scaling issues of this solution and offers alternative
solutions. We use the pseudoinverse solution in all our experiments because it can be
computed fast and none of the alternatives we tried made a significant difference in
WSD performance.
</bodyText>
<subsectionHeader confidence="0.97917">
2.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.95973175">
Section 2.1 described how to apply the noisy channel model for WSD in a single context.
In this section we present the steps we follow in our experiments to simultaneously
apply the noisy channel model to all the contexts in a given word sense disambiguation
task.
</bodyText>
<listItem confidence="0.975748923076923">
Algorithm 1
1. Let W be the vocabulary. In this study we took the vocabulary to be the
approximately 12,000 nouns in WordNet that have non-zero sense
frequencies.
2. Let S be the set of senses or semantic classes to be used. In this study we
used various partitions of noun synsets as semantic classes.
3. Let C be the set of contexts (nine-word windows for a 5-gram model)
surrounding each target word in the given WSD task.
4. Compute the matrix WC where WCik = P(W = i|C = k). Here i ranges
over the vocabulary W and k ranges over the contexts C. This matrix
concatenates the (w) word distribution vectors from Equation (4) for each
context. The entries of the matrix are computed using the n-gram language
model described in Section 2.3. This is the most expensive step in the
algorithm (see Appendix B for a discussion of implementation efficiency).
5. Compute the matrix WS where WSij = P(W = i|S = j). Here i ranges over
the vocabulary W and j ranges over the semantic classes S. The entries of
the matrix are computed using the WordNet sense frequencies.
6. Compute the matrix SC = WS+ x WC where SCjk = P(S = j|C = k).
Here j ranges over the semantic classes S and k ranges over the contexts C.
This step computes the pseudoinverse solution described in Section 2.1
simultaneously for all the contexts, and the resulting SC matrix is a
concatenation of the (s) solution vectors from Equation (4) for each context.
WS+ is the pseudoinverse of the matrix WS.
7. Compute the best semantic class for each WSD instance by using
argmaxSP(S|W, C) a P(W|S)P(S|C). Here P(S|C) comes from the column
of the SC matrix that corresponds to the context of the WSD instance
</listItem>
<page confidence="0.976625">
114
</page>
<bodyText confidence="0.359878">
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
</bodyText>
<listItem confidence="0.9224855">
and P(W|S) comes from the row of the WS matrix that corresponds to the
word to be disambiguated.
8. Compute the fine-grained answer for each WSD instance by taking the
most frequent (lowest numbered) sense in the chosen semantic class.
9. Apply the one sense per discourse heuristic: If a word is found to have
multiple senses in a document, replace them with the majority answer.
</listItem>
<subsectionHeader confidence="0.995339">
2.3 Estimation Procedure
</subsectionHeader>
<bodyText confidence="0.999087642857143">
In Section 2.1, we showed how the unsupervised WSD problem expressed as a noisy
channel model can be decomposed into the estimation of two distributions: P(W|S) and
P(W|C). In this section we detail our estimation procedure for these two distributions.
To estimate P(W|S), the distribution of words that can be used to express a given
meaning, we used the WordNet sense frequencies.1 We did not perform any smoothing
for the zero counts and used the maximum likelihood estimate: count(W,S)/count(S).
As described in later sections, we also experimented with grouping similar WordNet
senses into semantic classes. In this case S stands for the semantic class, and the counts
from various senses of a word in the same semantic class are added together to estimate
P(W|S).
To estimate the distribution of words in a given context, P(W|C), we used a 5-gram
language model. We define the context as the nine-word window centered on the target
word w1w2 ... w9, where W = w5. The probability of a word in the given context can be
expressed as:
</bodyText>
<equation confidence="0.9999045">
P(W = w5) ∝ P(w1 ...w9) (5)
= P(w1)P(w2|w1) ... P(w9|w1 ... w8) (6)
∝ P(w5|w1 ... w4)P(w6|w2 ... w5)P(w7|w3 ... w6) (7)
P(w8|w4 ... w7)P(w9|w5 ... w8)
</equation>
<bodyText confidence="0.990471411764706">
Equation (5) indicates that P(W|C) is proportional to P(w1 ... w9) because the other
words in the context are fixed for a given WSD instance. Equation (6) is the standard
decomposition of the probability of a word sequence into conditional probabilities.
The first four terms do not include the target word w5, and have been dropped in Equa-
tion (7). We also truncate the remaining conditionals to four words reflecting the Markov
assumption of the 5-gram model. Finally, using an expression that is proportional to
P(W|C) instead of P(W|C) itself will not change the WSD result because we are taking
the argmax in Equation (1).
Each term on the right hand side of Equation (7) is estimated using a 5-gram
language model. To get accurate domain-independent probability estimates we used the
Web 1T data set (Brants and Franz 2006), which contains the counts of word sequences
up to length five in a 1012 word corpus derived from publicly-accessible Web pages.
Estimation of P(W|C) is the most computationally expensive step of the algorithm, and
some implementation details are given in Appendix B.
1 The sense frequencies were obtained from the index.sense file included in the WordNet distribution.
We had to correct the counts of three words (person, group, and location) whose WordNet counts
unfortunately include the corresponding named entities and are thus inflated.
</bodyText>
<page confidence="0.985568">
115
</page>
<figure confidence="0.894388">
Computational Linguistics Volume 36, Number 1
</figure>
<figureCaption confidence="0.968636">
Figure 1
</figureCaption>
<bodyText confidence="0.734812">
Upper bound on fine-grained accuracy for a given number of semantic classes.
</bodyText>
<sectionHeader confidence="0.781425" genericHeader="method">
3. Semantic Classes
</sectionHeader>
<bodyText confidence="0.999877407407408">
Our algorithm internally differentiates semantic classes rather than fine-grained senses.
Using fine-grained senses in the noisy channel model would be computationally ex-
pensive because the word–sense matrix needs to be inverted (see Equation [4]). It is
also unclear whether using fine-grained senses for model building will lead to better
learning performance: The similarity between the distributions of related senses is
ignored and the data becomes unnecessarily fragmented.
Even though we use coarse-grained semantic classes for model building, we use
fine-grained senses for evaluation. During evaluation, the coarse-grained semantic
classes predicted by the model are mapped to fine-grained senses by picking the lowest
numbered WordNet sense in the chosen semantic class.2 This is necessary to perform a
meaningful comparison with published results.
We take semantic classes to be groups of WordNet synsets defined using the hyper-
nym hierarchy (see Section 6 for alternative definitions). Section 4 presents three WSD
experiments using different sets of semantic classes at different levels of granularity.
In each experiment we designate a number of synsets high in the WordNet hypernym
hierarchy as “head synsets” and use their descendants to form the separate semantic
classes.
An arbitrary set of head synsets will not necessarily have mutually exclusive and
collectively exhaustive descendants. To assign every synset to a unique semantic class,
we impose an ordering on the semantic classes. Each synset is assigned only to the first
semantic class whose head it is a descendant of according to this ordering. If there are
synsets that are not descendants of any of the heads, they are collected into a separate
semantic class created for that purpose.
Using the coarse-grained semantic classes for prediction, Algorithm 1 will be unable
to return the correct fine-grained sense when this is not the lowest numbered sense in
a semantic class. To quantify the restrictive effect of working with a small number of
semantic classes, Figure 1 plots the number of semantic classes versus the best possible
</bodyText>
<footnote confidence="0.6555">
2 The sense numbers are ordered by the frequency of occurrence in WordNet.
</footnote>
<page confidence="0.987685">
116
</page>
<bodyText confidence="0.8469405">
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
oracle accuracy for the nouns in the SemCor corpus. To compute the oracle accuracy, we
assume that the program can find the correct semantic class for each instance, but has to
pick the first sense in that class as the answer. To construct a given number of semantic
classes, we used the following algorithm:
Algorithm 2
</bodyText>
<listItem confidence="0.996560142857143">
1. Initialize all synsets to be in a single “default” semantic class.
2. For each synset, compute the following score: the oracle accuracy achieved
if that synset and all its descendants are split into a new semantic class.
3. Take the synset with the highest score and split that synset and its
descendants into a new semantic class.
4. Repeat steps 2 and 3 until the desired number of semantic classes is
achieved.
</listItem>
<bodyText confidence="0.996377333333333">
The upper bound on fine-grained accuracy given a small number of semantic
classes is surprisingly high. In particular, the best reported noun WSD accuracy (78%)
is achievable if we could perfectly distinguish between five semantic classes.
</bodyText>
<sectionHeader confidence="0.838436" genericHeader="method">
4. Three Experiments
</sectionHeader>
<bodyText confidence="0.999894653846154">
We ran three experiments with the noisy channel model using different sets of semantic
classes. The first experiment uses the 25 WordNet semantic categories for nouns, the
second experiment looks at what happens when we group all the senses to just two
or three semantic classes, and the final experiment optimizes the number of semantic
classes using one data set (which gives 135 classes) and reports the out-of-sample result
using another data set.
The noun instances from the last three SensEval/SemEval English all-words tasks
are used for evaluation. We focus on the disambiguation of nouns for several reasons.
Nouns constitute the largest portion of content words (48% of the content words in the
Brown corpus [Kucera and Francis 1967] are nouns). For many tasks and applications
(e.g., Web queries [Jansen, Spink, and Pfaff 2000]) nouns are the most frequently encoun-
tered and important part of speech. Finally, WordNet has a more complete coverage
of noun semantic relations than other parts of speech, which is important for our
experiments with semantic classes.
As described in Section 2.2 we use the model to assign each ambiguous word to its
most likely semantic class in all the experiments. The lowest numbered sense in that
class is taken as the fine-grained answer. Finally we apply the one sense per discourse
heuristic: If the same word has been assigned more than one sense within the same
document, we take a majority vote and use sense numbers to break the ties.
Table 1 gives some baselines for comparison. The performance of the best super-
vised and unsupervised systems on noun disambiguation for each data set are given.
The first-sense baseline (FSB) is obtained by always picking the lowest numbered sense
for the word in the appropriate WordNet version. We prefer the FSB baseline over the
commonly used most-frequent-sense baseline because the tie breaking is unambiguous.
All the results reported are for fine-grained sense disambiguation. The top three systems
given in the table for each task are all supervised systems; the result for the best
</bodyText>
<page confidence="0.988161">
117
</page>
<table confidence="0.422561">
Computational Linguistics Volume 36, Number 1
</table>
<tableCaption confidence="0.5937116">
Table 1
Baselines for the three SensEval English all-words tasks; the WordNet version used (WN);
number of noun instances (Nouns); percentage accuracy of the first-sense baseline (FSB); the top
three supervised systems; and the best unsupervised system (Unsup). The last row gives the
total score of the best systems on the three tasks.
</tableCaption>
<table confidence="0.9903492">
Task WN Nouns FSB 1st 2nd 3rd Unsup
senseval2 1.7 1,067 71.9 78.0 74.5 70.0 61.8
senseval3 1.7.1 892 71.0 72.0 71.2 71.0 62.6
semeval07 2.1 159 64.2 68.6 66.7 66.7 63.5
total 2,118 70.9 74.4 72.5 70.2 62.2
</table>
<bodyText confidence="0.8823805">
unsupervised system is given in the last column. The reported unsupervised systems
do use the sense ordering and frequency information from WordNet.
</bodyText>
<subsectionHeader confidence="0.675481">
4.1 First Experiment: The 25 WordNet Categories
</subsectionHeader>
<bodyText confidence="0.999967142857143">
In previous work, descendants of 25 special WordNet synsets (known as the unique
beginners) have been used as the coarse-grained semantic classes for nouns (Crestan,
El-B`eze, and De Loupy 2001; Kohomban and Lee 2005). These unique beginners were
used to organize the nouns into 25 lexicographer files based on their semantic category
during WordNet development. Figure 2 shows the synsets at the top of the noun
hierarchy in WordNet. The 25 unique beginners have been shaded, and the two graphics
show how the hierarchy evolved between the two WordNet versions used in this study.
</bodyText>
<figureCaption confidence="0.748394">
Figure 2
</figureCaption>
<bodyText confidence="0.648521">
The top of the WordNet noun hypernym hierarchy for version 1.7 (left) and version 2.1 (right).
The 25 WordNet noun categories are shaded.
</bodyText>
<page confidence="0.982126">
118
</page>
<note confidence="0.568725">
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
</note>
<tableCaption confidence="0.8578162">
Table 2
The performance of the noisy channel model with the 25 semantic classes based on WordNet
lexicographer files. The columns give the data set, the percentage of times the model picks the
correct semantic class, maximum possible fine-grained score if the model had always picked the
correct class, and the actual score.
</tableCaption>
<table confidence="0.9996174">
Data Set CorrClass MaxScore Score
senseval2 85.1 90.3 77.7
senseval3 78.0 88.7 70.1
semeval07 75.5 86.2 64.8
total 81.4 89.3 73.5
</table>
<bodyText confidence="0.996855692307693">
We ran our initial experiments using these 25 WordNet categories as semantic
classes. The distribution of words for each semantic class, P(W|S), is estimated based
on WordNet sense frequencies. The distribution of words for each context, P(WIC), is
estimated using a 5-gram model based on the Web 1T corpus. The system first finds the
most likely semantic class based on the noisy channel model, then picks the first sense in
that class. Table 2 gives the results for the three data sets, which are significantly higher
than the previously reported unsupervised results.
To illustrate which semantic classes are the most difficult to disambiguate, Table 3
gives the confusion matrix for the Senseval2 data set. We can see that frequently occur-
ring concrete classes like person and body are disambiguated well. The largest source
of errors are the abstract classes like act, attribute, cognition, and communication. These
25 classes may not be the ideal candidates for word sense disambiguation. Even though
they allow a sufficient degree of fine-grained distinction (Table 2 shows that we can get
</bodyText>
<tableCaption confidence="0.70426875">
Table 3
Confusion matrix for Senseval2 data with the 25 WordNet noun classes. The rows are actual
classes, the columns are predicted classes. Column names have been abbreviated to save space.
The last two columns give the frequency of the class (F) and the accuracy of the class (A).
</tableCaption>
<bodyText confidence="0.99243696">
ac an ar at bo co co ev fe fo gr lo mo ob pe ph po pr qu re sh st su ti F A
act 58 0 4 7 0 7 2 3 2 0 5 0 0 0 0 0 1 4 1 1 0 2 0 0 9.1 59.8
animal 0 17 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2.1 77.3
artifact 0 0 66 2 0 0 6 5 0 0 5 1 0 1 0 0 0 0 0 0 3 1 0 0 8.4 73.3
attribute 3 0 0 19 0 3 0 0 0 0 0 1 0 1 2 0 2 1 0 0 1 3 0 0 3.4 52.8
body 0 0 0 0 123 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 11.6 99.2
cognition 6 0 1 2 0 82 5 1 0 0 0 2 1 1 1 0 1 0 5 1 0 5 0 0 10.7 71.9
communicat 2 0 1 0 0 2 29 1 0 0 0 2 5 0 0 1 0 0 0 1 0 0 0 2 4.3 63.0
event 0 0 0 0 0 0 0 19 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 2.0 90.5
feeling 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.4 100.
food 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 100.
group 0 0 0 2 0 5 0 0 0 0 69 2 0 3 0 0 0 0 0 1 1 0 0 1 7.9 82.1
location 0 0 0 1 0 0 0 0 0 0 0 22 0 0 0 0 0 0 0 0 0 0 0 0 2.2 95.7
motive 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0.2 50.0
object 2 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0.7 14.3
person 2 4 0 0 0 1 1 0 0 0 1 0 0 0 168 0 0 0 0 0 0 0 0 0 16.6 94.9
phenomenon 1 0 0 1 0 1 0 2 0 0 0 0 0 0 0 3 0 0 0 3 0 0 0 0 1.0 27.3
possession 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0.4 100.
process 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 12 0 0 0 1 0 0 1.4 80.0
quantity 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 10 0 0 0 0 0 1.2 76.9
relation 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0.3 66.7
shape 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0.1 100.
state 1 0 1 5 0 1 1 2 0 0 1 0 0 0 1 0 0 0 0 0 0 98 0 0 10.4 88.3
substance 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 10 0 0.9 100.
time 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 44 4.8 86.3
</bodyText>
<page confidence="0.965398">
119
</page>
<table confidence="0.578025">
Computational Linguistics Volume 36, Number 1
</table>
<tableCaption confidence="0.9076785">
Table 4
The performance of the noisy channel model with two to three semantic classes. The columns
give the data set, the head synsets, the percentage of times the model picks the correct semantic
class, maximum possible fine-grained score if the model had always picked the correct class, and
</tableCaption>
<table confidence="0.846451272727273">
the actual score.
Data Set Heads
senseval2 entity/default
senseval3 entity/default
senseval3 object/entity/default
semeval07 psychological-feature/default
CorrClass MaxScore Score
86.6 76.8 74.9
94.2 75.8 71.2
93.8 77.4 72.9
91.2 74.8 68.6
</table>
<bodyText confidence="0.87859">
85–90% if we could pick the right class every time), they seem too easy to confuse. In the
next few experiments we will use these observations to design better sets of semantic
classes.
</bodyText>
<subsectionHeader confidence="0.902295">
4.2 Second Experiment: Distinguishing Mental and Physical Concepts
</subsectionHeader>
<bodyText confidence="0.9992708">
Figure 1 shows that the upper bound for fine-grained disambiguation is relatively high
even for a very small number of semantic classes. In our next experiment we look at
how well our approach can perform differentiating only two or three semantic classes.
We use Algorithm 2 applied to the appropriate version of SemCor to pick the head
synsets used to define the semantic classes. Figure 2 shows that the top level of the
hypernym hierarchy has changed significantly between the WordNet versions. Thus,
different head synsets are chosen for different data sets. However, the main distinction
captured by our semantic classes seems to be between mental and physical concepts.
Table 4 gives the results. The performance with a few semantic classes is comparable to
the top supervised algorithms in each of the three data sets.
</bodyText>
<subsectionHeader confidence="0.962147">
4.3 Third Experiment: Tuning the Number of Classes
</subsectionHeader>
<bodyText confidence="0.958857142857143">
Increasing the number of semantic classes has two opposite effects on WSD perfor-
mance. The higher the number, the finer distinctions we can make, and the maximum
possible fine-grained accuracy goes up. However, the more semantic classes we define,
the more difficult it becomes to distinguish them from one another. For an empirical
analysis of the effect of semantic class granularity on the fine-grained WSD accuracy,
we generated different sets of semantic classes using the following algorithm.
Algorithm 3
</bodyText>
<listItem confidence="0.9900294">
1. Sort all the synsets according to their “subtree frequency”: i.e., the total
frequency of each synset’s descendants in the hypernym tree.
2. Take the desired number of synsets with the highest subtree frequency and
use them as head synsets, that is, split their descendants into separate
semantic classes.
</listItem>
<bodyText confidence="0.9640635">
Figure 3 shows the fine-grained accuracy we achieved on the Senseval2 data set
with up to 600 semantic classes defined based on Algorithm 3. Note the differences: (i)
</bodyText>
<page confidence="0.957822">
120
</page>
<figure confidence="0.536168">
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
</figure>
<figureCaption confidence="0.955017">
Figure 3
The fine-grained accuracy on Senseval2 data set for a given number of semantic classes.
Figure 1 gives the best possible oracle accuracy, Figure 3 gives the actual WSD accuracy;
(ii) Algorithm 2 chooses the head synsets based on their oracle score, Algorithm 3
chooses them based on their subtree frequency.
</figureCaption>
<bodyText confidence="0.999289">
As we suspected, the relationship is not simple or monotonic. However, one can
identify distinct peaks at 3, 25, and 100–150 semantic classes. One hypothesis is that
these peaks correspond to “natural classes” at different levels of granularity. Here are
some example semantic classes from each peak:
</bodyText>
<sectionHeader confidence="0.389163333333333" genericHeader="method">
3 classes entity, abstraction
25 classes action, state, content, location, attribute,...
135 classes food, day, container, home, word, business, feeling, material, job, man, ...
</sectionHeader>
<bodyText confidence="0.999896666666667">
To test the out-of-sample effect of tuning the semantic classes based on the peaks
of Figure 3, we used the SemEval-2007 data set as our test sample. When the 135
semantic classes from the highest peak are used for the disambiguation of the nouns
in the SemEval-2007 data set, an accuracy of 69.8% was achieved. This is higher than
the accuracy of the best supervised system on this task (68.6%), although the difference
is not statistically significant.
</bodyText>
<sectionHeader confidence="0.991676" genericHeader="method">
5. Discussion
</sectionHeader>
<bodyText confidence="0.999745">
In this section we will address several questions raised by the results of the experi-
ments. Why do we get different results from different data sets? Are the best results
significantly different than the first-sense baseline? Can we improve our results using
better semantic classes?
</bodyText>
<subsectionHeader confidence="0.881386">
5.1 Why Do We Get Different Results from Different Data Sets?
</subsectionHeader>
<bodyText confidence="0.998198">
Table 5 summarizes our results from the three experiments of Section 4. There are some
significant differences between the data sets.
</bodyText>
<page confidence="0.994455">
121
</page>
<table confidence="0.412717">
Computational Linguistics Volume 36, Number 1
</table>
<tableCaption confidence="0.989888666666667">
Table 5
Result summary for the three data sets. The columns give the data set, the results of the three
experiments, best reported result, the first-sense baseline, and the number of instances.
</tableCaption>
<table confidence="0.9982875">
Data set Exp1 Exp2 Exp3 Best FSB Instances
senseval2 77.7 74.9 - 78.0 71.9 1,067
senseval3 70.1 72.9 - 72.0 71.0 892
semeval07 64.8 68.6 69.8 68.6 64.2 159
</table>
<bodyText confidence="0.989998111111111">
The SemEval-2007 data set appears to be significantly different from the other two
with its generally lower baseline and scores. The difference in accuracy is probably
due to the difference in data preparation. In the two Senseval data sets all content
words were targeted for disambiguation. In the SemEval-2007 data set only verbs and
their noun arguments were selected, targeting only about 465 lemmas from about 3,500
words of text. For the Senseval-3 data set none of our results, or any published result
we know of, is significantly above the baseline for noun disambiguation. This may be
due to extra noise in the data—the inter-annotator agreement for nouns in this data set
was 74.9%.
</bodyText>
<subsectionHeader confidence="0.998783">
5.2 Are the Best Results Significantly Different Than the FSB?
</subsectionHeader>
<bodyText confidence="0.999993545454545">
Among all the published results for these three data sets, our two results for the
Senseval-2 data set and the top supervised result for the Senseval-2 data set are the
only ones statistically significantly above the FSB for noun disambiguation at the 95%
confidence interval. This is partly because of the lack of sufficient data. For example, the
SemEval-2007 data set has only 159 nouns; and a result of 71.8% would be needed to
demonstrate a difference from the baseline of 64.2% at the 95% confidence interval.
More importantly, however, statistical significance should not be confused with
“significance” in general. A statistically significant difference may not be necessary or
sufficient for a significant impact on an application. Even a WSD system that is statis-
tically indistinguishable from the baseline according to the “total accuracy” metric is
most probably providing significantly different answers compared to always guessing
the first sense. There are metrics that can reveal these differences, such as “balanced
error rate” (i.e., arithmetic average of the error rates for different senses) or “accuracy in
detecting the use of a non-dominant sense.”
Finally, the relatively high first-sense baseline (e.g., 71.0% for Senseval-3 nouns)
combined with the relatively low inter-annotator agreement (e.g., 74.9% for Senseval-
3 nouns) makes progress in the traditional WSD task difficult. Annotators who are
perfectly proficient in comprehending language nevertheless find it difficult to distin-
guish between artificially-created dictionary senses. If our long term goal is to model
human competence in language comprehension, it would make sense to focus on tasks
at which humans are naturally competent. Dictionary-independent tasks such as lexical
substitution or textual entailment may be the right steps in this direction.
</bodyText>
<subsectionHeader confidence="0.968648">
5.3 Can We Improve Our Results Using Better Semantic Classes?
</subsectionHeader>
<bodyText confidence="0.9995055">
In order to get an upper bound for our approach, we searched for the best set of semantic
classes specific to each data set using the following greedy algorithm.
</bodyText>
<page confidence="0.994161">
122
</page>
<note confidence="0.631671">
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
</note>
<tableCaption confidence="0.972276">
Table 6
</tableCaption>
<bodyText confidence="0.96732925">
The performance of the noisy channel model with the best set of semantic classes picked for each
data set. The columns give the data set, the number of classes, maximum possible score if the
model always picks the correct class, percentage of times it actually picks the correct class, and
its fine-grained accuracy.
</bodyText>
<table confidence="0.8476632">
Data Set NumClass MaxScore CorrClass Score
senseval2 23 89.2 88.8 80.1
senseval3 29 87.2 87.4 77.4
semeval07 12 84.9 89.9 79.2
Algorithm 4
</table>
<listItem confidence="0.996875666666667">
1. Initialize all synsets to be in a single “default” semantic class.
2. For each synset, compute the following score: the WSD accuracy achieved
if that synset and all its descendants are split into a new semantic class.
3. Take the synset with the highest score and split that synset and its
descendants into a new semantic class.
4. Repeat steps 2 and 3 until the WSD accuracy can no longer be improved.
</listItem>
<bodyText confidence="0.999056384615385">
Algorithm 4 was run for each of the three data sets, which resulted in three different
sets of semantic classes. The noisy channel model was applied with the best set of
semantic classes for each data set. Table 6 summarizes the results. Note that these results
are not predictive of out-of-sample accuracy because Algorithm 4 picks a specific set of
semantic classes optimal for a given data set. But the results do indicate that a better
set of semantic classes may lead to significantly better WSD accuracy. In particular
each result in Table 6 is significantly higher than previously reported supervised or
unsupervised results.
How to construct a good set of semantic classes that balance specificity and identi-
fiability is a topic of ongoing research. See Kohomban and Lee (2007) for a supervised
solution using feature-based clustering that tries to maintain feature–class coherence.
Non-parametric Bayesian approaches such as Teh et al. (2006) applied to context distri-
butions could reveal latent senses in an unsupervised setting.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="method">
6. Related Work
</sectionHeader>
<bodyText confidence="0.999088">
For a general overview of different approaches to WSD, see Navigli (2009) and
Stevenson (2003). The Senseval and SemEval workshops (Cotton et al. 2001; Mihalcea
and Edmonds 2004; Agirre, M`arquez, and Wicentowski 2007) are good sources of recent
work, and have been used in this article to benchmark our results.
Generative models based on the noisy channel framework have previously been
used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation
(Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction
(Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among
others. To our knowledge our work is the first application of the noisy channel model
to unsupervised word sense disambiguation.
</bodyText>
<page confidence="0.995468">
123
</page>
<note confidence="0.591564">
Computational Linguistics Volume 36, Number 1
</note>
<bodyText confidence="0.999894967741936">
Using statistical language models based on large corpora for WSD has been ex-
plored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this
article see Yuret (2008); for a more general review of statistical language modeling see
Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001).
Grouping similar senses into semantic classes for WSD has been explored in previ-
ous work. Senses that are similar have been identified using WordNet relations (Peters,
Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee
2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski
and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE
(Dolan 1994), and ODE (Navigli 2006).
Ciaramita and Altun (2006) build a supervised HMM tagger using “supersenses,”
essentially the 25 WordNet noun categories we have used in our first experiment in
addition to 15 verb categories similarly defined. They report a supersense precision of
67.60 for nouns and verbs of Senseval-3. Table 2 gives our supersense score as 78% for
Senseval-3 nouns. However, the results are not directly comparable because they do not
report the noun and verb scores separately or calculate the corresponding fine-grained
score to compare with other Senseval-3 results.
Kohomban and Lee (2007) go beyond the WordNet categories based on lexicogra-
pher files and experiment with clustering techniques to construct their semantic classes.
Their classes are based on local features from sense-labeled data and optimize feature–
class coherence rather than adhering to the WordNet hierarchy. Their supervised system
achieves an accuracy of 74.7% on Senseval-2 nouns and 73.6% on Senseval-3 nouns.
The systems mentioned so far are supervised WSD systems. Agirre and Martinez
(2004) explore the large-scale acquisition of sense-tagged examples from the Web and
train supervised, minimally supervised (requiring sense bias information from hand-
tagged corpora, similar to our system), and fully unsupervised WSD algorithms using
this corpus. They report good results on the Senseval-2 lexical sample data compared to
other unsupervised systems. Martinez, de Lacalle, and Agirre (2008) test a similar set of
systems trained using automatically acquired corpora on Senseval-3 nouns. Their mini-
mally supervised system obtains 63.9% accuracy on polysemous nouns from Senseval-3
(corresponding to 71.86% on all nouns).
</bodyText>
<sectionHeader confidence="0.855295" genericHeader="method">
7. Contributions
</sectionHeader>
<bodyText confidence="0.9998650625">
We have introduced a new generative probabilistic model based on the noisy channel
framework for unsupervised word sense disambiguation. The main contribution of this
model is the reduction of the word sense disambiguation problem to the estimation of
two distributions: the distribution of words used to express a given sense, and the dis-
tribution of words that appear in a given context. In this framework, context similarity
is determined by the distribution of words that can be placed in the given context. This
replaces the ad hoc contextual feature design process by a statistical language model,
allowing the advances in language modeling and the availability of large unlabeled
corpora to have a direct impact on WSD performance.
We have provided a detailed analysis of using coarse-grained semantic classes for
fine-grained WSD. The noisy channel model is a good fit for class-based WSD, where
the model decides on a coarse-grained semantic class instead of a fine-grained sense.
The chosen semantic class is then mapped to a specific sense based on the WordNet
ordering during evaluation. We show that the potential loss from using coarse-grained
classes is limited, and state-of-the-art performance is possible using only a few semantic
classes. We explore semantic classes at various levels of granularity and show that
</bodyText>
<page confidence="0.987331">
124
</page>
<note confidence="0.427336">
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
</note>
<bodyText confidence="0.999734333333333">
the relationship between granularity and fine-grained accuracy is complex, thus more
work is needed to determine an ideal set of semantic classes.
In several experiments we compare the performance of our unsupervised WSD
system with the best systems from previous Senseval and SemEval workshops. We
consistently outperform any previously reported unsupervised results and achieve
comparable performance to the best supervised results.
</bodyText>
<sectionHeader confidence="0.812944" genericHeader="method">
Appendix A: Solutions for P(SIC)
</sectionHeader>
<bodyText confidence="0.999783692307692">
To solve for P(S|C) using P(W|C) and P(W|S), we represent the first two as vectors: sj =
P(S = j|C = k) and wi = P(W = i|C = k), and the last one as a matrix: WSij = P(W =
i|S = j). Our problem becomes finding a solution to the linear equation w = WS x s.
Using the Moore–Penrose pseudoinverse, WS+, we find a solution s = WS+ x w. This
solution minimizes the distance |WS x s − w|. There are two potential problems with
this pseudoinverse solution. First, it may violate the non-negativity and normalization
constraints of a probability distribution. Second, a maximum likelihood estimate should
minimize the cross entropy between WS x s and w, not the Euclidean distance. We
addressed the normalization problem using a constrained linear solver and the cross-
entropy problem using numerical optimization. However, our experiments showed the
difference in WSD performance to be less than 1% in each case. The pseudoinverse
solution, s = WS+ x w, can be computed quickly and works well in practice, so this
is the solution that is used in all our experiments.
</bodyText>
<sectionHeader confidence="0.768388" genericHeader="method">
Appendix B: Estimating P(WIC)
</sectionHeader>
<bodyText confidence="0.9999854">
Estimating P(W|C) for each context is expensive because the number of words that need
to be considered is large. The Web 1T data set contains 13.5 million unique words, and
WordNet defines about 150,000 lemmas. To make the computation feasible we needed
to limit the set of words for which P(W|C) needs to be estimated. We limited our set to
WordNet lemmas with the same part of speech as the target word. We further required
the word to have a non-zero count in WordNet sense frequencies. The inflection and
capitalization of each word W was automatically matched to the target word. As a
result, we estimated P(W|C) for about 10,000 words for each noun context and assumed
the other words had zero probability. The n-grams required for all the contexts were
listed, and their counts were extracted from the Web 1T data set in one pass. The P(W|C)
was estimated for all the words and contexts based on these counts. In the end, we only
used the 100 most likely words in each context for efficiency, as the difference in results
using the whole distribution was not significant. For more details on smoothing with a
large language model see Yuret (2008), although we did not see a significant difference
in WSD performance based on the smoothing method used.
</bodyText>
<sectionHeader confidence="0.623417" genericHeader="method">
Acknowledgments References
</sectionHeader>
<bodyText confidence="0.992273142857143">
This work was supported in part by Agirre, E. and D. Martinez. 2004.
the Scientific and Technical Research Unsupervised WSD based on
Council of Turkey (T ¨UB˙ITAK Project automatically retrieved examples:
108E228). We would like to thank Peter The importance of bias. In Proceedings
Turney, Rada Mihalcea, Diana McCarthy, of the Conference on Empirical Methods
and the four anonymous reviewers in Natural Language Processing (EMNLP),
for their helpful comments and suggestions. pages 25–32, Barcelona.
</bodyText>
<page confidence="0.989097">
125
</page>
<note confidence="0.545093">
Computational Linguistics Volume 36, Number 1
</note>
<reference confidence="0.998315593220339">
Agirre, Eneko, Lluis M`arquez, and Richard
Wicentowski, editors. 2007. Proceedings
of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007),
Prague.
Bahl, Lalit R., Frederick Jelinek, and Robert L.
Mercer. 1983. A maximum likelihood
approach to continuous speech
recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence,
5(2):179–190.
Banko, Michele and Eric Brill. 2001.
Scaling to very very large corpora
for natural language disambiguation.
In Proceedings of 39th Annual Meeting of
the Association for Computational Linguistics,
pages 26–33, Toulouse, France, July.
Association for Computational Linguistics.
Brants, Thorsten and Alex Franz. 2006. Web
1T 5-gram version 1. Linguistic Data
Consortium, Philadelphia. LDC2006T13.
Brill, Eric and Robert C. Moore. 2000. An
improved error model for noisy channel
spelling correction. In Proceedings of the 38th
Annual Meeting of the Association for
Computational Linguistics, pages 286–293,
Hong Kong.
Brown, Peter F., John Cocke, Stephen A.
Della Pietra, Vincent J. Della Pietra,
Frederick Jelinek, John D. Lafferty,
Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine
translation. Computational Linguistics,
16(2):79–85.
Chen, S. F. and J. Goodman. 1999. An
empirical study of smoothing techniques
for language modeling. Computer
Speech and Language, 13(4):359–394.
Chklovski, Timothy and Rada Mihalcea. 2003.
Exploiting agreement and disagreement
of human annotators for word sense
disambiguation. In Proceedings of the
Conference on Recent Advances in Natural
Language Processing, pages 357–366,
Borovetz.
Ciaramita, Massimiliano and
Yasemin Altun. 2006. Broad-coverage
sense disambiguation and information
extraction with a supersense sequence
tagger. In Proceedings of the 2006
Conference on Empirical Methods in
Natural Language Processing,
pages 594–602, Sydney.
Cotton, Scott, Phil Edmonds,
Adam Kilgarriff, and Martha Palmer,
editors. 2001. SENSEVAL-2: Second
International Workshop on Evaluating Word
Sense Disambiguation Systems, Toulouse,
France.
Crestan, E., M. El-B`eze, and C. De Loupy.
2001. Improving WSD with multi-level
view of context monitored by similarity
measure. In Proceedings of SENSEVAL-2:
Second International Workshop on Evaluating
Word Sense Disambiguation Systems,
Toulouse.
Daume III, Hal and Daniel Marcu. 2002.
A noisy-channel model for document
compression. In Proceedings of 40th
Annual Meeting of the Association for
Computational Linguistics, pages 449–456,
Philadelphia, PA.
Dolan, W. B. 1994. Word sense ambiguation:
clustering related senses. In Proceedings
of the 15th conference on Computational
Linguistics, pages 05–09, Kyoto.
Echihabi, Abdessamad and Daniel Marcu.
2003. A noisy-channel approach to
question answering. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics, pages 16–23,
Sapporo.
Fellbaum, Christiane, editor. 1998. Wordnet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Goodman, Joshua. 2001. A bit of progress in
language modeling. Computer Speech and
Language, 15:403–434.
Hawker, Tobias. 2007. Usyd: WSD and lexical
substitution using the Web1t corpus. In
Proceedings of the Fourth International
Workshop on Semantic Evaluations
(SemEval-2007), pages 446–453,
Prague.
Jansen, B. J., A. Spink, and A. Pfaff. 2000.
Linguistic aspects of Web queries. In
Proceedings of the ASIS Annual Meeting,
pages 169–176, Chicago, IL.
Kohomban, Upali Sathyajith and
Wee Sun Lee. 2005. Learning semantic
classes for word sense disambiguation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics
(ACL’05), pages 34–41, Ann Arbor, MI.
Kohomban, Upali Sathyajith and
Wee Sun Lee. 2007. Optimizing classifier
performance in word sense disambiguation
by redefining word sense classes. In
Proceedings of the International Joint
Conference on Artificial Intelligence,
pages 1635–1640, Hyderabad.
Kucera, Henry and W. Nelson Francis. 1967.
Computational Analysis of Present-Day
American English. Brown University Press,
Providence, RI.
Magnini, B., C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2003. The role of domain
information in word sense disambiguation.
</reference>
<page confidence="0.97426">
126
</page>
<note confidence="0.355716">
Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD
</note>
<reference confidence="0.999361529411765">
Natural Language Engineering,
8(04):359–373.
Martinez, D., O. Lopez de Lacalle, and
E. Agirre. 2008. On the use of automatically
acquired examples for all-nouns
word sense disambiguation. Journal
of Artificial Intelligence Research, 33:79–107.
Mihalcea, Rada and Phil Edmonds, editors.
2004. SENSEVAL-3: The Third International
Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona.
Navigli, Roberto. 2006. Meaningful clustering
of senses helps boost word sense
disambiguation performance. In
Proceedings of the 21st International
Conference on Computational Linguistics
and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 105–112, Sydney.
Navigli, Roberto. 2009. Word sense
disambiguation: A survey. ACM
Computing Surveys, 41(2):1–69.
Peters, W., I. Peters, and P. Vossen.1998.
Automatic sense clustering in
EuroWordNet. In Proceedings of the
International Conference on Language
Resources and Evaluation, pages 409–416,
Granada.
Rosenfeld, Ronald. 2000. Two decades
of statistical language modeling:
Where do we go from here? Proceedings
of the IEEE, 88:1270–1278.
Shannon, Claude Elwood. 1948. A
mathematical theory of communication.
The Bell System Technical Journal,
27:379–423, 623–656.
Stevenson, Mark. 2003. Word Sense
Disambiguation: The Case for Combinations
of Knowledge Sources. CSLI, Stanford, CA.
Teh, Y. W., M. I. Jordan, M. J. Beal, and
D. M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical
Association, 101(476):1566–1581.
Yarowsky, David. 1992. Word sense
disambiguation using statistical
models of Roget’s categories trained
on large corpora. In Proceedings of the 15th
International Conference on Computational
Linguistics, pages 454–460, Nantes.
Yarowsky, David and Radu Florian. 2002.
Evaluating sense disambiguation across
diverse parameter spaces. Natural Language
Engineering, 8(4):293–310.
Yuret, Deniz. 2004. Some experiments
with a Naive Bayes WSD system. In
Senseval-3: Third International Workshop
on the Evaluation of Systems for the
Semantic Analysis of Text, pages 265–268,
Barcelona.
Yuret, Deniz. 2007. KU: Word sense
disambiguation by substitution. In
Proceedings of the Fourth International
Workshop on Semantic Evaluations
(SemEval-2007), pages 207–214, Prague.
Yuret, Deniz. 2008. Smoothing a tera-word
language model. In Proceedings of
ACL-08: HLT, Short Papers,
pages 141–144, Columbus, OH.
</reference>
<page confidence="0.997304">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.310293">
<title confidence="0.636500333333333">The Noisy Channel Model for Unsupervised Word Sense Disambiguation University</title>
<author confidence="0.997065">Mehmet Ali Yatbaz</author>
<affiliation confidence="0.990621">University</affiliation>
<abstract confidence="0.998396538461539">We introduce a generative probabilistic model, the noisy channel model, for unsupervised word sense disambiguation. In our model, each context C is modeled as a distinct channel through which the speaker intends to transmit a particular meaning S using a possibly ambiguous word W. To reconstruct the intended meaning the hearer uses the distribution of possible meanings the given context possible words that can express each meaning We independent of the context and estimate it using WordNet sense frequencies. main problem of unsupervised WSD is estimating context-dependent access to any sense-tagged text. We show one way to solve this problem using a statistical language model based on large amounts of untagged text. Our model uses coarse-grained semantic classes for S internally and we explore the effect of using different levels of granularity on WSD performance. The system outputs fine-grained senses for evaluation, and its performance on noun disambiguation is better than most previously reported unsupervised systems and close to the best supervised systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2007</date>
<booktitle>Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<editor>Agirre, Eneko, Lluis M`arquez, and Richard Wicentowski, editors.</editor>
<location>Prague.</location>
<contexts>
<context position="34860" citStr="(2007)" startWordPosition="6035" endWordPosition="6035">classes for each data set. Table 6 summarizes the results. Note that these results are not predictive of out-of-sample accuracy because Algorithm 4 picks a specific set of semantic classes optimal for a given data set. But the results do indicate that a better set of semantic classes may lead to significantly better WSD accuracy. In particular each result in Table 6 is significantly higher than previously reported supervised or unsupervised results. How to construct a good set of semantic classes that balance specificity and identifiability is a topic of ongoing research. See Kohomban and Lee (2007) for a supervised solution using feature-based clustering that tries to maintain feature–class coherence. Non-parametric Bayesian approaches such as Teh et al. (2006) applied to context distributions could reveal latent senses in an unsupervised setting. 6. Related Work For a general overview of different approaches to WSD, see Navigli (2009) and Stevenson (2003). The Senseval and SemEval workshops (Cotton et al. 2001; Mihalcea and Edmonds 2004; Agirre, M`arquez, and Wicentowski 2007) are good sources of recent work, and have been used in this article to benchmark our results. Generative model</context>
<context position="36066" citStr="(2007)" startWordPosition="6217" endWordPosition="6217">s based on the noisy channel framework have previously been used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation (Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the first application of the noisy channel model to unsupervised word sense disambiguation. 123 Computational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarows</context>
<context position="37301" citStr="(2007)" startWordPosition="6405" endWordPosition="6405"> and ODE (Navigli 2006). Ciaramita and Altun (2006) build a supervised HMM tagger using “supersenses,” essentially the 25 WordNet noun categories we have used in our first experiment in addition to 15 verb categories similarly defined. They report a supersense precision of 67.60 for nouns and verbs of Senseval-3. Table 2 gives our supersense score as 78% for Senseval-3 nouns. However, the results are not directly comparable because they do not report the noun and verb scores separately or calculate the corresponding fine-grained score to compare with other Senseval-3 results. Kohomban and Lee (2007) go beyond the WordNet categories based on lexicographer files and experiment with clustering techniques to construct their semantic classes. Their classes are based on local features from sense-labeled data and optimize feature– class coherence rather than adhering to the WordNet hierarchy. Their supervised system achieves an accuracy of 74.7% on Senseval-2 nouns and 73.6% on Senseval-3 nouns. The systems mentioned so far are supervised WSD systems. Agirre and Martinez (2004) explore the large-scale acquisition of sense-tagged examples from the Web and train supervised, minimally supervised (</context>
</contexts>
<marker>2007</marker>
<rawString>Agirre, Eneko, Lluis M`arquez, and Richard Wicentowski, editors. 2007. Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>A maximum likelihood approach to continuous speech recognition.</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>5</volume>
<issue>2</issue>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Bahl, Lalit R., Frederick Jelinek, and Robert L. Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2):179–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Toulouse, France,</location>
<contexts>
<context position="2155" citStr="Banko and Brill (2001)" startWordPosition="327" endWordPosition="330">ems to date are based on supervised learning and trained on sense-tagged corpora. In this article we present an unsupervised WSD algorithm that can leverage untagged text and can perform at the level of the best supervised systems for the allnouns disambiguation task. The main drawback of the supervised approach is the difficulty of acquiring considerable amounts of training data, also known as the knowledge acquisition bottleneck. Yarowsky and Florian (2002) report that each successive doubling of the training data for WSD only leads to a 3–4% error reduction within their experimental range. Banko and Brill (2001) experiment with the problem of selection among confusable words and show that the learning curves do not converge even after * Koc¸ University, Department of Computer Engineering, 34450 Sarıyer, ˙Istanbul, Turkey. E-mail: dyuret@ku.edu.tr, myatbaz@ku.edu.tr. Submission received: 7 October 2008; revised submission received: 17 April 2009; accepted for publication: 12 September 2009. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 1 a billion words of training data. They suggest unsupervised, semi-supervised, or active learning to take advantage of l</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Banko, Michele and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 26–33, Toulouse, France, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram version 1. Linguistic Data Consortium,</title>
<date>2006</date>
<location>Philadelphia. LDC2006T13.</location>
<contexts>
<context position="3276" citStr="Brants and Franz 2006" startWordPosition="488" endWordPosition="491"> training data. They suggest unsupervised, semi-supervised, or active learning to take advantage of large data sets when labeling is expensive. Yuret (2004) observes that in a supervised naive Bayes WSD system trained on SemCor, approximately half of the test instances do not contain any of the contextual features (e.g., neighboring content words or local collocation patterns) observed in the training data. SemCor is the largest publicly available corpus of sense-tagged text, and has only about a quarter million sense-tagged words. In contrast, our unsupervised system uses the Web1T data set (Brants and Franz 2006) for unlabeled examples, which contains counts from a 1012 word corpus derived from publicly-available Web pages. A note on the term “unsupervised” may be appropriate here. In the WSD literature “unsupervised” is typically used to describe systems that do not directly use sensetagged corpora for training. However, many of these unsupervised systems, including ours, use sense ordering or sense frequencies from WordNet (Fellbaum 1998) or other dictionaries. Thus it might be more appropriate to call them weakly supervised or semi-supervised. More specifically, context–sense pairs or context–word–</context>
<context position="14294" citStr="Brants and Franz 2006" startWordPosition="2327" endWordPosition="2330">a word sequence into conditional probabilities. The first four terms do not include the target word w5, and have been dropped in Equation (7). We also truncate the remaining conditionals to four words reflecting the Markov assumption of the 5-gram model. Finally, using an expression that is proportional to P(W|C) instead of P(W|C) itself will not change the WSD result because we are taking the argmax in Equation (1). Each term on the right hand side of Equation (7) is estimated using a 5-gram language model. To get accurate domain-independent probability estimates we used the Web 1T data set (Brants and Franz 2006), which contains the counts of word sequences up to length five in a 1012 word corpus derived from publicly-accessible Web pages. Estimation of P(W|C) is the most computationally expensive step of the algorithm, and some implementation details are given in Appendix B. 1 The sense frequencies were obtained from the index.sense file included in the WordNet distribution. We had to correct the counts of three words (person, group, and location) whose WordNet counts unfortunately include the corresponding named entities and are thus inflated. 115 Computational Linguistics Volume 36, Number 1 Figure</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Brants, Thorsten and Alex Franz. 2006. Web 1T 5-gram version 1. Linguistic Data Consortium, Philadelphia. LDC2006T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>286--293</pages>
<location>Hong Kong.</location>
<contexts>
<context position="35711" citStr="Brill and Moore 2000" startWordPosition="6158" endWordPosition="6161">an unsupervised setting. 6. Related Work For a general overview of different approaches to WSD, see Navigli (2009) and Stevenson (2003). The Senseval and SemEval workshops (Cotton et al. 2001; Mihalcea and Edmonds 2004; Agirre, M`arquez, and Wicentowski 2007) are good sources of recent work, and have been used in this article to benchmark our results. Generative models based on the noisy channel framework have previously been used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation (Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the first application of the noisy channel model to unsupervised word sense disambiguation. 123 Computational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Brill, Eric and Robert C. Moore. 2000. An improved error model for noisy channel spelling correction. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 286–293, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="6982" citStr="Brown et al. 1990" startWordPosition="1051" endWordPosition="1054">ised systems from SensEval-2 (Cotton et al. 2001), SensEval-3 (Mihalcea and Edmonds 2004), and SemEval-2007 (Agirre, M`arquez, and Wicentowski 2007). Section 5 discusses these results and the idiosyncrasies of the data sets, baselines, and evaluation metrics used. Section 6 presents related work, and Section 7 summarizes our contributions. 112 Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD 2. The Noisy Channel Model for WSD 2.1 Model The noisy channel model has been the foundation of standard models in speech recognition (Bahl, Jelinek, and Mercer 1983) and machine translation (Brown et al. 1990). In this article we explore its application to WSD. The noisy channel model can be used whenever a signal received does not uniquely identify the message being sent. Bayes’ Law is used to interpret the ambiguous signal and identify the most probable intended message. In WSD, we model each context as a distinct channel where the intended message is a word sense (or semantic class) S, and the signal received is an ambiguous word W. In this section we will describe how to model a given context C as a noisy channel, and in particular how to estimate the context-specific sense distribution without</context>
<context position="35621" citStr="Brown et al. 1990" startWordPosition="6146" endWordPosition="6149">ch as Teh et al. (2006) applied to context distributions could reveal latent senses in an unsupervised setting. 6. Related Work For a general overview of different approaches to WSD, see Navigli (2009) and Stevenson (2003). The Senseval and SemEval workshops (Cotton et al. 2001; Mihalcea and Edmonds 2004; Agirre, M`arquez, and Wicentowski 2007) are good sources of recent work, and have been used in this article to benchmark our results. Generative models based on the noisy channel framework have previously been used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation (Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the first application of the noisy channel model to unsupervised word sense disambiguation. 123 Computational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Brown, Peter F., John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech and Language,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="36226" citStr="Chen and Goodman (1999)" startWordPosition="6239" endWordPosition="6242">Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the first application of the noisy channel model to unsupervised word sense disambiguation. 123 Computational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE (Dolan 1994), and ODE (Navigli 2006). Ciaramita and Altun (2006) build a supervised HMM tagger using “supersenses,” essentially the 25 WordNet n</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Chen, S. F. and J. Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 13(4):359–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Rada Mihalcea</author>
</authors>
<title>Exploiting agreement and disagreement of human annotators for word sense disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>357--366</pages>
<location>Borovetz.</location>
<contexts>
<context position="36615" citStr="Chklovski and Mihalcea 2003" startWordPosition="6296" endWordPosition="6299">ed on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE (Dolan 1994), and ODE (Navigli 2006). Ciaramita and Altun (2006) build a supervised HMM tagger using “supersenses,” essentially the 25 WordNet noun categories we have used in our first experiment in addition to 15 verb categories similarly defined. They report a supersense precision of 67.60 for nouns and verbs of Senseval-3. Table 2 gives our supersense score as 78% for Senseval-3 nouns. However, the results are not directly comparable because they do not report the noun and verb scores separately or calculate the correspondin</context>
</contexts>
<marker>Chklovski, Mihalcea, 2003</marker>
<rawString>Chklovski, Timothy and Rada Mihalcea. 2003. Exploiting agreement and disagreement of human annotators for word sense disambiguation. In Proceedings of the Conference on Recent Advances in Natural Language Processing, pages 357–366, Borovetz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>594--602</pages>
<location>Sydney.</location>
<contexts>
<context position="36746" citStr="Ciaramita and Altun (2006)" startWordPosition="6316" endWordPosition="6319">e see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE (Dolan 1994), and ODE (Navigli 2006). Ciaramita and Altun (2006) build a supervised HMM tagger using “supersenses,” essentially the 25 WordNet noun categories we have used in our first experiment in addition to 15 verb categories similarly defined. They report a supersense precision of 67.60 for nouns and verbs of Senseval-3. Table 2 gives our supersense score as 78% for Senseval-3 nouns. However, the results are not directly comparable because they do not report the noun and verb scores separately or calculate the corresponding fine-grained score to compare with other Senseval-3 results. Kohomban and Lee (2007) go beyond the WordNet categories based on le</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Ciaramita, Massimiliano and Yasemin Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 594–602, Sydney.</rawString>
</citation>
<citation valid="true">
<date>2001</date>
<booktitle>SENSEVAL-2: Second International Workshop on Evaluating Word Sense Disambiguation Systems,</booktitle>
<editor>Cotton, Scott, Phil Edmonds, Adam Kilgarriff, and Martha Palmer, editors.</editor>
<location>Toulouse, France.</location>
<contexts>
<context position="2155" citStr="(2001)" startWordPosition="330" endWordPosition="330">based on supervised learning and trained on sense-tagged corpora. In this article we present an unsupervised WSD algorithm that can leverage untagged text and can perform at the level of the best supervised systems for the allnouns disambiguation task. The main drawback of the supervised approach is the difficulty of acquiring considerable amounts of training data, also known as the knowledge acquisition bottleneck. Yarowsky and Florian (2002) report that each successive doubling of the training data for WSD only leads to a 3–4% error reduction within their experimental range. Banko and Brill (2001) experiment with the problem of selection among confusable words and show that the learning curves do not converge even after * Koc¸ University, Department of Computer Engineering, 34450 Sarıyer, ˙Istanbul, Turkey. E-mail: dyuret@ku.edu.tr, myatbaz@ku.edu.tr. Submission received: 7 October 2008; revised submission received: 17 April 2009; accepted for publication: 12 September 2009. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 1 a billion words of training data. They suggest unsupervised, semi-supervised, or active learning to take advantage of l</context>
<context position="36264" citStr="(2001)" startWordPosition="6247" endWordPosition="6247">rcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the first application of the noisy channel model to unsupervised word sense disambiguation. 123 Computational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE (Dolan 1994), and ODE (Navigli 2006). Ciaramita and Altun (2006) build a supervised HMM tagger using “supersenses,” essentially the 25 WordNet noun categories we have used in our fir</context>
</contexts>
<marker>2001</marker>
<rawString>Cotton, Scott, Phil Edmonds, Adam Kilgarriff, and Martha Palmer, editors. 2001. SENSEVAL-2: Second International Workshop on Evaluating Word Sense Disambiguation Systems, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Crestan</author>
<author>M El-B`eze</author>
<author>C De Loupy</author>
</authors>
<title>Improving WSD with multi-level view of context monitored by similarity measure.</title>
<date>2001</date>
<booktitle>In Proceedings of SENSEVAL-2: Second International Workshop on Evaluating Word Sense Disambiguation Systems,</booktitle>
<location>Toulouse.</location>
<marker>Crestan, El-B`eze, De Loupy, 2001</marker>
<rawString>Crestan, E., M. El-B`eze, and C. De Loupy. 2001. Improving WSD with multi-level view of context monitored by similarity measure. In Proceedings of SENSEVAL-2: Second International Workshop on Evaluating Word Sense Disambiguation Systems, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daume Hal</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel model for document compression.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>449--456</pages>
<location>Philadelphia, PA.</location>
<marker>Hal, Marcu, 2002</marker>
<rawString>Daume III, Hal and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 449–456, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Dolan</author>
</authors>
<title>Word sense ambiguation: clustering related senses.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th conference on Computational Linguistics,</booktitle>
<pages>05--09</pages>
<contexts>
<context position="36694" citStr="Dolan 1994" startWordPosition="6310" endWordPosition="6311">deling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE (Dolan 1994), and ODE (Navigli 2006). Ciaramita and Altun (2006) build a supervised HMM tagger using “supersenses,” essentially the 25 WordNet noun categories we have used in our first experiment in addition to 15 verb categories similarly defined. They report a supersense precision of 67.60 for nouns and verbs of Senseval-3. Table 2 gives our supersense score as 78% for Senseval-3 nouns. However, the results are not directly comparable because they do not report the noun and verb scores separately or calculate the corresponding fine-grained score to compare with other Senseval-3 results. Kohomban and Lee</context>
</contexts>
<marker>Dolan, 1994</marker>
<rawString>Dolan, W. B. 1994. Word sense ambiguation: clustering related senses. In Proceedings of the 15th conference on Computational Linguistics, pages 05–09, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdessamad Echihabi</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel approach to question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<location>Sapporo.</location>
<contexts>
<context position="35667" citStr="Echihabi and Marcu 2003" startWordPosition="6152" endWordPosition="6155">xt distributions could reveal latent senses in an unsupervised setting. 6. Related Work For a general overview of different approaches to WSD, see Navigli (2009) and Stevenson (2003). The Senseval and SemEval workshops (Cotton et al. 2001; Mihalcea and Edmonds 2004; Agirre, M`arquez, and Wicentowski 2007) are good sources of recent work, and have been used in this article to benchmark our results. Generative models based on the noisy channel framework have previously been used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation (Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the first application of the noisy channel model to unsupervised word sense disambiguation. 123 Computational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). G</context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>Echihabi, Abdessamad and Daniel Marcu. 2003. A noisy-channel approach to question answering. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 16–23, Sapporo.</rawString>
</citation>
<citation valid="true">
<title>Wordnet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Fellbaum, Christiane, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Fellbaum, Christiane, editor. 1998. Wordnet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<pages>15--403</pages>
<contexts>
<context position="36264" citStr="Goodman (2001)" startWordPosition="6246" endWordPosition="6247">i and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the first application of the noisy channel model to unsupervised word sense disambiguation. 123 Computational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE (Dolan 1994), and ODE (Navigli 2006). Ciaramita and Altun (2006) build a supervised HMM tagger using “supersenses,” essentially the 25 WordNet noun categories we have used in our fir</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Goodman, Joshua. 2001. A bit of progress in language modeling. Computer Speech and Language, 15:403–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Hawker</author>
</authors>
<title>Usyd: WSD and lexical substitution using the Web1t corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>446--453</pages>
<location>Prague.</location>
<contexts>
<context position="36066" citStr="Hawker (2007)" startWordPosition="6216" endWordPosition="6217">e models based on the noisy channel framework have previously been used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation (Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the first application of the noisy channel model to unsupervised word sense disambiguation. 123 Computational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarows</context>
</contexts>
<marker>Hawker, 2007</marker>
<rawString>Hawker, Tobias. 2007. Usyd: WSD and lexical substitution using the Web1t corpus. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 446–453, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Jansen</author>
<author>A Spink</author>
<author>A Pfaff</author>
</authors>
<title>Linguistic aspects of Web queries.</title>
<date>2000</date>
<booktitle>In Proceedings of the ASIS Annual Meeting,</booktitle>
<pages>169--176</pages>
<location>Chicago, IL.</location>
<marker>Jansen, Spink, Pfaff, 2000</marker>
<rawString>Jansen, B. J., A. Spink, and A. Pfaff. 2000. Linguistic aspects of Web queries. In Proceedings of the ASIS Annual Meeting, pages 169–176, Chicago, IL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kohomban</author>
</authors>
<institution>Upali Sathyajith and</institution>
<marker>Kohomban, </marker>
<rawString>Kohomban, Upali Sathyajith and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Sun Lee</author>
</authors>
<title>Learning semantic classes for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>34--41</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="21336" citStr="Lee 2005" startWordPosition="3451" endWordPosition="3452">ns FSB 1st 2nd 3rd Unsup senseval2 1.7 1,067 71.9 78.0 74.5 70.0 61.8 senseval3 1.7.1 892 71.0 72.0 71.2 71.0 62.6 semeval07 2.1 159 64.2 68.6 66.7 66.7 63.5 total 2,118 70.9 74.4 72.5 70.2 62.2 unsupervised system is given in the last column. The reported unsupervised systems do use the sense ordering and frequency information from WordNet. 4.1 First Experiment: The 25 WordNet Categories In previous work, descendants of 25 special WordNet synsets (known as the unique beginners) have been used as the coarse-grained semantic classes for nouns (Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005). These unique beginners were used to organize the nouns into 25 lexicographer files based on their semantic category during WordNet development. Figure 2 shows the synsets at the top of the noun hierarchy in WordNet. The 25 unique beginners have been shaded, and the two graphics show how the hierarchy evolved between the two WordNet versions used in this study. Figure 2 The top of the WordNet noun hypernym hierarchy for version 1.7 (left) and version 2.1 (right). The 25 WordNet noun categories are shaded. 118 Yuret and Yatbaz The Noisy Channel Model for Unsupervised WSD Table 2 The performanc</context>
<context position="36519" citStr="Lee 2005" startWordPosition="6286" endWordPosition="6287">ational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE (Dolan 1994), and ODE (Navigli 2006). Ciaramita and Altun (2006) build a supervised HMM tagger using “supersenses,” essentially the 25 WordNet noun categories we have used in our first experiment in addition to 15 verb categories similarly defined. They report a supersense precision of 67.60 for nouns and verbs of Senseval-3. Table 2 gives our supersense score as 78% for Senseval-3 nouns. However, the results are not directly compara</context>
</contexts>
<marker>Lee, 2005</marker>
<rawString>Wee Sun Lee. 2005. Learning semantic classes for word sense disambiguation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 34–41, Ann Arbor, MI.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kohomban</author>
</authors>
<institution>Upali Sathyajith and</institution>
<marker>Kohomban, </marker>
<rawString>Kohomban, Upali Sathyajith and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Sun Lee</author>
</authors>
<title>Optimizing classifier performance in word sense disambiguation by redefining word sense classes.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1635--1640</pages>
<location>Hyderabad.</location>
<contexts>
<context position="34860" citStr="Lee (2007)" startWordPosition="6034" endWordPosition="6035">tic classes for each data set. Table 6 summarizes the results. Note that these results are not predictive of out-of-sample accuracy because Algorithm 4 picks a specific set of semantic classes optimal for a given data set. But the results do indicate that a better set of semantic classes may lead to significantly better WSD accuracy. In particular each result in Table 6 is significantly higher than previously reported supervised or unsupervised results. How to construct a good set of semantic classes that balance specificity and identifiability is a topic of ongoing research. See Kohomban and Lee (2007) for a supervised solution using feature-based clustering that tries to maintain feature–class coherence. Non-parametric Bayesian approaches such as Teh et al. (2006) applied to context distributions could reveal latent senses in an unsupervised setting. 6. Related Work For a general overview of different approaches to WSD, see Navigli (2009) and Stevenson (2003). The Senseval and SemEval workshops (Cotton et al. 2001; Mihalcea and Edmonds 2004; Agirre, M`arquez, and Wicentowski 2007) are good sources of recent work, and have been used in this article to benchmark our results. Generative model</context>
<context position="37301" citStr="Lee (2007)" startWordPosition="6404" endWordPosition="6405">94), and ODE (Navigli 2006). Ciaramita and Altun (2006) build a supervised HMM tagger using “supersenses,” essentially the 25 WordNet noun categories we have used in our first experiment in addition to 15 verb categories similarly defined. They report a supersense precision of 67.60 for nouns and verbs of Senseval-3. Table 2 gives our supersense score as 78% for Senseval-3 nouns. However, the results are not directly comparable because they do not report the noun and verb scores separately or calculate the corresponding fine-grained score to compare with other Senseval-3 results. Kohomban and Lee (2007) go beyond the WordNet categories based on lexicographer files and experiment with clustering techniques to construct their semantic classes. Their classes are based on local features from sense-labeled data and optimize feature– class coherence rather than adhering to the WordNet hierarchy. Their supervised system achieves an accuracy of 74.7% on Senseval-2 nouns and 73.6% on Senseval-3 nouns. The systems mentioned so far are supervised WSD systems. Agirre and Martinez (2004) explore the large-scale acquisition of sense-tagged examples from the Web and train supervised, minimally supervised (</context>
</contexts>
<marker>Lee, 2007</marker>
<rawString>Wee Sun Lee. 2007. Optimizing classifier performance in word sense disambiguation by redefining word sense classes. In Proceedings of the International Joint Conference on Artificial Intelligence, pages 1635–1640, Hyderabad.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Kucera</author>
<author>W Nelson Francis</author>
</authors>
<title>Computational Analysis of Present-Day American English.</title>
<date>1967</date>
<publisher>Brown University Press,</publisher>
<location>Providence, RI.</location>
<contexts>
<context position="18992" citStr="Kucera and Francis 1967" startWordPosition="3068" endWordPosition="3071">es the 25 WordNet semantic categories for nouns, the second experiment looks at what happens when we group all the senses to just two or three semantic classes, and the final experiment optimizes the number of semantic classes using one data set (which gives 135 classes) and reports the out-of-sample result using another data set. The noun instances from the last three SensEval/SemEval English all-words tasks are used for evaluation. We focus on the disambiguation of nouns for several reasons. Nouns constitute the largest portion of content words (48% of the content words in the Brown corpus [Kucera and Francis 1967] are nouns). For many tasks and applications (e.g., Web queries [Jansen, Spink, and Pfaff 2000]) nouns are the most frequently encountered and important part of speech. Finally, WordNet has a more complete coverage of noun semantic relations than other parts of speech, which is important for our experiments with semantic classes. As described in Section 2.2 we use the model to assign each ambiguous word to its most likely semantic class in all the experiments. The lowest numbered sense in that class is taken as the fine-grained answer. Finally we apply the one sense per discourse heuristic: I</context>
</contexts>
<marker>Kucera, Francis, 1967</marker>
<rawString>Kucera, Henry and W. Nelson Francis. 1967. Computational Analysis of Present-Day American English. Brown University Press, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>C Strapparava</author>
<author>G Pezzulo</author>
<author>A Gliozzo</author>
</authors>
<title>The role of domain information in word sense disambiguation.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>04</issue>
<contexts>
<context position="36560" citStr="Magnini et al. 2003" startWordPosition="6290" endWordPosition="6293"> Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE (Dolan 1994), and ODE (Navigli 2006). Ciaramita and Altun (2006) build a supervised HMM tagger using “supersenses,” essentially the 25 WordNet noun categories we have used in our first experiment in addition to 15 verb categories similarly defined. They report a supersense precision of 67.60 for nouns and verbs of Senseval-3. Table 2 gives our supersense score as 78% for Senseval-3 nouns. However, the results are not directly comparable because they do not report the noun a</context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, Gliozzo, 2003</marker>
<rawString>Magnini, B., C. Strapparava, G. Pezzulo, and A. Gliozzo. 2003. The role of domain information in word sense disambiguation. Natural Language Engineering, 8(04):359–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Martinez</author>
<author>O Lopez de Lacalle</author>
<author>E Agirre</author>
</authors>
<title>On the use of automatically acquired examples for all-nouns word sense disambiguation.</title>
<date>2008</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>33--79</pages>
<marker>Martinez, de Lacalle, Agirre, 2008</marker>
<rawString>Martinez, D., O. Lopez de Lacalle, and E. Agirre. 2008. On the use of automatically acquired examples for all-nouns word sense disambiguation. Journal of Artificial Intelligence Research, 33:79–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Phil Edmonds</author>
<author>editors</author>
</authors>
<date>2004</date>
<booktitle>SENSEVAL-3: The Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<location>Barcelona.</location>
<marker>Mihalcea, Edmonds, editors, 2004</marker>
<rawString>Mihalcea, Rada and Phil Edmonds, editors. 2004. SENSEVAL-3: The Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Meaningful clustering of senses helps boost word sense disambiguation performance.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>105--112</pages>
<location>Sydney.</location>
<contexts>
<context position="36718" citStr="Navigli 2006" startWordPosition="6314" endWordPosition="6315"> in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE (Dolan 1994), and ODE (Navigli 2006). Ciaramita and Altun (2006) build a supervised HMM tagger using “supersenses,” essentially the 25 WordNet noun categories we have used in our first experiment in addition to 15 verb categories similarly defined. They report a supersense precision of 67.60 for nouns and verbs of Senseval-3. Table 2 gives our supersense score as 78% for Senseval-3 nouns. However, the results are not directly comparable because they do not report the noun and verb scores separately or calculate the corresponding fine-grained score to compare with other Senseval-3 results. Kohomban and Lee (2007) go beyond the Wo</context>
</contexts>
<marker>Navigli, 2006</marker>
<rawString>Navigli, Roberto. 2006. Meaningful clustering of senses helps boost word sense disambiguation performance. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 105–112, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="35204" citStr="Navigli (2009)" startWordPosition="6084" endWordPosition="6085">articular each result in Table 6 is significantly higher than previously reported supervised or unsupervised results. How to construct a good set of semantic classes that balance specificity and identifiability is a topic of ongoing research. See Kohomban and Lee (2007) for a supervised solution using feature-based clustering that tries to maintain feature–class coherence. Non-parametric Bayesian approaches such as Teh et al. (2006) applied to context distributions could reveal latent senses in an unsupervised setting. 6. Related Work For a general overview of different approaches to WSD, see Navigli (2009) and Stevenson (2003). The Senseval and SemEval workshops (Cotton et al. 2001; Mihalcea and Edmonds 2004; Agirre, M`arquez, and Wicentowski 2007) are good sources of recent work, and have been used in this article to benchmark our results. Generative models based on the noisy channel framework have previously been used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation (Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Navigli, Roberto. 2009. Word sense disambiguation: A survey. ACM Computing Surveys, 41(2):1–69.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W Peters</author>
<author>I Peters</author>
<author>P Vossen 1998</author>
</authors>
<title>Automatic sense clustering in EuroWordNet.</title>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation,</booktitle>
<pages>409--416</pages>
<location>Granada.</location>
<marker>Peters, Peters, 1998, </marker>
<rawString>Peters, W., I. Peters, and P. Vossen.1998. Automatic sense clustering in EuroWordNet. In Proceedings of the International Conference on Language Resources and Evaluation, pages 409–416, Granada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Two decades of statistical language modeling: Where do we go from here?</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>88--1270</pages>
<contexts>
<context position="36244" citStr="Rosenfeld (2000)" startWordPosition="6243" endWordPosition="6244">ion answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the first application of the noisy channel model to unsupervised word sense disambiguation. 123 Computational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE (Dolan 1994), and ODE (Navigli 2006). Ciaramita and Altun (2006) build a supervised HMM tagger using “supersenses,” essentially the 25 WordNet noun categories we </context>
</contexts>
<marker>Rosenfeld, 2000</marker>
<rawString>Rosenfeld, Ronald. 2000. Two decades of statistical language modeling: Where do we go from here? Proceedings of the IEEE, 88:1270–1278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude Elwood Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>The Bell System Technical Journal,</journal>
<volume>27</volume>
<pages>623--656</pages>
<contexts>
<context position="4496" citStr="Shannon 1948" startWordPosition="670" endWordPosition="671">riples are not observed in the training data, but context-word frequencies (from untagged text) and word-sense frequencies (from dictionaries or other sources) are used in model building. One of the main problems we explore in this study is the estimation of contextdependent sense probabilities when no context–sense pairs have been observed in the training data. The first contribution of this article is a probabilistic generative model for word sense disambiguation that seamlessly integrates unlabeled text data into the model building process. Our approach is based on the noisy channel model (Shannon 1948), which has been an essential ingredient in fields such as speech recognition and machine translation. In this study we demonstrate that the noisy channel model can also be the key component for unsupervised word sense disambiguation, provided we can solve the context-dependent sense distribution problem. In Section 2.1 we show one way to estimate the context-dependent sense distribution without using any sense-tagged data. Section 2.2 outlines the complete unsupervised WSD algorithm using this model. We estimate the distribution of coarse-grained semantic classes rather than fine-grained sens</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Shannon, Claude Elwood. 1948. A mathematical theory of communication. The Bell System Technical Journal, 27:379–423, 623–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
</authors>
<title>Word Sense Disambiguation: The Case for Combinations of Knowledge Sources.</title>
<date>2003</date>
<location>CSLI, Stanford, CA.</location>
<contexts>
<context position="35225" citStr="Stevenson (2003)" startWordPosition="6087" endWordPosition="6088">lt in Table 6 is significantly higher than previously reported supervised or unsupervised results. How to construct a good set of semantic classes that balance specificity and identifiability is a topic of ongoing research. See Kohomban and Lee (2007) for a supervised solution using feature-based clustering that tries to maintain feature–class coherence. Non-parametric Bayesian approaches such as Teh et al. (2006) applied to context distributions could reveal latent senses in an unsupervised setting. 6. Related Work For a general overview of different approaches to WSD, see Navigli (2009) and Stevenson (2003). The Senseval and SemEval workshops (Cotton et al. 2001; Mihalcea and Edmonds 2004; Agirre, M`arquez, and Wicentowski 2007) are good sources of recent work, and have been used in this article to benchmark our results. Generative models based on the noisy channel framework have previously been used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation (Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the first applica</context>
</contexts>
<marker>Stevenson, 2003</marker>
<rawString>Stevenson, Mark. 2003. Word Sense Disambiguation: The Case for Combinations of Knowledge Sources. CSLI, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
<author>M I Jordan</author>
<author>M J Beal</author>
<author>D M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="35026" citStr="Teh et al. (2006)" startWordPosition="6054" endWordPosition="6057"> specific set of semantic classes optimal for a given data set. But the results do indicate that a better set of semantic classes may lead to significantly better WSD accuracy. In particular each result in Table 6 is significantly higher than previously reported supervised or unsupervised results. How to construct a good set of semantic classes that balance specificity and identifiability is a topic of ongoing research. See Kohomban and Lee (2007) for a supervised solution using feature-based clustering that tries to maintain feature–class coherence. Non-parametric Bayesian approaches such as Teh et al. (2006) applied to context distributions could reveal latent senses in an unsupervised setting. 6. Related Work For a general overview of different approaches to WSD, see Navigli (2009) and Stevenson (2003). The Senseval and SemEval workshops (Cotton et al. 2001; Mihalcea and Edmonds 2004; Agirre, M`arquez, and Wicentowski 2007) are good sources of recent work, and have been used in this article to benchmark our results. Generative models based on the noisy channel framework have previously been used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation (Brown et al. 1990), que</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Teh, Y. W., M. I. Jordan, M. J. Beal, and D. M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word sense disambiguation using statistical models of Roget’s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>454--460</pages>
<location>Nantes.</location>
<contexts>
<context position="36674" citStr="Yarowsky 1992" startWordPosition="6307" endWordPosition="6308">(2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE (Dolan 1994), and ODE (Navigli 2006). Ciaramita and Altun (2006) build a supervised HMM tagger using “supersenses,” essentially the 25 WordNet noun categories we have used in our first experiment in addition to 15 verb categories similarly defined. They report a supersense precision of 67.60 for nouns and verbs of Senseval-3. Table 2 gives our supersense score as 78% for Senseval-3 nouns. However, the results are not directly comparable because they do not report the noun and verb scores separately or calculate the corresponding fine-grained score to compare with other Senseval-3 resul</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>Yarowsky, David. 1992. Word sense disambiguation using statistical models of Roget’s categories trained on large corpora. In Proceedings of the 15th International Conference on Computational Linguistics, pages 454–460, Nantes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Radu Florian</author>
</authors>
<title>Evaluating sense disambiguation across diverse parameter spaces.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="1996" citStr="Yarowsky and Florian (2002)" startWordPosition="301" endWordPosition="304">guous word in a given context. An accurate WSD system would benefit applications such as machine translation and information retrieval. The most successful WSD systems to date are based on supervised learning and trained on sense-tagged corpora. In this article we present an unsupervised WSD algorithm that can leverage untagged text and can perform at the level of the best supervised systems for the allnouns disambiguation task. The main drawback of the supervised approach is the difficulty of acquiring considerable amounts of training data, also known as the knowledge acquisition bottleneck. Yarowsky and Florian (2002) report that each successive doubling of the training data for WSD only leads to a 3–4% error reduction within their experimental range. Banko and Brill (2001) experiment with the problem of selection among confusable words and show that the learning curves do not converge even after * Koc¸ University, Department of Computer Engineering, 34450 Sarıyer, ˙Istanbul, Turkey. E-mail: dyuret@ku.edu.tr, myatbaz@ku.edu.tr. Submission received: 7 October 2008; revised submission received: 17 April 2009; accepted for publication: 12 September 2009. © 2010 Association for Computational Linguistics Comput</context>
</contexts>
<marker>Yarowsky, Florian, 2002</marker>
<rawString>Yarowsky, David and Radu Florian. 2002. Evaluating sense disambiguation across diverse parameter spaces. Natural Language Engineering, 8(4):293–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>Some experiments with a Naive Bayes WSD system.</title>
<date>2004</date>
<booktitle>In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>265--268</pages>
<location>Barcelona.</location>
<contexts>
<context position="2810" citStr="Yuret (2004)" startWordPosition="418" endWordPosition="419">mong confusable words and show that the learning curves do not converge even after * Koc¸ University, Department of Computer Engineering, 34450 Sarıyer, ˙Istanbul, Turkey. E-mail: dyuret@ku.edu.tr, myatbaz@ku.edu.tr. Submission received: 7 October 2008; revised submission received: 17 April 2009; accepted for publication: 12 September 2009. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 1 a billion words of training data. They suggest unsupervised, semi-supervised, or active learning to take advantage of large data sets when labeling is expensive. Yuret (2004) observes that in a supervised naive Bayes WSD system trained on SemCor, approximately half of the test instances do not contain any of the contextual features (e.g., neighboring content words or local collocation patterns) observed in the training data. SemCor is the largest publicly available corpus of sense-tagged text, and has only about a quarter million sense-tagged words. In contrast, our unsupervised system uses the Web1T data set (Brants and Franz 2006) for unlabeled examples, which contains counts from a 1012 word corpus derived from publicly-available Web pages. A note on the term “</context>
</contexts>
<marker>Yuret, 2004</marker>
<rawString>Yuret, Deniz. 2004. Some experiments with a Naive Bayes WSD system. In Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 265–268, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>KU: Word sense disambiguation by substitution.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>207--214</pages>
<location>Prague.</location>
<contexts>
<context position="36048" citStr="Yuret (2007)" startWordPosition="6213" endWordPosition="6214">esults. Generative models based on the noisy channel framework have previously been used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation (Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the first application of the noisy channel model to unsupervised word sense disambiguation. 123 Computational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources suc</context>
</contexts>
<marker>Yuret, 2007</marker>
<rawString>Yuret, Deniz. 2007. KU: Word sense disambiguation by substitution. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 207–214, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>Smoothing a tera-word language model.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<contexts>
<context position="36138" citStr="Yuret (2008)" startWordPosition="6227" endWordPosition="6228">or speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation (Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the first application of the noisy channel model to unsupervised word sense disambiguation. 123 Computational Linguistics Volume 36, Number 1 Using statistical language models based on large corpora for WSD has been explored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001). Grouping similar senses into semantic classes for WSD has been explored in previous work. Senses that are similar have been identified using WordNet relations (Peters, Peters, and Vossen 1998; Crestan, El-B`eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE (Dolan 1994), and ODE (Navigli 2006). Ciaramita and Altu</context>
<context position="42538" citStr="Yuret (2008)" startWordPosition="7246" endWordPosition="7247">s automatically matched to the target word. As a result, we estimated P(W|C) for about 10,000 words for each noun context and assumed the other words had zero probability. The n-grams required for all the contexts were listed, and their counts were extracted from the Web 1T data set in one pass. The P(W|C) was estimated for all the words and contexts based on these counts. In the end, we only used the 100 most likely words in each context for efficiency, as the difference in results using the whole distribution was not significant. For more details on smoothing with a large language model see Yuret (2008), although we did not see a significant difference in WSD performance based on the smoothing method used. Acknowledgments References This work was supported in part by Agirre, E. and D. Martinez. 2004. the Scientific and Technical Research Unsupervised WSD based on Council of Turkey (T ¨UB˙ITAK Project automatically retrieved examples: 108E228). We would like to thank Peter The importance of bias. In Proceedings Turney, Rada Mihalcea, Diana McCarthy, of the Conference on Empirical Methods and the four anonymous reviewers in Natural Language Processing (EMNLP), for their helpful comments and su</context>
</contexts>
<marker>Yuret, 2008</marker>
<rawString>Yuret, Deniz. 2008. Smoothing a tera-word language model. In Proceedings of ACL-08: HLT, Short Papers,</rawString>
</citation>
<citation valid="false">
<pages>141--144</pages>
<location>Columbus, OH.</location>
<marker></marker>
<rawString>pages 141–144, Columbus, OH.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>