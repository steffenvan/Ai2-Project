<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9836635">
Re-structuring, Re-labeling, and Re-aligning
for Syntax-Based Machine Translation
</title>
<author confidence="0.993447">
Wei Wang*
</author>
<affiliation confidence="0.447418">
Language Weaver, Inc.
</affiliation>
<author confidence="0.981669">
Jonathan May**
</author>
<affiliation confidence="0.852058">
USC/Information Sciences Institute
</affiliation>
<author confidence="0.965451">
Kevin Knightt
</author>
<affiliation confidence="0.902575">
USC/Information Sciences Institute
</affiliation>
<author confidence="0.872373">
Daniel Marcul
</author>
<affiliation confidence="0.424402">
Language Weaver, Inc.
</affiliation>
<bodyText confidence="0.998741090909091">
This article shows that the structure of bilingual material from standard parsing and alignment
tools is not optimal for training syntax-based statistical machine translation (SMT) systems.
We present three modifications to the MT training data to improve the accuracy of a state-of-the-
art syntax MT system: re-structuring changes the syntactic structure of training parse trees to
enable reuse of substructures; re-labeling alters bracket labels to enrich rule application context;
and re-aligning unifies word alignment across sentences to remove bad word alignments and
refine good ones. Better structures, labels, and word alignments are learned by the EM algorithm.
We show that each individual technique leads to improvement as measured by BLEU, and we
also show that the greatest improvement is achieved by combining them. We report an overall
1.48 BLEU improvement on the NIST08 evaluation set over a strong baseline in Chinese/English
translation.
</bodyText>
<sectionHeader confidence="0.975854" genericHeader="abstract">
1. Background
</sectionHeader>
<bodyText confidence="0.998382142857143">
Syntactic methods have recently proven useful in statistical machine translation (SMT).
In this article, we explore different ways of exploiting the structure of bilingual material
for syntax-based SMT. In particular, we ask what kinds of tree structures, tree labels,
and word alignments are best suited for improving end-to-end translation accuracy.
We begin with structures from standard parsing and alignment tools, then use the EM
algorithm to revise these structures in light of the translation task. We report an overall
+1.48 BLEU improvement on a standard Chinese-to-English test.
</bodyText>
<note confidence="0.5384815">
* 6060 Center Drive, Suite 150, Los Angeles, CA, 90045, USA. E-mail: wwang@languageweaver.com.
** 4676 Admiralty Way, Marina del Rey, CA, 90292, USA. E-mail: jonmay@isi.edu.
</note>
<footnote confidence="0.8498045">
t 4676 Admiralty Way, Marina del Rey, CA, 90292, USA. E-mail: knight@isi.edu.
t 6060 Center Drive, Suite 150, Los Angeles, CA, 90045, USA. E-mail: dmarcu@languageweaver.com.
</footnote>
<note confidence="0.826757">
Submission received: 6 November 2008; revised submission received: 10 September 2009; accepted for
publication: 1 January 2010.
© 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 2
</note>
<bodyText confidence="0.999934032258065">
We carry out our experiments in the context of a string-to-tree translation system.
This system accepts a Chinese string as input, and it searches through a multiplicity of
English tree outputs, seeking the one with the highest score. The string-to-tree frame-
work is motivated by a desire to improve target-language grammaticality. For example,
it is common for string-based MT systems to output sentences with no verb. By contrast,
the string-to-tree framework forces the output to respect syntactic requirements—for
example, if the output is a syntactic tree whose root is S (sentence), then the S will gen-
erally have a child of type VP (verb phrase), which will in turn contain a verb. Another
motivation is better treatment of function words. Often, these words are not literally
translated (either by themselves or as part of a phrase), but rather they control what
happens in the translation, as with case-marking particles or passive-voice particles.
Finally, much of the re-ordering we find in translation is syntactically motivated, and
this can be captured explicitly with syntax-based translation rules. Tree-to-tree systems
are also promising, but in this work we concentrate only on target-language syntax. The
target-language generation problem presents a difficult challenge, whereas the source
sentence is fixed and usually already grammatical.
To prepare training data for such a system, we begin with a bilingual text that has
been automatically processed into segment pairs. We require that the segments be single
sentences on the English side, whereas the corresponding Chinese segments may be
sentences, sentence fragments, or multiple sentences. We then parse the English side of
the bilingual text using a re-implementation of the Collins (1997) parsing model, which
we train on the Penn English Treebank (Marcus, Santorini, and Marcinkiewicz 1993).
Finally, we word-align the segment pairs according to IBM Model 4 (Brown et al. 1993).
Figure 1 shows a sample (tree, string, alignment) triple.
We build two generative statistical models from this data. First, we construct a
smoothed n-gram language model (Kneser and Ney 1995; Stolcke 2002) out of the
English side of the bilingual data. This model assigns a probability P(e) to any candidate
translation, rewarding translations whose subsequences have been observed frequently
in the training data.
Second, we build a syntax-based translation model that we can use to produce
candidate English trees from Chinese strings. Following previous work in noisy-channel
</bodyText>
<figureCaption confidence="0.814027">
Figure 1
</figureCaption>
<bodyText confidence="0.693241">
A sample learning case for the syntax-based machine translation system described in this article.
</bodyText>
<page confidence="0.995091">
248
</page>
<note confidence="0.851131">
Wang et al. Re-structuring, Re-labeling, and Re-aligning
SMT (Brown et al. 1993), our model operates in the English-to-Chinese direction—
</note>
<bodyText confidence="0.997737866666666">
we envision a generative top–down process by which an English tree is gradually
transformed (by probabilistic rules) into an observed Chinese string. We represent a
collection of such rules as a tree transducer (Knight and Graehl 2005). In order to
construct this transducer from parsed and word-aligned data, we use the GHKM rule
extraction algorithm of Galley et al. (2004). This algorithm computes the unique set of
minimal rules needed to explain any sentence pair in the data. Figure 2 shows all the
minimal rules extracted from the example (tree, string, alignment) triple in Figure 1.
Note that rules specify rotation (e.g., R1, R5), direct translation (R3, R10), insertion and
deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example
form a derivation tree.
We collect all rules over the entire bilingual corpus, and we normalize rule counts
in this way: P(rule) = count(rule)
count(LHS-root(rule)). When we apply these probabilities to derive an
English sentence e and a corresponding Chinese sentence c, we wind up computing the
joint probability P(e, c). We smooth the rule counts with Good–Turing smoothing (Good
1953).
This extraction method assigns each unaligned Chinese word to a default rule in
the derivation tree. We next follow Galley et al. (2006) in allowing unaligned Chinese
words to participate in multiple translation rules. In this case, we obtain a derivation
forest of minimal rules. Galley et al. show how to use EM to count rules over deriva-
tion forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley
et al. in collecting composed rules, namely, compositions of minimal rules. These larger
rules have been shown to substantially improve translation accuracy (Galley et al. 2006;
DeNeefe et al. 2007). Figure 3 shows some of the additional rules.
With these models, we can decode a new Chinese sentence by enumerating and
scoring all of the English trees that can be derived from it by rule. The score is a
weighted product of P(e) and P(e, c). To search efficiently, we employ the CKY dynamic-
programming parsing algorithm (Yamada and Knight 2002; Galley et al. 2006). This
algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix,
we store the non-terminal symbol at the root of the English tree being built up. We also
</bodyText>
<figureCaption confidence="0.740986">
Figure 2
</figureCaption>
<bodyText confidence="0.4766">
Minimal rules extracted from the learning case in Figure 1 using the GHKM procedure.
</bodyText>
<page confidence="0.989912">
249
</page>
<figure confidence="0.500083">
Computational Linguistics Volume 36, Number 2
</figure>
<figureCaption confidence="0.847271">
Figure 3
</figureCaption>
<bodyText confidence="0.987046305555555">
Additional rules extracted from the learning case in Figure 1.
store English words that appear at the left and right corners of the tree, as these are
needed for computing the P(e) score when cells are combined. For CKY to work, all
transducer rules must be broken down, or binarized, into rules that contain at most two
variables—more efficient search can be gained if this binarization produces rules that
can be incrementally scored by the language model (Melamed, Satta, and Wellington
2004; Zhang et al. 2006). Finally, we employ cube pruning (Chiang 2007) for further
efficiency in the search.
When scoring translation candidates, we add several smaller models. One model
rewards longer translation candidates, off-setting the language model’s desire for short
output. Other models punish rules that drop Chinese content words or introduce
spurious English content words. We also include lexical smoothing models (Gale and
Sampson 1996; Good 1953) to help distinguish good low-count rules from bad low-
count rules. The final score of a translation candidate is a weighted linear combination
of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain
weights through minimum error-rate training (Och 2003).
The system thus constructed performs fairly well at Chinese-to-English translation,
as reflected in the NIST06 common evaluation of machine translation quality.1
However, it would be surprising if the parse structures and word alignments in our
bilingual data were somehow perfectly suited to syntax-based SMT—we have so far
used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and
Knight (2006) already investigated whether different syntactic labels would be more
appropriate for SMT, though their study was carried out on a weak baseline translation
system. In this article, we take a broad view and investigate how changes to syntactic
structures, syntactic labels, and word alignments can lead to substantial improvements
in translation quality on top of a strong baseline. We design our methods around
problems that arise in MT data whose parses and alignments use some Penn Treebank-
style annotations. We believe that some of the techniques will apply to other annotation
schemes, but conclusions here are limited to Penn Treebank-style trees.
The rest of this article is structured as follows. Section 2 describes the corpora and
model configurations used in our experiments. In each of the next three sections we
present a technique for modifying the training data to improve syntax MT accuracy:
tree re-structuring in Section 3, tree re-labeling in Section 4, and re-aligning in Section 5.
In each of these three sections, we also present experiment results to show the impact
of each individual technique on end-to-end MT accuracy. Section 6 shows the improve-
ment made by combining all three techniques. We conclude in Section 7.
</bodyText>
<footnote confidence="0.893257">
1 http://nist.gov/speech/tests/mt/2006/doc/mt06eval official results.html.
</footnote>
<page confidence="0.988427">
250
</page>
<note confidence="0.987175">
Wang et al. Re-structuring, Re-labeling, and Re-aligning
</note>
<sectionHeader confidence="0.982018" genericHeader="method">
2. Corpora for Experiments
</sectionHeader>
<bodyText confidence="0.999919333333333">
For our experiments, we use a 245 million word Chinese/English bitext, available from
LDC. A re-implementation of the Collins (1997) parser runs on the English half of the
bitext to produce parse trees, and GIZA runs on the entire bitext to produce M4 word
alignments. We extract a subset of 36 million words from the entire bitext, by selecting
only sentences in the mainland news domain. We extract translation rules from these
selected 36 million words. Experiments show that our Chinese/English syntax MT
systems built from this selected bitext give as high BLEU scores as from the entire bitext.
Our development set consists of 1,453 lines and is extracted from the NIST02–
NIST05 evaluation sets, for tuning of feature weights. The development set is from the
newswire domain, and we chose it to represent a wide period of time rather than a single
year. We use the NIST08 evaluation set as our test set. Because the NIST08 evaluation set
is a mix of newswire text and Web text, we also report the BLEU scores on the newswire
portion.
We use two 5-gram language models. One is trained on the English half of the bitext.
The other is trained on one billion words of monolingual data. Kneser–Ney smoothing
(Kneser and Ney 1995) is applied to both language models. Language models are
represented using randomized data structures similar to those of Talbot and Osborne
(2007) in decoding for efficient RAM usage.
To test the significance of improvements over the baseline, we compute paired boot-
strap p-values (Koehn 2004) for BLEU between the baseline system and each improved
system.
</bodyText>
<sectionHeader confidence="0.915805" genericHeader="method">
3. Re-structuring Trees for Training
</sectionHeader>
<bodyText confidence="0.999961789473684">
Our translation system is trained on Chinese/English data, where the English side
has been automatically parsed into Penn Treebank-style trees. One striking fact about
these trees is that they contain many flat structures. For example, base noun phrases
frequently have five or more direct children. It is well known in monolingual parsing
research that these flat structures cause problems. Although thousands of rewrite rules
can be learned from the Penn Treebank, these rules still do not cover the new rewrites
observed in held-out test data. For this reason, and to extract more general knowl-
edge, many monolingual parsing models are markovized so that they can produce flat
structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing
systems binarize the training trees in a pre-processing step, then learn to model the
binarized corpus (Petrov et al. 2006); after parsing, their results are flattened back
in a post-processing step. In addition, Johnson (1998b) shows that different types of
tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II
representation) can have a large effect on the parsing performance of a PCFG estimated
from these trees.
We find that flat structures are also problematic for syntax-based machine transla-
tion. The rules we learn from tree/string/alignment triples often lack sufficient gener-
alization power. For example, consider the training samples in Figure 4. We should be
able to learn enough from these two samples to translate the new phrase
</bodyText>
<equation confidence="0.341541666666667">
IA4jG-�/TliAN/Tl� fn t0` M.
VIKTOR CHERNOMYRDIN AND HIS COLLEAGUE
into its English equivalent victor chernomyrdin and his colleagues.
</equation>
<page confidence="0.990936">
251
</page>
<figure confidence="0.866738">
Computational Linguistics Volume 36, Number 2
</figure>
<figureCaption confidence="0.970229">
Figure 4
</figureCaption>
<subsectionHeader confidence="0.863599">
Learning translation rules from flat English structures.
</subsectionHeader>
<bodyText confidence="0.99965716">
However, the learned rules R12 and R13 do not fit together nicely. R12 can translate
IAffc-t)],/�� into an English base noun phrase (NPB) that includes viktor
chernomyrdin, but only if it is preceded by words that translate into an English JJ and
an English NNP. Likewise, R13 can translate fn tO` nI into an NPB that includes
and his colleagues, but only if preceded by two NNPs. Both rules want to create an NPB,
and neither can supply the other with what it needs.
If we re-structure the training trees as shown in Figure 5, we get much better
behavior. Now rule R14 translates �9A49c-t)],/��,/� into a free-standing NPB. This
gives rule R15 the ability to translate fn t O` n 1, because it finds the necessary
NPB to its left.
Here, we are re-structuring the trees in our MT training data by binarizing them.
This allows us to extract better translation rules, though of course an extracted rule may
have more than two variables. Whether the rules themselves should be binarized is a
separate question, addressed in Melamed, Satta, and Wellington (2004) and Zhang et al.
(2006). One can decide to re-structure training data trees, binarize translation rules, or
do both, or do neither. Here we focus on English tree re-structuring.
In this section, we explore the generalization ability of simple re-structuring meth-
ods like left-, right-, and head-binarization, and also their combinations. Simple bina-
rization methods binarize syntax trees in a consistent fashion (left-, right-, or head-)
and thus cannot guarantee that all the substructures can be factored out. For example,
consistent right binarization of the training examples in Figure 4 makes available R14,
but misses R15. We therefore also introduce a parallel re-structuring method in which
we binarize both to the left and right at the same time, resulting in a binarization
forest. We employ the EM algorithm (Dempster, Laird, and Rubin 1977) to learn the
binarization bias for each tree node in the corpus from the parallel alternatives.
</bodyText>
<page confidence="0.995758">
252
</page>
<note confidence="0.744974">
Wang et al. Re-structuring, Re-labeling, and Re-aligning
</note>
<figureCaption confidence="0.869716">
Figure 5
</figureCaption>
<subsectionHeader confidence="0.983617">
Learning translation rules from binarized English structures.
3.1 Some Concepts
</subsectionHeader>
<bodyText confidence="0.999895761904762">
We now explain some concepts to facilitate the descriptions of the re-structuring meth-
ods. We train our translation model on alignment graphs (Galley et al. 2004). An
alignment graph is a tuple of a source-language sentence f, a target-language parse tree
that yields e and translates from f, and the word alignments a between e and f. The
graphs in Figures 1, 4, and 5 are examples of alignment graphs.
In the alignment graph, a node in the parse tree is called admissible if rules can be
extracted from it. We can extract rules from a node if and only if the yield of the tree node
is consistent with the word alignments—the f string covered by the node is contiguous
but not empty, and the f string does not align to any e string that is not covered by
the node. An admissible tree node is one where rules overlap. Figure 6 shows different
binarizations of the left tree in Figure 4. In this figure, the NPB node in tree (1) is not
admissible because the f string, V-C, that the node covers also aligns to NNP3, which is
not covered by the NPB. Node NPB in tree (2), on the other hand, is admissible.
A set of sibling tree nodes is called factorizable if we can form an admissible
new node dominating them. In Figure 6, sibling nodes NNP1, NNP2, and NNP3 are
factorizable because we can factorize them out and form a new node NPB, resulting
in tree (2). Sibling tree nodes JJ, NNP1, and NNP2 are not factorizable. Not all sibling
nodes are factorizable, so not all sub-phrases can be acquired and syntactified. Our main
purpose is to re-structure parse trees by factorization such that syntactified sub-phrases
can be employed in translation.
With these concepts defined, we now present the re-structuring methods.
</bodyText>
<page confidence="0.996946">
253
</page>
<figure confidence="0.86566">
Computational Linguistics Volume 36, Number 2
</figure>
<figureCaption confidence="0.974034">
Figure 6
</figureCaption>
<bodyText confidence="0.988127333333333">
Left, right, and head binarizations on the left tree in Figure 4. Tree leaves of nodes JJ and NNP1
are omitted for convenience. Heads are marked with ∗. New nonterminals introduced by
binarization are denoted by X-bars.
</bodyText>
<subsectionHeader confidence="0.999968">
3.2 Binarizing Syntax Trees
</subsectionHeader>
<bodyText confidence="0.9995854">
We re-structure parse trees by binarizing the trees. We are going to binarize a tree node
n that dominates r children n1, ..., nr. Binarization is performed by introducing new tree
nodes to dominate a subset of the children nodes. We allow ourselves to form only one
new node at a time to avoid over-generalization. Because labeling is not the concern of
this section, we re-label the newly formed nodes as n.
</bodyText>
<listItem confidence="0.796332">
3.2.1 Simple Binarization Methods. The left binarization of node n (e.g., the NPB in tree
(1) of Figure 6) factorizes the leftmost r − 1 children by forming a new node n (i.e., the
</listItem>
<bodyText confidence="0.99335015">
NPB in tree (1)) to dominate them, leaving the last child nr untouched; and then makes
the new node n the left child of n. The method then recursively left-binarizes the newly
formed node n until two leaves are reached. We left-binarize the left tree in Figure 4 into
Figure 6 (1).
The right binarization of node n factorizes the rightmost r − 1 children by forming
a new node n (i.e., the NPB in tree (2)) to dominate them, leaving the first child n1
untouched; and then makes the new node n the right child of n. The method then
recursively right-binarizes the newly formed node n. For instance, we right-binarize
the left tree in Figure 4 into Figure 6 (2) and then into Figure 6 (6).
The head binarization of node n left-binarizes n if the head is the first child;
otherwise, it right-binarizes n. We prefer right-binarization to left-binarization when
both are applicable under the head restriction because our initial motivation was to
generalize the NPB-rooted translation rules. As we show in experiments, binarization
of other types of phrases contributes to translation accuracy as well.
Any of these simple binarization methods is easy to implement, but each in itself
is incapable of giving us all the factorizable sub-phrases. Binarizing all the way to the
left, for example, from unbinarized tree to tree (1) and to tree (3) in Figure 6, does not
enable us to acquire a substructure that yields NNP2, NNP3, and their translational
equivalences. To obtain more factorizable sub-phrases, we need to parallel-binarize in
both directions.
</bodyText>
<page confidence="0.998516">
254
</page>
<note confidence="0.741618">
Wang et al. Re-structuring, Re-labeling, and Re-aligning
</note>
<figureCaption confidence="0.996877">
Figure 7
</figureCaption>
<bodyText confidence="0.936235235294118">
Packed forest obtained by packing trees (3) and (6) in Figure 6.
3.2.2 Parallel Binarization. Simple binarizations transform a parse tree into another single
parse tree. Parallel binarization transforms a parse tree into a binarization forest, packed
to enable dynamic programming when we extract translation rules from it.
Borrowing terms from parsing semirings (Goodman 1999), a packed forest is com-
posed of additive forest nodes (®-nodes) and multiplicative forest nodes (0-nodes). In
the binarization forest, a ®-node corresponds to a tree node in the unbinarized tree or a
new tree node introduced during tree binarization; and this ®-node composes several
®-nodes, forming a one-level substructure that is observed in the unbinarized tree or
in one of its binarized tree. A ®-node corresponds to alternative ways of binarizing the
same tree node and it contains one or more ®-nodes. The same ®-node can appear in
more than one place in the packed forest, enabling sharing. Figure 7 shows a packed
forest obtained by packing trees (3) and (6) in Figure 6 via the following tree parallel
binarization algorithm.
We use a memoization procedure to recursively parallel-binarize a parse tree.
To parallel-binarize a tree node n that has children n1,..., nr, we employ the following
steps:
</bodyText>
<listItem confidence="0.992805071428572">
• If r &lt; 2, parallel-binarize tree nodes n1, ..., nr, producing binarization
®-nodes ®(n1), ..., ®(nr), respectively. Construct node ®(n) as the parent
of ®(n1),..., ®(nr). Construct an additive node ®(n) as the parent of ®(n).
Otherwise, execute the following steps.
• Right-binarize n, if any contiguous2 subset of children n2,..., nr is
factorizable, by introducing an intermediate tree node labeled as n. We
recursively parallel-binarize n to generate a binarization forest node ®(n).
We also recursively parallel-binarize n1, forming a binarization forest
node ®(n1). We form a multiplicative forest node ®R as the parent of
®(n1) and ®(n).
• Left-binarize n if any contiguous subset of n1,..., nr−1 is factorizable and
if this subset contains n1. Similar to the previous right-binarization,
we introduce an intermediate tree node labeled as n, recursively
parallel-binarize n to generate a binarization forest node ®(n), recursively
</listItem>
<bodyText confidence="0.492835">
2 For practical purposes we factorize only subsets that cover contiguous spans to avoid introducing
discontiguous constituents. In principle, the algorithm works fine without this condition.
</bodyText>
<page confidence="0.99087">
255
</page>
<note confidence="0.290528">
Computational Linguistics Volume 36, Number 2
</note>
<bodyText confidence="0.97157">
parallel-binarize nr to generate a binarization forest node ⊕(nr), and then
form a multiplicative forest node ⊗L as the parent of ⊕(n) and ⊕(nr).
</bodyText>
<listItem confidence="0.901814">
• Form an additive node ⊕(n) as the parent of the two already formed
multiplicative nodes ⊗L and ⊗R.
</listItem>
<bodyText confidence="0.999924823529412">
The (left and right) binarization conditions consider any subset to enable the fac-
torization of small constituents. For example, in the left tree of Figure 4, although the
JJ, NNP1, and NNP2 children of the NPB are not factorizable, the subset JJ NNP1 is
factorizable. The binarization from this tree to the tree in Figure 6 (1) serves as a relaying
step for us to factorize JJ and NNP1 in the tree in Figure 6 (3). The left-binarization
condition is stricter than the right-binarization condition to avoid spurious binarization,
that is, to avoid the same subconstituent being reached via both binarizations.
In parallel binarization, nodes are not always binarizable in both directions. For
example, we do not need to right-binarize tree (2) because NNP2 and NNP3 are not
factorizable, and thus cannot be used to form sub-phrases. It is still possible to right-
binarize tree (2) without affecting the correctness of the parallel binarization algorithm,
but that will spuriously increase the branching factor of the search for the rule extrac-
tion, because we will have to expand more tree nodes.
A special version of parallel binarization is the parallel head binarization, where
both the left and the right binarization must respect the head propagation property
at the same time. Parallel head binarization guarantees that new nodes introduced by
binarization always contain the head constituent, which will become convenient when
head-driven syntax-based language models are integrated into a bottom–up decoding
search by intersecting with the trees inferred from the translation model.
Our re-structuring of MT training trees is realized by tree binarization, but this does
not mean that our re-structuring method can factor out phrases covered only by two
(binary) constituents. In fact, a nice property of parallel binarization is that for any
factorizable substructure in the unbinarized tree, we can always find a corresponding
admissible ⊕-node in the parallel-binarized packed forest, and thus we can always
extract that phrase. A leftmost substructure like the lowest NPB-subtree in tree (3) of
Figure 6 can be made factorizable by several successive left binarizations, resulting in
the ⊕5(NPB)-node in the packed forest in Figure 7. A substructure in the middle can be
factorized by the composition of several left- and right-binarizations. Therefore, after a
tree is parallel-binarized, to make the sub-phrases available to the MT system, all we
need to do is to extract rules from the admissible nodes in the packed forest. Rules that
can be extracted from the original unrestructured tree can be extracted from the packed
forest as well.
Parallel binarization results in parse forests. Thus translation rules need to be ex-
tracted from training data consisting of (e-forest, f, a)-tuples.
</bodyText>
<subsectionHeader confidence="0.999815">
3.3 Extracting Translation Rules from (e-forest, f, a)-tuples
</subsectionHeader>
<bodyText confidence="0.999936166666667">
Our algorithm to extract rules from (e-forest, f, a)-tuples is a natural generalization
of the (e-parse, f, a)-based rule extraction algorithm in Galley et al. (2006). A similar
problem is also elegantly addressed in Mi and Huang (2008) in detail. The forest-based
rule extraction algorithm takes as input a (e-forest, f, a)-triple, and outputs a derivation
forest (Galley et al. 2006), which consists of overlapping translation rules. The algorithm
recursively traverses the e-forest top–down, extracts rules only at admissible e-forest
</bodyText>
<page confidence="0.974879">
256
</page>
<note confidence="0.735362">
Wang et al. Re-structuring, Re-labeling, and Re-aligning
</note>
<bodyText confidence="0.996629">
nodes, and transforms e-forest nodes into synchronous derivation-forest nodes via the
following two procedures, depending on which condition is met.
</bodyText>
<listItem confidence="0.828937142857143">
• Condition 1: If we reach an additive e-forest node, for each of its children,
which are multiplicative e-forest nodes, we go to condition 2 to recursively
extract rules from it to obtain a set of multiplicative derivation-forest
nodes, respectively. We form an additive derivation-forest node, and take
these newly produced multiplicative derivation-forest nodes (by going to
condition 2) as children. After this, we return the additive
derivation-forest node.
</listItem>
<figureCaption confidence="0.89662675">
For instance, at node ⊕1(NPB) in Figure 7, for each of its children, e-forest
nodes ⊗2(NPB) and ⊗11(NPB), we go to condition 2 to extract rules on it,
to form multiplicative derivation forest nodes, ⊗(R16) and ⊗(R17) in
Figure 8.
</figureCaption>
<listItem confidence="0.553674">
• Condition 2: If we reach a multiplicative e-forest node, we extract a set of
rules rooted at it using the procedure in Galley et al. (2006); and for each
rule, we form a multiplicative derivation-forest node, and go to condition 1
to form the additive derivation-forest nodes for the additive frontier
e-forest nodes of the newly extracted rule, and then make these additive
derivation-forest nodes the children of the multiplicative derivation-forest
node. After this, we return a set of multiplicative derivation-forest nodes,
each corresponding to one rule extracted from the multiplicative e-forest
node we just reached.
</listItem>
<figureCaption confidence="0.982244666666667">
Figure 8
A synchronous derivation forest built from a (e-forest, f, a) triple. The e-forest is shown in
Figure 7.
</figureCaption>
<page confidence="0.846753">
257
</page>
<note confidence="0.243223">
Computational Linguistics Volume 36, Number 2
</note>
<bodyText confidence="0.99981025">
For example, at node 011(NPB) in Figure 7, we extract a rule from it and
form derivation-forest node ®(R17) in Figure 8. We then go to condition 1
to obtain, for each of the additive frontier e-forest nodes (in Figure 7) of
this rule, a derivation-forest node, namely, ®(NNP), ®(NNP), and ®(NPB)
in Figure 8. We make these derivation-forest ®-nodes the children of
derivation-forest node ®(R17).
This procedure transforms the packed e-forest in Figure 7 into a packed synchro-
nous derivation in Figure 8. This algorithm is an extension of the extraction algorithm
in Galley et al. (2006), in the sense that we have an extra condition (1) to relay rule
extraction on additive e-forest nodes.
The forest-based rule extraction algorithm produces much larger grammars than
the tree-based one, making it difficult to scale to very large training data. From a
50M-word Chinese-to-English parallel corpus, we can extract more than 300 million
translation rules, while the tree-based rule extraction algorithm gives approximately
100 million. However, the restructured trees from the simple binarization methods are
not guaranteed to give the best trees for syntax-based machine translation. What we
desire is a binarization method that still produces single parse trees, but is able to mix
left binarization and right binarization in the same tree. In the following, we use the EM
algorithm to learn the desirable binarization on the forest of binarization alternatives
proposed by the parallel binarization algorithm.
</bodyText>
<subsectionHeader confidence="0.994191">
3.4 Learning How to Binarize Via the EM Algorithm
</subsectionHeader>
<bodyText confidence="0.99712025">
The basic idea of applying the EM algorithm to choose a re-structuring is as follows. We
perform a set {R} of binarization operations on a parse tree -r. Each binarization R is
the sequence of binarizations on the necessary (i.e., factorizable) nodes in -r in pre-order.
Each binarization R results in a restructured tree -rp. We extract rules from (-rp, f, a),
generating a translation model consisting of parameters (i.e., rule probabilities) 0. Our
aim is to first obtain the rule probabilities that are the maximum likelihood estimate
of the training tuples, and then produce the Viterbi binarization tree for each training
tuple.
The probability P(-rp, f, a) of a (-rp, f, a)-tuple is what the basic syntax-based trans-
lation model is concerned with. It can be further computed by aggregating the rule
probabilities P(r) in each derivation w in the set of all derivations Ω (Galley et al. 2004).
That is,
</bodyText>
<equation confidence="0.9945175">
�P(-rp, f, a) = 11 P(r) (1)
w∈Ω r∈w
</equation>
<bodyText confidence="0.9999578">
The rule probabilities are estimated by the inside–outside algorithm (Lari and
Young 1990; Knight, Graehl, and May 2008), which needs to run on derivation forests.
Our previous sections have already presented algorithms to transform a parse tree into
a binarization forest, and then transform the (e-forest, f, a)-tuples into derivation forests
(e.g., Figure 8), on which the inside–outside algorithm can then be applied.
In the derivation forests, an additive node labeled as A dominates several mul-
tiplicative nodes, each corresponding to a translation rule resulting from either left
binarization or right binarization of the original structure. We use rule r to either refer
to a rule or to a multiplicative node in the derivation forest. We use root(r) to represent
the root label of the rule, and parent(r) to refer to the additive node that is the parent
</bodyText>
<page confidence="0.966589">
258
</page>
<note confidence="0.72962">
Wang et al. Re-structuring, Re-labeling, and Re-aligning
</note>
<bodyText confidence="0.999954571428572">
of the node corresponding to the r. Each rule node (or multiplicative node) dom-
inates several other additive children nodes, and we present the ith child node as
childi(r), among the total number of n children. For example, in Figure 8, for the rule r
corresponding to the left child of the forest root (labeled as NPB), parent(r) is NPB, and
child1(NPB) = r. Based on these notations, we can compute the inside probability α(A)
of an additive node labeled as A and the outside probability β(B) of an additive forest
node labeled as B as follows.
</bodyText>
<equation confidence="0.9981828">
� � α(childi(r)) (2)
α(A) = P(r) x
rE{child(A)} i=1...n
β(B) = � P(r) x β(parent(r)) x � α(C) (3)
r:BE{child(r)} CE{child(r)}−{B}
</equation>
<bodyText confidence="0.999791">
In the expectation step, the contribution of each occurrence of a rule in a derivation-
forest to the total expected count of that rule is computed as
</bodyText>
<equation confidence="0.98174">
β(parent(r)) x P(r) x � α(childi(r)) (4)
i=1...n
</equation>
<bodyText confidence="0.998737">
In the maximization step, we use the expected counts of rules, #r, to update the proba-
bilities of the rules.
</bodyText>
<equation confidence="0.9691475">
P(r) = �#r (5)
rule q:root(q)=root(r) #q
</equation>
<bodyText confidence="0.997389142857143">
Because it is well known that applying EM with tree fragments of different sizes
causes overfitting (Johnson 1998a), and because it is also known that syntax MT models
with larger composed rules in the mix significantly outperform rules that minimally
explain the training data (minimal rules) in translation accuracy (Galley et al. 2006), we
use minimal rules to construct the derivation forests from (e-binarization-forest, f, a)-
tuples during running of the EM algorithm, but, after the EM re-structuring is finished,
we build the final translation model using composed rules for MT evaluation.
</bodyText>
<figureCaption confidence="0.621404333333333">
Figure 9 is the actual pipeline that we use for EM binarization. We first generate a
packed e-forest via parallel binarization. We then extract minimal translation rules from
Figure 9
</figureCaption>
<bodyText confidence="0.51658">
Using the EM algorithm to choose re-structuring.
</bodyText>
<page confidence="0.955775">
259
</page>
<note confidence="0.273937">
Computational Linguistics Volume 36, Number 2
</note>
<bodyText confidence="0.999875823529412">
the (e-forest, f, a)-tuples, producing synchronous derivation forests. We run the inside–
outside algorithm on the derivation forests until convergence. We obtain the Viterbi
derivations and project the English parses from the derivations. Finally, we extract
composed rules using Galley et al. (2006)’s (e-tree, f, a)-based rule extraction algorithm.
When extracting composed rules from (e-parse, f, a)-tuples, we use an “ignoring-X-
node” trick to the rule extraction method in Galley et al. (2006) to avoid breaking the
local dependencies captured in complex rules. The trick is that new nodes introduced
by binarization are not counted when computing the rule size limit unless they appear
as the rule roots. The motivation is that newly introduced nodes break the local depen-
dencies, deepening the parses. In Galley et al., a composed rule is extracted only if the
number of internal nodes it contains does not exceed a limit, similar to the phrase length
limit in phrase-based systems. This means that rules extracted from the restructured
trees will be smaller than those from the unrestructured trees, if the X nodes are deleted
from the rules. As shown in Galley et al., smaller rules lose context, and thus give
lower translation accuracy. Ignoring X nodes when computing the rule sizes preserves
the unrestructured rules in the resulting translation model and adds substructures as
bonuses.
</bodyText>
<subsectionHeader confidence="0.852966">
3.5 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9996306875">
We carried out experiments to evaluate different tree binarization methods in terms
of translation accuracy for Chinese-to-English translation. The baseline syntax MT sys-
tem was trained on the original, non-restructured trees. We also built one MT system
by training on left-binarizations of training trees, and another by training on EM-
binarizations of training trees.
Table 1 shows the results on end-to-end MT. The bootstrap p-values were computed
for the pairwise BLEU comparison between the EM binarization and the baseline. The
results show that tree binarization improves MT system accuracy, and that EM binariza-
tion outperforms left binarization. The results also show that the EM re-structuring sig-
nificantly outperforms (p &lt;0.05) the no re-structuring baseline on the NIST08 eval set.
The MT improvement by tree re-structuring is also validated by our previous work
(Wang, Knight, and Marcu 2007), in which we reported a 1 BLEU point gain from EM
binarization under other training/testing conditions; other simple binarization methods
were examined in that work as well, showing that simple binarizations also improve MT
accuracy, and that EM binarization consistently outperforms the simple binarization
methods.
</bodyText>
<tableCaption confidence="0.6741366">
Table 1
Translation accuracy versus binarization algorithms. In this and all other tables reporting BLEU
performance, statistically significant improvements over the baseline are highlighted. p = the
paired bootstrap p-value computed between each system and the baseline, showing the level at
which the two systems are significantly different.
EXPERIMENT NIST08 NIST08-NW
BLEU p BLEU p
no binarization (baseline)
left binarization
EM binarization
</tableCaption>
<table confidence="0.858524">
29.12 — 35.33 —
29.35 0.184 35.46 0.360
29.74 0.010 36.12 0.016
260
Wang et al. Re-structuring, Re-labeling, and Re-aligning
</table>
<tableCaption confidence="0.972786">
Table 2
</tableCaption>
<figure confidence="0.92278175">
# admissible nodes, # rules versus re-structuring methods.
RE-STRUCTURING METHOD # ADMISSIBLE NODES (M) # RULES (M)
no binarization
left binarization
EM binarization
13 76.0
17.2 153.4
17.4 154.8
</figure>
<bodyText confidence="0.999717636363636">
We think that these improvements are explained by the fact that tree re-structuring
introduces more admissible trees nodes in the training trees and enables the forming
of additional rules. As a result, re-structuring produces more rules. Table 2 shows the
number of admissible nodes made available by each re-structuring method, as well as
by the baseline. Table 2 also shows the sizes of the resulting grammars.
The EM binarization is able to introduce more admissible nodes because it mixes
both left and right binarizations in the same tree. We computed the binarization biases
learned by the EM algorithm for each nonterminal from the binarization forest of
parallel head binarizations of the training trees (Table 3). Of course, the binarization
bias chosen by left-/right-binarization methods would be 100% deterministic. One
noticeable message from Table 3 is that most of the categories are actually biased toward
left-binarization. The reason might be that the head sub-constituents of most English
categories tend to be on the left.
Johnson (1998b) argues that the more nodes there are in a treebank, the stronger
the independence assumptions implicit in the PCFG model are, and the less accurate
the estimated PCFG will usually be—more nodes break more local dependencies. Our
experiments, on the other hand, show MT accuracy improvement by introducing more
admissible nodes. This initial contradiction actually makes sense. The key is that we use
composed rules to build our final MT system and that we introduce the “ignoring-X-
node” trick to preserve the local dependencies. More nodes in training trees weaken the
accuracy of a translation model of minimal rules, but boost the accuracy of a translation
model of composed rules.
</bodyText>
<tableCaption confidence="0.996271">
Table 3
</tableCaption>
<table confidence="0.919155555555556">
Binarization bias learned by the EM re-structuring method on the model 4 word alignments.
nonterminal left-binarization (%) right-binarization (%)
NP 98 2
NPB 1 99
VP 95 5
PP 86 14
ADJP 67 33
ADVP 76 24
S 94 6
S-C 17 83
SBAR 93 7
QP 89 11
WHNP 98 2
SINV 94 6
CONJP 69 31
261
Computational Linguistics Volume 36, Number 2
4. Re-Labeling Trees for Training
</table>
<bodyText confidence="0.999765892857143">
The syntax translation model explains (e-parse, f, a)-tuples by a series of applications
of translation rules. At each derivation step, which rule to apply next depends only
on the nonterminal label of the frontier node being expanded. In the Penn Treebank
annotation, the nonterminal labels are too coarse to encode enough context information
to accurately predict the next translation rule to apply. As a result, using the Penn
Treebank annotation can license ill-formed subtrees (Figure 10). This subtree contains
an error that induces a VP as an SG-C when the head of the VP is the finite verb dis-
cussed. The translation error leads to the ungrammatical “... confirmed discussed... ”. This
translation error occurs due to the fact that there is no distinction between finite VPs
and non-finite VPs in Penn Treebank annotation. Monolingual parsing suffers similarly,
but to a lesser degree.
Re-structuring of training trees enables the reuse of sub-constituent structures, but
further introduces new nonterminals and actually reduces the context for rules, thus
making this “coarse nonterminal” problem more severe. In Figure 11, R23 may be
extracted from a construct like S(S CC S) via tree binarization, and R24 may be extracted
from a construct like S(NP NP-C VP) via tree binarization. Composing R23 and R24
forms the structure in Figure 11(b), which, however, is ill-formed. This wrong structure
in Figure 11(b) yields ungrammatical translations like he likes reading she does not like
reading. Tree binarization enables the reuse of substructures, but causes over-generation
of trees at the same time.
We solve the coarse-nonterminal problem by refining/re-labeling the training tree
labels. Re-labeling is done by enriching the nonterminal label of each tree node based
on its context information.
Re-labeling has already been used in monolingual parsing research to improve
parsing accuracy of PCFGs. We are interested in two types of re-labeling methods:
Linguistically motivated re-labeling (Klein and Manning 2003; Johnson 1998b) enriches
the labels of parser training trees using parent labels, head word tag labels, and/or
sibling labels. Automatic category splitting (Petrov et al. 2006) refines a nonterminal
</bodyText>
<figureCaption confidence="0.389415">
Figure 10
</figureCaption>
<bodyText confidence="0.518992">
MT output errors due to coarse Penn Treebank annotations. Oval nodes in (b) are rule
overlapping nodes. Subtree (b) is formed by composing the LHSs of R20, R21, and R22.
</bodyText>
<page confidence="0.97858">
262
</page>
<note confidence="0.832472">
Wang et al. Re-structuring, Re-labeling, and Re-aligning
</note>
<figureCaption confidence="0.967177">
Figure 11
</figureCaption>
<bodyText confidence="0.985579272727273">
Tree binarization over-generalizes the parse tree. Translation rules R23 and R24 are acquired
from binarized training trees, aiming for reuse of substructures. Composing R23 and R24,
however, results in an ill-formed tree. The new nonterminal S introduced in tree binarization
needs to be refined into different sub-categories to prevent R23 and R24 from being composed.
Automatic category splitting can be employed for refining the S.
by classifying the nonterminal into a fine-grained sub-category, and this sub-classing
is learned via the EM algorithm. Category splitting is realized by several splitting-and-
merging cycles. In each cycle, the nonterminals in the PCFG rules are split by splitting
each nonterminal into two. The EM algorithm is employed to estimate the split PCFG on
the Penn Treebank training trees. After that, 50% of the new nonterminals are merged
based on some loss function, to avoid overfitting.
</bodyText>
<subsectionHeader confidence="0.984968">
4.1 Linguistic Re-labeling
</subsectionHeader>
<bodyText confidence="0.999568">
In the linguistically motivated approach, we employ the following set of rules to re-label
tree nodes. In our MT training data:
</bodyText>
<listItem confidence="0.9748708">
• SPLIT-VP: annotates each VP nodes with its head tag, and then merges all
finite VP forms to a single VPF.
• SPLIT-IN: annotates each IN node with the combination of IN and its
parent node label. IN is frequently overloaded in the Penn Treebank. For
instance, its parent can be PP or SBAR.
</listItem>
<bodyText confidence="0.9999765">
These two operations re-label the tree in Figure 12(a1) to Figure 12(b1). Example
rules extracted from these two trees are shown in Figure 12(a2) and Figure 12(b2),
respectively. We apply this re-labeling on the MT training tree nodes, and then acquire
rules from these re-labeled trees. We chose to split only these two categories because
our syntax MT system tends to frequently make parse errors in these two categories,
and because, as shown by Klein and Manning (2003), further refining the VP and IN
categories is very effective in improving monolingual parsing accuracy.
This type of re-labeling fixes the parse error in Figure 10. SPLIT-VP transforms the
R18 root to VPF, and the R17 frontier node to VP.VBN. Thus R17 and R18 can never be
composed, preventing the wrong tree being formed by the translation model.
</bodyText>
<subsectionHeader confidence="0.968057">
4.2 Statistical Re-labeling
</subsectionHeader>
<bodyText confidence="0.9987665">
Our second re-labeling approach is to learn the split categories for the node labels of
the training trees via the EM algorithm, as in Petrov et al. (2006). Rather than using
</bodyText>
<page confidence="0.991111">
263
</page>
<figure confidence="0.805121">
Computational Linguistics Volume 36, Number 2
</figure>
<figureCaption confidence="0.818105">
Figure 12
</figureCaption>
<bodyText confidence="0.548566">
Re-labeling of parse trees.
</bodyText>
<page confidence="0.989721">
264
</page>
<note confidence="0.866681">
Wang et al. Re-structuring, Re-labeling, and Re-aligning
</note>
<bodyText confidence="0.600461">
their parser to directly produce category-split parse trees for the MT training data, we
separate the parsing step and the re-labeling step. The re-labeling method is as follows.
</bodyText>
<listItem confidence="0.991494166666667">
1. Run a parser to produce the MT training trees.
2. Binarize the MT training trees via the EM binarization algorithm.
3. Learn an n-way split PCFG from the binarized trees via the algorithm
described in Petrov et al. (2006).
4. Produce the Viterbi split annotations on the binarized training trees with
the learned category-split PCFG.
</listItem>
<bodyText confidence="0.999850615384615">
As we mentioned earlier, tree binarization sometimes makes the decoder over-
generalize the trees in the MT outputs, but we still binarize the training trees before
performing category splitting, for two reasons. The first reason is that the improve-
ment on MT accuracy we achieved by tree re-structuring indicates that the benefit
we obtained from structure reuse triumphs the problem of tree over-generalization.
The second is that carrying out category splitting on unbinarized training trees blows
up the grammar—splitting a CFG rule of rank 10 results in 211 split rules. This re-
labeling procedure tries to achieve further improvement by trying to fix the tree over-
generalization problem of re-structuring while preserving the gain we have already
obtained from tree re-structuring.
Figure 12(c1) shows a category-split tree, and Figure 12(c2) shows the minimal xRs
rules extracted from the split tree. In Figure 12(c2), the two VPs (VP-0 and VP-2) now
belong to two different categories and cannot be used in the same context.
In this re-labeling procedure, we separate the re-labeling step from the parsing step,
rather than using a parser like the one in Petrov et al. (2006) to directly produce category-
split parse trees on the English corpus. We think that we benefit from this separation in
the following ways: First, this gives us the freedom to choose the parser to produce the
initial trees. Second, this enables us to train the re-labeler on the domains where the MT
system is trained, instead of on the Penn Treebank. Third, this enables us to choose our
own tree binarization methods.
Tree re-labeling fragments the translation rules. Each refined rule now fits in fewer
contexts than its corresponding coarse rule. Re-labeling, however, does not explode
the grammar size, nor does re-labeling deteriorate the reuse of substructures. This
is because the re-labeling (whether linguistic or automatic) results in very consistent
annotations. Table 4 shows the sizes of the translation grammars from different re-
labelings of the training trees, as well as that from the unrelabeled ones.
</bodyText>
<tableCaption confidence="0.990151">
Table 4
</tableCaption>
<table confidence="0.756791428571429">
Grammar size vs. re-labeling methods. Re-labeling does not explode the grammar size.
RE-LABELING METHOD # RULES (M) NONTERMINAL SET SIZE
No re-labeling 154.80 144
Linguistically motivated re-labeling 154.97 210
4-way splitting (90% merging) 158.89 178
8-way splitting (90% merging) 160.62 195
4-way splitting (50% merging) 164.15 326
</table>
<page confidence="0.97205">
265
</page>
<note confidence="0.478356">
Computational Linguistics Volume 36, Number 2
</note>
<bodyText confidence="0.99990725">
It would be very interesting to perform automatic category splitting with synchro-
nous translation rules and run the EM algorithm on the synchronous derivation forests.
Synchronous category splitting is computationally much more expensive, so we do not
study it here.
</bodyText>
<subsectionHeader confidence="0.998007">
4.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.998731952380952">
We ran end-to-end MT experiments by re-labeling the MT training trees. Our two base-
line systems were a syntax MT system with neither re-structuring nor re-labeling, and
a syntax MT system with re-structuring but no re-labeling. The linguistically motivated
re-labeling method was applied directly on the original (unrestructured) training trees,
so that it could be compared to the first baseline. The automatic category splitting re-
labeling method was applied to binarized trees so as to avoid the explosion of the split
grammar, so it is compared to the second baseline. The experiment results are shown in
Table 5.
Both re-labeling methods help MT accuracy. Putting both re-structuring and re-
labeling together results in 0.93 BLEU points improvement on NIST08 set, and 1 BLEU
point improvement on the newswire subset. All p-values are computed between the
re-labeling systems and Baseline1. The improvement made by the linguistically moti-
vated re-labeling method is significant at the 0.05 level. Because the automatic category
splitting is carried out on the top of EM re-structuring and because, as we have already
shown, EM re-structuring significantly improves Baseline1, putting them together re-
sults in better translations with more confidence.
If we compare these results to those in Table 1, we notice that re-structuring tends
to help MT accuracy more than re-labeling. We mentioned earlier that re-structuring
overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and
Table 1 show substructure reuse mitigates structure over-generalization in our tree re-
structuring method.
</bodyText>
<sectionHeader confidence="0.960566" genericHeader="method">
5. Re-aligning (Tree, String) Pairs for Training
</sectionHeader>
<bodyText confidence="0.999867833333333">
So far, we have improved the English structures in our parsed, aligned training corpus.
We now turn to improving the word alignments.
Some MT systems use the same model for alignment and translation—examples
include Brown et al. (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada
and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al.
for alignment, then collect counts for a completely different model, such as Och and Ney
</bodyText>
<tableCaption confidence="0.959705">
Table 5
Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was
carried out on EM-binarized trees; thus, it already benefits from tree re-structuring. All p-values
are computed against Baseline1.
</tableCaption>
<table confidence="0.992661833333333">
EXPERIMENT NIST08 NIST08-NW
BLEU p BLEU p
Baseline1 (no re-structuring and no re-labeling) 29.12 — 35.33 —
Linguistically motivated re-labeling 29.57 0.029 35.85 0.050
Baseline2 (EM re-structuring but no re-labeling) 29.74 — 36.12 —
4-way splitting (w/ 90% merging) 30.05 0.001 36.42 0.003
</table>
<page confidence="0.991857">
266
</page>
<note confidence="0.878913">
Wang et al. Re-structuring, Re-labeling, and Re-aligning
</note>
<bodyText confidence="0.99809125">
(2004) or Chiang (2007). Our basic syntax-based system falls into this second category,
as we learn our syntactic translation model from a corpus aligned with word-based
techniques. We would like to inject more syntactic reasoning into the alignment process.
We start by contrasting two generative translation models.
</bodyText>
<subsectionHeader confidence="0.94923">
5.1 The Traditional IBM Alignment Model
</subsectionHeader>
<listItem confidence="0.919498833333333">
IBM Model 4 (Brown et al. 1993) learns a set of four probability tables to compute P( f |e)
given a foreign sentence f and its target translation e via the following (simplified)
generative story:
1. A fertility y for each word ei in e is chosen with probability Pfert(y|ei).
2. A null word is inserted next to each fertility-expanded word with
probability Pnull.
3. Each token ei in the fertility-expanded word and null string is translated
into some foreign word fi in f with probability Ptrans(fi|ei).
4. The position of each foreign word fi that was translated from ei is changed
by ∆ (which may be positive, negative, or zero) with probability
Pdistortion(∆|A(ei), B( fi)), where A and B are functions over the source and
target vocabularies, respectively.
</listItem>
<bodyText confidence="0.96917875">
Brown et al. (1993) describe an EM algorithm for estimating values for the four
tables in the generative story. With those values in hand, we can calculate the highest-
probability (Viterbi) alignment for any given string pair.
Two scale problems arise in this algorithm. The first is the time complexity of enu-
merating alignments for fractional count collection. This is solved by considering only
a subset of alignments, and by bootstrapping the Ptrans table with a simpler model that
admits fast count collection via dynamic programming, such as IBM Model 1 (Brown
et al. 1993) or Aachen HMM (Vogel, Ney, and Tillmann 1996). The second problem is
one of space. In theory, the initial Ptrans table contains a cell for every English word
paired with every Chinese word—this would be infeasible. Fortunately, in practice, the
table can be initialized with only those word pairs observed co-occurring in the parallel
training text.
</bodyText>
<subsectionHeader confidence="0.995717">
5.2 A Syntax Re-alignment Model
</subsectionHeader>
<bodyText confidence="0.995973333333333">
Our syntax translation model learns a single probability table to compute P(etree, f )
given a foreign sentence f and a parsed target translation etree. In the following gen-
erative story we assume a starting variable with syntactic type v.
</bodyText>
<listItem confidence="0.993087">
1. Choose a rule r to replace v, with probability Prule(r|v).
2. For each variable with syntactic type vi in the partially completed (tree,
string) pair, continue to choose rules ri with probability Prule(ri|vi) to
replace these variables until there are no variables remaining.
</listItem>
<bodyText confidence="0.688927">
We can use this model to explain unaligned (tree, string) pairs from our training
data. With a large enough rule set, any given (tree, string) pair will admit many deriva-
tions. Consider again the example from Figure 1. The particular alignment associated
</bodyText>
<page confidence="0.983358">
267
</page>
<note confidence="0.484709">
Computational Linguistics Volume 36, Number 2
</note>
<bodyText confidence="0.999894590909091">
with that (tree, string) pair yields the minimal rules of Figure 2. A different alignment
yields different rules. Figure 13 shows two other alignments and their corresponding
minimal rules. As noted before, a set of minimal rules in proper sequence forms
a derivation tree of rules that explains the (tree, string) pair. Because rules explain
variable-size fragments (e.g., R35 vs. R6), the possible derivation trees of rules that
explain a sentence pair have varying sizes. The smallest such derivation tree has a single
large rule (which does not appear in Figure 13). When our model chooses a particular
derivation tree of minimal rules to explain a given (tree, string) pair it implicitly chooses
the alignment that produced these rules as well.3 Our model can choose a derivation by
using any of the rules in Figures 13 and 2. We would prefer it select the derivation that
yields the good alignment in Figure 1.
We can also develop an EM learning approach for this model. As in the IBM
approach, we have both time and space issues. Time complexity, as we will see sub-
sequently, is O(mn3), where m is the number of nodes in the English training tree and
n is the length of the corresponding Chinese string. Space is more of a problem. We
would like to initialize EM with all the rules that might conceivably be used to explain
the training data. However, this set is too large to practically enumerate.
To reduce the model space we first create a bootstrap alignment using a simpler
word-based model. Then we acquire a set of minimal translation rules from the (tree,
string, alignment) triples. Armed with these rules, we can discard the word-based
alignments and re-align with the syntax translation model.
We summarize the approach described in this section as:
</bodyText>
<listItem confidence="0.995235555555556">
1. Obtain bootstrap alignments for a training corpus using word-based
alignment.
2. Extract minimal rules from the corpus and alignments using GHKM,
noting the partial alignment that is used to extract each rule.
3. Construct derivation forests for each (tree, string) pair, ignoring the
alignments, and run EM to obtain Viterbi derivation trees, then use the
annotated partial alignments to obtain Viterbi alignments.
4. Use the new alignments to re-train the full MT system, this time collecting
composed rules as well as minimal rules.
</listItem>
<subsectionHeader confidence="0.99598">
5.3 EM Training for the Syntax Translation Model
</subsectionHeader>
<bodyText confidence="0.9999125">
Consider the example of Figure 13 again. The top alignment was the bootstrap align-
ment, and thus prior to any experiment we obtained the corresponding indicated
minimal rule set and derivation. This derivation is reasonable but there are some poorly
motivated rules, from a linguistic standpoint. The Chinese word M1Y- roughly means
the two shores in this context, but the rule R35 learned from the alignment incorrectly
includes between. However, other sentences in the training corpus have the correct
</bodyText>
<footnote confidence="0.9963282">
3 Strictly speaking there is actually a one-to-many mapping between a derivation tree of minimal rules and
the alignment that yields these rules, due to the handling of unaligned words. However, the choice of one
partial alignment over another does not affect results and in practice we impose a one-to-one mapping
between minimal rules and the partial alignments that imply them by selecting the most frequently
observed partial alignment for a given minimal rule.
</footnote>
<page confidence="0.989871">
268
</page>
<note confidence="0.909438">
Wang et al. Re-structuring, Re-labeling, and Re-aligning
</note>
<figureCaption confidence="0.887319">
Figure 13
</figureCaption>
<bodyText confidence="0.977299">
The minimal rules extracted from two different alignments of the sentence in Figure 1.
alignment, which yields rules in Figure 2, such as R8. Figure 2 also contains rules R4
and R6, learned from yet other sentences in the training corpus, which handle the tT ...
rp structure (which roughly translates to in between), thus allowing a derivation which
contains the minimal rule set of Figure 2 and implies the alignment in Figure 1.
EM distributes rule probabilities in such a way as to maximize the probability of the
training corpus. It thus prefers to use one rule many times instead of several different
</bodyText>
<page confidence="0.992434">
269
</page>
<note confidence="0.590827">
Computational Linguistics Volume 36, Number 2
</note>
<bodyText confidence="0.999707888888889">
rules for the same situation over several sentences, if possible. R35 is a possible rule
in 46 of the 329,031 sentence pairs in the training corpus, and R8 is a possible rule in
100 sentence pairs. Well-formed rules are more usable than ill-formed rules and the
partial alignments behind these rules, generally also well-formed, become favored as
well. The top row of Figure 14 contains an example of an alignment learned by the
bootstrap alignment model that includes an incorrect link. Rule R25, which is extracted
from this alignment, is a poor rule. A set of commonly seen rules learned from other
training sentences provide a more likely explanation of the data, and the consequent
alignment omits the spurious link.
</bodyText>
<figureCaption confidence="0.640514">
Figure 14
</figureCaption>
<bodyText confidence="0.840882666666667">
The impact of a bad alignment on rule extraction. Including the alignment link indicated by the
dotted line in the example leads to the rule set in the second row. The re-alignment procedure
described in Section 5.2 learns to prefer the rule set at bottom, which omits the bad link.
</bodyText>
<page confidence="0.994671">
270
</page>
<note confidence="0.95283">
Wang et al. Re-structuring, Re-labeling, and Re-aligning
</note>
<tableCaption confidence="0.994191">
Table 6
</tableCaption>
<table confidence="0.987265285714286">
Translation performance, grammar size versus the re-alignment algorithm proposed in
Section 5.2, and re-alignment as modified in Section 5.4.
EXPERIMENT NIST08 NIST08-NW # RULES (M)
BLEU p BLEU p
no re-alignment (baseline) 29.12 — 35.33 — 76.0
EM re-alignment 29.18 0.411 35.52 0.296 75.1
EM re-alignment with size prior 29.37 0.165 35.96 0.050 110.4
</table>
<bodyText confidence="0.398501333333333">
Now we need an EM algorithm for learning the parameters of the rule set that
�
maximize P(tree, string). Knight, Graehl, and May (2008) present a generic such algo-
</bodyText>
<subsubsectionHeader confidence="0.652666">
corpus
</subsubsectionHeader>
<bodyText confidence="0.999658222222222">
rithm for tree-to-string transducers that runs in O(mn3) time, as mentioned earlier. The
algorithm consists of two components: DERIV, which is a procedure for constructing
a packed forest of derivation trees of rules that explain a (tree, string) bitext corpus
given that corpus and a rule set, and TRAIN, which is an iterative parameter-setting
procedure.
We initially attempted to use the top-down DERIV algorithm of Knight, Graehl, and
May (2008), but as the constraints of the derivation forests are largely lexical, too much
time was spent on exploring dead-ends. Instead we build derivation forests using the
following sequence of operations:
</bodyText>
<listItem confidence="0.998390142857143">
1. Binarize rules using the synchronous binarization algorithm for
tree-to-string transducers described in Zhang et al. (2006).
2. Construct a parse chart with a CKY parser simultaneously constrained on
the foreign string and English tree, similar to the bilingual parsing of Wu
(1997).4
3. Recover all reachable edges by traversing the chart, starting from the
topmost entry.
</listItem>
<bodyText confidence="0.99974175">
Because the chart is constructed bottom-up, leaf lexical constraints are encountered
immediately, resulting in a narrower search space and faster running time than the
top-down DERIV algorithm for this application. The Viterbi derivation tree tells us
which English words produce which Chinese words, so we can extract a word-to-word
alignment from it.
Although in principle the re-alignment model and translation model learn parame-
ter weights over the same rule space, in practice we limit the rules used for re-alignment
to the set of minimal rules.
</bodyText>
<subsectionHeader confidence="0.999708">
5.4 Adding a Rule Size Prior
</subsectionHeader>
<bodyText confidence="0.998198">
An initial re-alignment experiment shows a small rise in BLEU scores from the baseline
(Table 6), but closer inspection of the rules favored by EM implies we can do even better.
</bodyText>
<footnote confidence="0.926195666666667">
4 In the cases where a rule is not synchronous-binarizable, standard left–right binarization is performed
and proper permutation of the disjoint English tree spans must be verified when building the part of the
chart that uses this rule.
</footnote>
<page confidence="0.98396">
271
</page>
<note confidence="0.588268">
Computational Linguistics Volume 36, Number 2
</note>
<bodyText confidence="0.999844588235294">
EM has a tendency to favor a few large rules over many small rules, even when the
small rules are more useful. Referring to the rules in Figures 2 and 13, note that possible
derivations for (taiwan’s, QA)5 are R33, R2–R3, and R38–R40. Clearly the third deriva-
tion is not desirable, and we do not discuss it further. Between the first two derivations,
R2–R3 is preferred over R33, as the conditioning for possessive insertion is not related to
the specific Chinese word being inserted. Of the 1,902 sentences in the training corpus
where this pair is seen, the bootstrap alignments yield the R33 derivation 1,649 times
and the R2–R3 derivation 0 times. Re-alignment does not change the result much; the
new alignments yield the R33 derivation 1,613 times and again never choose R2–R3. The
rules in the second derivation themselves are not rarely seen—R2 is in 13,311 forests
other than those where R33 is seen, and R3 is in 2,500 additional forests. EM gives R2 a
probability of e−7.72—better than 98.7% of rules, and R3 a probability of e−2.96. But R33
receives a probability of e−6.32 and is preferred over the R2–R3 derivation, which has a
combined probability of e−10.68.
The preference for shorter derivations containing large rules over longer derivations
containing small rules is due to a general tendency for EM to prefer derivations with
few atoms. Marcu and Wong (2002) note this preference but consider the phenomenon
a feature, rather than a bug. Zollmann and Sima’an (2005) combat the overfitting aspect
for parsing by using a held-out corpus and a straight maximum likelihood estimate,
rather than EM. DeNero, Bouchard-Cˆot´e, and Klein (2008) encourage small rules with a
modeling approach; they put a Dirichlet process prior of rule size over their model and
learn the parameters of the geometric distribution of that prior with Gibbs sampling. We
use a simpler modeling approach to accomplish the same goals as DeNero, Bouchard-
Cˆot´e, and Klein which, although less elegant, is more scalable and does not require a
separate Bayesian inference procedure.
As the probability of a derivation is determined by the product of its atom probabil-
ities, longer derivations with more probabilities to multiply have an inherent disadvan-
tage against shorter derivations, all else being equal. EM is an iterative procedure and
thus such a bias can lead the procedure to converge with artificially raised probabilities
for short derivations and the large rules that constitute them. The relatively rare ap-
plicability of large rules (and thus lower observed partial counts) does not overcome the
inherent advantage of large coverage. To combat this, we introduce size terms into our
generative story, ensuring that all competing derivations for the same sentence contain
the same number of atoms:
</bodyText>
<listItem confidence="0.898481">
1. Choose a rule size s with cost csize(s)s−1.
2. Choose a rule r (of size s) to replace the start symbol with probability
Prule(r|s, v).
3. For each variable in the partially completed (tree, string) pair, continue to
choose sizes followed by rules, recursively to replace these variables until
there are no variables remaining.
</listItem>
<bodyText confidence="0.82801">
This generative story changes the derivation comparison from R33 vs. R2–R3 to S2–
R33 vs. R2–R3, where S2 is the atom that represents the choice of size 2 (the size of a rule
</bodyText>
<page confidence="0.7291195">
5 The Chinese gloss is simply “taiwan”.
272
</page>
<note confidence="0.928096">
Wang et al. Re-structuring, Re-labeling, and Re-aligning
</note>
<bodyText confidence="0.999804666666666">
in this context is the number of non-leaf and non-root nodes in its tree fragment). Note
that the variable number of inclusions implied by the exponent in the generative story
above ensures that all derivations have the same size. For example, a derivation with
one size-3 rule, a derivation with one size-2 and one size-1 rule, and a derivation with
three size-1 rules would each have three atoms. With this revised model that allows for
fair comparison of derivations, the R2–R3 derivation is chosen 1,636 times, and S2–R33
is not chosen. R33 does, however, appear in the translation model, as the expanded rule
extraction described in Section 1 creates R33 by joining R2 and R3.
The probability of size atoms, like that of rule atoms, is decided by EM. The revised
generative story tends to encourage smaller sizes by virtue of the exponent. This does
not, however, simply ensure the largest number of rules per derivation is used in all
cases. Ill-fitting and poorly motivated rules such as R42, R43, and R44 in Figure 13 are
not preferred over R8, even though they are smaller. However, R6 and R8 are preferred
over R35, as the former are useful rules. Although the modified model does not sum to
1, it can nevertheless lead to an improvement in BLEU score.
</bodyText>
<subsectionHeader confidence="0.979794">
5.5 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999984454545455">
The end-to-end MT experiment used as baseline and described in Sections 3.5 and 4.3
was trained on IBM Model 4 word alignments, obtained by running GIZA, as described
in Section 2. We compared this baseline to an MT system that used alignments obtained
by re-aligning the GIZA alignments using the method of Section 5.2 with the 36 million
word subset of the training corpus used for re-alignment learning. We next compared
the baseline to an MT system that used re-alignments obtained by also incorporating
the size prior described in Section 5.4. As can be seen by the results in Table 6, the size
prior method is needed to obtain reasonable improvement in BLEU. These results are
consistent with those reported in May and Knight (2007), where gains in Chinese and
Arabic MT systems were observed, though over a weaker baseline and with less training
data than is used in this work.
</bodyText>
<sectionHeader confidence="0.973192" genericHeader="method">
6. Combining Techniques
</sectionHeader>
<bodyText confidence="0.9999943125">
We have thus far seen gains in BLEU score by independent improvements in training
data tree structure, syntax labeling, and alignment. This naturally raises the question
of whether the techniques can be combined, that is, if improvement in one aspect of
training data aids in improvement of another. As reported in Section 4.3 and Table 5,
we were able to improve re-labeling efforts and take advantage of the split-and-merge
technique of Petrov et al. (2006) by first re-structuring via the method described in Sec-
tion 3.4. It is unlikely that such re-structuring or re-labeling would aid in a subsequent
re-alignment procedure like that of Section 5.2, for re-structuring changes trees based
on a given alignment, and re-alignment can only change links when multiple instances
of a (subtree, substring) tuple are found in the data with different partial alignments.
Re-structuring beforehand changes the trees over different alignments differently. It is
unlikely that many (subtree, substring) tuples with more than one partial alignment
would remain after a re-structuring.
However, re-structuring may benefit from a prior re-alignment. We do not want re-
structuring decisions to be made over bad alignments, so unifying alignments based
on common syntax should lead EM to make a more confident binarization decision.
</bodyText>
<page confidence="0.996291">
273
</page>
<note confidence="0.450477">
Computational Linguistics Volume 36, Number 2
</note>
<tableCaption confidence="0.671322333333333">
Table 7
Summary of experiments in this article, including a combined experiment with re-alignment,
re-structuring, and re-labeling.
</tableCaption>
<table confidence="0.997608923076923">
EXPERIMENT NIST08 NIST08-NW # RULES (M)
BLEU p BLEU p
Baseline (no binarization, no re-labeling, 29.12 — 35.33 — 76.0
Model 4 alignments) 29.35 0.184 35.46 0.360 153.4
left binarization
EM binarization 29.74 0.010 36.12 0.016 154.8
Linguistic re-labeling 29.57 0.029 35.85 0.050 154.97
EM binarization + EM re-labeling 30.05 0.001 36.42 0.003 158.89
(4-way splitting w/ 90% merging)
EM re-alignment 29.18 0.411 35.52 0.296 75.1
Size prior EM re-alignment 29.37 0.165 35.96 0.050 110.4
Size prior EM re-alignment + 30.6 0.001 36.73 0.002 222.0
EM binarization + EM re-labeling
</table>
<bodyText confidence="0.99950975">
Better re-structuring should in turn lead to better re-labeling, and this should increase
the performance of the overall MT pipeline.
To test this hypothesis we pre-processed alignments using the modified re-
alignment procedure described in Section 5.4. We next used those alignments to obtain
new binarizations of trees following the EM binarization method described in Sec-
tion 3.4. Finally, re-labeling was done on these binarized trees using 4-way splitting with
90% merging, as described in Section 4. The final trees, along with the alignments used
to get these trees and of course the parallel Chinese sentences, were then used as the
training data of our MT pipeline. The results of this combined experiment are shown
in Table 7 along with the other experiments from this article, for ease of comparison.
As can be seen from this table, the progressive improvement of training data leads
to an overall improvement in MT system performance. As noted previously, there
tends to be a correspondence between the number of unique rules extracted and MT
performance. The final combined experiment has the greatest number of unique rules.
The improvements made to syntax and alignment described in this article unify these
two independently determined annotations over the bitext, and this thus leads to more
admissible nodes and a greater ability to extract rules. Such a unification can lead to
over-generalization, as rules lacking sufficient context may be extracted and used to the
system’s detriment. This is why a re-labeling technique is also needed, to ensure that
sufficient rule specificity is maintained.
</bodyText>
<sectionHeader confidence="0.974066" genericHeader="conclusions">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.9998218">
This article considered three modifications to MT training data that encourage im-
proved performance in a state-of-the-art syntactic MT system. The improvements
changed syntactic structure, altered bracket labels, and unified alignment across sen-
tences, and when combined led to an improvement of 1.48 BLEU points over a strong
baseline in Chinese–English translation. The techniques herein described require only
</bodyText>
<page confidence="0.99567">
274
</page>
<note confidence="0.900753">
Wang et al. Re-structuring, Re-labeling, and Re-aligning
the training data used in the original MT task and are thus applicable to a string-to-tree
MT system for any language pair.
</note>
<sectionHeader confidence="0.996993" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99934775">
Some of the results in this article appear
in Wang, Knight, and Marcu (2007) and
May and Knight (2007). The linguistically
motivated re-labeling method is due to
Steve DeNeefe, Kevin Knight, and David
Chiang. The authors also wish to thank
Slav Petrov for his help with the Berkeley
parser, and the anonymous reviewers
for their helpful comments. This research
was supported under DARPA Contract
No. HR0011-06-C-0022, BBN subcontract
9500008412.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999708989583333">
Alshawi, Hiyan, Srinivas Bangalore,
and Shona Douglas. 1998. Automatic
acquisition of hierarchical transduction
models for machine translation. In
Proceedings of the 36th Annual Meeting of
the Association for Computational Linguistics
(ACL) and 17th International Conference on
Computational Linguistics (COLING)1998,
pages 41–47, Montr´eal.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263–312.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings of
the 1st North American Chapter of the
Association for Computational Linguistics
Conference (NAACL), pages 132–139,
Seattle, WA.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201–228.
Cohn, Trevor and Phil Blunsom. 2009.
A Bayesian model of syntax-directed
tree to string grammar induction. In
Proceedings of the 2009 Conference on
Empirical Methods in Natural Language
Processing, pages 352–361, Singapore.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 16–23, Madrid.
Dempster, Arthur P., Nan M. Laird, and
Donald B. Rubin. 1977. Maximum
likelihood from incomplete data via the
EM algorithm. Journal of the Royal
Statistical Society, 39(1):1–38.
DeNeefe, Steve, Kevin Knight, Wei Wang,
and Daniel Marcu. 2007. What can
syntax-based MT learn from phrase-based
MT? In Proceedings of EMNLP–CoNLL-2007,
pages 755–763, Prague.
DeNero, John, Alexandre Bouchard-Cˆot´e,
and Dan Klein. 2008. Sampling alignment
structure under a Bayesian translation
model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 314–323,
Honolulu, HI.
Gale, William A. and Geoffrey Sampson.
1996. Good-Turing frequency estimation
without tears. Journal of Quantitative
Linguistics, 2(3):217–237.
Galley, Michel, Jonathan Graehl, Kevin
Knight, Daniel Marcu, Steve DeNeefe,
Wei Wang, and Ignacio Thayer. 2006.
Scalable inference and training of
context-rich syntactic translation models.
In Proceedings of the 21st International
Conference on Computational Linguistics
(COLING) and 44th Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 961–968, Sydney.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What’s in a
translation rule? In Proceedings of the
Human Language Technology Conference and
the North American Association for
Computational Linguistics (HLT-NAACL),
pages 273–280, Boston, MA.
Good, Irving J. 1953. The population
frequencies of species and the estimation
of population parameters. Biometrika,
40(3):237–264.
Goodman, Joshua. 1999. Semiring parsing.
Computational Linguistics, 25(4):573–605.
Huang, Bryant and Kevin Knight. 2006.
Relabeling syntax trees to improve
syntax-based machine translation
accuracy. In Proceedings of the main
conference on Human Language Technology
Conference of the North American Chapter
of the Association of Computational
Linguistics (NAACL-HLT), pages 240–247,
New York, NY.
Johnson, Mark. 1998a. The DOP estimation
method is biased and inconsistent.
Computational Linguistics, 28(1):71–76.
Johnson, Mark. 1998b. PCFG models of
linguistic tree representations.
Computational Linguistics, 24(4):613–632.
Klein, Dan and Chris Manning. 2003.
Accurate unlexicalized parsing. In
</reference>
<page confidence="0.940293">
275
</page>
<reference confidence="0.994112243697479">
Computational Linguistics Volume 36, Number 2
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 423–430, Sapporo.
Kneser, Reinhard and Hermann Ney.
1995. Improved backing-off for
m-gram language modeling. In
Proceedings of the International Conference
on Acoustics, Speech, and Signal
Processing (ICASSP) 1995, pages 181–184,
Detroit, MI.
Knight, Kevin and Jonathan Graehl. 2005.
An overview of probabilistic tree
transducers for natural language
processing. In Proceedings of the Sixth
International Conference on Intelligent
Text Processing and Computational
Linguistics (CICLing), pages 1–25,
Mexico City.
Knight, Kevin, Jonathan Graehl, and
Jonathan May. 2008. Training tree
transducers. Computational Linguistics,
34(3):391–427.
Koehn, Philipp. 2004. Statistical significance
tests for machine translation evaluation.
In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 388–395,
Barcelona.
Lari, Karim and Steve Young. 1990. The
estimation of stochastic context-free
grammars using the inside-outside
algorithm. Computer Speech and Language,
4:35–56.
Marcu, Daniel and William Wong. 2002.
A phrase-based, joint probability model
for statistical machine translation.
In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 133–139,
Philadelphia, PA.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building a
large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313–330.
May, Jonathan and Kevin Knight. 2007.
Syntactic re-alignment models for
machine translation. In Proceedings
of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP–CoNLL), pages 360–368,
Prague.
Melamed, I. Dan, Giorgio Satta, and
Benjamin Wellington. 2004. Generalized
multitext grammars. In Proceedings of the
42nd Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 662–669, Barcelona.
Mi, Haitao and Liang Huang. 2008.
Forest-based translation rule extraction.
In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 206–214,
Honolulu, HI.
Och, Franz and Hermann Ney. 2004. The
alignment template approach to statistical
machine translation. Computational
Linguistics, 30(4):417–449.
Och, Franz Josef. 2003. Minimum error rate
training for machine translation. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 160–167, Sapporo.
Petrov, Slav, Leon Barrett, Romain
Thibaux, and Dan Klein. 2006. Learning
accurate, compact, and interpretable
tree annotation. In Proceedings of the
21st International Conference on
Computational Linguistics (COLING) and
44th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 433–440, Sydney.
Stolcke, Andreas. 2002. SRILM—an
extensible language modeling toolkit.
In Proceedings of the 7th International
Conference on Spoken Language
Processing (ICSLP) 2002, pages 901–904,
Denver, CO.
Talbot, David and Miles Osborne. 2007.
Randomised language modelling for
statistical machine translation. In
Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics
(ACL), pages 512–519, Prague.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann.1996. HMM-based
word alignment in statistical translation. In
Proceedings of the International Conference on
Computational Linguistics (COLING)1996,
pages 836–841, Copenhagen.
Wang, Wei, Kevin Knight, and Daniel
Marcu. 2007. Binarizing syntax trees
to improve syntax-based machine
translation accuracy. In Proceedings
of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP–CoNLL), pages 746–754,
Prague.
Wu, Dekai.1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23(3):377–404.
Yamada, Kenji and Kevin Knight. 2001.
A syntax-based statistical translation
model. In Proceedings of the 39th
Annual Meeting of the Association for
</reference>
<page confidence="0.989073">
276
</page>
<note confidence="0.861174">
Wang et al. Re-structuring, Re-labeling, and Re-aligning
</note>
<reference confidence="0.999409863636364">
Computational Linguistics (ACL),
pages 523–530, Toulouse.
Yamada, Kenji and Kevin Knight. 2002.
A decoder for syntax-based statistical
MT. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 303–310,
Philadelphia, PA.
Zhang, Hao, Liang Huang, Daniel Gildea,
and Kevin Knight. 2006. Synchronous
binarization for machine translation.
In Proceedings of the main conference
on Human Language Technology
Conference of the North American Chapter
of the Association of Computational
Linguistics (HLT-NAACL), pages 256–263,
New York, NY.
Zollmann, Andreas and Khalil Sima’an.
2005. A consistent and efficient estimator
for data-oriented parsing. Journal of
Automata, Languages and Combinatorics,
10(2/3):367–388.
</reference>
<page confidence="0.997247">
277
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9826525">Re-structuring, Re-labeling, and Re-aligning for Syntax-Based Machine Translation</title>
<author confidence="0.447031">Language Weaver</author>
<author confidence="0.447031">Inc</author>
<affiliation confidence="0.9955085">USC/Information Sciences Institute USC/Information Sciences Institute</affiliation>
<abstract confidence="0.9379029">Language Weaver, Inc. This article shows that the structure of bilingual material from standard parsing and alignment tools is not optimal for training syntax-based statistical machine translation (SMT) systems. We present three modifications to the MT training data to improve the accuracy of a state-of-theart syntax MT system: re-structuring changes the syntactic structure of training parse trees to enable reuse of substructures; re-labeling alters bracket labels to enrich rule application context; and re-aligning unifies word alignment across sentences to remove bad word alignments and refine good ones. Better structures, labels, and word alignments are learned by the EM algorithm. We show that each individual technique leads to improvement as measured by BLEU, and we also show that the greatest improvement is achieved by combining them. We report an overall 1.48 BLEU improvement on the NIST08 evaluation set over a strong baseline in Chinese/English translation. 1. Background Syntactic methods have recently proven useful in statistical machine translation (SMT). In this article, we explore different ways of exploiting the structure of bilingual material for syntax-based SMT. In particular, we ask what kinds of tree structures, tree labels, and word alignments are best suited for improving end-to-end translation accuracy. We begin with structures from standard parsing and alignment tools, then use the EM algorithm to revise these structures in light of the translation task. We report an overall +1.48 BLEU improvement on a standard Chinese-to-English test.</abstract>
<note confidence="0.84407925">Center Drive, Suite 150, Los Angeles, CA, 90045, USA. E-mail: wwang@languageweaver.com. Admiralty Way, Marina del Rey, CA, 90292, USA. E-mail: jonmay@isi.edu. Admiralty Way, Marina del Rey, CA, 90292, USA. E-mail: knight@isi.edu. Center Drive, Suite 150, Los Angeles, CA, 90045, USA. E-mail: dmarcu@languageweaver.com. Submission received: 6 November 2008; revised submission received: 10 September 2009; accepted for publication: 1 January 2010. © 2010 Association for Computational Linguistics Computational Linguistics Volume 36, Number 2</note>
<abstract confidence="0.986521710583155">We carry out our experiments in the context of a string-to-tree translation system. This system accepts a Chinese string as input, and it searches through a multiplicity of English tree outputs, seeking the one with the highest score. The string-to-tree framework is motivated by a desire to improve target-language grammaticality. For example, it is common for string-based MT systems to output sentences with no verb. By contrast, the string-to-tree framework forces the output to respect syntactic requirements—for example, if the output is a syntactic tree whose root is S (sentence), then the S will generally have a child of type VP (verb phrase), which will in turn contain a verb. Another motivation is better treatment of function words. Often, these words are not literally translated (either by themselves or as part of a phrase), but rather they control what happens in the translation, as with case-marking particles or passive-voice particles. Finally, much of the re-ordering we find in translation is syntactically motivated, and this can be captured explicitly with syntax-based translation rules. Tree-to-tree systems are also promising, but in this work we concentrate only on target-language syntax. The target-language generation problem presents a difficult challenge, whereas the source sentence is fixed and usually already grammatical. To prepare training data for such a system, we begin with a bilingual text that has been automatically processed into segment pairs. We require that the segments be single sentences on the English side, whereas the corresponding Chinese segments may be sentences, sentence fragments, or multiple sentences. We then parse the English side of the bilingual text using a re-implementation of the Collins (1997) parsing model, which we train on the Penn English Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Finally, we word-align the segment pairs according to IBM Model 4 (Brown et al. 1993). Figure 1 shows a sample (tree, string, alignment) triple. We build two generative statistical models from this data. First, we construct a language model (Kneser and Ney 1995; Stolcke 2002) out of the side of the bilingual data. This model assigns a probability to any candidate translation, rewarding translations whose subsequences have been observed frequently in the training data. Second, we build a syntax-based translation model that we can use to produce candidate English trees from Chinese strings. Following previous work in noisy-channel Figure 1 A sample learning case for the syntax-based machine translation system described in this article. 248 Wang et al. Re-structuring, Re-labeling, and Re-aligning SMT (Brown et al. 1993), our model operates in the English-to-Chinese direction— we envision a generative top–down process by which an English tree is gradually transformed (by probabilistic rules) into an observed Chinese string. We represent a collection of such rules as a tree transducer (Knight and Graehl 2005). In order to construct this transducer from parsed and word-aligned data, we use the GHKM rule extraction algorithm of Galley et al. (2004). This algorithm computes the unique set of rules to explain any sentence pair in the data. Figure 2 shows all the minimal rules extracted from the example (tree, string, alignment) triple in Figure 1. Note that rules specify rotation (e.g., R1, R5), direct translation (R3, R10), insertion and deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example a We collect all rules over the entire bilingual corpus, and we normalize rule counts this way: When we apply these probabilities to derive an sentence a corresponding Chinese sentence we wind up computing the probability We smooth the rule counts with Good–Turing smoothing (Good 1953). This extraction method assigns each unaligned Chinese word to a default rule in the derivation tree. We next follow Galley et al. (2006) in allowing unaligned Chinese to participate in multiple translation rules. In this case, we obtain a minimal rules. Galley et al. show how to use EM to count rules over derivation forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley al. in collecting namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enumerating and scoring all of the English trees that can be derived from it by rule. The score is a product of and To search efficiently, we employ the CKY dynamicprogramming parsing algorithm (Yamada and Knight 2002; Galley et al. 2006). This algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix, we store the non-terminal symbol at the root of the English tree being built up. We also Figure 2 Minimal rules extracted from the learning case in Figure 1 using the GHKM procedure. 249 Computational Linguistics Volume 36, Number 2 Figure 3 Additional rules extracted from the learning case in Figure 1. store English words that appear at the left and right corners of the tree, as these are for computing the score when cells are combined. For CKY to work, all transducer rules must be broken down, or binarized, into rules that contain at most two variables—more efficient search can be gained if this binarization produces rules that can be incrementally scored by the language model (Melamed, Satta, and Wellington Zhang et al. 2006). Finally, we employ pruning 2007) for further efficiency in the search. When scoring translation candidates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The final score of a translation candidate is a weighted linear combination log log and the scores from these additional smaller models. We obtain weights through minimum error-rate training (Och 2003). The system thus constructed performs fairly well at Chinese-to-English translation, reflected in the NIST06 common evaluation of machine translation However, it would be surprising if the parse structures and word alignments in our bilingual data were somehow perfectly suited to syntax-based SMT—we have so far used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and Knight (2006) already investigated whether different syntactic labels would be more appropriate for SMT, though their study was carried out on a weak baseline translation system. In this article, we take a broad view and investigate how changes to syntactic structures, syntactic labels, and word alignments can lead to substantial improvements in translation quality on top of a strong baseline. We design our methods around problems that arise in MT data whose parses and alignments use some Penn Treebankstyle annotations. We believe that some of the techniques will apply to other annotation schemes, but conclusions here are limited to Penn Treebank-style trees. The rest of this article is structured as follows. Section 2 describes the corpora and model configurations used in our experiments. In each of the next three sections we present a technique for modifying the training data to improve syntax MT accuracy: tree re-structuring in Section 3, tree re-labeling in Section 4, and re-aligning in Section 5. In each of these three sections, we also present experiment results to show the impact of each individual technique on end-to-end MT accuracy. Section 6 shows the improvement made by combining all three techniques. We conclude in Section 7. official 250 Wang et al. Re-structuring, Re-labeling, and Re-aligning 2. Corpora for Experiments For our experiments, we use a 245 million word Chinese/English bitext, available from LDC. A re-implementation of the Collins (1997) parser runs on the English half of the bitext to produce parse trees, and GIZA runs on the entire bitext to produce M4 word alignments. We extract a subset of 36 million words from the entire bitext, by selecting only sentences in the mainland news domain. We extract translation rules from these selected 36 million words. Experiments show that our Chinese/English syntax MT systems built from this selected bitext give as high BLEU scores as from the entire bitext. Our development set consists of 1,453 lines and is extracted from the NIST02– NIST05 evaluation sets, for tuning of feature weights. The development set is from the newswire domain, and we chose it to represent a wide period of time rather than a single year. We use the NIST08 evaluation set as our test set. Because the NIST08 evaluation set is a mix of newswire text and Web text, we also report the BLEU scores on the newswire portion. We use two 5-gram language models. One is trained on the English half of the bitext. The other is trained on one billion words of monolingual data. Kneser–Ney smoothing (Kneser and Ney 1995) is applied to both language models. Language models are represented using randomized data structures similar to those of Talbot and Osborne (2007) in decoding for efficient RAM usage. To test the significance of improvements over the baseline, we compute paired bootstrap p-values (Koehn 2004) for BLEU between the baseline system and each improved system. 3. Re-structuring Trees for Training Our translation system is trained on Chinese/English data, where the English side has been automatically parsed into Penn Treebank-style trees. One striking fact about these trees is that they contain many flat structures. For example, base noun phrases frequently have five or more direct children. It is well known in monolingual parsing research that these flat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowlmany monolingual parsing models are that they can produce flat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are flattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We find that flat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment triples often lack sufficient generalization power. For example, consider the training samples in Figure 4. We should be able to learn enough from these two samples to translate the new phrase VIKTOR CHERNOMYRDIN AND HIS COLLEAGUE its English equivalent chernomyrdin and his 251 Computational Linguistics Volume 36, Number 2 Figure 4 Learning translation rules from flat English structures. However, the learned rules R12 and R13 do not fit together nicely. R12 can translate an English base noun phrase (NPB) that includes but only if it is preceded by words that translate into an English JJ and English NNP. Likewise, R13 can translate an NPB that includes his but only if preceded by two NNPs. Both rules want to create an NPB, and neither can supply the other with what it needs. If we re-structure the training trees as shown in Figure 5, we get much better Now rule R14 translates a free-standing NPB. This rule R15 the ability to translate n because it finds the necessary NPB to its left. we are re-structuring the trees in our MT training data by This allows us to extract better translation rules, though of course an extracted rule may have more than two variables. Whether the rules themselves should be binarized is a separate question, addressed in Melamed, Satta, and Wellington (2004) and Zhang et al. (2006). One can decide to re-structure training data trees, binarize translation rules, or do both, or do neither. Here we focus on English tree re-structuring. In this section, we explore the generalization ability of simple re-structuring methods like left-, right-, and head-binarization, and also their combinations. Simple binarization methods binarize syntax trees in a consistent fashion (left-, right-, or head-) and thus cannot guarantee that all the substructures can be factored out. For example, consistent right binarization of the training examples in Figure 4 makes available R14, misses R15. We therefore also introduce a re-structuring in which we binarize both to the left and right at the same time, resulting in a binarization forest. We employ the EM algorithm (Dempster, Laird, and Rubin 1977) to learn the binarization bias for each tree node in the corpus from the parallel alternatives. 252 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 5 Learning translation rules from binarized English structures. 3.1 Some Concepts We now explain some concepts to facilitate the descriptions of the re-structuring meth- We train our translation model on graphs et al. 2004). An graph is a tuple of a source-language sentence a target-language parse tree yields translates from and the word alignments The graphs in Figures 1, 4, and 5 are examples of alignment graphs. the alignment graph, a node in the parse tree is called rules can be extracted from it. We can extract rules from a node if and only if the yield of the tree node consistent with the word alignments—the covered by the node is contiguous not empty, and the does not align to any that is not covered by the node. An admissible tree node is one where rules overlap. Figure 6 shows different binarizations of the left tree in Figure 4. In this figure, the NPB node in tree (1) is not because the V-C, that the node covers also aligns to which is not covered by the NPB. Node NPB in tree (2), on the other hand, is admissible. set of sibling tree nodes is called we can form an admissible node dominating them. In Figure 6, sibling nodes and factorizable because we can factorize them out and form a new node NPB, resulting tree (2). Sibling tree nodes JJ, and not factorizable. Not all sibling nodes are factorizable, so not all sub-phrases can be acquired and syntactified. Our main purpose is to re-structure parse trees by factorization such that syntactified sub-phrases can be employed in translation. With these concepts defined, we now present the re-structuring methods. 253 Computational Linguistics Volume 36, Number 2 Figure 6 right, and head binarizations on the left tree in Figure 4. Tree leaves of nodes JJ and omitted for convenience. Heads are marked with New nonterminals introduced by binarization are denoted by X-bars. 3.2 Binarizing Syntax Trees We re-structure parse trees by binarizing the trees. We are going to binarize a tree node dominates ..., Binarization is performed by introducing new tree nodes to dominate a subset of the children nodes. We allow ourselves to form only one new node at a time to avoid over-generalization. Because labeling is not the concern of section, we re-label the newly formed nodes as Simple Binarization Methods. binarization node the NPB in tree ofFigure 6) factorizes the leftmost children by forming a new node the in tree (1)) to dominate them, leaving the last child untouched; and then makes new node left child of The method then recursively left-binarizes the newly node two leaves are reached. We left-binarize the left tree in Figure 4 into Figure 6 (1). binarization node the rightmost children by forming new node the NPB in tree (2)) to dominate them, leaving the first child and then makes the new node right child of The method then right-binarizes the newly formed node For instance, we right-binarize the left tree in Figure 4 into Figure 6 (2) and then into Figure 6 (6). binarization node the head is the first child; it right-binarizes We prefer right-binarization to left-binarization when both are applicable under the head restriction because our initial motivation was to generalize the NPB-rooted translation rules. As we show in experiments, binarization of other types of phrases contributes to translation accuracy as well. Any of these simple binarization methods is easy to implement, but each in itself is incapable of giving us all the factorizable sub-phrases. Binarizing all the way to the left, for example, from unbinarized tree to tree (1) and to tree (3) in Figure 6, does not us to acquire a substructure that yields and their translational To obtain more factorizable sub-phrases, we need to both directions. 254 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 7 Packed forest obtained by packing trees (3) and (6) in Figure 6. Parallel Binarization. binarizations transform a parse tree into another single parse tree. Parallel binarization transforms a parse tree into a binarization forest, packed to enable dynamic programming when we extract translation rules from it. Borrowing terms from parsing semirings (Goodman 1999), a packed forest is comof additive forest nodes and multiplicative forest nodes In binarization forest, a corresponds to a tree node in the unbinarized tree or a tree node introduced during tree binarization; and this composes several forming a one-level substructure that is observed in the unbinarized tree or one of its binarized tree. A corresponds to alternative ways of binarizing the tree node and it contains one or more The same can appear in more than one place in the packed forest, enabling sharing. Figure 7 shows a packed forest obtained by packing trees (3) and (6) in Figure 6 via the following tree parallel binarization algorithm. We use a memoization procedure to recursively parallel-binarize a parse tree. parallel-binarize a tree node has children we employ the following steps: If parallel-binarize tree nodes ..., producing binarization ..., respectively. Construct node as the parent Construct an additive node as the parent of Otherwise, execute the following steps. Right-binarize if any of children by introducing an intermediate tree node labeled as We parallel-binarize generate a binarization forest node also recursively parallel-binarize forming a binarization forest We form a multiplicative forest node the parent of and Left-binarize any contiguous subset of factorizable and this subset contains Similar to the previous right-binarization, introduce an intermediate tree node labeled as recursively generate a binarization forest node recursively 2 For practical purposes we factorize only subsets that cover contiguous spans to avoid introducing discontiguous constituents. In principle, the algorithm works fine without this condition. 255 Computational Linguistics Volume 36, Number 2 generate a binarization forest node and then a multiplicative forest node the parent of and Form an additive node as the parent of the two already formed nodes The (left and right) binarization conditions consider any subset to enable the factorization of small constituents. For example, in the left tree of Figure 4, although the and of the NPB are not factorizable, the subset JJ factorizable. The binarization from this tree to the tree in Figure 6 (1) serves as a relaying for us to factorize JJ and the tree in Figure 6 (3). The left-binarization condition is stricter than the right-binarization condition to avoid spurious binarization, that is, to avoid the same subconstituent being reached via both binarizations. In parallel binarization, nodes are not always binarizable in both directions. For we do not need to right-binarize tree (2) because not factorizable, and thus cannot be used to form sub-phrases. It is still possible to rightbinarize tree (2) without affecting the correctness of the parallel binarization algorithm, but that will spuriously increase the branching factor of the search for the rule extraction, because we will have to expand more tree nodes. special version of parallel binarization is the head where both the left and the right binarization must respect the head propagation property at the same time. Parallel head binarization guarantees that new nodes introduced by binarization always contain the head constituent, which will become convenient when head-driven syntax-based language models are integrated into a bottom–up decoding search by intersecting with the trees inferred from the translation model. Our re-structuring of MT training trees is realized by tree binarization, but this does not mean that our re-structuring method can factor out phrases covered only by two (binary) constituents. In fact, a nice property of parallel binarization is that for any factorizable substructure in the unbinarized tree, we can always find a corresponding in the parallel-binarized packed forest, and thus we can always extract that phrase. A leftmost substructure like the lowest NPB-subtree in tree (3) of Figure 6 can be made factorizable by several successive left binarizations, resulting in in the packed forest in Figure 7. A substructure in the middle can be factorized by the composition of several leftand right-binarizations. Therefore, after a tree is parallel-binarized, to make the sub-phrases available to the MT system, all we need to do is to extract rules from the admissible nodes in the packed forest. Rules that can be extracted from the original unrestructured tree can be extracted from the packed forest as well. Parallel binarization results in parse forests. Thus translation rules need to be exfrom training data consisting of (e-forest, 3.3 Extracting Translation Rules from (e-forest, f, a)-tuples algorithm to extract rules from (e-forest, is a natural generalization the (e-parse, rule extraction algorithm in Galley et al. (2006). A similar problem is also elegantly addressed in Mi and Huang (2008) in detail. The forest-based extraction algorithm takes as input a (e-forest, and outputs a derivation forest (Galley et al. 2006), which consists of overlapping translation rules. The algorithm recursively traverses the e-forest top–down, extracts rules only at admissible e-forest 256 Wang et al. Re-structuring, Re-labeling, and Re-aligning nodes, and transforms e-forest nodes into synchronous derivation-forest nodes via the following two procedures, depending on which condition is met. • Condition 1: If we reach an additive e-forest node, for each of its children, which are multiplicative e-forest nodes, we go to condition 2 to recursively extract rules from it to obtain a set of multiplicative derivation-forest nodes, respectively. We form an additive derivation-forest node, and take these newly produced multiplicative derivation-forest nodes (by going to condition 2) as children. After this, we return the additive derivation-forest node. instance, at node in Figure 7, for each of its children, e-forest and we go to condition 2 to extract rules on it, form multiplicative derivation forest nodes, and in Figure 8. • Condition 2: If we reach a multiplicative e-forest node, we extract a set of rules rooted at it using the procedure in Galley et al. (2006); and for each rule, we form a multiplicative derivation-forest node, and go to condition 1 to form the additive derivation-forest nodes for the additive frontier e-forest nodes of the newly extracted rule, and then make these additive derivation-forest nodes the children of the multiplicative derivation-forest node. After this, we return a set of multiplicative derivation-forest nodes, each corresponding to one rule extracted from the multiplicative e-forest node we just reached. Figure 8 synchronous derivation forest built from a (e-forest, triple. The e-forest is shown in Figure 7. 257 Computational Linguistics Volume 36, Number 2 example, at node in Figure 7, we extract a rule from it and derivation-forest node in Figure 8. We then go to condition 1 to obtain, for each of the additive frontier e-forest nodes (in Figure 7) of rule, a derivation-forest node, namely, and Figure 8. We make these derivation-forest the children of node This procedure transforms the packed e-forest in Figure 7 into a packed synchronous derivation in Figure 8. This algorithm is an extension of the extraction algorithm in Galley et al. (2006), in the sense that we have an extra condition (1) to relay rule extraction on additive e-forest nodes. The forest-based rule extraction algorithm produces much larger grammars than the tree-based one, making it difficult to scale to very large training data. From a 50M-word Chinese-to-English parallel corpus, we can extract more than 300 million translation rules, while the tree-based rule extraction algorithm gives approximately 100 million. However, the restructured trees from the simple binarization methods are not guaranteed to give the best trees for syntax-based machine translation. What we desire is a binarization method that still produces single parse trees, but is able to mix left binarization and right binarization in the same tree. In the following, we use the EM algorithm to learn the desirable binarization on the forest of binarization alternatives proposed by the parallel binarization algorithm. 3.4 Learning How to Binarize Via the EM Algorithm The basic idea of applying the EM algorithm to choose a re-structuring is as follows. We a set binarization operations on a parse tree Each binarization sequence of binarizations on the necessary (i.e., factorizable) nodes in pre-order. binarization in a restructured tree We extract rules from a translation model consisting of parameters (i.e., rule probabilities) Our aim is to first obtain the rule probabilities that are the maximum likelihood estimate of the training tuples, and then produce the Viterbi binarization tree for each training tuple. probability of a is what the basic syntax-based translation model is concerned with. It can be further computed by aggregating the rule in each derivation the set of all derivations et al. 2004). That is, 11 (1) The rule probabilities are estimated by the inside–outside algorithm (Lari and Young 1990; Knight, Graehl, and May 2008), which needs to run on derivation forests. Our previous sections have already presented algorithms to transform a parse tree into binarization forest, and then transform the (e-forest, into derivation forests (e.g., Figure 8), on which the inside–outside algorithm can then be applied. the derivation forests, an additive node labeled as several multiplicative nodes, each corresponding to a translation rule resulting from either left or right binarization of the original structure. We use rule either refer a rule or to a multiplicative node in the derivation forest. We use to represent root label of the rule, and to refer to the additive node that is the parent 258 Wang et al. Re-structuring, Re-labeling, and Re-aligning the node corresponding to the Each rule node (or multiplicative node) domseveral other additive children nodes, and we present the child node as among the total number of For example, in Figure 8, for the rule to the left child of the forest root (labeled as is and Based on these notations, we can compute the inside probability an additive node labeled as the outside probability of an additive forest labeled as follows. � � (2) (3) In the expectation step, the contribution of each occurrence of a rule in a derivationforest to the total expected count of that rule is computed as (4) the maximization step, we use the expected counts of rules, to update the probabilities of the rules. (5) Because it is well known that applying EM with tree fragments of different sizes causes overfitting (Johnson 1998a), and because it is also known that syntax MT models with larger composed rules in the mix significantly outperform rules that minimally explain the training data (minimal rules) in translation accuracy (Galley et al. 2006), we minimal rules to construct the derivation forests from (e-binarization-forest, tuples during running of the EM algorithm, but, after the EM re-structuring is finished, we build the final translation model using composed rules for MT evaluation. Figure 9 is the actual pipeline that we use for EM binarization. We first generate a packed e-forest via parallel binarization. We then extract minimal translation rules from Figure 9 Using the EM algorithm to choose re-structuring. 259 Computational Linguistics Volume 36, Number 2 (e-forest, producing synchronous derivation forests. We run the inside– outside algorithm on the derivation forests until convergence. We obtain the Viterbi derivations and project the English parses from the derivations. Finally, we extract rules using Galley et al. (2006)’s (e-tree, rule extraction algorithm. extracting composed rules from (e-parse, f, a)-tuples, we use an node” trick to the rule extraction method in Galley et al. (2006) to avoid breaking the local dependencies captured in complex rules. The trick is that new nodes introduced by binarization are not counted when computing the rule size limit unless they appear as the rule roots. The motivation is that newly introduced nodes break the local dependencies, deepening the parses. In Galley et al., a composed rule is extracted only if the number of internal nodes it contains does not exceed a limit, similar to the phrase length limit in phrase-based systems. This means that rules extracted from the restructured will be smaller than those from the unrestructured trees, if the are deleted from the rules. As shown in Galley et al., smaller rules lose context, and thus give translation accuracy. Ignoring when computing the rule sizes preserves the unrestructured rules in the resulting translation model and adds substructures as bonuses. 3.5 Experimental Results We carried out experiments to evaluate different tree binarization methods in terms of translation accuracy for Chinese-to-English translation. The baseline syntax MT system was trained on the original, non-restructured trees. We also built one MT system by training on left-binarizations of training trees, and another by training on EMbinarizations of training trees. Table 1 shows the results on end-to-end MT. The bootstrap p-values were computed for the pairwise BLEU comparison between the EM binarization and the baseline. The results show that tree binarization improves MT system accuracy, and that EM binarization outperforms left binarization. The results also show that the EM re-structuring sigoutperforms (p the no re-structuring baseline on the NIST08 eval set. The MT improvement by tree re-structuring is also validated by our previous work (Wang, Knight, and Marcu 2007), in which we reported a 1 BLEU point gain from EM binarization under other training/testing conditions; other simple binarization methods were examined in that work as well, showing that simple binarizations also improve MT accuracy, and that EM binarization consistently outperforms the simple binarization methods. Table 1 Translation accuracy versus binarization algorithms. In this and all other tables reporting BLEU performance, statistically significant improvements over the baseline are highlighted. p = the paired bootstrap p-value computed between each system and the baseline, showing the level at which the two systems are significantly different. NIST08-NW BLEU p BLEU p no binarization (baseline) left binarization EM binarization 29.12 — 35.33 — 29.35 0.184 35.46 0.360 29.74 0.010 36.12 0.016 260 Wang et al. Re-structuring, Re-labeling, and Re-aligning Table 2 METHOD # no left EM binarization 13 76.0 17.2 153.4 17.4 154.8 We think that these improvements are explained by the fact that tree re-structuring introduces more admissible trees nodes in the training trees and enables the forming of additional rules. As a result, re-structuring produces more rules. Table 2 shows the number of admissible nodes made available by each re-structuring method, as well as by the baseline. Table 2 also shows the sizes of the resulting grammars. The EM binarization is able to introduce more admissible nodes because it mixes both left and right binarizations in the same tree. We computed the binarization biases learned by the EM algorithm for each nonterminal from the binarization forest of parallel head binarizations of the training trees (Table 3). Of course, the binarization bias chosen by left-/right-binarization methods would be 100% deterministic. One noticeable message from Table 3 is that most of the categories are actually biased toward left-binarization. The reason might be that the head sub-constituents of most English categories tend to be on the left. Johnson (1998b) argues that the more nodes there are in a treebank, the stronger the independence assumptions implicit in the PCFG model are, and the less accurate the estimated PCFG will usually be—more nodes break more local dependencies. Our experiments, on the other hand, show MT accuracy improvement by introducing more admissible nodes. This initial contradiction actually makes sense. The key is that we use rules to build our final MT system and that we introduce the node” trick to preserve the local dependencies. More nodes in training trees weaken the accuracy of a translation model of minimal rules, but boost the accuracy of a translation model of composed rules. Table 3 Binarization bias learned by the EM re-structuring method on the model 4 word alignments. nonterminal left-binarization (%) right-binarization (%) 1 17 261 Computational Linguistics Volume 36, Number 2 4. Re-Labeling Trees for Training The syntax translation model explains (e-parse, f, a)-tuples by a series of applications of translation rules. At each derivation step, which rule to apply next depends only on the nonterminal label of the frontier node being expanded. In the Penn Treebank annotation, the nonterminal labels are too coarse to encode enough context information to accurately predict the next translation rule to apply. As a result, using the Penn Treebank annotation can license ill-formed subtrees (Figure 10). This subtree contains error that induces a VP as an SG-C when the head of the VP is the finite verb distranslation error leads to the ungrammatical confirmed discussed... This translation error occurs due to the fact that there is no distinction between finite VPs and non-finite VPs in Penn Treebank annotation. Monolingual parsing suffers similarly, but to a lesser degree. Re-structuring of training trees enables the reuse of sub-constituent structures, but further introduces new nonterminals and actually reduces the context for rules, thus making this “coarse nonterminal” problem more severe. In Figure 11, R23 may be extracted from a construct like S(S CC S) via tree binarization, and R24 may be extracted from a construct like S(NP NP-C VP) via tree binarization. Composing R23 and R24 forms the structure in Figure 11(b), which, however, is ill-formed. This wrong structure Figure 11(b) yields ungrammatical translations like likes reading she does not like Tree binarization enables the reuse of substructures, but causes over-generation of trees at the same time. We solve the coarse-nonterminal problem by refining/re-labeling the training tree labels. Re-labeling is done by enriching the nonterminal label of each tree node based on its context information. Re-labeling has already been used in monolingual parsing research to improve parsing accuracy of PCFGs. We are interested in two types of re-labeling methods: Linguistically motivated re-labeling (Klein and Manning 2003; Johnson 1998b) enriches the labels of parser training trees using parent labels, head word tag labels, and/or sibling labels. Automatic category splitting (Petrov et al. 2006) refines a nonterminal Figure 10 MT output errors due to coarse Penn Treebank annotations. Oval nodes in (b) are rule overlapping nodes. Subtree (b) is formed by composing the LHSs of R20, R21, and R22. 262 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 11 Tree binarization over-generalizes the parse tree. Translation rules R23 and R24 are acquired from binarized training trees, aiming for reuse of substructures. Composing R23 and R24, however, results in an ill-formed tree. The new nonterminal S introduced in tree binarization needs to be refined into different sub-categories to prevent R23 and R24 from being composed. Automatic category splitting can be employed for refining the S. by classifying the nonterminal into a fine-grained sub-category, and this sub-classing is learned via the EM algorithm. Category splitting is realized by several splitting-andmerging cycles. In each cycle, the nonterminals in the PCFG rules are split by splitting each nonterminal into two. The EM algorithm is employed to estimate the split PCFG on the Penn Treebank training trees. After that, 50% of the new nonterminals are merged based on some loss function, to avoid overfitting. 4.1 Linguistic Re-labeling In the linguistically motivated approach, we employ the following set of rules to re-label tree nodes. In our MT training data: • SPLIT-VP: annotates each VP nodes with its head tag, and then merges all finite VP forms to a single VPF. • SPLIT-IN: annotates each IN node with the combination of IN and its parent node label. IN is frequently overloaded in the Penn Treebank. For instance, its parent can be PP or SBAR. These two operations re-label the tree in Figure 12(a1) to Figure 12(b1). Example rules extracted from these two trees are shown in Figure 12(a2) and Figure 12(b2), respectively. We apply this re-labeling on the MT training tree nodes, and then acquire rules from these re-labeled trees. We chose to split only these two categories because our syntax MT system tends to frequently make parse errors in these two categories, and because, as shown by Klein and Manning (2003), further refining the VP and IN categories is very effective in improving monolingual parsing accuracy. This type of re-labeling fixes the parse error in Figure 10. SPLIT-VP transforms the R18 root to VPF, and the R17 frontier node to VP.VBN. Thus R17 and R18 can never be composed, preventing the wrong tree being formed by the translation model. 4.2 Statistical Re-labeling Our second re-labeling approach is to learn the split categories for the node labels of the training trees via the EM algorithm, as in Petrov et al. (2006). Rather than using 263 Computational Linguistics Volume 36, Number 2 Figure 12 Re-labeling of parse trees. 264 Wang et al. Re-structuring, Re-labeling, and Re-aligning their parser to directly produce category-split parse trees for the MT training data, we separate the parsing step and the re-labeling step. The re-labeling method is as follows. 1. Run a parser to produce the MT training trees. 2. Binarize the MT training trees via the EM binarization algorithm. Learn an split PCFG from the binarized trees via the algorithm described in Petrov et al. (2006). 4. Produce the Viterbi split annotations on the binarized training trees with the learned category-split PCFG. As we mentioned earlier, tree binarization sometimes makes the decoder overgeneralize the trees in the MT outputs, but we still binarize the training trees before performing category splitting, for two reasons. The first reason is that the improvement on MT accuracy we achieved by tree re-structuring indicates that the benefit we obtained from structure reuse triumphs the problem of tree over-generalization. The second is that carrying out category splitting on unbinarized training trees blows the grammar—splitting a CFG rule of rank 10 results in split rules. This relabeling procedure tries to achieve further improvement by trying to fix the tree overgeneralization problem of re-structuring while preserving the gain we have already obtained from tree re-structuring. Figure 12(c1) shows a category-split tree, and Figure 12(c2) shows the minimal xRs rules extracted from the split tree. In Figure 12(c2), the two VPs (VP-0 and VP-2) now belong to two different categories and cannot be used in the same context. In this re-labeling procedure, we separate the re-labeling step from the parsing step, rather than using a parser like the one in Petrov et al. (2006) to directly produce categorysplit parse trees on the English corpus. We think that we benefit from this separation in the following ways: First, this gives us the freedom to choose the parser to produce the initial trees. Second, this enables us to train the re-labeler on the domains where the MT system is trained, instead of on the Penn Treebank. Third, this enables us to choose our own tree binarization methods. Tree re-labeling fragments the translation rules. Each refined rule now fits in fewer contexts than its corresponding coarse rule. Re-labeling, however, does not explode the grammar size, nor does re-labeling deteriorate the reuse of substructures. This is because the re-labeling (whether linguistic or automatic) results in very consistent annotations. Table 4 shows the sizes of the translation grammars from different relabelings of the training trees, as well as that from the unrelabeled ones. Table 4 Grammar size vs. re-labeling methods. Re-labeling does not explode the grammar size. METHOD SET SIZE No re-labeling 154.80 144 Linguistically motivated re-labeling 154.97 210 4-way splitting (90% merging) 158.89 178 8-way splitting (90% merging) 160.62 195 4-way splitting (50% merging) 164.15 326 265 Computational Linguistics Volume 36, Number 2 It would be very interesting to perform automatic category splitting with synchronous translation rules and run the EM algorithm on the synchronous derivation forests. Synchronous category splitting is computationally much more expensive, so we do not study it here. 4.3 Experimental Results We ran end-to-end MT experiments by re-labeling the MT training trees. Our two baseline systems were a syntax MT system with neither re-structuring nor re-labeling, and a syntax MT system with re-structuring but no re-labeling. The linguistically motivated re-labeling method was applied directly on the original (unrestructured) training trees, so that it could be compared to the first baseline. The automatic category splitting relabeling method was applied to binarized trees so as to avoid the explosion of the split grammar, so it is compared to the second baseline. The experiment results are shown in Table 5. Both re-labeling methods help MT accuracy. Putting both re-structuring and relabeling together results in 0.93 BLEU points improvement on NIST08 set, and 1 BLEU point improvement on the newswire subset. All p-values are computed between the re-labeling systems and Baseline1. The improvement made by the linguistically motivated re-labeling method is significant at the 0.05 level. Because the automatic category splitting is carried out on the top of EM re-structuring and because, as we have already shown, EM re-structuring significantly improves Baseline1, putting them together results in better translations with more confidence. If we compare these results to those in Table 1, we notice that re-structuring tends to help MT accuracy more than re-labeling. We mentioned earlier that re-structuring overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and Table 1 show substructure reuse mitigates structure over-generalization in our tree restructuring method. 5. Re-aligning (Tree, String) Pairs for Training So far, we have improved the English structures in our parsed, aligned training corpus. We now turn to improving the word alignments. Some MT systems use the same model for alignment and translation—examples include Brown et al. (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al. for alignment, then collect counts for a completely different model, such as Och and Ney Table 5 Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was carried out on EM-binarized trees; thus, it already benefits from tree re-structuring. All p-values are computed against Baseline1. NIST08 NIST08-NW BLEU p BLEU p Baseline1 (no re-structuring and no re-labeling) 29.12 — 35.33 — Linguistically motivated re-labeling 29.57 0.029 35.85 0.050 Baseline2 (EM re-structuring but no re-labeling) 29.74 — 36.12 — 4-way splitting (w/ 90% merging) 30.05 0.001 36.42 0.003 266 Wang et al. Re-structuring, Re-labeling, and Re-aligning (2004) or Chiang (2007). Our basic syntax-based system falls into this second category, as we learn our syntactic translation model from a corpus aligned with word-based techniques. We would like to inject more syntactic reasoning into the alignment process. We start by contrasting two generative translation models. 5.1 The Traditional IBM Alignment Model Model 4 (Brown et al. 1993) learns a set of four probability tables to compute P( a foreign sentence its target translation the following (simplified) generative story: A fertility each word chosen with probability 2. A null word is inserted next to each fertility-expanded word with Each token the fertility-expanded word and null string is translated some foreign word probability The position of each foreign word was translated from changed may be positive, negative, or zero) with probability where functions over the source and target vocabularies, respectively. Brown et al. (1993) describe an EM algorithm for estimating values for the four tables in the generative story. With those values in hand, we can calculate the highestprobability (Viterbi) alignment for any given string pair. Two scale problems arise in this algorithm. The first is the time complexity of enumerating alignments for fractional count collection. This is solved by considering only subset of alignments, and by bootstrapping the table with a simpler model that admits fast count collection via dynamic programming, such as IBM Model 1 (Brown et al. 1993) or Aachen HMM (Vogel, Ney, and Tillmann 1996). The second problem is of space. In theory, the initial table contains a cell for every English word paired with every Chinese word—this would be infeasible. Fortunately, in practice, the table can be initialized with only those word pairs observed co-occurring in the parallel training text. 5.2 A Syntax Re-alignment Model syntax translation model learns a single probability table to compute a foreign sentence a parsed target translation In the following genstory we assume a starting variable with syntactic type Choose a rule replace with probability For each variable with syntactic type the partially completed (tree, pair, continue to choose rules probability to replace these variables until there are no variables remaining. We can use this model to explain unaligned (tree, string) pairs from our training data. With a large enough rule set, any given (tree, string) pair will admit many derivations. Consider again the example from Figure 1. The particular alignment associated 267 Computational Linguistics Volume 36, Number 2 with that (tree, string) pair yields the minimal rules of Figure 2. A different alignment yields different rules. Figure 13 shows two other alignments and their corresponding minimal rules. As noted before, a set of minimal rules in proper sequence forms a derivation tree of rules that explains the (tree, string) pair. Because rules explain variable-size fragments (e.g., R35 vs. R6), the possible derivation trees of rules that explain a sentence pair have varying sizes. The smallest such derivation tree has a single large rule (which does not appear in Figure 13). When our model chooses a particular derivation tree of minimal rules to explain a given (tree, string) pair it implicitly chooses alignment that produced these rules as Our model can choose a derivation by using any of the rules in Figures 13 and 2. We would prefer it select the derivation that yields the good alignment in Figure 1. We can also develop an EM learning approach for this model. As in the IBM approach, we have both time and space issues. Time complexity, as we will see subis where the number of nodes in the English training tree and the length of the corresponding Chinese string. Space is more of a problem. We would like to initialize EM with all the rules that might conceivably be used to explain the training data. However, this set is too large to practically enumerate. To reduce the model space we first create a bootstrap alignment using a simpler word-based model. Then we acquire a set of minimal translation rules from the (tree, string, alignment) triples. Armed with these rules, we can discard the word-based and the syntax translation model. We summarize the approach described in this section as: 1. Obtain bootstrap alignments for a training corpus using word-based alignment. 2. Extract minimal rules from the corpus and alignments using GHKM, noting the partial alignment that is used to extract each rule. 3. Construct derivation forests for each (tree, string) pair, ignoring alignments, and run EM to obtain Viterbi derivation trees, then use the annotated partial alignments to obtain Viterbi alignments. 4. Use the new alignments to re-train the full MT system, this time composed rules as well as minimal rules. 5.3 EM Training for the Syntax Translation Model Consider the example of Figure 13 again. The top alignment was the bootstrap alignment, and thus prior to any experiment we obtained the corresponding indicated minimal rule set and derivation. This derivation is reasonable but there are some poorly rules, from a linguistic standpoint. The Chinese word means two shores this context, but the rule R35 learned from the alignment incorrectly However, other sentences in the training corpus have the correct 3 Strictly speaking there is actually a one-to-many mapping between a derivation tree of minimal rules and the alignment that yields these rules, due to the handling of unaligned words. However, the choice of one partial alignment over another does not affect results and in practice we impose a one-to-one mapping between minimal rules and the partial alignments that imply them by selecting the most frequently observed partial alignment for a given minimal rule. 268 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 13 The minimal rules extracted from two different alignments of the sentence in Figure 1. alignment, which yields rules in Figure 2, such as R8. Figure 2 also contains rules R4 R6, learned from yet other sentences in the training corpus, which handle the (which roughly translates to thus allowing a derivation which contains the minimal rule set of Figure 2 and implies the alignment in Figure 1. EM distributes rule probabilities in such a way as to maximize the probability of the training corpus. It thus prefers to use one rule many times instead of several different 269 Computational Linguistics Volume 36, Number 2 rules for the same situation over several sentences, if possible. R35 is a possible rule in 46 of the 329,031 sentence pairs in the training corpus, and R8 is a possible rule in 100 sentence pairs. Well-formed rules are more usable than ill-formed rules and the partial alignments behind these rules, generally also well-formed, become favored as well. The top row of Figure 14 contains an example of an alignment learned by the bootstrap alignment model that includes an incorrect link. Rule R25, which is extracted from this alignment, is a poor rule. A set of commonly seen rules learned from other training sentences provide a more likely explanation of the data, and the consequent alignment omits the spurious link. Figure 14 The impact of a bad alignment on rule extraction. Including the alignment link indicated by the dotted line in the example leads to the rule set in the second row. The re-alignment procedure described in Section 5.2 learns to prefer the rule set at bottom, which omits the bad link. 270 Wang et al. Re-structuring, Re-labeling, and Re-aligning Table 6 Translation performance, grammar size versus the re-alignment algorithm proposed in Section 5.2, and re-alignment as modified in Section 5.4. NIST08 NIST08-NW BLEU p BLEU p no re-alignment (baseline) 29.12 — 35.33 — 76.0 EM re-alignment 29.18 0.411 35.52 0.296 75.1 EM re-alignment with size prior 29.37 0.165 35.96 0.050 110.4 Now we need an EM algorithm for learning the parameters of the rule set that � Knight, Graehl, and May (2008) present a generic such algocorpus for tree-to-string transducers that runs in time, as mentioned earlier. The consists of two components: which is a procedure for constructing a packed forest of derivation trees of rules that explain a (tree, string) bitext corpus that corpus and a rule set, and which is an iterative parameter-setting procedure. initially attempted to use the top-down of Knight, Graehl, and May (2008), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring dead-ends. Instead we build derivation forests using the following sequence of operations: 1. Binarize rules using the synchronous binarization algorithm for tree-to-string transducers described in Zhang et al. (2006). 2. Construct a parse chart with a CKY parser simultaneously constrained on the foreign string and English tree, similar to the bilingual parsing of Wu 3. Recover all reachable edges by traversing the chart, starting from the topmost entry. Because the chart is constructed bottom-up, leaf lexical constraints are encountered immediately, resulting in a narrower search space and faster running time than the for this application. The Viterbi derivation tree tells us which English words produce which Chinese words, so we can extract a word-to-word alignment from it. Although in principle the re-alignment model and translation model learn parameter weights over the same rule space, in practice we limit the rules used for re-alignment to the set of minimal rules. 5.4 Adding a Rule Size Prior An initial re-alignment experiment shows a small rise in BLEU scores from the baseline (Table 6), but closer inspection of the rules favored by EM implies we can do even better. 4 In the cases where a rule is not synchronous-binarizable, standard left–right binarization is performed and proper permutation of the disjoint English tree spans must be verified when building the part of the chart that uses this rule. 271 Computational Linguistics Volume 36, Number 2 EM has a tendency to favor a few large rules over many small rules, even when the small rules are more useful. Referring to the rules in Figures 2 and 13, note that possible for are R33, R2–R3, and R38–R40. Clearly the third derivation is not desirable, and we do not discuss it further. Between the first two derivations, R2–R3 is preferred over R33, as the conditioning for possessive insertion is not related to the specific Chinese word being inserted. Of the 1,902 sentences in the training corpus where this pair is seen, the bootstrap alignments yield the R33 derivation 1,649 times and the R2–R3 derivation 0 times. Re-alignment does not change the result much; the new alignments yield the R33 derivation 1,613 times and again never choose R2–R3. The rules in the second derivation themselves are not rarely seen—R2 is in 13,311 forests those where R33 is seen, and R3 is in 2,500 additional forests. EM gives R2 a of than 98.7% of rules, and R3 a probability of But R33 a probability of and is preferred over the R2–R3 derivation, which has a probability of The preference for shorter derivations containing large rules over longer derivations containing small rules is due to a general tendency for EM to prefer derivations with few atoms. Marcu and Wong (2002) note this preference but consider the phenomenon a feature, rather than a bug. Zollmann and Sima’an (2005) combat the overfitting aspect for parsing by using a held-out corpus and a straight maximum likelihood estimate, rather than EM. DeNero, Bouchard-Cˆot´e, and Klein (2008) encourage small rules with a modeling approach; they put a Dirichlet process prior of rule size over their model and learn the parameters of the geometric distribution of that prior with Gibbs sampling. We use a simpler modeling approach to accomplish the same goals as DeNero, Bouchard- Cˆot´e, and Klein which, although less elegant, is more scalable and does not require a separate Bayesian inference procedure. As the probability of a derivation is determined by the product of its atom probabilities, longer derivations with more probabilities to multiply have an inherent disadvantage against shorter derivations, all else being equal. EM is an iterative procedure and thus such a bias can lead the procedure to converge with artificially raised probabilities for short derivations and the large rules that constitute them. The relatively rare applicability of large rules (and thus lower observed partial counts) does not overcome the inherent advantage of large coverage. To combat this, we introduce size terms into our generative story, ensuring that all competing derivations for the same sentence contain the same number of atoms: Choose a rule size cost Choose a rule size to replace the start symbol with 3. For each variable in the partially completed (tree, string) pair, continue choose sizes followed by rules, recursively to replace these variables until there are no variables remaining. This generative story changes the derivation comparison from R33 vs. R2–R3 to S2– R33 vs. R2–R3, where S2 is the atom that represents the choice of size 2 (the size of a rule 5 The Chinese gloss is simply “taiwan”. 272 Wang et al. Re-structuring, Re-labeling, and Re-aligning in this context is the number of non-leaf and non-root nodes in its tree fragment). Note that the variable number of inclusions implied by the exponent in the generative story above ensures that all derivations have the same size. For example, a derivation with one size-3 rule, a derivation with one size-2 and one size-1 rule, and a derivation with three size-1 rules would each have three atoms. With this revised model that allows for fair comparison of derivations, the R2–R3 derivation is chosen 1,636 times, and S2–R33 is not chosen. R33 does, however, appear in the translation model, as the expanded rule extraction described in Section 1 creates R33 by joining R2 and R3. The probability of size atoms, like that of rule atoms, is decided by EM. The revised generative story tends to encourage smaller sizes by virtue of the exponent. This does not, however, simply ensure the largest number of rules per derivation is used in all cases. Ill-fitting and poorly motivated rules such as R42, R43, and R44 in Figure 13 are not preferred over R8, even though they are smaller. However, R6 and R8 are preferred over R35, as the former are useful rules. Although the modified model does not sum to 1, it can nevertheless lead to an improvement in BLEU score. 5.5 Experimental Results The end-to-end MT experiment used as baseline and described in Sections 3.5 and 4.3 was trained on IBM Model 4 word alignments, obtained by running GIZA, as described in Section 2. We compared this baseline to an MT system that used alignments obtained by re-aligning the GIZA alignments using the method of Section 5.2 with the 36 million word subset of the training corpus used for re-alignment learning. We next compared the baseline to an MT system that used re-alignments obtained by also incorporating the size prior described in Section 5.4. As can be seen by the results in Table 6, the size prior method is needed to obtain reasonable improvement in BLEU. These results are consistent with those reported in May and Knight (2007), where gains in Chinese and Arabic MT systems were observed, though over a weaker baseline and with less training data than is used in this work. 6. Combining Techniques We have thus far seen gains in BLEU score by independent improvements in training data tree structure, syntax labeling, and alignment. This naturally raises the question of whether the techniques can be combined, that is, if improvement in one aspect of training data aids in improvement of another. As reported in Section 4.3 and Table 5, we were able to improve re-labeling efforts and take advantage of the split-and-merge technique of Petrov et al. (2006) by first re-structuring via the method described in Section 3.4. It is unlikely that such re-structuring or re-labeling would aid in a subsequent re-alignment procedure like that of Section 5.2, for re-structuring changes trees based on a given alignment, and re-alignment can only change links when multiple instances of a (subtree, substring) tuple are found in the data with different partial alignments. Re-structuring beforehand changes the trees over different alignments differently. It is unlikely that many (subtree, substring) tuples with more than one partial alignment would remain after a re-structuring. However, re-structuring may benefit from a prior re-alignment. We do not want restructuring decisions to be made over bad alignments, so unifying alignments based on common syntax should lead EM to make a more confident binarization decision. 273 Computational Linguistics Volume 36, Number 2 Table 7 Summary of experiments in this article, including a combined experiment with re-alignment, re-structuring, and re-labeling. NIST08 NIST08-NW BLEU p BLEU p</abstract>
<note confidence="0.943673222222222">Baseline (no binarization, no re-labeling, 29.12 — 35.33 — 76.0 Model 4 alignments) left binarization 29.35 0.184 35.46 0.360 153.4 EM binarization 29.74 0.010 36.12 0.016 154.8 Linguistic re-labeling 29.57 0.029 35.85 0.050 154.97 EM binarization + EM re-labeling 30.05 0.001 36.42 0.003 158.89 (4-way splitting w/ 90% merging) EM re-alignment 29.18 0.411 35.52 0.296 75.1 Size prior EM re-alignment 29.37 0.165 35.96 0.050 110.4 Size prior EM re-alignment + 30.6 0.001 36.73 0.002 222.0</note>
<abstract confidence="0.954948408163266">EM binarization + EM re-labeling Better re-structuring should in turn lead to better re-labeling, and this should increase the performance of the overall MT pipeline. To test this hypothesis we pre-processed alignments using the modified realignment procedure described in Section 5.4. We next used those alignments to obtain new binarizations of trees following the EM binarization method described in Section 3.4. Finally, re-labeling was done on these binarized trees using 4-way splitting with 90% merging, as described in Section 4. The final trees, along with the alignments used to get these trees and of course the parallel Chinese sentences, were then used as the training data of our MT pipeline. The results of this combined experiment are shown in Table 7 along with the other experiments from this article, for ease of comparison. As can be seen from this table, the progressive improvement of training data leads to an overall improvement in MT system performance. As noted previously, there tends to be a correspondence between the number of unique rules extracted and MT performance. The final combined experiment has the greatest number of unique rules. The improvements made to syntax and alignment described in this article unify these two independently determined annotations over the bitext, and this thus leads to more admissible nodes and a greater ability to extract rules. Such a unification can lead to over-generalization, as rules lacking sufficient context may be extracted and used to the system’s detriment. This is why a re-labeling technique is also needed, to ensure that sufficient rule specificity is maintained. 7. Conclusion This article considered three modifications to MT training data that encourage improved performance in a state-of-the-art syntactic MT system. The improvements changed syntactic structure, altered bracket labels, and unified alignment across sentences, and when combined led to an improvement of 1.48 BLEU points over a strong baseline in Chinese–English translation. The techniques herein described require only 274 Wang et al. Re-structuring, Re-labeling, and Re-aligning the training data used in the original MT task and are thus applicable to a string-to-tree MT system for any language pair. Acknowledgments Some of the results in this article appear in Wang, Knight, and Marcu (2007) and May and Knight (2007). The linguistically motivated re-labeling method is due to Steve DeNeefe, Kevin Knight, and David Chiang. The authors also wish to thank Slav Petrov for his help with the Berkeley parser, and the anonymous reviewers for their helpful comments. This research was supported under DARPA Contract No. HR0011-06-C-0022, BBN subcontract 9500008412. References Alshawi, Hiyan, Srinivas Bangalore, and Shona Douglas. 1998. Automatic acquisition of hierarchical transduction models for machine translation. In</abstract>
<note confidence="0.877016142857143">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL) and 17th International Conference on Linguistics pages 41–47, Montr´eal. Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter 19(2):263–312. Charniak, Eugene. 2000. A maximumparser. In of the 1st North American Chapter of the Association for Computational Linguistics pages 132–139, Seattle, WA. Chiang, David. 2007. Hierarchical translation. 33(2):201–228. Cohn, Trevor and Phil Blunsom. 2009. A Bayesian model of syntax-directed tree to string grammar induction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language pages 352–361, Singapore. Collins, Michael. 1997. Three generative, lexicalized models for statistical parsing. of the 35th Annual Meeting of the Association for Computational Linguistics pages 16–23, Madrid. Dempster, Arthur P., Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the algorithm. of the Royal 39(1):1–38.</note>
<author confidence="0.410421">What can</author>
<abstract confidence="0.741448333333333">syntax-based MT learn from phrase-based In of pages 755–763, Prague. DeNero, John, Alexandre Bouchard-Cˆot´e, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation</abstract>
<note confidence="0.678581888888889">In of the 2008 Conference on Empirical Methods in Natural Language pages 314–323, Honolulu, HI. Gale, William A. and Geoffrey Sampson. 1996. Good-Turing frequency estimation tears. of Quantitative 2(3):217–237. Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. of the 21st International Conference on Computational Linguistics (COLING) and 44th Annual Meeting of the Association for Computational Linguistics pages 961–968, Sydney.</note>
<author confidence="0.778182">Michel Galley</author>
<author confidence="0.778182">Mark Hopkins</author>
<author confidence="0.778182">Kevin Knight</author>
<abstract confidence="0.849363115384615">and Daniel Marcu. 2004. What’s in a rule? In of the Human Language Technology Conference and the North American Association for Linguistics pages 273–280, Boston, MA. Good, Irving J. 1953. The population frequencies of species and the estimation population parameters. 40(3):237–264. Goodman, Joshua. 1999. Semiring parsing. 25(4):573–605. Huang, Bryant and Kevin Knight. 2006. Relabeling syntax trees to improve syntax-based machine translation In of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational pages 240–247, New York, NY. Johnson, Mark. 1998a. The DOP estimation method is biased and inconsistent. 28(1):71–76. Johnson, Mark. 1998b. PCFG models of linguistic tree representations.</abstract>
<note confidence="0.814573575757576">24(4):613–632. Klein, Dan and Chris Manning. 2003. Accurate unlexicalized parsing. In 275 Computational Linguistics Volume 36, Number 2 Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics pages 423–430, Sapporo. Kneser, Reinhard and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the International Conference on Acoustics, Speech, and Signal (ICASSP) pages 181–184, Detroit, MI. Knight, Kevin and Jonathan Graehl. 2005. An overview of probabilistic tree transducers for natural language In of the Sixth International Conference on Intelligent Text Processing and Computational pages 1–25, Mexico City. Knight, Kevin, Jonathan Graehl, and Jonathan May. 2008. Training tree 34(3):391–427. Koehn, Philipp. 2004. Statistical significance tests for machine translation evaluation. of the 2004 Conference on Empirical Methods in Natural Language pages 388–395, Barcelona. Lari, Karim and Steve Young. 1990. The</note>
<abstract confidence="0.577500181818182">estimation of stochastic context-free grammars using the inside-outside Speech and 4:35–56. Marcu, Daniel and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. of the 2002 Conference on Empirical Methods in Natural Language pages 133–139, Philadelphia, PA. Marcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Treebank. 19(2):313–330. May, Jonathan and Kevin Knight. 2007. Syntactic re-alignment models for translation. In of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language</abstract>
<note confidence="0.857288156862745">pages 360–368, Prague. Melamed, I. Dan, Giorgio Satta, and Benjamin Wellington. 2004. Generalized grammars. In of the 42nd Annual Meeting of the Association Computational Linguistics pages 662–669, Barcelona. Mi, Haitao and Liang Huang. 2008. Forest-based translation rule extraction. of the 2008 Conference on Empirical Methods in Natural Language pages 206–214, Honolulu, HI. Och, Franz and Hermann Ney. 2004. The alignment template approach to statistical translation. 30(4):417–449. Och, Franz Josef. 2003. Minimum error rate training for machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics pages 160–167, Sapporo. Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable annotation. In of the 21st International Conference on Computational Linguistics (COLING) and 44th Annual Meeting of the Association Computational Linguistics pages 433–440, Sydney. Stolcke, Andreas. 2002. SRILM—an extensible language modeling toolkit. of the 7th International Conference on Spoken Language (ICSLP) pages 901–904, Denver, CO. Talbot, David and Miles Osborne. 2007. Randomised language modelling for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics pages 512–519, Prague. Vogel, Stephan, Hermann Ney, and Christoph Tillmann.1996. HMM-based word alignment in statistical translation. In Proceedings of the International Conference on Linguistics pages 836–841, Copenhagen. Wang, Wei, Kevin Knight, and Daniel</note>
<abstract confidence="0.956070090909091">Marcu. 2007. Binarizing syntax trees to improve syntax-based machine accuracy. In of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language pages 746–754, Prague. Wu, Dekai.1997. Stochastic inversion transduction grammars and bilingual of parallel corpora.</abstract>
<note confidence="0.686219142857143">23(3):377–404. Yamada, Kenji and Kevin Knight. 2001. A syntax-based statistical translation In of the 39th Annual Meeting of the Association for 276 Wang et al. Re-structuring, Re-labeling, and Re-aligning Linguistics pages 523–530, Toulouse. Yamada, Kenji and Kevin Knight. 2002. A decoder for syntax-based statistical In of the 40th Annual Meeting of the Association for Computational pages 303–310,</note>
<address confidence="0.740408">Philadelphia, PA.</address>
<author confidence="0.913114">Hao Zhang</author>
<author confidence="0.913114">Liang Huang</author>
<author confidence="0.913114">Daniel Gildea</author>
<abstract confidence="0.486715538461538">and Kevin Knight. 2006. Synchronous binarization for machine translation. of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational pages 256–263, New York, NY. Zollmann, Andreas and Khalil Sima’an. 2005. A consistent and efficient estimator data-oriented parsing. of Languages and 10(2/3):367–388.</abstract>
<intro confidence="0.391606">277</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Srinivas Bangalore</author>
<author>Shona Douglas</author>
</authors>
<title>Automatic acquisition of hierarchical transduction models for machine translation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL) and 17th International Conference on Computational Linguistics (COLING)1998,</booktitle>
<pages>41--47</pages>
<location>Montr´eal.</location>
<marker>Alshawi, Bangalore, Douglas, 1998</marker>
<rawString>Alshawi, Hiyan, Srinivas Bangalore, and Shona Douglas. 1998. Automatic acquisition of hierarchical transduction models for machine translation. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL) and 17th International Conference on Computational Linguistics (COLING)1998, pages 41–47, Montr´eal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4339" citStr="Brown et al. 1993" startWordPosition="635" endWordPosition="638">already grammatical. To prepare training data for such a system, we begin with a bilingual text that has been automatically processed into segment pairs. We require that the segments be single sentences on the English side, whereas the corresponding Chinese segments may be sentences, sentence fragments, or multiple sentences. We then parse the English side of the bilingual text using a re-implementation of the Collins (1997) parsing model, which we train on the Penn English Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Finally, we word-align the segment pairs according to IBM Model 4 (Brown et al. 1993). Figure 1 shows a sample (tree, string, alignment) triple. We build two generative statistical models from this data. First, we construct a smoothed n-gram language model (Kneser and Ney 1995; Stolcke 2002) out of the English side of the bilingual data. This model assigns a probability P(e) to any candidate translation, rewarding translations whose subsequences have been observed frequently in the training data. Second, we build a syntax-based translation model that we can use to produce candidate English trees from Chinese strings. Following previous work in noisy-channel Figure 1 A sample l</context>
<context position="49449" citStr="Brown et al. (1993)" startWordPosition="7817" endWordPosition="7820"> to those in Table 1, we notice that re-structuring tends to help MT accuracy more than re-labeling. We mentioned earlier that re-structuring overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and Table 1 show substructure reuse mitigates structure over-generalization in our tree restructuring method. 5. Re-aligning (Tree, String) Pairs for Training So far, we have improved the English structures in our parsed, aligned training corpus. We now turn to improving the word alignments. Some MT systems use the same model for alignment and translation—examples include Brown et al. (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al. for alignment, then collect counts for a completely different model, such as Och and Ney Table 5 Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was carried out on EM-binarized trees; thus, it already benefits from tree re-structuring. All p-values are computed against Baseline1. EXPERIMENT NIST08 NIST08-NW BLEU p BLEU p Baseline1 (no re-structuring and no re-labeling) 29.12 — 35.33 — Linguistically motivated re-la</context>
<context position="51402" citStr="Brown et al. (1993)" startWordPosition="8126" endWordPosition="8129">ing (simplified) generative story: 1. A fertility y for each word ei in e is chosen with probability Pfert(y|ei). 2. A null word is inserted next to each fertility-expanded word with probability Pnull. 3. Each token ei in the fertility-expanded word and null string is translated into some foreign word fi in f with probability Ptrans(fi|ei). 4. The position of each foreign word fi that was translated from ei is changed by ∆ (which may be positive, negative, or zero) with probability Pdistortion(∆|A(ei), B( fi)), where A and B are functions over the source and target vocabularies, respectively. Brown et al. (1993) describe an EM algorithm for estimating values for the four tables in the generative story. With those values in hand, we can calculate the highestprobability (Viterbi) alignment for any given string pair. Two scale problems arise in this algorithm. The first is the time complexity of enumerating alignments for fractional count collection. This is solved by considering only a subset of alignments, and by bootstrapping the Ptrans table with a simpler model that admits fast count collection via dynamic programming, such as IBM Model 1 (Brown et al. 1993) or Aachen HMM (Vogel, Ney, and Tillmann </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximumentropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference (NAACL),</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="13111" citStr="Charniak 2000" startWordPosition="2029" endWordPosition="2030">trees. One striking fact about these trees is that they contain many flat structures. For example, base noun phrases frequently have five or more direct children. It is well known in monolingual parsing research that these flat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce flat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are flattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We find that flat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, Eugene. 2000. A maximumentropy-inspired parser. In Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference (NAACL), pages 132–139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="8224" citStr="Chiang 2007" startWordPosition="1265" endWordPosition="1266">utational Linguistics Volume 36, Number 2 Figure 3 Additional rules extracted from the learning case in Figure 1. store English words that appear at the left and right corners of the tree, as these are needed for computing the P(e) score when cells are combined. For CKY to work, all transducer rules must be broken down, or binarized, into rules that contain at most two variables—more efficient search can be gained if this binarization produces rules that can be incrementally scored by the language model (Melamed, Satta, and Wellington 2004; Zhang et al. 2006). Finally, we employ cube pruning (Chiang 2007) for further efficiency in the search. When scoring translation candidates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The final score of a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional sm</context>
<context position="50286" citStr="Chiang (2007)" startWordPosition="7946" endWordPosition="7947">h and Ney Table 5 Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was carried out on EM-binarized trees; thus, it already benefits from tree re-structuring. All p-values are computed against Baseline1. EXPERIMENT NIST08 NIST08-NW BLEU p BLEU p Baseline1 (no re-structuring and no re-labeling) 29.12 — 35.33 — Linguistically motivated re-labeling 29.57 0.029 35.85 0.050 Baseline2 (EM re-structuring but no re-labeling) 29.74 — 36.12 — 4-way splitting (w/ 90% merging) 30.05 0.001 36.42 0.003 266 Wang et al. Re-structuring, Re-labeling, and Re-aligning (2004) or Chiang (2007). Our basic syntax-based system falls into this second category, as we learn our syntactic translation model from a corpus aligned with word-based techniques. We would like to inject more syntactic reasoning into the alignment process. We start by contrasting two generative translation models. 5.1 The Traditional IBM Alignment Model IBM Model 4 (Brown et al. 1993) learns a set of four probability tables to compute P( f |e) given a foreign sentence f and its target translation e via the following (simplified) generative story: 1. A fertility y for each word ei in e is chosen with probability Pf</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>Chiang, David. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>A Bayesian model of syntax-directed tree to string grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>352--361</pages>
<contexts>
<context position="49561" citStr="Cohn and Blunsom (2009)" startWordPosition="7834" endWordPosition="7837">ioned earlier that re-structuring overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and Table 1 show substructure reuse mitigates structure over-generalization in our tree restructuring method. 5. Re-aligning (Tree, String) Pairs for Training So far, we have improved the English structures in our parsed, aligned training corpus. We now turn to improving the word alignments. Some MT systems use the same model for alignment and translation—examples include Brown et al. (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al. for alignment, then collect counts for a completely different model, such as Och and Ney Table 5 Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was carried out on EM-binarized trees; thus, it already benefits from tree re-structuring. All p-values are computed against Baseline1. EXPERIMENT NIST08 NIST08-NW BLEU p BLEU p Baseline1 (no re-structuring and no re-labeling) 29.12 — 35.33 — Linguistically motivated re-labeling 29.57 0.029 35.85 0.050 Baseline2 (EM re-structuring but no re-labeling) 29.74 — 36.12 — 4-way splitting </context>
</contexts>
<marker>Cohn, Blunsom, 2009</marker>
<rawString>Cohn, Trevor and Phil Blunsom. 2009. A Bayesian model of syntax-directed tree to string grammar induction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 352–361, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalized models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>16--23</pages>
<location>Madrid.</location>
<contexts>
<context position="4149" citStr="Collins (1997)" startWordPosition="607" endWordPosition="608">, but in this work we concentrate only on target-language syntax. The target-language generation problem presents a difficult challenge, whereas the source sentence is fixed and usually already grammatical. To prepare training data for such a system, we begin with a bilingual text that has been automatically processed into segment pairs. We require that the segments be single sentences on the English side, whereas the corresponding Chinese segments may be sentences, sentence fragments, or multiple sentences. We then parse the English side of the bilingual text using a re-implementation of the Collins (1997) parsing model, which we train on the Penn English Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Finally, we word-align the segment pairs according to IBM Model 4 (Brown et al. 1993). Figure 1 shows a sample (tree, string, alignment) triple. We build two generative statistical models from this data. First, we construct a smoothed n-gram language model (Kneser and Ney 1995; Stolcke 2002) out of the English side of the bilingual data. This model assigns a probability P(e) to any candidate translation, rewarding translations whose subsequences have been observed frequently in the training</context>
<context position="10866" citStr="Collins (1997)" startWordPosition="1663" endWordPosition="1664">ng in Section 3, tree re-labeling in Section 4, and re-aligning in Section 5. In each of these three sections, we also present experiment results to show the impact of each individual technique on end-to-end MT accuracy. Section 6 shows the improvement made by combining all three techniques. We conclude in Section 7. 1 http://nist.gov/speech/tests/mt/2006/doc/mt06eval official results.html. 250 Wang et al. Re-structuring, Re-labeling, and Re-aligning 2. Corpora for Experiments For our experiments, we use a 245 million word Chinese/English bitext, available from LDC. A re-implementation of the Collins (1997) parser runs on the English half of the bitext to produce parse trees, and GIZA runs on the entire bitext to produce M4 word alignments. We extract a subset of 36 million words from the entire bitext, by selecting only sentences in the mainland news domain. We extract translation rules from these selected 36 million words. Experiments show that our Chinese/English syntax MT systems built from this selected bitext give as high BLEU scores as from the entire bitext. Our development set consists of 1,453 lines and is extracted from the NIST02– NIST05 evaluation sets, for tuning of feature weights</context>
<context position="13095" citStr="Collins 1997" startWordPosition="2027" endWordPosition="2028">reebank-style trees. One striking fact about these trees is that they contain many flat structures. For example, base noun phrases frequently have five or more direct children. It is well known in monolingual parsing research that these flat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce flat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are flattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We find that flat structures are also problematic for syntax-based machine translation. The rules we learn from tree/</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, Michael. 1997. Three generative, lexicalized models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL), pages 16–23, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<issue>1</issue>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, Arthur P., Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve DeNeefe</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
<author>Daniel Marcu</author>
</authors>
<title>What can syntax-based MT learn from phrase-based MT?</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP–CoNLL-2007,</booktitle>
<pages>755--763</pages>
<location>Prague.</location>
<contexts>
<context position="6959" citStr="DeNeefe et al. 2007" startWordPosition="1046" endWordPosition="1049">traction method assigns each unaligned Chinese word to a default rule in the derivation tree. We next follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules. In this case, we obtain a derivation forest of minimal rules. Galley et al. show how to use EM to count rules over derivation forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enumerating and scoring all of the English trees that can be derived from it by rule. The score is a weighted product of P(e) and P(e, c). To search efficiently, we employ the CKY dynamicprogramming parsing algorithm (Yamada and Knight 2002; Galley et al. 2006). This algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix, we store the non-terminal symbol at the root of the English tree being built up. We also Figure 2 Minimal rules extracted from the learnin</context>
</contexts>
<marker>DeNeefe, Knight, Wang, Marcu, 2007</marker>
<rawString>DeNeefe, Steve, Kevin Knight, Wei Wang, and Daniel Marcu. 2007. What can syntax-based MT learn from phrase-based MT? In Proceedings of EMNLP–CoNLL-2007, pages 755–763, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>314--323</pages>
<location>Honolulu, HI.</location>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>DeNero, John, Alexandre Bouchard-Cˆot´e, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 314–323, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Geoffrey Sampson</author>
</authors>
<title>Good-Turing frequency estimation without tears.</title>
<date>1996</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="8604" citStr="Gale and Sampson 1996" startWordPosition="1317" endWordPosition="1320"> variables—more efficient search can be gained if this binarization produces rules that can be incrementally scored by the language model (Melamed, Satta, and Wellington 2004; Zhang et al. 2006). Finally, we employ cube pruning (Chiang 2007) for further efficiency in the search. When scoring translation candidates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The final score of a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain weights through minimum error-rate training (Och 2003). The system thus constructed performs fairly well at Chinese-to-English translation, as reflected in the NIST06 common evaluation of machine translation quality.1 However, it would be surprising if the parse structures and word alignments in our bilingual data were somehow perfectly suited to syntax-</context>
</contexts>
<marker>Gale, Sampson, 1996</marker>
<rawString>Gale, William A. and Geoffrey Sampson. 1996. Good-Turing frequency estimation without tears. Journal of Quantitative Linguistics, 2(3):217–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics (COLING) and 44th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>961--968</pages>
<location>Sydney.</location>
<contexts>
<context position="6468" citStr="Galley et al. (2006)" startWordPosition="967" endWordPosition="970">n and deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example form a derivation tree. We collect all rules over the entire bilingual corpus, and we normalize rule counts in this way: P(rule) = count(rule) count(LHS-root(rule)). When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up computing the joint probability P(e, c). We smooth the rule counts with Good–Turing smoothing (Good 1953). This extraction method assigns each unaligned Chinese word to a default rule in the derivation tree. We next follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules. In this case, we obtain a derivation forest of minimal rules. Galley et al. show how to use EM to count rules over derivation forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enu</context>
<context position="26296" citStr="Galley et al. (2006)" startWordPosition="4166" endWordPosition="4169"> sub-phrases available to the MT system, all we need to do is to extract rules from the admissible nodes in the packed forest. Rules that can be extracted from the original unrestructured tree can be extracted from the packed forest as well. Parallel binarization results in parse forests. Thus translation rules need to be extracted from training data consisting of (e-forest, f, a)-tuples. 3.3 Extracting Translation Rules from (e-forest, f, a)-tuples Our algorithm to extract rules from (e-forest, f, a)-tuples is a natural generalization of the (e-parse, f, a)-based rule extraction algorithm in Galley et al. (2006). A similar problem is also elegantly addressed in Mi and Huang (2008) in detail. The forest-based rule extraction algorithm takes as input a (e-forest, f, a)-triple, and outputs a derivation forest (Galley et al. 2006), which consists of overlapping translation rules. The algorithm recursively traverses the e-forest top–down, extracts rules only at admissible e-forest 256 Wang et al. Re-structuring, Re-labeling, and Re-aligning nodes, and transforms e-forest nodes into synchronous derivation-forest nodes via the following two procedures, depending on which condition is met. • Condition 1: If </context>
<context position="27712" citStr="Galley et al. (2006)" startWordPosition="4384" endWordPosition="4387">erivation-forest nodes, respectively. We form an additive derivation-forest node, and take these newly produced multiplicative derivation-forest nodes (by going to condition 2) as children. After this, we return the additive derivation-forest node. For instance, at node ⊕1(NPB) in Figure 7, for each of its children, e-forest nodes ⊗2(NPB) and ⊗11(NPB), we go to condition 2 to extract rules on it, to form multiplicative derivation forest nodes, ⊗(R16) and ⊗(R17) in Figure 8. • Condition 2: If we reach a multiplicative e-forest node, we extract a set of rules rooted at it using the procedure in Galley et al. (2006); and for each rule, we form a multiplicative derivation-forest node, and go to condition 1 to form the additive derivation-forest nodes for the additive frontier e-forest nodes of the newly extracted rule, and then make these additive derivation-forest nodes the children of the multiplicative derivation-forest node. After this, we return a set of multiplicative derivation-forest nodes, each corresponding to one rule extracted from the multiplicative e-forest node we just reached. Figure 8 A synchronous derivation forest built from a (e-forest, f, a) triple. The e-forest is shown in Figure 7. </context>
<context position="28948" citStr="Galley et al. (2006)" startWordPosition="4579" endWordPosition="4582">l Linguistics Volume 36, Number 2 For example, at node 011(NPB) in Figure 7, we extract a rule from it and form derivation-forest node ®(R17) in Figure 8. We then go to condition 1 to obtain, for each of the additive frontier e-forest nodes (in Figure 7) of this rule, a derivation-forest node, namely, ®(NNP), ®(NNP), and ®(NPB) in Figure 8. We make these derivation-forest ®-nodes the children of derivation-forest node ®(R17). This procedure transforms the packed e-forest in Figure 7 into a packed synchronous derivation in Figure 8. This algorithm is an extension of the extraction algorithm in Galley et al. (2006), in the sense that we have an extra condition (1) to relay rule extraction on additive e-forest nodes. The forest-based rule extraction algorithm produces much larger grammars than the tree-based one, making it difficult to scale to very large training data. From a 50M-word Chinese-to-English parallel corpus, we can extract more than 300 million translation rules, while the tree-based rule extraction algorithm gives approximately 100 million. However, the restructured trees from the simple binarization methods are not guaranteed to give the best trees for syntax-based machine translation. Wha</context>
<context position="33154" citStr="Galley et al. 2006" startWordPosition="5268" endWordPosition="5271">ule in a derivationforest to the total expected count of that rule is computed as β(parent(r)) x P(r) x � α(childi(r)) (4) i=1...n In the maximization step, we use the expected counts of rules, #r, to update the probabilities of the rules. P(r) = �#r (5) rule q:root(q)=root(r) #q Because it is well known that applying EM with tree fragments of different sizes causes overfitting (Johnson 1998a), and because it is also known that syntax MT models with larger composed rules in the mix significantly outperform rules that minimally explain the training data (minimal rules) in translation accuracy (Galley et al. 2006), we use minimal rules to construct the derivation forests from (e-binarization-forest, f, a)- tuples during running of the EM algorithm, but, after the EM re-structuring is finished, we build the final translation model using composed rules for MT evaluation. Figure 9 is the actual pipeline that we use for EM binarization. We first generate a packed e-forest via parallel binarization. We then extract minimal translation rules from Figure 9 Using the EM algorithm to choose re-structuring. 259 Computational Linguistics Volume 36, Number 2 the (e-forest, f, a)-tuples, producing synchronous deriv</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics (COLING) and 44th Annual Meeting of the Association for Computational Linguistics (ACL), pages 961–968, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>273--280</pages>
<location>Boston, MA.</location>
<contexts>
<context position="5544" citStr="Galley et al. (2004)" startWordPosition="818" endWordPosition="821">re 1 A sample learning case for the syntax-based machine translation system described in this article. 248 Wang et al. Re-structuring, Re-labeling, and Re-aligning SMT (Brown et al. 1993), our model operates in the English-to-Chinese direction— we envision a generative top–down process by which an English tree is gradually transformed (by probabilistic rules) into an observed Chinese string. We represent a collection of such rules as a tree transducer (Knight and Graehl 2005). In order to construct this transducer from parsed and word-aligned data, we use the GHKM rule extraction algorithm of Galley et al. (2004). This algorithm computes the unique set of minimal rules needed to explain any sentence pair in the data. Figure 2 shows all the minimal rules extracted from the example (tree, string, alignment) triple in Figure 1. Note that rules specify rotation (e.g., R1, R5), direct translation (R3, R10), insertion and deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example form a derivation tree. We collect all rules over the entire bilingual corpus, and we normalize rule counts in this way: P(rule) = count(rule) count(LHS-root(rule)). When we apply these probabilities to</context>
<context position="16474" citStr="Galley et al. 2004" startWordPosition="2561" endWordPosition="2564">also introduce a parallel re-structuring method in which we binarize both to the left and right at the same time, resulting in a binarization forest. We employ the EM algorithm (Dempster, Laird, and Rubin 1977) to learn the binarization bias for each tree node in the corpus from the parallel alternatives. 252 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 5 Learning translation rules from binarized English structures. 3.1 Some Concepts We now explain some concepts to facilitate the descriptions of the re-structuring methods. We train our translation model on alignment graphs (Galley et al. 2004). An alignment graph is a tuple of a source-language sentence f, a target-language parse tree that yields e and translates from f, and the word alignments a between e and f. The graphs in Figures 1, 4, and 5 are examples of alignment graphs. In the alignment graph, a node in the parse tree is called admissible if rules can be extracted from it. We can extract rules from a node if and only if the yield of the tree node is consistent with the word alignments—the f string covered by the node is contiguous but not empty, and the f string does not align to any e string that is not covered by the no</context>
<context position="30821" citStr="Galley et al. 2004" startWordPosition="4877" endWordPosition="4880">arization R results in a restructured tree -rp. We extract rules from (-rp, f, a), generating a translation model consisting of parameters (i.e., rule probabilities) 0. Our aim is to first obtain the rule probabilities that are the maximum likelihood estimate of the training tuples, and then produce the Viterbi binarization tree for each training tuple. The probability P(-rp, f, a) of a (-rp, f, a)-tuple is what the basic syntax-based translation model is concerned with. It can be further computed by aggregating the rule probabilities P(r) in each derivation w in the set of all derivations Ω (Galley et al. 2004). That is, �P(-rp, f, a) = 11 P(r) (1) w∈Ω r∈w The rule probabilities are estimated by the inside–outside algorithm (Lari and Young 1990; Knight, Graehl, and May 2008), which needs to run on derivation forests. Our previous sections have already presented algorithms to transform a parse tree into a binarization forest, and then transform the (e-forest, f, a)-tuples into derivation forests (e.g., Figure 8), on which the inside–outside algorithm can then be applied. In the derivation forests, an additive node labeled as A dominates several multiplicative nodes, each corresponding to a translatio</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Galley, Michel, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of the Human Language Technology Conference and the North American Association for Computational Linguistics (HLT-NAACL), pages 273–280, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irving J Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrika,</journal>
<volume>40</volume>
<issue>3</issue>
<contexts>
<context position="6330" citStr="Good 1953" startWordPosition="946" endWordPosition="947">, string, alignment) triple in Figure 1. Note that rules specify rotation (e.g., R1, R5), direct translation (R3, R10), insertion and deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example form a derivation tree. We collect all rules over the entire bilingual corpus, and we normalize rule counts in this way: P(rule) = count(rule) count(LHS-root(rule)). When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up computing the joint probability P(e, c). We smooth the rule counts with Good–Turing smoothing (Good 1953). This extraction method assigns each unaligned Chinese word to a default rule in the derivation tree. We next follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules. In this case, we obtain a derivation forest of minimal rules. Galley et al. show how to use EM to count rules over derivation forests and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et a</context>
<context position="8616" citStr="Good 1953" startWordPosition="1321" endWordPosition="1322">nt search can be gained if this binarization produces rules that can be incrementally scored by the language model (Melamed, Satta, and Wellington 2004; Zhang et al. 2006). Finally, we employ cube pruning (Chiang 2007) for further efficiency in the search. When scoring translation candidates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The final score of a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain weights through minimum error-rate training (Och 2003). The system thus constructed performs fairly well at Chinese-to-English translation, as reflected in the NIST06 common evaluation of machine translation quality.1 However, it would be surprising if the parse structures and word alignments in our bilingual data were somehow perfectly suited to syntax-based SMT—we</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Good, Irving J. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40(3):237–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Semiring parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="20924" citStr="Goodman 1999" startWordPosition="3325" endWordPosition="3326"> substructure that yields NNP2, NNP3, and their translational equivalences. To obtain more factorizable sub-phrases, we need to parallel-binarize in both directions. 254 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 7 Packed forest obtained by packing trees (3) and (6) in Figure 6. 3.2.2 Parallel Binarization. Simple binarizations transform a parse tree into another single parse tree. Parallel binarization transforms a parse tree into a binarization forest, packed to enable dynamic programming when we extract translation rules from it. Borrowing terms from parsing semirings (Goodman 1999), a packed forest is composed of additive forest nodes (®-nodes) and multiplicative forest nodes (0-nodes). In the binarization forest, a ®-node corresponds to a tree node in the unbinarized tree or a new tree node introduced during tree binarization; and this ®-node composes several ®-nodes, forming a one-level substructure that is observed in the unbinarized tree or in one of its binarized tree. A ®-node corresponds to alternative ways of binarizing the same tree node and it contains one or more ®-nodes. The same ®-node can appear in more than one place in the packed forest, enabling sharing</context>
</contexts>
<marker>Goodman, 1999</marker>
<rawString>Goodman, Joshua. 1999. Semiring parsing. Computational Linguistics, 25(4):573–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryant Huang</author>
<author>Kevin Knight</author>
</authors>
<title>Relabeling syntax trees to improve syntax-based machine translation accuracy.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (NAACL-HLT),</booktitle>
<pages>240--247</pages>
<location>New York, NY.</location>
<contexts>
<context position="9326" citStr="Huang and Knight (2006)" startWordPosition="1427" endWordPosition="1430"> translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain weights through minimum error-rate training (Och 2003). The system thus constructed performs fairly well at Chinese-to-English translation, as reflected in the NIST06 common evaluation of machine translation quality.1 However, it would be surprising if the parse structures and word alignments in our bilingual data were somehow perfectly suited to syntax-based SMT—we have so far used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and Knight (2006) already investigated whether different syntactic labels would be more appropriate for SMT, though their study was carried out on a weak baseline translation system. In this article, we take a broad view and investigate how changes to syntactic structures, syntactic labels, and word alignments can lead to substantial improvements in translation quality on top of a strong baseline. We design our methods around problems that arise in MT data whose parses and alignments use some Penn Treebankstyle annotations. We believe that some of the techniques will apply to other annotation schemes, but conc</context>
</contexts>
<marker>Huang, Knight, 2006</marker>
<rawString>Huang, Bryant and Kevin Knight. 2006. Relabeling syntax trees to improve syntax-based machine translation accuracy. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (NAACL-HLT), pages 240–247, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>The DOP estimation method is biased and inconsistent.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="13353" citStr="Johnson (1998" startWordPosition="2066" endWordPosition="2067">e problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce flat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are flattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We find that flat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment triples often lack sufficient generalization power. For example, consider the training samples in Figure 4. We should be able to learn enough from these two samples to translate the new phrase IA4jG-�/TliAN/Tl� fn t0` M. VIKTOR CHERNOMYRDIN </context>
<context position="32929" citStr="Johnson 1998" startWordPosition="5235" endWordPosition="5236">led as B as follows. � � α(childi(r)) (2) α(A) = P(r) x rE{child(A)} i=1...n β(B) = � P(r) x β(parent(r)) x � α(C) (3) r:BE{child(r)} CE{child(r)}−{B} In the expectation step, the contribution of each occurrence of a rule in a derivationforest to the total expected count of that rule is computed as β(parent(r)) x P(r) x � α(childi(r)) (4) i=1...n In the maximization step, we use the expected counts of rules, #r, to update the probabilities of the rules. P(r) = �#r (5) rule q:root(q)=root(r) #q Because it is well known that applying EM with tree fragments of different sizes causes overfitting (Johnson 1998a), and because it is also known that syntax MT models with larger composed rules in the mix significantly outperform rules that minimally explain the training data (minimal rules) in translation accuracy (Galley et al. 2006), we use minimal rules to construct the derivation forests from (e-binarization-forest, f, a)- tuples during running of the EM algorithm, but, after the EM re-structuring is finished, we build the final translation model using composed rules for MT evaluation. Figure 9 is the actual pipeline that we use for EM binarization. We first generate a packed e-forest via parallel </context>
<context position="38180" citStr="Johnson (1998" startWordPosition="6036" endWordPosition="6037"> introduce more admissible nodes because it mixes both left and right binarizations in the same tree. We computed the binarization biases learned by the EM algorithm for each nonterminal from the binarization forest of parallel head binarizations of the training trees (Table 3). Of course, the binarization bias chosen by left-/right-binarization methods would be 100% deterministic. One noticeable message from Table 3 is that most of the categories are actually biased toward left-binarization. The reason might be that the head sub-constituents of most English categories tend to be on the left. Johnson (1998b) argues that the more nodes there are in a treebank, the stronger the independence assumptions implicit in the PCFG model are, and the less accurate the estimated PCFG will usually be—more nodes break more local dependencies. Our experiments, on the other hand, show MT accuracy improvement by introducing more admissible nodes. This initial contradiction actually makes sense. The key is that we use composed rules to build our final MT system and that we introduce the “ignoring-Xnode” trick to preserve the local dependencies. More nodes in training trees weaken the accuracy of a translation mo</context>
<context position="41271" citStr="Johnson 1998" startWordPosition="6532" endWordPosition="6533">grammatical translations like he likes reading she does not like reading. Tree binarization enables the reuse of substructures, but causes over-generation of trees at the same time. We solve the coarse-nonterminal problem by refining/re-labeling the training tree labels. Re-labeling is done by enriching the nonterminal label of each tree node based on its context information. Re-labeling has already been used in monolingual parsing research to improve parsing accuracy of PCFGs. We are interested in two types of re-labeling methods: Linguistically motivated re-labeling (Klein and Manning 2003; Johnson 1998b) enriches the labels of parser training trees using parent labels, head word tag labels, and/or sibling labels. Automatic category splitting (Petrov et al. 2006) refines a nonterminal Figure 10 MT output errors due to coarse Penn Treebank annotations. Oval nodes in (b) are rule overlapping nodes. Subtree (b) is formed by composing the LHSs of R20, R21, and R22. 262 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 11 Tree binarization over-generalizes the parse tree. Translation rules R23 and R24 are acquired from binarized training trees, aiming for reuse of substructures. Com</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Johnson, Mark. 1998a. The DOP estimation method is biased and inconsistent. Computational Linguistics, 28(1):71–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="13353" citStr="Johnson (1998" startWordPosition="2066" endWordPosition="2067">e problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce flat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are flattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We find that flat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment triples often lack sufficient generalization power. For example, consider the training samples in Figure 4. We should be able to learn enough from these two samples to translate the new phrase IA4jG-�/TliAN/Tl� fn t0` M. VIKTOR CHERNOMYRDIN </context>
<context position="32929" citStr="Johnson 1998" startWordPosition="5235" endWordPosition="5236">led as B as follows. � � α(childi(r)) (2) α(A) = P(r) x rE{child(A)} i=1...n β(B) = � P(r) x β(parent(r)) x � α(C) (3) r:BE{child(r)} CE{child(r)}−{B} In the expectation step, the contribution of each occurrence of a rule in a derivationforest to the total expected count of that rule is computed as β(parent(r)) x P(r) x � α(childi(r)) (4) i=1...n In the maximization step, we use the expected counts of rules, #r, to update the probabilities of the rules. P(r) = �#r (5) rule q:root(q)=root(r) #q Because it is well known that applying EM with tree fragments of different sizes causes overfitting (Johnson 1998a), and because it is also known that syntax MT models with larger composed rules in the mix significantly outperform rules that minimally explain the training data (minimal rules) in translation accuracy (Galley et al. 2006), we use minimal rules to construct the derivation forests from (e-binarization-forest, f, a)- tuples during running of the EM algorithm, but, after the EM re-structuring is finished, we build the final translation model using composed rules for MT evaluation. Figure 9 is the actual pipeline that we use for EM binarization. We first generate a packed e-forest via parallel </context>
<context position="38180" citStr="Johnson (1998" startWordPosition="6036" endWordPosition="6037"> introduce more admissible nodes because it mixes both left and right binarizations in the same tree. We computed the binarization biases learned by the EM algorithm for each nonterminal from the binarization forest of parallel head binarizations of the training trees (Table 3). Of course, the binarization bias chosen by left-/right-binarization methods would be 100% deterministic. One noticeable message from Table 3 is that most of the categories are actually biased toward left-binarization. The reason might be that the head sub-constituents of most English categories tend to be on the left. Johnson (1998b) argues that the more nodes there are in a treebank, the stronger the independence assumptions implicit in the PCFG model are, and the less accurate the estimated PCFG will usually be—more nodes break more local dependencies. Our experiments, on the other hand, show MT accuracy improvement by introducing more admissible nodes. This initial contradiction actually makes sense. The key is that we use composed rules to build our final MT system and that we introduce the “ignoring-Xnode” trick to preserve the local dependencies. More nodes in training trees weaken the accuracy of a translation mo</context>
<context position="41271" citStr="Johnson 1998" startWordPosition="6532" endWordPosition="6533">grammatical translations like he likes reading she does not like reading. Tree binarization enables the reuse of substructures, but causes over-generation of trees at the same time. We solve the coarse-nonterminal problem by refining/re-labeling the training tree labels. Re-labeling is done by enriching the nonterminal label of each tree node based on its context information. Re-labeling has already been used in monolingual parsing research to improve parsing accuracy of PCFGs. We are interested in two types of re-labeling methods: Linguistically motivated re-labeling (Klein and Manning 2003; Johnson 1998b) enriches the labels of parser training trees using parent labels, head word tag labels, and/or sibling labels. Automatic category splitting (Petrov et al. 2006) refines a nonterminal Figure 10 MT output errors due to coarse Penn Treebank annotations. Oval nodes in (b) are rule overlapping nodes. Subtree (b) is formed by composing the LHSs of R20, R21, and R22. 262 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 11 Tree binarization over-generalizes the parse tree. Translation rules R23 and R24 are acquired from binarized training trees, aiming for reuse of substructures. Com</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Johnson, Mark. 1998b. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Computational Linguistics Volume 36, Number 2 Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>423--430</pages>
<location>Sapporo.</location>
<contexts>
<context position="41257" citStr="Klein and Manning 2003" startWordPosition="6528" endWordPosition="6531">n Figure 11(b) yields ungrammatical translations like he likes reading she does not like reading. Tree binarization enables the reuse of substructures, but causes over-generation of trees at the same time. We solve the coarse-nonterminal problem by refining/re-labeling the training tree labels. Re-labeling is done by enriching the nonterminal label of each tree node based on its context information. Re-labeling has already been used in monolingual parsing research to improve parsing accuracy of PCFGs. We are interested in two types of re-labeling methods: Linguistically motivated re-labeling (Klein and Manning 2003; Johnson 1998b) enriches the labels of parser training trees using parent labels, head word tag labels, and/or sibling labels. Automatic category splitting (Petrov et al. 2006) refines a nonterminal Figure 10 MT output errors due to coarse Penn Treebank annotations. Oval nodes in (b) are rule overlapping nodes. Subtree (b) is formed by composing the LHSs of R20, R21, and R22. 262 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 11 Tree binarization over-generalizes the parse tree. Translation rules R23 and R24 are acquired from binarized training trees, aiming for reuse of subs</context>
<context position="43549" citStr="Klein and Manning (2003)" startWordPosition="6896" endWordPosition="6899">de with the combination of IN and its parent node label. IN is frequently overloaded in the Penn Treebank. For instance, its parent can be PP or SBAR. These two operations re-label the tree in Figure 12(a1) to Figure 12(b1). Example rules extracted from these two trees are shown in Figure 12(a2) and Figure 12(b2), respectively. We apply this re-labeling on the MT training tree nodes, and then acquire rules from these re-labeled trees. We chose to split only these two categories because our syntax MT system tends to frequently make parse errors in these two categories, and because, as shown by Klein and Manning (2003), further refining the VP and IN categories is very effective in improving monolingual parsing accuracy. This type of re-labeling fixes the parse error in Figure 10. SPLIT-VP transforms the R18 root to VPF, and the R17 frontier node to VP.VBN. Thus R17 and R18 can never be composed, preventing the wrong tree being formed by the translation model. 4.2 Statistical Re-labeling Our second re-labeling approach is to learn the split categories for the node labels of the training trees via the EM algorithm, as in Petrov et al. (2006). Rather than using 263 Computational Linguistics Volume 36, Number </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, Dan and Chris Manning. 2003. Accurate unlexicalized parsing. In Computational Linguistics Volume 36, Number 2 Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 423–430, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</booktitle>
<pages>181--184</pages>
<location>Detroit, MI.</location>
<contexts>
<context position="4531" citStr="Kneser and Ney 1995" startWordPosition="665" endWordPosition="668">e sentences on the English side, whereas the corresponding Chinese segments may be sentences, sentence fragments, or multiple sentences. We then parse the English side of the bilingual text using a re-implementation of the Collins (1997) parsing model, which we train on the Penn English Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Finally, we word-align the segment pairs according to IBM Model 4 (Brown et al. 1993). Figure 1 shows a sample (tree, string, alignment) triple. We build two generative statistical models from this data. First, we construct a smoothed n-gram language model (Kneser and Ney 1995; Stolcke 2002) out of the English side of the bilingual data. This model assigns a probability P(e) to any candidate translation, rewarding translations whose subsequences have been observed frequently in the training data. Second, we build a syntax-based translation model that we can use to produce candidate English trees from Chinese strings. Following previous work in noisy-channel Figure 1 A sample learning case for the syntax-based machine translation system described in this article. 248 Wang et al. Re-structuring, Re-labeling, and Re-aligning SMT (Brown et al. 1993), our model operates</context>
<context position="11965" citStr="Kneser and Ney 1995" startWordPosition="1852" endWordPosition="1855">pment set consists of 1,453 lines and is extracted from the NIST02– NIST05 evaluation sets, for tuning of feature weights. The development set is from the newswire domain, and we chose it to represent a wide period of time rather than a single year. We use the NIST08 evaluation set as our test set. Because the NIST08 evaluation set is a mix of newswire text and Web text, we also report the BLEU scores on the newswire portion. We use two 5-gram language models. One is trained on the English half of the bitext. The other is trained on one billion words of monolingual data. Kneser–Ney smoothing (Kneser and Ney 1995) is applied to both language models. Language models are represented using randomized data structures similar to those of Talbot and Osborne (2007) in decoding for efficient RAM usage. To test the significance of improvements over the baseline, we compute paired bootstrap p-values (Koehn 2004) for BLEU between the baseline system and each improved system. 3. Re-structuring Trees for Training Our translation system is trained on Chinese/English data, where the English side has been automatically parsed into Penn Treebank-style trees. One striking fact about these trees is that they contain many</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Kneser, Reinhard and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing (ICASSP) 1995, pages 181–184, Detroit, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<title>An overview of probabilistic tree transducers for natural language processing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing),</booktitle>
<pages>1--25</pages>
<location>Mexico City.</location>
<contexts>
<context position="5404" citStr="Knight and Graehl 2005" startWordPosition="795" endWordPosition="798">-based translation model that we can use to produce candidate English trees from Chinese strings. Following previous work in noisy-channel Figure 1 A sample learning case for the syntax-based machine translation system described in this article. 248 Wang et al. Re-structuring, Re-labeling, and Re-aligning SMT (Brown et al. 1993), our model operates in the English-to-Chinese direction— we envision a generative top–down process by which an English tree is gradually transformed (by probabilistic rules) into an observed Chinese string. We represent a collection of such rules as a tree transducer (Knight and Graehl 2005). In order to construct this transducer from parsed and word-aligned data, we use the GHKM rule extraction algorithm of Galley et al. (2004). This algorithm computes the unique set of minimal rules needed to explain any sentence pair in the data. Figure 2 shows all the minimal rules extracted from the example (tree, string, alignment) triple in Figure 1. Note that rules specify rotation (e.g., R1, R5), direct translation (R3, R10), insertion and deletion (R2, R4), and tree traversal (R9, R7). The extracted rules for a given example form a derivation tree. We collect all rules over the entire b</context>
</contexts>
<marker>Knight, Graehl, 2005</marker>
<rawString>Knight, Kevin and Jonathan Graehl. 2005. An overview of probabilistic tree transducers for natural language processing. In Proceedings of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing), pages 1–25, Mexico City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
<author>Jonathan May</author>
</authors>
<title>Training tree transducers.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<marker>Knight, Graehl, May, 2008</marker>
<rawString>Knight, Kevin, Jonathan Graehl, and Jonathan May. 2008. Training tree transducers. Computational Linguistics, 34(3):391–427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>388--395</pages>
<location>Barcelona.</location>
<contexts>
<context position="12259" citStr="Koehn 2004" startWordPosition="1899" endWordPosition="1900">ause the NIST08 evaluation set is a mix of newswire text and Web text, we also report the BLEU scores on the newswire portion. We use two 5-gram language models. One is trained on the English half of the bitext. The other is trained on one billion words of monolingual data. Kneser–Ney smoothing (Kneser and Ney 1995) is applied to both language models. Language models are represented using randomized data structures similar to those of Talbot and Osborne (2007) in decoding for efficient RAM usage. To test the significance of improvements over the baseline, we compute paired bootstrap p-values (Koehn 2004) for BLEU between the baseline system and each improved system. 3. Re-structuring Trees for Training Our translation system is trained on Chinese/English data, where the English side has been automatically parsed into Penn Treebank-style trees. One striking fact about these trees is that they contain many flat structures. For example, base noun phrases frequently have five or more direct children. It is well known in monolingual parsing research that these flat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover t</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Koehn, Philipp. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 388–395, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karim Lari</author>
<author>Steve Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="30957" citStr="Lari and Young 1990" startWordPosition="4901" endWordPosition="4904">rs (i.e., rule probabilities) 0. Our aim is to first obtain the rule probabilities that are the maximum likelihood estimate of the training tuples, and then produce the Viterbi binarization tree for each training tuple. The probability P(-rp, f, a) of a (-rp, f, a)-tuple is what the basic syntax-based translation model is concerned with. It can be further computed by aggregating the rule probabilities P(r) in each derivation w in the set of all derivations Ω (Galley et al. 2004). That is, �P(-rp, f, a) = 11 P(r) (1) w∈Ω r∈w The rule probabilities are estimated by the inside–outside algorithm (Lari and Young 1990; Knight, Graehl, and May 2008), which needs to run on derivation forests. Our previous sections have already presented algorithms to transform a parse tree into a binarization forest, and then transform the (e-forest, f, a)-tuples into derivation forests (e.g., Figure 8), on which the inside–outside algorithm can then be applied. In the derivation forests, an additive node labeled as A dominates several multiplicative nodes, each corresponding to a translation rule resulting from either left binarization or right binarization of the original structure. We use rule r to either refer to a rule </context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>Lari, Karim and Steve Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>133--139</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="62280" citStr="Marcu and Wong (2002)" startWordPosition="9899" endWordPosition="9902">again never choose R2–R3. The rules in the second derivation themselves are not rarely seen—R2 is in 13,311 forests other than those where R33 is seen, and R3 is in 2,500 additional forests. EM gives R2 a probability of e−7.72—better than 98.7% of rules, and R3 a probability of e−2.96. But R33 receives a probability of e−6.32 and is preferred over the R2–R3 derivation, which has a combined probability of e−10.68. The preference for shorter derivations containing large rules over longer derivations containing small rules is due to a general tendency for EM to prefer derivations with few atoms. Marcu and Wong (2002) note this preference but consider the phenomenon a feature, rather than a bug. Zollmann and Sima’an (2005) combat the overfitting aspect for parsing by using a held-out corpus and a straight maximum likelihood estimate, rather than EM. DeNero, Bouchard-Cˆot´e, and Klein (2008) encourage small rules with a modeling approach; they put a Dirichlet process prior of rule size over their model and learn the parameters of the geometric distribution of that prior with Gibbs sampling. We use a simpler modeling approach to accomplish the same goals as DeNero, BouchardCˆot´e, and Klein which, although l</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Marcu, Daniel and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 133–139, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>Syntactic re-alignment models for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP–CoNLL),</booktitle>
<pages>360--368</pages>
<location>Prague.</location>
<contexts>
<context position="66332" citStr="May and Knight (2007)" startWordPosition="10570" endWordPosition="10573">lignments, obtained by running GIZA, as described in Section 2. We compared this baseline to an MT system that used alignments obtained by re-aligning the GIZA alignments using the method of Section 5.2 with the 36 million word subset of the training corpus used for re-alignment learning. We next compared the baseline to an MT system that used re-alignments obtained by also incorporating the size prior described in Section 5.4. As can be seen by the results in Table 6, the size prior method is needed to obtain reasonable improvement in BLEU. These results are consistent with those reported in May and Knight (2007), where gains in Chinese and Arabic MT systems were observed, though over a weaker baseline and with less training data than is used in this work. 6. Combining Techniques We have thus far seen gains in BLEU score by independent improvements in training data tree structure, syntax labeling, and alignment. This naturally raises the question of whether the techniques can be combined, that is, if improvement in one aspect of training data aids in improvement of another. As reported in Section 4.3 and Table 5, we were able to improve re-labeling efforts and take advantage of the split-and-merge tec</context>
</contexts>
<marker>May, Knight, 2007</marker>
<rawString>May, Jonathan and Kevin Knight. 2007. Syntactic re-alignment models for machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP–CoNLL), pages 360–368, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Giorgio Satta</author>
<author>Benjamin Wellington</author>
</authors>
<title>Generalized multitext grammars.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>662--669</pages>
<location>Barcelona.</location>
<marker>Melamed, Satta, Wellington, 2004</marker>
<rawString>Melamed, I. Dan, Giorgio Satta, and Benjamin Wellington. 2004. Generalized multitext grammars. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 662–669, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haitao Mi</author>
<author>Liang Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>206--214</pages>
<location>Honolulu, HI.</location>
<contexts>
<context position="26366" citStr="Mi and Huang (2008)" startWordPosition="4178" endWordPosition="4181">t rules from the admissible nodes in the packed forest. Rules that can be extracted from the original unrestructured tree can be extracted from the packed forest as well. Parallel binarization results in parse forests. Thus translation rules need to be extracted from training data consisting of (e-forest, f, a)-tuples. 3.3 Extracting Translation Rules from (e-forest, f, a)-tuples Our algorithm to extract rules from (e-forest, f, a)-tuples is a natural generalization of the (e-parse, f, a)-based rule extraction algorithm in Galley et al. (2006). A similar problem is also elegantly addressed in Mi and Huang (2008) in detail. The forest-based rule extraction algorithm takes as input a (e-forest, f, a)-triple, and outputs a derivation forest (Galley et al. 2006), which consists of overlapping translation rules. The algorithm recursively traverses the e-forest top–down, extracts rules only at admissible e-forest 256 Wang et al. Re-structuring, Re-labeling, and Re-aligning nodes, and transforms e-forest nodes into synchronous derivation-forest nodes via the following two procedures, depending on which condition is met. • Condition 1: If we reach an additive e-forest node, for each of its children, which ar</context>
</contexts>
<marker>Mi, Huang, 2008</marker>
<rawString>Mi, Haitao and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 206–214, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<marker>Och, Ney, 2004</marker>
<rawString>Och, Franz and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo.</location>
<contexts>
<context position="8902" citStr="Och 2003" startWordPosition="1367" endWordPosition="1368">dates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The final score of a translation candidate is a weighted linear combination of log P(e), log P(e, c), and the scores from these additional smaller models. We obtain weights through minimum error-rate training (Och 2003). The system thus constructed performs fairly well at Chinese-to-English translation, as reflected in the NIST06 common evaluation of machine translation quality.1 However, it would be surprising if the parse structures and word alignments in our bilingual data were somehow perfectly suited to syntax-based SMT—we have so far used out-of-the-box tools like IBM Model 4 and a Treebank-trained parser. Huang and Knight (2006) already investigated whether different syntactic labels would be more appropriate for SMT, though their study was carried out on a weak baseline translation system. In this ar</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, Franz Josef. 2003. Minimum error rate training for machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics (COLING) and 44th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>433--440</pages>
<location>Sydney.</location>
<contexts>
<context position="13250" citStr="Petrov et al. 2006" startWordPosition="2049" endWordPosition="2052">ive or more direct children. It is well known in monolingual parsing research that these flat structures cause problems. Although thousands of rewrite rules can be learned from the Penn Treebank, these rules still do not cover the new rewrites observed in held-out test data. For this reason, and to extract more general knowledge, many monolingual parsing models are markovized so that they can produce flat structures incrementally and horizontally (Collins 1997; Charniak 2000). Other parsing systems binarize the training trees in a pre-processing step, then learn to model the binarized corpus (Petrov et al. 2006); after parsing, their results are flattened back in a post-processing step. In addition, Johnson (1998b) shows that different types of tree structuring (e.g., the Chomsky adjunction representation vs. the Penn Treebank II representation) can have a large effect on the parsing performance of a PCFG estimated from these trees. We find that flat structures are also problematic for syntax-based machine translation. The rules we learn from tree/string/alignment triples often lack sufficient generalization power. For example, consider the training samples in Figure 4. We should be able to learn eno</context>
<context position="41434" citStr="Petrov et al. 2006" startWordPosition="6554" endWordPosition="6557">f trees at the same time. We solve the coarse-nonterminal problem by refining/re-labeling the training tree labels. Re-labeling is done by enriching the nonterminal label of each tree node based on its context information. Re-labeling has already been used in monolingual parsing research to improve parsing accuracy of PCFGs. We are interested in two types of re-labeling methods: Linguistically motivated re-labeling (Klein and Manning 2003; Johnson 1998b) enriches the labels of parser training trees using parent labels, head word tag labels, and/or sibling labels. Automatic category splitting (Petrov et al. 2006) refines a nonterminal Figure 10 MT output errors due to coarse Penn Treebank annotations. Oval nodes in (b) are rule overlapping nodes. Subtree (b) is formed by composing the LHSs of R20, R21, and R22. 262 Wang et al. Re-structuring, Re-labeling, and Re-aligning Figure 11 Tree binarization over-generalizes the parse tree. Translation rules R23 and R24 are acquired from binarized training trees, aiming for reuse of substructures. Composing R23 and R24, however, results in an ill-formed tree. The new nonterminal S introduced in tree binarization needs to be refined into different sub-categories</context>
<context position="44081" citStr="Petrov et al. (2006)" startWordPosition="6985" endWordPosition="6988"> parse errors in these two categories, and because, as shown by Klein and Manning (2003), further refining the VP and IN categories is very effective in improving monolingual parsing accuracy. This type of re-labeling fixes the parse error in Figure 10. SPLIT-VP transforms the R18 root to VPF, and the R17 frontier node to VP.VBN. Thus R17 and R18 can never be composed, preventing the wrong tree being formed by the translation model. 4.2 Statistical Re-labeling Our second re-labeling approach is to learn the split categories for the node labels of the training trees via the EM algorithm, as in Petrov et al. (2006). Rather than using 263 Computational Linguistics Volume 36, Number 2 Figure 12 Re-labeling of parse trees. 264 Wang et al. Re-structuring, Re-labeling, and Re-aligning their parser to directly produce category-split parse trees for the MT training data, we separate the parsing step and the re-labeling step. The re-labeling method is as follows. 1. Run a parser to produce the MT training trees. 2. Binarize the MT training trees via the EM binarization algorithm. 3. Learn an n-way split PCFG from the binarized trees via the algorithm described in Petrov et al. (2006). 4. Produce the Viterbi spl</context>
<context position="45946" citStr="Petrov et al. (2006)" startWordPosition="7282" endWordPosition="7285"> split rules. This relabeling procedure tries to achieve further improvement by trying to fix the tree overgeneralization problem of re-structuring while preserving the gain we have already obtained from tree re-structuring. Figure 12(c1) shows a category-split tree, and Figure 12(c2) shows the minimal xRs rules extracted from the split tree. In Figure 12(c2), the two VPs (VP-0 and VP-2) now belong to two different categories and cannot be used in the same context. In this re-labeling procedure, we separate the re-labeling step from the parsing step, rather than using a parser like the one in Petrov et al. (2006) to directly produce categorysplit parse trees on the English corpus. We think that we benefit from this separation in the following ways: First, this gives us the freedom to choose the parser to produce the initial trees. Second, this enables us to train the re-labeler on the domains where the MT system is trained, instead of on the Penn Treebank. Third, this enables us to choose our own tree binarization methods. Tree re-labeling fragments the translation rules. Each refined rule now fits in fewer contexts than its corresponding coarse rule. Re-labeling, however, does not explode the grammar</context>
<context position="66962" citStr="Petrov et al. (2006)" startWordPosition="10674" endWordPosition="10677">ins in Chinese and Arabic MT systems were observed, though over a weaker baseline and with less training data than is used in this work. 6. Combining Techniques We have thus far seen gains in BLEU score by independent improvements in training data tree structure, syntax labeling, and alignment. This naturally raises the question of whether the techniques can be combined, that is, if improvement in one aspect of training data aids in improvement of another. As reported in Section 4.3 and Table 5, we were able to improve re-labeling efforts and take advantage of the split-and-merge technique of Petrov et al. (2006) by first re-structuring via the method described in Section 3.4. It is unlikely that such re-structuring or re-labeling would aid in a subsequent re-alignment procedure like that of Section 5.2, for re-structuring changes trees based on a given alignment, and re-alignment can only change links when multiple instances of a (subtree, substring) tuple are found in the data with different partial alignments. Re-structuring beforehand changes the trees over different alignments differently. It is unlikely that many (subtree, substring) tuples with more than one partial alignment would remain after</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics (COLING) and 44th Annual Meeting of the Association for Computational Linguistics (ACL), pages 433–440, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP)</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="4546" citStr="Stolcke 2002" startWordPosition="669" endWordPosition="670">glish side, whereas the corresponding Chinese segments may be sentences, sentence fragments, or multiple sentences. We then parse the English side of the bilingual text using a re-implementation of the Collins (1997) parsing model, which we train on the Penn English Treebank (Marcus, Santorini, and Marcinkiewicz 1993). Finally, we word-align the segment pairs according to IBM Model 4 (Brown et al. 1993). Figure 1 shows a sample (tree, string, alignment) triple. We build two generative statistical models from this data. First, we construct a smoothed n-gram language model (Kneser and Ney 1995; Stolcke 2002) out of the English side of the bilingual data. This model assigns a probability P(e) to any candidate translation, rewarding translations whose subsequences have been observed frequently in the training data. Second, we build a syntax-based translation model that we can use to produce candidate English trees from Chinese strings. Following previous work in noisy-channel Figure 1 A sample learning case for the syntax-based machine translation system described in this article. 248 Wang et al. Re-structuring, Re-labeling, and Re-aligning SMT (Brown et al. 1993), our model operates in the English</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, Andreas. 2002. SRILM—an extensible language modeling toolkit. In Proceedings of the 7th International Conference on Spoken Language Processing (ICSLP) 2002, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL),</booktitle>
<pages>512--519</pages>
<location>Prague.</location>
<contexts>
<context position="12112" citStr="Talbot and Osborne (2007)" startWordPosition="1874" endWordPosition="1877"> is from the newswire domain, and we chose it to represent a wide period of time rather than a single year. We use the NIST08 evaluation set as our test set. Because the NIST08 evaluation set is a mix of newswire text and Web text, we also report the BLEU scores on the newswire portion. We use two 5-gram language models. One is trained on the English half of the bitext. The other is trained on one billion words of monolingual data. Kneser–Ney smoothing (Kneser and Ney 1995) is applied to both language models. Language models are represented using randomized data structures similar to those of Talbot and Osborne (2007) in decoding for efficient RAM usage. To test the significance of improvements over the baseline, we compute paired bootstrap p-values (Koehn 2004) for BLEU between the baseline system and each improved system. 3. Re-structuring Trees for Training Our translation system is trained on Chinese/English data, where the English side has been automatically parsed into Penn Treebank-style trees. One striking fact about these trees is that they contain many flat structures. For example, base noun phrases frequently have five or more direct children. It is well known in monolingual parsing research tha</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>Talbot, David and Miles Osborne. 2007. Randomised language modelling for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL), pages 512–519, Prague.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph</author>
</authors>
<title>Tillmann.1996. HMM-based word alignment in statistical translation.</title>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING)1996,</booktitle>
<pages>836--841</pages>
<location>Copenhagen.</location>
<marker>Vogel, Ney, Christoph, </marker>
<rawString>Vogel, Stephan, Hermann Ney, and Christoph Tillmann.1996. HMM-based word alignment in statistical translation. In Proceedings of the International Conference on Computational Linguistics (COLING)1996, pages 836–841, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Binarizing syntax trees to improve syntax-based machine translation accuracy.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP–CoNLL),</booktitle>
<pages>746--754</pages>
<location>Prague.</location>
<marker>Wang, Knight, Marcu, 2007</marker>
<rawString>Wang, Wei, Kevin Knight, and Daniel Marcu. 2007. Binarizing syntax trees to improve syntax-based machine translation accuracy. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP–CoNLL), pages 746–754, Prague.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dekai 1997 Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<marker>Wu, </marker>
<rawString>Wu, Dekai.1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>523--530</pages>
<location>Toulouse.</location>
<contexts>
<context position="49525" citStr="Yamada and Knight (2001" startWordPosition="7828" endWordPosition="7831">uracy more than re-labeling. We mentioned earlier that re-structuring overgeneralizes structures, but enables reuse of substructures. Results in Table 5 and Table 1 show substructure reuse mitigates structure over-generalization in our tree restructuring method. 5. Re-aligning (Tree, String) Pairs for Training So far, we have improved the English structures in our parsed, aligned training corpus. We now turn to improving the word alignments. Some MT systems use the same model for alignment and translation—examples include Brown et al. (1993), Wu (1997), Alshawi, Bangalore, and Douglas (1998), Yamada and Knight (2001, 2002), and Cohn and Blunsom (2009). Other systems use Brown et al. for alignment, then collect counts for a completely different model, such as Och and Ney Table 5 Impact of re-labeling methods on MT accuracy as measured by BLEU. Four-way splitting was carried out on EM-binarized trees; thus, it already benefits from tree re-structuring. All p-values are computed against Baseline1. EXPERIMENT NIST08 NIST08-NW BLEU p BLEU p Baseline1 (no re-structuring and no re-labeling) 29.12 — 35.33 — Linguistically motivated re-labeling 29.57 0.029 35.85 0.050 Baseline2 (EM re-structuring but no re-labeli</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Yamada, Kenji and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL), pages 523–530, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A decoder for syntax-based statistical MT.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>303--310</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="7305" citStr="Yamada and Knight 2002" startWordPosition="1107" endWordPosition="1110">s and obtain Viterbi derivation trees of minimal rules. We also follow Galley et al. in collecting composed rules, namely, compositions of minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al. 2006; DeNeefe et al. 2007). Figure 3 shows some of the additional rules. With these models, we can decode a new Chinese sentence by enumerating and scoring all of the English trees that can be derived from it by rule. The score is a weighted product of P(e) and P(e, c). To search efficiently, we employ the CKY dynamicprogramming parsing algorithm (Yamada and Knight 2002; Galley et al. 2006). This algorithm builds English trees on top of Chinese spans. In each cell of the CKY matrix, we store the non-terminal symbol at the root of the English tree being built up. We also Figure 2 Minimal rules extracted from the learning case in Figure 1 using the GHKM procedure. 249 Computational Linguistics Volume 36, Number 2 Figure 3 Additional rules extracted from the learning case in Figure 1. store English words that appear at the left and right corners of the tree, as these are needed for computing the P(e) score when cells are combined. For CKY to work, all transduce</context>
</contexts>
<marker>Yamada, Knight, 2002</marker>
<rawString>Yamada, Kenji and Kevin Knight. 2002. A decoder for syntax-based statistical MT. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 303–310, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL),</booktitle>
<pages>256--263</pages>
<location>New York, NY.</location>
<contexts>
<context position="8177" citStr="Zhang et al. 2006" startWordPosition="1256" endWordPosition="1259">g case in Figure 1 using the GHKM procedure. 249 Computational Linguistics Volume 36, Number 2 Figure 3 Additional rules extracted from the learning case in Figure 1. store English words that appear at the left and right corners of the tree, as these are needed for computing the P(e) score when cells are combined. For CKY to work, all transducer rules must be broken down, or binarized, into rules that contain at most two variables—more efficient search can be gained if this binarization produces rules that can be incrementally scored by the language model (Melamed, Satta, and Wellington 2004; Zhang et al. 2006). Finally, we employ cube pruning (Chiang 2007) for further efficiency in the search. When scoring translation candidates, we add several smaller models. One model rewards longer translation candidates, off-setting the language model’s desire for short output. Other models punish rules that drop Chinese content words or introduce spurious English content words. We also include lexical smoothing models (Gale and Sampson 1996; Good 1953) to help distinguish good low-count rules from bad lowcount rules. The final score of a translation candidate is a weighted linear combination of log P(e), log P</context>
<context position="15236" citStr="Zhang et al. (2006)" startWordPosition="2371" endWordPosition="2374">eeds. If we re-structure the training trees as shown in Figure 5, we get much better behavior. Now rule R14 translates �9A49c-t)],/��,/� into a free-standing NPB. This gives rule R15 the ability to translate fn t O` n 1, because it finds the necessary NPB to its left. Here, we are re-structuring the trees in our MT training data by binarizing them. This allows us to extract better translation rules, though of course an extracted rule may have more than two variables. Whether the rules themselves should be binarized is a separate question, addressed in Melamed, Satta, and Wellington (2004) and Zhang et al. (2006). One can decide to re-structure training data trees, binarize translation rules, or do both, or do neither. Here we focus on English tree re-structuring. In this section, we explore the generalization ability of simple re-structuring methods like left-, right-, and head-binarization, and also their combinations. Simple binarization methods binarize syntax trees in a consistent fashion (left-, right-, or head-) and thus cannot guarantee that all the substructures can be factored out. For example, consistent right binarization of the training examples in Figure 4 makes available R14, but misses</context>
<context position="59603" citStr="Zhang et al. (2006)" startWordPosition="9460" endWordPosition="9463">or constructing a packed forest of derivation trees of rules that explain a (tree, string) bitext corpus given that corpus and a rule set, and TRAIN, which is an iterative parameter-setting procedure. We initially attempted to use the top-down DERIV algorithm of Knight, Graehl, and May (2008), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring dead-ends. Instead we build derivation forests using the following sequence of operations: 1. Binarize rules using the synchronous binarization algorithm for tree-to-string transducers described in Zhang et al. (2006). 2. Construct a parse chart with a CKY parser simultaneously constrained on the foreign string and English tree, similar to the bilingual parsing of Wu (1997).4 3. Recover all reachable edges by traversing the chart, starting from the topmost entry. Because the chart is constructed bottom-up, leaf lexical constraints are encountered immediately, resulting in a narrower search space and faster running time than the top-down DERIV algorithm for this application. The Viterbi derivation tree tells us which English words produce which Chinese words, so we can extract a word-to-word alignment from </context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Zhang, Hao, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL), pages 256–263, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Khalil Sima’an</author>
</authors>
<title>A consistent and efficient estimator for data-oriented parsing.</title>
<date>2005</date>
<journal>Journal of Automata, Languages and Combinatorics,</journal>
<pages>10--2</pages>
<marker>Zollmann, Sima’an, 2005</marker>
<rawString>Zollmann, Andreas and Khalil Sima’an. 2005. A consistent and efficient estimator for data-oriented parsing. Journal of Automata, Languages and Combinatorics, 10(2/3):367–388.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>