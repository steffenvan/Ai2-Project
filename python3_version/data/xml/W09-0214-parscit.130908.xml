<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010133">
<title confidence="0.985774">
Semantic Density Analysis:
Comparing word meaning across time and phonetic space
</title>
<author confidence="0.990616">
Eyal Sagi
</author>
<affiliation confidence="0.8862205">
Northwestern University
Evanston, Illinois, USA
</affiliation>
<email confidence="0.99938">
eyal@u.northwestern.edu
</email>
<author confidence="0.997529">
Stefan Kaufmann Brady Clark
</author>
<affiliation confidence="0.9768075">
Northwestern University Northwestern University
Evanston, Illinois, USA Evanston, Illinois, USA
</affiliation>
<email confidence="0.999481">
kaufmann@northwestern.edu bzack@northwestern.edu
</email>
<sectionHeader confidence="0.995651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999541769230769">
This paper presents a new statistical method
for detecting and tracking changes in word
meaning, based on Latent Semantic Analysis.
By comparing the density of semantic vector
clusters this method allows researchers to
make statistical inferences on questions such
as whether the meaning of a word changed
across time or if a phonetic cluster is asso-
ciated with a specific meaning. Possible appli-
cations of this method are then illustrated in
tracing the semantic change of `dog&apos;, `do&apos;, and
`deer&apos; in early English and examining and
comparing phonaesthemes.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9989088">
The increase in available computing power over
the last few decades has led to an explosion in
the application of statistical methods to the anal-
ysis of texts. Researchers have applied these me-
thods to a wide range of tasks, from word-sense
disambiguation (Levin et al., 2006) to the sum-
marization of texts (Marcu, 2003) and the auto-
matic scoring of student essays (Riedel et al.,
2006). However, some fields of linguistics that
have traditionally employed corpora as their
source material, such as historical semantics,
have yet to benefit from the application of these
statistical methods.
In this paper we demonstrate how an existing
statistical tool (Latent Semantic Analysis) can be
adapted and used to automate and enhance some
aspects of research in historical semantics and
other fields whose focus is on the comparative
analysis of word meanings within a corpus. Our
method allows us to assess the semantic variation
within the set of individual occurrences of a giv-
en word type. This variation is inversely related
to a property of types that we call density – intui-
tively, a tendency to occur in highly similar con-
texts. In terms of our LSA-based spatial semantic
model, we calculate vectors representing the con-
text of each occurrence of a given term, and es-
timate the term&apos;s cohesiveness as the density
with which these token context vectors are
“packed” in space.
</bodyText>
<sectionHeader confidence="0.980632" genericHeader="method">
2 The method
</sectionHeader>
<bodyText confidence="0.998396518518518">
Latent Semantic Analysis (LSA) is a collective
term for a family of related methods, all of which
involve building numerical representations of
words based on occurrence patterns in a training
corpus. The basic underlying assumption is that
co-occurrence within the same contexts can be
used as a stand-in measure of semantic related-
ness (see Firth, 1957; Halliday and Hasan, 1976;
Hoey, 1991, for early articulations of this idea).
The success of the method in technical applica-
tions such as information retrieval and its popu-
larity as a research tool in psychology, education,
linguistics and other disciplines suggest that this
hypothesis holds up well for the purposes of
those applications.
The relevant notion of “context” varies. The
first and still widely used implementation of the
idea, developed in Information Retrieval and
originally known as Latent Semantic Indexing
(Deerwester et al., 1990), assembles a term-
document matrix in which each vocabulary item
(term) is associated with an n-dimensional vector
recording its distribution over the n documents in
the corpus. In contrast, the version we applied in
this work measures co-occurrence in a way that
is more independent of the characteristics of the
documents in the training corpus, building in-
</bodyText>
<note confidence="0.9495385">
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 104–111,
Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998519">
104
</page>
<bodyText confidence="0.999868181818182">
stead a term-term matrix associating vocabulary
items with vectors representing their frequency
of co-occurrence with each of a list of “content-
bearing” words. This approach originated with
the “WordSpace” paradigm developed by
Schütze (1996). The software we used is a ver-
sion of the “Infomap” package developed at
Stanford University and freely available (see also
Takayama et al., 1999). We describe it and the
steps we took in our experiments in some detail
below.
</bodyText>
<subsectionHeader confidence="0.986263">
2.1 Word vectors
</subsectionHeader>
<bodyText confidence="0.999964909090909">
The information encoded in the co-occurrence
matrix, and thus ultimately the similarity meas-
ure depends greatly on the genre and subject
matter of the training corpus (Takayama et al.,
1999; Kaufmann, 2000). In our case, we used the
entire available corpus as our training corpus.
The word types in the training corpus are ranked
by frequency of occurrence, and the Infomap
system automatically selects (i) a vocabulary W
for which vector representations are to be col-
lected, and (ii) a set C of 1,000 “content-bearing”
words whose occurrence or non-occurrence is
taken to be indicative of the subject matter of a
given passage of text. Usually, these choices are
guided by a stoplist of (mostly closed-class) lexi-
cal items that are to be excluded, but because we
were interested in tracing changes in the meaning
of lexical items we reduced this stoplist to a bare
minimum. To compensate, we increased the
number of “content-bearing” words to 2,000. The
vocabulary W consisted of the 40,000 most fre-
quent non-stoplist words. The set C of content-
bearing words contained the 50th through 2,049th
most frequent non-stoplist words. This method
may seem rather blunt, but it has the advantage
of not requiring any human intervention or ante-
cedently given information about the domain.
The cells in the resulting matrix of 40,000
rows and 2,000 columns were filled with co-
occurrence counts recording, for each
pair (w, c) E W x C, the number of times a token
of c occurred in the context of a token of w in
the corpus.1 The “context” of a token wi in our
</bodyText>
<footnote confidence="0.737353333333333">
1 Two details are glossed over here: First, the Infomap sys-
tem weighs this raw count with a tf. idf measure of the
column label c, calculated as follows: tf. idf(c) = tf (c) x
</footnote>
<equation confidence="0.903061">
(log(D + 1 ) − log(df(c))) where tf and df are the number
</equation>
<bodyText confidence="0.9723964375">
of occurrences of c and the number of documents in which
c occurs, respectively, and D is the total number of docu-
ments. Second, the number in each cell is replaced with its
square root, in order to approximate a normal distribution of
counts and attenuate the potentially distorting influence of
implementation is the set of tokens in a fixed-
width window from the 15th item preceding wi
to the 15th item following it (less if a document
boundary intervenes). The matrix was trans-
formed by Singular Value Decomposition
(SVD), whose implementation in the Infomap
system relies on the SVDPACKC package
(Berry, 1992; Berry et al., 1993). The output was
a reduced 40,000 X 100 matrix. Thus each item
w E W is associated with a 100-dimensional
vector w .
</bodyText>
<subsectionHeader confidence="0.999543">
2.2 Context vectors
</subsectionHeader>
<bodyText confidence="0.999516730769231">
Once the vector space is obtained from the
training corpus, vectors can be calculated for any
multi-word unit of text (e.g. paragraphs, queries,
or documents), regardless of whether it occurs in
the original training corpus or not, as the normal-
ized sum of the vectors associated with the words
it contains. In this way, for each occurrence of a
target word type under investigation, we calcu-
lated a context vector from the 15 words preced-
ing and the 15 words following that occurrence.
Context vectors were first used in Word Sense
Discrimination by Schütze (1998). Similarly to
that application, we assume that these “second-
order” vectors encode the aggregate meaning, or
topic, of the segment they represent, and thus,
following the reasoning behind LSA, are
indicative of the meaning with which it is being
used on that particular occurrence. Consequently,
for each target word of interest, the context
vectors associated with its occurrences constitute
the data points. The analysis is then a matter of
grouping these data points according to some
criterion (e.g., the period in which the text was
written) and conducting an appropriate statistical
test. In some cases it might also be possible to
use regression or apply a clustering analysis.
</bodyText>
<subsectionHeader confidence="0.999452">
2.3 Semantic Density Analysis
</subsectionHeader>
<bodyText confidence="0.782780142857143">
Conducting statistical tests comparing groups of
vectors is not trivial. Fortunately, some questions
can be answered based on the similarity of vec-
tors within each group rather than the vectors
themselves. The similarity between two vectors
w , v is measured as the cosine between them:2
high base frequencies (cf. Takayama, et al. 1998; Widdows,
2004).
2 While the cosine measure is the accepted measure of simi-
larity, the cosine function is non-linear and therefore prob-
lematic for many statistical methods. Several transforma-
tions can be used to correct this (e.g., Fisher‟s z). In this
paper we will use the angle, in degrees, between the two
vectors (i.e., cos−1) because it is easily interpretable.
</bodyText>
<page confidence="0.994266">
105
</page>
<figure confidence="0.432864333333333">
cos(W , V ) = W• V the angle between two context vectors the angle
between the documents in which they appear.
WV
</figure>
<bodyText confidence="0.998865188679246">
The average similarity of a group of vectors is
indicative of its density – a dense group of highly
similar vectors will have a high average cosine
(and a correspondingly low average angle)
whereas a sparse group of dissimilar vectors will
have an average cosine that approaches zero (and
a correspondingly high average angle).3 Thus
since a word that has a single, highly restricted
meaning (e.g. `palindrome&apos;) is likely to occur in
a very restricted set of contexts, its context vec-
tors are also likely to have a low average angle
between them, compared to a word that is highly
polysemous or appears in a large variety of con-
texts (e.g. `bank&apos;, `do&apos;). From this observation, it
follows that it should be possible to compare the
cohesiveness of groups of vectors in terms of the
average pairwise similarity of the vectors of
which they are comprised. Because the number
of such pairings tends to be prohibitively large
(e.g., nearly 1,000,000 for a group of 1,000 vec-
tors), it is useful to use only a sub-sample in any
single analysis. A Monte-Carlo analysis in which
n pair-wise similarity values are chosen at ran-
dom from each group of vectors is therefore ap-
propriate.4
However, there is one final complication to
consider in the analysis. The passage of time in-
fluences not only the meaning of words, but also
styles and variety of writing. For example, texts
in the 11th century were much less varied, on av-
erage, than those written in the 15th century.5
This will influence the calculation of context
vectors as those depend, in part, on the text they
are taken from. Because the document as a whole
is represented by a vector that is the average of
all of its words, it is possible to predict that, if no
other factors exist, two contexts are likely to be
related to one another to the same degree that
their documents are. Controlling for this effect
can therefore be achieved by subtracting from
3 Since the cosine ranges from -1 to +1, it is possible in
principle to obtain negative average cosines. In practice,
however, the overwhelming majority of vocabulary items
have a non-negative cosine with any given target word,
hence the average cosine usually does not fall below zero.
4 It is important to note that the number of independent
samples in the analysis is determined not by the number of
similarity values compared but by the number of individual
vectors used in the analysis.
5 Tracking changes in the distribution of the document
vectors in a corpus over time might itself be of interest to
some researchers but is beyond the scope of the current
paper.
</bodyText>
<sectionHeader confidence="0.984109" genericHeader="method">
3 Applications to Research
</sectionHeader>
<subsectionHeader confidence="0.99609">
3.1 A Diachronic Investigation: Semantic
Change
</subsectionHeader>
<bodyText confidence="0.924891916666667">
One of the central questions of historical seman-
tics is the following (Traugott, 1999):6
Given the form-meaning pair L (lexeme) what
changes did meaning M undergo?
For example, the form as long as underwent
the change `equal in length&apos; &gt; `equal in time&apos; &gt;
`provided that&apos;. Evidence for semantic change
comes from written records, cognates, and struc-
tural analysis (Bloomfield, 1933). Traditional
categories of semantic change include (Traugott,
2005: 2-4; Campbell, 2004:254-262; Forston,
2003: 648-650):
</bodyText>
<listItem confidence="0.995194666666667">
• Broadening (generalization, extension,
borrowing): A restricted meaning becomes less
restricted (e.g. Late Old English docga `a (spe-
cific) powerful breed of dog&apos; &gt; dog `any member
of the species Canis familiaris&apos;
• Narrowing (specialization, restriction): A
relatively general meaning becomes more specif-
ic (e.g. Old English deor `animal&apos; &gt; deer)
• Pejoration (degeneration): A meaning be-
comes more negative (e.g. Old English swlig
`blessed, blissful&apos; &gt; sely `happy, innocent, pitia-
ble&apos; &gt; silly `foolish, stupid&apos;)
</listItem>
<bodyText confidence="0.995162875">
Semantic change results from the use of lan-
guage in context, whether linguistic or extralin-
guistic. Later meanings of forms are connected to
earlier ones, where all semantic change arises by
polysemy, i.e. new meanings coexist with earlier
ones, typically in restricted contexts. Sometimes
new meanings split off from earlier ones and are
no longer considered variants by language users
(e.g. mistress `woman in a position of authority,
head of household&apos; &gt; `woman in a continuing
extra-marital relationship with a man&apos;).
Semantic change is often considered unsyste-
matic (Hock and Joseph, 1996: 252). However,
recent work (Traugott and Dasher, 2002) sug-
gests that there is, in fact, significant cross-
linguistic regularity in semantic change. For ex-
</bodyText>
<footnote confidence="0.6207885">
6 This is the semasiological perspective on semantic change.
Other perspectives include the onomasiological perspective
(“Given the concept C, what lexemes can it be expressed
by?”). See Traugott 1999 for discussion.
</footnote>
<page confidence="0.998364">
106
</page>
<tableCaption confidence="0.99706">
Table 1 - Mean angle between context vectors for target words in different periods in the Helsinki
corpus (standard deviations are given in parenthesis)
</tableCaption>
<table confidence="0.992367">
n Unknown composi- Early Middle Late Middle Early Modern
tion date English English English
(&lt;1250) (1150-1350) (1350-1500) (1500-1710)
dog 112 15.47 (14.19) 24.73(10.43)
do 4298 10.31(13.57) 13.02 (9.50) 24.54 (11.2)
deer 61 38.72 (17.59) 20.6 (18.18) 20.5 (9.82)
science 79 13.56 (13.33) 28.31 (12.24)
</table>
<bodyText confidence="0.999867133333334">
ample, in the Invited Inferencing Model of Se-
mantic Change proposed by Traugott and Dasher
(2002) the main mechanism of semantic change
is argued to be the semanticization of conversa-
tional implicatures, where conversational impli-
catures are a component of speaker meaning that
arises from the interaction between what the
speaker says and rational principles of communi-
cation (Grice, 1989 [1975]). Conversational im-
plicatures are suggested by an utterance but not
entailed. For example, the utterance Some stu-
dents came to the party strongly suggests that
some but not all students came to the party, even
though the utterance would be true strictly speak-
ing if all students came to the party. According to
the Invited Inferencing Model, conversational
implicatures become part of the semantic poly-
semies of particular forms over time.
Such changes in meaning should be evident
when examining the contexts in which the lex-
eme of interest appears. In other words, changes
in the meaning of a type should translate to dif-
ferences in the contexts in which its tokens are
used. For instance, semantic broadening results
in a meaning that is less restricted and as a result
can be used in a larger variety of contexts. In a
semantic space that encompasses the period of
such a change, this increase in variety can be
measured as a decrease in vector density across
the time span of the corpus. This decrease trans-
lates into an increase in the average angle be-
tween the context vectors for the word. For in-
stance, because the Old English word `docga&apos;
applied to a specific breed of dog, we predicted
that earlier occurrences of the lexemes `docga&apos;
and `dog&apos;, in a corpus of documents of the ap-
propriate time period, will show less variety than
later occurrences.
An even more extreme case of semantic broa-
dening is predicted to occur as part of the process
of grammaticalization (Traugot and Dasher,
2002) in which a content word becomes a func-
tion word. Because, as a general rule, a function
word can be used in a much larger variety of
contexts than a content word, a word that under-
went grammaticalization should appear in a sub-
stantially larger variety of contexts than it did
prior to becoming a function word. One well stu-
died case of grammaticalization is that of periph-
rastic `do&apos;. While in Old English `do&apos; was used
as a verb with a causative and habitual sense
(e.g. `do you harm&apos;), later in English it took on a
functional role that is nearly devoid of meaning
(e.g. `do you know him?&apos;). Because this change
occurred in Middle English, we predicted that
earlier occurrences of `do&apos; will show less variety
than later ones.
In contrast with broadening, semantic narrow-
ing results in a meaning that is more restricted,
and is therefore applicable in fewer contexts than
before. This decrease in variety results in an in-
crease in vector density and can be directly
measured as a decrease in the average angle be-
tween the context vectors for the word. As an
example, the Old English word `deor&apos; denoted a
larger group of living creatures than does the
Modern English word `deer&apos;. We therefore pre-
dicted that earlier occurrences of the lexemes
`deor&apos; and `deer&apos;, in a corpus of the appropriate
time period, will show more variety than later
occurrences.
We tested our predictions using a corpus de-
rived from the Helsinki corpus (Rissanen, 1994).
The Helsinki corpus is comprised of texts span-
ning the periods of Old English (prior to
1150A.D.), Middle English (1150-1500A.D.),
and Early Modern English (1500-1710A.D.).
Because spelling in Old English was highly vari-
able, we decided to exclude that part of the cor-
pus and focused our analysis on the Middle Eng-
lish and Early Modern English periods. The re-
sulting corpus included 504 distinct documents
totaling approximately 1.1 million words.
To test our predictions regarding semantic
change in the words `dog&apos;, `do&apos;, and `deer&apos;, we
collected all of the contexts in which they appear
in our subset of the Helsinki corpus. This re-
sulted in 112 contexts for `dog&apos;, 4298 contexts
for `do&apos;, and 61 contexts for `deer&apos;. Because
there were relatively few occurrences of `dog&apos;
</bodyText>
<page confidence="0.95713">
107
</page>
<figure confidence="0.998853">
Mean Angle between vectors
(current study)
30
20
10
1200 1300 1400 1500 1600 1700
Year
Current Study Ellegard&apos;s data
70
50
30
10
% of periphrastic ‘do’ uses
(Ellegård, 1953)
</figure>
<figureCaption confidence="0.9979025">
Figure 1 – A comparison of the rise of periphrastic &apos;do&apos; as measured by semantic density in our study and
the proportion of periphrastic &apos;do&apos; uses by Ellegfird (1953).
</figureCaption>
<bodyText confidence="0.999616444444445">
and `deer&apos; in the corpus it was practical to com-
pute the angles between all possible pairs of con-
text vectors. As a result, we elected to forgo the
Monte-Carlo analysis for those two words in fa-
vor of a full analysis. The results of our analysis
for all three words are given in Table 1. These
results were congruent with our prediction: The
density of the contexts decreases over time for
both `dog&apos; (t(110) = 2.17, p &lt; .05) and `do&apos;
(F(2,2997)=409.41, p &lt; .01) while in the case of
`deer&apos; there is an increase in the density of the
contexts over time (t(36) = 3.05, p &lt; .01).
Furthermore, our analysis corresponds with
the data collected by Ellegård (1953). Ellegård
traced the grammaticalization of `do&apos; by manual-
ly examining changes in the proportions of its
various uses between 1400 and 1700. His data
identifies an overall shift in the pattern of use
that occurred mainly between 1475 and 1575.
Our analysis identifies a similar shift in patterns
between the time periods spanning 1350-1500
and 1500-1570. Figure 1 depicts an overlay of
both datasets. The relative scale of the two sets
was set so that the proportions of `do&apos; uses at
1400 and 1700 (the beginning and end of El-
legård&apos;s data, respectively) match the semantic
density measured by our method at those times.
Finally, our method can be used not only to
test predictions based on established cases of
semantic change, but also to identify new ones.
For instance, in examining the contexts of the
word `science&apos; we can identify that it underwent
semantic broadening shortly after it first ap-
peared in the 14th century (t(77) = 4.51, p &lt; .01).
A subsequent examination of the contexts in
which the word appears indicated that this is
probably the result of a shift from a meaning re-
lated to generalized knowledge (e.g., `...and
learn science of school&apos;, John of Trevisa&apos;s Polyc-
hronicon, 1387) to one that can also be used to
refer to more specific disciplines (e.g., `...of the
seven liberal sciences&apos;, Simon Forman&apos;s Diary,
1602).
Our long term goal with respect to this type of
analysis is to use this method in a computer-
based tool that can scan a diachronic corpus and
automatically identify probable cases of semantic
change within it. Researchers can then use these
results to focus on identifying the specifics of
such changes, as well as examine the overall pat-
terns of change that exist in the corpus. It is our
belief that such a use will enable a more rigorous
testing and refinement of existing theories of se-
mantic change.
</bodyText>
<subsectionHeader confidence="0.9543365">
3.2 A Synchronic Investigation: Phonaes-
themes
</subsectionHeader>
<bodyText confidence="0.999831869565217">
In addition to examining changes in meaning
across time, it is also possible to employ our me-
thod to examine how the semantic space relates
to other possible partitioning of the lexemes
represented by it. For instance, while the rela-
tionship between the phonetic representation and
semantic content is largely considered to be arbi-
trary, there are some notable exceptions. One
interesting case is that of phonaesthemes (Firth,
1930), sub-morphemic units that have a predict-
able effect on the meaning of the word as a
whole. In English, one of the more frequently
mentioned phonaesthemes is a word-initial gl-
which is common in words related to the visual
modality (e.g., `glance&apos;, `gleam&apos;). While there
have been some scholastic explorations of these
non-morphological relationships between sound
and meaning, they have not been thoroughly ex-
plored by behavioral and computational research
(with some notable exceptions; e.g. Hutchins,
1998; Bergen, 2004). Recently, Otis and Sagi
(2008) used the semantic density of the cluster of
words sharing a phonaestheme as a measure of
</bodyText>
<page confidence="0.998384">
108
</page>
<bodyText confidence="0.992645419047619">
the strength of the relationship between the pho-
netic cluster and its proposed meaning.
Otis and Sagi used a corpus derived from
Project Gutenberg (http://www.gutenberg.org/)
as the basis for their analysis. Specifically, they
used the bulk of the English language literary
works available through the project&apos;s website.
This resulted in a corpus of 4034 separate docu-
ments consisting of over 290 million words.
The bulk of the candidate phonaesthemes they
tested were taken from the list used by Hutchins
(1998), with the addition of two candidate pho-
naesthemes (kn- and -ign). Two letter combina-
tions that were considered unlikely to be pho-
naesthemes (br- and z-) were also included in
order to test the method&apos;s capacity for discrimi-
nating between phonaesthemes and non-
phonaesthemes. Overall Otis and Sagi (2008)
examined 47 possible phonaesthemes.
In cases where a phonetic cluster represents a
phonaestheme, it intuitively follows that pairs of
words sharing that phonetic cluster are more
likely to share some aspect of their meaning than
pairs of words chosen at random. Otis and Sagi
tested whether this was true for any specific can-
didate phonaestheme using a Monte-Carlo analy-
sis. First they identified all of the words in the
corpus sharing a conjectured phonaestheme7 and
chose the most frequent representative word
form for each stem, resulting in a cluster of word
types representing each candidate phonaes-
theme.8 Next they tested the statistical signific-
ance of this relationship by running 100 t-test
comparisons. Each of these tests compared the
relationship of 50 pairs of words chosen at ran-
dom from the conjectured cluster with 50 pairs of
words chosen at random from a similarly sized
cluster, randomly generated from the entire cor-
pus. The number of times these t-tests resulted in
a statistically significant difference (α = .05) was
recorded. This analysis was repeated 3 times for
each conjectured phonaestheme and the median
value was used as the final result.
To determine whether a conjectured phonaes-
theme was statistically supported by their analy-
sis Otis and Sagi compared the overall frequency
7 It is important to note that due to the nature of a written
corpus, the match was orthographical rather than phonetic.
However, in most cases the two are highly congruent.
8 Because, in this case, Otis and Sagi were not interested in
temporal changes in meaning, they used the overall word
vectors rather than look at each context individually. As a
result, each of the vectors used in the analysis is based on
occurrences in many different documents and there was no
need to control for the variability of the documents.
of statistically significant t-tests with the binomi-
al distribution for their α (.05). After applying a
Bonferroni correction for performing 50 compar-
isons, the threshold for statistical significance of
the binomial test was for 14 t-tests out of 100 to
turn out as significant, with a frequency of 13
being marginally significant. Therefore, if the
significance frequency (#Sig below) of a candi-
date phonaestheme was 15 or higher, that pho-
naestheme was judged as being supported by
statistical evidence. Significance frequencies of
13 and 14 were considered as indicative of a
phonaestheme for which there was only marginal
statistical support.
Among Hutchins&apos; original list of 44 possible
phonaesthemes, 26 were found to be statistically
reliable and 2 were marginally reliable. Overall
the results were in line with the empirical data
collected by Hutchins. By way of comparing the
two datasets, #Sig and Hutchins&apos; average rating
measure were well correlated (r = .53). Neither
of the unlikely phonaestheme candidates we ex-
amined were statistically supported phonaes-
themes (#Sigbr- = 6; #Sigz- = 5), whereas both of
our newly hypothesized phonaesthemes were
statistically supported (#Sigkn- = 28; #Sig-ign =
23). In addition to being able to use this measure
as a decision criterion as to whether a specific
phonetic cluster might be phonaesthemic, it can
also be used to compare the relative strength of
two such clusters. For instance, in the Gutenberg
corpus the phonaesthemic ending –owl (e.g.,
„growl&apos;, „howl&apos;; #Sig=97) was comprised of a
cluster of words that were more similar to one
another than –oop (e.g., „hoop&apos;, „loop&apos;; #Sig=32).
Such results can then be used to test the cogni-
tive effects of phonaesthemes. For instance, fol-
lowing the comparison above, we might hypo-
thesize that the word „growl&apos; might be a better
semantic prime for „howl&apos; than the word „hoop&apos;
is for the word „loop&apos;. In contrast, because a
word-initial br- is not phonaesthemic, the word
„breeze&apos; is unlikely to be a semantic prime for
the word „brick&apos;. In addition, it might be interest-
ing to combine the diachronic analysis from the
previous section with the synchronic analysis in
this section to investigate questions such as when
and how phonaesthemes come to be part of a
language and what factors might affect the
strength of a phonaestheme.
</bodyText>
<sectionHeader confidence="0.998079" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.845108">
While the method presented in this paper is
aimed towards quantifying semantic relation-
</bodyText>
<page confidence="0.998267">
109
</page>
<bodyText confidence="0.9999296">
ships that were previously difficult to quantify, it
also raises an interesting theoretical issue, name-
ly the relationship between the statistically com-
puted semantic space and the actual semantic
content of words. On the one hand, simulations
based on Latent Semantic Analysis have been
shown to correlate with cognitive factors such as
the acquisition of vocabulary and the categoriza-
tion of texts (cf. Landauer &amp; Dumais, 1997). On
the other hand, in reality speakers&apos; use of lan-
guage relies on more than simple patterns of
word co-occurrence – For instance, we use syn-
tactic structures and pragmatic reasoning to sup-
plement the meaning of the individual lexemes
we come across (e.g., Fodor, 1995; Grice, 1989
[1975]). It is therefore likely that while LSA cap-
tures some of the variability in meaning exhi-
bited by words in context, it does not capture all
of it. Indeed, there is a growing body of methods
that propose to integrate these two disparate
sources of linguistic information (e.g., Pado and
Lapata, 2007; Widdows, 2003)
Certainly, the results reported in this paper
suggest that enough of the meaning of words and
contexts is captured to allow interesting infe-
rences about semantic change and the relatedness
of words to be drawn with a reasonable degree of
certainty. However, it is possible that some im-
portant aspects of meaning are systematically
ignored by the analysis. For instance, it remains
to be seen whether this method can distinguish
between processes like pejoration and amerliora-
tion as they require a fine grained distinction be-
tween `good&apos; and `bad&apos; meanings.
Regardless of any such limitations, it is clear
that important information about meaning can be
gathered through a systematic analysis of the
contexts in which words appear. Furthermore,
phenomena such as the existence of phonaes-
themes and the success of LSA in predicting vo-
cabulary acquisition rates, suggest that the acqui-
sition of new vocabulary involves the gleaning of
the meaning of words through their context. The
role of context in semantic change is therefore
likely to be an active one – when a listener en-
counters a word they are unfamiliar with they are
likely to use the context in which it appears, as
well as its phonetic composition, as clues to its
meaning. Furthermore, if a word is likewise en-
countered in context in which it is unlikely, this
unexpected observation may induce the listener
to adjust their representation of both the context
and the word in order to increase the overall co-
herence of the utterance or sentence. As a result,
it is possible that examining the contexts in
which a word is used in different documents and
time periods might be useful not only as a tool
for examining the history of a semantic change
but also as an instrument for predicting its future
progress. Overall, this suggests a dynamic view
of the field of semantics – semantics as an ever-
changing landscape of meaning. In such a view,
semantic change is the norm as the perceived
meaning of words keeps shifting to accommo-
date the contexts in which they are used.
</bodyText>
<sectionHeader confidence="0.992202" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.957587675675676">
Bergen, B. (2004). The Psychological Reality of
Phonaesthemes. Language, 80(2), 291-311.
Berry, M. W. (1992) SVDPACK: A Fortran-77
software library for the sparse singular value
decomposition. Tech. Rep. CS-92-159, Knox-
ville, TN: University of Tennessee.
Berry, M. W., Do, T., O&apos;Brien, G. Vijay, K. Va-
radh an, S. (1993) SVDPACKC (Version 1.0)
User’s Guide, Tech. Rep. UT-CS-93-194,
Knoxville, TN: University of Tennessee.
Bloomfield, L. (1933). Language. New York,
NY: Holt, Rinehart and Winston.
Campbell, L. (2004) Historical linguistics: An
introduction 2nd ed. Cambridge, MA: The MIT
Press.
Deerwester, S., Dumais, S. T., Furnas, G. W.,
Landauer, T. K., and Harshman, R. (1990) In-
dexing by Latent Semantic Analysis. Journal
of the American Society for Information
Science, 41, 391-407.
Ellegård, A. (1953) The Auxiliary Do: the Estab-
lishment and Regulation of its Use in English.
Gothenburg Studies in English, 2. Stockholm:
Almqvist and Wiksell.
Firth, J. (1930) Speech. London: Oxford Univer-
sity Press.
Firth, J. (1957) Papers in Linguistics, 1934-1951,
Oxford University Press.
Fodor, J. D. (1995) Comprehending sentence
structure. In L. R. Gleitman and M. Liberman,
(Eds.), Invitation to Cognitive Science, volume
1. MIT Press, Cambridge, MA. 209-246.
Forston, B. W. (2003) An Approach to Semantic
Change. In B. D. Joseph and R. D. Janda
(Eds.), The Handbook of Historical Linguis-
tics. Malden, MA: Blackwell Publishing. 648-
666.
</reference>
<page confidence="0.982804">
110
</page>
<reference confidence="0.997544958333334">
Grice, H. P. (1989) [1975]. Logic and Conversa-
tion. In Studies in the Way of Words. Cam-
bridge, MA: Harvard University Press. 22-40.
Halliday, M. A. K., &amp; Hasan, R. (1976) Cohe-
sion in English. London: Longman.
Hock, H. H., and Joseph, B. D. (1996) Language
History, Language Change, and Language Re-
lationship: An Introduction to Historical and
Comparative Linguistics. Berlin: Mouton de
Gruyter.
Hoey, M. (1991) Patterns of Lexis in Text. Lon-
don: Oxford University Press.
Hutchins, S. S. (1998). The psychological reality,
variability, and compositionality of English
phonesthemes. Dissertation Abstracts Interna-
tional, 59(08), 4500B. (University Microfilms
No. AAT 9901857).
Infomap [Computer Software]. (2007).
http://infomap-nlp.sourceforge.net/ Stanford,
CA.
Kaufmann, S. (2000) Second-order cohesion.
Computational Intelligence. 16, 511-524.
Landauer, T. K., &amp; Dumais, S. T. (1997). A solu-
tion to Plato&apos;s problem: The Latent Semantic
Analysis theory of the acquisition, induction,
and representation of knowledge. Psychologi-
cal Review, 104, 211-240.
Levin, E., Sharifi, M., &amp; Ball, J. (2006) Evalua-
tion of utility of LSA for word sense discrimi-
nation. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL,
Companion Volume: Short Papers, New York
City. 77-80.
Marcu, D (2003) Automatic Abstracting, Encyc-
lopedia of Library and Information Science,
Drake, M. A., ed. 245-256.
Otis K., &amp; Sagi E. (2008) Phonaesthemes: A
Corpora-based Analysis. In B. C. Love, K.
McRae, &amp; V. M. Sloutsky (Eds.), Proceedings
of the 30th Annual Conference of the Cognitive
Science Society. Austin, TX: Cognitive
Science Society.
Pado, S. &amp; Lapata, M. (2007) Dependency-based
Construction of Semantic Space Models.
Computational Linguistics, 33, 161-199.
Riedel E., Dexter S. L., Scharber C., Doering A.
(2006) Experimental Evidence on the Effec-
tiveness of Automated Essay Scoring in
Teacher Education Cases. Journal of Educa-
tional Computing Research, 35, 267-287.
Rissanen, M. (1994) The Helsinki Corpus of
English Texts. In Kytö, M., Rissanen, M. and
Wright S. (eds), Corpora Across the Centu-
ries: Proceedings of the First International
Colloquium on English Diachronic Corpora.
Amsterdam: Rodopi.
Schi tze, H. (1996) Ambiguity in language learn-
ing: computational and cognitive models. CA:
Stanford.
Schi tze, H. (1998) Automatic word sense dis-
crimination. Computational Linguistics
24(1):97-124.
Takayama, Y., Flournoy R., &amp; Kaufmann, S.
(1998) Information Mapping: Concept-Based
Information Retrieval Based on Word
Associations. CSLI Tech Report. CA:
Stanford.
Takayama, Y., Flournoy, R., Kaufmann, S. &amp;
Peters, S. (1999). Information retrieval based
on domain-specific word associations. In Cer-
cone, N. and Naruedomkul K. (eds.), Proceed-
ings of the Pacific Association for Computa-
tional Linguistics (PACLING’99), Waterloo,
Canada. 155-161.
Traugott, E. C. (1999) The Role of Pragmatics in
Semantic Change. In J. Verschueren (ed.),
Pragmatics in 1998: Selected Papers from the
6th International Pragmatics Conference, vol.
II. Antwerp: International Pragmatics Associa-
tion. 93-102.
Traugott, E. C. (2005) Semantic Change. In En-
cyclopedia of Language and Linguistics, 2nd
ed., Brown K. ed. Oxford: Elsevier.
Traugott, E. C., and Dasher R. B. (2002) Regu-
larity in Semantic Change. Cambridge: Cam-
bridge University Press.
Widdows, D. (2003) Unsupervised methods for
developing taxonomies by combining syntactic
and statistical information. In Proceedings of
the joint Human Language Technology Confe-
rence and Annual Meeting of the North Ameri-
can Chapter of the Association for Computa-
tional Linguistics. Edmonton, Canada:
Wiemer-Hastings. 197-204.
Widdows, D. (2004) Geometry and Meaning.
CSLI Publications, CA: Stanford.
</reference>
<page confidence="0.9988">
111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951592">
<title confidence="0.991704">Semantic Density Analysis: Comparing word meaning across time and phonetic space</title>
<author confidence="0.989906">Eyal Sagi</author>
<affiliation confidence="0.999996">Northwestern University</affiliation>
<address confidence="0.998979">Evanston, Illinois, USA</address>
<email confidence="0.999828">eyal@u.northwestern.edu</email>
<author confidence="0.987746">Stefan Kaufmann Brady Clark</author>
<affiliation confidence="0.999992">Northwestern University Northwestern University</affiliation>
<address confidence="0.998829">Evanston, Illinois, USA Evanston, Illinois, USA</address>
<email confidence="0.999693">kaufmann@northwestern.edubzack@northwestern.edu</email>
<abstract confidence="0.999282">This paper presents a new statistical method for detecting and tracking changes in word meaning, based on Latent Semantic Analysis. By comparing the density of semantic vector clusters this method allows researchers to make statistical inferences on questions such as whether the meaning of a word changed across time or if a phonetic cluster is associated with a specific meaning. Possible applications of this method are then illustrated in semantic change of `dog&apos;, `do&apos;, and in English and examining and comparing phonaesthemes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Bergen</author>
</authors>
<title>The Psychological Reality of Phonaesthemes.</title>
<date>2004</date>
<journal>Language,</journal>
<volume>80</volume>
<issue>2</issue>
<pages>291--311</pages>
<contexts>
<context position="21982" citStr="Bergen, 2004" startWordPosition="3625" endWordPosition="3626">re are some notable exceptions. One interesting case is that of phonaesthemes (Firth, 1930), sub-morphemic units that have a predictable effect on the meaning of the word as a whole. In English, one of the more frequently mentioned phonaesthemes is a word-initial glwhich is common in words related to the visual modality (e.g., `glance&apos;, `gleam&apos;). While there have been some scholastic explorations of these non-morphological relationships between sound and meaning, they have not been thoroughly explored by behavioral and computational research (with some notable exceptions; e.g. Hutchins, 1998; Bergen, 2004). Recently, Otis and Sagi (2008) used the semantic density of the cluster of words sharing a phonaestheme as a measure of 108 the strength of the relationship between the phonetic cluster and its proposed meaning. Otis and Sagi used a corpus derived from Project Gutenberg (http://www.gutenberg.org/) as the basis for their analysis. Specifically, they used the bulk of the English language literary works available through the project&apos;s website. This resulted in a corpus of 4034 separate documents consisting of over 290 million words. The bulk of the candidate phonaesthemes they tested were taken</context>
</contexts>
<marker>Bergen, 2004</marker>
<rawString>Bergen, B. (2004). The Psychological Reality of Phonaesthemes. Language, 80(2), 291-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Berry</author>
</authors>
<title>SVDPACK: A Fortran-77 software library for the sparse singular value decomposition.</title>
<date>1992</date>
<tech>Tech. Rep. CS-92-159,</tech>
<institution>University of Tennessee.</institution>
<location>Knoxville, TN:</location>
<contexts>
<context position="6650" citStr="Berry, 1992" startWordPosition="1074" endWordPosition="1075">f occurrences of c and the number of documents in which c occurs, respectively, and D is the total number of documents. Second, the number in each cell is replaced with its square root, in order to approximate a normal distribution of counts and attenuate the potentially distorting influence of implementation is the set of tokens in a fixedwidth window from the 15th item preceding wi to the 15th item following it (less if a document boundary intervenes). The matrix was transformed by Singular Value Decomposition (SVD), whose implementation in the Infomap system relies on the SVDPACKC package (Berry, 1992; Berry et al., 1993). The output was a reduced 40,000 X 100 matrix. Thus each item w E W is associated with a 100-dimensional vector w . 2.2 Context vectors Once the vector space is obtained from the training corpus, vectors can be calculated for any multi-word unit of text (e.g. paragraphs, queries, or documents), regardless of whether it occurs in the original training corpus or not, as the normalized sum of the vectors associated with the words it contains. In this way, for each occurrence of a target word type under investigation, we calculated a context vector from the 15 words preceding</context>
</contexts>
<marker>Berry, 1992</marker>
<rawString>Berry, M. W. (1992) SVDPACK: A Fortran-77 software library for the sparse singular value decomposition. Tech. Rep. CS-92-159, Knoxville, TN: University of Tennessee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Berry</author>
<author>T Do</author>
<author>G Vijay O&apos;Brien</author>
<author>K Varadh an</author>
<author>S</author>
</authors>
<date>1993</date>
<journal>SVDPACKC (Version</journal>
<tech>User’s Guide, Tech. Rep. UT-CS-93-194,</tech>
<volume>1</volume>
<institution>University of Tennessee.</institution>
<location>Knoxville, TN:</location>
<contexts>
<context position="6671" citStr="Berry et al., 1993" startWordPosition="1076" endWordPosition="1079"> of c and the number of documents in which c occurs, respectively, and D is the total number of documents. Second, the number in each cell is replaced with its square root, in order to approximate a normal distribution of counts and attenuate the potentially distorting influence of implementation is the set of tokens in a fixedwidth window from the 15th item preceding wi to the 15th item following it (less if a document boundary intervenes). The matrix was transformed by Singular Value Decomposition (SVD), whose implementation in the Infomap system relies on the SVDPACKC package (Berry, 1992; Berry et al., 1993). The output was a reduced 40,000 X 100 matrix. Thus each item w E W is associated with a 100-dimensional vector w . 2.2 Context vectors Once the vector space is obtained from the training corpus, vectors can be calculated for any multi-word unit of text (e.g. paragraphs, queries, or documents), regardless of whether it occurs in the original training corpus or not, as the normalized sum of the vectors associated with the words it contains. In this way, for each occurrence of a target word type under investigation, we calculated a context vector from the 15 words preceding and the 15 words fol</context>
</contexts>
<marker>Berry, Do, O&apos;Brien, an, S, 1993</marker>
<rawString>Berry, M. W., Do, T., O&apos;Brien, G. Vijay, K. Varadh an, S. (1993) SVDPACKC (Version 1.0) User’s Guide, Tech. Rep. UT-CS-93-194, Knoxville, TN: University of Tennessee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bloomfield</author>
</authors>
<date>1933</date>
<location>Language. New York, NY: Holt, Rinehart and Winston.</location>
<contexts>
<context position="11946" citStr="Bloomfield, 1933" startWordPosition="1968" endWordPosition="1969">in the distribution of the document vectors in a corpus over time might itself be of interest to some researchers but is beyond the scope of the current paper. 3 Applications to Research 3.1 A Diachronic Investigation: Semantic Change One of the central questions of historical semantics is the following (Traugott, 1999):6 Given the form-meaning pair L (lexeme) what changes did meaning M undergo? For example, the form as long as underwent the change `equal in length&apos; &gt; `equal in time&apos; &gt; `provided that&apos;. Evidence for semantic change comes from written records, cognates, and structural analysis (Bloomfield, 1933). Traditional categories of semantic change include (Traugott, 2005: 2-4; Campbell, 2004:254-262; Forston, 2003: 648-650): • Broadening (generalization, extension, borrowing): A restricted meaning becomes less restricted (e.g. Late Old English docga `a (specific) powerful breed of dog&apos; &gt; dog `any member of the species Canis familiaris&apos; • Narrowing (specialization, restriction): A relatively general meaning becomes more specific (e.g. Old English deor `animal&apos; &gt; deer) • Pejoration (degeneration): A meaning becomes more negative (e.g. Old English swlig `blessed, blissful&apos; &gt; sely `happy, innocent</context>
</contexts>
<marker>Bloomfield, 1933</marker>
<rawString>Bloomfield, L. (1933). Language. New York, NY: Holt, Rinehart and Winston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Campbell</author>
</authors>
<title>Historical linguistics: An introduction 2nd ed.</title>
<date>2004</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="12034" citStr="Campbell, 2004" startWordPosition="1979" endWordPosition="1980">st to some researchers but is beyond the scope of the current paper. 3 Applications to Research 3.1 A Diachronic Investigation: Semantic Change One of the central questions of historical semantics is the following (Traugott, 1999):6 Given the form-meaning pair L (lexeme) what changes did meaning M undergo? For example, the form as long as underwent the change `equal in length&apos; &gt; `equal in time&apos; &gt; `provided that&apos;. Evidence for semantic change comes from written records, cognates, and structural analysis (Bloomfield, 1933). Traditional categories of semantic change include (Traugott, 2005: 2-4; Campbell, 2004:254-262; Forston, 2003: 648-650): • Broadening (generalization, extension, borrowing): A restricted meaning becomes less restricted (e.g. Late Old English docga `a (specific) powerful breed of dog&apos; &gt; dog `any member of the species Canis familiaris&apos; • Narrowing (specialization, restriction): A relatively general meaning becomes more specific (e.g. Old English deor `animal&apos; &gt; deer) • Pejoration (degeneration): A meaning becomes more negative (e.g. Old English swlig `blessed, blissful&apos; &gt; sely `happy, innocent, pitiable&apos; &gt; silly `foolish, stupid&apos;) Semantic change results from the use of language </context>
</contexts>
<marker>Campbell, 2004</marker>
<rawString>Campbell, L. (2004) Historical linguistics: An introduction 2nd ed. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<pages>391--407</pages>
<contexts>
<context position="3215" citStr="Deerwester et al., 1990" startWordPosition="494" endWordPosition="497">n be used as a stand-in measure of semantic relatedness (see Firth, 1957; Halliday and Hasan, 1976; Hoey, 1991, for early articulations of this idea). The success of the method in technical applications such as information retrieval and its popularity as a research tool in psychology, education, linguistics and other disciplines suggest that this hypothesis holds up well for the purposes of those applications. The relevant notion of “context” varies. The first and still widely used implementation of the idea, developed in Information Retrieval and originally known as Latent Semantic Indexing (Deerwester et al., 1990), assembles a termdocument matrix in which each vocabulary item (term) is associated with an n-dimensional vector recording its distribution over the n documents in the corpus. In contrast, the version we applied in this work measures co-occurrence in a way that is more independent of the characteristics of the documents in the training corpus, building inProceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 104–111, Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics 104 stead a term-term matrix associating vocabulary</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and Harshman, R. (1990) Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41, 391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ellegård</author>
</authors>
<title>The Auxiliary Do: the Establishment and Regulation of its Use</title>
<date>1953</date>
<booktitle>in English. Gothenburg Studies in English, 2. Stockholm: Almqvist and Wiksell.</booktitle>
<contexts>
<context position="18310" citStr="Ellegård, 1953" startWordPosition="3005" endWordPosition="3006">glish periods. The resulting corpus included 504 distinct documents totaling approximately 1.1 million words. To test our predictions regarding semantic change in the words `dog&apos;, `do&apos;, and `deer&apos;, we collected all of the contexts in which they appear in our subset of the Helsinki corpus. This resulted in 112 contexts for `dog&apos;, 4298 contexts for `do&apos;, and 61 contexts for `deer&apos;. Because there were relatively few occurrences of `dog&apos; 107 Mean Angle between vectors (current study) 30 20 10 1200 1300 1400 1500 1600 1700 Year Current Study Ellegard&apos;s data 70 50 30 10 % of periphrastic ‘do’ uses (Ellegård, 1953) Figure 1 – A comparison of the rise of periphrastic &apos;do&apos; as measured by semantic density in our study and the proportion of periphrastic &apos;do&apos; uses by Ellegfird (1953). and `deer&apos; in the corpus it was practical to compute the angles between all possible pairs of context vectors. As a result, we elected to forgo the Monte-Carlo analysis for those two words in favor of a full analysis. The results of our analysis for all three words are given in Table 1. These results were congruent with our prediction: The density of the contexts decreases over time for both `dog&apos; (t(110) = 2.17, p &lt; .05) and `</context>
</contexts>
<marker>Ellegård, 1953</marker>
<rawString>Ellegård, A. (1953) The Auxiliary Do: the Establishment and Regulation of its Use in English. Gothenburg Studies in English, 2. Stockholm: Almqvist and Wiksell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Firth</author>
</authors>
<title>Speech. London:</title>
<date>1930</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="21460" citStr="Firth, 1930" startWordPosition="3546" endWordPosition="3547"> corpus. It is our belief that such a use will enable a more rigorous testing and refinement of existing theories of semantic change. 3.2 A Synchronic Investigation: Phonaesthemes In addition to examining changes in meaning across time, it is also possible to employ our method to examine how the semantic space relates to other possible partitioning of the lexemes represented by it. For instance, while the relationship between the phonetic representation and semantic content is largely considered to be arbitrary, there are some notable exceptions. One interesting case is that of phonaesthemes (Firth, 1930), sub-morphemic units that have a predictable effect on the meaning of the word as a whole. In English, one of the more frequently mentioned phonaesthemes is a word-initial glwhich is common in words related to the visual modality (e.g., `glance&apos;, `gleam&apos;). While there have been some scholastic explorations of these non-morphological relationships between sound and meaning, they have not been thoroughly explored by behavioral and computational research (with some notable exceptions; e.g. Hutchins, 1998; Bergen, 2004). Recently, Otis and Sagi (2008) used the semantic density of the cluster of w</context>
</contexts>
<marker>Firth, 1930</marker>
<rawString>Firth, J. (1930) Speech. London: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Firth</author>
</authors>
<title>Papers in Linguistics, 1934-1951,</title>
<date>1957</date>
<publisher>University Press.</publisher>
<location>Oxford</location>
<contexts>
<context position="2663" citStr="Firth, 1957" startWordPosition="412" endWordPosition="413"> terms of our LSA-based spatial semantic model, we calculate vectors representing the context of each occurrence of a given term, and estimate the term&apos;s cohesiveness as the density with which these token context vectors are “packed” in space. 2 The method Latent Semantic Analysis (LSA) is a collective term for a family of related methods, all of which involve building numerical representations of words based on occurrence patterns in a training corpus. The basic underlying assumption is that co-occurrence within the same contexts can be used as a stand-in measure of semantic relatedness (see Firth, 1957; Halliday and Hasan, 1976; Hoey, 1991, for early articulations of this idea). The success of the method in technical applications such as information retrieval and its popularity as a research tool in psychology, education, linguistics and other disciplines suggest that this hypothesis holds up well for the purposes of those applications. The relevant notion of “context” varies. The first and still widely used implementation of the idea, developed in Information Retrieval and originally known as Latent Semantic Indexing (Deerwester et al., 1990), assembles a termdocument matrix in which each </context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>Firth, J. (1957) Papers in Linguistics, 1934-1951, Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Fodor</author>
</authors>
<title>Comprehending sentence structure. In</title>
<date>1995</date>
<journal>(Eds.), Invitation to Cognitive Science,</journal>
<volume>1</volume>
<pages>209--246</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="27893" citStr="Fodor, 1995" startWordPosition="4584" endWordPosition="4585">ical issue, namely the relationship between the statistically computed semantic space and the actual semantic content of words. On the one hand, simulations based on Latent Semantic Analysis have been shown to correlate with cognitive factors such as the acquisition of vocabulary and the categorization of texts (cf. Landauer &amp; Dumais, 1997). On the other hand, in reality speakers&apos; use of language relies on more than simple patterns of word co-occurrence – For instance, we use syntactic structures and pragmatic reasoning to supplement the meaning of the individual lexemes we come across (e.g., Fodor, 1995; Grice, 1989 [1975]). It is therefore likely that while LSA captures some of the variability in meaning exhibited by words in context, it does not capture all of it. Indeed, there is a growing body of methods that propose to integrate these two disparate sources of linguistic information (e.g., Pado and Lapata, 2007; Widdows, 2003) Certainly, the results reported in this paper suggest that enough of the meaning of words and contexts is captured to allow interesting inferences about semantic change and the relatedness of words to be drawn with a reasonable degree of certainty. However, it is p</context>
</contexts>
<marker>Fodor, 1995</marker>
<rawString>Fodor, J. D. (1995) Comprehending sentence structure. In L. R. Gleitman and M. Liberman, (Eds.), Invitation to Cognitive Science, volume 1. MIT Press, Cambridge, MA. 209-246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B W Forston</author>
</authors>
<title>An Approach to Semantic Change. In</title>
<date>2003</date>
<pages>648--666</pages>
<publisher>Blackwell Publishing.</publisher>
<location>Malden, MA:</location>
<contexts>
<context position="12057" citStr="Forston, 2003" startWordPosition="1981" endWordPosition="1982">ut is beyond the scope of the current paper. 3 Applications to Research 3.1 A Diachronic Investigation: Semantic Change One of the central questions of historical semantics is the following (Traugott, 1999):6 Given the form-meaning pair L (lexeme) what changes did meaning M undergo? For example, the form as long as underwent the change `equal in length&apos; &gt; `equal in time&apos; &gt; `provided that&apos;. Evidence for semantic change comes from written records, cognates, and structural analysis (Bloomfield, 1933). Traditional categories of semantic change include (Traugott, 2005: 2-4; Campbell, 2004:254-262; Forston, 2003: 648-650): • Broadening (generalization, extension, borrowing): A restricted meaning becomes less restricted (e.g. Late Old English docga `a (specific) powerful breed of dog&apos; &gt; dog `any member of the species Canis familiaris&apos; • Narrowing (specialization, restriction): A relatively general meaning becomes more specific (e.g. Old English deor `animal&apos; &gt; deer) • Pejoration (degeneration): A meaning becomes more negative (e.g. Old English swlig `blessed, blissful&apos; &gt; sely `happy, innocent, pitiable&apos; &gt; silly `foolish, stupid&apos;) Semantic change results from the use of language in context, whether lin</context>
</contexts>
<marker>Forston, 2003</marker>
<rawString>Forston, B. W. (2003) An Approach to Semantic Change. In B. D. Joseph and R. D. Janda (Eds.), The Handbook of Historical Linguistics. Malden, MA: Blackwell Publishing. 648-666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Logic and Conversation.</title>
<date>1989</date>
<booktitle>In Studies in the Way of Words.</booktitle>
<pages>22--40</pages>
<publisher>Harvard University Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="14399" citStr="Grice, 1989" startWordPosition="2334" endWordPosition="2335">h English (&lt;1250) (1150-1350) (1350-1500) (1500-1710) dog 112 15.47 (14.19) 24.73(10.43) do 4298 10.31(13.57) 13.02 (9.50) 24.54 (11.2) deer 61 38.72 (17.59) 20.6 (18.18) 20.5 (9.82) science 79 13.56 (13.33) 28.31 (12.24) ample, in the Invited Inferencing Model of Semantic Change proposed by Traugott and Dasher (2002) the main mechanism of semantic change is argued to be the semanticization of conversational implicatures, where conversational implicatures are a component of speaker meaning that arises from the interaction between what the speaker says and rational principles of communication (Grice, 1989 [1975]). Conversational implicatures are suggested by an utterance but not entailed. For example, the utterance Some students came to the party strongly suggests that some but not all students came to the party, even though the utterance would be true strictly speaking if all students came to the party. According to the Invited Inferencing Model, conversational implicatures become part of the semantic polysemies of particular forms over time. Such changes in meaning should be evident when examining the contexts in which the lexeme of interest appears. In other words, changes in the meaning of</context>
<context position="27906" citStr="Grice, 1989" startWordPosition="4586" endWordPosition="4587">amely the relationship between the statistically computed semantic space and the actual semantic content of words. On the one hand, simulations based on Latent Semantic Analysis have been shown to correlate with cognitive factors such as the acquisition of vocabulary and the categorization of texts (cf. Landauer &amp; Dumais, 1997). On the other hand, in reality speakers&apos; use of language relies on more than simple patterns of word co-occurrence – For instance, we use syntactic structures and pragmatic reasoning to supplement the meaning of the individual lexemes we come across (e.g., Fodor, 1995; Grice, 1989 [1975]). It is therefore likely that while LSA captures some of the variability in meaning exhibited by words in context, it does not capture all of it. Indeed, there is a growing body of methods that propose to integrate these two disparate sources of linguistic information (e.g., Pado and Lapata, 2007; Widdows, 2003) Certainly, the results reported in this paper suggest that enough of the meaning of words and contexts is captured to allow interesting inferences about semantic change and the relatedness of words to be drawn with a reasonable degree of certainty. However, it is possible that </context>
</contexts>
<marker>Grice, 1989</marker>
<rawString>Grice, H. P. (1989) [1975]. Logic and Conversation. In Studies in the Way of Words. Cambridge, MA: Harvard University Press. 22-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>R Hasan</author>
</authors>
<title>Cohesion in English.</title>
<date>1976</date>
<publisher>Longman.</publisher>
<location>London:</location>
<contexts>
<context position="2689" citStr="Halliday and Hasan, 1976" startWordPosition="414" endWordPosition="417"> LSA-based spatial semantic model, we calculate vectors representing the context of each occurrence of a given term, and estimate the term&apos;s cohesiveness as the density with which these token context vectors are “packed” in space. 2 The method Latent Semantic Analysis (LSA) is a collective term for a family of related methods, all of which involve building numerical representations of words based on occurrence patterns in a training corpus. The basic underlying assumption is that co-occurrence within the same contexts can be used as a stand-in measure of semantic relatedness (see Firth, 1957; Halliday and Hasan, 1976; Hoey, 1991, for early articulations of this idea). The success of the method in technical applications such as information retrieval and its popularity as a research tool in psychology, education, linguistics and other disciplines suggest that this hypothesis holds up well for the purposes of those applications. The relevant notion of “context” varies. The first and still widely used implementation of the idea, developed in Information Retrieval and originally known as Latent Semantic Indexing (Deerwester et al., 1990), assembles a termdocument matrix in which each vocabulary item (term) is </context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Halliday, M. A. K., &amp; Hasan, R. (1976) Cohesion in English. London: Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Hock</author>
<author>B D Joseph</author>
</authors>
<title>Language History, Language Change, and Language Relationship: An Introduction to Historical and Comparative Linguistics.</title>
<date>1996</date>
<location>Berlin: Mouton</location>
<note>de Gruyter.</note>
<contexts>
<context position="13178" citStr="Hock and Joseph, 1996" startWordPosition="2148" endWordPosition="2151"> &gt; silly `foolish, stupid&apos;) Semantic change results from the use of language in context, whether linguistic or extralinguistic. Later meanings of forms are connected to earlier ones, where all semantic change arises by polysemy, i.e. new meanings coexist with earlier ones, typically in restricted contexts. Sometimes new meanings split off from earlier ones and are no longer considered variants by language users (e.g. mistress `woman in a position of authority, head of household&apos; &gt; `woman in a continuing extra-marital relationship with a man&apos;). Semantic change is often considered unsystematic (Hock and Joseph, 1996: 252). However, recent work (Traugott and Dasher, 2002) suggests that there is, in fact, significant crosslinguistic regularity in semantic change. For ex6 This is the semasiological perspective on semantic change. Other perspectives include the onomasiological perspective (“Given the concept C, what lexemes can it be expressed by?”). See Traugott 1999 for discussion. 106 Table 1 - Mean angle between context vectors for target words in different periods in the Helsinki corpus (standard deviations are given in parenthesis) n Unknown composi- Early Middle Late Middle Early Modern tion date Engl</context>
</contexts>
<marker>Hock, Joseph, 1996</marker>
<rawString>Hock, H. H., and Joseph, B. D. (1996) Language History, Language Change, and Language Relationship: An Introduction to Historical and Comparative Linguistics. Berlin: Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hoey</author>
</authors>
<title>Patterns of Lexis in Text. London:</title>
<date>1991</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="2701" citStr="Hoey, 1991" startWordPosition="418" endWordPosition="419">c model, we calculate vectors representing the context of each occurrence of a given term, and estimate the term&apos;s cohesiveness as the density with which these token context vectors are “packed” in space. 2 The method Latent Semantic Analysis (LSA) is a collective term for a family of related methods, all of which involve building numerical representations of words based on occurrence patterns in a training corpus. The basic underlying assumption is that co-occurrence within the same contexts can be used as a stand-in measure of semantic relatedness (see Firth, 1957; Halliday and Hasan, 1976; Hoey, 1991, for early articulations of this idea). The success of the method in technical applications such as information retrieval and its popularity as a research tool in psychology, education, linguistics and other disciplines suggest that this hypothesis holds up well for the purposes of those applications. The relevant notion of “context” varies. The first and still widely used implementation of the idea, developed in Information Retrieval and originally known as Latent Semantic Indexing (Deerwester et al., 1990), assembles a termdocument matrix in which each vocabulary item (term) is associated w</context>
</contexts>
<marker>Hoey, 1991</marker>
<rawString>Hoey, M. (1991) Patterns of Lexis in Text. London: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Hutchins</author>
</authors>
<title>The psychological reality, variability, and compositionality of English phonesthemes.</title>
<date>1998</date>
<journal>Dissertation Abstracts International,</journal>
<tech>AAT 9901857).</tech>
<volume>59</volume>
<issue>08</issue>
<pages>4500</pages>
<institution>University Microfilms No.</institution>
<contexts>
<context position="21967" citStr="Hutchins, 1998" startWordPosition="3623" endWordPosition="3624">e arbitrary, there are some notable exceptions. One interesting case is that of phonaesthemes (Firth, 1930), sub-morphemic units that have a predictable effect on the meaning of the word as a whole. In English, one of the more frequently mentioned phonaesthemes is a word-initial glwhich is common in words related to the visual modality (e.g., `glance&apos;, `gleam&apos;). While there have been some scholastic explorations of these non-morphological relationships between sound and meaning, they have not been thoroughly explored by behavioral and computational research (with some notable exceptions; e.g. Hutchins, 1998; Bergen, 2004). Recently, Otis and Sagi (2008) used the semantic density of the cluster of words sharing a phonaestheme as a measure of 108 the strength of the relationship between the phonetic cluster and its proposed meaning. Otis and Sagi used a corpus derived from Project Gutenberg (http://www.gutenberg.org/) as the basis for their analysis. Specifically, they used the bulk of the English language literary works available through the project&apos;s website. This resulted in a corpus of 4034 separate documents consisting of over 290 million words. The bulk of the candidate phonaesthemes they te</context>
</contexts>
<marker>Hutchins, 1998</marker>
<rawString>Hutchins, S. S. (1998). The psychological reality, variability, and compositionality of English phonesthemes. Dissertation Abstracts International, 59(08), 4500B. (University Microfilms No. AAT 9901857).</rawString>
</citation>
<citation valid="false">
<date>2007</date>
<institution>Infomap [Computer Software].</institution>
<location>http://infomap-nlp.sourceforge.net/ Stanford, CA.</location>
<marker>2007</marker>
<rawString>Infomap [Computer Software]. (2007). http://infomap-nlp.sourceforge.net/ Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kaufmann</author>
</authors>
<title>Second-order cohesion.</title>
<date>2000</date>
<journal>Computational Intelligence.</journal>
<volume>16</volume>
<pages>511--524</pages>
<contexts>
<context position="4460" citStr="Kaufmann, 2000" startWordPosition="689" endWordPosition="690">ng their frequency of co-occurrence with each of a list of “contentbearing” words. This approach originated with the “WordSpace” paradigm developed by Schütze (1996). The software we used is a version of the “Infomap” package developed at Stanford University and freely available (see also Takayama et al., 1999). We describe it and the steps we took in our experiments in some detail below. 2.1 Word vectors The information encoded in the co-occurrence matrix, and thus ultimately the similarity measure depends greatly on the genre and subject matter of the training corpus (Takayama et al., 1999; Kaufmann, 2000). In our case, we used the entire available corpus as our training corpus. The word types in the training corpus are ranked by frequency of occurrence, and the Infomap system automatically selects (i) a vocabulary W for which vector representations are to be collected, and (ii) a set C of 1,000 “content-bearing” words whose occurrence or non-occurrence is taken to be indicative of the subject matter of a given passage of text. Usually, these choices are guided by a stoplist of (mostly closed-class) lexical items that are to be excluded, but because we were interested in tracing changes in the </context>
</contexts>
<marker>Kaufmann, 2000</marker>
<rawString>Kaufmann, S. (2000) Second-order cohesion. Computational Intelligence. 16, 511-524.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato&apos;s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<pages>211--240</pages>
<contexts>
<context position="27624" citStr="Landauer &amp; Dumais, 1997" startWordPosition="4536" endWordPosition="4539">ome to be part of a language and what factors might affect the strength of a phonaestheme. 4 Discussion While the method presented in this paper is aimed towards quantifying semantic relation109 ships that were previously difficult to quantify, it also raises an interesting theoretical issue, namely the relationship between the statistically computed semantic space and the actual semantic content of words. On the one hand, simulations based on Latent Semantic Analysis have been shown to correlate with cognitive factors such as the acquisition of vocabulary and the categorization of texts (cf. Landauer &amp; Dumais, 1997). On the other hand, in reality speakers&apos; use of language relies on more than simple patterns of word co-occurrence – For instance, we use syntactic structures and pragmatic reasoning to supplement the meaning of the individual lexemes we come across (e.g., Fodor, 1995; Grice, 1989 [1975]). It is therefore likely that while LSA captures some of the variability in meaning exhibited by words in context, it does not capture all of it. Indeed, there is a growing body of methods that propose to integrate these two disparate sources of linguistic information (e.g., Pado and Lapata, 2007; Widdows, 20</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, T. K., &amp; Dumais, S. T. (1997). A solution to Plato&apos;s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104, 211-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>M Sharifi</author>
<author>J Ball</author>
</authors>
<title>Evaluation of utility of LSA for word sense discrimination.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>77--80</pages>
<location>New York City.</location>
<contexts>
<context position="1194" citStr="Levin et al., 2006" startWordPosition="169" endWordPosition="172">tical inferences on questions such as whether the meaning of a word changed across time or if a phonetic cluster is associated with a specific meaning. Possible applications of this method are then illustrated in tracing the semantic change of `dog&apos;, `do&apos;, and `deer&apos; in early English and examining and comparing phonaesthemes. 1 Introduction The increase in available computing power over the last few decades has led to an explosion in the application of statistical methods to the analysis of texts. Researchers have applied these methods to a wide range of tasks, from word-sense disambiguation (Levin et al., 2006) to the summarization of texts (Marcu, 2003) and the automatic scoring of student essays (Riedel et al., 2006). However, some fields of linguistics that have traditionally employed corpora as their source material, such as historical semantics, have yet to benefit from the application of these statistical methods. In this paper we demonstrate how an existing statistical tool (Latent Semantic Analysis) can be adapted and used to automate and enhance some aspects of research in historical semantics and other fields whose focus is on the comparative analysis of word meanings within a corpus. Our </context>
</contexts>
<marker>Levin, Sharifi, Ball, 2006</marker>
<rawString>Levin, E., Sharifi, M., &amp; Ball, J. (2006) Evaluation of utility of LSA for word sense discrimination. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, New York City. 77-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>Automatic Abstracting,</title>
<date>2003</date>
<booktitle>Encyclopedia of Library and Information Science,</booktitle>
<pages>245--256</pages>
<editor>Drake, M. A., ed.</editor>
<contexts>
<context position="1238" citStr="Marcu, 2003" startWordPosition="179" endWordPosition="180">eaning of a word changed across time or if a phonetic cluster is associated with a specific meaning. Possible applications of this method are then illustrated in tracing the semantic change of `dog&apos;, `do&apos;, and `deer&apos; in early English and examining and comparing phonaesthemes. 1 Introduction The increase in available computing power over the last few decades has led to an explosion in the application of statistical methods to the analysis of texts. Researchers have applied these methods to a wide range of tasks, from word-sense disambiguation (Levin et al., 2006) to the summarization of texts (Marcu, 2003) and the automatic scoring of student essays (Riedel et al., 2006). However, some fields of linguistics that have traditionally employed corpora as their source material, such as historical semantics, have yet to benefit from the application of these statistical methods. In this paper we demonstrate how an existing statistical tool (Latent Semantic Analysis) can be adapted and used to automate and enhance some aspects of research in historical semantics and other fields whose focus is on the comparative analysis of word meanings within a corpus. Our method allows us to assess the semantic vari</context>
</contexts>
<marker>Marcu, 2003</marker>
<rawString>Marcu, D (2003) Automatic Abstracting, Encyclopedia of Library and Information Science, Drake, M. A., ed. 245-256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Otis</author>
<author>E Sagi</author>
</authors>
<title>Phonaesthemes: A Corpora-based Analysis. In</title>
<date>2008</date>
<booktitle>Proceedings of the 30th Annual Conference of the Cognitive Science Society.</booktitle>
<publisher>Cognitive Science Society.</publisher>
<location>Austin, TX:</location>
<contexts>
<context position="22014" citStr="Otis and Sagi (2008)" startWordPosition="3628" endWordPosition="3631">tions. One interesting case is that of phonaesthemes (Firth, 1930), sub-morphemic units that have a predictable effect on the meaning of the word as a whole. In English, one of the more frequently mentioned phonaesthemes is a word-initial glwhich is common in words related to the visual modality (e.g., `glance&apos;, `gleam&apos;). While there have been some scholastic explorations of these non-morphological relationships between sound and meaning, they have not been thoroughly explored by behavioral and computational research (with some notable exceptions; e.g. Hutchins, 1998; Bergen, 2004). Recently, Otis and Sagi (2008) used the semantic density of the cluster of words sharing a phonaestheme as a measure of 108 the strength of the relationship between the phonetic cluster and its proposed meaning. Otis and Sagi used a corpus derived from Project Gutenberg (http://www.gutenberg.org/) as the basis for their analysis. Specifically, they used the bulk of the English language literary works available through the project&apos;s website. This resulted in a corpus of 4034 separate documents consisting of over 290 million words. The bulk of the candidate phonaesthemes they tested were taken from the list used by Hutchins </context>
</contexts>
<marker>Otis, Sagi, 2008</marker>
<rawString>Otis K., &amp; Sagi E. (2008) Phonaesthemes: A Corpora-based Analysis. In B. C. Love, K. McRae, &amp; V. M. Sloutsky (Eds.), Proceedings of the 30th Annual Conference of the Cognitive Science Society. Austin, TX: Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pado</author>
<author>M Lapata</author>
</authors>
<title>Dependency-based Construction of Semantic Space Models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<pages>161--199</pages>
<contexts>
<context position="28211" citStr="Pado and Lapata, 2007" startWordPosition="4637" endWordPosition="4640">exts (cf. Landauer &amp; Dumais, 1997). On the other hand, in reality speakers&apos; use of language relies on more than simple patterns of word co-occurrence – For instance, we use syntactic structures and pragmatic reasoning to supplement the meaning of the individual lexemes we come across (e.g., Fodor, 1995; Grice, 1989 [1975]). It is therefore likely that while LSA captures some of the variability in meaning exhibited by words in context, it does not capture all of it. Indeed, there is a growing body of methods that propose to integrate these two disparate sources of linguistic information (e.g., Pado and Lapata, 2007; Widdows, 2003) Certainly, the results reported in this paper suggest that enough of the meaning of words and contexts is captured to allow interesting inferences about semantic change and the relatedness of words to be drawn with a reasonable degree of certainty. However, it is possible that some important aspects of meaning are systematically ignored by the analysis. For instance, it remains to be seen whether this method can distinguish between processes like pejoration and amerlioration as they require a fine grained distinction between `good&apos; and `bad&apos; meanings. Regardless of any such li</context>
</contexts>
<marker>Pado, Lapata, 2007</marker>
<rawString>Pado, S. &amp; Lapata, M. (2007) Dependency-based Construction of Semantic Space Models. Computational Linguistics, 33, 161-199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riedel</author>
<author>S L Dexter</author>
<author>C Scharber</author>
<author>A Doering</author>
</authors>
<title>Experimental Evidence on the Effectiveness of Automated Essay Scoring in Teacher Education Cases.</title>
<date>2006</date>
<journal>Journal of Educational Computing Research,</journal>
<volume>35</volume>
<pages>267--287</pages>
<contexts>
<context position="1304" citStr="Riedel et al., 2006" startWordPosition="189" endWordPosition="192">ter is associated with a specific meaning. Possible applications of this method are then illustrated in tracing the semantic change of `dog&apos;, `do&apos;, and `deer&apos; in early English and examining and comparing phonaesthemes. 1 Introduction The increase in available computing power over the last few decades has led to an explosion in the application of statistical methods to the analysis of texts. Researchers have applied these methods to a wide range of tasks, from word-sense disambiguation (Levin et al., 2006) to the summarization of texts (Marcu, 2003) and the automatic scoring of student essays (Riedel et al., 2006). However, some fields of linguistics that have traditionally employed corpora as their source material, such as historical semantics, have yet to benefit from the application of these statistical methods. In this paper we demonstrate how an existing statistical tool (Latent Semantic Analysis) can be adapted and used to automate and enhance some aspects of research in historical semantics and other fields whose focus is on the comparative analysis of word meanings within a corpus. Our method allows us to assess the semantic variation within the set of individual occurrences of a given word typ</context>
</contexts>
<marker>Riedel, Dexter, Scharber, Doering, 2006</marker>
<rawString>Riedel E., Dexter S. L., Scharber C., Doering A. (2006) Experimental Evidence on the Effectiveness of Automated Essay Scoring in Teacher Education Cases. Journal of Educational Computing Research, 35, 267-287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rissanen</author>
</authors>
<title>The Helsinki Corpus of English Texts. In</title>
<date>1994</date>
<booktitle>Corpora Across the Centuries: Proceedings of the First International Colloquium on English Diachronic Corpora.</booktitle>
<location>Amsterdam: Rodopi.</location>
<contexts>
<context position="17355" citStr="Rissanen, 1994" startWordPosition="2846" endWordPosition="2847">therefore applicable in fewer contexts than before. This decrease in variety results in an increase in vector density and can be directly measured as a decrease in the average angle between the context vectors for the word. As an example, the Old English word `deor&apos; denoted a larger group of living creatures than does the Modern English word `deer&apos;. We therefore predicted that earlier occurrences of the lexemes `deor&apos; and `deer&apos;, in a corpus of the appropriate time period, will show more variety than later occurrences. We tested our predictions using a corpus derived from the Helsinki corpus (Rissanen, 1994). The Helsinki corpus is comprised of texts spanning the periods of Old English (prior to 1150A.D.), Middle English (1150-1500A.D.), and Early Modern English (1500-1710A.D.). Because spelling in Old English was highly variable, we decided to exclude that part of the corpus and focused our analysis on the Middle English and Early Modern English periods. The resulting corpus included 504 distinct documents totaling approximately 1.1 million words. To test our predictions regarding semantic change in the words `dog&apos;, `do&apos;, and `deer&apos;, we collected all of the contexts in which they appear in our s</context>
</contexts>
<marker>Rissanen, 1994</marker>
<rawString>Rissanen, M. (1994) The Helsinki Corpus of English Texts. In Kytö, M., Rissanen, M. and Wright S. (eds), Corpora Across the Centuries: Proceedings of the First International Colloquium on English Diachronic Corpora. Amsterdam: Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Schi tze</author>
<author>H</author>
</authors>
<title>Ambiguity in language learning: computational and cognitive models.</title>
<date>1996</date>
<publisher>CA: Stanford.</publisher>
<marker>tze, H, 1996</marker>
<rawString>Schi tze, H. (1996) Ambiguity in language learning: computational and cognitive models. CA: Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Schi tze</author>
<author>H</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<pages>24--1</pages>
<marker>tze, H, 1998</marker>
<rawString>Schi tze, H. (1998) Automatic word sense discrimination. Computational Linguistics 24(1):97-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Takayama</author>
<author>R Flournoy</author>
<author>S Kaufmann</author>
</authors>
<title>Information Mapping: Concept-Based Information Retrieval Based on Word Associations.</title>
<date>1998</date>
<tech>CSLI Tech Report. CA: Stanford.</tech>
<contexts>
<context position="8424" citStr="Takayama, et al. 1998" startWordPosition="1363" endWordPosition="1366"> a matter of grouping these data points according to some criterion (e.g., the period in which the text was written) and conducting an appropriate statistical test. In some cases it might also be possible to use regression or apply a clustering analysis. 2.3 Semantic Density Analysis Conducting statistical tests comparing groups of vectors is not trivial. Fortunately, some questions can be answered based on the similarity of vectors within each group rather than the vectors themselves. The similarity between two vectors w , v is measured as the cosine between them:2 high base frequencies (cf. Takayama, et al. 1998; Widdows, 2004). 2 While the cosine measure is the accepted measure of similarity, the cosine function is non-linear and therefore problematic for many statistical methods. Several transformations can be used to correct this (e.g., Fisher‟s z). In this paper we will use the angle, in degrees, between the two vectors (i.e., cos−1) because it is easily interpretable. 105 cos(W , V ) = W• V the angle between two context vectors the angle between the documents in which they appear. WV The average similarity of a group of vectors is indicative of its density – a dense group of highly similar vecto</context>
</contexts>
<marker>Takayama, Flournoy, Kaufmann, 1998</marker>
<rawString>Takayama, Y., Flournoy R., &amp; Kaufmann, S. (1998) Information Mapping: Concept-Based Information Retrieval Based on Word Associations. CSLI Tech Report. CA: Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Takayama</author>
<author>R Flournoy</author>
<author>S Kaufmann</author>
<author>S Peters</author>
</authors>
<title>Information retrieval based on domain-specific word associations.</title>
<date>1999</date>
<booktitle>Proceedings of the Pacific Association for Computational Linguistics (PACLING’99),</booktitle>
<pages>155--161</pages>
<editor>In Cercone, N. and Naruedomkul K. (eds.),</editor>
<location>Waterloo, Canada.</location>
<contexts>
<context position="4157" citStr="Takayama et al., 1999" startWordPosition="637" endWordPosition="640">ts in the training corpus, building inProceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 104–111, Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics 104 stead a term-term matrix associating vocabulary items with vectors representing their frequency of co-occurrence with each of a list of “contentbearing” words. This approach originated with the “WordSpace” paradigm developed by Schütze (1996). The software we used is a version of the “Infomap” package developed at Stanford University and freely available (see also Takayama et al., 1999). We describe it and the steps we took in our experiments in some detail below. 2.1 Word vectors The information encoded in the co-occurrence matrix, and thus ultimately the similarity measure depends greatly on the genre and subject matter of the training corpus (Takayama et al., 1999; Kaufmann, 2000). In our case, we used the entire available corpus as our training corpus. The word types in the training corpus are ranked by frequency of occurrence, and the Infomap system automatically selects (i) a vocabulary W for which vector representations are to be collected, and (ii) a set C of 1,000 “</context>
</contexts>
<marker>Takayama, Flournoy, Kaufmann, Peters, 1999</marker>
<rawString>Takayama, Y., Flournoy, R., Kaufmann, S. &amp; Peters, S. (1999). Information retrieval based on domain-specific word associations. In Cercone, N. and Naruedomkul K. (eds.), Proceedings of the Pacific Association for Computational Linguistics (PACLING’99), Waterloo, Canada. 155-161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E C Traugott</author>
</authors>
<title>The Role of Pragmatics in Semantic Change.</title>
<date>1999</date>
<booktitle>Pragmatics in 1998: Selected Papers from the 6th International Pragmatics Conference, vol. II. Antwerp: International Pragmatics Association.</booktitle>
<pages>93--102</pages>
<editor>In J. Verschueren (ed.),</editor>
<contexts>
<context position="11650" citStr="Traugott, 1999" startWordPosition="1921" endWordPosition="1922">rget word, hence the average cosine usually does not fall below zero. 4 It is important to note that the number of independent samples in the analysis is determined not by the number of similarity values compared but by the number of individual vectors used in the analysis. 5 Tracking changes in the distribution of the document vectors in a corpus over time might itself be of interest to some researchers but is beyond the scope of the current paper. 3 Applications to Research 3.1 A Diachronic Investigation: Semantic Change One of the central questions of historical semantics is the following (Traugott, 1999):6 Given the form-meaning pair L (lexeme) what changes did meaning M undergo? For example, the form as long as underwent the change `equal in length&apos; &gt; `equal in time&apos; &gt; `provided that&apos;. Evidence for semantic change comes from written records, cognates, and structural analysis (Bloomfield, 1933). Traditional categories of semantic change include (Traugott, 2005: 2-4; Campbell, 2004:254-262; Forston, 2003: 648-650): • Broadening (generalization, extension, borrowing): A restricted meaning becomes less restricted (e.g. Late Old English docga `a (specific) powerful breed of dog&apos; &gt; dog `any member</context>
<context position="13533" citStr="Traugott 1999" startWordPosition="2203" endWordPosition="2204">are no longer considered variants by language users (e.g. mistress `woman in a position of authority, head of household&apos; &gt; `woman in a continuing extra-marital relationship with a man&apos;). Semantic change is often considered unsystematic (Hock and Joseph, 1996: 252). However, recent work (Traugott and Dasher, 2002) suggests that there is, in fact, significant crosslinguistic regularity in semantic change. For ex6 This is the semasiological perspective on semantic change. Other perspectives include the onomasiological perspective (“Given the concept C, what lexemes can it be expressed by?”). See Traugott 1999 for discussion. 106 Table 1 - Mean angle between context vectors for target words in different periods in the Helsinki corpus (standard deviations are given in parenthesis) n Unknown composi- Early Middle Late Middle Early Modern tion date English English English (&lt;1250) (1150-1350) (1350-1500) (1500-1710) dog 112 15.47 (14.19) 24.73(10.43) do 4298 10.31(13.57) 13.02 (9.50) 24.54 (11.2) deer 61 38.72 (17.59) 20.6 (18.18) 20.5 (9.82) science 79 13.56 (13.33) 28.31 (12.24) ample, in the Invited Inferencing Model of Semantic Change proposed by Traugott and Dasher (2002) the main mechanism of sem</context>
</contexts>
<marker>Traugott, 1999</marker>
<rawString>Traugott, E. C. (1999) The Role of Pragmatics in Semantic Change. In J. Verschueren (ed.), Pragmatics in 1998: Selected Papers from the 6th International Pragmatics Conference, vol. II. Antwerp: International Pragmatics Association. 93-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E C Traugott</author>
</authors>
<title>Semantic Change.</title>
<date>2005</date>
<booktitle>In Encyclopedia of Language and Linguistics, 2nd</booktitle>
<editor>ed., Brown K. ed.</editor>
<publisher>Elsevier.</publisher>
<location>Oxford:</location>
<contexts>
<context position="12013" citStr="Traugott, 2005" startWordPosition="1976" endWordPosition="1977">t itself be of interest to some researchers but is beyond the scope of the current paper. 3 Applications to Research 3.1 A Diachronic Investigation: Semantic Change One of the central questions of historical semantics is the following (Traugott, 1999):6 Given the form-meaning pair L (lexeme) what changes did meaning M undergo? For example, the form as long as underwent the change `equal in length&apos; &gt; `equal in time&apos; &gt; `provided that&apos;. Evidence for semantic change comes from written records, cognates, and structural analysis (Bloomfield, 1933). Traditional categories of semantic change include (Traugott, 2005: 2-4; Campbell, 2004:254-262; Forston, 2003: 648-650): • Broadening (generalization, extension, borrowing): A restricted meaning becomes less restricted (e.g. Late Old English docga `a (specific) powerful breed of dog&apos; &gt; dog `any member of the species Canis familiaris&apos; • Narrowing (specialization, restriction): A relatively general meaning becomes more specific (e.g. Old English deor `animal&apos; &gt; deer) • Pejoration (degeneration): A meaning becomes more negative (e.g. Old English swlig `blessed, blissful&apos; &gt; sely `happy, innocent, pitiable&apos; &gt; silly `foolish, stupid&apos;) Semantic change results from</context>
</contexts>
<marker>Traugott, 2005</marker>
<rawString>Traugott, E. C. (2005) Semantic Change. In Encyclopedia of Language and Linguistics, 2nd ed., Brown K. ed. Oxford: Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E C Traugott</author>
<author>R B Dasher</author>
</authors>
<title>Regularity in Semantic Change. Cambridge:</title>
<date>2002</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="13234" citStr="Traugott and Dasher, 2002" startWordPosition="2156" endWordPosition="2159"> from the use of language in context, whether linguistic or extralinguistic. Later meanings of forms are connected to earlier ones, where all semantic change arises by polysemy, i.e. new meanings coexist with earlier ones, typically in restricted contexts. Sometimes new meanings split off from earlier ones and are no longer considered variants by language users (e.g. mistress `woman in a position of authority, head of household&apos; &gt; `woman in a continuing extra-marital relationship with a man&apos;). Semantic change is often considered unsystematic (Hock and Joseph, 1996: 252). However, recent work (Traugott and Dasher, 2002) suggests that there is, in fact, significant crosslinguistic regularity in semantic change. For ex6 This is the semasiological perspective on semantic change. Other perspectives include the onomasiological perspective (“Given the concept C, what lexemes can it be expressed by?”). See Traugott 1999 for discussion. 106 Table 1 - Mean angle between context vectors for target words in different periods in the Helsinki corpus (standard deviations are given in parenthesis) n Unknown composi- Early Middle Late Middle Early Modern tion date English English English (&lt;1250) (1150-1350) (1350-1500) (150</context>
</contexts>
<marker>Traugott, Dasher, 2002</marker>
<rawString>Traugott, E. C., and Dasher R. B. (2002) Regularity in Semantic Change. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
</authors>
<title>Unsupervised methods for developing taxonomies by combining syntactic and statistical information.</title>
<date>2003</date>
<booktitle>In Proceedings of the joint Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<location>Edmonton, Canada: Wiemer-Hastings.</location>
<contexts>
<context position="28227" citStr="Widdows, 2003" startWordPosition="4641" endWordPosition="4642">mais, 1997). On the other hand, in reality speakers&apos; use of language relies on more than simple patterns of word co-occurrence – For instance, we use syntactic structures and pragmatic reasoning to supplement the meaning of the individual lexemes we come across (e.g., Fodor, 1995; Grice, 1989 [1975]). It is therefore likely that while LSA captures some of the variability in meaning exhibited by words in context, it does not capture all of it. Indeed, there is a growing body of methods that propose to integrate these two disparate sources of linguistic information (e.g., Pado and Lapata, 2007; Widdows, 2003) Certainly, the results reported in this paper suggest that enough of the meaning of words and contexts is captured to allow interesting inferences about semantic change and the relatedness of words to be drawn with a reasonable degree of certainty. However, it is possible that some important aspects of meaning are systematically ignored by the analysis. For instance, it remains to be seen whether this method can distinguish between processes like pejoration and amerlioration as they require a fine grained distinction between `good&apos; and `bad&apos; meanings. Regardless of any such limitations, it is</context>
</contexts>
<marker>Widdows, 2003</marker>
<rawString>Widdows, D. (2003) Unsupervised methods for developing taxonomies by combining syntactic and statistical information. In Proceedings of the joint Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics. Edmonton, Canada: Wiemer-Hastings. 197-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
</authors>
<title>Geometry and Meaning.</title>
<date>2004</date>
<publisher>CSLI Publications, CA: Stanford.</publisher>
<contexts>
<context position="8440" citStr="Widdows, 2004" startWordPosition="1367" endWordPosition="1368">hese data points according to some criterion (e.g., the period in which the text was written) and conducting an appropriate statistical test. In some cases it might also be possible to use regression or apply a clustering analysis. 2.3 Semantic Density Analysis Conducting statistical tests comparing groups of vectors is not trivial. Fortunately, some questions can be answered based on the similarity of vectors within each group rather than the vectors themselves. The similarity between two vectors w , v is measured as the cosine between them:2 high base frequencies (cf. Takayama, et al. 1998; Widdows, 2004). 2 While the cosine measure is the accepted measure of similarity, the cosine function is non-linear and therefore problematic for many statistical methods. Several transformations can be used to correct this (e.g., Fisher‟s z). In this paper we will use the angle, in degrees, between the two vectors (i.e., cos−1) because it is easily interpretable. 105 cos(W , V ) = W• V the angle between two context vectors the angle between the documents in which they appear. WV The average similarity of a group of vectors is indicative of its density – a dense group of highly similar vectors will have a h</context>
</contexts>
<marker>Widdows, 2004</marker>
<rawString>Widdows, D. (2004) Geometry and Meaning. CSLI Publications, CA: Stanford.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>