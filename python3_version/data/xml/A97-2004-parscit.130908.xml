<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.285309">
<title confidence="0.9876175">
Duke&apos;s Trainable Information and Meaning Extraction System
(Duke TIMES) *
</title>
<author confidence="0.892369">
Amit Bagga
Joyce Yue Chai
</author>
<affiliation confidence="0.99979">
Department of Computer Science
</affiliation>
<address confidence="0.9244735">
Box 90129, Duke University
Durham, NC 27708-0129
</address>
<email confidence="0.993409">
Internet: {amit, chai}@cs.duke.edu
</email>
<sectionHeader confidence="0.995909" genericHeader="abstract">
1 Introduction and Background
</sectionHeader>
<bodyText confidence="0.999262333333333">
The explosion in the amount of free text materials
on the Internet, and the use of this information by
people from all walks of life, has made the issue of
generalized information extraction a central one in
Natural Language Processing. Many systems includ-
ing ones from NYU, BBN, SRI, SRA, and MITRE
have taken steps to make the process of customizing
a system for a particular domain an easy one.
We have built a system that attempts to provide
any user with the ability to efficiently create and
customize, for his or her own application, an infor-
mation extraction system with competitive precision
and recall statistics.
More details about the system can be found in
(Bagga, 1997).
</bodyText>
<figure confidence="0.991029727272727">
Training Article
Tolcenizer,
Preprocessor &amp; Partial Parser WolNet
v.(
Rule Generalization Routines
New Article
Training Interface
&apos;a
Eti
Specific Rules
C.,1y11_11Net____&gt;
Generalized Rule--
IBM &amp; Local Dim
WordNet
Rule Matching Routines
Generalization
anning Process
Sense Classifier
Tokenizer,
Preprocessor &amp; Partial Parser
IBM 8e Local Dict
Semantic Network
</figure>
<sectionHeader confidence="0.921016" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.999942666666667">
As illustrated in Figure 1, there are three main
stages in the running of the system: the Training
Process, Rule Generalization, and the Scanning Pro-
cess. During the Training Process, the user, with the
help of a graphical user interface, takes a few pro-
totypical articles from the domain that the system
is being trained on, and creates rules (patterns) for
the target information contained in the training arti-
cles. These rules are specific to the training articles
and they are generalized so that they can be run on
other articles from the domain. The Rule General-
ization routines, with the help of WordNet 1 (Miller,
1990), generalize the specific rules generated by the
Training Process. The system can now be run on a
large number of articles from the domain (Scanning
Process). The output of the Scanning Process, for
each article, is a semantic network for that article
which can then be used by a Postprocessor to fill
</bodyText>
<footnote confidence="0.992447666666667">
Supported by Fellowships from IBM Corporation.
&apos;WordNet is an on-line lexical reference system de-
veloped by George Miller at Princeton University.
</footnote>
<figureCaption confidence="0.999984">
Figure 1: The Architecture
</figureCaption>
<bodyText confidence="0.968583">
templates, answer queries, of generate abstracts.
</bodyText>
<subsectionHeader confidence="0.996944">
2.1 Tools Used By the System
</subsectionHeader>
<bodyText confidence="0.999857166666667">
In addition to WordNet, the system uses IBM&apos;s
LanguageWare English Dictionary, IBM&apos;s Comput-
ing Terms Dictionary, and a local dictionary of our
choice. The system also uses a gazetteer consist-
ing of approximately 250 names of cities, states, and
countries.
</bodyText>
<subsectionHeader confidence="0.9952695">
2.2 The Tokenizer, the Preprocessor, and
the Partial Parser
</subsectionHeader>
<bodyText confidence="0.999967285714286">
The Tokenizer accepts ASCII characters as input
and produces a stream of tokens (words) as output.
It also determines sentence boundaries.
The preprocessor tries to identify some important
entities like names of companies, proper names, etc.
contained in the article. Groups of words that com-
prise these entities are collected together and con-
</bodyText>
<page confidence="0.997221">
7
</page>
<bodyText confidence="0.999837666666667">
sidered as one item for all future processing.
The Partial Parser produces a sequence of non-
overlapping phrases as output. The headword of
each phrase is also identified. The parser recognizes
noun groups, verb groups and preposition groups 2
(Hobbs, 1993).
</bodyText>
<subsectionHeader confidence="0.999118">
2.3 The Training Interface
</subsectionHeader>
<bodyText confidence="0.9985014">
There are two parts to the Training Process: identi-
fication of the (WordNet) sense usage of headwords
of interest, and the building of specific rules. Train-
ing is done by a user with the help of a graphical
user Training Interface.
</bodyText>
<sectionHeader confidence="0.997371" genericHeader="method">
3 Generalization
</sectionHeader>
<bodyText confidence="0.99746925">
Rules created as a result of the Training Process
are very specific and can only be applied to exactly
the same patterns as the ones present during the
training. Generalization consists of replacing each
concept in a rule by a more generalized concept (ob-
tained from WordNet). Figure 2 shows the different
degrees of generalization of the concept &amp;quot;IBM Cor-
poration.&amp;quot;
</bodyText>
<equation confidence="0.907512222222222">
sp = (IBM Corporation, NG, 1, company)
1 generalized at degree 1
Generalize(sp, 1) = (business, concern)
1, generalized at degree 2
Generalize(sp, 2) = (enterprise)
1, generalized at degree 3
Generalize(sp, 3) = (organization)
1 generalized at degree 5
Generalize(sp, 5) = (group, social group)
</equation>
<figureCaption confidence="0.997036">
Figure 2: Degrees of Generalization
</figureCaption>
<sectionHeader confidence="0.996252" genericHeader="conclusions">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999899769230769">
We designed an experiment to investigate how train-
ing and the generalization strategy affect meaning
extraction. We trained our system on three sets of
articles from the triangle.jobs USENET newsgroup,
with emphasis on the following seven facts: Com-
pany Name, Position/Title, Experience/Skill, Loca-
tion, Benefit, Salary, and Contact Information.
The first training set contained 8 articles; the
second set contained 16 articles including the first
set; and the third set contained 24 articles includ-
ing those in the first two sets. For rules from each
training set, seven levels of generalization were per-
formed. Based on the generalized rules at each level,
</bodyText>
<footnote confidence="0.991538">
2We wish to thank Jerry Hobbs of SRI for providing
us with the finite-state rules for the parser.
</footnote>
<figureCaption confidence="0.9999545">
Figure 3: Precision vs. Degree of Generalization
Figure 4: Recall vs. Degree of Generalization
</figureCaption>
<bodyText confidence="0.9974655">
the system was run on 80 unseen articles from the
same newsgroup to test its performance on the ex-
traction of the seven facts.
The precision and recall curves with respect to the
degree of generalization are shown in Figure 3 and
Figure 4 respectively.
</bodyText>
<sectionHeader confidence="0.99182" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9796483">
Bagga, Amit, and Joyce Y. Chai. 1997. A Trainable
Message Understanding System, Submitted to the
Fifteenth International Joint Conference on Arti-
ficial Intelligence (IJCAI&apos;97).
Hobbs, J., et al. 1995. FASTUS: A system for Ex-
tracting Information from Text, Human Language
Technology, pp. 133-137, 1993.
Miller, G.A., et al. 1990. Five Papers on WordNet,
Cognitive Science Laboratory, Princeton Univer-
sity, No. 43, July 1990.
</reference>
<figure confidence="0.996684465116279">
100
95
90
85
80
75
70
65
• 60
O 55
O 50
22
40
30
20
10
0 o
8 train-arts —
16 train-arts
4 train-arts
2 3 4 5 6
degree of generalization
8 train-arts
16 train-ads
24 train-arts
2 3 4 5 6
degree of generalization
fE 100
95
90
85
80
75
70
65
60
55
50
40
30
20
10
0 o
</figure>
<page confidence="0.955503">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.646410">
<title confidence="0.999656">Duke&apos;s Trainable Information and Meaning Extraction System</title>
<author confidence="0.938608">Amit Bagga Joyce Yue Chai</author>
<affiliation confidence="0.999972">Department of Computer Science</affiliation>
<address confidence="0.9234695">Box 90129, Duke University Durham, NC 27708-0129</address>
<email confidence="0.891604">Internet:{amit,chai}@cs.duke.edu</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Joyce Y Chai</author>
</authors>
<title>A Trainable Message Understanding System,</title>
<date>1997</date>
<booktitle>Submitted to the Fifteenth International Joint Conference on Artificial Intelligence (IJCAI&apos;97).</booktitle>
<marker>Bagga, Chai, 1997</marker>
<rawString>Bagga, Amit, and Joyce Y. Chai. 1997. A Trainable Message Understanding System, Submitted to the Fifteenth International Joint Conference on Artificial Intelligence (IJCAI&apos;97).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>FASTUS: A system for Extracting Information from Text, Human Language Technology,</title>
<date>1995</date>
<pages>133--137</pages>
<marker>Hobbs, 1995</marker>
<rawString>Hobbs, J., et al. 1995. FASTUS: A system for Extracting Information from Text, Human Language Technology, pp. 133-137, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>Five Papers on WordNet,</title>
<date>1990</date>
<volume>43</volume>
<institution>Cognitive Science Laboratory, Princeton University,</institution>
<contexts>
<context position="1939" citStr="Miller, 1990" startWordPosition="305" endWordPosition="306"> in Figure 1, there are three main stages in the running of the system: the Training Process, Rule Generalization, and the Scanning Process. During the Training Process, the user, with the help of a graphical user interface, takes a few prototypical articles from the domain that the system is being trained on, and creates rules (patterns) for the target information contained in the training articles. These rules are specific to the training articles and they are generalized so that they can be run on other articles from the domain. The Rule Generalization routines, with the help of WordNet 1 (Miller, 1990), generalize the specific rules generated by the Training Process. The system can now be run on a large number of articles from the domain (Scanning Process). The output of the Scanning Process, for each article, is a semantic network for that article which can then be used by a Postprocessor to fill Supported by Fellowships from IBM Corporation. &apos;WordNet is an on-line lexical reference system developed by George Miller at Princeton University. Figure 1: The Architecture templates, answer queries, of generate abstracts. 2.1 Tools Used By the System In addition to WordNet, the system uses IBM&apos;s</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>Miller, G.A., et al. 1990. Five Papers on WordNet, Cognitive Science Laboratory, Princeton University, No. 43, July 1990.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>