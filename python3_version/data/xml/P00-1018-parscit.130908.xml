<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000072">
<title confidence="0.9908005">
Distributing Representation for Robust Interpretation of
Dialogue Utterances
</title>
<author confidence="0.942628">
David Milward
</author>
<affiliation confidence="0.860713">
SRI International Cambridge Computer Science Research Centre
</affiliation>
<address confidence="0.8843295">
23 Millers Yard. Mill Lane
Cambridge. CB2 1R.Q. G.B.
</address>
<email confidence="0.893333">
milwardAcam.sri.com
</email>
<sectionHeader confidence="0.976558" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999081">
A syntax tree or standard semantic
representation can be represented as
a set of indexed constraints. This
paper describes how this idea can be
used in task oriented dialogue sys-
tems to provide interpretation rules
which incorporate structural and
contextual constraints where avail-
able, and degrade gracefully on un-
grammatical input.
</bodyText>
<sectionHeader confidence="0.99379" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995646875">
This paper applies the idea of distributed
representation to the interpretation of dia-
logue utterances. The approach provides
the robustness you might expect from key-
word/phrase spotting, with the ability to use
higher level structural information where this
can provide a more accurate analysis. It is
particularly aimed at flexible dialogue sys-
tems where user utterances may be long and
not necessarily grammatical. In this situation
the system needs to extract out as much rele-
vant information as possible even if it cannot
provide a grammatical analysis for the utter-
ance as a whole.
As an example task we chose the route
planning domain where we already had two
existing systems, one based on full seman-
tic analysis, and one on phrase spotting. All
three systems allow flexible dialogues where
the user can contribute more (or different) in-
formation than that directly requested e.g.
S: Where would you like to go?
U: I would like the shortest route
from Cambridge to London
S: When would you like to arrive?
U: I would like to leave at 5
The paper motivates the use of distributed
representations, and suggests one possibility.
a &apos;semantic chart&apos;. It then describes how this
kind of representation can be mapped into
something more suitable for the task (in this
case a slot filler notation). The mapping rules
use information from multiple sources includ-
ing the dialogue history, the previous utter-
ance and other parts of the chart. Distinc-
tive features of the approach are discussed,
and an implementation is described and eval-
uated. The paper provides a more concrete
instantiation of some of the ideas presented
in (Milward. 1999).
</bodyText>
<sectionHeader confidence="0.972428" genericHeader="introduction">
2 Distributed Representation
</sectionHeader>
<bodyText confidence="0.997823888888889">
By using and exploiting distributed represen-
tation schemes, we can provide algorithms
that are less liable to failure in the face of
missing information. The classic (though
largely outdated) contrast is between NL al-
gorithms which rely on finding a fully con-
nected syntax tree for each sentence. and In-
formation Retrieval style approaches that rep-
resent meaning as a bag of content words.
Representing the content of dialogue utter-
ances as bags of words (even including func-
tion words) loses too much information e.g.
&amp;quot;from Boston to London Heathrow&amp;quot; and &amp;quot;to
Boston from London Heathrow&amp;quot; get the same
representation.
to. from. Boston; London. Heathrow
The next level is a bag of constraints encoding
the original information in the surface string
</bodyText>
<figure confidence="0.989068833333333">
PPi6 PPio
/ /NN
Pil NPi2 P13 NP17
from Boston to
PNi4 PNi5
London Heathrow
</figure>
<figureCaption confidence="0.999984">
Figure 1: Indexed Tree Structures
</figureCaption>
<bodyText confidence="0.986793">
by indexing each word and providing linear
order constraints e.g.
il:from. i2:Boston. i3:to.
i4:I,ondon, i5:Heathrow,
i1&lt;i2; i2&lt;i3; i3&lt;i4;i4&lt;i5
The representations for from Boston to Lon-
don Heathrow&amp;quot; and &amp;quot;to Boston from London
Heathrow&amp;quot; now differ, however, the represen-
tation is not particularly convenient for pick-
ing out information required by a dialogue
system. To find the destination we want the
noun phrase argument. London Heathrow.
This suggests we need a representation en-
coding structural information not just linear
order. Structural information can be included
by adding extra constraints. For the case
above; we can create a node, VT; corresponding
to the phrase &amp;quot;London Heathrow&amp;quot; and state
that this dominates both i4 and i5. To see
how this can be done systematically, consider
the pair of syntax trees for &amp;quot;from Boston to
London Heathrow&amp;quot; in Figure 1 in which each
node is uniquely indexed.
The following constraints encode the brack-
eting:
ii :from, i2:Boston, i3:to,
i4:London, i5:Heathrow,
i8: (i3;i7). i7: (i4;i5)
To encode the full tree, including the syntac-
tic labels we would also need to assert the
category for each index e.g. cat(i6; PH). The
set of constraints can be considered as an al-
ternative representation of the tree, or as a
description of the tree c.f. D-Theory (Marcus
et al., 1983).
As an alternative to working with a syntax
tree we can use a semantic representation. In
this case, adding indices to the predicate ar-
gument structures gives:
</bodyText>
<equation confidence="0.703275833333333">
fromij (Bostoni2)i6
t0i3 ( London _Heat hrowi7),i8
The equivalent constraints would be:
ii :from, i2:Boston, i3:to,
i7:London_Heathrow.
i6:il (i2), i8:i3(i7)
</equation>
<bodyText confidence="0.96444904950495">
This provides the lexemes, from, to, Boston
and London_Heathrow, along with constraints
specifying their relationship. Note that we
have only changed how we represent the se-
mantics: there is a one-to-one mapping be-
tween the set of constraints and the origi-
nal recursive representation (assuming index
renumbering). The representation is thus
closer in spirit to the use of labelling in under-
specified semantics e.g. (Iteyle, 1993), (Fgg et
al., 1998) than event based approaches in the
tradition of Davidson e.g. (Hobbs. 1983).
In spoken dialogue systems we may be
starting with a word lattice. and have an
ambiguous grammar. It is then convenient
to use the indices to pack ambiguity. The
same index can be used more than once to
give alternative readings (i.e. meta-level dis-
junction). For example, i4:P i4:Q is taken to
mean that i4 has the two readings, I or Q1.
If the recogniser hypothesised either &amp;quot;Boston&amp;quot;
or &amp;quot;Bolton&amp;quot; in the case above we get:
&apos;Note that in Minimal Recursion Semantics
(Copestake et al.. 1995). there is a similar ability to
use the same index more than once. hut the inter-
pretation is rather different. In MRS. i4:P. i4:Q is
equivalent to conjoining P arid Q. similar to hang-
il:from. i2:Boston, i2:Bolton, i3:to,
i7:London_Heathrow,
i6:il(i2). i8:i3(i7)
This representation can be obtained using in-
dices corresponding to edges in a chart or lat-
tice (this exploits the context free assumption
that any two readings of the same span of
utterance which have the same category can
be interchanged). The result for our example
is as follows, numbering spans according to
word counts:
0-1-p:from. 1-2-np:Boston.
1-2-np:Bolton, 2-3-p:to,
3-5-nplondon_Heathrow.
0-2-pp:0-1-p(1-2-np).
2-5-pp:2-3-p (3-5-np)
We have ended up with a &apos;semantic chart&apos;2.
This should not be surprising. Although
a chart is not always thought of as a dis-
tributed representation. its distributed nature
is what allows packing to occur (representa-
tions are split up so that bits in common can
be shared).
To generate semantic charts we use an in-
cremental parser based on Categorial Gram-
mar which builds the chart essentially left
to right and bottom up. The Categorial
Grammar is compiled into a simple Depen-
dency Grammar to allow packing c.f. (Mil-
ward. 1994). An alternative would be to gen-
erate the semantic chart from semantic anal-
ysis records. as used in (Moore and Alshawi.
ing two predicates off the same event variable. MRS
also differs in not splitting up the representation quite
as much e.g. instead of fi13:i3(i2). i3:tol MRS would
have a single constraint equivalent to i13:to(i2).
2Semantic charts are similar to the packed se-
mantic structures used in the Core Language Engine
(Moore and Alshawi. 1992). The main difference is
that in the CLE the semantic analysis records more
closely follow phrase structure syntax, and semantic
representations are not reduced (i.e. we have an ap-
plication structure saying what applies to what, rather
than what is the argument of which predicate). Con-
sider the following record:
0-6-s: apply( 1-6-vp.0-1-np)
This states that the semantics for the verb phrase
(from positions 1 to 6) is to be applied to the seman-
tics for the noun phrase (from positions 0 to 1).
1992), using a more standard bottom up or
left corner parser.
The semantic chart is regarded as a seman-
tic representation in its own right. It may be
underspecified in the sense that it corresponds
to more than one reading. It may be partial
if there is no edge spanning the whole utter-
ance. Mapping to the task specific language is
performed directly on the chart. There is no
attempt to choose a particular analysis, or a
particular set of fragments before task specific
information is brought to bear.
3 Mapping from a Semantic Chart
to Slot Values
Consider the following utterance and its asso-
ciated semantic chart:
I leave Boston at 3
0-1-np:I, 1-2-v:leave, 2-3-np:boston.
3-4-p:at. 4-5-np:3.
0-3-s:1-2-v(0-1-np.2-3-np).
0-5-s:3-4-p(0-3-s. 4-5-np)
The &apos;departure time&apos; can be extracted using
the following rule, which requires the lexemes
&apos;at&apos; and `leave&apos; and treats the second argu-
ment of &apos;at&apos; as the departure-time.
</bodyText>
<equation confidence="0.9082715">
J:at. L:T, M:leave. I:J(K.L)
departure-time = T
</equation>
<bodyText confidence="0.999342461538461">
The following components match the left-
hand side of the rule, giving the result
&apos;departure-time = 33.
3-4-p:at, 4-5-np:3, 1-2-v:leave,
0-5-s:3-4-p(0-3-s.4-5-np)
Checking for an occurrence of the word &apos;leave&apos;
in the rest of the utterance ensures that the
user is likely to be talking about a departure
time rather than an arrival time. Note that
there is no structural constraint on `leave&apos; so
the departure time rule will apply equally well
to the following utterance where `leave&apos; is an
intransitive rather than a transitive verb:
</bodyText>
<footnote confidence="0.928935">
3The system treats &apos;cleparture-time=3. as an as-
sertion move. Currently we only have this move plus
replace moves.
</footnote>
<bodyText confidence="0.982307625">
I leave at 3
The uses of leave&apos; in the two sentences
above are not regarded as involving two dif-
ferent senses, hence both satisfy the con-
straint `M:leave&apos;. If the different subcategori-
sation possibilities had corresponded to differ-
ent senses, separate lexemes would have been
used e.g. leavei and leave2.
The actual mapping rule is more complex,
with constraints split according to whether
they concern the term which is being mapped,
are from the rest of the current utterance.
sortal constraints constraints concerning the
prior utterance. or constraints on the current
dialogue context (e.g. that a particular slot
has a particular value):
</bodyText>
<equation confidence="0.958922166666667">
Term mapped: L:T
Utt context: I :J(K.L). J:at. M:leave
Sortal constraints: time(T)
Prior utt: -
Dialogue context: -
departure-time = T
</equation>
<bodyText confidence="0.997133111111111">
Weights are attached to outputs according to
how specific rules are. This is determined
by the number of constraints with utter-
ance constraints counting more than contex-
tual constraints. The motivation is that map-
pings which require more specific contexts are
likely to be the better ones and what a person
said counts more than the prior context. For
transcribed dialogues a weighting of x 2 for
utterance constraints works well. This may
need to be reestimated for speech recogniser
output. To see how the weighting scheme
works in practice, consider the exchange:
S: When do you want to arrive?
U: I&apos;d like to leave now let&apos;s see. yes.
at 3pm
The system includes the following rule for
arrival-time:
</bodyText>
<equation confidence="0.5305675">
Term mapped: L:T
Utt context: -
Sortal constraints: time(T)
Prior utt: question(arrival-time)
Dialogue context: _
arrival-time = T
</equation>
<bodyText confidence="0.999361415730338">
The arrival-time rule and departure-time
rules both fire. There is a subsequent filter-
ing stage which ensures that overlapping ma-
terial (in this case. &amp;quot;3pm&amp;quot;) cannot be used
twice. The departure-time output is chosen
since more utterance constraints are satisfied
giving a higher weighting.
When deciding between rules, the aim is to
provide the most likely mapping given the ev-
idence available. The arrival time rule should
not be read as stating that an arrival-time
question expects an arrival-time for an an-
swer. The rule merely states that if there is no
other evidence, then a time phrase should be
interpreted as an arrival-time in the context
of a question about the arrival time. The rules
above may still be valid even if it happens that
the most common response to an arrival-time
question is a statement about the departure
time (assuming the departure time is always
flagged with extra linguistic information).
The rules given so far look like the kind
of rules you might see in a shallow NLP sys-
tem e.g. an Information Extraction system
based on pattern matching over a chunked list
of words. However, we can include as much
structural information as we want. Thus we
can include a more specific arrival-time rule
which would over-ride the departure-time rule
in a scenario such as:
S: When do you want to arrive?
U: I want to arrive at 3pm leaving
from Cambridge
Here we need to use the structural relation-
ship between &apos;arrive&apos; and `3pm. to override
the appearance of `leave&apos; in the same sentence.
The ability to use higher level linguistic
structure is a key difference from shallow pro-
cessing approaches where the level of analysis
is limited; even if the parser could have re-
liably extracted higher level structural infor-
mation for the particular sentence.
We are currently investigating various ways
in which to simplify or automate the construc-
tion of mapping rules. The simplest method
would be to allow optional constraints, and
this could be further extended to an inheri-
tance hierarchy to minimise redundancy&apos;.
Another area which is being investigated is
the way outputs are put together. So far, out-
puts have been at the top level i.e. slot-values
which we would expect to be asserted into
the context. However even when doing simple
slot-filling it is natural to use some recursive
constructions. For example some corrections
are naturally modelled via a replace operator
e.g.
But I want to go to London
Heathrow; not to London Stansted
replace(destination=London_Stansted,
destination,London_Heathrow)
The current mapping rule is specific to this
construction i.e. an occurrence of a nega-
tion between two prepositional phrases. This
could be generalised by separately mapping
the destination slot-values and the negation
to get an indexed representation in the slot
filling language i.e.
I: replace(J,K).
J:destination,London_Stansted,
K: destination,London_Heathrow
(Copestake et al.. 1995) and (Trujillo. 1995)
use a similar approach for machine transla-
tion, mapping from an indexed representa-
tion of the source to an indexed representa-
tion of the target. The difference here is that
we would not necessarily translate all the in-
put material just what we are interested in.
Everything else is assumed to have a null or
identity translation. As a side note, this gives
a perspective on why leaving out negation is
a problem in Information Retrieval: it is a
particularly bad assumption to assume that
negation is an identity mapping.
Both potential changes preserve a key fea-
ture of the approach that structural informa-
tion (encompassing constituency and scope)
is used where it is available from the parse,
but, if it is not available, the system backs
</bodyText>
<footnote confidence="0.498656">
4As suggested by one of the reviewers
</footnote>
<bodyText confidence="0.999613666666667">
off to more relaxed versions of the mapping
rules which in the extreme are equivalent to
just keyword spotting. These backed off rules
are defeasible rules: they assume that the re-
lationships that are missing will not affect the
mapping.
</bodyText>
<sectionHeader confidence="0.918157" genericHeader="method">
4 Distinctive features of the
approach
</sectionHeader>
<subsectionHeader confidence="0.915103">
4.1 Task specific interpretation
</subsectionHeader>
<bodyText confidence="0.987801605263158">
Consider the following sentence in the Air
Traffic Domain:
Show flights to Boston
This has two readings, one where &amp;quot;flights to
Boston&amp;quot; is a constituent, the other where
&amp;quot;to Boston&amp;quot; is an adverbial modifier (simi-
lar to &amp;quot;to Fred&amp;quot; in &amp;quot;Show flights to Fred&amp;quot;).
In full semantics approaches, the system is
specialised to the domain to achieve the cor-
rect reading either via specialisation of the
grammar c.f. OVIS. (van Noord et al.. 1999),
or via domain specific preference mechanisms
c.f. (Carter. 1997).
In contrast, in this approach, all domain de-
pendent information necessary for the task is
incorporated into the mapping rules. For the
example above a rule would pick up &amp;quot;flights
to &lt;city&gt;&amp;quot; but there would be no rule looking
for &amp;quot;show to &lt;city&gt;&amp;quot;, so the second reading
is simply ignored. Note that ambiguities ir-
relevant to the task are left unresolved. Thus
incorporating necessary information into task
specific mapping rules is a smaller task than
training to a domain and trying to resolve all
domain specific ambiguities.
4.2 Choosing the best fragments, not
just the largest
Many grammar based systems e.g. SmartS-
peak (Boye et al.. 1999) and Verbmobil (Go-
erz et al.. 1999). (Kasper et al., 1999) try to
find a full sentence analysis first and back
off to considering fragments on failure. The
common strategies on failure are to find the
largest possible single fragment e.g. SmartS-
peak or the set of largest possible fragments
e.g. Verbmobil. This is defined as the small-
est set of fragments which span the utterance,
i.e. the shortest path.
However; by always selecting the full anal-
ysis, you can end up with an implausible
sentence. as opposed to plausible fragments.
This occurs commonly when the recogniser
suggests an extra bogus word at the end of
an utterance. A comprehensive grammar may
still find an (implausible) full analysis. Sim-
ilarly the largest possible fragments are not
necessarily the most plausible ones. This has
been recognised in the OVIS system which is
experimenting with a limited amount of con-
textual information to weight fragments.
In our approach task specific mapping
rules apply to the whole chart (including
edges corresponding to large and small frag-
ments). Preference is given to more specific
mapping rules, but this may not always corre-
spond to choosing a larger fragment. A larger
fragment will only be preferred if it satisfies
more constraints relevant to the task (includ-
ing contextual constraints). The addition of
bogus words is not rewarded, and is more
likely to cause a constraint to fail. By retain-
ing a lattice or chart throughout nothing is
thrown away until there is a chance to bring
task specific information to bear.
Our approach is tailored to task oriented
dialogue: we are only looking for relevant in-
formation and there is no need to come up
with a single path through the lattice or even
make a hypothesise about exactly what was
said (except for the relevant words). Verbmo-
bil has the more difficult task of translating
dialogue utterances. However, some of the
same issues are relevant. For example, frag-
ment choice could be determined according
to which fragments are most relevant to the
task rather than according to their length.
</bodyText>
<subsectionHeader confidence="0.999492">
4.3 Exploitation of underspecification
</subsectionHeader>
<bodyText confidence="0.997379238095239">
The most obvious gain by working directly
with an underspecified representation (in this
case a chart or lattice) should be an efficiency
one. This is particularly true when work-
ing with a lattice where many of the words
hypothesised will be irrelevant for the task.
and we only home in on the bits of the lat-
tice which are mentioned in task specific rules.
The current implementation applies the map-
ping rules in every possible way to provide a
set of potential slot-value pairs. It then fil-
ters the set to obtain a consistent set of pairs.
The first stage is to filter out any cases where
the translated material overlaps (we cannot
use &amp;quot;3pm&amp;quot; in both a departure time and an
arrival time). In these cases the more specific
mapping is retained. Next the algorithm uses
task specific constraints. e.g. there can be only
one departure time to prune the outputs.
The current algorithm is &apos;greedy&apos; taking the
best local choices not necessarily providing
the best global solution. We have not yet
come across a real example where this would
have made a difference, but consider the fol-
lowing possible exchange:
S: When would you like to arrive
U: I would like to leave at 3 to get
in at 4
The system currently has no rules for the con-
struction &apos;get in&apos;, so the most specific rule to
apply to &apos;4&apos; is the departure time rule (`4&apos;
is a time in a sentence containing the word
&apos;leave.). However. &apos;3&apos; is also mapped to a
departure time and receives a higher weight
since it is in a construction with `leave&apos;. Thus
at the next filtering stage the mapping to
&apos;departure-time = 4&apos; is discarded and we are
left with the single output &apos;departure-time =
3&apos;. In contrast an algorithm which looked
for the best global solution might have pro-
vided the required result. &apos;departure-time =
3. arrival-time = 4&apos;.
</bodyText>
<subsectionHeader confidence="0.997158">
4.4 Context dependent interpretation
</subsectionHeader>
<bodyText confidence="0.979935411764706">
A key feature of the approach is that inter-
pretation of one item can be dependent upon
interpretation of another part of the utter-
ance. This includes cases where the two items
would occur in separate fragments in a gram-
mar based analysis. Consider the following
examples:
I&apos;d like to leave York, now let&apos;s see,
yes. at 3pm
at 3pm departure_time(3pm)
I&apos;d like to arrive at York; now let&apos;s
see; yes. at 3 pm
at 3pm arrival_time(3pm)
The translation of the phrase &amp;quot;at 3pm&amp;quot; is de-
pendent here not on any outside context but
on the rest of the utterance. This is naturally
incorporated in the rule given earlier which
just looks for &apos;leave* anywhere in the utter-
ance.
4.5 Relationship to other Approaches
The use of a distributed representation (flat
structured semantics) to enable mapping
rules to take pieces of information from dif-
ferent parts of an utterance is a key idea in
the work of (Copestake et al.. 1995). The
use of underspecified representations to pro-
vide a semantic representation for fragments
has been discussed by Pinkal in the context
of Radical Underspecification (Pinkal. 1995).
Viewing interpretation as the process of find-
ing the most likely meaning in the given con-
text is a view shared with statistical models
of dialogue interpretation such as (Miller et
al.. 1996).
</bodyText>
<sectionHeader confidence="0.990159" genericHeader="method">
5 Reconfigurability
</sectionHeader>
<bodyText confidence="0.9855495">
Reconfiguring to a new task requires the in-
troduction of new mapping rules and the ad-
dition of lexical entries for at least the words
mentioned in the mapping rules. Parsing and
morphology modules are shared across differ-
ent tasks. The robust nature of the approach
means that we can provide a working system
without providing a full parse for all. or even
for a majority of the utterances in the domain.
There is no need to deal with new words or
constructions in a new domain unless they are
specifically mentioned in task specific map-
pings.
For route planning we were able to obtain
better performance than our previous sys-
tems using just 70 mapping rules to cover
the four slots. &apos;destination&apos;. &apos;origin&apos;. &apos;arrival-
time&apos;. `trip-mode&apos; and `departure-time&apos;, and
just over a hundred lexical entries (excluding
city names).
6 An example spoken dialogue
system
The system was built as a web-based demo.
User input is via an HTML text box, and can
be entered by typing or by using an appropri-
ate speech recogniser. The system has been
tested using a generic recogniser, trained for
the user&apos;s voice, but not to route planning.
Although the recogniser makes more errors
than you would expect from a domain trained
recogniser, the errors tend to be less critical.
Incorrect hypotheses can be any word from a
200.000 word vocabulary, so are likely to be
ignored by the mapping rules. There is usu-
ally enough redundancy between what was
said and the dialogue context to choose the
correct slot to fill though not necessarily the
right value.
Since this approach was designed for lat-
tice input the current set up using one-best
recogniser output is far from ideal. We will
shortly be experimenting with lattice input
from both domain trained and generic recog-
nisers and looking at various levels of lattice
pruning and ways to incorporate recogniser
scoring into the weighting scheme.
Currently the one-best recogniser output is
converted into a trivial lattice. This is then
parsed by the incremental parser. which adds
semantic arcs word by word. The dialogue
strategy employed is not particularly sophis-
ticated: the system just asks a question ap-
propriate to filling the first empty slot in its
list c.f. (Aust et al.. 1995). This strategy al-
lows more than one slot to be filled at any
point or for a question not to be answered at
all (as in cases where a user performs a cor-
rection rather than answering). 20 word sen-
tences take less than a second to go through
all stages on a basic PC.
</bodyText>
<sectionHeader confidence="0.5564625" genericHeader="method">
7 Evaluation on a transcribed
corpus
</sectionHeader>
<bodyText confidence="0.999848064516129">
The approach has been evaluated using a cor-
pus of transcribed spoken dialogues collected
by the Wizard of Oz technique (i.e. a human
pretending to be a computer: in this case
by typing responses which were relayed down
the phone using a speech synthesiser). The
transcriptions include repairs and hesitations,
but not recognition errors. The system was
trained on one third of the corpus; and tested
on two thirds. The test set included 200 user
replies. Precision and recall were measured
on slot value pairings i.e. for a pairing to be
correct both the slot and value had to be cor-
rect.
The first evaluation was against an existing
phrase spotting system which had performed
well when evaluated against a full semantics
approach (Lewin et al.. 1999) in a different
domain. A significant improvement in recall
and precision was achieved, but coverage dif-
ferences meant it was unclear how valid the
comparison was. We therefore performed a
second evaluation which investigated to what
extent different knowledge source made a dif-
ference. In all cases we used sortal informa-
tion but we tried the system with and with-
out access to the previous utterance, access
to utterance context outside the phrase we
are interested in, and without phrasal infor-
mation. The precision and recall results were
as follows:
</bodyText>
<table confidence="0.800229285714286">
Prey Utt Utt Cxt Phr Cxt R P
22 96
yes 34 75
yes 38 89
yes yes 40 87
yes yes 51 78
yes yes yes 52 79
</table>
<bodyText confidence="0.999603305555555">
All six systems are relatively conservative
giving good precision. The first system corre-
sponds to only taking sortal information into
account. For this domain, sortal information
safely determines the slot in the case of &apos;trip
mode&apos; e.g. whether the user wants the short-
est journey or the quickest. A phrase spot-
ter which does not use context corresponds
to recall of 38 percent. This will pick up &apos;to
Cambridge as the destination, but does not
have enough information to decide whether
&apos;at 5pm&apos; is an arrival time or a departure
time. As hoped, use of more linguistic infor-
mation from within the utterance improves
recall and performance, though the improve-
ments are not huge.
The poor recall figures for all six systems
are also reflected in the training set. Al-
most all the loss is due to inadequate lexi-
cal/syntactic coverage of potential slot values
(e.g. complex city names and time expressions
etc.). There are however some cases where
even the most relaxed versions of the rules
were still too restrictive e.g.
from uhm Evesham to uh Winder-
mere
This particular case could be dealt with eas-
ily by the use of simple reconstruction rules
(deleting `uh&apos; and `uhm.). Other options we
are considering are to expand the use of dis-
tributed representation to include positional
and syntactic constraints, or to allow non-
exact matches between &apos;ideal&apos; scenarios rep-
resented by the left hand side of mapping
rules and actual input (weighting then be-
comes more akin to a distance measure).
</bodyText>
<sectionHeader confidence="0.990946" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999630733333333">
In this paper we have described an approach
which exploits a distributed representation
which includes both content and structure.
Mapping rules (from semantics to slot-fillers)
apply directly to the underspecified seman-
tic interpretation. Ungrammatical input is
treated by relaxing the mapping rules rather
than by trying to recreate a connected parse.
In being able to use structural and contex-
tual information where it is available and rel-
evant to the task, the approach can improve
on keyword or phrase spotting approaches;
while avoiding many of the pitfalls of over-
early commitment (e.g. to longest fragments)
found in many grammar based systems.
</bodyText>
<sectionHeader confidence="0.991443" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<reference confidence="0.920252">
This work was made possible by the sup-
port of the European Union through the LE
projects. Trindi and Siridus. Various people
have given very useful feedback on this work
including Ian Lewin, Robin Cooper. Manfred
Pinkal. Steve Pulman, Bob Carpenter. Rob
Koeling, James Thomas, Massimo Poesio and
Roger Evans. I would also like to thank the
Defence and Evaluation Research Agency for
</reference>
<bodyText confidence="0.507695">
making their corpus of route planning dia-
logues available and to Toby Hudson for an-
notating them with slot-values.
</bodyText>
<sectionHeader confidence="0.879348" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999794112499999">
H. Aust. M. Oerder. F. Siede. and V. Steinbiss.
1995. A spoken language enquiry system for
automatic train timetable information. Philips
Journal of Research. 49(4):399-418.
J. Boye, M. Wiren, M. Rayner, I. Lewin,
D. Carter, and R. Becket. 1999. Language-
processing strategies and mixed-initiative dia-
logues. In IJCAI-99 Workshop on Knowledge
and Reasoning in Practical Dialogue Systems.
D. Carter. 1997. The treebanker: a tool for su-
pervised training of parsed corpora. In ACL
Workshop on Computational Environments for
Grammar Development and Linguistic Engi-
neering. Available as SRI Cambridge Technical
Report CRC-068.
A. Copestake, D. Flickinger, R. Malouf, S. Riehe-
mann, and I.Sag. 1995. Translation using mini-
mal recursion semantics. In Sixth International
Conference on Theoretical and Mehodological
Issues in Machine Translation, Leuven Bel-
gium.
processing for spoken dialogue systems: is shal-
low parsing enough? In ESCA ETRW Work-
shop on Accessing information in Spoken Au-
dio, Cambridge. Available as SRI Cambridge
Technical Report CRC-074.
M. Marcus, D. Hindle, and M. Fleck. 1983. D-
theory: Talking about talking about trees. In
21st Annual Meeting of the ACL, pages 129-
136. Cambridge, Mass.
S. Miller, D. Stallard, R. Bobrow, and R.Schwartz.
1996. A fully statistical approach to natural
language interfaces. In Proceedings of the 34th
Annual Meeting of the ACL, pages 55-61, Uni-
versity of California.
D. Milward. 1994. Dynamic dependency gram-
mar. Linguistics and Philosophy. 17:561-605.
D. Milward. 1999. Towards a robust semantics for
dialogue using flat structures. In Amstelogue
&apos;99, Workshop on the Semantics and Pragmat-
ics of Dialogue. volume II. University of Ams-
terdam.
R. C. Moore and H. Alshawi. 1992. Syntactic
and semantic processing. In The Core Lan-
guage Engine. pages 129-146. MIT Press.
M. Pinkal. 1995. Radical underspecification. In
10th Amsterdam Colloquium. volume III. pages
587-606.
U. Reyle. 1993. Dealing with ambiguities by un-
derspecification: Construction representation
and deduction. Journal of Semantics. 10:123-
179.
A.
M. Egg. J. Niehren. P. Ruhrberg. and F. Xu.
1998. Constraints over lambda-structures in
semantic underspecification. In COLING-ACL
&apos;98. Montreal. Canada.
G. Goerz. J. Spilker. V. Strom. and H. Weber. G.
1999. Architectural considerations for conver-
sational systems — the verbmobil/intarc experi-
ence. In First International Workshop on Hu-
man Computer Conversation. Bellagio. Italy.
Trujillo. 1995. Bi-lexical rules for multi-lexeme
translation in lexicalist mt. In Sixth Interna-
tional Conference on Theoretical and Method-
ological Issues in Machine Translation, pages
48-66, Leuven Belgium.
van Noord, G. Bouma, R. Koeling, and M-J.
Nederhof. 1999. Robust grammatical analysis
for spoken dialogue systems. Natural Language
Engineering, 5(1) :45-93.
J. Hobbs. 1983. An improper treatment of quan-
tification in ordinary english. In 21st Annual
Meeting of the ACL. Cambridge. Mass.
W. Kasper. B. Kiefer. H.U. Krieger. C. J. Rupp.
and K.L.Worm. 1999. Charting the depths of
robust speech processing. In Proceedings of the
37th ACL.
I. Lewin. R. Becket. J. Boye. D. Carter.
M. Rayner. and M. Wiren. 1999. Language
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.707203">
<title confidence="0.999738">Distributing Representation for Robust Interpretation of Dialogue Utterances</title>
<author confidence="0.999974">David Milward</author>
<affiliation confidence="0.999955">SRI International Cambridge Computer Science Research Centre</affiliation>
<address confidence="0.976886">23 Millers Yard. Mill Lane Cambridge. CB2 1R.Q. G.B.</address>
<email confidence="0.999671">milwardAcam.sri.com</email>
<abstract confidence="0.976061181818182">A syntax tree or standard semantic representation can be represented as a set of indexed constraints. This paper describes how this idea can be used in task oriented dialogue systems to provide interpretation rules which incorporate structural and contextual constraints where available, and degrade gracefully on ungrammatical input.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Ian Lewin</author>
<author>Robin Cooper</author>
</authors>
<title>This work was made possible by the support of the European Union through the LE projects. Trindi and Siridus. Various people have given very useful feedback on this work including</title>
<publisher>Manfred</publisher>
<marker>Lewin, Cooper, </marker>
<rawString>This work was made possible by the support of the European Union through the LE projects. Trindi and Siridus. Various people have given very useful feedback on this work including Ian Lewin, Robin Cooper. Manfred</rawString>
</citation>
<citation valid="false">
<authors>
<author>Steve Pulman</author>
<author>Bob Carpenter Rob Koeling</author>
<author>James Thomas</author>
</authors>
<title>Massimo Poesio and Roger Evans. I would also like to thank the Defence and Evaluation Research Agency for</title>
<marker>Pulman, Koeling, Thomas, </marker>
<rawString>Pinkal. Steve Pulman, Bob Carpenter. Rob Koeling, James Thomas, Massimo Poesio and Roger Evans. I would also like to thank the Defence and Evaluation Research Agency for</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Oerder F Siede</author>
<author>V Steinbiss</author>
</authors>
<title>A spoken language enquiry system for automatic train timetable information.</title>
<date>1995</date>
<journal>Philips Journal of Research.</journal>
<pages>49--4</pages>
<marker>Siede, Steinbiss, 1995</marker>
<rawString>H. Aust. M. Oerder. F. Siede. and V. Steinbiss. 1995. A spoken language enquiry system for automatic train timetable information. Philips Journal of Research. 49(4):399-418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Boye</author>
<author>M Wiren</author>
<author>M Rayner</author>
<author>I Lewin</author>
<author>D Carter</author>
<author>R Becket</author>
</authors>
<title>Languageprocessing strategies and mixed-initiative dialogues.</title>
<date>1999</date>
<booktitle>In IJCAI-99 Workshop on Knowledge and Reasoning in Practical Dialogue Systems.</booktitle>
<marker>Boye, Wiren, Rayner, Lewin, Carter, Becket, 1999</marker>
<rawString>J. Boye, M. Wiren, M. Rayner, I. Lewin, D. Carter, and R. Becket. 1999. Languageprocessing strategies and mixed-initiative dialogues. In IJCAI-99 Workshop on Knowledge and Reasoning in Practical Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Carter</author>
</authors>
<title>The treebanker: a tool for supervised training of parsed corpora.</title>
<date>1997</date>
<booktitle>In ACL Workshop on Computational Environments for Grammar Development and Linguistic Engineering. Available as SRI Cambridge</booktitle>
<tech>Technical Report CRC-068.</tech>
<marker>Carter, 1997</marker>
<rawString>D. Carter. 1997. The treebanker: a tool for supervised training of parsed corpora. In ACL Workshop on Computational Environments for Grammar Development and Linguistic Engineering. Available as SRI Cambridge Technical Report CRC-068.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>R Malouf</author>
<author>S Riehemann</author>
<author>I Sag</author>
</authors>
<title>Translation using minimal recursion semantics.</title>
<date>1995</date>
<booktitle>In Sixth International Conference on Theoretical and Mehodological Issues in Machine Translation,</booktitle>
<location>Leuven</location>
<marker>Copestake, Flickinger, Malouf, Riehemann, Sag, 1995</marker>
<rawString>A. Copestake, D. Flickinger, R. Malouf, S. Riehemann, and I.Sag. 1995. Translation using minimal recursion semantics. In Sixth International Conference on Theoretical and Mehodological Issues in Machine Translation, Leuven Belgium.</rawString>
</citation>
<citation valid="false">
<title>processing for spoken dialogue systems: is shallow parsing enough?</title>
<booktitle>In ESCA ETRW Workshop on Accessing information in Spoken Audio, Cambridge. Available as SRI Cambridge</booktitle>
<tech>Technical Report CRC-074.</tech>
<marker></marker>
<rawString>processing for spoken dialogue systems: is shallow parsing enough? In ESCA ETRW Workshop on Accessing information in Spoken Audio, Cambridge. Available as SRI Cambridge Technical Report CRC-074.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>D Hindle</author>
<author>M Fleck</author>
</authors>
<title>Dtheory: Talking about talking about trees.</title>
<date>1983</date>
<booktitle>In 21st Annual Meeting of the ACL,</booktitle>
<pages>129--136</pages>
<location>Cambridge, Mass.</location>
<contexts>
<context position="4432" citStr="Marcus et al., 1983" startWordPosition="705" endWordPosition="708">state that this dominates both i4 and i5. To see how this can be done systematically, consider the pair of syntax trees for &amp;quot;from Boston to London Heathrow&amp;quot; in Figure 1 in which each node is uniquely indexed. The following constraints encode the bracketing: ii :from, i2:Boston, i3:to, i4:London, i5:Heathrow, i8: (i3;i7). i7: (i4;i5) To encode the full tree, including the syntactic labels we would also need to assert the category for each index e.g. cat(i6; PH). The set of constraints can be considered as an alternative representation of the tree, or as a description of the tree c.f. D-Theory (Marcus et al., 1983). As an alternative to working with a syntax tree we can use a semantic representation. In this case, adding indices to the predicate argument structures gives: fromij (Bostoni2)i6 t0i3 ( London _Heat hrowi7),i8 The equivalent constraints would be: ii :from, i2:Boston, i3:to, i7:London_Heathrow. i6:il (i2), i8:i3(i7) This provides the lexemes, from, to, Boston and London_Heathrow, along with constraints specifying their relationship. Note that we have only changed how we represent the semantics: there is a one-to-one mapping between the set of constraints and the original recursive representat</context>
</contexts>
<marker>Marcus, Hindle, Fleck, 1983</marker>
<rawString>M. Marcus, D. Hindle, and M. Fleck. 1983. Dtheory: Talking about talking about trees. In 21st Annual Meeting of the ACL, pages 129-136. Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>D Stallard</author>
<author>R Bobrow</author>
<author>R Schwartz</author>
</authors>
<title>A fully statistical approach to natural language interfaces.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the ACL,</booktitle>
<pages>55--61</pages>
<institution>University of California.</institution>
<marker>Miller, Stallard, Bobrow, Schwartz, 1996</marker>
<rawString>S. Miller, D. Stallard, R. Bobrow, and R.Schwartz. 1996. A fully statistical approach to natural language interfaces. In Proceedings of the 34th Annual Meeting of the ACL, pages 55-61, University of California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milward</author>
</authors>
<title>Dynamic dependency grammar. Linguistics and Philosophy.</title>
<date>1994</date>
<pages>17--561</pages>
<marker>Milward, 1994</marker>
<rawString>D. Milward. 1994. Dynamic dependency grammar. Linguistics and Philosophy. 17:561-605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milward</author>
</authors>
<title>Towards a robust semantics for dialogue using flat structures.</title>
<date>1999</date>
<booktitle>In Amstelogue &apos;99, Workshop on the Semantics and Pragmatics of Dialogue.</booktitle>
<volume>volume</volume>
<institution>II. University of Amsterdam.</institution>
<marker>Milward, 1999</marker>
<rawString>D. Milward. 1999. Towards a robust semantics for dialogue using flat structures. In Amstelogue &apos;99, Workshop on the Semantics and Pragmatics of Dialogue. volume II. University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>H Alshawi</author>
</authors>
<title>Syntactic and semantic processing.</title>
<date>1992</date>
<booktitle>In The Core Language Engine.</booktitle>
<pages>129--146</pages>
<publisher>MIT Press.</publisher>
<marker>Moore, Alshawi, 1992</marker>
<rawString>R. C. Moore and H. Alshawi. 1992. Syntactic and semantic processing. In The Core Language Engine. pages 129-146. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pinkal</author>
</authors>
<title>Radical underspecification.</title>
<date>1995</date>
<booktitle>In 10th Amsterdam Colloquium. volume III.</booktitle>
<pages>587--606</pages>
<marker>Pinkal, 1995</marker>
<rawString>M. Pinkal. 1995. Radical underspecification. In 10th Amsterdam Colloquium. volume III. pages 587-606.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Reyle</author>
</authors>
<title>Dealing with ambiguities by underspecification: Construction representation and deduction.</title>
<date>1993</date>
<journal>Journal of Semantics.</journal>
<pages>10--123</pages>
<marker>Reyle, 1993</marker>
<rawString>U. Reyle. 1993. Dealing with ambiguities by underspecification: Construction representation and deduction. Journal of Semantics. 10:123-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Niehren P Ruhrberg</author>
<author>F Xu</author>
</authors>
<title>Constraints over lambda-structures in semantic underspecification.</title>
<date>1998</date>
<booktitle>In COLING-ACL &apos;98.</booktitle>
<location>Montreal. Canada.</location>
<marker>Ruhrberg, Xu, 1998</marker>
<rawString>A. M. Egg. J. Niehren. P. Ruhrberg. and F. Xu. 1998. Constraints over lambda-structures in semantic underspecification. In COLING-ACL &apos;98. Montreal. Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Spilker V Strom</author>
<author>H Weber G</author>
</authors>
<title>Architectural considerations for conversational systems — the verbmobil/intarc experience.</title>
<date>1999</date>
<booktitle>In First International Workshop on Human Computer Conversation. Bellagio. Italy.</booktitle>
<marker>Strom, G, 1999</marker>
<rawString>G. Goerz. J. Spilker. V. Strom. and H. Weber. G. 1999. Architectural considerations for conversational systems — the verbmobil/intarc experience. In First International Workshop on Human Computer Conversation. Bellagio. Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trujillo</author>
</authors>
<title>Bi-lexical rules for multi-lexeme translation in lexicalist mt.</title>
<date>1995</date>
<booktitle>In Sixth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>48--66</pages>
<location>Leuven</location>
<marker>Trujillo, 1995</marker>
<rawString>Trujillo. 1995. Bi-lexical rules for multi-lexeme translation in lexicalist mt. In Sixth International Conference on Theoretical and Methodological Issues in Machine Translation, pages 48-66, Leuven Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma van Noord</author>
<author>R Koeling</author>
<author>M-J Nederhof</author>
</authors>
<title>Robust grammatical analysis for spoken dialogue systems.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<volume>5</volume>
<issue>1</issue>
<pages>45--93</pages>
<marker>van Noord, Koeling, Nederhof, 1999</marker>
<rawString>van Noord, G. Bouma, R. Koeling, and M-J. Nederhof. 1999. Robust grammatical analysis for spoken dialogue systems. Natural Language Engineering, 5(1) :45-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>An improper treatment of quantification in ordinary english.</title>
<date>1983</date>
<booktitle>In 21st Annual Meeting of the ACL.</booktitle>
<location>Cambridge. Mass.</location>
<marker>Hobbs, 1983</marker>
<rawString>J. Hobbs. 1983. An improper treatment of quantification in ordinary english. In 21st Annual Meeting of the ACL. Cambridge. Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kiefer H U Krieger C J Rupp</author>
<author>K L Worm</author>
</authors>
<title>Charting the depths of robust speech processing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th ACL.</booktitle>
<marker>Rupp, Worm, 1999</marker>
<rawString>W. Kasper. B. Kiefer. H.U. Krieger. C. J. Rupp. and K.L.Worm. 1999. Charting the depths of robust speech processing. In Proceedings of the 37th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Becket J Boye D Carter M Rayner</author>
<author>M Wiren</author>
</authors>
<date>1999</date>
<note>Language</note>
<marker>Rayner, Wiren, 1999</marker>
<rawString>I. Lewin. R. Becket. J. Boye. D. Carter. M. Rayner. and M. Wiren. 1999. Language</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>