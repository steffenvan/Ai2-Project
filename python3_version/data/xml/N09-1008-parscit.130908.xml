<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.484437">
Improved Reconstruction of Protolanguage Word Forms
</title>
<author confidence="0.847406">
Alexandre Bouchard-Cˆot´e* Thomas L. Griffiths† Dan Klein*
</author>
<affiliation confidence="0.850859">
*Computer Science Division †Department of Psychology
University of California at Berkeley
</affiliation>
<address confidence="0.271073">
Berkeley, CA 94720
</address>
<sectionHeader confidence="0.967442" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999868058823529">
We present an unsupervised approach to re-
constructing ancient word forms. The present
work addresses three limitations of previous
work. First, previous work focused on faith-
fulness features, which model changes be-
tween successive languages. We add marked-
ness features, which model well-formedness
within each language. Second, we introduce
universal features, which support generaliza-
tions across languages. Finally, we increase
the number of languages to which these meth-
ods can be applied by an order of magni-
tude by using improved inference methods.
Experiments on the reconstruction of Proto-
Oceanic, Proto-Malayo-Javanic, and Classical
Latin show substantial reductions in error rate,
giving the best results to date.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99588485">
A central problem in diachronic linguistics is the re-
construction of ancient languages from their modern
descendants (Campbell, 1998). Here, we consider
the problem of reconstructing phonological forms,
given a known linguistic phylogeny and known cog-
nate groups. For example, Figure 1 (a) shows a col-
lection of word forms in several Oceanic languages,
all meaning to cry. The ancestral form in this case
has been presumed to be /taNis/ in Blust (1993). We
are interested in models which take as input many
such word tuples, each representing a cognate group,
along with a language tree, and induce word forms
for hidden ancestral languages.
The traditional approach to this problem has been
the comparative method, in which reconstructions
are done manually using assumptions about the rel-
ative probability of different kinds of sound change
(Hock, 1986). There has been work attempting to
automate part (Durham and Rogers, 1969; Eastlack,
1977; Lowe and Mazaudon, 1994; Covington, 1998;
</bodyText>
<page confidence="0.994434">
65
</page>
<bodyText confidence="0.998984384615385">
Kondrak, 2002) or all of the process (Oakes, 2000;
Bouchard-Cˆot´e et al., 2008). However, previous au-
tomated methods have been unable to leverage three
important ideas a linguist would employ. We ad-
dress these omissions here, resulting in a more pow-
erful method for automatically reconstructing an-
cient protolanguages.
First, linguists triangulate reconstructions from
many languages, while past work has been lim-
ited to small numbers of languages. For example,
Oakes (2000) used four languages to reconstruct
Proto-Malayo-Javanic (PMJ) and Bouchard-Cˆot´e et
al. (2008) used two languages to reconstruct Clas-
sical Latin (La). We revisit these small datasets
and show that our method significantly outperforms
these previous systems. However, we also show that
our method can be applied to a much larger data
set (Greenhill et al., 2008), reconstructing Proto-
Oceanic (POc) from 64 modern languages. In ad-
dition, performance improves with more languages,
which was not the case for previous methods.
Second, linguists exploit knowledge of phonolog-
ical universals. For example, small changes in vowel
height or consonant place are more likely than large
changes, and much more likely than change to ar-
bitrarily different phonemes. In a statistical system,
one could imagine either manually encoding or auto-
matically inferring such preferences. We show that
both strategies are effective.
Finally, linguists consider not only how languages
change, but also how they are internally consistent.
Past models described how sounds do (or, more of-
ten, do not) change between nodes in the tree. To
borrow broad terminology from the Optimality The-
ory literature (Prince and Smolensky, 1993), such
models incorporated faithfulness features, captur-
ing the ways in which successive forms remained
similar to one another. However, each language
has certain regular phonotactic patterns which con-
</bodyText>
<note confidence="0.8062695">
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 65–73,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999417428571429">
strain these changes. We encode such patterns us-
ing markedness features, characterizing the internal
phonotactic structure of each language. Faithfulness
and markedness play roles analogous to the channel
and language models of a noisy-channel system. We
show that markedness features improve reconstruc-
tion, and can be used efficiently.
</bodyText>
<sectionHeader confidence="0.999739" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.995346692307692">
Our focus in this section is on describing the prop-
erties of the two previous systems for reconstruct-
ing ancient word forms to which we compare our
method. Citations for other related work, such as
similar approaches to using faithfulness and marked-
ness features, appear in the body of the paper.
In Oakes (2000), the word forms in a given pro-
tolanguage are reconstructed using a Viterbi multi-
alignment between a small number of its descendant
languages. The alignment is computed using hand-
set parameters. Deterministic rules characterizing
changes between pairs of observed languages are ex-
tracted from the alignment when their frequency is
higher than a threshold, and a proto-phoneme inven-
tory is built using linguistically motivated rules and
parsimony. A reconstruction of each observed word
is first proposed independently for each language. If
at least two reconstructions agree, a majority vote
is taken, otherwise no reconstruction is proposed.
This approach has several limitations. First, it is not
tractable for larger trees, since the time complexity
of their multi-alignment algorithm grows exponen-
tially in the number of languages. Second, deter-
ministic rules, while elegant in theory, are not robust
to noise: even in experiments with only four daugh-
ter languages, a large fraction of the words could not
be reconstructed.
In Bouchard-Cˆot´e et al. (2008), a stochastic model
of sound change is used and reconstructions are in-
ferred by performing probabilistic inference over an
evolutionary tree expressing the relationships be-
tween languages. The model does not support gener-
alizations across languages, and has no way to cap-
ture phonotactic regularities within languages. As a
consequence, the resulting method does not scale to
large phylogenies. The work we present here ad-
dresses both of these issues, with a richer model
and faster inference allowing improved reconstruc-
tion and increased scale.
</bodyText>
<sectionHeader confidence="0.988718" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999877676470588">
We start this section by introducing some notation.
Let T be a tree of languages, such as the examples
in Figure 3 (c-e). In such a tree, the modern lan-
guages, whose word forms will be observed, are the
leaves of T. All internal nodes, particularly the root,
are languages whose word forms are not observed.
Let L denote all languages, modern and otherwise.
All word forms are assumed to be strings E∗ in the
International Phonological Alphabet (IPA).1
We assume that word forms evolve along the
branches of the tree T. However, it is not the case
that each cognate set exists in each modern lan-
guage. Formally, we assume there to be a known
list of C cognate sets. For each c E 11, ... , C}
let L(c) denote the subset of modern languages that
have a word form in the c-th cognate set. For each
set c E 11, ... , C} and each language E E L(c), we
denote the modern word form by we. For cognate
set c, only the minimal subtree T(c) containing L(c)
and the root is relevant to the reconstruction infer-
ence problem for that set.
From a high-level perspective, the generative pro-
cess is quite simple. Let c be the index of the cur-
rent cognate set, with topology T(c). First, a word
is generated for the root of T(c) using an (initially
unknown) root language model (distribution over
strings). The other nodes of the tree are drawn incre-
mentally as follows: for each edge E —* E&apos; in T(c) use
a branch-specific distribution over changes in strings
to generate the word at node E&apos;.
In the remainder of this section, we clarify the ex-
act form of the conditional distributions over string
changes, the distribution over strings at the root, and
the parameterization of this process.
</bodyText>
<subsectionHeader confidence="0.999364">
3.1 Markedness and Faithfulness
</subsectionHeader>
<bodyText confidence="0.9994405">
In Optimality Theory (OT) (Prince and Smolensky,
1993), two types of constraints influence the selec-
tion of a realized output given an input form: faith-
fulness and markedness constraints. Faithfulness en-
</bodyText>
<footnote confidence="0.9989">
1The choice of a phonemic representation is motivated by
the fact that most of the data available comes in this form. Dia-
critics are available in a smaller number of languages and may
vary across dialects, so we discarded them in this work.
</footnote>
<page confidence="0.865228">
66
</page>
<figure confidence="0.999557666666667">
edes
/angi/
/angi/
roles a
(a) (b)
Language Word form
Proto Oceanic /tagis/
Lau /agi/
Kwara’ae /angi/
Taiof /tagis/
(c)
(f)
/taŋi/
(e)
/taŋi/
(d)
g
/aŋi/ /aŋi/
feat aract
θS
erŋ
nt g i
ŋ
a n arg
</figure>
<bodyText confidence="0.862974375">
The yi’s may be a single character, multiple charac-
ters, or even empty. In the example shown, all three
bi ve
of these cases occur.
sing the relationships be
proxmate inferene a
To generate yi, we define a mutation Markov
stic
θI chain that incrementally adds zero or more charac-
some f the imtations of
li hd i
consequently does not scale
ters to an initially empty yi. First, we decide whether
he high compuational cost
the current phoneme in the top word t = xi will be
deleted, in which case yi = � as in the example of
</bodyText>
<figure confidence="0.965164485714286">
ao iit th aturs
dl (ii glbl f
pportin
1[Subst] lizaions across languaes
/s/ being deleted. If t is not deleted, we chose a sin-
1[(n)@Kw]
markednes
1[(n g)@Kw] s wihin languages). The
gle substitution character in the bottom word. This
we presen
1[Insert] drse tse ue,
1[(g)@Kw] is the case both when /a/ is unchanged and when /N/
h dl lli i
X1 X2 X3 X4 X5 X6 X7
t a
ŋ
b i
#
# f
#
#
a
n g
i
s
Y1 Y2 Y3volionary t
Y4 Y5 Y6 Y7
be include
1[ŋ⟶g@Kw]
1[ŋ⟶g]
? . .
hlo
s
g
overeconstruction quality and we show how o
</figure>
<figureCaption confidence="0.997821">
Figure 1: (a) A cognate set from the Austronesian dataset.
</figureCaption>
<figure confidence="0.196401315789474">
3 Modl
All word forms mean to cry. (b-d) The mutation model
with them efficiently
used in this paper. (b) The mutation of P
Kw. /angi/. (c) Graphical model depicting
cies amongevariables in onenstep ofothe mutation Markov
chain.w(d)pActivesyfeaturesrforoone step p
(e-f) Comparison of two inference procedures
Single sequence resampling (e) draws one sequence at a
time, conditioned ondits parent and children,
try resamplingw(f)d draws angalignedoslice from all words
simultaneously. In largeatrees,bthe latter
than the former.
N
Oc /ta
the dependen-
cies
while ances-
try
</figure>
<figureCaption confidence="0.094708">
is more efficient
</figureCaption>
<bodyText confidence="0.966085000000001">
courages similarity between the i
w
Viewednfromothiseperspective, p
tional approaches to reconstruction
exclusivelynon faithfulness, expres
t ti model. Only thed th
f the
root o tree, if any, are explic
tring transducer (Varadarajan et
bile markedness favors well-forme
a on mo e. words m e
in
We now make precise the conditional distribu-
tions over pairs of evolving strings, referring to Fig-
ure 1 (b-d). Consider a language `&apos; evolving to `
for cognate set c. Assume we have a word form
x = wcl,. The generative process for producing
y = wcl works as follows. First, we consider
x to be composed of characters x1x2 ... xn, with
the first and last being a special boundary symbol
x1 = # E E which is never deleted, mutated, or
created. The process generates y = y1y2 ... yn in
n chunks yi E E*, i E {1, ... , n}, one for each xi.
/n/. We write S = E U {ζ} for this set
of outcomes, where ζ is the special outcome indi-
cating deletion. Importantly, the probabilities of this
on both the previous char-
,far (i.e. the rightmost character
the current character in the previous
. As we will see shortly, this al-
arkedness and faithfulness at every
y. This multinomial decision acts as
ons continue until
is selected. In
the example,rawe follow the substitution of
/ to /n/
with anninserti
ζ
N
on of /g/, followed by a decision to
stop that yi. We will use θS,t,p,` and θI,t,p,` to denote
the probabilities over the substitution and insertion
decisions in the current branch `&apos; _* `.
A similar process generates the word at the root
E of a tree, treating this word as a single string
yi generated from a dummy ancestor t = x1. In
</bodyText>
<subsectionHeader confidence="0.803319">
s a y
</subsectionHeader>
<bodyText confidence="0.932192571428571">
rk
this case, only the insertion probabilities matter, and
we separately parameterize these probabilities with
θR,t,p,`. There is no actual dependence on t at the
root, but this formulation allows us to unify the pa-
rameterization, with each θω,t,p,` E R|F-|+1 where
ω E {R, S, I}.
</bodyText>
<subsectionHeader confidence="0.999379">
3.2 Parameterization
</subsectionHeader>
<bodyText confidence="0.975104090909091">
r
are
ed
Instead of directly estimating the transition proba-
bilities of the mutation Markov chain (as the param-
eters of a collection of multinomial distributions) we
.s
t
i ly encou
traints on markedness for each 1
be well-formed. In contrast, we in
s angu
general and branch-specific constraints
ness. This is done using a lexical
al.,
substitutes to
de
is/ to multinomial can d
acter generated so
in this process.np of yi_1) and
on trees: generation string (t)
lowsomodelling m
branch, jointl
ted i
and output
d output.
evious computa-
based almost p in yi. Inserti
through a mu-
language atmthe
raged to
corporate con-
age with both
on faithful-
ized stochastic
2008).
the initial distribution of the mutation Markov chain.
Weoconsider insertions only if a deletion was not
selec n the first step. Here, we draw from a
multinomial over Y, where this time the special out-
comethcorresponds to stopping insertions, and the
other elements of correspond to symbols that are
appendedrtowyz.fInmthis case, the conditioning envi-
ronment is t =sxi and the current rightmost symbol
</bodyText>
<page confidence="0.996989">
67
</page>
<bodyText confidence="0.999586074074074">
express them as the output of a log-linear model. We
used the following feature templates:
OPERATION identifies whether an operation in the
mutation Markov chain is an insertion, a deletion,
a substitution, a self-substitution (i.e. of the form
x —* y, x = y), or the end of an insertion event.
Examples in Figure 1 (d): 1[Subst] and 1[Insert].
MARKEDNESS consists of language-specific n-
gram indicator functions for all symbols in E. Only
unigram and bigram features are used for computa-
tional reasons, but we show in Section 5 that this
already captures important constraints. Examples in
Figure 1 (d): the bigram indicator 1[(n g)@Kw] (Kw
stands for Kwara’ae, a language of the Solomon
Islands), the unigram indicators 1[(n)@Kw] and
1[(g)@Kw].
FAITHFULNESS consists of indicators for muta-
tion events of the form 1[x —* y], where x E E,
y E Y. Examples: 1[q —* n], 1[q —* n@Kw].
Feature templates similar to these can be found
for instance in Dreyer et al. (2008) and Chen (2003),
in the context of string-to-string transduction. Note
also the connection with stochastic OT (Goldwater
and Johnson, 2003; Wilson, 2006), where a log-
linear model mediates markedness and faithfulness
of the production of an output form from an under-
lying input form.
</bodyText>
<subsectionHeader confidence="0.999449">
3.3 Parameter sharing
</subsectionHeader>
<bodyText confidence="0.999978268292683">
Data sparsity is a significant challenge in protolan-
guage reconstruction. While the experiments we
present here use an order of magnitude more lan-
guages than previous computational approaches, the
increase in observed data also brings with it addi-
tional unknowns in the form of intermediate pro-
tolanguages. Since there is one set of parameters
for each language, adding more data is not sufficient
for increasing the quality of the reconstruction: we
show in Section 5.2 that adding extra languages can
actually hurt reconstruction using previous methods.
It is therefore important to share parameters across
different branches in the tree in order to benefit from
having observations from more languages.
As an example of useful parameter sharing, con-
sider the faithfulness features 1[/p/ —* /b/] and
1[/p/ —* /r/], which are indicator functions for the
appearance of two substitutions for /p/. We would
like the model to learn that the former event (a sim-
ple voicing change) should be preferred over the lat-
ter. In Bouchard-Cˆot´e et al. (2008), this has to be
learned for each branch in the tree. The difficulty is
that not all branches will have enough information
to learn this preference, meaning that we need to de-
fine the model in such a way that it can generalize
across languages.
We used the following technique to address this
problem: we augment the sufficient statistics of
Bouchard-Cˆot´e et al. (2008) to include the current
language (or language at the bottom of the current
branch) and use a single, global weight vector in-
stead of a set of branch-specific weights. Gener-
alization across branches is then achieved by using
features that ignore `, while branch-specific features
depend on `.
For instance, in Figure 1 (d), 1[q —* n] is
an example of a universal (global) feature shared
across all branches while 1[q —* n@Kw] is branch-
specific. Similarly, all of the features in OPERA-
TION, MARKEDNESS and FAITHFULNESS have uni-
versal and branch-specific versions.
</bodyText>
<subsectionHeader confidence="0.950313">
3.4 Objective function
</subsectionHeader>
<bodyText confidence="0.946228818181818">
Concretely, the transition probabilities of the muta-
tion and root generation are given by:
exp{(λ,f(ω,t,p,`, ξ))} θω,t,p,�(ξ) =x µ(ω,t,ξ),
Z (ω, t, p, `, λ)
where ξ E Y, f : {S,I,R}xExExLxY —* Rk
is the sufficient statistics or feature function, (·, ·)
denotes inner product and λ E Rk is a weight vector.
Here, k is the dimensionality of the feature space of
the log-linear model. In the terminology of exponen-
tial families, Z and µ are the normalization function
and reference measure respectively:
</bodyText>
<equation confidence="0.9894965">
Z(ω, t, p, `, λ) = E exp{(λ, f(ω,t,p,`,ξ/))}
ξ�∈S
I 0 if ω = S,t = #, ξ =�#
0 if ω = R,ξ = ζ
0 if ω =� R,ξ = #
1 o.w.
</equation>
<bodyText confidence="0.992256142857143">
Here, µ is used to handle boundary conditions.
We will also need the following notation: let
P),(·), P),(·|·) denote the root and branch probabil-
ity models described in Section 3.1 (with transition
probabilities given by the above log-linear model),
I(c), the set of internal (non-leaf) nodes in τ(c),
pa(`), the parent of language `, r(c), the root of τ(c)
</bodyText>
<equation confidence="0.873933">
µ(ω, t, ξ) =
</equation>
<page confidence="0.977805">
68
</page>
<bodyText confidence="0.998064">
and W(c) = (E∗)|I(c)|. We can summarize our ob-
jective function as follows:
</bodyText>
<equation confidence="0.765337">
Pλ(wc,`|wc,pa(`)) − ||λ||2 2
2σ2
</equation>
<bodyText confidence="0.9996505">
The second term is a standard L2 regularization
penalty (we used Q2 = 1).
</bodyText>
<sectionHeader confidence="0.958584" genericHeader="method">
4 Learning algorithm
</sectionHeader>
<bodyText confidence="0.999966166666667">
Learning is done using a Monte Carlo variant of the
Expectation-Maximization (EM) algorithm (Demp-
ster et al., 1977). The M step is convex and com-
puted using L-BFGS (Liu et al., 1989); but the E
step is intractable (Lunter et al., 2003), so we used
a Markov chain Monte Carlo (MCMC) approxima-
tion (Tierney, 1994). At E step t = 1, 2, ... , we
simulated the chain for O(t) iterations; this regime
is necessary for convergence (Jank, 2005).
In the E step, the inference problem is to com-
pute an expectation under the posterior over strings
in a protolanguage given observed word forms at the
leaves of the tree. The typical approach in biology
or historical linguistics (Holmes and Bruno, 2001;
Bouchard-Cˆot´e et al., 2008) is to use Gibbs sam-
pling, where the entire string at a single node in the
tree is sampled, conditioned on its parent and chil-
dren. This sampling domain is shown in Figure 1 (e),
where the middle word is completely resampled but
adjacent words are fixed. We will call this method
Single Sequence Resampling (SSR). While concep-
tually simple, this approach suffers from problems
in large trees (Holmes and Bruno, 2001). Con-
sequently, we use a different MCMC procedure,
called Ancestry Resampling (AR) that alleviates
the mixing problems (Figure 1 (f)). This method
was originally introduced for biological applications
(Bouchard-Cˆot´e et al., 2009), but commonalities be-
tween the biological and linguistic cases make it
possible to use it in our model.
Concretely, the problem with SSR arises when the
tree under consideration is large or unbalanced. In
this case, it can take a long time for information
from the observed languages to propagate to the root
of the tree. Indeed, samples at the root will ini-
tially be independent of the observations. AR ad-
dresses this problem by resampling one thin vertical
slice of all sequences at a time, called an ancestry.
For the precise definition, see Bouchard-Cˆot´e et al.
(2009). Slices condition on observed data, avoiding
the problems mentioned above, and can propagate
information rapidly across the tree.
</bodyText>
<sectionHeader confidence="0.998938" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999987">
We performed a comprehensive set of experiments
to test the new method for reconstruction outlined
above. In Section 5.1, we analyze in isolation the
effects of varying the set of features, the number of
observed languages, the topology, and the number
of iterations of EM. In Section 5.2 we compare per-
formance to an oracle and to three other systems.
Evaluation of all methods was done by computing
the Levenshtein distance (Levenshtein, 1966) be-
tween the reconstruction produced by each method
and the reconstruction produced by linguists. We
averaged this distance across reconstructed words to
report a single number for each method. We show
in Table 2 the average word length in each corpus;
note that the Latin average is much larger, giving
an explanation to the higher errors in the Romance
dataset. The statistical significance of all perfor-
mance differences are assessed using a paired t-test
with significance level of 0.05.
</bodyText>
<subsectionHeader confidence="0.995221">
5.1 Evaluating system performance
</subsectionHeader>
<bodyText confidence="0.999529842105263">
We used the Austronesian Basic Vocabulary
Database (Greenhill et al., 2008) as the basis for
a series of experiments used to evaluate the per-
formance of our system and the factors relevant to
its success. The database includes partial cognacy
judgments and IPA transcriptions, as well as a few
reconstructed protolanguages. A reconstruction of
Proto-Oceanic (POc) originally developed by Blust
(1993) using the comparative method was the basis
for evaluation.
We used the cognate information provided in
the database, automatically constructing a global
tree2 and set of subtrees from the cognate set in-
dicator matrix M(E, c) = 1[E E L(c)], c E
{1, ... , C}, E E L. For constructing the global tree,
we used the implementation of neighbor joining in
the Phylip package (Felsenstein, 1989). We used
a distance based on cognates overlap, dc(E1, E2) =
ECc=1 M(E1, c)M(E2, c). We bootstrapped 1000
</bodyText>
<footnote confidence="0.924518">
2The dataset included a tree, but it was out of date as of
November 2008 (Greenhill et al., 2008).
</footnote>
<equation confidence="0.983672">
C
X log X 11 Pλ(wc,r(c))
c=1 ~w∈W(c) `∈I(c)
</equation>
<page confidence="0.992063">
69
</page>
<figure confidence="0.961603472222222">
Condition Edit dist.
Unsupervised full system 1.87
-FAITHFULNESS 2.02
-MARKEDNESS 2.18
-Sharing 1.99
-Topology 2.06
Semi-supervised system 1.75
10
Itrat
0 20
Error
MARK
2.2 2
Sh i
2.6
AIO
1.4 1.4
0 30 60
rre-
1.8 1.
t
ean istnce to the targt recons
n laguags to proto Oceanic. Th
N. of modern lang
di
. EM iteration
stnce to the target re
3.63.6
2
3
8
6
2.42.4
2
2
1.81.8
</figure>
<figureCaption confidence="0.981346">
Figure 2: Left: Mean distance to the target reconstruction
</figureCaption>
<figure confidence="0.848760641025641">
s 5.2 Comparsons againt other methods
ed an pemen p The first two competng methods, PRAGUE and
d of POc as a function of the number of modern languages
ones bg
model fo ech un It is semisupervisd in
by increasing he number of language BCLKG are described in Oakes (2000) and
sense tha gold econsructin for many nternal
c used by the inference procedure. Right: Mean distance
Te reslts are reported in Figure 4. They cn-
Bouchardˆot´e et al. (208) respectvely and sum-
aibl
t h an
ir Se p
of g Kw and Lau in Fire 6)3
mtic proto-language rconstrution ging from 2-
well to large dataets. In the rst cas, the bottleneck
n and confidence intervals as a function of the EM iteration,
8, 16to gu gfi th le utn
e gure 6 shows th results of a cncrete run ove
2 averaged over 20 random seeds and ran on 4 languages.
cantly helped reconsuction There was sill an av-
withou guide trees nd the vanishing probabilit
zoi i i f h Sli
d iaram
rus e
samples and formed an accurate (90%) consensus
uc) , o
This third baelne, CENTROID, computes the
global and the lcal feaues the expectaios
- identify the ontributn made by diffrent ftors it
ac etree.nTheetreesobtainedyis not binary, but the AR
inferenceralgorithm scales linearly in the branching 5.
R factor ofothe tree (in contrast, SSR scaleseexponen-
tially (Lunter/et al.,t2003)).
y p
ons r pg aur
Thegfirst claim wedverified experimentallyeriscthat ii
o e
having more observed languages aids reconstruction
</figure>
<figureCaption confidence="0.649481">
is intractable. As an approximation, we considered
</figureCaption>
<bodyText confidence="0.964036285714286">
- f thes chang which ar active in that branh,
ments are shown in Table 2.
onl ing bl b k i bi
f op uded sme
nt of protolanguages. To test this hypothesis we added
taken from the word forms in O If k = 1, then it
able the performance of a semiupervsed sysem
i il to taki h i O At th
y d Th y
observedumodern languagesnin increasing=orderxaof i
id d
distancewdc to the target reconstructionkof POcdso
that the languages that are most useful for POc re-
construction are added first. This prevents the ef-
fects of adding a close language after several distant
ones being confused with an improvement produced
by increasing the number of languages.
The results are reported in Figure 2 (a). They con-
firm that large-scale inference is desirable for au-
tomatic protolanguage reconstruction: reconstruc-
tion improved statistically significantly with each in-
crease except from 32 to 64 languages, where the
average edit distance improvement was 0.05.
We then conducted a number of experiments in-
tended to assess the robustness of the system, and to
identify the contribution made by different factors it
incorporates. First, we ran the system with 20 dif-
ferent random seeds to assess the stability of the so-
lutions found. In each case, learning was stable and
accuracy improved during training. See Figure 2 (b).
Next, we found that all of the following ablations
significantly hurt reconstruction: using a flat tree (in
which all languages are equidistant from the recon-
structed root and from each other) instead of the con-
sensus tree, dropping the markedness features, drop-
</bodyText>
<tableCaption confidence="0.646938777777778">
Table 1: Effects of ablation of various aspects of our
unsupervised system on mean edit distance to POc.
-Sharing corresponds to the restriction to the subset of the
features in OPERATION, FAITHFULNESS and MARKED-
NESS that are branch-specific, -Topology corresponds to
using a flat topology where the only edges in the tree con-
nect modern languages to POc. The semi-supervised sys-
tem is described in the text. All differences (compared to
the unsupervised full system) are statistically significant.
</tableCaption>
<bodyText confidence="0.996548807692308">
ping the faithfulness features, and disabling sharing
across branches. The results of these experiments
are shown in Table 1.
For comparison, we also included in the same
table the performance of a semi-supervised system
trained by K-fold validation. The system was ran
K = 5 times, with 1− K−1 of the POc words given
to the system as observations in the graphical model
for each run. It is semi-supervised in the sense that
gold reconstruction for many internal nodes are not
available in the dataset (for example the common an-
cestor of Kwara’ae (Kw.) and Lau in Figure 3 (b)),
so they are still not filled.3
Figure 3 (b) shows the results of a concrete run
over 32 languages, zooming in to a pair of the
Solomonic languages and the cognate set from Fig-
ure 1 (a). In the example shown, the reconstruc-
tion is as good as the ORACLE (described in Sec-
tion 5.2), though off by one character (the final /s/
is not present in any of the 32 inputs and therefore
is not reconstructed). In (a), diagrams show, for
both the global and the local (Kwara’ae) features,
the expectations of each substitution superimposed
on an IPA sound chart, as well as a list of the top
changes. Darker lines indicate higher counts. This
run did not use natural class constraints, but it can
</bodyText>
<footnote confidence="0.99445175">
3We also tried a fully supervised system where a flat topol-
ogy is used so that all of these latent internal nodes are avoided;
but it did not perform as well—this is consistent with the
-Topology experiment of Table 1.
</footnote>
<page confidence="0.997997">
70
</page>
<bodyText confidence="0.999897555555555">
be seen that linguistically plausible substitutions are
learned. The global features prefer a range of voic-
ing changes, manner changes, adjacent vowel mo-
tion, and so on, including mutations like /s/ to /h/
which are common but poorly represented in a naive
attribute-based natural class scheme. On the other
hand, the features local to the language Kwara’ae
pick out the subset of these changes which are ac-
tive in that branch, such as /s/—*/t/ fortition.
</bodyText>
<subsectionHeader confidence="0.999877">
5.2 Comparisons against other methods
</subsectionHeader>
<bodyText confidence="0.999914945945946">
The first two competing methods, PRAGUE and
BCLKG, are described in Oakes (2000) and
Bouchard-Cˆot´e et al. (2008) respectively and sum-
marized in Section 1. Neither approach scales well
to large datasets. In the first case, the bottleneck is
the complexity of computing multi-alignments with-
out guide trees and the vanishing probability that in-
dependent reconstructions agree. In the second case,
the problem comes from the unregularized prolifera-
tion of parameters and slow mixing of the inference
algorithm. For this reason, we built a third baseline
that scales well in large datasets.
This third baseline, CENTROID, computes the
centroid of the observed word forms in Leven-
shtein distance. Let L(x, y) denote the Lev-
enshtein distance between word forms x and
y. Ideally, we would like the baseline to
return argminxEΣ∗ EyEO L(x, y), where O =
{y1, ... , y|O|1 is the set of observed word forms.
Note that the optimum is not changed if we restrict
the minimization to be taken on x E E(O)* such
that m &lt; |x |&lt; M where m = mini |yi|, M =
maxi |yi |and E(O) is the set of characters occurring
in O. Even with this restriction, this optimization
is intractable. As an approximation, we considered
only strings built by at most k contiguous substrings
taken from the word forms in O. If k = 1, then it
is equivalent to taking the min over x E O. At the
other end of the spectrum, if k = M, it is exact.
This scheme is exponential in k, but since words are
relatively short, we found that k = 2 often finds the
same solution as higher values of k. The difference
was in all the cases not statistically significant, so we
report the approximation k = 2 in what follows.
We also compared against an oracle, denoted OR-
ACLE, which returns argminyEOL(y, x*), where x*
is the target reconstruction. We will denote it by OR-
</bodyText>
<table confidence="0.999519857142857">
Comparison CENTROID PRAGUE BCLKG
Protolanguage POc PMJ La
Heldout (prop.) 243 (1.0) 79 (1.0) 293 (0.5)
Modern languages 70 4 2
Cognate sets 1321 179 583
Observed words 10783 470 1463
Mean word length 4.5 5.0 7.4
</table>
<tableCaption confidence="0.86853025">
Table 2: Experimental setup: number of held-out proto-
word from (absolute and relative), of modern languages,
cognate sets and total observed words. The split for
BCLKG is the same as in Bouchard-Cˆot´e et al. (2008).
</tableCaption>
<bodyText confidence="0.999907777777778">
ACLE. This is superior to picking a single closest
language to be used for all word forms, but it is pos-
sible for systems to perform better than the oracle
since it has to return one of the observed word forms.
We performed the comparison against Oakes
(2000) and Bouchard-Cˆot´e et al. (2008) on the same
dataset and experimental conditions as those used in
the respective papers (see Table 2). Note that the
setup of Bouchard-Cˆot´e et al. (2008) provides super-
vision (half of the Latin word forms are provided);
all of the other comparisons are performed in a com-
pletely unsupervised manner.
The PMJ dataset was compiled by Nothofer
(1975), who also reconstructed the corresponding
protolanguage. Since PRAGUE is not guaranteed to
return a reconstruction for each cognate set, only 55
word forms could be directly compared to our sys-
tem. We restricted comparison to this subset of the
data. This favors PRAGUE since the system only pro-
poses a reconstruction when it is certain. Still, our
system outperformed PRAGUE, with an average dis-
tance of 1.60 compared to 2.02 for PRAGUE. The
difference is marginally significant, p = 0.06, partly
due to the small number of word forms involved.
We also exceeded the performance of BCLKG on
the Romance dataset. Our system’s reconstruction
had an edit distance of 3.02 to the truth against 3.10
for BCLKG. However, this difference was not signifi-
cant (p = 0.15). We think this is because of the high
level of noise in the data (the Romance dataset is the
only dataset we consider that was automatically con-
structed rather than curated by linguists). A second
factor contributing to this small difference may be
that the the experimental setup of BCLKG used very
few languages, while the performance of our system
improves markedly with more languages.
</bodyText>
<page confidence="0.990424">
71
</page>
<figure confidence="0.999812741666667">
Snd
(a) m 8 n 6 r � (b) (c)
a −→ e Jv (d) POc (e) La
l −→ r ; kg qc 2 7 Mad
s −→ h
k −→ g
r −→ l
/t
fi Mal
N −→ v
g −→ k
s −→ t
e − i
−→ n
01 a
�
pb t d t d c
pb td td c
s
m rq n
s
fv
fv
u
u
BS sz J3 f ; 5j xx a MxS
08 sz J f
3 ; qj xx a fi5 HS
i
i
A
Q P 7
t
i
i
� kg q
w
W
N
G
2
7
hfi
h
/angi/ (Kw.)
/axii/
c
asi/ (PO
)
/axii/ (Lau)
....
Nggela
Bugotu
Tape
Avava
Neveei
Naman
Nese
SantaAna
Nahavaq
Nati
KwaraaeSol
Lau
Kwamera
Tolo
Marshalles
PuloAnna
ChuukeseAK
SaipanCaro
Puluwatese
Woleaian
PuloAnnan
Carolinian
Woleai
Chuukese
Nauna
PaameseSou
Anuta
VaeakauTau
Takuu
Tokelau
Tongan
Samoan
IfiraMeleM
Tikopia
Tuvalu
Niue
FutunaEast
UveaEast
Rennellese
Emae
Kapingamar
Sikaiana
Nukuoro
Luangiua
Hawaiian
Marquesan
Tahitianth
Rurutuan
Maori
Tuamotu
Mangareva
Rarotongan
Penrhyn
RapanuiEas
Pukapuka
Mwotlap
Mota
FijianBau
Namakir
Nguna
ArakiSouth
Saa
Raga
PeteraraMa
MJ
P
It
Es
Pt
</figure>
<figureCaption confidence="0.958615">
Figure 3: (a) A visualization of two learned faithfulness parameters: on the top, from the universal features, on
the bottom, for one particular branch. Each pair of phonemes have a link with grayscale value proportional to the
expectation of a transition between them. The five strongest links are also included at the right. (b) A sample taken
from our POc experiments (see text). (c-e) Phylogenetic trees for three language families: Proto-Malayo-Javanic,
Austronesian and Romance.
</figureCaption>
<bodyText confidence="0.953037">
We conducted another experiment to verify this
by running both systems in larger trees. Because the
</bodyText>
<figure confidence="0.39429125">
1
Romance dataset had only three modern languages
transcribed in IPA, we used the Austronesian dataset
1
</figure>
<bodyText confidence="0.999581625">
to perform the test. The results were all significant in
this setup: while our method went from an edit dis-
tance of 2.01 to 1.79 in the 4-to-8 languages exper-
iment described in Section 5.1, BCLKG went from
3.30 to 3.38. This suggests that more languages can
actually hurt systems that do not support parameter
sharing.
Since we have shown evidence that PRAGUE and
BCLKG do not scale well to large datasets, we
also compared against ORACLE and CENTROID in a
large-scale setting. Specifically, we compare to the
experimental setup on 64 modern languages used to
reconstruct POc described before. Encouragingly,
while the system’s average distance (1.49) does not
attain that of the ORACLE (1.13), we significantly
outperform the CENTROID baseline (1.79).
</bodyText>
<subsectionHeader confidence="0.993106">
5.3 Incorporating prior linguistic knowledge
</subsectionHeader>
<bodyText confidence="0.999615714285714">
The model also supports the addition of prior lin-
guistic knowledge. This takes the form of feature
templates with more internal structure. We per-
formed experiments with an additional feature tem-
plate:
STRUCT-FAITHFULNESS is a structured version of
FAITHFULNESS, replacing x and y with their natu-
ral classes Nβ(x) and Nβ(y) where Q indexes types
of classes, ranging over {manner, place, phonation,
isOral, isCentral, height, backness, roundedness}.
This feature set is reminiscent of the featurized rep-
resentation of Kondrak (2000).
We compared the performance of the system with
and without STRUCT-FAITHFULNESS to check if the
algorithm can recover the structure of natural classes
in an unsupervised fashion. We found that with
2 or 4 observed languages, FAITHFULNESS under-
performed STRUCT-FAITHFULNESS, but for larger
trees, the difference was not significant. FAITH-
FULNESS even slightly outperformed its structured
cousin with 16 observed languages.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999943">
By enriching our model to include important fea-
tures like markedness, and by scaling up to much
larger data sets than were previously possible, we
obtained substantial improvements in reconstruc-
tion quality, giving the best results on past data
sets. While many more complex phenomena are
still unmodeled, from reduplication to borrowing to
chained sound shifts, the current approach signifi-
cantly increases the power, accuracy, and efficiency
of automatic reconstruction.
</bodyText>
<sectionHeader confidence="0.998822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9984708">
We would like to thank Anna Rafferty and our re-
viewers for their comments. This work was sup-
ported by a NSERC fellowship to the first author and
NSF grant number BCS-0631518 to the second au-
thor.
</bodyText>
<page confidence="0.997608">
72
</page>
<sectionHeader confidence="0.993903" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999896639534884">
R. Blust. 1993. Central and central-Eastern Malayo-
Polynesian. Oceanic Linguistics, 32:241–293.
A. Bouchard-Cˆot´e, P. Liang, D. Klein, and T. L. Griffiths.
2008. A probabilistic approach to language change. In
Advances in Neural Information Processing Systems
20.
A. Bouchard-Cˆot´e, M. I. Jordan, and D. Klein. 2009.
Efficient inference in phylogenetic InDel trees. In Ad-
vances in Neural Information Processing Systems 21.
L. Campbell. 1998. Historical Linguistics. The MIT
Press.
S. F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proceedings of
Eurospeech.
M. A. Covington. 1998. Alignment of multiple lan-
guages for historical comparison. In Proceedings of
ACL 1998.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1–38.
M. Dreyer, J. R. Smith, and J. Eisner. 2008. Latent-
variable modeling of string transductions with finite-
state methods. In Proceedings of EMNLP 2008.
S. P. Durham and D. E. Rogers. 1969. An application
of computer programming to the reconstruction of a
proto-language. In Proceedings of the 1969 confer-
ence on Computational linguistics.
C. L. Eastlack. 1977. Iberochange: A program to
simulate systematic sound change in Ibero-Romance.
Computers and the Humanities.
J. Felsenstein. 1989. PHYLIP - PHYLogeny Inference
Package (Version 3.2). Cladistics, 5:164–166.
S. Goldwater and M. Johnson. 2003. Learning OT
constraint rankings using a maximum entropy model.
Proceedings of the Workshop on Variation within Op-
timality Theory.
S. J. Greenhill, R. Blust, and R. D. Gray. 2008. The
Austronesian basic vocabulary database: From bioin-
formatics to lexomics. Evolutionary Bioinformatics,
4:271–283.
H. H. Hock. 1986. Principles of Historical Linguistics.
Walter de Gruyter.
I. Holmes and W. J. Bruno. 2001. Evolutionary HMM:
a Bayesian approach to multiple alignment. Bioinfor-
matics, 17:803–820.
W. Jank. 2005. Stochastic variants of EM: Monte Carlo,
quasi-Monte Carlo and more. In Proceedings of the
American Statistical Association.
G. Kondrak. 2000. A new algorithm for the alignment of
phonetic sequences. In Proceedings of NAACL 2000.
G. Kondrak. 2002. Algorithms for Language Recon-
struction. Ph.D. thesis, University of Toronto.
V. I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions and reversals. Soviet Physics
Doklady, 10, February.
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503–528.
J. B. Lowe and M. Mazaudon. 1994. The reconstruction
engine: a computer implementation of the comparative
method. Comput. Linguist., 20(3):381–417.
G. A. Lunter, I. Mikl´os, Y. S. Song, and J. Hein. 2003.
An efficient algorithm for statistical multiple align-
ment on arbitrary phylogenetic trees. Journal of Com-
putational Biology, 10:869–889.
B. Nothofer. 1975. The reconstruction of Proto-Malayo-
Javanic. M. Nijhoff.
M. P. Oakes. 2000. Computer estimation of vocabu-
lary in a protolanguage from word lists in four daugh-
ter languages. Journal of Quantitative Linguistics,
7(3):233–244.
A. Prince and P. Smolensky. 1993. Optimality theory:
Constraint interaction in generative grammar. Techni-
cal Report 2, Rutgers University Center for Cognitive
Science.
L. Tierney. 1994. Markov chains for exploring posterior
distributions. The Annals of Statistics, 22(4):1701–
1728.
A. Varadarajan, R. K. Bradley, and I. H. Holmes. 2008.
Tools for simulating evolution of aligned genomic re-
gions with integrated parameter estimation. Genome
Biology, 9:R147.
C. Wilson. 2006. Learning phonology with substantive
bias: An experimental and computational study of ve-
lar palatalization. Cognitive Science, 30.5:945–982.
</reference>
<page confidence="0.999297">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.854745">
<title confidence="0.999934">Improved Reconstruction of Protolanguage Word Forms</title>
<author confidence="0.999987">Thomas L Dan</author>
<affiliation confidence="0.9949465">Science Division of University of California at</affiliation>
<address confidence="0.999263">Berkeley, CA 94720</address>
<abstract confidence="0.975739111111111">We present an unsupervised approach to reconstructing ancient word forms. The present work addresses three limitations of previous First, previous work focused on faithwhich model changes besuccessive languages. We add markedwhich model well-formedness within each language. Second, we introduce universal features, which support generalizations across languages. Finally, we increase the number of languages to which these methods can be applied by an order of magnitude by using improved inference methods. Experiments on the reconstruction of Proto- Oceanic, Proto-Malayo-Javanic, and Classical Latin show substantial reductions in error rate, giving the best results to date.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Blust</author>
</authors>
<date>1993</date>
<booktitle>Central and central-Eastern MalayoPolynesian. Oceanic Linguistics,</booktitle>
<pages>32--241</pages>
<contexts>
<context position="1419" citStr="Blust (1993)" startWordPosition="207" endWordPosition="208">struction of ProtoOceanic, Proto-Malayo-Javanic, and Classical Latin show substantial reductions in error rate, giving the best results to date. 1 Introduction A central problem in diachronic linguistics is the reconstruction of ancient languages from their modern descendants (Campbell, 1998). Here, we consider the problem of reconstructing phonological forms, given a known linguistic phylogeny and known cognate groups. For example, Figure 1 (a) shows a collection of word forms in several Oceanic languages, all meaning to cry. The ancestral form in this case has been presumed to be /taNis/ in Blust (1993). We are interested in models which take as input many such word tuples, each representing a cognate group, along with a language tree, and induce word forms for hidden ancestral languages. The traditional approach to this problem has been the comparative method, in which reconstructions are done manually using assumptions about the relative probability of different kinds of sound change (Hock, 1986). There has been work attempting to automate part (Durham and Rogers, 1969; Eastlack, 1977; Lowe and Mazaudon, 1994; Covington, 1998; 65 Kondrak, 2002) or all of the process (Oakes, 2000; Bouchard-</context>
<context position="21264" citStr="Blust (1993)" startWordPosition="3541" endWordPosition="3542">tion to the higher errors in the Romance dataset. The statistical significance of all performance differences are assessed using a paired t-test with significance level of 0.05. 5.1 Evaluating system performance We used the Austronesian Basic Vocabulary Database (Greenhill et al., 2008) as the basis for a series of experiments used to evaluate the performance of our system and the factors relevant to its success. The database includes partial cognacy judgments and IPA transcriptions, as well as a few reconstructed protolanguages. A reconstruction of Proto-Oceanic (POc) originally developed by Blust (1993) using the comparative method was the basis for evaluation. We used the cognate information provided in the database, automatically constructing a global tree2 and set of subtrees from the cognate set indicator matrix M(E, c) = 1[E E L(c)], c E {1, ... , C}, E E L. For constructing the global tree, we used the implementation of neighbor joining in the Phylip package (Felsenstein, 1989). We used a distance based on cognates overlap, dc(E1, E2) = ECc=1 M(E1, c)M(E2, c). We bootstrapped 1000 2The dataset included a tree, but it was out of date as of November 2008 (Greenhill et al., 2008). C X log</context>
</contexts>
<marker>Blust, 1993</marker>
<rawString>R. Blust. 1993. Central and central-Eastern MalayoPolynesian. Oceanic Linguistics, 32:241–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bouchard-Cˆot´e</author>
<author>P Liang</author>
<author>D Klein</author>
<author>T L Griffiths</author>
</authors>
<title>A probabilistic approach to language change.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 20.</booktitle>
<marker>Bouchard-Cˆot´e, Liang, Klein, Griffiths, 2008</marker>
<rawString>A. Bouchard-Cˆot´e, P. Liang, D. Klein, and T. L. Griffiths. 2008. A probabilistic approach to language change. In Advances in Neural Information Processing Systems 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bouchard-Cˆot´e</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Efficient inference in phylogenetic InDel trees.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 21.</booktitle>
<marker>Bouchard-Cˆot´e, Jordan, Klein, 2009</marker>
<rawString>A. Bouchard-Cˆot´e, M. I. Jordan, and D. Klein. 2009. Efficient inference in phylogenetic InDel trees. In Advances in Neural Information Processing Systems 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Campbell</author>
</authors>
<title>Historical Linguistics.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="1100" citStr="Campbell, 1998" startWordPosition="153" endWordPosition="154">ess features, which model well-formedness within each language. Second, we introduce universal features, which support generalizations across languages. Finally, we increase the number of languages to which these methods can be applied by an order of magnitude by using improved inference methods. Experiments on the reconstruction of ProtoOceanic, Proto-Malayo-Javanic, and Classical Latin show substantial reductions in error rate, giving the best results to date. 1 Introduction A central problem in diachronic linguistics is the reconstruction of ancient languages from their modern descendants (Campbell, 1998). Here, we consider the problem of reconstructing phonological forms, given a known linguistic phylogeny and known cognate groups. For example, Figure 1 (a) shows a collection of word forms in several Oceanic languages, all meaning to cry. The ancestral form in this case has been presumed to be /taNis/ in Blust (1993). We are interested in models which take as input many such word tuples, each representing a cognate group, along with a language tree, and induce word forms for hidden ancestral languages. The traditional approach to this problem has been the comparative method, in which reconstr</context>
</contexts>
<marker>Campbell, 1998</marker>
<rawString>L. Campbell. 1998. Historical Linguistics. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
</authors>
<title>Conditional and joint models for grapheme-to-phoneme conversion.</title>
<date>2003</date>
<booktitle>In Proceedings of Eurospeech.</booktitle>
<contexts>
<context position="14317" citStr="Chen (2003)" startWordPosition="2384" endWordPosition="2385">anguage-specific ngram indicator functions for all symbols in E. Only unigram and bigram features are used for computational reasons, but we show in Section 5 that this already captures important constraints. Examples in Figure 1 (d): the bigram indicator 1[(n g)@Kw] (Kw stands for Kwara’ae, a language of the Solomon Islands), the unigram indicators 1[(n)@Kw] and 1[(g)@Kw]. FAITHFULNESS consists of indicators for mutation events of the form 1[x —* y], where x E E, y E Y. Examples: 1[q —* n], 1[q —* n@Kw]. Feature templates similar to these can be found for instance in Dreyer et al. (2008) and Chen (2003), in the context of string-to-string transduction. Note also the connection with stochastic OT (Goldwater and Johnson, 2003; Wilson, 2006), where a loglinear model mediates markedness and faithfulness of the production of an output form from an underlying input form. 3.3 Parameter sharing Data sparsity is a significant challenge in protolanguage reconstruction. While the experiments we present here use an order of magnitude more languages than previous computational approaches, the increase in observed data also brings with it additional unknowns in the form of intermediate protolanguages. Sin</context>
</contexts>
<marker>Chen, 2003</marker>
<rawString>S. F. Chen. 2003. Conditional and joint models for grapheme-to-phoneme conversion. In Proceedings of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Covington</author>
</authors>
<title>Alignment of multiple languages for historical comparison.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="1954" citStr="Covington, 1998" startWordPosition="290" endWordPosition="291"> The ancestral form in this case has been presumed to be /taNis/ in Blust (1993). We are interested in models which take as input many such word tuples, each representing a cognate group, along with a language tree, and induce word forms for hidden ancestral languages. The traditional approach to this problem has been the comparative method, in which reconstructions are done manually using assumptions about the relative probability of different kinds of sound change (Hock, 1986). There has been work attempting to automate part (Durham and Rogers, 1969; Eastlack, 1977; Lowe and Mazaudon, 1994; Covington, 1998; 65 Kondrak, 2002) or all of the process (Oakes, 2000; Bouchard-Cˆot´e et al., 2008). However, previous automated methods have been unable to leverage three important ideas a linguist would employ. We address these omissions here, resulting in a more powerful method for automatically reconstructing ancient protolanguages. First, linguists triangulate reconstructions from many languages, while past work has been limited to small numbers of languages. For example, Oakes (2000) used four languages to reconstruct Proto-Malayo-Javanic (PMJ) and Bouchard-Cˆot´e et al. (2008) used two languages to r</context>
</contexts>
<marker>Covington, 1998</marker>
<rawString>M. A. Covington. 1998. Alignment of multiple languages for historical comparison. In Proceedings of ACL 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="17920" citStr="Dempster et al., 1977" startWordPosition="2991" endWordPosition="2995">ollowing notation: let P),(·), P),(·|·) denote the root and branch probability models described in Section 3.1 (with transition probabilities given by the above log-linear model), I(c), the set of internal (non-leaf) nodes in τ(c), pa(`), the parent of language `, r(c), the root of τ(c) µ(ω, t, ξ) = 68 and W(c) = (E∗)|I(c)|. We can summarize our objective function as follows: Pλ(wc,`|wc,pa(`)) − ||λ||2 2 2σ2 The second term is a standard L2 regularization penalty (we used Q2 = 1). 4 Learning algorithm Learning is done using a Monte Carlo variant of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). The M step is convex and computed using L-BFGS (Liu et al., 1989); but the E step is intractable (Lunter et al., 2003), so we used a Markov chain Monte Carlo (MCMC) approximation (Tierney, 1994). At E step t = 1, 2, ... , we simulated the chain for O(t) iterations; this regime is necessary for convergence (Jank, 2005). In the E step, the inference problem is to compute an expectation under the posterior over strings in a protolanguage given observed word forms at the leaves of the tree. The typical approach in biology or historical linguistics (Holmes and Bruno, 2001; Bouchard-Cˆot´e et al.,</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dreyer</author>
<author>J R Smith</author>
<author>J Eisner</author>
</authors>
<title>Latentvariable modeling of string transductions with finitestate methods.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="14301" citStr="Dreyer et al. (2008)" startWordPosition="2379" endWordPosition="2382"> MARKEDNESS consists of language-specific ngram indicator functions for all symbols in E. Only unigram and bigram features are used for computational reasons, but we show in Section 5 that this already captures important constraints. Examples in Figure 1 (d): the bigram indicator 1[(n g)@Kw] (Kw stands for Kwara’ae, a language of the Solomon Islands), the unigram indicators 1[(n)@Kw] and 1[(g)@Kw]. FAITHFULNESS consists of indicators for mutation events of the form 1[x —* y], where x E E, y E Y. Examples: 1[q —* n], 1[q —* n@Kw]. Feature templates similar to these can be found for instance in Dreyer et al. (2008) and Chen (2003), in the context of string-to-string transduction. Note also the connection with stochastic OT (Goldwater and Johnson, 2003; Wilson, 2006), where a loglinear model mediates markedness and faithfulness of the production of an output form from an underlying input form. 3.3 Parameter sharing Data sparsity is a significant challenge in protolanguage reconstruction. While the experiments we present here use an order of magnitude more languages than previous computational approaches, the increase in observed data also brings with it additional unknowns in the form of intermediate pro</context>
</contexts>
<marker>Dreyer, Smith, Eisner, 2008</marker>
<rawString>M. Dreyer, J. R. Smith, and J. Eisner. 2008. Latentvariable modeling of string transductions with finitestate methods. In Proceedings of EMNLP 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Durham</author>
<author>D E Rogers</author>
</authors>
<title>An application of computer programming to the reconstruction of a proto-language.</title>
<date>1969</date>
<booktitle>In Proceedings of the 1969 conference on Computational linguistics.</booktitle>
<contexts>
<context position="1896" citStr="Durham and Rogers, 1969" startWordPosition="280" endWordPosition="283">on of word forms in several Oceanic languages, all meaning to cry. The ancestral form in this case has been presumed to be /taNis/ in Blust (1993). We are interested in models which take as input many such word tuples, each representing a cognate group, along with a language tree, and induce word forms for hidden ancestral languages. The traditional approach to this problem has been the comparative method, in which reconstructions are done manually using assumptions about the relative probability of different kinds of sound change (Hock, 1986). There has been work attempting to automate part (Durham and Rogers, 1969; Eastlack, 1977; Lowe and Mazaudon, 1994; Covington, 1998; 65 Kondrak, 2002) or all of the process (Oakes, 2000; Bouchard-Cˆot´e et al., 2008). However, previous automated methods have been unable to leverage three important ideas a linguist would employ. We address these omissions here, resulting in a more powerful method for automatically reconstructing ancient protolanguages. First, linguists triangulate reconstructions from many languages, while past work has been limited to small numbers of languages. For example, Oakes (2000) used four languages to reconstruct Proto-Malayo-Javanic (PMJ)</context>
</contexts>
<marker>Durham, Rogers, 1969</marker>
<rawString>S. P. Durham and D. E. Rogers. 1969. An application of computer programming to the reconstruction of a proto-language. In Proceedings of the 1969 conference on Computational linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Eastlack</author>
</authors>
<title>Iberochange: A program to simulate systematic sound change in Ibero-Romance. Computers and the Humanities.</title>
<date>1977</date>
<contexts>
<context position="1912" citStr="Eastlack, 1977" startWordPosition="284" endWordPosition="285">al Oceanic languages, all meaning to cry. The ancestral form in this case has been presumed to be /taNis/ in Blust (1993). We are interested in models which take as input many such word tuples, each representing a cognate group, along with a language tree, and induce word forms for hidden ancestral languages. The traditional approach to this problem has been the comparative method, in which reconstructions are done manually using assumptions about the relative probability of different kinds of sound change (Hock, 1986). There has been work attempting to automate part (Durham and Rogers, 1969; Eastlack, 1977; Lowe and Mazaudon, 1994; Covington, 1998; 65 Kondrak, 2002) or all of the process (Oakes, 2000; Bouchard-Cˆot´e et al., 2008). However, previous automated methods have been unable to leverage three important ideas a linguist would employ. We address these omissions here, resulting in a more powerful method for automatically reconstructing ancient protolanguages. First, linguists triangulate reconstructions from many languages, while past work has been limited to small numbers of languages. For example, Oakes (2000) used four languages to reconstruct Proto-Malayo-Javanic (PMJ) and Bouchard-Cˆ</context>
</contexts>
<marker>Eastlack, 1977</marker>
<rawString>C. L. Eastlack. 1977. Iberochange: A program to simulate systematic sound change in Ibero-Romance. Computers and the Humanities.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Felsenstein</author>
</authors>
<title>PHYLIP - PHYLogeny Inference Package (Version 3.2). Cladistics,</title>
<date>1989</date>
<pages>5--164</pages>
<contexts>
<context position="21652" citStr="Felsenstein, 1989" startWordPosition="3608" endWordPosition="3609">e factors relevant to its success. The database includes partial cognacy judgments and IPA transcriptions, as well as a few reconstructed protolanguages. A reconstruction of Proto-Oceanic (POc) originally developed by Blust (1993) using the comparative method was the basis for evaluation. We used the cognate information provided in the database, automatically constructing a global tree2 and set of subtrees from the cognate set indicator matrix M(E, c) = 1[E E L(c)], c E {1, ... , C}, E E L. For constructing the global tree, we used the implementation of neighbor joining in the Phylip package (Felsenstein, 1989). We used a distance based on cognates overlap, dc(E1, E2) = ECc=1 M(E1, c)M(E2, c). We bootstrapped 1000 2The dataset included a tree, but it was out of date as of November 2008 (Greenhill et al., 2008). C X log X 11 Pλ(wc,r(c)) c=1 ~w∈W(c) `∈I(c) 69 Condition Edit dist. Unsupervised full system 1.87 -FAITHFULNESS 2.02 -MARKEDNESS 2.18 -Sharing 1.99 -Topology 2.06 Semi-supervised system 1.75 10 Itrat 0 20 Error MARK 2.2 2 Sh i 2.6 AIO 1.4 1.4 0 30 60 rre1.8 1. t ean istnce to the targt recons n laguags to proto Oceanic. Th N. of modern lang di . EM iteration stnce to the target re 3.63.6 2 3 </context>
</contexts>
<marker>Felsenstein, 1989</marker>
<rawString>J. Felsenstein. 1989. PHYLIP - PHYLogeny Inference Package (Version 3.2). Cladistics, 5:164–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>M Johnson</author>
</authors>
<title>Learning OT constraint rankings using a maximum entropy model.</title>
<date>2003</date>
<booktitle>Proceedings of the Workshop on Variation within Optimality Theory.</booktitle>
<contexts>
<context position="14440" citStr="Goldwater and Johnson, 2003" startWordPosition="2399" endWordPosition="2402">for computational reasons, but we show in Section 5 that this already captures important constraints. Examples in Figure 1 (d): the bigram indicator 1[(n g)@Kw] (Kw stands for Kwara’ae, a language of the Solomon Islands), the unigram indicators 1[(n)@Kw] and 1[(g)@Kw]. FAITHFULNESS consists of indicators for mutation events of the form 1[x —* y], where x E E, y E Y. Examples: 1[q —* n], 1[q —* n@Kw]. Feature templates similar to these can be found for instance in Dreyer et al. (2008) and Chen (2003), in the context of string-to-string transduction. Note also the connection with stochastic OT (Goldwater and Johnson, 2003; Wilson, 2006), where a loglinear model mediates markedness and faithfulness of the production of an output form from an underlying input form. 3.3 Parameter sharing Data sparsity is a significant challenge in protolanguage reconstruction. While the experiments we present here use an order of magnitude more languages than previous computational approaches, the increase in observed data also brings with it additional unknowns in the form of intermediate protolanguages. Since there is one set of parameters for each language, adding more data is not sufficient for increasing the quality of the r</context>
</contexts>
<marker>Goldwater, Johnson, 2003</marker>
<rawString>S. Goldwater and M. Johnson. 2003. Learning OT constraint rankings using a maximum entropy model. Proceedings of the Workshop on Variation within Optimality Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Greenhill</author>
<author>R Blust</author>
<author>R D Gray</author>
</authors>
<title>The Austronesian basic vocabulary database: From bioinformatics to lexomics. Evolutionary Bioinformatics,</title>
<date>2008</date>
<pages>4--271</pages>
<contexts>
<context position="2797" citStr="Greenhill et al., 2008" startWordPosition="417" endWordPosition="420">sions here, resulting in a more powerful method for automatically reconstructing ancient protolanguages. First, linguists triangulate reconstructions from many languages, while past work has been limited to small numbers of languages. For example, Oakes (2000) used four languages to reconstruct Proto-Malayo-Javanic (PMJ) and Bouchard-Cˆot´e et al. (2008) used two languages to reconstruct Classical Latin (La). We revisit these small datasets and show that our method significantly outperforms these previous systems. However, we also show that our method can be applied to a much larger data set (Greenhill et al., 2008), reconstructing ProtoOceanic (POc) from 64 modern languages. In addition, performance improves with more languages, which was not the case for previous methods. Second, linguists exploit knowledge of phonological universals. For example, small changes in vowel height or consonant place are more likely than large changes, and much more likely than change to arbitrarily different phonemes. In a statistical system, one could imagine either manually encoding or automatically inferring such preferences. We show that both strategies are effective. Finally, linguists consider not only how languages </context>
<context position="20939" citStr="Greenhill et al., 2008" startWordPosition="3489" endWordPosition="3492">ce (Levenshtein, 1966) between the reconstruction produced by each method and the reconstruction produced by linguists. We averaged this distance across reconstructed words to report a single number for each method. We show in Table 2 the average word length in each corpus; note that the Latin average is much larger, giving an explanation to the higher errors in the Romance dataset. The statistical significance of all performance differences are assessed using a paired t-test with significance level of 0.05. 5.1 Evaluating system performance We used the Austronesian Basic Vocabulary Database (Greenhill et al., 2008) as the basis for a series of experiments used to evaluate the performance of our system and the factors relevant to its success. The database includes partial cognacy judgments and IPA transcriptions, as well as a few reconstructed protolanguages. A reconstruction of Proto-Oceanic (POc) originally developed by Blust (1993) using the comparative method was the basis for evaluation. We used the cognate information provided in the database, automatically constructing a global tree2 and set of subtrees from the cognate set indicator matrix M(E, c) = 1[E E L(c)], c E {1, ... , C}, E E L. For const</context>
</contexts>
<marker>Greenhill, Blust, Gray, 2008</marker>
<rawString>S. J. Greenhill, R. Blust, and R. D. Gray. 2008. The Austronesian basic vocabulary database: From bioinformatics to lexomics. Evolutionary Bioinformatics, 4:271–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Hock</author>
</authors>
<title>Principles of Historical Linguistics. Walter de Gruyter.</title>
<date>1986</date>
<contexts>
<context position="1822" citStr="Hock, 1986" startWordPosition="270" endWordPosition="271">own cognate groups. For example, Figure 1 (a) shows a collection of word forms in several Oceanic languages, all meaning to cry. The ancestral form in this case has been presumed to be /taNis/ in Blust (1993). We are interested in models which take as input many such word tuples, each representing a cognate group, along with a language tree, and induce word forms for hidden ancestral languages. The traditional approach to this problem has been the comparative method, in which reconstructions are done manually using assumptions about the relative probability of different kinds of sound change (Hock, 1986). There has been work attempting to automate part (Durham and Rogers, 1969; Eastlack, 1977; Lowe and Mazaudon, 1994; Covington, 1998; 65 Kondrak, 2002) or all of the process (Oakes, 2000; Bouchard-Cˆot´e et al., 2008). However, previous automated methods have been unable to leverage three important ideas a linguist would employ. We address these omissions here, resulting in a more powerful method for automatically reconstructing ancient protolanguages. First, linguists triangulate reconstructions from many languages, while past work has been limited to small numbers of languages. For example, </context>
</contexts>
<marker>Hock, 1986</marker>
<rawString>H. H. Hock. 1986. Principles of Historical Linguistics. Walter de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Holmes</author>
<author>W J Bruno</author>
</authors>
<title>Evolutionary HMM: a Bayesian approach to multiple alignment.</title>
<date>2001</date>
<journal>Bioinformatics,</journal>
<pages>17--803</pages>
<contexts>
<context position="18495" citStr="Holmes and Bruno, 2001" startWordPosition="3096" endWordPosition="3099">mization (EM) algorithm (Dempster et al., 1977). The M step is convex and computed using L-BFGS (Liu et al., 1989); but the E step is intractable (Lunter et al., 2003), so we used a Markov chain Monte Carlo (MCMC) approximation (Tierney, 1994). At E step t = 1, 2, ... , we simulated the chain for O(t) iterations; this regime is necessary for convergence (Jank, 2005). In the E step, the inference problem is to compute an expectation under the posterior over strings in a protolanguage given observed word forms at the leaves of the tree. The typical approach in biology or historical linguistics (Holmes and Bruno, 2001; Bouchard-Cˆot´e et al., 2008) is to use Gibbs sampling, where the entire string at a single node in the tree is sampled, conditioned on its parent and children. This sampling domain is shown in Figure 1 (e), where the middle word is completely resampled but adjacent words are fixed. We will call this method Single Sequence Resampling (SSR). While conceptually simple, this approach suffers from problems in large trees (Holmes and Bruno, 2001). Consequently, we use a different MCMC procedure, called Ancestry Resampling (AR) that alleviates the mixing problems (Figure 1 (f)). This method was or</context>
</contexts>
<marker>Holmes, Bruno, 2001</marker>
<rawString>I. Holmes and W. J. Bruno. 2001. Evolutionary HMM: a Bayesian approach to multiple alignment. Bioinformatics, 17:803–820.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Jank</author>
</authors>
<title>Stochastic variants of EM: Monte Carlo, quasi-Monte Carlo and more.</title>
<date>2005</date>
<booktitle>In Proceedings of the American Statistical Association.</booktitle>
<contexts>
<context position="18241" citStr="Jank, 2005" startWordPosition="3055" endWordPosition="3056">n summarize our objective function as follows: Pλ(wc,`|wc,pa(`)) − ||λ||2 2 2σ2 The second term is a standard L2 regularization penalty (we used Q2 = 1). 4 Learning algorithm Learning is done using a Monte Carlo variant of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). The M step is convex and computed using L-BFGS (Liu et al., 1989); but the E step is intractable (Lunter et al., 2003), so we used a Markov chain Monte Carlo (MCMC) approximation (Tierney, 1994). At E step t = 1, 2, ... , we simulated the chain for O(t) iterations; this regime is necessary for convergence (Jank, 2005). In the E step, the inference problem is to compute an expectation under the posterior over strings in a protolanguage given observed word forms at the leaves of the tree. The typical approach in biology or historical linguistics (Holmes and Bruno, 2001; Bouchard-Cˆot´e et al., 2008) is to use Gibbs sampling, where the entire string at a single node in the tree is sampled, conditioned on its parent and children. This sampling domain is shown in Figure 1 (e), where the middle word is completely resampled but adjacent words are fixed. We will call this method Single Sequence Resampling (SSR). W</context>
</contexts>
<marker>Jank, 2005</marker>
<rawString>W. Jank. 2005. Stochastic variants of EM: Monte Carlo, quasi-Monte Carlo and more. In Proceedings of the American Statistical Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kondrak</author>
</authors>
<title>A new algorithm for the alignment of phonetic sequences.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="34802" citStr="Kondrak (2000)" startWordPosition="5904" endWordPosition="5905">tperform the CENTROID baseline (1.79). 5.3 Incorporating prior linguistic knowledge The model also supports the addition of prior linguistic knowledge. This takes the form of feature templates with more internal structure. We performed experiments with an additional feature template: STRUCT-FAITHFULNESS is a structured version of FAITHFULNESS, replacing x and y with their natural classes Nβ(x) and Nβ(y) where Q indexes types of classes, ranging over {manner, place, phonation, isOral, isCentral, height, backness, roundedness}. This feature set is reminiscent of the featurized representation of Kondrak (2000). We compared the performance of the system with and without STRUCT-FAITHFULNESS to check if the algorithm can recover the structure of natural classes in an unsupervised fashion. We found that with 2 or 4 observed languages, FAITHFULNESS underperformed STRUCT-FAITHFULNESS, but for larger trees, the difference was not significant. FAITHFULNESS even slightly outperformed its structured cousin with 16 observed languages. 6 Conclusion By enriching our model to include important features like markedness, and by scaling up to much larger data sets than were previously possible, we obtained substant</context>
</contexts>
<marker>Kondrak, 2000</marker>
<rawString>G. Kondrak. 2000. A new algorithm for the alignment of phonetic sequences. In Proceedings of NAACL 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kondrak</author>
</authors>
<title>Algorithms for Language Reconstruction.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Toronto.</institution>
<contexts>
<context position="1973" citStr="Kondrak, 2002" startWordPosition="293" endWordPosition="294">in this case has been presumed to be /taNis/ in Blust (1993). We are interested in models which take as input many such word tuples, each representing a cognate group, along with a language tree, and induce word forms for hidden ancestral languages. The traditional approach to this problem has been the comparative method, in which reconstructions are done manually using assumptions about the relative probability of different kinds of sound change (Hock, 1986). There has been work attempting to automate part (Durham and Rogers, 1969; Eastlack, 1977; Lowe and Mazaudon, 1994; Covington, 1998; 65 Kondrak, 2002) or all of the process (Oakes, 2000; Bouchard-Cˆot´e et al., 2008). However, previous automated methods have been unable to leverage three important ideas a linguist would employ. We address these omissions here, resulting in a more powerful method for automatically reconstructing ancient protolanguages. First, linguists triangulate reconstructions from many languages, while past work has been limited to small numbers of languages. For example, Oakes (2000) used four languages to reconstruct Proto-Malayo-Javanic (PMJ) and Bouchard-Cˆot´e et al. (2008) used two languages to reconstruct Classica</context>
</contexts>
<marker>Kondrak, 2002</marker>
<rawString>G. Kondrak. 2002. Algorithms for Language Reconstruction. Ph.D. thesis, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<contexts>
<context position="20338" citStr="Levenshtein, 1966" startWordPosition="3397" endWordPosition="3398">ouchard-Cˆot´e et al. (2009). Slices condition on observed data, avoiding the problems mentioned above, and can propagate information rapidly across the tree. 5 Experiments We performed a comprehensive set of experiments to test the new method for reconstruction outlined above. In Section 5.1, we analyze in isolation the effects of varying the set of features, the number of observed languages, the topology, and the number of iterations of EM. In Section 5.2 we compare performance to an oracle and to three other systems. Evaluation of all methods was done by computing the Levenshtein distance (Levenshtein, 1966) between the reconstruction produced by each method and the reconstruction produced by linguists. We averaged this distance across reconstructed words to report a single number for each method. We show in Table 2 the average word length in each corpus; note that the Latin average is much larger, giving an explanation to the higher errors in the Romance dataset. The statistical significance of all performance differences are assessed using a paired t-test with significance level of 0.05. 5.1 Evaluating system performance We used the Austronesian Basic Vocabulary Database (Greenhill et al., 2008</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V. I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
<author>C Dong</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>45--503</pages>
<contexts>
<context position="17987" citStr="Liu et al., 1989" startWordPosition="3006" endWordPosition="3009">bility models described in Section 3.1 (with transition probabilities given by the above log-linear model), I(c), the set of internal (non-leaf) nodes in τ(c), pa(`), the parent of language `, r(c), the root of τ(c) µ(ω, t, ξ) = 68 and W(c) = (E∗)|I(c)|. We can summarize our objective function as follows: Pλ(wc,`|wc,pa(`)) − ||λ||2 2 2σ2 The second term is a standard L2 regularization penalty (we used Q2 = 1). 4 Learning algorithm Learning is done using a Monte Carlo variant of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). The M step is convex and computed using L-BFGS (Liu et al., 1989); but the E step is intractable (Lunter et al., 2003), so we used a Markov chain Monte Carlo (MCMC) approximation (Tierney, 1994). At E step t = 1, 2, ... , we simulated the chain for O(t) iterations; this regime is necessary for convergence (Jank, 2005). In the E step, the inference problem is to compute an expectation under the posterior over strings in a protolanguage given observed word forms at the leaves of the tree. The typical approach in biology or historical linguistics (Holmes and Bruno, 2001; Bouchard-Cˆot´e et al., 2008) is to use Gibbs sampling, where the entire string at a singl</context>
</contexts>
<marker>Liu, Nocedal, Dong, 1989</marker>
<rawString>D. C. Liu, J. Nocedal, and C. Dong. 1989. On the limited memory BFGS method for large scale optimization. Mathematical Programming, 45:503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Lowe</author>
<author>M Mazaudon</author>
</authors>
<title>The reconstruction engine: a computer implementation of the comparative method.</title>
<date>1994</date>
<journal>Comput. Linguist.,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="1937" citStr="Lowe and Mazaudon, 1994" startWordPosition="286" endWordPosition="289">ages, all meaning to cry. The ancestral form in this case has been presumed to be /taNis/ in Blust (1993). We are interested in models which take as input many such word tuples, each representing a cognate group, along with a language tree, and induce word forms for hidden ancestral languages. The traditional approach to this problem has been the comparative method, in which reconstructions are done manually using assumptions about the relative probability of different kinds of sound change (Hock, 1986). There has been work attempting to automate part (Durham and Rogers, 1969; Eastlack, 1977; Lowe and Mazaudon, 1994; Covington, 1998; 65 Kondrak, 2002) or all of the process (Oakes, 2000; Bouchard-Cˆot´e et al., 2008). However, previous automated methods have been unable to leverage three important ideas a linguist would employ. We address these omissions here, resulting in a more powerful method for automatically reconstructing ancient protolanguages. First, linguists triangulate reconstructions from many languages, while past work has been limited to small numbers of languages. For example, Oakes (2000) used four languages to reconstruct Proto-Malayo-Javanic (PMJ) and Bouchard-Cˆot´e et al. (2008) used t</context>
</contexts>
<marker>Lowe, Mazaudon, 1994</marker>
<rawString>J. B. Lowe and M. Mazaudon. 1994. The reconstruction engine: a computer implementation of the comparative method. Comput. Linguist., 20(3):381–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Lunter</author>
<author>I Mikl´os</author>
<author>Y S Song</author>
<author>J Hein</author>
</authors>
<title>An efficient algorithm for statistical multiple alignment on arbitrary phylogenetic trees.</title>
<date>2003</date>
<journal>Journal of Computational Biology,</journal>
<pages>10--869</pages>
<marker>Lunter, Mikl´os, Song, Hein, 2003</marker>
<rawString>G. A. Lunter, I. Mikl´os, Y. S. Song, and J. Hein. 2003. An efficient algorithm for statistical multiple alignment on arbitrary phylogenetic trees. Journal of Computational Biology, 10:869–889.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Nothofer</author>
</authors>
<title>The reconstruction of Proto-MalayoJavanic.</title>
<date>1975</date>
<journal>M. Nijhoff.</journal>
<contexts>
<context position="30747" citStr="Nothofer (1975)" startWordPosition="5203" endWordPosition="5204">icking a single closest language to be used for all word forms, but it is possible for systems to perform better than the oracle since it has to return one of the observed word forms. We performed the comparison against Oakes (2000) and Bouchard-Cˆot´e et al. (2008) on the same dataset and experimental conditions as those used in the respective papers (see Table 2). Note that the setup of Bouchard-Cˆot´e et al. (2008) provides supervision (half of the Latin word forms are provided); all of the other comparisons are performed in a completely unsupervised manner. The PMJ dataset was compiled by Nothofer (1975), who also reconstructed the corresponding protolanguage. Since PRAGUE is not guaranteed to return a reconstruction for each cognate set, only 55 word forms could be directly compared to our system. We restricted comparison to this subset of the data. This favors PRAGUE since the system only proposes a reconstruction when it is certain. Still, our system outperformed PRAGUE, with an average distance of 1.60 compared to 2.02 for PRAGUE. The difference is marginally significant, p = 0.06, partly due to the small number of word forms involved. We also exceeded the performance of BCLKG on the Roma</context>
</contexts>
<marker>Nothofer, 1975</marker>
<rawString>B. Nothofer. 1975. The reconstruction of Proto-MalayoJavanic. M. Nijhoff.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Oakes</author>
</authors>
<title>Computer estimation of vocabulary in a protolanguage from word lists in four daughter languages.</title>
<date>2000</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="2008" citStr="Oakes, 2000" startWordPosition="300" endWordPosition="301">taNis/ in Blust (1993). We are interested in models which take as input many such word tuples, each representing a cognate group, along with a language tree, and induce word forms for hidden ancestral languages. The traditional approach to this problem has been the comparative method, in which reconstructions are done manually using assumptions about the relative probability of different kinds of sound change (Hock, 1986). There has been work attempting to automate part (Durham and Rogers, 1969; Eastlack, 1977; Lowe and Mazaudon, 1994; Covington, 1998; 65 Kondrak, 2002) or all of the process (Oakes, 2000; Bouchard-Cˆot´e et al., 2008). However, previous automated methods have been unable to leverage three important ideas a linguist would employ. We address these omissions here, resulting in a more powerful method for automatically reconstructing ancient protolanguages. First, linguists triangulate reconstructions from many languages, while past work has been limited to small numbers of languages. For example, Oakes (2000) used four languages to reconstruct Proto-Malayo-Javanic (PMJ) and Bouchard-Cˆot´e et al. (2008) used two languages to reconstruct Classical Latin (La). We revisit these smal</context>
<context position="4695" citStr="Oakes (2000)" startWordPosition="707" endWordPosition="708">arkedness features, characterizing the internal phonotactic structure of each language. Faithfulness and markedness play roles analogous to the channel and language models of a noisy-channel system. We show that markedness features improve reconstruction, and can be used efficiently. 2 Related work Our focus in this section is on describing the properties of the two previous systems for reconstructing ancient word forms to which we compare our method. Citations for other related work, such as similar approaches to using faithfulness and markedness features, appear in the body of the paper. In Oakes (2000), the word forms in a given protolanguage are reconstructed using a Viterbi multialignment between a small number of its descendant languages. The alignment is computed using handset parameters. Deterministic rules characterizing changes between pairs of observed languages are extracted from the alignment when their frequency is higher than a threshold, and a proto-phoneme inventory is built using linguistically motivated rules and parsimony. A reconstruction of each observed word is first proposed independently for each language. If at least two reconstructions agree, a majority vote is taken</context>
<context position="22603" citStr="Oakes (2000)" startWordPosition="3788" endWordPosition="3789">-Sharing 1.99 -Topology 2.06 Semi-supervised system 1.75 10 Itrat 0 20 Error MARK 2.2 2 Sh i 2.6 AIO 1.4 1.4 0 30 60 rre1.8 1. t ean istnce to the targt recons n laguags to proto Oceanic. Th N. of modern lang di . EM iteration stnce to the target re 3.63.6 2 3 8 6 2.42.4 2 2 1.81.8 Figure 2: Left: Mean distance to the target reconstruction s 5.2 Comparsons againt other methods ed an pemen p The first two competng methods, PRAGUE and d of POc as a function of the number of modern languages ones bg model fo ech un It is semisupervisd in by increasing he number of language BCLKG are described in Oakes (2000) and sense tha gold econsructin for many nternal c used by the inference procedure. Right: Mean distance Te reslts are reported in Figure 4. They cnBouchardˆot´e et al. (208) respectvely and sumaibl t h an ir Se p of g Kw and Lau in Fire 6)3 mtic proto-language rconstrution ging from 2- well to large dataets. In the rst cas, the bottleneck n and confidence intervals as a function of the EM iteration, 8, 16to gu gfi th le utn e gure 6 shows th results of a cncrete run ove 2 averaged over 20 random seeds and ran on 4 languages. cantly helped reconsuction There was sill an avwithou guide trees nd</context>
<context position="27942" citStr="Oakes (2000)" startWordPosition="4707" endWordPosition="4708">e -Topology experiment of Table 1. 70 be seen that linguistically plausible substitutions are learned. The global features prefer a range of voicing changes, manner changes, adjacent vowel motion, and so on, including mutations like /s/ to /h/ which are common but poorly represented in a naive attribute-based natural class scheme. On the other hand, the features local to the language Kwara’ae pick out the subset of these changes which are active in that branch, such as /s/—*/t/ fortition. 5.2 Comparisons against other methods The first two competing methods, PRAGUE and BCLKG, are described in Oakes (2000) and Bouchard-Cˆot´e et al. (2008) respectively and summarized in Section 1. Neither approach scales well to large datasets. In the first case, the bottleneck is the complexity of computing multi-alignments without guide trees and the vanishing probability that independent reconstructions agree. In the second case, the problem comes from the unregularized proliferation of parameters and slow mixing of the inference algorithm. For this reason, we built a third baseline that scales well in large datasets. This third baseline, CENTROID, computes the centroid of the observed word forms in Levensht</context>
<context position="30364" citStr="Oakes (2000)" startWordPosition="5140" endWordPosition="5141">3 (1.0) 79 (1.0) 293 (0.5) Modern languages 70 4 2 Cognate sets 1321 179 583 Observed words 10783 470 1463 Mean word length 4.5 5.0 7.4 Table 2: Experimental setup: number of held-out protoword from (absolute and relative), of modern languages, cognate sets and total observed words. The split for BCLKG is the same as in Bouchard-Cˆot´e et al. (2008). ACLE. This is superior to picking a single closest language to be used for all word forms, but it is possible for systems to perform better than the oracle since it has to return one of the observed word forms. We performed the comparison against Oakes (2000) and Bouchard-Cˆot´e et al. (2008) on the same dataset and experimental conditions as those used in the respective papers (see Table 2). Note that the setup of Bouchard-Cˆot´e et al. (2008) provides supervision (half of the Latin word forms are provided); all of the other comparisons are performed in a completely unsupervised manner. The PMJ dataset was compiled by Nothofer (1975), who also reconstructed the corresponding protolanguage. Since PRAGUE is not guaranteed to return a reconstruction for each cognate set, only 55 word forms could be directly compared to our system. We restricted comp</context>
</contexts>
<marker>Oakes, 2000</marker>
<rawString>M. P. Oakes. 2000. Computer estimation of vocabulary in a protolanguage from word lists in four daughter languages. Journal of Quantitative Linguistics, 7(3):233–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Prince</author>
<author>P Smolensky</author>
</authors>
<title>Optimality theory: Constraint interaction in generative grammar.</title>
<date>1993</date>
<tech>Technical Report 2,</tech>
<institution>Rutgers University Center for Cognitive Science.</institution>
<contexts>
<context position="3639" citStr="Prince and Smolensky, 1993" startWordPosition="546" endWordPosition="549">niversals. For example, small changes in vowel height or consonant place are more likely than large changes, and much more likely than change to arbitrarily different phonemes. In a statistical system, one could imagine either manually encoding or automatically inferring such preferences. We show that both strategies are effective. Finally, linguists consider not only how languages change, but also how they are internally consistent. Past models described how sounds do (or, more often, do not) change between nodes in the tree. To borrow broad terminology from the Optimality Theory literature (Prince and Smolensky, 1993), such models incorporated faithfulness features, capturing the ways in which successive forms remained similar to one another. However, each language has certain regular phonotactic patterns which conHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 65–73, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics strain these changes. We encode such patterns using markedness features, characterizing the internal phonotactic structure of each language. Faithfulness and markedness play roles analogous to the channel and </context>
<context position="8072" citStr="Prince and Smolensky, 1993" startWordPosition="1272" endWordPosition="1275">nate set, with topology T(c). First, a word is generated for the root of T(c) using an (initially unknown) root language model (distribution over strings). The other nodes of the tree are drawn incrementally as follows: for each edge E —* E&apos; in T(c) use a branch-specific distribution over changes in strings to generate the word at node E&apos;. In the remainder of this section, we clarify the exact form of the conditional distributions over string changes, the distribution over strings at the root, and the parameterization of this process. 3.1 Markedness and Faithfulness In Optimality Theory (OT) (Prince and Smolensky, 1993), two types of constraints influence the selection of a realized output given an input form: faithfulness and markedness constraints. Faithfulness en1The choice of a phonemic representation is motivated by the fact that most of the data available comes in this form. Diacritics are available in a smaller number of languages and may vary across dialects, so we discarded them in this work. 66 edes /angi/ /angi/ roles a (a) (b) Language Word form Proto Oceanic /tagis/ Lau /agi/ Kwara’ae /angi/ Taiof /tagis/ (c) (f) /taŋi/ (e) /taŋi/ (d) g /aŋi/ /aŋi/ feat aract θS erŋ nt g i ŋ a n arg The yi’s may</context>
</contexts>
<marker>Prince, Smolensky, 1993</marker>
<rawString>A. Prince and P. Smolensky. 1993. Optimality theory: Constraint interaction in generative grammar. Technical Report 2, Rutgers University Center for Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tierney</author>
</authors>
<title>Markov chains for exploring posterior distributions.</title>
<date>1994</date>
<journal>The Annals of Statistics,</journal>
<volume>22</volume>
<issue>4</issue>
<pages>1728</pages>
<contexts>
<context position="18116" citStr="Tierney, 1994" startWordPosition="3031" endWordPosition="3032">l (non-leaf) nodes in τ(c), pa(`), the parent of language `, r(c), the root of τ(c) µ(ω, t, ξ) = 68 and W(c) = (E∗)|I(c)|. We can summarize our objective function as follows: Pλ(wc,`|wc,pa(`)) − ||λ||2 2 2σ2 The second term is a standard L2 regularization penalty (we used Q2 = 1). 4 Learning algorithm Learning is done using a Monte Carlo variant of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977). The M step is convex and computed using L-BFGS (Liu et al., 1989); but the E step is intractable (Lunter et al., 2003), so we used a Markov chain Monte Carlo (MCMC) approximation (Tierney, 1994). At E step t = 1, 2, ... , we simulated the chain for O(t) iterations; this regime is necessary for convergence (Jank, 2005). In the E step, the inference problem is to compute an expectation under the posterior over strings in a protolanguage given observed word forms at the leaves of the tree. The typical approach in biology or historical linguistics (Holmes and Bruno, 2001; Bouchard-Cˆot´e et al., 2008) is to use Gibbs sampling, where the entire string at a single node in the tree is sampled, conditioned on its parent and children. This sampling domain is shown in Figure 1 (e), where the m</context>
</contexts>
<marker>Tierney, 1994</marker>
<rawString>L. Tierney. 1994. Markov chains for exploring posterior distributions. The Annals of Statistics, 22(4):1701– 1728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Varadarajan</author>
<author>R K Bradley</author>
<author>I H Holmes</author>
</authors>
<title>Tools for simulating evolution of aligned genomic regions with integrated parameter estimation. Genome Biology,</title>
<date>2008</date>
<marker>Varadarajan, Bradley, Holmes, 2008</marker>
<rawString>A. Varadarajan, R. K. Bradley, and I. H. Holmes. 2008. Tools for simulating evolution of aligned genomic regions with integrated parameter estimation. Genome Biology, 9:R147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Wilson</author>
</authors>
<title>Learning phonology with substantive bias: An experimental and computational study of velar palatalization.</title>
<date>2006</date>
<journal>Cognitive Science,</journal>
<pages>30--5</pages>
<contexts>
<context position="14455" citStr="Wilson, 2006" startWordPosition="2403" endWordPosition="2404">t we show in Section 5 that this already captures important constraints. Examples in Figure 1 (d): the bigram indicator 1[(n g)@Kw] (Kw stands for Kwara’ae, a language of the Solomon Islands), the unigram indicators 1[(n)@Kw] and 1[(g)@Kw]. FAITHFULNESS consists of indicators for mutation events of the form 1[x —* y], where x E E, y E Y. Examples: 1[q —* n], 1[q —* n@Kw]. Feature templates similar to these can be found for instance in Dreyer et al. (2008) and Chen (2003), in the context of string-to-string transduction. Note also the connection with stochastic OT (Goldwater and Johnson, 2003; Wilson, 2006), where a loglinear model mediates markedness and faithfulness of the production of an output form from an underlying input form. 3.3 Parameter sharing Data sparsity is a significant challenge in protolanguage reconstruction. While the experiments we present here use an order of magnitude more languages than previous computational approaches, the increase in observed data also brings with it additional unknowns in the form of intermediate protolanguages. Since there is one set of parameters for each language, adding more data is not sufficient for increasing the quality of the reconstruction: </context>
</contexts>
<marker>Wilson, 2006</marker>
<rawString>C. Wilson. 2006. Learning phonology with substantive bias: An experimental and computational study of velar palatalization. Cognitive Science, 30.5:945–982.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>