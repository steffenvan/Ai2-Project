<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.945791">
Reference Scope Identification in Citing Sentences
</title>
<author confidence="0.867447">
Amjad Abu-Jbara
</author>
<affiliation confidence="0.9251405">
EECS Department
University of Michigan
</affiliation>
<address confidence="0.861122">
Ann Arbor, MI, USA
</address>
<email confidence="0.999104">
amjbara@umich.edu
</email>
<author confidence="0.897933">
Dragomir Radev
</author>
<affiliation confidence="0.9421655">
EECS Department
University of Michigan
</affiliation>
<address confidence="0.861585">
Ann Arbor, MI, USA
</address>
<email confidence="0.999501">
radev@umich.edu
</email>
<sectionHeader confidence="0.996669" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909043478261">
A citing sentence is one that appears in a sci-
entific article and cites previous work. Cit-
ing sentences have been studied and used in
many applications. For example, they have
been used in scientific paper summarization,
automatic survey generation, paraphrase iden-
tification, and citation function classification.
Citing sentences that cite multiple papers are
common in scientific writing. This observa-
tion should be taken into consideration when
using citing sentences in applications. For in-
stance, when a citing sentence is used in a
summary of a scientific paper, only the frag-
ments of the sentence that are relevant to the
summarized paper should be included in the
summary. In this paper, we present and com-
pare three different approaches for identifying
the fragments of a citing sentence that are re-
lated to a given target reference. Our methods
are: word classification, sequence labeling,
and segment classification. Our experiments
show that segment classification achieves the
best results.
</bodyText>
<sectionHeader confidence="0.999163" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.918206363636364">
Citation plays an important role in science. It makes
the accumulation of knowledge possible. When a
reference appears in a scientific article, it is usually
accompanied by a span of text that highlights the
important contributions of the cited article. We
call a sentence that contains an explicit reference
to previous work a citation sentence. For example,
sentence (1) below is a citing sentence that cites a
paper by Philip Resnik and describes the problem
Resnik addressed in his paper.
(1) Resnik (1999) addressed the issue of language identification
for finding Web pages in the languages of interest.
Previous work has studied and used citation sen-
tences in various applications such as: scientific pa-
per summarization (Elkiss et al., 2008; Qazvinian
and Radev, 2008; Mei and Zhai, 2008; Qazvinian
et al., 2010; Qazvinian and Radev, 2010; Abu-
Jbara and Radev, 2011), automatic survey genera-
tion (Nanba et al., 2000; Mohammad et al., 2009),
citation function classification (Nanba et al., 2000;
Teufel et al., 2006; Siddharthan and Teufel, 2007;
Teufel, 2007), and paraphrase recognition (Nakov et
al., 2004; Schwartz et al., 2007).
Sentence (1) above contains one reference, and
the whole sentence is talking about that reference.
This is not always the case in scientific writing.
Sentences that contain references to multiple papers
are very common. For example, sentence (2) below
contains three references.
(2) Grefenstette and Nioche (2000) and Jones and Ghani (2000)
use the web to generate corpora for languages where electronic
resources are scarce, while Resnik (1999) describes a method
for mining the web for bilingual texts.
</bodyText>
<page confidence="0.65303">
80
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 80–90,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999620222222222">
The first fragment describes the contribution of
Grefenstette and Nioche (2000) and Jones and Ghani
(2000). The second fragment describes the contribu-
tion of Resnik (1999).
This observation should be taken into considera-
tion when using citing sentences in the aforemen-
tioned applications. For example, in citation-based
summarization of scientific papers, a subset of cit-
ing sentences that cite a given target paper is se-
lected and used to form a summary of that paper.
It is very likely that one or more of the selected sen-
tences cite multiple papers besides the target. This
means that some of the text included in the sum-
mary might be irrelevant to the summarized paper.
Including irrelevant text in the summary introduces
several problems. First, the summarization task aims
at summarizing the contributions of the target paper
using minimal text. Extraneous text takes space in
the summary while being irrelevant and less impor-
tant. Second, including irrelevant text in the sum-
mary breaks the context and confuses the reader.
Therefore, if sentence (2) above is to be added to
a citation-based summary of Resnik´s (1999) paper,
only the underlined fragment should be added to the
summary and the rest of the sentence should be ex-
cluded.
For another example, consider the task of citation
function classification. The goal of this task is to
determine the reason for citing paper B by paper A
based on linguistic and structural features extracted
from citing sentences that appear in A and cite B. If
a citing sentence in A cites multiple papers besides
B, classification features should be extracted only
from the fragments of the sentence that are relevant
to B. Sentence (3) below shows an examples of this
case.
</bodyText>
<listItem confidence="0.904724">
(3) Cohn and Lapata (2008) used the GHKM extraction method (Galley
</listItem>
<tableCaption confidence="0.442754">
et al., 2004), which is limited to constituent phrases and thus produces
a reasonably small set of syntactic rules.
</tableCaption>
<bodyText confidence="0.996850382352941">
If the target reference is Cohn and Lapata (2008),
only the underlined segment should be used for fea-
ture extraction. The limitation stated in the sec-
ond segment of sentence is referring to Galley et al.,
(2004).
In this paper, we address the problem of identi-
fying the fragments of a citing sentence that are re-
lated to a given target reference. Henceforth, we use
the term Reference Scope to refer to those fragments.
We present and compare three different approaches
to this problem.
In the first approach, we define the problem as a
word classification task. We classify each word in
the sentence as inside or outside the scope of the tar-
get reference.
In the second approach, we define the problem as
a sequence labeling problem. This is different from
the first approach in that the label assigned to each
word is dependent on the labels of nearby words. In
the third approach, instead of classifying individual
words, we split the sentence into segments and clas-
sify each segment as inside or outside the scope of
the target reference.
Applying any of the three approaches is pre-
ceded by a preprocessing stage. In this stage, cit-
ing sentences are analyzed to tag references, iden-
tify groups of references, and distinguish between
syntactic and non-syntactic references.
The rest of this paper is organized as follows. Sec-
tion 2 examines the related work. We define the
problem in Section3. Section 4 presents our ap-
proaches. Experiments, results and analysis are pre-
sented in Section 5. We conclude and provide direc-
tions to future work in Section 6
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998640555555556">
Our work is related to a large body of research on
citations (Hodges, 1972; Garfield et al., 1984). The
interest in studying citations stems from the fact that
bibliometric measures are commonly used to esti-
mate the impact of a researcher’s work (Borgman
and Furner, 2002; Luukkonen, 1992). White (2004)
provides a good recent survey of the different re-
search lines that use citations. In this section we re-
view the research lines that are relevant to our work
</bodyText>
<page confidence="0.997539">
81
</page>
<bodyText confidence="0.999914906976744">
and show how our work is different.
One line of research that is related to our work
has to do with identifying what Nanba and Oku-
mura (1999) call the citing area They define the cit-
ing area as the succession of sentences that appear
around the location of a given reference in a sci-
entific paper and have connection to it. Their al-
gorithm starts by adding the sentence that contains
the target reference as the first member sentence in
the citing area. Then, they use a set of cue words
and hand-crafted rules to determine whether the sur-
rounding sentences should be added to the citing
area or not. In (Nanba et al., 2000) they use their cit-
ing area identification algorithm to improve citation
type classification and automatic survey generation.
Qazvinian and Radev (2010) addressed a simi-
lar problem. They proposed a method based on
probabilistic inference to extract non-explicit cit-
ing sentences; i.e., sentences that appear around
the sentence that contains the target reference and
are related to it. They showed experimentally that
citation-based survey generation produces better re-
sults when using both explicit and non-explicit cit-
ing sentences rather than using the explicit ones
alone.
Although this work shares the same general goal
with ours (i.e identifying the pieces of text that are
relevant to a given target reference), our work is dif-
ferent in two ways. First, previous work mostly ig-
nored the fact that the citing sentence itself might
be citing multiple references. Second, it defined the
citing area (Nanba and Okumura, 1999) or the ci-
tation context (Qazvinian and Radev, 2010) as a set
of whole contiguous sentences. In our work, we ad-
dress the case where one citing sentence cites mul-
tiple papers, and define what we call the reference
scope to be the fragments (not necessarily contigu-
ous) of the citing sentence that are related to the tar-
get reference.
In a recent work on citation-based summarization
by Abu-Jbara and Radev (2011), the authors noticed
the issue of having multiple references in one sen-
tence. They raised this issue when they discussed
the factors that impede the coherence and the read-
ability of citation-based summaries. They suggested
that removing the fragments of a citing sentence that
are not relevant to the summarized paper will sig-
nificantly improve the quality of the produced sum-
maries. In their work, they defined the scope of a
given reference as the shortest fragment of the citing
sentence that contains the reference and could form
a grammatical sentence if the rest of the sentence
was removed. They identify the scope by generating
the syntactic parse tree of the sentence and then find-
ing the text that corresponds to the smallest subtree
rooted at an 5 node and contains the target reference
node as one of its leaf nodes. They admitted that
their method was very basic and works only when
the scope forms one grammatical fragment, which
is not true in many cases.
Athar (2011) noticed the same issue with cit-
ing sentences that cite multiple references, but this
time in the context of sentiment analysis in ci-
tations. He showed experimentally that identify-
ing what he termed the scope of citation influ-
ence improves sentiment classification accuracy. He
adapted the same basic method proposed by Abu-
Jbara and Radev (2011). We use this method as a
baseline in our evaluation below.
In addition to this related work, there is a large
body of research that used citing sentences in differ-
ent applications. For example, citing sentences have
been used to summarize the contributions of a scien-
tific paper (Qazvinian and Radev, 2008; Qazvinian
et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara
and Radev, 2011). They have been also used to
generate surveys of scientific paradigms (Nanba and
Okumura, 1999; Mohammad et al., 2009). Several
other papers analyzed citing sentences to recognize
the citation function; i.e., the author’s reason for cit-
ing a given paper (Nanba et al., 2000; Teufel et al.,
2006; Teufel, 2007). Schwartz et al. (2007) pro-
posed a method for aligning the words within citing
sentences that cite the same paper. The goal of his
work was to aid named entity recognition and para-
phrase identification in scientific papers.
</bodyText>
<page confidence="0.9923">
82
</page>
<bodyText confidence="0.99969">
We believe that all the these applications will ben-
efit from the output of our work.
</bodyText>
<sectionHeader confidence="0.969843" genericHeader="method">
3 Problem Definition
</sectionHeader>
<bodyText confidence="0.999849411764706">
The problem that we are trying to solve is to iden-
tify which fragments of a given citing sentence that
cites multiple references are semantically related
to a given target reference. As stated above, we
call these fragments the reference scope. Formally,
given a citing sentence 5 = {w1, w2,..., wJ where
w1, w2, ..., w,,, are the tokens of the sentence and
given that 5 contains a set of two or more references
R, we want to assign the label 1 to the word wi if it
falls in the scope of a given target reference r E R,
and 0 otherwise.
For example, sentences (4) and (5) below are
labeled for the target references Tetreault and
Chodorow (2008), and Cutting et al.(1992) respec-
tively. The underlined words are labeled 1 (i.e.,
inside the target reference scope), while all others
are labeled 0.
</bodyText>
<listItem confidence="0.485851333333333">
(4) For example, Tetreault and Chodorow (2008) use a maximum
entropy classifier to build a model of correct preposition usage, with 7
million instances in their training set, and Lee and Knutsson (2008)
use memory-based learning, with 10 million sentences in their training
set.
(5) There are many POS taggers developed using different techniques
</listItem>
<bodyText confidence="0.67372325">
for many major languages such as transformation-based error-driven
learning (Brill, 1995), decision trees (Black et al., 1992), Markov
model (Cutting et al., 1992), maximum entropy methods (Ratnaparkhi,
1996) etc for English.
</bodyText>
<sectionHeader confidence="0.994128" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.999975">
In this section, we present our approach for address-
ing the problem defined in the previous section. Our
approach involves two stages: 1) preprocessing and
2) reference scope identification. We present three
alternative methods for the second stage. The fol-
lowing two subsections describe the two stages.
</bodyText>
<subsectionHeader confidence="0.99912">
4.1 Stage 1: Preprocessing
</subsectionHeader>
<bodyText confidence="0.999997375">
The goal of the preprocessing stage is to clean and
prepare the citing sentence for the next processing
steps. The second stage involves higher level text
processing such as part-of-speech tagging, syntac-
tic parsing, and dependency parsing. The available
tools for these tasks are not trained on citing sen-
tences which contain references written in a special
format. For example, it is very common in scien-
tific writing to have references (usually written be-
tween parentheses) that are not a syntactic part of the
sentence. It is also common to cite a group of ref-
erences who share the same contribution by listing
them between parentheses separated by a comma or
a semi-colon. We address these issues to improve
the accuracy of the processing done in the second
stage. The preprocessing stage involves three tasks:
</bodyText>
<subsectionHeader confidence="0.621493">
4.1.1 Reference Tagging
</subsectionHeader>
<bodyText confidence="0.999931727272727">
The first preprocessing task is to find and tag all
the references that appear in the citing sentence.
Authors of scientific articles use standard patterns
to include references in text. We apply a regular
expression to find all the references that appear
in a sentence. We replace each reference with a
placeholder. The target reference is replaced by
TREF. Each other reference is replaced by REF.
We keep track of the original text of each replaced
reference. Sentence (6) below shows an example of
a citing sentence with the references replaced.
</bodyText>
<construct confidence="0.782497333333333">
(6) These constraints can be lexicalized (REF.1; REF.2), un-
lexicalized (REF.3; TREF.4) or automatically learned (REF.5;
REF.6).
</construct>
<subsectionHeader confidence="0.60503">
4.1.2 Reference Grouping
</subsectionHeader>
<bodyText confidence="0.999961625">
It is common in scientific writing to attribute one
contribution to a group of references. Sentence (6)
above contains three groups of references. Each
group constitutes one entity. Therefore, we replace
each group with a placeholder. We use GTREF
to replace a group of references that contains the
target reference, and GREF to replace a group of
references that does not contain the target reference.
</bodyText>
<page confidence="0.994678">
83
</page>
<bodyText confidence="0.994979">
Sentence (7) below is the same as sentence (6) but
with the three groups of references replaced.
</bodyText>
<listItem confidence="0.830626">
(7) These constraints can be lexicalized (GREF.1), unlexicalized
(GTREF.2) or automatically learned (GREF.3).
</listItem>
<subsubsectionHeader confidence="0.476604">
4.1.3 Non-syntactic Reference Removal
</subsubsectionHeader>
<bodyText confidence="0.940995576923077">
A reference (REF or TREF) or a group of refer-
ences (GREF or GTREF) could either be a syntactic
constituent and has a semantic role in the sentence
(e.g. GTREF.1 in sentence (8) below) or not (e.g.
REF.2 in sentence (8)).
(8) (GTREF.1) apply fuzzy techniques for integrating source
syntax into hierarchical phrase-based systems (REF.2).
The task in this step is to determine whether a ref-
erence is a syntactic component in the sentence or
not. If yes, we keep it as is. If not, we remove it
from the sentence and keep track of its position. Ac-
cordingly, after this step, REF.2 in sentence (8) will
be removed. We use a rule-based algorithm to deter-
mine whether a reference should be removed from
the sentence or kept. Our algorithm (Algorithm 1)
uses stylistic and linguistic features such as the style
of the reference, the position of the reference, and
the surrounding words to make the decision.
When a reference is removed, we pick a word
from the sentence to represent it. This is needed for
feature extraction in the next stage. We use as a rep-
resentative the head of the closest noun phrase (NP)
that comes before the position of the removed refer-
ence. For example, in sentence (8) above, the closest
NP before REF.2 is hierarchical phrase-based sys-
tems and the head is the noun systems.
</bodyText>
<subsectionHeader confidence="0.989567">
4.2 Stage 2: Reference Scope Identification
</subsectionHeader>
<bodyText confidence="0.881658">
In this section we present three different methods
for identifying the scope of a given reference within
a citing sentence. We compare the performance of
these methods in Section 5. The following three sub-
sections describe the methods.
Algorithm 1 Remove Non-syntactic References
</bodyText>
<listItem confidence="0.946005466666667">
Require: A citing sentence S
1: for all Reference R (REF, TREF, GREF, or GTREF)
in S do
2: if R style matches “Authors (year)” then
3: Keep R // syntactic
4: else if R is the first word in the sentence or in a
clause then
5: Keep R // syntactic
6: else if R is preceded by a preposition (in, of, by,
etc.) then
7: Keep R // syntactic
8: else
9: Remove R // non-syntactic
10: end if
11: end for
</listItem>
<subsubsectionHeader confidence="0.888978">
4.2.1 Word Classification
</subsubsectionHeader>
<bodyText confidence="0.999892">
In this method we define reference scope identifi-
cation as a classification task of the individual words
of the citing sentence. Each word is classified as
inside or outside the scope of a given target refer-
ence. We use a number of linguistic and structural
features to train a classification model on a set of
labeled sentences. The trained model is then used
to label new sentences. The features that we use to
train the model are listed in Table 1. We use the
Stanford parser (Klein and Manning, 2003) for syn-
tactic and dependency parsing. We experiment with
two classification algorithms: Support Vector Ma-
chines (SVM) and logistic regression.
</bodyText>
<subsectionHeader confidence="0.688084">
4.2.2 Sequence Labeling
</subsectionHeader>
<bodyText confidence="0.99999325">
In the method described in Section 4.2.1 above,
we classify each word independently from the la-
bels of the nearby words. The nature of our task,
however, suggests that the accuracy of word classifi-
cation can be improved by considering the labels of
the words surrounding the word being classified. It
is very likely that the word takes the same label as
the word before and after it if they all belong to the
same clause in the sentence. In this method we de-
fine the problem as a sequence labeling task. Now,
instead of looking for the best label for each word
individually, we look for the globally best sequence
</bodyText>
<page confidence="0.989862">
84
</page>
<table confidence="0.999862882352941">
Feature Description
Distance The distance (in words) between the word and the target reference.
Position This feature takes the value 1 if the word comes before the target reference, and 0 otherwise.
Segment After splitting the sentence into segments by punctuation and coordination conjunctions, this feature takes
the value 1 if the word occurs in the same segment with the target reference, and 0 otherwise.
Part of speech tag The part of speech tag of the word, the word before, and the word after.
Dependency Distance Length of the shortest dependency path (in the dependency parse tree) that connects the word to the tar-
get reference or its representative. It has been shown in previous work on relation extraction that the
shortest path between any two entities captures the information required to assert a relationship between
them (Bunescu and Mooney, 2005)
Dependency Relations This item includes a set of features. Each features corresponds to a dependency relation type. If the relation
appears in the dependency path that connects the word to the target reference or its representative, its
corresponding feature takes the value 1, and 0 otherwise.
Common Ancestor Node The type of the node in the syntactic parse tree that is the least common ancestor of the word and the target
reference.
Syntactic Distance The number of edges in the shortest path that connects the word and the target reference in the syntactic
parse tree.
</table>
<tableCaption confidence="0.999962">
Table 1: The features used for word classification and sequence labeling
</tableCaption>
<bodyText confidence="0.991927083333333">
of labels for all the words in the sentence at once.
We use Conditional Random Fields (CRF) as our
sequence labeling algorithm. In particular, we use
first-order chain-structured CRF. The chain consists
of two sets of nodes: a set of hidden nodes Y which
represent the scope labels (0 or 1) in our case, and
a set of observed nodes X which represent the ob-
served features. The task is to estimate the probabil-
ity of a sequence of labels Y given the sequence of
observed features X: P(Y|X)
Lafferty et al. (2001) define this probability to be
a normalized product of potential functions O:
</bodyText>
<equation confidence="0.9980828">
P (y|x) = 11 Ok(yt,yt−1, x) (1)
t
Where Ok(yt, yt−1, x) is defined as
�Ok(yt, yt−1, x) = exp( Akf(yt, yt−1, x)) (2)
k
</equation>
<bodyText confidence="0.999923666666667">
where f(yt, yt−1, x) is a transition feature func-
tion of the label at positions i − 1 and i and the
observation sequence x; and Aj is parameter to be
estimated from training data. We use, as the obser-
vations at each position, the same features that we
used in Section 4.2.1 above (Table 1).
</bodyText>
<subsectionHeader confidence="0.582867">
4.2.3 Segment Classification
</subsectionHeader>
<bodyText confidence="0.937231454545454">
We noticed that the scope of a given reference
often consists of units of higher granularity than
words. Therefore, in this method, we split the
sentence into segments of contiguous words and,
instead of labeling individual words, we label
the whole segment as inside or outside the scope
of the target reference. We experimented with
two different segmentation methods. In the first
method (method-1), we segment the sentence at
punctuation marks, coordination conjunctions, and
a set of special expressions such as “for example”,
“for instance”, “including”, “includes”, “such as”,
“like”, etc. Sentence (8) below shows an example of
this segmentation method (Segments are enclosed
in square brackets).
(8) [Rerankers have been successfully applied to numerous NLP
tasks such as] [parse selection (GTREF)], [parse reranking (GREF)],
[question-answering (REF)].
In the second segmentation method (method-2),
we split the sentence into segments of finer gran-
ularity. We use a chunking tool to identify noun
groups, verb groups, preposition groups, adjective
</bodyText>
<page confidence="0.999455">
85
</page>
<bodyText confidence="0.952297708333333">
groups, and adverb groups. Each such group (or
chunk) forms a segment. If a word does not belong
to any chunk, it forms a singleton segment by
itself. Sentence (9) below shows an example of this
segmentation method (Segments are enclosed in
square brackets).
(9) [To] [score] [the output] [of] [the coreference models],
[we] [employ] [the commonly-used MUC scoring program (REF)]
[and] [the recently-developed CEAF scoring program (TREF)].
We assign a label to each segment in two steps. In
the first step, we use the sequence labeling method
described in Section 4.2.2 to assign labels to all the
individual words in the sentence. In the second step,
we aggregate the labels of all the words contained in
a segment to assign a label to the whole segment. We
experimented with three different label aggregation
rules: 1) rule-1: assign to the segment the majority
label of the words it contains, and 2) rule-2: assign
to the segment the label 1 (i.e., inside) if at least one
of the words contained in the segment is labeled 1,
and assign the label 0 to the segment otherwise, and
3) rule-3: assign the label 0 to the segment if at least
of the words it contains is labeled 0, and assign 1
otherwise.
</bodyText>
<sectionHeader confidence="0.997842" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.949924">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.9999846">
We use the ACL Anthology Network corpus
(AAN) (Radev et al., 2009) in our evaluation. AAN
is a publicly available collection of more than 19,000
NLP papers. AAN provides a manually curated cita-
tion network of its papers and the citing sentence(s)
associated with each edge. The current release of
AAN contains about 76,000 unique citing sentences
56% of which contain 2 or more references and 44%
contain 1 reference only. From this set, we ran-
domly selected 3500 citing sentences, each contain-
ing at least two references (3.75 references on aver-
age with a standard deviation of 2.5). The total num-
ber of references in this set of sentences is 19,591.
We split the data set into two random subsets:
a development set (200 sentences) and a train-
ing/testing set (3300 sentences). We used the devel-
opment set to study the data and develop our strate-
gies of addressing the problem. The second set was
used to train and test the system in a cross-validation
mode.
</bodyText>
<subsectionHeader confidence="0.99885">
5.2 Annotation
</subsectionHeader>
<bodyText confidence="0.99994128">
We asked graduate students with good background
in NLP (the area of the annotated sentences) to pro-
vide three annotations for each sentence in the data
set described above. First, we asked them to de-
termine whether each of the references in the sen-
tence was correctly tagged or not. Second, we asked
them to determine for each reference whether it is a
syntactic constituent in the sentence or not. Third,
we asked them to determine and label the scope of
one reference in each sentence which was marked
as a target reference (TREF). We designed a user-
friendly tool to collect the annotations from the stu-
dents.
To estimate the inter-annotator agreement, we
picked 500 random sentences from our data set and
assigned them to two different annotators. The inter-
annotator agreement was perfect on both the refer-
ence tagging annotation and the reference syntacti-
cality annotation. This is expected since both are ob-
jective, clear, and easy tasks. To measure the inter-
annotator agreement on the scope annotation task,
we deal with it as a word classification task. This
allows us to use the popular classification agreement
measure, the Kappa coefficient (Cohen, 1968). The
Kappa coefficient is defined as follows:
</bodyText>
<equation confidence="0.999061333333333">
P(A) − P(E)
K = (3)
1 − P(E)
</equation>
<bodyText confidence="0.999678857142857">
where P(A) is the relative observed agreement
among raters and P(E) is the hypothetical probabil-
ity of chance agreement. The agreement between
the two annotators on the scope identification task
was K = 0.61. On Landis and Kochs (Landis and
Koch, 1977) scale, this value indicates substantial
agreement.
</bodyText>
<page confidence="0.991224">
86
</page>
<subsectionHeader confidence="0.987683">
5.3 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999988466666667">
We use the Edinburgh Language Technology Text
Tokenization Toolkit (LT-TTT) (Grover et al., 2000)
for text tokenization, part-of-speech tagging, chunk-
ing, and noun phrase head identification. We use
the Stanford parser (Klein and Manning, 2003) for
syntactic and dependency parsing. We use Lib-
SVM (Chang and Lin, 2011) for Support Vector Ma-
chines (SVM) classification. Our SVM model uses a
linear kernel. We use Weka (Hall et al., 2009) for lo-
gistic regression classification. We use the Machine
Learning for Language Toolkit (MALLET) (McCal-
lum, 2002) for CRF-based sequence labeling. In
all the scope identification experiments and results
below, we use 10-fold cross validation for train-
ing/testing.
</bodyText>
<subsectionHeader confidence="0.996913">
5.4 Preprocessing Component Evaluation
</subsectionHeader>
<bodyText confidence="0.999981222222222">
We ran our three rule-based preprocessing modules
on the testing data set and compared the output to
the human annotations. The test set was not used
in the tuning of the system but was done using the
development data set as described above. We report
the results for each of the preprocessing modules.
Our reference tagging module achieved 98.3% pre-
cision and 93.1% recall. Most of the errors were
due to issues with text extraction from PDF or due
to bad references practices by some authors (i.e., not
following scientific referencing standards). Our ref-
erence grouping module achieved perfect accuracy
for all the correctly tagged references. This was
expected since this is a straightforward task. The
non-syntactic reference removal module achieved
90.08% precision and 90.1% recall. Again, most of
the errors were the result of bad referencing prac-
tices by the authors.
</bodyText>
<subsectionHeader confidence="0.9690565">
5.5 Reference Scope Identification
Experiments
</subsectionHeader>
<bodyText confidence="0.996284">
We conducted several experiments to compare the
methods proposed in Section 4 and their variants.
We ran all the experiments on the training/testing
set (the 3300 sentences) described in Section 5.1.
</bodyText>
<table confidence="0.999643909090909">
Method Accuracy Precision Recall F-measure
AR-2011 54.0% 63.3% 33.1% 41.5%
WC-SVM 74.9% 74.5% 93.4% 82.9%
WC-LR 74.3% 76.8% 88.0% 82.0%
SL-CRF 78.2% 80.1% 94.2% 86.6%
SC-S1-R1 73.7% 72.1% 97.8% 83.0%
SC-S1-R2 69.3% 68.4% 98.9% 80.8%
SC-S1-R3 60.0% 61.8% 73.3% 60.9%
SC-S2-R1 81.8% 81.2% 93.8% 87.0%
SC-S2-R2 78.2% 77.3% 94.9% 85.2%
SC-S2-R3 66.1% 67.1% 71.2% 69.1%
</table>
<tableCaption confidence="0.987473">
Table 3: Results of scope identification using the different
algorithms described in the paper
</tableCaption>
<bodyText confidence="0.999583695652174">
The experiments that we ran are as follows: 1) word
classification using a SVM classifier (WC-SVM);
2) word classification using a logistic regression
classifier(WC-LR); 3) CRF-based sequence labeling
(SL-CRF); 4) segment classification using segmen-
tation method-1 and label aggregation rule-1 (SC-
S1-R1); 5,6,7,8,9) same as (4) but using different
combinations of segmentation methods 1 and 2, and
label aggregation rules 1,2 and 3: SC-S1-R2, SC-
S1-R3, SC-S2-R1, SC-S2-R2, SC-S2-R3 (where Sx
refers to segmentation method x and Ry refers to
label aggregation rule y all as explained in Sec-
tion 4.2.3). Finally, 10) we compare our meth-
ods to the baseline method proposed by Abu-Jbara
and Radev (2011) which was described in Section 4
(AR-2011).
To better understand which of the features listed
in Table 1 are more important for the task, we use
Guyon et al.’s (2002) method for feature selection
using SVM to rank the features based on their im-
portance. The results of the experiments and the
feature analysis are presented and discussed in the
following subsection.
</bodyText>
<subsectionHeader confidence="0.763727">
5.6 Results and Discussion
5.6.1 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9985965">
We ran the experiments described in the previ-
ous subsection on the testing data described in Sec-
</bodyText>
<page confidence="0.997223">
87
</page>
<table confidence="0.999832318181818">
Method Output
Example 2 Example 1 Word Classification A wide range of contextual information, such as surrounding words (GREF ), dependency or case structure
(WC-SVM)
(GTREF ), and dependency path (GREF ), has been utilized for similarity calculation, and achieved consid-
erable success.
Sequence Labeling (SL- A wide range of contextual information, such as surrounding words (GREF), dependency or case structure
CRF)
(GTREF), and dependency path (GREF ), has been utilized for similarity calculation, and achieved consid-
erable success.
Segment Classification A wide range of contextual information, such as surrounding words (GREF ), dependency or case structure
(SC-S2-R1)
(GTREF ), and dependency path (GREF ), has been utilized for similarity calculation, and achieved
considerable success.
Word Classification Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering
(WC-SVM)
(REF).
Sequence Labeling (SL- Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering
CRF)
(REF).
Segment Classification Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering
(SC-S2-R1)
(REF).
</table>
<tableCaption confidence="0.999704">
Table 2: Two example outputs produced by the three methods
</tableCaption>
<bodyText confidence="0.999974809523809">
tion 5.1. Table 3 compares the precision, recall, F1,
and accuracy for the three methods described in Sec-
tion 4 and their variations. All the metrics were com-
puted at the word level. The results show that all our
methods outperform the baseline method AR-2011
that was proposed by Abu-Jbara and Radev (2011).
In the word classification method, we notice no sig-
nificant difference between the performance of the
SVM vs Logistic Regression classifier. We also no-
tice that the CRF-based sequence labeling method
performs significantly better than the word classi-
fication method. This result corroborates our intu-
ition that the labels of neighboring words are de-
pendent. The results also show that segment la-
beling generally performs better than word label-
ing. More specifically, the results indicate that seg-
mentation based on chunking and the label aggre-
gation based on plurality when used together (i.e.,
SC-S2-R1) achieve higher precision, accuracy, and
F-measure than the punctuation-based segmentation
and the other label aggregation rules.
</bodyText>
<subsectionHeader confidence="0.576883">
5.6.2 Feature Analysis
</subsectionHeader>
<bodyText confidence="0.999996571428571">
We performed an analysis of our classification
features using Guyon et al. (2002) method. The
analysis revealed that both structural and syntactic
features are important. Among the syntactic fea-
tures, the dependency path is the most important.
Among the structural features, the segment feature
(as described in Table 1) is the most important.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999943785714286">
We presented and compared three different meth-
ods for reference scope identification: word classi-
fication, sequence labeling, and segment classifica-
tion. Our results indicate that segment classification
achieves the best performance. The next direction in
this research is to extract the scope of a given refer-
ence as a standalone grammatical sentence. In many
cases, the scope identified by our method can form
a grammatical sentence with no or minimal postpro-
cessing. In other cases, more advanced text regener-
ation techniques are needed for scope extraction.
Table 2 shows the output of the three methods on
two example sentences. The underlined words are
labeled by the system as scope words.
</bodyText>
<sectionHeader confidence="0.995821" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.88433975">
Amjad Abu-Jbara and Dragomir Radev. 2011. Coherent
citation-based summarization of scientific papers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
</reference>
<page confidence="0.995327">
88
</page>
<reference confidence="0.997905537037037">
guage Technologies, pages 500–509, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Awais Athar. 2011. Sentiment analysis of citations us-
ing sentence structure-based features. In Proceedings
of the ACL 2011 Student Session, pages 81–87, Port-
land, OR, USA, June. Association for Computational
Linguistics.
Christine L. Borgman and Jonathan Furner. 2002. Schol-
arly communication and bibliometrics. ANNUAL RE-
VIEW OF INFORMATION SCIENCE AND TECH-
NOLOGY, 36(1):2–72.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Nat-
ural Language Processing, pages 724–731, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1–
27:27.
J. Cohen. 1968. Weighted kappa: Nominal scale agree-
ment with provision for scaled disagreement or partial
credit. Psychological Bulletin, 70:213–220.
Aaron Elkiss, Siwei Shen, Anthony Fader, G¨unes¸ Erkan,
David States, and Dragomir Radev. 2008. Blind men
and elephants: What do citation summaries tell us
about a research article? J. Am. Soc. Inf. Sci. Tech-
nol., 59(1):51–62.
E. Garfield, Irving H. Sher, and R. J. Torpie. 1984. The
Use of Citation Data in Writing the History of Science.
Institute for Scientific Information Inc., Philadelphia,
Pennsylvania, USA.
Claire Grover, Colin Matheson, Andrei Mikheev, and
Marc Moens. 2000. Lt ttt - a flexible tokenisation
tool. In In Proceedings of Second International Con-
ference on Language Resources and Evaluation, pages
1147–1154.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene selection for cancer
classification using support vector machines. Mach.
Learn., 46:389–422, March.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDDExplor. Newsl., 11(1):10–18.
T. L. Hodges. 1972. Citation indexing-its theory
and application in science, technology, and humani-
ties. Ph.D. thesis, University of California at Berke-
ley.Ph.D. thesis, University of California at Berkeley.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In IN PROCEEDINGS OF
THE 41ST ANNUAL MEETING OF THE ASSOCIA-
TION FOR COMPUTATIONAL LINGUISTICS, pages
423–430.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ’01, pages
282–289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
J. Richard Landis and Gary G. Koch. 1977. The Mea-
surement of Observer Agreement for Categorical Data.
Biometrics, 33(1):159–174, March.
Terttu Luukkonen. 1992. Is scientists’ publishing be-
haviour rewardseeking? Scientometrics, 24:297–319.
10.1007/BF02017913.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings ofACL-08: HLT, pages 816–824, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 584–592, Boulder, Colorado, June. Association
for Computational Linguistics.
Preslav I. Nakov, Ariel S. Schwartz, and Marti A. Hearst.
2004. Citances: Citation sentences for semantic anal-
ysis of bioscience text. In In Proceedings of the SI-
GIR04 workshop on Search and Discovery in Bioin-
formatics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference in-
formation. In IJCAI ’99: Proceedings of the Six-
teenth International Joint Conference on Artificial In-
telligence, pages 926–931, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Hidetsugu Nanba, Noriko Kando, Manabu Okumura, and
Of Information Science. 2000. Classification of re-
search papers using citation links and citation types:
Towards automatic review article generation.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 689–696, Manchester, UK, August. Coling 2008
Organizing Committee.
</reference>
<page confidence="0.994224">
89
</page>
<reference confidence="0.997730513513514">
Vahed Qazvinian and Dragomir R. Radev. 2010. Identi-
fying non-explicit citing sentences for citation-based
summarization. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 555–564, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
Ozgur. 2010. Citation summarization through
keyphrase extraction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(Coling 2010), pages 895–903, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The acl anthology network corpus.
In NLPIR4DL ’09: Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital Li-
braries, pages 54–61, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Ariel Schwartz, Anna Divoli, and Marti Hearst. 2007.
Multiple alignment of citation sentences with con-
ditional random fields and posterior decoding. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 847–857.
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attributing
scientific work to citations. In In Proceedings of
NAACL/HLT-07.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
In Proc. of EMNLP-06.
Simone Teufel. 2007. Argumentative zoning for im-
proved citation indexing. computing attitude and affect
in text. In Theory and Applications, pages 159170.
Howard D. White. 2004. Citation analysis and discourse
analysis revisited. Applied Linguistics, 25(1):89–116.
</reference>
<page confidence="0.998541">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.090741">
<title confidence="0.999144">Reference Scope Identification in Citing Sentences</title>
<author confidence="0.746632">Amjad</author>
<affiliation confidence="0.960357">EECS University of</affiliation>
<author confidence="0.54382">Ann Arbor</author>
<author confidence="0.54382">MI</author>
<email confidence="0.998745">amjbara@umich.edu</email>
<author confidence="0.533867">Dragomir</author>
<affiliation confidence="0.9825285">EECS University of</affiliation>
<author confidence="0.445121">Ann Arbor</author>
<author confidence="0.445121">MI</author>
<email confidence="0.999616">radev@umich.edu</email>
<abstract confidence="0.999660083333333">A citing sentence is one that appears in a scientific article and cites previous work. Citing sentences have been studied and used in many applications. For example, they have been used in scientific paper summarization, automatic survey generation, paraphrase identification, and citation function classification. Citing sentences that cite multiple papers are common in scientific writing. This observation should be taken into consideration when using citing sentences in applications. For instance, when a citing sentence is used in a summary of a scientific paper, only the fragments of the sentence that are relevant to the summarized paper should be included in the summary. In this paper, we present and compare three different approaches for identifying the fragments of a citing sentence that are related to a given target reference. Our methods are: word classification, sequence labeling, and segment classification. Our experiments show that segment classification achieves the best results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amjad Abu-Jbara</author>
<author>Dragomir Radev</author>
</authors>
<title>Coherent citation-based summarization of scientific papers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>500--509</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="9033" citStr="Abu-Jbara and Radev (2011)" startWordPosition="1470" endWordPosition="1473">nce), our work is different in two ways. First, previous work mostly ignored the fact that the citing sentence itself might be citing multiple references. Second, it defined the citing area (Nanba and Okumura, 1999) or the citation context (Qazvinian and Radev, 2010) as a set of whole contiguous sentences. In our work, we address the case where one citing sentence cites multiple papers, and define what we call the reference scope to be the fragments (not necessarily contiguous) of the citing sentence that are related to the target reference. In a recent work on citation-based summarization by Abu-Jbara and Radev (2011), the authors noticed the issue of having multiple references in one sentence. They raised this issue when they discussed the factors that impede the coherence and the readability of citation-based summaries. They suggested that removing the fragments of a citing sentence that are not relevant to the summarized paper will significantly improve the quality of the produced summaries. In their work, they defined the scope of a given reference as the shortest fragment of the citing sentence that contains the reference and could form a grammatical sentence if the rest of the sentence was removed. T</context>
<context position="10747" citStr="Abu-Jbara and Radev, 2011" startWordPosition="1758" endWordPosition="1761">t of sentiment analysis in citations. He showed experimentally that identifying what he termed the scope of citation influence improves sentiment classification accuracy. He adapted the same basic method proposed by AbuJbara and Radev (2011). We use this method as a baseline in our evaluation below. In addition to this related work, there is a large body of research that used citing sentences in different applications. For example, citing sentences have been used to summarize the contributions of a scientific paper (Qazvinian and Radev, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011). They have been also used to generate surveys of scientific paradigms (Nanba and Okumura, 1999; Mohammad et al., 2009). Several other papers analyzed citing sentences to recognize the citation function; i.e., the author’s reason for citing a given paper (Nanba et al., 2000; Teufel et al., 2006; Teufel, 2007). Schwartz et al. (2007) proposed a method for aligning the words within citing sentences that cite the same paper. The goal of his work was to aid named entity recognition and paraphrase identification in scientific papers. 82 We believe that all the these applications will benefit from t</context>
<context position="29018" citStr="Abu-Jbara and Radev (2011)" startWordPosition="4792" endWordPosition="4795"> a SVM classifier (WC-SVM); 2) word classification using a logistic regression classifier(WC-LR); 3) CRF-based sequence labeling (SL-CRF); 4) segment classification using segmentation method-1 and label aggregation rule-1 (SCS1-R1); 5,6,7,8,9) same as (4) but using different combinations of segmentation methods 1 and 2, and label aggregation rules 1,2 and 3: SC-S1-R2, SCS1-R3, SC-S2-R1, SC-S2-R2, SC-S2-R3 (where Sx refers to segmentation method x and Ry refers to label aggregation rule y all as explained in Section 4.2.3). Finally, 10) we compare our methods to the baseline method proposed by Abu-Jbara and Radev (2011) which was described in Section 4 (AR-2011). To better understand which of the features listed in Table 1 are more important for the task, we use Guyon et al.’s (2002) method for feature selection using SVM to rank the features based on their importance. The results of the experiments and the feature analysis are presented and discussed in the following subsection. 5.6 Results and Discussion 5.6.1 Experimental Results We ran the experiments described in the previous subsection on the testing data described in Sec87 Method Output Example 2 Example 1 Word Classification A wide range of contextua</context>
<context position="31106" citStr="Abu-Jbara and Radev (2011)" startWordPosition="5109" endWordPosition="5112">eling (SL- Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering CRF) (REF). Segment Classification Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering (SC-S2-R1) (REF). Table 2: Two example outputs produced by the three methods tion 5.1. Table 3 compares the precision, recall, F1, and accuracy for the three methods described in Section 4 and their variations. All the metrics were computed at the word level. The results show that all our methods outperform the baseline method AR-2011 that was proposed by Abu-Jbara and Radev (2011). In the word classification method, we notice no significant difference between the performance of the SVM vs Logistic Regression classifier. We also notice that the CRF-based sequence labeling method performs significantly better than the word classification method. This result corroborates our intuition that the labels of neighboring words are dependent. The results also show that segment labeling generally performs better than word labeling. More specifically, the results indicate that segmentation based on chunking and the label aggregation based on plurality when used together (i.e., SC-</context>
</contexts>
<marker>Abu-Jbara, Radev, 2011</marker>
<rawString>Amjad Abu-Jbara and Dragomir Radev. 2011. Coherent citation-based summarization of scientific papers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 500–509, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Awais Athar</author>
</authors>
<title>Sentiment analysis of citations using sentence structure-based features.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL 2011 Student Session,</booktitle>
<pages>81--87</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, OR, USA,</location>
<contexts>
<context position="10017" citStr="Athar (2011)" startWordPosition="1640" endWordPosition="1641">d summaries. In their work, they defined the scope of a given reference as the shortest fragment of the citing sentence that contains the reference and could form a grammatical sentence if the rest of the sentence was removed. They identify the scope by generating the syntactic parse tree of the sentence and then finding the text that corresponds to the smallest subtree rooted at an 5 node and contains the target reference node as one of its leaf nodes. They admitted that their method was very basic and works only when the scope forms one grammatical fragment, which is not true in many cases. Athar (2011) noticed the same issue with citing sentences that cite multiple references, but this time in the context of sentiment analysis in citations. He showed experimentally that identifying what he termed the scope of citation influence improves sentiment classification accuracy. He adapted the same basic method proposed by AbuJbara and Radev (2011). We use this method as a baseline in our evaluation below. In addition to this related work, there is a large body of research that used citing sentences in different applications. For example, citing sentences have been used to summarize the contributio</context>
</contexts>
<marker>Athar, 2011</marker>
<rawString>Awais Athar. 2011. Sentiment analysis of citations using sentence structure-based features. In Proceedings of the ACL 2011 Student Session, pages 81–87, Portland, OR, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine L Borgman</author>
<author>Jonathan Furner</author>
</authors>
<title>Scholarly communication and bibliometrics.</title>
<date>2002</date>
<journal>ANNUAL REVIEW OF INFORMATION SCIENCE AND TECHNOLOGY,</journal>
<volume>36</volume>
<issue>1</issue>
<contexts>
<context position="6877" citStr="Borgman and Furner, 2002" startWordPosition="1104" endWordPosition="1107">stinguish between syntactic and non-syntactic references. The rest of this paper is organized as follows. Section 2 examines the related work. We define the problem in Section3. Section 4 presents our approaches. Experiments, results and analysis are presented in Section 5. We conclude and provide directions to future work in Section 6 2 Related Work Our work is related to a large body of research on citations (Hodges, 1972; Garfield et al., 1984). The interest in studying citations stems from the fact that bibliometric measures are commonly used to estimate the impact of a researcher’s work (Borgman and Furner, 2002; Luukkonen, 1992). White (2004) provides a good recent survey of the different research lines that use citations. In this section we review the research lines that are relevant to our work 81 and show how our work is different. One line of research that is related to our work has to do with identifying what Nanba and Okumura (1999) call the citing area They define the citing area as the succession of sentences that appear around the location of a given reference in a scientific paper and have connection to it. Their algorithm starts by adding the sentence that contains the target reference as</context>
</contexts>
<marker>Borgman, Furner, 2002</marker>
<rawString>Christine L. Borgman and Jonathan Furner. 2002. Scholarly communication and bibliometrics. ANNUAL REVIEW OF INFORMATION SCIENCE AND TECHNOLOGY, 36(1):2–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>724--731</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="19492" citStr="Bunescu and Mooney, 2005" startWordPosition="3228" endWordPosition="3231">segments by punctuation and coordination conjunctions, this feature takes the value 1 if the word occurs in the same segment with the target reference, and 0 otherwise. Part of speech tag The part of speech tag of the word, the word before, and the word after. Dependency Distance Length of the shortest dependency path (in the dependency parse tree) that connects the word to the target reference or its representative. It has been shown in previous work on relation extraction that the shortest path between any two entities captures the information required to assert a relationship between them (Bunescu and Mooney, 2005) Dependency Relations This item includes a set of features. Each features corresponds to a dependency relation type. If the relation appears in the dependency path that connects the word to the target reference or its representative, its corresponding feature takes the value 1, and 0 otherwise. Common Ancestor Node The type of the node in the syntactic parse tree that is the least common ancestor of the word and the target reference. Syntactic Distance The number of edges in the shortest path that connects the word and the target reference in the syntactic parse tree. Table 1: The features use</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 724–731, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<pages>27--27</pages>
<contexts>
<context position="26313" citStr="Chang and Lin, 2011" startWordPosition="4374" endWordPosition="4377">bserved agreement among raters and P(E) is the hypothetical probability of chance agreement. The agreement between the two annotators on the scope identification task was K = 0.61. On Landis and Kochs (Landis and Koch, 1977) scale, this value indicates substantial agreement. 86 5.3 Experimental Setup We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) (Grover et al., 2000) for text tokenization, part-of-speech tagging, chunking, and noun phrase head identification. We use the Stanford parser (Klein and Manning, 2003) for syntactic and dependency parsing. We use LibSVM (Chang and Lin, 2011) for Support Vector Machines (SVM) classification. Our SVM model uses a linear kernel. We use Weka (Hall et al., 2009) for logistic regression classification. We use the Machine Learning for Language Toolkit (MALLET) (McCallum, 2002) for CRF-based sequence labeling. In all the scope identification experiments and results below, we use 10-fold cross validation for training/testing. 5.4 Preprocessing Component Evaluation We ran our three rule-based preprocessing modules on the testing data set and compared the output to the human annotations. The test set was not used in the tuning of the system</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1– 27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial credit.</title>
<date>1968</date>
<journal>Psychological Bulletin,</journal>
<pages>70--213</pages>
<contexts>
<context position="25589" citStr="Cohen, 1968" startWordPosition="4260" endWordPosition="4261">friendly tool to collect the annotations from the students. To estimate the inter-annotator agreement, we picked 500 random sentences from our data set and assigned them to two different annotators. The interannotator agreement was perfect on both the reference tagging annotation and the reference syntacticality annotation. This is expected since both are objective, clear, and easy tasks. To measure the interannotator agreement on the scope annotation task, we deal with it as a word classification task. This allows us to use the popular classification agreement measure, the Kappa coefficient (Cohen, 1968). The Kappa coefficient is defined as follows: P(A) − P(E) K = (3) 1 − P(E) where P(A) is the relative observed agreement among raters and P(E) is the hypothetical probability of chance agreement. The agreement between the two annotators on the scope identification task was K = 0.61. On Landis and Kochs (Landis and Koch, 1977) scale, this value indicates substantial agreement. 86 5.3 Experimental Setup We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) (Grover et al., 2000) for text tokenization, part-of-speech tagging, chunking, and noun phrase head identification. We</context>
</contexts>
<marker>Cohen, 1968</marker>
<rawString>J. Cohen. 1968. Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial credit. Psychological Bulletin, 70:213–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Elkiss</author>
<author>Siwei Shen</author>
<author>Anthony Fader</author>
<author>G¨unes¸ Erkan</author>
<author>David States</author>
<author>Dragomir Radev</author>
</authors>
<title>Blind men and elephants: What do citation summaries tell us about a research article?</title>
<date>2008</date>
<journal>J. Am. Soc. Inf. Sci. Technol.,</journal>
<volume>59</volume>
<issue>1</issue>
<contexts>
<context position="2009" citStr="Elkiss et al., 2008" startWordPosition="305" endWordPosition="308">tific article, it is usually accompanied by a span of text that highlights the important contributions of the cited article. We call a sentence that contains an explicit reference to previous work a citation sentence. For example, sentence (1) below is a citing sentence that cites a paper by Philip Resnik and describes the problem Resnik addressed in his paper. (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are v</context>
</contexts>
<marker>Elkiss, Shen, Fader, Erkan, States, Radev, 2008</marker>
<rawString>Aaron Elkiss, Siwei Shen, Anthony Fader, G¨unes¸ Erkan, David States, and Dragomir Radev. 2008. Blind men and elephants: What do citation summaries tell us about a research article? J. Am. Soc. Inf. Sci. Technol., 59(1):51–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Garfield</author>
<author>Irving H Sher</author>
<author>R J Torpie</author>
</authors>
<date>1984</date>
<booktitle>The Use of Citation Data in Writing the History of Science. Institute for Scientific Information Inc.,</booktitle>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="6704" citStr="Garfield et al., 1984" startWordPosition="1076" endWordPosition="1079">ing any of the three approaches is preceded by a preprocessing stage. In this stage, citing sentences are analyzed to tag references, identify groups of references, and distinguish between syntactic and non-syntactic references. The rest of this paper is organized as follows. Section 2 examines the related work. We define the problem in Section3. Section 4 presents our approaches. Experiments, results and analysis are presented in Section 5. We conclude and provide directions to future work in Section 6 2 Related Work Our work is related to a large body of research on citations (Hodges, 1972; Garfield et al., 1984). The interest in studying citations stems from the fact that bibliometric measures are commonly used to estimate the impact of a researcher’s work (Borgman and Furner, 2002; Luukkonen, 1992). White (2004) provides a good recent survey of the different research lines that use citations. In this section we review the research lines that are relevant to our work 81 and show how our work is different. One line of research that is related to our work has to do with identifying what Nanba and Okumura (1999) call the citing area They define the citing area as the succession of sentences that appear </context>
</contexts>
<marker>Garfield, Sher, Torpie, 1984</marker>
<rawString>E. Garfield, Irving H. Sher, and R. J. Torpie. 1984. The Use of Citation Data in Writing the History of Science. Institute for Scientific Information Inc., Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Grover</author>
<author>Colin Matheson</author>
<author>Andrei Mikheev</author>
<author>Marc Moens</author>
</authors>
<title>Lt ttt - a flexible tokenisation tool. In</title>
<date>2000</date>
<booktitle>In Proceedings of Second International Conference on Language Resources and Evaluation,</booktitle>
<pages>1147--1154</pages>
<contexts>
<context position="26092" citStr="Grover et al., 2000" startWordPosition="4340" endWordPosition="4343">ication task. This allows us to use the popular classification agreement measure, the Kappa coefficient (Cohen, 1968). The Kappa coefficient is defined as follows: P(A) − P(E) K = (3) 1 − P(E) where P(A) is the relative observed agreement among raters and P(E) is the hypothetical probability of chance agreement. The agreement between the two annotators on the scope identification task was K = 0.61. On Landis and Kochs (Landis and Koch, 1977) scale, this value indicates substantial agreement. 86 5.3 Experimental Setup We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) (Grover et al., 2000) for text tokenization, part-of-speech tagging, chunking, and noun phrase head identification. We use the Stanford parser (Klein and Manning, 2003) for syntactic and dependency parsing. We use LibSVM (Chang and Lin, 2011) for Support Vector Machines (SVM) classification. Our SVM model uses a linear kernel. We use Weka (Hall et al., 2009) for logistic regression classification. We use the Machine Learning for Language Toolkit (MALLET) (McCallum, 2002) for CRF-based sequence labeling. In all the scope identification experiments and results below, we use 10-fold cross validation for training/test</context>
</contexts>
<marker>Grover, Matheson, Mikheev, Moens, 2000</marker>
<rawString>Claire Grover, Colin Matheson, Andrei Mikheev, and Marc Moens. 2000. Lt ttt - a flexible tokenisation tool. In In Proceedings of Second International Conference on Language Resources and Evaluation, pages 1147–1154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabelle Guyon</author>
<author>Jason Weston</author>
<author>Stephen Barnhill</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Gene selection for cancer classification using support vector machines.</title>
<date>2002</date>
<location>Mach. Learn., 46:389–422,</location>
<contexts>
<context position="31946" citStr="Guyon et al. (2002)" startWordPosition="5235" endWordPosition="5238">tly better than the word classification method. This result corroborates our intuition that the labels of neighboring words are dependent. The results also show that segment labeling generally performs better than word labeling. More specifically, the results indicate that segmentation based on chunking and the label aggregation based on plurality when used together (i.e., SC-S2-R1) achieve higher precision, accuracy, and F-measure than the punctuation-based segmentation and the other label aggregation rules. 5.6.2 Feature Analysis We performed an analysis of our classification features using Guyon et al. (2002) method. The analysis revealed that both structural and syntactic features are important. Among the syntactic features, the dependency path is the most important. Among the structural features, the segment feature (as described in Table 1) is the most important. 6 Conclusions We presented and compared three different methods for reference scope identification: word classification, sequence labeling, and segment classification. Our results indicate that segment classification achieves the best performance. The next direction in this research is to extract the scope of a given reference as a sta</context>
</contexts>
<marker>Guyon, Weston, Barnhill, Vapnik, 2002</marker>
<rawString>Isabelle Guyon, Jason Weston, Stephen Barnhill, and Vladimir Vapnik. 2002. Gene selection for cancer classification using support vector machines. Mach. Learn., 46:389–422, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDDExplor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="26431" citStr="Hall et al., 2009" startWordPosition="4395" endWordPosition="4398">wo annotators on the scope identification task was K = 0.61. On Landis and Kochs (Landis and Koch, 1977) scale, this value indicates substantial agreement. 86 5.3 Experimental Setup We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) (Grover et al., 2000) for text tokenization, part-of-speech tagging, chunking, and noun phrase head identification. We use the Stanford parser (Klein and Manning, 2003) for syntactic and dependency parsing. We use LibSVM (Chang and Lin, 2011) for Support Vector Machines (SVM) classification. Our SVM model uses a linear kernel. We use Weka (Hall et al., 2009) for logistic regression classification. We use the Machine Learning for Language Toolkit (MALLET) (McCallum, 2002) for CRF-based sequence labeling. In all the scope identification experiments and results below, we use 10-fold cross validation for training/testing. 5.4 Preprocessing Component Evaluation We ran our three rule-based preprocessing modules on the testing data set and compared the output to the human annotations. The test set was not used in the tuning of the system but was done using the development data set as described above. We report the results for each of the preprocessing m</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: an update. SIGKDDExplor. Newsl., 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Hodges</author>
</authors>
<title>Citation indexing-its theory and application in science, technology, and humanities.</title>
<date>1972</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Berkeley.Ph.D. thesis, University of California at Berkeley.</institution>
<contexts>
<context position="6680" citStr="Hodges, 1972" startWordPosition="1074" endWordPosition="1075">ference. Applying any of the three approaches is preceded by a preprocessing stage. In this stage, citing sentences are analyzed to tag references, identify groups of references, and distinguish between syntactic and non-syntactic references. The rest of this paper is organized as follows. Section 2 examines the related work. We define the problem in Section3. Section 4 presents our approaches. Experiments, results and analysis are presented in Section 5. We conclude and provide directions to future work in Section 6 2 Related Work Our work is related to a large body of research on citations (Hodges, 1972; Garfield et al., 1984). The interest in studying citations stems from the fact that bibliometric measures are commonly used to estimate the impact of a researcher’s work (Borgman and Furner, 2002; Luukkonen, 1992). White (2004) provides a good recent survey of the different research lines that use citations. In this section we review the research lines that are relevant to our work 81 and show how our work is different. One line of research that is related to our work has to do with identifying what Nanba and Okumura (1999) call the citing area They define the citing area as the succession o</context>
</contexts>
<marker>Hodges, 1972</marker>
<rawString>T. L. Hodges. 1972. Citation indexing-its theory and application in science, technology, and humanities. Ph.D. thesis, University of California at Berkeley.Ph.D. thesis, University of California at Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In IN PROCEEDINGS OF THE 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="17840" citStr="Klein and Manning, 2003" startWordPosition="2952" endWordPosition="2955">etc.) then 7: Keep R // syntactic 8: else 9: Remove R // non-syntactic 10: end if 11: end for 4.2.1 Word Classification In this method we define reference scope identification as a classification task of the individual words of the citing sentence. Each word is classified as inside or outside the scope of a given target reference. We use a number of linguistic and structural features to train a classification model on a set of labeled sentences. The trained model is then used to label new sentences. The features that we use to train the model are listed in Table 1. We use the Stanford parser (Klein and Manning, 2003) for syntactic and dependency parsing. We experiment with two classification algorithms: Support Vector Machines (SVM) and logistic regression. 4.2.2 Sequence Labeling In the method described in Section 4.2.1 above, we classify each word independently from the labels of the nearby words. The nature of our task, however, suggests that the accuracy of word classification can be improved by considering the labels of the words surrounding the word being classified. It is very likely that the word takes the same label as the word before and after it if they all belong to the same clause in the sent</context>
<context position="26239" citStr="Klein and Manning, 2003" startWordPosition="4361" endWordPosition="4364"> defined as follows: P(A) − P(E) K = (3) 1 − P(E) where P(A) is the relative observed agreement among raters and P(E) is the hypothetical probability of chance agreement. The agreement between the two annotators on the scope identification task was K = 0.61. On Landis and Kochs (Landis and Koch, 1977) scale, this value indicates substantial agreement. 86 5.3 Experimental Setup We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) (Grover et al., 2000) for text tokenization, part-of-speech tagging, chunking, and noun phrase head identification. We use the Stanford parser (Klein and Manning, 2003) for syntactic and dependency parsing. We use LibSVM (Chang and Lin, 2011) for Support Vector Machines (SVM) classification. Our SVM model uses a linear kernel. We use Weka (Hall et al., 2009) for logistic regression classification. We use the Machine Learning for Language Toolkit (MALLET) (McCallum, 2002) for CRF-based sequence labeling. In all the scope identification experiments and results below, we use 10-fold cross validation for training/testing. 5.4 Preprocessing Component Evaluation We ran our three rule-based preprocessing modules on the testing data set and compared the output to th</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In IN PROCEEDINGS OF THE 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="20651" citStr="Lafferty et al. (2001)" startWordPosition="3427" endWordPosition="3430">eference in the syntactic parse tree. Table 1: The features used for word classification and sequence labeling of labels for all the words in the sentence at once. We use Conditional Random Fields (CRF) as our sequence labeling algorithm. In particular, we use first-order chain-structured CRF. The chain consists of two sets of nodes: a set of hidden nodes Y which represent the scope labels (0 or 1) in our case, and a set of observed nodes X which represent the observed features. The task is to estimate the probability of a sequence of labels Y given the sequence of observed features X: P(Y|X) Lafferty et al. (2001) define this probability to be a normalized product of potential functions O: P (y|x) = 11 Ok(yt,yt−1, x) (1) t Where Ok(yt, yt−1, x) is defined as �Ok(yt, yt−1, x) = exp( Akf(yt, yt−1, x)) (2) k where f(yt, yt−1, x) is a transition feature function of the label at positions i − 1 and i and the observation sequence x; and Aj is parameter to be estimated from training data. We use, as the observations at each position, the same features that we used in Section 4.2.1 above (Table 1). 4.2.3 Segment Classification We noticed that the scope of a given reference often consists of units of higher gra</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The Measurement of Observer Agreement for Categorical Data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="25917" citStr="Landis and Koch, 1977" startWordPosition="4316" endWordPosition="4319">tation. This is expected since both are objective, clear, and easy tasks. To measure the interannotator agreement on the scope annotation task, we deal with it as a word classification task. This allows us to use the popular classification agreement measure, the Kappa coefficient (Cohen, 1968). The Kappa coefficient is defined as follows: P(A) − P(E) K = (3) 1 − P(E) where P(A) is the relative observed agreement among raters and P(E) is the hypothetical probability of chance agreement. The agreement between the two annotators on the scope identification task was K = 0.61. On Landis and Kochs (Landis and Koch, 1977) scale, this value indicates substantial agreement. 86 5.3 Experimental Setup We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) (Grover et al., 2000) for text tokenization, part-of-speech tagging, chunking, and noun phrase head identification. We use the Stanford parser (Klein and Manning, 2003) for syntactic and dependency parsing. We use LibSVM (Chang and Lin, 2011) for Support Vector Machines (SVM) classification. Our SVM model uses a linear kernel. We use Weka (Hall et al., 2009) for logistic regression classification. We use the Machine Learning for Language Tool</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. The Measurement of Observer Agreement for Categorical Data. Biometrics, 33(1):159–174, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terttu Luukkonen</author>
</authors>
<title>Is scientists’ publishing behaviour rewardseeking?</title>
<date>1992</date>
<journal>Scientometrics,</journal>
<volume>24</volume>
<pages>10--1007</pages>
<contexts>
<context position="6895" citStr="Luukkonen, 1992" startWordPosition="1108" endWordPosition="1109">c and non-syntactic references. The rest of this paper is organized as follows. Section 2 examines the related work. We define the problem in Section3. Section 4 presents our approaches. Experiments, results and analysis are presented in Section 5. We conclude and provide directions to future work in Section 6 2 Related Work Our work is related to a large body of research on citations (Hodges, 1972; Garfield et al., 1984). The interest in studying citations stems from the fact that bibliometric measures are commonly used to estimate the impact of a researcher’s work (Borgman and Furner, 2002; Luukkonen, 1992). White (2004) provides a good recent survey of the different research lines that use citations. In this section we review the research lines that are relevant to our work 81 and show how our work is different. One line of research that is related to our work has to do with identifying what Nanba and Okumura (1999) call the citing area They define the citing area as the succession of sentences that appear around the location of a given reference in a scientific paper and have connection to it. Their algorithm starts by adding the sentence that contains the target reference as the first member </context>
</contexts>
<marker>Luukkonen, 1992</marker>
<rawString>Terttu Luukkonen. 1992. Is scientists’ publishing behaviour rewardseeking? Scientometrics, 24:297–319. 10.1007/BF02017913.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="26546" citStr="McCallum, 2002" startWordPosition="4413" endWordPosition="4415">alue indicates substantial agreement. 86 5.3 Experimental Setup We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) (Grover et al., 2000) for text tokenization, part-of-speech tagging, chunking, and noun phrase head identification. We use the Stanford parser (Klein and Manning, 2003) for syntactic and dependency parsing. We use LibSVM (Chang and Lin, 2011) for Support Vector Machines (SVM) classification. Our SVM model uses a linear kernel. We use Weka (Hall et al., 2009) for logistic regression classification. We use the Machine Learning for Language Toolkit (MALLET) (McCallum, 2002) for CRF-based sequence labeling. In all the scope identification experiments and results below, we use 10-fold cross validation for training/testing. 5.4 Preprocessing Component Evaluation We ran our three rule-based preprocessing modules on the testing data set and compared the output to the human annotations. The test set was not used in the tuning of the system but was done using the development data set as described above. We report the results for each of the preprocessing modules. Our reference tagging module achieved 98.3% precision and 93.1% recall. Most of the errors were due to issu</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Generating impact-based summaries for scientific literature.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>816--824</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2056" citStr="Mei and Zhai, 2008" startWordPosition="313" endWordPosition="316">an of text that highlights the important contributions of the cited article. We call a sentence that contains an explicit reference to previous work a citation sentence. For example, sentence (1) below is a citing sentence that cites a paper by Philip Resnik and describes the problem Resnik addressed in his paper. (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, sentence (2) below con</context>
</contexts>
<marker>Mei, Zhai, 2008</marker>
<rawString>Qiaozhu Mei and ChengXiang Zhai. 2008. Generating impact-based summaries for scientific literature. In Proceedings ofACL-08: HLT, pages 816–824, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Melissa Egan</author>
<author>Ahmed Hassan</author>
<author>Pradeep Muthukrishan</author>
<author>Vahed Qazvinian</author>
<author>Dragomir Radev</author>
<author>David Zajic</author>
</authors>
<title>Using citations to generate surveys of scientific paradigms.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>584--592</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="2207" citStr="Mohammad et al., 2009" startWordPosition="338" endWordPosition="341">k a citation sentence. For example, sentence (1) below is a citing sentence that cites a paper by Philip Resnik and describes the problem Resnik addressed in his paper. (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, sentence (2) below contains three references. (2) Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the web to generate corpora for languages where electronic re</context>
<context position="10866" citStr="Mohammad et al., 2009" startWordPosition="1777" endWordPosition="1780">nce improves sentiment classification accuracy. He adapted the same basic method proposed by AbuJbara and Radev (2011). We use this method as a baseline in our evaluation below. In addition to this related work, there is a large body of research that used citing sentences in different applications. For example, citing sentences have been used to summarize the contributions of a scientific paper (Qazvinian and Radev, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011). They have been also used to generate surveys of scientific paradigms (Nanba and Okumura, 1999; Mohammad et al., 2009). Several other papers analyzed citing sentences to recognize the citation function; i.e., the author’s reason for citing a given paper (Nanba et al., 2000; Teufel et al., 2006; Teufel, 2007). Schwartz et al. (2007) proposed a method for aligning the words within citing sentences that cite the same paper. The goal of his work was to aid named entity recognition and paraphrase identification in scientific papers. 82 We believe that all the these applications will benefit from the output of our work. 3 Problem Definition The problem that we are trying to solve is to identify which fragments of a</context>
</contexts>
<marker>Mohammad, Dorr, Egan, Hassan, Muthukrishan, Qazvinian, Radev, Zajic, 2009</marker>
<rawString>Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan, Pradeep Muthukrishan, Vahed Qazvinian, Dragomir Radev, and David Zajic. 2009. Using citations to generate surveys of scientific paradigms. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 584–592, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav I Nakov</author>
<author>Ariel S Schwartz</author>
<author>Marti A Hearst</author>
</authors>
<title>Citances: Citation sentences for semantic analysis of bioscience text.</title>
<date>2004</date>
<booktitle>In In Proceedings of the SIGIR04 workshop on Search and Discovery in Bioinformatics.</booktitle>
<contexts>
<context position="2375" citStr="Nakov et al., 2004" startWordPosition="362" endWordPosition="365"> Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, sentence (2) below contains three references. (2) Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the web to generate corpora for languages where electronic resources are scarce, while Resnik (1999) describes a method for mining the web for bilingual texts. 80 2012 Conference of the North American Chapter of the Association f</context>
</contexts>
<marker>Nakov, Schwartz, Hearst, 2004</marker>
<rawString>Preslav I. Nakov, Ariel S. Schwartz, and Marti A. Hearst. 2004. Citances: Citation sentences for semantic analysis of bioscience text. In In Proceedings of the SIGIR04 workshop on Search and Discovery in Bioinformatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetsugu Nanba</author>
<author>Manabu Okumura</author>
</authors>
<title>Towards multi-paper summarization using reference information.</title>
<date>1999</date>
<booktitle>In IJCAI ’99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>926--931</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="7211" citStr="Nanba and Okumura (1999)" startWordPosition="1165" endWordPosition="1169">n 6 2 Related Work Our work is related to a large body of research on citations (Hodges, 1972; Garfield et al., 1984). The interest in studying citations stems from the fact that bibliometric measures are commonly used to estimate the impact of a researcher’s work (Borgman and Furner, 2002; Luukkonen, 1992). White (2004) provides a good recent survey of the different research lines that use citations. In this section we review the research lines that are relevant to our work 81 and show how our work is different. One line of research that is related to our work has to do with identifying what Nanba and Okumura (1999) call the citing area They define the citing area as the succession of sentences that appear around the location of a given reference in a scientific paper and have connection to it. Their algorithm starts by adding the sentence that contains the target reference as the first member sentence in the citing area. Then, they use a set of cue words and hand-crafted rules to determine whether the surrounding sentences should be added to the citing area or not. In (Nanba et al., 2000) they use their citing area identification algorithm to improve citation type classification and automatic survey gen</context>
<context position="8622" citStr="Nanba and Okumura, 1999" startWordPosition="1398" endWordPosition="1401">ear around the sentence that contains the target reference and are related to it. They showed experimentally that citation-based survey generation produces better results when using both explicit and non-explicit citing sentences rather than using the explicit ones alone. Although this work shares the same general goal with ours (i.e identifying the pieces of text that are relevant to a given target reference), our work is different in two ways. First, previous work mostly ignored the fact that the citing sentence itself might be citing multiple references. Second, it defined the citing area (Nanba and Okumura, 1999) or the citation context (Qazvinian and Radev, 2010) as a set of whole contiguous sentences. In our work, we address the case where one citing sentence cites multiple papers, and define what we call the reference scope to be the fragments (not necessarily contiguous) of the citing sentence that are related to the target reference. In a recent work on citation-based summarization by Abu-Jbara and Radev (2011), the authors noticed the issue of having multiple references in one sentence. They raised this issue when they discussed the factors that impede the coherence and the readability of citati</context>
<context position="10842" citStr="Nanba and Okumura, 1999" startWordPosition="1773" endWordPosition="1776"> scope of citation influence improves sentiment classification accuracy. He adapted the same basic method proposed by AbuJbara and Radev (2011). We use this method as a baseline in our evaluation below. In addition to this related work, there is a large body of research that used citing sentences in different applications. For example, citing sentences have been used to summarize the contributions of a scientific paper (Qazvinian and Radev, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011). They have been also used to generate surveys of scientific paradigms (Nanba and Okumura, 1999; Mohammad et al., 2009). Several other papers analyzed citing sentences to recognize the citation function; i.e., the author’s reason for citing a given paper (Nanba et al., 2000; Teufel et al., 2006; Teufel, 2007). Schwartz et al. (2007) proposed a method for aligning the words within citing sentences that cite the same paper. The goal of his work was to aid named entity recognition and paraphrase identification in scientific papers. 82 We believe that all the these applications will benefit from the output of our work. 3 Problem Definition The problem that we are trying to solve is to ident</context>
</contexts>
<marker>Nanba, Okumura, 1999</marker>
<rawString>Hidetsugu Nanba and Manabu Okumura. 1999. Towards multi-paper summarization using reference information. In IJCAI ’99: Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pages 926–931, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetsugu Nanba</author>
</authors>
<title>Noriko Kando, Manabu Okumura, and Of Information Science.</title>
<date>2000</date>
<marker>Nanba, 2000</marker>
<rawString>Hidetsugu Nanba, Noriko Kando, Manabu Okumura, and Of Information Science. 2000. Classification of research papers using citation links and citation types: Towards automatic review article generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
</authors>
<title>Scientific paper summarization using citation summary networks.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>689--696</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2036" citStr="Qazvinian and Radev, 2008" startWordPosition="309" endWordPosition="312">usually accompanied by a span of text that highlights the important contributions of the cited article. We call a sentence that contains an explicit reference to previous work a citation sentence. For example, sentence (1) below is a citing sentence that cites a paper by Philip Resnik and describes the problem Resnik addressed in his paper. (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, se</context>
<context position="10668" citStr="Qazvinian and Radev, 2008" startWordPosition="1746" endWordPosition="1749">th citing sentences that cite multiple references, but this time in the context of sentiment analysis in citations. He showed experimentally that identifying what he termed the scope of citation influence improves sentiment classification accuracy. He adapted the same basic method proposed by AbuJbara and Radev (2011). We use this method as a baseline in our evaluation below. In addition to this related work, there is a large body of research that used citing sentences in different applications. For example, citing sentences have been used to summarize the contributions of a scientific paper (Qazvinian and Radev, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011). They have been also used to generate surveys of scientific paradigms (Nanba and Okumura, 1999; Mohammad et al., 2009). Several other papers analyzed citing sentences to recognize the citation function; i.e., the author’s reason for citing a given paper (Nanba et al., 2000; Teufel et al., 2006; Teufel, 2007). Schwartz et al. (2007) proposed a method for aligning the words within citing sentences that cite the same paper. The goal of his work was to aid named entity recognition and paraphrase identification in scien</context>
</contexts>
<marker>Qazvinian, Radev, 2008</marker>
<rawString>Vahed Qazvinian and Dragomir R. Radev. 2008. Scientific paper summarization using citation summary networks. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 689–696, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
</authors>
<title>Identifying non-explicit citing sentences for citation-based summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>555--564</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2107" citStr="Qazvinian and Radev, 2010" startWordPosition="321" endWordPosition="324">tributions of the cited article. We call a sentence that contains an explicit reference to previous work a citation sentence. For example, sentence (1) below is a citing sentence that cites a paper by Philip Resnik and describes the problem Resnik addressed in his paper. (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, sentence (2) below contains three references. (2) Grefenstette and Nioche</context>
<context position="7846" citStr="Qazvinian and Radev (2010)" startWordPosition="1275" endWordPosition="1278"> citing area They define the citing area as the succession of sentences that appear around the location of a given reference in a scientific paper and have connection to it. Their algorithm starts by adding the sentence that contains the target reference as the first member sentence in the citing area. Then, they use a set of cue words and hand-crafted rules to determine whether the surrounding sentences should be added to the citing area or not. In (Nanba et al., 2000) they use their citing area identification algorithm to improve citation type classification and automatic survey generation. Qazvinian and Radev (2010) addressed a similar problem. They proposed a method based on probabilistic inference to extract non-explicit citing sentences; i.e., sentences that appear around the sentence that contains the target reference and are related to it. They showed experimentally that citation-based survey generation produces better results when using both explicit and non-explicit citing sentences rather than using the explicit ones alone. Although this work shares the same general goal with ours (i.e identifying the pieces of text that are relevant to a given target reference), our work is different in two ways</context>
<context position="10719" citStr="Qazvinian and Radev, 2010" startWordPosition="1754" endWordPosition="1757">but this time in the context of sentiment analysis in citations. He showed experimentally that identifying what he termed the scope of citation influence improves sentiment classification accuracy. He adapted the same basic method proposed by AbuJbara and Radev (2011). We use this method as a baseline in our evaluation below. In addition to this related work, there is a large body of research that used citing sentences in different applications. For example, citing sentences have been used to summarize the contributions of a scientific paper (Qazvinian and Radev, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011). They have been also used to generate surveys of scientific paradigms (Nanba and Okumura, 1999; Mohammad et al., 2009). Several other papers analyzed citing sentences to recognize the citation function; i.e., the author’s reason for citing a given paper (Nanba et al., 2000; Teufel et al., 2006; Teufel, 2007). Schwartz et al. (2007) proposed a method for aligning the words within citing sentences that cite the same paper. The goal of his work was to aid named entity recognition and paraphrase identification in scientific papers. 82 We believe that all the these appl</context>
</contexts>
<marker>Qazvinian, Radev, 2010</marker>
<rawString>Vahed Qazvinian and Dragomir R. Radev. 2010. Identifying non-explicit citing sentences for citation-based summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 555–564, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
<author>Arzucan Ozgur</author>
</authors>
<title>Citation summarization through keyphrase extraction.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>895--903</pages>
<location>Beijing, China,</location>
<contexts>
<context position="2080" citStr="Qazvinian et al., 2010" startWordPosition="317" endWordPosition="320">lights the important contributions of the cited article. We call a sentence that contains an explicit reference to previous work a citation sentence. For example, sentence (1) below is a citing sentence that cites a paper by Philip Resnik and describes the problem Resnik addressed in his paper. (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, sentence (2) below contains three references. </context>
<context position="10692" citStr="Qazvinian et al., 2010" startWordPosition="1750" endWordPosition="1753">te multiple references, but this time in the context of sentiment analysis in citations. He showed experimentally that identifying what he termed the scope of citation influence improves sentiment classification accuracy. He adapted the same basic method proposed by AbuJbara and Radev (2011). We use this method as a baseline in our evaluation below. In addition to this related work, there is a large body of research that used citing sentences in different applications. For example, citing sentences have been used to summarize the contributions of a scientific paper (Qazvinian and Radev, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011). They have been also used to generate surveys of scientific paradigms (Nanba and Okumura, 1999; Mohammad et al., 2009). Several other papers analyzed citing sentences to recognize the citation function; i.e., the author’s reason for citing a given paper (Nanba et al., 2000; Teufel et al., 2006; Teufel, 2007). Schwartz et al. (2007) proposed a method for aligning the words within citing sentences that cite the same paper. The goal of his work was to aid named entity recognition and paraphrase identification in scientific papers. 82 We beli</context>
</contexts>
<marker>Qazvinian, Radev, Ozgur, 2010</marker>
<rawString>Vahed Qazvinian, Dragomir R. Radev, and Arzucan Ozgur. 2010. Citation summarization through keyphrase extraction. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 895–903, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Pradeep Muthukrishnan</author>
<author>Vahed Qazvinian</author>
</authors>
<title>The acl anthology network corpus.</title>
<date>2009</date>
<booktitle>In NLPIR4DL ’09: Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries,</booktitle>
<pages>54--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="23517" citStr="Radev et al., 2009" startWordPosition="3906" endWordPosition="3909">abels of all the words contained in a segment to assign a label to the whole segment. We experimented with three different label aggregation rules: 1) rule-1: assign to the segment the majority label of the words it contains, and 2) rule-2: assign to the segment the label 1 (i.e., inside) if at least one of the words contained in the segment is labeled 1, and assign the label 0 to the segment otherwise, and 3) rule-3: assign the label 0 to the segment if at least of the words it contains is labeled 0, and assign 1 otherwise. 5 Evaluation 5.1 Data We use the ACL Anthology Network corpus (AAN) (Radev et al., 2009) in our evaluation. AAN is a publicly available collection of more than 19,000 NLP papers. AAN provides a manually curated citation network of its papers and the citing sentence(s) associated with each edge. The current release of AAN contains about 76,000 unique citing sentences 56% of which contain 2 or more references and 44% contain 1 reference only. From this set, we randomly selected 3500 citing sentences, each containing at least two references (3.75 references on average with a standard deviation of 2.5). The total number of references in this set of sentences is 19,591. We split the d</context>
</contexts>
<marker>Radev, Muthukrishnan, Qazvinian, 2009</marker>
<rawString>Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed Qazvinian. 2009. The acl anthology network corpus. In NLPIR4DL ’09: Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, pages 54–61, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel Schwartz</author>
<author>Anna Divoli</author>
<author>Marti Hearst</author>
</authors>
<title>Multiple alignment of citation sentences with conditional random fields and posterior decoding.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>847--857</pages>
<contexts>
<context position="2399" citStr="Schwartz et al., 2007" startWordPosition="366" endWordPosition="369">ssed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, sentence (2) below contains three references. (2) Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the web to generate corpora for languages where electronic resources are scarce, while Resnik (1999) describes a method for mining the web for bilingual texts. 80 2012 Conference of the North American Chapter of the Association for Computational Linguis</context>
<context position="11081" citStr="Schwartz et al. (2007)" startWordPosition="1812" endWordPosition="1815">re is a large body of research that used citing sentences in different applications. For example, citing sentences have been used to summarize the contributions of a scientific paper (Qazvinian and Radev, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011). They have been also used to generate surveys of scientific paradigms (Nanba and Okumura, 1999; Mohammad et al., 2009). Several other papers analyzed citing sentences to recognize the citation function; i.e., the author’s reason for citing a given paper (Nanba et al., 2000; Teufel et al., 2006; Teufel, 2007). Schwartz et al. (2007) proposed a method for aligning the words within citing sentences that cite the same paper. The goal of his work was to aid named entity recognition and paraphrase identification in scientific papers. 82 We believe that all the these applications will benefit from the output of our work. 3 Problem Definition The problem that we are trying to solve is to identify which fragments of a given citing sentence that cites multiple references are semantically related to a given target reference. As stated above, we call these fragments the reference scope. Formally, given a citing sentence 5 = {w1, w2</context>
</contexts>
<marker>Schwartz, Divoli, Hearst, 2007</marker>
<rawString>Ariel Schwartz, Anna Divoli, and Marti Hearst. 2007. Multiple alignment of citation sentences with conditional random fields and posterior decoding. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 847–857.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
<author>Simone Teufel</author>
</authors>
<title>Whose idea was this, and why does it matter? attributing scientific work to citations. In</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL/HLT-07.</booktitle>
<contexts>
<context position="2312" citStr="Siddharthan and Teufel, 2007" startWordPosition="353" endWordPosition="356">hilip Resnik and describes the problem Resnik addressed in his paper. (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, sentence (2) below contains three references. (2) Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the web to generate corpora for languages where electronic resources are scarce, while Resnik (1999) describes a method for mining the web for bilingual texts. 80 201</context>
</contexts>
<marker>Siddharthan, Teufel, 2007</marker>
<rawString>Advaith Siddharthan and Simone Teufel. 2007. Whose idea was this, and why does it matter? attributing scientific work to citations. In In Proceedings of NAACL/HLT-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Advaith Siddharthan</author>
<author>Dan Tidhar</author>
</authors>
<title>Automatic classification of citation function. In</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP-06.</booktitle>
<contexts>
<context position="2282" citStr="Teufel et al., 2006" startWordPosition="349" endWordPosition="352">at cites a paper by Philip Resnik and describes the problem Resnik addressed in his paper. (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, sentence (2) below contains three references. (2) Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the web to generate corpora for languages where electronic resources are scarce, while Resnik (1999) describes a method for mining the w</context>
<context position="11042" citStr="Teufel et al., 2006" startWordPosition="1806" endWordPosition="1809">n addition to this related work, there is a large body of research that used citing sentences in different applications. For example, citing sentences have been used to summarize the contributions of a scientific paper (Qazvinian and Radev, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011). They have been also used to generate surveys of scientific paradigms (Nanba and Okumura, 1999; Mohammad et al., 2009). Several other papers analyzed citing sentences to recognize the citation function; i.e., the author’s reason for citing a given paper (Nanba et al., 2000; Teufel et al., 2006; Teufel, 2007). Schwartz et al. (2007) proposed a method for aligning the words within citing sentences that cite the same paper. The goal of his work was to aid named entity recognition and paraphrase identification in scientific papers. 82 We believe that all the these applications will benefit from the output of our work. 3 Problem Definition The problem that we are trying to solve is to identify which fragments of a given citing sentence that cites multiple references are semantically related to a given target reference. As stated above, we call these fragments the reference scope. Formal</context>
</contexts>
<marker>Teufel, Siddharthan, Tidhar, 2006</marker>
<rawString>Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006. Automatic classification of citation function. In In Proc. of EMNLP-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
</authors>
<title>Argumentative zoning for improved citation indexing. computing attitude and affect in text.</title>
<date>2007</date>
<booktitle>In Theory and Applications,</booktitle>
<pages>159170</pages>
<contexts>
<context position="2312" citStr="Teufel, 2007" startWordPosition="355" endWordPosition="356"> describes the problem Resnik addressed in his paper. (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, sentence (2) below contains three references. (2) Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the web to generate corpora for languages where electronic resources are scarce, while Resnik (1999) describes a method for mining the web for bilingual texts. 80 201</context>
<context position="11057" citStr="Teufel, 2007" startWordPosition="1810" endWordPosition="1811">lated work, there is a large body of research that used citing sentences in different applications. For example, citing sentences have been used to summarize the contributions of a scientific paper (Qazvinian and Radev, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011). They have been also used to generate surveys of scientific paradigms (Nanba and Okumura, 1999; Mohammad et al., 2009). Several other papers analyzed citing sentences to recognize the citation function; i.e., the author’s reason for citing a given paper (Nanba et al., 2000; Teufel et al., 2006; Teufel, 2007). Schwartz et al. (2007) proposed a method for aligning the words within citing sentences that cite the same paper. The goal of his work was to aid named entity recognition and paraphrase identification in scientific papers. 82 We believe that all the these applications will benefit from the output of our work. 3 Problem Definition The problem that we are trying to solve is to identify which fragments of a given citing sentence that cites multiple references are semantically related to a given target reference. As stated above, we call these fragments the reference scope. Formally, given a cit</context>
</contexts>
<marker>Teufel, 2007</marker>
<rawString>Simone Teufel. 2007. Argumentative zoning for improved citation indexing. computing attitude and affect in text. In Theory and Applications, pages 159170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard D White</author>
</authors>
<title>Citation analysis and discourse analysis revisited.</title>
<date>2004</date>
<journal>Applied Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="6909" citStr="White (2004)" startWordPosition="1110" endWordPosition="1111">c references. The rest of this paper is organized as follows. Section 2 examines the related work. We define the problem in Section3. Section 4 presents our approaches. Experiments, results and analysis are presented in Section 5. We conclude and provide directions to future work in Section 6 2 Related Work Our work is related to a large body of research on citations (Hodges, 1972; Garfield et al., 1984). The interest in studying citations stems from the fact that bibliometric measures are commonly used to estimate the impact of a researcher’s work (Borgman and Furner, 2002; Luukkonen, 1992). White (2004) provides a good recent survey of the different research lines that use citations. In this section we review the research lines that are relevant to our work 81 and show how our work is different. One line of research that is related to our work has to do with identifying what Nanba and Okumura (1999) call the citing area They define the citing area as the succession of sentences that appear around the location of a given reference in a scientific paper and have connection to it. Their algorithm starts by adding the sentence that contains the target reference as the first member sentence in th</context>
</contexts>
<marker>White, 2004</marker>
<rawString>Howard D. White. 2004. Citation analysis and discourse analysis revisited. Applied Linguistics, 25(1):89–116.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>