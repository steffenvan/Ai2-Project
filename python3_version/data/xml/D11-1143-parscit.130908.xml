<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000551">
<title confidence="0.996955">
Active Learning with Amazon Mechanical Turk
</title>
<author confidence="0.995535">
Florian Laws Christian Scheible Hinrich Sch¨utze
</author>
<affiliation confidence="0.828185">
Institute for Natural Language Processing
Universit¨at Stuttgart
</affiliation>
<email confidence="0.996351">
{lawsfn, scheibcn}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.995592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999610916666667">
Supervised classification needs large amounts
of annotated training data that is expensive to
create. Two approaches that reduce the cost
of annotation are active learning and crowd-
sourcing. However, these two approaches
have not been combined successfully to date.
We evaluate the utility of active learning in
crowdsourcing on two tasks, named entity
recognition and sentiment detection, and show
that active learning outperforms random selec-
tion of annotation examples in a noisy crowd-
sourcing scenario.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895346153846">
Supervised classification is the predominant tech-
nique for a large number of natural language pro-
cessing (NLP) tasks. The large amount of labeled
training data that supervised classification relies on
is time-consuming and expensive to create, espe-
cially when experts perform the data annotation.
Recently, crowdsourcing services like Amazon Me-
chanical Turk (MTurk) have become available as an
alternative that offers acquisition of non-expert an-
notations at low cost. MTurk is a software service
that outsources small annotation tasks – called HITs
– to a large group of freelance workers. The cost of
MTurk annotation is low, but a consequence of us-
ing non-expert annotators is much lower annotation
quality. This requires strategies for quality control
of the annotations.
Another promising approach to the data acqui-
sition bottleneck for supervised learning is active
learning (AL). AL reduces annotation effort by set-
ting up an annotation loop where, starting from a
small seed set, only the maximally informative ex-
amples are chosen for annotation. With these an-
notated examples, the classifier is then retrained to
again select more informative examples for further
annotation. In general, AL needs a lot fewer anno-
tations to achieve a desired performance level than
random sampling.
AL has been successfully applied to a number of
NLP tasks such as part-of-speech tagging (Ringger
et al., 2007), parsing (Osborne and Baldridge, 2004),
text classification (Tong and Koller, 2002), senti-
ment detection (Brew et al., 2010), and named entity
recognition (NER) (Tomanek et al., 2007). Until
recently, most AL studies focused on simulating the
annotation process by using already available gold
standard data. In reality, however, human annota-
tors make mistakes, leading to noise in the annota-
tions. For this reason, some authors have questioned
the applicability of AL to noisy annotation scenarios
such as MTurk (Baldridge and Palmer, 2009; Re-
hbein et al., 2010).
AL and crowdsourcing are complementary ap-
proaches: AL reduces the number of annotations
used while crowdsourcing reduces the cost per an-
notation. Combined, the two approaches could sub-
stantially lower the cost of creating training sets.
Our main contribution in this paper is that we
show for the first time that AL is significantly bet-
ter than randomly selected annotation examples in
a real crowdsourcing annotation scenario. Our
experiments directly address two tasks, named en-
tity recognition and sentiment detection, but our
</bodyText>
<page confidence="0.946274">
1546
</page>
<note confidence="0.9602455">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1546–1556,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.997464043010753">
evidence suggests that AL is of general benefit in on sentiment active learning through crowdsourcing.
crowdsourcing. We also show that the effectiveness However, they use a small set of volunteer labelers
of MTurk annotation with AL can be further en- instead of anonymous paid workers.
hanced by using two techniques that increase label Donmez and Carbonell (2008) propose a method
quality: adaptive voting and fragment recovery. to choose annotators from a set of noisy annotators.
2 Related Work However, in a crowdsourcing scenario, it is not pos-
2.1 Crowdsourcing sible to ask specific annotators for a label, as crowd-
Pioneered by Snow et al. (2008), Crowdsourcing, sourcing workers join and leave the site. Further-
especially using MTurk, has become a widely used more, they only evaluate their approach in simula-
service in the NLP community. A number of stud- tions. We use the actual labels of human annotators
ies have looked at crowdsourcing for NER. Voyer et to avoid the risk of unrealistic assumptions when
al. (2010) use a combination of expert and crowd- modeling annotators.
sourced annotations. Finin et al. (2010) annotate We are not aware of any study that shows that AL
Twitter messages – short sequences of words – and is significantly better than a simple baseline of hav-
this is reflected in their vertically oriented user in- ing annotators annotate randomly selected examples
terface. Lawson et al. (2010) choose an annotation in a highly noisy annotation setting like crowdsourc-
interface where annotators have to drag the mouse ing. While AL generally is superior to this base-
to select entities. Carpenter and Poesio (2010) ar- line in simulated experiments, it is not clear that
gue that dragging is less convenient for workers than this result carries over to crowdsourcing annotation.
marking tokens. Crowdsourcing differs in a number of ways from
These papers do not address AL in crowdsourc- simulated experiments: the difficulty and annotation
ing. Another important difference is that previous consistency of examples drawn by AL differs from
studies on NER have used data sets for which no that drawn by random sampling; crowdsourcing la-
“linguistic” gold annotation is available. In con- bels are noisy; and because of the noisiness of labels
trast, we reannotate the CoNLL-2003 English NER statistical classifiers behave differently in simulated
dataset. This allows us to conduct a detailed com- and real annotation experiments.
parison of MTurk AL to conventional expert anno- 3 Annotation System
tation. One fundamental design criterion for our annotation
2.2 Active Learning with Noisy Labels system was the ability to select examples in real time
Hachey et al. (2005) were among the first to in- to support, e.g., the interactive annotation experi-
vestigate the effect of actively sampled instances ments presented in this paper. Thus, we could not
on agreement of labels and annotation time. They use the standard MTurk workflow or services like
demonstrate applicability of AL when annotators are CrowdFlower.1
trained experts. This is an important result. How- We therefore designed our own system for anno-
ever, AL depends on accurate assessments of uncer- tation experiments. It consists of a two-tiered ap-
tainty and informativeness and such an accurate as- plication architecture. The frontend tier is a web
sessment is made more difficult if labels are noisy application that serves two purposes. First, the ad-
as is the case in crowdsourcing. For this reason, the ministrator can manage annotation experiments us-
problem of AL performance with noisy labels has ing a web interface and publish annotation tasks as-
become a topic of interest in the AL community. Re- sociated with an experiment on MTurk. The front-
hbein et al. (2010) investigate AL with human expert end also provides tools for efficient review of the
annotators for word sense disambiguation, but do received answers. Second, the frontend web appli-
not find convincing evidence that AL reduces anno- cation presents annotation tasks to MTurk workers.
tation cost in a realistic (non-simulated) annotation Because we wanted to implement interactive anno-
scenario. Brew et al. (2010) carried out experiments tation experiments, we used the “external question”
1547
1http://crowdflower.com/
feature of MTurk. An external question contains Mn =  |Pˆ (c1|xn) − Pˆ(c2|xn)|
an URL to our frontend web application, which is Here, xn is the instance to be classified, c1 and c2
queried when a worker views an annotation task. are the two most likely classes, and Pˆ the classifier’s
Our frontend then in turn queries our backend com- estimate of probability.
ponent for an example to be annotated and renders it For NER, the margins of the tokens are averaged
in HTML. to get an uncertainty assessment of the sentence. For
The backend component is responsible for selec- sentiment, whole documents are classified, thus un-
tion of an example to be annotated in response to a certainties can be used directly.
worker’s request for an annotation task. The back- After annotation, the selected example is removed
end implements a diverse choice of random and ac- from the unlabeled pool and, together with its la-
tive selection strategies as well as the multilabel- bel(s), added to the set of labeled examples. The
ing strategies described in section 3.2. The backend classifier is then retrained on the labeled examples
component runs as a standalone server and is queried and the informativeness of the remaining examples
by the frontend via REST-like HTTP calls. in the pool is re-evaluated.
For the NER task, we present one sentence per Depending on the classifier and the sizes of pool
HIT, segmented into tokens, with a select box under- and labeled set, retraining and reevaluation can take
neath each token containing the classes. The defini- some time. To minimize wait times, traditional AL
tion of the classes is based on the CoNLL-2003 an- implementations select examples in batches of the
notation guidelines (Tjong Kim Sang and De Meul- n most informative examples. However, batch se-
der, 2003). Examples were given for every class. lection might not give the optimum selection (exam-
Annotators are forced to make a selection for upper- ples in a batch are likely to be redundant, see Brinker
case tokens. Lowercase tokens are prelabeled with (2003)) and wait times can still occur between one
“O” (no named entity), but annotators are encour- batch and the next.
aged to change this label if the token is in fact part When performing annotation with MTurk, wait
of an entity phrase. times are unacceptable. Thus, we perform the re-
For sentiment annotation, we found in prelim- training and uncertainty rescoring concurrently with
inary experiments that using simple radio button the annotation user interface. The unlabeled pool is
selection for the choice of the document label stored in a priority queue that is ordered according to
(positive or negative) leads to a very high the examples’ informativeness. The annotation user
amount of spam submissions, taking the overall clas- interface takes the most informative example from
sification accuracy down to around 55%. We then the pool and presents it to the annotator. The la-
designed a template that forced annotators to type beled example is then inserted into a second queue
the label as well as a randomly chosen word from that feeds and updates retraining and rescoring pro-
the text. Individual label accuracy was around 75% cesses. The pool queue then is resorted according to
in this scheme. the new informativeness. In this way, annotation and
3.1 Concurrent example selection example selection can run in parallel. This is similar
AL works by setting up an interactive annotation to Haertel et al. (2010).
loop where at each iteration, the most informative 3.2 Adaptive voting and fragment recovery
example is selected for annotation. We use a pool- MTurk labels often have a high error rate. A com-
based AL setup where the most informative exam- mon strategy for improving label quality is to ac-
ple is selected from a pool of unlabeled examples. quire multiple labels by different workers for each
Informativeness is calculated as uncertainty (Lewis example and then consolidate the annotations into
and Gale, 1994) using the margin metric (Schein a single label of higher quality. To trade off num-
and Ungar, 2007). This metric chooses examples for ber of annotated examples against quality of anno-
which the margin of probabilities from the classifier tations, we adopt adaptive voting. It uses majority
between the two most probable classes is the small-
</bodyText>
<table confidence="0.935476307692308">
est:
1548
Budget #train F, NER 6931 #train Acc Sentiment 1756
5820 #train F, 1130 #train Acc
cost/sent w.-accuracy cost/doc w.-accuracy
RS 1 S 5820 59.6 1.00 51.6 1130 70.4 1 74.8 – –
2 3-v 1624 61.4† 3.58 70.1 – – – – – –
3 5/4-v 1488 63.0† 3.91 71.6 1774 63.5 450 71.2 2.51 89.6 735 79.2
4 5-v+f 1996 63.6† 2.91 71.8 2385 64.9† – –
AL 5 S 5820 67.0 1.00 66.5 – – 1130 74.8 1 76.0 – –
6 3-v 1808 70.0† 3.21 78.8 – – – – – – – –
7 5/4-v 1679 70.4† 3.46 79.6 1966 70.6 455 77.4 2.48 89.0 715 81.8
8 5-v+f 2165 70.5 2.68 79.3 2691 71.2 – – – – – –
</table>
<tableCaption confidence="0.80557525">
Table 1: For NER, active learning consistently beats random sampling on MTurk. NER F1 evaluated on
CoNLL test set A. #train = number of sentences in training set, S = single, 3-v = 3-voting, 5/4-voting = 5-
and 4-voting for NER and sentiment resp., +f = using fragments; sentiment budget 1130 for run 1, sentiment
budget 1756 averaged over 2 runs.
</tableCaption>
<bodyText confidence="0.999814863636364">
voting and is adaptive in the number of repeated an-
notations. For NER, a sentence is first annotated by
two workers. Then majority voting is performed for
each token individually. If there is a majority for ev-
ery token that is greater than an agreement threshold
α, the sentence is accepted with each token labeled
with the majority label. Otherwise additional anno-
tations are requested. A sentence is discarded if the
number of repeated annotations exceeds a discard
threshold d (d-voting).2 We use the same scheme
for sentiment; note that there is just one decision per
HIT in this case, not several as in NER.
For NER, we also use fragment recovery: we sal-
vage tokens with agreeing labels from discarded sen-
tences. We cut the token sequence of a discarded
sentence into several fragments that have agreeing
tokens and discard only those parts that disagree. We
then include these recovered fragments in the train-
ing data just like complete sentences.
Software release. Our active learning framework
used can be downloaded at http://www.ims.
uni-stuttgart.de/˜lawsfn/active/.
</bodyText>
<sectionHeader confidence="0.905203" genericHeader="method">
4 Experiments, Results and Analysis
</sectionHeader>
<subsectionHeader confidence="0.849602">
4.1 Experiments
</subsectionHeader>
<bodyText confidence="0.998521512195122">
In our NER experiments, we have workers reanno-
tate the English corpus of the CoNLL-2003 NER
shared task. We chose this corpus to be able to com-
pare crowdsourced annotations with gold standard
2It can take a while in this scheme for annotators to agree
on a final annotation for a sentence. We make tentative labels
of a sentence available to the classifier immediately and replace
them with the final labels once voting is completed.
annotations. A HIT is one sentence and is offered
for abase payment of $0.01. We filtered out answers
that contained unannotated tokens or were obvious
spam (e.g., all tokens labeled as MISC). For test-
ing NER performance, we used a system based on
conditional random fields with standard named en-
tity features including the token itself, orthographic
features like the occurrence of capitalization or spe-
cial characters and context information about the to-
kens to the left/right of the current token.
The sentiment detection task was modeled after a
well-known document analysis setup for sentiment
classification, introduced by Pang et al. (2002). We
use their corpus of 1000 positive and 1000 negative
movie reviews and the Stanford maximum entropy
classifier (Manning and Klein, 2003) to predict the
sentiment label of each document d from a unigram
representation of d. We randomly split this corpus
into a test set of 500 reviews and an active learn-
ing pool of 1500 reviews. Each HIT consists of one
document, valued at $0.01.
We compare random sampling (RS) and AL in
combination with the proposed voting and fragment
strategies with different parameters. We want to
avoid rerunning experiments on MTurk over and
over again, but on the other hand, we believe that us-
ing synthetic data for simulations is problematic be-
cause it is difficult to generate synthetic data with a
realistic model of annotator errors. Thus, we logged
a play-by-play record of the annotator interactions
and labels. With this recording, we can then rerun
strategies with different parameters.
We chose voting with at most d = 5 repetitions as
</bodyText>
<page confidence="0.981059">
1549
</page>
<bodyText confidence="0.999882823529412">
our main reannotation strategy for both random and
active sampling for NER annotation. We use simple
majority voting (α = .5) for NER.
For sentiment, we set d = 4 and minimum agree-
ment α = .75 because the number of labels is
smaller (2 vs. 5) and so random agreement is more
likely for sentiment.
To get results for 3-voting NER, we take the
recording and discard 5-voting votes not needed in
3-voting. This will result in roughly the same num-
ber of annotated sentences, but at a lower cost. This
simulation of 3-voting is not exactly what would
have happened on MTurk (e.g., the final vote on a
sentence might be different, which then influences
AL example selection), but we will assume that dif-
ferences are rare and simulated and actual results
are similar. The same considerations apply to sin-
gle votes and to the sentiment experiments.
We always compare two strategies for the same
annotation budget. For example, the number of
training sentences in Table 1 differ in the two rel-
evant columns, but all strategies compared use ex-
actly the same annotation budget (5820, 6931, 1130,
and 1756, respectively).
For the single annotation strategy, each interac-
tion record contained only about 40% usable anno-
tations, the rest were repeats. A comparison with
the single annotation strategy over approx. 2000 sen-
tences or 450 documents would not have been mean-
ingful; therefore we chose to run an extra experiment
with the single annotation strategy to match this up
with the budgets of the voting strategies. The re-
sults are presented in two separate columns of Ta-
ble 1 (budgets 6931 and 1756).
</bodyText>
<subsectionHeader confidence="0.576251">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999886600000001">
For sentiment detection, worker accuracy or label
quality – the percentage of correctly annotated doc-
uments – is 74.8. In contrast, for NER, worker accu-
racy – the percentage of non-O tokens annotated cor-
rectly – is only 51.6 (Table 1, line 1). This demon-
strates the challenge of using MTurk for NLP an-
notation tasks. When we use single annotations of
each sentence, NER performance is 59.6 F1 for ran-
dom sampling (line 1). When training with gold la-
bels on the same sentences, the performance is 80.0
(not shown). This means we lose more than 20%
due to poor worker accuracy. Adaptive voting and
fragment recovery manage to recover a small part of
the lost performance (lines 2–4); each of the three
F1 scores is significantly better than the one above
it as indicated by † (Approximate Randomization
Test (Noreen, 1989; Chinchor et al., 1993) as im-
plemented by Pad´o (2006)).
Using AL turns out to be quite successful for NER
performance. For single annotations, NER perfor-
mance is 67.0 (line 5), an improvement of 7.4%
compared to random sampling. Adaptive voting
and fragment recovery again increase worker accu-
racy (lines 6–8) although total improvement of 3.5%
(lines 8 vs. 5) is smaller than 4% for random (lines
4 vs. 1). The learning curves of AL vs. random in
Figure 1 (top left) confirm this good result for AL.
These learning curves are for tokens – not for sen-
tences – to show that the reason for AL’s better per-
formance is not that it selects slightly longer sen-
tences than random. In addition, the relative advan-
tage of AL vs random decreases over time, which is
typical of pool-based AL experiments.
We carried out two runs of the same experiment
for sentiment to validate our first positive result since
the difference between the two conditions is not as
large as in NER (Figure 1, top right). After about
300 documents, active learning consistently outper-
forms random sampling. The first AL run performs
better because of higher label quality in the begin-
ning. The overall advantage of AL over random
is lower than for NER because the set of labels is
smaller in sentiment, making the classification task
easier. Second, there is a large amount of simple lex-
ical clues for detecting sentiment (cf. Wilson et al.
(2005)). It is likely that some of them can be learned
well through random sampling at first; however, ac-
tive learning can gain accuracy over time because it
selects examples with more difficult clues.
In Figure 1 (bottom), we compare single annota-
tion with adaptive voting. The graphs show F1 as
a function of cost. Adaptive voting trades quantity
of sampled sentences for quality of labels and thus
incurs higher net costs per sentence. This results in
a smaller dataset for a given budget, but this dataset
is still more useful for classifier training. For NER
(Figure 1, bottom left), the single annotation strat-
egy has a faster start; so for small budgets, cover-
ing a somewhat larger portion of the sample space
is beneficial. For larger budgets, however, quality of
</bodyText>
<page confidence="0.892781">
1550
</page>
<figure confidence="0.9982014">
active, 5−voting
random, 5−voting
0.4 0.5 0.6 0.7 0.8
F−Score
active 1
active 2
random 1
random 2
0 200 400 600
0.4 0.5 0.6 0.7 0.8 0.9
Accuracy
0 5000 10000 15000 20000
Tokens Documents
F−Score
0.4 0.5 0.6 0.7 0.8
Accuracy
0.4 0.5 0.6 0.7 0.8 0.9
single ann.
4−voting
single ann.
3−voting
5−voting
5−voting Orags
0 1000 2000 3000 4000 5000 6000 0 500 1000 1500 2000
Cost Cost
</figure>
<figureCaption confidence="0.997617">
Figure 1: Top: Active learning vs. Random sampling for NER (left) and sentiment (right). Bottom: Active
learning: adaptive voting vs. single annotation for NER (left) and sentiment (right).
</figureCaption>
<bodyText confidence="0.985509666666667">
the voted labels trumps quantity.
For sentiment (Figure 1, bottom right), results are
similar: voting has no benefit initially, but as find-
ing maximally informative examples to annotate be-
comes harder in later stages of learning, adaptive
voting gains an advantage over single annotations.
The main result of the experiment is that active
learning is better by about 7% F1 than random sam-
pling for NER and by 2.6% accuracy for sentiment
(averaged over two runs at budget 1756). Adaptive
voting further improves AL performance for both
NER and sentiment.
</bodyText>
<subsectionHeader confidence="0.999393">
4.3 Annotation time per token
</subsectionHeader>
<bodyText confidence="0.9998748">
Most AL work assumes constant cost per annotation
unit. This assumption has been questioned because
AL often selects hard examples that take longer to
annotate (Hachey et al., 2005; Settles et al., 2008).
In annotation with MTurk, cost is not a function
of annotation time because workers are paid a fixed
amount per HIT. Nevertheless, annotation time plays
a part in whether workers are willing to work on a
given task for the offered reward. This is particularly
problematic for NER since workers have to examine
each token individually. We therefore investigate
for NER whether the time MTurk workers spend on
annotating sentences differs for random vs. AL.
We first compute median and mean annotation
times and number of tokens per sentence:
</bodyText>
<table confidence="0.519941">
sec/sentence tokens/sentence
strategy median mean all required
random 17.2 33.1 15.0 3.4
AL 17.8 33.0 17.7 4.0
</table>
<bodyText confidence="0.998341">
We see that most sentences are annotated in a very
short time; but the mean is much larger than the me-
dian because there are outliers of up to eight min-
utes. AL tends to select slightly longer sentences as
</bodyText>
<page confidence="0.945665">
1551
</page>
<figure confidence="0.75389">
F−Score
0 500 1000 1500 2000
Sentences
Documents
gold selection, gold labels
MTurk selection, gold labels
MTurk selection, MTurk labels
0.4 0.5 0.6 0.7 0.8 0.9
gold selection, gold labels
MTurk selection, gold labels
MTurk selection, MTurk labels
0 200 400 600
0.50 0.60 0.70 0.80
Accuracy
</figure>
<figureCaption confidence="0.996163">
Figure 3: Performance on gold labels. Left: NER. Right: sentiment (run 1).
</figureCaption>
<figure confidence="0.866601">
Number of uppercase tokens
</figure>
<figureCaption confidence="0.999959">
Figure 2: Annotation time vs. # uppercase tokens
</figureCaption>
<bodyText confidence="0.999273888888889">
well as sentences with slightly more uppercase to-
kens that require annotation.
In a more detailed analysis, we attempt to distin-
guish between (i) the effect of more uppercase (“an-
notation required”) tokens vs. (ii) the effect of ex-
ample difficulty. We fit a linear regression model
to annotation time vs. the number of uppercase to-
kens. For the regression fit, we removed all annota-
tion times &gt; 60 seconds. Such long times indicate
distraction of the worker and are not a reliable mea-
sure of difficulty.
Figure 2 shows the distribution of annotation
times for both cases combined and the fitted models
for each. The model estimated an annotation time of
2.3 secs for each required token for random vs. 2.7
secs for AL. We conclude that the difference in dif-
ficulty between sentences selected by random sam-
pling vs. AL is small, but noticeable.
</bodyText>
<subsectionHeader confidence="0.994626">
4.4 Influence of noise on the selection process
</subsectionHeader>
<bodyText confidence="0.99969556">
While NER performance for AL is much higher than
for random sampling, it is still quite a bit lower than
what is possible on gold labels. In the case of AL,
there are two reasons why this happens: (i) The
noisy labels negatively affect the classifier’s ability
to learn a good model that is used for classifying the
test set. (ii) The noisy labels result in bad interme-
diate models that then select suboptimal examples
to be annotated next. The AL selection process is
“misled” by the noisy examples.
We conduct an experiment to determine the con-
tribution of factors (i) and (ii) to the performance
loss. First, we preserve the sequence of sentences
chosen by our AL experiments on MTurk, with 5-
voting for NER and 4-voting for sentiment but re-
place the noisy worker-provided labels by gold la-
bels. The performance of classifiers trained on this
sequence is the dashed line “MTurk selection, gold
labels” in Figure 3 for NER (left) and sentiment
(right).
Second, we compare with a traditional simulated
AL experiment with gold labels. Here, the selection
too is controlled by gold labels, so the selection has
a noiseless classifier available for scoring and can
perform optimal uncertainty selection. These are the
</bodyText>
<figure confidence="0.972269">
0 2 4 6 8 10 13 16 19 23 29
Annotation time (seconds)
0 100 200 300 400
random
active
1552
Quality (% correct entity tokens)
0.0 0.2 0.4 0.6 0.8 1.0
Quality (% correct document labels)
0.0 0.2 0.4 0.6 0.8 1.0
1 5 10 50 100 500 1 2 5 10 20 50 100 200
Number of sentences Number of documents
</figure>
<figureCaption confidence="0.997746">
Figure 4: Worker accuracy vs. number of HITs. Each point corresponds to one worker (◦ = active, +
=random sampling; black and grey for different runs). Left: NER. Right: Sentiment.
</figureCaption>
<bodyText confidence="0.960669708333333">
dotted lines “gold selection, gold labels” in Figure 3.
We used a batch-mode AL setup for this compari-
son experiment. For a fair comparison, we adjust the
batchsize to be equal to the average staleness of a se-
lected example in concurrent MTurk active learning.
The staleness of an example is defined as the num-
ber of annotations the system has received, but not
yet incorporated in the computation of an example’s
uncertainty score (Haertel et al., 2010).
For our concurrent NER system, the average stal-
eness of an example was about 12 (min: 1, max: 40),
for sentiment it was about 2. The figure for NER is
higher than the number cited by Haertel et al. (2010)
because there are more annotators accessing our sys-
tem at the same time via MTurk but not as high for
sentiment since documents are longer and retraining
the sentiment classifier is faster. The average stale-
ness of an example in a batch-mode system is half
the batch size. Thus, we set the batch size of our
comparison system to 25 for NER and to 4 for sen-
timent.
Returning to the two factors introduced above –
(i) final effect of noise on test set performance vs.
(ii) intermediate effect of noise on example selec-
tion – we see in Figure 3 that (i) has a large effect
on NER whereas (ii) has a noticeable, but small ef-
fect.3 For example, at 1966 sentences, F1 scores are
3Our comparison unit for NER is the sentence. We can-
not compare on cost here since we do not know what the per-
sentence cost of a “gold” expert annotation is.
70.6 (MTurk-MTurk), 81.4 (MTurk-gold) and 84.9
(gold-gold). This means that a performance differ-
ence of 10 points F1 has to be attributed to noisy
labels resulting in a worse final classifier (effect i),
and another 3.5 points are lost due to sub-optimal
example selection (effect ii).
For sentiment, the results are different. There is
no clear difference between the three runs. We at-
tribute this to the fact that the quality of the labels
is higher in sentiment than in NER. Our initial ex-
periments on sentiment were all negative (showing
no improvement of AL compared to random) be-
cause label quality was too low. Only after we intro-
duced the template described in Section 3 and used
4-voting with α = .75 did we get positive results for
AL. This leads to an overall label quality of about
90% (over all runs) which is so high that the differ-
ence to using gold labels is small if present at all.
</bodyText>
<sectionHeader confidence="0.930771" genericHeader="method">
5 Worker Quality
</sectionHeader>
<bodyText confidence="0.999964222222222">
So far we have assumed that all workers provide
annotations of the same quality. However, this is
not the case. Figure 4 shows plots of worker accu-
racy as a function of worker productivity (number
of annotated examples). Some workers submit only
one or two HITs just to try out the task. For NER,
the majority of workers submit between 5 and 10
sentences, with label qualities between 0.5 and 0.8.
The chance level for correctness is around 0.25 (four
</bodyText>
<page confidence="0.975939">
1553
</page>
<bodyText confidence="0.99997352631579">
different named entity categories for uppercase to-
kens). For sentiment, most workers submit 1 to 5
documents, with label qualities between 0.5 and 1.
Chance level lies at around 0.5 (for two equally dis-
tributed labels).
While quality for highly productive workers is
mediocre in our experiments, other researchers have
found extremely bad quality for their most prolific
workers (Callison-Burch, 2009). Some of these
workers might be spammers who try to submit an-
swers with automatic scripts. We encountered some
spammers that our heuristics did not detect (shown
in the bottom-right areas of Figure 4, left), but the
voting mechanism was able to mitigate their nega-
tive influence.
Given the large variation in Figure 4, using worker
quality in crowdsourcing for improved training set
creation seems promising. We now test two such
strategies for NER in an oracle setup.
</bodyText>
<subsectionHeader confidence="0.999829">
5.1 Blocking low-quality workers
</subsectionHeader>
<bodyText confidence="0.999962285714286">
A simple approach is to refuse annotations from
workers that have been determined to provide low
quality answers. We simulated this strategy on NER
data using oracle quality ratings. We chose NER be-
cause of its lower overall label quality. The re-
sults are presented in Figure 5 for random (a) and
AL (b). For random, quality filtering with low cut-
offs helps by removing bad annotations that likely
come from spammers. While the voting strategy
prevented a performance decrease with bad anno-
tations, it needed to expend many extra annotations
for correction. With filtering, these extra annotations
become unnecessary and the system can learn faster.
When low-quality workers are less active, as in the
AL dataset, we find no meaningful performance in-
crease for low cutoffs up to 0.4. For very high cut-
offs (0.7), the beginning of the performance curve
shows that further cost reductions can be achieved.
However, we did not have enough recorded human
annotations available to perform a simulation for the
full budget.
</bodyText>
<subsectionHeader confidence="0.999886">
5.2 Trusting high-quality workers
</subsectionHeader>
<bodyText confidence="0.999987333333333">
The complementary approach is to take annotations
from highly rated workers at face value and imme-
diately accept them as the correct label, bypassing
the voting procedure. Bypassing saves the cost of
repeated annotation of the same sentence. Figure 5
shows learning curves for two bypass thresholds on
worker quality (measured as proportion of correct
non-O tokens) for random (c) and AL (d). Bypass-
ing performs surprisingly well. We find a steeper
rise of the learning curve, meaning less cost for the
same performance. Not only do we find substantial
cost reductions, but also higher overall performance.
We believe this is because high-quality annotations
can sometimes be voted down by other annotations.
If we can identify high-quality workers and directly
use their annotations, this can be avoided.
These experiments are oracle experiments using
gold data that is normally not available. In future
work, we would like to repeat the experiments using
methods for worker quality estimation (Ipeirotis et
al., 2010; Donmez et al., 2009). For AL, the choice
as to which labels are used (as a result of voting, by-
passing or other) also has an influence on the selec-
tion. However, we had to keep the sequence of the
selected sentences fixed in the simulations reported
above. While our method of sample selection for
AL proved to be quite robust even in the presence
of noise, higher quality labels do have an influence
on the sample selection (see section 4.4), so the im-
provement could be even better than indicated here.
</bodyText>
<subsectionHeader confidence="0.6042705">
5.3 Differences in quality between AL and
random
</subsectionHeader>
<bodyText confidence="0.999944769230769">
The essence of AL is to select examples that are dif-
ficult to classify. As observed in our experiments
on annotation time, this difficulty is reflected in the
amount of time a human needs to work on examples
selected through AL. Another effect to expect from
difficulty could be lower annotation accuracy. We
therefore examined the accuracies for each worker
who contributed to both the AL and the random ex-
periment. We found that in the NER task, the 20
workers in this group had a slightly higher (0.07) av-
erage quality for randomly selected examples. This
difference is low and does not suggest a significant
drop in accuracy for examples selected in AL.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999893333333333">
We have investigated the use of AL in a real-life
annotation experiment with human annotators in-
stead of traditional simulations with gold labels for
</bodyText>
<page confidence="0.989016">
1554
</page>
<figure confidence="0.99969584">
(a) (b) (c) (d)
F−Score
0.50 0.55 0.60 0.65 0.70 0.75
baseline 5−voting
min. quality 0.1
min. quality 0.4
min. quality 0.7
F−Score
0.50 0.55 0.60 0.65 0.70 0.75
baseline 5−voting
min. quality 0.1
min. quality 0.4
min. quality 0.7
F−Score
0.50 0.55 0.60 0.65 0.70 0.75
baseline 5−voting
bypass 0.9
bypass 0.7
F−Score
0.50 0.55 0.60 0.65 0.70 0.75
baseline 5−voting
bypass 0.9
bypass 0.7
0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000 0 1000 2000 3000 4000
Cost Cost Cost Cost
</figure>
<figureCaption confidence="0.999928">
Figure 5: Blocking low-quality workers: (a) random, (b) AL. Bypass voting: (c) random, (d) AL.
</figureCaption>
<bodyText confidence="0.999909125">
named entity recognition and sentiment classifica-
tion. The annotation was performed using MTurk in
an AL framework that features concurrent example
selection without wait times. We also evaluated two
strategies, adaptive voting and fragment recovery, to
improve label quality at low additional cost. We find
that even for the relatively high noise levels of anno-
tations gathered with MTurk, AL is successful, im-
proving performance by +6.9 points F1 compared to
random sampling for NER and by +2.6% accuracy
for sentiment. Furthermore, this performance level
is reached at a smaller MTurk cost compared to ran-
dom sampling. Thus AL not only reduces annotation
costs, but also offers an improvement in absolute
performance for these tasks. This is clear evidence
that active learning and crowdsourcing are comple-
mentary methods for lowering annotation cost and
should be used together in training set creation for
natural language processing tasks.
We have also conducted oracle experiments that
show that further performance gains and cost sav-
ings can be achieved by using information about
worker quality. We plan to confirm these results by
using estimates of quality in the future.
</bodyText>
<sectionHeader confidence="0.998388" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.8239305">
Florian Laws is a recipient of the Google Europe
Fellowship in Natural Language Processing, and
this research is supported in part by his fellowship.
Christian Scheible is supported by the Deutsche
Forschungsgemeinschaft project Sonderforschungs-
bereich 732.
</bodyText>
<sectionHeader confidence="0.997069" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987281777777778">
Jason Baldridge and Alexis Palmer. 2009. How well
does active learning actually work? Time-based eval-
uation of cost-reduction strategies for language docu-
mentation. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 296–305.
Anthony Brew, Derek Greene, and P´adraig Cunningham.
2010. Using crowdsourcing and active learning to
track sentiment in online media. In Proceeding of the
2010 conference on ECAI 2010: 19th European Con-
ference on Artificial Intelligence, pages 145–150.
Klaus Brinker. 2003. Incorporating diversity in active
learning with support vector machines. In Proceed-
ings of the Twentieth International Conference on Ma-
chine Learning (ICML 2003), pages 59–66.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using Amazon’s
Mechanical Turk. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 286–295.
Bob Carpenter and Massimo Poesio. 2010. Models
of data annotation. Tutorial at the seventh interna-
tional conference on Language Resources and Eval-
uation (LREC 2010).
Nancy Chinchor, David D. Lewis, and Lynette
Hirschman. 1993. Evaluating message understanding
systems: an analysis of the third message understand-
ing conference (muc-3). Computational Linguistics,
19(3):409–449.
Pinar Donmez and Jaime G. Carbonell. 2008. Proactive
learning: cost-sensitive active learning with multiple
imperfect oracles. In Proceeding ofthe 17th ACMcon-
ference on Information and knowledge management,
pages 619–628.
Pinar Donmez, Jaime G. Carbonell, and Jeff Schnei-
der. 2009. Efficiently learning the accuracy of la-
</reference>
<page confidence="0.945948">
1555
</page>
<reference confidence="0.997221961538462">
beling sources for selective sampling. In Proceedings
of the 15th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 259–
268.
Tim Finin, William Murnane, Anand Karandikar,
Nicholas Keller, Justin Martineau, and Mark Dredze.
2010. Annotating named entities in twitter data with
crowdsourcing. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon’s Mechanical Turk, pages 80–88.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In CoNLL ’05: Proceedings of the
9th Conference on Computational Natural Language
Learning, pages 144–151.
Robbie Haertel, Paul Felt, Eric K. Ringger, and Kevin
Seppi. 2010. Parallel active learning: Eliminating
wait time with minimal staleness. In Proceedings of
the NAACL HLT 2010 Workshop on Active Learning
for Natural Language Processing, pages 33–41.
Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.
2010. Quality management on amazon mechanical
turk. In Proceedings of the ACM SIGKDD Workshop
on Human Computation (HCOMP ’10).
Nolan Lawson, Kevin Eustice, Mike Perkowitz, and
Meliha Yetisgen-Yildiz. 2010. Annotating large email
datasets for named entity recognition with mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon’s Mechanical Turk, pages 71–79.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the 17th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 3–12.
Christopher Manning and Dan Klein. 2003. Optimiza-
tion, maxent models, and conditional estimation with-
out magic. In Proceedings of the 2003 Conference
of the North American Chapter of the Association for
Computational Linguistics on Human Language Tech-
nology: Tutorials - Volume 5, pages 8–8.
Eric W. Noreen. 1989. Computer-intensive methods for
testing hypotheses: an introduction. Wiley.
Miles Osborne and Jason Baldridge. 2004. Ensemble-
based active learning for parse selection. In
Daniel Marcu Susan Dumais and Salim Roukos, edi-
tors, HLT-NAACL 2004: Main Proceedings, pages 89–
96.
Sebastian Pad´o, 2006. User’s guide to sigf: Signifi-
cance testing by approximate randomisation.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79–86.
Ines Rehbein, Josef Ruppenhofer, and Alexis Palmer.
2010. Bringing active learning to life. In Proceed-
ings of the 23rd International Conference on Compu-
tational Linguistics (Coling 2010), pages 949–957.
Eric Ringger, Peter McClanahan, Robbie Haertel, George
Busby, Marc Carmen, James Carroll, Kevin Seppi, and
Deryle Lonsdale. 2007. Active learning for part-of-
speech tagging: Accelerating corpus annotation. In
Proceedings of the Linguistic Annotation Workshop at
ACL-2007, pages 101–108.
Andrew Schein and Lyle Ungar. 2007. Active learn-
ing for logistic regression: An evaluation. Machine
Learning, 68(3):235–265.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Proceed-
ings of the NIPS Workshop on Cost-Sensitive Learn-
ing, pages 1069–1078.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast – but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 254–263.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL (CoNLL 2003), pages 142–147.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An approach to text corpus construction which
cuts annotation costs and maintains reusability of an-
notated data. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 486–495.
Simon Tong and Daphne Koller. 2002. Support vec-
tor machine active learning with applications to text
classification. The Journal of Machine Learning Re-
search, 2:45–66.
Robert Voyer, Valerie Nygaard, Will Fitzgerald, and Han-
nah Copperman. 2010. A hybrid model for anno-
tating named entity training corpora. In Proceedings
of the Fourth Linguistic Annotation Workshop, pages
243–246.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347–354.
</reference>
<page confidence="0.993916">
1556
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.785299">
<title confidence="0.999667">Active Learning with Amazon Mechanical Turk</title>
<author confidence="0.99251">Florian Laws Christian Scheible Hinrich</author>
<affiliation confidence="0.9029965">Institute for Natural Language Universit¨at</affiliation>
<abstract confidence="0.997332307692308">Supervised classification needs large amounts of annotated training data that is expensive to create. Two approaches that reduce the cost annotation are learning crowd- However, these two approaches have not been combined successfully to date. We evaluate the utility of active learning in crowdsourcing on two tasks, named entity recognition and sentiment detection, and show that active learning outperforms random selection of annotation examples in a noisy crowdsourcing scenario.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Alexis Palmer</author>
</authors>
<title>How well does active learning actually work? Time-based evaluation of cost-reduction strategies for language documentation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>296--305</pages>
<contexts>
<context position="2672" citStr="Baldridge and Palmer, 2009" startWordPosition="396" endWordPosition="399">pplied to a number of NLP tasks such as part-of-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002), sentiment detection (Brew et al., 2010), and named entity recognition (NER) (Tomanek et al., 2007). Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data. In reality, however, human annotators make mistakes, leading to noise in the annotations. For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk (Baldridge and Palmer, 2009; Rehbein et al., 2010). AL and crowdsourcing are complementary approaches: AL reduces the number of annotations used while crowdsourcing reduces the cost per annotation. Combined, the two approaches could substantially lower the cost of creating training sets. Our main contribution in this paper is that we show for the first time that AL is significantly better than randomly selected annotation examples in a real crowdsourcing annotation scenario. Our experiments directly address two tasks, named entity recognition and sentiment detection, but our 1546 Proceedings of the 2011 Conference on Em</context>
</contexts>
<marker>Baldridge, Palmer, 2009</marker>
<rawString>Jason Baldridge and Alexis Palmer. 2009. How well does active learning actually work? Time-based evaluation of cost-reduction strategies for language documentation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 296–305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Brew</author>
<author>Derek Greene</author>
<author>P´adraig Cunningham</author>
</authors>
<title>Using crowdsourcing and active learning to track sentiment in online media.</title>
<date>2010</date>
<booktitle>In Proceeding of the 2010 conference on ECAI 2010: 19th European Conference on Artificial Intelligence,</booktitle>
<pages>145--150</pages>
<contexts>
<context position="2256" citStr="Brew et al., 2010" startWordPosition="332" endWordPosition="335">effort by setting up an annotation loop where, starting from a small seed set, only the maximally informative examples are chosen for annotation. With these annotated examples, the classifier is then retrained to again select more informative examples for further annotation. In general, AL needs a lot fewer annotations to achieve a desired performance level than random sampling. AL has been successfully applied to a number of NLP tasks such as part-of-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002), sentiment detection (Brew et al., 2010), and named entity recognition (NER) (Tomanek et al., 2007). Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data. In reality, however, human annotators make mistakes, leading to noise in the annotations. For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk (Baldridge and Palmer, 2009; Rehbein et al., 2010). AL and crowdsourcing are complementary approaches: AL reduces the number of annotations used while crowdsourcing reduces the cost per annotation. Combined, the</context>
<context position="7615" citStr="Brew et al. (2010)" startWordPosition="1175" endWordPosition="1178"> performance with noisy labels has ing a web interface and publish annotation tasks asbecome a topic of interest in the AL community. Re- sociated with an experiment on MTurk. The fronthbein et al. (2010) investigate AL with human expert end also provides tools for efficient review of the annotators for word sense disambiguation, but do received answers. Second, the frontend web applinot find convincing evidence that AL reduces anno- cation presents annotation tasks to MTurk workers. tation cost in a realistic (non-simulated) annotation Because we wanted to implement interactive annoscenario. Brew et al. (2010) carried out experiments tation experiments, we used the “external question” 1547 1http://crowdflower.com/ feature of MTurk. An external question contains Mn = |Pˆ (c1|xn) − Pˆ(c2|xn)| an URL to our frontend web application, which is Here, xn is the instance to be classified, c1 and c2 queried when a worker views an annotation task. are the two most likely classes, and Pˆ the classifier’s Our frontend then in turn queries our backend com- estimate of probability. ponent for an example to be annotated and renders it For NER, the margins of the tokens are averaged in HTML. to get an uncertainty </context>
</contexts>
<marker>Brew, Greene, Cunningham, 2010</marker>
<rawString>Anthony Brew, Derek Greene, and P´adraig Cunningham. 2010. Using crowdsourcing and active learning to track sentiment in online media. In Proceeding of the 2010 conference on ECAI 2010: 19th European Conference on Artificial Intelligence, pages 145–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Brinker</author>
</authors>
<title>Incorporating diversity in active learning with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twentieth International Conference on Machine Learning (ICML</booktitle>
<pages>59--66</pages>
<marker>Brinker, 2003</marker>
<rawString>Klaus Brinker. 2003. Incorporating diversity in active learning with support vector machines. In Proceedings of the Twentieth International Conference on Machine Learning (ICML 2003), pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: evaluating translation quality using Amazon’s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>286--295</pages>
<contexts>
<context position="29252" citStr="Callison-Burch, 2009" startWordPosition="4894" endWordPosition="4895">mit only one or two HITs just to try out the task. For NER, the majority of workers submit between 5 and 10 sentences, with label qualities between 0.5 and 0.8. The chance level for correctness is around 0.25 (four 1553 different named entity categories for uppercase tokens). For sentiment, most workers submit 1 to 5 documents, with label qualities between 0.5 and 1. Chance level lies at around 0.5 (for two equally distributed labels). While quality for highly productive workers is mediocre in our experiments, other researchers have found extremely bad quality for their most prolific workers (Callison-Burch, 2009). Some of these workers might be spammers who try to submit answers with automatic scripts. We encountered some spammers that our heuristics did not detect (shown in the bottom-right areas of Figure 4, left), but the voting mechanism was able to mitigate their negative influence. Given the large variation in Figure 4, using worker quality in crowdsourcing for improved training set creation seems promising. We now test two such strategies for NER in an oracle setup. 5.1 Blocking low-quality workers A simple approach is to refuse annotations from workers that have been determined to provide low </context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: evaluating translation quality using Amazon’s Mechanical Turk. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
<author>Massimo Poesio</author>
</authors>
<title>Models of data annotation.</title>
<date>2010</date>
<booktitle>Tutorial at the seventh international conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="5076" citStr="Carpenter and Poesio (2010)" startWordPosition="776" endWordPosition="779">l. (2010) use a combination of expert and crowd- modeling annotators. sourced annotations. Finin et al. (2010) annotate We are not aware of any study that shows that AL Twitter messages – short sequences of words – and is significantly better than a simple baseline of havthis is reflected in their vertically oriented user in- ing annotators annotate randomly selected examples terface. Lawson et al. (2010) choose an annotation in a highly noisy annotation setting like crowdsourcinterface where annotators have to drag the mouse ing. While AL generally is superior to this baseto select entities. Carpenter and Poesio (2010) ar- line in simulated experiments, it is not clear that gue that dragging is less convenient for workers than this result carries over to crowdsourcing annotation. marking tokens. Crowdsourcing differs in a number of ways from These papers do not address AL in crowdsourc- simulated experiments: the difficulty and annotation ing. Another important difference is that previous consistency of examples drawn by AL differs from studies on NER have used data sets for which no that drawn by random sampling; crowdsourcing la“linguistic” gold annotation is available. In con- bels are noisy; and because</context>
</contexts>
<marker>Carpenter, Poesio, 2010</marker>
<rawString>Bob Carpenter and Massimo Poesio. 2010. Models of data annotation. Tutorial at the seventh international conference on Language Resources and Evaluation (LREC 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
<author>David D Lewis</author>
<author>Lynette Hirschman</author>
</authors>
<title>Evaluating message understanding systems: an analysis of the third message understanding conference (muc-3). Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="18581" citStr="Chinchor et al., 1993" startWordPosition="3037" endWordPosition="3040">1.6 (Table 1, line 1). This demonstrates the challenge of using MTurk for NLP annotation tasks. When we use single annotations of each sentence, NER performance is 59.6 F1 for random sampling (line 1). When training with gold labels on the same sentences, the performance is 80.0 (not shown). This means we lose more than 20% due to poor worker accuracy. Adaptive voting and fragment recovery manage to recover a small part of the lost performance (lines 2–4); each of the three F1 scores is significantly better than the one above it as indicated by † (Approximate Randomization Test (Noreen, 1989; Chinchor et al., 1993) as implemented by Pad´o (2006)). Using AL turns out to be quite successful for NER performance. For single annotations, NER performance is 67.0 (line 5), an improvement of 7.4% compared to random sampling. Adaptive voting and fragment recovery again increase worker accuracy (lines 6–8) although total improvement of 3.5% (lines 8 vs. 5) is smaller than 4% for random (lines 4 vs. 1). The learning curves of AL vs. random in Figure 1 (top left) confirm this good result for AL. These learning curves are for tokens – not for sentences – to show that the reason for AL’s better performance is not tha</context>
</contexts>
<marker>Chinchor, Lewis, Hirschman, 1993</marker>
<rawString>Nancy Chinchor, David D. Lewis, and Lynette Hirschman. 1993. Evaluating message understanding systems: an analysis of the third message understanding conference (muc-3). Computational Linguistics, 19(3):409–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pinar Donmez</author>
<author>Jaime G Carbonell</author>
</authors>
<title>Proactive learning: cost-sensitive active learning with multiple imperfect oracles.</title>
<date>2008</date>
<booktitle>In Proceeding ofthe 17th ACMconference on Information and knowledge management,</booktitle>
<pages>619--628</pages>
<contexts>
<context position="3795" citStr="Donmez and Carbonell (2008)" startWordPosition="566" endWordPosition="569"> named entity recognition and sentiment detection, but our 1546 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1546–1556, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics evidence suggests that AL is of general benefit in on sentiment active learning through crowdsourcing. crowdsourcing. We also show that the effectiveness However, they use a small set of volunteer labelers of MTurk annotation with AL can be further en- instead of anonymous paid workers. hanced by using two techniques that increase label Donmez and Carbonell (2008) propose a method quality: adaptive voting and fragment recovery. to choose annotators from a set of noisy annotators. 2 Related Work However, in a crowdsourcing scenario, it is not pos2.1 Crowdsourcing sible to ask specific annotators for a label, as crowdPioneered by Snow et al. (2008), Crowdsourcing, sourcing workers join and leave the site. Furtherespecially using MTurk, has become a widely used more, they only evaluate their approach in simulaservice in the NLP community. A number of stud- tions. We use the actual labels of human annotators ies have looked at crowdsourcing for NER. Voyer </context>
</contexts>
<marker>Donmez, Carbonell, 2008</marker>
<rawString>Pinar Donmez and Jaime G. Carbonell. 2008. Proactive learning: cost-sensitive active learning with multiple imperfect oracles. In Proceeding ofthe 17th ACMconference on Information and knowledge management, pages 619–628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pinar Donmez</author>
<author>Jaime G Carbonell</author>
<author>Jeff Schneider</author>
</authors>
<title>Efficiently learning the accuracy of labeling sources for selective sampling.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>259--268</pages>
<contexts>
<context position="31847" citStr="Donmez et al., 2009" startWordPosition="5310" endWordPosition="5313">ngly well. We find a steeper rise of the learning curve, meaning less cost for the same performance. Not only do we find substantial cost reductions, but also higher overall performance. We believe this is because high-quality annotations can sometimes be voted down by other annotations. If we can identify high-quality workers and directly use their annotations, this can be avoided. These experiments are oracle experiments using gold data that is normally not available. In future work, we would like to repeat the experiments using methods for worker quality estimation (Ipeirotis et al., 2010; Donmez et al., 2009). For AL, the choice as to which labels are used (as a result of voting, bypassing or other) also has an influence on the selection. However, we had to keep the sequence of the selected sentences fixed in the simulations reported above. While our method of sample selection for AL proved to be quite robust even in the presence of noise, higher quality labels do have an influence on the sample selection (see section 4.4), so the improvement could be even better than indicated here. 5.3 Differences in quality between AL and random The essence of AL is to select examples that are difficult to clas</context>
</contexts>
<marker>Donmez, Carbonell, Schneider, 2009</marker>
<rawString>Pinar Donmez, Jaime G. Carbonell, and Jeff Schneider. 2009. Efficiently learning the accuracy of labeling sources for selective sampling. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 259– 268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Finin</author>
<author>William Murnane</author>
<author>Anand Karandikar</author>
<author>Nicholas Keller</author>
<author>Justin Martineau</author>
<author>Mark Dredze</author>
</authors>
<title>Annotating named entities in twitter data with crowdsourcing.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>80--88</pages>
<contexts>
<context position="4559" citStr="Finin et al. (2010)" startWordPosition="691" endWordPosition="694"> a crowdsourcing scenario, it is not pos2.1 Crowdsourcing sible to ask specific annotators for a label, as crowdPioneered by Snow et al. (2008), Crowdsourcing, sourcing workers join and leave the site. Furtherespecially using MTurk, has become a widely used more, they only evaluate their approach in simulaservice in the NLP community. A number of stud- tions. We use the actual labels of human annotators ies have looked at crowdsourcing for NER. Voyer et to avoid the risk of unrealistic assumptions when al. (2010) use a combination of expert and crowd- modeling annotators. sourced annotations. Finin et al. (2010) annotate We are not aware of any study that shows that AL Twitter messages – short sequences of words – and is significantly better than a simple baseline of havthis is reflected in their vertically oriented user in- ing annotators annotate randomly selected examples terface. Lawson et al. (2010) choose an annotation in a highly noisy annotation setting like crowdsourcinterface where annotators have to drag the mouse ing. While AL generally is superior to this baseto select entities. Carpenter and Poesio (2010) ar- line in simulated experiments, it is not clear that gue that dragging is less </context>
</contexts>
<marker>Finin, Murnane, Karandikar, Keller, Martineau, Dredze, 2010</marker>
<rawString>Tim Finin, William Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating named entities in twitter data with crowdsourcing. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 80–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Beatrice Alex</author>
<author>Markus Becker</author>
</authors>
<title>Investigating the effects of selective sampling on the annotation task.</title>
<date>2005</date>
<booktitle>In CoNLL ’05: Proceedings of the 9th Conference on Computational Natural Language Learning,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="6134" citStr="Hachey et al. (2005)" startWordPosition="940" endWordPosition="943">NER have used data sets for which no that drawn by random sampling; crowdsourcing la“linguistic” gold annotation is available. In con- bels are noisy; and because of the noisiness of labels trast, we reannotate the CoNLL-2003 English NER statistical classifiers behave differently in simulated dataset. This allows us to conduct a detailed com- and real annotation experiments. parison of MTurk AL to conventional expert anno- 3 Annotation System tation. One fundamental design criterion for our annotation 2.2 Active Learning with Noisy Labels system was the ability to select examples in real time Hachey et al. (2005) were among the first to in- to support, e.g., the interactive annotation experivestigate the effect of actively sampled instances ments presented in this paper. Thus, we could not on agreement of labels and annotation time. They use the standard MTurk workflow or services like demonstrate applicability of AL when annotators are CrowdFlower.1 trained experts. This is an important result. How- We therefore designed our own system for annoever, AL depends on accurate assessments of uncer- tation experiments. It consists of a two-tiered aptainty and informativeness and such an accurate as- plicat</context>
<context position="22076" citStr="Hachey et al., 2005" startWordPosition="3635" endWordPosition="3638">ximally informative examples to annotate becomes harder in later stages of learning, adaptive voting gains an advantage over single annotations. The main result of the experiment is that active learning is better by about 7% F1 than random sampling for NER and by 2.6% accuracy for sentiment (averaged over two runs at budget 1756). Adaptive voting further improves AL performance for both NER and sentiment. 4.3 Annotation time per token Most AL work assumes constant cost per annotation unit. This assumption has been questioned because AL often selects hard examples that take longer to annotate (Hachey et al., 2005; Settles et al., 2008). In annotation with MTurk, cost is not a function of annotation time because workers are paid a fixed amount per HIT. Nevertheless, annotation time plays a part in whether workers are willing to work on a given task for the offered reward. This is particularly problematic for NER since workers have to examine each token individually. We therefore investigate for NER whether the time MTurk workers spend on annotating sentences differs for random vs. AL. We first compute median and mean annotation times and number of tokens per sentence: sec/sentence tokens/sentence strat</context>
</contexts>
<marker>Hachey, Alex, Becker, 2005</marker>
<rawString>Ben Hachey, Beatrice Alex, and Markus Becker. 2005. Investigating the effects of selective sampling on the annotation task. In CoNLL ’05: Proceedings of the 9th Conference on Computational Natural Language Learning, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robbie Haertel</author>
<author>Paul Felt</author>
<author>Eric K Ringger</author>
<author>Kevin Seppi</author>
</authors>
<title>Parallel active learning: Eliminating wait time with minimal staleness.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing,</booktitle>
<pages>33--41</pages>
<contexts>
<context position="11197" citStr="Haertel et al. (2010)" startWordPosition="1763" endWordPosition="1766">ccuracy down to around 55%. We then the pool and presents it to the annotator. The ladesigned a template that forced annotators to type beled example is then inserted into a second queue the label as well as a randomly chosen word from that feeds and updates retraining and rescoring prothe text. Individual label accuracy was around 75% cesses. The pool queue then is resorted according to in this scheme. the new informativeness. In this way, annotation and 3.1 Concurrent example selection example selection can run in parallel. This is similar AL works by setting up an interactive annotation to Haertel et al. (2010). loop where at each iteration, the most informative 3.2 Adaptive voting and fragment recovery example is selected for annotation. We use a pool- MTurk labels often have a high error rate. A combased AL setup where the most informative exam- mon strategy for improving label quality is to acple is selected from a pool of unlabeled examples. quire multiple labels by different workers for each Informativeness is calculated as uncertainty (Lewis example and then consolidate the annotations into and Gale, 1994) using the margin metric (Schein a single label of higher quality. To trade off numand Un</context>
<context position="26439" citStr="Haertel et al., 2010" startWordPosition="4389" endWordPosition="4392">er accuracy vs. number of HITs. Each point corresponds to one worker (◦ = active, + =random sampling; black and grey for different runs). Left: NER. Right: Sentiment. dotted lines “gold selection, gold labels” in Figure 3. We used a batch-mode AL setup for this comparison experiment. For a fair comparison, we adjust the batchsize to be equal to the average staleness of a selected example in concurrent MTurk active learning. The staleness of an example is defined as the number of annotations the system has received, but not yet incorporated in the computation of an example’s uncertainty score (Haertel et al., 2010). For our concurrent NER system, the average staleness of an example was about 12 (min: 1, max: 40), for sentiment it was about 2. The figure for NER is higher than the number cited by Haertel et al. (2010) because there are more annotators accessing our system at the same time via MTurk but not as high for sentiment since documents are longer and retraining the sentiment classifier is faster. The average staleness of an example in a batch-mode system is half the batch size. Thus, we set the batch size of our comparison system to 25 for NER and to 4 for sentiment. Returning to the two factors </context>
</contexts>
<marker>Haertel, Felt, Ringger, Seppi, 2010</marker>
<rawString>Robbie Haertel, Paul Felt, Eric K. Ringger, and Kevin Seppi. 2010. Parallel active learning: Eliminating wait time with minimal staleness. In Proceedings of the NAACL HLT 2010 Workshop on Active Learning for Natural Language Processing, pages 33–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Panagiotis G Ipeirotis</author>
<author>Foster Provost</author>
<author>Jing Wang</author>
</authors>
<title>Quality management on amazon mechanical turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACM SIGKDD Workshop on Human Computation (HCOMP ’10).</booktitle>
<contexts>
<context position="31825" citStr="Ipeirotis et al., 2010" startWordPosition="5306" endWordPosition="5309">assing performs surprisingly well. We find a steeper rise of the learning curve, meaning less cost for the same performance. Not only do we find substantial cost reductions, but also higher overall performance. We believe this is because high-quality annotations can sometimes be voted down by other annotations. If we can identify high-quality workers and directly use their annotations, this can be avoided. These experiments are oracle experiments using gold data that is normally not available. In future work, we would like to repeat the experiments using methods for worker quality estimation (Ipeirotis et al., 2010; Donmez et al., 2009). For AL, the choice as to which labels are used (as a result of voting, bypassing or other) also has an influence on the selection. However, we had to keep the sequence of the selected sentences fixed in the simulations reported above. While our method of sample selection for AL proved to be quite robust even in the presence of noise, higher quality labels do have an influence on the sample selection (see section 4.4), so the improvement could be even better than indicated here. 5.3 Differences in quality between AL and random The essence of AL is to select examples that</context>
</contexts>
<marker>Ipeirotis, Provost, Wang, 2010</marker>
<rawString>Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang. 2010. Quality management on amazon mechanical turk. In Proceedings of the ACM SIGKDD Workshop on Human Computation (HCOMP ’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nolan Lawson</author>
<author>Kevin Eustice</author>
<author>Mike Perkowitz</author>
<author>Meliha Yetisgen-Yildiz</author>
</authors>
<title>Annotating large email datasets for named entity recognition with mechanical turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>71--79</pages>
<contexts>
<context position="4857" citStr="Lawson et al. (2010)" startWordPosition="741" endWordPosition="744"> in simulaservice in the NLP community. A number of stud- tions. We use the actual labels of human annotators ies have looked at crowdsourcing for NER. Voyer et to avoid the risk of unrealistic assumptions when al. (2010) use a combination of expert and crowd- modeling annotators. sourced annotations. Finin et al. (2010) annotate We are not aware of any study that shows that AL Twitter messages – short sequences of words – and is significantly better than a simple baseline of havthis is reflected in their vertically oriented user in- ing annotators annotate randomly selected examples terface. Lawson et al. (2010) choose an annotation in a highly noisy annotation setting like crowdsourcinterface where annotators have to drag the mouse ing. While AL generally is superior to this baseto select entities. Carpenter and Poesio (2010) ar- line in simulated experiments, it is not clear that gue that dragging is less convenient for workers than this result carries over to crowdsourcing annotation. marking tokens. Crowdsourcing differs in a number of ways from These papers do not address AL in crowdsourc- simulated experiments: the difficulty and annotation ing. Another important difference is that previous con</context>
</contexts>
<marker>Lawson, Eustice, Perkowitz, Yetisgen-Yildiz, 2010</marker>
<rawString>Nolan Lawson, Kevin Eustice, Mike Perkowitz, and Meliha Yetisgen-Yildiz. 2010. Annotating large email datasets for named entity recognition with mechanical turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 71–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>3--12</pages>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and William A. Gale. 1994. A sequential algorithm for training text classifiers. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Dan Klein</author>
</authors>
<title>Optimization, maxent models, and conditional estimation without magic.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Tutorials -</booktitle>
<volume>5</volume>
<pages>8--8</pages>
<contexts>
<context position="15292" citStr="Manning and Klein, 2003" startWordPosition="2470" endWordPosition="2473"> all tokens labeled as MISC). For testing NER performance, we used a system based on conditional random fields with standard named entity features including the token itself, orthographic features like the occurrence of capitalization or special characters and context information about the tokens to the left/right of the current token. The sentiment detection task was modeled after a well-known document analysis setup for sentiment classification, introduced by Pang et al. (2002). We use their corpus of 1000 positive and 1000 negative movie reviews and the Stanford maximum entropy classifier (Manning and Klein, 2003) to predict the sentiment label of each document d from a unigram representation of d. We randomly split this corpus into a test set of 500 reviews and an active learning pool of 1500 reviews. Each HIT consists of one document, valued at $0.01. We compare random sampling (RS) and AL in combination with the proposed voting and fragment strategies with different parameters. We want to avoid rerunning experiments on MTurk over and over again, but on the other hand, we believe that using synthetic data for simulations is problematic because it is difficult to generate synthetic data with a realist</context>
</contexts>
<marker>Manning, Klein, 2003</marker>
<rawString>Christopher Manning and Dan Klein. 2003. Optimization, maxent models, and conditional estimation without magic. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Tutorials - Volume 5, pages 8–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-intensive methods for testing hypotheses: an introduction.</title>
<date>1989</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="18557" citStr="Noreen, 1989" startWordPosition="3035" endWordPosition="3036">ly – is only 51.6 (Table 1, line 1). This demonstrates the challenge of using MTurk for NLP annotation tasks. When we use single annotations of each sentence, NER performance is 59.6 F1 for random sampling (line 1). When training with gold labels on the same sentences, the performance is 80.0 (not shown). This means we lose more than 20% due to poor worker accuracy. Adaptive voting and fragment recovery manage to recover a small part of the lost performance (lines 2–4); each of the three F1 scores is significantly better than the one above it as indicated by † (Approximate Randomization Test (Noreen, 1989; Chinchor et al., 1993) as implemented by Pad´o (2006)). Using AL turns out to be quite successful for NER performance. For single annotations, NER performance is 67.0 (line 5), an improvement of 7.4% compared to random sampling. Adaptive voting and fragment recovery again increase worker accuracy (lines 6–8) although total improvement of 3.5% (lines 8 vs. 5) is smaller than 4% for random (lines 4 vs. 1). The learning curves of AL vs. random in Figure 1 (top left) confirm this good result for AL. These learning curves are for tokens – not for sentences – to show that the reason for AL’s bette</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-intensive methods for testing hypotheses: an introduction. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
<author>Jason Baldridge</author>
</authors>
<title>Ensemblebased active learning for parse selection.</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,</booktitle>
<pages>89--96</pages>
<contexts>
<context position="2170" citStr="Osborne and Baldridge, 2004" startWordPosition="319" endWordPosition="322">a acquisition bottleneck for supervised learning is active learning (AL). AL reduces annotation effort by setting up an annotation loop where, starting from a small seed set, only the maximally informative examples are chosen for annotation. With these annotated examples, the classifier is then retrained to again select more informative examples for further annotation. In general, AL needs a lot fewer annotations to achieve a desired performance level than random sampling. AL has been successfully applied to a number of NLP tasks such as part-of-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002), sentiment detection (Brew et al., 2010), and named entity recognition (NER) (Tomanek et al., 2007). Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data. In reality, however, human annotators make mistakes, leading to noise in the annotations. For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk (Baldridge and Palmer, 2009; Rehbein et al., 2010). AL and crowdsourcing are complementary approaches: AL reduces the number </context>
</contexts>
<marker>Osborne, Baldridge, 2004</marker>
<rawString>Miles Osborne and Jason Baldridge. 2004. Ensemblebased active learning for parse selection. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 89– 96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
</authors>
<title>User’s guide to sigf: Significance testing by approximate randomisation.</title>
<date>2006</date>
<marker>Pad´o, 2006</marker>
<rawString>Sebastian Pad´o, 2006. User’s guide to sigf: Significance testing by approximate randomisation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>79--86</pages>
<contexts>
<context position="15152" citStr="Pang et al. (2002)" startWordPosition="2448" endWordPosition="2451">tence and is offered for abase payment of $0.01. We filtered out answers that contained unannotated tokens or were obvious spam (e.g., all tokens labeled as MISC). For testing NER performance, we used a system based on conditional random fields with standard named entity features including the token itself, orthographic features like the occurrence of capitalization or special characters and context information about the tokens to the left/right of the current token. The sentiment detection task was modeled after a well-known document analysis setup for sentiment classification, introduced by Pang et al. (2002). We use their corpus of 1000 positive and 1000 negative movie reviews and the Stanford maximum entropy classifier (Manning and Klein, 2003) to predict the sentiment label of each document d from a unigram representation of d. We randomly split this corpus into a test set of 500 reviews and an active learning pool of 1500 reviews. Each HIT consists of one document, valued at $0.01. We compare random sampling (RS) and AL in combination with the proposed voting and fragment strategies with different parameters. We want to avoid rerunning experiments on MTurk over and over again, but on the other</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef Ruppenhofer</author>
<author>Alexis Palmer</author>
</authors>
<title>Bringing active learning to life.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>949--957</pages>
<contexts>
<context position="2695" citStr="Rehbein et al., 2010" startWordPosition="400" endWordPosition="404">sks such as part-of-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002), sentiment detection (Brew et al., 2010), and named entity recognition (NER) (Tomanek et al., 2007). Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data. In reality, however, human annotators make mistakes, leading to noise in the annotations. For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk (Baldridge and Palmer, 2009; Rehbein et al., 2010). AL and crowdsourcing are complementary approaches: AL reduces the number of annotations used while crowdsourcing reduces the cost per annotation. Combined, the two approaches could substantially lower the cost of creating training sets. Our main contribution in this paper is that we show for the first time that AL is significantly better than randomly selected annotation examples in a real crowdsourcing annotation scenario. Our experiments directly address two tasks, named entity recognition and sentiment detection, but our 1546 Proceedings of the 2011 Conference on Empirical Methods in Natu</context>
</contexts>
<marker>Rehbein, Ruppenhofer, Palmer, 2010</marker>
<rawString>Ines Rehbein, Josef Ruppenhofer, and Alexis Palmer. 2010. Bringing active learning to life. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 949–957.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Ringger</author>
<author>Peter McClanahan</author>
<author>Robbie Haertel</author>
<author>George Busby</author>
<author>Marc Carmen</author>
<author>James Carroll</author>
<author>Kevin Seppi</author>
<author>Deryle Lonsdale</author>
</authors>
<title>Active learning for part-ofspeech tagging: Accelerating corpus annotation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop at ACL-2007,</booktitle>
<pages>101--108</pages>
<contexts>
<context position="2131" citStr="Ringger et al., 2007" startWordPosition="314" endWordPosition="317">er promising approach to the data acquisition bottleneck for supervised learning is active learning (AL). AL reduces annotation effort by setting up an annotation loop where, starting from a small seed set, only the maximally informative examples are chosen for annotation. With these annotated examples, the classifier is then retrained to again select more informative examples for further annotation. In general, AL needs a lot fewer annotations to achieve a desired performance level than random sampling. AL has been successfully applied to a number of NLP tasks such as part-of-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002), sentiment detection (Brew et al., 2010), and named entity recognition (NER) (Tomanek et al., 2007). Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data. In reality, however, human annotators make mistakes, leading to noise in the annotations. For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk (Baldridge and Palmer, 2009; Rehbein et al., 2010). AL and crowdsourcing are complemen</context>
</contexts>
<marker>Ringger, McClanahan, Haertel, Busby, Carmen, Carroll, Seppi, Lonsdale, 2007</marker>
<rawString>Eric Ringger, Peter McClanahan, Robbie Haertel, George Busby, Marc Carmen, James Carroll, Kevin Seppi, and Deryle Lonsdale. 2007. Active learning for part-ofspeech tagging: Accelerating corpus annotation. In Proceedings of the Linguistic Annotation Workshop at ACL-2007, pages 101–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Schein</author>
<author>Lyle Ungar</author>
</authors>
<title>Active learning for logistic regression: An evaluation.</title>
<date>2007</date>
<booktitle>Machine Learning,</booktitle>
<volume>68</volume>
<issue>3</issue>
<marker>Schein, Ungar, 2007</marker>
<rawString>Andrew Schein and Lyle Ungar. 2007. Active learning for logistic regression: An evaluation. Machine Learning, 68(3):235–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
<author>Mark Craven</author>
<author>Lewis Friedland</author>
</authors>
<title>Active learning with real annotation costs.</title>
<date>2008</date>
<booktitle>In Proceedings of the NIPS Workshop on Cost-Sensitive Learning,</booktitle>
<pages>1069--1078</pages>
<contexts>
<context position="22099" citStr="Settles et al., 2008" startWordPosition="3639" endWordPosition="3642">xamples to annotate becomes harder in later stages of learning, adaptive voting gains an advantage over single annotations. The main result of the experiment is that active learning is better by about 7% F1 than random sampling for NER and by 2.6% accuracy for sentiment (averaged over two runs at budget 1756). Adaptive voting further improves AL performance for both NER and sentiment. 4.3 Annotation time per token Most AL work assumes constant cost per annotation unit. This assumption has been questioned because AL often selects hard examples that take longer to annotate (Hachey et al., 2005; Settles et al., 2008). In annotation with MTurk, cost is not a function of annotation time because workers are paid a fixed amount per HIT. Nevertheless, annotation time plays a part in whether workers are willing to work on a given task for the offered reward. This is particularly problematic for NER since workers have to examine each token individually. We therefore investigate for NER whether the time MTurk workers spend on annotating sentences differs for random vs. AL. We first compute median and mean annotation times and number of tokens per sentence: sec/sentence tokens/sentence strategy median mean all req</context>
</contexts>
<marker>Settles, Craven, Friedland, 2008</marker>
<rawString>Burr Settles, Mark Craven, and Lewis Friedland. 2008. Active learning with real annotation costs. In Proceedings of the NIPS Workshop on Cost-Sensitive Learning, pages 1069–1078.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>254--263</pages>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Ng. 2008. Cheap and fast – but is it good? evaluating non-expert annotations for natural language tasks. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 254–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Fien De Meulder</author>
</authors>
<title>Introduction to the conll-2003 shared task: languageindependent named entity recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of the seventh conference on Natural language learning at HLT-NAACL (CoNLL</booktitle>
<pages>142--147</pages>
<marker>Sang, De Meulder, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: languageindependent named entity recognition. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL (CoNLL 2003), pages 142–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Joachim Wermter</author>
<author>Udo Hahn</author>
</authors>
<title>An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>486--495</pages>
<contexts>
<context position="2315" citStr="Tomanek et al., 2007" startWordPosition="341" endWordPosition="344">from a small seed set, only the maximally informative examples are chosen for annotation. With these annotated examples, the classifier is then retrained to again select more informative examples for further annotation. In general, AL needs a lot fewer annotations to achieve a desired performance level than random sampling. AL has been successfully applied to a number of NLP tasks such as part-of-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002), sentiment detection (Brew et al., 2010), and named entity recognition (NER) (Tomanek et al., 2007). Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data. In reality, however, human annotators make mistakes, leading to noise in the annotations. For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk (Baldridge and Palmer, 2009; Rehbein et al., 2010). AL and crowdsourcing are complementary approaches: AL reduces the number of annotations used while crowdsourcing reduces the cost per annotation. Combined, the two approaches could substantially lower the cost of creat</context>
</contexts>
<marker>Tomanek, Wermter, Hahn, 2007</marker>
<rawString>Katrin Tomanek, Joachim Wermter, and Udo Hahn. 2007. An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 486–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tong</author>
<author>Daphne Koller</author>
</authors>
<title>Support vector machine active learning with applications to text classification.</title>
<date>2002</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>2--45</pages>
<contexts>
<context position="2215" citStr="Tong and Koller, 2002" startWordPosition="325" endWordPosition="328"> active learning (AL). AL reduces annotation effort by setting up an annotation loop where, starting from a small seed set, only the maximally informative examples are chosen for annotation. With these annotated examples, the classifier is then retrained to again select more informative examples for further annotation. In general, AL needs a lot fewer annotations to achieve a desired performance level than random sampling. AL has been successfully applied to a number of NLP tasks such as part-of-speech tagging (Ringger et al., 2007), parsing (Osborne and Baldridge, 2004), text classification (Tong and Koller, 2002), sentiment detection (Brew et al., 2010), and named entity recognition (NER) (Tomanek et al., 2007). Until recently, most AL studies focused on simulating the annotation process by using already available gold standard data. In reality, however, human annotators make mistakes, leading to noise in the annotations. For this reason, some authors have questioned the applicability of AL to noisy annotation scenarios such as MTurk (Baldridge and Palmer, 2009; Rehbein et al., 2010). AL and crowdsourcing are complementary approaches: AL reduces the number of annotations used while crowdsourcing reduc</context>
</contexts>
<marker>Tong, Koller, 2002</marker>
<rawString>Simon Tong and Daphne Koller. 2002. Support vector machine active learning with applications to text classification. The Journal of Machine Learning Research, 2:45–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Voyer</author>
<author>Valerie Nygaard</author>
<author>Will Fitzgerald</author>
<author>Hannah Copperman</author>
</authors>
<title>A hybrid model for annotating named entity training corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth Linguistic Annotation Workshop,</booktitle>
<pages>243--246</pages>
<marker>Voyer, Nygaard, Fitzgerald, Copperman, 2010</marker>
<rawString>Robert Voyer, Valerie Nygaard, Will Fitzgerald, and Hannah Copperman. 2010. A hybrid model for annotating named entity training corpora. In Proceedings of the Fourth Linguistic Annotation Workshop, pages 243–246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="19974" citStr="Wilson et al. (2005)" startWordPosition="3280" endWordPosition="3283">e carried out two runs of the same experiment for sentiment to validate our first positive result since the difference between the two conditions is not as large as in NER (Figure 1, top right). After about 300 documents, active learning consistently outperforms random sampling. The first AL run performs better because of higher label quality in the beginning. The overall advantage of AL over random is lower than for NER because the set of labels is smaller in sentiment, making the classification task easier. Second, there is a large amount of simple lexical clues for detecting sentiment (cf. Wilson et al. (2005)). It is likely that some of them can be learned well through random sampling at first; however, active learning can gain accuracy over time because it selects examples with more difficult clues. In Figure 1 (bottom), we compare single annotation with adaptive voting. The graphs show F1 as a function of cost. Adaptive voting trades quantity of sampled sentences for quality of labels and thus incurs higher net costs per sentence. This results in a smaller dataset for a given budget, but this dataset is still more useful for classifier training. For NER (Figure 1, bottom left), the single annota</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>