<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<title confidence="0.760719">
Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation
</title>
<author confidence="0.817498">
Roy W. Tromble1 and Shankar Kumar2 and Franz Och2 and Wolfgang Macherey2
</author>
<affiliation confidence="0.8674375">
1Department of Computer Science 2 Google Inc.
Johns Hopkins University 1600 Amphitheatre Pkwy.
</affiliation>
<address confidence="0.751706">
Baltimore, MD 21218, USA Mountain View, CA 94043, USA
</address>
<email confidence="0.998542">
royt@jhu.edu {shankarkumar,och,wmach}@google.com
</email>
<sectionHeader confidence="0.996655" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9988073">
We present Minimum Bayes-Risk (MBR) de-
coding over translation lattices that compactly
encode a huge number of translation hypothe-
ses. We describe conditions on the loss func-
tion that will enable efficient implementation
of MBR decoders on lattices. We introduce
an approximation to the BLEU score (Pap-
ineni et al., 2001) that satisfies these condi-
tions. The MBR decoding under this approx-
imate BLEU is realized using Weighted Fi-
nite State Automata. Our experiments show
that the Lattice MBR decoder yields mod-
erate, consistent gains in translation perfor-
mance over N-best MBR decoding on Arabic-
to-English, Chinese-to-English and English-
to-Chinese translation tasks. We conduct a
range of experiments to understand why Lat-
tice MBR improves upon N-best MBR and
study the impact of various parameters on
MBR performance.
</bodyText>
<sectionHeader confidence="0.998889" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999893958333334">
Statistical language processing systems for speech
recognition, machine translation or parsing typically
employ the Maximum A Posteriori (MAP) deci-
sion rule which optimizes the 0-1 loss function. In
contrast, these systems are evaluated using metrics
based on string-edit distance (Word Error Rate), n-
gram overlap (BLEU score (Papineni et al., 2001)),
or precision/recall relative to human annotations.
Minimum Bayes-Risk (MBR) decoding (Bickel and
Doksum, 1977) aims to address this mismatch by se-
lecting the hypothesis that minimizes the expected
error in classification. Thus it directly incorporates
the loss function into the decision criterion. The ap-
proach has been shown to give improvements over
the MAP classifier in many areas of natural lan-
guage processing including automatic speech recog-
nition (Goel and Byrne, 2000), machine transla-
tion (Kumar and Byrne, 2004; Zhang and Gildea,
2008), bilingual word alignment (Kumar and Byrne,
2002), and parsing (Goodman, 1996; Titov and Hen-
derson, 2006; Smith and Smith, 2007).
In statistical machine translation, MBR decoding
is generally implemented by re-ranking an N-best
list of translations produced by a first-pass decoder;
this list typically contains between 100 and 10, 000
hypotheses. Kumar and Byrne (2004) show that
MBR decoding gives optimal performance when the
loss function is matched to the evaluation criterion;
in particular, MBR under the sentence-level BLEU
loss function (Papineni et al., 2001) gives gains on
BLEU. This is despite the fact that the sentence-level
BLEU loss function is an approximation to the exact
corpus-level BLEU.
A different MBR inspired decoding approach is
pursued in Zhang and Gildea (2008) for machine
translation using Synchronous Context Free Gram-
mars. A forest generated by an initial decoding pass
is rescored using dynamic programming to maxi-
mize the expected count of synchronous constituents
in the tree that corresponds to the translation. Since
each constituent adds a new 4-gram to the existing
translation, this approach approximately maximizes
the expected BLEU.
In this paper we explore a different strategy
to perform MBR decoding over Translation Lat-
tices (Ueffing et al., 2002) that compactly encode a
huge number of translation alternatives relative to an
N-best list. This is a model-independent approach
</bodyText>
<page confidence="0.967135">
620
</page>
<note confidence="0.961985">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 620–629,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999907352941176">
in that the lattices could be produced by any statis-
tical MT system — both phrase-based and syntax-
based systems would work in this framework. We
will introduce conditions on the loss functions that
can be incorporated in Lattice MBR decoding. We
describe an approximation to the BLEU score (Pa-
pineni et al., 2001) that will satisfy these condi-
tions. Our Lattice MBR decoding is realized using
Weighted Finite State Automata.
We expect Lattice MBR decoding to improve
upon N-best MBR primarily because lattices con-
tain many more candidate translations than the N-
best list. This has been demonstrated in speech
recognition (Goel and Byrne, 2000). We conduct
a range of translation experiments to analyze lattice
MBR and compare it with N-best MBR. An impor-
tant aspect of our lattice MBR is the linear approxi-
mation to the BLEU score. We will show that MBR
decoding under this score achieves a performance
that is at least as good as the performance obtained
under sentence-level BLEU score.
The rest of the paper is organized as follows. We
review MBR decoding in Section 2 and give the for-
mulation in terms of a gain function. In Section 3,
we describe the conditions on the gain function for
efficient decoding over a lattice. The implementa-
tion of lattice MBR with Weighted Finite State Au-
tomata is presented in Section 4. In Section 5, we in-
troduce the corpus BLEU approximation that makes
it possible to perform efficient lattice MBR decod-
ing. An example of lattice MBR with a toy lattice
is presented in Section 6. We present lattice MBR
experiments in Section 7. A final discussion is pre-
sented in Section 8.
</bodyText>
<sectionHeader confidence="0.987413" genericHeader="method">
2 Minimum Bayes Risk Decoding
</sectionHeader>
<bodyText confidence="0.999207352941176">
Minimum Bayes-Risk (MBR) decoding aims to find
the candidate hypothesis that has the least expected
loss under the probability model (Bickel and Dok-
sum, 1977). We begin with a review of MBR decod-
ing for Statistical Machine Translation (SMT).
Statistical MT (Brown et al., 1990; Och and Ney,
2004) can be described as a mapping of a word se-
quence F in the source language to a word sequence
E in the target language; this mapping is produced
by the MT decoder S(F). If the reference transla-
tion E is known, the decoder performance can be
measured by the loss function L(E, S(F)). Given
such a loss function L(E, E&apos;) between an automatic
translation E&apos; and the reference E, and an under-
lying probability model P(E|F), the MBR decoder
has the following form (Goel and Byrne, 2000; Ku-
mar and Byrne, 2004):
</bodyText>
<equation confidence="0.9932602">
E� = argmin R(E&apos;)
E&apos;E£
= argmin L(E, E&apos;)P(E|F),
E&apos;E£ EE£
1:
</equation>
<bodyText confidence="0.920283166666667">
where R(E&apos;) denotes the Bayes risk of candidate
translation E&apos; under the loss function L.
If the loss function between any two hypotheses
can be bounded: L(E, E&apos;) G Lmax, the MBR de-
coder can be rewritten in terms of a gain function
G(E, E&apos;) = Lmax − L(E, E&apos;):
</bodyText>
<equation confidence="0.993224666666667">
1:
E� = argmax G(E, E&apos;)P(E|F). (1)
E&apos;E£ EE£
</equation>
<bodyText confidence="0.99971555">
We are interested in performing MBR decoding
under a sentence-level BLEU score (Papineni et al.,
2001) which behaves like a gain function: it varies
between 0 and 1, and a larger value reflects a higher
similarity. We will therefore use Equation 1 as the
MBR decoder.
We note that £ represents the space of transla-
tions. For N-best MBR, this space £ is the N-best
list produced by a baseline decoder. We will investi-
gate the use of a translation lattice for MBR decod-
ing; in this case, £ will represent the set of candi-
dates encoded in the lattice.
In general, MBR decoding can use different
spaces for hypothesis selection and risk computa-
tion: argmax and the sum in Equation 1 (Goel,
2001). As an example, the hypothesis could be se-
lected from the N-best list while the risk is com-
puted based on the entire lattice. Therefore, the
MBR decoder can be more generally written as fol-
lows:
</bodyText>
<equation confidence="0.968507333333333">
1:
E� = argmax G(E, E&apos;)P(E|F), (2)
E&apos;E£h EE£e
</equation>
<bodyText confidence="0.9999768">
where £h refers to the Hypothesis space from where
the translations are chosen, and £e refers to the Evi-
dence space that is used for computing the Bayes-
risk. We will present experiments (Section 7) to
show the relative importance of these two spaces.
</bodyText>
<page confidence="0.999133">
621
</page>
<sectionHeader confidence="0.996679" genericHeader="method">
3 Lattice MBR Decoding
</sectionHeader>
<bodyText confidence="0.9999715">
We now present MBR decoding on translation lat-
tices. A translation word lattice is a compact rep-
resentation for very large N-best lists of transla-
tion hypotheses and their likelihoods. Formally,
it is an acyclic Weighted Finite State Acceptor
(WFSA) (Mohri, 2002) consisting of states and arcs
representing transitions between states. Each arc is
labeled with a word and a weight. Each path in the
lattice, consisting of consecutive transitions begin-
ning at the distinguished initial state and ending at a
final state, expresses a candidate translation. Aggre-
gation of the weights along the path1 produces the
weight of the path’s candidate H(E, F) according
to the model. In our setting, this weight will imply
the posterior probability of the translation E given
the source sentence F:
</bodyText>
<equation confidence="0.94304">
P(E |F) =
exp (αH(E, F))
</equation>
<bodyText confidence="0.999834555555556">
The scaling factor α E [0, oc) flattens the distribu-
tion when α &lt; 1, and sharpens it when α &gt; 1.
Because a lattice may represent a number of can-
didates exponential in the size of its state set, it is of-
ten impractical to compute the MBR decoder (Equa-
tion 1) directly. However, if we can express the gain
function G as a sum of local gain functions gi, then
we now show that Equation 1 can be refactored and
the MBR decoder can be computed efficiently. We
loosely call a gain function local if it can be ap-
plied to all paths in the lattice via WFSA intersec-
tion (Mohri, 2002) without significantly multiplying
the number of states.
In this paper, we are primarily concerned with lo-
cal gain functions that weight n-grams. Let N =
{w1, ... , w|N|} be the set of n-grams and let a local
gain function gw : £ x £ —* R, for w E N, be as
follows:
</bodyText>
<equation confidence="0.90419">
gw(E, E0) = θw#w(E0)δw(E), (4)
</equation>
<bodyText confidence="0.999560833333333">
where θw is a constant, #w(E0) is the number of
times that w occurs in E0, and δw(E) is 1 if w E E
and 0 otherwise. That is, gw is θw times the number
of occurrences of w in E0, or zero if w does not oc-
cur in E. We first assume that the overall gain func-
tion G(E, E0) can then be written as a sum of local
</bodyText>
<subsectionHeader confidence="0.489861">
1using the log semiring’s extend operator
</subsectionHeader>
<bodyText confidence="0.798664666666667">
gain functions and a constant θ0 times the length of
the hypothesis E0. 1:
G(E, E0) = θ0|E0 |+
</bodyText>
<equation confidence="0.99761475">
w∈N
1:
= θ0|E0 |+
w∈N
</equation>
<bodyText confidence="0.99483">
Given a gain function of this form, we can rewrite
the risk (sum in Equation 1) as follows
</bodyText>
<equation confidence="0.997019166666667">
1: G(E, E0)P(E|F)
E∈E
1: =
E∈E
= θ0|E0 |+ 1: θw#w(E0) 1: P(E|F),
w∈N E∈Ew
</equation>
<bodyText confidence="0.999754416666667">
where £w = {E E £|δw(E) &gt; 01 represents the
paths of the lattice containing the n-gram w at least
once. The MBR decoder on lattices (Equation 1) can
therefore be written as
Here p(w|£) = EE∈Ew P(E|F) is the posterior
probability of the n-gram w in the lattice. We have
thus replaced a summation over a possibly exponen-
tial number of items (E E £) with a summation over
the number of n-grams that occur in £, which is at
worst polynomial in the number of edges in the lat-
tice that defines £. We compute the posterior proba-
bility of each n-gram w as:
</bodyText>
<equation confidence="0.999019">
Z(£w)
P(E|F) = Z(£) , (7)
</equation>
<bodyText confidence="0.8605572">
where Z(£) = EE&apos;∈E exp(αH(E0, F)) (denomi-
nator in Equation 3) and
Z(£w) = EE&apos;∈Ew exp(αH(E0, F)). Z(£) and
Z(£w) represent the sums2 of weights of all paths
in the lattices £w and £ respectively.
</bodyText>
<sectionHeader confidence="0.999304" genericHeader="method">
4 WFSA MBR Computations
</sectionHeader>
<bodyText confidence="0.882515375">
We now show how the Lattice MBR Decision Rule
(Equation 6) can be implemented using Weighted
Finite State Automata (Mohri, 1997). There are four
steps involved in decoding starting from weighted
finite-state automata representing the candidate out-
puts of a translation system. We will describe these
2in the log semiring, where log+(x, y) = log(ex + e&apos;Y) is
the collect operator (Mohri, 2002)
</bodyText>
<equation confidence="0.990233">
EE&apos;∈E exp (αH(E0, F)). (3)
gw(E, E0) (5)
θw#w(E0)δw(E)
( 1: )
θ0|E0 |+ θw#w(E0)δw(E) P (E|F )
w∈N
E = argmax { 1: 1
E&apos;∈E θ0|E0 |+ θw#w(E0)p(w|£) . (6)
w∈N
p(w|£) = 1:
E∈Ew
</equation>
<page confidence="0.96777">
622
</page>
<bodyText confidence="0.992988">
steps in the setting where the evidence lattice £e may
be different from the hypothesis lattice £h (Equa-
tion 2).
</bodyText>
<listItem confidence="0.991568666666667">
1. Extract the set of n-grams that occur in the ev-
idence lattice £e. For the usual BLEU score, n
ranges from one to four.
2. Compute the posterior probability p(wj£) of
each of these n-grams.
3. Intersect each n-gram w, with an appropriate
weight (from Equation 6), to an initially un-
weighted copy of the hypothesis lattice £h.
4. Find the best path in the resulting automaton.
</listItem>
<bodyText confidence="0.891582333333333">
Computing the set of n-grams N that occur in a
finite automaton requires a traversal, in topological
order, of all the arcs in the automaton. Because the
lattice is acyclic, this is possible. Each state q in the
automaton has a corresponding set of n-grams Nq
ending there.
</bodyText>
<listItem confidence="0.996770888888889">
1. For each state q, Nq is initialized to {E}, the set
containing the empty n-gram.
2. Each arc in the automaton extends each of its
source state’s n-grams by its word label, and
adds the resulting n-grams to the set of its tar-
get state. (E arcs do not extend n-grams, but
transfer them unchanged.) n-grams longer than
the desired order are discarded.
3. N is the union over all states q of Nq.
</listItem>
<bodyText confidence="0.998378846153846">
Given an n-gram, w, we construct an automaton
matching any path containing the n-gram, and in-
tersect that automaton with the lattice to find the set
of paths containing the n-gram (£w in Equation 7).
Suppose £ represent the weighted lattice, we com-
pute3: £w = £ n (w w E*), where w = (E* w E*)
is the language that contains all strings that do not
contain the n-gram w. The posterior probability
p(wj£) of n-gram w can be computed as a ratio of
the total weights of paths in £w to the total weights
of paths in the original lattice (Equation 7).
For each n-gram w E N, we then construct
an automaton that accepts an input E with weight
</bodyText>
<subsectionHeader confidence="0.473584">
3in the log semiring (Mohri, 2002)
</subsectionHeader>
<bodyText confidence="0.98992905">
equal to the product of the number of times the n-
gram occurs in the input (#w(E)), the n-gram fac-
tor θw from Equation 6, and the posterior proba-
bility p(wj£). The automaton corresponds to the
weighted regular expression (Karttunen et al., 1996):
w(w/(θwp(wj£)) w)*.
We successively intersect each of these automata
with an automaton that begins as an unweighted
copy of the lattice £h. This automaton must also
incorporate the factor θ0 of each word. This can
be accomplished by intersecting the unweighted lat-
tice with the automaton accepting (E/θ0)*. The
resulting MBR automaton computes the total ex-
pected gain of each path. A path in this automa-
ton that corresponds to the word sequence E&apos; has
cost: θ0jE&apos;j+EwcAr θw#w(E)p(wj£) (expression
within the curly brackets in Equation 6).
Finally, we extract the best path from the resulting
automaton4, giving the lattice MBR candidate trans-
lation according to the gain function (Equation 6).
</bodyText>
<sectionHeader confidence="0.989155" genericHeader="method">
5 Linear Corpus BLEU
</sectionHeader>
<bodyText confidence="0.995986647058824">
Our Lattice MBR formulation relies on the decom-
position of the overall gain function as a sum of lo-
cal gain functions (Equation 5). We here describe a
linear approximation to the log(BLEU score) (Pap-
ineni et al., 2001) which allows such a decomposi-
tion. This will enable us to rewrite the log(BLEU)
as a linear function of n-gram matches and the hy-
pothesis length. Our strategy will be to use a first
order Taylor-series approximation to what we call
the corpus log(BLEU) gain: the change in corpus
log(BLEU) contributed by the sentence relative to
not including that sentence in the corpus.
Let r be the reference length of the corpus, c0 the
candidate length, and {cnj1 &lt; n &lt; 41 the number
of n-gram matches. Then, the corpus BLEU score
B(r, c0, cn) can be defined as follows (Papineni et
al., 2001):
</bodyText>
<equation confidence="0.948055">
4
,� min C�
0,1 − r + 1 log cn
c0 4 c0n=1
</equation>
<bodyText confidence="0.960098333333333">
where we have ignored On, the difference between
the number of words in the candidate and the num-
the (max, +) semiri
</bodyText>
<equation confidence="0.605620857142857">
4in
ng (Mohri, 2002)
4
log B = min C0,1 − r / + 1 log cn ,
c0 / 4 c0 − On
n=1
,
</equation>
<page confidence="0.990492">
623
</page>
<bodyText confidence="0.9799644">
ber of n-grams. If L is the average sentence length
in the corpus, Δn Pz� (n − 1)c�L .
The corpus log(BLEU) gain is defined as the
change in log(BLEU) when a new sentence’s (E0)
statistics are added to the corpus statistics:
</bodyText>
<equation confidence="0.961117">
G = log B0 − log B,
</equation>
<bodyText confidence="0.999750333333333">
where the counts in B0 are those of B plus those for
the current sentence. We will assume that the brevity
penalty (first term in the above approximation) does
not change when adding the new sentence. In exper-
iments not reported here, we found that taking into
account the brevity penalty at the sentence level can
cause large fluctuations in lattice MBR performance
on different test sets. We therefore treat only cns as
variables.
The corpus log BLEU gain is approximated by a
first-order vector Taylor series expansion about the
initial values of cn.
</bodyText>
<equation confidence="0.9196955">
(c0n − cn) ∂ log B0
∂c0n
</equation>
<bodyText confidence="0.989504">
where the partial derivatives are given by
</bodyText>
<equation confidence="0.959101625">
, (9)
∂ log B 1
∂cn 4cn
.
log B �
Δ
−Δc0
cn
</equation>
<bodyText confidence="0.9998935">
where each Δcn = c0n − cn counts the statistic in
the sentence of interest, rather than the corpus as a
whole. This score is therefore a linear function in
counts of words Δc0 and n-gram matches Δcn. Our
approach ignores the count clipping present in the
exact BLEU score where a correct n-gram present
once in the reference but several times in the hypoth-
esis will be counted only once as correct. Such an
approach is also followed in Dreyer et al. (2007).
Using the above first-order approximation to gain
in log corpus BLEU, Equation 9 implies that θ0, θw
from Section 3 would have the following values:
</bodyText>
<equation confidence="0.9417016">
θ0 = −1 (11)
c0
1
θw = .
4c|w|
</equation>
<subsectionHeader confidence="0.934436">
5.1 N-gram Factors
</subsectionHeader>
<bodyText confidence="0.996097095238095">
We now describe how the n-gram factors (Equa-
tion 11) are computed. The factors depend on
a set of n-gram matches and counts (cn; n E
10, 1, 2, 3, 4}). These factors could be obtained from
a decoding run on a development set. However, do-
ing so could make the performance of lattice MBR
very sensitive to the actual BLEU scores on a partic-
ular run. We would like to avoid such a dependence
and instead, obtain a set of parameters which can
be estimated from multiple decoding runs without
MBR. To achieve this, we make use of the properties
of n-gram matches. It is known that the average n-
gram precisions decay approximately exponentially
with n (Papineni et al., 2001). We now assume that
the number of matches of each n-gram is a constant
ratio r times the matches of the corresponding n −1
gram.
If the unigram precision is p, we can obtain the
n-gram factors (n E 11, 2, 3, 4}) (Equation 11) as a
function of the parameters p and r, and the number
of unigram tokens T:
</bodyText>
<equation confidence="0.996889">
T
−1 (12)
1
θn =
4Tp x rn−1
</equation>
<bodyText confidence="0.99998">
We set p and r to the average values of unigram pre-
cision and precision ratio across multiple develop-
ment sets. Substituting the above factors in Equa-
tion 6, we find that the MBR decision does not de-
pend on T; therefore any value of T can be used.
</bodyText>
<sectionHeader confidence="0.963851" genericHeader="method">
6 An Example
</sectionHeader>
<bodyText confidence="0.993872909090909">
Figure 1 shows a toy lattice and the final MBR au-
tomaton (Section 4) for BLEU with a maximum n-
gram order of 2. We note that the MBR hypothesis
(bcde) has a higher decoder cost relative to the MAP
hypothesis (abde). However, bcde gets a higher ex-
pected gain (Equation 6) than abde since it shares
more n-grams with the Rank-3 hypothesis (bcda).
This illustrates how a lattice can help select MBR
translations that can differ from the MAP transla-
tion.
Substituting the derivatives in Equation 8 gives
</bodyText>
<equation confidence="0.836349">
G =
</equation>
<sectionHeader confidence="0.996631" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<bodyText confidence="0.998943">
We now present experiments to evaluate MBR de-
coding on lattices under the linear corpus BLEU
</bodyText>
<figure confidence="0.759104105263158">
N
GPz�
n=0
, (8)
=cn
����cn
n
∂ log B −1
∂c0 c0
c0
4
1
4
+
�
n=1
Δcn
, (10)
θ0 =
</figure>
<page confidence="0.840808">
624
</page>
<figureCaption confidence="0.9960952">
Figure 1: An example translation lattice with decoder
costs (top) and its MBR Automaton for BLEU-2 (bot-
tom). The bold path in the top is the MAP hypothesis
and the bold path in the bottom is the MBR hypothe-
sis. The precision parameters in Equation 12 are set to:
</figureCaption>
<equation confidence="0.919915">
T = 10,p = 0.85, r = 0.72.
</equation>
<table confidence="0.9987762">
Dataset # of sentences
aren zhen enzh
dev1 1353 1788 1664
dev2 663 919 919
blind 1360 1357 1859
</table>
<tableCaption confidence="0.999893">
Table 1: Statistics over the development and test sets.
</tableCaption>
<bodyText confidence="0.948711">
gain. We start with a description of the data sets
and the SMT system.
</bodyText>
<subsectionHeader confidence="0.993082">
7.1 Development and Blind Test Sets
</subsectionHeader>
<bodyText confidence="0.998326125">
We present our experiments on the constrained data
track of the NIST 2008 Arabic-to-English (aren),
Chinese-to-English (zhen), and English-to-Chinese
(enzh) machine translation tasks.5 In all language
pairs, the parallel and monolingual data consists of
all the allowed training sets in the constrained track.
For each language pair, we use two development
sets: one for Minimum Error Rate Training (Och,
2003; Macherey et al., 2008), and the other for tun-
ing the scale factor for MBR decoding. Our devel-
opment sets consists of the NIST 2004/2003 evalu-
ation sets for both aren and zhen, and NIST 2006
(NIST portion)/2003 evaluation sets for enzh. We
report results on NIST 2008 which is our blind test
set. Statistics computed over these data sets are re-
ported in Table 1.
</bodyText>
<footnote confidence="0.951066">
5http://www.nist.gov/speech/tests/mt/
</footnote>
<subsectionHeader confidence="0.96197">
7.2 MT System Description
</subsectionHeader>
<bodyText confidence="0.999563785714286">
Our phrase-based statistical MT system is similar to
the alignment template system described in Och and
Ney (2004). The system is trained on parallel cor-
pora allowed in the constrained track. We first per-
form sentence and sub-sentence chunk alignment on
the parallel documents. We then train word align-
ment models (Och and Ney, 2003) using 6 Model-1
iterations and 6 HMM iterations. An additional 2 it-
erations of Model-4 are performed for zhen and enzh
pairs. Word Alignments in both source-to-target
and target-to-source directions are obtained using
the Maximum A-Posteriori (MAP) framework (Ma-
tusov et al., 2004). An inventory of phrase-pairs
up to length 5 is then extracted from the union of
source-target and target-source alignments. Several
feature functions are then computed over the phrase-
pairs. 5-gram word language models are trained on
the allowed monolingual corpora. Minimum Error
Rate Training under BLEU is used for estimating
approximately 20 feature function weights over the
dev1 development set.
Translation is performed using a standard dy-
namic programming beam-search decoder (Och and
Ney, 2004) using two decoding passes. The first de-
coder pass generates either a lattice or an N-best list.
MBR decoding is performed in the second pass. The
MBR scaling parameter (α in Equation 3) is tuned
on the dev2 development set.
</bodyText>
<subsectionHeader confidence="0.996391">
7.3 Translation Results
</subsectionHeader>
<bodyText confidence="0.9999814375">
We next report translation results from lattice MBR
decoding. All results will be presented on the NIST
2008 evaluation sets. We report results using the
NIST implementation of the BLEU score which
computes the brevity penalty using the shortest ref-
erence translation for each segment (NIST, 2002
2008). The BLEU scores are reported at the word-
level for aren and zhen but at the character level for
enzh. We measure statistical significance using 95%
confidence intervals computed with paired bootstrap
resampling (Koehn, 2004). In all tables, systems in a
column show statistically significant differences un-
less marked with an asterisk.
We first compare lattice MBR to N-best MBR de-
coding and MAP decoding (Table 2). In these ex-
periments, we hold the likelihood scaling factor α a
</bodyText>
<figure confidence="0.997875564102564">
1
a/0.5
b/0.6
0
b/0.6
b/0.6
2
c/0.6
4
d/0.3
5
e/0.5
a/0.5
6
c/0.6
d/0.4
3
7
8
a/0.063
1
b/0.013
4 d/0.013
d/−0.008
7
e/0.004
9
0
b/0.043
2
c/0.013
5
b/0.043
3
c/0.013
d/−0.008
6 8
a/0.038
10
</figure>
<page confidence="0.990138">
625
</page>
<table confidence="0.9969594">
BLEU(%)
aren zhen enzh
MAP 43.7 27.9 41.4
N-best MBR 43.9 28.3* 42.0
Lattice MBR 44.9 28.5* 42.6
</table>
<tableCaption confidence="0.974658">
Table 2: Lattice MBR, N-best MBR &amp; MAP decoding.
On zhen, Lattice MBR and N-best MBR do not show
statistically significant differences.
</tableCaption>
<bodyText confidence="0.984029583333333">
constant; it is set to 0.2 for aren and enzh, and 0.1
for zhen. The translation lattices are pruned using
Forward-Backward pruning (Sixtus and Ortmanns,
1999) so that the average numbers of arcs per word
(lattice density) is 30. For N-best MBR, we use
N-best lists of size 1000. To match the loss func-
tion, Lattice MBR is performed at the word level for
aren/zhen and at the character level for enzh. Our
lattice MBR is implemented using the Google Open-
Fst library.6 In our experiments, p, r (Equation 12)
have values of 0.85/0.72, 0.80/0.62, and 0.63/0.48
for aren, zhen, and enzh respectively.
We note that Lattice MBR provides gains of 0.2-
1.0 BLEU points over N-best MBR, which in turn
gives 0.2-0.6 BLEU points over MAP. These gains
are obtained on top of a baseline system that has
competitive performance relative to the results re-
ported in the NIST 2008 Evaluation.7 This demon-
strates the effectiveness of lattice MBR decoding as
a realization of MBR decoding which yields sub-
stantial gains over the N-best implementation.
The gains from lattice MBR over N-best MBR
could be due to a combination of factors. These in-
clude: 1) better approximation of the corpus BLEU
score, 2) larger hypothesis space, and 3) larger evi-
dence space. We now present experiments to tease
apart these factors.
Our first experiment restricts both the hypothesis
and evidence spaces in lattice MBR to the 1000-best
list (Table 3). We compare this to N-best MBR with:
a) sentence-level BLEU, and b) sentence-level log
BLEU.
The results show that when restricted to the 1000-
best list, Lattice MBR performs slightly better than
N-best MBR (with sentence BLEU) on aren/enzh
while N-best MBR is better on zhen. We hypothe-
</bodyText>
<footnote confidence="0.9670335">
6http://www.openfst.org/
7 http://www.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html
</footnote>
<table confidence="0.9843516">
BLEU(%)
aren zhen enzh
Lattice MBR, Lin. Corpus BLEU 44.2 28.1 42.2
N-best MBR, Sent. BLEU 43.9* 28.3* 42.0*
N-best MBR, Sent. Log BLEU 44.0* 28.3* 41.9*
</table>
<tableCaption confidence="0.9852245">
Table 3: Lattice and N-best MBR (with Sentence
BLEU/Sentence log BLEU) on a 1000-best list. In each
column, entries with an asterisk do not show statistically
significant differences.
</tableCaption>
<table confidence="0.999820333333333">
BLEU(%)
Hyp Space Evid Space aren zhen enzh
Lattice Lattice 44.9 28.5 42.6
1000-best Lattice 44.6 28.5 42.6
Lattice 1000-best 44.1* 28.0* 42.1
1000-best 1000-best 44.2* 28.1* 42.2
</table>
<tableCaption confidence="0.980796333333333">
Table 4: Lattice MBR with restrictions on hypothesis and
evidence spaces. In each column, entries with an asterisk
do not show statistically significant differences.
</tableCaption>
<bodyText confidence="0.999959103448276">
size that on aren/enzh, the linear corpus BLEU gain
(Equation 10) is better correlated to the actual cor-
pus BLEU than sentence-level BLEU while the op-
posite is true on zhen. N-best MBR gives similar
results with either sentence BLEU or sentence log
BLEU. This confirms that using a log BLEU score
does not change the outcome of MBR decoding and
further justifies our Taylor-series approximation of
the log BLEU score.
We next attempt to understand factors 2 and 3. To
do that, we carry out lattice MBR when either the
hypothesis or the evidence space in Equation 2 is re-
stricted to 1000-best hypotheses (Table 4). For com-
parison, we also include results from lattice MBR
when both hypothesis and evidence spaces are iden-
tical: either the full lattice or the 1000-best list (from
Tables 2 and 3).
These results show that lattice MBR results are
almost unchanged when the hypothesis space is re-
stricted to a 1000-best list. However, when the ev-
idence space is shrunk to a 1000-best list, there is
a significant degradation in performance; these lat-
ter results are almost identical to the scenario when
both evidence and hypothesis spaces are restricted
to the 1000-best list. This experiment throws light
on what makes lattice MBR effective over N-best
MBR. Relative to the N-best list, the translation lat-
tice provides a better estimate of the expected BLEU
score. On the other hand, there are few hypotheses
</bodyText>
<page confidence="0.997914">
626
</page>
<bodyText confidence="0.99925775">
outside the 1000-best list which are selected by lat-
tice MBR.
Finally, we show how the performance of lattice
MBR changes as a function of the lattice density.
The lattice density is the average number of arcs per
word and can be varied using Forward-Backward
pruning (Sixtus and Ortmanns, 1999). Figure 2 re-
ports the average number of lattice paths and BLEU
scores as a function of lattice density. The results
show that Lattice MBR performance generally im-
proves when the size of the lattice is increased.
However, on zhen, there is a small drop beyond a
density of 10. This could be due to low quality (low
posterior probability) hypotheses that get included at
the larger densities and result in a poorer estimate of
the expected BLEU score. On aren and enzh, there
are some gains beyond a lattice density of 30. These
gains are relatively small and come at the expense
of higher memory usage; we therefore work with a
lattice density of 30 in all our experiments. We note
that Lattice MBR is operating over lattices which are
gigantic in comparison to the number of paths in an
N-best list. At a lattice density of 30, the lattices in
aren contain on an average about 1081 hypotheses!
</bodyText>
<subsectionHeader confidence="0.992688">
7.4 Lattice MBR Scale Factor
</subsectionHeader>
<bodyText confidence="0.999897833333333">
We next examine the role of the scale factor α in
lattice MBR decoding. The MBR scale factor de-
termines the flatness of the posterior distribution
(Equation 3). It is chosen using a grid search on the
dev2 set (Table 1). Figure 3 shows the variation in
BLEU scores on eval08 as this parameter is varied.
The results show that it is important to tune this fac-
tor. The optimal scale factor is identical for all three
language pairs. In experiments not reported in this
paper, we have found that the optimal scaling factor
on a moderately sized development set carries over
to unseen test sets.
</bodyText>
<subsectionHeader confidence="0.997145">
7.5 Maximum n-gram Order
</subsectionHeader>
<bodyText confidence="0.967812625">
Lattice MBR Decoding (Equation 6) involves com-
puting a posterior probability for each n-gram in the
lattice. We would like to speed up the Lattice MBR
computation (Section 4) by restricting the maximum
order of the n-grams in the procedure. The results
(Table 5) show that on aren, there is no degradation
if we limit the maximum order of the n-grams to
3. However, on zhen/enzh, there is improvement by
</bodyText>
<table confidence="0.9982345">
BLEU(%)
Max n-gram order aren zhen enzh
1 38.7 26.8 40.0
2 44.1 27.4 42.2
3 44.9 28.0 42.4
4 44.9 28.5 42.6
</table>
<tableCaption confidence="0.999575">
Table 5: Lattice MBR as a function of max n-gram order.
</tableCaption>
<bodyText confidence="0.944465">
considering 4-grams. We can therefore reduce Lat-
tice MBR computations in aren.
</bodyText>
<sectionHeader confidence="0.999106" genericHeader="conclusions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.999991742857143">
We have presented a procedure for performing Min-
imum Bayes-Risk Decoding on translation lattices.
This is a significant development in that the MBR
decoder operates over a very large number of trans-
lations. In contrast, the current N-best implementa-
tion of MBR can be scaled to, at most, a few thou-
sands of hypotheses. If the number of hypotheses
is greater than, say 20,000, the N-best MBR be-
comes computationally expensive. The lattice MBR
technique is efficient when performed over enor-
mous number of hypotheses (up to 1080) since it
takes advantage of the compact structure of the lat-
tice. Lattice MBR gives consistent improvements in
translation performance over N-best MBR decod-
ing, which is used in many state-of-the-art research
translation systems. Moreover, we see gains on three
different language pairs.
There are two potential reasons why Lattice MBR
decoding could outperform N-best MBR: a larger
hypothesis space from which translations could be
selected or a larger evidence space for computing the
expected loss. Our experiments show that the main
improvement comes from the larger evidence space:
a larger set of translations in the lattice provides a
better estimate of the expected BLEU score. In other
words, the lattice provides a better posterior distri-
bution over translation hypotheses relative to an N-
best list. This is a novel insight into the workings
of MBR decoding. We believe this could be possi-
bly employed when designing discriminative train-
ing approaches for machine translation. More gener-
ally, we have found a component in machine transla-
tion where the posterior distribution over hypotheses
plays a crucial role.
We have shown the effect of the MBR scaling fac-
</bodyText>
<page confidence="0.976782">
627
</page>
<figure confidence="0.986172804878049">
10 20 30 40
Lattice Density
Lattice Density
10 20 30 40
Lattice Density
208
187
161
121
85
33
10 20 30 40
28.7
28.6
28.5
28.4
28.3
28.2
6
22
37
59 65
49
zhen
enzh
34
25 30
17
10
3
BLEU(%) 45
44.8
44.6
44.4
44.2
44
42.6
42.4
42.2
42
41.8
</figure>
<figureCaption confidence="0.928997">
Figure 2: Lattice MBR vs. lattice density: aren/zhen/enzh. Each point also shows the log,(Avg. # of paths).
</figureCaption>
<figure confidence="0.995609478260869">
44.8
44.6
BLEUr%)
44.4
44.2
44
zhen
enzh
28.5
28.4
28.3
28.2
28.1
28
27.9
42.8
42.6
42.4
42.2
42
41.8
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1
Scale Factor Scale Factor Scale Factor
</figure>
<figureCaption confidence="0.999863">
Figure 3: Lattice MBR with various scale factors α: aren/zhen/enzh.
</figureCaption>
<bodyText confidence="0.999934717391304">
tor on the performance of lattice MBR. The scale
factor determines the flatness of the posterior distri-
bution over translation hypotheses. A scale of 0.0
means a uniform distribution while 1.0 implies that
there is no scaling. This is an important parameter
that needs to be tuned on a development set. There
has been prior work in MBR speech recognition and
machine translation (Goel and Byrne, 2000; Ehling
et al., 2007) which has shown the need for tuning
this factor. Our MT system parameters are trained
with Minimum Error Rate Training which assigns a
very high posterior probability to the MAP transla-
tion. As a result, it is necessary to flatten the prob-
ability distribution so that MBR decoding can select
hypotheses other than the MAP hypothesis.
Our Lattice MBR implementation is made pos-
sible due to the linear approximation of the BLEU
score. This linearization technique has been applied
elsewhere when working with BLEU: Smith and
Eisner (2006) approximate the expectation of log
BLEU score. In both cases, a linear metric makes
it easier to compute the expectation. While we have
applied lattice MBR decoding to the approximate
BLEU score, we note that our procedure (Section 3)
is applicable to other gain functions which can be
decomposed as a sum of local gain functions. In par-
ticular, our framework might be useful with transla-
tion metrics such as TER (Snover et al., 2006) or
METEOR (Lavie and Agarwal, 2007).
In contrast to a phrase-based SMT system, a syn-
tax based SMT system (e.g. Zollmann and Venu-
gopal (2006)) can generate a hypergraph that rep-
resents a generalized translation lattice with words
and hidden tree structures. We believe that our lat-
tice MBR framework can be extended to such hy-
pergraphs with loss functions that take into account
both BLEU scores as well as parse tree structures.
Lattice and Forest based search and training pro-
cedures are not yet common in statistical machine
translation. However, they are promising because
the search space of translations is much larger than
the typical N-best list (Mi et al., 2008). We hope
that our approach will provide some insight into the
design of lattice-based search procedures along with
the use of non-linear, global loss functions such as
BLEU.
</bodyText>
<sectionHeader confidence="0.99923" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99990854945055">
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected topics. Holden-
Day Inc., Oakland, CA, USA.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J . Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S.
Roossin. 1990. A Statistical Approach to Machine
Translation. Computational Linguistics, 16(2):79–85.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar-
ing Reordering Constraints for SMT Using Efficient
BLEU Oracle Computation. In SSST, NAACL-HLT
2007, pages 103–110, Rochester, NY, USA, April.
N. Ehling, R. Zens, and H. Ney. 2007. Minimum Bayes
Risk Decoding for BLEU. In ACL 2007, pages 101–
104, Prague, Czech Republic, June.
V. Goel and W. Byrne. 2000. Minimum Bayes-Risk Au-
tomatic Speech Recognition. Computer Speech and
Language, 14(2):115–135.
V. Goel. 2001. Minimum Bayes-Risk Automatic Speech
Recognition. Ph.D. thesis, Johns Hopkins University,
Baltimore, MD, USA.
J. Goodman. 1996. Parsing Algorithms and Metrics. In
ACL, pages 177–183, Santa Cruz, CA, USA.
L. Karttunen, J-p. Chanod, G. Grefenstette, and
A. Schiller. 1996. Regular Expressions for Language
Engineering. Natural Language Engineering, 2:305–
328.
P. Koehn. 2004. Statistical Significance Tests for Ma-
chine Translation Evaluation. In EMNLP, Barcelona,
Spain.
S. Kumar and W. Byrne. 2002. Minimum Bayes-Risk
word alignments of bilingual texts. In EMNLP, pages
140–147, Philadelphia, PA, USA.
S. Kumar and W. Byrne. 2004. Minimum Bayes-Risk
Decoding for Statistical Machine Translation. In HLT-
NAACL, pages 169–176, Boston, MA, USA.
A. Lavie and A. Agarwal. 2007. METEOR: An Auto-
matic Metric for MT Evaluation with High Levels of
Correlation with Human Judgments. In SMT Work-
shop, ACL, pages 228–231, Prague, Czech Republic.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008.
Lattice-based Minimum Error Rate Training for Sta-
tistical Machine Translation. In EMNLP, Honolulu,
Hawaii, USA.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
Word Alignments for Statistical Machine Translation.
In COLING, Geneva, Switzerland.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-Based Trans-
lation. In ACL, Columbus, OH, USA.
M. Mohri. 1997. Finite-state transducers in language and
speech processing. Computational Linguistics, 23(3).
M. Mohri. 2002. Semiring frameworks and algorithms
for shortest-distance problems. Journal of Automata,
Languages and Combinatorics, 7(3):321–350.
NIST. 2002-2008. The NIST Machine Translation Eval-
uations. http://www.nist.gov/speech/tests/mt/.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19 – 51.
F. Och and H. Ney. 2004. The Alignment Template Ap-
proach to Statistical Machine Translation. Computa-
tional Linguistics, 30(4):417 – 449.
F. Och. 2003. Minimum Error Rate Training in Statisti-
cal Machine Translation. In ACL, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Machine
Translation. Technical Report RC22176 (W0109-
022), IBM Research Division.
A. Sixtus and S. Ortmanns. 1999. High Quality
Word Graphs Using Forward-Backward Pruning. In
ICASSP, Phoenix, AZ, USA.
D. Smith and J. Eisner. 2006. Minimum Risk Anneal-
ing for Training Log-Linear Models. In ACL, Sydney,
Australia.
D. Smith and N. Smith. 2007. Probabilistic models of
nonprojective dependency trees. In EMNLP-CoNLL,
Prague, Czech Republic.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In AMTA, Boston,
MA, USA.
I. Titov and J. Henderson. 2006. Loss Minimization in
Parse Reranking. In EMNLP, Sydney, Australia.
N. Ueffing, F. Och, and H. Ney. 2002. Generation of
Word Graphs in Statistical Machine Translation. In
EMNLP, Philadelphia, PA, USA.
H. Zhang and D. Gildea. 2008. Efficient Multi-pass De-
coding for Synchronous Context Free Grammars. In
ACL, Columbus, OH, USA.
A. Zollmann and A. Venugopal. 2006. Syntax Aug-
mented Machine Translation via Chart Parsing. In
HLT-NAACL, New York, NY, USA.
</reference>
<page confidence="0.998884">
629
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.795559">
<title confidence="0.999832">Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation</title>
<author confidence="0.979718">W</author>
<affiliation confidence="0.999261">of Computer Science Inc.</affiliation>
<address confidence="0.966071">Johns Hopkins University 1600 Amphitheatre Pkwy. Baltimore, MD 21218, USA Mountain View, CA 94043, USA</address>
<abstract confidence="0.993233">We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode a huge number of translation hypotheses. We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices. We introduce an approximation to the BLEU score (Papineni et al., 2001) that satisfies these conditions. The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata. Our experiments show that the Lattice MBR decoder yields moderate, consistent gains in translation performance over N-best MBR decoding on Arabicto-English, Chinese-to-English and Englishto-Chinese translation tasks. We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P J Bickel</author>
<author>K A Doksum</author>
</authors>
<title>Mathematical Statistics: Basic Ideas and Selected topics. HoldenDay Inc.,</title>
<date>1977</date>
<location>Oakland, CA, USA.</location>
<contexts>
<context position="1647" citStr="Bickel and Doksum, 1977" startWordPosition="236" endWordPosition="239">nduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance. 1 Introduction Statistical language processing systems for speech recognition, machine translation or parsing typically employ the Maximum A Posteriori (MAP) decision rule which optimizes the 0-1 loss function. In contrast, these systems are evaluated using metrics based on string-edit distance (Word Error Rate), ngram overlap (BLEU score (Papineni et al., 2001)), or precision/recall relative to human annotations. Minimum Bayes-Risk (MBR) decoding (Bickel and Doksum, 1977) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion. The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition (Goel and Byrne, 2000), machine translation (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translati</context>
<context position="5491" citStr="Bickel and Doksum, 1977" startWordPosition="852" endWordPosition="856"> function for efficient decoding over a lattice. The implementation of lattice MBR with Weighted Finite State Automata is presented in Section 4. In Section 5, we introduce the corpus BLEU approximation that makes it possible to perform efficient lattice MBR decoding. An example of lattice MBR with a toy lattice is presented in Section 6. We present lattice MBR experiments in Section 7. A final discussion is presented in Section 8. 2 Minimum Bayes Risk Decoding Minimum Bayes-Risk (MBR) decoding aims to find the candidate hypothesis that has the least expected loss under the probability model (Bickel and Doksum, 1977). We begin with a review of MBR decoding for Statistical Machine Translation (SMT). Statistical MT (Brown et al., 1990; Och and Ney, 2004) can be described as a mapping of a word sequence F in the source language to a word sequence E in the target language; this mapping is produced by the MT decoder S(F). If the reference translation E is known, the decoder performance can be measured by the loss function L(E, S(F)). Given such a loss function L(E, E&apos;) between an automatic translation E&apos; and the reference E, and an underlying probability model P(E|F), the MBR decoder has the following form (Go</context>
</contexts>
<marker>Bickel, Doksum, 1977</marker>
<rawString>P. J. Bickel and K. A. Doksum. 1977. Mathematical Statistics: Basic Ideas and Selected topics. HoldenDay Inc., Oakland, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A Statistical Approach to Machine Translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<marker>Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>P. F. Brown, J. Cocke, S. A. Della Pietra, V. J . Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. 1990. A Statistical Approach to Machine Translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dreyer</author>
<author>K Hall</author>
<author>S Khudanpur</author>
</authors>
<title>Comparing Reordering Constraints for SMT Using Efficient BLEU Oracle Computation.</title>
<date>2007</date>
<booktitle>In SSST, NAACL-HLT 2007,</booktitle>
<pages>103--110</pages>
<location>Rochester, NY, USA,</location>
<contexts>
<context position="16592" citStr="Dreyer et al. (2007)" startWordPosition="2910" endWordPosition="2913">or series expansion about the initial values of cn. (c0n − cn) ∂ log B0 ∂c0n where the partial derivatives are given by , (9) ∂ log B 1 ∂cn 4cn . log B � Δ −Δc0 cn where each Δcn = c0n − cn counts the statistic in the sentence of interest, rather than the corpus as a whole. This score is therefore a linear function in counts of words Δc0 and n-gram matches Δcn. Our approach ignores the count clipping present in the exact BLEU score where a correct n-gram present once in the reference but several times in the hypothesis will be counted only once as correct. Such an approach is also followed in Dreyer et al. (2007). Using the above first-order approximation to gain in log corpus BLEU, Equation 9 implies that θ0, θw from Section 3 would have the following values: θ0 = −1 (11) c0 1 θw = . 4c|w| 5.1 N-gram Factors We now describe how the n-gram factors (Equation 11) are computed. The factors depend on a set of n-gram matches and counts (cn; n E 10, 1, 2, 3, 4}). These factors could be obtained from a decoding run on a development set. However, doing so could make the performance of lattice MBR very sensitive to the actual BLEU scores on a particular run. We would like to avoid such a dependence and instead</context>
</contexts>
<marker>Dreyer, Hall, Khudanpur, 2007</marker>
<rawString>M. Dreyer, K. Hall, and S. Khudanpur. 2007. Comparing Reordering Constraints for SMT Using Efficient BLEU Oracle Computation. In SSST, NAACL-HLT 2007, pages 103–110, Rochester, NY, USA, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ehling</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Minimum Bayes Risk Decoding for BLEU. In ACL</title>
<date>2007</date>
<pages>101--104</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="31849" citStr="Ehling et al., 2007" startWordPosition="5549" endWordPosition="5552">.9 42.8 42.6 42.4 42.2 42 41.8 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Scale Factor Scale Factor Scale Factor Figure 3: Lattice MBR with various scale factors α: aren/zhen/enzh. tor on the performance of lattice MBR. The scale factor determines the flatness of the posterior distribution over translation hypotheses. A scale of 0.0 means a uniform distribution while 1.0 implies that there is no scaling. This is an important parameter that needs to be tuned on a development set. There has been prior work in MBR speech recognition and machine translation (Goel and Byrne, 2000; Ehling et al., 2007) which has shown the need for tuning this factor. Our MT system parameters are trained with Minimum Error Rate Training which assigns a very high posterior probability to the MAP translation. As a result, it is necessary to flatten the probability distribution so that MBR decoding can select hypotheses other than the MAP hypothesis. Our Lattice MBR implementation is made possible due to the linear approximation of the BLEU score. This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. In both cases, </context>
</contexts>
<marker>Ehling, Zens, Ney, 2007</marker>
<rawString>N. Ehling, R. Zens, and H. Ney. 2007. Minimum Bayes Risk Decoding for BLEU. In ACL 2007, pages 101– 104, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Goel</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayes-Risk Automatic Speech Recognition.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="2015" citStr="Goel and Byrne, 2000" startWordPosition="294" endWordPosition="297">these systems are evaluated using metrics based on string-edit distance (Word Error Rate), ngram overlap (BLEU score (Papineni et al., 2001)), or precision/recall relative to human annotations. Minimum Bayes-Risk (MBR) decoding (Bickel and Doksum, 1977) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion. The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition (Goel and Byrne, 2000), machine translation (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translation, MBR decoding is generally implemented by re-ranking an N-best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLE</context>
<context position="4332" citStr="Goel and Byrne, 2000" startWordPosition="650" endWordPosition="653">ould be produced by any statistical MT system — both phrase-based and syntaxbased systems would work in this framework. We will introduce conditions on the loss functions that can be incorporated in Lattice MBR decoding. We describe an approximation to the BLEU score (Papineni et al., 2001) that will satisfy these conditions. Our Lattice MBR decoding is realized using Weighted Finite State Automata. We expect Lattice MBR decoding to improve upon N-best MBR primarily because lattices contain many more candidate translations than the Nbest list. This has been demonstrated in speech recognition (Goel and Byrne, 2000). We conduct a range of translation experiments to analyze lattice MBR and compare it with N-best MBR. An important aspect of our lattice MBR is the linear approximation to the BLEU score. We will show that MBR decoding under this score achieves a performance that is at least as good as the performance obtained under sentence-level BLEU score. The rest of the paper is organized as follows. We review MBR decoding in Section 2 and give the formulation in terms of a gain function. In Section 3, we describe the conditions on the gain function for efficient decoding over a lattice. The implementati</context>
<context position="6109" citStr="Goel and Byrne, 2000" startWordPosition="965" endWordPosition="968">7). We begin with a review of MBR decoding for Statistical Machine Translation (SMT). Statistical MT (Brown et al., 1990; Och and Ney, 2004) can be described as a mapping of a word sequence F in the source language to a word sequence E in the target language; this mapping is produced by the MT decoder S(F). If the reference translation E is known, the decoder performance can be measured by the loss function L(E, S(F)). Given such a loss function L(E, E&apos;) between an automatic translation E&apos; and the reference E, and an underlying probability model P(E|F), the MBR decoder has the following form (Goel and Byrne, 2000; Kumar and Byrne, 2004): E� = argmin R(E&apos;) E&apos;E£ = argmin L(E, E&apos;)P(E|F), E&apos;E£ EE£ 1: where R(E&apos;) denotes the Bayes risk of candidate translation E&apos; under the loss function L. If the loss function between any two hypotheses can be bounded: L(E, E&apos;) G Lmax, the MBR decoder can be rewritten in terms of a gain function G(E, E&apos;) = Lmax − L(E, E&apos;): 1: E� = argmax G(E, E&apos;)P(E|F). (1) E&apos;E£ EE£ We are interested in performing MBR decoding under a sentence-level BLEU score (Papineni et al., 2001) which behaves like a gain function: it varies between 0 and 1, and a larger value reflects a higher similar</context>
<context position="31827" citStr="Goel and Byrne, 2000" startWordPosition="5545" endWordPosition="5548">4 28.3 28.2 28.1 28 27.9 42.8 42.6 42.4 42.2 42 41.8 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Scale Factor Scale Factor Scale Factor Figure 3: Lattice MBR with various scale factors α: aren/zhen/enzh. tor on the performance of lattice MBR. The scale factor determines the flatness of the posterior distribution over translation hypotheses. A scale of 0.0 means a uniform distribution while 1.0 implies that there is no scaling. This is an important parameter that needs to be tuned on a development set. There has been prior work in MBR speech recognition and machine translation (Goel and Byrne, 2000; Ehling et al., 2007) which has shown the need for tuning this factor. Our MT system parameters are trained with Minimum Error Rate Training which assigns a very high posterior probability to the MAP translation. As a result, it is necessary to flatten the probability distribution so that MBR decoding can select hypotheses other than the MAP hypothesis. Our Lattice MBR implementation is made possible due to the linear approximation of the BLEU score. This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU </context>
</contexts>
<marker>Goel, Byrne, 2000</marker>
<rawString>V. Goel and W. Byrne. 2000. Minimum Bayes-Risk Automatic Speech Recognition. Computer Speech and Language, 14(2):115–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Goel</author>
</authors>
<title>Minimum Bayes-Risk Automatic Speech Recognition.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University,</institution>
<location>Baltimore, MD, USA.</location>
<contexts>
<context position="7190" citStr="Goel, 2001" startWordPosition="1167" endWordPosition="1168">Papineni et al., 2001) which behaves like a gain function: it varies between 0 and 1, and a larger value reflects a higher similarity. We will therefore use Equation 1 as the MBR decoder. We note that £ represents the space of translations. For N-best MBR, this space £ is the N-best list produced by a baseline decoder. We will investigate the use of a translation lattice for MBR decoding; in this case, £ will represent the set of candidates encoded in the lattice. In general, MBR decoding can use different spaces for hypothesis selection and risk computation: argmax and the sum in Equation 1 (Goel, 2001). As an example, the hypothesis could be selected from the N-best list while the risk is computed based on the entire lattice. Therefore, the MBR decoder can be more generally written as follows: 1: E� = argmax G(E, E&apos;)P(E|F), (2) E&apos;E£h EE£e where £h refers to the Hypothesis space from where the translations are chosen, and £e refers to the Evidence space that is used for computing the Bayesrisk. We will present experiments (Section 7) to show the relative importance of these two spaces. 621 3 Lattice MBR Decoding We now present MBR decoding on translation lattices. A translation word lattice </context>
</contexts>
<marker>Goel, 2001</marker>
<rawString>V. Goel. 2001. Minimum Bayes-Risk Automatic Speech Recognition. Ph.D. thesis, Johns Hopkins University, Baltimore, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Parsing Algorithms and Metrics. In</title>
<date>1996</date>
<booktitle>ACL,</booktitle>
<pages>177--183</pages>
<location>Santa Cruz, CA, USA.</location>
<contexts>
<context position="2162" citStr="Goodman, 1996" startWordPosition="318" endWordPosition="319">/recall relative to human annotations. Minimum Bayes-Risk (MBR) decoding (Bickel and Doksum, 1977) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion. The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition (Goel and Byrne, 2000), machine translation (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translation, MBR decoding is generally implemented by re-ranking an N-best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) gives gains on BLEU. This is despite the fact that the sentence-level BLEU loss function is an approximatio</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>J. Goodman. 1996. Parsing Algorithms and Metrics. In ACL, pages 177–183, Santa Cruz, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Karttunen</author>
<author>J-p Chanod</author>
<author>G Grefenstette</author>
<author>A Schiller</author>
</authors>
<title>Regular Expressions for Language Engineering. Natural Language Engineering,</title>
<date>1996</date>
<pages>2--305</pages>
<contexts>
<context position="13462" citStr="Karttunen et al., 1996" startWordPosition="2334" endWordPosition="2337">) is the language that contains all strings that do not contain the n-gram w. The posterior probability p(wj£) of n-gram w can be computed as a ratio of the total weights of paths in £w to the total weights of paths in the original lattice (Equation 7). For each n-gram w E N, we then construct an automaton that accepts an input E with weight 3in the log semiring (Mohri, 2002) equal to the product of the number of times the ngram occurs in the input (#w(E)), the n-gram factor θw from Equation 6, and the posterior probability p(wj£). The automaton corresponds to the weighted regular expression (Karttunen et al., 1996): w(w/(θwp(wj£)) w)*. We successively intersect each of these automata with an automaton that begins as an unweighted copy of the lattice £h. This automaton must also incorporate the factor θ0 of each word. This can be accomplished by intersecting the unweighted lattice with the automaton accepting (E/θ0)*. The resulting MBR automaton computes the total expected gain of each path. A path in this automaton that corresponds to the word sequence E&apos; has cost: θ0jE&apos;j+EwcAr θw#w(E)p(wj£) (expression within the curly brackets in Equation 6). Finally, we extract the best path from the resulting automa</context>
</contexts>
<marker>Karttunen, Chanod, Grefenstette, Schiller, 1996</marker>
<rawString>L. Karttunen, J-p. Chanod, G. Grefenstette, and A. Schiller. 1996. Regular Expressions for Language Engineering. Natural Language Engineering, 2:305– 328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation. In EMNLP,</title>
<date>2004</date>
<location>Barcelona,</location>
<contexts>
<context position="22030" citStr="Koehn, 2004" startWordPosition="3865" endWordPosition="3866">eter (α in Equation 3) is tuned on the dev2 development set. 7.3 Translation Results We next report translation results from lattice MBR decoding. All results will be presented on the NIST 2008 evaluation sets. We report results using the NIST implementation of the BLEU score which computes the brevity penalty using the shortest reference translation for each segment (NIST, 2002 2008). The BLEU scores are reported at the wordlevel for aren and zhen but at the character level for enzh. We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling (Koehn, 2004). In all tables, systems in a column show statistically significant differences unless marked with an asterisk. We first compare lattice MBR to N-best MBR decoding and MAP decoding (Table 2). In these experiments, we hold the likelihood scaling factor α a 1 a/0.5 b/0.6 0 b/0.6 b/0.6 2 c/0.6 4 d/0.3 5 e/0.5 a/0.5 6 c/0.6 d/0.4 3 7 8 a/0.063 1 b/0.013 4 d/0.013 d/−0.008 7 e/0.004 9 0 b/0.043 2 c/0.013 5 b/0.043 3 c/0.013 d/−0.008 6 8 a/0.038 10 625 BLEU(%) aren zhen enzh MAP 43.7 27.9 41.4 N-best MBR 43.9 28.3* 42.0 Lattice MBR 44.9 28.5* 42.6 Table 2: Lattice MBR, N-best MBR &amp; MAP decoding. On </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In EMNLP, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayes-Risk word alignments of bilingual texts.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>140--147</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="2134" citStr="Kumar and Byrne, 2002" startWordPosition="312" endWordPosition="315">Papineni et al., 2001)), or precision/recall relative to human annotations. Minimum Bayes-Risk (MBR) decoding (Bickel and Doksum, 1977) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion. The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition (Goel and Byrne, 2000), machine translation (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translation, MBR decoding is generally implemented by re-ranking an N-best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) gives gains on BLEU. This is despite the fact that the sentence-level BLEU loss</context>
</contexts>
<marker>Kumar, Byrne, 2002</marker>
<rawString>S. Kumar and W. Byrne. 2002. Minimum Bayes-Risk word alignments of bilingual texts. In EMNLP, pages 140–147, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayes-Risk Decoding for Statistical Machine Translation. In</title>
<date>2004</date>
<booktitle>HLTNAACL,</booktitle>
<pages>169--176</pages>
<location>Boston, MA, USA.</location>
<contexts>
<context position="2059" citStr="Kumar and Byrne, 2004" startWordPosition="301" endWordPosition="304">sed on string-edit distance (Word Error Rate), ngram overlap (BLEU score (Papineni et al., 2001)), or precision/recall relative to human annotations. Minimum Bayes-Risk (MBR) decoding (Bickel and Doksum, 1977) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion. The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition (Goel and Byrne, 2000), machine translation (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translation, MBR decoding is generally implemented by re-ranking an N-best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) give</context>
<context position="6133" citStr="Kumar and Byrne, 2004" startWordPosition="969" endWordPosition="973">view of MBR decoding for Statistical Machine Translation (SMT). Statistical MT (Brown et al., 1990; Och and Ney, 2004) can be described as a mapping of a word sequence F in the source language to a word sequence E in the target language; this mapping is produced by the MT decoder S(F). If the reference translation E is known, the decoder performance can be measured by the loss function L(E, S(F)). Given such a loss function L(E, E&apos;) between an automatic translation E&apos; and the reference E, and an underlying probability model P(E|F), the MBR decoder has the following form (Goel and Byrne, 2000; Kumar and Byrne, 2004): E� = argmin R(E&apos;) E&apos;E£ = argmin L(E, E&apos;)P(E|F), E&apos;E£ EE£ 1: where R(E&apos;) denotes the Bayes risk of candidate translation E&apos; under the loss function L. If the loss function between any two hypotheses can be bounded: L(E, E&apos;) G Lmax, the MBR decoder can be rewritten in terms of a gain function G(E, E&apos;) = Lmax − L(E, E&apos;): 1: E� = argmax G(E, E&apos;)P(E|F). (1) E&apos;E£ EE£ We are interested in performing MBR decoding under a sentence-level BLEU score (Papineni et al., 2001) which behaves like a gain function: it varies between 0 and 1, and a larger value reflects a higher similarity. We will therefore u</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>S. Kumar and W. Byrne. 2004. Minimum Bayes-Risk Decoding for Statistical Machine Translation. In HLTNAACL, pages 169–176, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>A Agarwal</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments.</title>
<date>2007</date>
<booktitle>In SMT Workshop, ACL,</booktitle>
<pages>228--231</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="32857" citStr="Lavie and Agarwal, 2007" startWordPosition="5718" endWordPosition="5721">ue to the linear approximation of the BLEU score. This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. In both cases, a linear metric makes it easier to compute the expectation. While we have applied lattice MBR decoding to the approximate BLEU score, we note that our procedure (Section 3) is applicable to other gain functions which can be decomposed as a sum of local gain functions. In particular, our framework might be useful with translation metrics such as TER (Snover et al., 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with words and hidden tree structures. We believe that our lattice MBR framework can be extended to such hypergraphs with loss functions that take into account both BLEU scores as well as parse tree structures. Lattice and Forest based search and training procedures are not yet common in statistical machine translation. However, they are promising because the search space of translations is much larger than the ty</context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>A. Lavie and A. Agarwal. 2007. METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments. In SMT Workshop, ACL, pages 228–231, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Macherey</author>
<author>F Och</author>
<author>I Thayer</author>
<author>J Uszkoreit</author>
</authors>
<title>Lattice-based Minimum Error Rate Training for Statistical Machine Translation. In EMNLP,</title>
<date>2008</date>
<location>Honolulu, Hawaii, USA.</location>
<contexts>
<context position="19731" citStr="Macherey et al., 2008" startWordPosition="3497" endWordPosition="3500"> 919 blind 1360 1357 1859 Table 1: Statistics over the development and test sets. gain. We start with a description of the data sets and the SMT system. 7.1 Development and Blind Test Sets We present our experiments on the constrained data track of the NIST 2008 Arabic-to-English (aren), Chinese-to-English (zhen), and English-to-Chinese (enzh) machine translation tasks.5 In all language pairs, the parallel and monolingual data consists of all the allowed training sets in the constrained track. For each language pair, we use two development sets: one for Minimum Error Rate Training (Och, 2003; Macherey et al., 2008), and the other for tuning the scale factor for MBR decoding. Our development sets consists of the NIST 2004/2003 evaluation sets for both aren and zhen, and NIST 2006 (NIST portion)/2003 evaluation sets for enzh. We report results on NIST 2008 which is our blind test set. Statistics computed over these data sets are reported in Table 1. 5http://www.nist.gov/speech/tests/mt/ 7.2 MT System Description Our phrase-based statistical MT system is similar to the alignment template system described in Och and Ney (2004). The system is trained on parallel corpora allowed in the constrained track. We f</context>
</contexts>
<marker>Macherey, Och, Thayer, Uszkoreit, 2008</marker>
<rawString>W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008. Lattice-based Minimum Error Rate Training for Statistical Machine Translation. In EMNLP, Honolulu, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Symmetric Word Alignments for Statistical Machine Translation. In COLING,</title>
<date>2004</date>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="20750" citStr="Matusov et al., 2004" startWordPosition="3661" endWordPosition="3665">ption Our phrase-based statistical MT system is similar to the alignment template system described in Och and Ney (2004). The system is trained on parallel corpora allowed in the constrained track. We first perform sentence and sub-sentence chunk alignment on the parallel documents. We then train word alignment models (Och and Ney, 2003) using 6 Model-1 iterations and 6 HMM iterations. An additional 2 iterations of Model-4 are performed for zhen and enzh pairs. Word Alignments in both source-to-target and target-to-source directions are obtained using the Maximum A-Posteriori (MAP) framework (Matusov et al., 2004). An inventory of phrase-pairs up to length 5 is then extracted from the union of source-target and target-source alignments. Several feature functions are then computed over the phrasepairs. 5-gram word language models are trained on the allowed monolingual corpora. Minimum Error Rate Training under BLEU is used for estimating approximately 20 feature function weights over the dev1 development set. Translation is performed using a standard dynamic programming beam-search decoder (Och and Ney, 2004) using two decoding passes. The first decoder pass generates either a lattice or an N-best list.</context>
</contexts>
<marker>Matusov, Zens, Ney, 2004</marker>
<rawString>E. Matusov, R. Zens, and H. Ney. 2004. Symmetric Word Alignments for Statistical Machine Translation. In COLING, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Mi</author>
<author>L Huang</author>
<author>Q Liu</author>
</authors>
<title>Forest-Based Translation.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<location>Columbus, OH, USA.</location>
<marker>Mi, Huang, Liu, 2008</marker>
<rawString>H. Mi, L. Huang, and Q. Liu. 2008. Forest-Based Translation. In ACL, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
</authors>
<title>Finite-state transducers in language and speech processing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="10946" citStr="Mohri, 1997" startWordPosition="1875" endWordPosition="1876"> exponential number of items (E E £) with a summation over the number of n-grams that occur in £, which is at worst polynomial in the number of edges in the lattice that defines £. We compute the posterior probability of each n-gram w as: Z(£w) P(E|F) = Z(£) , (7) where Z(£) = EE&apos;∈E exp(αH(E0, F)) (denominator in Equation 3) and Z(£w) = EE&apos;∈Ew exp(αH(E0, F)). Z(£) and Z(£w) represent the sums2 of weights of all paths in the lattices £w and £ respectively. 4 WFSA MBR Computations We now show how the Lattice MBR Decision Rule (Equation 6) can be implemented using Weighted Finite State Automata (Mohri, 1997). There are four steps involved in decoding starting from weighted finite-state automata representing the candidate outputs of a translation system. We will describe these 2in the log semiring, where log+(x, y) = log(ex + e&apos;Y) is the collect operator (Mohri, 2002) EE&apos;∈E exp (αH(E0, F)). (3) gw(E, E0) (5) θw#w(E0)δw(E) ( 1: ) θ0|E0 |+ θw#w(E0)δw(E) P (E|F ) w∈N E = argmax { 1: 1 E&apos;∈E θ0|E0 |+ θw#w(E0)p(w|£) . (6) w∈N p(w|£) = 1: E∈Ew 622 steps in the setting where the evidence lattice £e may be different from the hypothesis lattice £h (Equation 2). 1. Extract the set of n-grams that occur in th</context>
</contexts>
<marker>Mohri, 1997</marker>
<rawString>M. Mohri. 1997. Finite-state transducers in language and speech processing. Computational Linguistics, 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mohri</author>
</authors>
<title>Semiring frameworks and algorithms for shortest-distance problems.</title>
<date>2002</date>
<journal>Journal of Automata, Languages and Combinatorics,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="7973" citStr="Mohri, 2002" startWordPosition="1303" endWordPosition="1304">ally written as follows: 1: E� = argmax G(E, E&apos;)P(E|F), (2) E&apos;E£h EE£e where £h refers to the Hypothesis space from where the translations are chosen, and £e refers to the Evidence space that is used for computing the Bayesrisk. We will present experiments (Section 7) to show the relative importance of these two spaces. 621 3 Lattice MBR Decoding We now present MBR decoding on translation lattices. A translation word lattice is a compact representation for very large N-best lists of translation hypotheses and their likelihoods. Formally, it is an acyclic Weighted Finite State Acceptor (WFSA) (Mohri, 2002) consisting of states and arcs representing transitions between states. Each arc is labeled with a word and a weight. Each path in the lattice, consisting of consecutive transitions beginning at the distinguished initial state and ending at a final state, expresses a candidate translation. Aggregation of the weights along the path1 produces the weight of the path’s candidate H(E, F) according to the model. In our setting, this weight will imply the posterior probability of the translation E given the source sentence F: P(E |F) = exp (αH(E, F)) The scaling factor α E [0, oc) flattens the distri</context>
<context position="11210" citStr="Mohri, 2002" startWordPosition="1917" endWordPosition="1918"> where Z(£) = EE&apos;∈E exp(αH(E0, F)) (denominator in Equation 3) and Z(£w) = EE&apos;∈Ew exp(αH(E0, F)). Z(£) and Z(£w) represent the sums2 of weights of all paths in the lattices £w and £ respectively. 4 WFSA MBR Computations We now show how the Lattice MBR Decision Rule (Equation 6) can be implemented using Weighted Finite State Automata (Mohri, 1997). There are four steps involved in decoding starting from weighted finite-state automata representing the candidate outputs of a translation system. We will describe these 2in the log semiring, where log+(x, y) = log(ex + e&apos;Y) is the collect operator (Mohri, 2002) EE&apos;∈E exp (αH(E0, F)). (3) gw(E, E0) (5) θw#w(E0)δw(E) ( 1: ) θ0|E0 |+ θw#w(E0)δw(E) P (E|F ) w∈N E = argmax { 1: 1 E&apos;∈E θ0|E0 |+ θw#w(E0)p(w|£) . (6) w∈N p(w|£) = 1: E∈Ew 622 steps in the setting where the evidence lattice £e may be different from the hypothesis lattice £h (Equation 2). 1. Extract the set of n-grams that occur in the evidence lattice £e. For the usual BLEU score, n ranges from one to four. 2. Compute the posterior probability p(wj£) of each of these n-grams. 3. Intersect each n-gram w, with an appropriate weight (from Equation 6), to an initially unweighted copy of the hypot</context>
<context position="13217" citStr="Mohri, 2002" startWordPosition="2293" endWordPosition="2294"> path containing the n-gram, and intersect that automaton with the lattice to find the set of paths containing the n-gram (£w in Equation 7). Suppose £ represent the weighted lattice, we compute3: £w = £ n (w w E*), where w = (E* w E*) is the language that contains all strings that do not contain the n-gram w. The posterior probability p(wj£) of n-gram w can be computed as a ratio of the total weights of paths in £w to the total weights of paths in the original lattice (Equation 7). For each n-gram w E N, we then construct an automaton that accepts an input E with weight 3in the log semiring (Mohri, 2002) equal to the product of the number of times the ngram occurs in the input (#w(E)), the n-gram factor θw from Equation 6, and the posterior probability p(wj£). The automaton corresponds to the weighted regular expression (Karttunen et al., 1996): w(w/(θwp(wj£)) w)*. We successively intersect each of these automata with an automaton that begins as an unweighted copy of the lattice £h. This automaton must also incorporate the factor θ0 of each word. This can be accomplished by intersecting the unweighted lattice with the automaton accepting (E/θ0)*. The resulting MBR automaton computes the total</context>
<context position="15161" citStr="Mohri, 2002" startWordPosition="2633" endWordPosition="2634">trategy will be to use a first order Taylor-series approximation to what we call the corpus log(BLEU) gain: the change in corpus log(BLEU) contributed by the sentence relative to not including that sentence in the corpus. Let r be the reference length of the corpus, c0 the candidate length, and {cnj1 &lt; n &lt; 41 the number of n-gram matches. Then, the corpus BLEU score B(r, c0, cn) can be defined as follows (Papineni et al., 2001): 4 ,� min C� 0,1 − r + 1 log cn c0 4 c0n=1 where we have ignored On, the difference between the number of words in the candidate and the numthe (max, +) semiri 4in ng (Mohri, 2002) 4 log B = min C0,1 − r / + 1 log cn , c0 / 4 c0 − On n=1 , 623 ber of n-grams. If L is the average sentence length in the corpus, Δn Pz� (n − 1)c�L . The corpus log(BLEU) gain is defined as the change in log(BLEU) when a new sentence’s (E0) statistics are added to the corpus statistics: G = log B0 − log B, where the counts in B0 are those of B plus those for the current sentence. We will assume that the brevity penalty (first term in the above approximation) does not change when adding the new sentence. In experiments not reported here, we found that taking into account the brevity penalty at</context>
</contexts>
<marker>Mohri, 2002</marker>
<rawString>M. Mohri. 2002. Semiring frameworks and algorithms for shortest-distance problems. Journal of Automata, Languages and Combinatorics, 7(3):321–350.</rawString>
</citation>
<citation valid="false">
<title>The NIST Machine Translation Evaluations.</title>
<note>http://www.nist.gov/speech/tests/mt/.</note>
<marker></marker>
<rawString>NIST. 2002-2008. The NIST Machine Translation Evaluations. http://www.nist.gov/speech/tests/mt/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="20468" citStr="Och and Ney, 2003" startWordPosition="3619" endWordPosition="3622">ation sets for both aren and zhen, and NIST 2006 (NIST portion)/2003 evaluation sets for enzh. We report results on NIST 2008 which is our blind test set. Statistics computed over these data sets are reported in Table 1. 5http://www.nist.gov/speech/tests/mt/ 7.2 MT System Description Our phrase-based statistical MT system is similar to the alignment template system described in Och and Ney (2004). The system is trained on parallel corpora allowed in the constrained track. We first perform sentence and sub-sentence chunk alignment on the parallel documents. We then train word alignment models (Och and Ney, 2003) using 6 Model-1 iterations and 6 HMM iterations. An additional 2 iterations of Model-4 are performed for zhen and enzh pairs. Word Alignments in both source-to-target and target-to-source directions are obtained using the Maximum A-Posteriori (MAP) framework (Matusov et al., 2004). An inventory of phrase-pairs up to length 5 is then extracted from the union of source-target and target-source alignments. Several feature functions are then computed over the phrasepairs. 5-gram word language models are trained on the allowed monolingual corpora. Minimum Error Rate Training under BLEU is used for</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19 – 51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>The Alignment Template Approach to Statistical Machine Translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<pages>449</pages>
<contexts>
<context position="5629" citStr="Och and Ney, 2004" startWordPosition="877" endWordPosition="880">. In Section 5, we introduce the corpus BLEU approximation that makes it possible to perform efficient lattice MBR decoding. An example of lattice MBR with a toy lattice is presented in Section 6. We present lattice MBR experiments in Section 7. A final discussion is presented in Section 8. 2 Minimum Bayes Risk Decoding Minimum Bayes-Risk (MBR) decoding aims to find the candidate hypothesis that has the least expected loss under the probability model (Bickel and Doksum, 1977). We begin with a review of MBR decoding for Statistical Machine Translation (SMT). Statistical MT (Brown et al., 1990; Och and Ney, 2004) can be described as a mapping of a word sequence F in the source language to a word sequence E in the target language; this mapping is produced by the MT decoder S(F). If the reference translation E is known, the decoder performance can be measured by the loss function L(E, S(F)). Given such a loss function L(E, E&apos;) between an automatic translation E&apos; and the reference E, and an underlying probability model P(E|F), the MBR decoder has the following form (Goel and Byrne, 2000; Kumar and Byrne, 2004): E� = argmin R(E&apos;) E&apos;E£ = argmin L(E, E&apos;)P(E|F), E&apos;E£ EE£ 1: where R(E&apos;) denotes the Bayes risk</context>
<context position="20249" citStr="Och and Ney (2004)" startWordPosition="3582" endWordPosition="3585"> we use two development sets: one for Minimum Error Rate Training (Och, 2003; Macherey et al., 2008), and the other for tuning the scale factor for MBR decoding. Our development sets consists of the NIST 2004/2003 evaluation sets for both aren and zhen, and NIST 2006 (NIST portion)/2003 evaluation sets for enzh. We report results on NIST 2008 which is our blind test set. Statistics computed over these data sets are reported in Table 1. 5http://www.nist.gov/speech/tests/mt/ 7.2 MT System Description Our phrase-based statistical MT system is similar to the alignment template system described in Och and Ney (2004). The system is trained on parallel corpora allowed in the constrained track. We first perform sentence and sub-sentence chunk alignment on the parallel documents. We then train word alignment models (Och and Ney, 2003) using 6 Model-1 iterations and 6 HMM iterations. An additional 2 iterations of Model-4 are performed for zhen and enzh pairs. Word Alignments in both source-to-target and target-to-source directions are obtained using the Maximum A-Posteriori (MAP) framework (Matusov et al., 2004). An inventory of phrase-pairs up to length 5 is then extracted from the union of source-target and</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. Och and H. Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. Computational Linguistics, 30(4):417 – 449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation. In ACL,</title>
<date>2003</date>
<location>Sapporo, Japan.</location>
<contexts>
<context position="19707" citStr="Och, 2003" startWordPosition="3495" endWordPosition="3496">ev2 663 919 919 blind 1360 1357 1859 Table 1: Statistics over the development and test sets. gain. We start with a description of the data sets and the SMT system. 7.1 Development and Blind Test Sets We present our experiments on the constrained data track of the NIST 2008 Arabic-to-English (aren), Chinese-to-English (zhen), and English-to-Chinese (enzh) machine translation tasks.5 In all language pairs, the parallel and monolingual data consists of all the allowed training sets in the constrained track. For each language pair, we use two development sets: one for Minimum Error Rate Training (Och, 2003; Macherey et al., 2008), and the other for tuning the scale factor for MBR decoding. Our development sets consists of the NIST 2004/2003 evaluation sets for both aren and zhen, and NIST 2006 (NIST portion)/2003 evaluation sets for enzh. We report results on NIST 2008 which is our blind test set. Statistics computed over these data sets are reported in Table 1. 5http://www.nist.gov/speech/tests/mt/ 7.2 MT System Description Our phrase-based statistical MT system is similar to the alignment template system described in Och and Ney (2004). The system is trained on parallel corpora allowed in the</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In ACL, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<tech>Technical Report RC22176 (W0109-022),</tech>
<institution>IBM Research Division.</institution>
<contexts>
<context position="672" citStr="Papineni et al., 2001" startWordPosition="91" endWordPosition="95">l Machine Translation Roy W. Tromble1 and Shankar Kumar2 and Franz Och2 and Wolfgang Macherey2 1Department of Computer Science 2 Google Inc. Johns Hopkins University 1600 Amphitheatre Pkwy. Baltimore, MD 21218, USA Mountain View, CA 94043, USA royt@jhu.edu {shankarkumar,och,wmach}@google.com Abstract We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode a huge number of translation hypotheses. We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices. We introduce an approximation to the BLEU score (Papineni et al., 2001) that satisfies these conditions. The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata. Our experiments show that the Lattice MBR decoder yields moderate, consistent gains in translation performance over N-best MBR decoding on Arabicto-English, Chinese-to-English and Englishto-Chinese translation tasks. We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance. 1 Introduction Statistical language processing systems for speech recognition, machine translation or </context>
<context position="2654" citStr="Papineni et al., 2001" startWordPosition="390" endWordPosition="393">tion (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translation, MBR decoding is generally implemented by re-ranking an N-best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) gives gains on BLEU. This is despite the fact that the sentence-level BLEU loss function is an approximation to the exact corpus-level BLEU. A different MBR inspired decoding approach is pursued in Zhang and Gildea (2008) for machine translation using Synchronous Context Free Grammars. A forest generated by an initial decoding pass is rescored using dynamic programming to maximize the expected count of synchronous constituents in the tree that corresponds to the translation. Since each constituent adds a new 4-gram to the existing translation, this approach approximately maximizes the expecte</context>
<context position="4002" citStr="Papineni et al., 2001" startWordPosition="597" endWordPosition="601"> that compactly encode a huge number of translation alternatives relative to an N-best list. This is a model-independent approach 620 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 620–629, Honolulu, October 2008.c�2008 Association for Computational Linguistics in that the lattices could be produced by any statistical MT system — both phrase-based and syntaxbased systems would work in this framework. We will introduce conditions on the loss functions that can be incorporated in Lattice MBR decoding. We describe an approximation to the BLEU score (Papineni et al., 2001) that will satisfy these conditions. Our Lattice MBR decoding is realized using Weighted Finite State Automata. We expect Lattice MBR decoding to improve upon N-best MBR primarily because lattices contain many more candidate translations than the Nbest list. This has been demonstrated in speech recognition (Goel and Byrne, 2000). We conduct a range of translation experiments to analyze lattice MBR and compare it with N-best MBR. An important aspect of our lattice MBR is the linear approximation to the BLEU score. We will show that MBR decoding under this score achieves a performance that is at</context>
<context position="6601" citStr="Papineni et al., 2001" startWordPosition="1057" endWordPosition="1060">n E&apos; and the reference E, and an underlying probability model P(E|F), the MBR decoder has the following form (Goel and Byrne, 2000; Kumar and Byrne, 2004): E� = argmin R(E&apos;) E&apos;E£ = argmin L(E, E&apos;)P(E|F), E&apos;E£ EE£ 1: where R(E&apos;) denotes the Bayes risk of candidate translation E&apos; under the loss function L. If the loss function between any two hypotheses can be bounded: L(E, E&apos;) G Lmax, the MBR decoder can be rewritten in terms of a gain function G(E, E&apos;) = Lmax − L(E, E&apos;): 1: E� = argmax G(E, E&apos;)P(E|F). (1) E&apos;E£ EE£ We are interested in performing MBR decoding under a sentence-level BLEU score (Papineni et al., 2001) which behaves like a gain function: it varies between 0 and 1, and a larger value reflects a higher similarity. We will therefore use Equation 1 as the MBR decoder. We note that £ represents the space of translations. For N-best MBR, this space £ is the N-best list produced by a baseline decoder. We will investigate the use of a translation lattice for MBR decoding; in this case, £ will represent the set of candidates encoded in the lattice. In general, MBR decoding can use different spaces for hypothesis selection and risk computation: argmax and the sum in Equation 1 (Goel, 2001). As an exa</context>
<context position="14397" citStr="Papineni et al., 2001" startWordPosition="2486" endWordPosition="2490">resulting MBR automaton computes the total expected gain of each path. A path in this automaton that corresponds to the word sequence E&apos; has cost: θ0jE&apos;j+EwcAr θw#w(E)p(wj£) (expression within the curly brackets in Equation 6). Finally, we extract the best path from the resulting automaton4, giving the lattice MBR candidate translation according to the gain function (Equation 6). 5 Linear Corpus BLEU Our Lattice MBR formulation relies on the decomposition of the overall gain function as a sum of local gain functions (Equation 5). We here describe a linear approximation to the log(BLEU score) (Papineni et al., 2001) which allows such a decomposition. This will enable us to rewrite the log(BLEU) as a linear function of n-gram matches and the hypothesis length. Our strategy will be to use a first order Taylor-series approximation to what we call the corpus log(BLEU) gain: the change in corpus log(BLEU) contributed by the sentence relative to not including that sentence in the corpus. Let r be the reference length of the corpus, c0 the candidate length, and {cnj1 &lt; n &lt; 41 the number of n-gram matches. Then, the corpus BLEU score B(r, c0, cn) can be defined as follows (Papineni et al., 2001): 4 ,� min C� 0,1</context>
<context position="17461" citStr="Papineni et al., 2001" startWordPosition="3070" endWordPosition="3073">on 11) are computed. The factors depend on a set of n-gram matches and counts (cn; n E 10, 1, 2, 3, 4}). These factors could be obtained from a decoding run on a development set. However, doing so could make the performance of lattice MBR very sensitive to the actual BLEU scores on a particular run. We would like to avoid such a dependence and instead, obtain a set of parameters which can be estimated from multiple decoding runs without MBR. To achieve this, we make use of the properties of n-gram matches. It is known that the average ngram precisions decay approximately exponentially with n (Papineni et al., 2001). We now assume that the number of matches of each n-gram is a constant ratio r times the matches of the corresponding n −1 gram. If the unigram precision is p, we can obtain the n-gram factors (n E 11, 2, 3, 4}) (Equation 11) as a function of the parameters p and r, and the number of unigram tokens T: T −1 (12) 1 θn = 4Tp x rn−1 We set p and r to the average values of unigram precision and precision ratio across multiple development sets. Substituting the above factors in Equation 6, we find that the MBR decision does not depend on T; therefore any value of T can be used. 6 An Example Figure </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001. Bleu: a Method for Automatic Evaluation of Machine Translation. Technical Report RC22176 (W0109-022), IBM Research Division.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sixtus</author>
<author>S Ortmanns</author>
</authors>
<title>High Quality Word Graphs Using Forward-Backward Pruning.</title>
<date>1999</date>
<booktitle>In ICASSP,</booktitle>
<location>Phoenix, AZ, USA.</location>
<contexts>
<context position="22872" citStr="Sixtus and Ortmanns, 1999" startWordPosition="4015" endWordPosition="4018"> hold the likelihood scaling factor α a 1 a/0.5 b/0.6 0 b/0.6 b/0.6 2 c/0.6 4 d/0.3 5 e/0.5 a/0.5 6 c/0.6 d/0.4 3 7 8 a/0.063 1 b/0.013 4 d/0.013 d/−0.008 7 e/0.004 9 0 b/0.043 2 c/0.013 5 b/0.043 3 c/0.013 d/−0.008 6 8 a/0.038 10 625 BLEU(%) aren zhen enzh MAP 43.7 27.9 41.4 N-best MBR 43.9 28.3* 42.0 Lattice MBR 44.9 28.5* 42.6 Table 2: Lattice MBR, N-best MBR &amp; MAP decoding. On zhen, Lattice MBR and N-best MBR do not show statistically significant differences. constant; it is set to 0.2 for aren and enzh, and 0.1 for zhen. The translation lattices are pruned using Forward-Backward pruning (Sixtus and Ortmanns, 1999) so that the average numbers of arcs per word (lattice density) is 30. For N-best MBR, we use N-best lists of size 1000. To match the loss function, Lattice MBR is performed at the word level for aren/zhen and at the character level for enzh. Our lattice MBR is implemented using the Google OpenFst library.6 In our experiments, p, r (Equation 12) have values of 0.85/0.72, 0.80/0.62, and 0.63/0.48 for aren, zhen, and enzh respectively. We note that Lattice MBR provides gains of 0.2- 1.0 BLEU points over N-best MBR, which in turn gives 0.2-0.6 BLEU points over MAP. These gains are obtained on top</context>
<context position="26910" citStr="Sixtus and Ortmanns, 1999" startWordPosition="4682" endWordPosition="4685"> the scenario when both evidence and hypothesis spaces are restricted to the 1000-best list. This experiment throws light on what makes lattice MBR effective over N-best MBR. Relative to the N-best list, the translation lattice provides a better estimate of the expected BLEU score. On the other hand, there are few hypotheses 626 outside the 1000-best list which are selected by lattice MBR. Finally, we show how the performance of lattice MBR changes as a function of the lattice density. The lattice density is the average number of arcs per word and can be varied using Forward-Backward pruning (Sixtus and Ortmanns, 1999). Figure 2 reports the average number of lattice paths and BLEU scores as a function of lattice density. The results show that Lattice MBR performance generally improves when the size of the lattice is increased. However, on zhen, there is a small drop beyond a density of 10. This could be due to low quality (low posterior probability) hypotheses that get included at the larger densities and result in a poorer estimate of the expected BLEU score. On aren and enzh, there are some gains beyond a lattice density of 30. These gains are relatively small and come at the expense of higher memory usag</context>
</contexts>
<marker>Sixtus, Ortmanns, 1999</marker>
<rawString>A. Sixtus and S. Ortmanns. 1999. High Quality Word Graphs Using Forward-Backward Pruning. In ICASSP, Phoenix, AZ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Smith</author>
<author>J Eisner</author>
</authors>
<title>Minimum Risk Annealing for Training Log-Linear Models.</title>
<date>2006</date>
<booktitle>In ACL,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="32386" citStr="Smith and Eisner (2006)" startWordPosition="5637" endWordPosition="5640">speech recognition and machine translation (Goel and Byrne, 2000; Ehling et al., 2007) which has shown the need for tuning this factor. Our MT system parameters are trained with Minimum Error Rate Training which assigns a very high posterior probability to the MAP translation. As a result, it is necessary to flatten the probability distribution so that MBR decoding can select hypotheses other than the MAP hypothesis. Our Lattice MBR implementation is made possible due to the linear approximation of the BLEU score. This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. In both cases, a linear metric makes it easier to compute the expectation. While we have applied lattice MBR decoding to the approximate BLEU score, we note that our procedure (Section 3) is applicable to other gain functions which can be decomposed as a sum of local gain functions. In particular, our framework might be useful with translation metrics such as TER (Snover et al., 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergr</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>D. Smith and J. Eisner. 2006. Minimum Risk Annealing for Training Log-Linear Models. In ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Smith</author>
<author>N Smith</author>
</authors>
<title>Probabilistic models of nonprojective dependency trees.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2213" citStr="Smith and Smith, 2007" startWordPosition="325" endWordPosition="328">imum Bayes-Risk (MBR) decoding (Bickel and Doksum, 1977) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion. The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition (Goel and Byrne, 2000), machine translation (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translation, MBR decoding is generally implemented by re-ranking an N-best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) gives gains on BLEU. This is despite the fact that the sentence-level BLEU loss function is an approximation to the exact corpus-level BLEU. A different MBR i</context>
</contexts>
<marker>Smith, Smith, 2007</marker>
<rawString>D. Smith and N. Smith. 2007. Probabilistic models of nonprojective dependency trees. In EMNLP-CoNLL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation. In AMTA,</title>
<date>2006</date>
<location>Boston, MA, USA.</location>
<contexts>
<context position="32821" citStr="Snover et al., 2006" startWordPosition="5712" endWordPosition="5715">mplementation is made possible due to the linear approximation of the BLEU score. This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. In both cases, a linear metric makes it easier to compute the expectation. While we have applied lattice MBR decoding to the approximate BLEU score, we note that our procedure (Section 3) is applicable to other gain functions which can be decomposed as a sum of local gain functions. In particular, our framework might be useful with translation metrics such as TER (Snover et al., 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with words and hidden tree structures. We believe that our lattice MBR framework can be extended to such hypergraphs with loss functions that take into account both BLEU scores as well as parse tree structures. Lattice and Forest based search and training procedures are not yet common in statistical machine translation. However, they are promising because the search space of tra</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In AMTA, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
</authors>
<title>Loss Minimization in Parse Reranking.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="2189" citStr="Titov and Henderson, 2006" startWordPosition="320" endWordPosition="324">e to human annotations. Minimum Bayes-Risk (MBR) decoding (Bickel and Doksum, 1977) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion. The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition (Goel and Byrne, 2000), machine translation (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translation, MBR decoding is generally implemented by re-ranking an N-best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) gives gains on BLEU. This is despite the fact that the sentence-level BLEU loss function is an approximation to the exact corpus-level</context>
</contexts>
<marker>Titov, Henderson, 2006</marker>
<rawString>I. Titov and J. Henderson. 2006. Loss Minimization in Parse Reranking. In EMNLP, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ueffing</author>
<author>F Och</author>
<author>H Ney</author>
</authors>
<date>2002</date>
<booktitle>Generation of Word Graphs in Statistical Machine Translation. In EMNLP,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="3380" citStr="Ueffing et al., 2002" startWordPosition="503" endWordPosition="506">tion to the exact corpus-level BLEU. A different MBR inspired decoding approach is pursued in Zhang and Gildea (2008) for machine translation using Synchronous Context Free Grammars. A forest generated by an initial decoding pass is rescored using dynamic programming to maximize the expected count of synchronous constituents in the tree that corresponds to the translation. Since each constituent adds a new 4-gram to the existing translation, this approach approximately maximizes the expected BLEU. In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al., 2002) that compactly encode a huge number of translation alternatives relative to an N-best list. This is a model-independent approach 620 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 620–629, Honolulu, October 2008.c�2008 Association for Computational Linguistics in that the lattices could be produced by any statistical MT system — both phrase-based and syntaxbased systems would work in this framework. We will introduce conditions on the loss functions that can be incorporated in Lattice MBR decoding. We describe an approximation to the BLEU score (</context>
</contexts>
<marker>Ueffing, Och, Ney, 2002</marker>
<rawString>N. Ueffing, F. Och, and H. Ney. 2002. Generation of Word Graphs in Statistical Machine Translation. In EMNLP, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>D Gildea</author>
</authors>
<title>Efficient Multi-pass Decoding for Synchronous Context Free Grammars.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="2084" citStr="Zhang and Gildea, 2008" startWordPosition="305" endWordPosition="308">ance (Word Error Rate), ngram overlap (BLEU score (Papineni et al., 2001)), or precision/recall relative to human annotations. Minimum Bayes-Risk (MBR) decoding (Bickel and Doksum, 1977) aims to address this mismatch by selecting the hypothesis that minimizes the expected error in classification. Thus it directly incorporates the loss function into the decision criterion. The approach has been shown to give improvements over the MAP classifier in many areas of natural language processing including automatic speech recognition (Goel and Byrne, 2000), machine translation (Kumar and Byrne, 2004; Zhang and Gildea, 2008), bilingual word alignment (Kumar and Byrne, 2002), and parsing (Goodman, 1996; Titov and Henderson, 2006; Smith and Smith, 2007). In statistical machine translation, MBR decoding is generally implemented by re-ranking an N-best list of translations produced by a first-pass decoder; this list typically contains between 100 and 10, 000 hypotheses. Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) gives gains on BLEU. This is </context>
</contexts>
<marker>Zhang, Gildea, 2008</marker>
<rawString>H. Zhang and D. Gildea. 2008. Efficient Multi-pass Decoding for Synchronous Context Free Grammars. In ACL, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zollmann</author>
<author>A Venugopal</author>
</authors>
<title>Syntax Augmented Machine Translation via Chart Parsing. In HLT-NAACL,</title>
<date>2006</date>
<location>New York, NY, USA.</location>
<contexts>
<context position="32962" citStr="Zollmann and Venugopal (2006)" startWordPosition="5736" endWordPosition="5740">where when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. In both cases, a linear metric makes it easier to compute the expectation. While we have applied lattice MBR decoding to the approximate BLEU score, we note that our procedure (Section 3) is applicable to other gain functions which can be decomposed as a sum of local gain functions. In particular, our framework might be useful with translation metrics such as TER (Snover et al., 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with words and hidden tree structures. We believe that our lattice MBR framework can be extended to such hypergraphs with loss functions that take into account both BLEU scores as well as parse tree structures. Lattice and Forest based search and training procedures are not yet common in statistical machine translation. However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al., 2008). We hope that our approach will provide some insight into the design </context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>A. Zollmann and A. Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing. In HLT-NAACL, New York, NY, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>