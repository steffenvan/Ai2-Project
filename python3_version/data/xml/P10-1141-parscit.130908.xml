<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025668">
<title confidence="0.996906">
A study of Information Retrieval weighting schemes for sentiment analysis
</title>
<author confidence="0.983004">
Georgios Paltoglou
</author>
<affiliation confidence="0.962107">
University of Wolverhampton
Wolverhampton, United Kingdom
</affiliation>
<email confidence="0.980341">
g.paltoglou@wlv.ac.uk
</email>
<author confidence="0.991505">
Mike Thelwall
</author>
<affiliation confidence="0.964796">
University of Wolverhampton
Wolverhampton, United Kingdom
</affiliation>
<email confidence="0.992324">
m.thelwall@wlv.ac.uk
</email>
<sectionHeader confidence="0.993947" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997218933333334">
Most sentiment analysis approaches use as
baseline a support vector machines (SVM)
classifier with binary unigram weights.
In this paper, we explore whether more
sophisticated feature weighting schemes
from Information Retrieval can enhance
classification accuracy. We show that vari-
ants of the classic tf.idf scheme adapted
to sentiment analysis provide significant
increases in accuracy, especially when us-
ing a sublinear function for term frequency
weights and document frequency smooth-
ing. The techniques are tested on a wide
selection of data sets and produce the best
accuracy to our knowledge.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999875">
The increase of user-generated content on the web
in the form of reviews, blogs, social networks,
tweets, fora, etc. has resulted in an environ-
ment where everyone can publicly express their
opinion about events, products or people. This
wealth of information is potentially of vital im-
portance to institutions and companies, providing
them with ways to research their consumers, man-
age their reputations and identify new opportuni-
ties. Wright (2009) claims that “for many busi-
nesses, online opinion has turned into a kind of
virtual currency that can make or break a product
in the marketplace”.
Sentiment analysis, also known as opinion min-
ing, provides mechanisms and techniques through
which this vast amount of information can be pro-
cessed and harnessed. Research in the field has
mainly, but not exclusively, focused in two sub-
problems: detecting whether a segment of text, ei-
ther a whole document or a sentence, is subjective
or objective, i.e. contains an expression of opin-
ion, and detecting the overall polarity of the text,
i.e. positive or negative.
Most of the work in sentiment analysis has fo-
cused on supervised learning techniques (Sebas-
tiani, 2002), although there are some notable ex-
ceptions (Turney, 2002; Lin and He, 2009). Pre-
vious research has shown that in general the per-
formance of the former tend to be superior to that
of the latter (Mullen and Collier, 2004; Lin and
He, 2009). One of the main issues for supervised
approaches has been the representation of docu-
ments. Usually a bag of words representation is
adopted, according to which a document is mod-
eled as an unordered collection of the words that
it contains. Early research by Pang et al. (2002) in
sentiment analysis showed that a binary unigram-
based representation of documents, according to
which a document is modeled only by the pres-
ence or absence of words, provides the best base-
line classification accuracy in sentiment analysis
in comparison to other more intricate representa-
tions using bigrams, adjectives, etc.
Later research has focused on extending the
document representation with more complex fea-
tures such as structural or syntactic informa-
tion (Wilson et al., 2005), favorability mea-
sures from diverse sources (Mullen and Collier,
2004), implicit syntactic indicators (Greene and
Resnik, 2009), stylistic and syntactic feature selec-
tion (Abbasi et al., 2008), “annotator rationales”
(Zaidan et al., 2007) and others, but no systematic
study has been presented exploring the benefits of
employing more sophisticated models for assign-
ing weights to word features.
In this paper, we examine whether term weight-
ing functions adopted from Information Retrieval
(IR) based on the standard tf.idf formula and
adapted to the particular setting of sentiment anal-
ysis can help classification accuracy. We demon-
strate that variants of the original tf.idf weighting
scheme provide significant increases in classifica-
tion performance. The advantages of the approach
are that it is intuitive, computationally efficient
</bodyText>
<page confidence="0.93373">
1386
</page>
<note confidence="0.9421135">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1386–1395,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999635615384615">
and doesn’t require additional human annotation
or external sources. Experiments conducted on a
number of publicly available data sets improve on
the previous state-of-the art.
The next section provides an overview of rel-
evant work in sentiment analysis. In section 3
we provide a brief overview of the original tf.idf
weighting scheme along with a number of variants
and show how they can be applied to a classifica-
tion scenario. Section 4 describes the corpora that
were used to test the proposed weighting schemes
and section 5 discusses the results. Finally, we
conclude and propose future work in section 6.
</bodyText>
<sectionHeader confidence="0.996753" genericHeader="introduction">
2 Prior Work
</sectionHeader>
<bodyText confidence="0.99992251724138">
Sentiment analysis has been a popular research
topic in recent years. Most of the work has fo-
cused on analyzing the content of movie or gen-
eral product reviews, but there are also applica-
tions to other domains such as debates (Thomas et
al., 2006; Lin et al., 2006), news (Devitt and Ah-
mad, 2007) and blogs (Ounis et al., 2008; Mishne,
2005). The book of Pang and Lee (2008) presents
a thorough overview of the research in the field.
This section presents the most relevant work.
Pang et al. (2002) conducted early polarity
classification of reviews using supervised ap-
proaches. They employed Support Vector Ma-
chines (SVMs), Naive Bayes and Maximum En-
tropy classifiers using a diverse set of features,
such as unigrams, bigrams, binary and term fre-
quency feature weights and others. They con-
cluded that sentiment classification is more dif-
ficult that standard topic-based classification and
that using a SVM classifier with binary unigram-
based features produces the best results.
A subsequent innovation was the detection and
removal of the objective parts of documents and
the application of a polarity classifier on the rest
(Pang and Lee, 2004). This exploited text coher-
ence with adjacent text spans which were assumed
to belong to the same subjectivity or objectivity
class. Documents were represented as graphs with
sentences as nodes and association scores between
them as edges. Two additional nodes represented
the subjective and objective poles. The weights
between the nodes were calculated using three dif-
ferent, heuristic decaying functions. Finding apar-
tition that minimized a cost function separated the
objective from the subjective sentences. They re-
ported a statistically significant improvement over
a Naive Bayes baseline using the whole text but
only slight increase compared to using a SVM
classifier on the entire document.
Mullen and Collier (2004) used SVMs and ex-
panded the feature set for representing documents
with favorability measures from a variety of di-
verse sources. They introduced features based on
Osgood’s Theory of Semantic Differentiation (Os-
good, 1967) using WordNet to derive the values
of potency, activity and evaluative of adjectives
and Turney’s semantic orientation (Turney, 2002).
Their results showed that using a hybrid SVM
classifier, that uses as features the distance of doc-
uments from the separating hyperplane, with all
the above features produces the best results.
Whitelaw et al. (2005) added fine-grained se-
mantic distinctions in the feature set. Their ap-
proach was based on a lexicon created in a semi-
supervised fashion and then manually refined It
consists of 1329 adjectives and their modifiers cat-
egorized under several taxonomies of appraisal at-
tributes based on Martin and White’s Appraisal
Theory (2005). They combined the produced ap-
praisal groups with unigram-based document rep-
resentations as features to a Support Vector Ma-
chine classifier (Witten and Frank, 1999), result-
ing in significant increases in accuracy.
Zaidan et al. (2007) introduced “annotator ra-
tionales”, i.e. words or phrases that explain the
polarity of the document according to human an-
notators. By deleting rationale text spans from the
original documents they created several contrast
documents and constrained the SVM classifier to
classify them less confidently than the originals.
Using the largest training set size, their approach
significantly increased the accuracy on a standard
data set (see section 4).
Prabowo and Thelwall (2009) proposed a hy-
brid classification process by combining in se-
quence several ruled-based classifiers with a SVM
classifier. The former were based on the Gen-
eral Inquirer lexicon (Wilson et al., 2005), the
MontyLingua part-of-speech tagger (Liu, 2004)
and co-occurrence statistics of words with a set
of predefined reference words. Their experiments
showed that combining multiple classifiers can
result in better effectiveness than any individual
classifier, especially when sufficient training data
isn’t available.
In contrast to machine learning approaches
that require labeled corpora for training, Lin and
</bodyText>
<page confidence="0.989687">
1387
</page>
<bodyText confidence="0.999517461538462">
He (2009) proposed an unsupervised probabilis-
tic modeling framework, based on Latent Dirich-
let Allocation (LDA). The approach assumes that
documents are a mixture of topics, i.e. proba-
bility distribution of words, according to which
each document is generated through an hierarchi-
cal process and adds an extra sentiment layer to
accommodate the opinionated nature (positive or
negative) of the document. Their best attained per-
formance, using a filtered subjectivity lexicon and
removing objective sentences in a manner similar
to Pang and Lee (2004), is only slightly lower than
that of a fully-supervised approach.
</bodyText>
<sectionHeader confidence="0.989484" genericHeader="method">
3 A study of non-binary weights
</sectionHeader>
<bodyText confidence="0.999953">
We use the terms “features”, “words” and “terms”
interchangeably in this paper, since we mainly fo-
cus on unigrams. The approach nonetheless can
easily be extended to higher order n-grams. Each
document D therefore is represented as a bag-of-
words feature vector: D = {w1, w2, ..., w|V  |}
where |V  |is the size of the vocabulary (i.e. the
number of unique words) and wz, i = 1, ... , |V |
is the weight of term i in document D.
Despite the significant attention that sentiment
analysis has received in recent years, the best ac-
curacy without using complex features (Mullen
and Collier, 2004; Whitelaw et al., 2005) or ad-
ditional human annotations (Zaidan et al., 2007) is
achieved by employing a binary weighting scheme
(Pang et al., 2002), where wz = 1, if tfz &gt; 0 and
wz = 0, if tfz = 0, where tfz is the number of
times that term i appears in document D (hence-
forth raw term frequency) and utilizing a SVM
classifier. It is of particular interest that using tfz
in the document representation usually results in
decreased accuracy, a result that appears to be in
contrast with topic classification (Mccallum and
Nigam, 1998; Pang et al., 2002).
In this paper, we also utilize SVMs but our
study is centered on whether more sophisticated
than binary or raw term frequency weighting func-
tions can improve classification accuracy. We
base our approach on the classic tf.idf weighting
scheme from Information Retrieval (IR) and adapt
it to the domain of sentiment classification.
</bodyText>
<subsectionHeader confidence="0.998395">
3.1 The classic tf.idf weighting schemes
</subsectionHeader>
<bodyText confidence="0.94831">
The classic tf.idf formula assigns weight wz to
term i in document D as:
</bodyText>
<equation confidence="0.97982">
wz = tfz · idfz = tfz · log fZ (1)
</equation>
<bodyText confidence="0.991856076923077">
where tfz is the number of times term i occurs in
D, idfz is the inverse document frequency of term
i, N is the total number of documents and dfz is
the number of documents that contain term i.
The utilization of tfz in classification is rather
straightforward and intuitive but, as previously
discussed, usually results in decreased accuracy
in sentiment analysis. On the other hand, using
idf to assign weights to features is less intuitive,
since it only provides information about the gen-
eral distribution of term i amongst documents of
all classes, without providing any additional evi-
dence of class preference. The utilization of idf
in information retrieval is based on its ability to
distinguish between content-bearing words (words
with some semantical meaning) and simple func-
tion words, but this behavior is at least ambiguous
in classification.
Table 1: SMART notation for term frequency vari-
ants. maxt(tf) is the maximum frequency of any
term in the document and avg dl is the average
number of terms in all the documents. For ease of
reference, we also include the BM25 tf scheme.
The k1 and b parameters of BM25 are set to their
default values of 1.2 and 0.95 respectively (Jones
et al., 2000).
</bodyText>
<table confidence="0.997204538461538">
Notation Term frequency
n (natural) tf
l (logarithm) 1 + log(tf)
a (augmented) 0.5·tf
0.5
+ maxt (t f)
b (boolean) 1, t f &gt; 0
� 0, otherwise
L (log ave) 1+log(tf)
1+log(avg dl)
o (BM25) (k1+1)·tf
k1 −b)+b·+tf
((1avg dl)
</table>
<subsectionHeader confidence="0.996942">
3.2 Delta tf.idf
</subsectionHeader>
<bodyText confidence="0.9999638">
Martineau and Finin (2009) provide a solution to
the above issue of idf utilization in a classification
scenario by localizing the estimation of idf to the
documents of one or the other class and subtract-
ing the two values. Therefore, the weight of term
</bodyText>
<page confidence="0.986319">
1388
</page>
<tableCaption confidence="0.70779">
Table 2: SMART notation for inverse document
</tableCaption>
<bodyText confidence="0.9799565">
frequency variants. For ease of reference we also
include the BM25 idf factor and also present the
extensions of the original formulations with their
A variants.
</bodyText>
<table confidence="0.96786625">
Notation Inverse Document Fre-
quency
n (no) 1
t (idf) log df N
p (prob idf) log N−df
df
k (BM25 idf) logN−df+0.5
df+0.5
A(t) (Delta idf) log N1·df2
N2·df1
0(t′) (Delta smoothed log N1·df2+0.5
idf)
N2·df1+0.5
0(p) (Delta prob idf) log (N1−df1)·df2
df1·(N2−df2)
�(p′) (Delta smoothed log (N1−df1)·df2+0.5
prob idf)
(N2−df2)·df1+0.5
0(k) (Delta BM25 idf) log(N1−df1+0.5)·df2+0.5
(N2−df2+0.5)·df1+0.5
</table>
<bodyText confidence="0.894761">
i in document D is estimated as:
</bodyText>
<equation confidence="0.9994905">
wi = tfi · log2( N1 ) − tfi · log2( N2 )
dfi,1 dfi,2
= tfi · log2(N1 · dfi,2 ) (2)
dfi,1 · N2
</equation>
<bodyText confidence="0.998499238095238">
where Nj is the total number of training docu-
ments in class cj and dfi,j is the number of train-
ing documents in class cj that contain term i. The
above weighting scheme was appropriately named
Delta tf.idf.
The produced results (Martineau and Finin,
2009) show that the approach produces better
results than the simple tf or binary weighting
scheme. Nonetheless, the approach doesn’t take
into consideration a number of tested notions from
IR, such as the non-linearity of term frequency to
document relevancy (e.g. Robertson et al. (2004))
according to which, the probability of a document
being relevant to a query term is typically sub-
linear in relation to the number of times a query
term appears in the document. Additionally, their
approach doesn’t provide any sort of smoothing
for the dfi,j factor and is therefore susceptible to
errors in corpora where a term occurs in docu-
ments of only one or the other class and therefore
dfi,j = 0 .
</bodyText>
<subsectionHeader confidence="0.979458">
3.3 SMART and BM25 tf.idf variants
</subsectionHeader>
<bodyText confidence="0.999923833333333">
The SMART retrieval system by Salton (1971) is
a retrieval system based on the vector space model
(Salton and McGill, 1986). Salton and Buckley
(1987) provide a number of variants of the tf.idf
weighting approach and present the SMART nota-
tion scheme, according to which each weighting
function is defined by triples of letters; the first
one denotes the term frequency factor, the sec-
ond one corresponds to the inverse document fre-
quency function and the last one declares the nor-
malization that is being applied. The upper rows
of tables 1, 2 and 3 present the three most com-
monly used weighting functions for each factor re-
spectively. For example, a binary document repre-
sentation would be equivalent to SMART.bnn1
or more simply bnn, while a simple raw term fre-
quency based would be notated as nnn or nnc
with cosine normalization.
</bodyText>
<tableCaption confidence="0.998112">
Table 3: SMART normalization.
</tableCaption>
<table confidence="0.9970385">
Notation Normalization
n (none) 1
c (cosine) 1
�w2+w22+...+wn
</table>
<bodyText confidence="0.995420518518519">
Significant research has been done in IR on di-
verse weighting functions and not all versions of
SMART notations are consistent (Manning et al.,
2008). Zobel and Moffat (1998) provide an ex-
haustive study but in this paper, due to space con-
straints, we will follow the concise notation pre-
sented by Singhal et al. (1995).
The BM25 weighting scheme (Robertson et al.,
1994; Robertson et al., 1996) is a probabilistic
model for information retrieval and is one of the
most popular and effective algorithms used in in-
formation retrieval. For ease of reference, we in-
corporate the BM25 tf and idf factors into the
SMART annotation scheme (last row of table 1
and 4th row of table 2), therefore the weight wi
of term i in document D according to the BM25
scheme is notated as SMART.okn or okn.
Most of the tf weighting functions in SMART
and the BM25 model take into consideration the
non-linearity of document relevance to term fre-
1Typically, a weighting function in the SMART system is
defined as a pair of triples, i.e. ddd.qqq where the first triple
corresponds to the document representation and the second
to the query representation. In the context that the SMART
annotation is used here, we will use the prefix SMART for
the first part and a triple for the document representation in
the second part, i.e. SMART.ddd, or more simply ddd.
</bodyText>
<page confidence="0.980156">
1389
</page>
<bodyText confidence="0.999629">
quency and thus employ tf factors that scale sub-
linearly in relation to term frequency. Addition-
ally, the BM25 tf variant also incorporates a scal-
ing for the length of the document, taking into con-
sideration that longer documents will by definition
have more term occurences2. Effective weighting
functions is a very active research area in infor-
mation retrieval and it is outside the scope of this
paper to provide an in-depth analysis but signifi-
cant research can be found in Salton and McGill
(1986), Robertson et al. (2004), Manning et al.
(2008) or Armstrong et al. (2009) for a more re-
cent study.
</bodyText>
<sectionHeader confidence="0.6053125" genericHeader="method">
3.4 Introducing SMART and BM25 Delta
tf.idf variants
</sectionHeader>
<bodyText confidence="0.999994529411765">
We apply the idea of localizing the estimation
of idf values to documents of one class but em-
ploy more sophisticated term weighting functions
adapted from the SMART retrieval system and
the BM25 probabilistic model. The resulting idf
weighting functions are presented in the lower part
of table 2. We extend the original SMART anno-
tation scheme by adding Delta (A) variants of the
original idf functions and additionally introduce
smoothed Delta variants of the idf and the prob
idf factors for completeness and comparative rea-
sons, noted by their accented counterparts. For
example, the weight of term i in document D ac-
cording to the oA(k)n weighting scheme where
we employ the BM25 tf weighting function and
utilize the difference of class-based BM25 idf val-
ues would be calculated as:
</bodyText>
<equation confidence="0.9997072">
(k1 + 1) · tfi
K+tfi
�(N1 − dfi,1 + 0.5) · (dfi,2 + 0.5) �
· log
(N2 − dfi,2 + 0.5) · (dfi,1 + 0.5)
</equation>
<bodyText confidence="0.967976297297297">
where K is defined as k1 ((1 − b) + b · avdl dl )
However, we used a minor variation of the above
formulation for all the final accented weighting
functions in which the smoothing factor is added
to the product of dfi with Ni (or its variation for
A(p′) and A(k)), rather than to the dfi alone as the
2We deliberately didn’t extract the normalization compo-
nent from the BM25 tf variant, as that would unnecessarily
complicate the notation.
above formulation would imply (see table 2). The
above variation was made for two reasons: firstly,
when the dfi’s are larger than 1 then the smooth-
ing factor influences the final idf value only in a
minor way in the revised formulation, since it is
added only after the multiplication of the dfi with
Ni (or its variation). Secondly, when dfi = 0, then
the smoothing factor correctly adds only a small
mass, avoiding a potential division by zero, where
otherwise it would add a much greater mass, be-
cause it would be multiplied by Ni.
According to this annotation scheme therefore,
the original approach by Martineau and Finin
(2009) can be represented as nA(t)n.
We hypothesize that the utilization of sophisti-
cated term weighting functions that have proved
effective in information retrieval, thus providing
an indication that they appropriately model the
distinctive power of terms to documents and the
smoothed, localized estimation of idf values will
prove beneficial in sentiment classification.
Table 4: Reported accuracies on the Movie Re-
view data set. Only the best reported accuracy for
each approach is presented, measured by 10-fold
cross validation. The list is not exhaustive and be-
cause of differences in training/testing data splits
the results are not directly comparable. It is pro-
duced here only for reference.
</bodyText>
<figureCaption confidence="0.95785425">
Approach Acc.
SVM with unigrams &amp; binary 87.15%
weights (Pang et al., 2002), reported
at (Pang and Lee, 2004)
Hybrid SVM with Turney/Osgood 86%
Lemmas (Mullen and Collier, 2004)
SVM with min-cuts (Pang and Lee, 87.2%
2004)
SVM with appraisal groups 90.2%
(Whitelaw et al., 2005)
SVM with log likehood ratio feature 90.45%
selection (Aue and Gamon, 2005)
SVM with annotator rationales 92.2%
(Zaidan et al., 2007)
LDA with filtered lexicon, subjectiv- 84.6%
ity detection (Lin and He, 2009)
</figureCaption>
<bodyText confidence="0.9995514">
The approach is straightforward, intuitive, com-
putationally efficient, doesn’t require additional
human effort and takes into consideration stan-
dardized and tested notions from IR. The re-
sults presented in section 5 show that a number
</bodyText>
<equation confidence="0.999967285714286">
(k1 + 1) · tfi
K+tfi
· N2 − dfi,2 + 0.5
log( dfi,2 + 0.5 )
wi = · log(
(k1 + 1) · tfi N1 − dfi,1 + 0.5
K + tfi dfi,1 + 0.5 )
</equation>
<page confidence="0.900121">
1390
</page>
<bodyText confidence="0.9999795">
of weighting functions solidly outperform other
state-of-the-art approaches. In the next section, we
present the corpora that were used to study the ef-
fectiveness of different weighting schemes.
</bodyText>
<sectionHeader confidence="0.997021" genericHeader="method">
4 Experimental setup
</sectionHeader>
<bodyText confidence="0.9999644">
We have experimented with a number of publicly
available data sets.
The movie review dataset by Pang et al. (2002)
has been used extensively in the past by a number
of researchers (see Table 4), presenting the oppor-
tunity to compare the produced results with pre-
vious approaches. The dataset comprises 2,000
movie reviews, equally divided between positive
and negative, extracted from the Internet Movie
Database3 archive of the rec.arts.movies.reviews
newsgroup. In order to avoid reviewer bias, only
20 reviews per author were kept, resulting in a to-
tal of 312 reviewers4. The best attained accuracies
by previous research on the specific data are pre-
sented in table 4. We do not claim that those re-
sults are directly comparable to ours, because of
potential subtle differences in tokenization, classi-
fier implementations etc, but we present them here
for reference.
The Multi-Domain Sentiment data set (MDSD)
by Blitzer et al. (2007) contains Amazon reviews
for four different product types: books, electron-
ics, DVDs and kitchen appliances. Reviews with
ratings of 3 or higher, on a 5-scale system, were
labeled as positive and reviews with a rating less
than 3 as negative. The data set contains 1,000
positive and 1,000 negative reviews for each prod-
uct category for a total of 8,000 reviews. Typically,
the data set is used for domain adaptation applica-
tions but in our setting we only split the reviews
between positive and negative5.
Lastly, we present results from the BLOGS06
(Macdonald and Ounis, 2006) collection that is
comprised of an uncompressed 148GB crawl of
approximately 100,000 blogs and their respective
RSS feeds. The collection has been used for 3 con-
secutive years by the Text REtrieval Conferences
(TREC)6. Participants of the conference are pro-
vided with the task of finding documents (i.e. web
pages) expressing an opinion about specific enti-
</bodyText>
<footnote confidence="0.995902666666667">
3http://www.imdb.com
4The dataset can be found at: http://www.cs.cornell.edu/
People/pabo/movie-review-data/review polarity.tar.gz.
5The data set can be found at http://www.cs.jhu.edu/
mdredze/datasets/sentiment/
6http://www.trec.nist.gov
</footnote>
<bodyText confidence="0.999958571428571">
ties X, which may be people, companies, films
etc. The results are given to human assessors who
then judge the content of the webpages (i.e. blog
post and comments) and assign each webpage a
score: “1” if the document contains relevant, fac-
tual information about the entity but no expression
of opinion, “2” if the document contains an ex-
plicit negative opinion towards the entity and “4”
is the document contains an explicit positive opin-
ion towards the entity. We used the produced as-
sessments from all 3 years of the conference in our
data set, resulting in 150 different entity searches
and, after duplicate removal, 7,930 negative docu-
ments (i.e. having an assessment of “2”) and 9,968
positive documents (i.e. having an assessment of
“4”), which were used as the “gold standard” 7.
Documents are annotated at the document-level,
rather than at the post level, making this data set
somewhat noisy. Additionally, the data set is par-
ticularly large compared to the other ones, making
classification especially challenging and interest-
ing. More information about all data sets can be
found at table 5.
We have kept the pre-processing of the docu-
ments to a minimum. Thus, we have lower-cased
all words and removed all punctuation but we have
not removed stop words or applied stemming. We
have also refrained from removing words with
low or high occurrence. Additionally, for the
BLOGS06 data set, we have removed all html for-
matting.
We utilize the implementation of a support vec-
tor classifier from the LIBLINEAR library (Fan et
al., 2008). We use a linear kernel and default
parameters. All results are based on leave-one
out cross validation accuracy. The reason for this
choice of cross-validation setting, instead of the
most standard ten-fold, is that all of the proposed
approaches that use some form of idf utilize the
training documents for extracting document fre-
quency statistics, therefore more information is
available to them in this experimental setting.
Because of the high number of possible combi-
nations between tf and idf variants (6·9·2 = 108)
and due to space constraints we only present re-
sults from a subset of the most representative com-
binations. Generally, we’ll use the cosine nor-
malized variants of unsmoothed delta weighting
schemes, since they perform better than their un-
</bodyText>
<footnote confidence="0.972491666666667">
7More information about the data set, as well as in-
formation on how it can be obtained can be found at:
http://ir.dcs.gla.ac.uk/test collections/blogs06info.html
</footnote>
<page confidence="0.995609">
1391
</page>
<tableCaption confidence="0.999507">
Table 5: Statistics about the data sets used.
</tableCaption>
<table confidence="0.996126666666667">
Data set #Documents #Terms #Unique Average #Terms
Terms per Document
Movie Reviews 2,000 1,336,883 39,399 668
Multi-Domain Sentiment 8,000 1,741,085 455,943 217
Dataset (MDSD)
BLOGS06 17,898 51,252,850 367,899 2,832
</table>
<figureCaption confidence="0.998615">
Figure 1: Reported accuracy on the Movie Review data set.
</figureCaption>
<bodyText confidence="0.99967325">
normalized counterparts. We’ll avoid using nor-
malization for the smoothed versions, in order to
focus our attention on the results of smoothing,
rather than normalization.
</bodyText>
<sectionHeader confidence="0.999813" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999874">
Results for the Movie Reviews, Multi-Domain
Sentiment Dataset and BLOGS06 corpora are re-
ported in figures 1, 2 and 3 respectively.
On the Movie Review data set, the results re-
confirm that using binary features (bnc) is bet-
ter than raw term frequency (nnc) (83.40%) fea-
tures. For reference, in this setting the unnor-
malized vector using the raw tf approach (nnn)
performs similar to the normalized (nnc) (83.40%
vs. 83.60%), the former not present in the graph.
Nonetheless, using any scaled tf weighting func-
tion (anc or onc) performs as well as the binary
approach (87.90% and 87.50% respectively). Of
interest is the fact that although the BM25 tf algo-
rithm has proved much more successful in IR, the
same doesn’t apply in this setting and its accuracy
is similar to the simpler augmented tf approach.
Incorporating un-localized variants of idf (mid-
dle graph section) produces only small increases
in accuracy. Smoothing also doesn’t provide any
particular advantage, e.g. btc (88.20%) vs. bt′c
(88.45%), since no zero idf values are present.
Again, using more sophisticated tf functions pro-
vides an advantage over raw tf, e.g. nt′c at-
tains an accuracy of 86.6% in comparison to at′c’s
88.25%, although the simpler at′c is again as ef-
fective than the BM25 tf (ot′c), which performs at
88%. The actual idf weighting function is of some
importance, e.g. ot′c (88%) vs. okc (87.65%) and
akc (88%) vs. at′c (88.25%), with simpler idf fac-
tors performing similarly, although slightly better
than BM25.
Introducing smoothed, localized variants of idf
and scaled or binary tf weighting schemes pro-
duces significant advantages. In this setting,
smoothing plays a role, e.g. n0(t)c8 (91.60%)
vs. n0(t′)n (95.80%) and a0(p)c (92.80%)
vs. a0(p′)n (96.55%), since we can expect zero
class-based estimations of idf values, supporting
our initial hypothesis on its importance. Addition-
ally, using augmented, BM25 or binary tf weights
is always better than raw term frequency, pro-
viding further support on the advantages of us-
ing sublinear tf weighting functions9. In this set-
ting, the best accuracy of 96.90% is attained using
BM25 tf weights with the BM25 delta idf variant,
although binary or augmented tf weights using
</bodyText>
<footnote confidence="0.983798375">
8The original Delta tf.idf by Martineau and Finin (2009)
has a limitation of utilizing features with df &gt; 2. In our
experiments it performed similarly to nA(t)n (90.60%) but
still lower than the cosine normalized variant nA(t)c in-
cluded in the graph (91.60%).
9Although not present in the graph, for completeness rea-
sons it should be noted that lA(s)n and LA(s)n also per-
form very well, both reaching accuracies of approx. 96%.
</footnote>
<page confidence="0.990452">
1392
</page>
<figureCaption confidence="0.999821">
Figure 2: Reported accuracy on the Multi-Domain Sentiment data set.
</figureCaption>
<bodyText confidence="0.999344777777778">
delta idf perform similarly (96.50% and 96.60%
respectively). The results indicate that the tf and
the idf factor themselves aren’t of significant im-
portance, as long as the former are scaled and the
latter smoothed in some manner. For example,
a0(p′)n vs. a0(t′)n perform quite similarly.
The results from the Multi-Domain Sentiment
data set (figure 2) largely agree with the find-
ings on the Movie Review data set, providing a
strong indication that the approach isn’t limited
to a specific domain. Binary weights outperform
raw term frequency weights and perform similarly
with scaled tf’s. Non-localized variants of idf
weights do provide a small advantage in this data
set although the actual idf variant isn’t important,
e.g. btc, bt′c, and okc all perform similarly. The
utilized tf variant also isn’t important, e.g. at′c
(88.39%) vs. bt′c (88.25%).
We focus our attention on the delta idf vari-
ants which provide the more interesting results.
The importance of smoothing becomes apparent
when comparing the accuracy of a0(p)c and its
smoothed variant a0(p′)n (92.56% vs. 95.6%).
Apart from that, all smoothed delta idf variants
perform very well in this data set, including some-
what surprisingly, n0(t′)n which uses raw tf
(94.54%). Considering that the average tf per
document is approx. 1.9 in the Movie Review
data set and 1.1 in the MDSD, the results can be
attributed to the fact that words tend to typically
appear only once per document in the latter, there-
fore minimizing the difference of the weights at-
tributed by different tf functions10. The best at-
tained accuracy is 96.40% but as the MDSD has
mainly been used for domain adaptation applica-
tions, there is no clear baseline to compare it with.
</bodyText>
<footnote confidence="0.528887">
10For reference, the average tf per document in the
BLOGS06 data set is 2.4.
</footnote>
<bodyText confidence="0.999965">
Lastly, we present results on the BLOGS06
dataset in figure 3. As previously noted, this data
set is particularly noisy, because it has been an-
notated at the document-level rather than the post-
level and as a result, the differences aren’t as pro-
found as in the previous corpora, although they
do follow the same patterns. Focusing on the
delta idf variants, the importance of smoothing
becomes apparent, e.g. a0(p)c vs. a0(p′)n and
n0(t)c vs. n0(t′)n. Additionally, because of the
fact that documents tend to be more verbose in
this data set, the scaled tf variants also perform
better than the simple raw tf ones, n0(t′)n vs.
a0(t′)n. Lastly, as previously, the smoothed lo-
calized idf variants perform better than their un-
smoothed counterparts, e.g. n0(t)n vs. n0(t′)n
and a0(p)c vs. a0(p′)n.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999962333333334">
In this paper, we presented a study of document
representations for sentiment analysis using term
weighting functions adopted from information re-
trieval and adapted to classification. The pro-
posed weighting schemes were tested on a num-
ber of publicly available datasets and a number
of them repeatedly demonstrated significant in-
creases in accuracy compared to other state-of-the-
art approaches. We demonstrated that for accurate
classification it is important to use term weight-
ing functions that scale sublinearly in relation to
the number of times a term occurs in a document
and that document frequency smoothing is a sig-
nificant factor.
In the future we plan to test the proposed
weighting functions in other domains such as topic
classification and additionally extend the approach
to accommodate multi-class classification.
</bodyText>
<page confidence="0.986198">
1393
</page>
<figureCaption confidence="0.999031">
Figure 3: Reported accuracy on the BLOGS06 data set.
</figureCaption>
<sectionHeader confidence="0.995868" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9988476">
This work was supported by a European Union
grant by the 7th Framework Programme, Theme
3: Science of complex systems for socially intelli-
gent ICT. It is part of the CyberEmotions Project
(Contract 231323).
</bodyText>
<sectionHeader confidence="0.998101" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999553364864865">
Ahmed Abbasi, Hsinchun Chen, and Arab Salem.
2008. Sentiment analysis in multiple languages:
Feature selection for opinion classification in web
forums. ACM Trans. Inf. Syst., 26(3):1–34.
Timothy G. Armstrong, Alistair Moffat, William Web-
ber, and Justin Zobel. 2009. Improvements that
don’t add up: ad-hoc retrieval results since 1998.
In David Wai Lok Cheung, Il Y. Song, Wesley W.
Chu, Xiaohua Hu, Jimmy J. Lin, David Wai Lok
Cheung, Il Y. Song, Wesley W. Chu, Xiaohua Hu,
and Jimmy J. Lin, editors, CIKM, pages 601–610,
New York, NY, USA. ACM.
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: A case
study. In Proceedings of Recent Advances in Nat-
ural Language Processing (RANLP).
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 440–447, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Ann Devitt and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 984–991, Prague, Czech Republic,
June. Association for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 503–511, Boulder, Colorado, June.
Association for Computational Linguistics.
K. Sparck Jones, S. Walker, and S. E. Robertson. 2000.
A probabilistic model of information retrieval: de-
velopment and comparative experiments. Inf. Pro-
cess. Manage., 36(6):779–808.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In CIKM
’09: Proceeding of the 18th ACM conference on In-
formation and knowledge management, pages 375–
384, New York, NY, USA. ACM.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? identifying perspectives at the document and
sentence levels. In Proceedings of the Conference
on Natural Language Learning (CoNLL).
Hugo Liu. 2004. MontyLingua: An end-to-end natural
language processor with common sense. Technical
report, MIT.
C. Macdonald and I. Ounis. 2006. The trec blogs06
collection : Creating and analysing a blog test col-
lection. DCS Technical Report Series.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, 1 edition,
July.
J. R. Martin and P. R. R. White. 2005. The language of
evaluation : appraisal in English / J.R. Martin and
P.R.R. White. Palgrave Macmillan, Basingstoke :.
Justin Martineau and Tim Finin. 2009. Delta TFIDF:
An Improved Feature Space for Sentiment Analysis.
In Proceedings of the Third AAAI Internatonal Con-
ference on Weblogs and Social Media, San Jose, CA,
May. AAAI Press. (poster paper).
A. Mccallum and K. Nigam. 1998. A comparison of
event models for naive bayes text classification.
</reference>
<page confidence="0.873034">
1394
</page>
<reference confidence="0.999916478723405">
G. Mishne. 2005. Experiments with mood classifi-
cation in blog posts. In 1st Workshop on Stylistic
Analysis Of Text For Information Access.
Tony Mullen and Nigel Collier. 2004. Sentiment anal-
ysis using support vector machines with diverse in-
formation sources. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 412–
418, Barcelona, Spain, July. Association for Com-
putational Linguistics.
Charles E. Osgood. 1967. The measurement of mean-
ing / [by] [Charles E. Osgood, George J. Suci [and]
Percy H. Tannenbaum]. University of Illinois Press,
Urbana:, 2nd ed. edition.
Iadh Ounis, Craig Macdonald, and Ian Soboroff. 2008.
Overview of the trec-2008 blog trac. In The Seven-
teenth Text REtrieval Conference (TREC 2008) Pro-
ceedings. NIST.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In In Proceedings
of the ACL, pages 271–278.
B. Pang and L. Lee. 2008. Opinion Mining and Senti-
ment Analysis. Now Publishers Inc.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP).
Rudy Prabowo and Mike Thelwall. 2009. Sentiment
analysis: A combined approach. Journal of Infor-
metrics, 3(2):143–157, April.
Stephen E. Robertson, Steve Walker, Susan Jones,
Micheline Hancock-Beaulieu, and Mike Gatford.
1994. Okapi at trec-3. In TREC, pages 0–.
S E Robertson, S Walker, S Jones, M M Hancock-
Beaulieu, and M Gatford. 1996. Okapi at trec-2.
In In The Second Text REtrieval Conference (TREC-
2), NIST Special Special Publication 500-215, pages
21–34.
Stephen Robertson, Hugo Zaragoza, and Michael Tay-
lor. 2004. Simple bm25 extension to multiple
weighted fields. In CIKM ’04: Proceedings of the
thirteenth ACM international conference on Infor-
mation and knowledge management, pages 42–49,
New York, NY, USA. ACM.
Gerard Salton and Chris Buckley. 1987. Term weight-
ing approaches in automatic text retrieval. Technical
report, Ithaca, NY, USA.
Gerard Salton and Michael J. McGill. 1986. Intro-
duction to Modern Information Retrieval. McGraw-
Hill, Inc., New York, NY, USA.
G. Salton. 1971. The SMART Retrieval System—
Experiments in Automatic Document Processing.
Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Computing Sur-
veys, 34(1):1˜n47.
Amit Singhal, Gerard Salton, and Chris Buckley. 1995.
Length normalization in degraded text collections.
Technical report, Ithaca, NY, USA.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition
from congressional floor-debate transcripts. CoRR,
abs/cs/0607062.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
fication of reviews. In ACL, pages 417–424.
Casey Whitelaw, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal groups for sentiment
analysis. In CIKM ’05: Proceedings of the 14th
ACM international conference on Information and
knowledge management, pages 625–631, New York,
NY, USA. ACM.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technologies Conference/Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP 2005), Vancouver, CA.
Ian H. Witten and Eibe Frank. 1999. Data Mining:
Practical Machine Learning Tools and Techniques
with Java Implementations (The Morgan Kaufmann
Series in Data Management Systems). Morgan
Kaufmann, 1st edition, October.
Alex Wright. 2009. Mining the web for feelings, not
facts. August 23, NY Times, last accessed October
2, 2009, http://http://www.nytimes.com/2009/08/24/
technology/internet/ 24emotion.html? r=1.
O.F. Zaidan, J. Eisner, and C.D. Piatko. 2007. Using
Annotator Rationales to Improve Machine Learn-
ing for Text Categorization. Proceedings ofNAACL
HLT, pages 260–267.
Justin Zobel and Alistair Moffat. 1998. Exploring the
similarity space. SIGIR Forum, 32(1):18–34.
</reference>
<page confidence="0.992646">
1395
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.829763">
<title confidence="0.999225">A study of Information Retrieval weighting schemes for sentiment analysis</title>
<author confidence="0.988694">Georgios Paltoglou</author>
<affiliation confidence="0.999955">University of Wolverhampton</affiliation>
<address confidence="0.968944">Wolverhampton, United Kingdom</address>
<email confidence="0.998605">g.paltoglou@wlv.ac.uk</email>
<author confidence="0.997111">Mike Thelwall</author>
<affiliation confidence="0.999974">University of Wolverhampton</affiliation>
<address confidence="0.88065">Wolverhampton, United Kingdom</address>
<email confidence="0.998594">m.thelwall@wlv.ac.uk</email>
<abstract confidence="0.9993301875">Most sentiment analysis approaches use as baseline a support vector machines (SVM) classifier with binary unigram weights. In this paper, we explore whether more sophisticated feature weighting schemes from Information Retrieval can enhance classification accuracy. We show that variof the classic adapted to sentiment analysis provide significant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing. The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmed Abbasi</author>
<author>Hsinchun Chen</author>
<author>Arab Salem</author>
</authors>
<title>Sentiment analysis in multiple languages: Feature selection for opinion classification in web forums.</title>
<date>2008</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="3253" citStr="Abbasi et al., 2008" startWordPosition="499" endWordPosition="502">esentation of documents, according to which a document is modeled only by the presence or absence of words, provides the best baseline classification accuracy in sentiment analysis in comparison to other more intricate representations using bigrams, adjectives, etc. Later research has focused on extending the document representation with more complex features such as structural or syntactic information (Wilson et al., 2005), favorability measures from diverse sources (Mullen and Collier, 2004), implicit syntactic indicators (Greene and Resnik, 2009), stylistic and syntactic feature selection (Abbasi et al., 2008), “annotator rationales” (Zaidan et al., 2007) and others, but no systematic study has been presented exploring the benefits of employing more sophisticated models for assigning weights to word features. In this paper, we examine whether term weighting functions adopted from Information Retrieval (IR) based on the standard tf.idf formula and adapted to the particular setting of sentiment analysis can help classification accuracy. We demonstrate that variants of the original tf.idf weighting scheme provide significant increases in classification performance. The advantages of the approach are t</context>
</contexts>
<marker>Abbasi, Chen, Salem, 2008</marker>
<rawString>Ahmed Abbasi, Hsinchun Chen, and Arab Salem. 2008. Sentiment analysis in multiple languages: Feature selection for opinion classification in web forums. ACM Trans. Inf. Syst., 26(3):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy G Armstrong</author>
<author>Alistair Moffat</author>
<author>William Webber</author>
<author>Justin Zobel</author>
</authors>
<title>Improvements that don’t add up: ad-hoc retrieval results since 1998. In</title>
<date>2009</date>
<pages>601--610</pages>
<editor>David Wai Lok Cheung, Il Y. Song, Wesley W. Chu, Xiaohua Hu, Jimmy J. Lin, David Wai Lok Cheung, Il Y. Song, Wesley W. Chu, Xiaohua Hu, and Jimmy J. Lin, editors, CIKM,</editor>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="17354" citStr="Armstrong et al. (2009)" startWordPosition="2821" endWordPosition="2824">art, i.e. SMART.ddd, or more simply ddd. 1389 quency and thus employ tf factors that scale sublinearly in relation to term frequency. Additionally, the BM25 tf variant also incorporates a scaling for the length of the document, taking into consideration that longer documents will by definition have more term occurences2. Effective weighting functions is a very active research area in information retrieval and it is outside the scope of this paper to provide an in-depth analysis but significant research can be found in Salton and McGill (1986), Robertson et al. (2004), Manning et al. (2008) or Armstrong et al. (2009) for a more recent study. 3.4 Introducing SMART and BM25 Delta tf.idf variants We apply the idea of localizing the estimation of idf values to documents of one class but employ more sophisticated term weighting functions adapted from the SMART retrieval system and the BM25 probabilistic model. The resulting idf weighting functions are presented in the lower part of table 2. We extend the original SMART annotation scheme by adding Delta (A) variants of the original idf functions and additionally introduce smoothed Delta variants of the idf and the prob idf factors for completeness and comparati</context>
</contexts>
<marker>Armstrong, Moffat, Webber, Zobel, 2009</marker>
<rawString>Timothy G. Armstrong, Alistair Moffat, William Webber, and Justin Zobel. 2009. Improvements that don’t add up: ad-hoc retrieval results since 1998. In David Wai Lok Cheung, Il Y. Song, Wesley W. Chu, Xiaohua Hu, Jimmy J. Lin, David Wai Lok Cheung, Il Y. Song, Wesley W. Chu, Xiaohua Hu, and Jimmy J. Lin, editors, CIKM, pages 601–610, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Aue</author>
<author>Michael Gamon</author>
</authors>
<title>Customizing sentiment classifiers to new domains: A case study.</title>
<date>2005</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing (RANLP).</booktitle>
<contexts>
<context position="20446" citStr="Aue and Gamon, 2005" startWordPosition="3348" endWordPosition="3351">ly the best reported accuracy for each approach is presented, measured by 10-fold cross validation. The list is not exhaustive and because of differences in training/testing data splits the results are not directly comparable. It is produced here only for reference. Approach Acc. SVM with unigrams &amp; binary 87.15% weights (Pang et al., 2002), reported at (Pang and Lee, 2004) Hybrid SVM with Turney/Osgood 86% Lemmas (Mullen and Collier, 2004) SVM with min-cuts (Pang and Lee, 87.2% 2004) SVM with appraisal groups 90.2% (Whitelaw et al., 2005) SVM with log likehood ratio feature 90.45% selection (Aue and Gamon, 2005) SVM with annotator rationales 92.2% (Zaidan et al., 2007) LDA with filtered lexicon, subjectiv- 84.6% ity detection (Lin and He, 2009) The approach is straightforward, intuitive, computationally efficient, doesn’t require additional human effort and takes into consideration standardized and tested notions from IR. The results presented in section 5 show that a number (k1 + 1) · tfi K+tfi · N2 − dfi,2 + 0.5 log( dfi,2 + 0.5 ) wi = · log( (k1 + 1) · tfi N1 − dfi,1 + 0.5 K + tfi dfi,1 + 0.5 ) 1390 of weighting functions solidly outperform other state-of-the-art approaches. In the next section, w</context>
</contexts>
<marker>Aue, Gamon, 2005</marker>
<rawString>Anthony Aue and Michael Gamon. 2005. Customizing sentiment classifiers to new domains: A case study. In Proceedings of Recent Advances in Natural Language Processing (RANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>440--447</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="22099" citStr="Blitzer et al. (2007)" startWordPosition="3625" endWordPosition="3628">eviews, equally divided between positive and negative, extracted from the Internet Movie Database3 archive of the rec.arts.movies.reviews newsgroup. In order to avoid reviewer bias, only 20 reviews per author were kept, resulting in a total of 312 reviewers4. The best attained accuracies by previous research on the specific data are presented in table 4. We do not claim that those results are directly comparable to ours, because of potential subtle differences in tokenization, classifier implementations etc, but we present them here for reference. The Multi-Domain Sentiment data set (MDSD) by Blitzer et al. (2007) contains Amazon reviews for four different product types: books, electronics, DVDs and kitchen appliances. Reviews with ratings of 3 or higher, on a 5-scale system, were labeled as positive and reviews with a rating less than 3 as negative. The data set contains 1,000 positive and 1,000 negative reviews for each product category for a total of 8,000 reviews. Typically, the data set is used for domain adaptation applications but in our setting we only split the reviews between positive and negative5. Lastly, we present results from the BLOGS06 (Macdonald and Ounis, 2006) collection that is com</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Devitt</author>
<author>Khurshid Ahmad</author>
</authors>
<title>Sentiment polarity identification in financial news: A cohesionbased approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>984--991</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5016" citStr="Devitt and Ahmad, 2007" startWordPosition="776" endWordPosition="780">the original tf.idf weighting scheme along with a number of variants and show how they can be applied to a classification scenario. Section 4 describes the corpora that were used to test the proposed weighting schemes and section 5 discusses the results. Finally, we conclude and propose future work in section 6. 2 Prior Work Sentiment analysis has been a popular research topic in recent years. Most of the work has focused on analyzing the content of movie or general product reviews, but there are also applications to other domains such as debates (Thomas et al., 2006; Lin et al., 2006), news (Devitt and Ahmad, 2007) and blogs (Ounis et al., 2008; Mishne, 2005). The book of Pang and Lee (2008) presents a thorough overview of the research in the field. This section presents the most relevant work. Pang et al. (2002) conducted early polarity classification of reviews using supervised approaches. They employed Support Vector Machines (SVMs), Naive Bayes and Maximum Entropy classifiers using a diverse set of features, such as unigrams, bigrams, binary and term frequency feature weights and others. They concluded that sentiment classification is more difficult that standard topic-based classification and that </context>
</contexts>
<marker>Devitt, Ahmad, 2007</marker>
<rawString>Ann Devitt and Khurshid Ahmad. 2007. Sentiment polarity identification in financial news: A cohesionbased approach. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 984–991, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="24814" citStr="Fan et al., 2008" startWordPosition="4056" endWordPosition="4059"> set is particularly large compared to the other ones, making classification especially challenging and interesting. More information about all data sets can be found at table 5. We have kept the pre-processing of the documents to a minimum. Thus, we have lower-cased all words and removed all punctuation but we have not removed stop words or applied stemming. We have also refrained from removing words with low or high occurrence. Additionally, for the BLOGS06 data set, we have removed all html formatting. We utilize the implementation of a support vector classifier from the LIBLINEAR library (Fan et al., 2008). We use a linear kernel and default parameters. All results are based on leave-one out cross validation accuracy. The reason for this choice of cross-validation setting, instead of the most standard ten-fold, is that all of the proposed approaches that use some form of idf utilize the training documents for extracting document frequency statistics, therefore more information is available to them in this experimental setting. Because of the high number of possible combinations between tf and idf variants (6·9·2 = 108) and due to space constraints we only present results from a subset of the mo</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
<author>Philip Resnik</author>
</authors>
<title>More than words: Syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>503--511</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="3188" citStr="Greene and Resnik, 2009" startWordPosition="489" endWordPosition="492">. (2002) in sentiment analysis showed that a binary unigrambased representation of documents, according to which a document is modeled only by the presence or absence of words, provides the best baseline classification accuracy in sentiment analysis in comparison to other more intricate representations using bigrams, adjectives, etc. Later research has focused on extending the document representation with more complex features such as structural or syntactic information (Wilson et al., 2005), favorability measures from diverse sources (Mullen and Collier, 2004), implicit syntactic indicators (Greene and Resnik, 2009), stylistic and syntactic feature selection (Abbasi et al., 2008), “annotator rationales” (Zaidan et al., 2007) and others, but no systematic study has been presented exploring the benefits of employing more sophisticated models for assigning weights to word features. In this paper, we examine whether term weighting functions adopted from Information Retrieval (IR) based on the standard tf.idf formula and adapted to the particular setting of sentiment analysis can help classification accuracy. We demonstrate that variants of the original tf.idf weighting scheme provide significant increases in</context>
</contexts>
<marker>Greene, Resnik, 2009</marker>
<rawString>Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 503–511, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck Jones</author>
<author>S Walker</author>
<author>S E Robertson</author>
</authors>
<title>A probabilistic model of information retrieval: development and comparative experiments.</title>
<date>2000</date>
<journal>Inf. Process. Manage.,</journal>
<volume>36</volume>
<issue>6</issue>
<contexts>
<context position="12297" citStr="Jones et al., 2000" startWordPosition="1951" endWordPosition="1954">idence of class preference. The utilization of idf in information retrieval is based on its ability to distinguish between content-bearing words (words with some semantical meaning) and simple function words, but this behavior is at least ambiguous in classification. Table 1: SMART notation for term frequency variants. maxt(tf) is the maximum frequency of any term in the document and avg dl is the average number of terms in all the documents. For ease of reference, we also include the BM25 tf scheme. The k1 and b parameters of BM25 are set to their default values of 1.2 and 0.95 respectively (Jones et al., 2000). Notation Term frequency n (natural) tf l (logarithm) 1 + log(tf) a (augmented) 0.5·tf 0.5 + maxt (t f) b (boolean) 1, t f &gt; 0 � 0, otherwise L (log ave) 1+log(tf) 1+log(avg dl) o (BM25) (k1+1)·tf k1 −b)+b·+tf ((1avg dl) 3.2 Delta tf.idf Martineau and Finin (2009) provide a solution to the above issue of idf utilization in a classification scenario by localizing the estimation of idf to the documents of one or the other class and subtracting the two values. Therefore, the weight of term 1388 Table 2: SMART notation for inverse document frequency variants. For ease of reference we also include</context>
</contexts>
<marker>Jones, Walker, Robertson, 2000</marker>
<rawString>K. Sparck Jones, S. Walker, and S. E. Robertson. 2000. A probabilistic model of information retrieval: development and comparative experiments. Inf. Process. Manage., 36(6):779–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenghua Lin</author>
<author>Yulan He</author>
</authors>
<title>Joint sentiment/topic model for sentiment analysis.</title>
<date>2009</date>
<booktitle>In CIKM ’09: Proceeding of the 18th ACM conference on Information and knowledge management,</booktitle>
<pages>375--384</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2132" citStr="Lin and He, 2009" startWordPosition="318" endWordPosition="321">wn as opinion mining, provides mechanisms and techniques through which this vast amount of information can be processed and harnessed. Research in the field has mainly, but not exclusively, focused in two subproblems: detecting whether a segment of text, either a whole document or a sentence, is subjective or objective, i.e. contains an expression of opinion, and detecting the overall polarity of the text, i.e. positive or negative. Most of the work in sentiment analysis has focused on supervised learning techniques (Sebastiani, 2002), although there are some notable exceptions (Turney, 2002; Lin and He, 2009). Previous research has shown that in general the performance of the former tend to be superior to that of the latter (Mullen and Collier, 2004; Lin and He, 2009). One of the main issues for supervised approaches has been the representation of documents. Usually a bag of words representation is adopted, according to which a document is modeled as an unordered collection of the words that it contains. Early research by Pang et al. (2002) in sentiment analysis showed that a binary unigrambased representation of documents, according to which a document is modeled only by the presence or absence o</context>
<context position="20581" citStr="Lin and He, 2009" startWordPosition="3369" endWordPosition="3372"> of differences in training/testing data splits the results are not directly comparable. It is produced here only for reference. Approach Acc. SVM with unigrams &amp; binary 87.15% weights (Pang et al., 2002), reported at (Pang and Lee, 2004) Hybrid SVM with Turney/Osgood 86% Lemmas (Mullen and Collier, 2004) SVM with min-cuts (Pang and Lee, 87.2% 2004) SVM with appraisal groups 90.2% (Whitelaw et al., 2005) SVM with log likehood ratio feature 90.45% selection (Aue and Gamon, 2005) SVM with annotator rationales 92.2% (Zaidan et al., 2007) LDA with filtered lexicon, subjectiv- 84.6% ity detection (Lin and He, 2009) The approach is straightforward, intuitive, computationally efficient, doesn’t require additional human effort and takes into consideration standardized and tested notions from IR. The results presented in section 5 show that a number (k1 + 1) · tfi K+tfi · N2 − dfi,2 + 0.5 log( dfi,2 + 0.5 ) wi = · log( (k1 + 1) · tfi N1 − dfi,1 + 0.5 K + tfi dfi,1 + 0.5 ) 1390 of weighting functions solidly outperform other state-of-the-art approaches. In the next section, we present the corpora that were used to study the effectiveness of different weighting schemes. 4 Experimental setup We have experiment</context>
</contexts>
<marker>Lin, He, 2009</marker>
<rawString>Chenghua Lin and Yulan He. 2009. Joint sentiment/topic model for sentiment analysis. In CIKM ’09: Proceeding of the 18th ACM conference on Information and knowledge management, pages 375– 384, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Alexander Hauptmann</author>
</authors>
<title>Which side are you on? identifying perspectives at the document and sentence levels.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="4985" citStr="Lin et al., 2006" startWordPosition="771" endWordPosition="774">vide a brief overview of the original tf.idf weighting scheme along with a number of variants and show how they can be applied to a classification scenario. Section 4 describes the corpora that were used to test the proposed weighting schemes and section 5 discusses the results. Finally, we conclude and propose future work in section 6. 2 Prior Work Sentiment analysis has been a popular research topic in recent years. Most of the work has focused on analyzing the content of movie or general product reviews, but there are also applications to other domains such as debates (Thomas et al., 2006; Lin et al., 2006), news (Devitt and Ahmad, 2007) and blogs (Ounis et al., 2008; Mishne, 2005). The book of Pang and Lee (2008) presents a thorough overview of the research in the field. This section presents the most relevant work. Pang et al. (2002) conducted early polarity classification of reviews using supervised approaches. They employed Support Vector Machines (SVMs), Naive Bayes and Maximum Entropy classifiers using a diverse set of features, such as unigrams, bigrams, binary and term frequency feature weights and others. They concluded that sentiment classification is more difficult that standard topic</context>
</contexts>
<marker>Lin, Wilson, Wiebe, Hauptmann, 2006</marker>
<rawString>Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexander Hauptmann. 2006. Which side are you on? identifying perspectives at the document and sentence levels. In Proceedings of the Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Liu</author>
</authors>
<title>MontyLingua: An end-to-end natural language processor with common sense.</title>
<date>2004</date>
<tech>Technical report, MIT.</tech>
<contexts>
<context position="8448" citStr="Liu, 2004" startWordPosition="1309" endWordPosition="1310">man annotators. By deleting rationale text spans from the original documents they created several contrast documents and constrained the SVM classifier to classify them less confidently than the originals. Using the largest training set size, their approach significantly increased the accuracy on a standard data set (see section 4). Prabowo and Thelwall (2009) proposed a hybrid classification process by combining in sequence several ruled-based classifiers with a SVM classifier. The former were based on the General Inquirer lexicon (Wilson et al., 2005), the MontyLingua part-of-speech tagger (Liu, 2004) and co-occurrence statistics of words with a set of predefined reference words. Their experiments showed that combining multiple classifiers can result in better effectiveness than any individual classifier, especially when sufficient training data isn’t available. In contrast to machine learning approaches that require labeled corpora for training, Lin and 1387 He (2009) proposed an unsupervised probabilistic modeling framework, based on Latent Dirichlet Allocation (LDA). The approach assumes that documents are a mixture of topics, i.e. probability distribution of words, according to which e</context>
</contexts>
<marker>Liu, 2004</marker>
<rawString>Hugo Liu. 2004. MontyLingua: An end-to-end natural language processor with common sense. Technical report, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Macdonald</author>
<author>I Ounis</author>
</authors>
<title>The trec blogs06 collection : Creating and analysing a blog test collection.</title>
<date>2006</date>
<tech>DCS Technical Report Series.</tech>
<contexts>
<context position="22676" citStr="Macdonald and Ounis, 2006" startWordPosition="3721" endWordPosition="3724">entiment data set (MDSD) by Blitzer et al. (2007) contains Amazon reviews for four different product types: books, electronics, DVDs and kitchen appliances. Reviews with ratings of 3 or higher, on a 5-scale system, were labeled as positive and reviews with a rating less than 3 as negative. The data set contains 1,000 positive and 1,000 negative reviews for each product category for a total of 8,000 reviews. Typically, the data set is used for domain adaptation applications but in our setting we only split the reviews between positive and negative5. Lastly, we present results from the BLOGS06 (Macdonald and Ounis, 2006) collection that is comprised of an uncompressed 148GB crawl of approximately 100,000 blogs and their respective RSS feeds. The collection has been used for 3 consecutive years by the Text REtrieval Conferences (TREC)6. Participants of the conference are provided with the task of finding documents (i.e. web pages) expressing an opinion about specific enti3http://www.imdb.com 4The dataset can be found at: http://www.cs.cornell.edu/ People/pabo/movie-review-data/review polarity.tar.gz. 5The data set can be found at http://www.cs.jhu.edu/ mdredze/datasets/sentiment/ 6http://www.trec.nist.gov ties</context>
</contexts>
<marker>Macdonald, Ounis, 2006</marker>
<rawString>C. Macdonald and I. Ounis. 2006. The trec blogs06 collection : Creating and analysing a blog test collection. DCS Technical Report Series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<volume>1</volume>
<pages>edition,</pages>
<publisher>Cambridge University Press,</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, 1 edition, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Martin</author>
<author>P R R White</author>
</authors>
<title>The language of evaluation :</title>
<date>2005</date>
<publisher>Palgrave Macmillan,</publisher>
<location>Basingstoke :.</location>
<note>appraisal in English</note>
<marker>Martin, White, 2005</marker>
<rawString>J. R. Martin and P. R. R. White. 2005. The language of evaluation : appraisal in English / J.R. Martin and P.R.R. White. Palgrave Macmillan, Basingstoke :.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Martineau</author>
<author>Tim Finin</author>
</authors>
<title>Delta TFIDF: An Improved Feature Space for Sentiment Analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third AAAI Internatonal Conference on Weblogs and Social Media,</booktitle>
<publisher>AAAI Press.</publisher>
<location>San Jose, CA,</location>
<note>(poster paper).</note>
<contexts>
<context position="12562" citStr="Martineau and Finin (2009)" startWordPosition="2000" endWordPosition="2003">fication. Table 1: SMART notation for term frequency variants. maxt(tf) is the maximum frequency of any term in the document and avg dl is the average number of terms in all the documents. For ease of reference, we also include the BM25 tf scheme. The k1 and b parameters of BM25 are set to their default values of 1.2 and 0.95 respectively (Jones et al., 2000). Notation Term frequency n (natural) tf l (logarithm) 1 + log(tf) a (augmented) 0.5·tf 0.5 + maxt (t f) b (boolean) 1, t f &gt; 0 � 0, otherwise L (log ave) 1+log(tf) 1+log(avg dl) o (BM25) (k1+1)·tf k1 −b)+b·+tf ((1avg dl) 3.2 Delta tf.idf Martineau and Finin (2009) provide a solution to the above issue of idf utilization in a classification scenario by localizing the estimation of idf to the documents of one or the other class and subtracting the two values. Therefore, the weight of term 1388 Table 2: SMART notation for inverse document frequency variants. For ease of reference we also include the BM25 idf factor and also present the extensions of the original formulations with their A variants. Notation Inverse Document Frequency n (no) 1 t (idf) log df N p (prob idf) log N−df df k (BM25 idf) logN−df+0.5 df+0.5 A(t) (Delta idf) log N1·df2 N2·df1 0(t′) </context>
<context position="19396" citStr="Martineau and Finin (2009)" startWordPosition="3184" endWordPosition="3187">rmulation would imply (see table 2). The above variation was made for two reasons: firstly, when the dfi’s are larger than 1 then the smoothing factor influences the final idf value only in a minor way in the revised formulation, since it is added only after the multiplication of the dfi with Ni (or its variation). Secondly, when dfi = 0, then the smoothing factor correctly adds only a small mass, avoiding a potential division by zero, where otherwise it would add a much greater mass, because it would be multiplied by Ni. According to this annotation scheme therefore, the original approach by Martineau and Finin (2009) can be represented as nA(t)n. We hypothesize that the utilization of sophisticated term weighting functions that have proved effective in information retrieval, thus providing an indication that they appropriately model the distinctive power of terms to documents and the smoothed, localized estimation of idf values will prove beneficial in sentiment classification. Table 4: Reported accuracies on the Movie Review data set. Only the best reported accuracy for each approach is presented, measured by 10-fold cross validation. The list is not exhaustive and because of differences in training/test</context>
<context position="28510" citStr="Martineau and Finin (2009)" startWordPosition="4645" endWordPosition="4648">moothing plays a role, e.g. n0(t)c8 (91.60%) vs. n0(t′)n (95.80%) and a0(p)c (92.80%) vs. a0(p′)n (96.55%), since we can expect zero class-based estimations of idf values, supporting our initial hypothesis on its importance. Additionally, using augmented, BM25 or binary tf weights is always better than raw term frequency, providing further support on the advantages of using sublinear tf weighting functions9. In this setting, the best accuracy of 96.90% is attained using BM25 tf weights with the BM25 delta idf variant, although binary or augmented tf weights using 8The original Delta tf.idf by Martineau and Finin (2009) has a limitation of utilizing features with df &gt; 2. In our experiments it performed similarly to nA(t)n (90.60%) but still lower than the cosine normalized variant nA(t)c included in the graph (91.60%). 9Although not present in the graph, for completeness reasons it should be noted that lA(s)n and LA(s)n also perform very well, both reaching accuracies of approx. 96%. 1392 Figure 2: Reported accuracy on the Multi-Domain Sentiment data set. delta idf perform similarly (96.50% and 96.60% respectively). The results indicate that the tf and the idf factor themselves aren’t of significant importan</context>
</contexts>
<marker>Martineau, Finin, 2009</marker>
<rawString>Justin Martineau and Tim Finin. 2009. Delta TFIDF: An Improved Feature Space for Sentiment Analysis. In Proceedings of the Third AAAI Internatonal Conference on Weblogs and Social Media, San Jose, CA, May. AAAI Press. (poster paper).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mccallum</author>
<author>K Nigam</author>
</authors>
<title>A comparison of event models for naive bayes text classification.</title>
<date>1998</date>
<contexts>
<context position="10587" citStr="Mccallum and Nigam, 1998" startWordPosition="1659" endWordPosition="1662">recent years, the best accuracy without using complex features (Mullen and Collier, 2004; Whitelaw et al., 2005) or additional human annotations (Zaidan et al., 2007) is achieved by employing a binary weighting scheme (Pang et al., 2002), where wz = 1, if tfz &gt; 0 and wz = 0, if tfz = 0, where tfz is the number of times that term i appears in document D (henceforth raw term frequency) and utilizing a SVM classifier. It is of particular interest that using tfz in the document representation usually results in decreased accuracy, a result that appears to be in contrast with topic classification (Mccallum and Nigam, 1998; Pang et al., 2002). In this paper, we also utilize SVMs but our study is centered on whether more sophisticated than binary or raw term frequency weighting functions can improve classification accuracy. We base our approach on the classic tf.idf weighting scheme from Information Retrieval (IR) and adapt it to the domain of sentiment classification. 3.1 The classic tf.idf weighting schemes The classic tf.idf formula assigns weight wz to term i in document D as: wz = tfz · idfz = tfz · log fZ (1) where tfz is the number of times term i occurs in D, idfz is the inverse document frequency of ter</context>
</contexts>
<marker>Mccallum, Nigam, 1998</marker>
<rawString>A. Mccallum and K. Nigam. 1998. A comparison of event models for naive bayes text classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mishne</author>
</authors>
<title>Experiments with mood classification in blog posts.</title>
<date>2005</date>
<booktitle>In 1st Workshop on Stylistic Analysis Of Text For Information Access.</booktitle>
<contexts>
<context position="5061" citStr="Mishne, 2005" startWordPosition="787" endWordPosition="788">r of variants and show how they can be applied to a classification scenario. Section 4 describes the corpora that were used to test the proposed weighting schemes and section 5 discusses the results. Finally, we conclude and propose future work in section 6. 2 Prior Work Sentiment analysis has been a popular research topic in recent years. Most of the work has focused on analyzing the content of movie or general product reviews, but there are also applications to other domains such as debates (Thomas et al., 2006; Lin et al., 2006), news (Devitt and Ahmad, 2007) and blogs (Ounis et al., 2008; Mishne, 2005). The book of Pang and Lee (2008) presents a thorough overview of the research in the field. This section presents the most relevant work. Pang et al. (2002) conducted early polarity classification of reviews using supervised approaches. They employed Support Vector Machines (SVMs), Naive Bayes and Maximum Entropy classifiers using a diverse set of features, such as unigrams, bigrams, binary and term frequency feature weights and others. They concluded that sentiment classification is more difficult that standard topic-based classification and that using a SVM classifier with binary unigrambas</context>
</contexts>
<marker>Mishne, 2005</marker>
<rawString>G. Mishne. 2005. Experiments with mood classification in blog posts. In 1st Workshop on Stylistic Analysis Of Text For Information Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Mullen</author>
<author>Nigel Collier</author>
</authors>
<title>Sentiment analysis using support vector machines with diverse information sources.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>412--418</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2275" citStr="Mullen and Collier, 2004" startWordPosition="345" endWordPosition="348">esearch in the field has mainly, but not exclusively, focused in two subproblems: detecting whether a segment of text, either a whole document or a sentence, is subjective or objective, i.e. contains an expression of opinion, and detecting the overall polarity of the text, i.e. positive or negative. Most of the work in sentiment analysis has focused on supervised learning techniques (Sebastiani, 2002), although there are some notable exceptions (Turney, 2002; Lin and He, 2009). Previous research has shown that in general the performance of the former tend to be superior to that of the latter (Mullen and Collier, 2004; Lin and He, 2009). One of the main issues for supervised approaches has been the representation of documents. Usually a bag of words representation is adopted, according to which a document is modeled as an unordered collection of the words that it contains. Early research by Pang et al. (2002) in sentiment analysis showed that a binary unigrambased representation of documents, according to which a document is modeled only by the presence or absence of words, provides the best baseline classification accuracy in sentiment analysis in comparison to other more intricate representations using b</context>
<context position="6592" citStr="Mullen and Collier (2004)" startWordPosition="1022" endWordPosition="1025">bjectivity or objectivity class. Documents were represented as graphs with sentences as nodes and association scores between them as edges. Two additional nodes represented the subjective and objective poles. The weights between the nodes were calculated using three different, heuristic decaying functions. Finding apartition that minimized a cost function separated the objective from the subjective sentences. They reported a statistically significant improvement over a Naive Bayes baseline using the whole text but only slight increase compared to using a SVM classifier on the entire document. Mullen and Collier (2004) used SVMs and expanded the feature set for representing documents with favorability measures from a variety of diverse sources. They introduced features based on Osgood’s Theory of Semantic Differentiation (Osgood, 1967) using WordNet to derive the values of potency, activity and evaluative of adjectives and Turney’s semantic orientation (Turney, 2002). Their results showed that using a hybrid SVM classifier, that uses as features the distance of documents from the separating hyperplane, with all the above features produces the best results. Whitelaw et al. (2005) added fine-grained semantic </context>
<context position="10051" citStr="Mullen and Collier, 2004" startWordPosition="1562" endWordPosition="1565">ch. 3 A study of non-binary weights We use the terms “features”, “words” and “terms” interchangeably in this paper, since we mainly focus on unigrams. The approach nonetheless can easily be extended to higher order n-grams. Each document D therefore is represented as a bag-ofwords feature vector: D = {w1, w2, ..., w|V |} where |V |is the size of the vocabulary (i.e. the number of unique words) and wz, i = 1, ... , |V | is the weight of term i in document D. Despite the significant attention that sentiment analysis has received in recent years, the best accuracy without using complex features (Mullen and Collier, 2004; Whitelaw et al., 2005) or additional human annotations (Zaidan et al., 2007) is achieved by employing a binary weighting scheme (Pang et al., 2002), where wz = 1, if tfz &gt; 0 and wz = 0, if tfz = 0, where tfz is the number of times that term i appears in document D (henceforth raw term frequency) and utilizing a SVM classifier. It is of particular interest that using tfz in the document representation usually results in decreased accuracy, a result that appears to be in contrast with topic classification (Mccallum and Nigam, 1998; Pang et al., 2002). In this paper, we also utilize SVMs but ou</context>
<context position="20270" citStr="Mullen and Collier, 2004" startWordPosition="3319" endWordPosition="3322">ms to documents and the smoothed, localized estimation of idf values will prove beneficial in sentiment classification. Table 4: Reported accuracies on the Movie Review data set. Only the best reported accuracy for each approach is presented, measured by 10-fold cross validation. The list is not exhaustive and because of differences in training/testing data splits the results are not directly comparable. It is produced here only for reference. Approach Acc. SVM with unigrams &amp; binary 87.15% weights (Pang et al., 2002), reported at (Pang and Lee, 2004) Hybrid SVM with Turney/Osgood 86% Lemmas (Mullen and Collier, 2004) SVM with min-cuts (Pang and Lee, 87.2% 2004) SVM with appraisal groups 90.2% (Whitelaw et al., 2005) SVM with log likehood ratio feature 90.45% selection (Aue and Gamon, 2005) SVM with annotator rationales 92.2% (Zaidan et al., 2007) LDA with filtered lexicon, subjectiv- 84.6% ity detection (Lin and He, 2009) The approach is straightforward, intuitive, computationally efficient, doesn’t require additional human effort and takes into consideration standardized and tested notions from IR. The results presented in section 5 show that a number (k1 + 1) · tfi K+tfi · N2 − dfi,2 + 0.5 log( dfi,2 + </context>
</contexts>
<marker>Mullen, Collier, 2004</marker>
<rawString>Tony Mullen and Nigel Collier. 2004. Sentiment analysis using support vector machines with diverse information sources. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 412– 418, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles E Osgood</author>
</authors>
<title>The measurement of meaning /</title>
<date>1967</date>
<editor>[by] [Charles E. Osgood, George J. Suci [and] Percy H. Tannenbaum].</editor>
<publisher>University of Illinois Press,</publisher>
<location>Urbana:, 2nd</location>
<note>ed. edition.</note>
<contexts>
<context position="6813" citStr="Osgood, 1967" startWordPosition="1057" endWordPosition="1059">nodes were calculated using three different, heuristic decaying functions. Finding apartition that minimized a cost function separated the objective from the subjective sentences. They reported a statistically significant improvement over a Naive Bayes baseline using the whole text but only slight increase compared to using a SVM classifier on the entire document. Mullen and Collier (2004) used SVMs and expanded the feature set for representing documents with favorability measures from a variety of diverse sources. They introduced features based on Osgood’s Theory of Semantic Differentiation (Osgood, 1967) using WordNet to derive the values of potency, activity and evaluative of adjectives and Turney’s semantic orientation (Turney, 2002). Their results showed that using a hybrid SVM classifier, that uses as features the distance of documents from the separating hyperplane, with all the above features produces the best results. Whitelaw et al. (2005) added fine-grained semantic distinctions in the feature set. Their approach was based on a lexicon created in a semisupervised fashion and then manually refined It consists of 1329 adjectives and their modifiers categorized under several taxonomies </context>
</contexts>
<marker>Osgood, 1967</marker>
<rawString>Charles E. Osgood. 1967. The measurement of meaning / [by] [Charles E. Osgood, George J. Suci [and] Percy H. Tannenbaum]. University of Illinois Press, Urbana:, 2nd ed. edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iadh Ounis</author>
<author>Craig Macdonald</author>
<author>Ian Soboroff</author>
</authors>
<title>Overview of the trec-2008 blog trac.</title>
<date>2008</date>
<booktitle>In The Seventeenth Text REtrieval Conference (TREC 2008) Proceedings. NIST.</booktitle>
<contexts>
<context position="5046" citStr="Ounis et al., 2008" startWordPosition="783" endWordPosition="786">e along with a number of variants and show how they can be applied to a classification scenario. Section 4 describes the corpora that were used to test the proposed weighting schemes and section 5 discusses the results. Finally, we conclude and propose future work in section 6. 2 Prior Work Sentiment analysis has been a popular research topic in recent years. Most of the work has focused on analyzing the content of movie or general product reviews, but there are also applications to other domains such as debates (Thomas et al., 2006; Lin et al., 2006), news (Devitt and Ahmad, 2007) and blogs (Ounis et al., 2008; Mishne, 2005). The book of Pang and Lee (2008) presents a thorough overview of the research in the field. This section presents the most relevant work. Pang et al. (2002) conducted early polarity classification of reviews using supervised approaches. They employed Support Vector Machines (SVMs), Naive Bayes and Maximum Entropy classifiers using a diverse set of features, such as unigrams, bigrams, binary and term frequency feature weights and others. They concluded that sentiment classification is more difficult that standard topic-based classification and that using a SVM classifier with bi</context>
</contexts>
<marker>Ounis, Macdonald, Soboroff, 2008</marker>
<rawString>Iadh Ounis, Craig Macdonald, and Ian Soboroff. 2008. Overview of the trec-2008 blog trac. In The Seventeenth Text REtrieval Conference (TREC 2008) Proceedings. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="5867" citStr="Pang and Lee, 2004" startWordPosition="913" endWordPosition="916">sification of reviews using supervised approaches. They employed Support Vector Machines (SVMs), Naive Bayes and Maximum Entropy classifiers using a diverse set of features, such as unigrams, bigrams, binary and term frequency feature weights and others. They concluded that sentiment classification is more difficult that standard topic-based classification and that using a SVM classifier with binary unigrambased features produces the best results. A subsequent innovation was the detection and removal of the objective parts of documents and the application of a polarity classifier on the rest (Pang and Lee, 2004). This exploited text coherence with adjacent text spans which were assumed to belong to the same subjectivity or objectivity class. Documents were represented as graphs with sentences as nodes and association scores between them as edges. Two additional nodes represented the subjective and objective poles. The weights between the nodes were calculated using three different, heuristic decaying functions. Finding apartition that minimized a cost function separated the objective from the subjective sentences. They reported a statistically significant improvement over a Naive Bayes baseline using</context>
<context position="9364" citStr="Pang and Lee (2004)" startWordPosition="1441" endWordPosition="1444">g approaches that require labeled corpora for training, Lin and 1387 He (2009) proposed an unsupervised probabilistic modeling framework, based on Latent Dirichlet Allocation (LDA). The approach assumes that documents are a mixture of topics, i.e. probability distribution of words, according to which each document is generated through an hierarchical process and adds an extra sentiment layer to accommodate the opinionated nature (positive or negative) of the document. Their best attained performance, using a filtered subjectivity lexicon and removing objective sentences in a manner similar to Pang and Lee (2004), is only slightly lower than that of a fully-supervised approach. 3 A study of non-binary weights We use the terms “features”, “words” and “terms” interchangeably in this paper, since we mainly focus on unigrams. The approach nonetheless can easily be extended to higher order n-grams. Each document D therefore is represented as a bag-ofwords feature vector: D = {w1, w2, ..., w|V |} where |V |is the size of the vocabulary (i.e. the number of unique words) and wz, i = 1, ... , |V | is the weight of term i in document D. Despite the significant attention that sentiment analysis has received in r</context>
<context position="20202" citStr="Pang and Lee, 2004" startWordPosition="3309" endWordPosition="3312">ion that they appropriately model the distinctive power of terms to documents and the smoothed, localized estimation of idf values will prove beneficial in sentiment classification. Table 4: Reported accuracies on the Movie Review data set. Only the best reported accuracy for each approach is presented, measured by 10-fold cross validation. The list is not exhaustive and because of differences in training/testing data splits the results are not directly comparable. It is produced here only for reference. Approach Acc. SVM with unigrams &amp; binary 87.15% weights (Pang et al., 2002), reported at (Pang and Lee, 2004) Hybrid SVM with Turney/Osgood 86% Lemmas (Mullen and Collier, 2004) SVM with min-cuts (Pang and Lee, 87.2% 2004) SVM with appraisal groups 90.2% (Whitelaw et al., 2005) SVM with log likehood ratio feature 90.45% selection (Aue and Gamon, 2005) SVM with annotator rationales 92.2% (Zaidan et al., 2007) LDA with filtered lexicon, subjectiv- 84.6% ity detection (Lin and He, 2009) The approach is straightforward, intuitive, computationally efficient, doesn’t require additional human effort and takes into consideration standardized and tested notions from IR. The results presented in section 5 show</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In In Proceedings of the ACL, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis.</title>
<date>2008</date>
<publisher>Now Publishers Inc.</publisher>
<contexts>
<context position="5094" citStr="Pang and Lee (2008)" startWordPosition="792" endWordPosition="795">they can be applied to a classification scenario. Section 4 describes the corpora that were used to test the proposed weighting schemes and section 5 discusses the results. Finally, we conclude and propose future work in section 6. 2 Prior Work Sentiment analysis has been a popular research topic in recent years. Most of the work has focused on analyzing the content of movie or general product reviews, but there are also applications to other domains such as debates (Thomas et al., 2006; Lin et al., 2006), news (Devitt and Ahmad, 2007) and blogs (Ounis et al., 2008; Mishne, 2005). The book of Pang and Lee (2008) presents a thorough overview of the research in the field. This section presents the most relevant work. Pang et al. (2002) conducted early polarity classification of reviews using supervised approaches. They employed Support Vector Machines (SVMs), Naive Bayes and Maximum Entropy classifiers using a diverse set of features, such as unigrams, bigrams, binary and term frequency feature weights and others. They concluded that sentiment classification is more difficult that standard topic-based classification and that using a SVM classifier with binary unigrambased features produces the best res</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>B. Pang and L. Lee. 2008. Opinion Mining and Sentiment Analysis. Now Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2572" citStr="Pang et al. (2002)" startWordPosition="397" endWordPosition="400">st of the work in sentiment analysis has focused on supervised learning techniques (Sebastiani, 2002), although there are some notable exceptions (Turney, 2002; Lin and He, 2009). Previous research has shown that in general the performance of the former tend to be superior to that of the latter (Mullen and Collier, 2004; Lin and He, 2009). One of the main issues for supervised approaches has been the representation of documents. Usually a bag of words representation is adopted, according to which a document is modeled as an unordered collection of the words that it contains. Early research by Pang et al. (2002) in sentiment analysis showed that a binary unigrambased representation of documents, according to which a document is modeled only by the presence or absence of words, provides the best baseline classification accuracy in sentiment analysis in comparison to other more intricate representations using bigrams, adjectives, etc. Later research has focused on extending the document representation with more complex features such as structural or syntactic information (Wilson et al., 2005), favorability measures from diverse sources (Mullen and Collier, 2004), implicit syntactic indicators (Greene a</context>
<context position="5218" citStr="Pang et al. (2002)" startWordPosition="813" endWordPosition="816">ng schemes and section 5 discusses the results. Finally, we conclude and propose future work in section 6. 2 Prior Work Sentiment analysis has been a popular research topic in recent years. Most of the work has focused on analyzing the content of movie or general product reviews, but there are also applications to other domains such as debates (Thomas et al., 2006; Lin et al., 2006), news (Devitt and Ahmad, 2007) and blogs (Ounis et al., 2008; Mishne, 2005). The book of Pang and Lee (2008) presents a thorough overview of the research in the field. This section presents the most relevant work. Pang et al. (2002) conducted early polarity classification of reviews using supervised approaches. They employed Support Vector Machines (SVMs), Naive Bayes and Maximum Entropy classifiers using a diverse set of features, such as unigrams, bigrams, binary and term frequency feature weights and others. They concluded that sentiment classification is more difficult that standard topic-based classification and that using a SVM classifier with binary unigrambased features produces the best results. A subsequent innovation was the detection and removal of the objective parts of documents and the application of a pol</context>
<context position="10200" citStr="Pang et al., 2002" startWordPosition="1587" endWordPosition="1590">approach nonetheless can easily be extended to higher order n-grams. Each document D therefore is represented as a bag-ofwords feature vector: D = {w1, w2, ..., w|V |} where |V |is the size of the vocabulary (i.e. the number of unique words) and wz, i = 1, ... , |V | is the weight of term i in document D. Despite the significant attention that sentiment analysis has received in recent years, the best accuracy without using complex features (Mullen and Collier, 2004; Whitelaw et al., 2005) or additional human annotations (Zaidan et al., 2007) is achieved by employing a binary weighting scheme (Pang et al., 2002), where wz = 1, if tfz &gt; 0 and wz = 0, if tfz = 0, where tfz is the number of times that term i appears in document D (henceforth raw term frequency) and utilizing a SVM classifier. It is of particular interest that using tfz in the document representation usually results in decreased accuracy, a result that appears to be in contrast with topic classification (Mccallum and Nigam, 1998; Pang et al., 2002). In this paper, we also utilize SVMs but our study is centered on whether more sophisticated than binary or raw term frequency weighting functions can improve classification accuracy. We base </context>
<context position="20168" citStr="Pang et al., 2002" startWordPosition="3303" endWordPosition="3306">rieval, thus providing an indication that they appropriately model the distinctive power of terms to documents and the smoothed, localized estimation of idf values will prove beneficial in sentiment classification. Table 4: Reported accuracies on the Movie Review data set. Only the best reported accuracy for each approach is presented, measured by 10-fold cross validation. The list is not exhaustive and because of differences in training/testing data splits the results are not directly comparable. It is produced here only for reference. Approach Acc. SVM with unigrams &amp; binary 87.15% weights (Pang et al., 2002), reported at (Pang and Lee, 2004) Hybrid SVM with Turney/Osgood 86% Lemmas (Mullen and Collier, 2004) SVM with min-cuts (Pang and Lee, 87.2% 2004) SVM with appraisal groups 90.2% (Whitelaw et al., 2005) SVM with log likehood ratio feature 90.45% selection (Aue and Gamon, 2005) SVM with annotator rationales 92.2% (Zaidan et al., 2007) LDA with filtered lexicon, subjectiv- 84.6% ity detection (Lin and He, 2009) The approach is straightforward, intuitive, computationally efficient, doesn’t require additional human effort and takes into consideration standardized and tested notions from IR. The r</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudy Prabowo</author>
<author>Mike Thelwall</author>
</authors>
<title>Sentiment analysis: A combined approach.</title>
<date>2009</date>
<journal>Journal of Informetrics,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="8200" citStr="Prabowo and Thelwall (2009)" startWordPosition="1269" endWordPosition="1272">ions as features to a Support Vector Machine classifier (Witten and Frank, 1999), resulting in significant increases in accuracy. Zaidan et al. (2007) introduced “annotator rationales”, i.e. words or phrases that explain the polarity of the document according to human annotators. By deleting rationale text spans from the original documents they created several contrast documents and constrained the SVM classifier to classify them less confidently than the originals. Using the largest training set size, their approach significantly increased the accuracy on a standard data set (see section 4). Prabowo and Thelwall (2009) proposed a hybrid classification process by combining in sequence several ruled-based classifiers with a SVM classifier. The former were based on the General Inquirer lexicon (Wilson et al., 2005), the MontyLingua part-of-speech tagger (Liu, 2004) and co-occurrence statistics of words with a set of predefined reference words. Their experiments showed that combining multiple classifiers can result in better effectiveness than any individual classifier, especially when sufficient training data isn’t available. In contrast to machine learning approaches that require labeled corpora for training,</context>
</contexts>
<marker>Prabowo, Thelwall, 2009</marker>
<rawString>Rudy Prabowo and Mike Thelwall. 2009. Sentiment analysis: A combined approach. Journal of Informetrics, 3(2):143–157, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Robertson</author>
<author>Steve Walker</author>
<author>Susan Jones</author>
<author>Micheline Hancock-Beaulieu</author>
<author>Mike Gatford</author>
</authors>
<title>Okapi at trec-3. In</title>
<date>1994</date>
<booktitle>TREC,</booktitle>
<pages>0</pages>
<contexts>
<context position="15803" citStr="Robertson et al., 1994" startWordPosition="2553" endWordPosition="2556">entation would be equivalent to SMART.bnn1 or more simply bnn, while a simple raw term frequency based would be notated as nnn or nnc with cosine normalization. Table 3: SMART normalization. Notation Normalization n (none) 1 c (cosine) 1 �w2+w22+...+wn Significant research has been done in IR on diverse weighting functions and not all versions of SMART notations are consistent (Manning et al., 2008). Zobel and Moffat (1998) provide an exhaustive study but in this paper, due to space constraints, we will follow the concise notation presented by Singhal et al. (1995). The BM25 weighting scheme (Robertson et al., 1994; Robertson et al., 1996) is a probabilistic model for information retrieval and is one of the most popular and effective algorithms used in information retrieval. For ease of reference, we incorporate the BM25 tf and idf factors into the SMART annotation scheme (last row of table 1 and 4th row of table 2), therefore the weight wi of term i in document D according to the BM25 scheme is notated as SMART.okn or okn. Most of the tf weighting functions in SMART and the BM25 model take into consideration the non-linearity of document relevance to term fre1Typically, a weighting function in the SMAR</context>
</contexts>
<marker>Robertson, Walker, Jones, Hancock-Beaulieu, Gatford, 1994</marker>
<rawString>Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at trec-3. In TREC, pages 0–.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
<author>S Jones</author>
<author>M M HancockBeaulieu</author>
<author>M Gatford</author>
</authors>
<title>Okapi at trec-2. In</title>
<date>1996</date>
<booktitle>In The Second Text REtrieval Conference (TREC2), NIST Special Special Publication 500-215,</booktitle>
<pages>21--34</pages>
<contexts>
<context position="15828" citStr="Robertson et al., 1996" startWordPosition="2557" endWordPosition="2560">lent to SMART.bnn1 or more simply bnn, while a simple raw term frequency based would be notated as nnn or nnc with cosine normalization. Table 3: SMART normalization. Notation Normalization n (none) 1 c (cosine) 1 �w2+w22+...+wn Significant research has been done in IR on diverse weighting functions and not all versions of SMART notations are consistent (Manning et al., 2008). Zobel and Moffat (1998) provide an exhaustive study but in this paper, due to space constraints, we will follow the concise notation presented by Singhal et al. (1995). The BM25 weighting scheme (Robertson et al., 1994; Robertson et al., 1996) is a probabilistic model for information retrieval and is one of the most popular and effective algorithms used in information retrieval. For ease of reference, we incorporate the BM25 tf and idf factors into the SMART annotation scheme (last row of table 1 and 4th row of table 2), therefore the weight wi of term i in document D according to the BM25 scheme is notated as SMART.okn or okn. Most of the tf weighting functions in SMART and the BM25 model take into consideration the non-linearity of document relevance to term fre1Typically, a weighting function in the SMART system is defined as a </context>
</contexts>
<marker>Robertson, Walker, Jones, HancockBeaulieu, Gatford, 1996</marker>
<rawString>S E Robertson, S Walker, S Jones, M M HancockBeaulieu, and M Gatford. 1996. Okapi at trec-2. In In The Second Text REtrieval Conference (TREC2), NIST Special Special Publication 500-215, pages 21–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Robertson</author>
<author>Hugo Zaragoza</author>
<author>Michael Taylor</author>
</authors>
<title>Simple bm25 extension to multiple weighted fields.</title>
<date>2004</date>
<booktitle>In CIKM ’04: Proceedings of the thirteenth ACM international conference on Information and knowledge management,</booktitle>
<pages>42--49</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="14064" citStr="Robertson et al. (2004)" startWordPosition="2254" endWordPosition="2257"> log2( N1 ) − tfi · log2( N2 ) dfi,1 dfi,2 = tfi · log2(N1 · dfi,2 ) (2) dfi,1 · N2 where Nj is the total number of training documents in class cj and dfi,j is the number of training documents in class cj that contain term i. The above weighting scheme was appropriately named Delta tf.idf. The produced results (Martineau and Finin, 2009) show that the approach produces better results than the simple tf or binary weighting scheme. Nonetheless, the approach doesn’t take into consideration a number of tested notions from IR, such as the non-linearity of term frequency to document relevancy (e.g. Robertson et al. (2004)) according to which, the probability of a document being relevant to a query term is typically sublinear in relation to the number of times a query term appears in the document. Additionally, their approach doesn’t provide any sort of smoothing for the dfi,j factor and is therefore susceptible to errors in corpora where a term occurs in documents of only one or the other class and therefore dfi,j = 0 . 3.3 SMART and BM25 tf.idf variants The SMART retrieval system by Salton (1971) is a retrieval system based on the vector space model (Salton and McGill, 1986). Salton and Buckley (1987) provide</context>
<context position="17304" citStr="Robertson et al. (2004)" startWordPosition="2812" endWordPosition="2815">le for the document representation in the second part, i.e. SMART.ddd, or more simply ddd. 1389 quency and thus employ tf factors that scale sublinearly in relation to term frequency. Additionally, the BM25 tf variant also incorporates a scaling for the length of the document, taking into consideration that longer documents will by definition have more term occurences2. Effective weighting functions is a very active research area in information retrieval and it is outside the scope of this paper to provide an in-depth analysis but significant research can be found in Salton and McGill (1986), Robertson et al. (2004), Manning et al. (2008) or Armstrong et al. (2009) for a more recent study. 3.4 Introducing SMART and BM25 Delta tf.idf variants We apply the idea of localizing the estimation of idf values to documents of one class but employ more sophisticated term weighting functions adapted from the SMART retrieval system and the BM25 probabilistic model. The resulting idf weighting functions are presented in the lower part of table 2. We extend the original SMART annotation scheme by adding Delta (A) variants of the original idf functions and additionally introduce smoothed Delta variants of the idf and t</context>
</contexts>
<marker>Robertson, Zaragoza, Taylor, 2004</marker>
<rawString>Stephen Robertson, Hugo Zaragoza, and Michael Taylor. 2004. Simple bm25 extension to multiple weighted fields. In CIKM ’04: Proceedings of the thirteenth ACM international conference on Information and knowledge management, pages 42–49, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Chris Buckley</author>
</authors>
<title>Term weighting approaches in automatic text retrieval.</title>
<date>1987</date>
<tech>Technical report,</tech>
<location>Ithaca, NY, USA.</location>
<contexts>
<context position="14656" citStr="Salton and Buckley (1987)" startWordPosition="2358" endWordPosition="2361">ncy (e.g. Robertson et al. (2004)) according to which, the probability of a document being relevant to a query term is typically sublinear in relation to the number of times a query term appears in the document. Additionally, their approach doesn’t provide any sort of smoothing for the dfi,j factor and is therefore susceptible to errors in corpora where a term occurs in documents of only one or the other class and therefore dfi,j = 0 . 3.3 SMART and BM25 tf.idf variants The SMART retrieval system by Salton (1971) is a retrieval system based on the vector space model (Salton and McGill, 1986). Salton and Buckley (1987) provide a number of variants of the tf.idf weighting approach and present the SMART notation scheme, according to which each weighting function is defined by triples of letters; the first one denotes the term frequency factor, the second one corresponds to the inverse document frequency function and the last one declares the normalization that is being applied. The upper rows of tables 1, 2 and 3 present the three most commonly used weighting functions for each factor respectively. For example, a binary document representation would be equivalent to SMART.bnn1 or more simply bnn, while a simp</context>
</contexts>
<marker>Salton, Buckley, 1987</marker>
<rawString>Gerard Salton and Chris Buckley. 1987. Term weighting approaches in automatic text retrieval. Technical report, Ithaca, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1986</date>
<publisher>McGrawHill, Inc.,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="14629" citStr="Salton and McGill, 1986" startWordPosition="2354" endWordPosition="2357">equency to document relevancy (e.g. Robertson et al. (2004)) according to which, the probability of a document being relevant to a query term is typically sublinear in relation to the number of times a query term appears in the document. Additionally, their approach doesn’t provide any sort of smoothing for the dfi,j factor and is therefore susceptible to errors in corpora where a term occurs in documents of only one or the other class and therefore dfi,j = 0 . 3.3 SMART and BM25 tf.idf variants The SMART retrieval system by Salton (1971) is a retrieval system based on the vector space model (Salton and McGill, 1986). Salton and Buckley (1987) provide a number of variants of the tf.idf weighting approach and present the SMART notation scheme, according to which each weighting function is defined by triples of letters; the first one denotes the term frequency factor, the second one corresponds to the inverse document frequency function and the last one declares the normalization that is being applied. The upper rows of tables 1, 2 and 3 present the three most commonly used weighting functions for each factor respectively. For example, a binary document representation would be equivalent to SMART.bnn1 or mo</context>
<context position="17279" citStr="Salton and McGill (1986)" startWordPosition="2808" endWordPosition="2811"> the first part and a triple for the document representation in the second part, i.e. SMART.ddd, or more simply ddd. 1389 quency and thus employ tf factors that scale sublinearly in relation to term frequency. Additionally, the BM25 tf variant also incorporates a scaling for the length of the document, taking into consideration that longer documents will by definition have more term occurences2. Effective weighting functions is a very active research area in information retrieval and it is outside the scope of this paper to provide an in-depth analysis but significant research can be found in Salton and McGill (1986), Robertson et al. (2004), Manning et al. (2008) or Armstrong et al. (2009) for a more recent study. 3.4 Introducing SMART and BM25 Delta tf.idf variants We apply the idea of localizing the estimation of idf values to documents of one class but employ more sophisticated term weighting functions adapted from the SMART retrieval system and the BM25 probabilistic model. The resulting idf weighting functions are presented in the lower part of table 2. We extend the original SMART annotation scheme by adding Delta (A) variants of the original idf functions and additionally introduce smoothed Delta </context>
</contexts>
<marker>Salton, McGill, 1986</marker>
<rawString>Gerard Salton and Michael J. McGill. 1986. Introduction to Modern Information Retrieval. McGrawHill, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<date>1971</date>
<booktitle>The SMART Retrieval System— Experiments in Automatic Document Processing.</booktitle>
<publisher>Prentice-Hall, Inc.,</publisher>
<location>Upper Saddle River, NJ, USA.</location>
<contexts>
<context position="14549" citStr="Salton (1971)" startWordPosition="2342" endWordPosition="2343">umber of tested notions from IR, such as the non-linearity of term frequency to document relevancy (e.g. Robertson et al. (2004)) according to which, the probability of a document being relevant to a query term is typically sublinear in relation to the number of times a query term appears in the document. Additionally, their approach doesn’t provide any sort of smoothing for the dfi,j factor and is therefore susceptible to errors in corpora where a term occurs in documents of only one or the other class and therefore dfi,j = 0 . 3.3 SMART and BM25 tf.idf variants The SMART retrieval system by Salton (1971) is a retrieval system based on the vector space model (Salton and McGill, 1986). Salton and Buckley (1987) provide a number of variants of the tf.idf weighting approach and present the SMART notation scheme, according to which each weighting function is defined by triples of letters; the first one denotes the term frequency factor, the second one corresponds to the inverse document frequency function and the last one declares the normalization that is being applied. The upper rows of tables 1, 2 and 3 present the three most commonly used weighting functions for each factor respectively. For e</context>
</contexts>
<marker>Salton, 1971</marker>
<rawString>G. Salton. 1971. The SMART Retrieval System— Experiments in Automatic Document Processing. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="2055" citStr="Sebastiani, 2002" startWordPosition="306" endWordPosition="308">can make or break a product in the marketplace”. Sentiment analysis, also known as opinion mining, provides mechanisms and techniques through which this vast amount of information can be processed and harnessed. Research in the field has mainly, but not exclusively, focused in two subproblems: detecting whether a segment of text, either a whole document or a sentence, is subjective or objective, i.e. contains an expression of opinion, and detecting the overall polarity of the text, i.e. positive or negative. Most of the work in sentiment analysis has focused on supervised learning techniques (Sebastiani, 2002), although there are some notable exceptions (Turney, 2002; Lin and He, 2009). Previous research has shown that in general the performance of the former tend to be superior to that of the latter (Mullen and Collier, 2004; Lin and He, 2009). One of the main issues for supervised approaches has been the representation of documents. Usually a bag of words representation is adopted, according to which a document is modeled as an unordered collection of the words that it contains. Early research by Pang et al. (2002) in sentiment analysis showed that a binary unigrambased representation of document</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1˜n47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Singhal</author>
<author>Gerard Salton</author>
<author>Chris Buckley</author>
</authors>
<title>Length normalization in degraded text collections.</title>
<date>1995</date>
<tech>Technical report,</tech>
<location>Ithaca, NY, USA.</location>
<contexts>
<context position="15752" citStr="Singhal et al. (1995)" startWordPosition="2545" endWordPosition="2548">espectively. For example, a binary document representation would be equivalent to SMART.bnn1 or more simply bnn, while a simple raw term frequency based would be notated as nnn or nnc with cosine normalization. Table 3: SMART normalization. Notation Normalization n (none) 1 c (cosine) 1 �w2+w22+...+wn Significant research has been done in IR on diverse weighting functions and not all versions of SMART notations are consistent (Manning et al., 2008). Zobel and Moffat (1998) provide an exhaustive study but in this paper, due to space constraints, we will follow the concise notation presented by Singhal et al. (1995). The BM25 weighting scheme (Robertson et al., 1994; Robertson et al., 1996) is a probabilistic model for information retrieval and is one of the most popular and effective algorithms used in information retrieval. For ease of reference, we incorporate the BM25 tf and idf factors into the SMART annotation scheme (last row of table 1 and 4th row of table 2), therefore the weight wi of term i in document D according to the BM25 scheme is notated as SMART.okn or okn. Most of the tf weighting functions in SMART and the BM25 model take into consideration the non-linearity of document relevance to t</context>
</contexts>
<marker>Singhal, Salton, Buckley, 1995</marker>
<rawString>Amit Singhal, Gerard Salton, and Chris Buckley. 1995. Length normalization in degraded text collections. Technical report, Ithaca, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from congressional floor-debate transcripts.</title>
<date>2006</date>
<location>CoRR, abs/cs/0607062.</location>
<contexts>
<context position="4966" citStr="Thomas et al., 2006" startWordPosition="767" endWordPosition="770">. In section 3 we provide a brief overview of the original tf.idf weighting scheme along with a number of variants and show how they can be applied to a classification scenario. Section 4 describes the corpora that were used to test the proposed weighting schemes and section 5 discusses the results. Finally, we conclude and propose future work in section 6. 2 Prior Work Sentiment analysis has been a popular research topic in recent years. Most of the work has focused on analyzing the content of movie or general product reviews, but there are also applications to other domains such as debates (Thomas et al., 2006; Lin et al., 2006), news (Devitt and Ahmad, 2007) and blogs (Ounis et al., 2008; Mishne, 2005). The book of Pang and Lee (2008) presents a thorough overview of the research in the field. This section presents the most relevant work. Pang et al. (2002) conducted early polarity classification of reviews using supervised approaches. They employed Support Vector Machines (SVMs), Naive Bayes and Maximum Entropy classifiers using a diverse set of features, such as unigrams, bigrams, binary and term frequency feature weights and others. They concluded that sentiment classification is more difficult </context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from congressional floor-debate transcripts. CoRR, abs/cs/0607062.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="2113" citStr="Turney, 2002" startWordPosition="316" endWordPosition="317">ysis, also known as opinion mining, provides mechanisms and techniques through which this vast amount of information can be processed and harnessed. Research in the field has mainly, but not exclusively, focused in two subproblems: detecting whether a segment of text, either a whole document or a sentence, is subjective or objective, i.e. contains an expression of opinion, and detecting the overall polarity of the text, i.e. positive or negative. Most of the work in sentiment analysis has focused on supervised learning techniques (Sebastiani, 2002), although there are some notable exceptions (Turney, 2002; Lin and He, 2009). Previous research has shown that in general the performance of the former tend to be superior to that of the latter (Mullen and Collier, 2004; Lin and He, 2009). One of the main issues for supervised approaches has been the representation of documents. Usually a bag of words representation is adopted, according to which a document is modeled as an unordered collection of the words that it contains. Early research by Pang et al. (2002) in sentiment analysis showed that a binary unigrambased representation of documents, according to which a document is modeled only by the pr</context>
<context position="6947" citStr="Turney, 2002" startWordPosition="1077" endWordPosition="1078"> the objective from the subjective sentences. They reported a statistically significant improvement over a Naive Bayes baseline using the whole text but only slight increase compared to using a SVM classifier on the entire document. Mullen and Collier (2004) used SVMs and expanded the feature set for representing documents with favorability measures from a variety of diverse sources. They introduced features based on Osgood’s Theory of Semantic Differentiation (Osgood, 1967) using WordNet to derive the values of potency, activity and evaluative of adjectives and Turney’s semantic orientation (Turney, 2002). Their results showed that using a hybrid SVM classifier, that uses as features the distance of documents from the separating hyperplane, with all the above features produces the best results. Whitelaw et al. (2005) added fine-grained semantic distinctions in the feature set. Their approach was based on a lexicon created in a semisupervised fashion and then manually refined It consists of 1329 adjectives and their modifiers categorized under several taxonomies of appraisal attributes based on Martin and White’s Appraisal Theory (2005). They combined the produced appraisal groups with unigram-</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In ACL, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Whitelaw</author>
<author>Navendu Garg</author>
<author>Shlomo Argamon</author>
</authors>
<title>Using appraisal groups for sentiment analysis.</title>
<date>2005</date>
<booktitle>In CIKM ’05: Proceedings of the 14th ACM international conference on Information and knowledge management,</booktitle>
<pages>625--631</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7163" citStr="Whitelaw et al. (2005)" startWordPosition="1110" endWordPosition="1113">r on the entire document. Mullen and Collier (2004) used SVMs and expanded the feature set for representing documents with favorability measures from a variety of diverse sources. They introduced features based on Osgood’s Theory of Semantic Differentiation (Osgood, 1967) using WordNet to derive the values of potency, activity and evaluative of adjectives and Turney’s semantic orientation (Turney, 2002). Their results showed that using a hybrid SVM classifier, that uses as features the distance of documents from the separating hyperplane, with all the above features produces the best results. Whitelaw et al. (2005) added fine-grained semantic distinctions in the feature set. Their approach was based on a lexicon created in a semisupervised fashion and then manually refined It consists of 1329 adjectives and their modifiers categorized under several taxonomies of appraisal attributes based on Martin and White’s Appraisal Theory (2005). They combined the produced appraisal groups with unigram-based document representations as features to a Support Vector Machine classifier (Witten and Frank, 1999), resulting in significant increases in accuracy. Zaidan et al. (2007) introduced “annotator rationales”, i.e.</context>
<context position="10075" citStr="Whitelaw et al., 2005" startWordPosition="1566" endWordPosition="1569">y weights We use the terms “features”, “words” and “terms” interchangeably in this paper, since we mainly focus on unigrams. The approach nonetheless can easily be extended to higher order n-grams. Each document D therefore is represented as a bag-ofwords feature vector: D = {w1, w2, ..., w|V |} where |V |is the size of the vocabulary (i.e. the number of unique words) and wz, i = 1, ... , |V | is the weight of term i in document D. Despite the significant attention that sentiment analysis has received in recent years, the best accuracy without using complex features (Mullen and Collier, 2004; Whitelaw et al., 2005) or additional human annotations (Zaidan et al., 2007) is achieved by employing a binary weighting scheme (Pang et al., 2002), where wz = 1, if tfz &gt; 0 and wz = 0, if tfz = 0, where tfz is the number of times that term i appears in document D (henceforth raw term frequency) and utilizing a SVM classifier. It is of particular interest that using tfz in the document representation usually results in decreased accuracy, a result that appears to be in contrast with topic classification (Mccallum and Nigam, 1998; Pang et al., 2002). In this paper, we also utilize SVMs but our study is centered on w</context>
<context position="20371" citStr="Whitelaw et al., 2005" startWordPosition="3336" endWordPosition="3339">classification. Table 4: Reported accuracies on the Movie Review data set. Only the best reported accuracy for each approach is presented, measured by 10-fold cross validation. The list is not exhaustive and because of differences in training/testing data splits the results are not directly comparable. It is produced here only for reference. Approach Acc. SVM with unigrams &amp; binary 87.15% weights (Pang et al., 2002), reported at (Pang and Lee, 2004) Hybrid SVM with Turney/Osgood 86% Lemmas (Mullen and Collier, 2004) SVM with min-cuts (Pang and Lee, 87.2% 2004) SVM with appraisal groups 90.2% (Whitelaw et al., 2005) SVM with log likehood ratio feature 90.45% selection (Aue and Gamon, 2005) SVM with annotator rationales 92.2% (Zaidan et al., 2007) LDA with filtered lexicon, subjectiv- 84.6% ity detection (Lin and He, 2009) The approach is straightforward, intuitive, computationally efficient, doesn’t require additional human effort and takes into consideration standardized and tested notions from IR. The results presented in section 5 show that a number (k1 + 1) · tfi K+tfi · N2 − dfi,2 + 0.5 log( dfi,2 + 0.5 ) wi = · log( (k1 + 1) · tfi N1 − dfi,1 + 0.5 K + tfi dfi,1 + 0.5 ) 1390 of weighting functions s</context>
</contexts>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>Casey Whitelaw, Navendu Garg, and Shlomo Argamon. 2005. Using appraisal groups for sentiment analysis. In CIKM ’05: Proceedings of the 14th ACM international conference on Information and knowledge management, pages 625–631, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP</booktitle>
<location>Vancouver, CA.</location>
<contexts>
<context position="3060" citStr="Wilson et al., 2005" startWordPosition="472" endWordPosition="475">ording to which a document is modeled as an unordered collection of the words that it contains. Early research by Pang et al. (2002) in sentiment analysis showed that a binary unigrambased representation of documents, according to which a document is modeled only by the presence or absence of words, provides the best baseline classification accuracy in sentiment analysis in comparison to other more intricate representations using bigrams, adjectives, etc. Later research has focused on extending the document representation with more complex features such as structural or syntactic information (Wilson et al., 2005), favorability measures from diverse sources (Mullen and Collier, 2004), implicit syntactic indicators (Greene and Resnik, 2009), stylistic and syntactic feature selection (Abbasi et al., 2008), “annotator rationales” (Zaidan et al., 2007) and others, but no systematic study has been presented exploring the benefits of employing more sophisticated models for assigning weights to word features. In this paper, we examine whether term weighting functions adopted from Information Retrieval (IR) based on the standard tf.idf formula and adapted to the particular setting of sentiment analysis can hel</context>
<context position="8397" citStr="Wilson et al., 2005" startWordPosition="1301" endWordPosition="1304">ses that explain the polarity of the document according to human annotators. By deleting rationale text spans from the original documents they created several contrast documents and constrained the SVM classifier to classify them less confidently than the originals. Using the largest training set size, their approach significantly increased the accuracy on a standard data set (see section 4). Prabowo and Thelwall (2009) proposed a hybrid classification process by combining in sequence several ruled-based classifiers with a SVM classifier. The former were based on the General Inquirer lexicon (Wilson et al., 2005), the MontyLingua part-of-speech tagger (Liu, 2004) and co-occurrence statistics of words with a set of predefined reference words. Their experiments showed that combining multiple classifiers can result in better effectiveness than any individual classifier, especially when sufficient training data isn’t available. In contrast to machine learning approaches that require labeled corpora for training, Lin and 1387 He (2009) proposed an unsupervised probabilistic modeling framework, based on Latent Dirichlet Allocation (LDA). The approach assumes that documents are a mixture of topics, i.e. prob</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of Human Language Technologies Conference/Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), Vancouver, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>1999</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations (The Morgan Kaufmann Series in Data Management Systems).</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<note>edition,</note>
<contexts>
<context position="7653" citStr="Witten and Frank, 1999" startWordPosition="1187" endWordPosition="1190">e distance of documents from the separating hyperplane, with all the above features produces the best results. Whitelaw et al. (2005) added fine-grained semantic distinctions in the feature set. Their approach was based on a lexicon created in a semisupervised fashion and then manually refined It consists of 1329 adjectives and their modifiers categorized under several taxonomies of appraisal attributes based on Martin and White’s Appraisal Theory (2005). They combined the produced appraisal groups with unigram-based document representations as features to a Support Vector Machine classifier (Witten and Frank, 1999), resulting in significant increases in accuracy. Zaidan et al. (2007) introduced “annotator rationales”, i.e. words or phrases that explain the polarity of the document according to human annotators. By deleting rationale text spans from the original documents they created several contrast documents and constrained the SVM classifier to classify them less confidently than the originals. Using the largest training set size, their approach significantly increased the accuracy on a standard data set (see section 4). Prabowo and Thelwall (2009) proposed a hybrid classification process by combinin</context>
</contexts>
<marker>Witten, Frank, 1999</marker>
<rawString>Ian H. Witten and Eibe Frank. 1999. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations (The Morgan Kaufmann Series in Data Management Systems). Morgan Kaufmann, 1st edition, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Wright</author>
</authors>
<title>Mining the web for feelings, not facts.</title>
<date>2009</date>
<note>23, NY Times, last accessed October 2, 2009, http://http://www.nytimes.com/2009/08/24/ technology/internet/ 24emotion.html? r=1.</note>
<contexts>
<context position="1340" citStr="Wright (2009)" startWordPosition="188" endWordPosition="189">weights and document frequency smoothing. The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge. 1 Introduction The increase of user-generated content on the web in the form of reviews, blogs, social networks, tweets, fora, etc. has resulted in an environment where everyone can publicly express their opinion about events, products or people. This wealth of information is potentially of vital importance to institutions and companies, providing them with ways to research their consumers, manage their reputations and identify new opportunities. Wright (2009) claims that “for many businesses, online opinion has turned into a kind of virtual currency that can make or break a product in the marketplace”. Sentiment analysis, also known as opinion mining, provides mechanisms and techniques through which this vast amount of information can be processed and harnessed. Research in the field has mainly, but not exclusively, focused in two subproblems: detecting whether a segment of text, either a whole document or a sentence, is subjective or objective, i.e. contains an expression of opinion, and detecting the overall polarity of the text, i.e. positive o</context>
</contexts>
<marker>Wright, 2009</marker>
<rawString>Alex Wright. 2009. Mining the web for feelings, not facts. August 23, NY Times, last accessed October 2, 2009, http://http://www.nytimes.com/2009/08/24/ technology/internet/ 24emotion.html? r=1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O F Zaidan</author>
<author>J Eisner</author>
<author>C D Piatko</author>
</authors>
<title>Using Annotator Rationales to Improve Machine Learning for Text Categorization.</title>
<date>2007</date>
<booktitle>Proceedings ofNAACL HLT,</booktitle>
<pages>260--267</pages>
<contexts>
<context position="3299" citStr="Zaidan et al., 2007" startWordPosition="505" endWordPosition="508">document is modeled only by the presence or absence of words, provides the best baseline classification accuracy in sentiment analysis in comparison to other more intricate representations using bigrams, adjectives, etc. Later research has focused on extending the document representation with more complex features such as structural or syntactic information (Wilson et al., 2005), favorability measures from diverse sources (Mullen and Collier, 2004), implicit syntactic indicators (Greene and Resnik, 2009), stylistic and syntactic feature selection (Abbasi et al., 2008), “annotator rationales” (Zaidan et al., 2007) and others, but no systematic study has been presented exploring the benefits of employing more sophisticated models for assigning weights to word features. In this paper, we examine whether term weighting functions adopted from Information Retrieval (IR) based on the standard tf.idf formula and adapted to the particular setting of sentiment analysis can help classification accuracy. We demonstrate that variants of the original tf.idf weighting scheme provide significant increases in classification performance. The advantages of the approach are that it is intuitive, computationally efficient</context>
<context position="7723" citStr="Zaidan et al. (2007)" startWordPosition="1198" endWordPosition="1201">e features produces the best results. Whitelaw et al. (2005) added fine-grained semantic distinctions in the feature set. Their approach was based on a lexicon created in a semisupervised fashion and then manually refined It consists of 1329 adjectives and their modifiers categorized under several taxonomies of appraisal attributes based on Martin and White’s Appraisal Theory (2005). They combined the produced appraisal groups with unigram-based document representations as features to a Support Vector Machine classifier (Witten and Frank, 1999), resulting in significant increases in accuracy. Zaidan et al. (2007) introduced “annotator rationales”, i.e. words or phrases that explain the polarity of the document according to human annotators. By deleting rationale text spans from the original documents they created several contrast documents and constrained the SVM classifier to classify them less confidently than the originals. Using the largest training set size, their approach significantly increased the accuracy on a standard data set (see section 4). Prabowo and Thelwall (2009) proposed a hybrid classification process by combining in sequence several ruled-based classifiers with a SVM classifier. T</context>
<context position="10129" citStr="Zaidan et al., 2007" startWordPosition="1575" endWordPosition="1578">s” interchangeably in this paper, since we mainly focus on unigrams. The approach nonetheless can easily be extended to higher order n-grams. Each document D therefore is represented as a bag-ofwords feature vector: D = {w1, w2, ..., w|V |} where |V |is the size of the vocabulary (i.e. the number of unique words) and wz, i = 1, ... , |V | is the weight of term i in document D. Despite the significant attention that sentiment analysis has received in recent years, the best accuracy without using complex features (Mullen and Collier, 2004; Whitelaw et al., 2005) or additional human annotations (Zaidan et al., 2007) is achieved by employing a binary weighting scheme (Pang et al., 2002), where wz = 1, if tfz &gt; 0 and wz = 0, if tfz = 0, where tfz is the number of times that term i appears in document D (henceforth raw term frequency) and utilizing a SVM classifier. It is of particular interest that using tfz in the document representation usually results in decreased accuracy, a result that appears to be in contrast with topic classification (Mccallum and Nigam, 1998; Pang et al., 2002). In this paper, we also utilize SVMs but our study is centered on whether more sophisticated than binary or raw term freq</context>
<context position="20504" citStr="Zaidan et al., 2007" startWordPosition="3357" endWordPosition="3360">ed, measured by 10-fold cross validation. The list is not exhaustive and because of differences in training/testing data splits the results are not directly comparable. It is produced here only for reference. Approach Acc. SVM with unigrams &amp; binary 87.15% weights (Pang et al., 2002), reported at (Pang and Lee, 2004) Hybrid SVM with Turney/Osgood 86% Lemmas (Mullen and Collier, 2004) SVM with min-cuts (Pang and Lee, 87.2% 2004) SVM with appraisal groups 90.2% (Whitelaw et al., 2005) SVM with log likehood ratio feature 90.45% selection (Aue and Gamon, 2005) SVM with annotator rationales 92.2% (Zaidan et al., 2007) LDA with filtered lexicon, subjectiv- 84.6% ity detection (Lin and He, 2009) The approach is straightforward, intuitive, computationally efficient, doesn’t require additional human effort and takes into consideration standardized and tested notions from IR. The results presented in section 5 show that a number (k1 + 1) · tfi K+tfi · N2 − dfi,2 + 0.5 log( dfi,2 + 0.5 ) wi = · log( (k1 + 1) · tfi N1 − dfi,1 + 0.5 K + tfi dfi,1 + 0.5 ) 1390 of weighting functions solidly outperform other state-of-the-art approaches. In the next section, we present the corpora that were used to study the effectiv</context>
</contexts>
<marker>Zaidan, Eisner, Piatko, 2007</marker>
<rawString>O.F. Zaidan, J. Eisner, and C.D. Piatko. 2007. Using Annotator Rationales to Improve Machine Learning for Text Categorization. Proceedings ofNAACL HLT, pages 260–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Zobel</author>
<author>Alistair Moffat</author>
</authors>
<title>Exploring the similarity space.</title>
<date>1998</date>
<journal>SIGIR Forum,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="15608" citStr="Zobel and Moffat (1998)" startWordPosition="2518" endWordPosition="2521">ormalization that is being applied. The upper rows of tables 1, 2 and 3 present the three most commonly used weighting functions for each factor respectively. For example, a binary document representation would be equivalent to SMART.bnn1 or more simply bnn, while a simple raw term frequency based would be notated as nnn or nnc with cosine normalization. Table 3: SMART normalization. Notation Normalization n (none) 1 c (cosine) 1 �w2+w22+...+wn Significant research has been done in IR on diverse weighting functions and not all versions of SMART notations are consistent (Manning et al., 2008). Zobel and Moffat (1998) provide an exhaustive study but in this paper, due to space constraints, we will follow the concise notation presented by Singhal et al. (1995). The BM25 weighting scheme (Robertson et al., 1994; Robertson et al., 1996) is a probabilistic model for information retrieval and is one of the most popular and effective algorithms used in information retrieval. For ease of reference, we incorporate the BM25 tf and idf factors into the SMART annotation scheme (last row of table 1 and 4th row of table 2), therefore the weight wi of term i in document D according to the BM25 scheme is notated as SMART</context>
</contexts>
<marker>Zobel, Moffat, 1998</marker>
<rawString>Justin Zobel and Alistair Moffat. 1998. Exploring the similarity space. SIGIR Forum, 32(1):18–34.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>