<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003280">
<title confidence="0.978275">
User Participation Prediction in Online Forums
</title>
<author confidence="0.989706">
Zhonghua Qu and Yang Liu
</author>
<affiliation confidence="0.985709">
The University of Texas at Dallas
</affiliation>
<email confidence="0.996664">
{qzh,yangl@hlt.utdallas.edu}
</email>
<sectionHeader confidence="0.996614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99983804">
Online community is an important source
for latest news and information. Accurate
prediction of a user’s interest can help pro-
vide better user experience. In this paper,
we develop a recommendation system for
online forums. There are a lot of differ-
ences between online forums and formal me-
dia. For example, content generated by users
in online forums contains more noise com-
pared to formal documents. Content topics
in the same forum are more focused than
sources like news websites. Some of these
differences present challenges to traditional
word-based user profiling and recommenda-
tion systems, but some also provide oppor-
tunities for better recommendation perfor-
mance. In our recommendation system, we
propose to (a) use latent topics to interpo-
late with content-based recommendation; (b)
model latent user groups to utilize informa-
tion from other users. We have collected
three types of forum data sets. Our experi-
mental results demonstrate that our proposed
hybrid approach works well in all three types
of forums.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999933108108108">
Internet is an important source of information. It
has become a habit of many people to go to the in-
ternet for latest news and updates. However, not all
articles are equally interesting for different users.
In order to intelligently predict interesting articles
for individual users, personalized news recommen-
dation systems have been developed. There are in
general two types of approaches upon which rec-
ommendation systems are built. Content based rec-
ommendation systems use the textual information
of news articles and user generated content to rank
items. Collaborative filtering, on the other hand,
uses co-occurrence information from a collection
of users for recommendation.
During the past few years, online community
has become a large part of internet. More often,
latest information and knowledge appear at on-
line community earlier than other formal media.
This makes it a favorable place for people seeking
timely update and latest information. Online com-
munity sites appear in many forms, for example,
online forums, blogs, and social networking web-
sites. Here we focus our study on online forums. It
is very helpful to build an automatic system to sug-
gest latest information a user would be interested
in. However, unlike formal news media, user gen-
erated content in forums is usually less organized
and not well formed. This presents a great chal-
lenge to many existing news article recommenda-
tion systems. In addition, what makes online fo-
rums different from other media is that users of
online communities are not only the information
consumers but also active providers as participants.
Therefore in this study we develop a recommen-
dation system to account for these characteristics
of forums. We propose several improvements over
previous work:
</bodyText>
<listItem confidence="0.636935">
• Latent topic interpolation: This is to address
the issue with the word-based content repre-
sentation. In this paper we used Latent Dirich-
let Allocation (LDA), a generative multino-
mial mixture model, for topic inference inside
threads. We build a system based on words
</listItem>
<page confidence="0.970488">
367
</page>
<note confidence="0.976875">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 367–376,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9434905">
and latent topics, and linearly interpolate their
results.
</bodyText>
<listItem confidence="0.896208125">
• User modeling: We model users’ participa-
tion inside threads as latent user groups. Each
latent group is a multinomial distribution on
users. Then LDA is used to infer the group
mixture inside each thread, based on which
the probability of a user’s participation can be
derived.
• Hybrid system: Since content and user-
</listItem>
<bodyText confidence="0.995134214285714">
based methods rely on different information
sources, we combine the results from them for
further improvement.
We have evaluated our proposed method using
three data sets collected from three representative
forums. Our experimental results show that in all
forums, by using latent topics information, system
can achieve better accuracy in predicting threads
for recommendation. In addition, by modeling la-
tent user groups in thread participation, further im-
provement is achieved in the hybrid system. Our
analysis also showed that each forum has its nature,
resulting in different optimal parameters in the dif-
ferent forums.
</bodyText>
<sectionHeader confidence="0.999789" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999393371428572">
Recommendation systems can help make informa-
tion retrieving process more intelligent. Generally,
recommendation methods are categorized into two
types (Adomavicius and Tuzhilin, 2005), content-
based filtering and collaborative filtering.
Systems using content-based filtering use the
content information of recommendation items a
user is interested in to recommend new items to
the user. For example, in a news recommendation
system, in order to recommend appropriate news
articles to a user, it finds the most prominent fea-
tures (e.g., key words, tags, category) in the docu-
ment that a user likes, then suggests similar articles
based on this “personal profile”. In Fabs system
(Balabanovic and Shoham, 1997), Skyskill &amp; We-
bert system (Pazzani et al., 1997), documents are
represented using a set of most important words
according to a weighting measure. The most popu-
lar measure of word “importance” is TF-IDF (term
frequency, inverse document frequency) (Salton
and Buckley, 1988), which gives weights to words
according to its “informativeness”. Then, base on
this “personal profile” a ranking machine is applied
to give a ranked recommendation list. In Fabs sys-
tem, Rocchio’ algorithm (Rocchio, 1971) is used
to learn the average TF-IDF vector of highly rated
documents. Skyskill &amp; Webert’s system uses Naive
Bayes classifiers to give the probability of docu-
ments being liked. Winnow’s algorithm (Little-
stone, 1988), which is similar to perception algo-
rithm, has been shown to perform well when there
are many features. An adaptive framework is intro-
duced in (Li et al., 2010) using forum comments
for news recommendation. In (Wu et al., 2010),
a topic-specific topic flow model is introduced to
rank the likelihood of user participating in a thread
in online forums.
Collaborative-filtering based systems, unlike
content-based systems, predict the recommending
items using co-occurrence information between
users. For example, in a news recommendation
system, in order to recommend an article to user
c, the system tries to find users with similar taste
as c. Items favored by similar users would be rec-
ommended. Grundy (Rich, 1979) is known to be
one of the first collaborative-filtering based sys-
tems. Collaborative filtering systems can be ei-
ther model based or memory based (Breese et al.,
1998). Memory-based algorithms, such as (Del-
gado and Ishii, 1999; Nakamura and Abe, 1998;
Shardanand and Maes, 1995), use a utility function
to measure the similarity between users. Then rec-
ommendation of an item is made according to the
sum of the utility values of active users that partic-
ipate in it. Model-based algorithms, on the other
hand, try to formulate the probability function of
one item being liked statistically using active user
information. (Ungar et al., 1998) clustered sim-
ilar users into groups for recommendation. Dif-
ferent clustering methods have been experimented,
including K-means and Gibbs Sampling. Other
probabilistic models have also been used to model
collaborative relationships, including a Bayesian
model (Chien and George, 1999), linear regres-
sion model (Sarwar et al., 2001), Gaussian mix-
ture models (Hofmann, 2003; Hofmann, 2004). In
(Blei et al., 2001) a collaborative filtering appli-
cation is discussed using LDA. However in this
model, re-estimation of parameters for the whole
system is needed when a new item comes in. In
</bodyText>
<page confidence="0.997851">
368
</page>
<bodyText confidence="0.999844">
this paper, we formulate users’ participation differ-
ently using the LDA mixture model.
Some previous work has also evaluated using
a hybrid model with both content and collabora-
tive features and showed outstanding performance.
For example, in (Basu et al., 1998), hybrid features
are used to make recommendation using inductive
learning.
</bodyText>
<sectionHeader confidence="0.987162" genericHeader="method">
3 Forum Data
</sectionHeader>
<bodyText confidence="0.999974243243244">
We have collected data from three forums in this
study.1 Ubuntu community forum is a technical
support forum; World of Warcraft (WoW) forum is
about gaming; Fitness forum is about how to live
a healthy life. These three forums are quite rep-
resentative of online forums on the internet. Us-
ing three different types of forums for task eval-
uation helps to demonstrate the robustness of our
proposed method. In addition, it can show how the
same method could have substantial performance
difference on forums of different nature. Users’
behaviors in these three forums are very differ-
ent. Casual forums like “Wow gaming” have much
more posts in each thread. However its posts are
the shortest in length. This is because discussions
inside these types of forums are more like casual
conversation, and there is not much requirement
on the user’s background, and thus there is more
user participation. In contrast, technical forums
like “Ubuntu” have fewer average posts in each
thread, and have the longest post length. This is
because a Question and Answer (QA) forum tends
to be very goal oriented. If a user finds the thread
is unrelated, then there will be no motivation for
participation.
Inside forums, different boards are created to
categorize the topics allowed for discussion. From
the data we find that users tend to participate in a
few selected boards of their choices. To create a
data set for user interest prediction in this study,
we pick the most popular boards in each forum.
Even within the same board, users tend to partici-
pate in different threads base on their interest. We
use a user’s participation information as an indica-
tion whether a thread is interesting to a user or not.
Hence, our task is to predict the user participation
in forum threads. Note this approach could intro-
</bodyText>
<footnote confidence="0.69879">
1Please contact the authors to obtain the data.
</footnote>
<bodyText confidence="0.99950372">
duce some bias toward negative instances in terms
of user interests. A users’ absence from a thread
does not necessarily mean the user is not interested
in that thread; it may be a result of the user being
offline by that time or the thread is too behind in
pages. As a matter of fact, we found most users
read only the threads on the first page during their
time of visit of a forum. This makes participation
prediction an even harder task than interest predic-
tion.
In online forums, threads are ordered by the time
stamp of their last participating post. Provided with
the time stamp for each post, we can calculate the
order of a thread on its board during a user’s par-
ticipation. Figure 1 shows the distribution of post
location during users’ participation. We found that
most of the users read only the posts on the first
page. In order to minimize the false negative in-
stances from the data set, we did thread location
filtering. That is, we want to filter out messages
that actually interest the user but do not have the
user’s participation because they are not on the first
page. For any user, only those threads appearing in
the first 10 entries on a page during a user’s visit
are included in the data set.
</bodyText>
<figureCaption confidence="0.999109">
Figure 1: Thread position during users’ participation.
</figureCaption>
<bodyText confidence="0.9999771">
In the pre-processing step of the experiment, first
we use online status filtering discussed above to
remove threads that a user does not see while of-
fline. The statistics of the boards we have used in
each forum are shown in Table 1. The statistics
are consistent with the full forum statistics. For
example, users in technical forums tend to post
less than casual forums. We define active users as
those who have participated in 10 or more threads.
Column “Part. @300” shows the average number
</bodyText>
<page confidence="0.995207">
369
</page>
<bodyText confidence="0.999976875">
of threads the top 300 users have participated in.
“Filt. Threads@300” shows the average number of
threads after using online filtering with a window
of 10. Thread participation in “Ubuntu” forum is
very sparse for each user, having only 10.01% par-
ticipating threads for each user after filtering. “Fit-
ness” and “Wow Forum” have denser participation,
at 18.97% and 13.86% respectively.
</bodyText>
<sectionHeader confidence="0.970969" genericHeader="method">
4 Interesting Thread Prediction
</sectionHeader>
<bodyText confidence="0.9999891">
In the task of interesting thread prediction, the sys-
tem generates a ranked list of threads a user is
likely to be interested in based on users’ past his-
tory of thread participation. Here, instead of pre-
dicting the true interestedness, we predict the par-
ticipation of the user, which is a sufficient condi-
tion for interestedness. This approach is also used
by (Wu et al., 2010) for their task evaluation. In
this section, we describe our proposed approaches
for thread participation prediction.
</bodyText>
<subsectionHeader confidence="0.996182">
4.1 Content-based Filtering
</subsectionHeader>
<bodyText confidence="0.999570857142857">
In the content-based filtering approach, only con-
tent of a thread is used as features for prediction.
Recommendation through content-based filtering
has its deep root in information retrieval. Here we
use a Naive Bayes classifier for ranking the threads
using information based on the words and the la-
tent topic analysis.
</bodyText>
<subsectionHeader confidence="0.542409">
4.1.1 Naive Bayes Classification
</subsectionHeader>
<bodyText confidence="0.9995454">
In (Pazzani et al., 1997) Naive Bayesian classi-
fier showed outstanding performance in web page
recommendation compared to several other clas-
sifiers. A Naive Bayes classifier is a generative
model in which words inside a document are as-
sumed to be conditionally independent. That is,
given the class of a document, words are generated
independently. The posterior probability of a test
instance in Naive Bayes classifier takes the follow-
ing form:
</bodyText>
<equation confidence="0.944431">
1 �
P (Ci|f1..k) = Z P (Ci)
j
</equation>
<bodyText confidence="0.9992214">
where Z is the class label independent normaliza-
tion term, fi..k is the bag-of-word feature vector
for the document. Naive Bayes classifier is known
for not having a well calibrated posterior probabil-
ity (Bennett, 2000). (Pavlov et al., 2004) showed
that normalization by document length yielded
good empirical results in approximating a well cal-
ibrated posterior probability for Naive Bayes clas-
sifier. The normalized Naive Bayes classifier they
used is as follows:
</bodyText>
<equation confidence="0.967348">
1 �
P (Ci|f1..k) = Z P (Ci)
j
</equation>
<bodyText confidence="0.998786">
In this equation, the probability of generat-
ing each word is normalized by the length of
the feature vector |f|. The posterior probabil-
ity P(interested|f1..k) from (normalized) Naive
Bayes classifier is used for recommendation item
ranking.
</bodyText>
<subsubsectionHeader confidence="0.614008">
4.1.2 Latent Topics based Interpolation
</subsubsectionHeader>
<bodyText confidence="0.999989212121212">
Because of noisy forum writing and limited
training data, the above bag-of-word model used in
naive Bayes classifier may suffer from data sparsity
issues. We thus propose to use latent topic model-
ing to alleviate this problem. Latent Dirichlet Allo-
cation (LDA) is a generative model based on latent
topics. The major difference between LDA and
previous methods such as probabilistic Latent Se-
mantic Analysis (pLSA) is that LDA can efficiently
infer topic composition of new documents, regard-
less of the training data size (Blei et al., 2001). This
makes it ideal for efficiently reducing the dimen-
sion of incoming documents.
In an online forum, words contained in threads
tend to be very noisy. Irregular words, such as
abbreviation, misspelling and synonyms, are very
common in an online environment. From our ex-
periments, we observe that LDA seems to be quite
robust to these phenomena and able to capture
word relationship semantically. To illustrate the
words inside latent topics in the LDA model in-
ferred from online forums, we show in Table 2 the
top words in 3 out of 20 latent topics inferred from
“Ubuntu” forum according to its multinomial dis-
tribution. We can see that variations of the same
words are grouped into the same topic.
Since each post could be very short and LDA is
generally known not to work well with short docu-
ments, we concatenated the content of posts inside
each thread to form documents. In order to build
a valid evaluation configuration, only posts before
the first time the testing user participated are used
for model fitting and inference.
</bodyText>
<equation confidence="0.999030333333333">
P(fj|Ci) (1)
isi (2)
P(fj|Ci)
</equation>
<page confidence="0.971391">
370
</page>
<table confidence="0.99989025">
Forum Name Threads Posts Active Users Part. @300 Filt. Threads @300
Ubuntu 185,747 940,230 1,700 464.72 4641.25
Fitness 27,250 529,201 2,808 613.15 3231.04
Wow Gaming 34,187 1,639,720 19,173 313.77 2264.46
</table>
<tableCaption confidence="0.988411">
Table 1: Data statistics after filtering.
</tableCaption>
<table confidence="0.999256666666667">
Topic 1 Topic 2 Topic 3
lol’d wine email
lol. Wine mail
imo. game Thunderbird
,’ fixme evolution
-, stub send
lulz. not emails
lmao. WINE gmail
rofl. play postfix
</table>
<tableCaption confidence="0.9910995">
Table 2: Example of LDA topics that capture words
with different variations.
</tableCaption>
<bodyText confidence="0.999964192307692">
After model fitting for LDA, the topic distri-
butions on new threads can be inferred using the
model. Compared to the original bag-of-word fea-
ture vector, the topic distribution vector is not only
more robust against noise, but also closer to hu-
man interpretation of words. For example in topic
3 in Table 2, people who care about “Thunder-
bird”, an email client, are also very likely to show
interest in “postfix”, which is a Linux email ser-
vice. These closely related words, however, might
not be captured using the bag-of-word model since
that would require the exact words to appear in the
training set.
In order to take advantage of the topic level in-
formation while not losing the “fine-grained” word
level feature, we use the topic distribution as ad-
ditional features in combination with the bag-of-
word features. To tune the contribution of topic
level features in classifiers like Naive Bayes clas-
sifiers, we normalize the topic level feature to a
length of Lt = -y|f |and bag-of-word feature to
L,,, = (1−-y)|f|. -y is a tuning parameter from 0 to
1 that determines the proportion of the topic infor-
mation used in the features. |f |is from the original
bag-of-word feature vector. The final feature vec-
tor for each thread can be represented as:
</bodyText>
<equation confidence="0.992516">
F = L,,,w1, ..., L,,,wk U LtO1, ..., LtOT (3)
</equation>
<bodyText confidence="0.9992645">
where O1, ..., Ot is the multinomial distribution of
topics for the thread.
</bodyText>
<subsectionHeader confidence="0.990895">
4.2 Collaborative Filtering
</subsectionHeader>
<bodyText confidence="0.99995252173913">
Collaborative filtering techniques make prediction
using information from similar users. It has ad-
vantages over content-based filtering in that it can
correctly predict items that are vastly different in
content but similar in concepts indicated by users’
participation.
In some previous work, clustering methods were
used to partition users into several groups, Then,
predictions were made using information from
users in the same group. However, in the case
of thread recommendation, we found that users’
interest does not form clean clusters. Figure 2
shows the mutual information between users after
doing an average-link clustering on their pairwise
mutual information. In a clean clustering, intra-
cluster mutual information should be high, while
inter-cluster mutual information is very low. If so,
we would expect that the figure shows clear rect-
angles along the diagonal. Unfortunately, from this
figure it appears that users far away in the hierarchy
tree still have a lot of common thread participation.
Here, we propose to model user similarity based on
latent user groups.
</bodyText>
<subsectionHeader confidence="0.898426">
4.2.1 Latent User Groups
</subsectionHeader>
<bodyText confidence="0.999838846153846">
In this paper, we model users’ participation in-
side threads as an LDA generative model. We
model each user group as a multinomial distribu-
tion. Users inside each group are assumed to have
common interests in certain topic(s). A thread in an
online forum typically contains several such top-
ics. We could model a user’s participation in a
thread as a mixture of several different user groups.
Since one thread typically attracts a subset of user
groups, it is reasonable to add a Dirichlet prior on
the user group mixture.
The generative process is the same as the LDA
used above for topic modeling, except now users
</bodyText>
<page confidence="0.996017">
371
</page>
<figureCaption confidence="0.9798125">
Figure 2: Mutual information between users in Average
Link Hierarchical clustering.
</figureCaption>
<bodyText confidence="0.999797333333333">
are ‘words’ and user groups are ‘topics’. Using
LDA to model user participation can be viewed
as soft-clustering of users in a sense that one user
could appear in multiple groups at the same time.
The generative process for participating users is as
follows.
</bodyText>
<listItem confidence="0.99451775">
1. Choose θ — Dir(α)
2. For each of N participating users, un:
(a) Choose a group zn — Multinomial(θ)
(b) Choose a user un — p(un|zn)
</listItem>
<bodyText confidence="0.999798875">
One thing worth noting is that in LDA model a
document is assumed to consist of many words. In
the case of modeling user participation, a thread
typically has far fewer users than words inside a
document. This could potentially cause problem
during variable estimation and inference. How-
ever, we show that this approach actually works
well in practice (experimental results in Section 5).
</bodyText>
<subsectionHeader confidence="0.9212715">
4.2.2 Using Latent User Groups for
Prediction
</subsectionHeader>
<bodyText confidence="0.9999892">
For an incoming new thread, first the latent
group distribution is inferred using collapsed Gibbs
Sampling (Griffiths and Steyvers, 2004). The pos-
terior probability of a user ui participating in thread
j given the user group distribution is as follows.
</bodyText>
<equation confidence="0.975883">
P(ui|θj,φ) = � P(ui|φk)P(k|θj) (4)
kET
</equation>
<bodyText confidence="0.999949">
In the equation, φk is the multinomial distribution
of users in group k, T is the number of latent user
groups, and θj is the group composition in thread
j after inference using the training data. In gen-
eral, the probability of user ui appearing in thread
j is proportional to the membership probabilities
of this user in the groups that compose the partici-
pating users.
</bodyText>
<subsectionHeader confidence="0.99958">
4.3 Hybrid System
</subsectionHeader>
<bodyText confidence="0.999679">
Up to this point we have two separate systems that
can generate ranked recommendation lists based on
different factors of threads. In order to generate the
final ranked list, we give each item a score accord-
ing to the ranked lists from the two systems. Then
the two scores are linearly interpolated using a tun-
ing parameter λ as shown in Equation 5. The final
ranked list is generated accordingly.
</bodyText>
<equation confidence="0.974172">
Ci =(1 — λ)Scorecontent (5)
</equation>
<bodyText confidence="0.994793">
We propose several different rescoring methods
to generate the scores in the above formula for the
two individual systems.
</bodyText>
<listItem confidence="0.995352666666667">
• Posterior: The posterior probabilities of each
item from the two systems are used directly as
the score.
</listItem>
<equation confidence="0.991326">
Scoredir = p(clike|itemi) (6)
</equation>
<bodyText confidence="0.9995515">
This way the confidence of “how likely” an
item is interesting is preserved. However,
the downside is that the two different sys-
tems have different calibration on its posterior
probability, which could be problematic when
directly adding them together.
</bodyText>
<listItem confidence="0.97316925">
• Linear rescore: To counter the problem asso-
ciated with posterior probability calibration,
we use linear rescoring based on the ranked
list:
</listItem>
<equation confidence="0.977663">
Scorelin = 1 — posi (7)
N
</equation>
<bodyText confidence="0.9997812">
In the formula, posi is the position of item i
in the ranked list, and N is the total number
of items being ranked. The resulting score is
between 0 and 1, 1 being the first item on the
list and 0 being the last.
</bodyText>
<listItem confidence="0.722266">
• Sigmoid rescore: In a ranked list, usually
items on the top and bottom of the list have
+ λScorecollaborative
</listItem>
<page confidence="0.99179">
372
</page>
<bodyText confidence="0.9991926">
higher confidence than those in the middle.
That is to say more “emphasis” should be put
on both ends of the list. Hence we use a sig-
moid function on the Scorelinear to capture
this.
</bodyText>
<equation confidence="0.9975025">
1
Score�i� = 1 + e−l(Scorejzn−0.5) (8)
</equation>
<bodyText confidence="0.999933">
A sigmoid function is relatively flat on both
ends while being steep in the middle. In the
equation, l is a tuning parameter that decides
how “flat” the score of both ends of the list is
going to be. Determining the best value for l
is not a trivial problem. Here we empirically
assign l = 10.
</bodyText>
<sectionHeader confidence="0.99223" genericHeader="evaluation">
5 Experiment and Evaluation
</sectionHeader>
<bodyText confidence="0.999988846153846">
In this section, we evaluate our approach empiri-
cally on the three forum data sets described in Sec-
tion 3. We pick the top 300 most active users from
each forum for the evaluation. Among the 300
users, 100 of them are randomly selected as the de-
velopment set for parameter tuning, while the rest
is test set. All the data sets are filtered using an on-
line filter as previously described, with a window
size of 10 threads.
Threads are tokenized into words and filtered us-
ing a simple English stop word list. All words
are then ordered by their occurrences multiplied by
their inverse document frequencies (IDF).
</bodyText>
<equation confidence="0.9984445">
idf,,, = log |� |(9)
|{d : � � d}|
</equation>
<bodyText confidence="0.99996436">
The top 4,000 words from this list are then used to
form the vocabulary.
We used standard mean average precision
(MAP) as the evaluation metric. This standard in-
formation retrieval evaluation metric measures the
quality of the returned rank lists from a system.
Entries higher in the rank are more accurate than
lower ones. For an interesting thread recommenda-
tion system, it is preferable to provide a short and
high-quality list of recommendation; therefore, in-
stead of reporting full-range MAP, we report MAP
on top 10 relevant threads (MAP@10). The reason
why we picked 10 as the number of relevant doc-
ument for MAP evaluation is that users might not
have time to read too many posts, even if they are
relevant.
During evaluation, a 3-fold cross-validation is
performed for each user in the test set. In each fold,
MAP@10 score is calculated from the ranked list
generated by the system. Then the average from all
the folds and all the users is computed as the final
result.
To make a proper evaluation configuration, for
each user, only posts up to the first participation of
the testing user are used for the test set.
</bodyText>
<subsectionHeader confidence="0.996835">
5.1 Content-based Results
</subsectionHeader>
<bodyText confidence="0.9999695">
Here we evaluate the performance of interest
thread prediction using only features from text.
First we use the ranking model with latent topic
information only on the development set to deter-
mine an optimal number of topics. Empirically,
we use hyper parameter Q = 0.1 and α = 1/K
(K is the number of topics). We use the perfor-
mance of content-based recommendation directly
to determine the optimal topic number K. We var-
ied the latent topic number K from 10 to 100, and
found that the best performance was achieved us-
ing 30 topics in all three forums. Hence we use
K = 30 for content based recommendation unless
otherwise specified.
Next, we show how topic information can help
content-based recommendation achieve better re-
sults. We tune the parameter -y described in Sec-
tion 4.1.2 and show corresponding performances.
We compare the performance using Naive Bayes
classifier, before and after normalization. The
MAP@10 results on the test set are shown in Fig-
ure 3 for three forums. When -y = 0, no latent topic
information is used, and when -y = 1, latent topics
are used without any word features.
When using Naive Bayes classifier without nor-
malization, we find relatively larger performance
gain from adding topic information for the -y val-
ues of close to 0. This phenomenon is probably
because of the poor posterior probabilities of the
Naive Bayes classifier, which are close to either 1
or 0.
For normalized Naive Bayes classifier, interpo-
lating with latent topics based ranking yields per-
formance improvement compared to word-based
results consistently for the three forums. In
“Wow Gaming” corpus, the optimal performance
is achieved with a relatively high -y value (at around
0.5), and it is even higher for the “Fitness” forum.
</bodyText>
<page confidence="0.998255">
373
</page>
<bodyText confidence="0.999924571428571">
This means that the system relies more on the la-
tent topics information. This is because in these fo-
rums, casual conversation contains more irregular
words, causing more severe data sparsity problem
than others.
Between the two naive Bayes classifiers, we
can see that using normalized probabilities out-
performs the original one in “Wow Gaming” and
“Ubuntu” forums. This observation is consistent
with previous work (e.g., (Pavlov et al., 2004)).
However, we found that in “Fitness Forum”, the
performance degrades with normalization. Further
work is still needed to understand why this is the
case.
</bodyText>
<subsectionHeader confidence="0.999757">
5.2 Latent User Group Classification
</subsectionHeader>
<bodyText confidence="0.997351454545454">
In this section, collaborative filtering using latent
user groups is evaluated. First, participating users
from the training set are used to estimate an LDA
model. Then, users participating in a thread are
used to infer the topic distribution of the thread.
Candidate threads are then sorted by the proba-
bility of a target user’s participation according to
Equation 4. Note that all the users in the forum are
used to estimate the latent user groups, but only the
top 300 active users are used in evaluation. Here,
we vary the number of latent user groups G from
5 to 100. Hyper parameters were set empirically:
α = 1/G,0 = 0.1.
Figure 4 shows the MAP@10 results using dif-
ferent numbers of latent groups for the three fo-
rums. We compare the performance using latent
groups with a baseline using SVM ranking. In
the baseline system, users’ participation in a thread
is used as a binary feature. LibSVM with radius
based function (RBF) kernel is used to estimate the
probability of a user’s participation.
From the results, we find that ranking using la-
tent groups information outperforms the baseline
in almost all non-trivial cases. In the case of
“Ubuntu” forum, the performance gain is less com-
pared to other forums. We believe this is because
in this technical support forum, the average user
participation in threads is much less, thus making
it hard to infer a reliable group distribution in a
thread. In addition, the optimal number of user
groups differs greatly between “Fitness” forum and
“Wow Gaming” forum. We conjecture the reason
behind this is that in the “Fitness” forum, users
</bodyText>
<figureCaption confidence="0.994275666666667">
Figure 5: Position of items with different #users and
#words in a ranked list. (red=0 being higher on the
ranked list and green being lower)
</figureCaption>
<bodyText confidence="0.963275222222222">
may be interested in a larger variety of topics and
thus the user distribution in different topics is not
very obvious. In contrast, people in the gaming
forum are more specific to the topics they are inter-
ested in.
It is known that LDA tends to perform poorly
when there are too few words/users. To have a
general idea of how much user participation is
“enough” for decent prediction, we show a graph
(Figure 5) depicting the relationships among the
number of users, the number of words, and the po-
sition of the positive instances in the ranked lists.
In this graph, every dot is a positive thread instance
in “Wow Gaming” forum. Red color shows that
the positive thread is indeed getting higher ranks
than others. We observe that threads with around
16 participants can already achieve a decent perfor-
mance.
</bodyText>
<subsectionHeader confidence="0.998648">
5.3 Hybrid System Performance
</subsectionHeader>
<bodyText confidence="0.9998935">
In this section, we evaluate the performance of the
hybrid system output. Parameters used in each fo-
rum data set are the optimal parameters found in
the previous sections. Here we show the effect of
the tuning parameter A (described in Section 4.3).
Also, we compare three different scoring schemes
used to generate the final ranked list. Performance
of the hybrid system is shown in Table 3.
We can see that the combination of the two sys-
tems always outperforms any one model alone.
</bodyText>
<page confidence="0.991246">
374
</page>
<figure confidence="0.999504833333333">
Ubuntu Forum
Wow Gaming
Fitness Forum
Ubuntu Forum
Wow Gaming
Fitness Forum
0 0.2 0.4 0.6 0.8 1
Gamma
0 0.2 0.4 0.6 0.8 1
Gamma
0 0.2 0.4 0.6 0.8 1
Gamma
</figure>
<figureCaption confidence="0.97835">
Figure 3: Content-based filtering results: MAP@10 vs. y (contribution of topic-based features).
</figureCaption>
<figure confidence="0.9992785">
1 10 100
Number of Groups
1 10 100
Number of Groups
1 10 100
Number of Groups
</figure>
<figureCaption confidence="0.985629">
Figure 4: Collaborative filtering results: MAP@10 vs. user group number.
</figureCaption>
<figure confidence="0.996914285714285">
1
0.3
Naive Bayes
Normalized NB
0.9
0.28
0.8
0.7
MAP 10
MAP 10
0.26
0.6
0.5
0.24
0.4
0.3
0.22
0.2
0.2
0.1
0.54
Naive Bayes
Normalized NB
0.51
0.48
MAP 10
0.45
0.42
0.39
0.36
Naive Bayes
Normalized NB
0.35
0.6
Latent Group
SVM
0.3
0.5
MAP 10
MAP 10
0.4
0.25
0.2
0.3
0.2
0.15
0.22
Latent Group
SVM
0.2
MAP 10
0.18
0.16
0.14
Latent Group
SVM
</figure>
<table confidence="0.899947">
Forum Contribution Factor A
0.0 1.0 Optimal
Ubuntu 0.523 0.198 0.534 (A = 0.9)
Wow 0.278 0.283 0.304 (A = 0.1)
Fitness 0.545 0.457 0.551 (A = 0.85)
</table>
<tableCaption confidence="0.963512">
Table 3: Performance of the hybrid system with differ-
ent A values.
</tableCaption>
<bodyText confidence="0.999930588235294">
This is intuitive since the two models use differ-
ent information sources. A MAP@10 score of 0.5
means that around half of the suggested results do
have user participation. We think this is a good re-
sult considering that this is not a trivial task.
We also notice that based on the nature of differ-
ent forums, the optimal A value could be substan-
tially different. For example, in “Wow gaming”
forum where people participate in more threads, a
higher A value is observed which favors collabo-
rative filtering score. In contrast, in “Ubuntu” fo-
rum, where people participate in far fewer threads,
the content-based system is more reliable in thread
prediction, hence a lower A is used. This observa-
tion also shows that the hybrid system is more ro-
bust against differences among forums compared
with single model systems.
</bodyText>
<sectionHeader confidence="0.999404" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999973">
In this paper, we proposed a new system that can
intelligently recommend threads from online com-
munity according to a user’s interest. The system
uses both content-based filtering and collaborative-
filtering techniques. In content-based filtering, we
solve the problem of data sparsity in online con-
tent by smoothing using latent topic information.
In collaborative filtering, we model users’ partici-
pation in threads with latent groups under an LDA
framework. The two systems compliment each
other and their combination achieves better per-
formance than individual ones. Our experiments
across different forums demonstrate the robustness
of our methods and the difference among forums.
In the future work, we plan to explore how social
information could help further refine a user’s inter-
est.
</bodyText>
<sectionHeader confidence="0.99923" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991400666666667">
Gediminas Adomavicius and Alexander Tuzhilin.
2005. Toward the next generation of recommender
systems: A survey of the state-of-the-art and possi-
ble extensions. IEEE TRANSACTIONS ON KNOWL-
EDGE AND DATA ENGINEERING, 17(6):734–749.
Marko Balabanovic and Yoav Shoham. 1997.
</reference>
<page confidence="0.987514">
375
</page>
<reference confidence="0.999822964285714">
Fab: Content-based, collaborative recommendation.
Communications of the ACM, 40:66–72.
Chumki Basu, Haym Hirsh, and William Cohen. 1998.
Recommendation as classification: Using social and
content-based information in recommendation. In In
Proceedings of the Fifteenth National Conference on
Artificial Intelligence, pages 714–720. AAAI Press.
Paul N. Bennett. 2000. Assessing the calibration of
naive bayes’ posterior estimates.
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2001. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:2003.
John S. Breese, David Heckerman, and Carl Kadie.
1998. Empirical analysis of predictive algorithms for
collaborative filtering. pages 43–52. Morgan Kauf-
mann.
Y H Chien and E I George, 1999. A bayesian model for
collaborative filtering. Number 1.
Joaquin Delgado and Naohiro Ishii. 1999. Memory-
based weighted-majority prediction for recom-
mender systems.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228–5235, April.
Thomas Hofmann. 2003. Collaborative filtering via
gaussian probabilistic latent semantic analysis. In
Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in
informaion retrieval, SIGIR ’03, pages 259–266,
New York, NY, USA. ACM.
Thomas Hofmann. 2004. Latent semantic models
for collaborative filtering. ACM Trans. Inf. Syst.,
22(1):89–115.
Qing Li, Jia Wang, Yuanzhu Peter Chen, and Zhangxi
Lin. 2010. User comments for news recom-
mendation in forum-based social media. Inf. Sci.,
180:4929–4939, December.
Nick Littlestone. 1988. Learning quickly when irrele-
vant attributes abound: A new linear-threshold algo-
rithm. In Machine Learning, pages 285–318.
Atsuyoshi Nakamura and Naoki Abe. 1998. Collab-
orative filtering using weighted majority prediction
algorithms. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, ICML ’98,
pages 395–403, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Dmitry Pavlov, Ramnath Balasubramanyan, Byron
Dom, Shyam Kapur, and Jignashu Parikh. 2004.
Document preprocessing for naive bayes classifica-
tion and clustering with mixture of multinomials. In
Proceedings of the tenth ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, KDD ’04, pages 829–834, New York, NY, USA.
ACM.
Michael Pazzani, Daniel Billsus, S. Michalski, and
Janusz Wnek. 1997. Learning and revising user pro-
files: The identification of interesting web sites. In
Machine Learning, pages 313–331.
Elaine Rich. 1979. User modeling via stereotypes.
Cognitive Science, 3(4):329–354.
J. Rocchio, 1971. Relevance Feedback in Information
Retrieval.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval.
In INFORMATION PROCESSING AND MANAGE-
MENT, pages 513–523.
Badrul Sarwar, George Karypis, Joseph Konstan, and
John Reidl. 2001. Item-based collaborative fil-
tering recommendation algorithms. In WWW ’01:
Proceedings of the 10th international conference on
World Wide Web, pages 285–295, New York, NY,
USA. ACM.
Upendra Shardanand and Pattie Maes. 1995. So-
cial information filtering: Algorithms for automating
“word of mouth”. In CHI, pages 210–217.
Lyle Ungar, Dean Foster, Ellen Andre, Star Wars,
Fred Star Wars, Dean Star Wars, and Jason Hiver
Whispers. 1998. Clustering methods for collabo-
rative filtering. AAAI Press.
Hao Wu, Jiajun Bu, Chun Chen, Can Wang, Guang Qiu,
Lijun Zhang, and Jianfeng Shen. 2010. Modeling
dynamic multi-topic discussions in online forums. In
AAAI.
</reference>
<page confidence="0.999101">
376
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912301">
<title confidence="0.999133">User Participation Prediction in Online Forums</title>
<author confidence="0.966128">Qu</author>
<affiliation confidence="0.935841">The University of Texas at</affiliation>
<abstract confidence="0.999416269230769">Online community is an important source for latest news and information. Accurate prediction of a user’s interest can help provide better user experience. In this paper, we develop a recommendation system for online forums. There are a lot of differences between online forums and formal media. For example, content generated by users in online forums contains more noise compared to formal documents. Content topics in the same forum are more focused than sources like news websites. Some of these differences present challenges to traditional word-based user profiling and recommendation systems, but some also provide opportunities for better recommendation performance. In our recommendation system, we propose to (a) use latent topics to interpolate with content-based recommendation; (b) model latent user groups to utilize information from other users. We have collected three types of forum data sets. Our experimental results demonstrate that our proposed hybrid approach works well in all three types of forums.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gediminas Adomavicius</author>
<author>Alexander Tuzhilin</author>
</authors>
<title>Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions.</title>
<date>2005</date>
<journal>IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING,</journal>
<volume>17</volume>
<issue>6</issue>
<contexts>
<context position="4628" citStr="Adomavicius and Tuzhilin, 2005" startWordPosition="715" endWordPosition="718">representative forums. Our experimental results show that in all forums, by using latent topics information, system can achieve better accuracy in predicting threads for recommendation. In addition, by modeling latent user groups in thread participation, further improvement is achieved in the hybrid system. Our analysis also showed that each forum has its nature, resulting in different optimal parameters in the different forums. 2 Related Work Recommendation systems can help make information retrieving process more intelligent. Generally, recommendation methods are categorized into two types (Adomavicius and Tuzhilin, 2005), contentbased filtering and collaborative filtering. Systems using content-based filtering use the content information of recommendation items a user is interested in to recommend new items to the user. For example, in a news recommendation system, in order to recommend appropriate news articles to a user, it finds the most prominent features (e.g., key words, tags, category) in the document that a user likes, then suggests similar articles based on this “personal profile”. In Fabs system (Balabanovic and Shoham, 1997), Skyskill &amp; Webert system (Pazzani et al., 1997), documents are represente</context>
</contexts>
<marker>Adomavicius, Tuzhilin, 2005</marker>
<rawString>Gediminas Adomavicius and Alexander Tuzhilin. 2005. Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 17(6):734–749.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marko Balabanovic</author>
<author>Yoav Shoham</author>
</authors>
<title>Fab: Content-based, collaborative recommendation.</title>
<date>1997</date>
<journal>Communications of the ACM,</journal>
<pages>40--66</pages>
<contexts>
<context position="5153" citStr="Balabanovic and Shoham, 1997" startWordPosition="797" endWordPosition="800">igent. Generally, recommendation methods are categorized into two types (Adomavicius and Tuzhilin, 2005), contentbased filtering and collaborative filtering. Systems using content-based filtering use the content information of recommendation items a user is interested in to recommend new items to the user. For example, in a news recommendation system, in order to recommend appropriate news articles to a user, it finds the most prominent features (e.g., key words, tags, category) in the document that a user likes, then suggests similar articles based on this “personal profile”. In Fabs system (Balabanovic and Shoham, 1997), Skyskill &amp; Webert system (Pazzani et al., 1997), documents are represented using a set of most important words according to a weighting measure. The most popular measure of word “importance” is TF-IDF (term frequency, inverse document frequency) (Salton and Buckley, 1988), which gives weights to words according to its “informativeness”. Then, base on this “personal profile” a ranking machine is applied to give a ranked recommendation list. In Fabs system, Rocchio’ algorithm (Rocchio, 1971) is used to learn the average TF-IDF vector of highly rated documents. Skyskill &amp; Webert’s system uses N</context>
</contexts>
<marker>Balabanovic, Shoham, 1997</marker>
<rawString>Marko Balabanovic and Yoav Shoham. 1997. Fab: Content-based, collaborative recommendation. Communications of the ACM, 40:66–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chumki Basu</author>
<author>Haym Hirsh</author>
<author>William Cohen</author>
</authors>
<title>Recommendation as classification: Using social and content-based information in recommendation. In</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth National Conference on Artificial Intelligence,</booktitle>
<pages>714--720</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="8073" citStr="Basu et al., 1998" startWordPosition="1258" endWordPosition="1261">including a Bayesian model (Chien and George, 1999), linear regression model (Sarwar et al., 2001), Gaussian mixture models (Hofmann, 2003; Hofmann, 2004). In (Blei et al., 2001) a collaborative filtering application is discussed using LDA. However in this model, re-estimation of parameters for the whole system is needed when a new item comes in. In 368 this paper, we formulate users’ participation differently using the LDA mixture model. Some previous work has also evaluated using a hybrid model with both content and collaborative features and showed outstanding performance. For example, in (Basu et al., 1998), hybrid features are used to make recommendation using inductive learning. 3 Forum Data We have collected data from three forums in this study.1 Ubuntu community forum is a technical support forum; World of Warcraft (WoW) forum is about gaming; Fitness forum is about how to live a healthy life. These three forums are quite representative of online forums on the internet. Using three different types of forums for task evaluation helps to demonstrate the robustness of our proposed method. In addition, it can show how the same method could have substantial performance difference on forums of dif</context>
</contexts>
<marker>Basu, Hirsh, Cohen, 1998</marker>
<rawString>Chumki Basu, Haym Hirsh, and William Cohen. 1998. Recommendation as classification: Using social and content-based information in recommendation. In In Proceedings of the Fifteenth National Conference on Artificial Intelligence, pages 714–720. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul N Bennett</author>
</authors>
<title>Assessing the calibration of naive bayes’ posterior estimates.</title>
<date>2000</date>
<contexts>
<context position="13772" citStr="Bennett, 2000" startWordPosition="2224" endWordPosition="2225">e in web page recommendation compared to several other classifiers. A Naive Bayes classifier is a generative model in which words inside a document are assumed to be conditionally independent. That is, given the class of a document, words are generated independently. The posterior probability of a test instance in Naive Bayes classifier takes the following form: 1 � P (Ci|f1..k) = Z P (Ci) j where Z is the class label independent normalization term, fi..k is the bag-of-word feature vector for the document. Naive Bayes classifier is known for not having a well calibrated posterior probability (Bennett, 2000). (Pavlov et al., 2004) showed that normalization by document length yielded good empirical results in approximating a well calibrated posterior probability for Naive Bayes classifier. The normalized Naive Bayes classifier they used is as follows: 1 � P (Ci|f1..k) = Z P (Ci) j In this equation, the probability of generating each word is normalized by the length of the feature vector |f|. The posterior probability P(interested|f1..k) from (normalized) Naive Bayes classifier is used for recommendation item ranking. 4.1.2 Latent Topics based Interpolation Because of noisy forum writing and limite</context>
</contexts>
<marker>Bennett, 2000</marker>
<rawString>Paul N. Bennett. 2000. Assessing the calibration of naive bayes’ posterior estimates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2001</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--2003</pages>
<contexts>
<context position="7633" citStr="Blei et al., 2001" startWordPosition="1187" endWordPosition="1190">ve users that participate in it. Model-based algorithms, on the other hand, try to formulate the probability function of one item being liked statistically using active user information. (Ungar et al., 1998) clustered similar users into groups for recommendation. Different clustering methods have been experimented, including K-means and Gibbs Sampling. Other probabilistic models have also been used to model collaborative relationships, including a Bayesian model (Chien and George, 1999), linear regression model (Sarwar et al., 2001), Gaussian mixture models (Hofmann, 2003; Hofmann, 2004). In (Blei et al., 2001) a collaborative filtering application is discussed using LDA. However in this model, re-estimation of parameters for the whole system is needed when a new item comes in. In 368 this paper, we formulate users’ participation differently using the LDA mixture model. Some previous work has also evaluated using a hybrid model with both content and collaborative features and showed outstanding performance. For example, in (Basu et al., 1998), hybrid features are used to make recommendation using inductive learning. 3 Forum Data We have collected data from three forums in this study.1 Ubuntu communi</context>
<context position="14872" citStr="Blei et al., 2001" startWordPosition="2397" endWordPosition="2400">sed for recommendation item ranking. 4.1.2 Latent Topics based Interpolation Because of noisy forum writing and limited training data, the above bag-of-word model used in naive Bayes classifier may suffer from data sparsity issues. We thus propose to use latent topic modeling to alleviate this problem. Latent Dirichlet Allocation (LDA) is a generative model based on latent topics. The major difference between LDA and previous methods such as probabilistic Latent Semantic Analysis (pLSA) is that LDA can efficiently infer topic composition of new documents, regardless of the training data size (Blei et al., 2001). This makes it ideal for efficiently reducing the dimension of incoming documents. In an online forum, words contained in threads tend to be very noisy. Irregular words, such as abbreviation, misspelling and synonyms, are very common in an online environment. From our experiments, we observe that LDA seems to be quite robust to these phenomena and able to capture word relationship semantically. To illustrate the words inside latent topics in the LDA model inferred from online forums, we show in Table 2 the top words in 3 out of 20 latent topics inferred from “Ubuntu” forum according to its mu</context>
</contexts>
<marker>Blei, Ng, Jordan, 2001</marker>
<rawString>David Blei, Andrew Y. Ng, and Michael I. Jordan. 2001. Latent dirichlet allocation. Journal of Machine Learning Research, 3:2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Breese</author>
<author>David Heckerman</author>
<author>Carl Kadie</author>
</authors>
<title>Empirical analysis of predictive algorithms for collaborative filtering.</title>
<date>1998</date>
<pages>43--52</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="6749" citStr="Breese et al., 1998" startWordPosition="1050" endWordPosition="1053">s introduced to rank the likelihood of user participating in a thread in online forums. Collaborative-filtering based systems, unlike content-based systems, predict the recommending items using co-occurrence information between users. For example, in a news recommendation system, in order to recommend an article to user c, the system tries to find users with similar taste as c. Items favored by similar users would be recommended. Grundy (Rich, 1979) is known to be one of the first collaborative-filtering based systems. Collaborative filtering systems can be either model based or memory based (Breese et al., 1998). Memory-based algorithms, such as (Delgado and Ishii, 1999; Nakamura and Abe, 1998; Shardanand and Maes, 1995), use a utility function to measure the similarity between users. Then recommendation of an item is made according to the sum of the utility values of active users that participate in it. Model-based algorithms, on the other hand, try to formulate the probability function of one item being liked statistically using active user information. (Ungar et al., 1998) clustered similar users into groups for recommendation. Different clustering methods have been experimented, including K-means</context>
</contexts>
<marker>Breese, Heckerman, Kadie, 1998</marker>
<rawString>John S. Breese, David Heckerman, and Carl Kadie. 1998. Empirical analysis of predictive algorithms for collaborative filtering. pages 43–52. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y H Chien</author>
<author>E I George</author>
</authors>
<title>A bayesian model for collaborative filtering.</title>
<date>1999</date>
<journal>Number</journal>
<volume>1</volume>
<contexts>
<context position="7506" citStr="Chien and George, 1999" startWordPosition="1166" endWordPosition="1169">n to measure the similarity between users. Then recommendation of an item is made according to the sum of the utility values of active users that participate in it. Model-based algorithms, on the other hand, try to formulate the probability function of one item being liked statistically using active user information. (Ungar et al., 1998) clustered similar users into groups for recommendation. Different clustering methods have been experimented, including K-means and Gibbs Sampling. Other probabilistic models have also been used to model collaborative relationships, including a Bayesian model (Chien and George, 1999), linear regression model (Sarwar et al., 2001), Gaussian mixture models (Hofmann, 2003; Hofmann, 2004). In (Blei et al., 2001) a collaborative filtering application is discussed using LDA. However in this model, re-estimation of parameters for the whole system is needed when a new item comes in. In 368 this paper, we formulate users’ participation differently using the LDA mixture model. Some previous work has also evaluated using a hybrid model with both content and collaborative features and showed outstanding performance. For example, in (Basu et al., 1998), hybrid features are used to mak</context>
</contexts>
<marker>Chien, George, 1999</marker>
<rawString>Y H Chien and E I George, 1999. A bayesian model for collaborative filtering. Number 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joaquin Delgado</author>
<author>Naohiro Ishii</author>
</authors>
<title>Memorybased weighted-majority prediction for recommender systems.</title>
<date>1999</date>
<contexts>
<context position="6808" citStr="Delgado and Ishii, 1999" startWordPosition="1058" endWordPosition="1062">g in a thread in online forums. Collaborative-filtering based systems, unlike content-based systems, predict the recommending items using co-occurrence information between users. For example, in a news recommendation system, in order to recommend an article to user c, the system tries to find users with similar taste as c. Items favored by similar users would be recommended. Grundy (Rich, 1979) is known to be one of the first collaborative-filtering based systems. Collaborative filtering systems can be either model based or memory based (Breese et al., 1998). Memory-based algorithms, such as (Delgado and Ishii, 1999; Nakamura and Abe, 1998; Shardanand and Maes, 1995), use a utility function to measure the similarity between users. Then recommendation of an item is made according to the sum of the utility values of active users that participate in it. Model-based algorithms, on the other hand, try to formulate the probability function of one item being liked statistically using active user information. (Ungar et al., 1998) clustered similar users into groups for recommendation. Different clustering methods have been experimented, including K-means and Gibbs Sampling. Other probabilistic models have also b</context>
</contexts>
<marker>Delgado, Ishii, 1999</marker>
<rawString>Joaquin Delgado and Naohiro Ishii. 1999. Memorybased weighted-majority prediction for recommender systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl 1):5228–5235,</booktitle>
<contexts>
<context position="20610" citStr="Griffiths and Steyvers, 2004" startWordPosition="3352" endWordPosition="3355">roup zn — Multinomial(θ) (b) Choose a user un — p(un|zn) One thing worth noting is that in LDA model a document is assumed to consist of many words. In the case of modeling user participation, a thread typically has far fewer users than words inside a document. This could potentially cause problem during variable estimation and inference. However, we show that this approach actually works well in practice (experimental results in Section 5). 4.2.2 Using Latent User Groups for Prediction For an incoming new thread, first the latent group distribution is inferred using collapsed Gibbs Sampling (Griffiths and Steyvers, 2004). The posterior probability of a user ui participating in thread j given the user group distribution is as follows. P(ui|θj,φ) = � P(ui|φk)P(k|θj) (4) kET In the equation, φk is the multinomial distribution of users in group k, T is the number of latent user groups, and θj is the group composition in thread j after inference using the training data. In general, the probability of user ui appearing in thread j is proportional to the membership probabilities of this user in the groups that compose the participating users. 4.3 Hybrid System Up to this point we have two separate systems that can g</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl 1):5228–5235, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Collaborative filtering via gaussian probabilistic latent semantic analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR ’03,</booktitle>
<pages>259--266</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7593" citStr="Hofmann, 2003" startWordPosition="1182" endWordPosition="1183">e sum of the utility values of active users that participate in it. Model-based algorithms, on the other hand, try to formulate the probability function of one item being liked statistically using active user information. (Ungar et al., 1998) clustered similar users into groups for recommendation. Different clustering methods have been experimented, including K-means and Gibbs Sampling. Other probabilistic models have also been used to model collaborative relationships, including a Bayesian model (Chien and George, 1999), linear regression model (Sarwar et al., 2001), Gaussian mixture models (Hofmann, 2003; Hofmann, 2004). In (Blei et al., 2001) a collaborative filtering application is discussed using LDA. However in this model, re-estimation of parameters for the whole system is needed when a new item comes in. In 368 this paper, we formulate users’ participation differently using the LDA mixture model. Some previous work has also evaluated using a hybrid model with both content and collaborative features and showed outstanding performance. For example, in (Basu et al., 1998), hybrid features are used to make recommendation using inductive learning. 3 Forum Data We have collected data from thr</context>
</contexts>
<marker>Hofmann, 2003</marker>
<rawString>Thomas Hofmann. 2003. Collaborative filtering via gaussian probabilistic latent semantic analysis. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR ’03, pages 259–266, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Latent semantic models for collaborative filtering.</title>
<date>2004</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="7609" citStr="Hofmann, 2004" startWordPosition="1184" endWordPosition="1185">ility values of active users that participate in it. Model-based algorithms, on the other hand, try to formulate the probability function of one item being liked statistically using active user information. (Ungar et al., 1998) clustered similar users into groups for recommendation. Different clustering methods have been experimented, including K-means and Gibbs Sampling. Other probabilistic models have also been used to model collaborative relationships, including a Bayesian model (Chien and George, 1999), linear regression model (Sarwar et al., 2001), Gaussian mixture models (Hofmann, 2003; Hofmann, 2004). In (Blei et al., 2001) a collaborative filtering application is discussed using LDA. However in this model, re-estimation of parameters for the whole system is needed when a new item comes in. In 368 this paper, we formulate users’ participation differently using the LDA mixture model. Some previous work has also evaluated using a hybrid model with both content and collaborative features and showed outstanding performance. For example, in (Basu et al., 1998), hybrid features are used to make recommendation using inductive learning. 3 Forum Data We have collected data from three forums in thi</context>
</contexts>
<marker>Hofmann, 2004</marker>
<rawString>Thomas Hofmann. 2004. Latent semantic models for collaborative filtering. ACM Trans. Inf. Syst., 22(1):89–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Li</author>
<author>Jia Wang</author>
<author>Yuanzhu Peter Chen</author>
<author>Zhangxi Lin</author>
</authors>
<title>User comments for news recommendation in forum-based social media.</title>
<date>2010</date>
<journal>Inf. Sci.,</journal>
<volume>180</volume>
<contexts>
<context position="6025" citStr="Li et al., 2010" startWordPosition="937" endWordPosition="940">and Buckley, 1988), which gives weights to words according to its “informativeness”. Then, base on this “personal profile” a ranking machine is applied to give a ranked recommendation list. In Fabs system, Rocchio’ algorithm (Rocchio, 1971) is used to learn the average TF-IDF vector of highly rated documents. Skyskill &amp; Webert’s system uses Naive Bayes classifiers to give the probability of documents being liked. Winnow’s algorithm (Littlestone, 1988), which is similar to perception algorithm, has been shown to perform well when there are many features. An adaptive framework is introduced in (Li et al., 2010) using forum comments for news recommendation. In (Wu et al., 2010), a topic-specific topic flow model is introduced to rank the likelihood of user participating in a thread in online forums. Collaborative-filtering based systems, unlike content-based systems, predict the recommending items using co-occurrence information between users. For example, in a news recommendation system, in order to recommend an article to user c, the system tries to find users with similar taste as c. Items favored by similar users would be recommended. Grundy (Rich, 1979) is known to be one of the first collaborat</context>
</contexts>
<marker>Li, Wang, Chen, Lin, 2010</marker>
<rawString>Qing Li, Jia Wang, Yuanzhu Peter Chen, and Zhangxi Lin. 2010. User comments for news recommendation in forum-based social media. Inf. Sci., 180:4929–4939, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Littlestone</author>
</authors>
<title>Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm.</title>
<date>1988</date>
<booktitle>In Machine Learning,</booktitle>
<pages>285--318</pages>
<contexts>
<context position="5864" citStr="Littlestone, 1988" startWordPosition="909" endWordPosition="911">ost important words according to a weighting measure. The most popular measure of word “importance” is TF-IDF (term frequency, inverse document frequency) (Salton and Buckley, 1988), which gives weights to words according to its “informativeness”. Then, base on this “personal profile” a ranking machine is applied to give a ranked recommendation list. In Fabs system, Rocchio’ algorithm (Rocchio, 1971) is used to learn the average TF-IDF vector of highly rated documents. Skyskill &amp; Webert’s system uses Naive Bayes classifiers to give the probability of documents being liked. Winnow’s algorithm (Littlestone, 1988), which is similar to perception algorithm, has been shown to perform well when there are many features. An adaptive framework is introduced in (Li et al., 2010) using forum comments for news recommendation. In (Wu et al., 2010), a topic-specific topic flow model is introduced to rank the likelihood of user participating in a thread in online forums. Collaborative-filtering based systems, unlike content-based systems, predict the recommending items using co-occurrence information between users. For example, in a news recommendation system, in order to recommend an article to user c, the system</context>
</contexts>
<marker>Littlestone, 1988</marker>
<rawString>Nick Littlestone. 1988. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. In Machine Learning, pages 285–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsuyoshi Nakamura</author>
<author>Naoki Abe</author>
</authors>
<title>Collaborative filtering using weighted majority prediction algorithms.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning, ICML ’98,</booktitle>
<pages>395--403</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="6832" citStr="Nakamura and Abe, 1998" startWordPosition="1063" endWordPosition="1066">orums. Collaborative-filtering based systems, unlike content-based systems, predict the recommending items using co-occurrence information between users. For example, in a news recommendation system, in order to recommend an article to user c, the system tries to find users with similar taste as c. Items favored by similar users would be recommended. Grundy (Rich, 1979) is known to be one of the first collaborative-filtering based systems. Collaborative filtering systems can be either model based or memory based (Breese et al., 1998). Memory-based algorithms, such as (Delgado and Ishii, 1999; Nakamura and Abe, 1998; Shardanand and Maes, 1995), use a utility function to measure the similarity between users. Then recommendation of an item is made according to the sum of the utility values of active users that participate in it. Model-based algorithms, on the other hand, try to formulate the probability function of one item being liked statistically using active user information. (Ungar et al., 1998) clustered similar users into groups for recommendation. Different clustering methods have been experimented, including K-means and Gibbs Sampling. Other probabilistic models have also been used to model collab</context>
</contexts>
<marker>Nakamura, Abe, 1998</marker>
<rawString>Atsuyoshi Nakamura and Naoki Abe. 1998. Collaborative filtering using weighted majority prediction algorithms. In Proceedings of the Fifteenth International Conference on Machine Learning, ICML ’98, pages 395–403, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Pavlov</author>
<author>Ramnath Balasubramanyan</author>
<author>Byron Dom</author>
<author>Shyam Kapur</author>
<author>Jignashu Parikh</author>
</authors>
<title>Document preprocessing for naive bayes classification and clustering with mixture of multinomials.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’04,</booktitle>
<pages>829--834</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="13795" citStr="Pavlov et al., 2004" startWordPosition="2226" endWordPosition="2229">ommendation compared to several other classifiers. A Naive Bayes classifier is a generative model in which words inside a document are assumed to be conditionally independent. That is, given the class of a document, words are generated independently. The posterior probability of a test instance in Naive Bayes classifier takes the following form: 1 � P (Ci|f1..k) = Z P (Ci) j where Z is the class label independent normalization term, fi..k is the bag-of-word feature vector for the document. Naive Bayes classifier is known for not having a well calibrated posterior probability (Bennett, 2000). (Pavlov et al., 2004) showed that normalization by document length yielded good empirical results in approximating a well calibrated posterior probability for Naive Bayes classifier. The normalized Naive Bayes classifier they used is as follows: 1 � P (Ci|f1..k) = Z P (Ci) j In this equation, the probability of generating each word is normalized by the length of the feature vector |f|. The posterior probability P(interested|f1..k) from (normalized) Naive Bayes classifier is used for recommendation item ranking. 4.1.2 Latent Topics based Interpolation Because of noisy forum writing and limited training data, the ab</context>
<context position="27120" citStr="Pavlov et al., 2004" startWordPosition="4480" endWordPosition="4483">e three forums. In “Wow Gaming” corpus, the optimal performance is achieved with a relatively high -y value (at around 0.5), and it is even higher for the “Fitness” forum. 373 This means that the system relies more on the latent topics information. This is because in these forums, casual conversation contains more irregular words, causing more severe data sparsity problem than others. Between the two naive Bayes classifiers, we can see that using normalized probabilities outperforms the original one in “Wow Gaming” and “Ubuntu” forums. This observation is consistent with previous work (e.g., (Pavlov et al., 2004)). However, we found that in “Fitness Forum”, the performance degrades with normalization. Further work is still needed to understand why this is the case. 5.2 Latent User Group Classification In this section, collaborative filtering using latent user groups is evaluated. First, participating users from the training set are used to estimate an LDA model. Then, users participating in a thread are used to infer the topic distribution of the thread. Candidate threads are then sorted by the probability of a target user’s participation according to Equation 4. Note that all the users in the forum a</context>
</contexts>
<marker>Pavlov, Balasubramanyan, Dom, Kapur, Parikh, 2004</marker>
<rawString>Dmitry Pavlov, Ramnath Balasubramanyan, Byron Dom, Shyam Kapur, and Jignashu Parikh. 2004. Document preprocessing for naive bayes classification and clustering with mixture of multinomials. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’04, pages 829–834, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Pazzani</author>
<author>Daniel Billsus</author>
<author>S Michalski</author>
<author>Janusz Wnek</author>
</authors>
<title>Learning and revising user profiles: The identification of interesting web sites.</title>
<date>1997</date>
<booktitle>In Machine Learning,</booktitle>
<pages>313--331</pages>
<contexts>
<context position="5202" citStr="Pazzani et al., 1997" startWordPosition="806" endWordPosition="809">into two types (Adomavicius and Tuzhilin, 2005), contentbased filtering and collaborative filtering. Systems using content-based filtering use the content information of recommendation items a user is interested in to recommend new items to the user. For example, in a news recommendation system, in order to recommend appropriate news articles to a user, it finds the most prominent features (e.g., key words, tags, category) in the document that a user likes, then suggests similar articles based on this “personal profile”. In Fabs system (Balabanovic and Shoham, 1997), Skyskill &amp; Webert system (Pazzani et al., 1997), documents are represented using a set of most important words according to a weighting measure. The most popular measure of word “importance” is TF-IDF (term frequency, inverse document frequency) (Salton and Buckley, 1988), which gives weights to words according to its “informativeness”. Then, base on this “personal profile” a ranking machine is applied to give a ranked recommendation list. In Fabs system, Rocchio’ algorithm (Rocchio, 1971) is used to learn the average TF-IDF vector of highly rated documents. Skyskill &amp; Webert’s system uses Naive Bayes classifiers to give the probability of</context>
<context position="13102" citStr="Pazzani et al., 1997" startWordPosition="2112" endWordPosition="2115">h is a sufficient condition for interestedness. This approach is also used by (Wu et al., 2010) for their task evaluation. In this section, we describe our proposed approaches for thread participation prediction. 4.1 Content-based Filtering In the content-based filtering approach, only content of a thread is used as features for prediction. Recommendation through content-based filtering has its deep root in information retrieval. Here we use a Naive Bayes classifier for ranking the threads using information based on the words and the latent topic analysis. 4.1.1 Naive Bayes Classification In (Pazzani et al., 1997) Naive Bayesian classifier showed outstanding performance in web page recommendation compared to several other classifiers. A Naive Bayes classifier is a generative model in which words inside a document are assumed to be conditionally independent. That is, given the class of a document, words are generated independently. The posterior probability of a test instance in Naive Bayes classifier takes the following form: 1 � P (Ci|f1..k) = Z P (Ci) j where Z is the class label independent normalization term, fi..k is the bag-of-word feature vector for the document. Naive Bayes classifier is known </context>
</contexts>
<marker>Pazzani, Billsus, Michalski, Wnek, 1997</marker>
<rawString>Michael Pazzani, Daniel Billsus, S. Michalski, and Janusz Wnek. 1997. Learning and revising user profiles: The identification of interesting web sites. In Machine Learning, pages 313–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elaine Rich</author>
</authors>
<title>User modeling via stereotypes.</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="6582" citStr="Rich, 1979" startWordPosition="1024" endWordPosition="1025"> adaptive framework is introduced in (Li et al., 2010) using forum comments for news recommendation. In (Wu et al., 2010), a topic-specific topic flow model is introduced to rank the likelihood of user participating in a thread in online forums. Collaborative-filtering based systems, unlike content-based systems, predict the recommending items using co-occurrence information between users. For example, in a news recommendation system, in order to recommend an article to user c, the system tries to find users with similar taste as c. Items favored by similar users would be recommended. Grundy (Rich, 1979) is known to be one of the first collaborative-filtering based systems. Collaborative filtering systems can be either model based or memory based (Breese et al., 1998). Memory-based algorithms, such as (Delgado and Ishii, 1999; Nakamura and Abe, 1998; Shardanand and Maes, 1995), use a utility function to measure the similarity between users. Then recommendation of an item is made according to the sum of the utility values of active users that participate in it. Model-based algorithms, on the other hand, try to formulate the probability function of one item being liked statistically using activ</context>
</contexts>
<marker>Rich, 1979</marker>
<rawString>Elaine Rich. 1979. User modeling via stereotypes. Cognitive Science, 3(4):329–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rocchio</author>
</authors>
<title>Relevance Feedback in Information Retrieval.</title>
<date>1971</date>
<contexts>
<context position="5649" citStr="Rocchio, 1971" startWordPosition="876" endWordPosition="877"> likes, then suggests similar articles based on this “personal profile”. In Fabs system (Balabanovic and Shoham, 1997), Skyskill &amp; Webert system (Pazzani et al., 1997), documents are represented using a set of most important words according to a weighting measure. The most popular measure of word “importance” is TF-IDF (term frequency, inverse document frequency) (Salton and Buckley, 1988), which gives weights to words according to its “informativeness”. Then, base on this “personal profile” a ranking machine is applied to give a ranked recommendation list. In Fabs system, Rocchio’ algorithm (Rocchio, 1971) is used to learn the average TF-IDF vector of highly rated documents. Skyskill &amp; Webert’s system uses Naive Bayes classifiers to give the probability of documents being liked. Winnow’s algorithm (Littlestone, 1988), which is similar to perception algorithm, has been shown to perform well when there are many features. An adaptive framework is introduced in (Li et al., 2010) using forum comments for news recommendation. In (Wu et al., 2010), a topic-specific topic flow model is introduced to rank the likelihood of user participating in a thread in online forums. Collaborative-filtering based sy</context>
</contexts>
<marker>Rocchio, 1971</marker>
<rawString>J. Rocchio, 1971. Relevance Feedback in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Christopher Buckley</author>
</authors>
<title>Termweighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>In INFORMATION PROCESSING AND MANAGEMENT,</booktitle>
<pages>513--523</pages>
<contexts>
<context position="5427" citStr="Salton and Buckley, 1988" startWordPosition="840" endWordPosition="843">ommend new items to the user. For example, in a news recommendation system, in order to recommend appropriate news articles to a user, it finds the most prominent features (e.g., key words, tags, category) in the document that a user likes, then suggests similar articles based on this “personal profile”. In Fabs system (Balabanovic and Shoham, 1997), Skyskill &amp; Webert system (Pazzani et al., 1997), documents are represented using a set of most important words according to a weighting measure. The most popular measure of word “importance” is TF-IDF (term frequency, inverse document frequency) (Salton and Buckley, 1988), which gives weights to words according to its “informativeness”. Then, base on this “personal profile” a ranking machine is applied to give a ranked recommendation list. In Fabs system, Rocchio’ algorithm (Rocchio, 1971) is used to learn the average TF-IDF vector of highly rated documents. Skyskill &amp; Webert’s system uses Naive Bayes classifiers to give the probability of documents being liked. Winnow’s algorithm (Littlestone, 1988), which is similar to perception algorithm, has been shown to perform well when there are many features. An adaptive framework is introduced in (Li et al., 2010) u</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Gerard Salton and Christopher Buckley. 1988. Termweighting approaches in automatic text retrieval. In INFORMATION PROCESSING AND MANAGEMENT, pages 513–523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Badrul Sarwar</author>
<author>George Karypis</author>
<author>Joseph Konstan</author>
<author>John Reidl</author>
</authors>
<title>Item-based collaborative filtering recommendation algorithms.</title>
<date>2001</date>
<booktitle>In WWW ’01: Proceedings of the 10th international conference on World Wide Web,</booktitle>
<pages>285--295</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7553" citStr="Sarwar et al., 2001" startWordPosition="1174" endWordPosition="1177">commendation of an item is made according to the sum of the utility values of active users that participate in it. Model-based algorithms, on the other hand, try to formulate the probability function of one item being liked statistically using active user information. (Ungar et al., 1998) clustered similar users into groups for recommendation. Different clustering methods have been experimented, including K-means and Gibbs Sampling. Other probabilistic models have also been used to model collaborative relationships, including a Bayesian model (Chien and George, 1999), linear regression model (Sarwar et al., 2001), Gaussian mixture models (Hofmann, 2003; Hofmann, 2004). In (Blei et al., 2001) a collaborative filtering application is discussed using LDA. However in this model, re-estimation of parameters for the whole system is needed when a new item comes in. In 368 this paper, we formulate users’ participation differently using the LDA mixture model. Some previous work has also evaluated using a hybrid model with both content and collaborative features and showed outstanding performance. For example, in (Basu et al., 1998), hybrid features are used to make recommendation using inductive learning. 3 Fo</context>
</contexts>
<marker>Sarwar, Karypis, Konstan, Reidl, 2001</marker>
<rawString>Badrul Sarwar, George Karypis, Joseph Konstan, and John Reidl. 2001. Item-based collaborative filtering recommendation algorithms. In WWW ’01: Proceedings of the 10th international conference on World Wide Web, pages 285–295, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Upendra Shardanand</author>
<author>Pattie Maes</author>
</authors>
<title>Social information filtering: Algorithms for automating “word of mouth”.</title>
<date>1995</date>
<booktitle>In CHI,</booktitle>
<pages>210--217</pages>
<contexts>
<context position="6860" citStr="Shardanand and Maes, 1995" startWordPosition="1067" endWordPosition="1070">tering based systems, unlike content-based systems, predict the recommending items using co-occurrence information between users. For example, in a news recommendation system, in order to recommend an article to user c, the system tries to find users with similar taste as c. Items favored by similar users would be recommended. Grundy (Rich, 1979) is known to be one of the first collaborative-filtering based systems. Collaborative filtering systems can be either model based or memory based (Breese et al., 1998). Memory-based algorithms, such as (Delgado and Ishii, 1999; Nakamura and Abe, 1998; Shardanand and Maes, 1995), use a utility function to measure the similarity between users. Then recommendation of an item is made according to the sum of the utility values of active users that participate in it. Model-based algorithms, on the other hand, try to formulate the probability function of one item being liked statistically using active user information. (Ungar et al., 1998) clustered similar users into groups for recommendation. Different clustering methods have been experimented, including K-means and Gibbs Sampling. Other probabilistic models have also been used to model collaborative relationships, inclu</context>
</contexts>
<marker>Shardanand, Maes, 1995</marker>
<rawString>Upendra Shardanand and Pattie Maes. 1995. Social information filtering: Algorithms for automating “word of mouth”. In CHI, pages 210–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyle Ungar</author>
<author>Dean Foster</author>
<author>Ellen Andre</author>
<author>Star Wars</author>
</authors>
<title>Fred Star Wars, Dean Star Wars, and Jason Hiver Whispers.</title>
<date>1998</date>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="7222" citStr="Ungar et al., 1998" startWordPosition="1127" endWordPosition="1130">f the first collaborative-filtering based systems. Collaborative filtering systems can be either model based or memory based (Breese et al., 1998). Memory-based algorithms, such as (Delgado and Ishii, 1999; Nakamura and Abe, 1998; Shardanand and Maes, 1995), use a utility function to measure the similarity between users. Then recommendation of an item is made according to the sum of the utility values of active users that participate in it. Model-based algorithms, on the other hand, try to formulate the probability function of one item being liked statistically using active user information. (Ungar et al., 1998) clustered similar users into groups for recommendation. Different clustering methods have been experimented, including K-means and Gibbs Sampling. Other probabilistic models have also been used to model collaborative relationships, including a Bayesian model (Chien and George, 1999), linear regression model (Sarwar et al., 2001), Gaussian mixture models (Hofmann, 2003; Hofmann, 2004). In (Blei et al., 2001) a collaborative filtering application is discussed using LDA. However in this model, re-estimation of parameters for the whole system is needed when a new item comes in. In 368 this paper,</context>
</contexts>
<marker>Ungar, Foster, Andre, Wars, 1998</marker>
<rawString>Lyle Ungar, Dean Foster, Ellen Andre, Star Wars, Fred Star Wars, Dean Star Wars, and Jason Hiver Whispers. 1998. Clustering methods for collaborative filtering. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Wu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
<author>Can Wang</author>
<author>Guang Qiu</author>
<author>Lijun Zhang</author>
<author>Jianfeng Shen</author>
</authors>
<title>Modeling dynamic multi-topic discussions in online forums.</title>
<date>2010</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="6092" citStr="Wu et al., 2010" startWordPosition="948" endWordPosition="951">informativeness”. Then, base on this “personal profile” a ranking machine is applied to give a ranked recommendation list. In Fabs system, Rocchio’ algorithm (Rocchio, 1971) is used to learn the average TF-IDF vector of highly rated documents. Skyskill &amp; Webert’s system uses Naive Bayes classifiers to give the probability of documents being liked. Winnow’s algorithm (Littlestone, 1988), which is similar to perception algorithm, has been shown to perform well when there are many features. An adaptive framework is introduced in (Li et al., 2010) using forum comments for news recommendation. In (Wu et al., 2010), a topic-specific topic flow model is introduced to rank the likelihood of user participating in a thread in online forums. Collaborative-filtering based systems, unlike content-based systems, predict the recommending items using co-occurrence information between users. For example, in a news recommendation system, in order to recommend an article to user c, the system tries to find users with similar taste as c. Items favored by similar users would be recommended. Grundy (Rich, 1979) is known to be one of the first collaborative-filtering based systems. Collaborative filtering systems can be</context>
<context position="12576" citStr="Wu et al., 2010" startWordPosition="2033" endWordPosition="2036">n “Ubuntu” forum is very sparse for each user, having only 10.01% participating threads for each user after filtering. “Fitness” and “Wow Forum” have denser participation, at 18.97% and 13.86% respectively. 4 Interesting Thread Prediction In the task of interesting thread prediction, the system generates a ranked list of threads a user is likely to be interested in based on users’ past history of thread participation. Here, instead of predicting the true interestedness, we predict the participation of the user, which is a sufficient condition for interestedness. This approach is also used by (Wu et al., 2010) for their task evaluation. In this section, we describe our proposed approaches for thread participation prediction. 4.1 Content-based Filtering In the content-based filtering approach, only content of a thread is used as features for prediction. Recommendation through content-based filtering has its deep root in information retrieval. Here we use a Naive Bayes classifier for ranking the threads using information based on the words and the latent topic analysis. 4.1.1 Naive Bayes Classification In (Pazzani et al., 1997) Naive Bayesian classifier showed outstanding performance in web page reco</context>
</contexts>
<marker>Wu, Bu, Chen, Wang, Qiu, Zhang, Shen, 2010</marker>
<rawString>Hao Wu, Jiajun Bu, Chun Chen, Can Wang, Guang Qiu, Lijun Zhang, and Jianfeng Shen. 2010. Modeling dynamic multi-topic discussions in online forums. In AAAI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>