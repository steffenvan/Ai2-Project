<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004967">
<title confidence="0.990727">
A Unified Local and Global Model for Discourse Coherence
</title>
<author confidence="0.926145">
Micha Elsner, Joseph Austerweil, and Eugene Charniak
</author>
<affiliation confidence="0.8860425">
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
</affiliation>
<address confidence="0.88326">
Providence, RI 02912
</address>
<email confidence="0.998732">
{melsner,ec}@cs.brown.edu, joseph.austerweil@gmail.com
</email>
<sectionHeader confidence="0.993888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999864823529412">
We present a model for discourse co-
herence which combines the local entity-
based approach of (Barzilay and Lapata,
2005) and the HMM-based content model
of (Barzilay and Lee, 2004). Unlike the
mixture model of (Soricut and Marcu,
2006), we learn local and global features
jointly, providing a better theoretical ex-
planation of how they are useful. As the
local component of our model we adapt
(Barzilay and Lapata, 2005) by relaxing
independence assumptions so that it is ef-
fective when estimated generatively. Our
model performs the ordering task compet-
itively with (Soricut and Marcu, 2006),
and significantly better than either of the
models it is based on.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992028">
Models of coherent discourse are central to several
tasks in natural language processing: such mod-
els have been used in text generation (Kibble and
Power, 2004) and evaluation of human-produced
text in educational applications (Miltsakaki and Ku-
kich, 2004; Higgins et al., 2004). Moreover, an ac-
curate model can reveal information about document
structure, aiding in such tasks as supervised summa-
rization (Barzilay and Lapata, 2005).
Models of coherence tend to fall into two classes.
Local models (Lapata, 2003; Barzilay and Lapata,
2005; Foltz et al., 1998) attempt to capture the gen-
eralization that adjacent sentences often have similar
content, and therefore tend to contain related words.
Models of this type are good at finding sentences
that belong near one another in the document. How-
ever, they have trouble finding the beginning or end
of the document, or recovering from sudden shifts in
topic (such as occur at paragraph boundaries). Some
local models also have trouble deciding which of a
pair of related sentences ought to come first.
In contrast, the global HMM model of Barzilay
and Lee (2004) tries to track the predictable changes
in topic between sentences. This gives it a pro-
nounced advantage in ordering sentences, since it
can learn to represent beginnings, ends and bound-
aries as separate states. However, it has no local
features; the particular words in each sentence are
generated based only on the current state of the doc-
ument. Since information can pass from sentence
to sentence only in this restricted manner, the model
sometimes fails to place sentences next to the correct
neighbors.
We attempt here to unify the two approaches by
constructing a model with both sentence-to-sentence
dependencies providing local cues, and a hidden
topic variable for global structure. Our local fea-
tures are based on the entity grid model of (Barzilay
and Lapata, 2005; Lapata and Barzilay, 2005). This
model has previously been most successful in a con-
ditional setting; to integrate it into our model, we
first relax its independence assumptions to improve
its performance when used generatively. Our global
model is an HMM like that of Barzilay and Lee
(2004), but with emission probabilities drawn from
the entity grid. We present results for two tasks,
the ordering task, on which global models usually
do well, and the discrimination task, on which lo-
cal models tend to outperform them. Our model im-
proves on purely global or local approaches on both
</bodyText>
<page confidence="0.989386">
436
</page>
<note confidence="0.7976495">
Proceedings of NAACL HLT 2007, pages 436–443,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.997886615384615">
tasks.
Previous work by Soricut and Marcu (2006) has
also attempted to integrate local and global fea-
tures using a mixture model, with promising results.
However, mixture models lack explanatory power;
since each of the individual component models is
known to be flawed, it is difficult to say that the com-
bination is theoretically more sound than the parts,
even if it usually works better. Moreover, since the
model we describe uses a strict subset of the fea-
tures used in the component models of (Soricut and
Marcu, 2006), we suspect that adding it to the mix-
ture would lead to still further improved results.
</bodyText>
<sectionHeader confidence="0.982289" genericHeader="method">
2 Naive Entity Grids
</sectionHeader>
<bodyText confidence="0.999895407407408">
Entity grids, first described in (Lapata and Barzilay,
2005), are designed to capture some ideas of Cen-
tering Theory (Grosz et al., 1995), namely that ad-
jacent utterances in a locally coherent discourses are
likely to contain the same nouns, and that important
nouns often appear in syntactically important roles
such as subject or object. An entity grid represents
a document as a matrix with a column for each en-
tity, and a row for each sentence. The entry ri,j de-
scribes the syntactic role of entity j in sentence i:
these roles are subject (S), object (O), or some other
role (X)1. In addition there is a special marker (-)
for nouns which do not appear at all in a given sen-
tence. Each noun appears only once in a given row
of the grid; if a noun appears multiple times, its grid
symbol describes the most important of its syntac-
tic roles: subject if possible, then object, or finally
other. An example text is figure 1, whose grid is fig-
ure 2.
Nouns are also treated as salient or non-salient,
another important concern of Centering Theory. We
condition events involving a noun on the frequency
of that noun. Unfortunately, this way of representing
salience makes our model slightly deficient, since
the model conditions on a particular noun occurring
e.g. 2 times, but assigns nonzero probabilities to
documents where it occurs 3 times. This is theo-
</bodyText>
<footnote confidence="0.8372314">
1Roles are determined heuristically using trees produced by
the parser of (Charniak and Johnson, 2005). Following previous
work, we slightly conflate thematic and syntactic roles, marking
the subject of a passive verb as O.
2The numeric token “1300” is removed in preprocessing,
and “Nuevo Laredo” is marked as “PROPER”.
0 [The commercial pilot]O , [sole occupant of [the airplane]X]X
, was not injured.
1 [The airplane]O was owned and operated by [a private
owner]X .
2 [Visual meteorological conditions]S prevailed for [the per-
sonal cross country flight for which [a VFR flight plan]O was
filed]X .
3 [The flight]S originated at [Nuevo Laredo , Mexico]X , at
[approximately 1300]X.
</footnote>
<figureCaption confidence="0.9741205">
Figure 1: A section of a document, with syntactic
roles of noun phrases marked.
</figureCaption>
<equation confidence="0.998128333333334">
0 1 2 3
PLAN - - O -
AIRPLANE X O - -
CONDITION - - S -
FLIGHT - - X S
PILOT O - - -
PROPER - - - X
OWNER - X - -
OCCUPANT X - - -
</equation>
<figureCaption confidence="0.998171">
Figure 2: The entity grid for figure 12.
</figureCaption>
<bodyText confidence="0.999339941176471">
retically quite unpleasant but in comparing different
orderings of the same document, it seems not to do
too much damage.
Properly speaking entities may be referents of
many different nouns and pronouns throughout the
discourse, and both (Lapata and Barzilay, 2005) and
(Barzilay and Lapata, 2005) present models which
use coreference resolution systems to group nouns.
We follow (Soricut and Marcu, 2006) in dropping
this component of the system, and treat each head
noun as having an individual single referent.
To model transitions in this entity grid model,
Lapata and Barzilay (2005) takes a generative ap-
proach. First, the probability of a document is de-
fined as P(D) = P(Si..Sn), the joint probability of
all the sentences. Sentences are generated in order
conditioned on all previous sentences:
</bodyText>
<equation confidence="0.8633575">
P (D) = � P(Si|S0..(i−1)). (1)
i
</equation>
<bodyText confidence="0.999961875">
We make a Markov assumption of order h (in our
experiments h = 2) to shorten the history. We repre-
sent the truncated history as �Shi−1 = S(i−h)..S(i−1).
Each sentence Si can be split up into a set of
nouns representing entities, Ei, and their corre-
sponding syntactic roles Ri, plus a set of words
which are not entities, Wi. The model treats Wi as
independent of the previous sentences. For any fixed
</bodyText>
<page confidence="0.997816">
437
</page>
<bodyText confidence="0.9997775">
set of sentences Si, ni P(Wi) is always constant,
and so cannot help in finding a coherent ordering.
The probability of a sentence is therefore dependent
only on the entities:
</bodyText>
<equation confidence="0.991423">
P(Si |~Sh (i−1)) = P(Ei, Ri|~Sh (i−1)). (2)
</equation>
<bodyText confidence="0.9628911">
Next, the model assumes that each entity ej ap-
pears in sentences and takes on syntactic roles in-
dependent of all the other entities. As we show
in section 3, this assumption can be problem-
atic. Once we assume this, however, we can sim-
plify P(Ei,Ri|~Sh(i−1)) by calculating for each en-
tity whether it occurs in sentence i and if so, which
role it takes. This is equivalent to predicting ri,j.
We represent the history of the specific entity ej as
r~h(i−1),j = r(i−h),j..r(i−1),j, and write:
</bodyText>
<equation confidence="0.8100305">
P (Ei, Ri|~Sh (i−1)) ≈ 11 P(ri,j|~rh(i−1),j). (3)
j
</equation>
<bodyText confidence="0.999028871794872">
For instance, in figure 2, the probability of S3 with
horizon 1 is the product of P(S|X) (for FLIGHT),
P(X|−) (for PROPER), and likewise for each other
entity, P(−|O), P(−|S), P(−|−)3.
Although this generative approach outperforms
several models in correlation with coherence ratings
assigned by human judges, it suffers in comparison
with later systems. Barzilay and Lapata (2005) uses
the same grid representation, but treats the transi-
tion probabilities P(ri,j|~ri,j) for each document as
features for input to an SVM classifier. Soricut and
Marcu (2006)’s implementation of the entity-based
model also uses discriminative training.
The generative model’s main weakness in com-
parison to these conditional models is its assump-
tion of independence between entities. In real doc-
uments, each sentence tends to contain only a few
nouns, and even fewer of them can fill roles like
subject and object. In other words, nouns compete
with each other for the available syntactic positions
in sentences; once one noun is chosen as the sub-
ject, the probability that any other will also become
a subject (of a different subclause of the same sen-
tence) is drastically lowered. Since the generative
entity grid does not take this into account, it learns
that in general, the probability of any given entity
appearing in a specific sentence is low. Thus it gen-
erates blank sentences (those without any nouns at
all) with overwhelmingly high probability.
It may not be obvious that this misallocation of
probability mass also reduces the effectiveness of
the generative entity grid in ordering fixed sets of
sentences. However, consider the case where an en-
tity has a history r~ h, and then does not appear in
the next sentence. The model treats this as evidence
that entities generally do not occur immediately af-
ter r~h– but it may also happen that the entity was
outcompeted by some other word with even more
significance.
</bodyText>
<sectionHeader confidence="0.995543" genericHeader="method">
3 Relaxed Entity Grid
</sectionHeader>
<bodyText confidence="0.99166996875">
In this section, we relax the troublesome assump-
tion of independence between entities, thus mov-
ing the probability distribution over documents away
from blank sentences. We begin at the same point as
above: sequential generation of sentences: P(D) =
n
i P (Si|S0..(i−1)). We similarly separate the words
into entities and non-entities, treat the non-entities as
~
independent of the history S and omit them. We also
distinguish two types of entities. Let the known set
Ki = ej : ej ∈ ~S(i−1), the set of all entities which
have appeared before sentence i. Of the entities ap-
pearing in Si, those in Ki are known entities, and
those which are not are new entities. Since each en-
tity in the document is new precisely once, we treat
these as independent and omit them from our calcu-
lations as we did the non-entities. We return to both
groups of omitted words in section 4 below when
discussing our topic-based models.
To model a sentence, then, we generate the set of
known entities it contains along with their syntac-
tic roles, given the history and the known set Ki.
We truncate the history, as above, with horizon h;
note that this does not make the model Markovian,
since the known set has no horizon. Finally, we con-
sider only the portion of the history which relates to
known nouns (since all non-known nouns have the
same history - -). In all the equations below, we re-
strict Ei to known entities which actually appear in
sentence i, and Ri to roles filled by known entities.
The probability of a sentence is now:
</bodyText>
<equation confidence="0.999068">
P(Si|~Sh(i−1)) = P(Ei, Ri|~Rh(i−1)). (4)
</equation>
<bodyText confidence="0.999939">
We make one further simplification before begin-
ning to approximate: we first generate the set of syn-
tactic slots Ri which we intend to fill with known en-
tities, and then decide which entities from the known
</bodyText>
<page confidence="0.995848">
438
</page>
<bodyText confidence="0.996515">
set to select. Again, we assume independence from
the history, so that the contribution of P(Ri) for any
ordering of a fixed set of sentences is constant and
we omit it:
</bodyText>
<equation confidence="0.993127">
P(Ei, Ri |~Rh(i−1),j) = P(Ei|Ri, ~Rh(i−1),j). (5)
</equation>
<bodyText confidence="0.9913804">
Estimating P(Ei|Ri, ~Rh(i−1),j) proves to be dif-
ficult, since the contexts are very sparse. To con-
tinue, we make a series of approximations. First let
each role be filled individually (where r ← e is the
boolean indicator function “noun e fills role r”):
</bodyText>
<equation confidence="0.992639333333333">
P(Ei|Ri, ~Rh (i−1),j) ≈ 11 P(r ← ej|r, ~Rh(i−1),j).
r∈Ri
(6)
</equation>
<bodyText confidence="0.999361615384615">
Notice that this process can select the same noun ej
to fill multiple roles r, while the entity grid cannot
represent such an occurrence. The resulting distri-
bution is therefore slightly deficient.
Unfortunately, we are still faced with the sparse
context ~Rh(i−1),j, the set of histories of all currently
known nouns. It is much easier to estimate P(r ←
ej|r,~rh(i−1),j), where we condition only on the his-
tory of the particular noun which is chosen to fill
slot r. However, in this case we do not have a proper
probability distribution: i.e. the probabilities do not
sum to 1. To overcome this difficulty we simply nor-
malize by force3:
</bodyText>
<equation confidence="0.99947">
P(r ← ej|r, ~Rh(i−1),j) ≈ (7)
P(r ← ej|r,~rh(i−1),j)
Ej∈Ki P(r ← ej|r,~rh(i−1),j)
</equation>
<bodyText confidence="0.9406386875">
The individual probabilities P(r ← ej|r,~rh(i−1),j)
are calculated by counting situations in the train-
ing documents in which a known noun has his-
tory r~ h(i−1),j and fills slot r in the next sentence,
versus situations where the slot r exists but is
filled by some other noun. Some rare contexts are
still sparse, and so we smooth by adding a pseu-
docount of 1 for all events. Our model is ex-
pressed by equations (1),(4),(5),(6) and (7). In
3Unfortunately this estimator is not consistent (that is, given
infinite training data produced by the model, the estimated pa-
rameters do not converge to the true parameters). We are in-
vestigating maximum entropy estimation as a solution to this
problem.
figure 2, the probability of S3 with horizon 1 is
now calculated as follows: the known set con-
tains PLAN, AIRPLANE, CONDITION, FLIGHT,
PILOT, OWNER and OCCUPANT. There is one syn-
tactic role filled by a known noun, S. The proba-
bility is then calculated as P(+|S, X) (the proba-
bility of selecting a noun with history X to fill the
role of S) normalized by P(+|S, O) +P(+|S, S) +
P(+|S, X) + 4 × P(+|S, −).
Like Lapata and Barzilay (2005), our relaxed
model assigns low probability to sentences where
nouns with important-seeming histories do not ap-
pear. However, in our model, the penalty is less
severe if there are many competitor nouns. On the
other hand, if the sentence contains many slots, giv-
ing the noun more opportunity to fill one of them,
the penalty is proportionally greater if it does not
appear.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="method">
4 Topic-Based Model
</sectionHeader>
<bodyText confidence="0.999769851851852">
The model we describe above is a purely local one,
and moreover it relies on a particular set of local fea-
tures which capture the way adjacent sentences tend
to share lexical choices. Its lack of any global struc-
ture makes it impossible for the model to recover at
a paragraph boundary, or to accurately guess which
sentence should begin a document. Its lack of lexi-
calization, meanwhile, renders it incapable of learn-
ing dependences between pairs of words: for in-
stance, that a sentence discussing a crash is often
followed by a casualty report.
We remedy both these problems by extending our
model of document generation. Like Barzilay and
Lee (2004), we learn an HMM in which each sen-
tence has a hidden topic qi, which is chosen con-
ditioned on the previous state qi−1. The emission
model of each state is an instance of the relaxed en-
tity grid model as described above, but in addition
to conditioning on the role and history, we condi-
tion also on the state and on the particular set of
lexical items lex(Ki) which may be selected to fill
the role: P(r ← ej|r, ~Rh(i−1),j, qi, lex(Ki)). This
distribution is approximated as above by the nor-
malized value of P(r ← ej|r,~rh(i−1),j, qi, lex(ej)).
However, due to our use of lexical information,
even this may be too sparse for accurate estima-
tion, so we back off by interpolating with the pre-
</bodyText>
<page confidence="0.999356">
439
</page>
<figureCaption confidence="0.999667">
Figure 3: A single time-slice of our HMM.
</figureCaption>
<equation confidence="0.999697">
Wi — PY (·|qi; BLM, discountLM)
Ni — PY (·|qi; BNN, discountNN)
Ei — EGrid(·|R, �RZ1, qi, lex(Ki); BEG)
qi — DP(·|qi−1)
</equation>
<bodyText confidence="0.998792142857143">
In the equations above, only the manually set inter-
polation hyperparameters are indicated.
vious model. In each context, we introduce BEG
pseudo-observations, split fractionally according to
the backoff distribution: if we abbreviate the context
in the relaxed entity grid as C and the event as e, this
smoothing corresponds to:
</bodyText>
<equation confidence="0.967074">
P(e|C, qi, ej) = #(e, C, qi, ej) + BEGP(e|C) .
#(e, C, qi, ej) + BEG
</equation>
<bodyText confidence="0.999984327868852">
This is equivalent to defining the topic-based entity
grid as a Dirichlet process with parameter BEG sam-
pling from the relaxed entity grid.
In addition, we are now in a position to gener-
ate the non-entity words Wi and new entities Ni in
an informative way, by conditioning on the sentence
topic qi. Since they are interrupted by the known
entities, they do not form contiguous sequences of
words, so we make a bag-of-words assumption. To
model these sets of words, we use unigram ver-
sions of the hierarchical Pitman-Yor processes of
(Teh, 2006), which implement a Bayesian version
of Kneser-Ney smoothing.
To represent the HMM itself, we adapt the non-
parametric HMM of (Beal et al., 2001). This is
a Bayesian alternative to the conventional HMM
model learned using EM, chosen mostly for conve-
nience. Our variant of it, unlike (Beal et al., 2001),
has no parameter ry to control self-transitions; our
emission model is complex enough to make it un-
necessary.
The actual number of states found by the model
depends mostly on the backoff constants, the Bs
(and, for Pitman-Yor processes, discounts) chosen
for the emission models (the entity grid, non-entity
word model and new noun model), and is relatively
insensitive to particular choices of prior for the other
hyperparameters. As the backoff constants decrease,
the emission models become more dependent on the
state variable q, which leads to more states (and
eventually to memorization of the training data). If
instead the backoff rate increases, the emission mod-
els all become close to the general distribution and
the model prefers relatively few states. We train with
interpolations which generally result in around 40
states.
Once the interpolation constants are set, the
model can be trained by Gibbs sampling. We also
do inference over the remaining hyperparameters of
the model by Metropolis sampling from uninforma-
tive priors. Convergence is generally very rapid; we
obtain good results after about 10 iterations. Unlike
Barzilay and Lee (2004), we do not initialize with
an informative starting distribution.
When finding the probability of a test document,
we do not do inference over the full Bayesian model,
because the number of states, and the probability of
different transitions, can change with every new ob-
servation, making dynamic programming impossi-
ble. Beal et al. (2001) proposes an inference algo-
rithm based on particle filters, but we feel that in
this case, the effects are relatively minor, so we ap-
proximate by treating the model as a standard HMM,
using a fixed transition function based only on the
training data. This allows us to use the conventional
Viterbi algorithm. The backoff rates we choose at
training time are typically too small for optimal in-
ference in the ordering task. Before doing tests, we
set them to higher values (determined to optimize
ordering performance on held-out data) so that our
emission distributions are properly smoothed.
</bodyText>
<sectionHeader confidence="0.999605" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.998754333333333">
Our experiments use the popular AIRPLANE cor-
pus, a collection of documents describing airplane
crashes taken from the database of the National
</bodyText>
<page confidence="0.994513">
440
</page>
<bodyText confidence="0.999477115384616">
Transportation Safety Board, used in (Barzilay and
Lee, 2004; Barzilay and Lapata, 2005; Soricut and
Marcu, 2006). We use the standard division of
the corpus into 100 training and 100 test docu-
ments; for development purposes we did 10-fold
cross-validation on the training data. The AIRPLANE
documents have some advantages for coherence re-
search: they are short (11.5 sentences on average)
and quite formulaic, which makes it easy to find lex-
ical and structural patterns. On the other hand, they
do have some oddities. 46 of the training documents
begin with a standard preamble: “This is prelimi-
nary information, subject to change, and may con-
tain errors. Any errors in this report will be corrected
when the final report has been completed,” which
essentially gives coherence models the first two sen-
tences for free. Others, however, begin abruptly with
no introductory material whatsoever, and sometimes
without even providing references for their definite
noun phrases; one document begins: “At V1, the
DC-10-30’s number 1 engine, a General Electric
CF6-50C2, experienced a casing breach when the
2nd-stage low pressure turbine (LPT) anti-rotation
nozzle locks failed.” Even humans might have trou-
ble identifying this sentence as the beginning of a
document.
</bodyText>
<subsectionHeader confidence="0.998658">
5.1 Sentence Ordering
</subsectionHeader>
<bodyText confidence="0.999943736842105">
In the sentence ordering task, (Lapata, 2003; Barzi-
lay and Lee, 2004; Barzilay and Lapata, 2005; Sori-
cut and Marcu, 2006), we view a document as an
unordered bag of sentences and try to find the or-
dering of the sentences which maximizes coherence
according to our model. This type of ordering pro-
cess has applications in natural language generation
and multi-document summarization. Unfortunately,
finding the optimal ordering according to a prob-
abilistic model with local features is NP-complete
and non-approximable (Althaus et al., 2004). More-
over, since our model is not Markovian, the relax-
ation used as a heuristic for A* search by Soricut
and Marcu (2006) is ineffective. We therefore use
simulated annealing to find a high-probability order-
ing, starting from a random permutation of the sen-
tences. Our search system has few Estimated Search
Errors as defined by Soricut and Marcu (2006); it
rarely proposes an ordering which has lower proba-
</bodyText>
<table confidence="0.684328">
T Discr. (%)
(Barzilay and Lapata, 2005) - 90
(Barzilay and Lee, 2004) .44 745
(Soricut and Marcu, 2006) .50 -6
Topic-based (relaxed) .50 94
</table>
<tableCaption confidence="0.997329">
Table 1: Results for AIRPLANE test data.
</tableCaption>
<bodyText confidence="0.994788764705883">
bility than the original ordering4.
To evaluate the quality of the orderings we predict
as optimal, we use Kendall’s T, a measurement of
the number of pairwise swaps needed to transform
our proposed ordering into the original document,
normalized to lie between −1 (reverse order) and 1
(original order). Lapata (2006) shows that it corre-
sponds well with human judgements of coherence
and reading times. A slight problem with T is that
it does not always distinguish between proposed or-
derings of a document which disrupt local relation-
ships at random, and orderings in which paragraph-
like units move as a whole. In longer documents, it
may be worth taking this problem into account when
selecting a metric; however, the documents in the
AIRPLANE corpus are mostly short and have little
paragraph structure, so T is an effective metric.
</bodyText>
<subsectionHeader confidence="0.996586">
5.2 Discrimination
</subsectionHeader>
<bodyText confidence="0.9997718125">
Our second task is the discriminative test used by
(Barzilay and Lapata, 2005). In this task we gen-
erate random permutations of a test document, and
measure how often the probability of a permutation
is higher than that of the original document. This
task bears some resemblance to the task of discrim-
inating coherent from incoherent essays in (Milt-
sakaki and Kukich, 2004), and is also equivalent
in the limit to the ranking metric of (Barzilay and
Lee, 2004), which we cannot calculate because our
model does not produce k-best output. As opposed
to the ordering task, which tries to measure how
close the model’s preferred orderings are to the orig-
inal, this measurement assesses how many orderings
the model prefers. We use 20 random permutations
per document, for 2000 total tests.
</bodyText>
<page confidence="0.995033">
441
</page>
<table confidence="0.9985704">
τ Discr. (%)
Naive Entity Grid .17 81
Relaxed Entity Grid .02 87
Topic-based (naive) .39 85
Topic-based (relaxed) .54 96
</table>
<tableCaption confidence="0.9265625">
Table 2: Results for 10-fold cross-validation on AIR-
PLANE training data.
</tableCaption>
<sectionHeader confidence="0.999838" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999966193548387">
Since the ordering task requires a model to propose
the complete structure for a set of sentences, it is
very dependent on global features. To perform ad-
equately, a model must be able to locate the begin-
ning and end of the document, and place intermedi-
ate sentences relative to these two points. Without
any way of doing this, our relaxed entity grid model
has τ of approximately 0, meaning its optimal or-
derings are essentially uncorrelated with the correct
orderings7. The HMM content model of (Barzilay
and Lee, 2004), which does have global structure,
performs much better on ordering, at τ of .44. How-
ever, local features can help substantially for this
task, since models which use them are better at plac-
ing related sentences next to one another. Using both
sets of features, our topic-based model achieves state
of the art performance (τ = .5) on the ordering task,
comparable with the mixture model of (Soricut and
Marcu, 2006).
The need for good local coherence features is es-
pecially clear from the results on the discrimination
task (table 1). Permuting a document may leave ob-
vious “signposts” like the introduction and conclu-
sion in place, but it almost always splits up many
pairs of neighboring sentences, reducing local co-
herence. (Barzilay and Lee, 2004), which lacks lo-
cal features, does quite poorly on this task (74%),
while our model performs extremely well (94%).
It is also clear from the results that our relaxed en-
tity grid model (87%) improves substantially on the
generative naive entity grid (81%). When used on
</bodyText>
<footnote confidence="0.938819285714286">
40 times on test data, 3 times in cross-validation.
5Calculated on our test permutations using the code at
http://people.csail.mit.edu/regina/code.html.
6Soricut and Marcu (2006) do not report results on this task,
except to say that their implementation of the entity grid per-
forms comparably to (Barzilay and Lapata, 2005).
7Barzilay and Lapata (2005) do not report T scores.
</footnote>
<bodyText confidence="0.99994462962963">
its own, it performs much better on the discrimina-
tion task, which is the one for which it was designed.
(The naive entity grid has a higher τ score, .17, es-
sentially by accident. It slightly prefers to generate
infrequent nouns from the start context rather than
the context - -, which happens to produce the correct
placement for the “preliminary information” pream-
ble.) When used as the emission model for known
entities in our topic-based system, the relaxed en-
tity grid shows its improved performance even more
strongly (table 2); its results are about 10% higher
than the naive version under both metrics.
Our combined model uses only entity-grid fea-
tures and unigram language models,a strict subset of
the feature set of (Soricut and Marcu, 2006). Their
mixture includes an entity grid model and a version
of the HMM of (Barzilay and Lee, 2004), which
uses n-gram language modeling. It also uses a model
of lexical generation based on the IBM-1 model for
machine translation, which produces all words in the
document conditioned on words from previous sen-
tences. In contrast, we generate only entities con-
ditioned on words from previous sentences; other
words are conditionally independent given the topic
variable. It seems likely therefore that using our
model as a component of a mixture might improve
on the state of the art result.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999940277777778">
Ordering in the AIRPLANE corpus and similar con-
strained sets of short documents is by no means a
solved problem, but the results so far show a good
deal of promise. Unfortunately, in longer and less
formulaic corpora, the models, inference algorithms
and even evaluation metrics used thus far may prove
extremely difficult to scale up. Domains with more
natural writing styles will make lexical prediction a
much more difficult problem. On the other hand,
the wider variety of grammatical constructions used
may motivate more complex syntactic features, for
instance as proposed by (Siddharthan et al., 2004) in
sentence clustering.
Finding optimal orderings is a difficult task even
for short documents, and will become exponen-
tially more challenging in longer ones. For multi-
paragraph documents, it is probably impractical to
use full-scale coherence models to find optimal or-
</bodyText>
<page confidence="0.995001">
442
</page>
<bodyText confidence="0.999754666666667">
derings directly. A better approach may be a coarse-
to-fine or hierarchical system which cuts up longer
documents into more manageable chunks that can be
ordered as a unit.
Multi-paragraph documents also pose a problem
for the τ metric itself. In documents with clear the-
matic divisions between their different sections, a
good ordering metric should treat transposed para-
graphs differently than transposed sentences.
</bodyText>
<sectionHeader confidence="0.998401" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999920125">
We are extremely grateful to Regina Barzilay, for her
code, data and extensive support, Mirella Lapata for
code and advice, and the BLLIP group, especially
Tom Griffiths, Sharon Goldwater and Mark Johnson,
for comments and criticism. We were supported by
DARPA GALE contract HR0011-06-2-0001 and the
Karen T. Romer Foundation. Finally we thank three
anonymous reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.999105" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999237105263158">
Ernst Althaus, Nikiforos Karamanis, and Alexander
Koller. 2004. Computing locally coherent discourses.
In Proceedings of the 42nd ACL, Barcelona.
Regina Barzilay and Mirella Lapata. 2005. Modeling lo-
cal coherence: an entity-based approach. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL’05).
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113–120.
Matthew J. Beal, Zoubin Ghahramani, and Carl Ed-
ward Rasmussen. 2001. The infinite Hidden Markov
Model. In NIPS, pages 577–584.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. of the 2005 Meeting of the Assoc. for
Computational Linguistics (ACL), pages 173–180.
Peter Foltz, Walter Kintsch, and Thomas Landauer.
1998. The measurement of textual coherence with
latent semantic analysis. Discourse Processes,
25(2&amp;3):285–307.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203–225.
Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-
dia Gentile. 2004. Evaluating multiple aspects of co-
herence in student essays. In HLT-NAACL, pages 185–
192.
Roger Kibble and Richard Power. 2004. Optimising ref-
erential coherence in text generation. Computational
Linguistics, 30(4):401–416.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In IJCAI, pages 1085–1090.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the annual meeting ofACL, 2003.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering: Kendall’s tau. Computational Linguis-
tics, 32(4):1–14.
E. Miltsakaki and K. Kukich. 2004. Evaluation of text
coherence for electronic essay scoring systems. Nat.
Lang. Eng., 10(1):25–55.
Advaith Siddharthan, Ani Nenkova, and Kathleen McK-
eown. 2004. Syntactic simplification for improving
content selection in multi-document summarization.
In COLING04, pages 896–902.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Pro-
ceedings of the Association for Computational Lin-
guistics Conference (ACL-2006).
Y.W. Teh. 2006. A Bayesian interpretation of interpo-
lated Kneser-Ney. Technical Report TRA2/06, Na-
tional University of Singapore.
</reference>
<page confidence="0.999358">
443
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.816511">
<title confidence="0.995174">A Unified Local and Global Model for Discourse Coherence</title>
<author confidence="0.982764">Micha Elsner</author>
<author confidence="0.982764">Joseph Austerweil</author>
<author confidence="0.982764">Eugene</author>
<affiliation confidence="0.938238">Brown Laboratory for Linguistic Information Processing Brown</affiliation>
<address confidence="0.92154">Providence, RI</address>
<email confidence="0.99954">joseph.austerweil@gmail.com</email>
<abstract confidence="0.999602055555556">We present a model for discourse coherence which combines the local entitybased approach of (Barzilay and Lapata, 2005) and the HMM-based content model of (Barzilay and Lee, 2004). Unlike the mixture model of (Soricut and Marcu, 2006), we learn local and global features jointly, providing a better theoretical explanation of how they are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ernst Althaus</author>
<author>Nikiforos Karamanis</author>
<author>Alexander Koller</author>
</authors>
<title>Computing locally coherent discourses.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd ACL,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="21795" citStr="Althaus et al., 2004" startWordPosition="3659" endWordPosition="3662"> identifying this sentence as the beginning of a document. 5.1 Sentence Ordering In the sentence ordering task, (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Soricut and Marcu, 2006), we view a document as an unordered bag of sentences and try to find the ordering of the sentences which maximizes coherence according to our model. This type of ordering process has applications in natural language generation and multi-document summarization. Unfortunately, finding the optimal ordering according to a probabilistic model with local features is NP-complete and non-approximable (Althaus et al., 2004). Moreover, since our model is not Markovian, the relaxation used as a heuristic for A* search by Soricut and Marcu (2006) is ineffective. We therefore use simulated annealing to find a high-probability ordering, starting from a random permutation of the sentences. Our search system has few Estimated Search Errors as defined by Soricut and Marcu (2006); it rarely proposes an ordering which has lower probaT Discr. (%) (Barzilay and Lapata, 2005) - 90 (Barzilay and Lee, 2004) .44 745 (Soricut and Marcu, 2006) .50 -6 Topic-based (relaxed) .50 94 Table 1: Results for AIRPLANE test data. bility tha</context>
</contexts>
<marker>Althaus, Karamanis, Koller, 2004</marker>
<rawString>Ernst Althaus, Nikiforos Karamanis, and Alexander Koller. 2004. Computing locally coherent discourses. In Proceedings of the 42nd ACL, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: an entity-based approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05).</booktitle>
<contexts>
<context position="694" citStr="Barzilay and Lapata, 2005" startWordPosition="97" endWordPosition="100">r, Joseph Austerweil, and Eugene Charniak Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University Providence, RI 02912 {melsner,ec}@cs.brown.edu, joseph.austerweil@gmail.com Abstract We present a model for discourse coherence which combines the local entitybased approach of (Barzilay and Lapata, 2005) and the HMM-based content model of (Barzilay and Lee, 2004). Unlike the mixture model of (Soricut and Marcu, 2006), we learn local and global features jointly, providing a better theoretical explanation of how they are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about documen</context>
<context position="2837" citStr="Barzilay and Lapata, 2005" startWordPosition="442" endWordPosition="445">epresent beginnings, ends and boundaries as separate states. However, it has no local features; the particular words in each sentence are generated based only on the current state of the document. Since information can pass from sentence to sentence only in this restricted manner, the model sometimes fails to place sentences next to the correct neighbors. We attempt here to unify the two approaches by constructing a model with both sentence-to-sentence dependencies providing local cues, and a hidden topic variable for global structure. Our local features are based on the entity grid model of (Barzilay and Lapata, 2005; Lapata and Barzilay, 2005). This model has previously been most successful in a conditional setting; to integrate it into our model, we first relax its independence assumptions to improve its performance when used generatively. Our global model is an HMM like that of Barzilay and Lee (2004), but with emission probabilities drawn from the entity grid. We present results for two tasks, the ordering task, on which global models usually do well, and the discrimination task, on which local models tend to outperform them. Our model improves on purely global or local approaches on both 436 Proceedi</context>
<context position="6774" citStr="Barzilay and Lapata, 2005" startWordPosition="1124" endWordPosition="1127"> flight]S originated at [Nuevo Laredo , Mexico]X , at [approximately 1300]X. Figure 1: A section of a document, with syntactic roles of noun phrases marked. 0 1 2 3 PLAN - - O - AIRPLANE X O - - CONDITION - - S - FLIGHT - - X S PILOT O - - - PROPER - - - X OWNER - X - - OCCUPANT X - - - Figure 2: The entity grid for figure 12. retically quite unpleasant but in comparing different orderings of the same document, it seems not to do too much damage. Properly speaking entities may be referents of many different nouns and pronouns throughout the discourse, and both (Lapata and Barzilay, 2005) and (Barzilay and Lapata, 2005) present models which use coreference resolution systems to group nouns. We follow (Soricut and Marcu, 2006) in dropping this component of the system, and treat each head noun as having an individual single referent. To model transitions in this entity grid model, Lapata and Barzilay (2005) takes a generative approach. First, the probability of a document is defined as P(D) = P(Si..Sn), the joint probability of all the sentences. Sentences are generated in order conditioned on all previous sentences: P (D) = � P(Si|S0..(i−1)). (1) i We make a Markov assumption of order h (in our experiments h </context>
<context position="8861" citStr="Barzilay and Lapata (2005)" startWordPosition="1477" endWordPosition="1480">it occurs in sentence i and if so, which role it takes. This is equivalent to predicting ri,j. We represent the history of the specific entity ej as r~h(i−1),j = r(i−h),j..r(i−1),j, and write: P (Ei, Ri|~Sh (i−1)) ≈ 11 P(ri,j|~rh(i−1),j). (3) j For instance, in figure 2, the probability of S3 with horizon 1 is the product of P(S|X) (for FLIGHT), P(X|−) (for PROPER), and likewise for each other entity, P(−|O), P(−|S), P(−|−)3. Although this generative approach outperforms several models in correlation with coherence ratings assigned by human judges, it suffers in comparison with later systems. Barzilay and Lapata (2005) uses the same grid representation, but treats the transition probabilities P(ri,j|~ri,j) for each document as features for input to an SVM classifier. Soricut and Marcu (2006)’s implementation of the entity-based model also uses discriminative training. The generative model’s main weakness in comparison to these conditional models is its assumption of independence between entities. In real documents, each sentence tends to contain only a few nouns, and even fewer of them can fill roles like subject and object. In other words, nouns compete with each other for the available syntactic positions</context>
<context position="20057" citStr="Barzilay and Lapata, 2005" startWordPosition="3386" endWordPosition="3389">on based only on the training data. This allows us to use the conventional Viterbi algorithm. The backoff rates we choose at training time are typically too small for optimal inference in the ordering task. Before doing tests, we set them to higher values (determined to optimize ordering performance on held-out data) so that our emission distributions are properly smoothed. 5 Experiments Our experiments use the popular AIRPLANE corpus, a collection of documents describing airplane crashes taken from the database of the National 440 Transportation Safety Board, used in (Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Soricut and Marcu, 2006). We use the standard division of the corpus into 100 training and 100 test documents; for development purposes we did 10-fold cross-validation on the training data. The AIRPLANE documents have some advantages for coherence research: they are short (11.5 sentences on average) and quite formulaic, which makes it easy to find lexical and structural patterns. On the other hand, they do have some oddities. 46 of the training documents begin with a standard preamble: “This is preliminary information, subject to change, and may contain errors. Any errors in this report will</context>
<context position="21350" citStr="Barzilay and Lapata, 2005" startWordPosition="3590" endWordPosition="3593">tially gives coherence models the first two sentences for free. Others, however, begin abruptly with no introductory material whatsoever, and sometimes without even providing references for their definite noun phrases; one document begins: “At V1, the DC-10-30’s number 1 engine, a General Electric CF6-50C2, experienced a casing breach when the 2nd-stage low pressure turbine (LPT) anti-rotation nozzle locks failed.” Even humans might have trouble identifying this sentence as the beginning of a document. 5.1 Sentence Ordering In the sentence ordering task, (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Soricut and Marcu, 2006), we view a document as an unordered bag of sentences and try to find the ordering of the sentences which maximizes coherence according to our model. This type of ordering process has applications in natural language generation and multi-document summarization. Unfortunately, finding the optimal ordering according to a probabilistic model with local features is NP-complete and non-approximable (Althaus et al., 2004). Moreover, since our model is not Markovian, the relaxation used as a heuristic for A* search by Soricut and Marcu (2006) is ineffective. We therefore use</context>
<context position="23319" citStr="Barzilay and Lapata, 2005" startWordPosition="3914" endWordPosition="3917">ata (2006) shows that it corresponds well with human judgements of coherence and reading times. A slight problem with T is that it does not always distinguish between proposed orderings of a document which disrupt local relationships at random, and orderings in which paragraphlike units move as a whole. In longer documents, it may be worth taking this problem into account when selecting a metric; however, the documents in the AIRPLANE corpus are mostly short and have little paragraph structure, so T is an effective metric. 5.2 Discrimination Our second task is the discriminative test used by (Barzilay and Lapata, 2005). In this task we generate random permutations of a test document, and measure how often the probability of a permutation is higher than that of the original document. This task bears some resemblance to the task of discriminating coherent from incoherent essays in (Miltsakaki and Kukich, 2004), and is also equivalent in the limit to the ranking metric of (Barzilay and Lee, 2004), which we cannot calculate because our model does not produce k-best output. As opposed to the ordering task, which tries to measure how close the model’s preferred orderings are to the original, this measurement asse</context>
<context position="26101" citStr="Barzilay and Lapata, 2005" startWordPosition="4378" endWordPosition="4381">herence. (Barzilay and Lee, 2004), which lacks local features, does quite poorly on this task (74%), while our model performs extremely well (94%). It is also clear from the results that our relaxed entity grid model (87%) improves substantially on the generative naive entity grid (81%). When used on 40 times on test data, 3 times in cross-validation. 5Calculated on our test permutations using the code at http://people.csail.mit.edu/regina/code.html. 6Soricut and Marcu (2006) do not report results on this task, except to say that their implementation of the entity grid performs comparably to (Barzilay and Lapata, 2005). 7Barzilay and Lapata (2005) do not report T scores. its own, it performs much better on the discrimination task, which is the one for which it was designed. (The naive entity grid has a higher τ score, .17, essentially by accident. It slightly prefers to generate infrequent nouns from the start context rather than the context - -, which happens to produce the correct placement for the “preliminary information” preamble.) When used as the emission model for known entities in our topic-based system, the relaxed entity grid shows its improved performance even more strongly (table 2); its result</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: an entity-based approach. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Proceedings of the Main Conference,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="2061" citStr="Barzilay and Lee (2004)" startWordPosition="316" endWordPosition="319">cal models (Lapata, 2003; Barzilay and Lapata, 2005; Foltz et al., 1998) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words. Models of this type are good at finding sentences that belong near one another in the document. However, they have trouble finding the beginning or end of the document, or recovering from sudden shifts in topic (such as occur at paragraph boundaries). Some local models also have trouble deciding which of a pair of related sentences ought to come first. In contrast, the global HMM model of Barzilay and Lee (2004) tries to track the predictable changes in topic between sentences. This gives it a pronounced advantage in ordering sentences, since it can learn to represent beginnings, ends and boundaries as separate states. However, it has no local features; the particular words in each sentence are generated based only on the current state of the document. Since information can pass from sentence to sentence only in this restricted manner, the model sometimes fails to place sentences next to the correct neighbors. We attempt here to unify the two approaches by constructing a model with both sentence-to-s</context>
<context position="15616" citStr="Barzilay and Lee (2004)" startWordPosition="2644" endWordPosition="2647">s a purely local one, and moreover it relies on a particular set of local features which capture the way adjacent sentences tend to share lexical choices. Its lack of any global structure makes it impossible for the model to recover at a paragraph boundary, or to accurately guess which sentence should begin a document. Its lack of lexicalization, meanwhile, renders it incapable of learning dependences between pairs of words: for instance, that a sentence discussing a crash is often followed by a casualty report. We remedy both these problems by extending our model of document generation. Like Barzilay and Lee (2004), we learn an HMM in which each sentence has a hidden topic qi, which is chosen conditioned on the previous state qi−1. The emission model of each state is an instance of the relaxed entity grid model as described above, but in addition to conditioning on the role and history, we condition also on the state and on the particular set of lexical items lex(Ki) which may be selected to fill the role: P(r ← ej|r, ~Rh(i−1),j, qi, lex(Ki)). This distribution is approximated as above by the normalized value of P(r ← ej|r,~rh(i−1),j, qi, lex(ej)). However, due to our use of lexical information, even th</context>
<context position="18877" citStr="Barzilay and Lee (2004)" startWordPosition="3196" endWordPosition="3199">ch leads to more states (and eventually to memorization of the training data). If instead the backoff rate increases, the emission models all become close to the general distribution and the model prefers relatively few states. We train with interpolations which generally result in around 40 states. Once the interpolation constants are set, the model can be trained by Gibbs sampling. We also do inference over the remaining hyperparameters of the model by Metropolis sampling from uninformative priors. Convergence is generally very rapid; we obtain good results after about 10 iterations. Unlike Barzilay and Lee (2004), we do not initialize with an informative starting distribution. When finding the probability of a test document, we do not do inference over the full Bayesian model, because the number of states, and the probability of different transitions, can change with every new observation, making dynamic programming impossible. Beal et al. (2001) proposes an inference algorithm based on particle filters, but we feel that in this case, the effects are relatively minor, so we approximate by treating the model as a standard HMM, using a fixed transition function based only on the training data. This allo</context>
<context position="21323" citStr="Barzilay and Lee, 2004" startWordPosition="3585" endWordPosition="3589"> completed,” which essentially gives coherence models the first two sentences for free. Others, however, begin abruptly with no introductory material whatsoever, and sometimes without even providing references for their definite noun phrases; one document begins: “At V1, the DC-10-30’s number 1 engine, a General Electric CF6-50C2, experienced a casing breach when the 2nd-stage low pressure turbine (LPT) anti-rotation nozzle locks failed.” Even humans might have trouble identifying this sentence as the beginning of a document. 5.1 Sentence Ordering In the sentence ordering task, (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Soricut and Marcu, 2006), we view a document as an unordered bag of sentences and try to find the ordering of the sentences which maximizes coherence according to our model. This type of ordering process has applications in natural language generation and multi-document summarization. Unfortunately, finding the optimal ordering according to a probabilistic model with local features is NP-complete and non-approximable (Althaus et al., 2004). Moreover, since our model is not Markovian, the relaxation used as a heuristic for A* search by Soricut and Marcu (2006) is in</context>
<context position="23701" citStr="Barzilay and Lee, 2004" startWordPosition="3980" endWordPosition="3983">ting a metric; however, the documents in the AIRPLANE corpus are mostly short and have little paragraph structure, so T is an effective metric. 5.2 Discrimination Our second task is the discriminative test used by (Barzilay and Lapata, 2005). In this task we generate random permutations of a test document, and measure how often the probability of a permutation is higher than that of the original document. This task bears some resemblance to the task of discriminating coherent from incoherent essays in (Miltsakaki and Kukich, 2004), and is also equivalent in the limit to the ranking metric of (Barzilay and Lee, 2004), which we cannot calculate because our model does not produce k-best output. As opposed to the ordering task, which tries to measure how close the model’s preferred orderings are to the original, this measurement assesses how many orderings the model prefers. We use 20 random permutations per document, for 2000 total tests. 441 τ Discr. (%) Naive Entity Grid .17 81 Relaxed Entity Grid .02 87 Topic-based (naive) .39 85 Topic-based (relaxed) .54 96 Table 2: Results for 10-fold cross-validation on AIRPLANE training data. 6 Results Since the ordering task requires a model to propose the complete </context>
<context position="25508" citStr="Barzilay and Lee, 2004" startWordPosition="4284" endWordPosition="4287"> for this task, since models which use them are better at placing related sentences next to one another. Using both sets of features, our topic-based model achieves state of the art performance (τ = .5) on the ordering task, comparable with the mixture model of (Soricut and Marcu, 2006). The need for good local coherence features is especially clear from the results on the discrimination task (table 1). Permuting a document may leave obvious “signposts” like the introduction and conclusion in place, but it almost always splits up many pairs of neighboring sentences, reducing local coherence. (Barzilay and Lee, 2004), which lacks local features, does quite poorly on this task (74%), while our model performs extremely well (94%). It is also clear from the results that our relaxed entity grid model (87%) improves substantially on the generative naive entity grid (81%). When used on 40 times on test data, 3 times in cross-validation. 5Calculated on our test permutations using the code at http://people.csail.mit.edu/regina/code.html. 6Soricut and Marcu (2006) do not report results on this task, except to say that their implementation of the entity grid performs comparably to (Barzilay and Lapata, 2005). 7Barz</context>
<context position="27006" citStr="Barzilay and Lee, 2004" startWordPosition="4532" endWordPosition="4535"> from the start context rather than the context - -, which happens to produce the correct placement for the “preliminary information” preamble.) When used as the emission model for known entities in our topic-based system, the relaxed entity grid shows its improved performance even more strongly (table 2); its results are about 10% higher than the naive version under both metrics. Our combined model uses only entity-grid features and unigram language models,a strict subset of the feature set of (Soricut and Marcu, 2006). Their mixture includes an entity grid model and a version of the HMM of (Barzilay and Lee, 2004), which uses n-gram language modeling. It also uses a model of lexical generation based on the IBM-1 model for machine translation, which produces all words in the document conditioned on words from previous sentences. In contrast, we generate only entities conditioned on words from previous sentences; other words are conditionally independent given the topic variable. It seems likely therefore that using our model as a component of a mixture might improve on the state of the art result. 7 Future Work Ordering in the AIRPLANE corpus and similar constrained sets of short documents is by no mean</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In HLT-NAACL 2004: Proceedings of the Main Conference, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew J Beal</author>
<author>Zoubin Ghahramani</author>
<author>Carl Edward Rasmussen</author>
</authors>
<title>The infinite Hidden Markov Model. In</title>
<date>2001</date>
<booktitle>NIPS,</booktitle>
<pages>577--584</pages>
<contexts>
<context position="17558" citStr="Beal et al., 2001" startWordPosition="2987" endWordPosition="2990">let process with parameter BEG sampling from the relaxed entity grid. In addition, we are now in a position to generate the non-entity words Wi and new entities Ni in an informative way, by conditioning on the sentence topic qi. Since they are interrupted by the known entities, they do not form contiguous sequences of words, so we make a bag-of-words assumption. To model these sets of words, we use unigram versions of the hierarchical Pitman-Yor processes of (Teh, 2006), which implement a Bayesian version of Kneser-Ney smoothing. To represent the HMM itself, we adapt the nonparametric HMM of (Beal et al., 2001). This is a Bayesian alternative to the conventional HMM model learned using EM, chosen mostly for convenience. Our variant of it, unlike (Beal et al., 2001), has no parameter ry to control self-transitions; our emission model is complex enough to make it unnecessary. The actual number of states found by the model depends mostly on the backoff constants, the Bs (and, for Pitman-Yor processes, discounts) chosen for the emission models (the entity grid, non-entity word model and new noun model), and is relatively insensitive to particular choices of prior for the other hyperparameters. As the ba</context>
<context position="19217" citStr="Beal et al. (2001)" startWordPosition="3250" endWordPosition="3253">e model can be trained by Gibbs sampling. We also do inference over the remaining hyperparameters of the model by Metropolis sampling from uninformative priors. Convergence is generally very rapid; we obtain good results after about 10 iterations. Unlike Barzilay and Lee (2004), we do not initialize with an informative starting distribution. When finding the probability of a test document, we do not do inference over the full Bayesian model, because the number of states, and the probability of different transitions, can change with every new observation, making dynamic programming impossible. Beal et al. (2001) proposes an inference algorithm based on particle filters, but we feel that in this case, the effects are relatively minor, so we approximate by treating the model as a standard HMM, using a fixed transition function based only on the training data. This allows us to use the conventional Viterbi algorithm. The backoff rates we choose at training time are typically too small for optimal inference in the ordering task. Before doing tests, we set them to higher values (determined to optimize ordering performance on held-out data) so that our emission distributions are properly smoothed. 5 Experi</context>
</contexts>
<marker>Beal, Ghahramani, Rasmussen, 2001</marker>
<rawString>Matthew J. Beal, Zoubin Ghahramani, and Carl Edward Rasmussen. 2001. The infinite Hidden Markov Model. In NIPS, pages 577–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of the 2005 Meeting of the Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>173--180</pages>
<contexts>
<context position="5643" citStr="Charniak and Johnson, 2005" startWordPosition="918" endWordPosition="921">tactic roles: subject if possible, then object, or finally other. An example text is figure 1, whose grid is figure 2. Nouns are also treated as salient or non-salient, another important concern of Centering Theory. We condition events involving a noun on the frequency of that noun. Unfortunately, this way of representing salience makes our model slightly deficient, since the model conditions on a particular noun occurring e.g. 2 times, but assigns nonzero probabilities to documents where it occurs 3 times. This is theo1Roles are determined heuristically using trees produced by the parser of (Charniak and Johnson, 2005). Following previous work, we slightly conflate thematic and syntactic roles, marking the subject of a passive verb as O. 2The numeric token “1300” is removed in preprocessing, and “Nuevo Laredo” is marked as “PROPER”. 0 [The commercial pilot]O , [sole occupant of [the airplane]X]X , was not injured. 1 [The airplane]O was owned and operated by [a private owner]X . 2 [Visual meteorological conditions]S prevailed for [the personal cross country flight for which [a VFR flight plan]O was filed]X . 3 [The flight]S originated at [Nuevo Laredo , Mexico]X , at [approximately 1300]X. Figure 1: A sectio</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and MaxEnt discriminative reranking. In Proc. of the 2005 Meeting of the Assoc. for Computational Linguistics (ACL), pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Foltz</author>
<author>Walter Kintsch</author>
<author>Thomas Landauer</author>
</authors>
<title>The measurement of textual coherence with latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="1510" citStr="Foltz et al., 1998" startWordPosition="224" endWordPosition="227">r than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Models of coherence tend to fall into two classes. Local models (Lapata, 2003; Barzilay and Lapata, 2005; Foltz et al., 1998) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words. Models of this type are good at finding sentences that belong near one another in the document. However, they have trouble finding the beginning or end of the document, or recovering from sudden shifts in topic (such as occur at paragraph boundaries). Some local models also have trouble deciding which of a pair of related sentences ought to come first. In contrast, the global HMM model of Barzilay and Lee (2004) tries to track the predictable changes in topic </context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Peter Foltz, Walter Kintsch, and Thomas Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse Processes, 25(2&amp;3):285–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Aravind K Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modeling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="4322" citStr="Grosz et al., 1995" startWordPosition="689" endWordPosition="692">. However, mixture models lack explanatory power; since each of the individual component models is known to be flawed, it is difficult to say that the combination is theoretically more sound than the parts, even if it usually works better. Moreover, since the model we describe uses a strict subset of the features used in the component models of (Soricut and Marcu, 2006), we suspect that adding it to the mixture would lead to still further improved results. 2 Naive Entity Grids Entity grids, first described in (Lapata and Barzilay, 2005), are designed to capture some ideas of Centering Theory (Grosz et al., 1995), namely that adjacent utterances in a locally coherent discourses are likely to contain the same nouns, and that important nouns often appear in syntactically important roles such as subject or object. An entity grid represents a document as a matrix with a column for each entity, and a row for each sentence. The entry ri,j describes the syntactic role of entity j in sentence i: these roles are subject (S), object (O), or some other role (X)1. In addition there is a special marker (-) for nouns which do not appear at all in a given sentence. Each noun appears only once in a given row of the g</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derrick Higgins</author>
<author>Jill Burstein</author>
<author>Daniel Marcu</author>
<author>Claudia Gentile</author>
</authors>
<title>Evaluating multiple aspects of coherence in student essays.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>185--192</pages>
<contexts>
<context position="1228" citStr="Higgins et al., 2004" startWordPosition="180" endWordPosition="183">ey are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Models of coherence tend to fall into two classes. Local models (Lapata, 2003; Barzilay and Lapata, 2005; Foltz et al., 1998) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words. Models of this type are good at finding sentences that belong near one another in the document. However, they have trouble finding the beginning or end of the document, or recovering</context>
</contexts>
<marker>Higgins, Burstein, Marcu, Gentile, 2004</marker>
<rawString>Derrick Higgins, Jill Burstein, Daniel Marcu, and Claudia Gentile. 2004. Evaluating multiple aspects of coherence in student essays. In HLT-NAACL, pages 185– 192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Kibble</author>
<author>Richard Power</author>
</authors>
<title>Optimising referential coherence in text generation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1110" citStr="Kibble and Power, 2004" startWordPosition="163" endWordPosition="166">ricut and Marcu, 2006), we learn local and global features jointly, providing a better theoretical explanation of how they are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Models of coherence tend to fall into two classes. Local models (Lapata, 2003; Barzilay and Lapata, 2005; Foltz et al., 1998) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words. Models of this type are good at finding sentences that belong ne</context>
</contexts>
<marker>Kibble, Power, 2004</marker>
<rawString>Roger Kibble and Richard Power. 2004. Optimising referential coherence in text generation. Computational Linguistics, 30(4):401–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatic evaluation of text coherence: Models and representations.</title>
<date>2005</date>
<booktitle>In IJCAI,</booktitle>
<pages>1085--1090</pages>
<contexts>
<context position="2865" citStr="Lapata and Barzilay, 2005" startWordPosition="446" endWordPosition="449">nd boundaries as separate states. However, it has no local features; the particular words in each sentence are generated based only on the current state of the document. Since information can pass from sentence to sentence only in this restricted manner, the model sometimes fails to place sentences next to the correct neighbors. We attempt here to unify the two approaches by constructing a model with both sentence-to-sentence dependencies providing local cues, and a hidden topic variable for global structure. Our local features are based on the entity grid model of (Barzilay and Lapata, 2005; Lapata and Barzilay, 2005). This model has previously been most successful in a conditional setting; to integrate it into our model, we first relax its independence assumptions to improve its performance when used generatively. Our global model is an HMM like that of Barzilay and Lee (2004), but with emission probabilities drawn from the entity grid. We present results for two tasks, the ordering task, on which global models usually do well, and the discrimination task, on which local models tend to outperform them. Our model improves on purely global or local approaches on both 436 Proceedings of NAACL HLT 2007, pages</context>
<context position="4245" citStr="Lapata and Barzilay, 2005" startWordPosition="675" endWordPosition="678">to integrate local and global features using a mixture model, with promising results. However, mixture models lack explanatory power; since each of the individual component models is known to be flawed, it is difficult to say that the combination is theoretically more sound than the parts, even if it usually works better. Moreover, since the model we describe uses a strict subset of the features used in the component models of (Soricut and Marcu, 2006), we suspect that adding it to the mixture would lead to still further improved results. 2 Naive Entity Grids Entity grids, first described in (Lapata and Barzilay, 2005), are designed to capture some ideas of Centering Theory (Grosz et al., 1995), namely that adjacent utterances in a locally coherent discourses are likely to contain the same nouns, and that important nouns often appear in syntactically important roles such as subject or object. An entity grid represents a document as a matrix with a column for each entity, and a row for each sentence. The entry ri,j describes the syntactic role of entity j in sentence i: these roles are subject (S), object (O), or some other role (X)1. In addition there is a special marker (-) for nouns which do not appear at</context>
<context position="6742" citStr="Lapata and Barzilay, 2005" startWordPosition="1119" endWordPosition="1122">ight plan]O was filed]X . 3 [The flight]S originated at [Nuevo Laredo , Mexico]X , at [approximately 1300]X. Figure 1: A section of a document, with syntactic roles of noun phrases marked. 0 1 2 3 PLAN - - O - AIRPLANE X O - - CONDITION - - S - FLIGHT - - X S PILOT O - - - PROPER - - - X OWNER - X - - OCCUPANT X - - - Figure 2: The entity grid for figure 12. retically quite unpleasant but in comparing different orderings of the same document, it seems not to do too much damage. Properly speaking entities may be referents of many different nouns and pronouns throughout the discourse, and both (Lapata and Barzilay, 2005) and (Barzilay and Lapata, 2005) present models which use coreference resolution systems to group nouns. We follow (Soricut and Marcu, 2006) in dropping this component of the system, and treat each head noun as having an individual single referent. To model transitions in this entity grid model, Lapata and Barzilay (2005) takes a generative approach. First, the probability of a document is defined as P(D) = P(Si..Sn), the joint probability of all the sentences. Sentences are generated in order conditioned on all previous sentences: P (D) = � P(Si|S0..(i−1)). (1) i We make a Markov assumption o</context>
<context position="14570" citStr="Lapata and Barzilay (2005)" startWordPosition="2466" endWordPosition="2469">infinite training data produced by the model, the estimated parameters do not converge to the true parameters). We are investigating maximum entropy estimation as a solution to this problem. figure 2, the probability of S3 with horizon 1 is now calculated as follows: the known set contains PLAN, AIRPLANE, CONDITION, FLIGHT, PILOT, OWNER and OCCUPANT. There is one syntactic role filled by a known noun, S. The probability is then calculated as P(+|S, X) (the probability of selecting a noun with history X to fill the role of S) normalized by P(+|S, O) +P(+|S, S) + P(+|S, X) + 4 × P(+|S, −). Like Lapata and Barzilay (2005), our relaxed model assigns low probability to sentences where nouns with important-seeming histories do not appear. However, in our model, the penalty is less severe if there are many competitor nouns. On the other hand, if the sentence contains many slots, giving the noun more opportunity to fill one of them, the penalty is proportionally greater if it does not appear. 4 Topic-Based Model The model we describe above is a purely local one, and moreover it relies on a particular set of local features which capture the way adjacent sentences tend to share lexical choices. Its lack of any global</context>
</contexts>
<marker>Lapata, Barzilay, 2005</marker>
<rawString>Mirella Lapata and Regina Barzilay. 2005. Automatic evaluation of text coherence: Models and representations. In IJCAI, pages 1085–1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In Proceedings of the annual meeting ofACL,</booktitle>
<contexts>
<context position="1462" citStr="Lapata, 2003" startWordPosition="218" endWordPosition="219">and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Models of coherence tend to fall into two classes. Local models (Lapata, 2003; Barzilay and Lapata, 2005; Foltz et al., 1998) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words. Models of this type are good at finding sentences that belong near one another in the document. However, they have trouble finding the beginning or end of the document, or recovering from sudden shifts in topic (such as occur at paragraph boundaries). Some local models also have trouble deciding which of a pair of related sentences ought to come first. In contrast, the global HMM model of Barzilay and Lee (2004) </context>
<context position="21299" citStr="Lapata, 2003" startWordPosition="3583" endWordPosition="3584">eport has been completed,” which essentially gives coherence models the first two sentences for free. Others, however, begin abruptly with no introductory material whatsoever, and sometimes without even providing references for their definite noun phrases; one document begins: “At V1, the DC-10-30’s number 1 engine, a General Electric CF6-50C2, experienced a casing breach when the 2nd-stage low pressure turbine (LPT) anti-rotation nozzle locks failed.” Even humans might have trouble identifying this sentence as the beginning of a document. 5.1 Sentence Ordering In the sentence ordering task, (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Soricut and Marcu, 2006), we view a document as an unordered bag of sentences and try to find the ordering of the sentences which maximizes coherence according to our model. This type of ordering process has applications in natural language generation and multi-document summarization. Unfortunately, finding the optimal ordering according to a probabilistic model with local features is NP-complete and non-approximable (Althaus et al., 2004). Moreover, since our model is not Markovian, the relaxation used as a heuristic for A* search by Soricu</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the annual meeting ofACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Automatic evaluation of information ordering: Kendall’s tau.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="22703" citStr="Lapata (2006)" startWordPosition="3813" endWordPosition="3814">imated Search Errors as defined by Soricut and Marcu (2006); it rarely proposes an ordering which has lower probaT Discr. (%) (Barzilay and Lapata, 2005) - 90 (Barzilay and Lee, 2004) .44 745 (Soricut and Marcu, 2006) .50 -6 Topic-based (relaxed) .50 94 Table 1: Results for AIRPLANE test data. bility than the original ordering4. To evaluate the quality of the orderings we predict as optimal, we use Kendall’s T, a measurement of the number of pairwise swaps needed to transform our proposed ordering into the original document, normalized to lie between −1 (reverse order) and 1 (original order). Lapata (2006) shows that it corresponds well with human judgements of coherence and reading times. A slight problem with T is that it does not always distinguish between proposed orderings of a document which disrupt local relationships at random, and orderings in which paragraphlike units move as a whole. In longer documents, it may be worth taking this problem into account when selecting a metric; however, the documents in the AIRPLANE corpus are mostly short and have little paragraph structure, so T is an effective metric. 5.2 Discrimination Our second task is the discriminative test used by (Barzilay a</context>
</contexts>
<marker>Lapata, 2006</marker>
<rawString>Mirella Lapata. 2006. Automatic evaluation of information ordering: Kendall’s tau. Computational Linguistics, 32(4):1–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Miltsakaki</author>
<author>K Kukich</author>
</authors>
<title>Evaluation of text coherence for electronic essay scoring systems.</title>
<date>2004</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="1205" citStr="Miltsakaki and Kukich, 2004" startWordPosition="175" endWordPosition="179">retical explanation of how they are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Models of coherence tend to fall into two classes. Local models (Lapata, 2003; Barzilay and Lapata, 2005; Foltz et al., 1998) attempt to capture the generalization that adjacent sentences often have similar content, and therefore tend to contain related words. Models of this type are good at finding sentences that belong near one another in the document. However, they have trouble finding the beginning or end of the </context>
<context position="23614" citStr="Miltsakaki and Kukich, 2004" startWordPosition="3963" endWordPosition="3967">as a whole. In longer documents, it may be worth taking this problem into account when selecting a metric; however, the documents in the AIRPLANE corpus are mostly short and have little paragraph structure, so T is an effective metric. 5.2 Discrimination Our second task is the discriminative test used by (Barzilay and Lapata, 2005). In this task we generate random permutations of a test document, and measure how often the probability of a permutation is higher than that of the original document. This task bears some resemblance to the task of discriminating coherent from incoherent essays in (Miltsakaki and Kukich, 2004), and is also equivalent in the limit to the ranking metric of (Barzilay and Lee, 2004), which we cannot calculate because our model does not produce k-best output. As opposed to the ordering task, which tries to measure how close the model’s preferred orderings are to the original, this measurement assesses how many orderings the model prefers. We use 20 random permutations per document, for 2000 total tests. 441 τ Discr. (%) Naive Entity Grid .17 81 Relaxed Entity Grid .02 87 Topic-based (naive) .39 85 Topic-based (relaxed) .54 96 Table 2: Results for 10-fold cross-validation on AIRPLANE tra</context>
</contexts>
<marker>Miltsakaki, Kukich, 2004</marker>
<rawString>E. Miltsakaki and K. Kukich. 2004. Evaluation of text coherence for electronic essay scoring systems. Nat. Lang. Eng., 10(1):25–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>Syntactic simplification for improving content selection in multi-document summarization.</title>
<date>2004</date>
<booktitle>In COLING04,</booktitle>
<pages>896--902</pages>
<contexts>
<context position="28121" citStr="Siddharthan et al., 2004" startWordPosition="4712" endWordPosition="4715">t. 7 Future Work Ordering in the AIRPLANE corpus and similar constrained sets of short documents is by no means a solved problem, but the results so far show a good deal of promise. Unfortunately, in longer and less formulaic corpora, the models, inference algorithms and even evaluation metrics used thus far may prove extremely difficult to scale up. Domains with more natural writing styles will make lexical prediction a much more difficult problem. On the other hand, the wider variety of grammatical constructions used may motivate more complex syntactic features, for instance as proposed by (Siddharthan et al., 2004) in sentence clustering. Finding optimal orderings is a difficult task even for short documents, and will become exponentially more challenging in longer ones. For multiparagraph documents, it is probably impractical to use full-scale coherence models to find optimal or442 derings directly. A better approach may be a coarseto-fine or hierarchical system which cuts up longer documents into more manageable chunks that can be ordered as a unit. Multi-paragraph documents also pose a problem for the τ metric itself. In documents with clear thematic divisions between their different sections, a good</context>
</contexts>
<marker>Siddharthan, Nenkova, McKeown, 2004</marker>
<rawString>Advaith Siddharthan, Ani Nenkova, and Kathleen McKeown. 2004. Syntactic simplification for improving content selection in multi-document summarization. In COLING04, pages 896–902.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Discourse generation using utility-trained coherence models.</title>
<date>2006</date>
<booktitle>In Proceedings of the Association for Computational Linguistics Conference (ACL-2006).</booktitle>
<contexts>
<context position="866" citStr="Soricut and Marcu, 2006" startWordPosition="123" endWordPosition="126">ph.austerweil@gmail.com Abstract We present a model for discourse coherence which combines the local entitybased approach of (Barzilay and Lapata, 2005) and the HMM-based content model of (Barzilay and Lee, 2004). Unlike the mixture model of (Soricut and Marcu, 2006), we learn local and global features jointly, providing a better theoretical explanation of how they are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on. 1 Introduction Models of coherent discourse are central to several tasks in natural language processing: such models have been used in text generation (Kibble and Power, 2004) and evaluation of human-produced text in educational applications (Miltsakaki and Kukich, 2004; Higgins et al., 2004). Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Models of coherence tend to fall into two classes. Local models (Lapata, 2003; Ba</context>
<context position="3599" citStr="Soricut and Marcu (2006)" startWordPosition="565" endWordPosition="568">first relax its independence assumptions to improve its performance when used generatively. Our global model is an HMM like that of Barzilay and Lee (2004), but with emission probabilities drawn from the entity grid. We present results for two tasks, the ordering task, on which global models usually do well, and the discrimination task, on which local models tend to outperform them. Our model improves on purely global or local approaches on both 436 Proceedings of NAACL HLT 2007, pages 436–443, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics tasks. Previous work by Soricut and Marcu (2006) has also attempted to integrate local and global features using a mixture model, with promising results. However, mixture models lack explanatory power; since each of the individual component models is known to be flawed, it is difficult to say that the combination is theoretically more sound than the parts, even if it usually works better. Moreover, since the model we describe uses a strict subset of the features used in the component models of (Soricut and Marcu, 2006), we suspect that adding it to the mixture would lead to still further improved results. 2 Naive Entity Grids Entity grids, </context>
<context position="6882" citStr="Soricut and Marcu, 2006" startWordPosition="1140" endWordPosition="1143">, with syntactic roles of noun phrases marked. 0 1 2 3 PLAN - - O - AIRPLANE X O - - CONDITION - - S - FLIGHT - - X S PILOT O - - - PROPER - - - X OWNER - X - - OCCUPANT X - - - Figure 2: The entity grid for figure 12. retically quite unpleasant but in comparing different orderings of the same document, it seems not to do too much damage. Properly speaking entities may be referents of many different nouns and pronouns throughout the discourse, and both (Lapata and Barzilay, 2005) and (Barzilay and Lapata, 2005) present models which use coreference resolution systems to group nouns. We follow (Soricut and Marcu, 2006) in dropping this component of the system, and treat each head noun as having an individual single referent. To model transitions in this entity grid model, Lapata and Barzilay (2005) takes a generative approach. First, the probability of a document is defined as P(D) = P(Si..Sn), the joint probability of all the sentences. Sentences are generated in order conditioned on all previous sentences: P (D) = � P(Si|S0..(i−1)). (1) i We make a Markov assumption of order h (in our experiments h = 2) to shorten the history. We represent the truncated history as �Shi−1 = S(i−h)..S(i−1). Each sentence Si</context>
<context position="9037" citStr="Soricut and Marcu (2006)" startWordPosition="1504" endWordPosition="1507">,j, and write: P (Ei, Ri|~Sh (i−1)) ≈ 11 P(ri,j|~rh(i−1),j). (3) j For instance, in figure 2, the probability of S3 with horizon 1 is the product of P(S|X) (for FLIGHT), P(X|−) (for PROPER), and likewise for each other entity, P(−|O), P(−|S), P(−|−)3. Although this generative approach outperforms several models in correlation with coherence ratings assigned by human judges, it suffers in comparison with later systems. Barzilay and Lapata (2005) uses the same grid representation, but treats the transition probabilities P(ri,j|~ri,j) for each document as features for input to an SVM classifier. Soricut and Marcu (2006)’s implementation of the entity-based model also uses discriminative training. The generative model’s main weakness in comparison to these conditional models is its assumption of independence between entities. In real documents, each sentence tends to contain only a few nouns, and even fewer of them can fill roles like subject and object. In other words, nouns compete with each other for the available syntactic positions in sentences; once one noun is chosen as the subject, the probability that any other will also become a subject (of a different subclause of the same sentence) is drastically </context>
<context position="20083" citStr="Soricut and Marcu, 2006" startWordPosition="3390" endWordPosition="3393">ng data. This allows us to use the conventional Viterbi algorithm. The backoff rates we choose at training time are typically too small for optimal inference in the ordering task. Before doing tests, we set them to higher values (determined to optimize ordering performance on held-out data) so that our emission distributions are properly smoothed. 5 Experiments Our experiments use the popular AIRPLANE corpus, a collection of documents describing airplane crashes taken from the database of the National 440 Transportation Safety Board, used in (Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Soricut and Marcu, 2006). We use the standard division of the corpus into 100 training and 100 test documents; for development purposes we did 10-fold cross-validation on the training data. The AIRPLANE documents have some advantages for coherence research: they are short (11.5 sentences on average) and quite formulaic, which makes it easy to find lexical and structural patterns. On the other hand, they do have some oddities. 46 of the training documents begin with a standard preamble: “This is preliminary information, subject to change, and may contain errors. Any errors in this report will be corrected when the fin</context>
<context position="21376" citStr="Soricut and Marcu, 2006" startWordPosition="3594" endWordPosition="3598">ls the first two sentences for free. Others, however, begin abruptly with no introductory material whatsoever, and sometimes without even providing references for their definite noun phrases; one document begins: “At V1, the DC-10-30’s number 1 engine, a General Electric CF6-50C2, experienced a casing breach when the 2nd-stage low pressure turbine (LPT) anti-rotation nozzle locks failed.” Even humans might have trouble identifying this sentence as the beginning of a document. 5.1 Sentence Ordering In the sentence ordering task, (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Soricut and Marcu, 2006), we view a document as an unordered bag of sentences and try to find the ordering of the sentences which maximizes coherence according to our model. This type of ordering process has applications in natural language generation and multi-document summarization. Unfortunately, finding the optimal ordering according to a probabilistic model with local features is NP-complete and non-approximable (Althaus et al., 2004). Moreover, since our model is not Markovian, the relaxation used as a heuristic for A* search by Soricut and Marcu (2006) is ineffective. We therefore use simulated annealing to fi</context>
<context position="25172" citStr="Soricut and Marcu, 2006" startWordPosition="4229" endWordPosition="4232">of doing this, our relaxed entity grid model has τ of approximately 0, meaning its optimal orderings are essentially uncorrelated with the correct orderings7. The HMM content model of (Barzilay and Lee, 2004), which does have global structure, performs much better on ordering, at τ of .44. However, local features can help substantially for this task, since models which use them are better at placing related sentences next to one another. Using both sets of features, our topic-based model achieves state of the art performance (τ = .5) on the ordering task, comparable with the mixture model of (Soricut and Marcu, 2006). The need for good local coherence features is especially clear from the results on the discrimination task (table 1). Permuting a document may leave obvious “signposts” like the introduction and conclusion in place, but it almost always splits up many pairs of neighboring sentences, reducing local coherence. (Barzilay and Lee, 2004), which lacks local features, does quite poorly on this task (74%), while our model performs extremely well (94%). It is also clear from the results that our relaxed entity grid model (87%) improves substantially on the generative naive entity grid (81%). When use</context>
<context position="26908" citStr="Soricut and Marcu, 2006" startWordPosition="4514" endWordPosition="4517">as a higher τ score, .17, essentially by accident. It slightly prefers to generate infrequent nouns from the start context rather than the context - -, which happens to produce the correct placement for the “preliminary information” preamble.) When used as the emission model for known entities in our topic-based system, the relaxed entity grid shows its improved performance even more strongly (table 2); its results are about 10% higher than the naive version under both metrics. Our combined model uses only entity-grid features and unigram language models,a strict subset of the feature set of (Soricut and Marcu, 2006). Their mixture includes an entity grid model and a version of the HMM of (Barzilay and Lee, 2004), which uses n-gram language modeling. It also uses a model of lexical generation based on the IBM-1 model for machine translation, which produces all words in the document conditioned on words from previous sentences. In contrast, we generate only entities conditioned on words from previous sentences; other words are conditionally independent given the topic variable. It seems likely therefore that using our model as a component of a mixture might improve on the state of the art result. 7 Future </context>
</contexts>
<marker>Soricut, Marcu, 2006</marker>
<rawString>Radu Soricut and Daniel Marcu. 2006. Discourse generation using utility-trained coherence models. In Proceedings of the Association for Computational Linguistics Conference (ACL-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Teh</author>
</authors>
<title>A Bayesian interpretation of interpolated Kneser-Ney.</title>
<date>2006</date>
<tech>Technical Report TRA2/06,</tech>
<institution>National University of Singapore.</institution>
<contexts>
<context position="17414" citStr="Teh, 2006" startWordPosition="2965" endWordPosition="2966">|C, qi, ej) = #(e, C, qi, ej) + BEGP(e|C) . #(e, C, qi, ej) + BEG This is equivalent to defining the topic-based entity grid as a Dirichlet process with parameter BEG sampling from the relaxed entity grid. In addition, we are now in a position to generate the non-entity words Wi and new entities Ni in an informative way, by conditioning on the sentence topic qi. Since they are interrupted by the known entities, they do not form contiguous sequences of words, so we make a bag-of-words assumption. To model these sets of words, we use unigram versions of the hierarchical Pitman-Yor processes of (Teh, 2006), which implement a Bayesian version of Kneser-Ney smoothing. To represent the HMM itself, we adapt the nonparametric HMM of (Beal et al., 2001). This is a Bayesian alternative to the conventional HMM model learned using EM, chosen mostly for convenience. Our variant of it, unlike (Beal et al., 2001), has no parameter ry to control self-transitions; our emission model is complex enough to make it unnecessary. The actual number of states found by the model depends mostly on the backoff constants, the Bs (and, for Pitman-Yor processes, discounts) chosen for the emission models (the entity grid, </context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Y.W. Teh. 2006. A Bayesian interpretation of interpolated Kneser-Ney. Technical Report TRA2/06, National University of Singapore.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>