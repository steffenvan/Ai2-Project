<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012624">
<title confidence="0.997663">
Coherent Back-Channel Feedback Tagging of
In-Car Spoken Dialogue Corpus
</title>
<author confidence="0.99874">
Yuki Kamiya
</author>
<affiliation confidence="0.980441666666667">
Graduate School of
Information Science,
Nagoya University, Japan
</affiliation>
<email confidence="0.996625">
kamiya@el.itc.nagoya-u.ac.jp
</email>
<author confidence="0.953346">
Tomohiro Ohno
</author>
<affiliation confidence="0.928652">
Graduate School of
International Development,
Nagoya University, Japan
</affiliation>
<email confidence="0.788613">
ohno@nagoya-u.jp
</email>
<author confidence="0.983718">
Shigeki Matsubara
</author>
<affiliation confidence="0.975333666666667">
Graduate School of
Information Science,
Nagoya University, Japan
</affiliation>
<email confidence="0.864049">
matubara@nagoya-u.jp
</email>
<sectionHeader confidence="0.991297" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999574">
This paper describes the design of a back-
channel feedback corpus and its evalua-
tion, aiming at realizing in-car spoken di-
alogue systems with high responsiveness.
We constructed our corpus by annotating
the existing in-car spoken dialogue data
with back-channel feedback timing infor-
mation in an off-line environment. Our
corpus can be practically used in devel-
oping dialogue systems which can pro-
vide verbal back-channel feedbacks. As
the results of our evaluation, we confirmed
that our proposed design enabled the con-
struction of back-channel feedback cor-
pora with high coherency and naturalness.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982703703704">
In-car spoken dialogue processing is one of the
most prevailing applications of speech technol-
ogy. Until now, to realize the system which can
surely achieve such tasks navigation and informa-
tion retrieval, the development of speech recogni-
tion, speech understanding, dialogue control and
so on has been promoted. Now, it becomes impor-
tant to increase responsiveness of the system not
only for the efficient achievement of the task but
for increasing drivers’ comfortableness in a dia-
logue.
One way to increase responsiveness of a sys-
tem is to timely disclose system’s state of under-
standing, by making the system show some kind
of reaction during user’s utterances. In human
dialogues, such disclosure is performed by ac-
tions such as nods, facial expressions, gestures and
back-channel feedbacks. However, since drivers
do not look towards a spoken dialogue system
while driving, the system has to inevitably use
voice responses, that is, back-channel feedbacks.
Furthermore, in the response strategy for realiz-
ing in-car dialogues in which drivers feel com-
fortable, it is necessary for the system to provide
back-channel feedbacks during driver’s utterances
aggressively as well as timely.
This paper describes the design of a back-
channel feedback corpus having coherency (tag-
ging is performed by different annotators equally)
and naturalness, and its evaluation, aiming at re-
alizing in-car spoken dialogue systems with high
responsiveness. Although there have been sev-
eral researches on back-channel feedback timings
(Cathcart et al., 2003; Maynard, 1989; Takeuchi
et al., 2004; Ward and Tsukahara, 2000), in many
of them, back-channel feedback timings in human
dialogues were observed and analyzed by using
a general spoken dialogue corpus. On the other
hand, we constructed our corpus by annotating the
existing in-car spoken dialogue data with back-
channel feedback timing information in an off-line
environment. Our corpus can be practically used
in developing dialogue systems which can provide
back-channel feedbacks.
In our research, the driver utterances (11,181
turns) in the CIAIR in-car spoken dialogue corpus
(Kawaguchi et al., 2005) were used as the existing
data. We created the Web interface for the anno-
tation of back-channel feedbacks and constructed
the corpus including 5,416 back-channel feed-
backs. Experiments have shown that our proposed
corpus design enabled the construction of back-
channel feedback corpora with high coherency and
naturalness.
</bodyText>
<sectionHeader confidence="0.968578" genericHeader="method">
2 Corpus Design
</sectionHeader>
<bodyText confidence="0.999473555555556">
A back-channel feedback is a sign to inform a
speaker that the listener received the speaker’s ut-
terances. Thus, in an in-car dialogue between a
driver and a system, it is preferable that the sys-
tem provides as many back-channel feedbacks as
possible. However, if back-channel feedbacks are
unnecessarily provided, they can not play the pri-
mary role because the driver wonders if the system
really comprehends the speech.
</bodyText>
<subsubsectionHeader confidence="0.676825">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 205–208,
</subsubsectionHeader>
<affiliation confidence="0.890731">
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.997908">
205
</page>
<figure confidence="0.99927075">
0035 - 03:10:170-03:13:119 F:D:I:D:
(F t) (well...) &amp; to
@a£ (clothes) &amp; fuku-o
II0t=0kt?lit&apos; (I wan to buy, so) &amp; kai-tai-n-da-kedo
t&apos; h% (somewhere) &amp; dok-ka
��1Z&lt;H&gt; (near here) &amp; chikaku-ni&lt;H&gt;
0036 - 03:15:132-03:16:623 F:D:I:DI:
V0 (an inexpensive) &amp; yasui
J3J9 (shop) &amp; o-mise
,§6h%U,§&lt;SB&gt; (is there) &amp; aru-ka-na&lt;SB&gt;
0037 - 03:17:302-03:20:887 F:O:I:AO:
BOJ (here) &amp; kono
���&apos;rt (near) &amp; chikaku-desu-to
7*7hzt (ANNEX) &amp; anekkusu-to
$S2rIWht (Nagoya PARCO) &amp; nagoya-paruko-ga
�iY0*&apos;rht&lt;SB&gt; (there are) &amp; gozai-masu-ga&lt;SB&gt;
driver’s turn
driver’s
utterance
Well..., I want to
buy clothes, so, is
there an inexpensive
shop somewhere
near here?
driver’s
utterance
operator’s turn
operator’s
utterance
Near here, there are
ANNEX and
Nagoya PARCO.
</figure>
<figureCaption confidence="0.999997">
Figure 1: Sample of transcribed text
</figureCaption>
<bodyText confidence="0.992019058823529">
for smaller restrictions. The discretization of
tagging points enables not only coherent tag-
ging but also the reduction of tagging cost.
• Elaboration using synthesized sound: An
annotator checks the validity of the anno-
tation by listening to the sounds. In other
words, an annotator elaborates the annotation
by revising it many times by listening to the
automatically created dialogue sound which
includes not only driver’s voices but also
sounds of back-channel feedbacks generated
according to the provided timings. The back-
channel feedbacks had been synthesized by
using a speech synthesizer because our cor-
pus aims to be used for implementing the
system which can provide back-channel feed-
backs.
</bodyText>
<sectionHeader confidence="0.912515" genericHeader="method">
3 Corpus Construction
</sectionHeader>
<bodyText confidence="0.9998355">
We constructed the back-channel feedback corpus
by annotating an in-car speech dialogue corpus.
</bodyText>
<subsectionHeader confidence="0.660108">
3.1 CIAIR in-car spoken dialogue corpus
</subsectionHeader>
<bodyText confidence="0.999979666666667">
We used the CIAIR in-car spoken dialogue corpus
(Kawaguchi et al., 2005) as the target of annota-
tion. The corpus consists of the speech and tran-
scription data of dialogues between a driver and
an operator about shopping guides, driving direc-
tions, and so on. Figure 1 shows an example of
the transcription. We used only the utterances of
drivers in the corpus. We divided the utterances
into morphemes by using the morphological ana-
lyzer Chasen1. In addition, each morpheme was
provided start and end times estimated by using
the continuous speech recognition system Julius2.
</bodyText>
<subsectionHeader confidence="0.958011">
3.2 Tagging of spoken dialogue corpus
</subsectionHeader>
<bodyText confidence="0.9996843125">
We constructed the corpus by providing the back-
channel feedback tags at the proper timings for
the driver’s utterances, according to the design de-
scribed in Section 2.
lhttp://chasen-legacy.sourceforge.jp
zhttp://julius.sourceforge.jp
For this reason, the timings at which the sys-
tem provides back-channel feedbacks become im-
portant. Several researches investigated back-
channel feedback timings in human-human dia-
logues (Cathcart et al., 2003; Maynard, 1989;
Takeuchi et al., 2004; Ward and Tsukahara, 2000).
They reported back-channel feedbacks had the fol-
lowing tendencies: “within or after a pause,” “after
a conjunction or sentence-final particle,” and “af-
ter a clause wherein the final pitch descends.”
However, it is difficult to systematize the ap-
propriate timings of back-channel feedbacks since
their detection is intertwined in a complex way
with various acoustic and linguistic factors. Al-
though machine learning using large-scale data
would be a solution to the problem, existing spo-
ken dialogue corpora are not suitable for direct
use as data, because the timings of the back-
channel feedbacks lack coherency due to the in-
fluence of factors such as the psychological state
of a speaker, the environment and so on.
In our research, to create more pragmatic data
in which the above-mentioned problem is solved,
we constructed the back-channel feedback corpus
with coherency. To this end, we established the
following policies for annotation:
</bodyText>
<listItem confidence="0.9299178">
• Comprehensive tagging: Back-channel
feedback tags are provided for all timings
which are not unnatural. In human-human
dialogues, there are some cases that even if a
timing is suited for providing a back-channel
feedback, no back-channel feedback is not
provided (Ward and Tsukahara, 2000). On
the other hand, in our corpus, comprehensive
tagging enables coherent tagging.
• Off-line tagging: Annotators tag all tim-
ings at which back-channel feedbacks can be
provided after listening to the target speech
one or more times. Compared with providing
back-channel feedbacks in on-line environ-
ment, the off-line annotation decreases the
chances of tagging wrong positions or failing
in tagging back-channel feedbacks, realizing
coherent tagging.
• Discretization of tagging points: Tagging
is performed for each segment into which
driver’s utterances are divided. In a nor-
mal dialogue, the listener can provide back-
channel feedbacks whenever he/she wants to,
but the inconsistency in the timings to give
such feedbacks becomes larger in exchange
</listItem>
<page confidence="0.993598">
206
</page>
<figure confidence="0.476424285714286">
turn ID
driver ID
play button
update button
list of turn IDs
content start time end time
sp [short pause] 0.000 0.030
(Ft) (Well...) 0.030 0.090
A� (clothes) 0.090 0.340
� (no translation) 0.340 0.520
sp [short pause] 0.520 0.610
WLN (buy) 0.610 0.850
kLN (want to) 0.850 1.080
/v (no translation) 1.080 1.150
t_ (no translation) 1.150 1.240
IfE (so) 1.240 1.420
E-.) (somewhere) 1.420 1.670
bN (no translation) 1.670 1.850
jff( (near hear) 1.850 2.190
IZ (no translation) 2.190 2.880
sp [short pause] 2.880 3.080
pause [pause] 3.080 4.992
VLN (inexpensive) 4.992 5.362
8 (no translation) 5.362 5.422
ix (shop) 5.422 5.652
toZD (is there) 5.652 5.832
bN (no translation) 5.832 5.982
td;to (no translation) 5.982 6.272
</figure>
<figureCaption confidence="0.993391">
Figure 2: Sample of division of a dialogue turn
into basic segments
</figureCaption>
<bodyText confidence="0.999964193548387">
For “comprehensive tagging,” an annotator lis-
tens to each dialogue turn3 from the start and tags
a position where a back-channel feedback can be
provided when the timing is found. Here, the tim-
ing of the last back-channel feedback is also used
for judging whether or not the timing is unnatural.
For “off-line tagging,” an annotator tags the
transcribed text of each dialogue turn of drivers.
To perform “discretization of tagging points,” a
dialogue turn is assumed to be a sequence of mor-
phemes or pauses (hereafter, we call them basic
segments), which are continuously arranged on
the time axis, and it is judged whether or not a
back-channel feedback should be provided at each
basic segment. Here, in consideration of the un-
equal pause durations, if the length of a pause is
over 200ms, the pause is divided into the initial
200ms pause and the subsequent pause, each of
which is considered as a basic segment. Figure 2
shows an example of a dialogue turn divided into
basic segments.
Furthermore, for “elaboration using synthesized
sound,” we prepared the annotation environment
where the dialogue sound including not only
driver’s voice but also back-channel feedbacks
generated according to the provided timings is au-
tomatically created in real time for annotators to
listen to. There are several types of back-channel
feedbacks and in normal conversations, we choose
and use appropriate back-channel feedbacks from
among them according to the scene. In our study,
</bodyText>
<footnote confidence="0.99524725">
3A dialogue turn is defined as the interval between the
time at which the driver starts to utter just after the opera-
tor finishes uttering and the time at which the driver finishes
uttering just before the operator starts to utter.
</footnote>
<figureCaption confidence="0.831916">
Figure 3: Web interface for tagging
</figureCaption>
<tableCaption confidence="0.6756">
Table 1: Size of back-channel feedback corpus
</tableCaption>
<table confidence="0.959262571428572">
drivers 346
dialogue turns 11,181
clauses 16,896
bunsetsus4 12,689
morpheme segments 94,030
pause segments 19,142
back-channel feedbacks 5,416
</table>
<bodyText confidence="0.999909923076923">
we used the most general form ”はい hai (yes)”
for the synthesized speech since our focus was
on the timing of back-channel feedbacks. The
back-channel feedbacks had been created by us-
ing Hitachi’s speech synthesizer “HitVoice,” and
one feedback was placed 50 milli-seconds after the
start time of a tagged basic segment.
We developed a Web interface for tagging back-
channel feedbacks. Figure 3 shows the Web inter-
face. The interface displays a sequence of basic
segments in a dialogue turn in table format. Anno-
tators perform tagging by checking basic segments
where a back-channel feedback can be provided.
</bodyText>
<subsectionHeader confidence="0.999236">
3.3 Size of back-channel feedback corpus
</subsectionHeader>
<bodyText confidence="0.9816446">
Table 1 shows the size of our corpus constructed
by two trained annotators. The corpus includes
5,416 back-channel feedbacks. This means that a
back-channel feedback is generated at intervals of
about 21 basic segments.
</bodyText>
<sectionHeader confidence="0.971584" genericHeader="method">
4 Corpus Evaluation
</sectionHeader>
<bodyText confidence="0.9991825">
We conducted experiments for evaluating the tag-
ging in the constructed corpus.
</bodyText>
<footnote confidence="0.990880333333333">
4Bunsetsu is a linguistic unit in Japanese that roughly cor-
responds to a basic phrase in English. A bunsetsu consists of
one independent word and zero or more ancillary words.
</footnote>
<page confidence="0.991225">
207
</page>
<subsectionHeader confidence="0.997653">
4.1 Coherency of corpus tagging
</subsectionHeader>
<bodyText confidence="0.999902857142857">
We conducted an evaluation experiment to con-
firm that the tagging is coherently performed in
the corpus. In the experiment, two different an-
notators performed tagging on the same data, and
then we measured the degree of the agreement be-
tween them. As the indicator, we used Cohen’s
kappa value (Cohen, 1960), calculated as follows:
</bodyText>
<equation confidence="0.9638185">
P(O) − P(E)
1 − P(E)
</equation>
<bodyText confidence="0.999984913043478">
where P(O) is the observed agreement between
annotators, and P(E) is the hypothetical proba-
bility of chance agreement. A subject who has
a certain level of knowledge annotated 673 dia-
logue turns. The kappa value was 0.731 (P(O) =
0.975, P(E) = 0.907), and thus we can see the
substantial agreement between annotators.
As the target for comparison, we used the kappa
value in the existing back-channel feedback cor-
pus (Kamiya et al., 2010). The corpus had been
constructed by the way that the recorded driver’s
voice was replayed and 4 subjects independently
produced back-channel feedbacks for the same
sound. This means that the policies for tagging
the existing corpus differ from those of our corpus,
and are “on-line tagging,” “tagging on the time
axis” and “tagging without elaborating.” In the
exisiting corpus, 297 dialogue turns were used as
driver’s sound. Table 2 shows the kappa value be-
tween two among the 4 subjects. The kappa value
of our corpus was higher than that between any
subjects of the existing corpus, substantiating the
high coherency of our corpus.
</bodyText>
<subsectionHeader confidence="0.999556">
4.2 Validity of corpus tagging
</subsectionHeader>
<bodyText confidence="0.999991476190476">
In our corpus, we discretized the tagging points
to enhance the coherency of tagging. However,
such constraint restricts the points available for
tagging and may make annotators provide tags at
the unnatural timings. Therefore, we conducted
a subjective experiment to evaluate the natural-
ness of the back-channel feedback timings. In
the experiment, one subject listened to the replay
of our back-channel feedback corpus and subjec-
tively judged the naturalness of each timing. The
back-channel feedback sound was generated in the
same way described in Section 3.2.
In the experiment, we used 345 dialogue turns
including 131 back-channel feedbacks. 98.47%
of all the back-channel feedbacks were judged to
be natural. Only 2 back-channel feedbacks were
judged to be unnatural because the intervals be-
tween them and the back-channel feedbacks pro-
vided immediately before them were felt too short.
This showed the validity of our discretization of
tagging points.
</bodyText>
<sectionHeader confidence="0.999198" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999776058823529">
This paper described the design, construction and
evaluation of the back-channel feedback corpus
which had the coherency of tagged back-channel
feedback timings. We constructed the spoken di-
alogue corpus including 5,416 back-channel feed-
backs in 11,181 dialogue turns. The results of our
evaluation confirmed high coherency and enough
naturalness of our corpus.
In the future, we will use our corpus to see
to what extent the timings of back-channel feed-
backs that have been annotated correlate with the
cues provided by earlier researchers. Then we will
develop a system which can detect back-channel
feedback timings comprehensively.
Acknowledgments: This research was supported
in part by the Grant-in-Aid for Challenging Ex-
ploratory Research (No.21650028) of JSPS.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999917">
N. Cathcart, J. Carletta, and E. Klein. 2003. A shal-
low model of backchannel continuers in spoken dia-
logue. In Proc. of 10th EACL, pages 51–58.
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20:37–46.
Y. Kamiya, T. Ohno, S. Matsubara, and H. Kashioka.
2010. Construction of back-channel utterance cor-
pus for responsive spoken dialogue system develop-
ment. In Proc. of 7th LREC.
N. Kawaguchi, S. Matsubara, K. Takeda, and
F. Itakura. 2005. CIAIR in-car speech corpus –
influence of driving status–. IEICE Trans. on Info.
and Sys., E88-D(3):578–582.
S. K. Maynard. 1989. Japanese conversation :
self-contextualization through structure and interac-
tional management. Ablex.
M. Takeuchi, N. Kitaoka, and S. Nakagawa. 2004.
Timing detection for realtime dialog systems using
prosodic and linguistic information. In Proc. of
Speech Prosody 2004, pages 529–532.
N. Ward and W. Tsukahara. 2000. Prosodic features
which cue back-channel responses in English and
Japanese. Journal of Pragmatics, 32:1177–1207.
</reference>
<tableCaption confidence="0.855678">
Table 2: Kappa values of the existing corpus
</tableCaption>
<bodyText confidence="0.7469675">
a,c a,d a,b c,d b,c b,d
r, 0.536 0.438 0.322 0.311 0.310 0.167
</bodyText>
<equation confidence="0.843002">
K =
</equation>
<page confidence="0.990712">
208
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.118167">
<title confidence="0.9098675">Coherent Back-Channel Feedback Tagging In-Car Spoken Dialogue Corpus</title>
<author confidence="0.800102">Yuki</author>
<affiliation confidence="0.975884666666667">Graduate School Information Nagoya University, Japan</affiliation>
<email confidence="0.932705">kamiya@el.itc.nagoya-u.ac.jp</email>
<author confidence="0.532212">Tomohiro</author>
<affiliation confidence="0.923462666666667">Graduate School International Nagoya University, Japan</affiliation>
<email confidence="0.906164">ohno@nagoya-u.jp</email>
<author confidence="0.59735">Shigeki Matsubara</author>
<affiliation confidence="0.980856333333333">Graduate School of Information Science, Nagoya University, Japan</affiliation>
<email confidence="0.956626">matubara@nagoya-u.jp</email>
<abstract confidence="0.991576375">This paper describes the design of a backchannel feedback corpus and its evaluation, aiming at realizing in-car spoken dialogue systems with high responsiveness. We constructed our corpus by annotating the existing in-car spoken dialogue data with back-channel feedback timing information in an off-line environment. Our corpus can be practically used in developing dialogue systems which can provide verbal back-channel feedbacks. As the results of our evaluation, we confirmed that our proposed design enabled the construction of back-channel feedback corpora with high coherency and naturalness.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Cathcart</author>
<author>J Carletta</author>
<author>E Klein</author>
</authors>
<title>A shallow model of backchannel continuers in spoken dialogue.</title>
<date>2003</date>
<booktitle>In Proc. of 10th EACL,</booktitle>
<pages>51--58</pages>
<contexts>
<context position="2548" citStr="Cathcart et al., 2003" startWordPosition="370" endWordPosition="373">esponses, that is, back-channel feedbacks. Furthermore, in the response strategy for realizing in-car dialogues in which drivers feel comfortable, it is necessary for the system to provide back-channel feedbacks during driver’s utterances aggressively as well as timely. This paper describes the design of a backchannel feedback corpus having coherency (tagging is performed by different annotators equally) and naturalness, and its evaluation, aiming at realizing in-car spoken dialogue systems with high responsiveness. Although there have been several researches on back-channel feedback timings (Cathcart et al., 2003; Maynard, 1989; Takeuchi et al., 2004; Ward and Tsukahara, 2000), in many of them, back-channel feedback timings in human dialogues were observed and analyzed by using a general spoken dialogue corpus. On the other hand, we constructed our corpus by annotating the existing in-car spoken dialogue data with backchannel feedback timing information in an off-line environment. Our corpus can be practically used in developing dialogue systems which can provide back-channel feedbacks. In our research, the driver utterances (11,181 turns) in the CIAIR in-car spoken dialogue corpus (Kawaguchi et al., </context>
<context position="6840" citStr="Cathcart et al., 2003" startWordPosition="1020" endWordPosition="1023">er Chasen1. In addition, each morpheme was provided start and end times estimated by using the continuous speech recognition system Julius2. 3.2 Tagging of spoken dialogue corpus We constructed the corpus by providing the backchannel feedback tags at the proper timings for the driver’s utterances, according to the design described in Section 2. lhttp://chasen-legacy.sourceforge.jp zhttp://julius.sourceforge.jp For this reason, the timings at which the system provides back-channel feedbacks become important. Several researches investigated backchannel feedback timings in human-human dialogues (Cathcart et al., 2003; Maynard, 1989; Takeuchi et al., 2004; Ward and Tsukahara, 2000). They reported back-channel feedbacks had the following tendencies: “within or after a pause,” “after a conjunction or sentence-final particle,” and “after a clause wherein the final pitch descends.” However, it is difficult to systematize the appropriate timings of back-channel feedbacks since their detection is intertwined in a complex way with various acoustic and linguistic factors. Although machine learning using large-scale data would be a solution to the problem, existing spoken dialogue corpora are not suitable for direc</context>
</contexts>
<marker>Cathcart, Carletta, Klein, 2003</marker>
<rawString>N. Cathcart, J. Carletta, and E. Klein. 2003. A shallow model of backchannel continuers in spoken dialogue. In Proc. of 10th EACL, pages 51–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="13100" citStr="Cohen, 1960" startWordPosition="2022" endWordPosition="2023">us Evaluation We conducted experiments for evaluating the tagging in the constructed corpus. 4Bunsetsu is a linguistic unit in Japanese that roughly corresponds to a basic phrase in English. A bunsetsu consists of one independent word and zero or more ancillary words. 207 4.1 Coherency of corpus tagging We conducted an evaluation experiment to confirm that the tagging is coherently performed in the corpus. In the experiment, two different annotators performed tagging on the same data, and then we measured the degree of the agreement between them. As the indicator, we used Cohen’s kappa value (Cohen, 1960), calculated as follows: P(O) − P(E) 1 − P(E) where P(O) is the observed agreement between annotators, and P(E) is the hypothetical probability of chance agreement. A subject who has a certain level of knowledge annotated 673 dialogue turns. The kappa value was 0.731 (P(O) = 0.975, P(E) = 0.907), and thus we can see the substantial agreement between annotators. As the target for comparison, we used the kappa value in the existing back-channel feedback corpus (Kamiya et al., 2010). The corpus had been constructed by the way that the recorded driver’s voice was replayed and 4 subjects independen</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Kamiya</author>
<author>T Ohno</author>
<author>S Matsubara</author>
<author>H Kashioka</author>
</authors>
<title>Construction of back-channel utterance corpus for responsive spoken dialogue system development.</title>
<date>2010</date>
<booktitle>In Proc. of 7th LREC.</booktitle>
<contexts>
<context position="13584" citStr="Kamiya et al., 2010" startWordPosition="2103" endWordPosition="2106">he same data, and then we measured the degree of the agreement between them. As the indicator, we used Cohen’s kappa value (Cohen, 1960), calculated as follows: P(O) − P(E) 1 − P(E) where P(O) is the observed agreement between annotators, and P(E) is the hypothetical probability of chance agreement. A subject who has a certain level of knowledge annotated 673 dialogue turns. The kappa value was 0.731 (P(O) = 0.975, P(E) = 0.907), and thus we can see the substantial agreement between annotators. As the target for comparison, we used the kappa value in the existing back-channel feedback corpus (Kamiya et al., 2010). The corpus had been constructed by the way that the recorded driver’s voice was replayed and 4 subjects independently produced back-channel feedbacks for the same sound. This means that the policies for tagging the existing corpus differ from those of our corpus, and are “on-line tagging,” “tagging on the time axis” and “tagging without elaborating.” In the exisiting corpus, 297 dialogue turns were used as driver’s sound. Table 2 shows the kappa value between two among the 4 subjects. The kappa value of our corpus was higher than that between any subjects of the existing corpus, substantiati</context>
</contexts>
<marker>Kamiya, Ohno, Matsubara, Kashioka, 2010</marker>
<rawString>Y. Kamiya, T. Ohno, S. Matsubara, and H. Kashioka. 2010. Construction of back-channel utterance corpus for responsive spoken dialogue system development. In Proc. of 7th LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kawaguchi</author>
<author>S Matsubara</author>
<author>K Takeda</author>
<author>F Itakura</author>
</authors>
<title>CIAIR in-car speech corpus – influence of driving status–.</title>
<date>2005</date>
<booktitle>IEICE Trans. on Info. and Sys., E88-D(3):578–582.</booktitle>
<contexts>
<context position="3153" citStr="Kawaguchi et al., 2005" startWordPosition="461" endWordPosition="464">hcart et al., 2003; Maynard, 1989; Takeuchi et al., 2004; Ward and Tsukahara, 2000), in many of them, back-channel feedback timings in human dialogues were observed and analyzed by using a general spoken dialogue corpus. On the other hand, we constructed our corpus by annotating the existing in-car spoken dialogue data with backchannel feedback timing information in an off-line environment. Our corpus can be practically used in developing dialogue systems which can provide back-channel feedbacks. In our research, the driver utterances (11,181 turns) in the CIAIR in-car spoken dialogue corpus (Kawaguchi et al., 2005) were used as the existing data. We created the Web interface for the annotation of back-channel feedbacks and constructed the corpus including 5,416 back-channel feedbacks. Experiments have shown that our proposed corpus design enabled the construction of backchannel feedback corpora with high coherency and naturalness. 2 Corpus Design A back-channel feedback is a sign to inform a speaker that the listener received the speaker’s utterances. Thus, in an in-car dialogue between a driver and a system, it is preferable that the system provides as many back-channel feedbacks as possible. However, </context>
<context position="5856" citStr="Kawaguchi et al., 2005" startWordPosition="870" endWordPosition="873"> by revising it many times by listening to the automatically created dialogue sound which includes not only driver’s voices but also sounds of back-channel feedbacks generated according to the provided timings. The backchannel feedbacks had been synthesized by using a speech synthesizer because our corpus aims to be used for implementing the system which can provide back-channel feedbacks. 3 Corpus Construction We constructed the back-channel feedback corpus by annotating an in-car speech dialogue corpus. 3.1 CIAIR in-car spoken dialogue corpus We used the CIAIR in-car spoken dialogue corpus (Kawaguchi et al., 2005) as the target of annotation. The corpus consists of the speech and transcription data of dialogues between a driver and an operator about shopping guides, driving directions, and so on. Figure 1 shows an example of the transcription. We used only the utterances of drivers in the corpus. We divided the utterances into morphemes by using the morphological analyzer Chasen1. In addition, each morpheme was provided start and end times estimated by using the continuous speech recognition system Julius2. 3.2 Tagging of spoken dialogue corpus We constructed the corpus by providing the backchannel fee</context>
</contexts>
<marker>Kawaguchi, Matsubara, Takeda, Itakura, 2005</marker>
<rawString>N. Kawaguchi, S. Matsubara, K. Takeda, and F. Itakura. 2005. CIAIR in-car speech corpus – influence of driving status–. IEICE Trans. on Info. and Sys., E88-D(3):578–582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K Maynard</author>
</authors>
<title>Japanese conversation : self-contextualization through structure and interactional management.</title>
<date>1989</date>
<journal>Ablex.</journal>
<contexts>
<context position="2563" citStr="Maynard, 1989" startWordPosition="374" endWordPosition="375">-channel feedbacks. Furthermore, in the response strategy for realizing in-car dialogues in which drivers feel comfortable, it is necessary for the system to provide back-channel feedbacks during driver’s utterances aggressively as well as timely. This paper describes the design of a backchannel feedback corpus having coherency (tagging is performed by different annotators equally) and naturalness, and its evaluation, aiming at realizing in-car spoken dialogue systems with high responsiveness. Although there have been several researches on back-channel feedback timings (Cathcart et al., 2003; Maynard, 1989; Takeuchi et al., 2004; Ward and Tsukahara, 2000), in many of them, back-channel feedback timings in human dialogues were observed and analyzed by using a general spoken dialogue corpus. On the other hand, we constructed our corpus by annotating the existing in-car spoken dialogue data with backchannel feedback timing information in an off-line environment. Our corpus can be practically used in developing dialogue systems which can provide back-channel feedbacks. In our research, the driver utterances (11,181 turns) in the CIAIR in-car spoken dialogue corpus (Kawaguchi et al., 2005) were used</context>
<context position="6855" citStr="Maynard, 1989" startWordPosition="1024" endWordPosition="1025">, each morpheme was provided start and end times estimated by using the continuous speech recognition system Julius2. 3.2 Tagging of spoken dialogue corpus We constructed the corpus by providing the backchannel feedback tags at the proper timings for the driver’s utterances, according to the design described in Section 2. lhttp://chasen-legacy.sourceforge.jp zhttp://julius.sourceforge.jp For this reason, the timings at which the system provides back-channel feedbacks become important. Several researches investigated backchannel feedback timings in human-human dialogues (Cathcart et al., 2003; Maynard, 1989; Takeuchi et al., 2004; Ward and Tsukahara, 2000). They reported back-channel feedbacks had the following tendencies: “within or after a pause,” “after a conjunction or sentence-final particle,” and “after a clause wherein the final pitch descends.” However, it is difficult to systematize the appropriate timings of back-channel feedbacks since their detection is intertwined in a complex way with various acoustic and linguistic factors. Although machine learning using large-scale data would be a solution to the problem, existing spoken dialogue corpora are not suitable for direct use as data, </context>
</contexts>
<marker>Maynard, 1989</marker>
<rawString>S. K. Maynard. 1989. Japanese conversation : self-contextualization through structure and interactional management. Ablex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Takeuchi</author>
<author>N Kitaoka</author>
<author>S Nakagawa</author>
</authors>
<title>Timing detection for realtime dialog systems using prosodic and linguistic information.</title>
<date>2004</date>
<booktitle>In Proc. of Speech Prosody</booktitle>
<pages>529--532</pages>
<contexts>
<context position="2586" citStr="Takeuchi et al., 2004" startWordPosition="376" endWordPosition="379">cks. Furthermore, in the response strategy for realizing in-car dialogues in which drivers feel comfortable, it is necessary for the system to provide back-channel feedbacks during driver’s utterances aggressively as well as timely. This paper describes the design of a backchannel feedback corpus having coherency (tagging is performed by different annotators equally) and naturalness, and its evaluation, aiming at realizing in-car spoken dialogue systems with high responsiveness. Although there have been several researches on back-channel feedback timings (Cathcart et al., 2003; Maynard, 1989; Takeuchi et al., 2004; Ward and Tsukahara, 2000), in many of them, back-channel feedback timings in human dialogues were observed and analyzed by using a general spoken dialogue corpus. On the other hand, we constructed our corpus by annotating the existing in-car spoken dialogue data with backchannel feedback timing information in an off-line environment. Our corpus can be practically used in developing dialogue systems which can provide back-channel feedbacks. In our research, the driver utterances (11,181 turns) in the CIAIR in-car spoken dialogue corpus (Kawaguchi et al., 2005) were used as the existing data. </context>
<context position="6878" citStr="Takeuchi et al., 2004" startWordPosition="1026" endWordPosition="1029"> was provided start and end times estimated by using the continuous speech recognition system Julius2. 3.2 Tagging of spoken dialogue corpus We constructed the corpus by providing the backchannel feedback tags at the proper timings for the driver’s utterances, according to the design described in Section 2. lhttp://chasen-legacy.sourceforge.jp zhttp://julius.sourceforge.jp For this reason, the timings at which the system provides back-channel feedbacks become important. Several researches investigated backchannel feedback timings in human-human dialogues (Cathcart et al., 2003; Maynard, 1989; Takeuchi et al., 2004; Ward and Tsukahara, 2000). They reported back-channel feedbacks had the following tendencies: “within or after a pause,” “after a conjunction or sentence-final particle,” and “after a clause wherein the final pitch descends.” However, it is difficult to systematize the appropriate timings of back-channel feedbacks since their detection is intertwined in a complex way with various acoustic and linguistic factors. Although machine learning using large-scale data would be a solution to the problem, existing spoken dialogue corpora are not suitable for direct use as data, because the timings of </context>
</contexts>
<marker>Takeuchi, Kitaoka, Nakagawa, 2004</marker>
<rawString>M. Takeuchi, N. Kitaoka, and S. Nakagawa. 2004. Timing detection for realtime dialog systems using prosodic and linguistic information. In Proc. of Speech Prosody 2004, pages 529–532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ward</author>
<author>W Tsukahara</author>
</authors>
<title>Prosodic features which cue back-channel responses in English and Japanese.</title>
<date>2000</date>
<journal>Journal of Pragmatics,</journal>
<pages>32--1177</pages>
<contexts>
<context position="2613" citStr="Ward and Tsukahara, 2000" startWordPosition="380" endWordPosition="383">e response strategy for realizing in-car dialogues in which drivers feel comfortable, it is necessary for the system to provide back-channel feedbacks during driver’s utterances aggressively as well as timely. This paper describes the design of a backchannel feedback corpus having coherency (tagging is performed by different annotators equally) and naturalness, and its evaluation, aiming at realizing in-car spoken dialogue systems with high responsiveness. Although there have been several researches on back-channel feedback timings (Cathcart et al., 2003; Maynard, 1989; Takeuchi et al., 2004; Ward and Tsukahara, 2000), in many of them, back-channel feedback timings in human dialogues were observed and analyzed by using a general spoken dialogue corpus. On the other hand, we constructed our corpus by annotating the existing in-car spoken dialogue data with backchannel feedback timing information in an off-line environment. Our corpus can be practically used in developing dialogue systems which can provide back-channel feedbacks. In our research, the driver utterances (11,181 turns) in the CIAIR in-car spoken dialogue corpus (Kawaguchi et al., 2005) were used as the existing data. We created the Web interfac</context>
<context position="6905" citStr="Ward and Tsukahara, 2000" startWordPosition="1030" endWordPosition="1033"> end times estimated by using the continuous speech recognition system Julius2. 3.2 Tagging of spoken dialogue corpus We constructed the corpus by providing the backchannel feedback tags at the proper timings for the driver’s utterances, according to the design described in Section 2. lhttp://chasen-legacy.sourceforge.jp zhttp://julius.sourceforge.jp For this reason, the timings at which the system provides back-channel feedbacks become important. Several researches investigated backchannel feedback timings in human-human dialogues (Cathcart et al., 2003; Maynard, 1989; Takeuchi et al., 2004; Ward and Tsukahara, 2000). They reported back-channel feedbacks had the following tendencies: “within or after a pause,” “after a conjunction or sentence-final particle,” and “after a clause wherein the final pitch descends.” However, it is difficult to systematize the appropriate timings of back-channel feedbacks since their detection is intertwined in a complex way with various acoustic and linguistic factors. Although machine learning using large-scale data would be a solution to the problem, existing spoken dialogue corpora are not suitable for direct use as data, because the timings of the backchannel feedbacks l</context>
<context position="8141" citStr="Ward and Tsukahara, 2000" startWordPosition="1222" endWordPosition="1225">cy due to the influence of factors such as the psychological state of a speaker, the environment and so on. In our research, to create more pragmatic data in which the above-mentioned problem is solved, we constructed the back-channel feedback corpus with coherency. To this end, we established the following policies for annotation: • Comprehensive tagging: Back-channel feedback tags are provided for all timings which are not unnatural. In human-human dialogues, there are some cases that even if a timing is suited for providing a back-channel feedback, no back-channel feedback is not provided (Ward and Tsukahara, 2000). On the other hand, in our corpus, comprehensive tagging enables coherent tagging. • Off-line tagging: Annotators tag all timings at which back-channel feedbacks can be provided after listening to the target speech one or more times. Compared with providing back-channel feedbacks in on-line environment, the off-line annotation decreases the chances of tagging wrong positions or failing in tagging back-channel feedbacks, realizing coherent tagging. • Discretization of tagging points: Tagging is performed for each segment into which driver’s utterances are divided. In a normal dialogue, the lis</context>
</contexts>
<marker>Ward, Tsukahara, 2000</marker>
<rawString>N. Ward and W. Tsukahara. 2000. Prosodic features which cue back-channel responses in English and Japanese. Journal of Pragmatics, 32:1177–1207.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>