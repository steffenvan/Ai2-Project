<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<page confidence="0.557959">
2
</page>
<figure confidence="0.806766739130435">
Ei
@ ACL2010
5 Hal Daumé III (me@hal3.name) SP2IRL @ ACL2010
Structured prediction 101
Learn a function mapping inputs to complex outputs:
Input Space Decoding Output Space
I
f : X � Y
can
can
a
can
Why is structure important?
&gt;- Correlations among outputs
-• Determiners often precede nouns
-• Sentences usually have verbs
• Global coherence
- It just doesn&apos;t make sense to have three determiners next to
each other
My objective (aka &amp;quot;loss function&amp;quot;) forces it
-• Translations should have good sequences of words
-• Summaries should be coherent
Outline: Part I
</figure>
<listItem confidence="0.9572947">
• What is Structured Prediction?
• Refresher on Binary Classification
• What does it mean to learn?
• Linear models for classification
• Batch versus stochastic optimization
• From Perceptron to Structured Perceptron
-• Linear models for Structured Prediction
-• The &amp;quot;argmax&amp;quot; problem
-• From Perceptron to margins
• Learning to Search
</listItem>
<figure confidence="0.842293432432432">
-• Stacking
-• Incremental Parsing
SP2IRL g ACL2010
SP2IRL g ACL2010
Hal Daume Ill (me@hal3.name)
Hal Daume III (meghal3.name)
Refresher on
Binary Classification
Hal Daume III (me@hal3.name) SP2IRL g ACL2010
Outline: Part ll
Refresher on Reinforcement Learning SP2IRL g ACL2010
Markov Decision Processes
Q learning
• Inverse Reinforcement Learning
-• Determining rewards given policies
-• Maximum margin planning
Apprenticeship Learning
Seamn
-• Dagger
• Discussion
Hal Daurne Ill (me@hal3.name)
What does it mean to learn?
Informally:
- to predict the future based on the past
Slightly-less-informally:
- to take labeled examples and construct a function that will
label them as a human would
Formally:
-• Given:
-• A fixed unknown distribution D over X*Y
-• A loss function over Y&amp;quot;Y
-• A finite sample of (x,y) pairs drawn i.i.d. from D
-• Construct a function f that has low expected loss with
respect to D
Hal Daume III (meghal3.name) SP2IRL g ACL2010
Feature extractors
&gt;. A feature extractor (I) maps examples to vectors
</figure>
<table confidence="0.9606595">
Dear Sir. W=dear : 1
First, I must solicit W=sir 1
your confidence in W=this : 2
this transaction, W=wish : 0
this is by virture of MISSPELLED : 2
its nature as being NAMELESS : 1
utterly confidencial
and top secret. ...
ALL CAPS : 0
NUM URLS : 0
&gt;- Feature vectors in NLP are frequently sparse
Hal Daume III (meghal3.name) SP2IRL g ACL2010
</table>
<figure confidence="0.970908230769231">
Linear models for binary classification
Decision boundary
is the set of
&amp;quot;uncertain&amp;quot;points
&gt;- Linear decision
boundaries are
characterized by
weight vectors
&amp;quot;free 0(x) Ei Wi l(x)
money&amp;quot; (1)(-3)
(1)(4) +
(1)(2) +
(0)(0) +
</figure>
<page confidence="0.746233">
=3
</page>
<figureCaption confidence="0.66809025">
BIAS : 1 BIAS : -3
free : 1 free : 4
money : 1 money : 2
the : the : 0
</figureCaption>
<figure confidence="0.939510315789474">
Hal Daume Ill (me@hal3.name) SP2IRL g ACL2010
The perceptron
• Inputs = feature values
• Params = weights
• Sum is the response
(1)
1:1)
&gt;0?
(I) \1\H3
-
If the response is:
- Positive, output +1
Negative, output -1
When training,
update on errors:
w=w+ yoh(x)
Call bocy or Soma
&amp;quot;Error&amp;quot; when:
yW•ch(X)0
</figure>
<page confidence="0.749173">
13
</page>
<figure confidence="0.992626976377953">
ir0
Why does that update work?
ld
&gt;- When ywo •cMx)-0
, updateznew= w old ± y (x)
max
max
min
margin s.t.
margin s.t.
1114&apos;112 s.t.
31,,w*.45(x„)±„1
, Vn
, Vn
+cn
For n=1..N:
If yo47.4)(xn)0
w=w+Yncti(x„)
Support vector machines
Explicitly optimize
the margin
&gt;- Enforce that
all training points
are correctly
classified
Hal Daurne III (meghal3.name) SP2IRL g ACL2010
all points are
correctly classified
ynw.cp(x„)1 , Vn
yo47.4)(xn)1 , Vn
Ywnew wold Y &apos;104(x)
= Y wok&apos; 95(x)+ YY (15(x)(15(x)
&lt;0 &gt;
Hal Daurne Ill (meghal3.nan SP2IRL g ACL2010
Support vector machines with slack
Explicitly optimize
the margin
Allow some &amp;quot;noisy&amp;quot;
points to be
misclassified
yo47.4)(x)+ &gt;1 , Vn
, Vn
Hal Daurne Ill (me@hal3.name) SP2IRL g ACL2010
Batch versus stochastic optimization
&gt;- Batch = read in all the data, then process it
&gt;- Stochastic = (roughly) process a bit at a time
Hal Daume III (me@hal3.name) SP2IRL g ACL2010
Hal Daume III (meghal3.name) SP2IRL g ACL2010
Hal Daume III (me@hal3.name) SP2IRL g ACL2010
Wi
w2
W3
Stochastically optimized SVMs
SVM Objective
SOME
MATH
Implementation Note:
Weight shrinkage is SLOW.
Implement it lazily, at the
cost of double storage.
From Perceptron
to Structured Perceptron
For n=1..N:
• If ynw.(t.(x„)-1
w=w + n(1)(x)
w=(1-1 )w
CN
For n=1..N:
If y nw..1)(x„)0
w=w+ Y nch(xn)
Perceptron with multiple classes
&gt;- Store separate weight
vector for each class
W2, • • • WK
For n=1..N:
• Predict:
ji=arg maxkwk•ch(xn)
• If j&gt;#31:„
Y Y
W =W +01)(Xn)
Yn Yn
I Why does this
do the right thing?
Hal Daurne Ill (me@hal3.name)
SP2IRL g ACL2010
w1(x)
biggest
w3•0(x)
biggest
Perceptron with multiple classes v2
Originally:
For n=1..N:
Predict:
= arg maxkuik•ch(xn)
If 5/#3in
w,.=w-4(x1)
Y Y
W =141 + 4;1( Xn)
Yn Yn
For n=1..N:
Predict:
ji=arg maxkw4(x„,k)
5&gt;
w=w—ch(xn , Si)
Hal Daume III (me@hal3.name) SP2IRL g ACL2010
Md
Vb
Dt
Nn
Pro
can
can
a
can
I 2&apos;6
SP2IRL @ ACL2010
Hal Daume III (me@hal3.name)
SP2IRL @ ACL2010
24 Hal Daume Ill (me@hal3.name)
wql Pro]
Md]
w.[I_Vb]
w[can_Pro]
1Prw[Pro-Md]
/t
w[can Md]
w[Pro-Vb]
</figure>
<footnote confidence="0.750193833333333">
w[can \Mb]
can
can
(plus some work to account for boundary conditions)
la! Daume Ill (me@hal3.name)
SP2IRL @ ACL2010
</footnote>
<figure confidence="0.996278269938651">
Features for structured prediction
- Allowed to encode anything you want
- Output features, Markov features, other features
Argmax for sequences
)• If we only have output and Markov features, we can use
Viterbi algorithm:
vv.[Pro-Pro]
Structured perceptron
Enumeration Enumeration
over 1..K over all outputs
For n=1..N:
)• Predict:
57 = arg max k w • ch (x,,,k) = arg maxkw • ch( x,,, k)
• If .i1Yri
w=w—ch(xn,j))
+ch(xn,yr)
• IfS/yri
w = w —(x , )
+ca(xn,y,)
For n=1..N:
• Predict:
Structured perceptron as ranking
- For n=1..N:
- Run Viterbi: = arg maxkw • ch(x,,,k)
If 5/y w=w—ch(xn,5&apos;)+0(xn,yn)
- When does this make an update?
Pro Md Vb Dt Nn
Pro Md Md Dt Vb
Pro Md Md Dt Nn
Pro Md Nn Dt Md
Pro Md Nn Dt Nn
Pro Md Vb Dt Md
Pro Md Vb Dt Vb
can can a can
26 Hal Daume III (me@hal3.name)
05,
I Pro : 1 &lt;s›-Pro : 1 has verb : 1
can Md : 1 Pro-Md : 1 has nn lft : 0
can Vb : 1 Md-Vb : 1 has n lft : 1
a Dt : 1 Vb-Dt : 1 has nn rgt : 1
can Nn : 1 Dt-Nn : 1 has n rgt : 1
Nn-&lt;/s&gt; : 1 ...
SP2IRL @ ACL2010
447&apos;
From perceptron to margins
( x2, .Y2 )
( y )
—0 (X2,
Hal Dare Ill (meghal3.name) SP2IRL @ ACL2010
28
S. t.
W.95(Xn)Yn)
—W*95(Xn,51)
is
Ranking margins
Hal Dare Ill (me@hal3.8ar8e)
SP2IRL @ ACL2010
Margin Errors
Ilwil2 + c En n
yr, 14&apos;*96 (xn)-F
, V n
Each point is correctly
classified, modulo k
11W112+CIALS, q
W,
Response
for truth
s.L w•ch(xn,yn)
—w.ch(x,„51)
+„1,en,j/y
min
cn
7.
a)
r-
cn
CA
0
n
Each true output is more
highly ranked, modulo k
r-
cn
Hal Dare III (me@hal3.8ame) SP2IRL @ ACL2010
27
min
S. t.
Response
for other
- Some errors are worse than others...
t)
0
:F‘
Margin
P&apos;)
f one
I-
X
0
01
-I.
W
7-
0
=
0.3
N0
7
a
a
W
K
1—
71
o
cn
Pro Md Vb Dt Nn
Pro Md Md Dt Vb
Pro Md Md Dt Nn
Pro Md Nn Dt Md
Pro Md Nn Dt Nn
Pro Md Vb Dt Md
Pro Md Vb Dt Vb
can can a can
From perceptron to margins
( x SI)
—0(x2,S,)
(x2,5&apos;) 7(xi,5,)
min 1
c —11/47211 +CEjn,j,
AV,C, 2
Response
for truth
CA
0
n 0
o_
Et:
Each true output is more
highly ranked, modulo k
01
cn
J:1
0
01
Response
for other
Md
Vb
Pro
Dt
Nn
can
can
a
can
Accounting for a loss function
- Some errors are worse than others...
Pm H Md
prn Mn
Pro - Md
30 Hal Daume Ill (me@hal3.8ame)
argin
</figure>
<equation confidence="0.754321772727273">
f y,y)
SP2IRL @ ACL2010
[Taskar+al, JMLR05; Tshochandaritis, JMLR05]
Nn
Nn
nt
nt
Dt
44•,•,t
Hal Daume III (me@hal3.name) SP2IRL @ ACL2010
-0( (13(x,,y2)
—0(xi.)))
—0(x2S1)
—0(x, .P)
—0(x25&apos;) 7(xi5&apos;)
w•cp(x,„y,,)—w•cp(x„,SI)+„
&gt;1
31
r
w•cf.(x,„y)—w•cp(x,„5/)+„
1(YnyY
w.[Pro-Pro]
</equation>
<figure confidence="0.931188338289962">
32
Hal Daub III (meghal3.name)
SP2IRL @ ACL2010
0,
11)
—
w•[can Pro] a)
w [can_Vb]
+1
w [can Pro]
w.[Pro-Md]
w•[can Md]
w.[Pro-Vb]
w[can tit]
w•[can+/ Md]
IWhat are we
assuming here?
cc
Augmented argmax for sequences
- Add &amp;quot;loss&amp;quot; to each wrong node,
Accounting for a loss function
ca(xiyi)
Learning to Search
34 Hal Daume III (me@hal3.0ame) SP2IRL @ ACL2010
Stochastically optimizing Markov nets*
)&gt; M3N Objective For n=1..N: k
33 Viterbi: •
= arg max kw • ch x ri , 51, u
If s )
&amp;quot; S&apos;&apos;;, Cl)
w=w—ch(xn, ji) if;
+ cp.( xn, yn)
SP2IRL @ ACL2010
Hal Daume Ill (me@hal3.name)
SOME
MATH
maxkw
+1( y„,k)
— ch(xn,
1
Augmented
w=w
For n=1..N: Viterbi:
= arg • (/)( x, k)
if j&gt; SI)
w=(1 )w
CN
Argmax is hard! Stacking
&gt;- Classic formulation of structured prediction: &gt;- Click to add an outline
something we learn
score (x y) = to make &amp;quot;good&amp;quot; x,y pairs
score highly
At test time:
f (x) = argmaxy„score(x ,y)
[610v &apos;ireaue6B1/1]
Hal Daume Ill (me@hal3.name) SP2IRL @ ACL2010
Combinatorial optimization problem
-• Efficient only in very limiting cases
-• Solved by heuristic search: beam + A&amp;quot; + local search
Hal Daume Ill (me@hal3.name) SP2IRL @ ACL2010
Incremental parsing, early 90s style
Train a classifier Right
to make decisions
VP
Left
Right
NP NP
Unary Left Up Right
Left
I / Pro can / Md can / Vb a / Dt can / Nn
Incremental parsing, mid 2000s style
can / Vb
can / Md
a / Dt
VP
NP
I / Pro
can / Nn
[17010V ‘)ImoH+susii00]
NP
.A
Train a classifier
to make decisions
39
a / Dt
can / Vb
Learning to beam-search
VP
40 Hal Daume Ill (me@hal3.name)
)&gt; For n=1..N:
ealv S Ctire
y = arg max, • ch ( x„ , k)
If 5,#3/;,
w= w— (xn, Si)
+ (xn, yn)
0
0
$11
0
4=.
NP
I / Pro
can / Md
can / Nn
SP2IRL @ ACL2010
41
SP2IRL @ ACL2010
Hal Daume III (me@hal3.name)
Learning to beam-search
For n=1..N:
• Run beam search until
truth falls out of beam
)• Update weights
immediately!
a / Dt
0
7
(0
0
VP
Learning to beam-search
NP
)&gt; For n=1..N:
• Run beam search until
truth falls out of beam
• Update weights
immediately!
• Restart at truth
VP
I / Pro
can / Md
can / Vb
can / Nn
NP
a / Dt
42 Hal ft., SP2IRL @ A,1-6,16
Incremental parsing results
• -0 - No early update, no repeated use of examples
-0- Early update, no repeated use of examples
-10- Early update, repeated use of examples
88
87
83
821 2 3 4 5
Number of passes over training data
43 Hal Daume III (me@hal3.name)
SP2IRL @ ACL20
86
85
o_
a) 84a
Online Learning Framework (LaS0)
Formulation
Generic Search
`ie-Fnx !gcnvuoi ‘noien+al
---Monotonicity: for any
node, we can tell if it
can lead to the correct
solution or not
Search Problem:
&gt; Search space
&gt; Operators
&gt; Goal-test function
&gt; Path-cost function
Search Variable:
Enqueue function
Varying the Enqueue
function can give us DFS,
BFS, beam search, A*
search, etc...
44
nodes :=
MakeQueue(S0)
while nodes is not empty
-• node :=
RemoveFront(nodes)
if node is a goal state
return node
-• next := Operators(node)
-• nodes := Enqueue(nodes,
next)
fail
SP2IRL @ ACL2010
• nodes := MakeQueue(S0)
• while nodes is not empty
&gt; node := RemoveFront(nodes)
if none of {node} u nodes is y-good or nose is a goal &amp; not y-
good
Where should we have gon-__?
• sibs := siblings(node, y
• w := update(w, x, sibs, {node} u nodes)
• nodes := MakeQueue(sibs)
else Update our weights based on
- if node is a goa Continue search... the good and the bad choices
- next := Operators(node)
- nodes := Enqueue(nodes, next)
Hal Daume Ill (me@hal3.name) SP2IRL @ ACL2010
&apos;An &apos;noievud-al
If we erred...
Hal Daume Ill (me@hal3.name)
Search-based Margin
The margin is the amount by which we are correct:
Note that the margin and hence linear separability is
also a function of the search algorithm!
Hal Daume III (me@hal3.name) SP2IRL @ AGL2010
[D+Marcu, ICML05; Xu+al, JMLR09]
Syntactic chunking Results
D4 C Large Margin (Exact) 20J02
Large Margin Serni-CRF
Penran
LaS0p-1
LaS0p-5
LaS0p-25
o LaS0p-exam
LSO-1
LSO-5
LSO-25
LaS0a-exam
(bea n25)
24 min
0
*
\
Large Margin Pen:Wain
ibe.rn Search (Exam)
Seat
prior
4 min maul=
[Zhang+Damerau+Johnson
935 . 2002]; timing unknown
8
.9enderd
Perception 22 min
LL
Upcloes
[Collins 2002]
93
33 min
Serni -CRF model
925
\\&apos;‘
920
503 1000 1530 2300 2500 3000 3500
Training Time (minutes)
47 Hal Daume III (me@hal3.name) SP2IRL @ AGL20
[D+Marcu, ICML05; Xu+al, JMLR09]
Tagging+chunking results Variations on a beam
Joint tagging/chunking accuracy
WS -
as -
95.5 -
95
94 5 -
`1B+nx :soivuoi ‘noievu+al
Sir-on model
[Sutton+McCallum 2004]
a Simon
LaS0p-1
o LaS0p-5
LaS0p-10
LaS0p-25
o • LaS0p-5)
LaS0a- 1
La502-5
• LaS0a-10
LaS0a-25
LaS0a-50
100
SP2IRL @ ACL2010
</figure>
<table confidence="0.843148846153846">
A- Observation:
A- We needn&apos;t use the same beam size for
training and decoding
&apos;A- Varying these values independently yields:
1 1 5 Decoding 50
.c E 5 93.9 92.8 Beam 90.9
E as 10 90.5 94.3 10 25 94.1
(1) 89.5 94.3 91.9 91.3 94.2
co 25 88.7 94.2 94.4 94.1 94.3
H 50 88.4 94.2 94.4 94.2 94.4
94.5 94.3
94.4 94.2
Hal Daume Ill (me@hal3.name) SP2IRL @ ACL2010
</table>
<figure confidence="0.89562096226415">
23 min
Laige igiri
(beam 23,5o)
Large Margin
(beam 10]
3 min 7 min
1 min
10&apos; 10=
Training Time (hours) [log scale]
Daume III (me@ha13.name)
`IP+nX !SO1VVOI nomini+Cl]
What if our model sucks?
Sometimes our model cannot produce the &amp;quot;correct&amp;quot;
output
canonical example: machine translation
-told&amp;quot; update
Current
Hypothesis
Best
achievable
output
0
r-
Si
7
0
SP2IRL @ AGL2010
Mô.eI
Outp
local&amp;quot; update
Hal Demme III (me@hal3.name)
Reference
300d TN-best list
utputs or &amp;quot;optimal
decoding&amp;quot;
or ...
Local versus bold updating...
Machine Translation Performance (Bleu)
35.5
Monotonic Distortion
Hal Daume III (me@hal3.name) SP2IRL @ ACL2010
• Bold
• Local
Pharoah
35
34.5
341
33.5
33
32.5
Take-home messages If not, this can be
a really bad idea!
[Kulesza+Pereira, NIPS07]
</figure>
<bodyText confidence="0.9909565">
&gt;- If you can predict (ie., solve argmax) you can learn (use
structured perceptron)
&gt;- If you can do loss-augmented search, you can do max
margin (add two lines of code to perceptron)
&gt;- If you can do beam search, you can learn using LaS0
(with no loss function)
&gt;- If you can do beam search, you can learn using Seamn
(with any loss function)
</bodyText>
<figure confidence="0.76252225">
Coffee Break!!!
Hal Daume III (me@hal3.name) SP2IRL @ ACL2010
Reinforcement learning
Basic idea:
</figure>
<listItem confidence="0.9014695">
-• Receive feedback in the form of rewards
• Agent&apos;s utility is defined by the reward function
• Must learn to act to maximize expected rewards
• Change the rewards, change the learned behavior
Examples:
-• Playing a game, reward at the end for outcome
-• Vacuuming, reward for each piece of dirt picked up
-• Driving a taxi, reward for each passenger delivered
</listItem>
<figure confidence="0.885208409090909">
55 Hal Daume III (me@hal3.name) SP2IRL @ ACL2010
Refresher on
Reinforcement
Learning
54 Hal Daume Ill (me@hal3.name) SP2IRL @ ACL2010
Markov decision processes
What are the values (expected future rewards) of
states and actions?
0.6 „.
0
Q(s,a1)* = 30 —
.4
V(s)* = 30
&apos;1/4:CV
—4
— *cc:1j
—
*cc:g
Hal Daume Ill (me@hal3.name) SP2IRL @ ACL2010
Markov Decision Processes
An MDP is defined by:
A set of states s S
</figure>
<listItem confidence="0.9666813">
• A set of actions a BA
&gt; A transition function T(s,a,s&apos;)
&gt; Prob that a from s leads to s&apos;
&gt; i.e., P(s&apos; I s,a)
&gt; Also called the model
&gt; A reward function R(s, a, s&apos;)
&gt; Sometimes just R(s) or R(s&apos;)
&gt; A start state (or distribution)
&gt; Maybe a terminal state
• MDPs are a family of non-
</listItem>
<figure confidence="0.950489763157895">
deterministic search problems
)&gt; Total utility is one of:
rt or Eytr
Hal Daurne III (me@hal3.name)
Q(s,a2)* = 23
1.0
0.1
Q(s,a3)* = 1740
3
2
Eil
START
2 3 4
0.8
0.1 0.1
SP2 RL g ACL2010
Solving MDPs
&gt;- In deterministic single-agent search problem, want an
optimal plan, or sequence of actions, from start to a
goal
In an MDP, we want an optimal policy pt(s)
• A policy gives an action for each state
• Optimal policy maximizes expected if followed
• Defines a reflex agent
Optimal policy 2
when R(s, a, s&apos;) =
-0.04 for all non-
terminals s
1 2 3 4
Hal Daume III (me(gmariname) (E0 AUL2010
Example Optimal Policies
1=1
R(s) = -0.01
i i I=1
1=1
i &apos;EJ
4
R(s) = -0.03
</figure>
<page confidence="0.456125666666667">
1=1
A
59 R(s) = -0.4 Hal Daume III (me@hal3.nan, R(s) = -2.0
</page>
<table confidence="0.9236335">
Optimal Utilities
&gt;- Fundamental operation: compute
the optimal utilities of states s (all
at once)
Why? Optimal values define
optimal policies!
0812 0868 0912
Define the utility of a state s:
0.E60
V*(s) = expected return starting in
s and acting optimally
G705 0 655 0 611
2
Define the utility of a q-state (s,a):
Q*(s,a) = expected return starting
in s, taking action a and
thereafter acting optimally 2
Define the optimal policy:
Tr*(s) = optimal action from state s 2 3 4
Hal Daurne III (meghal3.name) SP2IRL @ ACL2010
</table>
<subsectionHeader confidence="0.807311">
The Bellman Equations
</subsectionHeader>
<bodyText confidence="0.832246111111111">
&gt;- Definition of utility leads to a simple
one-step lookahead relationship
amongst optimal utility values:
Optimal rewards = maximize over
first action and then follow optimal
policy
s,a,s
&apos;
As
</bodyText>
<equation confidence="0.923805722222222">
Formally:
V*(s) = mcax Q* (s, a)
Q* (s, a) = sa, s&apos;) [R(s, a, 5!) ± 717* (si)1
s&apos;
T(s, a, sl) [R(s, a, sl) &apos;y V* (sl)-
s&apos;
Hal Daume III (meghal3.name) SP2IRL g ACL2010
&gt;-
V*(s) = max
a
Solving MDPs / memoized recursion
&gt;- Recurrences:
11(s) =0
V*(s) = mcax (s, a)
(s, a) = E T(s, a, sl) [R(s, a, sl) -07,* (s&apos;)
s&apos;
7i(s) = arg max Os, a)
a
</equation>
<bodyText confidence="0.872119333333333">
Cache all function call results so you never repeat
work
What happened to the evaluation function?
</bodyText>
<page confidence="0.771828">
62 Hal Daurne Ill (me0hal3.name) SP2IRL g ACL2010
</page>
<sectionHeader confidence="0.51394" genericHeader="abstract">
Q-Value Iteration
</sectionHeader>
<bodyText confidence="0.9403455">
Value iteration: iterate approx optimal values
), Start with V0*(s) = 0, which we know is right (why?)
), Given V,*, calculate the values for all states for depth
i+1:
</bodyText>
<figure confidence="0.6307365">
14±1(8) max E T(s, a, sl) [R(s, a, 81) Ms/ )1
s&apos;
</figure>
<subsectionHeader confidence="0.47343">
But Q-values are more useful!
</subsectionHeader>
<bodyText confidence="0.859831">
&gt;, Start with Q0*(s,a) = 0, which we know is right (why?)
&gt;, Given Qi*, calculate the q-values for all q-states for
depth i+1:
</bodyText>
<equation confidence="0.340753">
Qi+i(s, a) E T(s, a, s&apos;) [R(s, a, sl) max Qi(s/ , al)]
s&apos;
</equation>
<page confidence="0.450632">
63 Hal Daume III (me0hal3.name) SP2IRL g ACL2010
</page>
<note confidence="0.331092">
RL = Unknown MDPs
</note>
<bodyText confidence="0.445706">
&gt;- If we knew the MDP (i.e., the reward function and
transition function):
&gt;- Value iteration leads to optimal values
Q-value iteration leads to optimal Q-values
&gt;- Will always converge to the truth
</bodyText>
<figure confidence="0.395324875">
Q-Learning
Learn Q*(s,a) values
-• Receive a sample (s,a,s&apos;,r)
-• Consider your old estimate: Q(s, a)
Consider your new sample estimate:
Q* (s, a) = E T(s, a, s&apos;)[R(s, a, s&apos;) -ymaxQ*(si , a!)]
&gt;- Reinforcement learning is what we do when we do not
know the MDP
&gt;- All we observe is a trajectory
s2,a2,r2, s3,a3,r3,
-• Incorporate the new estimate into a running average:
sample = R(s, a, si) -y max Q(s&apos;, a&apos;)
a&apos;
Q (s , a) &lt;— (1 — a)Q(s, a) ± (a) [sample]
Hal Daume III (me@hal3.name) SP2IRL g ACL2010
Exploration / Exploitation
</figure>
<table confidence="0.913898705882353">
&apos;A- Several schemes for forcing exploration
Simplest: random actions (E greedy)
-• Every time step, flip a coin
-• With probability 8, act randomly
-• With probability 1-8, act according to current policy
Problems with random actions?
-• You do explore the space, but keep thrashing around once
learning is done
-• One solution: lower F, over time
-• Another solution: exploration functions
Hal Daurne Ill (me@hal3.name) SP2IRL g ACL2010
Hal Daume III (meghal3.name) SP2IRL g ACL2010
Q-Learning
In realistic situations, we cannot possibly learn about
every single state!
-• Too many states to visit them all in training
-• Too many states to hold the q-tables in memory
</table>
<bodyText confidence="0.436804">
Instead, we want to generalize:
</bodyText>
<figure confidence="0.532293846153846">
-• Learn about some small number of training states from
experience
-• Generalize that experience to new, similar states:
Cgs, a) = wlfl(s, a)±w2h(s, a)±. .-Fwnfn(s, a)
Very simple stochastic updates:
Q(s, a) &lt;— Q(s, a) ± a [error]
wi &lt;— wi ± a [error] fi(s, a)
6 Hal Daume III (me@hal3.name) SP2IRL g ACL2010
Inverse RL: Task
Inverse
Reinforcement
Learning
(aka Inverse Optimal Control)
</figure>
<listItem confidence="0.643617523809524">
• Given:
measurements of an agent&apos;s behavior over time, in
a variety of circumstances
&gt;- if needed, measurements of the sensory inputs to
that agent
&gt;- if available, a model of the environment.
&gt;- Determine: the reward function being optimized
&gt;- Proposed by [Kalman68]
&gt;- First solution, by [Boyd 94]
Hal Daume III (me@hal3.name) SP2IRL g ACL2010 Hal Daume III (meghal3.name) SP2IRL g ACL2010
Why inverse RL?
• Computational models for animal learning
-• &amp;quot;In examining animal and human behavior we must consider
the reward function as an unknown to be ascertained
through empirical investigation.&amp;quot;
• Agent construction
-• &amp;quot;An agent designer [...] may only have a very rough idea of
the reward function whose optimization would generate
&apos;desirable behavior.&amp;quot;
eg., &amp;quot;Driving well&amp;quot;
• Multi-agent systems and mechanism design
</listItem>
<bodyText confidence="0.908347272727273">
-• learning opponents&apos; reward functions that guide their actions
to devise strategies against them
Hal Daurne Ill (me@hal3.name) SP2IRL g ACL2010
IRL from Sample Traject Warning: need to be -
careful to avoid
Optimal policy available through trivial solutions!
(eg., driving a car)
&gt;- Want to find Reward function that makes this policy look
as good as possible
&gt;- Write Rw(s)=w ch(s) so the reward is linear
and VL(s0) be the value of the starting state
</bodyText>
<equation confidence="0.888459">
illwax f (v: (so)—v:k(so)
k = 1
</equation>
<bodyText confidence="0.55912225">
How good does the
some other policy look?
How good does the
&amp;quot;optimal policy&amp;quot; look?
</bodyText>
<table confidence="0.801750833333333">
[0011/101 Iiessna-F61\1]
Apprenticeship Learning via IRL
&gt;- For t = 1,2,...
Inverse RL step:
Estimate expert&apos;s reward function R(s)= wT (s) such
that under R(s) the expert performs better than all
previously found policies {;}.
RL step:
Compute optimal policy; for the estimated reward w
CD0
T
Car Driving Experiment
</table>
<listItem confidence="0.800451625">
• No explicit reward function at all!
• Expert demonstrates proper policy via 2 min. of driving
time on simulator (1200 data points).
• 5 different &amp;quot;driver types&amp;quot; tried.
• Features: which lane the car is in, distance to closest
car in current lane.
• Algorithm run for 30 iterations, policy hand-picked.
• Movie Time! (Expert left, IRL right)
</listItem>
<figure confidence="0.989232555555556">
0
()
0
Hal Daume III (meghal3.name) SP2IRL g ACL2010
73
Hal Daume III (me0hal3.name)
SP2IRL g ACL2010
&amp;quot;Nice&amp;quot; driver
Hal Daume Ill (me0hal3.name) SP2IRL ACL2010
</figure>
<page confidence="0.758165">
75
</page>
<table confidence="0.87773344117647">
&amp;quot;Evil&amp;quot; driver
Hal Daume III (me0hal3.name) SP2IRL @ ACL2010
76
82 Hal Daum4 III (me@hal3.name)
~ For n=1..N:
~ Augmented planning:
Run A* on current (augmented) c
to get q-state visitation frequenci
Optimizing MMP
~ Update:
~ Shrink:
M3N Objective
SOME
MATH
w=�1— ��1 w
w=w+Es
a
[N(s,a)—E
SP2IRL @ ACL2010
N(s,a)
[Ratliff+al, NIPS05]
83 Hal Daum4 III (me@hal3.name)
Maximum margin planning movies
SP2IRL @ ACL2010
[Ratliff+al, NIPS05]
Parsing via inverse optimal control
&gt;- State space = all partial parse trees over the full
sentence labeled &amp;quot;S&amp;quot;
&gt;- Actions: take a partial parse and split it anywhere in the
middle
&gt;- Transitions: obvious
&gt;- Terminal states: when there are no actions left
&gt;- Reward: parse score at completion
Parsing via inverse optimal control
</table>
<page confidence="0.575737285714286">
90
85
80
75
70
65
60
</page>
<figure confidence="0.990842460869565">
Small Medium Large
• Maximum • Projection Perceptron • Appren-
Likelihood ticeship
Learning
• Maximum • Maximum Policy
Margin Entropy Matching
Hal Dare III (me@hal3.name)
SP2IRL @ ACL2010
Hal Daume III (me@hal3.name) SP2IRL @ ACL2010
`penadazs+neN]
0
lienaclazg+naN]
0
35
Apprenticeship
Learning
Hal Dare Ill (me@hal3.name) SP2IRL @ ACL2010
Integrating search and learning
Input: Le homme mange l croissant.
Output: The man ate a croissant.
Hyp: The man ate
Coy:
I &apos; croissant.
Classifier &apos;h&apos;
Hyp: The man ate a croissant
Coy:
Hyp: The man ate a fox
Coy:
croissant.
Hyp: The man ate happy
Coy:
Hyp: The man ate a
Coy:
croissant.
Hyp: The man ate a
Coy:
SP2IRL @ ACL20 IL
1D+Marcu, ICML05; D+Lanciford+Marcu, MLJ091
Average
multiclass
classification
loss
SP2IRL g ACL2010
Hal Daume III (me@hal3.name)
firigtetnistienPaigrgiscitstgred
even in1994,12 years
after its defeat in the 74-day
war with Britain. The counhys
CadrirtiantO ftClatenrirgiggY
ain
sovereignty over the isiands.
-• Lay sentences out sequentially
-• Generate a dependency parse of each
sentence
-• Mark each root as a frontier node
-• Repeat:
• Choose a frontier node node to add to the
summary
• Add all its children to the frontier
• Finish when we have enough words
(1 0
00 0 0C)c3=
LP,
rA)
0 0 0 0 0 0 0
a
0
CD
S3 S4
S5
Sn
Reducing search to classification
&gt;- Natural chicken and egg problem:
-• Want h to get low expected future loss
... on future decisions made by h
... and starting from states visited by h
Iterative solution
Theoretical results
Theorem: After 2T3 In T iterations,
the loss of the learned policy
is bounded as follows:
ID+Lanaford+Marcu. ML1091
Input: Le homme mange l croissant.
Output: The man ate a croissant.
Hyp: The man ate a croissant LOSS = 0
Coy:
Hyp: The man ate
Coy:
I &apos; croissant.
—10.-
Loss = 1.8
a
Loss = 1.2
= 0.5
Loss
h(t) Hyp: The man ate a &apos;lox
Coy:
croissant.
Hyp: The man ate happy
Coy:
Hyp: The man ate a Hyp: The man ate a
Coy: Coy:
croissant.
Loss = 0
Hal Daume III (me@hal3.name) SP2IRL @ ACL2010
L(h) L(ho) + 2T1nTcg + (1+1nT)CTmax
h(t-1)
0
CD
[60191A1 &apos;nomw+paol6uel-FG]
Loss of the
optimal policy
Norst case
per-step
loss
</figure>
<figureCaption confidence="0.760859">
Example task: summarization
</figureCaption>
<bodyText confidence="0.953470666666667">
Standard approach is
sentence extraction, but that
is often deemed to &amp;quot;coarse&amp;quot;
to produce
good, very short summaries.
We wish to also drop words
and phrases =&gt; document
compression
Hal Daume III (me@hal3.name) SP2IRL g ACL2010
</bodyText>
<page confidence="0.721521">
90
</page>
<bodyText confidence="0.9345423">
Argentina was still obsessed
with the Falkland Islands
even in 1994, 12 years
after its defeat in the 74-day
war with Britain. The country&apos;s
overriding foreign policy aim
continued to be winning
sovereignty over the islands.
That&apos;s perfect!
Structure of search
</bodyText>
<figure confidence="0.959899174311927">
SP2IRL g ACL2010
000 0
(.)
0 0 0 0 0
\ /
Sio
S2
0 = frontier node 41= summary node
The Falkland islands
war, in 1982, was
fought between
Britain and Argentina
93 Hal Daumé III (me@hal3.name)
Example output (40 word limit)
6 Diplomatic ties restored
5 M
5 Exchanges were in 1992 2 War was 74-days long
3 War between Britain and Argentina
Argentina and Britain announced an agreement, nearly
eight years after they fought a 74-day war a populated
+13 archipelago off Argentina&apos;s coast. Argentina gets out
the red carpet, official royal visitor since the end of the
Falklands war in 1982.
Vine Growth (Searn):
Argentina and Britain announced to restore full ties,
eight years after they fought a 74-day war over the
+24 Falkland islands. Britain invited Argentina&apos;s minister
Cavallo to London in 1992 in the first official visit since
the Falklands war in 1982.
Sentence Extraction + Compression:
ajor cabinet member visits
3 Falkland war was in 1982
3 Cavallo visited UK
SP2IRL @ ACL2010
[D+Langford+Marcu, MLJ09]
94 Hal Daumé III (me@hal3.name)
Learning to Drive
Camera Image
Input: Output:
Hard left turn Hard right turn
Steering in [-1,1]
SP2IRL @ ACL2010
95 Hal Daumé III (me@hal3.name)
DAgger: Dataset Aggregation
Collect trajectories with expert rr*
SP2IRL @ ACL2010
96 Hal Daumé III (me@hal3.name)
Theoretical Guarantees
Best policy rr in sequence rr[1:N] guarantees:
Avg. Loss on
Aggregate Dataset
Avg. Regret of Tr[1:N]
J(;r) &lt;_ T(EN + YN ) + O(T / N)
SP2IRL @ ACL2010
Iterations
of DAgger
96
97 Hal Daumé III (me@hal3.name)
99 Hal Daumé III (me@hal3.name)
Super Mario Brothers
Experiments: Racing Game
Resized to 25x19
pixels (1425
features)
Extracted 27K+ binary features from last 4
observations
(14 binary features for every cell)
From Mario AI competition 2009
Input:
Output:
Input:
Steering in [-1,1]
Jump in {0,1}
Right in {0,1}
Left in {0,1}
Speed in {0,1}
Output:
SP2IRL @ ACL2010
SP2IRL @ ACL2010
3200
3000
2800
M2600
2 3 4 5 6 7 8
Number of Training Data
Average Distance per Stage
2000
2 1800
1600
1400
1200
10000
9 10
C 104
Perceptron vs. LaS0 vs. Seamn
Incremental perceptron
LaS0
Seamn
Un-learnable decision
Hal Daume III (meghal3.name)
SP2IRL g ACL2010
g ACL2010
..(2, 2400
2 2200
—9—DAgger
- Searn(1)
- Searn(0.4)
---SMILe(0.1)
• • Supervised
</figure>
<sectionHeader confidence="0.824141" genericHeader="keywords">
Discussion
</sectionHeader>
<bodyText confidence="0.281992">
Hal Dame III (me@hal3.name) SP2IRL g ACL2010
</bodyText>
<subsectionHeader confidence="0.690327">
Relationship between SP and IRL
</subsectionHeader>
<bodyText confidence="0.9785312">
Formally, they&apos;re (nearly) the same problem
-• See humans performing some task
-• Define some loss function
-• Try to mimic the humans
Difference is in philosophy:
(I)RL has little notion of beam search or dynamic
programming
-• SP doesn&apos;t think about separating reward estimation from
solving the prediction problem
(I)RL has to deal with stochastiticity in MDPs
</bodyText>
<footnote confidence="0.289923">
104 Hal Daume III (me@hal3.name) SP2IRL g ACL2010
</footnote>
<subsectionHeader confidence="0.669454">
Important Concepts
</subsectionHeader>
<bodyText confidence="0.885458">
&gt;- Search and loss-augmented search for margin-based
methods
&gt;- Bold versus local updates for approximate search
&gt;- Training on-path versus off-path
</bodyText>
<listItem confidence="0.819768">
&gt;- Stochastic versus deterministic worlds
• Q-states / values
&gt;- Learning reward functions vs. matching behavior
Hal&apos;s Wager
• Give me a structured prediction problem where:
</listItem>
<bodyText confidence="0.7265414">
-• Annotations are at the lexical level
-• Humans can do the annotation with reasonable agreement
-• You give me a few thousand labeled sentences
Then I can learn reasonably well...
- ...using one of the algorithms we talked about
</bodyText>
<figure confidence="0.903329387096774">
• Why do I say this?
-• Lots of positive experience
-• I&apos;m an optimist
-• I want your counter-examples!
Hal Daume III (me@hal3.name)
SP2IRL g ACL2010
Hal Daume III (me@hal3.name) SP2IRL @ ACL2010
Software
&gt;- Sequence labeling
-• Mallet http://mallet.cs.umass.edu
CRF++ http://crfpp.sourceforge.net
&gt;- Search-based structured prediction
• LaS0 http://hal3.name/TagChunk
• Seamn http://hal3.name/searn
Higher-level &amp;quot;feature template&amp;quot; approaches
-• Alchemy http://alchemy.cs.washington.edu
Factorie http://code.google.com/p/factorie
Hal Daume III (me@hal3.name) SP2IRL g ACL2010
Open problems
&gt;- How to do SP when argmax is intractable....
% Bad: simple algorithms diverge [Kulesza+Pereira, NIPS07]
% Good: some work well [Finley+Joachims, ICML08]
And you can make it fast! [Meshi+al, ICML10]
How to do SP with delayed feedback (credit assignment)
Kinda just works sometimes [D, ICML09; Chang+al, ICML10]
% Generic RL also works [Branavan+al, ACL09; Liang+al, ACL09]
What role does structure actually play?
-• Little: only constraints outputs [Punyakanok+al, IJCA105]
-• Little: only introduces non-linearities [Liang+al, ICML08]
-• Lots: ???
Hal Daurne Ill (me@hal3.name) SP2IRL g ACL2010
</figure>
<sectionHeader confidence="0.865937" genericHeader="introduction">
Summary
</sectionHeader>
<bodyText confidence="0.813644">
- Structured prediction is easy if you can do argmax
search (esp. loss-augmented!)
</bodyText>
<listItem confidence="0.887854">
• Label-bias can kill you, so iterate (Seam)
• Stochastic worlds modeled by MDPs
• IRL is all about learning reward functions
IRL has fewer assumptions
- More general
Less likely to work on easy problems
• We&apos;re a long way from a complete solution
• Hal&apos;s wager: we can learn pretty much anything
</listItem>
<figure confidence="0.695261698113207">
Thanks! Questions?
References
See also:
http://www.cs.utah.eduhsuresh/mediawiki/index.php/MLRG
http://braque.cc/ShowChannel?handle=P5BVAC34
112
dal Daume III (me@hal3.name) SP2IRL @ AGL2010
Hal Daume Ill (me@hal3.name)
SP2IRL @ ACL2010
Stuff we talked about explicitly
Apprenticeship learning via inverse reinforcement learning, P. Abbeel and A. Ng. ICML, 2004.
Incremental parsing with the Perceptron algorithm. M Collins and B. Roark. ACL 2004.
Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. M.
Collins. EMNLP 2002.
Search-based Structured Prediction. H. Daume III, J. Langford and D. Marcu. Machine Learning, 2009.
Learning as Search Optimization: Approximate Large Margin Methods for Structured Prediction. H. Daume III and D.
Marcu. ICML, 2005.
An End-to-end Discriminative Approach to Machine Translation. P. Liang, A. Bouchard-COte, D. Klein, B. Taskar.
ACL 2006.
Statistical Decision-Tree Models for Parsing. D. Magerman. ACL 1995.
Training Parsers by Inverse Reinforcement Learning. G. Neu and Cs. Szepesvari. Machine Learning 77, 2009.
Algorithms for inverse reinforcement learning, A. Ng and A. Russell. ICML, 2000.
(Online) Subgradient Methods for Structured Prediction. N. Ratliff, J. Bagnell, and M Zinkevich. AlStats 2007.
Maximum margin planning. N. Ratliff, J. Bagnell and M. Zinkevich. ICML, 2006.
Learning to search: Functional gradient techniques for imitation learning. N. Ratliff, D. Silver, and J. Bagnell.
Autonomous Robots, Vol. 27, No. 1, July, 2009.
Reduction of Imitation Learning to No-Regret Online Learning. S. Ross, G. Gordon and J. Bagnell. AlStats 2011.
Max-Margin Markov Networks. B. Taskar, C. Guestrin, V. Chatalbashev and D. Koller. JMLR 2005.
Large Margin Methods for Structured and Interdependent Output Variables. I. Tsochantaridis, T. Joachims, T.
Hofmann, and Y. Altun. JMLR 2005.
Learning Linear Ranking Functions for Beam Search with Application to Planning. Y. Xu, A. Fern, and S. Yoon.
JMLR 2009.
Maximum Entropy Inverse Reinforcement Learning. B. Ziebad, A. Maas, J. Bagnell, and A. Dey. AAAI 2008.
113 Hal Daume Ill (me@hal3.name) SP2IRL g AGL2010
Other good stuff
e. Reinforcement learning for mapping instructions to actions. S.R.K. Branavan, H. Chen, L. Zettlemoyer and R.
Barzilay. ACL, 2009.
1- Driving semantic parsing from the world&apos;s response. J. Clarke, D. Goldwasser, M.-IN. Chang, D. Roth. CoNLL 2010.
r- New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron.
M.Collins and N. Duffy. ACL 2002.
1- Unsupervised Search-based Structured Prediction. H. Daume III. ICML 2009.
1- Training structural SVMs when exact inference is intractable. T. Finley and T. Joachims. ICML, 2008.
1- Structured learning will approximate inference. A. Kulesza and F. Pereira. NIPS, 2007.
• Condffional random fields: Probabilistic models for segmenting and labeling sequence data. J. Lafferty, A McCallum,
F. Pereira. ICML 2001.
1- Structure compilation: trading structure for features. P. Liang, H. Daume, D. Klein. ICML 2008.
1- Learning semantic correspondences will less supervision. P. Liang, M. Jordan and D. Klein. ACL, 2009.
1- Generalization Bounds and Consistency for Structured Labeling. D. McAllester. In Predicting Structured Data, 2007.
1- Maximum entropy Markov models for information extraction and segmentation. A McCallum, D. Freitag, F. Pereira.
ICML 2000.
• FACTORIE: Efficient Probabilistic Programming for Relational Factor Graphs via Imperative Declarations of Structure,
Inference and Learning. A. McCallum, K. Rohanemanesh, M. Wok, K. Schultz, S. Singh. NIPS Workshop on
Probabilistic Programming, 2008
</figure>
<footnote confidence="0.871121285714286">
1- Learning efficiently will approximate inference via dual losses. 0. Meshi, D. Sontag, T. Jaakkola, A. Globerson.
ICML 2010.
1- Learning and inference over constrained output. V. Punyakanok, D. Roth, W. Yih, D. Zimak. IJCAI, 2005.
1- Boosting Structured Prediction for Imitation Learning. N. Ratliff, D. Bradley, J. Bagnell, and J. Chestnutt. NIPS 2007.
1- Efficient Reductions for Imitation Learning. S. Ross and J. Bagnell. AISTATS, 2010.
• Kernel Dependency Estimation. J. Weston, a Chapelle, A. Elisseeff, B. Schoelkopf and V. Vapnik. NIPS 2002.
4 Hal Daume Ill (me@hal3.name) SP2IRL g ACL2010
</footnote>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.8146962">2 Ei @ ACL2010 5 Hal Daumé III (me@hal3.name) SP2IRL @ ACL2010 Structured prediction 101</note>
<title confidence="0.824835">Learn a function mapping inputs to complex outputs: Input Space Decoding Output Space</title>
<abstract confidence="0.9130314375">I : X can can a can Why is structure important? &gt;-Correlations among outputs -•Determiners often precede nouns -•Sentences usually have verbs • Global coherence -It just make sense have three determiners next to each other My objective (aka &amp;quot;loss function&amp;quot;) forces it -•Translations should have good sequences of words -•Summaries should be coherent</abstract>
<title confidence="0.952777538461538">Outline: Part I • What is Structured Prediction? • Refresher on Binary Classification • What does it mean to learn? • Linear models for classification • Batch versus stochastic optimization • From Perceptron to Structured Perceptron -•Linear models for Structured Prediction -•The &amp;quot;argmax&amp;quot; problem -•From Perceptron to margins • Learning to Search -•Stacking -•Incremental Parsing</title>
<note confidence="0.805705">SP2IRLgACL2010 gACL2010 Hal Daume Ill (me@hal3.name) DaumeIII(meghal3.name)</note>
<title confidence="0.6612105">Refresher on Binary Classification</title>
<note confidence="0.635013333333333">DaumeIII(me@hal3.name) SP2IRLgACL2010 Outline: Part ll Refresher on Reinforcement Learning Markov Decision Processes Q learning SP2IRLgACL2010</note>
<title confidence="0.974671">Inverse Reinforcement Learning -•Determining rewards given policies -•Maximum margin planning Apprenticeship Learning Seamn -•Dagger • Discussion</title>
<author confidence="0.697843">Hal Daurne Ill</author>
<abstract confidence="0.909371">What does it mean to learn? Informally: -to predict the future based on the past Slightly-less-informally: -to take examples construct a function that will label them as a human would</abstract>
<title confidence="0.807042222222222">Formally: -•Given: -•A fixed unknown distribution D over X*Y -•A loss function over Y&amp;quot;Y -•A finite sample of (x,y) pairs drawn i.i.d. from D -•Construct a function f that has low expected loss with respect to D Daume III (meghal3.name) SP2IRL gACL2010 Feature extractors</title>
<abstract confidence="0.833868777777778">gt;.A feature extractor (I) maps examples to vectors Dear Sir. W=dear : 1 First, I must solicit W=sir 1 your confidence in this transaction, this is by virture of its nature as being utterly confidencial W=this : 2 and top secret. ... W=wish : 0 MISSPELLED : 2 NAMELESS : 1 ALL CAPS : 0 NUM URLS : 0 &gt;- Feature vectors in NLP are frequently sparse Hal Daume III (meghal3.name) SP2IRLgACL2010 Linear models for binary classification Decision boundary is the set of &amp;quot;uncertain&amp;quot;points &gt;-Linear decision boundaries are characterized by weight vectors &amp;quot;free money&amp;quot; 0(x) l(x) BIAS free money the : : : : 1 BIAS : -3 4 2 0 1 free : 1 money : the : Daume Ill (me@hal3.name) gACL2010 The perceptron Inputs =feature values Params =weights Sum is theresponse (1) 1:1) &gt;0? - If the response is: - Positive, output +1 Negative, output -1 When training, update on errors: w=w+ yoh(x) Call bocy or Soma &amp;quot;Error&amp;quot; when: yW•ch(X)0 13 ir0 Why does that update work? ld &gt;-When •cMx)-0 updateznew= ± min s.t. , Vn For n=1..N: w=w+Yncti(x„) Support vector machines Explicitly optimize themargin &gt;-Enforce that all training points are correctly classified Daurne III (meghal3.name) SP2IRL all points correctly classified , , Vn Y &apos;104(x) Y YY &lt;0&gt; Daurne Ill(meghal3.nan SP2IRL gACL2010 vector machines with Explicitly optimize themargin Allow some &amp;quot;noisy&amp;quot; points to be misclassified yo47.4)(x)+ &gt;1 , Vn , Vn Daurne Ill (me@hal3.name) SP2IRL gACL2010 Batch versus stochastic optimization &gt;-Batch = read in all the data, then process it &gt;-Stochastic = (roughly) process a bit at a time</abstract>
<note confidence="0.9503225">Daume III (me@hal3.name) SP2IRLg Daume III (meghal3.name) SP2IRL gACL2010 Daume III (me@hal3.name) SP2IRL gACL2010 w2</note>
<title confidence="0.7725248">Stochastically optimized SVMs SVM Objective SOME MATH Implementation Note:</title>
<abstract confidence="0.868778">shrinkage is Implement it lazily, at the cost of double storage.</abstract>
<note confidence="0.86500025">From Perceptron to Structured Perceptron For n=1..N: If + )w CN For n=1..N:</note>
<title confidence="0.5214154">w=w+ Y nch(xn) Perceptron with multiple classes &gt;-Store separate weight vector for each class • • • WK For n=1..N: • Predict: If Y Y =W</title>
<author confidence="0.585417">Yn Yn</author>
<abstract confidence="0.537666333333333">I Why does this do the right thing? Hal Daurne Ill (me@hal3.name) gACL2010 biggest biggest</abstract>
<note confidence="0.967436333333333">Perceptron with multiple classes v2 Originally: For n=1..N:</note>
<title confidence="0.785065666666667">Predict: Y Y +</title>
<author confidence="0.642215">Yn Yn</author>
<note confidence="0.681191">For n=1..N: Predict: 5&gt; , Si) Daume III (me@hal3.name) SP2IRL gACL2010 Md Vb Dt Nn Pro</note>
<abstract confidence="0.860077576923077">can can a can SP2IRL @ ACL2010 Hal Daume III (me@hal3.name) @ACL2010 24 Hal Daume Ill (me@hal3.name) wql Pro] Md] w.[I_Vb] w[can_Pro] /t Md] w[Pro-Vb] w[can \Mb] can can (plus some work to account for boundary conditions) la! Daume Ill (me@hal3.name) @ACL2010 Features for structured prediction -Allowed to encode want -Output features,Markov features,other features Argmax for sequences )•If we only have output and Markov features, we can use</abstract>
<note confidence="0.729369142857143">Viterbi algorithm: Structured perceptron Enumeration Enumeration over 1..K over all outputs For n=1..N: )•Predict: = max kw • ch (x,,,k) = arg • ch( x,,, k) • If • IfS/yri w = w —(x , ) For n=1..N: • Predict: Structured perceptron as ranking -For n=1..N:</note>
<title confidence="0.884102777777778">Run Viterbi: arg • ch(x,,,k) -When does this make an update? Pro Md Vb Dt Nn Pro Md Md Dt Vb Pro Md Md Dt Nn Pro Md Nn Dt Md Pro Md Nn Dt Nn Pro Md Vb Dt Md Pro Md Vb Dt Vb</title>
<abstract confidence="0.895742857142857">can can a can Daume III (me@hal3.name) I Pro : 1 : 1 has verb : 1 can Md : 1 : 1 has nn lft : 0 canVb : 1 Md-Vb : 1 has n lft : 1 a Dt : 1 Vb-Dt : 1 has nn rgt : 1 can Nn : 1 : 1 has n rgt : 1</abstract>
<note confidence="0.922617923076923">1 ... @ACL2010 From perceptron to margins .Y2 ) ( y ) 0 DareIll(meghal3.name) SP2IRL@ACL2010 28 S. t. is Ranking margins DareIll(me@hal3.8ar8e) SP2IRL@ ACL2010</note>
<title confidence="0.737249">Margin Errors</title>
<abstract confidence="0.931198142857143">n V Each point is correctly modulo k q W, Response for truth —w.ch(x,„51) +„1,en,j/y min cn a) rcn CA 0 n Each true output is more ranked, modulo cn Dare III (me@hal3.8ame) SP2IRL@ ACL2010 27 min S. t. Response for other -Some errors are worse than others...</abstract>
<note confidence="0.7394955">t) 0</note>
<title confidence="0.92186775">Margin Pro Md Vb Dt Nn Pro Md Md Dt Vb Pro Md Md Dt Nn Pro Md Nn Dt Md Pro Md Nn Dt Nn Pro Md Vb Dt Md Pro Md Vb Dt Vb</title>
<abstract confidence="0.707365696202532">can can a can From perceptron to margins xSI) — (x2,5&apos;) min 1 Response for truth CA 0 0 o_ Each true output is more ranked, modulo 01 cn J:1 0 01 Response for other Md Vb Pro Dt Nn can can a can Accounting for a loss function -Some errors are worse than others... prn Mn - Md Hal DaumeIll(me@hal3.8ame) argin f y,y) SP2IRL@ ACL2010 [Taskar+al, JMLR05; Tshochandaritis, JMLR05] Nn Nn nt nt Dt Daume III (me@hal3.name) SP2IRL @ACL2010 0( —0(x2S1) .P) 7(xi5&apos;) w•cp(x,„y,,)—w•cp(x„,SI)+„ &gt;1 31 r w•cf.(x,„y)—w•cp(x,„5/)+„ w.[Pro-Pro] 32 Hal Daub III (meghal3.name) SP2IRL@ACL2010 0, 11) — w•[can Pro] a) w [can_Vb] +1 w [can Pro] w•[can Md] w.[Pro-Vb] w[can tit] Md] are we assuming here? cc Augmented argmax for sequences -Add &amp;quot;loss&amp;quot; to each wrong Accounting for a loss function ca(xiyi) Learning to Search Hal Daume III @ACL2010 optimizing Markov</abstract>
<note confidence="0.849413818181818">gt; For n=1..N: Viterbi: k 33 max x ri, • If u &amp;quot;S&apos;&apos;;, s) Cl) if; @ACL2010 Hal Daume Ill (me@hal3.name) SOME MATH +1( y„,k) 1Augmented w=w For n=1..N: Viterbi: ifj&gt; • (/)( x, k) w=(1 SI)</note>
<abstract confidence="0.88288080952381">w CN is &gt;-Classic formulation of structured prediction:&gt;-Click to add an outline something we learn (x = make &amp;quot;good&amp;quot; x,y pairs score highly At test time: (x) = ,y) [610v &apos;ireaue6B1/1] Daume Ill (me@hal3.name) SP2IRL@ ACL2010 Combinatorial optimization problem -•Efficient only in very limiting cases -•Solved by heuristic search: beam + A&amp;quot; + local search Daume Ill (me@hal3.name) SP2IRL @ACL2010 Incremental parsing, early 90s style Train a classifier to make decisions Right VP Left Right NP NP Unary Left Up Right Left I / Pro can / Md can / Vb a / Dt can / Nn Incremental parsing, mid 2000s style can / Vb can / Md a / Dt VP NP I / Pro can / Nn [17010V ‘)ImoH+susii00] NP .A Train a classifier to make decisions 39 a / Dt can / Vb to beam-search VP</abstract>
<note confidence="0.84086555">Hal DaumeIll(me@hal3.name) )&gt;For n=1..N: S Ctire = arg max, • ch ( x„ , Si) 0 0 $11 0 4=. NP I / Pro can / Md can / Nn @ACL2010 41 SP2IRL@ACL2010 HalDaumeIII (me@hal3.name) to beam-search For n=1..N:</note>
<abstract confidence="0.6802645">Run beam search until truth falls out of beam )•Update weights immediately!</abstract>
<note confidence="0.821446777777778">a / Dt 0 7 (0 0 VP to beam-search NP )&gt;For n=1..N:</note>
<abstract confidence="0.716734257142857">Run beam search until truth falls out of beam • Update weights immediately! • Restart at truth VP I / Pro can / Md can / Vb can / Nn NP a / Dt 42 Hal ft., @A,1-6,16 Incremental parsing results • -0early update, no repeated use of examples 0update, no repeated use of examples 10- Earlyupdate, repeated of examples 88 87 83 2 3 4 5 Number of passes over training data 43Hal DaumeIII(me@hal3.name) SP2IRL@ACL20 86 85 o_ a) 84a Online Learning Framework (LaS0) Formulation Generic Search any node, we can tell if it can lead to the correct solution or not</abstract>
<note confidence="0.559668">Search Problem: &gt; Search space</note>
<title confidence="0.8539924">gt; Operators &gt; Goal-test function &gt; Path-cost function Search Variable: Enqueue function</title>
<abstract confidence="0.946385972222223">theEnqueue function can give us DFS, BFS, beam search, A* search, etc... 44 nodes := MakeQueue(S0) whilenodes is not empty -•node := RemoveFront(nodes) ifnode is a goal state returnnode -•next := Operators(node) -•nodes := Enqueue(nodes, next) fail @ACL2010 • nodes := MakeQueue(S0) while is not empty &gt;node := RemoveFront(nodes) of {node} u nodes is y-good is a goal &amp; not ygood should we have • sibs := siblings(node, y • w := update(w, x, sibs, {node} u nodes) • nodes := MakeQueue(sibs) else Update our weights based on the good and the bad choices -if is goa search...-next := Operators(node) -nodes := Enqueue(nodes, next) Hal Daume Ill (me@hal3.name) SP2IRL@ ACL2010 erred... Hal Daume Ill (me@hal3.name) Search-based Margin the amount by which we are correct: that the hence separability a function of the algorithm!</abstract>
<note confidence="0.9513175">Daume III (me@hal3.name) SP2IRL@ AGL2010 [D+Marcu, ICML05; Xu+al, JMLR09] Syntactic chunking Results D4 CLarge Margin (Exact) 20J02 Serni-CRF Penran LaS0p-1 LaS0p-5 LaS0p-25 Large Margin oLaS0p-exam LSO-1 LSO-5 LSO-25 LaS0a-exam (bea n25)</note>
<abstract confidence="0.815966142857143">24 min 0 * \ Large Margin Pen:Wain (Exam) Seat prior min [Zhang+Damerau+Johnson 935 .2002]; unknown 8 .9enderd min LL Upcloes [Collins 2002] 93 33 min Serni -CRF model 925 \\&apos;‘ 503 1000 1530 2300 2500 3000 3500 Training Time (minutes) 47 Hal Daume III (me@hal3.name) SP2IRL @ AGL20 ICML05; Xu+al,JMLR09] Tagging+chunking results Variations on a beam Joint tagging/chunking accuracy WS as - 95.5 - 95 94 5 - Sir-on model [Sutton+McCallum 2004] aSimon</abstract>
<note confidence="0.838338545454545">LaS0p-1 o LaS0p-5 LaS0p-10 LaS0p-25 • LaS0a- 1 La502-5 • LaS0a-10 LaS0a-25 LaS0a-50 SP2IRL @ ACL2010</note>
<abstract confidence="0.89898075">A-Observation: A-We needn&apos;t use the same beam size for training and decoding &apos;A-Varying these values independently yields:</abstract>
<note confidence="0.760961666666667">1 1 93.9 90.5 89.5 88.7 88.4 5 92.8 94.3 94.3 94.2 94.2 Decoding Beam 50 90.9 94.1 94.2 94.3 94.4 E 10 25 E as 10 91.9 91.3</note>
<phone confidence="0.801539">(1) 94.4 94.1</phone>
<address confidence="0.693954">94.4 94.2 H50 94.5 94.3</address>
<phone confidence="0.408281">94.4 94.2</phone>
<abstract confidence="0.725314705882353">Daume Ill (me@hal3.name) SP2IRL@ ACL2010 Laige igiri Large Margin (beam 10] 1 min 10&apos; 10= Training Time (hours) [log scale] Daume III (me@ha13.name) `IP+nX !SO1VVOI nomini+Cl] What if our model sucks? our model the &amp;quot;correct&amp;quot; output canonical example: machine translation update Hypothesis Best achievable output 0 Si 7 0 @AGL2010 Mô.eI Outp local&amp;quot; update Hal Demme III (me@hal3.name) Reference decoding&amp;quot; Local versus bold updating... Machine Translation Performance (Bleu) 35.5 Monotonic Distortion DaumeIII(me@hal3.name) SP2IRL@ ACL2010 • Bold • Local Pharoah 35 34.5 33.5 33 32.5 Take-home messages If not, this can be idea! [Kulesza+Pereira,NIPS07] &gt;-If you can predict (ie., solve argmax) you can learn(use structured perceptron) &gt;-If you can do loss-augmented search, you can do max margin(add two lines of code to perceptron) &gt;-If you can do beam search, you can learn using LaS0 (with no loss function) &gt;-If you can do beam search, you can learn using Seamn</abstract>
<note confidence="0.753643">(with any loss function) Coffee Break!!! Daume III (me@hal3.name) SP2IRL@ ACL2010</note>
<title confidence="0.9448805">Reinforcement learning Basic idea: -•Receive feedback in the form ofrewards • Agent&apos;s utility is defined by the reward function Must learn to act tomaximize expected rewards • Change the rewards, change the learned behavior Examples: -•Playing a game, reward at the end for outcome -•Vacuuming, reward for each piece of dirt picked up -•Driving a taxi, reward for each passenger delivered Hal DaumeIII(me@hal3.name) SP2IRL@ACL2010 Refresher on Reinforcement Learning</title>
<abstract confidence="0.6500798">Hal DaumeIll(me@hal3.name) SP2IRL@ ACL2010 Markov decision processes What are the values (expected future rewards) of states and actions? 0.6 „. 0 =30— .4 =30 &apos;1/4:CV —4 — *cc:1j — *cc:g Hal Daume Ill (me@hal3.name) SP2IRL @ ACL2010</abstract>
<title confidence="0.892677">Markov Decision Processes An MDP is defined by: Aset of states s S</title>
<abstract confidence="0.920089891025641">Aset of actions a &gt; Atransition function T(s,a,s&apos;) &gt;Prob that a from s leads to s&apos; i.e., P(s&apos; Also the model &gt; Areward function R(s, a, s&apos;) &gt; Sometimes just R(s) or R(s&apos;) &gt; Astart state(or distribution) Maybe aterminal state MDPs are a family of nondeterministic search problems )&gt;Total utility is one of: or Hal Daurne III (me@hal3.name) =23 1.0 0.1 = 3 2 Eil START 2 3 4 0.8 0.1 0.1 RL Solving MDPs &gt;-In deterministic single-agent search problem, want an optimalplan,or sequence of actions, from start to a goal an MDP, we want an optimalpolicy pt(s) • A policy gives an action for each state • Optimal policy maximizes expected if followed • Defines a reflex agent Optimal policy when R(s, a, s&apos;) = -0.04 for all non-terminals s 2 1 2 3 4 Daume III (me(gmariname) AUL2010 Example Optimal Policies 1=1 R(s) = -0.01 i i I=1 1=1 i &apos;EJ 4 R(s) = -0.03 1=1 A = -0.4 Daume III (me@hal3.nan, = -2.0 Optimal Utilities &gt;-Fundamental operation: compute the optimal utilities of states s (all at once) Why? Optimal values define optimal policies! 0812 0868 0912 Define the utility of a state s: 0.E60 V*(s) = expected return starting in s and acting optimally G705 0 655 0 611 2 Define the utility of a q-state (s,a): Q*(s,a) = expected return starting in s, taking action a and acting optimally2 Define the optimal policy: Tr*(s) = optimal action from state s 2 3 4 Hal Daurne III (meghal3.name) SP2IRL @ ACL2010 The Bellman Equations &gt;-Definition of utility leads to a simple one-step lookahead relationship amongst optimal utility values: Optimal rewards = maximize over first action and then follow optimal policy s,a,s &apos; As Formally: (s, a) (s, = s&apos;) 717* s&apos; a, [R(s, a, &apos;y V* s&apos; Daume III (meghal3.name) SP2IRL gACL2010 &gt;a Solving MDPs / memoized recursion &gt;-Recurrences: 11(s) =0 = a) a) = a, [R(s, a, -07,* s&apos; = max a) a Cache all function call results so you never repeat work What happened to the evaluation function? Daurne Ill (me0hal3.name) SP2IRL gACL2010 Q-Value Iteration Value iteration: iterate approx optimal ),Start with = 0, which we know is right (why?) ),Given V,*, calculate the values for all states for depth i+1: a, [R(s, a, )1 s&apos; But Q-values are more useful! &gt;,Start with = 0, which we know is right (why?) &gt;,Given calculate the q-values for all q-states for depth i+1: a) a, s&apos;) [R(s, a, , s&apos; Daume III (me0hal3.name) SP2IRL gACL2010 RL = Unknown MDPs &gt;-If we MDP (i.e., the reward function and transition function): &gt;-Value iteration leads to optimal values Q-value iteration leads to optimal Q-values &gt;-Will always converge to the truth Q-Learning Learn Q*(s,a) values -•Receive a sample(s,a,s&apos;,r) -•Consider your old estimate:Q(s, Consider your new sample estimate: a) a, s&apos;) , &gt;-Reinforcement learning is what we do when we not MDP &gt;-All we observe is a Incorporate the new estimate into a running average: = R(s, a, a&apos; (s , a) &lt;— (1—a)Q(s, a) ± (a) [sample] Daume III (me@hal3.name) SP2IRL gACL2010 Exploration / Exploitation &apos;A-Several schemes for forcing exploration Simplest: random actions (E greedy) -•Every time step, flip a coin -•With probability randomly -•With probability 1-8, act according to current policy Problems with random actions? -•You do explore the space, but keep thrashing around once learning is done -•One solution: lower time -•Another solution: exploration functions Daurne Ill (me@hal3.name) SP2IRLg Daume III (meghal3.name) SP2IRLgACL2010 Q-Learning In realistic situations, we cannot possibly learn about every single state! -•Too many states to visit them all in training -•Too many states to hold the q-tables in memory Instead, we want to generalize: -•Learn about some small number of training states from experience -•Generalize that experience to new, similar states: = a)±. Very simple stochastic updates: Q(s, a) &lt;— Q(s, a) ± a [error]</abstract>
<note confidence="0.4418245">wi &lt;— wi ± a [error] fi(s, a) DaumeIII(me@hal3.name) SP2IRLgACL2010</note>
<title confidence="0.9087988">Inverse RL: Task Inverse Reinforcement Learning (aka Inverse Optimal Control)</title>
<author confidence="0.620417">Given</author>
<abstract confidence="0.918708833333333">measurements of an agent&apos;s behavior over time, in a variety of circumstances &gt;-if needed, measurements of the sensory inputs to that agent &gt;-if available, a model of the environment. &gt;-Determine: the reward function being optimized</abstract>
<note confidence="0.916474">gt;-Proposed by [Kalman68] &gt;-First solution, by [Boyd 94] Daume III (me@hal3.name) SP2IRL gACL2010 DaumeIII(meghal3.name) SP2IRLgACL2010</note>
<title confidence="0.468487">Why inverse RL? • Computational models for animal learning</title>
<abstract confidence="0.989353837209302">amp;quot;In examining animal and human behavior we must consider the reward function as an unknown to be ascertained through empirical investigation.&amp;quot; • Agent construction -•&amp;quot;An agent designer [...] may only have a very rough idea of the reward function whose optimization would generate &apos;desirable behavior.&amp;quot; eg., &amp;quot;Driving well&amp;quot; • systems and mechanism design -•learning opponents&apos; reward functions that guide their actions to devise strategies against them Daurne Ill (me@hal3.name) SP2IRLgACL2010 from Sample TrajectWarning: to becareful to avoid policy available through (eg., driving a car) &gt;-Want to find that makes this policy look as good as possible &gt;-Write so the reward is linear be the value of the starting state f (v: (so)—v:k(so) k = 1 How good does some other policy look? How good does the &amp;quot;optimal policy&amp;quot; look? [0011/101 Iiessna-F61\1] Apprenticeship Learning via IRL &gt;-For t = 1,2,... Inverse RL step: Estimate expert&apos;s reward function R(s)= wT (s) such that under R(s) the expert performs better than all previously found policies {;}. RL step: Compute optimal policy; for the estimated reward w T Car Driving Experiment • No explicit reward function at all! • Expert demonstrates proper policy via 2 min. of driving time on simulator (1200 data points). • 5 different &amp;quot;driver types&amp;quot; tried. •Features: which lane the car is in, distance to closest car in current lane.</abstract>
<note confidence="0.9581291">Algorithm run for 30 iterations, policy hand-picked. • Movie Time! (Expert left, IRL right) 0 () 0 Daume III (meghal3.name) SP2IRLg 73 Hal Daume III (me0hal3.name) gACL2010 &amp;quot;Nice&amp;quot; driver Hal Daume Ill (me0hal3.name) SP2IRL ACL2010 75 &amp;quot;Evil&amp;quot; driver Hal Daume III (me0hal3.name) SP2IRL @ ACL2010 76 82 Hal Daum4 III (me@hal3.name) ~For n=1..N: ~Augmented planning: Run A* on current (augmented) c to get q-state visitation frequenci</note>
<title confidence="0.98299475">Optimizing MMP ~Update: ~Shrink: Objective</title>
<abstract confidence="0.888519944444444">SOME MATH ��1w a SP2IRL @ ACL2010 [Ratliff+al, NIPS05] 83 Hal Daum4 III (me@hal3.name) Maximum margin planning movies SP2IRL @ ACL2010 [Ratliff+al, NIPS05] Parsing via inverse optimal control &gt;-State space = all partial parse trees over the full sentence labeled &amp;quot;S&amp;quot; &gt;-Actions: take a partial parse and split it anywhere in the middle &gt;-Transitions: obvious &gt;-Terminal states: when there are no actions left &gt;-Reward: parse score at completion</abstract>
<note confidence="0.814703">Parsing via inverse optimal control 90 85 80 75 70 65 60</note>
<title confidence="0.963845">Small Medium Large • Maximum•Projection Perceptron•Appren- Likelihood ticeship Learning • Maximum•Maximum Policy Margin Entropy Matching</title>
<note confidence="0.86011075">Hal Dare III (me@hal3.name) SP2IRL@ ACL2010 Hal Daume III (me@hal3.name) SP2IRL @ ACL2010 `penadazs+neN] 0 lienaclazg+naN] 0 35</note>
<title confidence="0.68376">Apprenticeship Learning</title>
<address confidence="0.361382">Dare Ill (me@hal3.name) SP2IRL @ACL2010</address>
<abstract confidence="0.903376666666667">Integrating search and learning homme mange l croissant. man ate a croissant.</abstract>
<note confidence="0.940567611111111">Hyp: The man ate Coy: I &apos; croissant. Classifier &apos;h&apos; Hyp: The man ate a croissant Coy: Hyp: The man ate a fox Coy: croissant. Hyp: The man ate happy Coy: Hyp: The man ate a Coy: croissant. Hyp: The man ate a Coy: @ACL20 IL 1D+Marcu, ICML05; D+Lanciford+Marcu, MLJ091</note>
<abstract confidence="0.925181783783784">loss Hal Daume III (me@hal3.name) firigtetnistienPaigrgiscitstgred its in the 74-day war with Britain. The counhys ain sovereignty over the isiands. -•Lay sentences out sequentially -•Generate a dependency parse of each sentence -•Mark each root as a frontier node -•Repeat: • Choose a frontier node node to add to the summary • Add all its children to the frontier • Finish when we have enough words (1 0 0 0 0 0 0 0 0 a 0 CD S3 S4 S5 Sn Reducing search to classification &gt;-Natural chicken and egg problem: -•Want get low expected future loss on future decisions made by starting from states visited by Iterative solution Theoretical results Theorem:After In T iterations, the loss of the learned policy is bounded as follows: ID+Lanaford+Marcu. ML1091 Input: Le homme mange l croissant. Output: The man ate a croissant.</abstract>
<note confidence="0.850002588235294">Hyp: The man ate a croissant Coy: Hyp: The man ate Coy: I &apos; croissant. Loss = 1.8 a Loss = 1.2 = 0.5 Loss h(t) Hyp: The man ate a &apos;lox Coy: croissant. Hyp: The man ate happy Coy: Hyp: The man ate a Coy: Hyp: The man ate a Coy: croissant. Loss = 0 Daume III (me@hal3.name) SP2IRL @ACL2010 + 0</note>
<abstract confidence="0.919901983870968">CD [60191A1 &apos;nomw+paol6uel-FG] Loss of the optimal policy Norst loss Example task: summarization Standard approach is sentence extraction, but that is often deemed to &amp;quot;coarse&amp;quot; to produce good, very short summaries. We wish to also drop words and phrases =&gt; document compression DaumeIII(me@hal3.name) SP2IRLgACL2010 90 Argentina was still obsessed with the Falkland Islands even in 1994, 12 years after its defeat in the 74-day war with Britain. The country&apos;s overriding foreign policy aim continued to be winning sovereignty over the islands. That&apos;s perfect! Structure of search SP2IRLgACL2010 0 (.) 0 \ / S2 = frontier node41=summary node The Falkland islands war, in 1982, was fought between Britain and Argentina 93 Hal Daumé III (me@hal3.name) Example output (40 word limit) 6 Diplomatic ties restored 5 M 5 Exchanges were in 1992 2 War was 74-days long 3 War between Britain and Argentina Argentina and Britain announced an agreement, eight years after they fought a 74-day war a populated off Argentina&apos;s coast. Argentina gets out the red carpet, official royal visitor since the end of the Falklands war in 1982. Vine Growth (Searn): Argentina and Britain announced to restore full ties, eight years after they fought a 74-day war over the islands. Britain invited Argentina&apos;s minister Cavallo to London in 1992 in the first official visit since the Falklands war in 1982. Sentence Extraction + Compression: ajor cabinet member visits 3 Falkland war was in 1982 3 Cavallo visited UK SP2IRL @ ACL2010 [D+Langford+Marcu, MLJ09] 94 Hal Daumé III (me@hal3.name)</abstract>
<title confidence="0.617008333333333">to Drive Camera Image Input: Output:</title>
<author confidence="0.22708">Hard left turn Hard right turn</author>
<note confidence="0.839438684210526">Steering in [-1,1] SP2IRL @ ACL2010 95 Hal Daumé III (me@hal3.name) DAgger: Dataset Aggregation trajectories with expert SP2IRL @ ACL2010 96 Hal Daumé III (me@hal3.name) Theoretical Guarantees policy sequence guarantees: Avg. Loss on Dataset Regret of + ) / N) SP2IRL @ ACL2010 Iterations 96 97 Hal Daumé III (me@hal3.name) 99Hal Daumé III (me@hal3.name) Super Mario Brothers Game Resized to 25x19 pixels (1425 features) Extracted 27K+ binary features from last 4 observations (14 binary features for every cell) From Mario AI competition 2009 Input: Output: Input: Steering in [-1,1] Jump in {0,1} Right in {0,1} Left in {0,1} Speed in {0,1} Output: SP2IRL @ ACL2010 SP2IRL @ ACL2010</note>
<address confidence="0.90178075">3200 3000 2800 M2600</address>
<phone confidence="0.900052">2 3 4 5 6 7 8</phone>
<affiliation confidence="0.795366">Number of Training Data Average Distance per Stage</affiliation>
<address confidence="0.9113958">2000 21800 1600 1400 1200</address>
<phone confidence="0.525387">9 10</phone>
<note confidence="0.976077333333333">Perceptron vs. LaS0 vs. Seamn Incremental perceptron LaS0</note>
<title confidence="0.482616">Seamn Un-learnable decision</title>
<author confidence="0.451045">Hal Daume</author>
<date confidence="0.535306">2400</date>
<note confidence="0.721622333333333">Searn(1) - Searn(0.4) - • • Supervised Discussion Dame III (me@hal3.name) SP2IRL gACL2010</note>
<title confidence="0.4926912">Relationship between SP and IRL Formally, they&apos;re (nearly) the same problem -• See humans performing some task -• Define some loss function -• Try to mimic the humans</title>
<abstract confidence="0.960501692307692">Difference is in philosophy: (I)RL has little notion of beam search or dynamic programming -• SP doesn&apos;t think about separating reward estimation from solving the prediction problem (I)RL has to deal with stochastiticity in MDPs Daume III (me@hal3.name) SP2IRL gACL2010 Important Concepts &gt;-Search and loss-augmented search for margin-based methods &gt;-Bold versus local updates for approximate search &gt;-Training on-path versus off-path &gt;-Stochastic versus deterministic worlds • Q-states / values &gt;-Learning reward functions vs. matching behavior Hal&apos;s Wager • Give me a structured prediction problem where: -•Annotations are at the lexical level -•Humans can do the annotation with reasonable agreement -•You give me a few thousand labeled sentences Then I can learn reasonably well... -...using of the algorithms we talked about • Why do I say this? -•Lots of positive experience -•I&apos;m an optimist -•I want your</abstract>
<note confidence="0.955449">DaumeIII(me@hal3.name) SP2IRLgACL2010 Daume III (me@hal3.name) SP2IRL @ACL2010</note>
<title confidence="0.785919">Software &gt;-Sequence labeling</title>
<author confidence="0.77703">-•Mallet http mallet cs umass edu</author>
<web confidence="0.798645">CRF++ http://crfpp.sourceforge.net</web>
<note confidence="0.69924285">gt;-Search-based structured prediction • LaS0 http://hal3.name/TagChunk • Seamn http://hal3.name/searn Higher-level &amp;quot;feature template&amp;quot; approaches -•Alchemy http://alchemy.cs.washington.edu Factorie http://code.google.com/p/factorie DaumeIII(me@hal3.name) SP2IRLgACL2010 Open problems &gt;-How to do SP when argmax is intractable.... %Bad: simple algorithms diverge[Kulesza+Pereira, NIPS07] %Good: some work well[Finley+Joachims, ICML08] you can make it fast![Meshi+al, ICML10] to do SP with delayed feedback assignment) just works sometimes[D, ICML09; Chang+al, ICML10] %Generic RL also works[Branavan+al, ACL09; Liang+al, ACL09] What role does structure actually play? -•Little: only constraints outputs[Punyakanok+al, IJCA105] -•Little: only introduces non-linearities[Liang+al, -•Lots: ??? Daurne Ill (me@hal3.name) SP2IRLgACL2010</note>
<abstract confidence="0.913420916666667">Summary -Structured prediction is you can do argmax search (esp. loss-augmented!) • Label-bias can kill you, so iterate (Seam) • Stochastic worlds modeled by MDPs • IRL is all about learning reward functions IRL has fewer assumptions -More general Less likely to work on easy problems • We&apos;re a long way from a complete solution • Hal&apos;s wager: we can learn pretty much anything Thanks!Questions?</abstract>
<note confidence="0.911685">References See also:</note>
<web confidence="0.9598515">http://www.cs.utah.eduhsuresh/mediawiki/index.php/MLRG http://braque.cc/ShowChannel?handle=P5BVAC34</web>
<note confidence="0.981547109090909">112 Daume III (me@hal3.name) SP2IRL @AGL2010 DaumeIll(me@hal3.name) SP2IRL@ACL2010 Stuff we talked about explicitly Apprenticeship learning via inverse reinforcement learning, P. Abbeel and A. Ng. ICML, 2004. Incremental parsing with the Perceptron algorithm. M Collins and B. Roark. ACL 2004. Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. Collins. EMNLP 2002. Structured Prediction. Daume III,J.Langford and D.Marcu.Machine Learning, 2009. as Search Optimization: Approximate Large Margin Methods for Structured Prediction. Daume III and D. Marcu.ICML, End-to-end Discriminative Approach to Machine Translation. Liang, A. Bouchard-COte, D. Klein, B. Taskar. ACL 2006. Decision-Tree Models for Parsing. D. Magerman. ACL1995. Parsers by Inverse Reinforcement Learning. G. Neu andCs.Szepesvari. Machine Learning77,2009. Algorithmsforinverse reinforcement learning, A. Ng and A. Russell. ICML, 2000. (Online)Subgradient Methods for Structured Prediction. J. Bagnell, and M Zinkevich. AlStats 2007. margin planning. Ratliff,J.Bagnell and M. Zinkevich. ICML, 2006. to search: Functional gradient techniques for imitation learning. D. Silver, and J. Bagnell. Robots, Vol.27,No.1,July, 2009. ReductionofImitation Learning to No-Regret Online Learning. Ross, G. Gordon andJ.Bagnell. AlStats 2011. Markov Networks. Taskar, C. Guestrin, V. Chatalbashev and D. Koller. JMLR 2005. Margin Methods for Structured and Interdependent Output Variables. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. JMLR 2005. Linear Ranking Functions for Beam Search with Application to Planning. Y. A. Fern, and S. Yoon. JMLR 2009. Entropy Inverse ReinforcementLearning.B. Ziebad, A.Maas, J.Bagnell, and A.Dey.AAAI 2008. Daume Ill (me@hal3.name) SP2IRLgAGL2010 Other good stuff Reinforcement learning for mapping instructions to actions. Branavan, H. Chen, L. Zettlemoyer and R. Barzilay. ACL, 2009. 1-Driving semantic parsing from the world&apos;s response. Clarke, D. Goldwasser, M.-IN. Chang, D. Roth. CoNLL 2010. r-New Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. and N. Duffy. 2002. 1-Unsupervised Search-based Structured Prediction. Daume III. ICML 2009. 1-Training structural SVMs when is intractable. Finley and T. Joachims. ICML, 2008. 1-Structured learning will approximate inference. Kulesza and F. Pereira. NIPS, 2007. Condffional random fields: Probabilistic models for segmenting and labeling sequence data. Lafferty, A McCallum, F. Pereira. ICML 2001. 1-Structure compilation: trading structure for features. Liang, H. Daume, D. Klein. ICML 2008. 1-Learning semantic correspondences will less supervision. Liang, M. Jordan and D. Klein. ACL, 2009. 1-Generalization Bounds and Consistency for Structured Labeling. McAllester. In Structured Data, 2007. 1-Maximum entropy Markov models for information extraction and segmentation. McCallum, D. Freitag, F. Pereira. ICML 2000. • FACTORIE: Efficient Probabilistic Programming for Relational Factor Graphs via Imperative Declarations of Structure, and Learning. McCallum, K. Rohanemanesh, M. Wok, K. Schultz, S. Singh. NIPS Workshop on Probabilistic Programming, 2008 1-Learning efficiently will approximate inference via dual losses. 0. D. Sontag, T. Jaakkola, A. ICML 2010. 1-Learning and inference over constrained output. Punyakanok, D. Roth, W. Yih, D. Zimak. IJCAI, 2005. 1-Boosting Structured Prediction for Imitation Learning. Ratliff, D. Bradley, J. Bagnell, and J. Chestnutt. NIPS 2007. 1-Efficient Reductions for Imitation Learning. Ross and J. Bagnell. AISTATS, 2010. Kernel Dependency Estimation. Weston, A. Elisseeff, B. Schoelkopf and V. Vapnik. NIPS 2002. Hal DaumeIll (me@hal3.name)SP2IRLgACL2010</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>