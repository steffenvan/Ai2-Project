<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024413">
<title confidence="0.991095">
Text Comparison Using Machine-Generated Nuggets
</title>
<author confidence="0.998465">
Liang Zhou
</author>
<affiliation confidence="0.997561">
Information Sciences Institute
University of Southern California
</affiliation>
<address confidence="0.88514">
4676 Admiralty Way
Marina del Rey, CA 90292
</address>
<email confidence="0.999542">
liangz@isi.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911214285714">
This paper describes a novel text com-
parison environment that facilities text
comparison administered through assess-
ing and aggregating information nuggets
automatically created and extracted from
the texts in question. Our goal in design-
ing such a tool is to enable and improve
automatic nugget creation and present its
application for evaluations of various
natural language processing tasks. During
our demonstration at HLT, new users will
able to experience first hand text analysis
can be fun, enjoyable, and interesting us-
ing system-created nuggets.
</bodyText>
<sectionHeader confidence="0.998548" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996568627451">
In many natural language processing (NLP) tasks,
such as question answering (QA), summarization,
etc., we are faced with the problem of determining
the appropriate granularity level for information
units in order to conduct appropriate and effective
evaluations. Most commonly, we use sentences to
model individual pieces of information. However,
more and more NLP applications require us to de-
fine text units smaller than sentences, essentially
decomposing sentences into a collection of
phrases. Each phrase carries an independent piece
of information that can be used as a standalone
unit. These finer-grained information units are
usually referred to as nuggets.
Previous work shows that humans can create
nuggets in a relatively straightforward fashion. A
serious problem in manual nugget creation is the
inconsistency in human decisions (Lin and Hovy,
2003). The same nugget will not be marked consis-
tently with the same words when sentences con-
taining multiple instances of it are presented to
human annotators. And if the annotation is per-
formed over an extended period of time, the con-
sistency is even lower.
Given concerns over these issues, we have set
out to design an evaluation toolkit to address three
tasks in particular: 1) provide a consistent defini-
tion of what a nugget is; 2) automate the nugget
extraction process systematically; and 3) utilize
automatically extracted nuggets for text compari-
son and aggregation.
The idea of using semantic equivalent nuggets
to compare texts is not new. QA and summariza-
tion evaluations (Lin and Demner-Fushman, 2005;
Nenkova and Passonneau, 2004) have been carried
out by using a set of manually created nuggets and
the comparison procedure itself is either automatic
using n-gram overlap counting or manually per-
formed. We envisage the nuggetization process
being automated and nugget comparison and ag-
gregation being performed by humans. It’s crucial
to still involve humans in the process because rec-
ognizing semantic equivalent text units is not a
trivial task. In addition, since nuggets are system-
produced and can be imperfect, annotators are al-
lowed to reject and re-create them. We provide
easy-to-use editing functionalities that allow man-
ual overrides. Record keeping on edits over erro-
neous nuggets is conducted in the background so
that further improvements can be made for nugget
extraction.
</bodyText>
<page confidence="0.986433">
29
</page>
<affiliation confidence="0.789408">
NAACL HLT Demonstration Program, pages 29–30,
Rochester, New York, USA, April 2007. c�2007 Association for Computational Linguistics
</affiliation>
<sectionHeader confidence="0.989892" genericHeader="method">
2 Nugget Definition
</sectionHeader>
<bodyText confidence="0.996805">
Based on our manual analysis and computational
modeling of nuggets, we define them as follows:
</bodyText>
<listItem confidence="0.8946142">
Definition:
• A nugget is predicated on either an event or
an entity.
• Each nugget consists of two parts: the an-
chor and the content.
</listItem>
<bodyText confidence="0.700769">
The anchor is either:
</bodyText>
<listItem confidence="0.8208548">
• the head noun of the entity, or
• the head verb of the event, plus the head
noun of its associated entity (if more than
one entity is attached to the verb, then its
subject).
</listItem>
<bodyText confidence="0.9998254">
The content is a coherent single piece of infor-
mation associated with the anchor. Each anchor
may have several separate contents. When the
nugget contains nested sentences, this definition is
applied recursively.
</bodyText>
<sectionHeader confidence="0.995862" genericHeader="method">
3 Nugget Extraction
</sectionHeader>
<bodyText confidence="0.999970583333333">
We use syntactic parse trees produced by the
Collins parser (Collins, 1999) to obtain the struc-
tural representation of sentences. Nuggets are ex-
tracted by identifying subtrees that are descriptions
for entities and events. For entities, we examine
subtrees headed by “NP”; for events, subtrees
headed by “VP” are examined and their corre-
sponding subjects (siblings headed by “NP”) are
investigated as possible entity attachments for the
verb phrases. Figure 1 shows an example where
words in brackets represent corresponding nug-
gets’ anchors.
</bodyText>
<sectionHeader confidence="0.920403" genericHeader="method">
4 Comparing Texts
</sectionHeader>
<bodyText confidence="0.999870222222222">
When comparing multiple texts, we present the
annotator with each text’s sentences along with
nuggets extracted from individual sentences (see
Appendix A). Annotators can select multiple nug-
gets from sentences across texts to indicate their
semantic equivalence. Equivalent nuggets are
grouped into nugget groups. There is a frequency
score, the number of texts it appeared in, for each
nugget group. We allow annotators to modify the
</bodyText>
<figureCaption confidence="0.989786">
Figure 1. Nugget example. (words in brackets are
the anchors).
</figureCaption>
<bodyText confidence="0.999921692307692">
nugget groups’ contents, thus creating a new label
(or can be viewed as a super-nugget) for each nug-
get group. Record keeping is conducted in the
background automatically each time a nugget
group is created. When the annotator changes the
content of a nugget group, it indicates that either
the system-extracted nuggets are not perfect or a
super-nugget is created for the group (see Appen-
dix B and C). These editing changes are recorded.
The recorded information affords us the opportu-
nity to improve the nuggetizer and perform subse-
quence study phrase-level paraphrasing, text
entailment, etc.
</bodyText>
<sectionHeader confidence="0.997204" genericHeader="conclusions">
5 Hardware Requirement
</sectionHeader>
<bodyText confidence="0.998615">
Our toolkit is written in Java and can be run on any
machine with the latest Java installed.
</bodyText>
<sectionHeader confidence="0.990281" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.665730333333333">
Collins, M. 1999. Head-driven statistical models
for natural language processing. PhD Disserta-
tion, University of Pennsylvania.
Lin, C.Y. and E. Hovy. 2003. Automatic evalua-
tion of summaries using n-gram co-occurrence
statistics. In Proceedings of NAACL-HLT 2003.
Lin, J. and D. Demner-Fushman. 2005. Automati-
cally evaluating answers to definition questions.
In Proceedings of HLT-EMNLP 2005.
Nenkova, A. and R. Passonneau. 2004. Evaluating
content selection in summarization: the pyramid
method. In Proceedings of NAACL-HLT 2004.
</reference>
<bodyText confidence="0.8118272">
Sentence:
The girl working at the bookstore in Hollywood
talked to the diplomat living in Britain.
Nuggets are:
[girl] working at the bookstore in Hollywood
[girl] working at the bookstore
[bookstore] in Hollywood
girl [talked] to the diplomat living in Britain
girl [talked] to the diplomat
[diplomat] living in Britian
</bodyText>
<page confidence="0.970988">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.903698">
<title confidence="0.998967">Text Comparison Using Machine-Generated Nuggets</title>
<author confidence="0.93678">Liang</author>
<affiliation confidence="0.997257">Information Sciences University of Southern</affiliation>
<address confidence="0.9920025">4676 Admiralty Marina del Rey, CA 90292</address>
<email confidence="0.999612">liangz@isi.edu</email>
<abstract confidence="0.998799866666667">This paper describes a novel text comparison environment that facilities text comparison administered through assessing and aggregating information nuggets automatically created and extracted from the texts in question. Our goal in designing such a tool is to enable and improve automatic nugget creation and present its application for evaluations of various natural language processing tasks. During our demonstration at HLT, new users will able to experience first hand text analysis can be fun, enjoyable, and interesting using system-created nuggets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language processing.</title>
<date>1999</date>
<institution>PhD Dissertation, University of Pennsylvania.</institution>
<contexts>
<context position="4019" citStr="Collins, 1999" startWordPosition="624" endWordPosition="625">edicated on either an event or an entity. • Each nugget consists of two parts: the anchor and the content. The anchor is either: • the head noun of the entity, or • the head verb of the event, plus the head noun of its associated entity (if more than one entity is attached to the verb, then its subject). The content is a coherent single piece of information associated with the anchor. Each anchor may have several separate contents. When the nugget contains nested sentences, this definition is applied recursively. 3 Nugget Extraction We use syntactic parse trees produced by the Collins parser (Collins, 1999) to obtain the structural representation of sentences. Nuggets are extracted by identifying subtrees that are descriptions for entities and events. For entities, we examine subtrees headed by “NP”; for events, subtrees headed by “VP” are examined and their corresponding subjects (siblings headed by “NP”) are investigated as possible entity attachments for the verb phrases. Figure 1 shows an example where words in brackets represent corresponding nuggets’ anchors. 4 Comparing Texts When comparing multiple texts, we present the annotator with each text’s sentences along with nuggets extracted fr</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, M. 1999. Head-driven statistical models for natural language processing. PhD Dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<contexts>
<context position="1626" citStr="Lin and Hovy, 2003" startWordPosition="233" endWordPosition="236">ctive evaluations. Most commonly, we use sentences to model individual pieces of information. However, more and more NLP applications require us to define text units smaller than sentences, essentially decomposing sentences into a collection of phrases. Each phrase carries an independent piece of information that can be used as a standalone unit. These finer-grained information units are usually referred to as nuggets. Previous work shows that humans can create nuggets in a relatively straightforward fashion. A serious problem in manual nugget creation is the inconsistency in human decisions (Lin and Hovy, 2003). The same nugget will not be marked consistently with the same words when sentences containing multiple instances of it are presented to human annotators. And if the annotation is performed over an extended period of time, the consistency is even lower. Given concerns over these issues, we have set out to design an evaluation toolkit to address three tasks in particular: 1) provide a consistent definition of what a nugget is; 2) automate the nugget extraction process systematically; and 3) utilize automatically extracted nuggets for text comparison and aggregation. The idea of using semantic </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Lin, C.Y. and E. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of NAACL-HLT 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Demner-Fushman</author>
</authors>
<title>Automatically evaluating answers to definition questions.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP</booktitle>
<contexts>
<context position="2336" citStr="Lin and Demner-Fushman, 2005" startWordPosition="350" endWordPosition="353">es containing multiple instances of it are presented to human annotators. And if the annotation is performed over an extended period of time, the consistency is even lower. Given concerns over these issues, we have set out to design an evaluation toolkit to address three tasks in particular: 1) provide a consistent definition of what a nugget is; 2) automate the nugget extraction process systematically; and 3) utilize automatically extracted nuggets for text comparison and aggregation. The idea of using semantic equivalent nuggets to compare texts is not new. QA and summarization evaluations (Lin and Demner-Fushman, 2005; Nenkova and Passonneau, 2004) have been carried out by using a set of manually created nuggets and the comparison procedure itself is either automatic using n-gram overlap counting or manually performed. We envisage the nuggetization process being automated and nugget comparison and aggregation being performed by humans. It’s crucial to still involve humans in the process because recognizing semantic equivalent text units is not a trivial task. In addition, since nuggets are systemproduced and can be imperfect, annotators are allowed to reject and re-create them. We provide easy-to-use editi</context>
</contexts>
<marker>Lin, Demner-Fushman, 2005</marker>
<rawString>Lin, J. and D. Demner-Fushman. 2005. Automatically evaluating answers to definition questions. In Proceedings of HLT-EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: the pyramid method.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<contexts>
<context position="2367" citStr="Nenkova and Passonneau, 2004" startWordPosition="354" endWordPosition="357">es of it are presented to human annotators. And if the annotation is performed over an extended period of time, the consistency is even lower. Given concerns over these issues, we have set out to design an evaluation toolkit to address three tasks in particular: 1) provide a consistent definition of what a nugget is; 2) automate the nugget extraction process systematically; and 3) utilize automatically extracted nuggets for text comparison and aggregation. The idea of using semantic equivalent nuggets to compare texts is not new. QA and summarization evaluations (Lin and Demner-Fushman, 2005; Nenkova and Passonneau, 2004) have been carried out by using a set of manually created nuggets and the comparison procedure itself is either automatic using n-gram overlap counting or manually performed. We envisage the nuggetization process being automated and nugget comparison and aggregation being performed by humans. It’s crucial to still involve humans in the process because recognizing semantic equivalent text units is not a trivial task. In addition, since nuggets are systemproduced and can be imperfect, annotators are allowed to reject and re-create them. We provide easy-to-use editing functionalities that allow m</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Nenkova, A. and R. Passonneau. 2004. Evaluating content selection in summarization: the pyramid method. In Proceedings of NAACL-HLT 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>