<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017383">
<title confidence="0.9058225">
Using existing systems to supplement small amounts of
annotated grammatical relations training data ∗
</title>
<author confidence="0.925497">
Alexander Yeh
</author>
<affiliation confidence="0.694443">
Mitre Corp.
</affiliation>
<address confidence="0.920564333333333">
202 Burlington Rd.
Bedford, MA 01730
USA
</address>
<email confidence="0.99741">
asy@mitre.org
</email>
<sectionHeader confidence="0.979986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997705">
Grammatical relationships (GRs)
form an important level of natu-
ral language processing, but differ-
ent sets of GRs are useful for differ-
ent purposes. Therefore, one may of-
ten only have time to obtain a small
training corpus with the desired GR
annotations. To boost the perfor-
mance from using such a small train-
ing corpus on a transformation rule
learner, we use existing systems that
find related types of annotations.
</bodyText>
<sectionHeader confidence="0.996296" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.95340525">
Grammatical relationships (GRs), which in-
clude arguments (e.g., subject and object) and
modifiers, form an important level of natural
language processing. Examples of GRs in the
sentence
Today, my dog pushed the ball on the floor.
are pushed having the subject my dog, the
object the ball and the time modifier To-
day, and the ball having the location modifier
on (the floor�. The resulting annotation is
my dog −subj→ pushed
on −mod-loc→ the ball
</bodyText>
<footnote confidence="0.836187454545455">
∗ This paper reports on work performed at the
MITRE Corporation under the support of the MITRE
Sponsored Research Program. Marc Vilain provided
the motivation to find GRs. Warren Greiff suggested
using randomization-type techniques to determine sta-
tistical significance. Sabine Buchholz and John Car-
roll ran their GR finding systems over our data for the
experiments. Jun Wu provided some helpful explana-
tions. Christine Doran and John Henderson provided
helpful editing. Three anonymous reviewers provided
helpful suggestions.
</footnote>
<bodyText confidence="0.984230077669903">
etc. GRs are the objects of study in rela-
tional grammar (Perlmutter, 1983). In the
SPARKLE project (Carroll et al., 1997), GRs
form the top layer of a three layer syntax
scheme. Many systems (e.g., the KERNEL
system (Palmer et al., 1993)) use GRs as an
intermediate form when determining the se-
mantics of syntactically parsed text. GRs are
often stored in structures similar to the F-
structures of lexical-functional grammar (Ka-
plan, 1994).
A complication is that different sets of GRs
are useful for different purposes. For exam-
ple, Ferro et al. (1999) is interested in seman-
tic interpretation, and needs to differentiate
between time, location and other modifiers.
The SPARKLE project (Carroll et al., 1997),
on the other hand, does not differentiate be-
tween these types of modifiers. As has been
mentioned by John Carroll (personal commu-
nication), combining modifier types together
is fine for information retrieval. Also, having
less differentiation of the modifiers can make
it easier to find them (Ferro et al., 1999).
Furthermore, unless the desired set of GRs
matches the set already annotated in some
large training corpus,l one will have to either
manually write rules to find the GRs, as done
in Ait-Mokhtar and Chanod (1997), or anno-
tate a new training corpus for the desired set.
Manually writing rules is expensive, as is an-
notating a large corpus.
Often, one may only have the resources to
produce a small annotated training set, and
many of the less common features of the set&apos;s
&apos;One example is a memory-based GR finder (Buch-
holz et al., 1999) that uses the GRs annotated in the
Penn Treebank (Marcus et al., 1993).
domain may not appear at all in that set.
In contrast are existing systems that perform
well (probably due to a large annotated train-
ing set or a set of carefully hand-crafted rules)
on related (but different) annotation stan-
dards. Such systems will cover many more
domain features, but because the annotation
standards are slightly different, some of those
features will be annotated in a different way
than in the small training and test set.
A way to try to combine the different advan-
tages of these small training data sets and ex-
isting systems which produce related annota-
tions is to use a sequence of two systems. We
first use an existing annotation system which
can handle many of the less common features,
i.e., those which do not appear in the small
training set. We then train a second system
with that same small training set to take the
output of the first system and correct for the
differences in annotations. This approach was
used by Palmer (1997) for word segmentation.
Hwa (1999) describes a somewhat similar ap-
proach for finding parse brackets which com-
bines a fully annotated related training data
set and a large but incompletely annotated fi-
nal training data set. Both these works deal
with just one (word boundary) or two (start
and end parse bracket) annotation label types
and the same label types are used in both the
existing annotation system/training set and
the final (small) training set. In compari-
son, our work handles many annotation la-
bel types, and the translation from the types
used in the existing annotation system to the
types in the small training set tends to be both
more complicated and most easily determined
by empirical means. Also, the type of baseline
score being improved upon is different. Our
work adds an existing system to improve the
rules learned, while Palmer (1997) adds rules
to improve an existing system&apos;s performance.
We use this related system/small training
set combination to improve the performance
of the transformation-based error-driven
learner described in Ferro et al. (1999). So
far, this learner has started with a blank
initial labeling of the GRs. This paper
describes experiments where we replace this
blank initial labeling with the output from
an existing GR finder that is good at a
somewhat different set of GR annotations.
With each of the two existing GR finders that
we use, we obtained improved results, with
the improvement being more noticeable when
the training set is smaller.
We also find that the existing GR finders
are quite uneven on how they improve the re-
sults. They each tend to concentrate on im-
proving the recovery of a few kinds of rela-
tions, leaving most of the other kinds alone.
We use this tendency to further boost the
learner&apos;s performance by using a merger of
these existing GR finders&apos; output as the initial
labeling.
</bodyText>
<sectionHeader confidence="0.923557" genericHeader="method">
2 The Experiment
</sectionHeader>
<bodyText confidence="0.99903575">
We now improve the performance of the
Ferro et al. (1999) transformation rule
learner on a small annotated training set by
using an existing system to provide initial
GR annotations. This experiment is repeated
on two different existing systems, which
are reported in Buchholz et al. (1999) and
Carroll et al. (1999), respectively.
Both of these systems find a somewhat
different set of GR annotations than the
one learned by the Ferro et al. (1999) sys-
tem. For example, the Buchholz et al. (1999)
system ignores verb complements of verbs
and is designed to look for relationships
to verbs and not GRs that exist between
nouns, etc. This system also handles
relative clauses differently. For example,
in &amp;quot;Miller, who organized ...&amp;quot;, this system is
trained to indicate that &amp;quot;who&amp;quot; is the subject
of &amp;quot;organized&amp;quot;, while the Ferro et al. (1999)
system is trained to indicate that &amp;quot;Miller&amp;quot;
is the subject of &amp;quot;organized&amp;quot;. As for the
Carroll et al. (1999) system, among other
things, it does not distinguish between sub-
types of modifiers such as time, location and
possessive. Also, both systems handle copu-
las (usually using the verb &amp;quot;to be&amp;quot;) differently
than in Ferro et al. (1999).
</bodyText>
<subsectionHeader confidence="0.953769">
2.1 Experiment Set-Up
</subsectionHeader>
<bodyText confidence="0.999993714285714">
As described in Ferro et al. (1999), the trans-
formation rule learner starts with a p-o-s
tagged corpus that has been &amp;quot;chunked&amp;quot; into
noun chunks, etc. The starting state also in-
cludes imperfect estimates of pp-attachments
and a blank set of initial GR annotations.
In these experiments, this blank initial set
is changed to be a translated version of the
annotations produced by an existing system.
This is how the existing system transmits
what it found to the rule learner. The set-
up for this experiment is shown in figure 1.
The four components with + signs are taken
out when one wants the transformation rule
learner to start with a blank set of initial GR
annotations.
The two arcs in that figure with a * indicate
where the translations occur. These transla-
tions of the annotations produced by the ex-
isting system are basically just an attempt to
map each type of annotation that it produces
to the most likely type of corresponding an-
notation used in the Ferro et al. (1999) sys-
tem. For example, in our experiments, the
Buchholz et al. (1999) system uses the anno-
tation np-sbj to indicate a subject, while the
Ferro et al. (1999) system uses the annota-
tion subj. We create the mapping by ex-
amining the training set to be given to the
Ferro et al. (1999) system. For each type of
relation ei output by the existing system when
given the training set text, we look at what
relation types (which tk&apos;s) co-occur with ei in
the training set. We look at the tk&apos;s with the
highest number of co-occurrences with that
ei. If that tk is unique (no ties for the highest
number of co-occurrences) and translating ei
to that tk generates at least as many correct
annotations in the training set as false alarms,
then make that translation. Otherwise, trans-
late ei to no relation. This latter translation
is not uncommon. For example, in one run of
our experiments, 9% of the relation instances
in the training set were so translated, in an-
other run, 46% of the instances were so trans-
lated.
Some relations in the Carroll et al. (1999)
system are between three or four elements.
These relations are each first translated into
a set of two element sub-relations before the
examination process above is performed.
Even before applying the rules, the trans-
lations find many of the desired annotations.
However, the rules can considerably improve
what is found. For example, in two of our
early experiments, the translations by them-
selves produced F-scores (explained below)
of about 40% to 50%. After the learned
rules were applied, those F-scores increased
to about 70%.
An alternative to performing translations is
to use the untranslated initial annotations as
an additional type of input to the rule sys-
tem. This alternative, which we have yet
to try, has the advantage of fitting into the
transformation-based error-driven paradigm
(Brill and Resnik, 1994) more cleanly than
having a translation stage. However, this ad-
ditional type of input will also further slow-
down an already slow rule-learning module.
</bodyText>
<subsectionHeader confidence="0.998892">
2.2 Overall Results
</subsectionHeader>
<bodyText confidence="0.999096375">
For our experiment, we use the same
1151 word (748 GR) test set used in
Ferro et al. (1999), but for a training set, we
use only a subset of the 3299 word training set
used in Ferro et al. (1999). This subset con-
tains 1391 (71%) of the 1963 GR instances in
the original training set. The overall results
for the test set are
</bodyText>
<table confidence="0.999080166666667">
Smaller Set, Overall Results ER
Training P F
R
IaC 478 (63.9%) 77.2% 69.9% 7.7%
IaB 466 (62.3%) 78.1% 69.3% 5.8%
NI 448 (59.9%) 77.1% 67.4%
</table>
<bodyText confidence="0.999593090909091">
where row IaB is the result of using the rules
learned when the Buchholz et al. (1999) sys-
tem&apos;s translated GR annotations are used
as the Initial Annotations, row IaC is the
similar result with the Carroll et al. (1999)
system, and row NI is the result of using
the rules learned when No Initial GR an-
notations are used (the rule learner as run
in Ferro et al. (1999)). R(ecall) is the num-
ber (and percentage) of the keys that are
recalled. P(recision) is the number of cor-
</bodyText>
<figure confidence="0.996029966666667">
small training set
★❍❍❍❍❥ +
existing system
✧
★
+
existing system
test set
✔
rule learner
✲ ✖ ✕
✚✚✚✚❃
* ✛✘❄
rules
✚✙
③ ✗ * ❄ ✔
✲rule interpreter
✖ ✕
+
GR annotations
initial test
✲
+initial training
GR annotations
✲
◗◗◗� ✗
final test
GR annotations
✟✟✟✟✯
key GR annotations for small training set
</figure>
<figureCaption confidence="0.999986">
Figure 1: Set-up to use an existing system to improve performance
</figureCaption>
<bodyText confidence="0.997078510204081">
rectly recalled keys divided by the num-
ber of GRs the system claims to exist.
F(-score) is the harmonic mean of recall (r)
and precision (p) percentages. It equals
2pr/(p + r). ER stands for Error Reduc-
tion. It indicates how much adding the ini-
tial annotations reduced the missing F-score,
where the missing F-score is 100%−F. ER
100%X(FIA−FNI)/(100%−FNI), where FNI
is the F-score for the NI row, and FIA is the
F-score for using the Initial Annotations of
interest. Here, the differences in recall and F-
score between NI and either IaB or IaC (but
not between IaB and IaC) are statistically sig-
nificant. The differences in precision is not.�
In these results, most of the modest F-score
gain came from increasing recall.
One may note that the error reductions here
are smaller than Palmer (1997)&apos;s error reduc-
tions. Besides being for different tasks (word
segmentation versus GRs), the reductions are
also computed using a different type of base-
line. In Palmer (1997), the baseline is how
well an existing system performs before the
rules are run. In this paper, the baseline is
the performance of the rules learned without
When comparing differences in this paper, the
statistical significance of the higher score being bet-
ter than the lower score is tested with a one-sided
test. Differences deemed statistically significant are
significant at the 5% level. Differences deemed non-
statistically significant are not significant at the 10%
level. For recall, we use a sign test for matched-pairs
(Harnett, 1982, Sec. 15.5). For precision and F-score,
a &amp;quot;matched-pairs&amp;quot; randomization test (Cohen, 1995,
Sec. 5.3) is used.
first using an existing system. If we were to
use the same baseline as Palmer (1997), our
baseline would be an F of 37.5% for IaB and
52.6% for IaC. This would result in a much
higher ER of 51% and 36%, respectively.
We now repeat our experiment with the
full 1963 GR instance training set. These re-
sults indicate that as a small training set gets
larger, the overall results get better and the
initial annotations help less in improving the
overall results. So the initial annotations are
more helpful with smaller training sets. The
overall results on the test set are
</bodyText>
<table confidence="0.9963892">
Full Training Set, Overall Results ER
R P F
IaC 487 (65.1%) 79.7% 71.7% 6.3%
IaB 486 (65.0%) 76.5% 70.3% 1.7%
NI 476 (63.6%) 77.3% 69.8%
</table>
<bodyText confidence="0.998868571428572">
The differences in recall, etc. between IaB and
NI are now small enough to be not statisti-
cally significant. The differences between IaC
and NI are statistically significant,3 but the
difference in both the absolute F-score (1.9%
versus 2.5% with the smaller training set) and
ER (6.3% versus 7.7%) has decreased.
</bodyText>
<subsectionHeader confidence="0.872509">
2.3 Results by Relation
</subsectionHeader>
<bodyText confidence="0.998491333333333">
The overall result of using an existing system
is a modest increase in F-score. However, this
increase is quite unevenly distributed, with a
</bodyText>
<footnote confidence="0.514968">
3The recall difference is semi-significant, being sig-
nificant at the 10% level.
</footnote>
<bodyText confidence="0.999604135135135">
few relation(s) having a large increase, and
most relations not having much of a change.
Different existing systems seem to have differ-
ent relations where most of the increase oc-
curs.
As an example, take the results of using
the Buchholz et al. (1999) system on the 1391
GR instance training set. Many GRs, like pos-
sessive modifier, are not affected by the added
initial annotations. Some GRs, like location
modifier, do slightly better (as measured by
the F-score) with the added initial annota-
tions, but some, like subject, do better with-
out. With GRs like subject, some differences
between the initial and desired annotations
may be too subtle for the Ferro et al. (1999)
system to adjust for. Or those differences may
be just due to chance, as the result differences
in those GRs are not statistically significant.
The GRs with statistically significant result
differences are the time and &amp;quot;other&amp;quot;4 modifiers,
where adding the initial annotations helps.
The time modifiers results are quite different:
The difference in the number recalled (15) for
this GR accounts for nearly the entire differ-
ence in the overall recall results (18). The re-
call, precision and F-score differences are all
statistically significant.
Similarly, when using the
Carroll et al. (1999) system on this training
set, most GRs are not affected, while others
do slightly better. The only GR with a sta-
tistically significant result difference is object,
where again adding the initial annotations
helps:
The difference in the number recalled (19) for
this GR again accounts for most of the dif-
</bodyText>
<footnote confidence="0.9803005">
4Modifiers that do not fall into any of the subtypes
used, such as time, location, possessive, etc. Examples
of unused subtypes are purpose and modality.
5There are 45 instances in the test set key.
</footnote>
<bodyText confidence="0.994841466666667">
ference in the overall recall results (30). The
recall and F-score differences are statistically
significant. The precision difference is not.
As one changes from the smaller 1391 GR
instance training set to the larger 1963 GR
instance training set, these F-score improve-
ments become smaller. When using the
Buchholz et al. (1999) system, the improve-
ment in the &amp;quot;other&amp;quot; modifier is now no longer
statistically significant. However, the time
modifier F-score improvement stays statisti-
cally significant:
When using the Carroll et al. (1999) system,
the object F-score improvement stays statisti-
cally significant:
</bodyText>
<subsectionHeader confidence="0.985164">
2.4 Combining Sets of Initial
Annotations
</subsectionHeader>
<bodyText confidence="0.999856130434782">
So the initial annotations from different ex-
isting systems tend to each concentrate on
improving the performance of different GR
types. From this observation, one may wonder
about combining the annotations from these
different systems in order to increase the per-
formance on all the GR types affected by those
different existing systems.
Various works (van Halteren et al., 1998;
Henderson and Brill, 1999; Wilkes and
Stevenson, 1998) on combining different sys-
tems exist. These works use one or both of
two types of schemes. One is to have the
different systems simply vote. However, this
does not really make use of the fact that dif-
ferent systems are better at handling differ-
ent GR types. The other approach uses a
combiner that takes the systems&apos; output as
input and may perform such actions as de-
termining which system to use under which
circumstance. Unfortunately, this approach
needs extra training data to train such a com-
biner. Such data may be more useful when
</bodyText>
<figure confidence="0.980864678571428">
R
P
F
ER
IaB
53%
80.6%
71.6%
29 (64.4%)
NI
56.0%
40.0%
14 (31.1%)
Smaller Training Set, Time Modifiers
R
P
F
ER
IaC
17%
79.5%
79.5%
198 (79.5%)
NI
78.9%
75.2%
179 (71.9%)
Smaller Training Set, Object Relations
R
P
F
ER
IaB
46%
69.0%
74.4%
29 (64.4%)
NI
42.3%
57.7%
15 (33.3%)
Full Training Set, Time Modifiers
R
P
F
ER
IaC
16%
81.3%
85.1%
194 (77.9%)
NI
77.8%
80.3%
188 (75.5%)
Full Training Set, Object Relations
</figure>
<bodyText confidence="0.9978794375">
used instead as additional training data for
the individual methods that one is consider-
ing to combine, especially when the systems
being combined were originally given a small
amount of training data.
To avoid the disadvantages of these existing
schemes, we came up with a third method.
We combine the existing related systems by
taking a union of their translated annota-
tions as the new initial GR annotation for
our system. We rerun rule learning on the
smaller (1391 GR instance) training set with
a Union of the Buchholz et al. (1999) and
Carroll et al. (1999) systems&apos; translated GR
annotations. The overall results for the test
set are (shown in row IaU)
</bodyText>
<table confidence="0.998103714285714">
Smaller Set, Overall Results ER
Training P F
R
IaU 496 (66.3%) 76.4% 71.0% 11%
IaC 478 (63.9%) 77.2% 69.9% 7.7%
IaB 466 (62.3%) 78.1% 69.3% 5.8%
NI 448 (59.9%) 77.1% 67.4%
</table>
<bodyText confidence="0.999567421052632">
where the other rows are as shown in Sec-
tion 2.2. Compared to the F-score with
using Carroll et al. (1999) (IaC), the IaU
F-score is &amp;quot;borderline&amp;quot; statistically signifi-
cantly better (11% significance level). The
IaU F-score is statistically significantly bet-
ter than the F-scores with either using
Buchholz et al. (1999) (IaB) or not using any
initial annotations (NI).
As expected, most (42 of 48) of the overall
increase in recall going from NI to IaU comes
from increasing the recall of the object, time
modifier and other modifier relations, the re-
lations that IaC and IaB concentrate on. The
ER for object is 11% and for time modifier is
56%.
When this combining approach is repeated
the full 1963 GR instance training set, the
overall results for the test set are
</bodyText>
<table confidence="0.990391333333333">
Full Training Set, Overall Results ER
R P F
IaU 502 (67.1%) 77.7% 72.0% 7.3%
IaC 487 (65.1%) 79.7% 71.7% 6.3%
IaB 486 (65.0%) 76.5% 70.3% 1.7%
NI 476 (63.6%) 77.3% 69.8%
</table>
<bodyText confidence="0.999636125">
Compared to the smaller training set results,
the difference between IaU and IaC here is
smaller for both the absolute F-score (0.3%
versus 1.1%) and ER (1.0% versus 3.3%). In
fact, the F-score difference is small enough to
not be statistically significant. Given the pre-
vious results for IaC and IaB as a small train-
ing set gets larger, this is not surprising.
</bodyText>
<sectionHeader confidence="0.999534" genericHeader="conclusions">
3 Discussion
</sectionHeader>
<bodyText confidence="0.999990733333333">
GRs are important, but different sets of GRs
are useful for different purposes and different
systems are better at finding certain types of
GRs. Here, we have been looking at ways of
improving automatic GR finders when one has
only a small amount of data with the desired
GR annotations. In this paper, we improve
the performance of the Ferro et al. (1999) GR
transformation rule learner by using existing
systems to find related sets of GRs. The out-
put of these systems is used to supply ini-
tial sets of annotations for the rule learner.
We achieve modest gains with the existing
systems tried. When one examines the re-
sults, one notices that the gains tend to be
uneven, with a few GR types having large
gains, and the rest not being affected much.
The different systems concentrate on improv-
ing different GR types. We leverage this ten-
dency to make a further modest improvement
in the overall results by providing the rule
learner with the merged output of these ex-
isting systems. We have yet to try other ways
of combining the output of existing systems
that do not require extra training data. One
possibility is the example-based combiner in
Brill and Wu (1998, Sec. 3.2).6 Furthermore,
finding additional existing systems to add to
the combination may further improve the re-
sults.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991065">
S. Ait-Mokhtar and J.-P. Chanod. 1997. Subject
and object dependency extraction using finite-
state transducers. In Proc. ACL workshop on
automatic information extraction and building
&apos;Based on the paper, we were unsure if extra train-
ing data is needed for this combiner. One of the au-
thors, Wu, has told us that extra data is not needed.
of lexical semantic resources for NLP applica-
tions, Madrid.
E. Brill and P. Resnik. 1994. A rule-based ap-
proach to prepositional phrase attachment dis-
ambiguation. In 15th International Conf. on
Computational Linguistics (COLING).
E. Brill and J. Wu. 1998. Classifier combina-
tion for improved lexical disambiguation. In
COLING-ACL&apos;98, pages 191-195, Montréal,
Canada.
S. Buchholz, J. Veenstra, and W. Daelemans.
1999. Cascaded grammatical relation assign-
ment. In Joint SIGDAT Conference on Empir-
ical Methods in NLP and Very Large Corpora
(EMNLP/VLC&apos;99). cs.CL/9906004.
J. Carroll, T. Briscoe, N. Calzolari, S. Fed-
erici, S. Montemagni, V. Pirrelli, G. Grefen-
stette, A. Sanfilippo, G. Carroll, and M. Rooth.
1997. Sparkle work package 1, spec-
ification of phrasal parsing, final report.
Available at http://www.ilc.pi.cnr.it/-
sparkle/sparkle.htm, November.
J. Carroll, G. Minnen, and T. Briscoe. 1999.
Corpus annotation for parser evaluation. In
EACL99 workshop on Linguistically Interpreted
Corpora (LINC&apos;99). cs.CL/9907013.
P. Cohen. 1995. Empirical Methods for Artificial
Intelligence. MIT Press, Cambridge, MA, USA.
L. Ferro, M. Vilain, and A. Yeh. 1999. Learn-
ing transformation rules to find grammatical
relations. In Computational natural language
learning (CoNLL-99), pages 43-52. EACL&apos;99
workshop,cs.CL/9906015.
D. Harnett. 1982. Statistical Methods. Addison-
Wesley Publishing Co., Reading, MA, USA,
third edition.
J. Henderson and E. Brill. 1999. Exploiting diver-
sity in natural language processing: combining
parsers. In Joint SIGDAT Conference on Em-
pirical Methods in NLP and Very Large Cor-
pora (EMNLP/VLC&apos;99).
R. Hwa. 1999. Supervised grammar induction
using training data with limited constituent in-
formation. In ACL&apos;99. cs.CL/9905001.
R. Kaplan. 1994. The formal architecture of
lexical-functional grammar. In M. Dalrymple,
R. Kaplan, J. Maxwell III, and A. Zaenen, ed-
itors, Formal issues in lexical-functional gram-
mar. Stanford University.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: the penn treebank. Computational Lin-
guistics, 19(2).
M. Palmer, R. Passonneau, C. Weir, and T. Finin.
1993. The kernel text understanding system.
Artificial Intelligence, 63:17-68.
D. Palmer. 1997. A trainable rule-based algo-
rithm for word segmentation. In Proceedings of
ACL/EACL97.
D. Perlmutter. 1983. Studies in Relational Gram-
mar 1. U. Chicago Press.
H. van Halteren, J. Zavrel, and W. Daelemans.
1998. Improving data driven wordclass tagging
by system combination. In COLING-ACL&apos;98,
pages 491-497, Montréal, Canada.
Y. Wilkes and M. Stevenson. 1998. Word sense
disambiguation using optimized combinations
of knowledge sources. In COLING-ACL&apos;98,
pages 1398-1402, Montréal, Canada.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.916921">
<title confidence="0.996406">Using existing systems to supplement small amounts of grammatical relations training data</title>
<author confidence="0.999917">Alexander Yeh</author>
<affiliation confidence="0.933523">Mitre Corp.</affiliation>
<address confidence="0.999117">202 Burlington Rd. Bedford, MA 01730 USA</address>
<email confidence="0.99511">asy@mitre.org</email>
<abstract confidence="0.999706">Grammatical relationships (GRs) form an important level of natural language processing, but different sets of GRs are useful for different purposes. Therefore, one may often only have time to obtain a small training corpus with the desired GR annotations. To boost the performance from using such a small training corpus on a transformation rule learner, we use existing systems that find related types of annotations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Ait-Mokhtar</author>
<author>J-P Chanod</author>
</authors>
<title>Subject and object dependency extraction using finitestate transducers.</title>
<date>1997</date>
<booktitle>In Proc. ACL workshop</booktitle>
<location>Madrid.</location>
<contexts>
<context position="2836" citStr="Ait-Mokhtar and Chanod (1997)" startWordPosition="451" endWordPosition="454">o differentiate between time, location and other modifiers. The SPARKLE project (Carroll et al., 1997), on the other hand, does not differentiate between these types of modifiers. As has been mentioned by John Carroll (personal communication), combining modifier types together is fine for information retrieval. Also, having less differentiation of the modifiers can make it easier to find them (Ferro et al., 1999). Furthermore, unless the desired set of GRs matches the set already annotated in some large training corpus,l one will have to either manually write rules to find the GRs, as done in Ait-Mokhtar and Chanod (1997), or annotate a new training corpus for the desired set. Manually writing rules is expensive, as is annotating a large corpus. Often, one may only have the resources to produce a small annotated training set, and many of the less common features of the set&apos;s &apos;One example is a memory-based GR finder (Buchholz et al., 1999) that uses the GRs annotated in the Penn Treebank (Marcus et al., 1993). domain may not appear at all in that set. In contrast are existing systems that perform well (probably due to a large annotated training set or a set of carefully hand-crafted rules) on related (but diffe</context>
</contexts>
<marker>Ait-Mokhtar, Chanod, 1997</marker>
<rawString>S. Ait-Mokhtar and J.-P. Chanod. 1997. Subject and object dependency extraction using finitestate transducers. In Proc. ACL workshop on automatic information extraction and building &apos;Based on the paper, we were unsure if extra training data is needed for this combiner. One of the authors, Wu, has told us that extra data is not needed. of lexical semantic resources for NLP applications, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>P Resnik</author>
</authors>
<title>A rule-based approach to prepositional phrase attachment disambiguation.</title>
<date>1994</date>
<booktitle>In 15th International Conf. on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="10125" citStr="Brill and Resnik, 1994" startWordPosition="1699" endWordPosition="1702">e rules, the translations find many of the desired annotations. However, the rules can considerably improve what is found. For example, in two of our early experiments, the translations by themselves produced F-scores (explained below) of about 40% to 50%. After the learned rules were applied, those F-scores increased to about 70%. An alternative to performing translations is to use the untranslated initial annotations as an additional type of input to the rule system. This alternative, which we have yet to try, has the advantage of fitting into the transformation-based error-driven paradigm (Brill and Resnik, 1994) more cleanly than having a translation stage. However, this additional type of input will also further slowdown an already slow rule-learning module. 2.2 Overall Results For our experiment, we use the same 1151 word (748 GR) test set used in Ferro et al. (1999), but for a training set, we use only a subset of the 3299 word training set used in Ferro et al. (1999). This subset contains 1391 (71%) of the 1963 GR instances in the original training set. The overall results for the test set are Smaller Set, Overall Results ER Training P F R IaC 478 (63.9%) 77.2% 69.9% 7.7% IaB 466 (62.3%) 78.1% 69</context>
</contexts>
<marker>Brill, Resnik, 1994</marker>
<rawString>E. Brill and P. Resnik. 1994. A rule-based approach to prepositional phrase attachment disambiguation. In 15th International Conf. on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Wu</author>
</authors>
<title>Classifier combination for improved lexical disambiguation.</title>
<date>1998</date>
<booktitle>In COLING-ACL&apos;98,</booktitle>
<pages>191--195</pages>
<location>Montréal, Canada.</location>
<marker>Brill, Wu, 1998</marker>
<rawString>E. Brill and J. Wu. 1998. Classifier combination for improved lexical disambiguation. In COLING-ACL&apos;98, pages 191-195, Montréal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>J Veenstra</author>
<author>W Daelemans</author>
</authors>
<title>Cascaded grammatical relation assignment.</title>
<date>1999</date>
<booktitle>In Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora (EMNLP/VLC&apos;99). cs.CL/9906004.</booktitle>
<contexts>
<context position="3159" citStr="Buchholz et al., 1999" startWordPosition="509" endWordPosition="513"> differentiation of the modifiers can make it easier to find them (Ferro et al., 1999). Furthermore, unless the desired set of GRs matches the set already annotated in some large training corpus,l one will have to either manually write rules to find the GRs, as done in Ait-Mokhtar and Chanod (1997), or annotate a new training corpus for the desired set. Manually writing rules is expensive, as is annotating a large corpus. Often, one may only have the resources to produce a small annotated training set, and many of the less common features of the set&apos;s &apos;One example is a memory-based GR finder (Buchholz et al., 1999) that uses the GRs annotated in the Penn Treebank (Marcus et al., 1993). domain may not appear at all in that set. In contrast are existing systems that perform well (probably due to a large annotated training set or a set of carefully hand-crafted rules) on related (but different) annotation standards. Such systems will cover many more domain features, but because the annotation standards are slightly different, some of those features will be annotated in a different way than in the small training and test set. A way to try to combine the different advantages of these small training data sets</context>
<context position="6365" citStr="Buchholz et al. (1999)" startWordPosition="1054" endWordPosition="1057"> quite uneven on how they improve the results. They each tend to concentrate on improving the recovery of a few kinds of relations, leaving most of the other kinds alone. We use this tendency to further boost the learner&apos;s performance by using a merger of these existing GR finders&apos; output as the initial labeling. 2 The Experiment We now improve the performance of the Ferro et al. (1999) transformation rule learner on a small annotated training set by using an existing system to provide initial GR annotations. This experiment is repeated on two different existing systems, which are reported in Buchholz et al. (1999) and Carroll et al. (1999), respectively. Both of these systems find a somewhat different set of GR annotations than the one learned by the Ferro et al. (1999) system. For example, the Buchholz et al. (1999) system ignores verb complements of verbs and is designed to look for relationships to verbs and not GRs that exist between nouns, etc. This system also handles relative clauses differently. For example, in &amp;quot;Miller, who organized ...&amp;quot;, this system is trained to indicate that &amp;quot;who&amp;quot; is the subject of &amp;quot;organized&amp;quot;, while the Ferro et al. (1999) system is trained to indicate that &amp;quot;Miller&amp;quot; is the</context>
<context position="8323" citStr="Buchholz et al. (1999)" startWordPosition="1392" endWordPosition="1395">em transmits what it found to the rule learner. The setup for this experiment is shown in figure 1. The four components with + signs are taken out when one wants the transformation rule learner to start with a blank set of initial GR annotations. The two arcs in that figure with a * indicate where the translations occur. These translations of the annotations produced by the existing system are basically just an attempt to map each type of annotation that it produces to the most likely type of corresponding annotation used in the Ferro et al. (1999) system. For example, in our experiments, the Buchholz et al. (1999) system uses the annotation np-sbj to indicate a subject, while the Ferro et al. (1999) system uses the annotation subj. We create the mapping by examining the training set to be given to the Ferro et al. (1999) system. For each type of relation ei output by the existing system when given the training set text, we look at what relation types (which tk&apos;s) co-occur with ei in the training set. We look at the tk&apos;s with the highest number of co-occurrences with that ei. If that tk is unique (no ties for the highest number of co-occurrences) and translating ei to that tk generates at least as many </context>
<context position="10847" citStr="Buchholz et al. (1999)" startWordPosition="1834" endWordPosition="1837">her slowdown an already slow rule-learning module. 2.2 Overall Results For our experiment, we use the same 1151 word (748 GR) test set used in Ferro et al. (1999), but for a training set, we use only a subset of the 3299 word training set used in Ferro et al. (1999). This subset contains 1391 (71%) of the 1963 GR instances in the original training set. The overall results for the test set are Smaller Set, Overall Results ER Training P F R IaC 478 (63.9%) 77.2% 69.9% 7.7% IaB 466 (62.3%) 78.1% 69.3% 5.8% NI 448 (59.9%) 77.1% 67.4% where row IaB is the result of using the rules learned when the Buchholz et al. (1999) system&apos;s translated GR annotations are used as the Initial Annotations, row IaC is the similar result with the Carroll et al. (1999) system, and row NI is the result of using the rules learned when No Initial GR annotations are used (the rule learner as run in Ferro et al. (1999)). R(ecall) is the number (and percentage) of the keys that are recalled. P(recision) is the number of corsmall training set ★❍❍❍❍❥ + existing system ✧ ★ + existing system test set ✔ rule learner ✲ ✖ ✕ ✚✚✚✚❃ * ✛✘❄ rules ✚✙  ✗ * ❄ ✔ ✲rule interpreter ✖ ✕ + GR annotations initial test ✲ +initial training GR annotat</context>
<context position="14740" citStr="Buchholz et al. (1999)" startWordPosition="2505" endWordPosition="2508">erence in both the absolute F-score (1.9% versus 2.5% with the smaller training set) and ER (6.3% versus 7.7%) has decreased. 2.3 Results by Relation The overall result of using an existing system is a modest increase in F-score. However, this increase is quite unevenly distributed, with a 3The recall difference is semi-significant, being significant at the 10% level. few relation(s) having a large increase, and most relations not having much of a change. Different existing systems seem to have different relations where most of the increase occurs. As an example, take the results of using the Buchholz et al. (1999) system on the 1391 GR instance training set. Many GRs, like possessive modifier, are not affected by the added initial annotations. Some GRs, like location modifier, do slightly better (as measured by the F-score) with the added initial annotations, but some, like subject, do better without. With GRs like subject, some differences between the initial and desired annotations may be too subtle for the Ferro et al. (1999) system to adjust for. Or those differences may be just due to chance, as the result differences in those GRs are not statistically significant. The GRs with statistically signi</context>
<context position="16588" citStr="Buchholz et al. (1999)" startWordPosition="2805" endWordPosition="2808">ps: The difference in the number recalled (19) for this GR again accounts for most of the dif4Modifiers that do not fall into any of the subtypes used, such as time, location, possessive, etc. Examples of unused subtypes are purpose and modality. 5There are 45 instances in the test set key. ference in the overall recall results (30). The recall and F-score differences are statistically significant. The precision difference is not. As one changes from the smaller 1391 GR instance training set to the larger 1963 GR instance training set, these F-score improvements become smaller. When using the Buchholz et al. (1999) system, the improvement in the &amp;quot;other&amp;quot; modifier is now no longer statistically significant. However, the time modifier F-score improvement stays statistically significant: When using the Carroll et al. (1999) system, the object F-score improvement stays statistically significant: 2.4 Combining Sets of Initial Annotations So the initial annotations from different existing systems tend to each concentrate on improving the performance of different GR types. From this observation, one may wonder about combining the annotations from these different systems in order to increase the performance on a</context>
<context position="18838" citStr="Buchholz et al. (1999)" startWordPosition="3187" endWordPosition="3190">% 85.1% 194 (77.9%) NI 77.8% 80.3% 188 (75.5%) Full Training Set, Object Relations used instead as additional training data for the individual methods that one is considering to combine, especially when the systems being combined were originally given a small amount of training data. To avoid the disadvantages of these existing schemes, we came up with a third method. We combine the existing related systems by taking a union of their translated annotations as the new initial GR annotation for our system. We rerun rule learning on the smaller (1391 GR instance) training set with a Union of the Buchholz et al. (1999) and Carroll et al. (1999) systems&apos; translated GR annotations. The overall results for the test set are (shown in row IaU) Smaller Set, Overall Results ER Training P F R IaU 496 (66.3%) 76.4% 71.0% 11% IaC 478 (63.9%) 77.2% 69.9% 7.7% IaB 466 (62.3%) 78.1% 69.3% 5.8% NI 448 (59.9%) 77.1% 67.4% where the other rows are as shown in Section 2.2. Compared to the F-score with using Carroll et al. (1999) (IaC), the IaU F-score is &amp;quot;borderline&amp;quot; statistically significantly better (11% significance level). The IaU F-score is statistically significantly better than the F-scores with either using Buchholz</context>
</contexts>
<marker>Buchholz, Veenstra, Daelemans, 1999</marker>
<rawString>S. Buchholz, J. Veenstra, and W. Daelemans. 1999. Cascaded grammatical relation assignment. In Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora (EMNLP/VLC&apos;99). cs.CL/9906004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>T Briscoe</author>
<author>N Calzolari</author>
<author>S Federici</author>
<author>S Montemagni</author>
<author>V Pirrelli</author>
<author>G Grefenstette</author>
<author>A Sanfilippo</author>
<author>G Carroll</author>
<author>M Rooth</author>
</authors>
<title>Sparkle work package 1, specification of phrasal parsing, final report. Available at http://www.ilc.pi.cnr.it/-sparkle/sparkle.htm,</title>
<date>1997</date>
<contexts>
<context position="1722" citStr="Carroll et al., 1997" startWordPosition="269" endWordPosition="272"> performed at the MITRE Corporation under the support of the MITRE Sponsored Research Program. Marc Vilain provided the motivation to find GRs. Warren Greiff suggested using randomization-type techniques to determine statistical significance. Sabine Buchholz and John Carroll ran their GR finding systems over our data for the experiments. Jun Wu provided some helpful explanations. Christine Doran and John Henderson provided helpful editing. Three anonymous reviewers provided helpful suggestions. etc. GRs are the objects of study in relational grammar (Perlmutter, 1983). In the SPARKLE project (Carroll et al., 1997), GRs form the top layer of a three layer syntax scheme. Many systems (e.g., the KERNEL system (Palmer et al., 1993)) use GRs as an intermediate form when determining the semantics of syntactically parsed text. GRs are often stored in structures similar to the Fstructures of lexical-functional grammar (Kaplan, 1994). A complication is that different sets of GRs are useful for different purposes. For example, Ferro et al. (1999) is interested in semantic interpretation, and needs to differentiate between time, location and other modifiers. The SPARKLE project (Carroll et al., 1997), on the othe</context>
</contexts>
<marker>Carroll, Briscoe, Calzolari, Federici, Montemagni, Pirrelli, Grefenstette, Sanfilippo, Carroll, Rooth, 1997</marker>
<rawString>J. Carroll, T. Briscoe, N. Calzolari, S. Federici, S. Montemagni, V. Pirrelli, G. Grefenstette, A. Sanfilippo, G. Carroll, and M. Rooth. 1997. Sparkle work package 1, specification of phrasal parsing, final report. Available at http://www.ilc.pi.cnr.it/-sparkle/sparkle.htm, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>G Minnen</author>
<author>T Briscoe</author>
</authors>
<title>Corpus annotation for parser evaluation.</title>
<date>1999</date>
<booktitle>In EACL99 workshop on Linguistically Interpreted Corpora (LINC&apos;99). cs.CL/9907013.</booktitle>
<contexts>
<context position="6391" citStr="Carroll et al. (1999)" startWordPosition="1059" endWordPosition="1062">mprove the results. They each tend to concentrate on improving the recovery of a few kinds of relations, leaving most of the other kinds alone. We use this tendency to further boost the learner&apos;s performance by using a merger of these existing GR finders&apos; output as the initial labeling. 2 The Experiment We now improve the performance of the Ferro et al. (1999) transformation rule learner on a small annotated training set by using an existing system to provide initial GR annotations. This experiment is repeated on two different existing systems, which are reported in Buchholz et al. (1999) and Carroll et al. (1999), respectively. Both of these systems find a somewhat different set of GR annotations than the one learned by the Ferro et al. (1999) system. For example, the Buchholz et al. (1999) system ignores verb complements of verbs and is designed to look for relationships to verbs and not GRs that exist between nouns, etc. This system also handles relative clauses differently. For example, in &amp;quot;Miller, who organized ...&amp;quot;, this system is trained to indicate that &amp;quot;who&amp;quot; is the subject of &amp;quot;organized&amp;quot;, while the Ferro et al. (1999) system is trained to indicate that &amp;quot;Miller&amp;quot; is the subject of &amp;quot;organized&amp;quot;. A</context>
<context position="9302" citStr="Carroll et al. (1999)" startWordPosition="1570" endWordPosition="1573">o-occur with ei in the training set. We look at the tk&apos;s with the highest number of co-occurrences with that ei. If that tk is unique (no ties for the highest number of co-occurrences) and translating ei to that tk generates at least as many correct annotations in the training set as false alarms, then make that translation. Otherwise, translate ei to no relation. This latter translation is not uncommon. For example, in one run of our experiments, 9% of the relation instances in the training set were so translated, in another run, 46% of the instances were so translated. Some relations in the Carroll et al. (1999) system are between three or four elements. These relations are each first translated into a set of two element sub-relations before the examination process above is performed. Even before applying the rules, the translations find many of the desired annotations. However, the rules can considerably improve what is found. For example, in two of our early experiments, the translations by themselves produced F-scores (explained below) of about 40% to 50%. After the learned rules were applied, those F-scores increased to about 70%. An alternative to performing translations is to use the untranslat</context>
<context position="10980" citStr="Carroll et al. (1999)" startWordPosition="1857" endWordPosition="1860"> used in Ferro et al. (1999), but for a training set, we use only a subset of the 3299 word training set used in Ferro et al. (1999). This subset contains 1391 (71%) of the 1963 GR instances in the original training set. The overall results for the test set are Smaller Set, Overall Results ER Training P F R IaC 478 (63.9%) 77.2% 69.9% 7.7% IaB 466 (62.3%) 78.1% 69.3% 5.8% NI 448 (59.9%) 77.1% 67.4% where row IaB is the result of using the rules learned when the Buchholz et al. (1999) system&apos;s translated GR annotations are used as the Initial Annotations, row IaC is the similar result with the Carroll et al. (1999) system, and row NI is the result of using the rules learned when No Initial GR annotations are used (the rule learner as run in Ferro et al. (1999)). R(ecall) is the number (and percentage) of the keys that are recalled. P(recision) is the number of corsmall training set ★❍❍❍❍❥ + existing system ✧ ★ + existing system test set ✔ rule learner ✲ ✖ ✕ ✚✚✚✚❃ * ✛✘❄ rules ✚✙  ✗ * ❄ ✔ ✲rule interpreter ✖ ✕ + GR annotations initial test ✲ +initial training GR annotations ✲ ◗◗◗� ✗ final test GR annotations ✟✟✟✟✯ key GR annotations for small training set Figure 1: Set-up to use an existing system to</context>
<context position="15756" citStr="Carroll et al. (1999)" startWordPosition="2668" endWordPosition="2671"> for the Ferro et al. (1999) system to adjust for. Or those differences may be just due to chance, as the result differences in those GRs are not statistically significant. The GRs with statistically significant result differences are the time and &amp;quot;other&amp;quot;4 modifiers, where adding the initial annotations helps. The time modifiers results are quite different: The difference in the number recalled (15) for this GR accounts for nearly the entire difference in the overall recall results (18). The recall, precision and F-score differences are all statistically significant. Similarly, when using the Carroll et al. (1999) system on this training set, most GRs are not affected, while others do slightly better. The only GR with a statistically significant result difference is object, where again adding the initial annotations helps: The difference in the number recalled (19) for this GR again accounts for most of the dif4Modifiers that do not fall into any of the subtypes used, such as time, location, possessive, etc. Examples of unused subtypes are purpose and modality. 5There are 45 instances in the test set key. ference in the overall recall results (30). The recall and F-score differences are statistically s</context>
<context position="18864" citStr="Carroll et al. (1999)" startWordPosition="3192" endWordPosition="3195">% 80.3% 188 (75.5%) Full Training Set, Object Relations used instead as additional training data for the individual methods that one is considering to combine, especially when the systems being combined were originally given a small amount of training data. To avoid the disadvantages of these existing schemes, we came up with a third method. We combine the existing related systems by taking a union of their translated annotations as the new initial GR annotation for our system. We rerun rule learning on the smaller (1391 GR instance) training set with a Union of the Buchholz et al. (1999) and Carroll et al. (1999) systems&apos; translated GR annotations. The overall results for the test set are (shown in row IaU) Smaller Set, Overall Results ER Training P F R IaU 496 (66.3%) 76.4% 71.0% 11% IaC 478 (63.9%) 77.2% 69.9% 7.7% IaB 466 (62.3%) 78.1% 69.3% 5.8% NI 448 (59.9%) 77.1% 67.4% where the other rows are as shown in Section 2.2. Compared to the F-score with using Carroll et al. (1999) (IaC), the IaU F-score is &amp;quot;borderline&amp;quot; statistically significantly better (11% significance level). The IaU F-score is statistically significantly better than the F-scores with either using Buchholz et al. (1999) (IaB) or no</context>
</contexts>
<marker>Carroll, Minnen, Briscoe, 1999</marker>
<rawString>J. Carroll, G. Minnen, and T. Briscoe. 1999. Corpus annotation for parser evaluation. In EACL99 workshop on Linguistically Interpreted Corpora (LINC&apos;99). cs.CL/9907013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cohen</author>
</authors>
<title>Empirical Methods for Artificial Intelligence.</title>
<date>1995</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="13201" citStr="Cohen, 1995" startWordPosition="2240" endWordPosition="2241"> well an existing system performs before the rules are run. In this paper, the baseline is the performance of the rules learned without When comparing differences in this paper, the statistical significance of the higher score being better than the lower score is tested with a one-sided test. Differences deemed statistically significant are significant at the 5% level. Differences deemed nonstatistically significant are not significant at the 10% level. For recall, we use a sign test for matched-pairs (Harnett, 1982, Sec. 15.5). For precision and F-score, a &amp;quot;matched-pairs&amp;quot; randomization test (Cohen, 1995, Sec. 5.3) is used. first using an existing system. If we were to use the same baseline as Palmer (1997), our baseline would be an F of 37.5% for IaB and 52.6% for IaC. This would result in a much higher ER of 51% and 36%, respectively. We now repeat our experiment with the full 1963 GR instance training set. These results indicate that as a small training set gets larger, the overall results get better and the initial annotations help less in improving the overall results. So the initial annotations are more helpful with smaller training sets. The overall results on the test set are Full Tra</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>P. Cohen. 1995. Empirical Methods for Artificial Intelligence. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ferro</author>
<author>M Vilain</author>
<author>A Yeh</author>
</authors>
<title>Learning transformation rules to find grammatical relations.</title>
<date>1999</date>
<booktitle>In Computational natural language learning (CoNLL-99),</booktitle>
<pages>43--52</pages>
<contexts>
<context position="2153" citStr="Ferro et al. (1999)" startWordPosition="342" endWordPosition="345">l editing. Three anonymous reviewers provided helpful suggestions. etc. GRs are the objects of study in relational grammar (Perlmutter, 1983). In the SPARKLE project (Carroll et al., 1997), GRs form the top layer of a three layer syntax scheme. Many systems (e.g., the KERNEL system (Palmer et al., 1993)) use GRs as an intermediate form when determining the semantics of syntactically parsed text. GRs are often stored in structures similar to the Fstructures of lexical-functional grammar (Kaplan, 1994). A complication is that different sets of GRs are useful for different purposes. For example, Ferro et al. (1999) is interested in semantic interpretation, and needs to differentiate between time, location and other modifiers. The SPARKLE project (Carroll et al., 1997), on the other hand, does not differentiate between these types of modifiers. As has been mentioned by John Carroll (personal communication), combining modifier types together is fine for information retrieval. Also, having less differentiation of the modifiers can make it easier to find them (Ferro et al., 1999). Furthermore, unless the desired set of GRs matches the set already annotated in some large training corpus,l one will have to ei</context>
<context position="5279" citStr="Ferro et al. (1999)" startWordPosition="867" endWordPosition="870">rison, our work handles many annotation label types, and the translation from the types used in the existing annotation system to the types in the small training set tends to be both more complicated and most easily determined by empirical means. Also, the type of baseline score being improved upon is different. Our work adds an existing system to improve the rules learned, while Palmer (1997) adds rules to improve an existing system&apos;s performance. We use this related system/small training set combination to improve the performance of the transformation-based error-driven learner described in Ferro et al. (1999). So far, this learner has started with a blank initial labeling of the GRs. This paper describes experiments where we replace this blank initial labeling with the output from an existing GR finder that is good at a somewhat different set of GR annotations. With each of the two existing GR finders that we use, we obtained improved results, with the improvement being more noticeable when the training set is smaller. We also find that the existing GR finders are quite uneven on how they improve the results. They each tend to concentrate on improving the recovery of a few kinds of relations, leav</context>
<context position="6524" citStr="Ferro et al. (1999)" startWordPosition="1082" endWordPosition="1085">s alone. We use this tendency to further boost the learner&apos;s performance by using a merger of these existing GR finders&apos; output as the initial labeling. 2 The Experiment We now improve the performance of the Ferro et al. (1999) transformation rule learner on a small annotated training set by using an existing system to provide initial GR annotations. This experiment is repeated on two different existing systems, which are reported in Buchholz et al. (1999) and Carroll et al. (1999), respectively. Both of these systems find a somewhat different set of GR annotations than the one learned by the Ferro et al. (1999) system. For example, the Buchholz et al. (1999) system ignores verb complements of verbs and is designed to look for relationships to verbs and not GRs that exist between nouns, etc. This system also handles relative clauses differently. For example, in &amp;quot;Miller, who organized ...&amp;quot;, this system is trained to indicate that &amp;quot;who&amp;quot; is the subject of &amp;quot;organized&amp;quot;, while the Ferro et al. (1999) system is trained to indicate that &amp;quot;Miller&amp;quot; is the subject of &amp;quot;organized&amp;quot;. As for the Carroll et al. (1999) system, among other things, it does not distinguish between subtypes of modifiers such as time, locat</context>
<context position="8255" citStr="Ferro et al. (1999)" startWordPosition="1380" endWordPosition="1383">ons produced by an existing system. This is how the existing system transmits what it found to the rule learner. The setup for this experiment is shown in figure 1. The four components with + signs are taken out when one wants the transformation rule learner to start with a blank set of initial GR annotations. The two arcs in that figure with a * indicate where the translations occur. These translations of the annotations produced by the existing system are basically just an attempt to map each type of annotation that it produces to the most likely type of corresponding annotation used in the Ferro et al. (1999) system. For example, in our experiments, the Buchholz et al. (1999) system uses the annotation np-sbj to indicate a subject, while the Ferro et al. (1999) system uses the annotation subj. We create the mapping by examining the training set to be given to the Ferro et al. (1999) system. For each type of relation ei output by the existing system when given the training set text, we look at what relation types (which tk&apos;s) co-occur with ei in the training set. We look at the tk&apos;s with the highest number of co-occurrences with that ei. If that tk is unique (no ties for the highest number of co-oc</context>
<context position="10387" citStr="Ferro et al. (1999)" startWordPosition="1745" endWordPosition="1748"> learned rules were applied, those F-scores increased to about 70%. An alternative to performing translations is to use the untranslated initial annotations as an additional type of input to the rule system. This alternative, which we have yet to try, has the advantage of fitting into the transformation-based error-driven paradigm (Brill and Resnik, 1994) more cleanly than having a translation stage. However, this additional type of input will also further slowdown an already slow rule-learning module. 2.2 Overall Results For our experiment, we use the same 1151 word (748 GR) test set used in Ferro et al. (1999), but for a training set, we use only a subset of the 3299 word training set used in Ferro et al. (1999). This subset contains 1391 (71%) of the 1963 GR instances in the original training set. The overall results for the test set are Smaller Set, Overall Results ER Training P F R IaC 478 (63.9%) 77.2% 69.9% 7.7% IaB 466 (62.3%) 78.1% 69.3% 5.8% NI 448 (59.9%) 77.1% 67.4% where row IaB is the result of using the rules learned when the Buchholz et al. (1999) system&apos;s translated GR annotations are used as the Initial Annotations, row IaC is the similar result with the Carroll et al. (1999) system</context>
<context position="15163" citStr="Ferro et al. (1999)" startWordPosition="2576" endWordPosition="2579">ions not having much of a change. Different existing systems seem to have different relations where most of the increase occurs. As an example, take the results of using the Buchholz et al. (1999) system on the 1391 GR instance training set. Many GRs, like possessive modifier, are not affected by the added initial annotations. Some GRs, like location modifier, do slightly better (as measured by the F-score) with the added initial annotations, but some, like subject, do better without. With GRs like subject, some differences between the initial and desired annotations may be too subtle for the Ferro et al. (1999) system to adjust for. Or those differences may be just due to chance, as the result differences in those GRs are not statistically significant. The GRs with statistically significant result differences are the time and &amp;quot;other&amp;quot;4 modifiers, where adding the initial annotations helps. The time modifiers results are quite different: The difference in the number recalled (15) for this GR accounts for nearly the entire difference in the overall recall results (18). The recall, precision and F-score differences are all statistically significant. Similarly, when using the Carroll et al. (1999) system</context>
<context position="20803" citStr="Ferro et al. (1999)" startWordPosition="3533" endWordPosition="3536">solute F-score (0.3% versus 1.1%) and ER (1.0% versus 3.3%). In fact, the F-score difference is small enough to not be statistically significant. Given the previous results for IaC and IaB as a small training set gets larger, this is not surprising. 3 Discussion GRs are important, but different sets of GRs are useful for different purposes and different systems are better at finding certain types of GRs. Here, we have been looking at ways of improving automatic GR finders when one has only a small amount of data with the desired GR annotations. In this paper, we improve the performance of the Ferro et al. (1999) GR transformation rule learner by using existing systems to find related sets of GRs. The output of these systems is used to supply initial sets of annotations for the rule learner. We achieve modest gains with the existing systems tried. When one examines the results, one notices that the gains tend to be uneven, with a few GR types having large gains, and the rest not being affected much. The different systems concentrate on improving different GR types. We leverage this tendency to make a further modest improvement in the overall results by providing the rule learner with the merged output</context>
</contexts>
<marker>Ferro, Vilain, Yeh, 1999</marker>
<rawString>L. Ferro, M. Vilain, and A. Yeh. 1999. Learning transformation rules to find grammatical relations. In Computational natural language learning (CoNLL-99), pages 43-52. EACL&apos;99 workshop,cs.CL/9906015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harnett</author>
</authors>
<title>Statistical Methods.</title>
<date>1982</date>
<publisher>AddisonWesley Publishing Co.,</publisher>
<location>Reading, MA, USA,</location>
<note>third edition.</note>
<contexts>
<context position="13111" citStr="Harnett, 1982" startWordPosition="2228" endWordPosition="2229"> are also computed using a different type of baseline. In Palmer (1997), the baseline is how well an existing system performs before the rules are run. In this paper, the baseline is the performance of the rules learned without When comparing differences in this paper, the statistical significance of the higher score being better than the lower score is tested with a one-sided test. Differences deemed statistically significant are significant at the 5% level. Differences deemed nonstatistically significant are not significant at the 10% level. For recall, we use a sign test for matched-pairs (Harnett, 1982, Sec. 15.5). For precision and F-score, a &amp;quot;matched-pairs&amp;quot; randomization test (Cohen, 1995, Sec. 5.3) is used. first using an existing system. If we were to use the same baseline as Palmer (1997), our baseline would be an F of 37.5% for IaB and 52.6% for IaC. This would result in a much higher ER of 51% and 36%, respectively. We now repeat our experiment with the full 1963 GR instance training set. These results indicate that as a small training set gets larger, the overall results get better and the initial annotations help less in improving the overall results. So the initial annotations are</context>
</contexts>
<marker>Harnett, 1982</marker>
<rawString>D. Harnett. 1982. Statistical Methods. AddisonWesley Publishing Co., Reading, MA, USA, third edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
<author>E Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: combining parsers.</title>
<date>1999</date>
<booktitle>In Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora (EMNLP/VLC&apos;99).</booktitle>
<contexts>
<context position="17317" citStr="Henderson and Brill, 1999" startWordPosition="2915" endWordPosition="2918">e time modifier F-score improvement stays statistically significant: When using the Carroll et al. (1999) system, the object F-score improvement stays statistically significant: 2.4 Combining Sets of Initial Annotations So the initial annotations from different existing systems tend to each concentrate on improving the performance of different GR types. From this observation, one may wonder about combining the annotations from these different systems in order to increase the performance on all the GR types affected by those different existing systems. Various works (van Halteren et al., 1998; Henderson and Brill, 1999; Wilkes and Stevenson, 1998) on combining different systems exist. These works use one or both of two types of schemes. One is to have the different systems simply vote. However, this does not really make use of the fact that different systems are better at handling different GR types. The other approach uses a combiner that takes the systems&apos; output as input and may perform such actions as determining which system to use under which circumstance. Unfortunately, this approach needs extra training data to train such a combiner. Such data may be more useful when R P F ER IaB 53% 80.6% 71.6% 29 </context>
</contexts>
<marker>Henderson, Brill, 1999</marker>
<rawString>J. Henderson and E. Brill. 1999. Exploiting diversity in natural language processing: combining parsers. In Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora (EMNLP/VLC&apos;99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
</authors>
<title>Supervised grammar induction using training data with limited constituent information.</title>
<date>1999</date>
<booktitle>In ACL&apos;99. cs.CL/9905001.</booktitle>
<contexts>
<context position="4229" citStr="Hwa (1999)" startWordPosition="699" endWordPosition="700"> a different way than in the small training and test set. A way to try to combine the different advantages of these small training data sets and existing systems which produce related annotations is to use a sequence of two systems. We first use an existing annotation system which can handle many of the less common features, i.e., those which do not appear in the small training set. We then train a second system with that same small training set to take the output of the first system and correct for the differences in annotations. This approach was used by Palmer (1997) for word segmentation. Hwa (1999) describes a somewhat similar approach for finding parse brackets which combines a fully annotated related training data set and a large but incompletely annotated final training data set. Both these works deal with just one (word boundary) or two (start and end parse bracket) annotation label types and the same label types are used in both the existing annotation system/training set and the final (small) training set. In comparison, our work handles many annotation label types, and the translation from the types used in the existing annotation system to the types in the small training set ten</context>
</contexts>
<marker>Hwa, 1999</marker>
<rawString>R. Hwa. 1999. Supervised grammar induction using training data with limited constituent information. In ACL&apos;99. cs.CL/9905001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
</authors>
<title>The formal architecture of lexical-functional grammar.</title>
<date>1994</date>
<editor>In M. Dalrymple, R. Kaplan, J. Maxwell III, and A. Zaenen, editors,</editor>
<publisher>Stanford University.</publisher>
<contexts>
<context position="2039" citStr="Kaplan, 1994" startWordPosition="323" endWordPosition="325">e experiments. Jun Wu provided some helpful explanations. Christine Doran and John Henderson provided helpful editing. Three anonymous reviewers provided helpful suggestions. etc. GRs are the objects of study in relational grammar (Perlmutter, 1983). In the SPARKLE project (Carroll et al., 1997), GRs form the top layer of a three layer syntax scheme. Many systems (e.g., the KERNEL system (Palmer et al., 1993)) use GRs as an intermediate form when determining the semantics of syntactically parsed text. GRs are often stored in structures similar to the Fstructures of lexical-functional grammar (Kaplan, 1994). A complication is that different sets of GRs are useful for different purposes. For example, Ferro et al. (1999) is interested in semantic interpretation, and needs to differentiate between time, location and other modifiers. The SPARKLE project (Carroll et al., 1997), on the other hand, does not differentiate between these types of modifiers. As has been mentioned by John Carroll (personal communication), combining modifier types together is fine for information retrieval. Also, having less differentiation of the modifiers can make it easier to find them (Ferro et al., 1999). Furthermore, u</context>
</contexts>
<marker>Kaplan, 1994</marker>
<rawString>R. Kaplan. 1994. The formal architecture of lexical-functional grammar. In M. Dalrymple, R. Kaplan, J. Maxwell III, and A. Zaenen, editors, Formal issues in lexical-functional grammar. Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3230" citStr="Marcus et al., 1993" startWordPosition="523" endWordPosition="526">et al., 1999). Furthermore, unless the desired set of GRs matches the set already annotated in some large training corpus,l one will have to either manually write rules to find the GRs, as done in Ait-Mokhtar and Chanod (1997), or annotate a new training corpus for the desired set. Manually writing rules is expensive, as is annotating a large corpus. Often, one may only have the resources to produce a small annotated training set, and many of the less common features of the set&apos;s &apos;One example is a memory-based GR finder (Buchholz et al., 1999) that uses the GRs annotated in the Penn Treebank (Marcus et al., 1993). domain may not appear at all in that set. In contrast are existing systems that perform well (probably due to a large annotated training set or a set of carefully hand-crafted rules) on related (but different) annotation standards. Such systems will cover many more domain features, but because the annotation standards are slightly different, some of those features will be annotated in a different way than in the small training and test set. A way to try to combine the different advantages of these small training data sets and existing systems which produce related annotations is to use a seq</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>R Passonneau</author>
<author>C Weir</author>
<author>T Finin</author>
</authors>
<title>The kernel text understanding system.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--17</pages>
<contexts>
<context position="1838" citStr="Palmer et al., 1993" startWordPosition="290" endWordPosition="293">he motivation to find GRs. Warren Greiff suggested using randomization-type techniques to determine statistical significance. Sabine Buchholz and John Carroll ran their GR finding systems over our data for the experiments. Jun Wu provided some helpful explanations. Christine Doran and John Henderson provided helpful editing. Three anonymous reviewers provided helpful suggestions. etc. GRs are the objects of study in relational grammar (Perlmutter, 1983). In the SPARKLE project (Carroll et al., 1997), GRs form the top layer of a three layer syntax scheme. Many systems (e.g., the KERNEL system (Palmer et al., 1993)) use GRs as an intermediate form when determining the semantics of syntactically parsed text. GRs are often stored in structures similar to the Fstructures of lexical-functional grammar (Kaplan, 1994). A complication is that different sets of GRs are useful for different purposes. For example, Ferro et al. (1999) is interested in semantic interpretation, and needs to differentiate between time, location and other modifiers. The SPARKLE project (Carroll et al., 1997), on the other hand, does not differentiate between these types of modifiers. As has been mentioned by John Carroll (personal com</context>
</contexts>
<marker>Palmer, Passonneau, Weir, Finin, 1993</marker>
<rawString>M. Palmer, R. Passonneau, C. Weir, and T. Finin. 1993. The kernel text understanding system. Artificial Intelligence, 63:17-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Palmer</author>
</authors>
<title>A trainable rule-based algorithm for word segmentation.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL/EACL97.</booktitle>
<contexts>
<context position="4195" citStr="Palmer (1997)" startWordPosition="694" endWordPosition="695">f those features will be annotated in a different way than in the small training and test set. A way to try to combine the different advantages of these small training data sets and existing systems which produce related annotations is to use a sequence of two systems. We first use an existing annotation system which can handle many of the less common features, i.e., those which do not appear in the small training set. We then train a second system with that same small training set to take the output of the first system and correct for the differences in annotations. This approach was used by Palmer (1997) for word segmentation. Hwa (1999) describes a somewhat similar approach for finding parse brackets which combines a fully annotated related training data set and a large but incompletely annotated final training data set. Both these works deal with just one (word boundary) or two (start and end parse bracket) annotation label types and the same label types are used in both the existing annotation system/training set and the final (small) training set. In comparison, our work handles many annotation label types, and the translation from the types used in the existing annotation system to the t</context>
<context position="12397" citStr="Palmer (1997)" startWordPosition="2115" endWordPosition="2116">nds for Error Reduction. It indicates how much adding the initial annotations reduced the missing F-score, where the missing F-score is 100%−F. ER 100%X(FIA−FNI)/(100%−FNI), where FNI is the F-score for the NI row, and FIA is the F-score for using the Initial Annotations of interest. Here, the differences in recall and Fscore between NI and either IaB or IaC (but not between IaB and IaC) are statistically significant. The differences in precision is not.� In these results, most of the modest F-score gain came from increasing recall. One may note that the error reductions here are smaller than Palmer (1997)&apos;s error reductions. Besides being for different tasks (word segmentation versus GRs), the reductions are also computed using a different type of baseline. In Palmer (1997), the baseline is how well an existing system performs before the rules are run. In this paper, the baseline is the performance of the rules learned without When comparing differences in this paper, the statistical significance of the higher score being better than the lower score is tested with a one-sided test. Differences deemed statistically significant are significant at the 5% level. Differences deemed nonstatistically</context>
</contexts>
<marker>Palmer, 1997</marker>
<rawString>D. Palmer. 1997. A trainable rule-based algorithm for word segmentation. In Proceedings of ACL/EACL97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Perlmutter</author>
</authors>
<date>1983</date>
<booktitle>Studies in Relational Grammar 1. U.</booktitle>
<publisher>Chicago Press.</publisher>
<contexts>
<context position="1675" citStr="Perlmutter, 1983" startWordPosition="263" endWordPosition="264">-loc→ the ball ∗ This paper reports on work performed at the MITRE Corporation under the support of the MITRE Sponsored Research Program. Marc Vilain provided the motivation to find GRs. Warren Greiff suggested using randomization-type techniques to determine statistical significance. Sabine Buchholz and John Carroll ran their GR finding systems over our data for the experiments. Jun Wu provided some helpful explanations. Christine Doran and John Henderson provided helpful editing. Three anonymous reviewers provided helpful suggestions. etc. GRs are the objects of study in relational grammar (Perlmutter, 1983). In the SPARKLE project (Carroll et al., 1997), GRs form the top layer of a three layer syntax scheme. Many systems (e.g., the KERNEL system (Palmer et al., 1993)) use GRs as an intermediate form when determining the semantics of syntactically parsed text. GRs are often stored in structures similar to the Fstructures of lexical-functional grammar (Kaplan, 1994). A complication is that different sets of GRs are useful for different purposes. For example, Ferro et al. (1999) is interested in semantic interpretation, and needs to differentiate between time, location and other modifiers. The SPAR</context>
</contexts>
<marker>Perlmutter, 1983</marker>
<rawString>D. Perlmutter. 1983. Studies in Relational Grammar 1. U. Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Improving data driven wordclass tagging by system combination.</title>
<date>1998</date>
<booktitle>In COLING-ACL&apos;98,</booktitle>
<pages>491--497</pages>
<location>Montréal, Canada.</location>
<marker>van Halteren, Zavrel, Daelemans, 1998</marker>
<rawString>H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Improving data driven wordclass tagging by system combination. In COLING-ACL&apos;98, pages 491-497, Montréal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilkes</author>
<author>M Stevenson</author>
</authors>
<title>Word sense disambiguation using optimized combinations of knowledge sources.</title>
<date>1998</date>
<booktitle>In COLING-ACL&apos;98,</booktitle>
<pages>1398--1402</pages>
<location>Montréal, Canada.</location>
<contexts>
<context position="17346" citStr="Wilkes and Stevenson, 1998" startWordPosition="2919" endWordPosition="2922">rovement stays statistically significant: When using the Carroll et al. (1999) system, the object F-score improvement stays statistically significant: 2.4 Combining Sets of Initial Annotations So the initial annotations from different existing systems tend to each concentrate on improving the performance of different GR types. From this observation, one may wonder about combining the annotations from these different systems in order to increase the performance on all the GR types affected by those different existing systems. Various works (van Halteren et al., 1998; Henderson and Brill, 1999; Wilkes and Stevenson, 1998) on combining different systems exist. These works use one or both of two types of schemes. One is to have the different systems simply vote. However, this does not really make use of the fact that different systems are better at handling different GR types. The other approach uses a combiner that takes the systems&apos; output as input and may perform such actions as determining which system to use under which circumstance. Unfortunately, this approach needs extra training data to train such a combiner. Such data may be more useful when R P F ER IaB 53% 80.6% 71.6% 29 (64.4%) NI 56.0% 40.0% 14 (31</context>
</contexts>
<marker>Wilkes, Stevenson, 1998</marker>
<rawString>Y. Wilkes and M. Stevenson. 1998. Word sense disambiguation using optimized combinations of knowledge sources. In COLING-ACL&apos;98, pages 1398-1402, Montréal, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>