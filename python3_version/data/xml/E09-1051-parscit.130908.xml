<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000309">
<title confidence="0.9990945">
Optimization in Coreference Resolution Is Not Needed:
A Nearly-Optimal Algorithm with Intensional Constraints
</title>
<author confidence="0.992237">
Manfred Klenner &amp; ´Etienne Ailloud
</author>
<affiliation confidence="0.902037">
Computational Linguistics
Zurich University, Switzerland
</affiliation>
<email confidence="0.993829">
{klenner, ailloud}@cl.uzh.ch
</email>
<sectionHeader confidence="0.997218" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999840266666667">
We show how global constraints such as transitiv-
ity can be treated intensionally in a Zero-One Inte-
ger Linear Programming (ILP) framework which is
geared to find the optimal and coherent partition of
coreference sets given a number of candidate pairs
and their weights delivered by a pairwise classifier
(used as reliable clustering seed pairs). In order to
find out whether ILP optimization, which is NP-
complete, actually is the best we can do, we com-
pared the first consistent solution generated by our
adaptation of an efficient Zero-One algorithm with
the optimal solution. The first consistent solution,
which often can be found very fast, is already as
good as the optimal solution; optimization is thus
not needed.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980147058824">
One of the main advantages of Integer Linear Pro-
gramming (ILP) applied to NLP problems is that
prescriptive linguistic knowledge can be used to
pose global restrictions on the set of desirable so-
lutions. ILP tries to find an optimal solution while
adhering to the global constraints. One of the
central global constraints in the field of corefer-
ence resolution evolves from the interplay of intra-
sentential binding constraints and the transitivity
of the anaphoric relation. Consider the following
sentence taken from the Internet: ’He told him that
he deeply admired him’. ’He’ and ’him’ are ex-
clusive (i.e. they could never be coreferent) within
their clauses (the main and the subordinate clause,
respectively). A pairwise classifier could learn this
given appropriate features or, alternatively, bind-
ing constraints could act as a hard filter preventing
such pairs from being generated at all. But in ei-
ther case, since pairwise classification is trapped
in its local perspective, nothing can prevent the
classifier to resolve the ’he’ and ’him’ from the
subordinate clause in two independently carried
out steps to the same antecedent from the main
clause. It is transitivity that prohibits such an as-
signment: if two elements are both coreferent to
a common third element, then the two are (transi-
tively given) coreferent as well. If they are known
to be exclusive, such an assignment is disallowed.
But transitivity is beyond the scope of pairwise
classification—it is a global phenomena. The so-
lution is to take ILP as a clustering device, where
the probabilities of the pairwise classifier are in-
terpreted as weights and transitivity and other re-
strictions are acting as global constraints.
Unfortunately, in an ILP program every con-
straint has to be extensionalized (i.e. all instantia-
tions of the constraint are to be generated). Cap-
turing transitivity for e.g. 150 noun phrases (about
30 sentences) already produces 1,500,000 equa-
tions (cf. Section 4). Solving such ILP programs
is far too slow for real applications (let alone its
brute force character).
A closer look at existing ILP approaches to NLP
reveals that they are of a special kind, namely
Zero-One ILP with unweighted constraints. Al-
though still NP-complete there exist a number of
algorithms such as the Balas algorithm (Balas,
1965) that efficiently explore the search space and
reduce thereby run time complexity in the mean.
We have adapted Balas’ algorithm to the special
needs of coreference resolution. First and fore-
most, this results in an optimization algorithm that
treats global constraints intensionally, i.e. that
generates instantiations of a constraint only on de-
mand. Thus, transitivity can be captured for even
the longest texts. But more important, we found
out empirically that ’full optimization’ is not re-
ally needed. The first consistent solution, which
often can be found very fast, is already as good—
in terms of F-measure values—as the optimal so-
lution. This is good news, since it reduces runtime
and at same time maintains the empirical results.
We first introduce Zero-One ILP, discuss our
baseline model and give an ILP formalization of
coreference resolution. Then we go into the de-
tails of our Balas adaptation and provide empiri-
cal evidence for our central claim—that optimiza-
tion search can already be stopped (without qual-
</bodyText>
<note confidence="0.923017">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 442–450,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.996874">
442
</page>
<bodyText confidence="0.9923615">
ity loss) when the first consistent solution has been
found.
</bodyText>
<sectionHeader confidence="0.7439855" genericHeader="introduction">
2 Zero-One Integer Linear
Programming (ILP)
</sectionHeader>
<bodyText confidence="0.9998635625">
The algorithm in (Balas, 1965) solves Zero-
One Integer Linear Programming (ILP), where a
weighted linear function (the objective function)
of binary variables F(x1,...,xn) = w1x1 + ... +
wnxn is to be minimized under the regiment of
linear inequalities a1x1 +... + anxn &gt; A.1 Unlike
its real-valued counterpart, Zero-One ILP is NP-
complete (cf., say, (Papadimitriou and Steiglitz,
1998)), but branch-and-bound algorithms with ef-
ficient heuristics exist, as the Balas Algorithm:
Balas (1965) proposes an approach where the ob-
jective function’s addends are sorted according to
the magnitude of the weights: 0 &lt; w1 &lt; ... &lt;
wn. This preliminary ordering induces the follow-
ing functioning principles for the algorithm (see
(Chinneck, 2004, Chap. 13) for more details):
</bodyText>
<listItem confidence="0.7825202">
1. It seeks to minimize F, so that a solution with
as few 1s as possible is preferred.
2. If, during exploration of solutions, con-
straints force an xi to be set to 1, then it should
bear as small an index as possible.
</listItem>
<bodyText confidence="0.996391227272727">
The Balas algorithm follows a depth-first search
while checking feasibility (i.e., through the con-
straints) of the branches partially explored: Upon
branching, the algorithm bounds the cost of set-
ting the current variable xN to 1 by the costs ac-
cumulated so far: w1x1 +... +wN_1xN_1 +wN is
now the lowest cost this branch may yield. If,
on the contrary, xN is set to 0, a violated &gt;-
constraint may only be satisfied via an xi set to 1
(i &gt; N), so the cheapest change to ameliorate the
partial solution is to set the right-next variable to
1: w1x1 + ... + wN_1xN_1 + wN+1 would be the
cheapest through this branch.
If setting all weights past the branching variable
to 0 yields a cheaper solution than the so far mini-
mal solution obtained, then it is worthwile explor-
ing this branch, and the algorithms goes on to the
next weighted variable, until it reaches a feasible
solution; otherwise it backtracks to the last unex-
plored branching. The complexity thus remains
exponential in the worst case, but the initial order-
ing of weights is a clever guide.
</bodyText>
<tableCaption confidence="0.550883">
1Maximization and coping with &lt;-constraints are also ac-
cessible via simple transformations.
</tableCaption>
<sectionHeader confidence="0.991826" genericHeader="method">
3 Our Baseline Model
</sectionHeader>
<bodyText confidence="0.985288">
The memory-based learner TiMBL (Daelemans
et al., 2004) is used as a (pairwise) classifier.
TiMBL stores all training examples, learns fea-
ture weights and classifies test instances accord-
ing to the majority class of the k-nearest (i.e. most
</bodyText>
<listItem confidence="0.838242785714286">
similar) neighbors. We have experimented with
various features; Table 1 lists the set we have fi-
nally used (Soon et al. (2001) and Ng and Cardie
(2002) more thoroughly discuss different features
and their benefits):
- distance in sentences and markables
- part of speech of the head of the markables
- the grammatical functions
- parallelism of grammatical functions
- do the heads match or not
- where is the pronoun (if any): left or right
- word form if POS is pronoun
- salience of the non-pronominal phrases
- semantic class of noun phrase heads
</listItem>
<tableCaption confidence="0.965124">
Table 1: Features for Pairwise Classification
</tableCaption>
<bodyText confidence="0.999818111111111">
As a gold standard the T¨uBa-D/Z (Telljohann
et al., 2005; Naumann, 2006) coreference corpus
is used. The T¨uBa is a treebank (1,100 German
newspaper texts, 25,000 sentences) augmented
with coreference annotations2. In total, there are
13,818 anaphoric, 1,031 cataphoric and 12,752
coreferential relations. There are 3,295 relative
pronouns, 8,929 personal pronouns, 2,987 reflex-
ive pronouns, and 3,921 possessive pronouns.
There are some rather long texts in the T¨uBa
corpus. Which pair generation algorithm is rea-
sonable? Should we pair every markable (even
from the beginning of the text) with every other
succeeding markable? This is linguistically im-
plausible. Pronouns are acting as a kind of local
variables. A ’he’ at the beginning of a text and
a second distant ’he’ at the end of the text hardly
tend to corefer, except if there is a long chain of
coreference ’renewals’ that lead somehow from
the first ’he’ to the second ’he’. But the plain ’he&apos;-
&apos;he&apos;
he&apos;-
&apos;he&apos; pair does not reliably indicate coreference.
A smaller window seems to be appropriate. We
have experimented with various window sizes and
found that a size of 3 sentences worked best.
Candidate pairs are generated only within that
</bodyText>
<footnote confidence="0.9291435">
2Recently, a new version of the T¨uBa was released with
35,000 sentences with coreference annotations.
</footnote>
<page confidence="0.99916">
443
</page>
<bodyText confidence="0.9980915">
window, which is moved sentence-wise over the
whole text.
</bodyText>
<sectionHeader confidence="0.994509" genericHeader="method">
4 Our Constraint-Based Model
</sectionHeader>
<bodyText confidence="0.999799">
The output of the TiMBL classifier is the input
to the optimization step, it provides the set of
variables and their weights. In order to utilize
TiMBL’s classification results as weights in a min-
imization task, we have defined a measure called
classification costs (see Fig. 1).
</bodyText>
<equation confidence="0.731103666666667">
 |negij |
wi j =
 |negij ∪ posij |
</equation>
<figureCaption confidence="0.996981">
Figure 1: Score for Classification Costs
</figureCaption>
<bodyText confidence="0.999623176470588">
 |negij  |( |posij |) denotes the number of instances
similar (according to TiMBL’s metric) to hi, ji that
are negative (positive) examples. If no negative in-
stances are found, a safe positive classification de-
cision is proposed at zero cost. Accordingly, the
cost of a decision without any positive instances is
high, namely one. If both sets are non-empty, the
ratio of the negative instances to the total of all in-
stances is taken. For example, if TiMBL finds 10
positive and 5 negative examples similar to the yet
unclassified new example hi, ji the cost of a posi-
tive classification is 5/15 while a negative classifi-
cation costs 10/15.
We introduce our model in an ILP style. In sec-
tion 6 we discuss our Balas adaptation which al-
lows us to define constraints intensionally.
The objective function is:
</bodyText>
<equation confidence="0.987288">
min: E wij · cij + (1− wij) · cji (1)
hi,ji∈ O0.5
</equation>
<bodyText confidence="0.9996911">
O0.5 is the set of pairs hi, ji that have received
a weight ≤ 0.5 according to our weight function
(see Fig. 1). Any binary variable cij combines the
ith markable (of the text) with the jth markable
(i &lt; j) within a fixed window3.
cji represents the (complementary) decision that
i and j are not coreferent. The weight of this
decision is (1 − wij). Please note that every op-
timization model of coreference resolution must
include both variables4. Otherwise optimization
</bodyText>
<footnote confidence="0.980208">
3As already discussed, the window is realized as part of
the vector generation component, so O0.5 automatically only
captures pairs within the window.
4Even if an anaphoricity classifier is used.
</footnote>
<bodyText confidence="0.998008176470588">
would completely ignore the classification deci-
sions of the pairwise classifier (i.e., that ≤ 0.5 sug-
gests coreference). For example, the choice not
to set cij = 1 at costs wij ≤ 0.5 must be sanc-
tioned by instantiating its inverse variable cji = 1
and adding (1 − wij) to the objective function’s
value. Otherwise minimization would turn—in
the worst case—everything to be non-coreferent,
while maximization would preferentially set ev-
erything to be actually coreferent (as long as no
constraints are violated, of course).5
The first constraint then is:
cij +cji = 1, ∀hi, ji ∈ O0.5 (2)
A pair hi, ji is either coreferent or not.
Transitivity is captured by (see (Finkel and
Manning, 2008) for an alternative but equivalent
formalization):
</bodyText>
<equation confidence="0.838210333333333">
cij + cjk ≤ cik + 1, ∀i, j,k (i &lt; j &lt; k)
cik + cjk ≤ cij + 1, ∀i, j,k (i &lt; j &lt; k) (3)
cij +cik ≤ cjk + 1, ∀i, j,k (i &lt; j &lt; k)
</equation>
<bodyText confidence="0.998803625">
In order to take full advantage of ILP’s reason-
ing capacities, three equations are needed given
three markables. The extensionalization of tran-
sitivity thus produces n!
3!(n−3)! · 3 equations for n
markables. Note that transitivity—as a global
constraint—ought to spread over the whole can-
didate set, not just within in the window.
Transitivity without further constraints is point-
less.6 What we really can gain from transitivity is
consistency at the linguistic level, namely (glob-
ally) adhering to exclusiveness constraints (cf. the
example in the introduction). We have defined two
predicates that replace the traditional c-command
(which requires full syntactical analysis) and ap-
proximate it: clause bound and np bound.
Two mentions are clause-bound if they occur in
the same subclause, none of them being a reflex-
ive or a possessive pronoun, and they do not form
an apposition. There are only 16 cases in our data
set where this predicate produces false negatives
(e.g. in clauses with predicative verbs: ’Hei is still
prime ministeri’). We currently regard this short-
coming as noise.
</bodyText>
<footnote confidence="0.999017333333333">
5The need for optimization or other numerical preference
mechanisms originates from the fact that coreference reso-
lution is underconstrained—due to the lack of a deeper text
understanding.
6Although it might lead to a reordering of coreference sets
by better ’balancing the weights’.
</footnote>
<page confidence="0.998095">
444
</page>
<bodyText confidence="0.941805461538462">
Two markables that are clause-bound (in the
sense defined above) are exclusive, i.e.
cij = 0, Vi, j (clause bound(i, j)). (4)
A possessive pronoun is exclusive to all markables
in the noun phrase it is contained in (e.g. cij = 0
given a noun phrase “[heri managerj]”), but might
get coindexed with markables outside of such a lo-
cal context (“Annei talks to heri manager”). We
define a predicate np bound that is true of two
markables, if they occur in the same noun phrase.
In general, two markables that np-bind each other
are exclusive:
cij = 0, Vi, j (np bound(i, j)) (5)
</bodyText>
<sectionHeader confidence="0.996129" genericHeader="method">
5 Representing ILP Constraints
Intensionally
</sectionHeader>
<bodyText confidence="0.999693307692308">
Existing ILP-based approaches to NLP (e.g. (Pun-
yakanok et al., 2004; Althaus et al., 2004;
Marciniak and Strube, 2005)) belong to the
class of Zero-One ILP: only binary variables are
needed. This has been seldom remarked (but
see (Althaus et al., 2004)) and generic (out-of-
the-box) ILP implementations are used. More-
over, these models form a very restricted variant of
Zero-One ILP: the constraints come without any
weights. The reason for this lies in the logical na-
ture of NLP constraints. For example in the case of
coreference, we have the following types of con-
straints:
</bodyText>
<listItem confidence="0.965870666666667">
1. exclusivity of two instantiations (e.g. either
coreferent or not, equation 2)
2. dependencies among three instantiations
(transitivity: if two are coreferent then so the
third, equation 3)
3. the prohibition of pair instantiation (binding
constraints, equations 4 and 5)
4. enforcement of at least one instantiation of a
markable in some pair (equation 6 below).
</listItem>
<bodyText confidence="0.999912333333333">
We call the last type of constraints ’boundness en-
forcement constraints’. Only two classes of pro-
nouns strictly belong to this class: relative (POS
label ’PRELS’) and possessive pronouns (POS
label ’PPOSAT’)7. The corresponding ILP con-
straint is, e.g. for possessive pronouns:
</bodyText>
<equation confidence="0.7974525">
� cij &gt; 1, Vj s.t. pos(j) = &apos;PPOSAT&apos; (6)
i
</equation>
<footnote confidence="0.985069">
7In rare cases, even reflexive pronouns are (correctly)
used non-anaphorically, and, more surprisingly, 15% of the
personal pronouns in the T¨uBa are used non-anaphorically.
</footnote>
<bodyText confidence="0.999681176470588">
Note that boundness enforcement constraints lead
to exponential time in the worst case. Given that
such a constraint holds on a pair with the highest
costs of all pairs (thus being the last element of
the Balas ordered list with n elements): in order to
prove whether it can be bound (set to one), 2n (bi-
nary) variable flips need to be checked in the worst
case. All other constraints can be satisfied by set-
ting some cij = 0 (i.e. non-coreferent) which does
not affect already taken or (any) yet to be taken
assignments. Although exponential in the worst
case, the integration of constraint (6) has slowed
down CPU time only slightly in our experiments.
A closer look at these constraints reveals that
most of them can be treated intensionally in an
efficient manner. This is a big advantage, since
now transitivity can be captured even for long texts
(which is infeasible for most generic ILP models).
To intensionally capture transitivity, we only
need to explicitly maintain the evolving corefer-
ence sets. If a new markable is about to enter a
set (e.g. if it is related to another markable that is
already member of the set) it is verified that it is
compatible with all members of the set.
A markable i is compatible with a coreference
set if, for all members j of the set, (i, j) does
not violate binding constraints, agrees morpholog-
ically and semantically. Morphological agreement
depends on the POS tags of a pair. Two personal
pronouns must agree in person, number and gen-
der. In German, a possessive pronoun must only
agree in person with its antecedent. Two nouns
might even have different grammatical gender, so
no morphological agreement is checked here.
Checking binding for the clause bound con-
straint is simple: each markable has a subclause ID
attached (extracted from the T¨uBa). If two mark-
ables (except reflexive or possessive pronouns)
share an ID they are exclusive. Possessive pro-
nouns must not be np-bound. All members of the
noun phrase containing the possessive pronoun are
exclusive to it.
Note that such a representation of constraints is
intensional since we need not enumerate all exclu-
sive pairs as an ILP approach would have to. We
simply check (on demand) the identity of IDs.
There is also no need to explicitly maintain con-
straint (2), either, which states that a pair is either
coreferent or not. In the case that a pair cannot be
set to 1 (representing coreference), it is set to 0;
i.e. cij and cji are represented by the same index
</bodyText>
<page confidence="0.996571">
445
</page>
<bodyText confidence="0.999745333333333">
position p of a Balas solution v (cf. Section 6); no
extensional modelling is necessary.
Although our special-purpose Balas adaptation
no longer constitutes a general framework that can
be fed with each and every Zero-One ILP formal-
ization around, the algorithm is simple enough to
justify this. Even if one uses an ILP translator such
as Zimpl8, writing a program for a concrete ILP
problem quickly becomes comparably complex.
</bodyText>
<sectionHeader confidence="0.992999" genericHeader="method">
6 A Variant of the Balas Algorithm
</sectionHeader>
<bodyText confidence="0.999949763157895">
Our algorithm proceeds as follows: we generate
the first consistent solution according to the Balas
algorithm (Balas-First, henceforth). The result is a
vector v of dimension n, where n is the size of O0.5.
The dimensions take binary values: a value 1 at
position p represents the decision that the pth pair
cij from the (Balas-ordered) objective function is
coreferent (0 indicates non-coreference). One mi-
nor difference to the original Balas algorithm is
that the primary choice of our algorithm is to set a
variable to 1, not to 0—thus favoring coreference.
However, in our case, 1 is the cheapest solution
(with cost wij &lt; 0.5). Setting a variable to zero
has cost 1 − wij which is more expensive in any
case. But aside from this assignment convention,
the principal idea is preserved, namely that the as-
signment is guided by lowest cost decisions.
The search for less expensive solutions is done
a bit differently from the original. The Balas algo-
rithm takes profit from weighted constraints. As
discussed in Section 5, constraints in existing ILP
models for NLP are unweighted. Another differ-
ence is that in the case of coreference resolution
both decisions have costs: setting a variable to 1
(wij) and setting it to 0 (1−wij). This is the key to
our cost function that guides the search.
Let us first make some properties of the search
space explicit. First of all, given no constraints
were violated, the optimal solution would be the
one with all pairs from O0.5 set to 1 (since any 0
would add a suboptimal weight, namely 1− wij).
Now we can see that any less expensive solution
than Balas-First must be longer than Balas-First,
where the length (1-length, henceforth) of a Balas
solution is defined as the number of dimensions
with value 1. A shorter solution would turn at least
a single 1 into 0, which leads to a higher objective
function value.
</bodyText>
<footnote confidence="0.793866">
8http://zimpl.zib.de/
</footnote>
<bodyText confidence="0.998180162162162">
Any solution with the same 1-length is more ex-
pensive since it requires swapping a 1 to 0 at one
position and a 0 to 1 at a farther position. The per-
mutation of 1/0s from Balas-First is induced by
the weights and the constraints. A 0 at position q
is forced by (a constraint together with) some (or
more) 1 at position p (p &lt; q). Thus, we can only
swap a 0 to 1 if we swap at least one preceding 1
to 0. The costs of swapping a preceding 1 to 0 are
higher than the gain from swapping the 0 to 1 (as
a consequence of the Balas ordering). So no solu-
tion with the same 1-length can be less expensive
than Balas-First.
We then have to search for solutions with higher
1-length. In Section 7 we will argue that this actu-
ally goes in the wrong direction.
Any longer solution must swap—for every 1
swapped to 0—at least two 0s to 1. Otherwise the
costs are higher than the gain. We can utilize this
for a reduction of the search space.
Let p be a position index of Balas-First (v),
where the value of the dimension at p is 1 and
there exist at least two 0s with position indices
q &gt; p.
Consider v = (1,0,1,1,0,0). Positions 1, 3
and 4 are such positions (identifying the follow-
ing parts of v resp.: (1,0,1,1,0,0),(1,1,0,0) and
(1,0,0)).
We define a projection c(p) that returns the
weight wij of the pth pair cij from the Balas or-
dering. v(p) is the value of dimension p in v (0 or
1). The cost of swapping 1 at position p to 0 is the
difference between the cost of cji (1 − c(p)) and
cij (c(p)): costs(p) = 1− 2 · c(p).
We define the potential gain pg(p) of swapping
a 1 at position p to 0 and every succeeding 0 to 1
by:
</bodyText>
<equation confidence="0.872295">
pg(p) = costs(p) −� 1− 2 · c(q) (7)
q&gt;p s.t. v(q)=0
</equation>
<bodyText confidence="0.999722">
For example, let v = (1,0,1,1,0,0), p = 4,
c(4) = 0.2 and (the two 0s) c(5) = 0.3, c(6) =
0.35. costs(4) = 1− 0.4 = 0.6 and pg(4) = 0.6 −
(0.4+0.3) = −0.1. Even if all 0s (after position 4)
can be swapped to 1, the objective function value
is lower that before, namely by 0.1. Thus, we need
not consider this branch.
In general, each time a 0 is turned into 1, the
potential gain is preserved, but if we have to turn
another 1 to 0 (due to a constraint), or if a 0 cannot
be swapped to 1, the potential gain is decremented
</bodyText>
<page confidence="0.998338">
446
</page>
<bodyText confidence="0.9922425">
by a certain cost factor. If the potential gain is
exhausted that way, we can stop searching.
</bodyText>
<sectionHeader confidence="0.8602975" genericHeader="method">
7 Is Optimization Really Needed?
Empirical Evidence
</sectionHeader>
<bodyText confidence="0.970709185185185">
The first observation we made when running our
algorithm was that in more than 90% of all cases,
Balas-First already constitutes the optimal solu-
tion. That is, the time-consuming search for a less
expensive solution ended without further success.
As discussed in Section 6, any less expensive
solution must be longer (1-length) than Balas-
First. But can longer solutions be better (in terms
of F-measure scores) than shorter ones? They
might: if the 1-length re-assignment of variables
removes as much false positives as possible and
raises instead as much of the true positives as can
be found in O0.5. Such a solution might have a bet-
ter F-measure score. But what about its objective
function value? Is it less expensive than Balas-
First?
We have designed an experiment with all (true)
coreferent pairs from O0.5 (as indicated by the gold
standard) set to 1. Note that this is just another
kind of constraints: the enforcement of corefer-
ence (this time extensionally given).
The result was surprising: The objective func-
tion values that our algorithm finds under these
constraints were in any case higher than Balas-
First without that constraint.
Fig. 2 illustrates this schematically (Fig. 4 be-
low justifies the curve’s shape). The curve rep-
</bodyText>
<figureCaption confidence="0.997222">
Figure 2: The best solution is ’less optimal’
</figureCaption>
<bodyText confidence="0.994415256410256">
resents a function mapping objective values to F-
measure scores. Note that it is not monotoni-
cally decreasing (from lower objective values to
higher ones)—as one would expect (less expensive
= higher F-measure). The vertical line labelled b
identifies Balas-First. Starting with Balas-First,
optimization searches to the left, i.e. searching
for smaller objective function values. The hori-
zontal line labelled m shows the local maximum
of that search region (the arrow from left points to
it). But unfortunately, the global maximum (the
arrow from right), i.e. the 1-length solution with
all (true) coreferent pairs set to 1, lies to the right-
hand side of Balas-First.
This indicates that, in our experimental con-
ditions, optimization efforts can never reach the
global maximum, but it also indicates that search-
ing for less expensive solutions nevertheless might
lead (at least) to a local maximum. However, if
it is true that the goal function is not monotonic,
there is no guarantee that the optimal solution ac-
tually constitutes the local maximum, i.e. the best
solution in terms of F-measure scores.
Unfortunately, we cannot prove mathematically
any hypotheses about the optimal values and their
behavior. However, we can compare the opti-
mal value’s F-measure scores to the Balas-First
F-measure scores empirically. Two experiments
were designed to explore this. In the first exper-
iment, we computed for each text the difference
between the F-measure value of the optimal so-
lution and the F-measure value of Balas-First. It
is positive if the optimal solution has higher F-
measure score than Balas-First and negative oth-
erwise. This was done for each text (99) that has
more than one objective function value (remem-
ber that in more than 90% of texts Balas-First was
already the optimal solution).
Fig. 3 shows the results. The horizontal line is
</bodyText>
<figureCaption confidence="0.995236">
Figure 3: Balas-First or Optimal Solution
</figureCaption>
<bodyText confidence="0.996607">
separating gain from loss. Points above it indicate
that the optimal solution has a better F-measure
score, points below indicate a loss in percentage
</bodyText>
<page confidence="0.997101">
447
</page>
<bodyText confidence="0.99979">
(for readability, we have drawn a curve). Taking
the mean of loss and gain across all texts, we found
that the optimal solution shows no significant F-
measure difference with the Balas-First solution:
the optimal solution even slightly worsens the F-
measure compared to Balas-First by −0.086%.
The second experiment was meant to explore
the curve shape of the goal function that maps
an objective function value to a F-measure value.
This is shown in Fig. 4. The values of that func-
tion are empirically given, i.e. they are produced
by our algorithm. The x-axis shows the mean of
the nth objective function value better than Balas-
First. The y-value of the nth x-value thus marks the
effect (positive or negative) in F-measure scores
while proceeding to find the optimal solution. As
can be seen from the figure, the function (at least
empirically) is rather erratic. In other words,
searching for the optimal solution beyond Balas-
First does not seem to lead reliably (and monoton-
ically) to better F-measure values.
</bodyText>
<figureCaption confidence="0.995229">
Figure 4: 1st Compared to Balas-nth Value
</figureCaption>
<bodyText confidence="0.999929">
In the next section, we show that Balas-First as
the first optimization step actually is a significant
improvement over the classifier output. So we are
not saying that we should dispense with optimiza-
tion efforts completely.
</bodyText>
<sectionHeader confidence="0.978999" genericHeader="method">
8 Does Balas-First help? Empirical
Evidence
</sectionHeader>
<bodyText confidence="0.999787391304348">
Besides the empirical fact that Balas-First slightly
outperforms the optimal solution, we must demon-
strate that Balas-First actually improves the base-
line. Our experiments are based on a five-fold
cross-validation setting (1100 texts from the T¨uBa
coreference corpus). Each experiment was carried
out in two variants. One where all markables have
been taken as input—an application-oriented set-
ting, and one where only markables that represent
true mentions have been taken (cf. (Luo et al.,
2004; Ponzetto and Strube, 2006) for other ap-
proaches with an evaluation based on true men-
tions only). The assumption is that if only true
mentions are considered, the effects of a model
can be better measured.
We have used the Entity-Constrained Measure
(ECM), introduced in (Luo et al., 2004; Luo,
2005). As argued in (Klenner and Ailloud, 2008),
it is more appropriate to evaluate the quality of
coreference sets than the MUC score.9
To obtain the baseline, we merged all pairs that
TiMBL classified as coreferent into coreference
sets. Table 2 shows the results.
</bodyText>
<table confidence="0.9952184">
all mentions true mentions
Timbl B-First Timbl B-First
F 61.83 64.27 71.47 78.90
P 66.52 72.05 73.81 84.10
R 57.76 58.00 69.28 74.31
</table>
<tableCaption confidence="0.997107">
Table 2: Balas-First (B-First) vs. Baseline
</tableCaption>
<bodyText confidence="0.9999844375">
In the ’all mentions setting’, 2.4% F-measure im-
provement was achieved, with ’true mentions’ it is
7.43%. These improvements clearly demonstrate
that Balas-First is superior to the results based on
the classifier output.
But is the specific order proposed by the Balas
algorithm itself useful? Since we have dispensed
with ’full optimization’, why not dispense with the
Balas ordering as well? Since the ordering of the
pairs does not affect the rest of our algorithm we
have been able to compare the Balas order to the
more natural linear order. Note that all constraints
are applied in the linear variant as well, so the only
difference is the ordering. Linear ordering over
pairs is established by sorting according to the in-
dex of the first pair element (the i from cij).
</bodyText>
<table confidence="0.987845">
all mentions true mentions
linear B-First linear B-First
F 62.83 64.27 76.08 78.90
P 70.39 72.05 81.40 84.10
R 56.73 58.00 71.41 74.31
</table>
<tableCaption confidence="0.999149">
Table 3: Balas Order vs. Linear Order
</tableCaption>
<bodyText confidence="0.503765">
Our experiments (cf. Table 3) indicate that the
</bodyText>
<footnote confidence="0.995205333333333">
9Various authors have remarked on the shortcomings of
the MUC evaluation scheme (Bagga and Baldwin, 1998; Luo,
2005; Nicolae and Nicolae, 2006).
</footnote>
<page confidence="0.995775">
448
</page>
<bodyText confidence="0.9999598125">
Balas ordering does affect the empirical results.
The F-measure improvement is 1.44% (’all men-
tions’) and 2.82% (’true mentions’).
The search for Balas-First remains, in general,
NP-complete. However, constraint models with-
out boundness enforcement constraints (cf. Sec-
tion 5) pose no computational burden, they can be
solved in quadratic time. In the presence of bound-
ness enforcement constraints, exponential time is
required in the worst case. In our experiments,
boundness enforcement constraints have proved to
be unproblematic. Most of the time, the classi-
fier has assigned low costs to candidate pairs con-
taining a relative or a possessive pronoun, which
means that they get instantiated rather soon (al-
though this is not guaranteed).
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="related work">
9 Related Work
</sectionHeader>
<bodyText confidence="0.999959869565218">
The focus of our paper lies on the evaluation of the
benefits optimization could have for coreference
resolution. Accordingly, we restrict our discus-
sion to methodologically related approaches (i.e.
ILP approaches). Readers interested in other work
on anaphora resolution for German on the basis
of the T¨uBa coreference corpus should consider
(Hinrichs et al., 2005) (pronominal anaphora) and
(Versley, 2006) (nominal anaphora).
Common to all ILP approaches (incl. ours)
is that they apply ILP on the output of pairwise
machine-learning. Denis and Baldridge (2007;
2008) have an ILP model to jointly determine
anaphoricity and coreference, but take neither
transitivity nor exclusivity into account. So no
complexity problems arise in their approach. The
model from (Finkel and Manning, 2008) utilizes
transitivity, but not exclusivity. The benefits of
transitivity are thus restricted to an optimal bal-
ancing of the weights (e.g. given two positively
classified pairs, the transitively given third pair
in some cases is negative, ILP globally resolves
these cases to the optimal solution). The authors
do not mention complexity problems with exten-
sionalizing transitivity. Klenner (2007) utilizes
both transitivity and exclusivity. To overcome the
overhead of transitivity extensionalization, he pro-
poses a fixed transitivity window. This, however,
is bound to produce transitivity gaps, so the bene-
fits of complete transitivity propagation are lost.
Another attempt to overcome the problem of
complexity with ILP models is described in
(Riedel and Clarke, 2006) (dependency parsing).
Here an incremental—or better, cascaded—ILP
model is proposed, where at each cascade only
those constraints are added that have been vio-
lated in the preceding one. The search stops with
the first consistent solution (as we suggest in the
present paper). However, it is difficult to quantify
the number of cascades needed to come to it and
moreover, the full ILP machinery is being used (so
again, constraints need to be extensionalized).
To the best of our knowledge, our work is the
first that studies the proper utility of ILP optimiza-
tion for NLP, while offering an intensional alter-
native to ILP constraints.
</bodyText>
<sectionHeader confidence="0.99273" genericHeader="conclusions">
10 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999678055555556">
In this paper, we have argued that ILP for NLP
reduces to Zero-One ILP with unweighted con-
straints. We have proposed such a Zero-One ILP
model that combines exclusivity, transitivity and
boundness enforcement constraints in an inten-
sional model driven by best-first inference.
We furthermore claim and empirically demon-
strate for the domain of coreference resolution that
NLP approaches can take advantage from that new
perspective. The pitfall of ILP, namely the need
to extensionalize each and every constraint, can
be avoided. The solution is an easy to carry out
reimplementation of a Zero-One algorithm such
as Balas’, where (most) constraints can be treated
intensionally. Moreover, we have found empiri-
cal evidence that ’full optimization’ is not needed.
The first found consistent solution is as good as the
optimal one. Depending on the constraint model
this can reduce the costs from exponential time to
polynomial time.
Optimization efforts, however, are not superflu-
ous, as we have showed. The first consistent so-
lution found with our Balas reimplementation im-
proves the baseline significantly. Also, the Balas
ordering itself has proven superior over other or-
ders, e.g. linear order.
In the future, we will experiment with more
complex constraint models in the area of corefer-
ence resolution. But we will also consider other
domains in order to find out whether our results
actually are widely applicable.
Acknowledgement The work described herein
is partly funded by the Swiss National Science
Foundation (grant 105211-118108). We would
like to thank the anonymous reviewers for their
helpful comments.
</bodyText>
<page confidence="0.99905">
449
</page>
<sectionHeader confidence="0.998347" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999938086021505">
E. Althaus, N. Karamanis, and A. Koller. 2004. Com-
puting locally coherent discourses. In Proc. of the
ACL.
A. Bagga and B. Baldwin. 1998. Algorithms for scor-
ing coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at The First Interna-
tional Conference on Language Resources and Eval-
uation (LREC98), pages 563–566.
E. Balas. 1965. An additive algorithm for solving lin-
ear programs with zero-one variables. Operations
Research, 13(4):517–546.
J.W. Chinneck. 2004. Practical optimization:
a gentle introduction. Electronic document:
http://www.sce.carleton.ca/faculty/
chinneck/po.html.
W. Daelemans, J. Zavrel, K. van der Sloot, and
A. van den Bosch. 2004. TiMBL: Tilburg Memory-
Based Learner.
P. Denis and J. Baldridge. 2007. Joint determination
of anaphoricity and coreference resolution using in-
teger programming. Proceedings of NAACL HLT,
pages 236–243.
P. Denis and J. Baldridge. 2008. Specialized models
and ranking for coreference resolution. In Proceed-
ings of the Empirical Methods in Natural Language
Processing (EMNLP 2008), Hawaii, USA. To ap-
pear.
J.R. Finkel and C.D. Manning. 2008. Enforcing tran-
sitivity in coreference resolution. Association for
Computational Linguistics.
E. Hinrichs, K. Filippova, and H. Wunsch. 2005. A
data-driven approach to pronominal anaphora reso-
lution in German. In Proc. of RANLP ’05.
M. Klenner and ´E. Ailloud. 2008. Enhancing coref-
erence clustering. In C. Johansson, editor, Proc. of
the Second Workshop on Anaphora Resolution (WAR
II), volume 2 of NEALT Proceedings Series, pages
31–40, Bergen, Norway.
M. Klenner. 2007. Enforcing consistency on corefer-
ence sets. In Recent Advances in Natural Language
Processing (RANLP), pages 323–328, September.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous coref-
erence resolution algorithm based on the Bell tree.
Proceedings of the 42nd Annual Meeting on Associ-
ation for Computational Linguistics.
X. Luo. 2005. On coreference resolution perfor-
mance metrics. In Proceedings of the conference on
Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 25–32.
Association for Computational Linguistics Morris-
town, NJ, USA.
T. Marciniak and M. Strube. 2005. Beyond the
pipeline: Discrete optimization in NLP. In Proc. of
the CoNLL.
K. Naumann. 2006. Manual for the annotation of
indocument referential relations. Electronic doc-
ument: http://www.sfs.uni-tuebingen.de/
de_tuebadz.shtml.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proc.
of the ACL.
C. Nicolae and G. Nicolae. 2006. Best Cut: A graph
algorithm for coreference resolution. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 275–283.
Association for Computational Linguistics.
C.H. Papadimitriou and K. Steiglitz. 1998. Combi-
natorial Optimization: Algorithms and Complexity.
Dover Publications.
S.P. Ponzetto and M. Strube. 2006. Exploiting seman-
tic role labeling, WordNet and Wikipedia for coref-
erence resolution. In Proc. of HLT-NAACL, vol-
ume 6, pages 192–199.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Semantic role labeling via integer linear program-
ming inference. In Proc. of the COLING.
S. Riedel and J. Clarke. 2006. Incremental integer
linear programming for non-projective dependency
parsing. In Proc. of the EMNLP.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A
machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics,
27(4):521–544.
H. Telljohann, E.W. Hinrichs, S. K¨ubler, and H. Zins-
meister. 2005. Stylebook for the T¨ubingen
treebank of written German (T¨uBa-D/Z). Semi-
nar fur Sprachwissenschaft, Universit¨at T¨ubingen,
T¨ubingen, Germany.
Y. Versley. 2006. A constraint-based approach to noun
phrase coreference resolution in German newspa-
per text. In Konferenz zur Verarbeitung Nat¨urlicher
Sprache (KONVENS).
</reference>
<page confidence="0.997865">
450
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928068">
<title confidence="0.999913">Optimization in Coreference Resolution Is Not Needed: A Nearly-Optimal Algorithm with Intensional Constraints</title>
<author confidence="0.999308">Manfred Klenner</author>
<author confidence="0.999308">´Etienne Ailloud</author>
<affiliation confidence="0.973384">Computational Linguistics Zurich University, Switzerland</affiliation>
<abstract confidence="0.998663">We show how global constraints such as transitivity can be treated intensionally in a Zero-One Integer Linear Programming (ILP) framework which is geared to find the optimal and coherent partition of coreference sets given a number of candidate pairs and their weights delivered by a pairwise classifier (used as reliable clustering seed pairs). In order to find out whether ILP optimization, which is NPcomplete, actually is the best we can do, we compared the first consistent solution generated by our adaptation of an efficient Zero-One algorithm with the optimal solution. The first consistent solution, which often can be found very fast, is already as good as the optimal solution; optimization is thus not needed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Althaus</author>
<author>N Karamanis</author>
<author>A Koller</author>
</authors>
<title>Computing locally coherent discourses.</title>
<date>2004</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="13807" citStr="Althaus et al., 2004" startWordPosition="2288" endWordPosition="2291">= 0, Vi, j (clause bound(i, j)). (4) A possessive pronoun is exclusive to all markables in the noun phrase it is contained in (e.g. cij = 0 given a noun phrase “[heri managerj]”), but might get coindexed with markables outside of such a local context (“Annei talks to heri manager”). We define a predicate np bound that is true of two markables, if they occur in the same noun phrase. In general, two markables that np-bind each other are exclusive: cij = 0, Vi, j (np bound(i, j)) (5) 5 Representing ILP Constraints Intensionally Existing ILP-based approaches to NLP (e.g. (Punyakanok et al., 2004; Althaus et al., 2004; Marciniak and Strube, 2005)) belong to the class of Zero-One ILP: only binary variables are needed. This has been seldom remarked (but see (Althaus et al., 2004)) and generic (out-ofthe-box) ILP implementations are used. Moreover, these models form a very restricted variant of Zero-One ILP: the constraints come without any weights. The reason for this lies in the logical nature of NLP constraints. For example in the case of coreference, we have the following types of constraints: 1. exclusivity of two instantiations (e.g. either coreferent or not, equation 2) 2. dependencies among three inst</context>
</contexts>
<marker>Althaus, Karamanis, Koller, 2004</marker>
<rawString>E. Althaus, N. Karamanis, and A. Koller. 2004. Computing locally coherent discourses. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the Linguistic Coreference Workshop at The First International Conference on Language Resources and Evaluation (LREC98),</booktitle>
<pages>563--566</pages>
<contexts>
<context position="29314" citStr="Bagga and Baldwin, 1998" startWordPosition="4945" endWordPosition="4948">en able to compare the Balas order to the more natural linear order. Note that all constraints are applied in the linear variant as well, so the only difference is the ordering. Linear ordering over pairs is established by sorting according to the index of the first pair element (the i from cij). all mentions true mentions linear B-First linear B-First F 62.83 64.27 76.08 78.90 P 70.39 72.05 81.40 84.10 R 56.73 58.00 71.41 74.31 Table 3: Balas Order vs. Linear Order Our experiments (cf. Table 3) indicate that the 9Various authors have remarked on the shortcomings of the MUC evaluation scheme (Bagga and Baldwin, 1998; Luo, 2005; Nicolae and Nicolae, 2006). 448 Balas ordering does affect the empirical results. The F-measure improvement is 1.44% (’all mentions’) and 2.82% (’true mentions’). The search for Balas-First remains, in general, NP-complete. However, constraint models without boundness enforcement constraints (cf. Section 5) pose no computational burden, they can be solved in quadratic time. In the presence of boundness enforcement constraints, exponential time is required in the worst case. In our experiments, boundness enforcement constraints have proved to be unproblematic. Most of the time, the</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of the Linguistic Coreference Workshop at The First International Conference on Language Resources and Evaluation (LREC98), pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Balas</author>
</authors>
<title>An additive algorithm for solving linear programs with zero-one variables.</title>
<date>1965</date>
<journal>Operations Research,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="3286" citStr="Balas, 1965" startWordPosition="517" endWordPosition="518">s. Unfortunately, in an ILP program every constraint has to be extensionalized (i.e. all instantiations of the constraint are to be generated). Capturing transitivity for e.g. 150 noun phrases (about 30 sentences) already produces 1,500,000 equations (cf. Section 4). Solving such ILP programs is far too slow for real applications (let alone its brute force character). A closer look at existing ILP approaches to NLP reveals that they are of a special kind, namely Zero-One ILP with unweighted constraints. Although still NP-complete there exist a number of algorithms such as the Balas algorithm (Balas, 1965) that efficiently explore the search space and reduce thereby run time complexity in the mean. We have adapted Balas’ algorithm to the special needs of coreference resolution. First and foremost, this results in an optimization algorithm that treats global constraints intensionally, i.e. that generates instantiations of a constraint only on demand. Thus, transitivity can be captured for even the longest texts. But more important, we found out empirically that ’full optimization’ is not really needed. The first consistent solution, which often can be found very fast, is already as good— in term</context>
<context position="4625" citStr="Balas, 1965" startWordPosition="730" endWordPosition="731">ical results. We first introduce Zero-One ILP, discuss our baseline model and give an ILP formalization of coreference resolution. Then we go into the details of our Balas adaptation and provide empirical evidence for our central claim—that optimization search can already be stopped (without qualProceedings of the 12th Conference of the European Chapter of the ACL, pages 442–450, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 442 ity loss) when the first consistent solution has been found. 2 Zero-One Integer Linear Programming (ILP) The algorithm in (Balas, 1965) solves ZeroOne Integer Linear Programming (ILP), where a weighted linear function (the objective function) of binary variables F(x1,...,xn) = w1x1 + ... + wnxn is to be minimized under the regiment of linear inequalities a1x1 +... + anxn &gt; A.1 Unlike its real-valued counterpart, Zero-One ILP is NPcomplete (cf., say, (Papadimitriou and Steiglitz, 1998)), but branch-and-bound algorithms with efficient heuristics exist, as the Balas Algorithm: Balas (1965) proposes an approach where the objective function’s addends are sorted according to the magnitude of the weights: 0 &lt; w1 &lt; ... &lt; wn. This pre</context>
</contexts>
<marker>Balas, 1965</marker>
<rawString>E. Balas. 1965. An additive algorithm for solving linear programs with zero-one variables. Operations Research, 13(4):517–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Chinneck</author>
</authors>
<date>2004</date>
<note>Practical optimization:</note>
<contexts>
<context position="5326" citStr="Chinneck, 2004" startWordPosition="840" endWordPosition="841"> objective function) of binary variables F(x1,...,xn) = w1x1 + ... + wnxn is to be minimized under the regiment of linear inequalities a1x1 +... + anxn &gt; A.1 Unlike its real-valued counterpart, Zero-One ILP is NPcomplete (cf., say, (Papadimitriou and Steiglitz, 1998)), but branch-and-bound algorithms with efficient heuristics exist, as the Balas Algorithm: Balas (1965) proposes an approach where the objective function’s addends are sorted according to the magnitude of the weights: 0 &lt; w1 &lt; ... &lt; wn. This preliminary ordering induces the following functioning principles for the algorithm (see (Chinneck, 2004, Chap. 13) for more details): 1. It seeks to minimize F, so that a solution with as few 1s as possible is preferred. 2. If, during exploration of solutions, constraints force an xi to be set to 1, then it should bear as small an index as possible. The Balas algorithm follows a depth-first search while checking feasibility (i.e., through the constraints) of the branches partially explored: Upon branching, the algorithm bounds the cost of setting the current variable xN to 1 by the costs accumulated so far: w1x1 +... +wN_1xN_1 +wN is now the lowest cost this branch may yield. If, on the contrar</context>
</contexts>
<marker>Chinneck, 2004</marker>
<rawString>J.W. Chinneck. 2004. Practical optimization:</rawString>
</citation>
<citation valid="false">
<title>a gentle introduction. Electronic document:</title>
<note>http://www.sce.carleton.ca/faculty/ chinneck/po.html.</note>
<marker></marker>
<rawString>a gentle introduction. Electronic document: http://www.sce.carleton.ca/faculty/ chinneck/po.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K van der Sloot</author>
<author>A van den Bosch</author>
</authors>
<title>TiMBL: Tilburg MemoryBased Learner.</title>
<date>2004</date>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2004</marker>
<rawString>W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den Bosch. 2004. TiMBL: Tilburg MemoryBased Learner.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>Proceedings of NAACL HLT,</booktitle>
<pages>236--243</pages>
<contexts>
<context position="30680" citStr="Denis and Baldridge (2007" startWordPosition="5152" endWordPosition="5155">ather soon (although this is not guaranteed). 9 Related Work The focus of our paper lies on the evaluation of the benefits optimization could have for coreference resolution. Accordingly, we restrict our discussion to methodologically related approaches (i.e. ILP approaches). Readers interested in other work on anaphora resolution for German on the basis of the T¨uBa coreference corpus should consider (Hinrichs et al., 2005) (pronominal anaphora) and (Versley, 2006) (nominal anaphora). Common to all ILP approaches (incl. ours) is that they apply ILP on the output of pairwise machine-learning. Denis and Baldridge (2007; 2008) have an ILP model to jointly determine anaphoricity and coreference, but take neither transitivity nor exclusivity into account. So no complexity problems arise in their approach. The model from (Finkel and Manning, 2008) utilizes transitivity, but not exclusivity. The benefits of transitivity are thus restricted to an optimal balancing of the weights (e.g. given two positively classified pairs, the transitively given third pair in some cases is negative, ILP globally resolves these cases to the optimal solution). The authors do not mention complexity problems with extensionalizing tra</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>P. Denis and J. Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. Proceedings of NAACL HLT, pages 236–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>Specialized models and ranking for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP 2008),</booktitle>
<location>Hawaii, USA.</location>
<note>To appear.</note>
<marker>Denis, Baldridge, 2008</marker>
<rawString>P. Denis and J. Baldridge. 2008. Specialized models and ranking for coreference resolution. In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP 2008), Hawaii, USA. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C D Manning</author>
</authors>
<title>Enforcing transitivity in coreference resolution. Association for Computational Linguistics.</title>
<date>2008</date>
<contexts>
<context position="11542" citStr="Finkel and Manning, 2008" startWordPosition="1899" endWordPosition="1902">wise classifier (i.e., that ≤ 0.5 suggests coreference). For example, the choice not to set cij = 1 at costs wij ≤ 0.5 must be sanctioned by instantiating its inverse variable cji = 1 and adding (1 − wij) to the objective function’s value. Otherwise minimization would turn—in the worst case—everything to be non-coreferent, while maximization would preferentially set everything to be actually coreferent (as long as no constraints are violated, of course).5 The first constraint then is: cij +cji = 1, ∀hi, ji ∈ O0.5 (2) A pair hi, ji is either coreferent or not. Transitivity is captured by (see (Finkel and Manning, 2008) for an alternative but equivalent formalization): cij + cjk ≤ cik + 1, ∀i, j,k (i &lt; j &lt; k) cik + cjk ≤ cij + 1, ∀i, j,k (i &lt; j &lt; k) (3) cij +cik ≤ cjk + 1, ∀i, j,k (i &lt; j &lt; k) In order to take full advantage of ILP’s reasoning capacities, three equations are needed given three markables. The extensionalization of transitivity thus produces n! 3!(n−3)! · 3 equations for n markables. Note that transitivity—as a global constraint—ought to spread over the whole candidate set, not just within in the window. Transitivity without further constraints is pointless.6 What we really can gain from transi</context>
<context position="30909" citStr="Finkel and Manning, 2008" startWordPosition="5186" endWordPosition="5189">cally related approaches (i.e. ILP approaches). Readers interested in other work on anaphora resolution for German on the basis of the T¨uBa coreference corpus should consider (Hinrichs et al., 2005) (pronominal anaphora) and (Versley, 2006) (nominal anaphora). Common to all ILP approaches (incl. ours) is that they apply ILP on the output of pairwise machine-learning. Denis and Baldridge (2007; 2008) have an ILP model to jointly determine anaphoricity and coreference, but take neither transitivity nor exclusivity into account. So no complexity problems arise in their approach. The model from (Finkel and Manning, 2008) utilizes transitivity, but not exclusivity. The benefits of transitivity are thus restricted to an optimal balancing of the weights (e.g. given two positively classified pairs, the transitively given third pair in some cases is negative, ILP globally resolves these cases to the optimal solution). The authors do not mention complexity problems with extensionalizing transitivity. Klenner (2007) utilizes both transitivity and exclusivity. To overcome the overhead of transitivity extensionalization, he proposes a fixed transitivity window. This, however, is bound to produce transitivity gaps, so </context>
</contexts>
<marker>Finkel, Manning, 2008</marker>
<rawString>J.R. Finkel and C.D. Manning. 2008. Enforcing transitivity in coreference resolution. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hinrichs</author>
<author>K Filippova</author>
<author>H Wunsch</author>
</authors>
<title>A data-driven approach to pronominal anaphora resolution in German.</title>
<date>2005</date>
<booktitle>In Proc. of RANLP ’05.</booktitle>
<contexts>
<context position="30483" citStr="Hinrichs et al., 2005" startWordPosition="5123" endWordPosition="5126">ave proved to be unproblematic. Most of the time, the classifier has assigned low costs to candidate pairs containing a relative or a possessive pronoun, which means that they get instantiated rather soon (although this is not guaranteed). 9 Related Work The focus of our paper lies on the evaluation of the benefits optimization could have for coreference resolution. Accordingly, we restrict our discussion to methodologically related approaches (i.e. ILP approaches). Readers interested in other work on anaphora resolution for German on the basis of the T¨uBa coreference corpus should consider (Hinrichs et al., 2005) (pronominal anaphora) and (Versley, 2006) (nominal anaphora). Common to all ILP approaches (incl. ours) is that they apply ILP on the output of pairwise machine-learning. Denis and Baldridge (2007; 2008) have an ILP model to jointly determine anaphoricity and coreference, but take neither transitivity nor exclusivity into account. So no complexity problems arise in their approach. The model from (Finkel and Manning, 2008) utilizes transitivity, but not exclusivity. The benefits of transitivity are thus restricted to an optimal balancing of the weights (e.g. given two positively classified pai</context>
</contexts>
<marker>Hinrichs, Filippova, Wunsch, 2005</marker>
<rawString>E. Hinrichs, K. Filippova, and H. Wunsch. 2005. A data-driven approach to pronominal anaphora resolution in German. In Proc. of RANLP ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Klenner</author>
<author>´E Ailloud</author>
</authors>
<title>Enhancing coreference clustering.</title>
<date>2008</date>
<booktitle>Proc. of the Second Workshop on Anaphora Resolution (WAR II),</booktitle>
<volume>2</volume>
<pages>31--40</pages>
<editor>In C. Johansson, editor,</editor>
<location>Bergen, Norway.</location>
<contexts>
<context position="27816" citStr="Klenner and Ailloud, 2008" startWordPosition="4695" endWordPosition="4698">setting (1100 texts from the T¨uBa coreference corpus). Each experiment was carried out in two variants. One where all markables have been taken as input—an application-oriented setting, and one where only markables that represent true mentions have been taken (cf. (Luo et al., 2004; Ponzetto and Strube, 2006) for other approaches with an evaluation based on true mentions only). The assumption is that if only true mentions are considered, the effects of a model can be better measured. We have used the Entity-Constrained Measure (ECM), introduced in (Luo et al., 2004; Luo, 2005). As argued in (Klenner and Ailloud, 2008), it is more appropriate to evaluate the quality of coreference sets than the MUC score.9 To obtain the baseline, we merged all pairs that TiMBL classified as coreferent into coreference sets. Table 2 shows the results. all mentions true mentions Timbl B-First Timbl B-First F 61.83 64.27 71.47 78.90 P 66.52 72.05 73.81 84.10 R 57.76 58.00 69.28 74.31 Table 2: Balas-First (B-First) vs. Baseline In the ’all mentions setting’, 2.4% F-measure improvement was achieved, with ’true mentions’ it is 7.43%. These improvements clearly demonstrate that Balas-First is superior to the results based on the c</context>
</contexts>
<marker>Klenner, Ailloud, 2008</marker>
<rawString>M. Klenner and ´E. Ailloud. 2008. Enhancing coreference clustering. In C. Johansson, editor, Proc. of the Second Workshop on Anaphora Resolution (WAR II), volume 2 of NEALT Proceedings Series, pages 31–40, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Klenner</author>
</authors>
<title>Enforcing consistency on coreference sets.</title>
<date>2007</date>
<booktitle>In Recent Advances in Natural Language Processing (RANLP),</booktitle>
<pages>323--328</pages>
<contexts>
<context position="31305" citStr="Klenner (2007)" startWordPosition="5246" endWordPosition="5247">e an ILP model to jointly determine anaphoricity and coreference, but take neither transitivity nor exclusivity into account. So no complexity problems arise in their approach. The model from (Finkel and Manning, 2008) utilizes transitivity, but not exclusivity. The benefits of transitivity are thus restricted to an optimal balancing of the weights (e.g. given two positively classified pairs, the transitively given third pair in some cases is negative, ILP globally resolves these cases to the optimal solution). The authors do not mention complexity problems with extensionalizing transitivity. Klenner (2007) utilizes both transitivity and exclusivity. To overcome the overhead of transitivity extensionalization, he proposes a fixed transitivity window. This, however, is bound to produce transitivity gaps, so the benefits of complete transitivity propagation are lost. Another attempt to overcome the problem of complexity with ILP models is described in (Riedel and Clarke, 2006) (dependency parsing). Here an incremental—or better, cascaded—ILP model is proposed, where at each cascade only those constraints are added that have been violated in the preceding one. The search stops with the first consis</context>
</contexts>
<marker>Klenner, 2007</marker>
<rawString>M. Klenner. 2007. Enforcing consistency on coreference sets. In Recent Advances in Natural Language Processing (RANLP), pages 323–328, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>S Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the Bell tree.</title>
<date>2004</date>
<booktitle>Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="27473" citStr="Luo et al., 2004" startWordPosition="4637" endWordPosition="4640">e not saying that we should dispense with optimization efforts completely. 8 Does Balas-First help? Empirical Evidence Besides the empirical fact that Balas-First slightly outperforms the optimal solution, we must demonstrate that Balas-First actually improves the baseline. Our experiments are based on a five-fold cross-validation setting (1100 texts from the T¨uBa coreference corpus). Each experiment was carried out in two variants. One where all markables have been taken as input—an application-oriented setting, and one where only markables that represent true mentions have been taken (cf. (Luo et al., 2004; Ponzetto and Strube, 2006) for other approaches with an evaluation based on true mentions only). The assumption is that if only true mentions are considered, the effects of a model can be better measured. We have used the Entity-Constrained Measure (ECM), introduced in (Luo et al., 2004; Luo, 2005). As argued in (Klenner and Ailloud, 2008), it is more appropriate to evaluate the quality of coreference sets than the MUC score.9 To obtain the baseline, we merged all pairs that TiMBL classified as coreferent into coreference sets. Table 2 shows the results. all mentions true mentions Timbl B-Fi</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the Bell tree. Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>25--32</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="27774" citStr="Luo, 2005" startWordPosition="4690" endWordPosition="4691">ive-fold cross-validation setting (1100 texts from the T¨uBa coreference corpus). Each experiment was carried out in two variants. One where all markables have been taken as input—an application-oriented setting, and one where only markables that represent true mentions have been taken (cf. (Luo et al., 2004; Ponzetto and Strube, 2006) for other approaches with an evaluation based on true mentions only). The assumption is that if only true mentions are considered, the effects of a model can be better measured. We have used the Entity-Constrained Measure (ECM), introduced in (Luo et al., 2004; Luo, 2005). As argued in (Klenner and Ailloud, 2008), it is more appropriate to evaluate the quality of coreference sets than the MUC score.9 To obtain the baseline, we merged all pairs that TiMBL classified as coreferent into coreference sets. Table 2 shows the results. all mentions true mentions Timbl B-First Timbl B-First F 61.83 64.27 71.47 78.90 P 66.52 72.05 73.81 84.10 R 57.76 58.00 69.28 74.31 Table 2: Balas-First (B-First) vs. Baseline In the ’all mentions setting’, 2.4% F-measure improvement was achieved, with ’true mentions’ it is 7.43%. These improvements clearly demonstrate that Balas-First</context>
<context position="29325" citStr="Luo, 2005" startWordPosition="4949" endWordPosition="4950">las order to the more natural linear order. Note that all constraints are applied in the linear variant as well, so the only difference is the ordering. Linear ordering over pairs is established by sorting according to the index of the first pair element (the i from cij). all mentions true mentions linear B-First linear B-First F 62.83 64.27 76.08 78.90 P 70.39 72.05 81.40 84.10 R 56.73 58.00 71.41 74.31 Table 3: Balas Order vs. Linear Order Our experiments (cf. Table 3) indicate that the 9Various authors have remarked on the shortcomings of the MUC evaluation scheme (Bagga and Baldwin, 1998; Luo, 2005; Nicolae and Nicolae, 2006). 448 Balas ordering does affect the empirical results. The F-measure improvement is 1.44% (’all mentions’) and 2.82% (’true mentions’). The search for Balas-First remains, in general, NP-complete. However, constraint models without boundness enforcement constraints (cf. Section 5) pose no computational burden, they can be solved in quadratic time. In the presence of boundness enforcement constraints, exponential time is required in the worst case. In our experiments, boundness enforcement constraints have proved to be unproblematic. Most of the time, the classifier</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>X. Luo. 2005. On coreference resolution performance metrics. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 25–32. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Marciniak</author>
<author>M Strube</author>
</authors>
<title>Beyond the pipeline: Discrete optimization in NLP.</title>
<date>2005</date>
<booktitle>In Proc. of the CoNLL.</booktitle>
<contexts>
<context position="13836" citStr="Marciniak and Strube, 2005" startWordPosition="2292" endWordPosition="2295">nd(i, j)). (4) A possessive pronoun is exclusive to all markables in the noun phrase it is contained in (e.g. cij = 0 given a noun phrase “[heri managerj]”), but might get coindexed with markables outside of such a local context (“Annei talks to heri manager”). We define a predicate np bound that is true of two markables, if they occur in the same noun phrase. In general, two markables that np-bind each other are exclusive: cij = 0, Vi, j (np bound(i, j)) (5) 5 Representing ILP Constraints Intensionally Existing ILP-based approaches to NLP (e.g. (Punyakanok et al., 2004; Althaus et al., 2004; Marciniak and Strube, 2005)) belong to the class of Zero-One ILP: only binary variables are needed. This has been seldom remarked (but see (Althaus et al., 2004)) and generic (out-ofthe-box) ILP implementations are used. Moreover, these models form a very restricted variant of Zero-One ILP: the constraints come without any weights. The reason for this lies in the logical nature of NLP constraints. For example in the case of coreference, we have the following types of constraints: 1. exclusivity of two instantiations (e.g. either coreferent or not, equation 2) 2. dependencies among three instantiations (transitivity: if </context>
</contexts>
<marker>Marciniak, Strube, 2005</marker>
<rawString>T. Marciniak and M. Strube. 2005. Beyond the pipeline: Discrete optimization in NLP. In Proc. of the CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Naumann</author>
</authors>
<title>Manual for the annotation of indocument referential relations. Electronic document:</title>
<date>2006</date>
<note>http://www.sfs.uni-tuebingen.de/ de_tuebadz.shtml.</note>
<contexts>
<context position="7651" citStr="Naumann, 2006" startWordPosition="1246" endWordPosition="1247">atures; Table 1 lists the set we have finally used (Soon et al. (2001) and Ng and Cardie (2002) more thoroughly discuss different features and their benefits): - distance in sentences and markables - part of speech of the head of the markables - the grammatical functions - parallelism of grammatical functions - do the heads match or not - where is the pronoun (if any): left or right - word form if POS is pronoun - salience of the non-pronominal phrases - semantic class of noun phrase heads Table 1: Features for Pairwise Classification As a gold standard the T¨uBa-D/Z (Telljohann et al., 2005; Naumann, 2006) coreference corpus is used. The T¨uBa is a treebank (1,100 German newspaper texts, 25,000 sentences) augmented with coreference annotations2. In total, there are 13,818 anaphoric, 1,031 cataphoric and 12,752 coreferential relations. There are 3,295 relative pronouns, 8,929 personal pronouns, 2,987 reflexive pronouns, and 3,921 possessive pronouns. There are some rather long texts in the T¨uBa corpus. Which pair generation algorithm is reasonable? Should we pair every markable (even from the beginning of the text) with every other succeeding markable? This is linguistically implausible. Pronou</context>
</contexts>
<marker>Naumann, 2006</marker>
<rawString>K. Naumann. 2006. Manual for the annotation of indocument referential relations. Electronic document: http://www.sfs.uni-tuebingen.de/ de_tuebadz.shtml.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="7132" citStr="Ng and Cardie (2002)" startWordPosition="1156" endWordPosition="1159"> The complexity thus remains exponential in the worst case, but the initial ordering of weights is a clever guide. 1Maximization and coping with &lt;-constraints are also accessible via simple transformations. 3 Our Baseline Model The memory-based learner TiMBL (Daelemans et al., 2004) is used as a (pairwise) classifier. TiMBL stores all training examples, learns feature weights and classifies test instances according to the majority class of the k-nearest (i.e. most similar) neighbors. We have experimented with various features; Table 1 lists the set we have finally used (Soon et al. (2001) and Ng and Cardie (2002) more thoroughly discuss different features and their benefits): - distance in sentences and markables - part of speech of the head of the markables - the grammatical functions - parallelism of grammatical functions - do the heads match or not - where is the pronoun (if any): left or right - word form if POS is pronoun - salience of the non-pronominal phrases - semantic class of noun phrase heads Table 1: Features for Pairwise Classification As a gold standard the T¨uBa-D/Z (Telljohann et al., 2005; Naumann, 2006) coreference corpus is used. The T¨uBa is a treebank (1,100 German newspaper text</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nicolae</author>
<author>G Nicolae</author>
</authors>
<title>Best Cut: A graph algorithm for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>275--283</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="29353" citStr="Nicolae and Nicolae, 2006" startWordPosition="4951" endWordPosition="4954">o the more natural linear order. Note that all constraints are applied in the linear variant as well, so the only difference is the ordering. Linear ordering over pairs is established by sorting according to the index of the first pair element (the i from cij). all mentions true mentions linear B-First linear B-First F 62.83 64.27 76.08 78.90 P 70.39 72.05 81.40 84.10 R 56.73 58.00 71.41 74.31 Table 3: Balas Order vs. Linear Order Our experiments (cf. Table 3) indicate that the 9Various authors have remarked on the shortcomings of the MUC evaluation scheme (Bagga and Baldwin, 1998; Luo, 2005; Nicolae and Nicolae, 2006). 448 Balas ordering does affect the empirical results. The F-measure improvement is 1.44% (’all mentions’) and 2.82% (’true mentions’). The search for Balas-First remains, in general, NP-complete. However, constraint models without boundness enforcement constraints (cf. Section 5) pose no computational burden, they can be solved in quadratic time. In the presence of boundness enforcement constraints, exponential time is required in the worst case. In our experiments, boundness enforcement constraints have proved to be unproblematic. Most of the time, the classifier has assigned low costs to c</context>
</contexts>
<marker>Nicolae, Nicolae, 2006</marker>
<rawString>C. Nicolae and G. Nicolae. 2006. Best Cut: A graph algorithm for coreference resolution. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 275–283. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Papadimitriou</author>
<author>K Steiglitz</author>
</authors>
<title>Combinatorial Optimization: Algorithms and Complexity.</title>
<date>1998</date>
<publisher>Dover Publications.</publisher>
<contexts>
<context position="4979" citStr="Papadimitriou and Steiglitz, 1998" startWordPosition="784" endWordPosition="787">of the European Chapter of the ACL, pages 442–450, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 442 ity loss) when the first consistent solution has been found. 2 Zero-One Integer Linear Programming (ILP) The algorithm in (Balas, 1965) solves ZeroOne Integer Linear Programming (ILP), where a weighted linear function (the objective function) of binary variables F(x1,...,xn) = w1x1 + ... + wnxn is to be minimized under the regiment of linear inequalities a1x1 +... + anxn &gt; A.1 Unlike its real-valued counterpart, Zero-One ILP is NPcomplete (cf., say, (Papadimitriou and Steiglitz, 1998)), but branch-and-bound algorithms with efficient heuristics exist, as the Balas Algorithm: Balas (1965) proposes an approach where the objective function’s addends are sorted according to the magnitude of the weights: 0 &lt; w1 &lt; ... &lt; wn. This preliminary ordering induces the following functioning principles for the algorithm (see (Chinneck, 2004, Chap. 13) for more details): 1. It seeks to minimize F, so that a solution with as few 1s as possible is preferred. 2. If, during exploration of solutions, constraints force an xi to be set to 1, then it should bear as small an index as possible. The </context>
</contexts>
<marker>Papadimitriou, Steiglitz, 1998</marker>
<rawString>C.H. Papadimitriou and K. Steiglitz. 1998. Combinatorial Optimization: Algorithms and Complexity. Dover Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<volume>6</volume>
<pages>192--199</pages>
<contexts>
<context position="27501" citStr="Ponzetto and Strube, 2006" startWordPosition="4641" endWordPosition="4644">we should dispense with optimization efforts completely. 8 Does Balas-First help? Empirical Evidence Besides the empirical fact that Balas-First slightly outperforms the optimal solution, we must demonstrate that Balas-First actually improves the baseline. Our experiments are based on a five-fold cross-validation setting (1100 texts from the T¨uBa coreference corpus). Each experiment was carried out in two variants. One where all markables have been taken as input—an application-oriented setting, and one where only markables that represent true mentions have been taken (cf. (Luo et al., 2004; Ponzetto and Strube, 2006) for other approaches with an evaluation based on true mentions only). The assumption is that if only true mentions are considered, the effects of a model can be better measured. We have used the Entity-Constrained Measure (ECM), introduced in (Luo et al., 2004; Luo, 2005). As argued in (Klenner and Ailloud, 2008), it is more appropriate to evaluate the quality of coreference sets than the MUC score.9 To obtain the baseline, we merged all pairs that TiMBL classified as coreferent into coreference sets. Table 2 shows the results. all mentions true mentions Timbl B-First Timbl B-First F 61.83 64</context>
</contexts>
<marker>Ponzetto, Strube, 2006</marker>
<rawString>S.P. Ponzetto and M. Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. In Proc. of HLT-NAACL, volume 6, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
<author>D Zimak</author>
</authors>
<title>Semantic role labeling via integer linear programming inference.</title>
<date>2004</date>
<booktitle>In Proc. of the COLING.</booktitle>
<contexts>
<context position="13785" citStr="Punyakanok et al., 2004" startWordPosition="2283" endWordPosition="2287"> are exclusive, i.e. cij = 0, Vi, j (clause bound(i, j)). (4) A possessive pronoun is exclusive to all markables in the noun phrase it is contained in (e.g. cij = 0 given a noun phrase “[heri managerj]”), but might get coindexed with markables outside of such a local context (“Annei talks to heri manager”). We define a predicate np bound that is true of two markables, if they occur in the same noun phrase. In general, two markables that np-bind each other are exclusive: cij = 0, Vi, j (np bound(i, j)) (5) 5 Representing ILP Constraints Intensionally Existing ILP-based approaches to NLP (e.g. (Punyakanok et al., 2004; Althaus et al., 2004; Marciniak and Strube, 2005)) belong to the class of Zero-One ILP: only binary variables are needed. This has been seldom remarked (but see (Althaus et al., 2004)) and generic (out-ofthe-box) ILP implementations are used. Moreover, these models form a very restricted variant of Zero-One ILP: the constraints come without any weights. The reason for this lies in the logical nature of NLP constraints. For example in the case of coreference, we have the following types of constraints: 1. exclusivity of two instantiations (e.g. either coreferent or not, equation 2) 2. depende</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. Semantic role labeling via integer linear programming inference. In Proc. of the COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>J Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the EMNLP.</booktitle>
<contexts>
<context position="31680" citStr="Riedel and Clarke, 2006" startWordPosition="5299" endWordPosition="5302">o positively classified pairs, the transitively given third pair in some cases is negative, ILP globally resolves these cases to the optimal solution). The authors do not mention complexity problems with extensionalizing transitivity. Klenner (2007) utilizes both transitivity and exclusivity. To overcome the overhead of transitivity extensionalization, he proposes a fixed transitivity window. This, however, is bound to produce transitivity gaps, so the benefits of complete transitivity propagation are lost. Another attempt to overcome the problem of complexity with ILP models is described in (Riedel and Clarke, 2006) (dependency parsing). Here an incremental—or better, cascaded—ILP model is proposed, where at each cascade only those constraints are added that have been violated in the preceding one. The search stops with the first consistent solution (as we suggest in the present paper). However, it is difficult to quantify the number of cascades needed to come to it and moreover, the full ILP machinery is being used (so again, constraints need to be extensionalized). To the best of our knowledge, our work is the first that studies the proper utility of ILP optimization for NLP, while offering an intensio</context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>S. Riedel and J. Clarke. 2006. Incremental integer linear programming for non-projective dependency parsing. In Proc. of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D C Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="7107" citStr="Soon et al. (2001)" startWordPosition="1151" endWordPosition="1154">t unexplored branching. The complexity thus remains exponential in the worst case, but the initial ordering of weights is a clever guide. 1Maximization and coping with &lt;-constraints are also accessible via simple transformations. 3 Our Baseline Model The memory-based learner TiMBL (Daelemans et al., 2004) is used as a (pairwise) classifier. TiMBL stores all training examples, learns feature weights and classifies test instances according to the majority class of the k-nearest (i.e. most similar) neighbors. We have experimented with various features; Table 1 lists the set we have finally used (Soon et al. (2001) and Ng and Cardie (2002) more thoroughly discuss different features and their benefits): - distance in sentences and markables - part of speech of the head of the markables - the grammatical functions - parallelism of grammatical functions - do the heads match or not - where is the pronoun (if any): left or right - word form if POS is pronoun - salience of the non-pronominal phrases - semantic class of noun phrase heads Table 1: Features for Pairwise Classification As a gold standard the T¨uBa-D/Z (Telljohann et al., 2005; Naumann, 2006) coreference corpus is used. The T¨uBa is a treebank (1,</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Telljohann</author>
<author>E W Hinrichs</author>
<author>S K¨ubler</author>
<author>H Zinsmeister</author>
</authors>
<title>Stylebook for the T¨ubingen treebank of written German (T¨uBa-D/Z). Seminar fur Sprachwissenschaft, Universit¨at T¨ubingen,</title>
<date>2005</date>
<location>T¨ubingen, Germany.</location>
<marker>Telljohann, Hinrichs, K¨ubler, Zinsmeister, 2005</marker>
<rawString>H. Telljohann, E.W. Hinrichs, S. K¨ubler, and H. Zinsmeister. 2005. Stylebook for the T¨ubingen treebank of written German (T¨uBa-D/Z). Seminar fur Sprachwissenschaft, Universit¨at T¨ubingen, T¨ubingen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Versley</author>
</authors>
<title>A constraint-based approach to noun phrase coreference resolution in German newspaper text.</title>
<date>2006</date>
<booktitle>In Konferenz zur Verarbeitung Nat¨urlicher Sprache (KONVENS).</booktitle>
<contexts>
<context position="30525" citStr="Versley, 2006" startWordPosition="5130" endWordPosition="5131">the classifier has assigned low costs to candidate pairs containing a relative or a possessive pronoun, which means that they get instantiated rather soon (although this is not guaranteed). 9 Related Work The focus of our paper lies on the evaluation of the benefits optimization could have for coreference resolution. Accordingly, we restrict our discussion to methodologically related approaches (i.e. ILP approaches). Readers interested in other work on anaphora resolution for German on the basis of the T¨uBa coreference corpus should consider (Hinrichs et al., 2005) (pronominal anaphora) and (Versley, 2006) (nominal anaphora). Common to all ILP approaches (incl. ours) is that they apply ILP on the output of pairwise machine-learning. Denis and Baldridge (2007; 2008) have an ILP model to jointly determine anaphoricity and coreference, but take neither transitivity nor exclusivity into account. So no complexity problems arise in their approach. The model from (Finkel and Manning, 2008) utilizes transitivity, but not exclusivity. The benefits of transitivity are thus restricted to an optimal balancing of the weights (e.g. given two positively classified pairs, the transitively given third pair in s</context>
</contexts>
<marker>Versley, 2006</marker>
<rawString>Y. Versley. 2006. A constraint-based approach to noun phrase coreference resolution in German newspaper text. In Konferenz zur Verarbeitung Nat¨urlicher Sprache (KONVENS).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>