<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.998668">
A Generative Model for Parsing Natural Language to Meaning
Representations
</title>
<author confidence="0.989128">
Wei Lu1, Hwee Tou Ng1,2, Wee Sun Lee1,2
</author>
<affiliation confidence="0.96953">
1Singapore-MIT Alliance
2Department of Computer Science
National University of Singapore
</affiliation>
<email confidence="0.93766">
luwei@nus.edu.sg
{nght,leews}@comp.nus.edu.sg
</email>
<note confidence="0.9525095">
Luke S. Zettlemoyer
CSAIL
</note>
<affiliation confidence="0.633543">
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.991914">
lsz@csail.mit.edu
</email>
<sectionHeader confidence="0.99657" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999699611111111">
In this paper, we present an algorithm for
learning a generative model of natural lan-
guage sentences together with their for-
mal meaning representations with hierarchi-
cal structures. The model is applied to the
task of mapping sentences to hierarchical rep-
resentations of their underlying meaning. We
introduce dynamic programming techniques
for efficient training and decoding. In exper-
iments, we demonstrate that the model, when
coupled with a discriminative reranking tech-
nique, achieves state-of-the-art performance
when tested on two publicly available cor-
pora. The generative model degrades robustly
when presented with instances that are differ-
ent from those seen in training. This allows
a notable improvement in recall compared to
previous models.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972125">
To enable computers to understand natural human
language is one of the classic goals of research in
natural language processing. Recently, researchers
have developed techniques for learning to map sen-
tences to hierarchical representations of their under-
lying meaning (Wong and Mooney, 2006; Kate and
Mooney, 2006).
One common approach is to learn some form of
probabilistic grammar which includes a list of lexi-
cal items that models the meanings of input words
and also includes rules for combining lexical mean-
ings to analyze complete sentences. This approach
performs well but is constrained by the use of a sin-
gle, learned grammar that contains a fixed set of
lexical entries and productions. In practice, such
a grammar may lack the rules required to correctly
parse some of the new test examples.
In this paper, we develop an alternative approach
that learns a model which does not make use of
an explicit grammar but, instead, models the cor-
respondence between sentences and their meanings
with a generative process. This model is defined
over hybrid trees whose nodes include both natu-
ral language words and meaning representation to-
kens. Inspired by the work of Collins (2003), the
generative model builds trees by recursively creating
nodes at each level according to a Markov process.
This implicit grammar representation leads to flexi-
ble learned models that generalize well. In practice,
we observe that it can correctly parse a wider range
of test examples than previous approaches.
The generative model is learned from data that
consists of sentences paired with their meaning rep-
resentations. However, there is no explicit labeling
of the correspondence between words and meaning
tokens that is necessary for building the hybrid trees.
This creates a challenging, hidden-variable learning
problem that we address with the use of an inside-
outside algorithm. Specifically, we develop a dy-
namic programming parsing algorithm that leads to
O(n3m) time complexity for inference, where n is
the sentence length and m is the size of meaning
structure. This approach allows for efficient train-
ing and decoding.
In practice, we observe that the learned generative
models are able to assign a high score to the correct
meaning for input sentences, but that this correct
meaning is not always the highest scoring option.
</bodyText>
<page confidence="0.982099">
783
</page>
<note confidence="0.962025">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 783–792,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999947">
To address this problem, we use a simple rerank-
ing approach to select a parse from a k-best list of
parses. This pipelined approach achieves state-of-
the-art performance on two publicly available cor-
pora. In particular, the flexible generative model
leads to notable improvements in recall, the total
percentage of sentences that are correctly parsed.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99991076">
In Section 9, we will compare performance with
the three existing systems that were evaluated on
the same data sets we consider. SILT (Kate et al.,
2005) learns deterministic rules to transform either
sentences or their syntactic parse trees to meaning
structures. WASP (Wong and Mooney, 2006) is a
system motivated by statistical machine translation
techniques. It acquires a set of synchronous lexical
entries by running the IBM alignment model (Brown
et al., 1993) and learns a log-linear model to weight
parses. KRISP (Kate and Mooney, 2006) is a dis-
criminative approach where meaning representation
structures are constructed from the natural language
strings hierarchically. It is built on top of SVMstruct
with string kernels.
Additionally, there is substantial related research
that is not directly comparable to our approach.
Some of this work requires different levels of super-
vision, including labeled syntactic parse trees (Ge
and Mooney, 2005; Ge and Mooney, 2006). Others
do not perform lexical learning (Tang and Mooney,
2001). Finally, recent work has explored learning
to map sentences to lambda-calculus meaning rep-
resentations (Wong and Mooney, 2007; Zettlemoyer
and Collins, 2005; Zettlemoyer and Collins, 2007).
</bodyText>
<sectionHeader confidence="0.906272" genericHeader="method">
3 Meaning Representation
</sectionHeader>
<bodyText confidence="0.999759181818182">
We restrict our meaning representation (MR) for-
malism to a variable free version as presented in
(Wong and Mooney, 2006; Kate et al., 2005).
A training instance consists of a natural language
sentence (NL sentence) and its corresponding mean-
ing representation structure (MR structure). Con-
sider the following instance taken from the GEO-
QUERY corpus (Kate et al., 2005):
The NL sentence “How many states do
not have rivers ?” consists of 8 words, in-
cluding punctuation. The MR is a hierarchical tree
</bodyText>
<figure confidence="0.256601">
QUERY: answer (NUM)
NUM : count (STATE)
STATE: exclude (STATE STATE)
STATE: state (all) STATE: loc 1 (RIVER)
RIVER: river (all)
</figure>
<figureCaption confidence="0.998066">
Figure 1: An example MR structure
</figureCaption>
<bodyText confidence="0.9929632">
structure, as shown in Figure 1.
Following an inorder traversal of this MR tree, we
can equivalently represent it with the following list
of meaning representation productions (MR produc-
tions):
</bodyText>
<listItem confidence="0.999853">
(0) QUERY : answer (NUM)
(1) NUM : count (STATE)
(2) STATE : exclude (STATE1 STATE2)
(3) STATE : state (all)
(4) STATE : loc 1 (RIVER)
(5) RIVER : river (all)
</listItem>
<bodyText confidence="0.9946648">
Each such MR production consists of three com-
ponents: a semantic category, a function symbol
which can be omitted (considered empty), and a list
of arguments. An argument can be either a child se-
mantic category or a constant. Take production (1)
for example: it has a semantic category “NUM”, a
function symbol “count”, and a child semantic cate-
gory “STATE” as its only argument. Production (5)
has “RIVER” as its semantic category, “river” as the
function symbol, and “all” is a constant.
</bodyText>
<sectionHeader confidence="0.992935" genericHeader="method">
4 The Generative Model
</sectionHeader>
<bodyText confidence="0.999983294117647">
We describe in this section our proposed generative
model, which simultaneously generates a NL sen-
tence and an MR structure.
We denote a single NL word as w, a contiguous
sequence of NL words as w, and a complete NL
sentence as W. In the MR structure, we denote a
semantic category as M. We denote a single MR
production as ma, or Ma : pα(Mb, Mc), where Ma
is the semantic category for this production, pα is the
function symbol, and Mb, Mc are the child semantic
categories. We denote ma as an MR structure rooted
by an MR production ma, and mQ an MR structure
for a complete sentence rooted by an MR production
ma.
The model generates a hybrid tree that represents
a sentence W = w1 ... w2 ... paired with an MR
structure mQ rooted by ma.
</bodyText>
<page confidence="0.993873">
784
</page>
<figure confidence="0.9492225">
Ma
ma
</figure>
<figureCaption confidence="0.999907">
Figure 2: The generation process
</figureCaption>
<bodyText confidence="0.99757219047619">
Figure 2 shows part of a hybrid tree that is gen-
erated as follows. Given a semantic category Ma,
we first pick an MR production ma that has the form
Ma : pα(Mb, Mc), which gives us the function sym-
bol pα as well as the child semantic categories Mb
and Mc. Next, we generate the hybrid sequence of
child nodes w1 Mb w2 Mc, which consists of NL
words and semantic categories.
After that, two child MR productions mb and mc
are generated. These two productions will in turn
generate other hybrid sequences and productions, re-
cursively. This process produces a hybrid tree T,
whose nodes are either NL words or MR produc-
tions. Given this tree, we can recover a NL sentence
w by recording the NL words visited in depth-first
traversal order and can recover an MR structure m
by following a tree-specific traversal order, defined
by the hybrid-patterns we introduce below. Figure 3
gives a partial hybrid tree for the training example
from Section 3. Note that the leaves of a hybrid tree
are always NL tokens.
</bodyText>
<equation confidence="0.973072125">
. . .
STATE
STATE : exclude (STATE STATE)
do not STATE
STATE: loc 1(RIVER)
have RIVER
RIVER : river(all)
rivers
</equation>
<figureCaption confidence="0.998321">
Figure 3: A partial hybrid tree
</figureCaption>
<bodyText confidence="0.9986965">
With several independence assumptions, the
probability of generating (w, m, T) is defined as:
</bodyText>
<equation confidence="0.9998">
P(w, �m, T) = P(Ma) × P(ma|Ma) × P(w1 Mb w2 Mc|ma)
×P(mb|ma, arg = 1) × P(... |mb)
×P(mc|ma,arg = 2) × P(... |mc) (1)
</equation>
<bodyText confidence="0.999585818181818">
where “arg” refers to the position of the child se-
mantic category in the argument list.
Motivated by Collins’ syntactic parsing models
(Collins, 2003), we consider the generation process
for a hybrid sequence from an MR production as a
Markov process.
Given the assumption that each MR production
has at most two semantic categories in its arguments
(any production can be transformed into a sequence
of productions of this form), Table 1 includes the list
of all possible hybrid patterns.
</bodyText>
<table confidence="0.6528618">
# RHS Hybrid Pattern # Patterns
0 m → w 1
1 m → [w]Y[w] 4
2 m → [w]Y[w]Z[w] 8
m → [w]Z[w]Y[w] 8
</table>
<tableCaption confidence="0.995039">
Table 1: A list of hybrid patterns, [] denotes optional
</tableCaption>
<bodyText confidence="0.9995874">
In this table, m is an MR production, Y and Z
are respectively the first and second child seman-
tic category in m’s argument list. The symbol w
refers to a contiguous sequence of NL words, and
anything inside [] can be optionally omitted. The
last row contains hybrid patterns that reflect reorder-
ing of one production’s child semantic categories
during the generation process. For example, con-
sider the case that the MR production STATE :
exclude (STATE1 STATE2) generates a hybrid se-
quence STATE1 do not STATE2, the hybrid pattern
m → YwZ is associated with this generation step.
For the example hybrid tree in Figure 2, we can
decompose the probability for generating the hybrid
sequence as follows:
</bodyText>
<equation confidence="0.999647333333333">
P(w1 Mb w2 Mc|ma) = P(m → wYwZ|ma) × P(w1|ma)
×P(Mb|ma,w1) × P(w2|ma,w1,Mb)
×P(Mc|ma, w1, Mb, w2) × P(END|ma, w1, Mb, w2, Mc) (2)
</equation>
<bodyText confidence="0.9999595">
Note that unigram, bigram, or trigram assump-
tions can be made here for generating NL words and
semantic categories. For example, under a bigram
assumption, the second to last term can be written
as P(Mc|ma, w1, Mb, w2) ≡ P(Mc|ma, wk2), where
wk2 is the last word in w2. We call such additional
information that we condition on, the context.
Note that our generative model is different from
the synchronous context free grammars (SCFG) in
a number of ways. A standard SCFG produces a
correspondence between a pair of trees while our
model produces a single hybrid tree that represents
</bodyText>
<figure confidence="0.735646444444444">
w1 Mb
w2 Mc
mc
. . . . . .
. . . . . .
mb
STATE
STATE : state(all)
states
</figure>
<page confidence="0.985003">
785
</page>
<bodyText confidence="0.999987857142857">
the correspondence between a sentence and a tree.
Also, SCFGs use a finite set of context-free rewrite
rules to define the model, where the rules are possi-
bly weighted. In contrast, we make use of the more
flexible Markov models at each level of the genera-
tive process, which allows us to potentially produce
a far wider range of possible trees.
</bodyText>
<sectionHeader confidence="0.974408" genericHeader="method">
5 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.9999691">
There are three categories of parameters used in the
model. The first category of parameters models
the generation of new MR productions from their
parent MR productions: e.g., P(mb|mQ,arg = 1);
the second models the generation of a hybrid se-
quence from an MR production: e.g., P(w1|mQ),
P(Mb|mQ, w1); the last models the selection of a hy-
brid pattern given an MR production, e.g., P(m →
w til|mQ). We will estimate parameters from all cate-
gories, with the following constraints:
</bodyText>
<equation confidence="0.891764">
1.
Em′ p(m′|mj, arg=k)=1 for all j and k = 1, 2.
</equation>
<bodyText confidence="0.998063">
These parameters model the MR structures, and
can be referred to as MIt model parameters.
</bodyText>
<listItem confidence="0.744726666666667">
2. Et B(t|mj, A)=1 for all j, where t is a NL word,
the “END” symbol, or a semantic category. A
is the context associated with mj and t.
</listItem>
<bodyText confidence="0.960709">
These parameters model the emission of NL
words, the “END” symbol, and child semantic
categories from an MR production. We call
them emission parameters.
3. Er 0(r|mj) = 1 for all j, where r is a hybrid
pattern listed in Table 1.
These parameters model the selection of hybrid
patterns. We name them pattern parameters.
With different context assumptions, we reach dif-
ferent variations of the model. In particular, we con-
sider three assumptions, as follows:
</bodyText>
<equation confidence="0.8951565">
Model I We make the following assumption:
B(tk|mj,A) = P(tk|mj) (3)
</equation>
<bodyText confidence="0.9850235">
where tk is a semantic category or a NL word, and
mj is an MR production.
In other words, generation of the next NL word
depends on its direct parent MR production only.
Such a Unigram Model may help in recall (the num-
ber of correct outputs over the total number of in-
puts), because it requires the least data to estimate.
Model II We make the following assumption:
</bodyText>
<equation confidence="0.999681">
B(tk|mj,A) = P(tk|mj,tk−1) (4)
</equation>
<bodyText confidence="0.982051090909091">
where tk−1 is the semantic category or NL word to
the left of tk, i.e., the previous semantic category or
NL word.
In other words, generation of the next NL word
depends on its direct parent MR production as well
as the previously generated NL word or semantic
category only. This model is also referred to as Bi-
gram Model. This model may help in precision (the
number of correct outputs over the total number of
outputs), because it conditions on a larger context.
Model III We make the following assumption:
</bodyText>
<equation confidence="0.9999255">
1 ( )
B(tk|mj,A) = 2 X P(tk|mj) + P(tk|mj,tk−1) (5)
</equation>
<bodyText confidence="0.99983675">
We can view this model, called the Mixgram
Model, as an interpolation between Model I and II.
This model gives us a balanced score for both preci-
sion and recall.
</bodyText>
<subsectionHeader confidence="0.996324">
5.1 Modeling Meaning Representation
</subsectionHeader>
<bodyText confidence="0.999788333333334">
The MR model parameters can be estimated inde-
pendently from the other two. These parameters can
be viewed as the “language model” parameters for
the MR structure, and can be estimated directly from
the corpus by simply reading off the counts of occur-
rences of MR productions in MR structures over the
training corpus. To resolve data sparseness problem,
a variant of the bigram Katz Back-Off Model (Katz,
1987) is employed here for smoothing.
</bodyText>
<subsectionHeader confidence="0.999484">
5.2 Learning the Generative Parameters
</subsectionHeader>
<bodyText confidence="0.999917615384615">
Learning the remaining two categories ofparameters
is more challenging. In a conventional PCFG pars-
ing task, during the training phase, the correct cor-
respondence between NL words and syntactic struc-
tures is fully accessible. In other words, there is a
single deterministic derivation associated with each
training instance. Therefore model parameters can
be directly estimated from the training corpus by
counting. However, in our task, the correct corre-
spondence between NL words and MR structures is
unknown. Many possible derivations could reach
the same NL-MR pair, where each such derivation
forms a hybrid tree.
</bodyText>
<page confidence="0.991299">
786
</page>
<bodyText confidence="0.99994">
The hybrid tree is constructed using hidden vari-
ables and estimated from the training set. An effi-
cient inside-outside style algorithm can be used for
model estimation, similar to that used in (Yamada
and Knight, 2001), as discussed next.
</bodyText>
<subsubsectionHeader confidence="0.696872">
5.2.1 The Inside-Outside Algorithm with EM
</subsubsectionHeader>
<bodyText confidence="0.999673416666667">
In this section, we discuss how to estimate the
emission and pattern parameters with the Expecta-
tion Maximization (EM) algorithm (Dempster et al.,
1977), by using an inside-outside (Baker, 1979) dy-
namic programming approach.
Denote ni ≡ hmi, wii as the i-th training instance,
where mi and wi are the MR structure and the NL
sentence of the i-th instance respectively. We also
denote nv ≡ hmv, wvi as an aligned pair of MR
substructure and contiguous NL substring, where
the MR substructure rooted by MR production mv
will correspond to (i.e., hierarchically generate) the
NL substring wv. The symbol h is used to de-
note a hybrid sequence, and the function Parent(h)
gives the unique MR substructure-NL subsequence
pair which can be decomposed as h. Parent(nv) re-
turns the set of all possible hybrid sequences un-
der which the pair nv can be generated. Similarly,
Children(h) gives the NL-MR pairs that appear di-
rectly below the hybrid sequence h in a hybrid tree,
and Children(n) returns the set of all possible hybrid
sequences that n can be decomposed as. Figure 4
gives a packed tree structure representing the rela-
tions between the entities.
</bodyText>
<figure confidence="0.492763">
hp1 ∈ Parent(nv) hpm ∈ Parent(nv)
</figure>
<figureCaption confidence="0.9987375">
Figure 4: A packed tree structure representing the relations
between hybrid sequences and NL-MR pairs
</figureCaption>
<bodyText confidence="0.999974">
The formulas for computing inside and outside
probabilities as well as the equations for updating
parameters are given in Figure 5. We use a CKY-
style parse chart for tracking the probabilities.
</bodyText>
<subsectionHeader confidence="0.800696">
5.2.2 Smoothing
</subsectionHeader>
<bodyText confidence="0.9994002">
It is reasonable to believe that different MR pro-
ductions that share identical function symbols are
likely to generate NL words with similar distribu-
tion, regardless of semantic categories. For example,
The inside (β) probabilities are defined as
</bodyText>
<listItem confidence="0.981816">
• If nv ≡ hmv, wvi is leaf
</listItem>
<equation confidence="0.983944">
β(nv) = P(wv|mv) (6)
</equation>
<listItem confidence="0.505363">
• If nv ≡ hmv, wvi is not leaf
</listItem>
<equation confidence="0.997975333333333">
� ri �
P(h|mv) × β(nv′) (7)
h∈Children(nv) nv′∈Children(h)
</equation>
<bodyText confidence="0.983658">
The outside (α) probabilities are defined as
</bodyText>
<listItem confidence="0.980113">
• If nv ≡ hmv, wvi is root
</listItem>
<equation confidence="0.932783">
α(nv) = 1 (8)
</equation>
<listItem confidence="0.799852666666667">
• If nv ≡ hmv, wvi is not root
Parameter Update
• Update the emission parameter
</listItem>
<bodyText confidence="0.9496245">
The count ci(t, mv, Λk), where t is a NL word
or a semantic category, for an instance pair ni ≡
</bodyText>
<equation confidence="0.980624769230769">
hmi, wii:
�
ci(t, mv, Λk) = 1
×
β(ni)
(t,mv,Λk) in h∈Children(mv)
ri
×P(h|mv) ×
niv′ ∈Children(h)
The emission parameter is re-estimated as:
Ei ci(t,mv, Λk)
θ′(t|mv,Λk) = (10)
Et′ Ei ci(t′, mv, Λk)
</equation>
<bodyText confidence="0.936179">
• Update the pattern parameter
The count ci(r, mv), where r is a hybrid pattern,
for an instance pair ni ≡ hmi, wii:
</bodyText>
<equation confidence="0.9875108">
�
1
β(ni) × (r,mv) in h∈Children(mv)
ri
×P(h|mv) ×
niv′ ∈Children(h)
The pattern parameter is re-estimated as:
Ei ci(r,mv)
φ′(r|mv) = (11)
Er′ Ei ci(r′, mv)
</equation>
<figureCaption confidence="0.9822165">
Figure 5: The inside/outside formulas as well as update
equations for EM
</figureCaption>
<bodyText confidence="0.98860975">
RIVER: largest (RIVER) and CITY: largest (CITY)
are both likely to generate the word “biggest”.
In view of this, a smoothing technique is de-
ployed. We assume half of the time words can
</bodyText>
<equation confidence="0.982162826086957">
Hybrid Sequence Contains
hc1 ∈ Children(nv)
hcn ∈ Children(nv)
nv′ ≡ hmv′, wv′i
Can be Decomposed As
nv ≡ hmv, wvi
�β(nv) =
�α(nv) =
h∈Parent(nv)
� � ri
×P h|Parent(h) ×
nv′ ∈Children(h),v′*v
� � �
α Parent(h)
�
β(nv′) (9)
(α(niv)
�
β(ni v′)
ci(r,mv) =
(α(niv)
�
β(ni v′)
</equation>
<page confidence="0.985811">
787
</page>
<bodyText confidence="0.999108">
be generated from the production’s function symbol
alone if it is not empty. Mathematically, assuming
ma with function symbol pa, for a NL word or se-
mantic category t, we have:
</bodyText>
<equation confidence="0.980422">
� θe(t|ma,A) If pa is empty
( �
θ(t|ma,A) = θe(t|ma,A) + θe(t|pa,A) /2 otherwise
</equation>
<bodyText confidence="0.999488">
where θe models the generation of t from an MR
production or its function symbol, together with the
context A.
</bodyText>
<sectionHeader confidence="0.9450415" genericHeader="method">
6 A Dynamic Programming Algorithm for
Inside-Outside Computation
</sectionHeader>
<bodyText confidence="0.99999146875">
Though the inside-outside approach already em-
ploys packed representations for dynamic program-
ming, a naive implementation of the inference algo-
rithm will still require O(n6m) time for 1 EM iter-
ation, where n and m are the length of the NL sen-
tence and the size of the MR structure respectively.
This is not very practical as in one of the corpora we
look at, n and m can be up to 45 and 20 respectively.
In this section, we develop an efficient dynamic
programming algorithm that enables the inference
to run in O(n3m) time. The idea is as follows. In-
stead of treating each possible hybrid sequence as
a separate rule, we efficiently aggregate the already
computed probability scores for hybrid sequences
that share identical hybrid patterns. Such aggregated
scores can then be used for subsequent computa-
tions. By doing this, we can effectively avoid a large
amount of redundant computations. The algorithm
supports both unigram and bigram context assump-
tions. For clarity and ease of presentation, we pri-
marily make the unigram assumption throughout our
discussion.
We use β (mv, wv) to denote the inside probabil-
ity for mv-wv pair, br[mv, wv, c] to denote the aggre-
gated probabilities for the MR sub-structure mv to
generate all possible hybrid sequences based on wv
with pattern r that covers its c-th child only. In addi-
tion, we use w(i,j) to denote a subsequence of w with
start index i (inclusive) and end index j (exclusive).
We also use βr~mv, wv• to denote the aggregated in-
side probability for the pair hmv, wvi, if the hybrid
pattern is restricted to r only. By definition we have:
</bodyText>
<equation confidence="0.9953655">
Eβ (mv, wv) =
r φ(r|mv)×βr~mv, wv•×θ(END|mv) (12)
</equation>
<bodyText confidence="0.974932833333333">
Relations between βr and br can also be estab-
lished. For example, if mv has one child semantic
category, we have:
βm→wY~mv, wv• = bm→wY[mv, wv, 1] (13)
For the case when mv has two child semantic cat-
egories as arguments, we have, for example:
</bodyText>
<equation confidence="0.990741">
Eβm→wYZw~mv, w(i,j)• = bm→wY[mv, w(i,k), 1]
i+2≤k≤j−2
×bm→Yw[mv, w(k,j), 2] (14)
</equation>
<bodyText confidence="0.9857565">
Note that there also exist relations amongst b
terms for more efficient computation, for example:
</bodyText>
<equation confidence="0.834362666666667">
bm→wY[mv, w(i,j), c] = θ(wi|mv)
� �
× bm→wY[mv, w(i+1,j), c] + bm→Y[mv, w(i+1,j), c] (15)
</equation>
<bodyText confidence="0.9995885">
Analogous but more complex formulas are used
for computing the outside probabilities. Updating of
parameters can be incorporated into the computation
of outside probabilities efficiently.
</bodyText>
<sectionHeader confidence="0.991379" genericHeader="method">
7 Decoding
</sectionHeader>
<bodyText confidence="0.976264">
In the decoding phase, we want to find the optimal
MR structure m∗ given a new NL sentence w:
</bodyText>
<equation confidence="0.978544">
m∗ = arg max
m� P(�m|�w) = arg max P(m, T|w) (16)
m� T
E
</equation>
<bodyText confidence="0.9084214">
where T is a possible hybrid tree associated with
the m-w pair. However, it is expensive to compute
the summation over all possible hybrid trees. We
therefore find the most likely hybrid tree instead:
P(m, T|w)=arg max max
</bodyText>
<equation confidence="0.991745">
m� T P(w,�m, T) (17)
</equation>
<bodyText confidence="0.969094461538461">
We have implemented an exact top-k decoding al-
gorithm for this task. Dynamic programming tech-
niques similar to those discussed in Section 6 can
also be applied when retrieving the top candidates.
We also find the Viterbi hybrid tree given a NL-
MR pair, which can be done in an analogous way.
This tree will be useful for reranking.
8 Reranking and Filtering of Predictions
Due to the various independence assumptions we
have made, the model lacks the ability to express
some long range dependencies. We therefore post-
process the best candidate predictions with a dis-
criminative reranking algorithm.
</bodyText>
<equation confidence="0.670109">
m∗=arg max
m�
max
T
</equation>
<page confidence="0.983724">
788
</page>
<table confidence="0.750778">
Feature Type Description Example
1. Hybrid Rule A MR production and its child hybrid form f1 : STATE: lOC 1(RIVER) → have RIVER
2. Expanded Hybrid Rule A MR production and its child hybrid form expanded fp : STATE : lOC 1(RIVER) → (have, RIVER: river(all))
3. Long-range Unigram A MR production and a NL word appearing below in tree fj : STATE : eXClude(STATE STATE) → rivers
4. Grandchild Unigram A MR production and its grandchild NL word f4 : STATE : lOC 1(RIVER) → rivers
5. Two Level Unigram A MR production, its parent production, and its child NL word f5 : (RIVER: river(all), STATE: lOC 1(RIVER)) → rivers
6. Model Log-Probability Logarithm of base model’s joint probability log (�P(w, m, T) ).
</table>
<tableCaption confidence="0.9643385">
Table 2: All the features used. There is one feature for each possible combination, under feature type 1-5. It takes value 1 if
the combination is present, and 0 otherwise. Feature 6 takes real values.
</tableCaption>
<subsectionHeader confidence="0.99898">
8.1 The Averaged Perceptron Algorithm with
Separating Plane
</subsectionHeader>
<bodyText confidence="0.999986382352941">
The averaged perceptron algorithm (Collins, 2002)
has previously been applied to various NLP tasks
(Collins, 2002; Collins, 2001) for discriminative
reranking. The detailed algorithm can be found in
(Collins, 2002). In this section, we extend the con-
ventional averaged perceptron by introducing an ex-
plicit separating plane on the feature space.
Our reranking approach requires three compo-
nents during training: a GEN function that defines
for each NL sentence a set of candidate hybrid trees;
a single correct reference hybrid tree for each train-
ing instance; and a feature function (D that defines a
mapping from a hybrid tree to a feature vector. The
algorithm learns a weight vector w that associates a
weight to each feature, such that a score w·(D(T) can
be assigned to each candidate hybrid tree T. Given
a new instance, the hybrid tree with the highest score
is then picked by the algorithm as the output.
In this task, the GEN function is defined as the
output hybrid trees of the top-k (k is set to 50 in our
experiments) decoding algorithm, given the learned
model parameters. The correct reference hybrid tree
is determined by running the Viterbi algorithm on
each training NL-MR pair. The feature function is
discussed in section 8.2.
While conventional perceptron algorithms usually
optimize the accuracy measure, we extend it to allow
optimization of the F-measure by introducing an ex-
plicit separating plane on the feature space that re-
jects certain predictions even when they score high-
est. The idea is to find a threshold b after w is
learned, such that a prediction with score below b
gets rejected. We pick the threshold that leads to the
optimal F-measure when applied to the training set.
</bodyText>
<subsectionHeader confidence="0.898553">
8.2 Features
</subsectionHeader>
<bodyText confidence="0.899295777777778">
We list in Table 2 the set of features we used. Ex-
amples are given based on the hybrid tree in Figure
3. Some of the them are adapted from (Collins and
Koo, 2005) for a natural language parsing task. Fea-
tures 1-5 are indicator functions (i.e., it takes value
1 if a certain combination as the ones listed in Table
2 is present, 0 otherwise), while feature 6 is real val-
ued. Features that do not appear more than once in
the training set are discarded.
</bodyText>
<sectionHeader confidence="0.9825" genericHeader="evaluation">
9 Evaluation
</sectionHeader>
<bodyText confidence="0.999990857142857">
Our evaluations were performed on two corpora,
GEOQUERY and ROBOCUP. The GEOQUERY cor-
pus contains MR defined by a Prolog-based lan-
guage used in querying a database on U.S. geogra-
phy. The ROBOCUP corpus contains MR defined by
a coaching language used in a robot coaching com-
petition. There are in total 880 and 300 instances for
the two corpora respectively. Standard 10-fold cross
validations were performed and the micro-averaged
results are presented in this section. To make our
system directly comparable to previous systems, all
our experiments were based on identical training and
test data splits of both corpora as reported in the ex-
periments of Wong and Mooney (2006).
</bodyText>
<subsectionHeader confidence="0.998486">
9.1 Training Methodology
</subsectionHeader>
<bodyText confidence="0.999944714285714">
Given a training set, we first run a variant of IBM
alignment model 1 (Brown et al., 1993) for 100 iter-
ations, and then initialize Model I with the learned
parameter values. This IBM model is a word-to-
word alignment model that does not model word
order, so we do not have to linearize the hierarchi-
cal MR structure. Given this initialization, we train
Model I for 100 EM iterations and use the learned
parameters to initialize Model II which is trained for
another 100 EM iterations. Model III is simply an
interpolation of the above two models. As for the
reranking phase, we initialize the weight vector with
the zero vector 0, and run the averaged perceptron
algorithm for 10 iterations.
</bodyText>
<page confidence="0.995123">
789
</page>
<subsectionHeader confidence="0.984655">
9.2 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.999835352941177">
Following Wong (2007) and other previous work,
we report performance in terms of Precision (per-
centage of answered NL sentences that are correct),
Recall (percentage of correctly answered NL sen-
tences, out of all NL sentences) and F-score (har-
monic mean of Precision and Recall).
Again following Wong (2007), we define the cor-
rect output MR structure as follows. For the GEO-
QUERY corpus, an MR structure is considered cor-
rect if and only if it retrieves identical results as
the reference MR structure when both are issued as
queries to the underlying Prolog database. For the
ROBOCUP corpus, an MR structure is considered
correct if and only if it has the same string represen-
tation as the reference MR structure, up to reorder-
ing of children of MR productions whose function
symbols are commutative, such as and, or, etc.
</bodyText>
<subsectionHeader confidence="0.887903">
9.3 Comparison over Three Models
</subsectionHeader>
<table confidence="0.998799">
Model GEOQUERY (880) ROBOCUP (300)
Prec. Rec. F Prec. Rec. F
I 81.3 77.1 79.1 71.1 64.0 67.4
II 89.0 76.0 82.0 82.4 57.7 67.8
III 86.2 81.8 84.0 70.4 63.3 66.7
I+R 87.5 80.5 83.8 79.1 67.0 72.6
II+R 93.2 73.6 82.3 88.4 56.0 68.6
III+R 89.3 81.5 85.2 82.5 67.7 74.4
</table>
<tableCaption confidence="0.934288">
Table 3: Performance comparison over three models
(Prec.:precision, Rec.:recall, +R: with reranking)
</tableCaption>
<bodyText confidence="0.99998996">
We evaluated the three models, with and with-
out reranking. The results are presented in Table 3.
Comparing Model I and Model II, we noticed that
for both corpora, Model I in general achieves bet-
ter recall while Model II achieves better precision.
This observation conforms to our earlier expecta-
tions. Model III, as an interpolation of the above two
models, achieves a much better F-measure on GEO-
QUERY corpus. However, it is shown to be less ef-
fective on ROBOCUP corpus. We noticed that com-
pared to the GEOQUERY corpus, ROBOCUP corpus
contains longer sentences, larger MR structures, and
a significant amount of non-compositionality. These
factors combine to present a challenging problem for
parsing with the generative model. Interestingly, al-
though Model III fails to produce better best pre-
dictions for this corpus, we found that its top-k list
contains a relatively larger number of correct pre-
dictions than Model I or Model II. This indicates
the possibility of enhancing the performance with
reranking.
The reranking approach is shown to be quite ef-
fective. We observe a consistent improvement in
both precision and F-measure after employing the
reranking phase for each model.
</bodyText>
<subsectionHeader confidence="0.998223">
9.4 Comparison with Other Models
</subsectionHeader>
<bodyText confidence="0.99999680952381">
Among all the previous models, SILT, WASP, and
KRISP are directly comparable to our model. They
required the same amount of supervision as our sys-
tem and were evaluated on the same corpora.
We compare our model with these models in Ta-
ble 4, where the performance scores for the previous
systems are taken from (Wong, 2007). For GEO-
QUERY corpus, our model performs substantially
better than all the three previous models, with a no-
table improvement in the recall score. In fact, if we
look at the recall scores alone, our best-performing
model achieves a 6.7% and 9.8% absolute improve-
ment over two other state-of-the-art models WASP
and KRISP respectively. This indicates that over-
all, our model is able to handle over 25% of the
inputs that could not be handled by previous sys-
tems. On the other hand, in terms of F-measure,
we gain a 4.1% absolute improvement over KRISP,
which leads to an error reduction rate of 22%. On
the ROBOCUP corpus, our model’s performance is
also ranked the highest1.
</bodyText>
<table confidence="0.9988905">
System GEOQUERY (880) ROBOCUP (300)
Prec. Rec. F Prec. Rec. F
SILT 89.0 54.1 67.3 83.9 50.7 63.2
WASP 87.2 74.8 80.5 88.9 61.9 73.0
KRISP 93.3 71.7 81.1 85.2 61.9 71.7
Model III+R 89.3 81.5 85.2 82.5 67.7 74.4
</table>
<tableCaption confidence="0.9987865">
Table 4: Performance comparison with other directly com-
parable systems
</tableCaption>
<subsectionHeader confidence="0.973715">
9.5 Performance on Other Languages
</subsectionHeader>
<bodyText confidence="0.9999326">
As a generic model that requires minimal assump-
tions on the natural language, our model is natural
language independent and is able to handle various
other natural languages than English. To validate
this point, we evaluated our system on a subset of
</bodyText>
<footnote confidence="0.999464">
1We are unable to perform statistical significance tests be-
cause the detailed performance for each fold of previously pub-
lished research work is not available.
</footnote>
<page confidence="0.993766">
790
</page>
<bodyText confidence="0.872081428571429">
the GEOQUERY corpus consisting of 250 instances,
with four different NL annotations.
As we can see from Table 5, our model is able
to achieve performance comparable to WASP as re-
ported by Wong (2007).
ments on this paper. The research is partially sup-
ported by ARF grant R-252-000-240-112.
</bodyText>
<table confidence="0.979986666666667">
References
System English Spanish
Prec. Rec. F Prec. Rec. F
WASP 95.42 70.00 80.76 91.99 72.40 81.03
Model III+R 91.46 72.80 81.07 95.19 79.20 86.46
System Japanese Turkish
Prec. Rec. F Prec. Rec. F
WASP 91.98 74.40 82.86 96.96 62.40 75.93
Model III+R 87.56 76.00 81.37 93.82 66.80 78.04
</table>
<tableCaption confidence="0.9930935">
Table 5: Performance on different natural languages for
GEOQUERY-250 corpus
</tableCaption>
<bodyText confidence="0.999910777777778">
Our model is generic, which requires no domain-
dependent knowledge and should be applicable to
a wide range of different domains. Like all re-
search in this area, the ultimate goal is to scale to
more complex, open-domain language understand-
ing problems. In future, we would like to create a
larger corpus in another domain with multiple natu-
ral language annotations to further evaluate the scal-
ability and portability of our approach.
</bodyText>
<sectionHeader confidence="0.994152" genericHeader="conclusions">
10 Conclusions
</sectionHeader>
<bodyText confidence="0.999979">
We presented a new generative model that simulta-
neously produces both NL sentences and their cor-
responding MR structures. The model can be effec-
tively applied to the task of transforming NL sen-
tences to their MR structures. We also developed
a new dynamic programming algorithm for efficient
training and decoding. We demonstrated that this
approach, augmented with a discriminative rerank-
ing technique, achieves state-of-the-art performance
when tested on standard benchmark corpora.
In future, we would like to extend the current
model to have a wider range of support of MR for-
malisms, such as the one with lambda-calculus sup-
port. We are also interested in investigating ways to
apply the generative model to the inverse task: gen-
eration of a NL sentence that explains a given MR
structure.
</bodyText>
<sectionHeader confidence="0.998822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99942425">
The authors would like to thank Leslie Pack Kael-
bling for her valuable feedback and comments on
this research. The authors would also like to thank
the anonymous reviewers for their thoughtful com-
</bodyText>
<reference confidence="0.999121625">
J. K. Baker. 1979. Trainable grammars for speech recog-
nition. Journal of the Acoustical Society of America,
65:S132.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263–311.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25–70.
M. Collins. 2001. Ranking algorithms for named-entity
extraction: boosting and the voted perceptron. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2002), pages
489–496.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: theory and experiments with
perceptron algorithms. In Proceedings of the 2002
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002), pages 1–8.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589–637.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39(1):1–38.
R. Ge and R. J. Mooney. 2005. A statistical semantic
parser that integrates syntax and semantics. In Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL 2005), pages 9–
16.
R. Ge and R. J. Mooney. 2006. Discriminative rerank-
ing for semantic parsing. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the Association
for Computational Linguistics (COLING/ACL 2006),
pages 263–270.
R. J. Kate and R. J. Mooney. 2006. Using string-kernels
for learning semantic parsers. In Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING/ACL
2006), pages 913–920.
R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learn-
ing to transform natural to formal languages. In Pro-
ceedings of the Twentieth National Conference on Ar-
tificial Intelligence (AAAI 2005), pages 1062–1068.
</reference>
<page confidence="0.972083">
791
</page>
<reference confidence="0.999373384615385">
S. Katz. 1987. Estimation of probabilities from sparse
data for the language model component of a speech
recognizer. IEEE Transactions on Acoustics, Speech,
and Signal Processing, 35(3):400–401.
L. R. Tang and R. J. Mooney. 2001. Using multiple
clause constructors in inductive logic programming for
semantic parsing. In Proceedings of the 12th Euro-
pean Conference on Machine Learning (ECML 2001),
pages 466–477.
Y. W. Wong and R. J. Mooney. 2006. Learning for
semantic parsing with statistical machine translation.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (HLT-NAACL
2006), pages 439–446.
Y. W. Wong and R. J. Mooney. 2007. Learning syn-
chronous grammars for semantic parsing with lambda
calculus. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics (ACL
2007), pages 960–967.
Y. W. Wong. 2007. Learning for Semantic Parsing and
Natural Language Generation Using Statistical Ma-
chine Translation Techniques. Ph.D. thesis, The Uni-
versity of Texas at Austin.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proceedings of the 39th
Annual Meeting of the Association for Computational
Linguistics, pages 523–530.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
map sentences to logical form: Structured classifica-
tion with probabilistic categorial grammars. In Pro-
ceedings of the 21st Conference on Uncertainty in Ar-
tificial Intelligence.
L. S. Zettlemoyer and M. Collins. 2007. Online learning
of relaxed CCG grammars for parsing to logical form.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), pages 678–687.
</reference>
<page confidence="0.997412">
792
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.910205">
<title confidence="0.998636">A Generative Model for Parsing Natural Language to Representations</title>
<author confidence="0.997753">Hwee Tou Wee Sun</author>
<affiliation confidence="0.98270925">of Computer National University of Luke S. Massachusetts Institute of</affiliation>
<email confidence="0.999047">lsz@csail.mit.edu</email>
<abstract confidence="0.999014315789474">In this paper, we present an algorithm for learning a generative model of natural language sentences together with their formal meaning representations with hierarchical structures. The model is applied to the task of mapping sentences to hierarchical representations of their underlying meaning. We introduce dynamic programming techniques for efficient training and decoding. In experiments, we demonstrate that the model, when coupled with a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<journal>Journal of the Acoustical Society of America,</journal>
<pages>65--132</pages>
<contexts>
<context position="15437" citStr="Baker, 1979" startWordPosition="2581" endWordPosition="2582">R structures is unknown. Many possible derivations could reach the same NL-MR pair, where each such derivation forms a hybrid tree. 786 The hybrid tree is constructed using hidden variables and estimated from the training set. An efficient inside-outside style algorithm can be used for model estimation, similar to that used in (Yamada and Knight, 2001), as discussed next. 5.2.1 The Inside-Outside Algorithm with EM In this section, we discuss how to estimate the emission and pattern parameters with the Expectation Maximization (EM) algorithm (Dempster et al., 1977), by using an inside-outside (Baker, 1979) dynamic programming approach. Denote ni ≡ hmi, wii as the i-th training instance, where mi and wi are the MR structure and the NL sentence of the i-th instance respectively. We also denote nv ≡ hmv, wvi as an aligned pair of MR substructure and contiguous NL substring, where the MR substructure rooted by MR production mv will correspond to (i.e., hierarchically generate) the NL substring wv. The symbol h is used to denote a hybrid sequence, and the function Parent(h) gives the unique MR substructure-NL subsequence pair which can be decomposed as h. Parent(nv) returns the set of all possible h</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>J. K. Baker. 1979. Trainable grammars for speech recognition. Journal of the Acoustical Society of America, 65:S132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A D Pietra</author>
<author>V J D Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4449" citStr="Brown et al., 1993" startWordPosition="678" endWordPosition="681">lar, the flexible generative model leads to notable improvements in recall, the total percentage of sentences that are correctly parsed. 2 Related Work In Section 9, we will compare performance with the three existing systems that were evaluated on the same data sets we consider. SILT (Kate et al., 2005) learns deterministic rules to transform either sentences or their syntactic parse trees to meaning structures. WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has</context>
<context position="26141" citStr="Brown et al., 1993" startWordPosition="4435" endWordPosition="4438">ography. The ROBOCUP corpus contains MR defined by a coaching language used in a robot coaching competition. There are in total 880 and 300 instances for the two corpora respectively. Standard 10-fold cross validations were performed and the micro-averaged results are presented in this section. To make our system directly comparable to previous systems, all our experiments were based on identical training and test data splits of both corpora as reported in the experiments of Wong and Mooney (2006). 9.1 Training Methodology Given a training set, we first run a variant of IBM alignment model 1 (Brown et al., 1993) for 100 iterations, and then initialize Model I with the learned parameter values. This IBM model is a word-toword alignment model that does not model word order, so we do not have to linearize the hierarchical MR structure. Given this initialization, we train Model I for 100 EM iterations and use the learned parameters to initialize Model II which is trained for another 100 EM iterations. Model III is simply an interpolation of the above two models. As for the reranking phase, we initialize the weight vector with the zero vector 0, and run the averaged perceptron algorithm for 10 iterations.</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="25045" citStr="Collins and Koo, 2005" startWordPosition="4248" endWordPosition="4251">al perceptron algorithms usually optimize the accuracy measure, we extend it to allow optimization of the F-measure by introducing an explicit separating plane on the feature space that rejects certain predictions even when they score highest. The idea is to find a threshold b after w is learned, such that a prediction with score below b gets rejected. We pick the threshold that leads to the optimal F-measure when applied to the training set. 8.2 Features We list in Table 2 the set of features we used. Examples are given based on the hybrid tree in Figure 3. Some of the them are adapted from (Collins and Koo, 2005) for a natural language parsing task. Features 1-5 are indicator functions (i.e., it takes value 1 if a certain combination as the ones listed in Table 2 is present, 0 otherwise), while feature 6 is real valued. Features that do not appear more than once in the training set are discarded. 9 Evaluation Our evaluations were performed on two corpora, GEOQUERY and ROBOCUP. The GEOQUERY corpus contains MR defined by a Prolog-based language used in querying a database on U.S. geography. The ROBOCUP corpus contains MR defined by a coaching language used in a robot coaching competition. There are in t</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>M. Collins and T. Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Ranking algorithms for named-entity extraction: boosting and the voted perceptron.</title>
<date>2001</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>489--496</pages>
<contexts>
<context position="23288" citStr="Collins, 2001" startWordPosition="3947" endWordPosition="3948">wo Level Unigram A MR production, its parent production, and its child NL word f5 : (RIVER: river(all), STATE: lOC 1(RIVER)) → rivers 6. Model Log-Probability Logarithm of base model’s joint probability log (�P(w, m, T) ). Table 2: All the features used. There is one feature for each possible combination, under feature type 1-5. It takes value 1 if the combination is present, and 0 otherwise. Feature 6 takes real values. 8.1 The Averaged Perceptron Algorithm with Separating Plane The averaged perceptron algorithm (Collins, 2002) has previously been applied to various NLP tasks (Collins, 2002; Collins, 2001) for discriminative reranking. The detailed algorithm can be found in (Collins, 2002). In this section, we extend the conventional averaged perceptron by introducing an explicit separating plane on the feature space. Our reranking approach requires three components during training: a GEN function that defines for each NL sentence a set of candidate hybrid trees; a single correct reference hybrid tree for each training instance; and a feature function (D that defines a mapping from a hybrid tree to a feature vector. The algorithm learns a weight vector w that associates a weight to each feature</context>
</contexts>
<marker>Collins, 2001</marker>
<rawString>M. Collins. 2001. Ranking algorithms for named-entity extraction: boosting and the voted perceptron. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 489–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>1--8</pages>
<contexts>
<context position="23208" citStr="Collins, 2002" startWordPosition="3935" endWordPosition="3936">MR production and its grandchild NL word f4 : STATE : lOC 1(RIVER) → rivers 5. Two Level Unigram A MR production, its parent production, and its child NL word f5 : (RIVER: river(all), STATE: lOC 1(RIVER)) → rivers 6. Model Log-Probability Logarithm of base model’s joint probability log (�P(w, m, T) ). Table 2: All the features used. There is one feature for each possible combination, under feature type 1-5. It takes value 1 if the combination is present, and 0 otherwise. Feature 6 takes real values. 8.1 The Averaged Perceptron Algorithm with Separating Plane The averaged perceptron algorithm (Collins, 2002) has previously been applied to various NLP tasks (Collins, 2002; Collins, 2001) for discriminative reranking. The detailed algorithm can be found in (Collins, 2002). In this section, we extend the conventional averaged perceptron by introducing an explicit separating plane on the feature space. Our reranking approach requires three components during training: a GEN function that defines for each NL sentence a set of candidate hybrid trees; a single correct reference hybrid tree for each training instance; and a feature function (D that defines a mapping from a hybrid tree to a feature vector.</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="2296" citStr="Collins (2003)" startWordPosition="346" endWordPosition="347">well but is constrained by the use of a single, learned grammar that contains a fixed set of lexical entries and productions. In practice, such a grammar may lack the rules required to correctly parse some of the new test examples. In this paper, we develop an alternative approach that learns a model which does not make use of an explicit grammar but, instead, models the correspondence between sentences and their meanings with a generative process. This model is defined over hybrid trees whose nodes include both natural language words and meaning representation tokens. Inspired by the work of Collins (2003), the generative model builds trees by recursively creating nodes at each level according to a Markov process. This implicit grammar representation leads to flexible learned models that generalize well. In practice, we observe that it can correctly parse a wider range of test examples than previous approaches. The generative model is learned from data that consists of sentences paired with their meaning representations. However, there is no explicit labeling of the correspondence between words and meaning tokens that is necessary for building the hybrid trees. This creates a challenging, hidde</context>
<context position="9062" citStr="Collins, 2003" startWordPosition="1477" endWordPosition="1478">d tree for the training example from Section 3. Note that the leaves of a hybrid tree are always NL tokens. . . . STATE STATE : exclude (STATE STATE) do not STATE STATE: loc 1(RIVER) have RIVER RIVER : river(all) rivers Figure 3: A partial hybrid tree With several independence assumptions, the probability of generating (w, m, T) is defined as: P(w, �m, T) = P(Ma) × P(ma|Ma) × P(w1 Mb w2 Mc|ma) ×P(mb|ma, arg = 1) × P(... |mb) ×P(mc|ma,arg = 2) × P(... |mc) (1) where “arg” refers to the position of the child semantic category in the argument list. Motivated by Collins’ syntactic parsing models (Collins, 2003), we consider the generation process for a hybrid sequence from an MR production as a Markov process. Given the assumption that each MR production has at most two semantic categories in its arguments (any production can be transformed into a sequence of productions of this form), Table 1 includes the list of all possible hybrid patterns. # RHS Hybrid Pattern # Patterns 0 m → w 1 1 m → [w]Y[w] 4 2 m → [w]Y[w]Z[w] 8 m → [w]Z[w]Y[w] 8 Table 1: A list of hybrid patterns, [] denotes optional In this table, m is an MR production, Y and Z are respectively the first and second child semantic category </context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>M. Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="15395" citStr="Dempster et al., 1977" startWordPosition="2573" endWordPosition="2576">k, the correct correspondence between NL words and MR structures is unknown. Many possible derivations could reach the same NL-MR pair, where each such derivation forms a hybrid tree. 786 The hybrid tree is constructed using hidden variables and estimated from the training set. An efficient inside-outside style algorithm can be used for model estimation, similar to that used in (Yamada and Knight, 2001), as discussed next. 5.2.1 The Inside-Outside Algorithm with EM In this section, we discuss how to estimate the emission and pattern parameters with the Expectation Maximization (EM) algorithm (Dempster et al., 1977), by using an inside-outside (Baker, 1979) dynamic programming approach. Denote ni ≡ hmi, wii as the i-th training instance, where mi and wi are the MR structure and the NL sentence of the i-th instance respectively. We also denote nv ≡ hmv, wvi as an aligned pair of MR substructure and contiguous NL substring, where the MR substructure rooted by MR production mv will correspond to (i.e., hierarchically generate) the NL substring wv. The symbol h is used to denote a hybrid sequence, and the function Parent(h) gives the unique MR substructure-NL subsequence pair which can be decomposed as h. Pa</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ge</author>
<author>R J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL</booktitle>
<pages>9--16</pages>
<contexts>
<context position="4937" citStr="Ge and Mooney, 2005" startWordPosition="751" endWordPosition="754">ne translation techniques. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider th</context>
</contexts>
<marker>Ge, Mooney, 2005</marker>
<rawString>R. Ge and R. J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL 2005), pages 9– 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ge</author>
<author>R J Mooney</author>
</authors>
<title>Discriminative reranking for semantic parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL</booktitle>
<pages>263--270</pages>
<contexts>
<context position="4959" citStr="Ge and Mooney, 2006" startWordPosition="755" endWordPosition="758">ques. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider the following instance t</context>
</contexts>
<marker>Ge, Mooney, 2006</marker>
<rawString>R. Ge and R. J. Mooney. 2006. Discriminative reranking for semantic parsing. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL 2006), pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>R J Mooney</author>
</authors>
<title>Using string-kernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL</booktitle>
<pages>913--920</pages>
<contexts>
<context position="1425" citStr="Kate and Mooney, 2006" startWordPosition="197" endWordPosition="200">ranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models. 1 Introduction To enable computers to understand natural human language is one of the classic goals of research in natural language processing. Recently, researchers have developed techniques for learning to map sentences to hierarchical representations of their underlying meaning (Wong and Mooney, 2006; Kate and Mooney, 2006). One common approach is to learn some form of probabilistic grammar which includes a list of lexical items that models the meanings of input words and also includes rules for combining lexical meanings to analyze complete sentences. This approach performs well but is constrained by the use of a single, learned grammar that contains a fixed set of lexical entries and productions. In practice, such a grammar may lack the rules required to correctly parse some of the new test examples. In this paper, we develop an alternative approach that learns a model which does not make use of an explicit gr</context>
<context position="4527" citStr="Kate and Mooney, 2006" startWordPosition="691" endWordPosition="694"> the total percentage of sentences that are correctly parsed. 2 Related Work In Section 9, we will compare performance with the three existing systems that were evaluated on the same data sets we consider. SILT (Kate et al., 2005) learns deterministic rules to transform either sentences or their syntactic parse trees to meaning structures. WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>R. J. Kate and R. J. Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL 2006), pages 913–920.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning to transform natural to formal languages.</title>
<date>2005</date>
<booktitle>In Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI</booktitle>
<pages>1062--1068</pages>
<contexts>
<context position="4135" citStr="Kate et al., 2005" startWordPosition="631" endWordPosition="634">Processing, pages 783–792, Honolulu, October 2008.c�2008 Association for Computational Linguistics To address this problem, we use a simple reranking approach to select a parse from a k-best list of parses. This pipelined approach achieves state-ofthe-art performance on two publicly available corpora. In particular, the flexible generative model leads to notable improvements in recall, the total percentage of sentences that are correctly parsed. 2 Related Work In Section 9, we will compare performance with the three existing systems that were evaluated on the same data sets we consider. SILT (Kate et al., 2005) learns deterministic rules to transform either sentences or their syntactic parse trees to meaning structures. WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there</context>
<context position="5379" citStr="Kate et al., 2005" startWordPosition="818" endWordPosition="821">search that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider the following instance taken from the GEOQUERY corpus (Kate et al., 2005): The NL sentence “How many states do not have rivers ?” consists of 8 words, including punctuation. The MR is a hierarchical tree QUERY: answer (NUM) NUM : count (STATE) STATE: exclude (STATE STATE) STATE: state (all) STATE: loc 1 (RIVER) RIVER: river (all) Figure 1: An example MR structure structure, as shown in Figure 1. Following an inorder traversal of this MR tre</context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>R. J. Kate, Y. W. Wong, and R. J. Mooney. 2005. Learning to transform natural to formal languages. In Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI 2005), pages 1062–1068.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="14266" citStr="Katz, 1987" startWordPosition="2401" endWordPosition="2402">5) We can view this model, called the Mixgram Model, as an interpolation between Model I and II. This model gives us a balanced score for both precision and recall. 5.1 Modeling Meaning Representation The MR model parameters can be estimated independently from the other two. These parameters can be viewed as the “language model” parameters for the MR structure, and can be estimated directly from the corpus by simply reading off the counts of occurrences of MR productions in MR structures over the training corpus. To resolve data sparseness problem, a variant of the bigram Katz Back-Off Model (Katz, 1987) is employed here for smoothing. 5.2 Learning the Generative Parameters Learning the remaining two categories ofparameters is more challenging. In a conventional PCFG parsing task, during the training phase, the correct correspondence between NL words and syntactic structures is fully accessible. In other words, there is a single deterministic derivation associated with each training instance. Therefore model parameters can be directly estimated from the training corpus by counting. However, in our task, the correct correspondence between NL words and MR structures is unknown. Many possible de</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>S. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Tang</author>
<author>R J Mooney</author>
</authors>
<title>Using multiple clause constructors in inductive logic programming for semantic parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the 12th European Conference on Machine Learning (ECML</booktitle>
<pages>466--477</pages>
<contexts>
<context position="5023" citStr="Tang and Mooney, 2001" startWordPosition="765" endWordPosition="768">ing the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider the following instance taken from the GEOQUERY corpus (Kate et al., 2005): The NL senten</context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>L. R. Tang and R. J. Mooney. 2001. Using multiple clause constructors in inductive logic programming for semantic parsing. In Proceedings of the 12th European Conference on Machine Learning (ECML 2001), pages 466–477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL</booktitle>
<pages>439--446</pages>
<contexts>
<context position="1401" citStr="Wong and Mooney, 2006" startWordPosition="193" endWordPosition="196">ith a discriminative reranking technique, achieves state-of-the-art performance when tested on two publicly available corpora. The generative model degrades robustly when presented with instances that are different from those seen in training. This allows a notable improvement in recall compared to previous models. 1 Introduction To enable computers to understand natural human language is one of the classic goals of research in natural language processing. Recently, researchers have developed techniques for learning to map sentences to hierarchical representations of their underlying meaning (Wong and Mooney, 2006; Kate and Mooney, 2006). One common approach is to learn some form of probabilistic grammar which includes a list of lexical items that models the meanings of input words and also includes rules for combining lexical meanings to analyze complete sentences. This approach performs well but is constrained by the use of a single, learned grammar that contains a fixed set of lexical entries and productions. In practice, such a grammar may lack the rules required to correctly parse some of the new test examples. In this paper, we develop an alternative approach that learns a model which does not ma</context>
<context position="4275" citStr="Wong and Mooney, 2006" startWordPosition="651" endWordPosition="654">ple reranking approach to select a parse from a k-best list of parses. This pipelined approach achieves state-ofthe-art performance on two publicly available corpora. In particular, the flexible generative model leads to notable improvements in recall, the total percentage of sentences that are correctly parsed. 2 Related Work In Section 9, we will compare performance with the three existing systems that were evaluated on the same data sets we consider. SILT (Kate et al., 2005) learns deterministic rules to transform either sentences or their syntactic parse trees to meaning structures. WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques. It acquires a set of synchronous lexical entries by running the IBM alignment model (Brown et al., 1993) and learns a log-linear model to weight parses. KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision</context>
<context position="26024" citStr="Wong and Mooney (2006)" startWordPosition="4414" endWordPosition="4417">Y and ROBOCUP. The GEOQUERY corpus contains MR defined by a Prolog-based language used in querying a database on U.S. geography. The ROBOCUP corpus contains MR defined by a coaching language used in a robot coaching competition. There are in total 880 and 300 instances for the two corpora respectively. Standard 10-fold cross validations were performed and the micro-averaged results are presented in this section. To make our system directly comparable to previous systems, all our experiments were based on identical training and test data splits of both corpora as reported in the experiments of Wong and Mooney (2006). 9.1 Training Methodology Given a training set, we first run a variant of IBM alignment model 1 (Brown et al., 1993) for 100 iterations, and then initialize Model I with the learned parameter values. This IBM model is a word-toword alignment model that does not model word order, so we do not have to linearize the hierarchical MR structure. Given this initialization, we train Model I for 100 EM iterations and use the learned parameters to initialize Model II which is trained for another 100 EM iterations. Model III is simply an interpolation of the above two models. As for the reranking phase,</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Y. W. Wong and R. J. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2006), pages 439–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>960--967</pages>
<contexts>
<context position="5150" citStr="Wong and Mooney, 2007" startWordPosition="783" endWordPosition="786">s a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider the following instance taken from the GEOQUERY corpus (Kate et al., 2005): The NL sentence “How many states do not have rivers ?” consists of 8 words, including punctuation. The MR is a hierarchical tree QUERY: answ</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Y. W. Wong and R. J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL 2007), pages 960–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
</authors>
<title>Learning for Semantic Parsing and Natural Language Generation Using Statistical Machine Translation Techniques.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>The University of Texas at Austin.</institution>
<contexts>
<context position="26794" citStr="Wong (2007)" startWordPosition="4548" endWordPosition="4549">odel I with the learned parameter values. This IBM model is a word-toword alignment model that does not model word order, so we do not have to linearize the hierarchical MR structure. Given this initialization, we train Model I for 100 EM iterations and use the learned parameters to initialize Model II which is trained for another 100 EM iterations. Model III is simply an interpolation of the above two models. As for the reranking phase, we initialize the weight vector with the zero vector 0, and run the averaged perceptron algorithm for 10 iterations. 789 9.2 Evaluation Methodology Following Wong (2007) and other previous work, we report performance in terms of Precision (percentage of answered NL sentences that are correct), Recall (percentage of correctly answered NL sentences, out of all NL sentences) and F-score (harmonic mean of Precision and Recall). Again following Wong (2007), we define the correct output MR structure as follows. For the GEOQUERY corpus, an MR structure is considered correct if and only if it retrieves identical results as the reference MR structure when both are issued as queries to the underlying Prolog database. For the ROBOCUP corpus, an MR structure is considere</context>
<context position="29537" citStr="Wong, 2007" startWordPosition="5010" endWordPosition="5011"> II. This indicates the possibility of enhancing the performance with reranking. The reranking approach is shown to be quite effective. We observe a consistent improvement in both precision and F-measure after employing the reranking phase for each model. 9.4 Comparison with Other Models Among all the previous models, SILT, WASP, and KRISP are directly comparable to our model. They required the same amount of supervision as our system and were evaluated on the same corpora. We compare our model with these models in Table 4, where the performance scores for the previous systems are taken from (Wong, 2007). For GEOQUERY corpus, our model performs substantially better than all the three previous models, with a notable improvement in the recall score. In fact, if we look at the recall scores alone, our best-performing model achieves a 6.7% and 9.8% absolute improvement over two other state-of-the-art models WASP and KRISP respectively. This indicates that overall, our model is able to handle over 25% of the inputs that could not be handled by previous systems. On the other hand, in terms of F-measure, we gain a 4.1% absolute improvement over KRISP, which leads to an error reduction rate of 22%. O</context>
<context position="31142" citStr="Wong (2007)" startWordPosition="5284" endWordPosition="5285">a generic model that requires minimal assumptions on the natural language, our model is natural language independent and is able to handle various other natural languages than English. To validate this point, we evaluated our system on a subset of 1We are unable to perform statistical significance tests because the detailed performance for each fold of previously published research work is not available. 790 the GEOQUERY corpus consisting of 250 instances, with four different NL annotations. As we can see from Table 5, our model is able to achieve performance comparable to WASP as reported by Wong (2007). ments on this paper. The research is partially supported by ARF grant R-252-000-240-112. References System English Spanish Prec. Rec. F Prec. Rec. F WASP 95.42 70.00 80.76 91.99 72.40 81.03 Model III+R 91.46 72.80 81.07 95.19 79.20 86.46 System Japanese Turkish Prec. Rec. F Prec. Rec. F WASP 91.98 74.40 82.86 96.96 62.40 75.93 Model III+R 87.56 76.00 81.37 93.82 66.80 78.04 Table 5: Performance on different natural languages for GEOQUERY-250 corpus Our model is generic, which requires no domaindependent knowledge and should be applicable to a wide range of different domains. Like all researc</context>
</contexts>
<marker>Wong, 2007</marker>
<rawString>Y. W. Wong. 2007. Learning for Semantic Parsing and Natural Language Generation Using Statistical Machine Translation Techniques. Ph.D. thesis, The University of Texas at Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="15179" citStr="Yamada and Knight, 2001" startWordPosition="2540" endWordPosition="2543">essible. In other words, there is a single deterministic derivation associated with each training instance. Therefore model parameters can be directly estimated from the training corpus by counting. However, in our task, the correct correspondence between NL words and MR structures is unknown. Many possible derivations could reach the same NL-MR pair, where each such derivation forms a hybrid tree. 786 The hybrid tree is constructed using hidden variables and estimated from the training set. An efficient inside-outside style algorithm can be used for model estimation, similar to that used in (Yamada and Knight, 2001), as discussed next. 5.2.1 The Inside-Outside Algorithm with EM In this section, we discuss how to estimate the emission and pattern parameters with the Expectation Maximization (EM) algorithm (Dempster et al., 1977), by using an inside-outside (Baker, 1979) dynamic programming approach. Denote ni ≡ hmi, wii as the i-th training instance, where mi and wi are the MR structure and the NL sentence of the i-th instance respectively. We also denote nv ≡ hmv, wvi as an aligned pair of MR substructure and contiguous NL substring, where the MR substructure rooted by MR production mv will correspond to</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="5181" citStr="Zettlemoyer and Collins, 2005" startWordPosition="787" endWordPosition="790">oach where meaning representation structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider the following instance taken from the GEOQUERY corpus (Kate et al., 2005): The NL sentence “How many states do not have rivers ?” consists of 8 words, including punctuation. The MR is a hierarchical tree QUERY: answer (NUM) NUM : count (STATE) ST</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Zettlemoyer</author>
<author>M Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>678--687</pages>
<contexts>
<context position="5213" citStr="Zettlemoyer and Collins, 2007" startWordPosition="791" endWordPosition="794">on structures are constructed from the natural language strings hierarchically. It is built on top of SVMstruct with string kernels. Additionally, there is substantial related research that is not directly comparable to our approach. Some of this work requires different levels of supervision, including labeled syntactic parse trees (Ge and Mooney, 2005; Ge and Mooney, 2006). Others do not perform lexical learning (Tang and Mooney, 2001). Finally, recent work has explored learning to map sentences to lambda-calculus meaning representations (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). 3 Meaning Representation We restrict our meaning representation (MR) formalism to a variable free version as presented in (Wong and Mooney, 2006; Kate et al., 2005). A training instance consists of a natural language sentence (NL sentence) and its corresponding meaning representation structure (MR structure). Consider the following instance taken from the GEOQUERY corpus (Kate et al., 2005): The NL sentence “How many states do not have rivers ?” consists of 8 words, including punctuation. The MR is a hierarchical tree QUERY: answer (NUM) NUM : count (STATE) STATE: exclude (STATE STATE) STATE</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>L. S. Zettlemoyer and M. Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL 2007), pages 678–687.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>