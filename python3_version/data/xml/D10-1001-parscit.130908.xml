<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9965675">
On Dual Decomposition and Linear Programming Relaxations
for Natural Language Processing
</title>
<note confidence="0.57665">
Alexander M. Rush David Sontag Michael Collins Tommi Jaakkola
MIT CSAIL, Cambridge, MA 02139, USA
</note>
<email confidence="0.981787">
{srush,dsontag,mcollins,tommi}@csail.mit.edu
</email>
<sectionHeader confidence="0.994302" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999737421052632">
This paper introduces dual decomposition as a
framework for deriving inference algorithms
for NLP problems. The approach relies on
standard dynamic-programming algorithms as
oracle solvers for sub-problems, together with
a simple method for forcing agreement be-
tween the different oracles. The approach
provably solves a linear programming (LP) re-
laxation of the global inference problem. It
leads to algorithms that are simple, in that they
use existing decoding algorithms; efficient, in
that they avoid exact algorithms for the full
model; and often exact, in that empirically
they often recover the correct solution in spite
of using an LP relaxation. We give experimen-
tal results on two problems: 1) the combina-
tion of two lexicalized parsing models; and
2) the combination of a lexicalized parsing
model and a trigram part-of-speech tagger.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999882294117647">
Dynamic programming algorithms have been re-
markably useful for inference in many NLP prob-
lems. Unfortunately, as models become more com-
plex, for example through the addition of new fea-
tures or components, dynamic programming algo-
rithms can quickly explode in terms of computa-
tional or implementational complexity.1 As a re-
sult, efficiency of inference is a critical bottleneck
for many problems in statistical NLP.
This paper introduces dual decomposition
(Dantzig and Wolfe, 1960; Komodakis et al., 2007)
as a framework for deriving inference algorithms in
NLP. Dual decomposition leverages the observation
that complex inference problems can often be
decomposed into efficiently solvable sub-problems.
The approach leads to inference algorithms with the
following properties:
</bodyText>
<footnote confidence="0.9937965">
1The same is true for NLP inference algorithms based on
other exact combinatorial methods, for example methods based
on minimum-weight spanning trees (McDonald et al., 2005), or
graph cuts (Pang and Lee, 2004).
</footnote>
<page confidence="0.718332">
1
</page>
<listItem confidence="0.9942096">
• The resulting algorithms are simple and efficient,
building on standard dynamic-programming algo-
rithms as oracle solvers for sub-problems,2 to-
gether with a method for forcing agreement be-
tween the oracles.
• The algorithms provably solve a linear program-
ming (LP) relaxation of the original inference
problem.
• Empirically, the LP relaxation often leads to an
exact solution to the original problem.
</listItem>
<bodyText confidence="0.999874153846154">
The approach is very general, and should be appli-
cable to a wide range of problems in NLP. The con-
nection to linear programming ensures that the algo-
rithms provide a certificate of optimality when they
recover the exact solution, and also opens up the
possibility of methods that incrementally tighten the
LP relaxation until it is exact (Sherali and Adams,
1994; Sontag et al., 2008).
The structure of this paper is as follows. We
first give two examples as an illustration of the ap-
proach: 1) integrated parsing and trigram part-of-
speech (POS) tagging; and 2) combined phrase-
structure and dependency parsing. In both settings,
it is possible to solve the integrated problem through
an “intersected” dynamic program (e.g., for integra-
tion of parsing and tagging, the construction from
Bar-Hillel et al. (1964) can be used). However,
these methods, although polynomial time, are sub-
stantially less efficient than our algorithms, and are
considerably more complex to implement.
Next, we describe exact polyhedral formula-
tions for the two problems, building on connec-
tions between dynamic programming algorithms
and marginal polytopes, as described in Martin et al.
(1990). These allow us to precisely characterize the
relationship between the exact formulations and the
</bodyText>
<footnote confidence="0.999122333333333">
2More generally, other exact inference methods can be
used as oracles, for example spanning tree algorithms for non-
projective dependency structures.
</footnote>
<note confidence="0.8258395">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1–11,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999311341463415">
LP relaxations that we solve. We then give guaran-
tees of convergence for our algorithms by showing
that they are instantiations of Lagrangian relaxation,
a general method for solving linear programs of a
particular form.
Finally, we describe experiments that demonstrate
the effectiveness of our approach. First, we con-
sider the integration of the generative model for
phrase-structure parsing of Collins (2003), with the
second-order discriminative dependency parser of
Koo et al. (2008). This is an interesting problem
in its own right: the goal is to inject the high per-
formance of discriminative dependency models into
phrase-structure parsing. The method uses off-the-
shelf decoders for the two models. We find three
main results: 1) in spite of solving an LP relax-
ation, empirically the method finds an exact solution
on over 99% of the examples; 2) the method con-
verges quickly, typically requiring fewer than 10 it-
erations of decoding; 3) the method gives gains over
a baseline method that forces the phrase-structure
parser to produce the same dependencies as the first-
best output from the dependency parser (the Collins
(2003) model has an F1 score of 88.1%; the base-
line method has an F1 score of 89.7%; and the dual
decomposition method has an F1 score of 90.7%).
In a second set of experiments, we use dual de-
composition to integrate the trigram POS tagger of
Toutanova and Manning (2000) with the parser of
Collins (2003). We again find that the method finds
an exact solution in almost all cases, with conver-
gence in just a few iterations of decoding.
Although the focus of this paper is on dynamic
programming algorithms—both in the experiments,
and also in the formal results concerning marginal
polytopes—it is straightforward to use other com-
binatorial algorithms within the approach. For ex-
ample, Koo et al. (2010) describe a dual decompo-
sition approach for non-projective dependency pars-
ing, which makes use of both dynamic programming
and spanning tree inference algorithms.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999957473684211">
Dual decomposition is a classical method for solv-
ing optimization problems that can be decomposed
into efficiently solvable sub-problems. Our work is
inspired by dual decomposition methods for infer-
ence in Markov random fields (MRFs) (Wainwright
et al., 2005a; Komodakis et al., 2007; Globerson and
Jaakkola, 2007). In this approach, the MRF is de-
composed into sub-problems corresponding to tree-
structured subgraphs that together cover all edges
of the original graph. The resulting inference algo-
rithms provably solve an LP relaxation of the MRF
inference problem, often significantly faster than
commercial LP solvers (Yanover et al., 2006).
Our work is also related to methods that incorpo-
rate combinatorial solvers within loopy belief prop-
agation (LBP), either for MAP inference (Duchi et
al., 2007) or for computing marginals (Smith and
Eisner, 2008). Our approach similarly makes use
of combinatorial algorithms to efficiently solve sub-
problems of the global inference problem. However,
unlike LBP, our algorithms have strong theoretical
guarantees, such as guaranteed convergence and the
possibility of a certificate of optimality. These guar-
antees are possible because our algorithms directly
solve an LP relaxation.
Other work has considered LP or integer lin-
ear programming (ILP) formulations of inference in
NLP (Martins et al., 2009; Riedel and Clarke, 2006;
Roth and Yih, 2005). These approaches typically
use general-purpose LP or ILP solvers. Our method
has the advantage that it leverages underlying struc-
ture arising in LP formulations of NLP problems.
We will see that dynamic programming algorithms
such as CKY can be considered to be very effi-
cient solvers for particular LPs. In dual decomposi-
tion, these LPs—and their efficient solvers—can be
embedded within larger LPs corresponding to more
complex inference problems.
</bodyText>
<sectionHeader confidence="0.954216" genericHeader="method">
3 Background: Structured Models for NLP
</sectionHeader>
<bodyText confidence="0.999680692307692">
We now describe the type of models used throughout
the paper. We take some care to set up notation that
will allow us to make a clear connection between
inference problems and linear programming.
Our first example is weighted CFG parsing. We
assume a context-free grammar, in Chomsky normal
form, with a set of non-terminals N. The grammar
contains all rules of the form A —* B C and A —*
w where A, B, C E N and w E V (it is simple
to relax this assumption to give a more constrained
grammar). For rules of the form A —* w we refer
to A as the part-of-speech tag for w. We allow any
non-terminal to be at the root of the tree.
</bodyText>
<page confidence="0.987779">
2
</page>
<bodyText confidence="0.991016833333333">
Given a sentence with n words, w1, w2,... wn, a
parse tree is a set of rule productions of the form
hA → B C, i, k, ji where A, B, C ∈ N, and
1 ≤ i ≤ k &lt; j ≤ n. Each rule production rep-
resents the use of CFG rule A → B C where non-
terminal A spans words wi ... wj, non-terminal B
spans words wi ... wk, and non-terminal C spans
words wk+1 ... wj. There are O(|N|3n3) such rule
productions. Each parse tree corresponds to a subset
of these rule productions, of size n − 1, that forms a
well-formed parse tree.3
We now define the index set for CFG parsing as
</bodyText>
<equation confidence="0.999766">
I = {hA → B C,i,k,ji: A,B,C ∈ N,
1 ≤ i ≤ k &lt; j ≤ n}
</equation>
<bodyText confidence="0.99973188">
Each parse tree is a vector y = {yr : r ∈ I},
with yr = 1 if rule r is in the parse tree, and yr =
0 otherwise. Hence each parse tree is represented
as a vector in {0,1}&apos;, where m = |I|. We use Y
to denote the set of all valid parse-tree vectors; the
set Y is a subset of {0,1}&apos; (not all binary vectors
correspond to valid parse trees).
In addition, we assume a vector 0 = {0r : r ∈
I} that specifies a weight for each rule production.4
Each 0r can take any value in the reals. The optimal
parse tree is y* = arg maxyEY y · 0 where y · 0 =
Er yr0r is the inner product between y and 0.
We use yr and y(r) interchangeably (similarly for
0r and 0(r)) to refer to the r’th component of the
vector y. For example 0(A → B C, i, k, j) is a
weight for the rule hA → B C, i, k, ji.
We will use similar notation for other problems.
As a second example, in POS tagging the task is to
map a sentence of n words w1 ... wn to a tag se-
quence t1 ... tn, where each ti is chosen from a set
T of possible tags. We assume a trigram tagger,
where a tag sequence is represented through deci-
sions h(A, B) → C, ii where A, B, C ∈ T, and
i ∈ {3 ... n}. Each production represents a tran-
sition where C is the tag of word wi, and (A, B) are
</bodyText>
<footnote confidence="0.869318285714286">
3We do not require rules of the form A → wi in this repre-
sentation, as they are redundant: specifically, a rule production
hA → B C, i, k, ji implies a rule B → wi iff i = k, and
C → wj iff j = k + 1.
4We do not require parameters for rules of the form A → w,
as they can be folded into rule production parameters. E.g.,
under a PCFG we define θ(A → B C, i, k, j) = log P(A →
</footnote>
<equation confidence="0.7134885">
B C  |A) + δi,k logP(B → wi|B) + δk+1,j log P(C →
wj|C) where δ.,� = 1 if x = y, 0 otherwise.
</equation>
<bodyText confidence="0.977404571428572">
the previous two tags. The index set for tagging is
Itag = {h(A,B) → C,ii : A,B,C ∈ T, 3 ≤ i ≤ n}
Note that we do not need transitions for i = 1 or i =
2, because the transition h(A, B) → C, 3i specifies
the first three tags in the sentence.5
Each tag sequence is represented as a vector z =
{zr : r ∈ Itag}, and we denote the set of valid tag
sequences, a subset of {0,1}|z1ag|, as Z. Given a
parameter vector 0 = {0r : r ∈ Itag}, the optimal
tag sequence is arg maxzEZ z · 0.
As a modification to the above approach, we will
find it convenient to introduce extended index sets
for both the CFG and POS tagging examples. For
the CFG case we define the extended index set to be
</bodyText>
<equation confidence="0.9814415">
I� = I ∪ Iuni where
Iuni = {(i,t) : i ∈ {1 ... n},t ∈ T}
</equation>
<bodyText confidence="0.979338">
Here each pair (i, t) represents word wi being as-
signed the tag t. Thus each parse-tree vector y will
have additional (binary) components y(i, t) spec-
ifying whether or not word i is assigned tag t.
(Throughout this paper we will assume that the tag-
set used by the tagger, T, is a subset of the set of non-
terminals considered by the parser, N.) Note that
this representation is over-complete, since a parse
tree determines a unique tagging for a sentence:
more explicitly, for any i ∈ {1... n}, Y ∈ T, the
following linear constraint holds:
</bodyText>
<equation confidence="0.9931948">
y(i, Y ) = n E y(X → Y Z,i,i,k) +
E X,ZEN
k=i+1
�i − 1 11 y(X → Z Y,k,i − 1,i)
k=1 X,ZEN
</equation>
<bodyText confidence="0.999746222222222">
We apply the same extension to the tagging index
set, effectively mapping trigrams down to unigram
assignments, again giving an over-complete repre-
sentation. The extended index set for tagging is re-
ferred to as Itag.
From here on we will make exclusive use of ex-
tended index sets for CFG parsing and trigram tag-
ging. We use the set Y to refer to the set of valid
parse structures under the extended representation;
</bodyText>
<footnote confidence="0.956526">
5As one example, in an HMM, the parameter θ((A, B) →
C, 3) would be log P(A|∗∗)+log P(B|∗A)+log P(C|AB)+
log P(w1|A) + log P(w2|B) + log P(w3|C), where ∗ is the
start symbol.
</footnote>
<page confidence="0.997755">
3
</page>
<bodyText confidence="0.9977622">
each y ∈ Y is a binary vector of length |I0|. We
similarly use Z to refer to the set of valid tag struc-
tures under the extended representation. We assume
parameter vectors for the two problems, θcfg ∈ R|I&apos;|
and θtag ∈ R|I&apos;tar|.
</bodyText>
<sectionHeader confidence="0.962489" genericHeader="method">
4 Two Examples
</sectionHeader>
<bodyText confidence="0.999975">
This section describes the dual decomposition ap-
proach for two inference problems in NLP.
</bodyText>
<subsectionHeader confidence="0.997061">
4.1 Integrated Parsing and Trigram Tagging
</subsectionHeader>
<bodyText confidence="0.999985">
We now describe the dual decomposition approach
for integrated parsing and trigram tagging. First, de-
fine the set Q as follows:
</bodyText>
<equation confidence="0.9960365">
Q = {(y,z) : y ∈ Y,z ∈ Z,
y(i, t) = z(i, t) for all (i, t) ∈ Iuni} (1)
</equation>
<bodyText confidence="0.958939666666667">
Hence Q is the set of all (y, z) pairs that agree
on their part-of-speech assignments. The integrated
parsing and trigram tagging problem is then to solve
</bodyText>
<equation confidence="0.996568285714286">
(y · θcfg + z · θtag)
max (2)
(y,z)∈Q
This problem is equivalent to
m
ax (y · θcfg + g(y) · θtag)
y∈Y
</equation>
<bodyText confidence="0.999986142857143">
where g : Y → Z is a function that maps a parse
tree y to its set of trigrams z = g(y). The benefit of
the formulation in Eq. 2 is that it makes explicit the
idea of maximizing over all pairs (y, z) under a set
of agreement constraints y(i, t) = z(i, t)—this con-
cept will be central to the algorithms in this paper.
With this in mind, we note that we have effi-
cient methods for the inference problems of tagging
and parsing alone, and that our combined objective
almost separates into these two independent prob-
lems. In fact, if we drop the y(i, t) = z(i, t) con-
straints from the optimization problem, the problem
splits into two parts, each of which can be efficiently
solved using dynamic programming:
</bodyText>
<equation confidence="0.988808133333333">
(y∗, z∗) = (arg max y · θcfg, arg max z · θtag)
Set u(1)(i, t) ← 0 for all (i, t) ∈ Iuni
fork=1toKdo
Ey(k) ← arg max (y · θcfg −
yEY
(i,t)EZunf
E
z(k) ← arg max (z · θ`g +
zEZ
(i,t)EZunf
if y(k)(i, t) = z(k)(i, t) for all (i, t) ∈ Iuni then
return (y(k), z(k))
for all (i, t) ∈ Iuni,
u(k+1)(Z, t) _ u(k)(z, t)+αk(y(k)(z, t)−z(k)(z, t))
return (y(K), z(K))
</equation>
<figureCaption confidence="0.960738">
Figure 1: The algorithm for integrated parsing and tag-
ging. The parameters αk &gt; 0 for k = 1... K specify
step sizes for each iteration, and are discussed further in
the Appendix. The two arg max problems can be solved
using dynamic programming.
</figureCaption>
<bodyText confidence="0.999929142857143">
solves the harder optimization problem using an ex-
isting CFG parser and trigram tagger. After each
iteration the algorithm adjusts the weights u(i, t);
these updates modify the objective functions for the
two models, encouraging them to agree on the same
POS sequence. In section 6.1 we will show that the
variables u(i, t) are Lagrange multipliers enforcing
agreement constraints, and that the algorithm corre-
sponds to a (sub)gradient method for optimization
of a dual function. The algorithm is easy to imple-
ment: all that is required is a decoding algorithm for
each of the two models, and simple additive updates
to the Lagrange multipliers enforcing agreement be-
tween the two models.
</bodyText>
<subsectionHeader confidence="0.996662">
4.2 Integrating Two Lexicalized Parsers
</subsectionHeader>
<bodyText confidence="0.979273473684211">
Our second example problem is the integration of
a phrase-structure parser with a higher-order depen-
dency parser. The goal is to add higher-order fea-
tures to phrase-structure parsing without greatly in-
creasing the complexity of inference.
First, we define an index set for second-order un-
labeled projective dependency parsing. The second-
order parser considers first-order dependencies, as
well as grandparent and sibling second-order depen-
dencies (e.g., see Carreras (2007)). We assume that
Id,p is an index set containing all such dependen-
cies (for brevity we omit the details of this index
set). For convenience we define an extended index
set that makes explicit use of first-order dependen-
y∈Y z∈Z
Dual decomposition exploits this idea; it results in
the algorithm given in figure 1. The algorithm opti-
mizes the combined objective by repeatedly solving
the two sub-problems separately—that is, it directly
</bodyText>
<equation confidence="0.9470236">
u(k)(i, t)y(i, t))
u(k)(i, t)z(i, t))
4
cies, I0dep = Idep ∪ Ifirst, where
Ifirst = {(i, j) : i ∈ {0 ... n}, j ∈ {1... n}, i =6 j}
</equation>
<bodyText confidence="0.994479833333334">
Here (i, j) represents a dependency with head wi
and modifier wj (i = 0 corresponds to the root sym-
bol in the parse). We use D ⊆ {0, 1}|I&apos;dep |to denote
the set of valid projective dependency parses.
The second model we use is a lexicalized CFG.
Each symbol in the grammar takes the form A(h)
where A ∈ N is a non-terminal, and h ∈ {1... n}
is an index specifying that wh is the head of the con-
stituent. Rule productions take the form hA(a) →
B(b) C(c), i, k, ji where b ∈ {i ... k}, c ∈ {(k +
1) ... j}, and a is equal to b or c, depending on
whether A receives its head-word from its left or
right child. Each such rule implies a dependency
(a, b) if a = c, or (a, c) if a = b. We take Ihead
to be the index set of all such rules, and I0head =
Ihead ∪ Ifirst to be the extended index set. We define
H ⊆ {0, 1}|I&apos;head |to be the set of valid parse trees.
The integrated parsing problem is then to find
</bodyText>
<equation confidence="0.996697">
(y∗, d∗) = arg max (Y· Bhead + d · Bdepl (3)
(y,d)∈R \
</equation>
<bodyText confidence="0.973505857142857">
where R = {(y, d) : y ∈ H, d ∈ D,
y(i, j) = d(i, j) for all (i, j) ∈ Ifirst}
This problem has a very similar structure to the
problem of integrated parsing and tagging, and we
can derive a similar dual decomposition algorithm.
The Lagrange multipliers u are a vector in R|Ifirst|
enforcing agreement between dependency assign-
ments. The algorithm (omitted for brevity) is identi-
cal to the algorithm in figure 1, but with Iuni, Y, Z,
Bhead, and
Ocfg, and Btag replaced with Ifirst, H, D,
Bdep respectively. The algorithm only requires de-
coding algorithms for the two models, together with
simple updates to the Lagrange multipliers.
</bodyText>
<sectionHeader confidence="0.658472" genericHeader="method">
5 Marginal Polytopes and LP Relaxations
</sectionHeader>
<bodyText confidence="0.9999394">
We now give formal guarantees for the algorithms
in the previous section, showing that they solve LP
relaxations of the problems in Eqs. 2 and 3.
To make the connection to linear programming,
we first introduce the idea of marginal polytopes in
section 5.1. In section 5.2, we give a precise state-
ment of the LP relaxations that are being solved
by the example algorithms, making direct use of
marginal polytopes. In section 6 we will prove that
the example algorithms solve these LP relaxations.
</bodyText>
<subsectionHeader confidence="0.994674">
5.1 Marginal Polytopes
</subsectionHeader>
<bodyText confidence="0.9998415">
For a finite set Y, define the set of all distributions
over elements in Y as A = {a ∈ R|Y |: ay ≥
0, Ey∈Y ay = 1}. Each a ∈ A gives a vector of
marginals, µ = Ey∈Y ayy, where µr can be inter-
preted as the probability that yr = 1 for a y selected
at random from the distribution a.
The set of all possible marginal vectors, known as
the marginal polytope, is defined as follows:
</bodyText>
<equation confidence="0.991462">
M = {µ ∈ R&apos; : ∃� ∈ A such that µ = 1: ayy}
y∈Y
</equation>
<bodyText confidence="0.98350625">
M is also frequently referred to as the convex hull of
Y, written as conv(Y). We use the notation conv(Y)
in the remainder of this paper, instead of M.
For an arbitrary set Y, the marginal polytope
conv(Y) can be complex to describe.6 However,
Martin et al. (1990) show that for a very general
class of dynamic programming problems, the cor-
responding marginal polytope can be expressed as
</bodyText>
<equation confidence="0.999677">
conv(Y) = {µ ∈ R&apos; : Aµ = b,µ ≥ 0} (4)
</equation>
<bodyText confidence="0.9998974">
where A is a p × m matrix, b is vector in Rp, and the
value p is linear in the size of a hypergraph repre-
sentation of the dynamic program. Note that A and
b specify a set of p linear constraints.
We now give an explicit description of the re-
sulting constraints for CFG parsing:7 similar con-
straints arise for other dynamic programming algo-
rithms for parsing, for example the algorithms of
Eisner (2000). The exact form of the constraints, and
the fact that they are polynomial in number, is not
essential for the formal results in this paper. How-
ever, a description of the constraints gives valuable
intuition for the structure of the marginal polytope.
The constraints are given in figure 2. To develop
some intuition, consider the case where the variables
µr are restricted to be binary: hence each binary
vector µ specifies a parse tree. The second con-
straint in Eq. 5 specifies that exactly one rule must
be used at the top of the tree. The set of constraints
in Eq. 6 specify that for each production of the form
</bodyText>
<footnote confidence="0.905592333333333">
6For any finite set Y, conv(Y) can be expressed as {µ E
R&apos; : Aµ &lt; b} where A is a matrix of dimension p x m, and
b E R-v (see, e.g., Korte and Vygen (2008), pg. 65). The value
for p depends on the set Y, and can be exponential in size.
7Taskar et al. (2004) describe the same set of constraints, but
without proof of correctness or reference to Martin et al. (1990).
</footnote>
<page confidence="0.890442">
5
</page>
<equation confidence="0.970412645161291">
∀r ∈ I&apos;, µ,. ≥ 0 ; E µ(X → Y Z, 1, k, n) = 1 (5) E∀r ∈ I&apos; tag, v,. ≥ 0 ; v((X, Y ) → Z, 3) = 1
X,Y,ZEN X,Y,ZET
k=1...(n−1)
∀X ∈ N, ∀(i, j) such that 1 ≤ i &lt; j ≤ n and (i, j) =6 (1, n): ∀X ∈ T, ∀i ∈ {3 ... n − 1}:
µ(Y → Z X, k, i − 1, j)
E v((Y, Z) → X, i) = E
Y,ZET
∀X ∈ T, ∀i ∈ {3 ... n − 2}:
E v((Y, Z) → X, i) = E
Y,ZET Y,ZET
v((Y, X) → Z, i + 1)
v((X, Y ) → Z, i + 2)
E
Y,ZEN
k=i...(7−1)
E
µ(X → Y Z, i, k, j) =
Y,ZEN
k=(7+1)...n
Y,ZEN
k=1...(i−1)
+ E µ(Y → X Z, i, j, k) (6)
Y,ZET
v((Y, Z) → X, i)
µ(X → Z Y, k, i − 1, i) (7)
∀Y ∈ T, ∀i ∈ {1 ... n} : µ(i, Y ) =
E E
µ(X → Y Z, i, i, k) +
E
∀X ∈ T, ∀i ∈ {3 ... n} : v(i, X) =
Y,ZET
</equation>
<table confidence="0.826441333333333">
X,ZEN X,ZEN E v((X, Y ) → Z, 3)
k=(i+1)...n k=1...(i−1) ∀X ∈ T : v(1, X) =
Y,ZET
</table>
<figureCaption confidence="0.494945666666667">
Figure 2: The linear constraints defining the marginal E v((Y, X) → Z, 3)
polytope for CFG parsing. ∀X ∈ T : v(2, X) =
Y,ZET
</figureCaption>
<bodyText confidence="0.8093707">
hX → Y Z, i, k, ji in a parse tree, there must be
exactly one production higher in the tree that gener-
ates (X, i, j) as one of its children. The constraints
in Eq. 7 enforce consistency between the µ(i, Y )
variables and rule variables higher in the tree. Note
that the constraints in Eqs.(5–7) can be written in the
form Aµ = b, µ ≥ 0, as in Eq. 4.
Under these definitions, we have the following:
Theorem 5.1 Define Y to be the set of all CFG
parses, as defined in section 4. Then
</bodyText>
<equation confidence="0.983191">
conv(Y) = {µ ∈ R&apos; : µ satisifies Eqs.(5–7)}
</equation>
<bodyText confidence="0.994234583333333">
Proof: This theorem is a special case of Martin et al.
(1990), theorem 2.
The marginal polytope for tagging, conv(Z), can
also be expressed using linear constraints as in Eq. 4;
see figure 3. These constraints follow from re-
sults for graphical models (Wainwright and Jordan,
2008), or from the Martin et al. (1990) construction.
As a final point, the following theorem gives an
important property of marginal polytopes, which we
will use at several points in this paper:
Theorem 5.2 (Korte and Vygen (2008), page 66.)
For any set Y ⊆ {0, 1}k, and for any vector 0 ∈ Rk,
</bodyText>
<equation confidence="0.9946245">
max y · 0 = max µ · 0 (8)
yEY µEconv(Y)
</equation>
<bodyText confidence="0.968098666666667">
The theorem states that for a linear objective func-
tion, maximization over a discrete set Y can be
replaced by maximization over the convex hull
</bodyText>
<figureCaption confidence="0.9911505">
Figure 3: The linear constraints defining the marginal
polytope for trigram POS tagging.
</figureCaption>
<bodyText confidence="0.950236666666667">
conv(Y). The problem maxµEconv(Y) µ·0 is a linear
programming problem.
For parsing, this theorem implies that:
</bodyText>
<listItem confidence="0.99563275">
1. Weighted CFG parsing can be framed as a linear
programming problem, of the form maxµEconv(Y) µ·
0, where conv(Y) is specified by a polynomial num-
ber of linear constraints.
2. Conversely, dynamic programming algorithms
such as the CKY algorithm can be considered to
be oracles that efficiently solve LPs of the form
maxµEconv(Y) µ · 0.
</listItem>
<bodyText confidence="0.867051">
Similar results apply for the POS tagging case.
</bodyText>
<subsectionHeader confidence="0.991385">
5.2 Linear Programming Relaxations
</subsectionHeader>
<bodyText confidence="0.999503833333333">
We now describe the LP relaxations that are solved
by the example algorithms in section 4. We begin
with the algorithm in Figure 1.
The original optimization problem was to find
max(y,z)EQ (y · 0cfg + z · 0tag) (see Eq. 2). By the-
orem 5.2, this is equivalent to solving
</bodyText>
<equation confidence="0.9884595">
max \µ · 0cfg + v · 0tagl (9)
(µ,v)Econv(Q) \ JJ
</equation>
<bodyText confidence="0.989781666666667">
To formulate our approximation, we first define:
Q&apos; = {(µ, v) : µ ∈ conv(Y), v ∈ conv(Z),
µ(i, t) = v(i, t) for all (i, t) ∈ Iuni}
</bodyText>
<page confidence="0.995495">
6
</page>
<bodyText confidence="0.99874875">
The definition of Q0 is very similar to the definition
of Q (see Eq. 1), the only difference being that Y
and Z are replaced by conv(Y) and conv(Z) re-
spectively. Hence any point in Q is also in Q0. It fol-
lows that any point in conv(Q) is also in Q0, because
Q0 is a convex set defined by linear constraints.
The LP relaxation then corresponds to the follow-
ing optimization problem:
</bodyText>
<equation confidence="0.9952135">
max (µ · 0cfg + v · 0t-g) (10)
(µ,v)∈Q&apos; \\ J
</equation>
<bodyText confidence="0.985292538461539">
Q0 is defined by linear constraints, making this a
linear program. Since Q0 is an outer bound on
conv(Q), i.e. conv(Q) ⊆ Q0, we obtain the guaran-
tee that the value of Eq. 10 always upper bounds the
value of Eq. 9.
In Appendix A we give an example showing
that in general Q0 includes points that are not in
conv(Q). These points exist because the agreement
between the two parts is now enforced in expecta-
tion (µ(i, t) = v(i, t) for (i, t) ∈ Iuni) rather than
based on actual assignments. This agreement con-
straint is weaker since different distributions over
assignments can still result in the same first order
expectations. Thus, the solution to Eq. 10 may be
in Q0 but not in conv(Q). It can be shown that
all such solutions will be fractional, making them
easy to distinguish from Q. In many applications of
LP relaxations—including the examples discussed
in this paper—the relaxation in Eq. 10 turns out to
be tight, in that the solution is often integral (i.e., it
is in Q). In these cases, solving the LP relaxation
exactly solves the original problem of interest.
In the next section we prove that the algorithm
in Figure 1 solves the problem in Eq 10. A similar
result holds for the algorithm in section 4.2: it solves
a relaxation of Eq. 3, where R is replaced by
</bodyText>
<equation confidence="0.9964335">
R0 = {(µ, v) : µ ∈ conv(H), v ∈ conv(D),
µ(i,j) = v(i,j) for all (i, j) ∈ Ifirst}
</equation>
<sectionHeader confidence="0.938209" genericHeader="method">
6 Convergence Guarantees
</sectionHeader>
<subsectionHeader confidence="0.994077">
6.1 Lagrangian Relaxation
</subsectionHeader>
<bodyText confidence="0.994404">
We now show that the example algorithms solve
their respective LP relaxations given in the previ-
ous section. We do this by first introducing a gen-
eral class of linear programs, together with an op-
timization method, Lagrangian relaxation, for solv-
ing these LPs. We then show that the algorithms in
section 4 are special cases of the general algorithm.
The linear programs we consider take the form
</bodyText>
<equation confidence="0.984578">
max (01 · x1 + 02 · x2) such that Ex1 = Fx2
x1EX1,x2EX2
</equation>
<bodyText confidence="0.967567166666667">
The matrices E ∈ Rq×� and F ∈ Rq×l specify q lin-
ear “agreement” constraints between x1 ∈ R&apos; and
x2 ∈ Rl. The sets X1, X2 are also specified by linear
constraints, X1 = {x1 ∈ R1 : Ax1 = b, x1 ≥ 0}
and X2 = {x2 ∈ Rl : Cx2 = d, x2 ≥ 01, hence the
problem is an LP.
Note that if we set X1 = conv(Y), X2 =
conv(Z), and define E and F to specify the agree-
ment constraints µ(i, t) = v(i, t), then we have the
LP relaxation in Eq. 10.
It is natural to apply Lagrangian relaxation in
cases where the sub-problems maxx1∈X1 01 · x1 and
maxx2∈X2 02 · x2 can be efficiently solved by com-
binatorial algorithms for any values of 01, 02, but
where the constraints Ex1 = Fx2 “complicate” the
problem. We introduce Lagrange multipliers u ∈ Rq
that enforce the latter set of constraints, giving the
Lagrangian:
</bodyText>
<equation confidence="0.773122">
L(u, x1, x2) = 01 · x1 + 02 · x2 + u · (Ex1 − Fx2)
The dual objective function is
max
x1EX1,x2EX2
</equation>
<bodyText confidence="0.999679666666667">
and the dual problem is to find minu∈R&apos; L(u).
Because X1 and X2 are defined by linear con-
straints, by strong duality we have
</bodyText>
<equation confidence="0.9924425">
max (01 · x1 + 02 · x2)
x1∈X1,x2∈X2:Ex1=Fx2
</equation>
<bodyText confidence="0.999377307692308">
Hence minimizing L(u) will recover the maximum
value of the original problem. This leaves open the
question of how to recover the LP solution (i.e., the
pair (x∗1, x∗2) that achieves this maximum); we dis-
cuss this point in section 6.2.
The dual L(u) is convex. However, L(u) is
not differentiable, so we cannot use gradient-based
methods to optimize it. Instead, a standard approach
is to use a subgradient method. Subgradients are tan-
gent lines that lower bound a function even at points
of non-differentiability: formally, a subgradient of a
convex function L : Rn → R at a point u is a vector
gu such that for all v, L(v) ≥ L(u) + gu · (v − u).
</bodyText>
<equation confidence="0.9700825">
L(u) =
L(u, x1, x2)
min
u∈R&apos;
L(u) =
7
u(1) , 0
for k = 1 to K do
x(k) 1, arg maxx1EX1(01 + (u(k))TE) &apos; x1
k
x2 , arg maxx2EX2(02 − (u(k))TF) &apos; x2
if Ex(k) 1= Fx(k)
2 return u(k)
u(k+1) , u(k) − αk(Ex(k) 1− Fx(k)
2 )
return u(K)
</equation>
<figureCaption confidence="0.99974">
Figure 4: The Lagrangian relaxation algorithm.
</figureCaption>
<bodyText confidence="0.9974885">
By standard results, the subgradient for L at a point
u takes a simple form, gu = Ex* − Fx2, where
</bodyText>
<equation confidence="0.99771825">
(01 + (u(k))TE) &apos; x1
x1EX1
A = arg max (02 − (u(k))TF) &apos; x2
x2EX2
</equation>
<bodyText confidence="0.999811">
The beauty of this result is that the values of xi and
x2, and by implication the value of the subgradient,
can be computed using oracles for the two arg max
sub-problems.
Subgradient algorithms perform updates that are
similar to gradient descent:
</bodyText>
<equation confidence="0.771392444444444">
xi = arg max
the caveat that it returns
rather than u(k)
8with
(x(k)
1 ,x(k)
2 )
.
8 6.2 Recovering the LP Solution
</equation>
<bodyText confidence="0.94401835">
The previous section described how the method in
figure 4 can be used to minimize the dual L(u) of the
original linear program. We now turn to the problem
of recovering a primal solution
of the LP.
The method we propose considers two cases:
(Case 1) If
at any stage during
the algorithm, then simply take
to be the
primal solution. In this case the pair
ex-
actly solves the original LP.9 If this case arises in the
algorithm in figure 1, then the resulting solution is
binary (i.e., it is a member of Q), and the solution
exactly solves the ori
countered during the algorithm: --76
= Ek
x
= Ek x(2k)
</bodyText>
<subsectionHeader confidence="0.760734">
Results from
</subsectionHeader>
<bodyText confidence="0.963099909090909">
and Ozdaglar
(2009) show that as K —* oo, these averaged solu-
tions converge to the optimal primal solution.10 A
second strategy (as given in figure 1) is to simply
take
as an approximation to the primal
solution. This method is a heuristic, but previous
work (e.g., Komodakis et al. (2007)) has shown that
it is effective in practice; we use it in this paper.
In our experiments we found that in the vast ma-
jority of cases, case 1 applies, after a small
</bodyText>
<equation confidence="0.973650222222222">
1
x(k)
1 /K,
2
/K.
Nedi´c
(x(K)
1 ,x(K)
2 )
</equation>
<bodyText confidence="0.7824695">
number
of iterations; see the next section for more details.
</bodyText>
<sectionHeader confidence="0.981852" genericHeader="method">
7 Experiments
</sectionHeader>
<bodyText confidence="0.98780884">
u(k+1) , u(k)
where g(k) is the subgradient of L at u(k) and
&gt; 0
is the step size of the update. The complete sub-
gradient algorithm is given in figure 4. The follow-
ing convergence theorem is well-known (e.g., see
page 120 of Korte and Vygen (2008)):
Theorem 6.1 If limk,,,,,
= 0 and
=
oo, then limk,,,,, L(u(k)) =
L(u).
The following proposition is easily verified:
Proposition 6.1 The algorithm in figure
is an in-
stantiation of the algorithm in figure 4,8 with
=
X2 = conv(i), and the matrices E and
F defined to be binary matrices specifying the con-
straints µ(i, t) = v(i, t) for all (i, t) E Zuni.
Under an appropriate definition of the step sizes
it follows that the algorithm in figure 1 defines a
sequence of Lagrange multiplers u(k) minimizing a
dual of the LP relaxation in Eq. 10. A similar result
holds for the algori
</bodyText>
<equation confidence="0.96833175">
−αkg(k)
αk
αk
E,,,,k=1αk
minu
1
X1
conv(Y),
αk,
thm in section 4.2.
i
2
(x
,x
)
Ex(k)
1= Fx(k)
2
(x(k)
1 ,x(k)
2
(x(k)
1 ,x(k)
2
</equation>
<bodyText confidence="0.947942714285714">
ginal inference problem.
(Case 2) If case 1 does not arise, then a couple of
strategies are possible. (This situation could arise
in cases where the LP is not tight—i.e., it has a
fractional solution—or where K is not large enough
for convergence.) The first is to define the pri-
mal solution to be the average of the solutions en-
</bodyText>
<subsectionHeader confidence="0.9982455">
7.1 Integrated Phrase-Structure and
Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.95408375">
Our first set of experiments considers the integration
of Model 1 of Collins (2003) (a lexicalized phrase-
structure parser, fr
om here on referred to as Model
</bodyText>
<footnote confidence="0.724567">
9We have that
</footnote>
<equation confidence="0.392541">
+ B2
=
</equation>
<bodyText confidence="0.8188414">
the last equality is because
and
are de-
fined by the respective arg
Thus,
</bodyText>
<footnote confidence="0.774112">
and u(k)
are primal and dual optimal.
10The resulting fractional solution can be projected back to
the set Q, see (Smith an
</footnote>
<equation confidence="0.967056266666667">
B1·x(k)
1
·x(k)
2
L(u(k),x(k)
1 , x(k)
2 )=L(u(k)), where
x(k)
1
x(k)
2
max’s.
(x(k)
1 ,x(k)
2 )
</equation>
<bodyText confidence="0.377647">
d Eisner, 2008; Martins et al., 2009).
</bodyText>
<table confidence="0.998784">
Itn. 1 2 3 4 5-10 11-20 20-50 **
Dep 43.5 20.1 10.2 4.9 14.0 5.7 1.4 0.4
POS 58.7 15.4 6.3 3.6 10.3 3.8 0.8 1.1
</table>
<tableCaption confidence="0.596452833333333">
Table 1: Convergence results for Section 23 of the WSJ
Treebank for the dependency parsing and POS experi-
ments. Each column gives the percentage of sentences
whose exact solutions were found in a given range of sub-
gradient iterations. ** is the percentage of sentences that
did not converge by the iteration limit (K=50).
</tableCaption>
<bodyText confidence="0.999292666666667">
1),11 and the 2nd order discriminative dependency
parser of Koo et al. (2008). The inference problem
for a sentence x is to find
</bodyText>
<equation confidence="0.992088">
y* = arg maY (1
x (fl +&apos;Yf2(y)) 1)
YE
</equation>
<bodyText confidence="0.968337733333333">
where Y is the set of all lexicalized phrase-structure
trees for the sentence x; f1(y) is the score (log prob-
ability) under Model 1; f2(y) is the score under Koo
et al. (2008) for the dependency structure implied
by y; and γ &gt; 0 is a parameter dictating the relative
weight of the two models.12 This problem is simi-
lar to the second example in section 4; a very sim-
ilar dual decomposition algorithm to that described
in section 4.2 can be derived.
We used the Penn Wall Street Treebank (Marcus
et al., 1994) for the experiments, with sections 2-21
for training, section 22 for development, and section
23 for testing. The parameter γ was chosen to opti-
mize performance on the development set.
We ran the dual decomposition algorithm with a
limit of K = 50 iterations. The dual decomposi-
tion algorithm returns an exact solution if case 1 oc-
curs as defined in section 6.2; we found that of 2416
sentences in section 23, case 1 occurred for 2407
(99.6%) sentences. Table 1 gives statistics showing
the number of iterations required for convergence.
Over 80% of the examples converge in 5 iterations or
fewer; over 90% converge in 10 iterations or fewer.
We compare the accuracy of the dual decomposi-
tion approach to two baselines: first, Model 1; and
second, a naive integration method that enforces the
hard constraint that Model 1 must only consider de-
11We use a reimplementation that is a slight modification of
Collins Model 1, with very similar performance, and which uses
the TAG formalism of Carreras et al. (2008).
</bodyText>
<footnote confidence="0.945382333333333">
12Note that the models f1 and f2 were trained separately,
using the methods described by Collins (2003) and Koo et al.
(2008) respectively.
</footnote>
<table confidence="0.99997175">
Precision Recall F1 Dep
Model 1 88.4 87.8 88.1 91.4
Koo08 Baseline 89.9 89.6 89.7 93.3
DD Combination 91.0 90.4 90.7 93.8
</table>
<tableCaption confidence="0.94448375">
Table 2: Performance results for Section 23 of the WSJ
Treebank. Model 1: a reimplementation of the genera-
tive parser of (Collins, 2002). Koo08 Baseline: Model 1
with a hard restriction to dependencies predicted by the
discriminative dependency parser of (Koo et al., 2008).
DD Combination: a model that maximizes the joint score
of the two parsers. Dep shows the unlabeled dependency
accuracy of each system.
</tableCaption>
<figure confidence="0.966795">
0 10 20 30 40 50
Maximum Number of Dual Decomposition Iterations
</figure>
<figureCaption confidence="0.993474333333333">
Figure 5: Performance on the parsing task assuming a
fixed number of iterations K. f-score: accuracy of the
method. % certificates: percentage of examples for which
a certificate of optimality is provided. % match: percent-
age of cases where the output from the method is identical
to the output when using K = 50.
</figureCaption>
<bodyText confidence="0.998978666666667">
pendencies seen in the first-best output from the de-
pendency parser. Table 2 shows all three results. The
dual decomposition method gives a significant gain
in precision and recall over the naive combination
method, and boosts the performance of Model 1 to
a level that is close to some of the best single-pass
parsers on the Penn treebank test set. Dependency
accuracy is also improved over the Koo et al. (2008)
model, in spite of the relatively low dependency ac-
curacy of Model 1 alone.
Figure 5 shows performance of the approach as a
function of K, the maximum number of iterations of
dual decomposition. For this experiment, for cases
where the method has not converged for k &lt; K,
the output from the algorithm is chosen to be the
y(k) for k &lt; K that maximizes the objective func-
tion in Eq. 11. The graphs show that values of K
less than 50 produce almost identical performance to
K = 50, but with fewer cases giving certificates of
optimality (with K = 10, the f-score of the method
is 90.69%; with K = 5 it is 90.63%).
</bodyText>
<figure confidence="0.9977455">
Percentage
100
90
80
60
50
70
f score
% certificates
% match K=50
</figure>
<page confidence="0.970688">
9
</page>
<table confidence="0.999437666666667">
Precision Recall Fl POS Acc
Fixed Tags 88.1 87.6 87.9 96.7
DD Combination 88.7 88.0 88.3 97.1
</table>
<tableCaption confidence="0.954195">
Table 3: Performance results for Section 23 of the WSJ.
Model 1 (Fixed Tags): a baseline parser initialized to the
best tag sequence of from the tagger of Toutanova and
Manning (2000). DD Combination: a model that maxi-
mizes the joint score of parse and tag selection.
</tableCaption>
<subsectionHeader confidence="0.8377685">
7.2 Integrated Phrase-Structure Parsing and
Trigram POS tagging
</subsectionHeader>
<bodyText confidence="0.999979722222222">
In a second experiment, we used dual decomposi-
tion to integrate the Model 1 parser with the Stan-
ford max-ent trigram POS tagger (Toutanova and
Manning, 2000), using a very similar algorithm to
that described in section 4.1. We use the same train-
ing/dev/test split as in section 7.1. The two models
were again trained separately.
We ran the algorithm with a limit of K = 50 it-
erations. Out of 2416 test examples, the algorithm
found an exact solution in 98.9% of the cases. Ta-
ble 1 gives statistics showing the speed of conver-
gence for different examples: over 94% of the exam-
ples converge to an exact solution in 10 iterations or
fewer. In terms of accuracy, we compare to a base-
line approach of using the first-best tag sequence
as input to the parser. The dual decomposition ap-
proach gives 88.3 F1 measure in recovering parse-
tree constituents, compared to 87.9 for the baseline.
</bodyText>
<sectionHeader confidence="0.998857" genericHeader="method">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999962125">
We have introduced dual-decomposition algorithms
for inference in NLP, given formal properties of the
algorithms in terms of LP relaxations, and demon-
strated their effectiveness on problems that would
traditionally be solved using intersections of dy-
namic programs (Bar-Hillel et al., 1964). Given the
widespread use of dynamic programming in NLP,
there should be many applications for the approach.
There are several possible extensions of the
method we have described. We have focused on
cases where two models are being combined; the
extension to more than two models is straightfor-
ward (e.g., see Komodakis et al. (2007)). This paper
has considered approaches for MAP inference; for
closely related methods that compute approximate
marginals, see Wainwright et al. (2005b).
</bodyText>
<sectionHeader confidence="0.796327" genericHeader="method">
A Fractional Solutions
</sectionHeader>
<bodyText confidence="0.999886333333333">
We now give an example of a point (µ, v) E 20\conv(2)
that demonstrates that the relaxation 20 is strictly larger
than conv(2). Fractional points such as this one can arise
as solutions of the LP relaxation for worst case instances,
preventing us from finding an exact solution.
Recall that the constraints for 20 specify that µ E
conv(Y), v E conv(i), and µ(i, t) = v(i, t) for all
(i, t) E Zuni. Since µ E conv(Y), µ must be a con-
vex combination of 1 or more members of Y; a similar
property holds for v. The example is as follows. There
are two possible parts of speech, A and B, and an addi-
tional non-terminal symbol X. The sentence is of length
3, w1 w2 w3. Let v be the convex combination of the
following two tag sequences, each with probability 0.5:
w1/A w2/A w3/A and w1/A w2/B w3/B. Let µ be
the convex combination of the following two parses, each
with probability 0.5: (X(A w1)(X(A w2)(B w3))) and
(X(A w1)(X(B w2)(A w3))). It can be verified that
µ(i, t) = v(i, t) for all (i, t), i.e., the marginals for single
tags for µ and v agree. Thus, (µ, v) E 20.
To demonstrate that this fractional point is not in
conv(2), we give parameter values such that this frac-
tional point is optimal and all integral points (i.e., ac-
tual parses) are suboptimal. For the tagging model, set
0(AA → A, 3) = 0(AB → B, 3) = 0, with all other pa-
rameters having a negative value. For the parsing model,
set 0(X → A X,1,1, 3) = 0(X → A B, 2, 2, 3) =
0(X → B A, 2, 2, 3) = 0, with all other rule parameters
being negative. For this objective, the fractional solution
has value 0, while all integral points (i.e., all points in 2)
have a negative value. By Theorem 5.2, the maximum of
any linear objective over conv(2) is equal to the maxi-
mum over 2. Thus, (µ, v) E� conv(2).
</bodyText>
<subsectionHeader confidence="0.726301">
B Step Size
</subsectionHeader>
<bodyText confidence="0.9996585">
We used the following step size in our experiments. First,
we initialized α0 to equal 0.5, a relatively large value.
Then we defined αk = α0 * 2−7k, where 77k is the num-
ber of times that L(u(k&apos;)) &gt; L(u(k&apos;−1)) for k0 &lt; k. This
learning rate drops at a rate of 1/21, where t is the num-
ber of times that the dual increases from one iteration to
the next. See Koo et al. (2010) for a similar, but less ag-
gressive step size used to solve a different task.
</bodyText>
<sectionHeader confidence="0.861474" genericHeader="conclusions">
Acknowledgments MIT gratefully acknowledges the
</sectionHeader>
<reference confidence="0.9880969">
support of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-0181.
Any opinions, findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL, or the US
government. Alexander Rush was supported under the GALE
program of the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-C-0022. David Sontag was supported
by a Google PhD Fellowship.
</reference>
<page confidence="0.999266">
10
</page>
<sectionHeader confidence="0.993841" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999948255102041">
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Language and Information: Selected Essays on their
Theory and Application, pages 116–150.
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
namic programming, and the perceptron for efficient,
feature-rich parsing. In Proc CONLL, pages 9–16.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. CoNLL, pages
957–961.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP, page 8.
M. Collins. 2003. Head-driven statistical models for nat-
ural language parsing. In Computational linguistics,
volume 29, pages 589–637.
G.B. Dantzig and P. Wolfe. 1960. Decomposition princi-
ple for linear programs. In Operations research, vol-
ume 8, pages 101–111.
J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007.
Using combinatorial optimization within max-product
belief propagation. In NIPS, volume 19.
J. Eisner. 2000. Bilexical grammars and their cubic-time
parsing algorithms. In Advances in Probabilistic and
Other Parsing Technologies, pages 29–62.
A. Globerson and T. Jaakkola. 2007. Fixing max-
product: Convergent message passing algorithms for
MAP LP-relaxations. In NIPS, volume 21.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In International Conference on
Computer Vision.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. ACL/HLT.
T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual Decomposition for Parsing with Non-
Projective Head Automata. In Proc. EMNLP, pages
63–70.
B.H. Korte and J. Vygen. 2008. Combinatorial optimiza-
tion: theory and algorithms. Springer Verlag.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of English:
The Penn Treebank. In Computational linguistics, vol-
ume 19, pages 313–330.
R.K. Martin, R.L. Rardin, and B.A. Campbell. 1990.
Polyhedral characterization of discrete dynamic pro-
gramming. Operations research, 38(1):127–138.
A.F.T. Martins, N.A. Smith, and E.P. Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT/EMNLP, pages 523–
530.
Angelia Nedi´c and Asuman Ozdaglar. 2009. Approxi-
mate primal solutions and rate analysis for dual sub-
gradient methods. SIAM Journal on Optimization,
19(4):1757–1780.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. ACL.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proc. EMNLP, pages 129–137.
D. Roth and W. Yih. 2005. Integer linear program-
ming inference for conditional random fields. In Proc.
ICML, pages 737–744.
Hanif D. Sherali and Warren P. Adams. 1994. A hi-
erarchy of relaxations and convex hull characteriza-
tions for mixed-integer zero–one programming prob-
lems. Discrete Applied Mathematics, 52(1):83 – 106.
D.A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. EMNLP, pages 145–156.
D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP relaxations for MAP
using message passing. In Proc. UAI.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. EMNLP,
pages 1–8.
K. Toutanova and C.D. Manning. 2000. Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger. In Proc. EMNLP, pages 63–70.
M. Wainwright and M. I. Jordan. 2008. Graphical Mod-
els, Exponential Families, and Variational Inference.
Now Publishers Inc., Hanover, MA, USA.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005a.
MAP estimation via agreement on trees: message-
passing and linear programming. In IEEE Transac-
tions on Information Theory, volume 51, pages 3697–
3717.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005b. A
new class of upper bounds on the log partition func-
tion. In IEEE Transactions on Information Theory,
volume 51, pages 2313–2335.
C. Yanover, T. Meltzer, and Y. Weiss. 2006. Linear
Programming Relaxations and Belief Propagation–An
Empirical Study. In The Journal of Machine Learning
Research, volume 7, page 1907. MIT Press.
</reference>
<page confidence="0.999486">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.310878">
<title confidence="0.999582">On Dual Decomposition and Linear Programming for Natural Language Processing</title>
<author confidence="0.999661">Alexander M Rush David Sontag Michael Collins Tommi Jaakkola</author>
<affiliation confidence="0.31421">MIT CSAIL, Cambridge, MA 02139,</affiliation>
<abstract confidence="0.9995016">paper introduces decomposition a framework for deriving inference algorithms for NLP problems. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. It to algorithms that are in that they existing decoding algorithms; in that they avoid exact algorithms for the full and often in that empirically they often recover the correct solution in spite of using an LP relaxation. We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>support of Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the DARPA, AFRL, or the US government. Alexander Rush was supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-C-0022. David Sontag was supported by a Google PhD Fellowship.</title>
<marker></marker>
<rawString>support of Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the DARPA, AFRL, or the US government. Alexander Rush was supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-C-0022. David Sontag was supported by a Google PhD Fellowship.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bar-Hillel</author>
<author>M Perles</author>
<author>E Shamir</author>
</authors>
<title>On formal properties of simple phrase structure grammars.</title>
<date>1964</date>
<booktitle>In Language and Information: Selected Essays on their Theory and Application,</booktitle>
<pages>116--150</pages>
<contexts>
<context position="3307" citStr="Bar-Hillel et al. (1964)" startWordPosition="500" endWordPosition="503">ality when they recover the exact solution, and also opens up the possibility of methods that incrementally tighten the LP relaxation until it is exact (Sherali and Adams, 1994; Sontag et al., 2008). The structure of this paper is as follows. We first give two examples as an illustration of the approach: 1) integrated parsing and trigram part-ofspeech (POS) tagging; and 2) combined phrasestructure and dependency parsing. In both settings, it is possible to solve the integrated problem through an “intersected” dynamic program (e.g., for integration of parsing and tagging, the construction from Bar-Hillel et al. (1964) can be used). However, these methods, although polynomial time, are substantially less efficient than our algorithms, and are considerably more complex to implement. Next, we describe exact polyhedral formulations for the two problems, building on connections between dynamic programming algorithms and marginal polytopes, as described in Martin et al. (1990). These allow us to precisely characterize the relationship between the exact formulations and the 2More generally, other exact inference methods can be used as oracles, for example spanning tree algorithms for nonprojective dependency stru</context>
<context position="38084" citStr="Bar-Hillel et al., 1964" startWordPosition="7038" endWordPosition="7041">4% of the examples converge to an exact solution in 10 iterations or fewer. In terms of accuracy, we compare to a baseline approach of using the first-best tag sequence as input to the parser. The dual decomposition approach gives 88.3 F1 measure in recovering parsetree constituents, compared to 87.9 for the baseline. 8 Conclusions We have introduced dual-decomposition algorithms for inference in NLP, given formal properties of the algorithms in terms of LP relaxations, and demonstrated their effectiveness on problems that would traditionally be solved using intersections of dynamic programs (Bar-Hillel et al., 1964). Given the widespread use of dynamic programming in NLP, there should be many applications for the approach. There are several possible extensions of the method we have described. We have focused on cases where two models are being combined; the extension to more than two models is straightforward (e.g., see Komodakis et al. (2007)). This paper has considered approaches for MAP inference; for closely related methods that compute approximate marginals, see Wainwright et al. (2005b). A Fractional Solutions We now give an example of a point (µ, v) E 20\conv(2) that demonstrates that the relaxati</context>
</contexts>
<marker>Bar-Hillel, Perles, Shamir, 1964</marker>
<rawString>Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal properties of simple phrase structure grammars. In Language and Information: Selected Essays on their Theory and Application, pages 116–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proc CONLL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="34326" citStr="Carreras et al. (2008)" startWordPosition="6385" endWordPosition="6388">tences in section 23, case 1 occurred for 2407 (99.6%) sentences. Table 1 gives statistics showing the number of iterations required for convergence. Over 80% of the examples converge in 5 iterations or fewer; over 90% converge in 10 iterations or fewer. We compare the accuracy of the dual decomposition approach to two baselines: first, Model 1; and second, a naive integration method that enforces the hard constraint that Model 1 must only consider de11We use a reimplementation that is a slight modification of Collins Model 1, with very similar performance, and which uses the TAG formalism of Carreras et al. (2008). 12Note that the models f1 and f2 were trained separately, using the methods described by Collins (2003) and Koo et al. (2008) respectively. Precision Recall F1 Dep Model 1 88.4 87.8 88.1 91.4 Koo08 Baseline 89.9 89.6 89.7 93.3 DD Combination 91.0 90.4 90.7 93.8 Table 2: Performance results for Section 23 of the WSJ Treebank. Model 1: a reimplementation of the generative parser of (Collins, 2002). Koo08 Baseline: Model 1 with a hard restriction to dependencies predicted by the discriminative dependency parser of (Koo et al., 2008). DD Combination: a model that maximizes the joint score of the</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>X. Carreras, M. Collins, and T. Koo. 2008. TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proc CONLL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proc. CoNLL,</booktitle>
<pages>957--961</pages>
<contexts>
<context position="16219" citStr="Carreras (2007)" startWordPosition="2872" endWordPosition="2873">, and simple additive updates to the Lagrange multipliers enforcing agreement between the two models. 4.2 Integrating Two Lexicalized Parsers Our second example problem is the integration of a phrase-structure parser with a higher-order dependency parser. The goal is to add higher-order features to phrase-structure parsing without greatly increasing the complexity of inference. First, we define an index set for second-order unlabeled projective dependency parsing. The secondorder parser considers first-order dependencies, as well as grandparent and sibling second-order dependencies (e.g., see Carreras (2007)). We assume that Id,p is an index set containing all such dependencies (for brevity we omit the details of this index set). For convenience we define an extended index set that makes explicit use of first-order dependeny∈Y z∈Z Dual decomposition exploits this idea; it results in the algorithm given in figure 1. The algorithm optimizes the combined objective by repeatedly solving the two sub-problems separately—that is, it directly u(k)(i, t)y(i, t)) u(k)(i, t)z(i, t)) 4 cies, I0dep = Idep ∪ Ifirst, where Ifirst = {(i, j) : i ∈ {0 ... n}, j ∈ {1... n}, i =6 j} Here (i, j) represents a dependen</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>X. Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proc. CoNLL, pages 957–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>8</pages>
<contexts>
<context position="34726" citStr="Collins, 2002" startWordPosition="6455" endWordPosition="6456">d constraint that Model 1 must only consider de11We use a reimplementation that is a slight modification of Collins Model 1, with very similar performance, and which uses the TAG formalism of Carreras et al. (2008). 12Note that the models f1 and f2 were trained separately, using the methods described by Collins (2003) and Koo et al. (2008) respectively. Precision Recall F1 Dep Model 1 88.4 87.8 88.1 91.4 Koo08 Baseline 89.9 89.6 89.7 93.3 DD Combination 91.0 90.4 90.7 93.8 Table 2: Performance results for Section 23 of the WSJ Treebank. Model 1: a reimplementation of the generative parser of (Collins, 2002). Koo08 Baseline: Model 1 with a hard restriction to dependencies predicted by the discriminative dependency parser of (Koo et al., 2008). DD Combination: a model that maximizes the joint score of the two parsers. Dep shows the unlabeled dependency accuracy of each system. 0 10 20 30 40 50 Maximum Number of Dual Decomposition Iterations Figure 5: Performance on the parsing task assuming a fixed number of iterations K. f-score: accuracy of the method. % certificates: percentage of examples for which a certificate of optimality is provided. % match: percentage of cases where the output from the </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP, page 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<booktitle>In Computational linguistics,</booktitle>
<volume>29</volume>
<pages>589--637</pages>
<contexts>
<context position="4519" citStr="Collins (2003)" startWordPosition="677" endWordPosition="678">ructures. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1–11, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics LP relaxations that we solve. We then give guarantees of convergence for our algorithms by showing that they are instantiations of Lagrangian relaxation, a general method for solving linear programs of a particular form. Finally, we describe experiments that demonstrate the effectiveness of our approach. First, we consider the integration of the generative model for phrase-structure parsing of Collins (2003), with the second-order discriminative dependency parser of Koo et al. (2008). This is an interesting problem in its own right: the goal is to inject the high performance of discriminative dependency models into phrase-structure parsing. The method uses off-theshelf decoders for the two models. We find three main results: 1) in spite of solving an LP relaxation, empirically the method finds an exact solution on over 99% of the examples; 2) the method converges quickly, typically requiring fewer than 10 iterations of decoding; 3) the method gives gains over a baseline method that forces the phr</context>
<context position="31778" citStr="Collins (2003)" startWordPosition="5924" endWordPosition="5925">ds for the algori −αkg(k) αk αk E,,,,k=1αk minu 1 X1 conv(Y), αk, thm in section 4.2. i 2 (x ,x ) Ex(k) 1= Fx(k) 2 (x(k) 1 ,x(k) 2 (x(k) 1 ,x(k) 2 ginal inference problem. (Case 2) If case 1 does not arise, then a couple of strategies are possible. (This situation could arise in cases where the LP is not tight—i.e., it has a fractional solution—or where K is not large enough for convergence.) The first is to define the primal solution to be the average of the solutions en7.1 Integrated Phrase-Structure and Dependency Parsing Our first set of experiments considers the integration of Model 1 of Collins (2003) (a lexicalized phrasestructure parser, fr om here on referred to as Model 9We have that + B2 = the last equality is because and are defined by the respective arg Thus, and u(k) are primal and dual optimal. 10The resulting fractional solution can be projected back to the set Q, see (Smith an B1·x(k) 1 ·x(k) 2 L(u(k),x(k) 1 , x(k) 2 )=L(u(k)), where x(k) 1 x(k) 2 max’s. (x(k) 1 ,x(k) 2 ) d Eisner, 2008; Martins et al., 2009). Itn. 1 2 3 4 5-10 11-20 20-50 ** Dep 43.5 20.1 10.2 4.9 14.0 5.7 1.4 0.4 POS 58.7 15.4 6.3 3.6 10.3 3.8 0.8 1.1 Table 1: Convergence results for Section 23 of the WSJ Tree</context>
<context position="34431" citStr="Collins (2003)" startWordPosition="6404" endWordPosition="6405">iterations required for convergence. Over 80% of the examples converge in 5 iterations or fewer; over 90% converge in 10 iterations or fewer. We compare the accuracy of the dual decomposition approach to two baselines: first, Model 1; and second, a naive integration method that enforces the hard constraint that Model 1 must only consider de11We use a reimplementation that is a slight modification of Collins Model 1, with very similar performance, and which uses the TAG formalism of Carreras et al. (2008). 12Note that the models f1 and f2 were trained separately, using the methods described by Collins (2003) and Koo et al. (2008) respectively. Precision Recall F1 Dep Model 1 88.4 87.8 88.1 91.4 Koo08 Baseline 89.9 89.6 89.7 93.3 DD Combination 91.0 90.4 90.7 93.8 Table 2: Performance results for Section 23 of the WSJ Treebank. Model 1: a reimplementation of the generative parser of (Collins, 2002). Koo08 Baseline: Model 1 with a hard restriction to dependencies predicted by the discriminative dependency parser of (Koo et al., 2008). DD Combination: a model that maximizes the joint score of the two parsers. Dep shows the unlabeled dependency accuracy of each system. 0 10 20 30 40 50 Maximum Number</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>M. Collins. 2003. Head-driven statistical models for natural language parsing. In Computational linguistics, volume 29, pages 589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G B Dantzig</author>
<author>P Wolfe</author>
</authors>
<title>Decomposition principle for linear programs.</title>
<date>1960</date>
<journal>In Operations research,</journal>
<volume>8</volume>
<pages>101--111</pages>
<contexts>
<context position="1583" citStr="Dantzig and Wolfe, 1960" startWordPosition="230" endWordPosition="233">n of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger. 1 Introduction Dynamic programming algorithms have been remarkably useful for inference in many NLP problems. Unfortunately, as models become more complex, for example through the addition of new features or components, dynamic programming algorithms can quickly explode in terms of computational or implementational complexity.1 As a result, efficiency of inference is a critical bottleneck for many problems in statistical NLP. This paper introduces dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) as a framework for deriving inference algorithms in NLP. Dual decomposition leverages the observation that complex inference problems can often be decomposed into efficiently solvable sub-problems. The approach leads to inference algorithms with the following properties: 1The same is true for NLP inference algorithms based on other exact combinatorial methods, for example methods based on minimum-weight spanning trees (McDonald et al., 2005), or graph cuts (Pang and Lee, 2004). 1 • The resulting algorithms are simple and efficient, building on standard dynamic-program</context>
</contexts>
<marker>Dantzig, Wolfe, 1960</marker>
<rawString>G.B. Dantzig and P. Wolfe. 1960. Decomposition principle for linear programs. In Operations research, volume 8, pages 101–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>D Tarlow</author>
<author>G Elidan</author>
<author>D Koller</author>
</authors>
<title>Using combinatorial optimization within max-product belief propagation.</title>
<date>2007</date>
<booktitle>In NIPS,</booktitle>
<volume>19</volume>
<contexts>
<context position="6922" citStr="Duchi et al., 2007" startWordPosition="1064" endWordPosition="1067">hods for inference in Markov random fields (MRFs) (Wainwright et al., 2005a; Komodakis et al., 2007; Globerson and Jaakkola, 2007). In this approach, the MRF is decomposed into sub-problems corresponding to treestructured subgraphs that together cover all edges of the original graph. The resulting inference algorithms provably solve an LP relaxation of the MRF inference problem, often significantly faster than commercial LP solvers (Yanover et al., 2006). Our work is also related to methods that incorporate combinatorial solvers within loopy belief propagation (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). Our approach similarly makes use of combinatorial algorithms to efficiently solve subproblems of the global inference problem. However, unlike LBP, our algorithms have strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. These guarantees are possible because our algorithms directly solve an LP relaxation. Other work has considered LP or integer linear programming (ILP) formulations of inference in NLP (Martins et al., 2009; Riedel and Clarke, 2006; Roth and Yih, 2005). These appro</context>
</contexts>
<marker>Duchi, Tarlow, Elidan, Koller, 2007</marker>
<rawString>J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007. Using combinatorial optimization within max-product belief propagation. In NIPS, volume 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Bilexical grammars and their cubic-time parsing algorithms.</title>
<date>2000</date>
<booktitle>In Advances in Probabilistic and Other Parsing Technologies,</booktitle>
<pages>29--62</pages>
<contexts>
<context position="20189" citStr="Eisner (2000)" startWordPosition="3640" endWordPosition="3641">plex to describe.6 However, Martin et al. (1990) show that for a very general class of dynamic programming problems, the corresponding marginal polytope can be expressed as conv(Y) = {µ ∈ R&apos; : Aµ = b,µ ≥ 0} (4) where A is a p × m matrix, b is vector in Rp, and the value p is linear in the size of a hypergraph representation of the dynamic program. Note that A and b specify a set of p linear constraints. We now give an explicit description of the resulting constraints for CFG parsing:7 similar constraints arise for other dynamic programming algorithms for parsing, for example the algorithms of Eisner (2000). The exact form of the constraints, and the fact that they are polynomial in number, is not essential for the formal results in this paper. However, a description of the constraints gives valuable intuition for the structure of the marginal polytope. The constraints are given in figure 2. To develop some intuition, consider the case where the variables µr are restricted to be binary: hence each binary vector µ specifies a parse tree. The second constraint in Eq. 5 specifies that exactly one rule must be used at the top of the tree. The set of constraints in Eq. 6 specify that for each product</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>J. Eisner. 2000. Bilexical grammars and their cubic-time parsing algorithms. In Advances in Probabilistic and Other Parsing Technologies, pages 29–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Globerson</author>
<author>T Jaakkola</author>
</authors>
<title>Fixing maxproduct: Convergent message passing algorithms for MAP LP-relaxations.</title>
<date>2007</date>
<booktitle>In NIPS,</booktitle>
<volume>21</volume>
<contexts>
<context position="6433" citStr="Globerson and Jaakkola, 2007" startWordPosition="988" endWordPosition="991">erning marginal polytopes—it is straightforward to use other combinatorial algorithms within the approach. For example, Koo et al. (2010) describe a dual decomposition approach for non-projective dependency parsing, which makes use of both dynamic programming and spanning tree inference algorithms. 2 Related Work Dual decomposition is a classical method for solving optimization problems that can be decomposed into efficiently solvable sub-problems. Our work is inspired by dual decomposition methods for inference in Markov random fields (MRFs) (Wainwright et al., 2005a; Komodakis et al., 2007; Globerson and Jaakkola, 2007). In this approach, the MRF is decomposed into sub-problems corresponding to treestructured subgraphs that together cover all edges of the original graph. The resulting inference algorithms provably solve an LP relaxation of the MRF inference problem, often significantly faster than commercial LP solvers (Yanover et al., 2006). Our work is also related to methods that incorporate combinatorial solvers within loopy belief propagation (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). Our approach similarly makes use of combinatorial algorit</context>
</contexts>
<marker>Globerson, Jaakkola, 2007</marker>
<rawString>A. Globerson and T. Jaakkola. 2007. Fixing maxproduct: Convergent message passing algorithms for MAP LP-relaxations. In NIPS, volume 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Komodakis</author>
<author>N Paragios</author>
<author>G Tziritas</author>
</authors>
<title>MRF optimization via dual decomposition: Messagepassing revisited.</title>
<date>2007</date>
<booktitle>In International Conference on Computer Vision.</booktitle>
<contexts>
<context position="1608" citStr="Komodakis et al., 2007" startWordPosition="234" endWordPosition="237">ing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger. 1 Introduction Dynamic programming algorithms have been remarkably useful for inference in many NLP problems. Unfortunately, as models become more complex, for example through the addition of new features or components, dynamic programming algorithms can quickly explode in terms of computational or implementational complexity.1 As a result, efficiency of inference is a critical bottleneck for many problems in statistical NLP. This paper introduces dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) as a framework for deriving inference algorithms in NLP. Dual decomposition leverages the observation that complex inference problems can often be decomposed into efficiently solvable sub-problems. The approach leads to inference algorithms with the following properties: 1The same is true for NLP inference algorithms based on other exact combinatorial methods, for example methods based on minimum-weight spanning trees (McDonald et al., 2005), or graph cuts (Pang and Lee, 2004). 1 • The resulting algorithms are simple and efficient, building on standard dynamic-programming algorithms as oracle</context>
<context position="6402" citStr="Komodakis et al., 2007" startWordPosition="984" endWordPosition="987"> the formal results concerning marginal polytopes—it is straightforward to use other combinatorial algorithms within the approach. For example, Koo et al. (2010) describe a dual decomposition approach for non-projective dependency parsing, which makes use of both dynamic programming and spanning tree inference algorithms. 2 Related Work Dual decomposition is a classical method for solving optimization problems that can be decomposed into efficiently solvable sub-problems. Our work is inspired by dual decomposition methods for inference in Markov random fields (MRFs) (Wainwright et al., 2005a; Komodakis et al., 2007; Globerson and Jaakkola, 2007). In this approach, the MRF is decomposed into sub-problems corresponding to treestructured subgraphs that together cover all edges of the original graph. The resulting inference algorithms provably solve an LP relaxation of the MRF inference problem, often significantly faster than commercial LP solvers (Yanover et al., 2006). Our work is also related to methods that incorporate combinatorial solvers within loopy belief propagation (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). Our approach similarly mak</context>
<context position="30070" citStr="Komodakis et al. (2007)" startWordPosition="5600" endWordPosition="5603">lgorithm, then simply take to be the primal solution. In this case the pair exactly solves the original LP.9 If this case arises in the algorithm in figure 1, then the resulting solution is binary (i.e., it is a member of Q), and the solution exactly solves the ori countered during the algorithm: --76 = Ek x = Ek x(2k) Results from and Ozdaglar (2009) show that as K —* oo, these averaged solutions converge to the optimal primal solution.10 A second strategy (as given in figure 1) is to simply take as an approximation to the primal solution. This method is a heuristic, but previous work (e.g., Komodakis et al. (2007)) has shown that it is effective in practice; we use it in this paper. In our experiments we found that in the vast majority of cases, case 1 applies, after a small 1 x(k) 1 /K, 2 /K. Nedi´c (x(K) 1 ,x(K) 2 ) number of iterations; see the next section for more details. 7 Experiments u(k+1) , u(k) where g(k) is the subgradient of L at u(k) and &gt; 0 is the step size of the update. The complete subgradient algorithm is given in figure 4. The following convergence theorem is well-known (e.g., see page 120 of Korte and Vygen (2008)): Theorem 6.1 If limk,,,,, = 0 and = oo, then limk,,,,, L(u(k)) = L(</context>
<context position="38418" citStr="Komodakis et al. (2007)" startWordPosition="7093" endWordPosition="7096">e have introduced dual-decomposition algorithms for inference in NLP, given formal properties of the algorithms in terms of LP relaxations, and demonstrated their effectiveness on problems that would traditionally be solved using intersections of dynamic programs (Bar-Hillel et al., 1964). Given the widespread use of dynamic programming in NLP, there should be many applications for the approach. There are several possible extensions of the method we have described. We have focused on cases where two models are being combined; the extension to more than two models is straightforward (e.g., see Komodakis et al. (2007)). This paper has considered approaches for MAP inference; for closely related methods that compute approximate marginals, see Wainwright et al. (2005b). A Fractional Solutions We now give an example of a point (µ, v) E 20\conv(2) that demonstrates that the relaxation 20 is strictly larger than conv(2). Fractional points such as this one can arise as solutions of the LP relaxation for worst case instances, preventing us from finding an exact solution. Recall that the constraints for 20 specify that µ E conv(Y), v E conv(i), and µ(i, t) = v(i, t) for all (i, t) E Zuni. Since µ E conv(Y), µ must</context>
</contexts>
<marker>Komodakis, Paragios, Tziritas, 2007</marker>
<rawString>N. Komodakis, N. Paragios, and G. Tziritas. 2007. MRF optimization via dual decomposition: Messagepassing revisited. In International Conference on Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semisupervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. ACL/HLT.</booktitle>
<contexts>
<context position="4596" citStr="Koo et al. (2008)" startWordPosition="686" endWordPosition="689">al Language Processing, pages 1–11, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics LP relaxations that we solve. We then give guarantees of convergence for our algorithms by showing that they are instantiations of Lagrangian relaxation, a general method for solving linear programs of a particular form. Finally, we describe experiments that demonstrate the effectiveness of our approach. First, we consider the integration of the generative model for phrase-structure parsing of Collins (2003), with the second-order discriminative dependency parser of Koo et al. (2008). This is an interesting problem in its own right: the goal is to inject the high performance of discriminative dependency models into phrase-structure parsing. The method uses off-theshelf decoders for the two models. We find three main results: 1) in spite of solving an LP relaxation, empirically the method finds an exact solution on over 99% of the examples; 2) the method converges quickly, typically requiring fewer than 10 iterations of decoding; 3) the method gives gains over a baseline method that forces the phrase-structure parser to produce the same dependencies as the firstbest output</context>
<context position="32718" citStr="Koo et al. (2008)" startWordPosition="6099" endWordPosition="6102">1 , x(k) 2 )=L(u(k)), where x(k) 1 x(k) 2 max’s. (x(k) 1 ,x(k) 2 ) d Eisner, 2008; Martins et al., 2009). Itn. 1 2 3 4 5-10 11-20 20-50 ** Dep 43.5 20.1 10.2 4.9 14.0 5.7 1.4 0.4 POS 58.7 15.4 6.3 3.6 10.3 3.8 0.8 1.1 Table 1: Convergence results for Section 23 of the WSJ Treebank for the dependency parsing and POS experiments. Each column gives the percentage of sentences whose exact solutions were found in a given range of subgradient iterations. ** is the percentage of sentences that did not converge by the iteration limit (K=50). 1),11 and the 2nd order discriminative dependency parser of Koo et al. (2008). The inference problem for a sentence x is to find y* = arg maY (1 x (fl +&apos;Yf2(y)) 1) YE where Y is the set of all lexicalized phrase-structure trees for the sentence x; f1(y) is the score (log probability) under Model 1; f2(y) is the score under Koo et al. (2008) for the dependency structure implied by y; and γ &gt; 0 is a parameter dictating the relative weight of the two models.12 This problem is similar to the second example in section 4; a very similar dual decomposition algorithm to that described in section 4.2 can be derived. We used the Penn Wall Street Treebank (Marcus et al., 1994) fo</context>
<context position="34453" citStr="Koo et al. (2008)" startWordPosition="6407" endWordPosition="6410"> for convergence. Over 80% of the examples converge in 5 iterations or fewer; over 90% converge in 10 iterations or fewer. We compare the accuracy of the dual decomposition approach to two baselines: first, Model 1; and second, a naive integration method that enforces the hard constraint that Model 1 must only consider de11We use a reimplementation that is a slight modification of Collins Model 1, with very similar performance, and which uses the TAG formalism of Carreras et al. (2008). 12Note that the models f1 and f2 were trained separately, using the methods described by Collins (2003) and Koo et al. (2008) respectively. Precision Recall F1 Dep Model 1 88.4 87.8 88.1 91.4 Koo08 Baseline 89.9 89.6 89.7 93.3 DD Combination 91.0 90.4 90.7 93.8 Table 2: Performance results for Section 23 of the WSJ Treebank. Model 1: a reimplementation of the generative parser of (Collins, 2002). Koo08 Baseline: Model 1 with a hard restriction to dependencies predicted by the discriminative dependency parser of (Koo et al., 2008). DD Combination: a model that maximizes the joint score of the two parsers. Dep shows the unlabeled dependency accuracy of each system. 0 10 20 30 40 50 Maximum Number of Dual Decomposition</context>
<context position="35792" citStr="Koo et al. (2008)" startWordPosition="6633" endWordPosition="6636">thod. % certificates: percentage of examples for which a certificate of optimality is provided. % match: percentage of cases where the output from the method is identical to the output when using K = 50. pendencies seen in the first-best output from the dependency parser. Table 2 shows all three results. The dual decomposition method gives a significant gain in precision and recall over the naive combination method, and boosts the performance of Model 1 to a level that is close to some of the best single-pass parsers on the Penn treebank test set. Dependency accuracy is also improved over the Koo et al. (2008) model, in spite of the relatively low dependency accuracy of Model 1 alone. Figure 5 shows performance of the approach as a function of K, the maximum number of iterations of dual decomposition. For this experiment, for cases where the method has not converged for k &lt; K, the output from the algorithm is chosen to be the y(k) for k &lt; K that maximizes the objective function in Eq. 11. The graphs show that values of K less than 50 produce almost identical performance to K = 50, but with fewer cases giving certificates of optimality (with K = 10, the f-score of the method is 90.69%; with K = 5 it</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semisupervised dependency parsing. In Proc. ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A M Rush</author>
<author>M Collins</author>
<author>T Jaakkola</author>
<author>D Sontag</author>
</authors>
<title>Dual Decomposition for Parsing with NonProjective Head Automata. In</title>
<date>2010</date>
<booktitle>Proc. EMNLP,</booktitle>
<pages>63--70</pages>
<contexts>
<context position="5941" citStr="Koo et al. (2010)" startWordPosition="915" endWordPosition="918">d the dual decomposition method has an F1 score of 90.7%). In a second set of experiments, we use dual decomposition to integrate the trigram POS tagger of Toutanova and Manning (2000) with the parser of Collins (2003). We again find that the method finds an exact solution in almost all cases, with convergence in just a few iterations of decoding. Although the focus of this paper is on dynamic programming algorithms—both in the experiments, and also in the formal results concerning marginal polytopes—it is straightforward to use other combinatorial algorithms within the approach. For example, Koo et al. (2010) describe a dual decomposition approach for non-projective dependency parsing, which makes use of both dynamic programming and spanning tree inference algorithms. 2 Related Work Dual decomposition is a classical method for solving optimization problems that can be decomposed into efficiently solvable sub-problems. Our work is inspired by dual decomposition methods for inference in Markov random fields (MRFs) (Wainwright et al., 2005a; Komodakis et al., 2007; Globerson and Jaakkola, 2007). In this approach, the MRF is decomposed into sub-problems corresponding to treestructured subgraphs that t</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>T. Koo, A.M. Rush, M. Collins, T. Jaakkola, and D. Sontag. 2010. Dual Decomposition for Parsing with NonProjective Head Automata. In Proc. EMNLP, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B H Korte</author>
<author>J Vygen</author>
</authors>
<title>Combinatorial optimization: theory and algorithms.</title>
<date>2008</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="20960" citStr="Korte and Vygen (2008)" startWordPosition="3785" endWordPosition="3788"> a description of the constraints gives valuable intuition for the structure of the marginal polytope. The constraints are given in figure 2. To develop some intuition, consider the case where the variables µr are restricted to be binary: hence each binary vector µ specifies a parse tree. The second constraint in Eq. 5 specifies that exactly one rule must be used at the top of the tree. The set of constraints in Eq. 6 specify that for each production of the form 6For any finite set Y, conv(Y) can be expressed as {µ E R&apos; : Aµ &lt; b} where A is a matrix of dimension p x m, and b E R-v (see, e.g., Korte and Vygen (2008), pg. 65). The value for p depends on the set Y, and can be exponential in size. 7Taskar et al. (2004) describe the same set of constraints, but without proof of correctness or reference to Martin et al. (1990). 5 ∀r ∈ I&apos;, µ,. ≥ 0 ; E µ(X → Y Z, 1, k, n) = 1 (5) E∀r ∈ I&apos; tag, v,. ≥ 0 ; v((X, Y ) → Z, 3) = 1 X,Y,ZEN X,Y,ZET k=1...(n−1) ∀X ∈ N, ∀(i, j) such that 1 ≤ i &lt; j ≤ n and (i, j) =6 (1, n): ∀X ∈ T, ∀i ∈ {3 ... n − 1}: µ(Y → Z X, k, i − 1, j) E v((Y, Z) → X, i) = E Y,ZET ∀X ∈ T, ∀i ∈ {3 ... n − 2}: E v((Y, Z) → X, i) = E Y,ZET Y,ZET v((Y, X) → Z, i + 1) v((X, Y ) → Z, i + 2) E Y,ZEN k=i...</context>
<context position="23048" citStr="Korte and Vygen (2008)" startWordPosition="4262" endWordPosition="4265">Define Y to be the set of all CFG parses, as defined in section 4. Then conv(Y) = {µ ∈ R&apos; : µ satisifies Eqs.(5–7)} Proof: This theorem is a special case of Martin et al. (1990), theorem 2. The marginal polytope for tagging, conv(Z), can also be expressed using linear constraints as in Eq. 4; see figure 3. These constraints follow from results for graphical models (Wainwright and Jordan, 2008), or from the Martin et al. (1990) construction. As a final point, the following theorem gives an important property of marginal polytopes, which we will use at several points in this paper: Theorem 5.2 (Korte and Vygen (2008), page 66.) For any set Y ⊆ {0, 1}k, and for any vector 0 ∈ Rk, max y · 0 = max µ · 0 (8) yEY µEconv(Y) The theorem states that for a linear objective function, maximization over a discrete set Y can be replaced by maximization over the convex hull Figure 3: The linear constraints defining the marginal polytope for trigram POS tagging. conv(Y). The problem maxµEconv(Y) µ·0 is a linear programming problem. For parsing, this theorem implies that: 1. Weighted CFG parsing can be framed as a linear programming problem, of the form maxµEconv(Y) µ· 0, where conv(Y) is specified by a polynomial number</context>
<context position="30601" citStr="Korte and Vygen (2008)" startWordPosition="5705" endWordPosition="5708">imal solution. This method is a heuristic, but previous work (e.g., Komodakis et al. (2007)) has shown that it is effective in practice; we use it in this paper. In our experiments we found that in the vast majority of cases, case 1 applies, after a small 1 x(k) 1 /K, 2 /K. Nedi´c (x(K) 1 ,x(K) 2 ) number of iterations; see the next section for more details. 7 Experiments u(k+1) , u(k) where g(k) is the subgradient of L at u(k) and &gt; 0 is the step size of the update. The complete subgradient algorithm is given in figure 4. The following convergence theorem is well-known (e.g., see page 120 of Korte and Vygen (2008)): Theorem 6.1 If limk,,,,, = 0 and = oo, then limk,,,,, L(u(k)) = L(u). The following proposition is easily verified: Proposition 6.1 The algorithm in figure is an instantiation of the algorithm in figure 4,8 with = X2 = conv(i), and the matrices E and F defined to be binary matrices specifying the constraints µ(i, t) = v(i, t) for all (i, t) E Zuni. Under an appropriate definition of the step sizes it follows that the algorithm in figure 1 defines a sequence of Lagrange multiplers u(k) minimizing a dual of the LP relaxation in Eq. 10. A similar result holds for the algori −αkg(k) αk αk E,,,,</context>
</contexts>
<marker>Korte, Vygen, 2008</marker>
<rawString>B.H. Korte and J. Vygen. 2008. Combinatorial optimization: theory and algorithms. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1994</date>
<booktitle>In Computational linguistics,</booktitle>
<volume>19</volume>
<pages>313--330</pages>
<contexts>
<context position="33315" citStr="Marcus et al., 1994" startWordPosition="6213" endWordPosition="6216">er of Koo et al. (2008). The inference problem for a sentence x is to find y* = arg maY (1 x (fl +&apos;Yf2(y)) 1) YE where Y is the set of all lexicalized phrase-structure trees for the sentence x; f1(y) is the score (log probability) under Model 1; f2(y) is the score under Koo et al. (2008) for the dependency structure implied by y; and γ &gt; 0 is a parameter dictating the relative weight of the two models.12 This problem is similar to the second example in section 4; a very similar dual decomposition algorithm to that described in section 4.2 can be derived. We used the Penn Wall Street Treebank (Marcus et al., 1994) for the experiments, with sections 2-21 for training, section 22 for development, and section 23 for testing. The parameter γ was chosen to optimize performance on the development set. We ran the dual decomposition algorithm with a limit of K = 50 iterations. The dual decomposition algorithm returns an exact solution if case 1 occurs as defined in section 6.2; we found that of 2416 sentences in section 23, case 1 occurred for 2407 (99.6%) sentences. Table 1 gives statistics showing the number of iterations required for convergence. Over 80% of the examples converge in 5 iterations or fewer; o</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn Treebank. In Computational linguistics, volume 19, pages 313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R K Martin</author>
<author>R L Rardin</author>
<author>B A Campbell</author>
</authors>
<title>Polyhedral characterization of discrete dynamic programming.</title>
<date>1990</date>
<journal>Operations research,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="3667" citStr="Martin et al. (1990)" startWordPosition="553" endWordPosition="556">ing; and 2) combined phrasestructure and dependency parsing. In both settings, it is possible to solve the integrated problem through an “intersected” dynamic program (e.g., for integration of parsing and tagging, the construction from Bar-Hillel et al. (1964) can be used). However, these methods, although polynomial time, are substantially less efficient than our algorithms, and are considerably more complex to implement. Next, we describe exact polyhedral formulations for the two problems, building on connections between dynamic programming algorithms and marginal polytopes, as described in Martin et al. (1990). These allow us to precisely characterize the relationship between the exact formulations and the 2More generally, other exact inference methods can be used as oracles, for example spanning tree algorithms for nonprojective dependency structures. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1–11, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics LP relaxations that we solve. We then give guarantees of convergence for our algorithms by showing that they are instantiations of Lagrangian relaxation, a gen</context>
<context position="19624" citStr="Martin et al. (1990)" startWordPosition="3530" endWordPosition="3533"> in Y as A = {a ∈ R|Y |: ay ≥ 0, Ey∈Y ay = 1}. Each a ∈ A gives a vector of marginals, µ = Ey∈Y ayy, where µr can be interpreted as the probability that yr = 1 for a y selected at random from the distribution a. The set of all possible marginal vectors, known as the marginal polytope, is defined as follows: M = {µ ∈ R&apos; : ∃� ∈ A such that µ = 1: ayy} y∈Y M is also frequently referred to as the convex hull of Y, written as conv(Y). We use the notation conv(Y) in the remainder of this paper, instead of M. For an arbitrary set Y, the marginal polytope conv(Y) can be complex to describe.6 However, Martin et al. (1990) show that for a very general class of dynamic programming problems, the corresponding marginal polytope can be expressed as conv(Y) = {µ ∈ R&apos; : Aµ = b,µ ≥ 0} (4) where A is a p × m matrix, b is vector in Rp, and the value p is linear in the size of a hypergraph representation of the dynamic program. Note that A and b specify a set of p linear constraints. We now give an explicit description of the resulting constraints for CFG parsing:7 similar constraints arise for other dynamic programming algorithms for parsing, for example the algorithms of Eisner (2000). The exact form of the constraints</context>
<context position="21170" citStr="Martin et al. (1990)" startWordPosition="3824" endWordPosition="3827">e restricted to be binary: hence each binary vector µ specifies a parse tree. The second constraint in Eq. 5 specifies that exactly one rule must be used at the top of the tree. The set of constraints in Eq. 6 specify that for each production of the form 6For any finite set Y, conv(Y) can be expressed as {µ E R&apos; : Aµ &lt; b} where A is a matrix of dimension p x m, and b E R-v (see, e.g., Korte and Vygen (2008), pg. 65). The value for p depends on the set Y, and can be exponential in size. 7Taskar et al. (2004) describe the same set of constraints, but without proof of correctness or reference to Martin et al. (1990). 5 ∀r ∈ I&apos;, µ,. ≥ 0 ; E µ(X → Y Z, 1, k, n) = 1 (5) E∀r ∈ I&apos; tag, v,. ≥ 0 ; v((X, Y ) → Z, 3) = 1 X,Y,ZEN X,Y,ZET k=1...(n−1) ∀X ∈ N, ∀(i, j) such that 1 ≤ i &lt; j ≤ n and (i, j) =6 (1, n): ∀X ∈ T, ∀i ∈ {3 ... n − 1}: µ(Y → Z X, k, i − 1, j) E v((Y, Z) → X, i) = E Y,ZET ∀X ∈ T, ∀i ∈ {3 ... n − 2}: E v((Y, Z) → X, i) = E Y,ZET Y,ZET v((Y, X) → Z, i + 1) v((X, Y ) → Z, i + 2) E Y,ZEN k=i...(7−1) E µ(X → Y Z, i, k, j) = Y,ZEN k=(7+1)...n Y,ZEN k=1...(i−1) + E µ(Y → X Z, i, j, k) (6) Y,ZET v((Y, Z) → X, i) µ(X → Z Y, k, i − 1, i) (7) ∀Y ∈ T, ∀i ∈ {1 ... n} : µ(i, Y ) = E E µ(X → Y Z, i, i, k) + E ∀</context>
<context position="22603" citStr="Martin et al. (1990)" startWordPosition="4189" endWordPosition="4192">arsing. ∀X ∈ T : v(2, X) = Y,ZET hX → Y Z, i, k, ji in a parse tree, there must be exactly one production higher in the tree that generates (X, i, j) as one of its children. The constraints in Eq. 7 enforce consistency between the µ(i, Y ) variables and rule variables higher in the tree. Note that the constraints in Eqs.(5–7) can be written in the form Aµ = b, µ ≥ 0, as in Eq. 4. Under these definitions, we have the following: Theorem 5.1 Define Y to be the set of all CFG parses, as defined in section 4. Then conv(Y) = {µ ∈ R&apos; : µ satisifies Eqs.(5–7)} Proof: This theorem is a special case of Martin et al. (1990), theorem 2. The marginal polytope for tagging, conv(Z), can also be expressed using linear constraints as in Eq. 4; see figure 3. These constraints follow from results for graphical models (Wainwright and Jordan, 2008), or from the Martin et al. (1990) construction. As a final point, the following theorem gives an important property of marginal polytopes, which we will use at several points in this paper: Theorem 5.2 (Korte and Vygen (2008), page 66.) For any set Y ⊆ {0, 1}k, and for any vector 0 ∈ Rk, max y · 0 = max µ · 0 (8) yEY µEconv(Y) The theorem states that for a linear objective func</context>
</contexts>
<marker>Martin, Rardin, Campbell, 1990</marker>
<rawString>R.K. Martin, R.L. Rardin, and B.A. Campbell. 1990. Polyhedral characterization of discrete dynamic programming. Operations research, 38(1):127–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing. In</title>
<date>2009</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="7463" citStr="Martins et al., 2009" startWordPosition="1144" endWordPosition="1147">n loopy belief propagation (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). Our approach similarly makes use of combinatorial algorithms to efficiently solve subproblems of the global inference problem. However, unlike LBP, our algorithms have strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. These guarantees are possible because our algorithms directly solve an LP relaxation. Other work has considered LP or integer linear programming (ILP) formulations of inference in NLP (Martins et al., 2009; Riedel and Clarke, 2006; Roth and Yih, 2005). These approaches typically use general-purpose LP or ILP solvers. Our method has the advantage that it leverages underlying structure arising in LP formulations of NLP problems. We will see that dynamic programming algorithms such as CKY can be considered to be very efficient solvers for particular LPs. In dual decomposition, these LPs—and their efficient solvers—can be embedded within larger LPs corresponding to more complex inference problems. 3 Background: Structured Models for NLP We now describe the type of models used throughout the paper. </context>
<context position="32205" citStr="Martins et al., 2009" startWordPosition="6005" endWordPosition="6008">he primal solution to be the average of the solutions en7.1 Integrated Phrase-Structure and Dependency Parsing Our first set of experiments considers the integration of Model 1 of Collins (2003) (a lexicalized phrasestructure parser, fr om here on referred to as Model 9We have that + B2 = the last equality is because and are defined by the respective arg Thus, and u(k) are primal and dual optimal. 10The resulting fractional solution can be projected back to the set Q, see (Smith an B1·x(k) 1 ·x(k) 2 L(u(k),x(k) 1 , x(k) 2 )=L(u(k)), where x(k) 1 x(k) 2 max’s. (x(k) 1 ,x(k) 2 ) d Eisner, 2008; Martins et al., 2009). Itn. 1 2 3 4 5-10 11-20 20-50 ** Dep 43.5 20.1 10.2 4.9 14.0 5.7 1.4 0.4 POS 58.7 15.4 6.3 3.6 10.3 3.8 0.8 1.1 Table 1: Convergence results for Section 23 of the WSJ Treebank for the dependency parsing and POS experiments. Each column gives the percentage of sentences whose exact solutions were found in a given range of subgradient iterations. ** is the percentage of sentences that did not converge by the iteration limit (K=50). 1),11 and the 2nd order discriminative dependency parser of Koo et al. (2008). The inference problem for a sentence x is to find y* = arg maY (1 x (fl +&apos;Yf2(y)) 1) </context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>A.F.T. Martins, N.A. Smith, and E.P. Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. HLT/EMNLP,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="2054" citStr="McDonald et al., 2005" startWordPosition="296" endWordPosition="299">fficiency of inference is a critical bottleneck for many problems in statistical NLP. This paper introduces dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) as a framework for deriving inference algorithms in NLP. Dual decomposition leverages the observation that complex inference problems can often be decomposed into efficiently solvable sub-problems. The approach leads to inference algorithms with the following properties: 1The same is true for NLP inference algorithms based on other exact combinatorial methods, for example methods based on minimum-weight spanning trees (McDonald et al., 2005), or graph cuts (Pang and Lee, 2004). 1 • The resulting algorithms are simple and efficient, building on standard dynamic-programming algorithms as oracle solvers for sub-problems,2 together with a method for forcing agreement between the oracles. • The algorithms provably solve a linear programming (LP) relaxation of the original inference problem. • Empirically, the LP relaxation often leads to an exact solution to the original problem. The approach is very general, and should be applicable to a wide range of problems in NLP. The connection to linear programming ensures that the algorithms p</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proc. HLT/EMNLP, pages 523– 530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelia Nedi´c</author>
<author>Asuman Ozdaglar</author>
</authors>
<title>Approximate primal solutions and rate analysis for dual subgradient methods.</title>
<date>2009</date>
<journal>SIAM Journal on Optimization,</journal>
<volume>19</volume>
<issue>4</issue>
<marker>Nedi´c, Ozdaglar, 2009</marker>
<rawString>Angelia Nedi´c and Asuman Ozdaglar. 2009. Approximate primal solutions and rate analysis for dual subgradient methods. SIAM Journal on Optimization, 19(4):1757–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="2090" citStr="Pang and Lee, 2004" startWordPosition="303" endWordPosition="306">ttleneck for many problems in statistical NLP. This paper introduces dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) as a framework for deriving inference algorithms in NLP. Dual decomposition leverages the observation that complex inference problems can often be decomposed into efficiently solvable sub-problems. The approach leads to inference algorithms with the following properties: 1The same is true for NLP inference algorithms based on other exact combinatorial methods, for example methods based on minimum-weight spanning trees (McDonald et al., 2005), or graph cuts (Pang and Lee, 2004). 1 • The resulting algorithms are simple and efficient, building on standard dynamic-programming algorithms as oracle solvers for sub-problems,2 together with a method for forcing agreement between the oracles. • The algorithms provably solve a linear programming (LP) relaxation of the original inference problem. • Empirically, the LP relaxation often leads to an exact solution to the original problem. The approach is very general, and should be applicable to a wide range of problems in NLP. The connection to linear programming ensures that the algorithms provide a certificate of optimality w</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>J Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>129--137</pages>
<contexts>
<context position="7488" citStr="Riedel and Clarke, 2006" startWordPosition="1148" endWordPosition="1151">tion (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). Our approach similarly makes use of combinatorial algorithms to efficiently solve subproblems of the global inference problem. However, unlike LBP, our algorithms have strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. These guarantees are possible because our algorithms directly solve an LP relaxation. Other work has considered LP or integer linear programming (ILP) formulations of inference in NLP (Martins et al., 2009; Riedel and Clarke, 2006; Roth and Yih, 2005). These approaches typically use general-purpose LP or ILP solvers. Our method has the advantage that it leverages underlying structure arising in LP formulations of NLP problems. We will see that dynamic programming algorithms such as CKY can be considered to be very efficient solvers for particular LPs. In dual decomposition, these LPs—and their efficient solvers—can be embedded within larger LPs corresponding to more complex inference problems. 3 Background: Structured Models for NLP We now describe the type of models used throughout the paper. We take some care to set </context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>S. Riedel and J. Clarke. 2006. Incremental integer linear programming for non-projective dependency parsing. In Proc. EMNLP, pages 129–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Integer linear programming inference for conditional random fields.</title>
<date>2005</date>
<booktitle>In Proc. ICML,</booktitle>
<pages>737--744</pages>
<contexts>
<context position="7509" citStr="Roth and Yih, 2005" startWordPosition="1152" endWordPosition="1155">P inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). Our approach similarly makes use of combinatorial algorithms to efficiently solve subproblems of the global inference problem. However, unlike LBP, our algorithms have strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. These guarantees are possible because our algorithms directly solve an LP relaxation. Other work has considered LP or integer linear programming (ILP) formulations of inference in NLP (Martins et al., 2009; Riedel and Clarke, 2006; Roth and Yih, 2005). These approaches typically use general-purpose LP or ILP solvers. Our method has the advantage that it leverages underlying structure arising in LP formulations of NLP problems. We will see that dynamic programming algorithms such as CKY can be considered to be very efficient solvers for particular LPs. In dual decomposition, these LPs—and their efficient solvers—can be embedded within larger LPs corresponding to more complex inference problems. 3 Background: Structured Models for NLP We now describe the type of models used throughout the paper. We take some care to set up notation that will</context>
</contexts>
<marker>Roth, Yih, 2005</marker>
<rawString>D. Roth and W. Yih. 2005. Integer linear programming inference for conditional random fields. In Proc. ICML, pages 737–744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanif D Sherali</author>
<author>Warren P Adams</author>
</authors>
<title>A hierarchy of relaxations and convex hull characterizations for mixed-integer zero–one programming problems.</title>
<date>1994</date>
<journal>Discrete Applied Mathematics,</journal>
<volume>52</volume>
<issue>1</issue>
<pages>106</pages>
<contexts>
<context position="2859" citStr="Sherali and Adams, 1994" startWordPosition="428" endWordPosition="431">together with a method for forcing agreement between the oracles. • The algorithms provably solve a linear programming (LP) relaxation of the original inference problem. • Empirically, the LP relaxation often leads to an exact solution to the original problem. The approach is very general, and should be applicable to a wide range of problems in NLP. The connection to linear programming ensures that the algorithms provide a certificate of optimality when they recover the exact solution, and also opens up the possibility of methods that incrementally tighten the LP relaxation until it is exact (Sherali and Adams, 1994; Sontag et al., 2008). The structure of this paper is as follows. We first give two examples as an illustration of the approach: 1) integrated parsing and trigram part-ofspeech (POS) tagging; and 2) combined phrasestructure and dependency parsing. In both settings, it is possible to solve the integrated problem through an “intersected” dynamic program (e.g., for integration of parsing and tagging, the construction from Bar-Hillel et al. (1964) can be used). However, these methods, although polynomial time, are substantially less efficient than our algorithms, and are considerably more complex</context>
</contexts>
<marker>Sherali, Adams, 1994</marker>
<rawString>Hanif D. Sherali and Warren P. Adams. 1994. A hierarchy of relaxations and convex hull characterizations for mixed-integer zero–one programming problems. Discrete Applied Mathematics, 52(1):83 – 106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>145--156</pages>
<contexts>
<context position="6974" citStr="Smith and Eisner, 2008" startWordPosition="1072" endWordPosition="1075">) (Wainwright et al., 2005a; Komodakis et al., 2007; Globerson and Jaakkola, 2007). In this approach, the MRF is decomposed into sub-problems corresponding to treestructured subgraphs that together cover all edges of the original graph. The resulting inference algorithms provably solve an LP relaxation of the MRF inference problem, often significantly faster than commercial LP solvers (Yanover et al., 2006). Our work is also related to methods that incorporate combinatorial solvers within loopy belief propagation (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). Our approach similarly makes use of combinatorial algorithms to efficiently solve subproblems of the global inference problem. However, unlike LBP, our algorithms have strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. These guarantees are possible because our algorithms directly solve an LP relaxation. Other work has considered LP or integer linear programming (ILP) formulations of inference in NLP (Martins et al., 2009; Riedel and Clarke, 2006; Roth and Yih, 2005). These approaches typically use general-purpose LP or ILP solver</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>D.A. Smith and J. Eisner. 2008. Dependency parsing by belief propagation. In Proc. EMNLP, pages 145–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sontag</author>
<author>T Meltzer</author>
<author>A Globerson</author>
<author>T Jaakkola</author>
<author>Y Weiss</author>
</authors>
<title>Tightening LP relaxations for MAP using message passing.</title>
<date>2008</date>
<booktitle>In Proc. UAI.</booktitle>
<contexts>
<context position="2881" citStr="Sontag et al., 2008" startWordPosition="432" endWordPosition="435">r forcing agreement between the oracles. • The algorithms provably solve a linear programming (LP) relaxation of the original inference problem. • Empirically, the LP relaxation often leads to an exact solution to the original problem. The approach is very general, and should be applicable to a wide range of problems in NLP. The connection to linear programming ensures that the algorithms provide a certificate of optimality when they recover the exact solution, and also opens up the possibility of methods that incrementally tighten the LP relaxation until it is exact (Sherali and Adams, 1994; Sontag et al., 2008). The structure of this paper is as follows. We first give two examples as an illustration of the approach: 1) integrated parsing and trigram part-ofspeech (POS) tagging; and 2) combined phrasestructure and dependency parsing. In both settings, it is possible to solve the integrated problem through an “intersected” dynamic program (e.g., for integration of parsing and tagging, the construction from Bar-Hillel et al. (1964) can be used). However, these methods, although polynomial time, are substantially less efficient than our algorithms, and are considerably more complex to implement. Next, w</context>
</contexts>
<marker>Sontag, Meltzer, Globerson, Jaakkola, Weiss, 2008</marker>
<rawString>D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. 2008. Tightening LP relaxations for MAP using message passing. In Proc. UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="21062" citStr="Taskar et al. (2004)" startWordPosition="3806" endWordPosition="3809">he constraints are given in figure 2. To develop some intuition, consider the case where the variables µr are restricted to be binary: hence each binary vector µ specifies a parse tree. The second constraint in Eq. 5 specifies that exactly one rule must be used at the top of the tree. The set of constraints in Eq. 6 specify that for each production of the form 6For any finite set Y, conv(Y) can be expressed as {µ E R&apos; : Aµ &lt; b} where A is a matrix of dimension p x m, and b E R-v (see, e.g., Korte and Vygen (2008), pg. 65). The value for p depends on the set Y, and can be exponential in size. 7Taskar et al. (2004) describe the same set of constraints, but without proof of correctness or reference to Martin et al. (1990). 5 ∀r ∈ I&apos;, µ,. ≥ 0 ; E µ(X → Y Z, 1, k, n) = 1 (5) E∀r ∈ I&apos; tag, v,. ≥ 0 ; v((X, Y ) → Z, 3) = 1 X,Y,ZEN X,Y,ZET k=1...(n−1) ∀X ∈ N, ∀(i, j) such that 1 ≤ i &lt; j ≤ n and (i, j) =6 (1, n): ∀X ∈ T, ∀i ∈ {3 ... n − 1}: µ(Y → Z X, k, i − 1, j) E v((Y, Z) → X, i) = E Y,ZET ∀X ∈ T, ∀i ∈ {3 ... n − 2}: E v((Y, Z) → X, i) = E Y,ZET Y,ZET v((Y, X) → Z, i + 1) v((X, Y ) → Z, i + 2) E Y,ZEN k=i...(7−1) E µ(X → Y Z, i, k, j) = Y,ZEN k=(7+1)...n Y,ZEN k=1...(i−1) + E µ(Y → X Z, i, j, k) (6) Y,ZET v(</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning. 2004. Max-margin parsing. In Proc. EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy partof-speech tagger.</title>
<date>2000</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>63--70</pages>
<contexts>
<context position="5508" citStr="Toutanova and Manning (2000)" startWordPosition="844" endWordPosition="847">on, empirically the method finds an exact solution on over 99% of the examples; 2) the method converges quickly, typically requiring fewer than 10 iterations of decoding; 3) the method gives gains over a baseline method that forces the phrase-structure parser to produce the same dependencies as the firstbest output from the dependency parser (the Collins (2003) model has an F1 score of 88.1%; the baseline method has an F1 score of 89.7%; and the dual decomposition method has an F1 score of 90.7%). In a second set of experiments, we use dual decomposition to integrate the trigram POS tagger of Toutanova and Manning (2000) with the parser of Collins (2003). We again find that the method finds an exact solution in almost all cases, with convergence in just a few iterations of decoding. Although the focus of this paper is on dynamic programming algorithms—both in the experiments, and also in the formal results concerning marginal polytopes—it is straightforward to use other combinatorial algorithms within the approach. For example, Koo et al. (2010) describe a dual decomposition approach for non-projective dependency parsing, which makes use of both dynamic programming and spanning tree inference algorithms. 2 Re</context>
<context position="36750" citStr="Toutanova and Manning (2000)" startWordPosition="6814" endWordPosition="6817">r k &lt; K that maximizes the objective function in Eq. 11. The graphs show that values of K less than 50 produce almost identical performance to K = 50, but with fewer cases giving certificates of optimality (with K = 10, the f-score of the method is 90.69%; with K = 5 it is 90.63%). Percentage 100 90 80 60 50 70 f score % certificates % match K=50 9 Precision Recall Fl POS Acc Fixed Tags 88.1 87.6 87.9 96.7 DD Combination 88.7 88.0 88.3 97.1 Table 3: Performance results for Section 23 of the WSJ. Model 1 (Fixed Tags): a baseline parser initialized to the best tag sequence of from the tagger of Toutanova and Manning (2000). DD Combination: a model that maximizes the joint score of parse and tag selection. 7.2 Integrated Phrase-Structure Parsing and Trigram POS tagging In a second experiment, we used dual decomposition to integrate the Model 1 parser with the Stanford max-ent trigram POS tagger (Toutanova and Manning, 2000), using a very similar algorithm to that described in section 4.1. We use the same training/dev/test split as in section 7.1. The two models were again trained separately. We ran the algorithm with a limit of K = 50 iterations. Out of 2416 test examples, the algorithm found an exact solution i</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>K. Toutanova and C.D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy partof-speech tagger. In Proc. EMNLP, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wainwright</author>
<author>M I Jordan</author>
</authors>
<title>Graphical Models, Exponential Families, and Variational Inference.</title>
<date>2008</date>
<publisher>Now Publishers Inc.,</publisher>
<location>Hanover, MA, USA.</location>
<contexts>
<context position="22822" citStr="Wainwright and Jordan, 2008" startWordPosition="4224" endWordPosition="4227">istency between the µ(i, Y ) variables and rule variables higher in the tree. Note that the constraints in Eqs.(5–7) can be written in the form Aµ = b, µ ≥ 0, as in Eq. 4. Under these definitions, we have the following: Theorem 5.1 Define Y to be the set of all CFG parses, as defined in section 4. Then conv(Y) = {µ ∈ R&apos; : µ satisifies Eqs.(5–7)} Proof: This theorem is a special case of Martin et al. (1990), theorem 2. The marginal polytope for tagging, conv(Z), can also be expressed using linear constraints as in Eq. 4; see figure 3. These constraints follow from results for graphical models (Wainwright and Jordan, 2008), or from the Martin et al. (1990) construction. As a final point, the following theorem gives an important property of marginal polytopes, which we will use at several points in this paper: Theorem 5.2 (Korte and Vygen (2008), page 66.) For any set Y ⊆ {0, 1}k, and for any vector 0 ∈ Rk, max y · 0 = max µ · 0 (8) yEY µEconv(Y) The theorem states that for a linear objective function, maximization over a discrete set Y can be replaced by maximization over the convex hull Figure 3: The linear constraints defining the marginal polytope for trigram POS tagging. conv(Y). The problem maxµEconv(Y) µ·</context>
</contexts>
<marker>Wainwright, Jordan, 2008</marker>
<rawString>M. Wainwright and M. I. Jordan. 2008. Graphical Models, Exponential Families, and Variational Inference. Now Publishers Inc., Hanover, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wainwright</author>
<author>T Jaakkola</author>
<author>A Willsky</author>
</authors>
<title>MAP estimation via agreement on trees: messagepassing and linear programming.</title>
<date>2005</date>
<booktitle>In IEEE Transactions on Information Theory,</booktitle>
<volume>51</volume>
<pages>3697--3717</pages>
<contexts>
<context position="6377" citStr="Wainwright et al., 2005" startWordPosition="980" endWordPosition="983">e experiments, and also in the formal results concerning marginal polytopes—it is straightforward to use other combinatorial algorithms within the approach. For example, Koo et al. (2010) describe a dual decomposition approach for non-projective dependency parsing, which makes use of both dynamic programming and spanning tree inference algorithms. 2 Related Work Dual decomposition is a classical method for solving optimization problems that can be decomposed into efficiently solvable sub-problems. Our work is inspired by dual decomposition methods for inference in Markov random fields (MRFs) (Wainwright et al., 2005a; Komodakis et al., 2007; Globerson and Jaakkola, 2007). In this approach, the MRF is decomposed into sub-problems corresponding to treestructured subgraphs that together cover all edges of the original graph. The resulting inference algorithms provably solve an LP relaxation of the MRF inference problem, often significantly faster than commercial LP solvers (Yanover et al., 2006). Our work is also related to methods that incorporate combinatorial solvers within loopy belief propagation (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). O</context>
<context position="38568" citStr="Wainwright et al. (2005" startWordPosition="7114" endWordPosition="7117">trated their effectiveness on problems that would traditionally be solved using intersections of dynamic programs (Bar-Hillel et al., 1964). Given the widespread use of dynamic programming in NLP, there should be many applications for the approach. There are several possible extensions of the method we have described. We have focused on cases where two models are being combined; the extension to more than two models is straightforward (e.g., see Komodakis et al. (2007)). This paper has considered approaches for MAP inference; for closely related methods that compute approximate marginals, see Wainwright et al. (2005b). A Fractional Solutions We now give an example of a point (µ, v) E 20\conv(2) that demonstrates that the relaxation 20 is strictly larger than conv(2). Fractional points such as this one can arise as solutions of the LP relaxation for worst case instances, preventing us from finding an exact solution. Recall that the constraints for 20 specify that µ E conv(Y), v E conv(i), and µ(i, t) = v(i, t) for all (i, t) E Zuni. Since µ E conv(Y), µ must be a convex combination of 1 or more members of Y; a similar property holds for v. The example is as follows. There are two possible parts of speech,</context>
</contexts>
<marker>Wainwright, Jaakkola, Willsky, 2005</marker>
<rawString>M. Wainwright, T. Jaakkola, and A. Willsky. 2005a. MAP estimation via agreement on trees: messagepassing and linear programming. In IEEE Transactions on Information Theory, volume 51, pages 3697– 3717.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wainwright</author>
<author>T Jaakkola</author>
<author>A Willsky</author>
</authors>
<title>A new class of upper bounds on the log partition function.</title>
<date>2005</date>
<booktitle>In IEEE Transactions on Information Theory,</booktitle>
<volume>51</volume>
<pages>2313--2335</pages>
<contexts>
<context position="6377" citStr="Wainwright et al., 2005" startWordPosition="980" endWordPosition="983">e experiments, and also in the formal results concerning marginal polytopes—it is straightforward to use other combinatorial algorithms within the approach. For example, Koo et al. (2010) describe a dual decomposition approach for non-projective dependency parsing, which makes use of both dynamic programming and spanning tree inference algorithms. 2 Related Work Dual decomposition is a classical method for solving optimization problems that can be decomposed into efficiently solvable sub-problems. Our work is inspired by dual decomposition methods for inference in Markov random fields (MRFs) (Wainwright et al., 2005a; Komodakis et al., 2007; Globerson and Jaakkola, 2007). In this approach, the MRF is decomposed into sub-problems corresponding to treestructured subgraphs that together cover all edges of the original graph. The resulting inference algorithms provably solve an LP relaxation of the MRF inference problem, often significantly faster than commercial LP solvers (Yanover et al., 2006). Our work is also related to methods that incorporate combinatorial solvers within loopy belief propagation (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). O</context>
<context position="38568" citStr="Wainwright et al. (2005" startWordPosition="7114" endWordPosition="7117">trated their effectiveness on problems that would traditionally be solved using intersections of dynamic programs (Bar-Hillel et al., 1964). Given the widespread use of dynamic programming in NLP, there should be many applications for the approach. There are several possible extensions of the method we have described. We have focused on cases where two models are being combined; the extension to more than two models is straightforward (e.g., see Komodakis et al. (2007)). This paper has considered approaches for MAP inference; for closely related methods that compute approximate marginals, see Wainwright et al. (2005b). A Fractional Solutions We now give an example of a point (µ, v) E 20\conv(2) that demonstrates that the relaxation 20 is strictly larger than conv(2). Fractional points such as this one can arise as solutions of the LP relaxation for worst case instances, preventing us from finding an exact solution. Recall that the constraints for 20 specify that µ E conv(Y), v E conv(i), and µ(i, t) = v(i, t) for all (i, t) E Zuni. Since µ E conv(Y), µ must be a convex combination of 1 or more members of Y; a similar property holds for v. The example is as follows. There are two possible parts of speech,</context>
</contexts>
<marker>Wainwright, Jaakkola, Willsky, 2005</marker>
<rawString>M. Wainwright, T. Jaakkola, and A. Willsky. 2005b. A new class of upper bounds on the log partition function. In IEEE Transactions on Information Theory, volume 51, pages 2313–2335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yanover</author>
<author>T Meltzer</author>
<author>Y Weiss</author>
</authors>
<title>Linear Programming Relaxations and Belief Propagation–An Empirical Study.</title>
<date>2006</date>
<journal>In The Journal of Machine Learning Research,</journal>
<volume>7</volume>
<pages>page</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6761" citStr="Yanover et al., 2006" startWordPosition="1038" endWordPosition="1041"> a classical method for solving optimization problems that can be decomposed into efficiently solvable sub-problems. Our work is inspired by dual decomposition methods for inference in Markov random fields (MRFs) (Wainwright et al., 2005a; Komodakis et al., 2007; Globerson and Jaakkola, 2007). In this approach, the MRF is decomposed into sub-problems corresponding to treestructured subgraphs that together cover all edges of the original graph. The resulting inference algorithms provably solve an LP relaxation of the MRF inference problem, often significantly faster than commercial LP solvers (Yanover et al., 2006). Our work is also related to methods that incorporate combinatorial solvers within loopy belief propagation (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008). Our approach similarly makes use of combinatorial algorithms to efficiently solve subproblems of the global inference problem. However, unlike LBP, our algorithms have strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality. These guarantees are possible because our algorithms directly solve an LP relaxation. Other work has co</context>
</contexts>
<marker>Yanover, Meltzer, Weiss, 2006</marker>
<rawString>C. Yanover, T. Meltzer, and Y. Weiss. 2006. Linear Programming Relaxations and Belief Propagation–An Empirical Study. In The Journal of Machine Learning Research, volume 7, page 1907. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>