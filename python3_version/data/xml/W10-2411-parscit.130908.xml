<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000877">
<title confidence="0.867283">
English to Indian Languages Machine Transliteration System at
NEWS 2010
</title>
<author confidence="0.945438">
Amitava Das1, Tanik Saikh2, Tapabrata Mondal3, Asif Ekbal4, Sivaji Bandyopadhyay5
</author>
<affiliation confidence="0.7834455">
Department of Computer Science and Engineering1,2,3,5
Jadavpur University,
</affiliation>
<address confidence="0.448088">
Kolkata-700032, India
</address>
<email confidence="0.87017">
amitava.santu@gmail.com1, tanik4u@gmail.com2, tapabratamon-
dal@gmail.com3, sivaji_cse_ju@yahoo.com5
</email>
<affiliation confidence="0.893605">
Department of Computational Linguistics4
University of Heidelberg
</affiliation>
<address confidence="0.9222255">
Im Neuenheimer Feld 325
69120 Heidelberg, Germany
</address>
<email confidence="0.997792">
ekbal@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.995245" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99181884">
This paper reports about our work in the
NEWS 2010 Shared Task on Transliteration
Generation held as part of ACL 2010. One
standard run and two non-standard runs were
submitted for English to Hindi and Bengali
transliteration while one standard and one non-
standard run were submitted for Kannada and
Tamil. The transliteration systems are based
on Orthographic rules and Phoneme based
technology. The system has been trained on
the NEWS 2010 Shared Task on Translitera-
tion Generation datasets. For the standard run,
the system demonstrated mean F-Score values
of 0.818 for Bengali, 0.714 for Hindi, 0.663
for Kannada and 0.563 for Tamil. The reported
mean F-Score values of non-standard runs are
0.845 and 0.875 for Bengali non-standard run-
1 and 2, 0.752 and 0.739 for Hindi non-
standard run-1 and 2, 0.662 for Kannada non-
standard run-1 and 0.760 for Tamil non-
standard run-1. Non-Standard Run-2 for Ben-
gali has achieved the highest score among all
the submitted runs. Hindi Non-Standard Run-1
and Run-2 runs are ranked as the 5th and 6th
among all submitted Runs.
</bodyText>
<sectionHeader confidence="0.999315" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999064119047619">
Transliteration is the method of translating one
source language word into another target lan-
guage by expressing and preserving the original
pronunciation in their source language. Thus, the
central problem in transliteration is predicting the
pronunciation of the original word. Translitera-
tion between two languages that use the same set
of alphabets is trivial: the word is left as it is.
However, for languages those use different al-
phabet sets the names must be transliterated or
rendered in the target language alphabets. Trans-
literation of words is necessary in many applica-
tions, such as machine translation, corpus align-
ment, cross-language Information Retrieval, in-
formation extraction and automatic lexicon ac-
quisition. In the literature, a number of translite-
ration algorithms are available involving English
(Li et al., 2004; Vigra and Khudanpur, 2003; Go-
to et al., 2003), European languages (Marino et
al., 2005) and some of the Asian languages,
namely Chinese (Li et al., 2004; Vigra and Khu-
danpur, 2003), Japanese (Goto et al., 2003;
Knight and Graehl, 1998), Korean (Jung et al.,
2000) and Arabic (Al-Onaizan and Knight,
2002a; Al-Onaizan and Knight, 2002c). Recent-
ly, some works have been initiated involving
Indian languages (Ekbal et al., 2006; Ekbal et al.,
2007; Surana and Singh, 2008). The detailed re-
port of our participation in NEWS 2009 could be
found in (Das et al., 2009).
One standard run for Bengali (Bengali
Standard Run: BSR), Hindi (Hindi Standard
Run: HSR), Kannada (Kannada Standard Run:
KSR) and Tamil (Tamil Standard Run: TSR)
were submitted. Two non-standard runs for Eng-
lish to Hindi (Hindi Non-Standard Run 1 &amp; 2:
HNSR1 &amp; HNSR2) and Bengali (Bengali Non-
Standard Run 1 &amp; 2: BNSR1 &amp; BNSR1) transli-
teration were submitted. Only one non-standard
run were submitted for Kannada (Kannada Non-
Standard Run-1: KNSR1) and Tamil (Tamil
Non-Standard Run-1: TNSR1).
</bodyText>
<page confidence="0.989171">
71
</page>
<note confidence="0.8325385">
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 71–75,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.963536" genericHeader="introduction">
2 Machine Transliteration Systems
</sectionHeader>
<bodyText confidence="0.999927703703704">
Five different transliteration models have been
proposed in the present report that can generate
the transliteration in Indian language from an
English word. The transliteration models are
named as Trigram Model (Tri), Joint Source-
Channel Model (JSC), Modified Joint Source-
Channel Model (MJSC), Improved Modified
Joint Source-Channel Model (IMJSC) and Inter-
national Phonetic Alphabet Based Model (IPA).
Among all the models the first four are catego-
rized as orthographic model and the last one i.e.
IPA based model is categorized as phoneme
based model.
An English word is divided into Translitera-
tion Units (TUs) with patterns C*V*, where C
represents a consonant and V represents a vowel.
The targeted words in Indian languages are di-
vided into TUs with patterns C+M?, where C
represents a consonant or a vowel or a conjunct
and M represents the vowel modifier or matra.
The TUs are the basic lexical units for machine
transliteration. The system considers the English
and Indian languages contextual information in
the form of collocated TUs simultaneously to
calculate the plausibility of transliteration from
each English TU to various Indian languages
candidate TUs and chooses the one with maxi-
mum probability. The system learns the map-
pings automatically from the bilingual NEWS
2010 training set being guided by linguistic fea-
tures/knowledge. The output of the mapping
process is a decision-list classifier with collo-
cated TUs in the source language and their
equivalent TUs in collocation in the target lan-
guage along with the probability of each decision
obtained from the training set. A Direct example
base has been maintained that contains the bilin-
gual training examples that do not result in the
equal number of TUs in both the source and tar-
get sides during alignment. The Direct example
base is checked first during machine translitera-
tion of the input English word. If no match is
obtained, the system uses direct orthographic
mapping by identifying the equivalent TU in In-
dian languages for each English TU in the input
and then placing the target language TUs in or-
der. The IPA based model has been used for
English dictionary words. Words which are not
present in the dictionary are handled by other
orthographic models as Trigram, JSC, MJSC and
IMJSC.
The transliteration models are described below
in which S and T denotes the source and the tar-
get words respectively:
</bodyText>
<sectionHeader confidence="0.981402" genericHeader="method">
3 Orthographic Transliteration models
</sectionHeader>
<bodyText confidence="0.9983155">
The orthographic models work on the idea of
TUs from both source and target languages. The
orthographic models used in the present system
are described below. For transliteration, P(T),
i.e., the probability of transliteration in the target
language, is calculated from a English-Indian
languages bilingual database If, T is not found in
the dictionary, then a very small value is
assigned to P(T). These models have been
desribed in details in Ekbal et al. (2007).
</bodyText>
<subsectionHeader confidence="0.919798">
3.1 Trigram
</subsectionHeader>
<bodyText confidence="0.999945666666667">
This is basically the Trigram model where the
previous and the next source TUs are considered
as the context.
</bodyText>
<equation confidence="0.9496468">
K
P S T
(  |) = Õ P(&lt; s,t &gt;k  |sk−1,sk+1)
k=1
S →T(S)=arg max T{P(T)xP(S |T)}
</equation>
<subsectionHeader confidence="0.970203">
3.2 Joint Source-Channel Model (JSC)
</subsectionHeader>
<bodyText confidence="0.991143">
This is essentially the Joint Source-Channel
model (Hazhiou et al., 2004) where the
previous TUs with reference to the current TUs
in both the source (s) and the target sides (t) are
considered as the context.
</bodyText>
<equation confidence="0.967626">
K
P S T
(  |) = Õ1P(&lt; s,t &gt;k |&lt; s,t &gt;k−1)
S →T(S)=arg max T{P(T)xP(S |T)}
</equation>
<subsectionHeader confidence="0.548959">
3.3 Modified Joint Source-Channel Model
</subsectionHeader>
<bodyText confidence="0.974896">
(MJSC)
In this model, the previous and the next TUs in
the source and the previous target TU are
considered as the context. This is the Modified
Joint Source-Channel model.
</bodyText>
<equation confidence="0.9710806">
K
P S T
(  |) = Õ P(&lt; s,t &gt;k |&lt; s,t &gt;k−1, sk+1)
k=1
S →T(S)=arg max T{P(T)xP(S |T)}
</equation>
<sectionHeader confidence="0.6487615" genericHeader="method">
3.4 Improved Modified Joint Source-
Channel Model (IMJSC)
</sectionHeader>
<bodyText confidence="0.99947025">
In this model, the previous two and the next TUs
in the source and the previous target TU are
considered as the context. This is the Improved
Modified Joint Source-Channel model.
</bodyText>
<page confidence="0.787764">
72
</page>
<equation confidence="0.9819584">
K
P S T
(  |) = jj P(&lt; s,t &gt;k  |sk+1 &lt; s,t &gt;k−1, sk+1)
k=1
S →T(S)=arg max T{P(T)xP(S |T)}
</equation>
<sectionHeader confidence="0.987518" genericHeader="method">
4 International Phonetic Alphabet
(IPA) Model
</sectionHeader>
<bodyText confidence="0.998964">
The NEWS 2010 Shared Task on Transliteration
Generation challenge addresses general domain
transliteration problem rather than named entity
transliteration. Due to large number of dictionary
words as reported in Table 1 in NEWS 2010 data
set a phoneme based transliteration algorithm
has been devised.
</bodyText>
<table confidence="0.9995074">
Train Dev Test
Bengali 7.77% 5.14% 6.46%
Hindi 27.82% 15.80% 3.7%
Kannada 27.60% 14.63% 4.4%
Tamil 27.87% 17.31% 3.0%
</table>
<tableCaption confidence="0.999863">
Table 1: Statistics of Dictionary Words
</tableCaption>
<bodyText confidence="0.997721954545455">
The International Phonetic Alphabet (IPA) is a
system of representing phonetic notations based
primarily on the Latin alphabet and devised by
the International Phonetic Association as a
standardized representation of the sounds of
spoken language. The machine-readable
Carnegie Mellon Pronouncing Dictionary1 has
been used as an external resource to capture
source language IPA structure. The dictionary
contains over 125,000 words and their
transcriptions with mappings from words to their
pronunciations in the given phoneme set. The
current phoneme set contains 39 distinct
phonemes. As there is no such parallel IPA
dictionary available for Indian languages,
English IPA structures have been mapped to TUs
in Indian languages during training. An example
of such mapping between phonemes and TUs are
shown in Table 3, for which the vowels may
carry lexical stress as reported in Table 2. This
phone set is based on the ARPAbet2 symbol set
developed for speech recognition uses.
</bodyText>
<table confidence="0.8444845">
Representation Stress level
0 No
1 Primary
2 Secondary
</table>
<tableCaption confidence="0.984754">
Table 2: Stress Level on Vowel
</tableCaption>
<bodyText confidence="0.9982692">
A pre-processing module checks whether a
targeted source English word is a valid
dictionary word or not. The dictionary words are
then handled by phoneme based transliteration
module.
</bodyText>
<footnote confidence="0.9973275">
1 www.speech.cs.cmu.edu/cgi-bin/cmudict
2 http://en.wikipedia.org/wiki/Arpabet
</footnote>
<table confidence="0.99988075">
Phoneme Example Translation TUs
AA odd AA0-D ZT—;E
AH hut HH0-AH-T r21—�
D dee D-IY1 ;E—+5f
</table>
<tableCaption confidence="0.970955">
Table 3: Phoneme Map Patterns of English
Words and TUs
</tableCaption>
<bodyText confidence="0.999614416666667">
In the target side we use our TU segregation
logic to get phoneme wise transliteration pattern.
We present this problem as a sequence labelling
problem, because transliteration pattern changes
depending upon the contextual phonemes in
source side and TUs in the target side. We use a
standard machine learning based sequence
labeller Conditional Random Field (CRF)3 here.
IPA based model increased the performance
for Bengali, Hindi and Tamil languages as
reported in Section 6. The performance has
decreased for Kannada.
</bodyText>
<sectionHeader confidence="0.997106" genericHeader="method">
5 Ranking
</sectionHeader>
<bodyText confidence="0.9996905">
The ranking among the transliterated outputs
follow the order reported in Table 4: The ranking
decision is based on the experiments as described
in (Ekbal et al., 2006) and additionally based on
the experiments on NEWS 2010 development
dataset.
</bodyText>
<table confidence="0.9993884">
Word Type Ranking Order
1 2 3 4 5
Dictionary IPA IMJSC MJSC JSC Tri
Non- IMJSC MJSC JSC Tri -
Dictionary
</table>
<tableCaption confidence="0.999172">
Table 4: Phoneme Patterns of English Words
</tableCaption>
<bodyText confidence="0.999069785714286">
In BSR, HSR, KSR and TSR the orthographic
TU based models such as: IMJSC, MJSC, JSC
and Tri have been used only trained by NEWS
2010 dataset. In BNSR1 and HNSR1 all the or-
thographic models have been trained with addi-
tional census dataset as described in Section 6. In
case of BNSR2, HNSR2, KNSR1 and TNSR1
the output of the IPA based model has been add-
ed with highest priority. As no census data is
available for Kannada and Tamil therefore there
is only one Non-Standard Run was submitted for
these two languages only with the output of IPA
based model along with the output of Standard
Run.
</bodyText>
<sectionHeader confidence="0.992524" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.977897">
We have trained our transliteration models using
the NEWS 2010 datasets obtained from the
NEWS 2010 Machine Transliteration Shared
Task (Li et al., 2010). A brief statistics of the
</bodyText>
<footnote confidence="0.898842">
3 http://crfpp.sourceforge.net
</footnote>
<page confidence="0.998938">
73
</page>
<bodyText confidence="0.999601714285714">
datasets are presented in Table 5. During train-
ing, we have split multi-words into collections of
single word transliterations. It was observed that
the number of tokens in the source and target
sides mismatched in various multi-words and
these cases were not considered further. Follow-
ing are some examples:
</bodyText>
<table confidence="0.8636978">
Paris Charles de Gaulle 4ftTf
���� ����� �� �����
Suven Life Scie ati�3n-Y C34c&apos;Vi
M34n-Yf
Delta Air Lines QLGbLff
</table>
<subsectionHeader confidence="0.546883">
6jg65n6U66TG6 O
</subsectionHeader>
<bodyText confidence="0.998751">
In the training set, some multi-words were
partly translated and not transliterated. Such ex-
amples were dropped from the training set. In the
following example the English word “National”
is being translated in the target as “~~~~
</bodyText>
<table confidence="0.887002777777778">
~~
Australian National Univer-
sity �������� �����
����������
Set Number of examples
Bng Hnd Kn Tm
Training 11938 9975 7990 7974
Development 992 1974 1968 1987
Test 991 1000 1000 1000
</table>
<tableCaption confidence="0.996695">
Table 5: Statistics of Dataset
</tableCaption>
<bodyText confidence="0.844317666666667">
There is less number of known examples in
the NEWS 2010 test set from training set. The
exact figure is reported in the Table 6.
</bodyText>
<table confidence="0.9666002">
Matches with training
Bengali 14.73%
Hindi 0.2%
Kannada 0.0%
Tamil 0.0%
</table>
<tableCaption confidence="0.997324">
Table 6: Statistics of Dataset
</tableCaption>
<bodyText confidence="0.9945475">
If the outputs of any two transliteration models
are same for any word then only one output are
provided for that particular word. Evaluation re-
sults of the final system are shown in Table 7 for
Bengali, Table 8 for Hindi, Table 9 for Kannada
and Table 10 for Tamil.
</bodyText>
<table confidence="0.995668714285714">
Parameters Accuracy
BSR BNSR1 BNSR2
Accuracy in top-1 0.232 0.369 0.430
Mean F-score 0.818 0.845 0.875
Mean Reciprocal Rank (MRR) 0.325 0.451 0.526
Mean Average Precision 0.232 0.369 0.430
(MAP)ref
</table>
<tableCaption confidence="0.999169">
Table 7: Results on Bengali Test Set
</tableCaption>
<table confidence="0.999727285714286">
Parameters Accuracy
HSR HNSR1 HNSR2
Accuracy in top-1 0.150 0.254 0.170
Mean F-score 0.714 0.752 0.739
Mean Reciprocal Rank (MRR) 0.308 0.369 0.314
Mean Average Precision 0.150 0.254 0.170
(MAP)ref
</table>
<tableCaption confidence="0.998271">
Table 8: Results on Hindi Test Set
</tableCaption>
<table confidence="0.999646666666667">
Parameters Accuracy
KSR KNSR1
Accuracy in top-1 0.056 0.055
Mean F-score 0.663 0.662
Mean Reciprocal Rank (MRR) 0.112 0.169
Mean Average Precision (MAP)ref 0.056 0.055
</table>
<tableCaption confidence="0.998203">
Table 9: Results on Kannada Test Set
</tableCaption>
<table confidence="0.999674333333333">
Parameters Accuracy
TSR TNSR1
Accuracy in top-1 0.013 0.082
Mean F-score 0.563 0.760
Mean Reciprocal Rank (MRR) 0.121 0.142
Mean Average Precision (MAP)ref 0.013 0.082
</table>
<tableCaption confidence="0.999417">
Table 10: Results on Tamil Test Set
</tableCaption>
<bodyText confidence="0.9999363">
The additional dataset used for the non-
standard runs is mainly the census data consist-
ing of only Indian person names that have been
collected from the web4. In the BNSR1 and
HNSR1 we have used an English-Bengali/Hindi
bilingual census example dataset. English-Hindi
set consist of 961,890 examples and English-
Bengali set consist of 582984 examples. This
database contains the frequency of the corres-
ponding English-Bengali/Hindi name pair.
</bodyText>
<sectionHeader confidence="0.998489" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.996912727272727">
This paper reports about our works as part of the
NEWS 2010 Shared Task on Transliteration
Generation. We have used both the orthographic
and phoneme based transliteration modules for
the present task. As our all previous efforts was
for named entity transliteration. The
Transliteration Generation challenge addresses
general domain transliteration problem rather
than named entity transliteration. To handle
general transliteration problem we proposed a
IPA based methodology.
</bodyText>
<footnote confidence="0.930876">
4http://www.eci.gov.in/DevForum/Fullname.asp
</footnote>
<page confidence="0.998295">
74
</page>
<sectionHeader confidence="0.988091" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999597968253968">
A. Das, A. Ekbal, Tapabrata Mondal and S. Bandyo-
padhyay. English to Hindi Machine Transliteration
at NEWS 2009. In Proceedings of the NEWS 2009,
In Proceeding of ACL-IJCNLP 2009, August 7th,
2009, Singapore.
Al-Onaizan, Y. and Knight, K. 2002a. Named Entity
Translation: Extended Abstract. In Proceedings of
the Human Language Technology Conference,
122– 124.
Al-Onaizan, Y. and Knight, K. 2002b. Translating
Named Entities using Monolingual and Bilingual
Resources. In Proceedings of the 40th Annual
Meeting of the ACL, 400–408, USA.
Ekbal, A. Naskar, S. and Bandyopadhyay, S. 2007.
Named Entity Transliteration. International Journal
of Computer Processing of Oriental Languages
(IJCPOL), Volume (20:4), 289-310, World Scien-
tific Publishing Company, Singapore.
Ekbal, A., Naskar, S. and Bandyopadhyay, S. 2006. A
Modified Joint Source Channel Model for Transli-
teration. In Proceedings of the COLING-ACL
2006, 191-198, Australia.
Goto, I., Kato, N., Uratani, N. and Ehara, T. 2003.
Transliteration Considering Context Information
based on the Maximum Entropy Method. In Pro-
ceeding of the MT-Summit IX, 125–132, New Or-
leans, USA.
Jung, Sung Young , Sung Lim Hong and Eunok Paek.
2000. An English to Korean Transliteration Model
of Extended Markov Window. In Proceedings of
International Conference on Computational Lin-
guistics (COLING 2000), 383-389.
Knight, K. and Graehl, J. 1998. Machine Translitera-
tion, Computational Linguistics, Volume (24:4),
599–612.
Kumaran, A. and Tobias Kellner. 2007. A generic
framework for machine transliteration. In Proc. of
the 30th SIGIR.
Li, Haizhou, A Kumaran, Min Zhang and Vladimir
Pervouchine. 2010. Whitepaper: NEWS 2010
Shared Task on Transliteration Generation. In the
ACL 2010 Named Entities Workshop (NEWS-
2010), Uppsala, Sweden, Association for Computa-
tional Linguistics, July 2010.
Li, Haizhou, Min Zhang and Su Jian. 2004. A Joint
Source-Channel Model for Machine Translitera-
tion. In Proceedings of the 42nd Annual Meeting
of the ACL, 159-166. Spain.
Marino, J. B., R. Banchs, J. M. Crego, A. de Gispert,
P. Lambert, J. A. Fonollosa and M. Ruiz. 2005.
Bilingual n-gram Statistical Machine Translation.
In Proceedings of the MT-Summit X, 275–282.
Surana, Harshit, and Singh, Anil Kumar. 2008. A
More Discerning and Adaptable Multilingual
Transliteration Mechanism for Indian Languages.
In Proceedings of the 3rd International Joint Confe-
rence on Natural Language Processing (IJCNLP-
08), 64-71, India.
Vigra, Paola and Khudanpur, S. 2003. Transliteration
of Proper Names in Cross-Lingual Information Re-
trieval. In Proceedings of the ACL 2003 Workshop
on Multilingual and Mixed-Language Named Enti-
ty Recognition, 57–60.
</reference>
<page confidence="0.999134">
75
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.197300">
<note confidence="0.7536765">English to Indian Languages Machine Transliteration System at NEWS 2010</note>
<author confidence="0.96129">Tanik Tapabrata Asif Sivaji</author>
<affiliation confidence="0.910416">of Computer Science and Jadavpur</affiliation>
<address confidence="0.973921">Kolkata-700032, India</address>
<affiliation confidence="0.9891685">of Computational University of</affiliation>
<address confidence="0.806522">Im Neuenheimer Feld 69120 Heidelberg, Germany</address>
<email confidence="0.999646">ekbal@cl.uni-heidelberg.de</email>
<abstract confidence="0.956134423076923">This paper reports about our work in the NEWS 2010 Shared Task on Transliteration Generation held as part of ACL 2010. One standard run and two non-standard runs were submitted for English to Hindi and Bengali transliteration while one standard and one nonstandard run were submitted for Kannada and Tamil. The transliteration systems are based on Orthographic rules and Phoneme based technology. The system has been trained on the NEWS 2010 Shared Task on Transliteration Generation datasets. For the standard run, the system demonstrated mean F-Score values of 0.818 for Bengali, 0.714 for Hindi, 0.663 for Kannada and 0.563 for Tamil. The reported mean F-Score values of non-standard runs are 0.845 and 0.875 for Bengali non-standard run- 1 and 2, 0.752 and 0.739 for Hindi nonstandard run-1 and 2, 0.662 for Kannada nonstandard run-1 and 0.760 for Tamil nonstandard run-1. Non-Standard Run-2 for Bengali has achieved the highest score among all the submitted runs. Hindi Non-Standard Run-1 Run-2 runs are ranked as the and among all submitted Runs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>padhyay</author>
</authors>
<title>English to Hindi Machine Transliteration at NEWS</title>
<date>2009</date>
<booktitle>In Proceedings of the NEWS 2009, In Proceeding of ACL-IJCNLP</booktitle>
<marker>padhyay, 2009</marker>
<rawString>padhyay. English to Hindi Machine Transliteration at NEWS 2009. In Proceedings of the NEWS 2009, In Proceeding of ACL-IJCNLP 2009, August 7th, 2009, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Named Entity Translation: Extended Abstract.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<volume>122</volume>
<pages>124</pages>
<contexts>
<context position="2722" citStr="Al-Onaizan and Knight, 2002" startWordPosition="405" endWordPosition="408">e alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 &amp; 2: HNSR1 &amp; HNSR2) and Bengali (Bengali NonStandard Run 1 &amp; 2: BNSR1 &amp; BNSR1) transliterati</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Al-Onaizan, Y. and Knight, K. 2002a. Named Entity Translation: Extended Abstract. In Proceedings of the Human Language Technology Conference, 122– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Translating Named Entities using Monolingual and Bilingual Resources.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<location>400–408, USA.</location>
<contexts>
<context position="2722" citStr="Al-Onaizan and Knight, 2002" startWordPosition="405" endWordPosition="408">e alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 &amp; 2: HNSR1 &amp; HNSR2) and Bengali (Bengali NonStandard Run 1 &amp; 2: BNSR1 &amp; BNSR1) transliterati</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Al-Onaizan, Y. and Knight, K. 2002b. Translating Named Entities using Monolingual and Bilingual Resources. In Proceedings of the 40th Annual Meeting of the ACL, 400–408, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Naskar Ekbal</author>
<author>S</author>
<author>S Bandyopadhyay</author>
</authors>
<title>Named Entity Transliteration.</title>
<date>2007</date>
<journal>International Journal of Computer Processing of Oriental Languages (IJCPOL), Volume</journal>
<volume>20</volume>
<pages>289--310</pages>
<institution>World Scientific Publishing Company, Singapore.</institution>
<contexts>
<context position="2863" citStr="Ekbal et al., 2007" startWordPosition="427" endWordPosition="430">rieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 &amp; 2: HNSR1 &amp; HNSR2) and Bengali (Bengali NonStandard Run 1 &amp; 2: BNSR1 &amp; BNSR1) transliteration were submitted. Only one non-standard run were submitted for Kannada (Kannada NonStandard Run-1: KNSR1) and Tamil (Tamil Non-Standard Run-</context>
<context position="6558" citStr="Ekbal et al. (2007)" startWordPosition="1023" endWordPosition="1026">he transliteration models are described below in which S and T denotes the source and the target words respectively: 3 Orthographic Transliteration models The orthographic models work on the idea of TUs from both source and target languages. The orthographic models used in the present system are described below. For transliteration, P(T), i.e., the probability of transliteration in the target language, is calculated from a English-Indian languages bilingual database If, T is not found in the dictionary, then a very small value is assigned to P(T). These models have been desribed in details in Ekbal et al. (2007). 3.1 Trigram This is basically the Trigram model where the previous and the next source TUs are considered as the context. K P S T ( |) = Õ P(&lt; s,t &gt;k |sk−1,sk+1) k=1 S →T(S)=arg max T{P(T)xP(S |T)} 3.2 Joint Source-Channel Model (JSC) This is essentially the Joint Source-Channel model (Hazhiou et al., 2004) where the previous TUs with reference to the current TUs in both the source (s) and the target sides (t) are considered as the context. K P S T ( |) = Õ1P(&lt; s,t &gt;k |&lt; s,t &gt;k−1) S →T(S)=arg max T{P(T)xP(S |T)} 3.3 Modified Joint Source-Channel Model (MJSC) In this model, the previous and t</context>
</contexts>
<marker>Ekbal, S, Bandyopadhyay, 2007</marker>
<rawString>Ekbal, A. Naskar, S. and Bandyopadhyay, S. 2007. Named Entity Transliteration. International Journal of Computer Processing of Oriental Languages (IJCPOL), Volume (20:4), 289-310, World Scientific Publishing Company, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ekbal</author>
<author>S Naskar</author>
<author>S Bandyopadhyay</author>
</authors>
<title>A Modified Joint Source Channel Model for Transliteration.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING-ACL</booktitle>
<pages>191--198</pages>
<contexts>
<context position="2843" citStr="Ekbal et al., 2006" startWordPosition="423" endWordPosition="426">uage Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 &amp; 2: HNSR1 &amp; HNSR2) and Bengali (Bengali NonStandard Run 1 &amp; 2: BNSR1 &amp; BNSR1) transliteration were submitted. Only one non-standard run were submitted for Kannada (Kannada NonStandard Run-1: KNSR1) and Tamil (Tam</context>
<context position="10384" citStr="Ekbal et al., 2006" startWordPosition="1647" endWordPosition="1650">ation pattern. We present this problem as a sequence labelling problem, because transliteration pattern changes depending upon the contextual phonemes in source side and TUs in the target side. We use a standard machine learning based sequence labeller Conditional Random Field (CRF)3 here. IPA based model increased the performance for Bengali, Hindi and Tamil languages as reported in Section 6. The performance has decreased for Kannada. 5 Ranking The ranking among the transliterated outputs follow the order reported in Table 4: The ranking decision is based on the experiments as described in (Ekbal et al., 2006) and additionally based on the experiments on NEWS 2010 development dataset. Word Type Ranking Order 1 2 3 4 5 Dictionary IPA IMJSC MJSC JSC Tri Non- IMJSC MJSC JSC Tri - Dictionary Table 4: Phoneme Patterns of English Words In BSR, HSR, KSR and TSR the orthographic TU based models such as: IMJSC, MJSC, JSC and Tri have been used only trained by NEWS 2010 dataset. In BNSR1 and HNSR1 all the orthographic models have been trained with additional census dataset as described in Section 6. In case of BNSR2, HNSR2, KNSR1 and TNSR1 the output of the IPA based model has been added with highest priorit</context>
</contexts>
<marker>Ekbal, Naskar, Bandyopadhyay, 2006</marker>
<rawString>Ekbal, A., Naskar, S. and Bandyopadhyay, S. 2006. A Modified Joint Source Channel Model for Transliteration. In Proceedings of the COLING-ACL 2006, 191-198, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Goto</author>
<author>N Kato</author>
<author>N Uratani</author>
<author>T Ehara</author>
</authors>
<title>Transliteration Considering Context Information based on the Maximum Entropy Method.</title>
<date>2003</date>
<booktitle>In Proceeding of the MT-Summit IX,</booktitle>
<pages>125--132</pages>
<location>New Orleans, USA.</location>
<contexts>
<context position="2464" citStr="Goto et al., 2003" startWordPosition="361" endWordPosition="365">riginal word. Transliteration between two languages that use the same set of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Ru</context>
</contexts>
<marker>Goto, Kato, Uratani, Ehara, 2003</marker>
<rawString>Goto, I., Kato, N., Uratani, N. and Ehara, T. 2003. Transliteration Considering Context Information based on the Maximum Entropy Method. In Proceeding of the MT-Summit IX, 125–132, New Orleans, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sung Lim Hong</author>
<author>Eunok Paek</author>
</authors>
<title>An English to Korean Transliteration Model of Extended Markov Window.</title>
<date>2000</date>
<booktitle>In Proceedings of International Conference on Computational Linguistics (COLING</booktitle>
<pages>383--389</pages>
<marker>Hong, Paek, 2000</marker>
<rawString>Jung, Sung Young , Sung Lim Hong and Eunok Paek. 2000. An English to Korean Transliteration Model of Extended Markov Window. In Proceedings of International Conference on Computational Linguistics (COLING 2000), 383-389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<date>1998</date>
<journal>Machine Transliteration, Computational Linguistics, Volume</journal>
<volume>24</volume>
<pages>599--612</pages>
<contexts>
<context position="2654" citStr="Knight and Graehl, 1998" startWordPosition="394" endWordPosition="397">he names must be transliterated or rendered in the target language alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 &amp; 2: HNSR1 &amp; HNSR2) and </context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Knight, K. and Graehl, J. 1998. Machine Transliteration, Computational Linguistics, Volume (24:4), 599–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>Tobias Kellner</author>
</authors>
<title>A generic framework for machine transliteration.</title>
<date>2007</date>
<booktitle>In Proc. of the 30th SIGIR.</booktitle>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>Kumaran, A. and Tobias Kellner. 2007. A generic framework for machine transliteration. In Proc. of the 30th SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Min Zhang</author>
<author>Vladimir Pervouchine</author>
</authors>
<date>2010</date>
<booktitle>Whitepaper: NEWS 2010 Shared Task on Transliteration Generation. In the ACL 2010 Named Entities Workshop (NEWS2010), Uppsala, Sweden, Association for Computational Linguistics,</booktitle>
<contexts>
<context position="11378" citStr="Li et al., 2010" startWordPosition="1823" endWordPosition="1826">d HNSR1 all the orthographic models have been trained with additional census dataset as described in Section 6. In case of BNSR2, HNSR2, KNSR1 and TNSR1 the output of the IPA based model has been added with highest priority. As no census data is available for Kannada and Tamil therefore there is only one Non-Standard Run was submitted for these two languages only with the output of IPA based model along with the output of Standard Run. 6 Experimental Results We have trained our transliteration models using the NEWS 2010 datasets obtained from the NEWS 2010 Machine Transliteration Shared Task (Li et al., 2010). A brief statistics of the 3 http://crfpp.sourceforge.net 73 datasets are presented in Table 5. During training, we have split multi-words into collections of single word transliterations. It was observed that the number of tokens in the source and target sides mismatched in various multi-words and these cases were not considered further. Following are some examples: Paris Charles de Gaulle 4ftTf ���� ����� �� ����� Suven Life Scie ati�3n-Y C34c&apos;Vi M34n-Yf Delta Air Lines QLGbLff 6jg65n6U66TG6 O In the training set, some multi-words were partly translated and not transliterated. Such examples</context>
</contexts>
<marker>Li, Kumaran, Zhang, Pervouchine, 2010</marker>
<rawString>Li, Haizhou, A Kumaran, Min Zhang and Vladimir Pervouchine. 2010. Whitepaper: NEWS 2010 Shared Task on Transliteration Generation. In the ACL 2010 Named Entities Workshop (NEWS2010), Uppsala, Sweden, Association for Computational Linguistics, July 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Su Jian</author>
</authors>
<title>A Joint Source-Channel Model for Machine Transliteration.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the ACL,</booktitle>
<pages>159--166</pages>
<contexts>
<context position="2417" citStr="Li et al., 2004" startWordPosition="353" endWordPosition="356">ion is predicting the pronunciation of the original word. Transliteration between two languages that use the same set of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Benga</context>
</contexts>
<marker>Li, Zhang, Jian, 2004</marker>
<rawString>Li, Haizhou, Min Zhang and Su Jian. 2004. A Joint Source-Channel Model for Machine Transliteration. In Proceedings of the 42nd Annual Meeting of the ACL, 159-166. Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Marino</author>
<author>R Banchs</author>
<author>J M Crego</author>
<author>A de Gispert</author>
<author>P Lambert</author>
<author>J A Fonollosa</author>
<author>M Ruiz</author>
</authors>
<title>Bilingual n-gram Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the MT-Summit X,</booktitle>
<pages>275--282</pages>
<marker>Marino, Banchs, Crego, de Gispert, Lambert, Fonollosa, Ruiz, 2005</marker>
<rawString>Marino, J. B., R. Banchs, J. M. Crego, A. de Gispert, P. Lambert, J. A. Fonollosa and M. Ruiz. 2005. Bilingual n-gram Statistical Machine Translation. In Proceedings of the MT-Summit X, 275–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harshit Surana</author>
<author>Anil Kumar Singh</author>
</authors>
<title>A More Discerning and Adaptable Multilingual Transliteration Mechanism for Indian Languages.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing (IJCNLP08),</booktitle>
<pages>64--71</pages>
<contexts>
<context position="2888" citStr="Surana and Singh, 2008" startWordPosition="431" endWordPosition="434">extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 &amp; 2: HNSR1 &amp; HNSR2) and Bengali (Bengali NonStandard Run 1 &amp; 2: BNSR1 &amp; BNSR1) transliteration were submitted. Only one non-standard run were submitted for Kannada (Kannada NonStandard Run-1: KNSR1) and Tamil (Tamil Non-Standard Run-1: TNSR1). 71 Proceedings</context>
</contexts>
<marker>Surana, Singh, 2008</marker>
<rawString>Surana, Harshit, and Singh, Anil Kumar. 2008. A More Discerning and Adaptable Multilingual Transliteration Mechanism for Indian Languages. In Proceedings of the 3rd International Joint Conference on Natural Language Processing (IJCNLP08), 64-71, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Vigra</author>
<author>S Khudanpur</author>
</authors>
<title>Transliteration of Proper Names in Cross-Lingual Information Retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 Workshop on Multilingual and Mixed-Language Named Entity Recognition,</booktitle>
<pages>57--60</pages>
<contexts>
<context position="2444" citStr="Vigra and Khudanpur, 2003" startWordPosition="357" endWordPosition="360"> the pronunciation of the original word. Transliteration between two languages that use the same set of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hind</context>
</contexts>
<marker>Vigra, Khudanpur, 2003</marker>
<rawString>Vigra, Paola and Khudanpur, S. 2003. Transliteration of Proper Names in Cross-Lingual Information Retrieval. In Proceedings of the ACL 2003 Workshop on Multilingual and Mixed-Language Named Entity Recognition, 57–60.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>