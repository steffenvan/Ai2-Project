<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.052854">
<title confidence="0.9993635">
Exploiting Event Semantics to Parse the Rhetorical Structure of
Natural Language Text
</title>
<author confidence="0.997588">
Rajen Subba
</author>
<affiliation confidence="0.9989435">
Department of Computer Science
University of Illinois at Chicago
</affiliation>
<address confidence="0.794293">
Chicago, IL 60613
</address>
<email confidence="0.997621">
rsubba@cs.uic.edu
</email>
<sectionHeader confidence="0.995615" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999877692307692">
Previous work on discourse parsing has
mostly relied on surface syntactic and lex-
ical features; the use of semantics is lim-
ited to shallow semantics. The goal of this
thesis is to exploit event semantics in order
to build discourse parse trees (DPT) based
on informational rhetorical relations. Our
work employs an Inductive Logic Pro-
gramming (ILP) based rhetorical relation
classifier, a Neural Network based dis-
course segmenter, a bottom-up sentence
level discourse parser and a shift-reduce
document level discourse parser.
</bodyText>
<sectionHeader confidence="0.998966" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.756782666666667">
Discourse is a structurally organized set of coher-
ent text segments. The minimal unit of discourse is
called an elementary discourse unit (EDU). An EDU
or a span of EDUs constitute a segment. When we
read text, we automatically assign rhetorical (coher-
ence) relations to segments of text that we deem to
be related. Consider the segmented text below:
(Example 1) [Clean the walls thoroughly(1a)] [and allow them
to dry.(1b)] [If the walls are a dark color,(2a)] [apply
primer.(2b)] [Put a small amount of paste in the paint
tray;(3a)] [add enough water(4a)] [to thin the paste to
about the consistency of cream soup.(4b)]
It is plausible to state that the rhetorical relation
between (1a) and (1b) is preparation:act. We can
also posit that the relation act:goal holds between
</bodyText>
<page confidence="0.988797">
21
</page>
<bodyText confidence="0.999851545454546">
(4a) and (4b). Figure 1 shows the complete annota-
tion of the full text. Now, if we were to reorder these
segments as [(1b), (4a), (2a), (4b), (3a), (2b), (1a)],
the text would not make much sense. Therefore, it
is imperative that the contiguous spans of discourse
be coherent for comprehension. Rhetorical relations
help make the text coherent.
Rhetorical relations based on the subject matter
of the segments are called informational relations.
A common understanding in discourse study is that
informational relations are based on the underlying
content of the text segments. However, previous
work (Marcu, 2000; Polanyi et al., 2004; Soricut
and Marcu, 2005; Sporleder and Lascarides, 2005)
in discourse parsing has relied on syntactic and lex-
ical information, and shallow semantics only.
The goal of this thesis is to build a computa-
tional model for parsing the informational structure
of instructional text that exploits “deeper seman-
tics”, namely event semantics. Such discourse struc-
tures can be useful for applications such as informa-
tion extraction, question answering and intelligent
tutoring systems. Our approach makes use of a neu-
ral network discourse segmenter, a rhetorical rela-
tion classifier based on ILP and a discourse pars-
ing model that builds sentence level DPTs bottom-
up and document level DPTs using a shift-reduce
parser.
In section 2, we describe how we collected our
data. In section 3, we present our automatic dis-
course segmenter. Section 4 details our discourse
parsing model based on event semantics followed by
the conclusion in section 5.
</bodyText>
<note confidence="0.3183695">
Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 21–24,
Rochester, April 2007. c�2007 Association for Computational Linguistics
</note>
<figureCaption confidence="0.998149">
Figure 1: Discourse Annotation for Example 1
</figureCaption>
<sectionHeader confidence="0.97082" genericHeader="method">
2 Data Collection
</sectionHeader>
<bodyText confidence="0.999958">
Our work calls for the use of a supervised machine
learning approach. Therefore, we have manually an-
notated a corpus of instructional text with rhetorical
relations and event semantic information. We used
an existing corpus on home repair manuals (5Mb).1
</bodyText>
<subsectionHeader confidence="0.994088">
2.1 Manual Discourse Annotation
</subsectionHeader>
<bodyText confidence="0.999995615384616">
In order to carry out the manual discourse anno-
tation, a coding scheme was developed based on
Marcu (1999) and RDA (Moser et al., 1996). The
annotated data consists of 5744 EDUs and 5131 re-
lations with a kappa value of 0.66 on about 26% of
the corpus. We analyzed a total of 1217 examples
to determine whether a cue phrase was present or
not. Only 523 examples (43%) were judged to be
signalled. Furthermore, discourse cues can be am-
biguous with regard to which relation they signal.
In order to account for cases where discourse cues
are not present and to resolve such ambiguities, we
intend to exploit event semantics.
</bodyText>
<subsectionHeader confidence="0.982979">
2.2 Semi-Automatic Event Semantic
Annotation
</subsectionHeader>
<bodyText confidence="0.9999735">
Informational relations describe how the content of
two text segments are related. Therefore, it makes
intuitive sense that verb semantics can be useful in
determining these relations.2 In Subba et al. (2006),
</bodyText>
<footnote confidence="0.9966466">
1The corpus was collected opportunistically off the internet
and from other sources, and originally assembled at the Infor-
mation Technology Research Institute, University of Brighton.
2Especially in instructional manuals where the meaning of
most sentences is centered on verbs.
</footnote>
<bodyText confidence="0.999565894736842">
we integrated LCFLEX (Rose and Lavie, 2000) with
VerbNet (Kipper et al., 2000) and CoreLex (Buite-
laar, 1998) to compositionally build verb based
event semantic representations of our EDUs.
VerbNet groups together verbs that undergo the
same syntactic alternations and share similar seman-
tics. It accounts for about 4962 distinct verbs clas-
sified into 237 main classes. The semantic infor-
mation is described in terms of an event that is de-
composed into four stages, namely start, during, end
and result. Semantic predicates like motion and to-
gether describe the participants of an event at var-
ious stages. CoreLex provides meaning represen-
tations for about 40,000 nouns that are compatible
with VerbNet.
The parser was used to semi-automatically anno-
tate both our training and test data. Since the output
of the parser can be ambiguous with respect to the
verb sense, we manually pick the correct sense.3
</bodyText>
<sectionHeader confidence="0.993448" genericHeader="method">
3 Automatic Discourse Segmentation
</sectionHeader>
<bodyText confidence="0.999962">
The task of the discourse segmenter is to segment
sentences into EDUs. In the past, the problem
of sentence level discourse segmentation has been
tackled using both symbolic methods (Polanyi et al.,
2004; Huong et al., 2004) as well as statistical mod-
els (Soricut and Marcu, 2003; Marcu, 2000) that
have exploited syntactic and lexical features.
We have implemented a Neural Network model
</bodyText>
<footnote confidence="0.94899">
3In addition, the parser generates semantic representations
for fragments of the sentence to handle ungrammatical sen-
tences, etc.
</footnote>
<page confidence="0.99737">
22
</page>
<bodyText confidence="0.999991">
for sentence level discourse segmentation that uses
syntactic features and discourse cues. Our model
was trained and tested on RST-DT (2002) and
achieves a performance of up to 86.12% F-Score,
which is comparable to Soricut and Marcu (2003).
We plan to use this model on our corpus as well.
</bodyText>
<sectionHeader confidence="0.996683" genericHeader="method">
4 Discourse Parsing
</sectionHeader>
<bodyText confidence="0.999951">
Once the EDUs have been identified by the dis-
course segmenter, the entire discourse structure of
text needs to be constructed. This concerns deter-
mining which text segments are related and what re-
lation to assign to those segments. Our discourse
parsing model consists of a rhetorical relation clas-
sifier, a sentence level discourse parser and a docu-
ment level discourse parser.
</bodyText>
<subsectionHeader confidence="0.971683">
4.1 Rhetorical Relation Classifier
</subsectionHeader>
<bodyText confidence="0.999753615384616">
In a preliminary investigation (Subba et al., 2006),
we modeled the problem of identifying rhetorical re-
lations as a classification problem using rich verb se-
mantics only.
Most of the work in NLP that involves learn-
ing has used more traditional machine learning
paradigms like decision-tree algorithms and SVMs.
However, we did not find them suitable for our data
which is represented in first order logic (FOL). We
found Progol (Muggleton, 1995), an ILP system, ap-
propriate for our needs. The general problem spec-
ification for Progol (ILP) is given by the following
posterior sufficiency property:
</bodyText>
<equation confidence="0.966733">
B ∧ H �= E
</equation>
<bodyText confidence="0.999952818181818">
Given the background knowledge B and the ex-
amples E, Progol finds the simplest consistent hy-
pothesis H, such that B and H entails E. The rich
verb semantic representation of pairs of EDUs form
the background knowledge and the manually anno-
tated rhetorical relations between the pairs of EDUs
serve as the positive examples.4 An A*-like search
is used to search for the most probable hypothesis.
Given our model, we are able to learn rules such as
the ones given in Figure 2. Due to the lack of space
we only explain RULE1 here. RULE1 states that
</bodyText>
<footnote confidence="0.961239666666667">
4The output from the parser was further processed into def-
inite clauses. Positive examples are represented as ground unit
clauses.
</footnote>
<equation confidence="0.979474666666667">
RULE1:
relation(EDU1,EDU2,’before:after’) :- motion(EDU1,event0,during,C),
location(EDU2,event0,start,C,D).
RULE2:
relation(EDU1,EDU2,’act:goal’) :- cause(EDU1,C,event0),
together(EDU1,event0,end,physical,F,G),cause(EDU2,C,event0).
</equation>
<figureCaption confidence="0.998879">
Figure 2: Examples of Rules learned by Progol
</figureCaption>
<bodyText confidence="0.9996655">
there is a theme (C) in motion during the event in
EDU1 (the first EDU) and that C is located in loca-
tion D at the start of the event in EDU2 (the second
EDU).
We trained our classifier on 423 examples and
tested it on 85 examples.5 A majority function base-
line performs at a 51.7 F-Score. Our model outper-
forms this baseline with an F-Score of 60.24.
</bodyText>
<table confidence="0.999844833333333">
Relation Precision Recall F-Score
goal:act 31.57 26.08 28.57
step1:step2 75 75 75
before:after 54.5 54.5 54.5
criterion:act 71.4 71.4 71.4
Total 61.7 58.8 60.24
</table>
<tableCaption confidence="0.999703">
Table 1: Rhetorical Relation Classifier Result
</tableCaption>
<bodyText confidence="0.999886333333333">
This study has shown that it is possible to learn
rules from FOL semantic representations using In-
ductive Logic Programming to classify rhetorical re-
lations. However, it is not yet clear how useful event
semantics is for discourse parsing. In the future, we
intend to extend our model to incorporate syntactic
and lexical information as well. Such an extension
will allow us to assess the contribution of event se-
mantics.
</bodyText>
<subsectionHeader confidence="0.999572">
4.2 Building Discourse Parse Trees
</subsectionHeader>
<bodyText confidence="0.999760833333333">
In addition to extending the rhetorical relation clas-
sifier, our future work will involve building the dis-
course parse tree at the sentence level and at the doc-
ument level. At the document level, the input will
be the sentence level discourse parse trees and the
output will be the discourse structure of the entire
</bodyText>
<footnote confidence="0.9872365">
5For this preliminary experiment, we decided to use only
those relation sets that had more than 50 examples and those
that were classified as goal:act, step1:step2, criterion:act or be-
fore:after
</footnote>
<page confidence="0.99861">
23
</page>
<bodyText confidence="0.998266791666666">
document.
When combining two text segments, promotion
sets that approximate the most important EDUs of
the text segments will be used. As a starting point,
we propose to build sentence level DPTs bottom-up.
EDUs that are subsumed by the same syntactic con-
stituent (usually an S, S-Bar, VP) will be combined
together into a larger text segment recursively until
the the DPT at the root level has been constructed.
At the document level, the DPT will be built us-
ing a shift-reduce parser as in Marcu (2000). How-
ever, unlike Marcu (2000), there will only be one
shift and one reduce operation. The reduce oper-
ation will be determined by the rhetorical relation
classifier and an additional module that will deter-
mine all the possible attachment points for an in-
coming sentence level DPT. An incoming sentence
level DPT may be attached to any node on the right
frontier of the left DPT. Lexical cohesion will be
used to rank the possible attachment points. For both
sentence level discourse parsing and document level
discourse parsing, the rhetorical relation classifier
will be used to determine the informational relation
between the text segments.
</bodyText>
<sectionHeader confidence="0.998833" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999990214285714">
In conclusion, this thesis will provide a computa-
tional model for parsing the discourse structure of
text based on informational relations. Our approach
exploits event semantic information of the EDUs.
Hence, it will provide a measurement of how helpful
event semantics can be in uncovering the discourse
structure of text. As a consequence, it will also shed
some light on the coverage of the lexical resources
we are using. Other contributions of our work in-
clude a parser that builds event semantic represen-
tations of sentences based on rich verb semantics
and noun semantics and a data driven automatic dis-
course segmenter that determines the minimal units
of discourse.
</bodyText>
<sectionHeader confidence="0.999005" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998861327272728">
Buitelaar, P.: CoreLex: Systematic Polysemy and Under-
specification. Ph.D. thesis, Computer Science, Bran-
deis University, February 1998.
Huong Le Thanh, G. A. and Huyck., C.: Automated dis-
course segmentation by syntactic information and cue
phrases. International Conference on Artificial Intelli-
gence and Applications, 2004.
Kipper, K., H. T. D. and Palmer., M.: Class-based con-
struction of a verb lexicon. AAAI-2000, Proceedings
of the Seventeenth National Conference on Artificial
Intelligence, 2000.
Livia Polanyi, Christopher Culy, M. H. v. d. B. G. L. T.
and Ahn., D.: Sentential structure and discourse pars-
ing. ACL 2004, Workshop on Discourse Annotation,
2004.
Marcu, D.: Instructions for Manually Annotating the
Discourse Structures of Texts. Technical Report, Uni-
versity of Southern California, 1999.
Marcu, D.: The theory and practice of discourse parsing
and summarization. Cambridge, Massachusetts, Lon-
don, England, MIT Press, 2000.
Moser, M. G., Moore, J. D., and Glendening, E.: In-
structions for Coding Explanations: Identifying Seg-
ments, Relations and Minimal Units. University of
Pittsburgh, Department of Computer Science, 1996.
Muggleton., S. H.: Inverse entailment and progol.
In New Generation Computing Journal, 13:245–286,
1995.
Ros´e, C. P. and Lavie., A.: Balancing robustness and ef-
ficiency in unification-augmented context-free parsers
for large practical applications. In Jean-Clause Junqua
and Gertjan van Noord, editors, Robustness in Lan-
guage and Speech Technology, 2000.
RST-DT.: Rst discourse treebank. Linguistic Data Con-
sortium., 2002.
Sporleder, C. and Lascarides., A.: Exploiting linguistic
cues to classify rhetorical relations. Recent Advances
in Natural Language Processing, 2005.
Soricut, R. and Marcu., D.: Sentence level discourse
parsing using syntactic and lexical information. Pro-
ceedings of the HLT and NAACL Conference, 2003.
Subba, R., Di Eugenio, B., E. T.: Building lexical
resources for princpar, a large coverage parser that
generates principled semantic representations. LREC
2006, 2006.
Subba, R., Di Eugenio, B., S. N. K.: Learning FOL
rules based on rich verb semantic representations to
automatically label rhetorical relations. EACL 2006,
Workshop on Learning Structured Information in Nat-
ural Language Applications, 2006.
Wellner, B., Pustejovsky, J., C. H. R. S. and Rumshisky.,
A.: Classification of discourse coherence rela-
tions: An exploratory study using multiple knowledge
sources. SIGDIAL Workshop on Discourse and Dia-
logue, 2006.
</reference>
<page confidence="0.999178">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.552437">
<title confidence="0.99968">Exploiting Event Semantics to Parse the Rhetorical Structure of Natural Language Text</title>
<author confidence="0.969827">Rajen</author>
<affiliation confidence="0.859420333333333">Department of Computer University of Illinois at Chicago, IL</affiliation>
<email confidence="0.99908">rsubba@cs.uic.edu</email>
<abstract confidence="0.998508571428571">Previous work on discourse parsing has mostly relied on surface syntactic and lexical features; the use of semantics is limited to shallow semantics. The goal of this thesis is to exploit event semantics in order to build discourse parse trees (DPT) based on informational rhetorical relations. Our work employs an Inductive Logic Programming (ILP) based rhetorical relation classifier, a Neural Network based discourse segmenter, a bottom-up sentence level discourse parser and a shift-reduce document level discourse parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
</authors>
<title>CoreLex: Systematic Polysemy and Underspecification.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science, Brandeis University,</institution>
<contexts>
<context position="4855" citStr="Buitelaar, 1998" startWordPosition="763" endWordPosition="765">emantic Annotation Informational relations describe how the content of two text segments are related. Therefore, it makes intuitive sense that verb semantics can be useful in determining these relations.2 In Subba et al. (2006), 1The corpus was collected opportunistically off the internet and from other sources, and originally assembled at the Information Technology Research Institute, University of Brighton. 2Especially in instructional manuals where the meaning of most sentences is centered on verbs. we integrated LCFLEX (Rose and Lavie, 2000) with VerbNet (Kipper et al., 2000) and CoreLex (Buitelaar, 1998) to compositionally build verb based event semantic representations of our EDUs. VerbNet groups together verbs that undergo the same syntactic alternations and share similar semantics. It accounts for about 4962 distinct verbs classified into 237 main classes. The semantic information is described in terms of an event that is decomposed into four stages, namely start, during, end and result. Semantic predicates like motion and together describe the participants of an event at various stages. CoreLex provides meaning representations for about 40,000 nouns that are compatible with VerbNet. The p</context>
</contexts>
<marker>Buitelaar, 1998</marker>
<rawString>Buitelaar, P.: CoreLex: Systematic Polysemy and Underspecification. Ph.D. thesis, Computer Science, Brandeis University, February 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huong Le Thanh</author>
<author>G A</author>
<author>C Huyck</author>
</authors>
<title>Automated discourse segmentation by syntactic information and cue phrases.</title>
<date>2004</date>
<booktitle>International Conference on Artificial Intelligence and Applications,</booktitle>
<marker>Le Thanh, A, Huyck, 2004</marker>
<rawString>Huong Le Thanh, G. A. and Huyck., C.: Automated discourse segmentation by syntactic information and cue phrases. International Conference on Artificial Intelligence and Applications, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>H T D</author>
<author>M Palmer</author>
</authors>
<title>Class-based construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>AAAI-2000, Proceedings of the Seventeenth National Conference on Artificial Intelligence,</booktitle>
<contexts>
<context position="4825" citStr="Kipper et al., 2000" startWordPosition="757" endWordPosition="760">antics. 2.2 Semi-Automatic Event Semantic Annotation Informational relations describe how the content of two text segments are related. Therefore, it makes intuitive sense that verb semantics can be useful in determining these relations.2 In Subba et al. (2006), 1The corpus was collected opportunistically off the internet and from other sources, and originally assembled at the Information Technology Research Institute, University of Brighton. 2Especially in instructional manuals where the meaning of most sentences is centered on verbs. we integrated LCFLEX (Rose and Lavie, 2000) with VerbNet (Kipper et al., 2000) and CoreLex (Buitelaar, 1998) to compositionally build verb based event semantic representations of our EDUs. VerbNet groups together verbs that undergo the same syntactic alternations and share similar semantics. It accounts for about 4962 distinct verbs classified into 237 main classes. The semantic information is described in terms of an event that is decomposed into four stages, namely start, during, end and result. Semantic predicates like motion and together describe the participants of an event at various stages. CoreLex provides meaning representations for about 40,000 nouns that are </context>
</contexts>
<marker>Kipper, D, Palmer, 2000</marker>
<rawString>Kipper, K., H. T. D. and Palmer., M.: Class-based construction of a verb lexicon. AAAI-2000, Proceedings of the Seventeenth National Conference on Artificial Intelligence, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
<author>Christopher Culy</author>
<author>M H v d B G L T</author>
<author>D Ahn</author>
</authors>
<date>2004</date>
<booktitle>Sentential structure and discourse parsing. ACL 2004, Workshop on Discourse Annotation,</booktitle>
<contexts>
<context position="2165" citStr="Polanyi et al., 2004" startWordPosition="337" endWordPosition="340">Figure 1 shows the complete annotation of the full text. Now, if we were to reorder these segments as [(1b), (4a), (2a), (4b), (3a), (2b), (1a)], the text would not make much sense. Therefore, it is imperative that the contiguous spans of discourse be coherent for comprehension. Rhetorical relations help make the text coherent. Rhetorical relations based on the subject matter of the segments are called informational relations. A common understanding in discourse study is that informational relations are based on the underlying content of the text segments. However, previous work (Marcu, 2000; Polanyi et al., 2004; Soricut and Marcu, 2005; Sporleder and Lascarides, 2005) in discourse parsing has relied on syntactic and lexical information, and shallow semantics only. The goal of this thesis is to build a computational model for parsing the informational structure of instructional text that exploits “deeper semantics”, namely event semantics. Such discourse structures can be useful for applications such as information extraction, question answering and intelligent tutoring systems. Our approach makes use of a neural network discourse segmenter, a rhetorical relation classifier based on ILP and a discour</context>
<context position="5889" citStr="Polanyi et al., 2004" startWordPosition="928" endWordPosition="931"> like motion and together describe the participants of an event at various stages. CoreLex provides meaning representations for about 40,000 nouns that are compatible with VerbNet. The parser was used to semi-automatically annotate both our training and test data. Since the output of the parser can be ambiguous with respect to the verb sense, we manually pick the correct sense.3 3 Automatic Discourse Segmentation The task of the discourse segmenter is to segment sentences into EDUs. In the past, the problem of sentence level discourse segmentation has been tackled using both symbolic methods (Polanyi et al., 2004; Huong et al., 2004) as well as statistical models (Soricut and Marcu, 2003; Marcu, 2000) that have exploited syntactic and lexical features. We have implemented a Neural Network model 3In addition, the parser generates semantic representations for fragments of the sentence to handle ungrammatical sentences, etc. 22 for sentence level discourse segmentation that uses syntactic features and discourse cues. Our model was trained and tested on RST-DT (2002) and achieves a performance of up to 86.12% F-Score, which is comparable to Soricut and Marcu (2003). We plan to use this model on our corpus</context>
</contexts>
<marker>Polanyi, Culy, T, Ahn, 2004</marker>
<rawString>Livia Polanyi, Christopher Culy, M. H. v. d. B. G. L. T. and Ahn., D.: Sentential structure and discourse parsing. ACL 2004, Workshop on Discourse Annotation, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>Instructions for Manually Annotating the Discourse Structures of Texts.</title>
<date>1999</date>
<tech>Technical Report,</tech>
<institution>University of Southern California,</institution>
<contexts>
<context position="3697" citStr="Marcu (1999)" startWordPosition="578" endWordPosition="579">n in section 5. Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 21–24, Rochester, April 2007. c�2007 Association for Computational Linguistics Figure 1: Discourse Annotation for Example 1 2 Data Collection Our work calls for the use of a supervised machine learning approach. Therefore, we have manually annotated a corpus of instructional text with rhetorical relations and event semantic information. We used an existing corpus on home repair manuals (5Mb).1 2.1 Manual Discourse Annotation In order to carry out the manual discourse annotation, a coding scheme was developed based on Marcu (1999) and RDA (Moser et al., 1996). The annotated data consists of 5744 EDUs and 5131 relations with a kappa value of 0.66 on about 26% of the corpus. We analyzed a total of 1217 examples to determine whether a cue phrase was present or not. Only 523 examples (43%) were judged to be signalled. Furthermore, discourse cues can be ambiguous with regard to which relation they signal. In order to account for cases where discourse cues are not present and to resolve such ambiguities, we intend to exploit event semantics. 2.2 Semi-Automatic Event Semantic Annotation Informational relations describe how th</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Marcu, D.: Instructions for Manually Annotating the Discourse Structures of Texts. Technical Report, University of Southern California, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The theory and practice of discourse parsing and summarization.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts, London, England,</location>
<contexts>
<context position="2143" citStr="Marcu, 2000" startWordPosition="335" endWordPosition="336">a) and (4b). Figure 1 shows the complete annotation of the full text. Now, if we were to reorder these segments as [(1b), (4a), (2a), (4b), (3a), (2b), (1a)], the text would not make much sense. Therefore, it is imperative that the contiguous spans of discourse be coherent for comprehension. Rhetorical relations help make the text coherent. Rhetorical relations based on the subject matter of the segments are called informational relations. A common understanding in discourse study is that informational relations are based on the underlying content of the text segments. However, previous work (Marcu, 2000; Polanyi et al., 2004; Soricut and Marcu, 2005; Sporleder and Lascarides, 2005) in discourse parsing has relied on syntactic and lexical information, and shallow semantics only. The goal of this thesis is to build a computational model for parsing the informational structure of instructional text that exploits “deeper semantics”, namely event semantics. Such discourse structures can be useful for applications such as information extraction, question answering and intelligent tutoring systems. Our approach makes use of a neural network discourse segmenter, a rhetorical relation classifier base</context>
<context position="5979" citStr="Marcu, 2000" startWordPosition="946" endWordPosition="947">s meaning representations for about 40,000 nouns that are compatible with VerbNet. The parser was used to semi-automatically annotate both our training and test data. Since the output of the parser can be ambiguous with respect to the verb sense, we manually pick the correct sense.3 3 Automatic Discourse Segmentation The task of the discourse segmenter is to segment sentences into EDUs. In the past, the problem of sentence level discourse segmentation has been tackled using both symbolic methods (Polanyi et al., 2004; Huong et al., 2004) as well as statistical models (Soricut and Marcu, 2003; Marcu, 2000) that have exploited syntactic and lexical features. We have implemented a Neural Network model 3In addition, the parser generates semantic representations for fragments of the sentence to handle ungrammatical sentences, etc. 22 for sentence level discourse segmentation that uses syntactic features and discourse cues. Our model was trained and tested on RST-DT (2002) and achieves a performance of up to 86.12% F-Score, which is comparable to Soricut and Marcu (2003). We plan to use this model on our corpus as well. 4 Discourse Parsing Once the EDUs have been identified by the discourse segmente</context>
<context position="10533" citStr="Marcu (2000)" startWordPosition="1681" endWordPosition="1682"> than 50 examples and those that were classified as goal:act, step1:step2, criterion:act or before:after 23 document. When combining two text segments, promotion sets that approximate the most important EDUs of the text segments will be used. As a starting point, we propose to build sentence level DPTs bottom-up. EDUs that are subsumed by the same syntactic constituent (usually an S, S-Bar, VP) will be combined together into a larger text segment recursively until the the DPT at the root level has been constructed. At the document level, the DPT will be built using a shift-reduce parser as in Marcu (2000). However, unlike Marcu (2000), there will only be one shift and one reduce operation. The reduce operation will be determined by the rhetorical relation classifier and an additional module that will determine all the possible attachment points for an incoming sentence level DPT. An incoming sentence level DPT may be attached to any node on the right frontier of the left DPT. Lexical cohesion will be used to rank the possible attachment points. For both sentence level discourse parsing and document level discourse parsing, the rhetorical relation classifier will be used to determine the inform</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Marcu, D.: The theory and practice of discourse parsing and summarization. Cambridge, Massachusetts, London, England, MIT Press, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Moser</author>
<author>J D Moore</author>
<author>E Glendening</author>
</authors>
<title>Instructions for Coding Explanations: Identifying Segments, Relations and Minimal Units.</title>
<date>1996</date>
<institution>University of Pittsburgh, Department of Computer Science,</institution>
<contexts>
<context position="3726" citStr="Moser et al., 1996" startWordPosition="582" endWordPosition="585">dings of the NAACL-HLT 2007 Doctoral Consortium, pages 21–24, Rochester, April 2007. c�2007 Association for Computational Linguistics Figure 1: Discourse Annotation for Example 1 2 Data Collection Our work calls for the use of a supervised machine learning approach. Therefore, we have manually annotated a corpus of instructional text with rhetorical relations and event semantic information. We used an existing corpus on home repair manuals (5Mb).1 2.1 Manual Discourse Annotation In order to carry out the manual discourse annotation, a coding scheme was developed based on Marcu (1999) and RDA (Moser et al., 1996). The annotated data consists of 5744 EDUs and 5131 relations with a kappa value of 0.66 on about 26% of the corpus. We analyzed a total of 1217 examples to determine whether a cue phrase was present or not. Only 523 examples (43%) were judged to be signalled. Furthermore, discourse cues can be ambiguous with regard to which relation they signal. In order to account for cases where discourse cues are not present and to resolve such ambiguities, we intend to exploit event semantics. 2.2 Semi-Automatic Event Semantic Annotation Informational relations describe how the content of two text segment</context>
</contexts>
<marker>Moser, Moore, Glendening, 1996</marker>
<rawString>Moser, M. G., Moore, J. D., and Glendening, E.: Instructions for Coding Explanations: Identifying Segments, Relations and Minimal Units. University of Pittsburgh, Department of Computer Science, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S H Muggleton</author>
</authors>
<title>Inverse entailment and progol.</title>
<date>1995</date>
<journal>In New Generation Computing Journal,</journal>
<volume>13</volume>
<contexts>
<context position="7379" citStr="Muggleton, 1995" startWordPosition="1169" endWordPosition="1170">e parsing model consists of a rhetorical relation classifier, a sentence level discourse parser and a document level discourse parser. 4.1 Rhetorical Relation Classifier In a preliminary investigation (Subba et al., 2006), we modeled the problem of identifying rhetorical relations as a classification problem using rich verb semantics only. Most of the work in NLP that involves learning has used more traditional machine learning paradigms like decision-tree algorithms and SVMs. However, we did not find them suitable for our data which is represented in first order logic (FOL). We found Progol (Muggleton, 1995), an ILP system, appropriate for our needs. The general problem specification for Progol (ILP) is given by the following posterior sufficiency property: B ∧ H �= E Given the background knowledge B and the examples E, Progol finds the simplest consistent hypothesis H, such that B and H entails E. The rich verb semantic representation of pairs of EDUs form the background knowledge and the manually annotated rhetorical relations between the pairs of EDUs serve as the positive examples.4 An A*-like search is used to search for the most probable hypothesis. Given our model, we are able to learn rul</context>
</contexts>
<marker>Muggleton, 1995</marker>
<rawString>Muggleton., S. H.: Inverse entailment and progol. In New Generation Computing Journal, 13:245–286, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Ros´e</author>
<author>A Lavie</author>
</authors>
<title>Balancing robustness and efficiency in unification-augmented context-free parsers for large practical applications.</title>
<date>2000</date>
<booktitle>In Jean-Clause Junqua and Gertjan</booktitle>
<editor>van Noord, editors,</editor>
<marker>Ros´e, Lavie, 2000</marker>
<rawString>Ros´e, C. P. and Lavie., A.: Balancing robustness and efficiency in unification-augmented context-free parsers for large practical applications. In Jean-Clause Junqua and Gertjan van Noord, editors, Robustness in Language and Speech Technology, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RST-DT</author>
</authors>
<title>Rst discourse treebank. Linguistic Data Consortium.,</title>
<date>2002</date>
<contexts>
<context position="6348" citStr="RST-DT (2002)" startWordPosition="1000" endWordPosition="1001"> sentences into EDUs. In the past, the problem of sentence level discourse segmentation has been tackled using both symbolic methods (Polanyi et al., 2004; Huong et al., 2004) as well as statistical models (Soricut and Marcu, 2003; Marcu, 2000) that have exploited syntactic and lexical features. We have implemented a Neural Network model 3In addition, the parser generates semantic representations for fragments of the sentence to handle ungrammatical sentences, etc. 22 for sentence level discourse segmentation that uses syntactic features and discourse cues. Our model was trained and tested on RST-DT (2002) and achieves a performance of up to 86.12% F-Score, which is comparable to Soricut and Marcu (2003). We plan to use this model on our corpus as well. 4 Discourse Parsing Once the EDUs have been identified by the discourse segmenter, the entire discourse structure of text needs to be constructed. This concerns determining which text segments are related and what relation to assign to those segments. Our discourse parsing model consists of a rhetorical relation classifier, a sentence level discourse parser and a document level discourse parser. 4.1 Rhetorical Relation Classifier In a preliminar</context>
</contexts>
<marker>RST-DT, 2002</marker>
<rawString>RST-DT.: Rst discourse treebank. Linguistic Data Consortium., 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sporleder</author>
<author>A Lascarides</author>
</authors>
<title>Exploiting linguistic cues to classify rhetorical relations.</title>
<date>2005</date>
<booktitle>Recent Advances in Natural Language Processing,</booktitle>
<contexts>
<context position="2223" citStr="Sporleder and Lascarides, 2005" startWordPosition="345" endWordPosition="348">ull text. Now, if we were to reorder these segments as [(1b), (4a), (2a), (4b), (3a), (2b), (1a)], the text would not make much sense. Therefore, it is imperative that the contiguous spans of discourse be coherent for comprehension. Rhetorical relations help make the text coherent. Rhetorical relations based on the subject matter of the segments are called informational relations. A common understanding in discourse study is that informational relations are based on the underlying content of the text segments. However, previous work (Marcu, 2000; Polanyi et al., 2004; Soricut and Marcu, 2005; Sporleder and Lascarides, 2005) in discourse parsing has relied on syntactic and lexical information, and shallow semantics only. The goal of this thesis is to build a computational model for parsing the informational structure of instructional text that exploits “deeper semantics”, namely event semantics. Such discourse structures can be useful for applications such as information extraction, question answering and intelligent tutoring systems. Our approach makes use of a neural network discourse segmenter, a rhetorical relation classifier based on ILP and a discourse parsing model that builds sentence level DPTs bottomup </context>
</contexts>
<marker>Sporleder, Lascarides, 2005</marker>
<rawString>Sporleder, C. and Lascarides., A.: Exploiting linguistic cues to classify rhetorical relations. Recent Advances in Natural Language Processing, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>Proceedings of the HLT and NAACL Conference,</booktitle>
<contexts>
<context position="5965" citStr="Soricut and Marcu, 2003" startWordPosition="942" endWordPosition="945">s stages. CoreLex provides meaning representations for about 40,000 nouns that are compatible with VerbNet. The parser was used to semi-automatically annotate both our training and test data. Since the output of the parser can be ambiguous with respect to the verb sense, we manually pick the correct sense.3 3 Automatic Discourse Segmentation The task of the discourse segmenter is to segment sentences into EDUs. In the past, the problem of sentence level discourse segmentation has been tackled using both symbolic methods (Polanyi et al., 2004; Huong et al., 2004) as well as statistical models (Soricut and Marcu, 2003; Marcu, 2000) that have exploited syntactic and lexical features. We have implemented a Neural Network model 3In addition, the parser generates semantic representations for fragments of the sentence to handle ungrammatical sentences, etc. 22 for sentence level discourse segmentation that uses syntactic features and discourse cues. Our model was trained and tested on RST-DT (2002) and achieves a performance of up to 86.12% F-Score, which is comparable to Soricut and Marcu (2003). We plan to use this model on our corpus as well. 4 Discourse Parsing Once the EDUs have been identified by the disc</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>Soricut, R. and Marcu., D.: Sentence level discourse parsing using syntactic and lexical information. Proceedings of the HLT and NAACL Conference, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Subba</author>
<author>B Di Eugenio</author>
<author>E T</author>
</authors>
<title>Building lexical resources for princpar, a large coverage parser that generates principled semantic representations. LREC</title>
<date>2006</date>
<marker>Subba, Di Eugenio, T, 2006</marker>
<rawString>Subba, R., Di Eugenio, B., E. T.: Building lexical resources for princpar, a large coverage parser that generates principled semantic representations. LREC 2006, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Subba</author>
<author>B Di Eugenio</author>
<author>S N K</author>
</authors>
<title>Learning FOL rules based on rich verb semantic representations to automatically label rhetorical relations.</title>
<date>2006</date>
<booktitle>EACL 2006, Workshop on Learning Structured Information in Natural Language Applications,</booktitle>
<marker>Subba, Di Eugenio, K, 2006</marker>
<rawString>Subba, R., Di Eugenio, B., S. N. K.: Learning FOL rules based on rich verb semantic representations to automatically label rhetorical relations. EACL 2006, Workshop on Learning Structured Information in Natural Language Applications, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wellner</author>
<author>J Pustejovsky</author>
<author>C H R S</author>
<author>A Rumshisky</author>
</authors>
<title>Classification of discourse coherence relations: An exploratory study using multiple knowledge sources.</title>
<date>2006</date>
<booktitle>SIGDIAL Workshop on Discourse and Dialogue,</booktitle>
<marker>Wellner, Pustejovsky, S, Rumshisky, 2006</marker>
<rawString>Wellner, B., Pustejovsky, J., C. H. R. S. and Rumshisky., A.: Classification of discourse coherence relations: An exploratory study using multiple knowledge sources. SIGDIAL Workshop on Discourse and Dialogue, 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>