<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993435">
Incremental Processing and the
Hierarchical Lexicon
</title>
<author confidence="0.975786">
Erik-Jan van der Linden.
</author>
<affiliation confidence="0.91194">
Tilburg University
</affiliation>
<bodyText confidence="0.9895755">
Hierarchical lexicon structures are not only of great importance for the nonredundant represen-
tation of lexical information, they may also contribute to the efficiency of the actual processing
of natural language. Two parsing techniques that render the parsing process efficient are pre-
sented. Windowing is a technique for incrementally accessing the hierarchical lexicon. Lexical
preferencing implements preferences within the parsing process as a natural consequence of the
hierarchical structure of the lexicon. Within a proof-theoretic approach to Categorial Grammar
it is possible to implement these techniques in a formal and principled way. Special attention is
paid to idiomatic expressions.
</bodyText>
<sectionHeader confidence="0.990265" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999995038461539">
The main reasons mentioned for the considerable attention paid to hierarchical lexicon
structures are the fact that redundancy in the lexicon is avoided, and that structuring
the lexicon facilitates the development of large and complex lexicons. No attention has,
however, been paid to the role the hierarchical lexicon could play in natural language
processing. Categorial Grammar (CG) has an interest in efficient and psychologically
plausible, at least incremental, processing. Although CG is a radically lexicalist gram-
matical theory, little attention has been paid to the structure of the lexicon. The aim
of the present article is to bring CG, the hierarchical lexicon, and incremental pro-
cessing together, to investigate the role of the hierarchical lexicon during incremental
parsing with categorial grammars. The rules and derivations of a categorial grammar
do not describe syntactic structures, but represent the proceedings of the parser while
constructing a semantic representation of a sentence. This property of CG is referred
to as representational nonautonomy (Crain and Steedman 1982). It will be shown that
especially in the case of ambiguity, the combination of a hierarchical lexicon structure
and representational nonautonomy provides efficient ways of dealing with ambigu-
ities: within a proof-theoretic approach to CG, rules that allow the parser to reason
about the structure of the lexicon are presented. Two parsing techniques are presented.
Windowing is a technique for incrementally accessing the hierarchical lexicon. While
incrementally parsing the sentence, the parser commits itself to lexical information it
can commit to, leaving other choices implicit in the hierarchical lexical structure of
the elements in the input. Lexical preferencing implements preferences in the parsing
process as a natural consequence of the hierarchical structure of the lexicon: informa-
tion lower on in the hierarchical lexicon is preferred over more general information.
Idiomatic expressions are presented as an example of these preferences: an idiomatic
expression is preferably interpreted as such, and not in the nonidiomatic interpretation
of which the head of the idiom is a part.
</bodyText>
<note confidence="0.598543">
* Institute for Language Technology and Al (ITK), PO Box 90153, 5000 LE Tilburg, the Netherlands.
</note>
<email confidence="0.792398">
E-mail: vdlinden@kub.n1
</email>
<note confidence="0.8846335">
C) 1992 Association for Computational Linguistics
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.999423">
In Section 2 the proof-theoretic approach to CG is presented. Next, in Section 3 a
hierarchical lexicon structure for CG is presented. Two category-forming connectives
that make the structure of the lexicon visible for the parser are introduced. Windowing
is discussed in Section 4. In Section 5 parsing preferences are discussed in general, and
preferences for the interpretation of idioms are discussed in particular.
</bodyText>
<sectionHeader confidence="0.792307" genericHeader="keywords">
2. Categorial Grammar and Proof Theory
</sectionHeader>
<subsectionHeader confidence="0.999421">
2.1 The Lambek Calculus
</subsectionHeader>
<bodyText confidence="0.999980307692308">
Recently, proof theory has aroused the interest of categorial grammarians. In the Lam-
bek calculus (L-calculus, L; Lambek 1958), the most salient example of the application
of proof theory to categorial grammar, the rules of the grammar become a set of ax-
ioms and inference rules. Together, these form a logical calculus in which parsing of
a syntagm is an attempt to prove that it follows as a theorem from the set of axioms
and inference rules. Following the work of Van Benthem (1986) and Moortgat (1988;
1987) the Lambek calculus has become popular among a number of linguists (Barry
and Morrill 1990; Hendriks 1987).
Categories in CG can either be basic (np, s, n) or complex. A complex category
consists of a binary category forming connective and two categories, for instance, np\s.
In the product-free L-calculus the set of connectives (also called type constructors) is
{ \ /}. A complex category is a functor, an incomplete expression that forms a result
category if an argument category is found. Throughout this paper the Lambek notation,
in which the argument category is found under the slash, is applied. Consider for
example the categorial representation of an intransitive verb: np\s looks for an np to
its left and results in an s.
The elements the calculus operates upon are categories with semantic and pro-
sodic information added, denoted with (syntax, prosody, semantics), and referred to as
signs. Information not relevant for the discussion is omitted. In the version of L used
here, complex syntactic categories take signs as their arguments. Semantics is repre-
sented with formulas of the lambda-calculus. Prosodic information merely consists of
a prosodic bracketing; for instance, the string john sleeps is denoted as john + sleeps,
where + is a noncommutative, nonassociative concatenation operator. Concatenation
of some (/) with the empty prosodic element E results in 0.
The L-calculus extends the power of categorial grammar basically because it adds
so-called introduction rules to the proof-theoretic complements of categorial reduction
rules, elimination rules. For each category-forming connective, introduction and elim-
ination rules can be formulated. With respect to semantics, elimination corresponds
to functional application and introduction to lambda abstraction. Various approaches
have been proposed for deduction in L. In its standard representation the L-calculus
is a sequent calculus. More recently, natural deduction has been applied to the calculus
(Barry and Morrill 1990), as well as proof procedures from linear logic (Roorda 1990).1
Throughout this article the sequent format is used.
In definition (1), W and X are categories, Y and Z are signs, and P. T, Q, U and V
are sequences of signs, where P. T, and Q are nonempty. A sequent in L represents a
derivability relation, z, between a nonempty finite sequence of signs, the antecedent,
and a sign, the succedent. A sequent states that the string denoted by the antecedent
is in the set of strings denoted by the succedent. The axioms and inference rules of the
calculus define the theorems of the calculus with respect to this derivability relation.
</bodyText>
<footnote confidence="0.535852">
1 For a comparison, see Leslie (1990).
</footnote>
<page confidence="0.989509">
220
</page>
<note confidence="0.787142">
Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon
</note>
<bodyText confidence="0.993311666666667">
Recursive application of the inference rules on a sequent may result in the derivation
of a sequent as a theorem of the calculus. In definition (1) the calculus is presented.
The elimination of a type constructor is denoted by E; introduction by I.
</bodyText>
<equation confidence="0.720476">
Definition 1
(Lambek, sequent calculus)
U,((X/(W,V,,b)),0,a),T,V = Z r/El
</equation>
<construct confidence="0.8345714">
if T
and U,(X,0+0,a(b)),V = Z
U,T,(((W,O,b) \X),0,a),V = Z RE]
if T (W,O,b)
and U,(X,04-0,a(b)),V Z
</construct>
<equation confidence="0.9507158">
T =ix ((X/(W,e,b)),O,Ab.a) 1/Il
if T,(W,e,b) x
T = (((W,E,b) \X),4),Ab.a) RI]
if (W,E,b), T (X,E+4),a)
(X,O,a) = (X,4),a) [Axiom]
</equation>
<bodyText confidence="0.999908333333333">
The uppersequent of an inference rule is a theorem of the calculus if all of its
subsequents are theorems. In Example 1 a sentence containing a transitive verb is
parsed by proving that it reduces to s. To the sequence of lexical signs associated with
the strings in the input, the inference rules are recursively applied until all leaves of
the proof tree are axioms. The derivation results in the instantiation of the semantics
of the sentence.
</bodyText>
<construct confidence="0.725078333333333">
Example 1
(np,john,john) ((np \ s)/np,loves,loves) (np,mary,mary) (s,john+loves+mary,loves(mary)(john)) 1/El
if (np,mary,mary) (np,mary,mary) [Axiom]
and (np,john,john) (np \ s,loves+mary, loves(mary)) (s,john+loves+mary,loves(mary)(john)) [ \ El
if (np,john,john) (np,john,john) [Axiom]
and (s,john+loves+mary,loves(mary)(john)) (s,john+loves+mary,loves(mary)(john)) [Axiom]
</construct>
<subsectionHeader confidence="0.998152">
2.2 Other Connectives
</subsectionHeader>
<bodyText confidence="0.999158882352941">
The product-free version of the Lambek calculus includes two connectives, / and \ On
the basis of these connectives and the inference rules of the L-calculus, a range of lin-
guistic constructions and generalizations remain for which no linguistically adequate
accounts can be presented. In order to overcome this problem, new category-forming
connectives have been proposed (as a lexical alternative of the specialized rules of
for instance CCG [Steedman 1987]). An example is the connective 1- for unbounded
dependencies (Moortgat 1988). Xi Y denotes a category X that has an argument of
category Y missing somewhere within X. The constituent John put on the table in what
John put on the table has as its syntactic category s I np. To what the category s/(s I np)
is assigned, which takes the incomplete clause as its argument.
The A-connective (Morrill 1990) is one of a set of boolean connectives that can be
used to denote that a certain lexical item can occur in different categories: square can
be n/n and n, and is therefore assigned the category (n/n) An. The ?-connective (ibid.)
is used to denote optionality, for instance in the case of belief: n/ (sp?), which accounts
for belief in the belief and the belief that Mary lives.
These connectives are introduced to enable the inference engine behind the calcu-
lus to deal with lexical ambiguities and to &apos;reason&apos; about lexical items. This is in line
</bodyText>
<page confidence="0.991503">
221
</page>
<note confidence="0.337761">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.9989845">
with the principle of representational nonautonomy, which states that syntactic rules
describe what the processor does while assembling a semantic representation.
</bodyText>
<listItem confidence="0.521538">
3. Inheritance and the Hierarchical Lexicon
</listItem>
<bodyText confidence="0.9999032">
To allow the inference engine to reason about lexical structures in which inheritance
relations are present, the calculus should be extended, and a more sophisticated struc-
ture should be assigned to the categorial lexicon than the list or bag that is usually
considered in CG (with the exception of Bouma 1990). The current section deals with
the lexicon; the next sections deal with the extension of the calculus.
</bodyText>
<subsectionHeader confidence="0.999948">
3.1 An Example: Idioms
</subsectionHeader>
<bodyText confidence="0.890860277777778">
An idiomatic expression and its verbal head can be said to maintain a lexical inher-
itance relation: an idiomatic expression inherits part of its properties from its head.
Here, syntactic category, syntactic behavior, morphology, and semantics are discussed
briefly.
3.1.1 Syntactic Category. Idiomatic expressions can be represented as functor-argu-
ment-structures2 and have the same format as the verbs that are their heads. It is
therefore possible to relate the syntactic category of the idiom to that of its head (see
also Zernik and Dyer 1987). The verb itself does not specify prosodic information for
the argument and the idiom is a specialization of the verb because it does specify
prosodic information. In other words, the verb (kick) subcategorizes for the whole set
of strings with category np, whereas the idiom subcategorizes for the subset of that
set (the + bucket). The information that the object argument is specified for a certain
string can thus be added monotonically. Inheritance relations between lexical items
are denoted here with a category-forming connective &gt;-. Mother &gt;-- Daughter states that
Daughter is a specialization of Mother. The relation between verb and idiom is part
of the lexical structure which is associated with the lexical entry of the verb. KICK,
KICK_TV and KICK_THE_BUCKET are represented as in Example 2.
Example 2
</bodyText>
<listItem confidence="0.998391666666667">
a. KICK: (KICK_TV KICK_THE_BUCKET,kick)
b. KICK_TV: ((np \s)/ (np, _), _)
c. KICK_THE_BUCKET: (_/(_, the + bucket),)
</listItem>
<footnote confidence="0.6231449">
3.1.2 Morphological Properties. The verb that is the head of an idiomatic expression
has the same inflectional paradigm as the verb outside the expression: for instance, if
a verb is strong outside an idiom, it is strong within the idiom.
3.1.3 Syntactic Behavior. The syntactic behavior of idioms should partly be explained
in terms of properties of their heads. For example, it is not possible to form a passive
on the basis of predicative and copulative verbs, either inside or outside an idiomatic
expression.3 This information is inherited by the idiom from its verbal head.
2 Similar representations can be found for TAG (Abeille 1990; Abeille and Schabes 1989) and HPSG
(Erbach 1991).
3 See van der Linden (1991).
</footnote>
<page confidence="0.993272">
222
</page>
<bodyText confidence="0.910092333333333">
Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon
3.1.4 Semantics. The traditional definition of an idiom states that its meaning is not
a function of the meanings of its parts and the way these are syntactically combined;
that is, an idiom is a noncompositional expression. Under this definition, their meaning
can be subject to any other principle that describes in what way the meaning of an
expression should be derived (contextuality, meaning postulates...). A definition that
states what the meaning is, is preferable: the meaning of an idiom is exclusively a
property of the whole expression.4 The meaning of the idiom cannot be inherited
from the verb that is its head, but should be added nonmonotonically.
</bodyText>
<subsectionHeader confidence="0.387949">
Example 3
</subsectionHeader>
<listItem confidence="0.920019">
a. KICK: (KICK_TV &gt;-- KICK_THE_BUCKET, kick, AxAykick(x)(y))
b. KICK_TV: ((np\s)/np, _)
c. KICK_THE_BUCKET: (_, the + bucket, _), AxAydie(y))
</listItem>
<bodyText confidence="0.928379">
3.1.5 Inheritance. The full specification of a sign is derived by means of an operation
similar to priority union (Kaplan 1987, p. 180) or default unification (Bouma 1990), de-
noted by n. n is defined as a function from pairs of mother and daughter signs to
fully specified daughter signs and runs as follows. If unification, U, is successful for
the values of a certain property of mother and daughter, the result of n for that value
is the result of U, where unification is understood in its most basic sense: variables
unify with constants and variables; constants unify with variables and with constants
with an equal value (prosodic information in Example 4). If the values do not unify,
the value of the daughter is returned (semantic information in Example 4).
</bodyText>
<subsectionHeader confidence="0.416346">
Example 4
</subsectionHeader>
<construct confidence="0.596251">
(KICK n KICK_TV) n KICK_THE_BUCKET:
((np\s)/ (np, the + bucket, _), kick, AxAydie(y))
</construct>
<bodyText confidence="0.999863722222222">
The inheritance networks for which n is defined are unipolar, nonmonotonic, and
homogeneous (Touretzky, Horty, and Thomason 1987). For other networks, other rea-
soning mechanisms are necessary to determine the properties of a node (Touretzky,
Horty, and Thomason 1987; Touretzky 1986; Veltman 1990).5
More specific information thus takes precedence over more general information.
This is a common feature of inheritance systems, and is an application of &apos;proper in-
clusion precedence,&apos; which is acknowledged in knowledge representation and (com-
putational) linguistics (De Smedt 1990; Daelemans 1987; other papers in this special
issue).
There exists a clear relation between this principle and the linguistic notion block-
ing. Blocking is &amp;quot;the nonoccurrence of one form due to the simple existence of another&amp;quot;
(Aronoff 1976, p. 41). For instance, the nominal derivation *graciosity of gracious is
blocked by the existence of grace. Daelemans (1987) and De Smedt (1990) show that
in a hierarchical lexicon structure, blocking is equivalent to the prevalence of more
specific information over more general information. For instance, the more general
principle in the example is that a nominal derivation of some abstract adjectives equals
stem + ity, and the more specific information is that in the case of gracious the nominal
derivation is grace. In the hierarchical lexicon, the principle of priority to the instance
</bodyText>
<footnote confidence="0.973634">
4 See van der Linden and Kraaij (1990) and van der Linden (1991) for a more extensive comparison of
this definition and the traditional one.
5 Touretzky (1986) also discusses default logic and nonmonotonic logic.
</footnote>
<page confidence="0.9945">
223
</page>
<note confidence="0.332479">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.983178666666667">
also blocks ?graciousness (whereas this is not the case for Aronoff&apos;s model). In Dutch,
the participle *geslaapt that has been formed on the basis of regular morphological
processes is blocked because the past participle of sla pen is gesla pen.
</bodyText>
<subsectionHeader confidence="0.999859">
3.2 Other Lexical Relations
</subsectionHeader>
<bodyText confidence="0.9948145">
Verbs that can be either transitive or intransitive, such as kick, can in principle be
modeled with the use of the A-connective:
</bodyText>
<equation confidence="0.382193">
(np\s, Ayakick(x)(y)) A ((np\s)/ np, )xAykick(x)(y)).
</equation>
<bodyText confidence="0.999964583333333">
There are, however, two generalizations missing here. Firstly, the transitive and
the intransitive form share the syntactic information that their reducible category is s
and their subject argument is np. Secondly, the denotation of the transitive subsumes
the denotation of the intransitive: the semantics of the transitive verb is more specific
than the semantic representation of the intransitive. The use of the optionality oper-
ator ? (((np\s)/?np)) would imply that kick is in principle an intransitive verb, that
has one optional argument, whereas in fact the reverse is true: kick is a two-place-
functor of which one argument may be left unspecified syntactically. The transitive
and intransitive verb can be said to share their semantic value, but in the case of the
intransitive, the syntactically unspecified object is not bound by a )-operator but by
an (informationally richer) existential quantor. The transition from the transitive to the
intransitive is represented as a lexical type-transition (Dowty 1979, p. 308).
</bodyText>
<equation confidence="0.991948">
Definition 2 (detransitivization)
detrans :
AxAyD(x)(y) AyaD(x)(y)
</equation>
<bodyText confidence="0.960963916666667">
From a syntactic point of view, the transitive form of the verb can be said to
inherit the syntactic information from the intransitive and to add a syntactic argument.
From a semantic point of view, the transitive inherits the semantic information that is
specified for the KICK entry as a whole. The intransitive inherits the same information
and stipulates application of detransitivization to it. The lexical relation between the
transitive and the intransitive is thus different from that between a verb and an idiom:
in the case of the idiom a syntactic argument is further instantiated whereas here
a syntactic argument is added. To represent this distinction, a different connective is
used: &gt;. With the use of this type constructor, the intransitive and the transitive can
be placed in an inheritance relation (as seen in Example 5). &gt;&gt; is a category forming
connective which takes two signs to form a category.
Example 5
</bodyText>
<listItem confidence="0.994789333333333">
a. KICK: (KICK IV &gt;&gt; KICK_TV , kick, AxAykick(x)(y))
c. KICK_IV: (np\s, detrans(KICK))
b. KICK_TV: (synt(KICK_IV)/np, sem(KICK))
</listItem>
<bodyText confidence="0.993958">
The lexical structure presented here can be considered equal to that presented by
Flickinger (1987) and Pollard and Sag (1987) for HPSG. They present a hierarchy in
which not only transitive and intransitive verbs, but other classes of verbs are repre-
sented as well. A minor difference is that Flickinger and Pollard and Sag place classes
of verbs in hierarchical relations, whereas here individual verbs maintain inheritance
</bodyText>
<page confidence="0.979814">
224
</page>
<bodyText confidence="0.92406">
Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon
relations. The main difference with this and other previous approaches is that with
the introduction of connectives for inheritance relations, inference rules for these con-
nectives can be presented that describe the legal moves of the inference engine when
reasoning about these lexical structures. This will be discussed in the next section.
</bodyText>
<sectionHeader confidence="0.840876" genericHeader="introduction">
4. Windowing
</sectionHeader>
<subsectionHeader confidence="0.863899">
4.1 Incrementality and Immediacy
</subsectionHeader>
<bodyText confidence="0.99378525">
Left-to-right, incremental processing contributes to the speed of the parsing process
because parts of the input are processed as soon as they are encountered, and not
after the input has been completed. Besides, because of the fact that processing is
incremental, it is possible to give an interpretation of a sentence at any moment during
the parsing process.6
Immediate interpretation, which entails that the processor deals with semantics
as nearly as possible in parallel with syntax, contributes to the efficiency of the inter-
pretation process because ambiguities are solved as soon as possible, and processing
downstream is thus not bothered with alternative analyses.
Categorial grammar enables incremental and immediate processing since it allows
for flexible constituent structures: any two signs can be combined to form a larger
informational unit. For a parsing process to be incremental, it should reduce two
constituents if these maintain a head-argument relation. The incremental construction
of analyses for sentences with the use of phrase structure grammars is not in all cases
possible. For example, in case the input consists of a subject and a transitive verb it
is only possible to integrate these two into a sentence if the object has been parsed:
only then can the vp be formed and combined with the subject to form an s (Briscoe
1987). A process such as this cannot be called incremental. Although subject and verb
can be processed incrementally independently from each other, this is not the case for
their combination.
The strategy mostly used in incremental CG-processing is to enable the construc-
tion of a semantic structure with the use of principles that concatenate all possible ad-
jacent categories (although some exceptions are made for coordinate structures [Dowty
1988; Houtman 1987]). In Combinatory Categorial Grammar (Ades and Steedman 1982;
Steedman 1987), for instance, composition and lifting rules (Definitions 3 and 4) enable
incremental interpretation (Example 6). To make use of these rules in a proof-theoretic
approach to CG, a rule that cuts the result of these rules in the proof as a whole
(Definition 5) is necessary.
</bodyText>
<equation confidence="0.98036475">
Definition 3
(X /Y,a) (Y / Z,b) (X /Z, Ax.a(b(x))) [Comp]
Definition 4
(X,a) = (Z/ (X \ Z), )b.b(a)) [Lift]
</equation>
<bodyText confidence="0.645394">
6 The first parser that featured incremental processing can be found in Marcus (1980). This parser did
not consider lexical ambiguity and confined itself to syntactic processing. Other computational models
that entail the notion of incrementality can be found for Segment Grammar and in Word Expert
Parsing. The subsymbolic processing architecture for Segment Grammar presented by Kempen and
Vosse (1989) is a model of syntactic processing. The architecture allows for immediate interpretation,
but no semantic representation is actually constructed. Adriaens (1986) presents a lexicalist model,
Word Expert Parsing, which operates incrementally. Another lexicalist model that features incremental
processing can be found in Stock (1989). In none of the models is mention made of a structured lexicon.
</bodyText>
<page confidence="0.989913">
225
</page>
<figure confidence="0.5056105">
Computational Linguistics Volume 18, Number 2
Definition 5
</figure>
<construct confidence="0.891457230769231">
U, X, Y, V Z [Cut]
if X, Y = W
and U, W, V Z
Example 6
(np,john) ((np \ s)/np,kicks) (np/n,the) (n,boy) = (s, kicks(the(boy))(john)) [Cut]
if (np,john) (s/(np \ s), AX.X(john) ) [Lift]
and (s/(np \ s), )X.X(john) ) ((np \ s)/np,kicks) (np/n,the) (n,boy) (s, kicks(the(boy))(john)) [Cut]
if (s/(np \ s), AX.X(john) ) ((np \ s)/np,kicks) (s/np, Ax.kicks(x)(john)) [Comp]
and (s/np, Ax.kicks(x)(john)) (np/n,the) (n,boy) (s, kicks(the(boy))(john)) [Cut]
if (s/np, )x.kicks(x)(john)) (np/n,the) r (s/n, Ay.kick(the(y))(john)) [Comp]
and (s/n, )y.kick(the(y))(john)) (n,boy) = (s, kicks(the(boy))(john)) [/E]
if (n, boy) (n,boy) [Axiom]
and (s, kicks(the(boy))(john)) (s, kicks(the(boy))(john)) [Axiom]
</construct>
<bodyText confidence="0.999466833333333">
All words, also the function words such as the are in principle processed and thus
interpreted immediately; that is, their semantic representation is accessed from the
lexicon and combined with the semantic representation of the input so far.
A similar proposal is the M-calculus (Moortgat 1988; 1990). In M, the elimination
rules of L are traded in for a set of generalized application rules and a cut-rule that links
the derivation relation = and the derivation relation of the system of generalized
</bodyText>
<equation confidence="0.7702455625">
application, (Definition 6).
Definition 6
(X/ (Y,O,a),0,b), (Z,x,c) = (X,0+0,b(a))
if (Z,x,c) = (Y,&apos;/,a)
(Z,x,c), ((Y,O,a) \X,cb,b) (X4&apos;+0,b(a))
if (Z,x,c) = (Y,O,a)
(4,c), (X/Y,O,b) (W/Y,O+0,Aa.d)
if (Z,V),c), (X,,b(a)) (W,_,d)
(Y\X,O,b), (ZAP,c) (Y\W,O+0, )a.d)
if (X,,b(a)), (Z,O,c) (W,,d).
(X/Y,O,b), (Z,O,c) = (X/W,O+0,Ad.b(a))
if (Z,O,c), (W,,d) (Y,,a)
(Z4,c), (Y\X,O,b) (W\X4+0,Ad.b(a))
if (W,,d), (ZAb,c) (Y,,a)
U,(X,O,a),(Y,O,b),V = (Z)
if (X,O,a),(Y,O,b) (Cut,x,c)
</equation>
<bodyText confidence="0.714933333333333">
and U,(Cut,x,c),V (Z)
M is also capable of processing a sentence in an incremental fashion, as each word
is added to the semantic structure as it is encountered. These Categorial Grammars
</bodyText>
<figure confidence="0.986189571428571">
[M1/1
[M1 \]
[M2/]
[M2\]
[M3/]
[M3\]
[M-Cut]
</figure>
<page confidence="0.963728">
226
</page>
<bodyText confidence="0.9790166875">
Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon
thus implement incrementality and an all-or-none immediacy: there is at all times
during the parsing process a full interpretation of the input so far.&apos;
There are two problems with this approach. Firstly it is questionable whether it
agrees with the psycholinguistic notion of immediacy, and secondly it leads to an
unrealistic view of the parsing process. The second point will be discussed in the
following section.
With respect to the first point of criticism it should in the first place be noted that
immediacy as it was formulated by Just and Carpenter (1980) was only formulated
for content words. The immediacy assumption states that processing of content words
should be as immediate as possible. Firstly, CG has included function words under
immediacy. Haddock (1987),8 for instance, states that given a domain with two rabbits
that are &apos;in&apos; something, during incremental processing of the phrase the rabbit in the
hat.
&amp;quot;the incremental evaluation of the rabbit in the has created two distinct
sets of candidates for the two NPs in the phrase&amp;quot; (Haddock 1987,
</bodyText>
<equation confidence="0.806744">
p. 81)
</equation>
<bodyText confidence="0.9998498">
It is not clear from the psycholinguistic literature whether processing of function words
takes place this way, but it is at least unintuitive: in larger domains large intermediate
sets of candidates will be of little help for the interpretation in comparison to the
information the constituent as a whole provides. Secondly, the wish to be able to
give an interpretation of a sentence at any stage of the parsing process stems from
the fact that humans are able to make guesses about continuations of sentences that
stop before they have come to a proper ending (Schubert 1984). From this it follows
that humans are able to construct interpretations at any moment during NLP, but not
that they actually do construct full interpretations: the ability to complete incomplete
sentences says little about the ongoing automatic interpretation process.
</bodyText>
<subsectionHeader confidence="0.999826">
4.2 Windowing and Lexical Ambiguity
</subsectionHeader>
<bodyText confidence="0.999023">
The &gt;&gt;-operator is useful for incremental processing in case of lexical syntactic ambi-
guity and overcomes one of the problems of all-or-none immediacy.
One of the sources of lexical ambiguity is that a functor may have several sub-
categorization frames. During incremental processing, one of the subcategorization
frames of an ambiguous word has to be selected. How this choice is made is unclear
in most categorial work that claims to model incremental processing: ambiguity is not
an issue.9 With the use of operators like A the ambiguity can at least be described, but
truly incremental processing does not seem possible: the all-or-none immediacy leads
to a unrealistic parsing process. An example will illustrate this. In Example 7, part of
the derivation of John gave a book to Mary is presented.
</bodyText>
<footnote confidence="0.9189957">
7 In the P-calculus (Bouma 1989) a shift-reduce strategy is modeled for a categorial parser. Reduction
corresponds to the application of a categorial reduction rule; a shift is represented by connecting two
categories by means of the product operator &apos;*&apos;. During later stages of incremental processing this
product formula is taken apart, and the parts are used for constructing a semantic representation.
Bouma notes with respect to his P-calculus that connecting two semantic representations with a &apos;*&apos; can
hardly be called building up a semantic representation.
8 Haddock (1987) proposes a &apos;reduce-first&apos; strategy for the incremental categorial parsing. It &amp;quot;(... ) will
always reduce, remembering the shift option as an alternative which could be chosen in the event of
backtracking&amp;quot; (Haddock 1987, p. 75)
9 Ades and Steedman (1982) state this explicitly.
</footnote>
<page confidence="0.973927">
227
</page>
<figure confidence="0.97277925">
Computational Linguistics Volume 18, Number 2
Example 7
John gave
np ((np\s)/pp)/npn(np\s)/ppA(np\s)
</figure>
<bodyText confidence="0.999881666666667">
The problem that faces the parser here is that it is forced to choose one of the
subcategorization frames to make this step in the derivation. There is, however, no
indication which frame should be selected. In the case an incorrect frame is selected,
backtracking is necessary when further material in the input is contradictory with this
frame. For instance, the choice of a frame without a direct object will lead to a se-
mantic representation that includes the binding of the object position by an existential
quantifier. If the parser later on encounters a direct object, this will lead to a revision of
the choice of the category of give and to revision of the interpretation of the sentence.
After having encountered John gave, the parser only has to commit itself to the fact
that there is (at least) one np-argument to the verb, that the result category is s, and
that this argument semantically functions as the subject: AxAygive(x)(y)(john). What-
ever the continuation may be, intransitive, transitive, or ditransitive, this semantic
representation subsumes the semantics of the whole sentence. Whether the continu-
ation is intransitive, transitive, or ditransitive cannot be decided, and should be left
unspecified. In terms of the hierarchical relation between the frames as it was linguis-
tically motivated in the previous sections (compare the inheritance hierarchy for kick
in Example 3) the parser should commit itself to the information that is valid for the
inheritance hierarchy as a whole, and to the syntactic information of the intransitive
form, but it does not yet have to commit itself any further. The parser can, while
incrementally processing a sentence, keep a window on the lexical structure, which
becomes smaller iff there is evidence in the input that one of the frames is the right
frame. Since parts of the information are shared among the different frames, informa-
tion once gained is not lost, but is available for all frames. This technique of careful
incremental lexicon access will be referred to as windowing here. It can be considered
a syntactic counterpart of the semantic Polaroid Words of Hirst (1988), for which the
meanings become more specific (develop) in the light of evidence in the input, except
that Polaroid Words are active objects.
Since the hierarchical structure of the lexicon can be made visible to the parser by
means of the &gt;&gt;-operator, it is possible to model windowing by means of the infer-
ence rules for the &gt;&gt;-operator. In Definition 7 elimination rules for &gt; are presented.1°
Together with the M-system, these rules form a calculus that enables incremental pro-
cessing and incremental access to the lexicon. It will be referred to as the I-calculus (I
for inheritance), and will be used in what follows.&amp;quot;
To link derivability in the L calculus to 11, 11 &gt; relates a node from a hierarchy to its
specification: (hierarchy, node) n &gt; specification. Node can either be the top-node of the
hierarchy, mother, its daughter, or the granddaughter, which is the node in the hierarchy
that is linked to its mother with the &gt;&gt;-operator, but which has no &gt;&gt;-daughters.
The inference rule that eliminates the inheritance operator has three instances. In
the first case, the sign on top of the lexical hierarchy combines with an argument sign
</bodyText>
<footnote confidence="0.531144166666667">
10 No introduction rules are presented since these would allow inheritance connectives to be introduced
in a proof syntactically, whereas they can only originate lexically (cf. the /-operator in Hepple [19901):
a sequent of the form A B = A&gt;B would come down to the question of whether two unrelated
signs could maintain an inheritance relation that is not stipulated in the lexicon.
11 Inclusion of a notion of dependency constituency (Barry and Pickering 1990) excludes strings such as
John loves the from being a constituent in contrast to the original M-calculus.
</footnote>
<page confidence="0.974896">
228
</page>
<bodyText confidence="0.914631333333333">
Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon
in the input (this rule has a right-looking counterpart). In the second case, the result
of the elimination of &gt;&gt; is the daughter. In the third case, the result is the mother. In
line with representational nonautonomy these rules describe what the processor does
while assembling a semantic representation. In Example 8 an example is presented.
The prosodic terms are left out for reasons of clarity.
</bodyText>
<equation confidence="0.894961375">
Definition 7
(Inference rules for &gt;)
T, (((mother_arg \ mother_result),sem_mother)&gt;&gt; (syn_daughter,sem_daughter)), sem), V
((mother_result,sem_mother) (syn_daughter,sem_daughter))),sem_result) I&gt; E-argument]
if (((syn_mother,sem_mother)&gt; (syn_daughter,sem_daughter)), sem), granddaughter)
n &gt; (syn_grand,sem_grand)
and T, (syn_grand,sem) = (syn_result,sem_result)
(((syn_mother,sem_mother)&gt; (syn_daughter),sem_daughter)), sem), V Z I&gt; E-motherl
</equation>
<construct confidence="0.8967692">
if (((syn_mothersem_mother)&gt;&gt; (syn_daughter,sem_daughter)), sem), mother) n &gt; Z
((syn_mother,sem_mother) &gt; (syn_daughter,sem_daughter), sem), Vr Z 1&gt; E-daughter]
if (((syn_mother,sem_mother)&gt; (syn_daughter,sem_daughter)), sem), daughter) n &gt; aux
and (((aux),sem), daughter) n &gt; spec_daughter
and spec_daughter, V Z
</construct>
<equation confidence="0.892313777777778">
Example 8
john kicks mary
(np,john) ( ((np \s,detrans(sem)) &gt;&gt; (synt(IV)/np,sem))), AxAy.kick(x)(y)) (np,mary)
(s,kick(mary)(john)) [M-Cut]
if (np,john) (np \ s,AxAy.kick(x)(y)) (s,Ax.kick(x)(john)) r&gt;&gt; E-argument] (1)
and ((s,detrans(sem)) &gt;&gt; ( synt(IV)/np,sem)),Ax.kick(x)(john)) (np,mary)
(s,kick(mary)(john)) I&gt; E-daughterl (2)
if (synt(IV)/np,sem) (np,mary)) (s,kick(mary)(john) ) [1■43/] (3)
if (np,mary) (np,mary) [Axiom]
</equation>
<bodyText confidence="0.999956416666667">
The parser starts with the combination John and kicks (1). John serves as the argu-
ment of the intransitive form of kicks, resulting in a semantic representation that entails
that John is the subject argument. Next, the combination of the resulting category with
Mary is attempted. The intransitive frame does not fit here since there is one more np
in the input, but the transitive frame does (2). John kicks and Mary are combined (3).
In case the intransitive would apply, detransitivization would be applied. Since there
is no more material in the input, the parser stops.
Windowing commits the parser to the information that is present in the input:
constituents that maintain head-argument relations are reduced, so the process is in-
cremental. As a result of the reduction a semantic representation is constructed, so the
process is immediate. However, the parser does not commit itself to information it has
not yet access to. Therefore, erroneous parses are prevented.
</bodyText>
<sectionHeader confidence="0.60738" genericHeader="method">
5. Lexical Preferences and the Hierarchical Lexicon
</sectionHeader>
<bodyText confidence="0.999791666666667">
Besides windowing an equally important source of information that may be exploited
to render the interpretation process more efficient in case of ambiguity are lexical
preferences. To indicate the importance of lexical preferences, the present section opens
with a short discussion of preferences as they have been proposed in the literature.
Next, lexical preferences are modeled. They follow from the structure of the lexicon,
which was independently motivated to capture linguistic generalizations. Inference
</bodyText>
<page confidence="0.986832">
229
</page>
<note confidence="0.52397">
Computational Linguistics Volume 18, Number 2
</note>
<bodyText confidence="0.99913">
rules model the proceedings of the parser in this respect. Heuristic information is thus
integrated in a principled and formal way into the interpretation process. The behavior
of idiomatic expressions will be discussed as an example.
</bodyText>
<subsectionHeader confidence="0.999305">
5.1 Preference Strategies
</subsectionHeader>
<bodyText confidence="0.999335222222222">
Several preference strategies have been proposed for guiding parsers. Among these
are structural, syntactic preferences like Right Association (Kimball 1973), which entails
that a modifier should preferably be attached to the rightmost verb (phrase) or noun
(phrase) it can modify; and Minimal Attachment (Frazier and Fodor 1988), which
states that the analysis that assumes the minimal number of nodes in the syntactic
tree should be preferred.12
Semantic preferences are illustrated in Examples 9 and 10. The modifiers in both
cases are preferably attached contrary to expectations on the basis of syntactic prefer-
ences (see Schubert 1984, 1986; Wilks, Huang, and Fass 1985).
</bodyText>
<subsectionHeader confidence="0.630079">
Example 9
</subsectionHeader>
<bodyText confidence="0.88271">
John met the girl that he married at the dance.
</bodyText>
<subsectionHeader confidence="0.680735">
Example 10
</subsectionHeader>
<bodyText confidence="0.996903576923077">
John saw the bird with the red beak.
Evidence for the existence of preferences based upon contextual information has
been provided by Marslen-Wilson and Tyler (1980), who have shown in a number of
psycholinguistic experiments that contextual information influences word recognition
(see also Crain and Steedman [1982] and Taraban and McClelland [1988]).
Lexical preferencing (Ford, Bresnan, and Kaplan 1982) refers to the preference func-
tor categories have for certain arguments. For instance, the verb to go can either occur
as an intransitive verb that can be modified by a pp with the prosodic form to + X,
or it can take this pp as an argument. The second frame is the preferred frame. The
prepositional phrase should preferably be considered as an argument to the verb and
not as a vp-modifier.
Although the existence of all of these preferences should thus be acknowledged,
there are two arguments in favor of lexical preferences. Firstly, from empirical, corpus-
based studies it may be concluded that lexical preferences are successful heuristics
for resolving ambiguity (Whittemore, Ferrara, and Brunner 1990; Hobbs and Bear
1990). Secondly, although ambiguities may be resolved at any level of processing,
lexical processing takes place on a lower level, since higher levels depend upon lexical
information. Resolution of ambiguity on a low level ensures that higher levels of
processing are not bothered with ambiguities occurring on lower levels. Therefore, if
it is equally possible to model the behavior of the parser as a lexically guided or as, for
instance, a contextually guided process, the former should be preferred. For instance,
in the case of an idiomatic expression, it is more efficient to decide that the idiom
should be interpreted on the basis of the mere fact that it is an idiom than on the basis
of consultation of, for instance, some model of the context. Since lexical preferences
are successful heuristics that operate on a low level, there is sufficient reason to model
them in a principled and formal way.
</bodyText>
<page confidence="0.7454635">
12 See also Shieber (1983) and Hobbs and Bear (1990).
230
</page>
<note confidence="0.605566">
Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon
</note>
<subsectionHeader confidence="0.999112">
5.2 Formalization of Lexical Preferences
</subsectionHeader>
<bodyText confidence="0.999912111111111">
The formalization of lexical preferences proposed here is another application of the
principle of priority to the instance (Hudson 1984): the parser prefers information
lower on in the hierarchical structure of the lexicon over information on higher lev-
els in the hierarchy. If two subcategorization frames of, for instance, go maintain an
inheritance relation (np\s &gt;&gt; (np\s)/ (pp, to + _)), and both apply, the more specific
frame is preferred. The difference between windowing and lexical preferencing is that
windowing applies to the choice during incremental processing among a number of
frames of which only one applies eventually, whereas lexical preferencing applies to
a choice among frames all of which apply. Lexical preferences do not follow as some
statistically motivated preference, but as a linguistically motivated one: lexical prefer-
ences follow from the application of the principle of priority to the instance to the use
of the structured lexicon.
As was the case with windowing, lexical preferencing can be modeled by means of
the inference rules that operate upon inheritance connectives. The implementation of
this preference is quite simple. The rules for elimination of the &gt;&gt;-operator are ordered
in such a way that the inference engine firstly uses the category as a functor, and next
as the argument of a modifier (see Definition 8; A &lt; B denotes that A should be
applied before B).
</bodyText>
<equation confidence="0.494733">
Definition 8
(Order of application for &gt;)
1&gt;&gt; E-argument] &lt; 1&gt; E-motherl &lt; 1&gt; E-daughter]
</equation>
<bodyText confidence="0.99968825">
Note that the boolean operator A does not enable the implementation of this
kind of preference. It is, of course, possible to order the categories (((np\s)/ (pp, to) A
(np\s)) and to order the rules that eliminate boolean connectives (first category first).
However, the order of these categories must be stipulated, whereas in the case of the
hierarchical lexicon structure presented here, the relation between the categories is
linguistically motivated. Frequency of occurrence, that is, giving forms with higher
frequency prevalence over those with lower frequency, is not an alternative either:
more specific forms do not necessarily appear more frequently than the forms they
inherit from.
Examples. Schubert (1984; 1986) presents a number of sentences that he claims show
a preference for attachment that he claims cannot be explained on the basis of struc-
tural, syntactic preferences. The preference to attach, for example, (pp, from + _) to
disappearance can, however, be modeled as a lexical preference if disappearance (as well
as disappear) (optionally) subcategorizes for this prepositional phrase. The form with
the pp then prevails over the form without the pp. The same argument applies to
Examples 12-15 (daughter categories are fully specified).
</bodyText>
<subsectionHeader confidence="0.838783">
Example 11
</subsectionHeader>
<bodyText confidence="0.9003645">
John was alarmed by the disappearance of the administrator from head office.
disappearance: n&gt;&gt; (n/ (pp, from + _)/(pp, of + _)
</bodyText>
<subsectionHeader confidence="0.626424">
Example 12
</subsectionHeader>
<bodyText confidence="0.5721885">
John discussed the girl that he met with his mother.
discuss: ((np\s)/np ((np\s)/ (pp, with + _)) / np
</bodyText>
<page confidence="0.893591">
231
</page>
<figure confidence="0.9016195">
Computational Linguistics Volume 18, Number 2
Example 13
John abandoned the attempt to please Mary.
attempt: n &gt;&gt; (n/ (np, to + _)\sto_inf))
Example 14
Sue had difficulties with her teachers.
difficulties: n &gt;&gt; (n/ (pp,with + -))
Example 15
a. John met the girl that he married at a dance.
b. John married the girl that he met at a dance.
marry: ((np \s)/np)
met ((np\s)/np) (((np\s)/ pp)/ np)
</figure>
<subsectionHeader confidence="0.912823">
5.3 Idioms and Parsing Preferences
</subsectionHeader>
<subsubsectionHeader confidence="0.940725">
5.3.1 Conventionality and Idiom Processing. Idiomatic expressions can in most cases
</subsubsectionHeader>
<bodyText confidence="0.999915076923077">
be interpreted nonidiomatically as well.&amp;quot; It has, however, frequently been observed
that an idiomatic phrase should very rarely be interpreted nonidiomatically (Koller
1977, p. 13; Chafe 1968, p. 123; Gross 1984, p. 278; Swinney 1981, p. 208). Also, psy-
cholinguistic research indicates that in case of ambiguity there is clear preference for
the idiomatic reading (Gibbs 1980; Schraw et al. 1988; Schweigert 1986; Schweigert and
Moates 1988). The phenomenon that phrases should be interpreted according to their
idiomatic, noncompositional, lexical, conventional meaning will be referred to as the
&apos;conventionality&apos; principle (Gibbs 1980). The application of this principle is not limited
to idioms. For instance, compounds are not interpreted compositionally, but accord-
ing to the lexical, conventional meaning (Swinney 1981). Words are formed by regular
rules, but their meaning will undergo &apos;semantic drift,&apos; obscuring the compositional
nature of the complex word.
If this principle could be modeled in an appropriate way, this would be of consid-
erable help in dealing with idioms. As soon as the idiom has been identified, the ambi-
guity can be resolved and &apos;higher&apos; knowledge sources do not have to be used to solve
the ambiguity. In Stock&apos;s (1989) approach to ambiguity resolution the idiomatic and
the nonidiomatic analyses are processed in parallel. An external scheduling function
gives priority to one of these analyses. Higher knowledge sources are thus necessary
to decide upon the interpretation. In PHRAN (Wilensky and Arens 1980), specificity
plays a role, but only in suggesting patterns that match the input: evaluation takes
place on the basis of length and order of the patterns. Zernik and Dyer (1987) present
lexical representations for idioms, but do not discuss ambiguity. Van der Linden and
Kraaij (1990) discuss two alternative formalizations for conventionality. One extends
the notion continuation class from two-level morphology. The other is a simple localist
connectionist model. Here, another model based upon the specificity of information
in the hierarchical structure of the lexicon will be presented.
</bodyText>
<footnote confidence="0.953205">
5.3.2 Conventionality and the Hierarchical Lexicon. The ordering of rules for the &gt;&gt;-
operator can also be applied to the &gt;---operator, which relates idioms to verbs. Upon
13 Exceptions are idioms that contain words that occur in idioms only (spic and span, queer the pitch), and
idioms the syntactic form of which is limited to the idiom (trip the light fantastic).
</footnote>
<page confidence="0.980425">
232
</page>
<bodyText confidence="0.86623225">
Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon
encountering a situation where the &gt;--operator should be removed, the specific infor-
mation, the daughter, takes precedence over the more general information, the mother
(Definitions 9 and 10).
</bodyText>
<figure confidence="0.4804345">
Definition 9
(Order of application for &gt;-)
[&gt;- E-daughterl &lt; [&gt;- E-mother]
The two reduction rules then are presented as in Definition 10. 14
Definition 10
(&gt;-- -E)
</figure>
<construct confidence="0.926896">
(((syn_mother,sem_mother)&gt;-- (syn_daughter),sem_daughter)), sem), V-* Z I&gt;-. E-motherl
if (((syn_mother,sem_mother)&gt;- (syn_daughter,sem_daughter)), sem), mother) n &gt; type
and type,V * Z
((syn_mother,sem_mother) &gt;-- (syn_daughter,sem_daughter), sem), V-* Z E&gt;-- E-daughterl
if (((syn_mother,sem_rnother)&gt;- (syn_daughter,sem_daughter)), sem), daughter) n &gt; aux
and (((aux),sem), daughter) 11 &gt; type
and type, V * Z
</construct>
<bodyText confidence="0.999824461538461">
As was stated in Section 5.2, the boolean operator A does not enable the implemen-
tation of this kind of preference. Neither is it possible to model this kind of preference
with the use of frequency of occurrence of these forms. On the contrary, since verbs
occur within all idioms they are part of, and also occur independently of idioms, their
frequency will always be higher than that of the idiomatic expression. Therefore, verbs
would always be preferred over idioms, exactly the reverse of what is desired. Also in
the case the occurrences of the verb within the idiom are not counted as occurrences
of the verb proper, it will be unlikely that on the basis of the frequency criterion the
idiom will in all cases be preferred over the verb.
An example of the proceedings of the parser will be presented now to illustrate
the way windowing, incremental processing, and lexical preferences interact in the
case of an idiomatic expression. The sign that represents the idiom is abbreviated as
k_t_b (compare Example 2).
</bodyText>
<listItem confidence="0.9986023">
(1) After the lexicalization of John and kicked, it becomes possible to form a
flexible constituent on the basis of these two words. The result of this
step is that, semantically, John is considered the subject of any of the
verbs in the kick hierarchy.
(2) Upon encountering bucket, firstly the and bucket are reduced to an np
with a prosodic representation the+bucket. Now it becomes possible to
descend in the kick hierarchy.
(3) First the choice between the transitive and the intransitive form is made.
(4) Next the choice between the nonidiomatic and the idiomatic form is
made.
</listItem>
<bodyText confidence="0.772845666666667">
The derivation results in the assignment of the meaning die(john) to this sentence.
14 In case a verb occurs in more than one idiomatic expression, for instance kick the bucket and kick one&apos;s
heels, only the idiomatic expression that is possible on the basis of the input is used.
</bodyText>
<page confidence="0.995886">
233
</page>
<figure confidence="0.680954">
Computational Linguistics Volume 18, Number 2
Example 16
john kicks the bucket.
</figure>
<equation confidence="0.8824408">
(np,john) ((((np \ s),detrans(sem)) &gt;&gt; ( synt(IV)/np,sem&gt;-- (k_t_b)),AxAy.kick(x)(y)) (np/n,the) (n,bucket)
(s,die(john)) [Cut-MI (1)
if (np,john) (np \ s,AxAy.kick(x)(y)) = (s)oc.kick(x)(john) [&gt;-argumentl
and (((s,detrans(sem)) &gt;&gt; ( synt(IV)/np,seno- (k_t_b)),Ax.kick(x)(john)) (np/n,the) (n,bucket)
(s,die(john)) FM-Cut] (2)
if (np/n,the) (n,bucket) (np,the(bucket)) [M3/l
and (((s,detrans(sem)) &gt;&gt; ( synt(IV)/np,sen-O-- (k_t_b)),Ax.kick(x)(john)) (np,the(bucket))
(s,die(john)) FM-Cut]
if (((s,detrans(sem)) &gt;&gt; ( synt(IV)/np,sem›- (k_t_b)),Ax.kick(x)(john)) (np,the(bucket))
(s,die(john)) [&gt; E-daughterJ(3)
</equation>
<construct confidence="0.664883">
if (synt(IV)/np,sern&gt;- (k_t_b)) (np,the(bucket)) (s,die(john) E-daughter1(4)
if (k_t_b) (np,the(bucket)) =&gt;.(s,die(john)) [1■43/]
if (np,the(bucket)) r (np,the(bucket)) [Axiom]
and (s,die(john)) (s,die(john)) [Axiom]
</construct>
<subsectionHeader confidence="0.943931">
5.4 Determinism
</subsectionHeader>
<bodyText confidence="0.999938538461538">
Windowing and Lexical Preferencing are nondeterministic processes. Although the
parser commits itself only to information it is certain of and leaves other choices
implicit in the structure of the lexicon until it is able to choose (windowing), it can
mistake a vp-modifier for an argument. Lexical Preferencing is also a nondeterministic
process in that backtracking is necessary when interpretations do not fit in the context.
Although it is a linguistically motivated strategy, it does not guarantee that the correct
choice is made in all cases. In Example 17 the idiomatic reading is preferred, but later
on in the input it turns out that this is not the correct interpretation. Yet, Marcus&apos; De-
terminism Hypothesis states that &amp;quot;(...) all sentences which people can parse without conscious
difficulty can be parsed strictly deterministically&amp;quot; (Marcus 1980, P. 6). It remains to be
seen whether people do not garden-path in Example 17. Note also that backtracking
is modeled very easily—it amounts to making a different choice between two items
that maintain an inheritance relation.
</bodyText>
<subsectionHeader confidence="0.601301">
Example 17
</subsectionHeader>
<bodyText confidence="0.994473">
John kicked the bucket and Mary the small pail.
</bodyText>
<sectionHeader confidence="0.98452" genericHeader="method">
6. Implementation
</sectionHeader>
<bodyText confidence="0.999928166666667">
The parser described here has been implemented with the use of a slightly modified
version of the categorial calculi interpreter described in Moortgat (1988). This inter-
preter takes the rules of a calculus as data and applies these recursively to the sequent
associated with the input in order to prove that it is a theorem of the calculus. The sys-
tem is written in Quintus Prolog. No empirical studies of the efficiency of the system
have been undertaken so far.
</bodyText>
<sectionHeader confidence="0.635253" genericHeader="conclusions">
7. Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999872285714286">
The hierarchical structure of the lexicon can make a contribution to the speed and the
efficiency of the resolution of ambiguity during the process of understanding natural
language. With the use of other connectives, or other properties of lexical items like
frequency, it is not possible to model this. The hierarchical lexicon should thus not only
be considered as vital for the reduction of redundancy in the computational lexicon, or
as an aid for developing large lexicons, but also as a source for rendering the parsing
process faster and more efficient.
</bodyText>
<page confidence="0.977057">
234
</page>
<note confidence="0.557478">
Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon
</note>
<bodyText confidence="0.999900166666667">
The lexicalism and representational nonautonomy of categorial grammar enable
a principled and formal way to model the proceedings of a &apos;lexicon-sensitive&apos; parser.
Categorial rules not only model how categories are combined to form other categories,
but also represent parsing in the case of lexical ambiguities. The order in which the
inference rules are used implements the preferences of the parser.
Proper inclusion precedence seems to apply in generation too, except that seman-
tic instead of syntactic hierarchies should be used. During the generation of a sentence
containing a collocation, John commits a murder, the appropriate verb has to be gener-
ated on the basis of the noun. Since commit is more specific than, for instance, do or
make in that it subcategorizes for criminal acts and the like, commit is selected. Appli-
cation to generation is possible for Categorial Grammar: the Lambek-calculus can be
used bidirectionally, and the theorem proving framework is a natural candidate for a
uniform processing architecture (van der Linden and Minnen 1990).
Although representational nonautonomy is not a principle that applies to other
frameworks, there seems to be no objection to extend some of these frameworks. For
instance, besides the substitution and the adjunction operation of TAG, other, &apos;lexicon-
sensitive&apos; tree-forming operations could be added. Therefore, the approach taken here
might carry over to other frameworks.
</bodyText>
<sectionHeader confidence="0.909435" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992961695652174">
Thanks to Walter Daelemans for his
continuous plea in favor of the hierarchical
lexicon. Without it, I would not have started
the research reported on in this article.
Thanks are also owed to Michael Moortgat
for arousing my interest in categorial logic
and for his valuable feedback on all aspects
of it. Gosse Bouma&apos;s introduction of default
unification in CG initiated my thinking
about the application of inheritance to
idioms. Thanks to Harry Bunt, Koenraad De
Smedt, Martin Everaert, Hans Kerkman,
Glynn Morrill, André Schenk, Carl Vogel,
Ton van der Wouden, and three CL referees
for comments, suggestions, and discussion.
Michael Moortgat generously supplied a
copy of the categorial calculi interpreter
described in his 1988 thesis. André Schenk
and Mark Hepple provided some of the
IATEX macros used. Part of the research in
this article has been supported by a grant
from the Netherlands Organisation for
Scientific Research (NWO).
</bodyText>
<sectionHeader confidence="0.906302" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.972016413043478">
Abeille, A. (1990). &amp;quot;Lexical and syntactic
rules in a tree adjoining grammar.&amp;quot; In
Proceedings, ACL 1990, 292-298.
Abeille, A., and Schabes, Y. (1989). &amp;quot;Parsing
idioms in lexicalized TAGs.&amp;quot; In
Proceedings, EACL 1989, 1-9.
Ades, A., and Steedman, M. (1982). &amp;quot;On the
order of words.&amp;quot; Linguistics and Philosophy,
4, 517-558.
Adriaens, G. (1986). Process Linguistics.
Doctoral dissertation, University of
Leuven.
Aronoff, M. (1976). Word Formation in
Generative Grammar. Cambridge, MA: The
MIT Press.
Barry, G., and Morrill, G. (eds.) (1990).
Studies in Categorial Grammar. University
of Edinburgh Press.
Barry, G., and Pickering, M. (1990).
&amp;quot;Dependency and constituency in
categorial grammar.&amp;quot; In Studies in
Categorical Grammar, edited by G. Barry
and G. Morrill, 23-45. University of
Edinburgh Press.
Benthem, J. van (1986). &amp;quot;Categorial
grammar.&amp;quot; In Essays in Logical Semantics,
edited by J. van Benthem. Dordrecht:
Reidel.
Bouma, G. (1989). &amp;quot;Efficient processing of
flexible categorial grammar.&amp;quot; In
Proceedings, EACL 1989, 19-26.
Bouma, G. (1990). &amp;quot;Defaults in unification
grammar.&amp;quot; In Proceedings, ACL 1990,
165-172.
Briscoe, E. (1987). Modelling Human Speech
Comprehension. Chichester: Ellis Horwood.
Chafe, W. (1968). &amp;quot;Idiomaticity as an
anomaly in the Chomskyan paradigm.&amp;quot;
Foundations of Language, 4, 109-127.
CraM, S., and Steedman, M. (1982). &amp;quot;On not
being led up the garden path.&amp;quot; In Natural
Language Parsing, edited by D. Dowty,
L. Karttunen, and A. Zwicky, 320-358.
Cambridge: Cambridge University Press.
Daelemans, W. (1987). Studies in Language
Technology: An Object-oriented Model of
</reference>
<page confidence="0.991094">
235
</page>
<note confidence="0.600474">
Computational Linguistics Volume 18, Number 2
</note>
<reference confidence="0.996145918032787">
Morphophonological Aspects of Dutch.
Doctoral dissertation, University of
Leuven.
De Smedt, K. (1990). Incremental Sentence
Generation. Doctoral dissertation,
University of Nijmegen.
Dowty, R. (1979). Word Meaning and
Montague Grammar. Dordrecht: Reidel.
Dowty, R. (1988). &amp;quot;Type raising, functional
composition and non-constituent
conjunction.&amp;quot; In Categorical Grammar and
Natural Language Structure, edited by
R. Oehrle, E. Bach, and D. Wheeler,
153-197. Dordrecht: Reidel.
Erbach, G. (1991). &amp;quot;Lexical representation of
idioms.&amp;quot; IWBS report 169, IBM
TR-80.91-023, IBM, Germany.
Flickinger, D. (1987). Lexical Rules in the
Hierarchical Lexicon. Doctoral dissertation,
Stanford University.
Ford, M.; Bresnan, J.; and Kaplan, R. (1982).
&amp;quot;A competence-based theory of syntactic
closure.&amp;quot; In The Mental Representation of
Grammatical Relations, edited by
J. Bresnan. Cambridge, MA: The MIT
Press.
Frazier, L., and Fodor, J. (1978). &amp;quot;The
sausage machine: A new two-stage
parsing model.&amp;quot; Cognition, 6, 291-325.
Gibbs, R. (1980). &amp;quot;Spilling the beans on
understanding and memory for idioms in
conversation.&amp;quot; Memory and Cognition, 8,
149-156.
Gross, M. (1984). &amp;quot;Lexicon-grammar and the
syntactic analysis of French.&amp;quot; In
Proceedings, COLING 1984, 275-282.
Haddock, N. (1987). &amp;quot;Incremental
interpretation and combinatory categorial
grammar.&amp;quot; In Working Papers in Cognitive
Science, Volume 1. Categorical Grammar,
Unification Grammar and Parsing, edited by
N. Haddock, E. Klein, and G. Morrill,
71-84. Centre for Cognitive Science,
University of Edinburgh.
Haddock, N.; Klein, E.; and Morrill, G.
(eds.) (1987). Working Papers in Cognitive
Science, Volume 1. Categorial Grammar,
Unification Grammar and Parsing. Centre
for Cognitive Science, University of
Edinburgh.
Hendriks, H. (1987). &amp;quot;Type change in
semantics: The scope of quantification and
coordination.&amp;quot; In Categories, Polymorphism
and Unification, edited by E. Klein and
J. van Benthem, 95-119. University of
Edinburgh and University of Amsterdam.
Hepple, M. (1990). &amp;quot;Word order and
obliqueness in categorial grammar.&amp;quot; In
Studies in Categorical Grammar, edited by
G. Barry and G. Morrill, 47-64. University
of Edinburgh Press.
Hirst, G. (1988). &amp;quot;Resolving lexical
ambiguity computationally with
spreading activation and polaroid words.&amp;quot;
In Lexical Ambiguity Resolution, edited by
S. Small, G. Cottrell, and M. Tanenhaus,
73-107. San Mateo: Kaufmann.
Hobbs, J., and Bear, J. (1990). &amp;quot;Two
principles of parse preference.&amp;quot; In
Proceedings, COLING 1990, 162-167.
Houtman, J. (1987). &amp;quot;Coordination in
Dutch.&amp;quot; In Categories, Polymorphism and
Unification, edited by E. Klein and J. van
Benthem, 121-145. University of
Edinburgh and University of Amsterdam.
Hudson, R. (1984). Word Grammar. Oxford:
Blackwell.
Just, M., and Carpenter, P. (1980). &amp;quot;A theory
of reading, from eye fixations to
comprehension.&amp;quot; Psychological Review, 87,
329-354.
Kaplan, R. (1987). &amp;quot;Three seductions of
computational psycholinguistics.&amp;quot; In
Linguistic Theory and Computer Applications,
edited by P. Whitelock, M. McGee Wood,
H. Somers, R. Johnson, and P. Bennett,
149-188. London: Academic Press.
Kempen, G., and Vosse, Th. (1989).
&amp;quot;Incremental syntactic tree formation in
human sentence processing: A cognitive
architecture based on activation decay
and simulation annealing.&amp;quot; Connection
Science, 1, 275-292.
Kimball, J. (1973). &amp;quot;Seven principles of
surface structure parsing in natural
language.&amp;quot; Cognition, 2, 15-47.
Klein, E., and van Benthem, r. (eds.) (1987).
Categories, Polymorphism and Unification.
Centre for Cognitive Science, University
of Edinburgh, and Institute for Language,
Logic and Information, University of
Amsterdam.
Koller, W. (1977). Redensarten: Linguistische
Aspekte, Vorkommensanalysen, Sprachspiel.
Tubingen: Niemeyer.
Lambek, J. (1958). &amp;quot;The mathematics of
sentence structure.&amp;quot; Am. Math. Monthly,
65, 154-169.
Leslie, N. (1990). &amp;quot;Contrasting styles of
categorial derivations.&amp;quot; In Studies in
Categorical Grammar, edited by G. Barry
and G. Morrill, 113-126. University of
Edinburgh Press.
Van der Linden, E. (1991). &amp;quot;Idioms,
non-literal language and knowledge
representation.&amp;quot; In Proceedings,
IJCAI-Workshop Computational Approaches to
Non-literal Language.
Van der Linden, E., and Kraaij, W. (1990).
&amp;quot;Ambiguity resolution and the retrieval
of idioms: Two approaches.&amp;quot; In
Proceedings, COLING 1990, Vol. 2, 245-251.
</reference>
<page confidence="0.966482">
236
</page>
<note confidence="0.586997">
Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon
</note>
<reference confidence="0.999823368852459">
Van der Linden, E., and Minnen, G. (1990).
&amp;quot;Algorithms for generation in Lambek
theorem proving.&amp;quot; In Proceedings, ACL
1990, 220-226.
Marcus, M. (1980). A Theory of Syntactic
Recognition for Natural Language.
Cambridge, MA: The MIT Press.
Marslen-Wilson, W., and Tyler, L. (1980).
&amp;quot;The temporal structure of spoken
language understanding.&amp;quot; Cognition, 8,
1-71.
Moortgat, M. (1987). &amp;quot;Lambek theorem
proving.&amp;quot; In Categories, Polymorphism and
Unification, edited by E. Klein and J. van
Benthem, 169-200. University of
Edinburgh and University of Amsterdam.
Moortgat, M. (1988). Categorial Investigations,
Logical and Linguistic Aspects of the Lambek
Calculus. Doctoral dissertation, University
of Amsterdam.
Moortgat, M. (1990). &amp;quot;Categorial logics: A
computational perspective.&amp;quot; In
Computation in the Netherlands, edited by
A. J. van de Goor, 329-347.
Moortgat, M. (1992). &amp;quot;The logic of
discontinuous type constructors.&amp;quot; In
Discontinuous Constituency, edited by
W. Sijtsma, and A. van Horck. Berlin:
Mouton de Gruyter.
Morrill, G. (1990). &amp;quot;Grammar and logical
types.&amp;quot; In Studies in Categorical Grammar,
edited by G. Barry and G. Morrill,
127-148. University of Edinburgh Press.
Morrill, G.; Leslie, N.; Hepple, M.; and
Barry, G. (1990). &amp;quot;Categorial deductions
and structural operations.&amp;quot; In Studies in
Categorical Grammar, edited by G. Barry
and G. Morrill, 1-21.
Oehrle, R.; Bach, E.; and Wheeler, D. (eds).
(1988). Categorial Grammar and Natural
Language Structure. Dordrecht: Reidel.
Pollard, Carl, and Sag, Ivan A. (1987).
Information-based Syntax and Semantics,
Vol. 1, CSLI, Stanford.
Ristad, E. (1990). Computational Structure of
Human Language. Doctoral dissertation,
Department of Electrical Engineering and
Computer Science, MIT.
Roorda, D. (1990). &amp;quot;Proofnets for Lambek
calculus.&amp;quot; Ms. University of Amsterdam.
Schraw, G.; Trathen, W.; Reynolds, R.; and
Lapan R. (1988). &amp;quot;Preferences for idioms:
Restrictions due to lexicalization and
familiarity.&amp;quot; Journal of Psycholinguistic
Research, 17,413-424.
Schubert, L. (1984). &amp;quot;On parsing
preferences.&amp;quot; In Proceedings, COLING 1984,
247-250.
Schubert, L. (1986). &amp;quot;Are there preference
trade-offs in attachment decisions?&amp;quot; In
Proceedings, AAAI-86, 601-605.
Schweigert, W. (1986). &amp;quot;The comprehension
of familiar and less familiar idioms.&amp;quot;
Journal of Psycholinguistic Research, 15,
33-45.
Schweigert, W., and Moates, D. (1988).
&amp;quot;Familiar idiom comprehension.&amp;quot; Journal
of Psycholinguistic Research,17, 281-296.
Shieber, S. (1983). &amp;quot;Sentence disambiguation
by shift-reduce parsing technique.&amp;quot; In
Proceedings, IJCAI 1983, 699-703.
Small, S.; Cottrell, G.; and Tanenhaus, M.
(eds.) (1988). Lexical Ambiguity Resolution.
San Mateo: Kaufmann.
Steedman, M. (1987). &amp;quot;Combinatory
grammars and parasitic gaps.&amp;quot; In Working
Papers in Cognitive Science, Volume 1.
Categorical Grammar, Unification Grammar
and Parsing, edited by N. Haddock,
E. Klein, and G. Morrill, 30-70. Centre for
Cognitive Science, University of
Edinburgh.
Stock, 0. (1989). &amp;quot;Parsing with flexibility,
dynamic strategies, and idioms in mind.&amp;quot;
Computational Linguistics, 15,1-19.
Swinney, D. (1979). &amp;quot;Lexical access during
sentence comprehension:
(Re)consideration of context effects.&amp;quot;
Journal of Verbal Learning and Verbal
Behaviour, 18,645-659.
Swinney, D. (1981). &amp;quot;Lexical processing
during sentence comprehension: Effects of
higher order constraints and implications
for representation.&amp;quot; In The Cognitive
Representation of Speech, edited by
T. Meyers, J. Laver, and J. Anderson.
North-Holland.
Taraban, R., and McClelland, J. (1988).
&amp;quot;Constituent attachment and thematic
role assignment in sentence processing:
Influences of content-based expectations.&amp;quot;
Journal of Memory and Language, 27,
597-632.
Touretzky, D. (1986). The Mathematics of
Inheritance Systems. Los Altos, CA:
Morgan Kaufmann Publishers.
Touretzky, D.; Horty, J.; and Thomason, R.
(1987). &amp;quot;A clash of intuitions: The current
state of non-monotonic multiple
inheritance systems.&amp;quot; In Proceedings, IJCAI
1987, 476-482.
Veltman, F. (1990). &amp;quot;Defaults in update
semantics I.&amp;quot; In Conditionals, Defaults and
Belief Revision, edited by H. Kamp, 28-63.
DYANA Deliverable R2.5.A.
Whittemore, G.; Ferrara, K.; and Brunner, H.
(1990). &amp;quot;Empirical study of predictive
powers of simple attachment schemes for
post-modifier prepositional phrases.&amp;quot; In
Proceedings, ACL 1990, 23-30.
Wilensky, R., and Arens, Y. (1980).
&amp;quot;PHRAN, A knowledge-based natural
</reference>
<page confidence="0.961747">
237
</page>
<reference confidence="0.995552666666667">
Computational Linguistics Volume 18, Number 2
language understander.&amp;quot; In Proceedings, 779-784.
ACL 1980, 117-121. Zernik, U., and Dyer, M. (1987). &amp;quot;The
Wilks, Y.; Huang, X.; and Fass, D. (1985). self-extending phrasal lexicon.&amp;quot;
&amp;quot;Syntax, preference and right Computational Linguistics, 13, 308-327.
attachment.&amp;quot; In Proceedings, IJCAI 1985,
</reference>
<page confidence="0.997014">
238
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.890839">
<title confidence="0.981863">Incremental Processing and the Hierarchical Lexicon</title>
<author confidence="0.973382">Erik-Jan van_der_Linden</author>
<affiliation confidence="0.999926">Tilburg University</affiliation>
<abstract confidence="0.99278425">Hierarchical lexicon structures are not only of great importance for the nonredundant representation of lexical information, they may also contribute to the efficiency of the actual processing of natural language. Two parsing techniques that render the parsing process efficient are prea technique for incrementally accessing the hierarchical lexicon. preferences within the parsing process as a natural consequence of the hierarchical structure of the lexicon. Within a proof-theoretic approach to Categorial Grammar it is possible to implement these techniques in a formal and principled way. Special attention is paid to idiomatic expressions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abeille</author>
</authors>
<title>Lexical and syntactic rules in a tree adjoining grammar.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, ACL</booktitle>
<pages>292--298</pages>
<contexts>
<context position="12611" citStr="Abeille 1990" startWordPosition="1943" endWordPosition="1944">es. The verb that is the head of an idiomatic expression has the same inflectional paradigm as the verb outside the expression: for instance, if a verb is strong outside an idiom, it is strong within the idiom. 3.1.3 Syntactic Behavior. The syntactic behavior of idioms should partly be explained in terms of properties of their heads. For example, it is not possible to form a passive on the basis of predicative and copulative verbs, either inside or outside an idiomatic expression.3 This information is inherited by the idiom from its verbal head. 2 Similar representations can be found for TAG (Abeille 1990; Abeille and Schabes 1989) and HPSG (Erbach 1991). 3 See van der Linden (1991). 222 Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon 3.1.4 Semantics. The traditional definition of an idiom states that its meaning is not a function of the meanings of its parts and the way these are syntactically combined; that is, an idiom is a noncompositional expression. Under this definition, their meaning can be subject to any other principle that describes in what way the meaning of an expression should be derived (contextuality, meaning postulates...). A definition that states </context>
</contexts>
<marker>Abeille, 1990</marker>
<rawString>Abeille, A. (1990). &amp;quot;Lexical and syntactic rules in a tree adjoining grammar.&amp;quot; In Proceedings, ACL 1990, 292-298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Abeille</author>
<author>Y Schabes</author>
</authors>
<title>Parsing idioms in lexicalized TAGs.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, EACL</booktitle>
<pages>1--9</pages>
<contexts>
<context position="12638" citStr="Abeille and Schabes 1989" startWordPosition="1945" endWordPosition="1948">hat is the head of an idiomatic expression has the same inflectional paradigm as the verb outside the expression: for instance, if a verb is strong outside an idiom, it is strong within the idiom. 3.1.3 Syntactic Behavior. The syntactic behavior of idioms should partly be explained in terms of properties of their heads. For example, it is not possible to form a passive on the basis of predicative and copulative verbs, either inside or outside an idiomatic expression.3 This information is inherited by the idiom from its verbal head. 2 Similar representations can be found for TAG (Abeille 1990; Abeille and Schabes 1989) and HPSG (Erbach 1991). 3 See van der Linden (1991). 222 Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon 3.1.4 Semantics. The traditional definition of an idiom states that its meaning is not a function of the meanings of its parts and the way these are syntactically combined; that is, an idiom is a noncompositional expression. Under this definition, their meaning can be subject to any other principle that describes in what way the meaning of an expression should be derived (contextuality, meaning postulates...). A definition that states what the meaning is, is pre</context>
</contexts>
<marker>Abeille, Schabes, 1989</marker>
<rawString>Abeille, A., and Schabes, Y. (1989). &amp;quot;Parsing idioms in lexicalized TAGs.&amp;quot; In Proceedings, EACL 1989, 1-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ades</author>
<author>M Steedman</author>
</authors>
<title>On the order of words.&amp;quot;</title>
<date>1982</date>
<journal>Linguistics and Philosophy,</journal>
<volume>4</volume>
<pages>517--558</pages>
<contexts>
<context position="21519" citStr="Ades and Steedman 1982" startWordPosition="3335" endWordPosition="3338">s been parsed: only then can the vp be formed and combined with the subject to form an s (Briscoe 1987). A process such as this cannot be called incremental. Although subject and verb can be processed incrementally independently from each other, this is not the case for their combination. The strategy mostly used in incremental CG-processing is to enable the construction of a semantic structure with the use of principles that concatenate all possible adjacent categories (although some exceptions are made for coordinate structures [Dowty 1988; Houtman 1987]). In Combinatory Categorial Grammar (Ades and Steedman 1982; Steedman 1987), for instance, composition and lifting rules (Definitions 3 and 4) enable incremental interpretation (Example 6). To make use of these rules in a proof-theoretic approach to CG, a rule that cuts the result of these rules in the proof as a whole (Definition 5) is necessary. Definition 3 (X /Y,a) (Y / Z,b) (X /Z, Ax.a(b(x))) [Comp] Definition 4 (X,a) = (Z/ (X \ Z), )b.b(a)) [Lift] 6 The first parser that featured incremental processing can be found in Marcus (1980). This parser did not consider lexical ambiguity and confined itself to syntactic processing. Other computational mo</context>
<context position="28368" citStr="Ades and Steedman (1982)" startWordPosition="4391" endWordPosition="4394">gories by means of the product operator &apos;*&apos;. During later stages of incremental processing this product formula is taken apart, and the parts are used for constructing a semantic representation. Bouma notes with respect to his P-calculus that connecting two semantic representations with a &apos;*&apos; can hardly be called building up a semantic representation. 8 Haddock (1987) proposes a &apos;reduce-first&apos; strategy for the incremental categorial parsing. It &amp;quot;(... ) will always reduce, remembering the shift option as an alternative which could be chosen in the event of backtracking&amp;quot; (Haddock 1987, p. 75) 9 Ades and Steedman (1982) state this explicitly. 227 Computational Linguistics Volume 18, Number 2 Example 7 John gave np ((np\s)/pp)/npn(np\s)/ppA(np\s) The problem that faces the parser here is that it is forced to choose one of the subcategorization frames to make this step in the derivation. There is, however, no indication which frame should be selected. In the case an incorrect frame is selected, backtracking is necessary when further material in the input is contradictory with this frame. For instance, the choice of a frame without a direct object will lead to a semantic representation that includes the binding</context>
</contexts>
<marker>Ades, Steedman, 1982</marker>
<rawString>Ades, A., and Steedman, M. (1982). &amp;quot;On the order of words.&amp;quot; Linguistics and Philosophy, 4, 517-558.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Adriaens</author>
</authors>
<title>Process Linguistics. Doctoral dissertation,</title>
<date>1986</date>
<institution>University of Leuven.</institution>
<contexts>
<context position="22484" citStr="Adriaens (1986)" startWordPosition="3487" endWordPosition="3488">efinition 4 (X,a) = (Z/ (X \ Z), )b.b(a)) [Lift] 6 The first parser that featured incremental processing can be found in Marcus (1980). This parser did not consider lexical ambiguity and confined itself to syntactic processing. Other computational models that entail the notion of incrementality can be found for Segment Grammar and in Word Expert Parsing. The subsymbolic processing architecture for Segment Grammar presented by Kempen and Vosse (1989) is a model of syntactic processing. The architecture allows for immediate interpretation, but no semantic representation is actually constructed. Adriaens (1986) presents a lexicalist model, Word Expert Parsing, which operates incrementally. Another lexicalist model that features incremental processing can be found in Stock (1989). In none of the models is mention made of a structured lexicon. 225 Computational Linguistics Volume 18, Number 2 Definition 5 U, X, Y, V Z [Cut] if X, Y = W and U, W, V Z Example 6 (np,john) ((np \ s)/np,kicks) (np/n,the) (n,boy) = (s, kicks(the(boy))(john)) [Cut] if (np,john) (s/(np \ s), AX.X(john) ) [Lift] and (s/(np \ s), )X.X(john) ) ((np \ s)/np,kicks) (np/n,the) (n,boy) (s, kicks(the(boy))(john)) [Cut] if (s/(np \ s)</context>
</contexts>
<marker>Adriaens, 1986</marker>
<rawString>Adriaens, G. (1986). Process Linguistics. Doctoral dissertation, University of Leuven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Aronoff</author>
</authors>
<title>Word Formation in Generative Grammar.</title>
<date>1976</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="15273" citStr="Aronoff 1976" startWordPosition="2364" endWordPosition="2365"> properties of a node (Touretzky, Horty, and Thomason 1987; Touretzky 1986; Veltman 1990).5 More specific information thus takes precedence over more general information. This is a common feature of inheritance systems, and is an application of &apos;proper inclusion precedence,&apos; which is acknowledged in knowledge representation and (computational) linguistics (De Smedt 1990; Daelemans 1987; other papers in this special issue). There exists a clear relation between this principle and the linguistic notion blocking. Blocking is &amp;quot;the nonoccurrence of one form due to the simple existence of another&amp;quot; (Aronoff 1976, p. 41). For instance, the nominal derivation *graciosity of gracious is blocked by the existence of grace. Daelemans (1987) and De Smedt (1990) show that in a hierarchical lexicon structure, blocking is equivalent to the prevalence of more specific information over more general information. For instance, the more general principle in the example is that a nominal derivation of some abstract adjectives equals stem + ity, and the more specific information is that in the case of gracious the nominal derivation is grace. In the hierarchical lexicon, the principle of priority to the instance 4 Se</context>
</contexts>
<marker>Aronoff, 1976</marker>
<rawString>Aronoff, M. (1976). Word Formation in Generative Grammar. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Barry</author>
<author>G Morrill</author>
</authors>
<date>1990</date>
<booktitle>Studies in Categorial Grammar.</booktitle>
<publisher>University of Edinburgh Press.</publisher>
<contexts>
<context position="4318" citStr="Barry and Morrill 1990" startWordPosition="642" endWordPosition="645">he Lambek Calculus Recently, proof theory has aroused the interest of categorial grammarians. In the Lambek calculus (L-calculus, L; Lambek 1958), the most salient example of the application of proof theory to categorial grammar, the rules of the grammar become a set of axioms and inference rules. Together, these form a logical calculus in which parsing of a syntagm is an attempt to prove that it follows as a theorem from the set of axioms and inference rules. Following the work of Van Benthem (1986) and Moortgat (1988; 1987) the Lambek calculus has become popular among a number of linguists (Barry and Morrill 1990; Hendriks 1987). Categories in CG can either be basic (np, s, n) or complex. A complex category consists of a binary category forming connective and two categories, for instance, np\s. In the product-free L-calculus the set of connectives (also called type constructors) is { \ /}. A complex category is a functor, an incomplete expression that forms a result category if an argument category is found. Throughout this paper the Lambek notation, in which the argument category is found under the slash, is applied. Consider for example the categorial representation of an intransitive verb: np\s loo</context>
<context position="6236" citStr="Barry and Morrill 1990" startWordPosition="931" endWordPosition="934">. The L-calculus extends the power of categorial grammar basically because it adds so-called introduction rules to the proof-theoretic complements of categorial reduction rules, elimination rules. For each category-forming connective, introduction and elimination rules can be formulated. With respect to semantics, elimination corresponds to functional application and introduction to lambda abstraction. Various approaches have been proposed for deduction in L. In its standard representation the L-calculus is a sequent calculus. More recently, natural deduction has been applied to the calculus (Barry and Morrill 1990), as well as proof procedures from linear logic (Roorda 1990).1 Throughout this article the sequent format is used. In definition (1), W and X are categories, Y and Z are signs, and P. T, Q, U and V are sequences of signs, where P. T, and Q are nonempty. A sequent in L represents a derivability relation, z, between a nonempty finite sequence of signs, the antecedent, and a sign, the succedent. A sequent states that the string denoted by the antecedent is in the set of strings denoted by the succedent. The axioms and inference rules of the calculus define the theorems of the calculus with respe</context>
</contexts>
<marker>Barry, Morrill, 1990</marker>
<rawString>Barry, G., and Morrill, G. (eds.) (1990). Studies in Categorial Grammar. University of Edinburgh Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Barry</author>
<author>M Pickering</author>
</authors>
<title>Dependency and constituency in categorial grammar.&amp;quot;</title>
<date>1990</date>
<booktitle>In Studies in Categorical Grammar,</booktitle>
<pages>23--45</pages>
<publisher>University of Edinburgh Press.</publisher>
<note>edited by</note>
<contexts>
<context position="32252" citStr="Barry and Pickering 1990" startWordPosition="5027" endWordPosition="5030">ence rule that eliminates the inheritance operator has three instances. In the first case, the sign on top of the lexical hierarchy combines with an argument sign 10 No introduction rules are presented since these would allow inheritance connectives to be introduced in a proof syntactically, whereas they can only originate lexically (cf. the /-operator in Hepple [19901): a sequent of the form A B = A&gt;B would come down to the question of whether two unrelated signs could maintain an inheritance relation that is not stipulated in the lexicon. 11 Inclusion of a notion of dependency constituency (Barry and Pickering 1990) excludes strings such as John loves the from being a constituent in contrast to the original M-calculus. 228 Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon in the input (this rule has a right-looking counterpart). In the second case, the result of the elimination of &gt;&gt; is the daughter. In the third case, the result is the mother. In line with representational nonautonomy these rules describe what the processor does while assembling a semantic representation. In Example 8 an example is presented. The prosodic terms are left out for reasons of clarity. Definition 7 </context>
</contexts>
<marker>Barry, Pickering, 1990</marker>
<rawString>Barry, G., and Pickering, M. (1990). &amp;quot;Dependency and constituency in categorial grammar.&amp;quot; In Studies in Categorical Grammar, edited by G. Barry and G. Morrill, 23-45. University of Edinburgh Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J van Benthem</author>
</authors>
<title>Categorial grammar.&amp;quot;</title>
<date>1986</date>
<booktitle>In Essays in Logical Semantics,</booktitle>
<location>Dordrecht: Reidel.</location>
<note>edited by</note>
<contexts>
<context position="4201" citStr="Benthem (1986)" startWordPosition="625" endWordPosition="626">es for the interpretation of idioms are discussed in particular. 2. Categorial Grammar and Proof Theory 2.1 The Lambek Calculus Recently, proof theory has aroused the interest of categorial grammarians. In the Lambek calculus (L-calculus, L; Lambek 1958), the most salient example of the application of proof theory to categorial grammar, the rules of the grammar become a set of axioms and inference rules. Together, these form a logical calculus in which parsing of a syntagm is an attempt to prove that it follows as a theorem from the set of axioms and inference rules. Following the work of Van Benthem (1986) and Moortgat (1988; 1987) the Lambek calculus has become popular among a number of linguists (Barry and Morrill 1990; Hendriks 1987). Categories in CG can either be basic (np, s, n) or complex. A complex category consists of a binary category forming connective and two categories, for instance, np\s. In the product-free L-calculus the set of connectives (also called type constructors) is { \ /}. A complex category is a functor, an incomplete expression that forms a result category if an argument category is found. Throughout this paper the Lambek notation, in which the argument category is fo</context>
</contexts>
<marker>Benthem, 1986</marker>
<rawString>Benthem, J. van (1986). &amp;quot;Categorial grammar.&amp;quot; In Essays in Logical Semantics, edited by J. van Benthem. Dordrecht: Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma</author>
</authors>
<title>Efficient processing of flexible categorial grammar.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, EACL</booktitle>
<contexts>
<context position="27565" citStr="Bouma 1989" startWordPosition="4270" endWordPosition="4271">ral subcategorization frames. During incremental processing, one of the subcategorization frames of an ambiguous word has to be selected. How this choice is made is unclear in most categorial work that claims to model incremental processing: ambiguity is not an issue.9 With the use of operators like A the ambiguity can at least be described, but truly incremental processing does not seem possible: the all-or-none immediacy leads to a unrealistic parsing process. An example will illustrate this. In Example 7, part of the derivation of John gave a book to Mary is presented. 7 In the P-calculus (Bouma 1989) a shift-reduce strategy is modeled for a categorial parser. Reduction corresponds to the application of a categorial reduction rule; a shift is represented by connecting two categories by means of the product operator &apos;*&apos;. During later stages of incremental processing this product formula is taken apart, and the parts are used for constructing a semantic representation. Bouma notes with respect to his P-calculus that connecting two semantic representations with a &apos;*&apos; can hardly be called building up a semantic representation. 8 Haddock (1987) proposes a &apos;reduce-first&apos; strategy for the increme</context>
</contexts>
<marker>Bouma, 1989</marker>
<rawString>Bouma, G. (1989). &amp;quot;Efficient processing of flexible categorial grammar.&amp;quot; In Proceedings, EACL 1989, 19-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma</author>
</authors>
<title>Defaults in unification grammar.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, ACL</booktitle>
<pages>165--172</pages>
<contexts>
<context position="10362" citStr="Bouma 1990" startWordPosition="1585" endWordPosition="1586">eason&apos; about lexical items. This is in line 221 Computational Linguistics Volume 18, Number 2 with the principle of representational nonautonomy, which states that syntactic rules describe what the processor does while assembling a semantic representation. 3. Inheritance and the Hierarchical Lexicon To allow the inference engine to reason about lexical structures in which inheritance relations are present, the calculus should be extended, and a more sophisticated structure should be assigned to the categorial lexicon than the list or bag that is usually considered in CG (with the exception of Bouma 1990). The current section deals with the lexicon; the next sections deal with the extension of the calculus. 3.1 An Example: Idioms An idiomatic expression and its verbal head can be said to maintain a lexical inheritance relation: an idiomatic expression inherits part of its properties from its head. Here, syntactic category, syntactic behavior, morphology, and semantics are discussed briefly. 3.1.1 Syntactic Category. Idiomatic expressions can be represented as functor-argument-structures2 and have the same format as the verbs that are their heads. It is therefore possible to relate the syntacti</context>
<context position="13761" citStr="Bouma 1990" startWordPosition="2125" endWordPosition="2126">textuality, meaning postulates...). A definition that states what the meaning is, is preferable: the meaning of an idiom is exclusively a property of the whole expression.4 The meaning of the idiom cannot be inherited from the verb that is its head, but should be added nonmonotonically. Example 3 a. KICK: (KICK_TV &gt;-- KICK_THE_BUCKET, kick, AxAykick(x)(y)) b. KICK_TV: ((np\s)/np, _) c. KICK_THE_BUCKET: (_, the + bucket, _), AxAydie(y)) 3.1.5 Inheritance. The full specification of a sign is derived by means of an operation similar to priority union (Kaplan 1987, p. 180) or default unification (Bouma 1990), denoted by n. n is defined as a function from pairs of mother and daughter signs to fully specified daughter signs and runs as follows. If unification, U, is successful for the values of a certain property of mother and daughter, the result of n for that value is the result of U, where unification is understood in its most basic sense: variables unify with constants and variables; constants unify with variables and with constants with an equal value (prosodic information in Example 4). If the values do not unify, the value of the daughter is returned (semantic information in Example 4). Exam</context>
</contexts>
<marker>Bouma, 1990</marker>
<rawString>Bouma, G. (1990). &amp;quot;Defaults in unification grammar.&amp;quot; In Proceedings, ACL 1990, 165-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
</authors>
<title>Modelling Human Speech Comprehension.</title>
<date>1987</date>
<publisher>Ellis Horwood.</publisher>
<location>Chichester:</location>
<contexts>
<context position="21000" citStr="Briscoe 1987" startWordPosition="3259" endWordPosition="3260">it allows for flexible constituent structures: any two signs can be combined to form a larger informational unit. For a parsing process to be incremental, it should reduce two constituents if these maintain a head-argument relation. The incremental construction of analyses for sentences with the use of phrase structure grammars is not in all cases possible. For example, in case the input consists of a subject and a transitive verb it is only possible to integrate these two into a sentence if the object has been parsed: only then can the vp be formed and combined with the subject to form an s (Briscoe 1987). A process such as this cannot be called incremental. Although subject and verb can be processed incrementally independently from each other, this is not the case for their combination. The strategy mostly used in incremental CG-processing is to enable the construction of a semantic structure with the use of principles that concatenate all possible adjacent categories (although some exceptions are made for coordinate structures [Dowty 1988; Houtman 1987]). In Combinatory Categorial Grammar (Ades and Steedman 1982; Steedman 1987), for instance, composition and lifting rules (Definitions 3 and </context>
</contexts>
<marker>Briscoe, 1987</marker>
<rawString>Briscoe, E. (1987). Modelling Human Speech Comprehension. Chichester: Ellis Horwood.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chafe</author>
</authors>
<title>Idiomaticity as an anomaly in the Chomskyan paradigm.&amp;quot;</title>
<date>1968</date>
<journal>Foundations of Language,</journal>
<volume>4</volume>
<pages>109--127</pages>
<contexts>
<context position="42566" citStr="Chafe 1968" startWordPosition="6589" endWordPosition="6590">Mary. attempt: n &gt;&gt; (n/ (np, to + _)\sto_inf)) Example 14 Sue had difficulties with her teachers. difficulties: n &gt;&gt; (n/ (pp,with + -)) Example 15 a. John met the girl that he married at a dance. b. John married the girl that he met at a dance. marry: ((np \s)/np) met ((np\s)/np) (((np\s)/ pp)/ np) 5.3 Idioms and Parsing Preferences 5.3.1 Conventionality and Idiom Processing. Idiomatic expressions can in most cases be interpreted nonidiomatically as well.&amp;quot; It has, however, frequently been observed that an idiomatic phrase should very rarely be interpreted nonidiomatically (Koller 1977, p. 13; Chafe 1968, p. 123; Gross 1984, p. 278; Swinney 1981, p. 208). Also, psycholinguistic research indicates that in case of ambiguity there is clear preference for the idiomatic reading (Gibbs 1980; Schraw et al. 1988; Schweigert 1986; Schweigert and Moates 1988). The phenomenon that phrases should be interpreted according to their idiomatic, noncompositional, lexical, conventional meaning will be referred to as the &apos;conventionality&apos; principle (Gibbs 1980). The application of this principle is not limited to idioms. For instance, compounds are not interpreted compositionally, but according to the lexical, </context>
</contexts>
<marker>Chafe, 1968</marker>
<rawString>Chafe, W. (1968). &amp;quot;Idiomaticity as an anomaly in the Chomskyan paradigm.&amp;quot; Foundations of Language, 4, 109-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S CraM</author>
<author>M Steedman</author>
</authors>
<title>On not being led up the garden path.&amp;quot;</title>
<date>1982</date>
<journal>In Natural Language Parsing, edited</journal>
<pages>320--358</pages>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge:</location>
<marker>CraM, Steedman, 1982</marker>
<rawString>CraM, S., and Steedman, M. (1982). &amp;quot;On not being led up the garden path.&amp;quot; In Natural Language Parsing, edited by D. Dowty, L. Karttunen, and A. Zwicky, 320-358. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
</authors>
<title>Studies in Language Technology: An Object-oriented Model of Morphophonological Aspects of Dutch. Doctoral dissertation,</title>
<date>1987</date>
<institution>University of Leuven.</institution>
<contexts>
<context position="15049" citStr="Daelemans 1987" startWordPosition="2328" endWordPosition="2329">kick, AxAydie(y)) The inheritance networks for which n is defined are unipolar, nonmonotonic, and homogeneous (Touretzky, Horty, and Thomason 1987). For other networks, other reasoning mechanisms are necessary to determine the properties of a node (Touretzky, Horty, and Thomason 1987; Touretzky 1986; Veltman 1990).5 More specific information thus takes precedence over more general information. This is a common feature of inheritance systems, and is an application of &apos;proper inclusion precedence,&apos; which is acknowledged in knowledge representation and (computational) linguistics (De Smedt 1990; Daelemans 1987; other papers in this special issue). There exists a clear relation between this principle and the linguistic notion blocking. Blocking is &amp;quot;the nonoccurrence of one form due to the simple existence of another&amp;quot; (Aronoff 1976, p. 41). For instance, the nominal derivation *graciosity of gracious is blocked by the existence of grace. Daelemans (1987) and De Smedt (1990) show that in a hierarchical lexicon structure, blocking is equivalent to the prevalence of more specific information over more general information. For instance, the more general principle in the example is that a nominal derivati</context>
</contexts>
<marker>Daelemans, 1987</marker>
<rawString>Daelemans, W. (1987). Studies in Language Technology: An Object-oriented Model of Morphophonological Aspects of Dutch. Doctoral dissertation, University of Leuven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K De Smedt</author>
</authors>
<title>Incremental Sentence Generation. Doctoral dissertation,</title>
<date>1990</date>
<institution>University of Nijmegen.</institution>
<marker>De Smedt, 1990</marker>
<rawString>De Smedt, K. (1990). Incremental Sentence Generation. Doctoral dissertation, University of Nijmegen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dowty</author>
</authors>
<title>Word Meaning and Montague Grammar.</title>
<date>1979</date>
<location>Dordrecht: Reidel.</location>
<contexts>
<context position="17609" citStr="Dowty 1979" startWordPosition="2728" endWordPosition="2729">f the optionality operator ? (((np\s)/?np)) would imply that kick is in principle an intransitive verb, that has one optional argument, whereas in fact the reverse is true: kick is a two-placefunctor of which one argument may be left unspecified syntactically. The transitive and intransitive verb can be said to share their semantic value, but in the case of the intransitive, the syntactically unspecified object is not bound by a )-operator but by an (informationally richer) existential quantor. The transition from the transitive to the intransitive is represented as a lexical type-transition (Dowty 1979, p. 308). Definition 2 (detransitivization) detrans : AxAyD(x)(y) AyaD(x)(y) From a syntactic point of view, the transitive form of the verb can be said to inherit the syntactic information from the intransitive and to add a syntactic argument. From a semantic point of view, the transitive inherits the semantic information that is specified for the KICK entry as a whole. The intransitive inherits the same information and stipulates application of detransitivization to it. The lexical relation between the transitive and the intransitive is thus different from that between a verb and an idiom: </context>
</contexts>
<marker>Dowty, 1979</marker>
<rawString>Dowty, R. (1979). Word Meaning and Montague Grammar. Dordrecht: Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dowty</author>
</authors>
<title>Type raising, functional composition and non-constituent conjunction.&amp;quot;</title>
<date>1988</date>
<booktitle>In Categorical Grammar and Natural Language Structure,</booktitle>
<pages>153--197</pages>
<location>Dordrecht: Reidel.</location>
<note>edited by</note>
<contexts>
<context position="21444" citStr="Dowty 1988" startWordPosition="3327" endWordPosition="3328">ossible to integrate these two into a sentence if the object has been parsed: only then can the vp be formed and combined with the subject to form an s (Briscoe 1987). A process such as this cannot be called incremental. Although subject and verb can be processed incrementally independently from each other, this is not the case for their combination. The strategy mostly used in incremental CG-processing is to enable the construction of a semantic structure with the use of principles that concatenate all possible adjacent categories (although some exceptions are made for coordinate structures [Dowty 1988; Houtman 1987]). In Combinatory Categorial Grammar (Ades and Steedman 1982; Steedman 1987), for instance, composition and lifting rules (Definitions 3 and 4) enable incremental interpretation (Example 6). To make use of these rules in a proof-theoretic approach to CG, a rule that cuts the result of these rules in the proof as a whole (Definition 5) is necessary. Definition 3 (X /Y,a) (Y / Z,b) (X /Z, Ax.a(b(x))) [Comp] Definition 4 (X,a) = (Z/ (X \ Z), )b.b(a)) [Lift] 6 The first parser that featured incremental processing can be found in Marcus (1980). This parser did not consider lexical am</context>
</contexts>
<marker>Dowty, 1988</marker>
<rawString>Dowty, R. (1988). &amp;quot;Type raising, functional composition and non-constituent conjunction.&amp;quot; In Categorical Grammar and Natural Language Structure, edited by R. Oehrle, E. Bach, and D. Wheeler, 153-197. Dordrecht: Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erbach</author>
</authors>
<title>Lexical representation of idioms.&amp;quot;</title>
<date>1991</date>
<tech>IWBS report 169, IBM TR-80.91-023,</tech>
<location>IBM, Germany.</location>
<contexts>
<context position="12661" citStr="Erbach 1991" startWordPosition="1951" endWordPosition="1952">ession has the same inflectional paradigm as the verb outside the expression: for instance, if a verb is strong outside an idiom, it is strong within the idiom. 3.1.3 Syntactic Behavior. The syntactic behavior of idioms should partly be explained in terms of properties of their heads. For example, it is not possible to form a passive on the basis of predicative and copulative verbs, either inside or outside an idiomatic expression.3 This information is inherited by the idiom from its verbal head. 2 Similar representations can be found for TAG (Abeille 1990; Abeille and Schabes 1989) and HPSG (Erbach 1991). 3 See van der Linden (1991). 222 Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon 3.1.4 Semantics. The traditional definition of an idiom states that its meaning is not a function of the meanings of its parts and the way these are syntactically combined; that is, an idiom is a noncompositional expression. Under this definition, their meaning can be subject to any other principle that describes in what way the meaning of an expression should be derived (contextuality, meaning postulates...). A definition that states what the meaning is, is preferable: the meaning of</context>
</contexts>
<marker>Erbach, 1991</marker>
<rawString>Erbach, G. (1991). &amp;quot;Lexical representation of idioms.&amp;quot; IWBS report 169, IBM TR-80.91-023, IBM, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
</authors>
<title>Lexical Rules in the Hierarchical Lexicon. Doctoral dissertation,</title>
<date>1987</date>
<institution>Stanford University.</institution>
<contexts>
<context position="18845" citStr="Flickinger (1987)" startWordPosition="2922" endWordPosition="2923">idiom a syntactic argument is further instantiated whereas here a syntactic argument is added. To represent this distinction, a different connective is used: &gt;. With the use of this type constructor, the intransitive and the transitive can be placed in an inheritance relation (as seen in Example 5). &gt;&gt; is a category forming connective which takes two signs to form a category. Example 5 a. KICK: (KICK IV &gt;&gt; KICK_TV , kick, AxAykick(x)(y)) c. KICK_IV: (np\s, detrans(KICK)) b. KICK_TV: (synt(KICK_IV)/np, sem(KICK)) The lexical structure presented here can be considered equal to that presented by Flickinger (1987) and Pollard and Sag (1987) for HPSG. They present a hierarchy in which not only transitive and intransitive verbs, but other classes of verbs are represented as well. A minor difference is that Flickinger and Pollard and Sag place classes of verbs in hierarchical relations, whereas here individual verbs maintain inheritance 224 Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon relations. The main difference with this and other previous approaches is that with the introduction of connectives for inheritance relations, inference rules for these connectives can be prese</context>
</contexts>
<marker>Flickinger, 1987</marker>
<rawString>Flickinger, D. (1987). Lexical Rules in the Hierarchical Lexicon. Doctoral dissertation, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ford</author>
<author>J Bresnan</author>
<author>R Kaplan</author>
</authors>
<title>A competence-based theory of syntactic closure.&amp;quot;</title>
<date>1982</date>
<booktitle>In The Mental Representation of Grammatical Relations,</booktitle>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<note>edited by</note>
<marker>Ford, Bresnan, Kaplan, 1982</marker>
<rawString>Ford, M.; Bresnan, J.; and Kaplan, R. (1982). &amp;quot;A competence-based theory of syntactic closure.&amp;quot; In The Mental Representation of Grammatical Relations, edited by J. Bresnan. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
<author>J Fodor</author>
</authors>
<title>The sausage machine: A new two-stage parsing model.&amp;quot;</title>
<date>1978</date>
<journal>Cognition,</journal>
<volume>6</volume>
<pages>291--325</pages>
<marker>Frazier, Fodor, 1978</marker>
<rawString>Frazier, L., and Fodor, J. (1978). &amp;quot;The sausage machine: A new two-stage parsing model.&amp;quot; Cognition, 6, 291-325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gibbs</author>
</authors>
<title>Spilling the beans on understanding and memory for idioms in conversation.&amp;quot;</title>
<date>1980</date>
<journal>Memory and Cognition,</journal>
<volume>8</volume>
<pages>149--156</pages>
<contexts>
<context position="42750" citStr="Gibbs 1980" startWordPosition="6619" endWordPosition="6620">at a dance. b. John married the girl that he met at a dance. marry: ((np \s)/np) met ((np\s)/np) (((np\s)/ pp)/ np) 5.3 Idioms and Parsing Preferences 5.3.1 Conventionality and Idiom Processing. Idiomatic expressions can in most cases be interpreted nonidiomatically as well.&amp;quot; It has, however, frequently been observed that an idiomatic phrase should very rarely be interpreted nonidiomatically (Koller 1977, p. 13; Chafe 1968, p. 123; Gross 1984, p. 278; Swinney 1981, p. 208). Also, psycholinguistic research indicates that in case of ambiguity there is clear preference for the idiomatic reading (Gibbs 1980; Schraw et al. 1988; Schweigert 1986; Schweigert and Moates 1988). The phenomenon that phrases should be interpreted according to their idiomatic, noncompositional, lexical, conventional meaning will be referred to as the &apos;conventionality&apos; principle (Gibbs 1980). The application of this principle is not limited to idioms. For instance, compounds are not interpreted compositionally, but according to the lexical, conventional meaning (Swinney 1981). Words are formed by regular rules, but their meaning will undergo &apos;semantic drift,&apos; obscuring the compositional nature of the complex word. If this</context>
</contexts>
<marker>Gibbs, 1980</marker>
<rawString>Gibbs, R. (1980). &amp;quot;Spilling the beans on understanding and memory for idioms in conversation.&amp;quot; Memory and Cognition, 8, 149-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gross</author>
</authors>
<title>Lexicon-grammar and the syntactic analysis of French.&amp;quot;</title>
<date>1984</date>
<booktitle>In Proceedings, COLING</booktitle>
<pages>275--282</pages>
<contexts>
<context position="42586" citStr="Gross 1984" startWordPosition="6593" endWordPosition="6594">(n/ (np, to + _)\sto_inf)) Example 14 Sue had difficulties with her teachers. difficulties: n &gt;&gt; (n/ (pp,with + -)) Example 15 a. John met the girl that he married at a dance. b. John married the girl that he met at a dance. marry: ((np \s)/np) met ((np\s)/np) (((np\s)/ pp)/ np) 5.3 Idioms and Parsing Preferences 5.3.1 Conventionality and Idiom Processing. Idiomatic expressions can in most cases be interpreted nonidiomatically as well.&amp;quot; It has, however, frequently been observed that an idiomatic phrase should very rarely be interpreted nonidiomatically (Koller 1977, p. 13; Chafe 1968, p. 123; Gross 1984, p. 278; Swinney 1981, p. 208). Also, psycholinguistic research indicates that in case of ambiguity there is clear preference for the idiomatic reading (Gibbs 1980; Schraw et al. 1988; Schweigert 1986; Schweigert and Moates 1988). The phenomenon that phrases should be interpreted according to their idiomatic, noncompositional, lexical, conventional meaning will be referred to as the &apos;conventionality&apos; principle (Gibbs 1980). The application of this principle is not limited to idioms. For instance, compounds are not interpreted compositionally, but according to the lexical, conventional meaning</context>
</contexts>
<marker>Gross, 1984</marker>
<rawString>Gross, M. (1984). &amp;quot;Lexicon-grammar and the syntactic analysis of French.&amp;quot; In Proceedings, COLING 1984, 275-282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Haddock</author>
</authors>
<title>Incremental interpretation and combinatory categorial grammar.&amp;quot; In Working Papers in Cognitive Science, Volume 1. Categorical Grammar, Unification Grammar and Parsing, edited by</title>
<date>1987</date>
<pages>71--84</pages>
<institution>Centre for Cognitive Science, University of Edinburgh.</institution>
<contexts>
<context position="25548" citStr="Haddock (1987)" startWordPosition="3942" endWordPosition="3943">two problems with this approach. Firstly it is questionable whether it agrees with the psycholinguistic notion of immediacy, and secondly it leads to an unrealistic view of the parsing process. The second point will be discussed in the following section. With respect to the first point of criticism it should in the first place be noted that immediacy as it was formulated by Just and Carpenter (1980) was only formulated for content words. The immediacy assumption states that processing of content words should be as immediate as possible. Firstly, CG has included function words under immediacy. Haddock (1987),8 for instance, states that given a domain with two rabbits that are &apos;in&apos; something, during incremental processing of the phrase the rabbit in the hat. &amp;quot;the incremental evaluation of the rabbit in the has created two distinct sets of candidates for the two NPs in the phrase&amp;quot; (Haddock 1987, p. 81) It is not clear from the psycholinguistic literature whether processing of function words takes place this way, but it is at least unintuitive: in larger domains large intermediate sets of candidates will be of little help for the interpretation in comparison to the information the constituent as a w</context>
<context position="28114" citStr="Haddock (1987)" startWordPosition="4353" endWordPosition="4354">ave a book to Mary is presented. 7 In the P-calculus (Bouma 1989) a shift-reduce strategy is modeled for a categorial parser. Reduction corresponds to the application of a categorial reduction rule; a shift is represented by connecting two categories by means of the product operator &apos;*&apos;. During later stages of incremental processing this product formula is taken apart, and the parts are used for constructing a semantic representation. Bouma notes with respect to his P-calculus that connecting two semantic representations with a &apos;*&apos; can hardly be called building up a semantic representation. 8 Haddock (1987) proposes a &apos;reduce-first&apos; strategy for the incremental categorial parsing. It &amp;quot;(... ) will always reduce, remembering the shift option as an alternative which could be chosen in the event of backtracking&amp;quot; (Haddock 1987, p. 75) 9 Ades and Steedman (1982) state this explicitly. 227 Computational Linguistics Volume 18, Number 2 Example 7 John gave np ((np\s)/pp)/npn(np\s)/ppA(np\s) The problem that faces the parser here is that it is forced to choose one of the subcategorization frames to make this step in the derivation. There is, however, no indication which frame should be selected. In the ca</context>
</contexts>
<marker>Haddock, 1987</marker>
<rawString>Haddock, N. (1987). &amp;quot;Incremental interpretation and combinatory categorial grammar.&amp;quot; In Working Papers in Cognitive Science, Volume 1. Categorical Grammar, Unification Grammar and Parsing, edited by N. Haddock, E. Klein, and G. Morrill, 71-84. Centre for Cognitive Science, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<date>1987</date>
<booktitle>Working Papers in Cognitive Science, Volume 1. Categorial Grammar, Unification Grammar and Parsing. Centre</booktitle>
<editor>Haddock, N.; Klein, E.; and Morrill, G. (eds.)</editor>
<institution>for Cognitive Science, University of Edinburgh.</institution>
<contexts>
<context position="4227" citStr="(1988; 1987)" startWordPosition="629" endWordPosition="630"> idioms are discussed in particular. 2. Categorial Grammar and Proof Theory 2.1 The Lambek Calculus Recently, proof theory has aroused the interest of categorial grammarians. In the Lambek calculus (L-calculus, L; Lambek 1958), the most salient example of the application of proof theory to categorial grammar, the rules of the grammar become a set of axioms and inference rules. Together, these form a logical calculus in which parsing of a syntagm is an attempt to prove that it follows as a theorem from the set of axioms and inference rules. Following the work of Van Benthem (1986) and Moortgat (1988; 1987) the Lambek calculus has become popular among a number of linguists (Barry and Morrill 1990; Hendriks 1987). Categories in CG can either be basic (np, s, n) or complex. A complex category consists of a binary category forming connective and two categories, for instance, np\s. In the product-free L-calculus the set of connectives (also called type constructors) is { \ /}. A complex category is a functor, an incomplete expression that forms a result category if an argument category is found. Throughout this paper the Lambek notation, in which the argument category is found under the slash, is ap</context>
<context position="15398" citStr="(1987)" startWordPosition="2384" endWordPosition="2384">ecedence over more general information. This is a common feature of inheritance systems, and is an application of &apos;proper inclusion precedence,&apos; which is acknowledged in knowledge representation and (computational) linguistics (De Smedt 1990; Daelemans 1987; other papers in this special issue). There exists a clear relation between this principle and the linguistic notion blocking. Blocking is &amp;quot;the nonoccurrence of one form due to the simple existence of another&amp;quot; (Aronoff 1976, p. 41). For instance, the nominal derivation *graciosity of gracious is blocked by the existence of grace. Daelemans (1987) and De Smedt (1990) show that in a hierarchical lexicon structure, blocking is equivalent to the prevalence of more specific information over more general information. For instance, the more general principle in the example is that a nominal derivation of some abstract adjectives equals stem + ity, and the more specific information is that in the case of gracious the nominal derivation is grace. In the hierarchical lexicon, the principle of priority to the instance 4 See van der Linden and Kraaij (1990) and van der Linden (1991) for a more extensive comparison of this definition and the tradi</context>
<context position="18845" citStr="(1987)" startWordPosition="2923" endWordPosition="2923">tactic argument is further instantiated whereas here a syntactic argument is added. To represent this distinction, a different connective is used: &gt;. With the use of this type constructor, the intransitive and the transitive can be placed in an inheritance relation (as seen in Example 5). &gt;&gt; is a category forming connective which takes two signs to form a category. Example 5 a. KICK: (KICK IV &gt;&gt; KICK_TV , kick, AxAykick(x)(y)) c. KICK_IV: (np\s, detrans(KICK)) b. KICK_TV: (synt(KICK_IV)/np, sem(KICK)) The lexical structure presented here can be considered equal to that presented by Flickinger (1987) and Pollard and Sag (1987) for HPSG. They present a hierarchy in which not only transitive and intransitive verbs, but other classes of verbs are represented as well. A minor difference is that Flickinger and Pollard and Sag place classes of verbs in hierarchical relations, whereas here individual verbs maintain inheritance 224 Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon relations. The main difference with this and other previous approaches is that with the introduction of connectives for inheritance relations, inference rules for these connectives can be prese</context>
<context position="25548" citStr="(1987)" startWordPosition="3943" endWordPosition="3943">lems with this approach. Firstly it is questionable whether it agrees with the psycholinguistic notion of immediacy, and secondly it leads to an unrealistic view of the parsing process. The second point will be discussed in the following section. With respect to the first point of criticism it should in the first place be noted that immediacy as it was formulated by Just and Carpenter (1980) was only formulated for content words. The immediacy assumption states that processing of content words should be as immediate as possible. Firstly, CG has included function words under immediacy. Haddock (1987),8 for instance, states that given a domain with two rabbits that are &apos;in&apos; something, during incremental processing of the phrase the rabbit in the hat. &amp;quot;the incremental evaluation of the rabbit in the has created two distinct sets of candidates for the two NPs in the phrase&amp;quot; (Haddock 1987, p. 81) It is not clear from the psycholinguistic literature whether processing of function words takes place this way, but it is at least unintuitive: in larger domains large intermediate sets of candidates will be of little help for the interpretation in comparison to the information the constituent as a w</context>
<context position="28114" citStr="(1987)" startWordPosition="4354" endWordPosition="4354">ok to Mary is presented. 7 In the P-calculus (Bouma 1989) a shift-reduce strategy is modeled for a categorial parser. Reduction corresponds to the application of a categorial reduction rule; a shift is represented by connecting two categories by means of the product operator &apos;*&apos;. During later stages of incremental processing this product formula is taken apart, and the parts are used for constructing a semantic representation. Bouma notes with respect to his P-calculus that connecting two semantic representations with a &apos;*&apos; can hardly be called building up a semantic representation. 8 Haddock (1987) proposes a &apos;reduce-first&apos; strategy for the incremental categorial parsing. It &amp;quot;(... ) will always reduce, remembering the shift option as an alternative which could be chosen in the event of backtracking&amp;quot; (Haddock 1987, p. 75) 9 Ades and Steedman (1982) state this explicitly. 227 Computational Linguistics Volume 18, Number 2 Example 7 John gave np ((np\s)/pp)/npn(np\s)/ppA(np\s) The problem that faces the parser here is that it is forced to choose one of the subcategorization frames to make this step in the derivation. There is, however, no indication which frame should be selected. In the ca</context>
<context position="44095" citStr="(1987)" startWordPosition="6827" endWordPosition="6827"> identified, the ambiguity can be resolved and &apos;higher&apos; knowledge sources do not have to be used to solve the ambiguity. In Stock&apos;s (1989) approach to ambiguity resolution the idiomatic and the nonidiomatic analyses are processed in parallel. An external scheduling function gives priority to one of these analyses. Higher knowledge sources are thus necessary to decide upon the interpretation. In PHRAN (Wilensky and Arens 1980), specificity plays a role, but only in suggesting patterns that match the input: evaluation takes place on the basis of length and order of the patterns. Zernik and Dyer (1987) present lexical representations for idioms, but do not discuss ambiguity. Van der Linden and Kraaij (1990) discuss two alternative formalizations for conventionality. One extends the notion continuation class from two-level morphology. The other is a simple localist connectionist model. Here, another model based upon the specificity of information in the hierarchical structure of the lexicon will be presented. 5.3.2 Conventionality and the Hierarchical Lexicon. The ordering of rules for the &gt;&gt;- operator can also be applied to the &gt;---operator, which relates idioms to verbs. Upon 13 Exceptions</context>
</contexts>
<marker>1987</marker>
<rawString>Haddock, N.; Klein, E.; and Morrill, G. (eds.) (1987). Working Papers in Cognitive Science, Volume 1. Categorial Grammar, Unification Grammar and Parsing. Centre for Cognitive Science, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hendriks</author>
</authors>
<title>Type change in semantics: The scope of quantification and coordination.&amp;quot; In Categories, Polymorphism and Unification,</title>
<date>1987</date>
<pages>95--119</pages>
<institution>University of Edinburgh and University of Amsterdam.</institution>
<note>edited by</note>
<contexts>
<context position="4334" citStr="Hendriks 1987" startWordPosition="646" endWordPosition="647">tly, proof theory has aroused the interest of categorial grammarians. In the Lambek calculus (L-calculus, L; Lambek 1958), the most salient example of the application of proof theory to categorial grammar, the rules of the grammar become a set of axioms and inference rules. Together, these form a logical calculus in which parsing of a syntagm is an attempt to prove that it follows as a theorem from the set of axioms and inference rules. Following the work of Van Benthem (1986) and Moortgat (1988; 1987) the Lambek calculus has become popular among a number of linguists (Barry and Morrill 1990; Hendriks 1987). Categories in CG can either be basic (np, s, n) or complex. A complex category consists of a binary category forming connective and two categories, for instance, np\s. In the product-free L-calculus the set of connectives (also called type constructors) is { \ /}. A complex category is a functor, an incomplete expression that forms a result category if an argument category is found. Throughout this paper the Lambek notation, in which the argument category is found under the slash, is applied. Consider for example the categorial representation of an intransitive verb: np\s looks for an np to </context>
</contexts>
<marker>Hendriks, 1987</marker>
<rawString>Hendriks, H. (1987). &amp;quot;Type change in semantics: The scope of quantification and coordination.&amp;quot; In Categories, Polymorphism and Unification, edited by E. Klein and J. van Benthem, 95-119. University of Edinburgh and University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hepple</author>
</authors>
<title>Word order and obliqueness in categorial grammar.&amp;quot;</title>
<date>1990</date>
<booktitle>In Studies in Categorical Grammar, edited</booktitle>
<pages>47--64</pages>
<publisher>University of Edinburgh Press.</publisher>
<marker>Hepple, 1990</marker>
<rawString>Hepple, M. (1990). &amp;quot;Word order and obliqueness in categorial grammar.&amp;quot; In Studies in Categorical Grammar, edited by G. Barry and G. Morrill, 47-64. University of Edinburgh Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
</authors>
<title>Resolving lexical ambiguity computationally with spreading activation and polaroid words.&amp;quot; In Lexical Ambiguity Resolution,</title>
<date>1988</date>
<pages>73--107</pages>
<publisher>Kaufmann.</publisher>
<location>San Mateo:</location>
<note>edited by</note>
<contexts>
<context position="30640" citStr="Hirst (1988)" startWordPosition="4760" endWordPosition="4761">ormation of the intransitive form, but it does not yet have to commit itself any further. The parser can, while incrementally processing a sentence, keep a window on the lexical structure, which becomes smaller iff there is evidence in the input that one of the frames is the right frame. Since parts of the information are shared among the different frames, information once gained is not lost, but is available for all frames. This technique of careful incremental lexicon access will be referred to as windowing here. It can be considered a syntactic counterpart of the semantic Polaroid Words of Hirst (1988), for which the meanings become more specific (develop) in the light of evidence in the input, except that Polaroid Words are active objects. Since the hierarchical structure of the lexicon can be made visible to the parser by means of the &gt;&gt;-operator, it is possible to model windowing by means of the inference rules for the &gt;&gt;-operator. In Definition 7 elimination rules for &gt; are presented.1° Together with the M-system, these rules form a calculus that enables incremental processing and incremental access to the lexicon. It will be referred to as the I-calculus (I for inheritance), and will b</context>
</contexts>
<marker>Hirst, 1988</marker>
<rawString>Hirst, G. (1988). &amp;quot;Resolving lexical ambiguity computationally with spreading activation and polaroid words.&amp;quot; In Lexical Ambiguity Resolution, edited by S. Small, G. Cottrell, and M. Tanenhaus, 73-107. San Mateo: Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
<author>J Bear</author>
</authors>
<title>Two principles of parse preference.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, COLING</booktitle>
<pages>162--167</pages>
<contexts>
<context position="37762" citStr="Hobbs and Bear 1990" startWordPosition="5818" endWordPosition="5821">er occur as an intransitive verb that can be modified by a pp with the prosodic form to + X, or it can take this pp as an argument. The second frame is the preferred frame. The prepositional phrase should preferably be considered as an argument to the verb and not as a vp-modifier. Although the existence of all of these preferences should thus be acknowledged, there are two arguments in favor of lexical preferences. Firstly, from empirical, corpusbased studies it may be concluded that lexical preferences are successful heuristics for resolving ambiguity (Whittemore, Ferrara, and Brunner 1990; Hobbs and Bear 1990). Secondly, although ambiguities may be resolved at any level of processing, lexical processing takes place on a lower level, since higher levels depend upon lexical information. Resolution of ambiguity on a low level ensures that higher levels of processing are not bothered with ambiguities occurring on lower levels. Therefore, if it is equally possible to model the behavior of the parser as a lexically guided or as, for instance, a contextually guided process, the former should be preferred. For instance, in the case of an idiomatic expression, it is more efficient to decide that the idiom s</context>
</contexts>
<marker>Hobbs, Bear, 1990</marker>
<rawString>Hobbs, J., and Bear, J. (1990). &amp;quot;Two principles of parse preference.&amp;quot; In Proceedings, COLING 1990, 162-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Houtman</author>
</authors>
<title>Coordination in Dutch.&amp;quot; In Categories, Polymorphism and Unification, edited by</title>
<date>1987</date>
<pages>121--145</pages>
<institution>University of Edinburgh and University of Amsterdam.</institution>
<contexts>
<context position="21458" citStr="Houtman 1987" startWordPosition="3329" endWordPosition="3330">ntegrate these two into a sentence if the object has been parsed: only then can the vp be formed and combined with the subject to form an s (Briscoe 1987). A process such as this cannot be called incremental. Although subject and verb can be processed incrementally independently from each other, this is not the case for their combination. The strategy mostly used in incremental CG-processing is to enable the construction of a semantic structure with the use of principles that concatenate all possible adjacent categories (although some exceptions are made for coordinate structures [Dowty 1988; Houtman 1987]). In Combinatory Categorial Grammar (Ades and Steedman 1982; Steedman 1987), for instance, composition and lifting rules (Definitions 3 and 4) enable incremental interpretation (Example 6). To make use of these rules in a proof-theoretic approach to CG, a rule that cuts the result of these rules in the proof as a whole (Definition 5) is necessary. Definition 3 (X /Y,a) (Y / Z,b) (X /Z, Ax.a(b(x))) [Comp] Definition 4 (X,a) = (Z/ (X \ Z), )b.b(a)) [Lift] 6 The first parser that featured incremental processing can be found in Marcus (1980). This parser did not consider lexical ambiguity and co</context>
</contexts>
<marker>Houtman, 1987</marker>
<rawString>Houtman, J. (1987). &amp;quot;Coordination in Dutch.&amp;quot; In Categories, Polymorphism and Unification, edited by E. Klein and J. van Benthem, 121-145. University of Edinburgh and University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hudson</author>
</authors>
<title>Word Grammar.</title>
<date>1984</date>
<publisher>Blackwell.</publisher>
<location>Oxford:</location>
<contexts>
<context position="38977" citStr="Hudson 1984" startWordPosition="6016" endWordPosition="6017">ld be interpreted on the basis of the mere fact that it is an idiom than on the basis of consultation of, for instance, some model of the context. Since lexical preferences are successful heuristics that operate on a low level, there is sufficient reason to model them in a principled and formal way. 12 See also Shieber (1983) and Hobbs and Bear (1990). 230 Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon 5.2 Formalization of Lexical Preferences The formalization of lexical preferences proposed here is another application of the principle of priority to the instance (Hudson 1984): the parser prefers information lower on in the hierarchical structure of the lexicon over information on higher levels in the hierarchy. If two subcategorization frames of, for instance, go maintain an inheritance relation (np\s &gt;&gt; (np\s)/ (pp, to + _)), and both apply, the more specific frame is preferred. The difference between windowing and lexical preferencing is that windowing applies to the choice during incremental processing among a number of frames of which only one applies eventually, whereas lexical preferencing applies to a choice among frames all of which apply. Lexical preferen</context>
</contexts>
<marker>Hudson, 1984</marker>
<rawString>Hudson, R. (1984). Word Grammar. Oxford: Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Just</author>
<author>P Carpenter</author>
</authors>
<title>A theory of reading, from eye fixations to comprehension.&amp;quot;</title>
<date>1980</date>
<journal>Psychological Review,</journal>
<volume>87</volume>
<pages>329--354</pages>
<contexts>
<context position="25336" citStr="Just and Carpenter (1980)" startWordPosition="3909" endWordPosition="3912"> Linden Incremental Processing and the Hierarchical Lexicon thus implement incrementality and an all-or-none immediacy: there is at all times during the parsing process a full interpretation of the input so far.&apos; There are two problems with this approach. Firstly it is questionable whether it agrees with the psycholinguistic notion of immediacy, and secondly it leads to an unrealistic view of the parsing process. The second point will be discussed in the following section. With respect to the first point of criticism it should in the first place be noted that immediacy as it was formulated by Just and Carpenter (1980) was only formulated for content words. The immediacy assumption states that processing of content words should be as immediate as possible. Firstly, CG has included function words under immediacy. Haddock (1987),8 for instance, states that given a domain with two rabbits that are &apos;in&apos; something, during incremental processing of the phrase the rabbit in the hat. &amp;quot;the incremental evaluation of the rabbit in the has created two distinct sets of candidates for the two NPs in the phrase&amp;quot; (Haddock 1987, p. 81) It is not clear from the psycholinguistic literature whether processing of function words</context>
</contexts>
<marker>Just, Carpenter, 1980</marker>
<rawString>Just, M., and Carpenter, P. (1980). &amp;quot;A theory of reading, from eye fixations to comprehension.&amp;quot; Psychological Review, 87, 329-354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
</authors>
<title>Three seductions of computational psycholinguistics.&amp;quot; In Linguistic Theory and Computer Applications, edited by</title>
<date>1987</date>
<pages>149--188</pages>
<publisher>Academic Press.</publisher>
<location>London:</location>
<contexts>
<context position="13716" citStr="Kaplan 1987" startWordPosition="2118" endWordPosition="2119">aning of an expression should be derived (contextuality, meaning postulates...). A definition that states what the meaning is, is preferable: the meaning of an idiom is exclusively a property of the whole expression.4 The meaning of the idiom cannot be inherited from the verb that is its head, but should be added nonmonotonically. Example 3 a. KICK: (KICK_TV &gt;-- KICK_THE_BUCKET, kick, AxAykick(x)(y)) b. KICK_TV: ((np\s)/np, _) c. KICK_THE_BUCKET: (_, the + bucket, _), AxAydie(y)) 3.1.5 Inheritance. The full specification of a sign is derived by means of an operation similar to priority union (Kaplan 1987, p. 180) or default unification (Bouma 1990), denoted by n. n is defined as a function from pairs of mother and daughter signs to fully specified daughter signs and runs as follows. If unification, U, is successful for the values of a certain property of mother and daughter, the result of n for that value is the result of U, where unification is understood in its most basic sense: variables unify with constants and variables; constants unify with variables and with constants with an equal value (prosodic information in Example 4). If the values do not unify, the value of the daughter is retur</context>
</contexts>
<marker>Kaplan, 1987</marker>
<rawString>Kaplan, R. (1987). &amp;quot;Three seductions of computational psycholinguistics.&amp;quot; In Linguistic Theory and Computer Applications, edited by P. Whitelock, M. McGee Wood, H. Somers, R. Johnson, and P. Bennett, 149-188. London: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kempen</author>
<author>Th Vosse</author>
</authors>
<title>Incremental syntactic tree formation in human sentence processing: A cognitive architecture based on activation decay and simulation annealing.&amp;quot;</title>
<date>1989</date>
<journal>Connection Science,</journal>
<volume>1</volume>
<pages>275--292</pages>
<contexts>
<context position="22322" citStr="Kempen and Vosse (1989)" startWordPosition="3464" endWordPosition="3467">pproach to CG, a rule that cuts the result of these rules in the proof as a whole (Definition 5) is necessary. Definition 3 (X /Y,a) (Y / Z,b) (X /Z, Ax.a(b(x))) [Comp] Definition 4 (X,a) = (Z/ (X \ Z), )b.b(a)) [Lift] 6 The first parser that featured incremental processing can be found in Marcus (1980). This parser did not consider lexical ambiguity and confined itself to syntactic processing. Other computational models that entail the notion of incrementality can be found for Segment Grammar and in Word Expert Parsing. The subsymbolic processing architecture for Segment Grammar presented by Kempen and Vosse (1989) is a model of syntactic processing. The architecture allows for immediate interpretation, but no semantic representation is actually constructed. Adriaens (1986) presents a lexicalist model, Word Expert Parsing, which operates incrementally. Another lexicalist model that features incremental processing can be found in Stock (1989). In none of the models is mention made of a structured lexicon. 225 Computational Linguistics Volume 18, Number 2 Definition 5 U, X, Y, V Z [Cut] if X, Y = W and U, W, V Z Example 6 (np,john) ((np \ s)/np,kicks) (np/n,the) (n,boy) = (s, kicks(the(boy))(john)) [Cut] </context>
</contexts>
<marker>Kempen, Vosse, 1989</marker>
<rawString>Kempen, G., and Vosse, Th. (1989). &amp;quot;Incremental syntactic tree formation in human sentence processing: A cognitive architecture based on activation decay and simulation annealing.&amp;quot; Connection Science, 1, 275-292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kimball</author>
</authors>
<title>Seven principles of surface structure parsing in natural language.&amp;quot;</title>
<date>1973</date>
<journal>Cognition,</journal>
<volume>2</volume>
<pages>15--47</pages>
<contexts>
<context position="36035" citStr="Kimball 1973" startWordPosition="5544" endWordPosition="5545">s are modeled. They follow from the structure of the lexicon, which was independently motivated to capture linguistic generalizations. Inference 229 Computational Linguistics Volume 18, Number 2 rules model the proceedings of the parser in this respect. Heuristic information is thus integrated in a principled and formal way into the interpretation process. The behavior of idiomatic expressions will be discussed as an example. 5.1 Preference Strategies Several preference strategies have been proposed for guiding parsers. Among these are structural, syntactic preferences like Right Association (Kimball 1973), which entails that a modifier should preferably be attached to the rightmost verb (phrase) or noun (phrase) it can modify; and Minimal Attachment (Frazier and Fodor 1988), which states that the analysis that assumes the minimal number of nodes in the syntactic tree should be preferred.12 Semantic preferences are illustrated in Examples 9 and 10. The modifiers in both cases are preferably attached contrary to expectations on the basis of syntactic preferences (see Schubert 1984, 1986; Wilks, Huang, and Fass 1985). Example 9 John met the girl that he married at the dance. Example 10 John saw t</context>
</contexts>
<marker>Kimball, 1973</marker>
<rawString>Kimball, J. (1973). &amp;quot;Seven principles of surface structure parsing in natural language.&amp;quot; Cognition, 2, 15-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Klein</author>
<author>van Benthem</author>
</authors>
<title>Categories, Polymorphism and Unification.</title>
<date>1987</date>
<editor>r. (eds.)</editor>
<institution>Centre for Cognitive Science, University of Edinburgh, and Institute for Language, Logic and Information, University of Amsterdam.</institution>
<marker>Klein, van Benthem, 1987</marker>
<rawString>Klein, E., and van Benthem, r. (eds.) (1987). Categories, Polymorphism and Unification. Centre for Cognitive Science, University of Edinburgh, and Institute for Language, Logic and Information, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Koller</author>
</authors>
<title>Redensarten: Linguistische Aspekte,</title>
<date>1977</date>
<location>Vorkommensanalysen, Sprachspiel. Tubingen: Niemeyer.</location>
<contexts>
<context position="42547" citStr="Koller 1977" startWordPosition="6585" endWordPosition="6586">e attempt to please Mary. attempt: n &gt;&gt; (n/ (np, to + _)\sto_inf)) Example 14 Sue had difficulties with her teachers. difficulties: n &gt;&gt; (n/ (pp,with + -)) Example 15 a. John met the girl that he married at a dance. b. John married the girl that he met at a dance. marry: ((np \s)/np) met ((np\s)/np) (((np\s)/ pp)/ np) 5.3 Idioms and Parsing Preferences 5.3.1 Conventionality and Idiom Processing. Idiomatic expressions can in most cases be interpreted nonidiomatically as well.&amp;quot; It has, however, frequently been observed that an idiomatic phrase should very rarely be interpreted nonidiomatically (Koller 1977, p. 13; Chafe 1968, p. 123; Gross 1984, p. 278; Swinney 1981, p. 208). Also, psycholinguistic research indicates that in case of ambiguity there is clear preference for the idiomatic reading (Gibbs 1980; Schraw et al. 1988; Schweigert 1986; Schweigert and Moates 1988). The phenomenon that phrases should be interpreted according to their idiomatic, noncompositional, lexical, conventional meaning will be referred to as the &apos;conventionality&apos; principle (Gibbs 1980). The application of this principle is not limited to idioms. For instance, compounds are not interpreted compositionally, but accordi</context>
</contexts>
<marker>Koller, 1977</marker>
<rawString>Koller, W. (1977). Redensarten: Linguistische Aspekte, Vorkommensanalysen, Sprachspiel. Tubingen: Niemeyer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lambek</author>
</authors>
<title>The mathematics of sentence structure.&amp;quot;</title>
<date>1958</date>
<journal>Am. Math. Monthly,</journal>
<volume>65</volume>
<pages>154--169</pages>
<contexts>
<context position="3841" citStr="Lambek 1958" startWordPosition="560" endWordPosition="561"> 2 In Section 2 the proof-theoretic approach to CG is presented. Next, in Section 3 a hierarchical lexicon structure for CG is presented. Two category-forming connectives that make the structure of the lexicon visible for the parser are introduced. Windowing is discussed in Section 4. In Section 5 parsing preferences are discussed in general, and preferences for the interpretation of idioms are discussed in particular. 2. Categorial Grammar and Proof Theory 2.1 The Lambek Calculus Recently, proof theory has aroused the interest of categorial grammarians. In the Lambek calculus (L-calculus, L; Lambek 1958), the most salient example of the application of proof theory to categorial grammar, the rules of the grammar become a set of axioms and inference rules. Together, these form a logical calculus in which parsing of a syntagm is an attempt to prove that it follows as a theorem from the set of axioms and inference rules. Following the work of Van Benthem (1986) and Moortgat (1988; 1987) the Lambek calculus has become popular among a number of linguists (Barry and Morrill 1990; Hendriks 1987). Categories in CG can either be basic (np, s, n) or complex. A complex category consists of a binary categ</context>
</contexts>
<marker>Lambek, 1958</marker>
<rawString>Lambek, J. (1958). &amp;quot;The mathematics of sentence structure.&amp;quot; Am. Math. Monthly, 65, 154-169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Leslie</author>
</authors>
<title>Contrasting styles of categorial derivations.&amp;quot;</title>
<date>1990</date>
<booktitle>In Studies in Categorical Grammar, edited</booktitle>
<pages>113--126</pages>
<publisher>University of Edinburgh Press.</publisher>
<contexts>
<context position="6907" citStr="Leslie (1990)" startWordPosition="1052" endWordPosition="1053">90).1 Throughout this article the sequent format is used. In definition (1), W and X are categories, Y and Z are signs, and P. T, Q, U and V are sequences of signs, where P. T, and Q are nonempty. A sequent in L represents a derivability relation, z, between a nonempty finite sequence of signs, the antecedent, and a sign, the succedent. A sequent states that the string denoted by the antecedent is in the set of strings denoted by the succedent. The axioms and inference rules of the calculus define the theorems of the calculus with respect to this derivability relation. 1 For a comparison, see Leslie (1990). 220 Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon Recursive application of the inference rules on a sequent may result in the derivation of a sequent as a theorem of the calculus. In definition (1) the calculus is presented. The elimination of a type constructor is denoted by E; introduction by I. Definition 1 (Lambek, sequent calculus) U,((X/(W,V,,b)),0,a),T,V = Z r/El if T and U,(X,0+0,a(b)),V = Z U,T,(((W,O,b) \X),0,a),V = Z RE] if T (W,O,b) and U,(X,04-0,a(b)),V Z T =ix ((X/(W,e,b)),O,Ab.a) 1/Il if T,(W,e,b) x T = (((W,E,b) \X),4),Ab.a) RI] if (W,E,b), T (X,</context>
</contexts>
<marker>Leslie, 1990</marker>
<rawString>Leslie, N. (1990). &amp;quot;Contrasting styles of categorial derivations.&amp;quot; In Studies in Categorical Grammar, edited by G. Barry and G. Morrill, 113-126. University of Edinburgh Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Van der Linden</author>
</authors>
<title>Idioms, non-literal language and knowledge representation.&amp;quot;</title>
<date>1991</date>
<booktitle>In Proceedings, IJCAI-Workshop Computational Approaches to Non-literal Language.</booktitle>
<marker>Van der Linden, 1991</marker>
<rawString>Van der Linden, E. (1991). &amp;quot;Idioms, non-literal language and knowledge representation.&amp;quot; In Proceedings, IJCAI-Workshop Computational Approaches to Non-literal Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Van der Linden</author>
<author>W Kraaij</author>
</authors>
<title>Ambiguity resolution and the retrieval of idioms: Two approaches.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, COLING</booktitle>
<volume>2</volume>
<pages>245--251</pages>
<marker>Van der Linden, Kraaij, 1990</marker>
<rawString>Van der Linden, E., and Kraaij, W. (1990). &amp;quot;Ambiguity resolution and the retrieval of idioms: Two approaches.&amp;quot; In Proceedings, COLING 1990, Vol. 2, 245-251.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Van der Linden</author>
<author>G Minnen</author>
</authors>
<title>Algorithms for generation in Lambek theorem proving.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, ACL</booktitle>
<pages>220--226</pages>
<marker>Van der Linden, Minnen, 1990</marker>
<rawString>Van der Linden, E., and Minnen, G. (1990). &amp;quot;Algorithms for generation in Lambek theorem proving.&amp;quot; In Proceedings, ACL 1990, 220-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="22003" citStr="Marcus (1980)" startWordPosition="3420" endWordPosition="3421">eptions are made for coordinate structures [Dowty 1988; Houtman 1987]). In Combinatory Categorial Grammar (Ades and Steedman 1982; Steedman 1987), for instance, composition and lifting rules (Definitions 3 and 4) enable incremental interpretation (Example 6). To make use of these rules in a proof-theoretic approach to CG, a rule that cuts the result of these rules in the proof as a whole (Definition 5) is necessary. Definition 3 (X /Y,a) (Y / Z,b) (X /Z, Ax.a(b(x))) [Comp] Definition 4 (X,a) = (Z/ (X \ Z), )b.b(a)) [Lift] 6 The first parser that featured incremental processing can be found in Marcus (1980). This parser did not consider lexical ambiguity and confined itself to syntactic processing. Other computational models that entail the notion of incrementality can be found for Segment Grammar and in Word Expert Parsing. The subsymbolic processing architecture for Segment Grammar presented by Kempen and Vosse (1989) is a model of syntactic processing. The architecture allows for immediate interpretation, but no semantic representation is actually constructed. Adriaens (1986) presents a lexicalist model, Word Expert Parsing, which operates incrementally. Another lexicalist model that features</context>
<context position="49402" citStr="Marcus 1980" startWordPosition="7604" endWordPosition="7605"> can mistake a vp-modifier for an argument. Lexical Preferencing is also a nondeterministic process in that backtracking is necessary when interpretations do not fit in the context. Although it is a linguistically motivated strategy, it does not guarantee that the correct choice is made in all cases. In Example 17 the idiomatic reading is preferred, but later on in the input it turns out that this is not the correct interpretation. Yet, Marcus&apos; Determinism Hypothesis states that &amp;quot;(...) all sentences which people can parse without conscious difficulty can be parsed strictly deterministically&amp;quot; (Marcus 1980, P. 6). It remains to be seen whether people do not garden-path in Example 17. Note also that backtracking is modeled very easily—it amounts to making a different choice between two items that maintain an inheritance relation. Example 17 John kicked the bucket and Mary the small pail. 6. Implementation The parser described here has been implemented with the use of a slightly modified version of the categorial calculi interpreter described in Moortgat (1988). This interpreter takes the rules of a calculus as data and applies these recursively to the sequent associated with the input in order t</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Marcus, M. (1980). A Theory of Syntactic Recognition for Natural Language. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Marslen-Wilson</author>
<author>L Tyler</author>
</authors>
<title>The temporal structure of spoken language understanding.&amp;quot;</title>
<date>1980</date>
<journal>Cognition,</journal>
<volume>8</volume>
<pages>1--71</pages>
<contexts>
<context position="36790" citStr="Marslen-Wilson and Tyler (1980)" startWordPosition="5664" endWordPosition="5667"> and Minimal Attachment (Frazier and Fodor 1988), which states that the analysis that assumes the minimal number of nodes in the syntactic tree should be preferred.12 Semantic preferences are illustrated in Examples 9 and 10. The modifiers in both cases are preferably attached contrary to expectations on the basis of syntactic preferences (see Schubert 1984, 1986; Wilks, Huang, and Fass 1985). Example 9 John met the girl that he married at the dance. Example 10 John saw the bird with the red beak. Evidence for the existence of preferences based upon contextual information has been provided by Marslen-Wilson and Tyler (1980), who have shown in a number of psycholinguistic experiments that contextual information influences word recognition (see also Crain and Steedman [1982] and Taraban and McClelland [1988]). Lexical preferencing (Ford, Bresnan, and Kaplan 1982) refers to the preference functor categories have for certain arguments. For instance, the verb to go can either occur as an intransitive verb that can be modified by a pp with the prosodic form to + X, or it can take this pp as an argument. The second frame is the preferred frame. The prepositional phrase should preferably be considered as an argument to </context>
</contexts>
<marker>Marslen-Wilson, Tyler, 1980</marker>
<rawString>Marslen-Wilson, W., and Tyler, L. (1980). &amp;quot;The temporal structure of spoken language understanding.&amp;quot; Cognition, 8, 1-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Moortgat</author>
</authors>
<title>Lambek theorem proving.&amp;quot; In Categories, Polymorphism and Unification, edited by</title>
<date>1987</date>
<pages>169--200</pages>
<institution>University of Edinburgh and University of Amsterdam.</institution>
<marker>Moortgat, 1987</marker>
<rawString>Moortgat, M. (1987). &amp;quot;Lambek theorem proving.&amp;quot; In Categories, Polymorphism and Unification, edited by E. Klein and J. van Benthem, 169-200. University of Edinburgh and University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Moortgat</author>
</authors>
<title>Categorial Investigations, Logical and Linguistic Aspects of the Lambek Calculus. Doctoral dissertation,</title>
<date>1988</date>
<institution>University of Amsterdam.</institution>
<contexts>
<context position="4220" citStr="Moortgat (1988" startWordPosition="628" endWordPosition="629">tation of idioms are discussed in particular. 2. Categorial Grammar and Proof Theory 2.1 The Lambek Calculus Recently, proof theory has aroused the interest of categorial grammarians. In the Lambek calculus (L-calculus, L; Lambek 1958), the most salient example of the application of proof theory to categorial grammar, the rules of the grammar become a set of axioms and inference rules. Together, these form a logical calculus in which parsing of a syntagm is an attempt to prove that it follows as a theorem from the set of axioms and inference rules. Following the work of Van Benthem (1986) and Moortgat (1988; 1987) the Lambek calculus has become popular among a number of linguists (Barry and Morrill 1990; Hendriks 1987). Categories in CG can either be basic (np, s, n) or complex. A complex category consists of a binary category forming connective and two categories, for instance, np\s. In the product-free L-calculus the set of connectives (also called type constructors) is { \ /}. A complex category is a functor, an incomplete expression that forms a result category if an argument category is found. Throughout this paper the Lambek notation, in which the argument category is found under the slash</context>
<context position="8923" citStr="Moortgat 1988" startWordPosition="1344" endWordPosition="1345">y)(john)) (s,john+loves+mary,loves(mary)(john)) [Axiom] 2.2 Other Connectives The product-free version of the Lambek calculus includes two connectives, / and \ On the basis of these connectives and the inference rules of the L-calculus, a range of linguistic constructions and generalizations remain for which no linguistically adequate accounts can be presented. In order to overcome this problem, new category-forming connectives have been proposed (as a lexical alternative of the specialized rules of for instance CCG [Steedman 1987]). An example is the connective 1- for unbounded dependencies (Moortgat 1988). Xi Y denotes a category X that has an argument of category Y missing somewhere within X. The constituent John put on the table in what John put on the table has as its syntactic category s I np. To what the category s/(s I np) is assigned, which takes the incomplete clause as its argument. The A-connective (Morrill 1990) is one of a set of boolean connectives that can be used to denote that a certain lexical item can occur in different categories: square can be n/n and n, and is therefore assigned the category (n/n) An. The ?-connective (ibid.) is used to denote optionality, for instance in </context>
<context position="23775" citStr="Moortgat 1988" startWordPosition="3674" endWordPosition="3675"> Ax.kicks(x)(john)) (np/n,the) (n,boy) (s, kicks(the(boy))(john)) [Cut] if (s/np, )x.kicks(x)(john)) (np/n,the) r (s/n, Ay.kick(the(y))(john)) [Comp] and (s/n, )y.kick(the(y))(john)) (n,boy) = (s, kicks(the(boy))(john)) [/E] if (n, boy) (n,boy) [Axiom] and (s, kicks(the(boy))(john)) (s, kicks(the(boy))(john)) [Axiom] All words, also the function words such as the are in principle processed and thus interpreted immediately; that is, their semantic representation is accessed from the lexicon and combined with the semantic representation of the input so far. A similar proposal is the M-calculus (Moortgat 1988; 1990). In M, the elimination rules of L are traded in for a set of generalized application rules and a cut-rule that links the derivation relation = and the derivation relation of the system of generalized application, (Definition 6). Definition 6 (X/ (Y,O,a),0,b), (Z,x,c) = (X,0+0,b(a)) if (Z,x,c) = (Y,&apos;/,a) (Z,x,c), ((Y,O,a) \X,cb,b) (X4&apos;+0,b(a)) if (Z,x,c) = (Y,O,a) (4,c), (X/Y,O,b) (W/Y,O+0,Aa.d) if (Z,V),c), (X,,b(a)) (W,_,d) (Y\X,O,b), (ZAP,c) (Y\W,O+0, )a.d) if (X,,b(a)), (Z,O,c) (W,,d). (X/Y,O,b), (Z,O,c) = (X/W,O+0,Ad.b(a)) if (Z,O,c), (W,,d) (Y,,a) (Z4,c), (Y\X,O,b) (W\X4+0,Ad.b(a)</context>
<context position="49864" citStr="Moortgat (1988)" startWordPosition="7678" endWordPosition="7679">sm Hypothesis states that &amp;quot;(...) all sentences which people can parse without conscious difficulty can be parsed strictly deterministically&amp;quot; (Marcus 1980, P. 6). It remains to be seen whether people do not garden-path in Example 17. Note also that backtracking is modeled very easily—it amounts to making a different choice between two items that maintain an inheritance relation. Example 17 John kicked the bucket and Mary the small pail. 6. Implementation The parser described here has been implemented with the use of a slightly modified version of the categorial calculi interpreter described in Moortgat (1988). This interpreter takes the rules of a calculus as data and applies these recursively to the sequent associated with the input in order to prove that it is a theorem of the calculus. The system is written in Quintus Prolog. No empirical studies of the efficiency of the system have been undertaken so far. 7. Concluding Remarks The hierarchical structure of the lexicon can make a contribution to the speed and the efficiency of the resolution of ambiguity during the process of understanding natural language. With the use of other connectives, or other properties of lexical items like frequency, </context>
</contexts>
<marker>Moortgat, 1988</marker>
<rawString>Moortgat, M. (1988). Categorial Investigations, Logical and Linguistic Aspects of the Lambek Calculus. Doctoral dissertation, University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Moortgat</author>
</authors>
<title>Categorial logics: A computational perspective.&amp;quot;</title>
<date>1990</date>
<booktitle>In Computation in the Netherlands,</booktitle>
<pages>329--347</pages>
<note>edited by</note>
<marker>Moortgat, 1990</marker>
<rawString>Moortgat, M. (1990). &amp;quot;Categorial logics: A computational perspective.&amp;quot; In Computation in the Netherlands, edited by A. J. van de Goor, 329-347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Moortgat</author>
</authors>
<title>The logic of discontinuous type constructors.&amp;quot; In Discontinuous Constituency,</title>
<date>1992</date>
<location>Berlin: Mouton</location>
<note>edited by</note>
<marker>Moortgat, 1992</marker>
<rawString>Moortgat, M. (1992). &amp;quot;The logic of discontinuous type constructors.&amp;quot; In Discontinuous Constituency, edited by W. Sijtsma, and A. van Horck. Berlin: Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Morrill</author>
</authors>
<title>Grammar and logical types.&amp;quot;</title>
<date>1990</date>
<booktitle>In Studies in Categorical Grammar, edited</booktitle>
<pages>127--148</pages>
<publisher>University of Edinburgh Press.</publisher>
<contexts>
<context position="4318" citStr="Morrill 1990" startWordPosition="644" endWordPosition="645">Calculus Recently, proof theory has aroused the interest of categorial grammarians. In the Lambek calculus (L-calculus, L; Lambek 1958), the most salient example of the application of proof theory to categorial grammar, the rules of the grammar become a set of axioms and inference rules. Together, these form a logical calculus in which parsing of a syntagm is an attempt to prove that it follows as a theorem from the set of axioms and inference rules. Following the work of Van Benthem (1986) and Moortgat (1988; 1987) the Lambek calculus has become popular among a number of linguists (Barry and Morrill 1990; Hendriks 1987). Categories in CG can either be basic (np, s, n) or complex. A complex category consists of a binary category forming connective and two categories, for instance, np\s. In the product-free L-calculus the set of connectives (also called type constructors) is { \ /}. A complex category is a functor, an incomplete expression that forms a result category if an argument category is found. Throughout this paper the Lambek notation, in which the argument category is found under the slash, is applied. Consider for example the categorial representation of an intransitive verb: np\s loo</context>
<context position="6236" citStr="Morrill 1990" startWordPosition="933" endWordPosition="934">lculus extends the power of categorial grammar basically because it adds so-called introduction rules to the proof-theoretic complements of categorial reduction rules, elimination rules. For each category-forming connective, introduction and elimination rules can be formulated. With respect to semantics, elimination corresponds to functional application and introduction to lambda abstraction. Various approaches have been proposed for deduction in L. In its standard representation the L-calculus is a sequent calculus. More recently, natural deduction has been applied to the calculus (Barry and Morrill 1990), as well as proof procedures from linear logic (Roorda 1990).1 Throughout this article the sequent format is used. In definition (1), W and X are categories, Y and Z are signs, and P. T, Q, U and V are sequences of signs, where P. T, and Q are nonempty. A sequent in L represents a derivability relation, z, between a nonempty finite sequence of signs, the antecedent, and a sign, the succedent. A sequent states that the string denoted by the antecedent is in the set of strings denoted by the succedent. The axioms and inference rules of the calculus define the theorems of the calculus with respe</context>
<context position="9247" citStr="Morrill 1990" startWordPosition="1404" endWordPosition="1405">ly adequate accounts can be presented. In order to overcome this problem, new category-forming connectives have been proposed (as a lexical alternative of the specialized rules of for instance CCG [Steedman 1987]). An example is the connective 1- for unbounded dependencies (Moortgat 1988). Xi Y denotes a category X that has an argument of category Y missing somewhere within X. The constituent John put on the table in what John put on the table has as its syntactic category s I np. To what the category s/(s I np) is assigned, which takes the incomplete clause as its argument. The A-connective (Morrill 1990) is one of a set of boolean connectives that can be used to denote that a certain lexical item can occur in different categories: square can be n/n and n, and is therefore assigned the category (n/n) An. The ?-connective (ibid.) is used to denote optionality, for instance in the case of belief: n/ (sp?), which accounts for belief in the belief and the belief that Mary lives. These connectives are introduced to enable the inference engine behind the calculus to deal with lexical ambiguities and to &apos;reason&apos; about lexical items. This is in line 221 Computational Linguistics Volume 18, Number 2 wi</context>
</contexts>
<marker>Morrill, 1990</marker>
<rawString>Morrill, G. (1990). &amp;quot;Grammar and logical types.&amp;quot; In Studies in Categorical Grammar, edited by G. Barry and G. Morrill, 127-148. University of Edinburgh Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Morrill</author>
<author>N Leslie</author>
<author>M Hepple</author>
<author>G Barry</author>
</authors>
<title>Categorial deductions and structural operations.&amp;quot;</title>
<date>1990</date>
<journal>In Studies</journal>
<pages>1--21</pages>
<note>in Categorical Grammar, edited by</note>
<marker>Morrill, Leslie, Hepple, Barry, 1990</marker>
<rawString>Morrill, G.; Leslie, N.; Hepple, M.; and Barry, G. (1990). &amp;quot;Categorial deductions and structural operations.&amp;quot; In Studies in Categorical Grammar, edited by G. Barry and G. Morrill, 1-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Oehrle</author>
<author>E Bach</author>
<author>D Wheeler</author>
</authors>
<title>Categorial Grammar and Natural Language Structure.</title>
<date>1988</date>
<location>Dordrecht: Reidel.</location>
<marker>Oehrle, Bach, Wheeler, 1988</marker>
<rawString>Oehrle, R.; Bach, E.; and Wheeler, D. (eds). (1988). Categorial Grammar and Natural Language Structure. Dordrecht: Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<date>1987</date>
<journal>Information-based Syntax and Semantics,</journal>
<volume>1</volume>
<location>CSLI, Stanford.</location>
<contexts>
<context position="18872" citStr="Pollard and Sag (1987)" startWordPosition="2925" endWordPosition="2928">ment is further instantiated whereas here a syntactic argument is added. To represent this distinction, a different connective is used: &gt;. With the use of this type constructor, the intransitive and the transitive can be placed in an inheritance relation (as seen in Example 5). &gt;&gt; is a category forming connective which takes two signs to form a category. Example 5 a. KICK: (KICK IV &gt;&gt; KICK_TV , kick, AxAykick(x)(y)) c. KICK_IV: (np\s, detrans(KICK)) b. KICK_TV: (synt(KICK_IV)/np, sem(KICK)) The lexical structure presented here can be considered equal to that presented by Flickinger (1987) and Pollard and Sag (1987) for HPSG. They present a hierarchy in which not only transitive and intransitive verbs, but other classes of verbs are represented as well. A minor difference is that Flickinger and Pollard and Sag place classes of verbs in hierarchical relations, whereas here individual verbs maintain inheritance 224 Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon relations. The main difference with this and other previous approaches is that with the introduction of connectives for inheritance relations, inference rules for these connectives can be presented that describe the lega</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>Pollard, Carl, and Sag, Ivan A. (1987). Information-based Syntax and Semantics, Vol. 1, CSLI, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ristad</author>
</authors>
<title>Computational Structure of Human Language. Doctoral dissertation,</title>
<date>1990</date>
<institution>Department of Electrical Engineering and Computer Science, MIT.</institution>
<marker>Ristad, 1990</marker>
<rawString>Ristad, E. (1990). Computational Structure of Human Language. Doctoral dissertation, Department of Electrical Engineering and Computer Science, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roorda</author>
</authors>
<title>Proofnets for Lambek calculus.&amp;quot;</title>
<date>1990</date>
<institution>Ms. University of Amsterdam.</institution>
<contexts>
<context position="6297" citStr="Roorda 1990" startWordPosition="943" endWordPosition="944">se it adds so-called introduction rules to the proof-theoretic complements of categorial reduction rules, elimination rules. For each category-forming connective, introduction and elimination rules can be formulated. With respect to semantics, elimination corresponds to functional application and introduction to lambda abstraction. Various approaches have been proposed for deduction in L. In its standard representation the L-calculus is a sequent calculus. More recently, natural deduction has been applied to the calculus (Barry and Morrill 1990), as well as proof procedures from linear logic (Roorda 1990).1 Throughout this article the sequent format is used. In definition (1), W and X are categories, Y and Z are signs, and P. T, Q, U and V are sequences of signs, where P. T, and Q are nonempty. A sequent in L represents a derivability relation, z, between a nonempty finite sequence of signs, the antecedent, and a sign, the succedent. A sequent states that the string denoted by the antecedent is in the set of strings denoted by the succedent. The axioms and inference rules of the calculus define the theorems of the calculus with respect to this derivability relation. 1 For a comparison, see Les</context>
</contexts>
<marker>Roorda, 1990</marker>
<rawString>Roorda, D. (1990). &amp;quot;Proofnets for Lambek calculus.&amp;quot; Ms. University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Schraw</author>
<author>W Trathen</author>
<author>R Reynolds</author>
<author>R Lapan</author>
</authors>
<title>Preferences for idioms: Restrictions due to lexicalization and familiarity.&amp;quot;</title>
<date>1988</date>
<journal>Journal of Psycholinguistic Research,</journal>
<pages>17--413</pages>
<contexts>
<context position="42770" citStr="Schraw et al. 1988" startWordPosition="6621" endWordPosition="6624">b. John married the girl that he met at a dance. marry: ((np \s)/np) met ((np\s)/np) (((np\s)/ pp)/ np) 5.3 Idioms and Parsing Preferences 5.3.1 Conventionality and Idiom Processing. Idiomatic expressions can in most cases be interpreted nonidiomatically as well.&amp;quot; It has, however, frequently been observed that an idiomatic phrase should very rarely be interpreted nonidiomatically (Koller 1977, p. 13; Chafe 1968, p. 123; Gross 1984, p. 278; Swinney 1981, p. 208). Also, psycholinguistic research indicates that in case of ambiguity there is clear preference for the idiomatic reading (Gibbs 1980; Schraw et al. 1988; Schweigert 1986; Schweigert and Moates 1988). The phenomenon that phrases should be interpreted according to their idiomatic, noncompositional, lexical, conventional meaning will be referred to as the &apos;conventionality&apos; principle (Gibbs 1980). The application of this principle is not limited to idioms. For instance, compounds are not interpreted compositionally, but according to the lexical, conventional meaning (Swinney 1981). Words are formed by regular rules, but their meaning will undergo &apos;semantic drift,&apos; obscuring the compositional nature of the complex word. If this principle could be </context>
</contexts>
<marker>Schraw, Trathen, Reynolds, Lapan, 1988</marker>
<rawString>Schraw, G.; Trathen, W.; Reynolds, R.; and Lapan R. (1988). &amp;quot;Preferences for idioms: Restrictions due to lexicalization and familiarity.&amp;quot; Journal of Psycholinguistic Research, 17,413-424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Schubert</author>
</authors>
<title>On parsing preferences.&amp;quot;</title>
<date>1984</date>
<booktitle>In Proceedings, COLING</booktitle>
<pages>247--250</pages>
<contexts>
<context position="26425" citStr="Schubert 1984" startWordPosition="4090" endWordPosition="4091">n the phrase&amp;quot; (Haddock 1987, p. 81) It is not clear from the psycholinguistic literature whether processing of function words takes place this way, but it is at least unintuitive: in larger domains large intermediate sets of candidates will be of little help for the interpretation in comparison to the information the constituent as a whole provides. Secondly, the wish to be able to give an interpretation of a sentence at any stage of the parsing process stems from the fact that humans are able to make guesses about continuations of sentences that stop before they have come to a proper ending (Schubert 1984). From this it follows that humans are able to construct interpretations at any moment during NLP, but not that they actually do construct full interpretations: the ability to complete incomplete sentences says little about the ongoing automatic interpretation process. 4.2 Windowing and Lexical Ambiguity The &gt;&gt;-operator is useful for incremental processing in case of lexical syntactic ambiguity and overcomes one of the problems of all-or-none immediacy. One of the sources of lexical ambiguity is that a functor may have several subcategorization frames. During incremental processing, one of the</context>
<context position="36518" citStr="Schubert 1984" startWordPosition="5620" endWordPosition="5621">es have been proposed for guiding parsers. Among these are structural, syntactic preferences like Right Association (Kimball 1973), which entails that a modifier should preferably be attached to the rightmost verb (phrase) or noun (phrase) it can modify; and Minimal Attachment (Frazier and Fodor 1988), which states that the analysis that assumes the minimal number of nodes in the syntactic tree should be preferred.12 Semantic preferences are illustrated in Examples 9 and 10. The modifiers in both cases are preferably attached contrary to expectations on the basis of syntactic preferences (see Schubert 1984, 1986; Wilks, Huang, and Fass 1985). Example 9 John met the girl that he married at the dance. Example 10 John saw the bird with the red beak. Evidence for the existence of preferences based upon contextual information has been provided by Marslen-Wilson and Tyler (1980), who have shown in a number of psycholinguistic experiments that contextual information influences word recognition (see also Crain and Steedman [1982] and Taraban and McClelland [1988]). Lexical preferencing (Ford, Bresnan, and Kaplan 1982) refers to the preference functor categories have for certain arguments. For instance,</context>
<context position="41061" citStr="Schubert (1984" startWordPosition="6345" endWordPosition="6346"> course, possible to order the categories (((np\s)/ (pp, to) A (np\s)) and to order the rules that eliminate boolean connectives (first category first). However, the order of these categories must be stipulated, whereas in the case of the hierarchical lexicon structure presented here, the relation between the categories is linguistically motivated. Frequency of occurrence, that is, giving forms with higher frequency prevalence over those with lower frequency, is not an alternative either: more specific forms do not necessarily appear more frequently than the forms they inherit from. Examples. Schubert (1984; 1986) presents a number of sentences that he claims show a preference for attachment that he claims cannot be explained on the basis of structural, syntactic preferences. The preference to attach, for example, (pp, from + _) to disappearance can, however, be modeled as a lexical preference if disappearance (as well as disappear) (optionally) subcategorizes for this prepositional phrase. The form with the pp then prevails over the form without the pp. The same argument applies to Examples 12-15 (daughter categories are fully specified). Example 11 John was alarmed by the disappearance of the </context>
</contexts>
<marker>Schubert, 1984</marker>
<rawString>Schubert, L. (1984). &amp;quot;On parsing preferences.&amp;quot; In Proceedings, COLING 1984, 247-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Schubert</author>
</authors>
<title>Are there preference trade-offs in attachment decisions?&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings, AAAI-86,</booktitle>
<pages>601--605</pages>
<marker>Schubert, 1986</marker>
<rawString>Schubert, L. (1986). &amp;quot;Are there preference trade-offs in attachment decisions?&amp;quot; In Proceedings, AAAI-86, 601-605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Schweigert</author>
</authors>
<title>The comprehension of familiar and less familiar idioms.&amp;quot;</title>
<date>1986</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>15</volume>
<pages>33--45</pages>
<contexts>
<context position="42787" citStr="Schweigert 1986" startWordPosition="6625" endWordPosition="6626">girl that he met at a dance. marry: ((np \s)/np) met ((np\s)/np) (((np\s)/ pp)/ np) 5.3 Idioms and Parsing Preferences 5.3.1 Conventionality and Idiom Processing. Idiomatic expressions can in most cases be interpreted nonidiomatically as well.&amp;quot; It has, however, frequently been observed that an idiomatic phrase should very rarely be interpreted nonidiomatically (Koller 1977, p. 13; Chafe 1968, p. 123; Gross 1984, p. 278; Swinney 1981, p. 208). Also, psycholinguistic research indicates that in case of ambiguity there is clear preference for the idiomatic reading (Gibbs 1980; Schraw et al. 1988; Schweigert 1986; Schweigert and Moates 1988). The phenomenon that phrases should be interpreted according to their idiomatic, noncompositional, lexical, conventional meaning will be referred to as the &apos;conventionality&apos; principle (Gibbs 1980). The application of this principle is not limited to idioms. For instance, compounds are not interpreted compositionally, but according to the lexical, conventional meaning (Swinney 1981). Words are formed by regular rules, but their meaning will undergo &apos;semantic drift,&apos; obscuring the compositional nature of the complex word. If this principle could be modeled in an app</context>
</contexts>
<marker>Schweigert, 1986</marker>
<rawString>Schweigert, W. (1986). &amp;quot;The comprehension of familiar and less familiar idioms.&amp;quot; Journal of Psycholinguistic Research, 15, 33-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Schweigert</author>
<author>D Moates</author>
</authors>
<title>Familiar idiom comprehension.&amp;quot;</title>
<date>1988</date>
<journal>Journal of Psycholinguistic Research,17,</journal>
<pages>281--296</pages>
<contexts>
<context position="42816" citStr="Schweigert and Moates 1988" startWordPosition="6627" endWordPosition="6630">at a dance. marry: ((np \s)/np) met ((np\s)/np) (((np\s)/ pp)/ np) 5.3 Idioms and Parsing Preferences 5.3.1 Conventionality and Idiom Processing. Idiomatic expressions can in most cases be interpreted nonidiomatically as well.&amp;quot; It has, however, frequently been observed that an idiomatic phrase should very rarely be interpreted nonidiomatically (Koller 1977, p. 13; Chafe 1968, p. 123; Gross 1984, p. 278; Swinney 1981, p. 208). Also, psycholinguistic research indicates that in case of ambiguity there is clear preference for the idiomatic reading (Gibbs 1980; Schraw et al. 1988; Schweigert 1986; Schweigert and Moates 1988). The phenomenon that phrases should be interpreted according to their idiomatic, noncompositional, lexical, conventional meaning will be referred to as the &apos;conventionality&apos; principle (Gibbs 1980). The application of this principle is not limited to idioms. For instance, compounds are not interpreted compositionally, but according to the lexical, conventional meaning (Swinney 1981). Words are formed by regular rules, but their meaning will undergo &apos;semantic drift,&apos; obscuring the compositional nature of the complex word. If this principle could be modeled in an appropriate way, this would be o</context>
</contexts>
<marker>Schweigert, Moates, 1988</marker>
<rawString>Schweigert, W., and Moates, D. (1988). &amp;quot;Familiar idiom comprehension.&amp;quot; Journal of Psycholinguistic Research,17, 281-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>Sentence disambiguation by shift-reduce parsing technique.&amp;quot;</title>
<date>1983</date>
<booktitle>In Proceedings, IJCAI 1983,</booktitle>
<pages>699--703</pages>
<contexts>
<context position="38692" citStr="Shieber (1983)" startWordPosition="5975" endWordPosition="5976">ls. Therefore, if it is equally possible to model the behavior of the parser as a lexically guided or as, for instance, a contextually guided process, the former should be preferred. For instance, in the case of an idiomatic expression, it is more efficient to decide that the idiom should be interpreted on the basis of the mere fact that it is an idiom than on the basis of consultation of, for instance, some model of the context. Since lexical preferences are successful heuristics that operate on a low level, there is sufficient reason to model them in a principled and formal way. 12 See also Shieber (1983) and Hobbs and Bear (1990). 230 Erik-Jan van der Linden Incremental Processing and the Hierarchical Lexicon 5.2 Formalization of Lexical Preferences The formalization of lexical preferences proposed here is another application of the principle of priority to the instance (Hudson 1984): the parser prefers information lower on in the hierarchical structure of the lexicon over information on higher levels in the hierarchy. If two subcategorization frames of, for instance, go maintain an inheritance relation (np\s &gt;&gt; (np\s)/ (pp, to + _)), and both apply, the more specific frame is preferred. The </context>
</contexts>
<marker>Shieber, 1983</marker>
<rawString>Shieber, S. (1983). &amp;quot;Sentence disambiguation by shift-reduce parsing technique.&amp;quot; In Proceedings, IJCAI 1983, 699-703.</rawString>
</citation>
<citation valid="true">
<title>Lexical Ambiguity Resolution.</title>
<date>1988</date>
<editor>Small, S.; Cottrell, G.; and Tanenhaus, M. (eds.)</editor>
<publisher>Kaufmann.</publisher>
<location>San Mateo:</location>
<contexts>
<context position="36976" citStr="[1988]" startWordPosition="5693" endWordPosition="5693">ples 9 and 10. The modifiers in both cases are preferably attached contrary to expectations on the basis of syntactic preferences (see Schubert 1984, 1986; Wilks, Huang, and Fass 1985). Example 9 John met the girl that he married at the dance. Example 10 John saw the bird with the red beak. Evidence for the existence of preferences based upon contextual information has been provided by Marslen-Wilson and Tyler (1980), who have shown in a number of psycholinguistic experiments that contextual information influences word recognition (see also Crain and Steedman [1982] and Taraban and McClelland [1988]). Lexical preferencing (Ford, Bresnan, and Kaplan 1982) refers to the preference functor categories have for certain arguments. For instance, the verb to go can either occur as an intransitive verb that can be modified by a pp with the prosodic form to + X, or it can take this pp as an argument. The second frame is the preferred frame. The prepositional phrase should preferably be considered as an argument to the verb and not as a vp-modifier. Although the existence of all of these preferences should thus be acknowledged, there are two arguments in favor of lexical preferences. Firstly, from </context>
</contexts>
<marker>1988</marker>
<rawString>Small, S.; Cottrell, G.; and Tanenhaus, M. (eds.) (1988). Lexical Ambiguity Resolution. San Mateo: Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Combinatory grammars and parasitic gaps.&amp;quot; In Working Papers in Cognitive Science, Volume 1. Categorical Grammar, Unification Grammar and Parsing, edited by</title>
<date>1987</date>
<pages>30--70</pages>
<institution>Centre for Cognitive Science, University of Edinburgh.</institution>
<contexts>
<context position="8845" citStr="Steedman 1987" startWordPosition="1333" endWordPosition="1334"> El if (np,john,john) (np,john,john) [Axiom] and (s,john+loves+mary,loves(mary)(john)) (s,john+loves+mary,loves(mary)(john)) [Axiom] 2.2 Other Connectives The product-free version of the Lambek calculus includes two connectives, / and \ On the basis of these connectives and the inference rules of the L-calculus, a range of linguistic constructions and generalizations remain for which no linguistically adequate accounts can be presented. In order to overcome this problem, new category-forming connectives have been proposed (as a lexical alternative of the specialized rules of for instance CCG [Steedman 1987]). An example is the connective 1- for unbounded dependencies (Moortgat 1988). Xi Y denotes a category X that has an argument of category Y missing somewhere within X. The constituent John put on the table in what John put on the table has as its syntactic category s I np. To what the category s/(s I np) is assigned, which takes the incomplete clause as its argument. The A-connective (Morrill 1990) is one of a set of boolean connectives that can be used to denote that a certain lexical item can occur in different categories: square can be n/n and n, and is therefore assigned the category (n/n</context>
<context position="21535" citStr="Steedman 1987" startWordPosition="3339" endWordPosition="3340"> can the vp be formed and combined with the subject to form an s (Briscoe 1987). A process such as this cannot be called incremental. Although subject and verb can be processed incrementally independently from each other, this is not the case for their combination. The strategy mostly used in incremental CG-processing is to enable the construction of a semantic structure with the use of principles that concatenate all possible adjacent categories (although some exceptions are made for coordinate structures [Dowty 1988; Houtman 1987]). In Combinatory Categorial Grammar (Ades and Steedman 1982; Steedman 1987), for instance, composition and lifting rules (Definitions 3 and 4) enable incremental interpretation (Example 6). To make use of these rules in a proof-theoretic approach to CG, a rule that cuts the result of these rules in the proof as a whole (Definition 5) is necessary. Definition 3 (X /Y,a) (Y / Z,b) (X /Z, Ax.a(b(x))) [Comp] Definition 4 (X,a) = (Z/ (X \ Z), )b.b(a)) [Lift] 6 The first parser that featured incremental processing can be found in Marcus (1980). This parser did not consider lexical ambiguity and confined itself to syntactic processing. Other computational models that entail</context>
</contexts>
<marker>Steedman, 1987</marker>
<rawString>Steedman, M. (1987). &amp;quot;Combinatory grammars and parasitic gaps.&amp;quot; In Working Papers in Cognitive Science, Volume 1. Categorical Grammar, Unification Grammar and Parsing, edited by N. Haddock, E. Klein, and G. Morrill, 30-70. Centre for Cognitive Science, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stock</author>
</authors>
<title>Parsing with flexibility, dynamic strategies, and idioms in mind.&amp;quot;</title>
<date>1989</date>
<journal>Computational Linguistics,</journal>
<pages>15--1</pages>
<contexts>
<context position="22655" citStr="Stock (1989)" startWordPosition="3510" endWordPosition="3511">mbiguity and confined itself to syntactic processing. Other computational models that entail the notion of incrementality can be found for Segment Grammar and in Word Expert Parsing. The subsymbolic processing architecture for Segment Grammar presented by Kempen and Vosse (1989) is a model of syntactic processing. The architecture allows for immediate interpretation, but no semantic representation is actually constructed. Adriaens (1986) presents a lexicalist model, Word Expert Parsing, which operates incrementally. Another lexicalist model that features incremental processing can be found in Stock (1989). In none of the models is mention made of a structured lexicon. 225 Computational Linguistics Volume 18, Number 2 Definition 5 U, X, Y, V Z [Cut] if X, Y = W and U, W, V Z Example 6 (np,john) ((np \ s)/np,kicks) (np/n,the) (n,boy) = (s, kicks(the(boy))(john)) [Cut] if (np,john) (s/(np \ s), AX.X(john) ) [Lift] and (s/(np \ s), )X.X(john) ) ((np \ s)/np,kicks) (np/n,the) (n,boy) (s, kicks(the(boy))(john)) [Cut] if (s/(np \ s), AX.X(john) ) ((np \ s)/np,kicks) (s/np, Ax.kicks(x)(john)) [Comp] and (s/np, Ax.kicks(x)(john)) (np/n,the) (n,boy) (s, kicks(the(boy))(john)) [Cut] if (s/np, )x.kicks(x)</context>
</contexts>
<marker>Stock, 1989</marker>
<rawString>Stock, 0. (1989). &amp;quot;Parsing with flexibility, dynamic strategies, and idioms in mind.&amp;quot; Computational Linguistics, 15,1-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Swinney</author>
</authors>
<title>Lexical access during sentence comprehension: (Re)consideration of context effects.&amp;quot;</title>
<date>1979</date>
<journal>Journal of Verbal Learning and Verbal Behaviour,</journal>
<pages>18--645</pages>
<marker>Swinney, 1979</marker>
<rawString>Swinney, D. (1979). &amp;quot;Lexical access during sentence comprehension: (Re)consideration of context effects.&amp;quot; Journal of Verbal Learning and Verbal Behaviour, 18,645-659.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Swinney</author>
</authors>
<title>Lexical processing during sentence comprehension: Effects of higher order constraints and implications for representation.&amp;quot;</title>
<date>1981</date>
<booktitle>In The Cognitive Representation of Speech,</booktitle>
<publisher>North-Holland.</publisher>
<note>edited by</note>
<contexts>
<context position="42608" citStr="Swinney 1981" startWordPosition="6597" endWordPosition="6598">_inf)) Example 14 Sue had difficulties with her teachers. difficulties: n &gt;&gt; (n/ (pp,with + -)) Example 15 a. John met the girl that he married at a dance. b. John married the girl that he met at a dance. marry: ((np \s)/np) met ((np\s)/np) (((np\s)/ pp)/ np) 5.3 Idioms and Parsing Preferences 5.3.1 Conventionality and Idiom Processing. Idiomatic expressions can in most cases be interpreted nonidiomatically as well.&amp;quot; It has, however, frequently been observed that an idiomatic phrase should very rarely be interpreted nonidiomatically (Koller 1977, p. 13; Chafe 1968, p. 123; Gross 1984, p. 278; Swinney 1981, p. 208). Also, psycholinguistic research indicates that in case of ambiguity there is clear preference for the idiomatic reading (Gibbs 1980; Schraw et al. 1988; Schweigert 1986; Schweigert and Moates 1988). The phenomenon that phrases should be interpreted according to their idiomatic, noncompositional, lexical, conventional meaning will be referred to as the &apos;conventionality&apos; principle (Gibbs 1980). The application of this principle is not limited to idioms. For instance, compounds are not interpreted compositionally, but according to the lexical, conventional meaning (Swinney 1981). Words</context>
</contexts>
<marker>Swinney, 1981</marker>
<rawString>Swinney, D. (1981). &amp;quot;Lexical processing during sentence comprehension: Effects of higher order constraints and implications for representation.&amp;quot; In The Cognitive Representation of Speech, edited by T. Meyers, J. Laver, and J. Anderson. North-Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Taraban</author>
<author>J McClelland</author>
</authors>
<title>Constituent attachment and thematic role assignment in sentence processing: Influences of content-based expectations.&amp;quot;</title>
<date>1988</date>
<journal>Journal of Memory and Language,</journal>
<volume>27</volume>
<pages>597--632</pages>
<marker>Taraban, McClelland, 1988</marker>
<rawString>Taraban, R., and McClelland, J. (1988). &amp;quot;Constituent attachment and thematic role assignment in sentence processing: Influences of content-based expectations.&amp;quot; Journal of Memory and Language, 27, 597-632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Touretzky</author>
</authors>
<title>The Mathematics of Inheritance Systems.</title>
<date>1986</date>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>Los Altos, CA:</location>
<contexts>
<context position="14735" citStr="Touretzky 1986" startWordPosition="2284" endWordPosition="2285"> constants and variables; constants unify with variables and with constants with an equal value (prosodic information in Example 4). If the values do not unify, the value of the daughter is returned (semantic information in Example 4). Example 4 (KICK n KICK_TV) n KICK_THE_BUCKET: ((np\s)/ (np, the + bucket, _), kick, AxAydie(y)) The inheritance networks for which n is defined are unipolar, nonmonotonic, and homogeneous (Touretzky, Horty, and Thomason 1987). For other networks, other reasoning mechanisms are necessary to determine the properties of a node (Touretzky, Horty, and Thomason 1987; Touretzky 1986; Veltman 1990).5 More specific information thus takes precedence over more general information. This is a common feature of inheritance systems, and is an application of &apos;proper inclusion precedence,&apos; which is acknowledged in knowledge representation and (computational) linguistics (De Smedt 1990; Daelemans 1987; other papers in this special issue). There exists a clear relation between this principle and the linguistic notion blocking. Blocking is &amp;quot;the nonoccurrence of one form due to the simple existence of another&amp;quot; (Aronoff 1976, p. 41). For instance, the nominal derivation *graciosity of </context>
<context position="16028" citStr="Touretzky (1986)" startWordPosition="2486" endWordPosition="2487">(1990) show that in a hierarchical lexicon structure, blocking is equivalent to the prevalence of more specific information over more general information. For instance, the more general principle in the example is that a nominal derivation of some abstract adjectives equals stem + ity, and the more specific information is that in the case of gracious the nominal derivation is grace. In the hierarchical lexicon, the principle of priority to the instance 4 See van der Linden and Kraaij (1990) and van der Linden (1991) for a more extensive comparison of this definition and the traditional one. 5 Touretzky (1986) also discusses default logic and nonmonotonic logic. 223 Computational Linguistics Volume 18, Number 2 also blocks ?graciousness (whereas this is not the case for Aronoff&apos;s model). In Dutch, the participle *geslaapt that has been formed on the basis of regular morphological processes is blocked because the past participle of sla pen is gesla pen. 3.2 Other Lexical Relations Verbs that can be either transitive or intransitive, such as kick, can in principle be modeled with the use of the A-connective: (np\s, Ayakick(x)(y)) A ((np\s)/ np, )xAykick(x)(y)). There are, however, two generalizations</context>
</contexts>
<marker>Touretzky, 1986</marker>
<rawString>Touretzky, D. (1986). The Mathematics of Inheritance Systems. Los Altos, CA: Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Touretzky</author>
<author>J Horty</author>
<author>R Thomason</author>
</authors>
<title>A clash of intuitions: The current state of non-monotonic multiple inheritance systems.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, IJCAI 1987,</booktitle>
<pages>476--482</pages>
<marker>Touretzky, Horty, Thomason, 1987</marker>
<rawString>Touretzky, D.; Horty, J.; and Thomason, R. (1987). &amp;quot;A clash of intuitions: The current state of non-monotonic multiple inheritance systems.&amp;quot; In Proceedings, IJCAI 1987, 476-482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Veltman</author>
</authors>
<title>Defaults in update semantics I.&amp;quot; In Conditionals, Defaults and Belief Revision, edited by</title>
<date>1990</date>
<pages>28--63</pages>
<note>DYANA Deliverable R2.5.A.</note>
<contexts>
<context position="14750" citStr="Veltman 1990" startWordPosition="2286" endWordPosition="2287">ariables; constants unify with variables and with constants with an equal value (prosodic information in Example 4). If the values do not unify, the value of the daughter is returned (semantic information in Example 4). Example 4 (KICK n KICK_TV) n KICK_THE_BUCKET: ((np\s)/ (np, the + bucket, _), kick, AxAydie(y)) The inheritance networks for which n is defined are unipolar, nonmonotonic, and homogeneous (Touretzky, Horty, and Thomason 1987). For other networks, other reasoning mechanisms are necessary to determine the properties of a node (Touretzky, Horty, and Thomason 1987; Touretzky 1986; Veltman 1990).5 More specific information thus takes precedence over more general information. This is a common feature of inheritance systems, and is an application of &apos;proper inclusion precedence,&apos; which is acknowledged in knowledge representation and (computational) linguistics (De Smedt 1990; Daelemans 1987; other papers in this special issue). There exists a clear relation between this principle and the linguistic notion blocking. Blocking is &amp;quot;the nonoccurrence of one form due to the simple existence of another&amp;quot; (Aronoff 1976, p. 41). For instance, the nominal derivation *graciosity of gracious is blo</context>
</contexts>
<marker>Veltman, 1990</marker>
<rawString>Veltman, F. (1990). &amp;quot;Defaults in update semantics I.&amp;quot; In Conditionals, Defaults and Belief Revision, edited by H. Kamp, 28-63. DYANA Deliverable R2.5.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Whittemore</author>
<author>K Ferrara</author>
<author>H Brunner</author>
</authors>
<title>Empirical study of predictive powers of simple attachment schemes for post-modifier prepositional phrases.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, ACL</booktitle>
<pages>23--30</pages>
<marker>Whittemore, Ferrara, Brunner, 1990</marker>
<rawString>Whittemore, G.; Ferrara, K.; and Brunner, H. (1990). &amp;quot;Empirical study of predictive powers of simple attachment schemes for post-modifier prepositional phrases.&amp;quot; In Proceedings, ACL 1990, 23-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wilensky</author>
<author>Y Arens</author>
</authors>
<title>PHRAN, A knowledge-based natural Computational Linguistics Volume 18, Number 2 language understander.&amp;quot;</title>
<date>1980</date>
<booktitle>In Proceedings,</booktitle>
<pages>779--784</pages>
<contexts>
<context position="43918" citStr="Wilensky and Arens 1980" startWordPosition="6794" endWordPosition="6797">ng the compositional nature of the complex word. If this principle could be modeled in an appropriate way, this would be of considerable help in dealing with idioms. As soon as the idiom has been identified, the ambiguity can be resolved and &apos;higher&apos; knowledge sources do not have to be used to solve the ambiguity. In Stock&apos;s (1989) approach to ambiguity resolution the idiomatic and the nonidiomatic analyses are processed in parallel. An external scheduling function gives priority to one of these analyses. Higher knowledge sources are thus necessary to decide upon the interpretation. In PHRAN (Wilensky and Arens 1980), specificity plays a role, but only in suggesting patterns that match the input: evaluation takes place on the basis of length and order of the patterns. Zernik and Dyer (1987) present lexical representations for idioms, but do not discuss ambiguity. Van der Linden and Kraaij (1990) discuss two alternative formalizations for conventionality. One extends the notion continuation class from two-level morphology. The other is a simple localist connectionist model. Here, another model based upon the specificity of information in the hierarchical structure of the lexicon will be presented. 5.3.2 Co</context>
</contexts>
<marker>Wilensky, Arens, 1980</marker>
<rawString>Wilensky, R., and Arens, Y. (1980). &amp;quot;PHRAN, A knowledge-based natural Computational Linguistics Volume 18, Number 2 language understander.&amp;quot; In Proceedings, 779-784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Zernik</author>
<author>M Dyer</author>
</authors>
<date>1987</date>
<publisher>The</publisher>
<contexts>
<context position="11037" citStr="Zernik and Dyer 1987" startWordPosition="1689" endWordPosition="1692"> sections deal with the extension of the calculus. 3.1 An Example: Idioms An idiomatic expression and its verbal head can be said to maintain a lexical inheritance relation: an idiomatic expression inherits part of its properties from its head. Here, syntactic category, syntactic behavior, morphology, and semantics are discussed briefly. 3.1.1 Syntactic Category. Idiomatic expressions can be represented as functor-argument-structures2 and have the same format as the verbs that are their heads. It is therefore possible to relate the syntactic category of the idiom to that of its head (see also Zernik and Dyer 1987). The verb itself does not specify prosodic information for the argument and the idiom is a specialization of the verb because it does specify prosodic information. In other words, the verb (kick) subcategorizes for the whole set of strings with category np, whereas the idiom subcategorizes for the subset of that set (the + bucket). The information that the object argument is specified for a certain string can thus be added monotonically. Inheritance relations between lexical items are denoted here with a category-forming connective &gt;-. Mother &gt;-- Daughter states that Daughter is a specializat</context>
<context position="44095" citStr="Zernik and Dyer (1987)" startWordPosition="6824" endWordPosition="6827">e idiom has been identified, the ambiguity can be resolved and &apos;higher&apos; knowledge sources do not have to be used to solve the ambiguity. In Stock&apos;s (1989) approach to ambiguity resolution the idiomatic and the nonidiomatic analyses are processed in parallel. An external scheduling function gives priority to one of these analyses. Higher knowledge sources are thus necessary to decide upon the interpretation. In PHRAN (Wilensky and Arens 1980), specificity plays a role, but only in suggesting patterns that match the input: evaluation takes place on the basis of length and order of the patterns. Zernik and Dyer (1987) present lexical representations for idioms, but do not discuss ambiguity. Van der Linden and Kraaij (1990) discuss two alternative formalizations for conventionality. One extends the notion continuation class from two-level morphology. The other is a simple localist connectionist model. Here, another model based upon the specificity of information in the hierarchical structure of the lexicon will be presented. 5.3.2 Conventionality and the Hierarchical Lexicon. The ordering of rules for the &gt;&gt;- operator can also be applied to the &gt;---operator, which relates idioms to verbs. Upon 13 Exceptions</context>
</contexts>
<marker>Zernik, Dyer, 1987</marker>
<rawString>ACL 1980, 117-121. Zernik, U., and Dyer, M. (1987). &amp;quot;The</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
<author>X Huang</author>
<author>D Fass</author>
</authors>
<title>self-extending phrasal lexicon.&amp;quot; &amp;quot;Syntax, preference and right</title>
<date>1985</date>
<journal>Computational Linguistics,</journal>
<volume>13</volume>
<pages>308--327</pages>
<marker>Wilks, Huang, Fass, 1985</marker>
<rawString>Wilks, Y.; Huang, X.; and Fass, D. (1985). self-extending phrasal lexicon.&amp;quot; &amp;quot;Syntax, preference and right Computational Linguistics, 13, 308-327.</rawString>
</citation>
<citation valid="true">
<title>attachment.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings, IJCAI</booktitle>
<marker>1985</marker>
<rawString>attachment.&amp;quot; In Proceedings, IJCAI 1985,</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>