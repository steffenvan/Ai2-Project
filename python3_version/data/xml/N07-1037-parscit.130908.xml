<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001223">
<title confidence="0.996595">
Extracting Semantic Orientations of Phrases from Dictionary
</title>
<author confidence="0.976393">
Hiroya Takamura Takashi Inui
</author>
<affiliation confidence="0.984626">
Precision and Intelligence Laboratory Integrated Research Institute
Tokyo Institute of Technology Tokyo Institute of Technology
</affiliation>
<email confidence="0.99299">
takamura@pi.titech.ac.jp inui@iri.titech.ac.jp
</email>
<author confidence="0.992752">
Manabu Okumura
</author>
<affiliation confidence="0.991251">
Precision and Intelligence Laboratory
Tokyo Institute of Technology
</affiliation>
<email confidence="0.997904">
oku@pi.titech.ac.jp
</email>
<sectionHeader confidence="0.995637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999932217391304">
We propose a method for extracting se-
mantic orientations of phrases (pairs of an
adjective and a noun): positive, negative,
or neutral. Given an adjective, the seman-
tic orientation classification of phrases can
be reduced to the classification of words.
We construct a lexical network by con-
necting similar/related words. In the net-
work, each node has one of the three ori-
entation values and the neighboring nodes
tend to have the same value. We adopt
the Potts model for the probability model
of the lexical network. For each adjec-
tive, we estimate the states of the nodes,
which indicate the semantic orientations
of the adjective-noun pairs. Unlike ex-
isting methods for phrase classification,
the proposed method can classify phrases
consisting of unseen words. We also pro-
pose to use unlabeled data for a seed set of
probability computation. Empirical evalu-
ation shows the effectiveness of the pro-
posed method.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945425">
Technology for affect analysis of texts has recently
gained attention in both academic and industrial ar-
eas. It can be applied to, for example, a survey of
new products or a questionnaire analysis. Automatic
sentiment analysis enables a fast and comprehensive
investigation.
The most fundamental step for sentiment analy-
sis is to acquire the semantic orientations of words:
positive or negative (desirable or undesirable). For
example, the word “beautiful” is positive, while the
word “dirty” is negative. Many researchers have de-
veloped several methods for this purpose and ob-
tained good results. One of the next problems to be
solved is to acquire semantic orientations of phrases,
or multi-term expressions, such as “high+risk” and
“light+laptop-computer”. Indeed the semantic ori-
entations of phrases depend on context just as the se-
mantic orientations of words do, but we would like
to obtain the orientations of phrases as basic units
for sentiment analysis. We believe that we can use
the obtained basic orientations of phrases for affect
analysis of higher linguistic units such as sentences
and documents.
A computational model for the semantic orienta-
tions of phrases has been proposed by Takamura et
al. (2006). However, their method cannot deal with
the words that did not appear in the training data.
The purpose of this paper is to propose a method for
extracting semantic orientations of phrases, which is
applicable also to expressions consisting of unseen
words. In our method, we regard this task as the
noun classification problem for each adjective; the
nouns that become respectively positive (negative,
or neutral) when combined with a given adjective
are distinguished from the other nouns. We create
a lexical network with words being nodes, by con-
necting two words if one of the two appears in the
gloss of the other. In the network, each node has one
of the three orientation values and the neighboring
nodes expectedly tend to have the same value. For
</bodyText>
<page confidence="0.957151">
292
</page>
<note confidence="0.7992">
Proceedings of NAACL HLT 2007, pages 292–299,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999595">
example, the gloss of “cost” is “a sacrifice, loss, or
penalty” and these words (cost, sacrifice, loss, and
penalty) have the same orientation. To capture this
tendency of the network, we adopt the Potts model
for the probability distribution of the lexical net-
work. For each adjective, we estimate the states of
the nodes, which indicate the semantic orientations
of the adjective-noun pairs. Information from seed
words is diffused to unseen nouns on the network.
We also propose a method for enlarging the seed
set by using the output of an existing method for the
seed words of the probability computation.
Empirical evaluation shows that our method
works well both for seen and unseen nouns, and that
the enlarged seed set significantly improves the clas-
sification performance of the proposed model.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999980395348837">
The semantic orientation classification of words has
been pursued by several researchers. Some of
them used corpora (Hatzivassiloglou and McKeown,
1997; Turney and Littman, 2003), while others used
dictionaries (Kobayashi et al., 2001; Kamps et al.,
2004; Takamura et al., 2005; Esuli and Sebastiani,
2005).
Turney (2002) applied an internet-based tech-
nique to the semantic orientation classification of
phrases, which had originally been developed for
word sentiment classification. In their method, the
number of hits returned by a search-engine, with a
query consisting of a phrase and a seed word (e.g.,
“phrase NEAR good”) is used to determine the ori-
entation. Baron and Hirst (2004) extracted colloca-
tions with Xtract (Smadja, 1993) and classified the
collocations using the orientations of the words in
the neighboring sentences. Their method is similar
to Turney’s in the sense that cooccurrence with seed
words is used. In addition to individual seed words,
Kanayama and Nasukawa (2006) used more compli-
cated syntactic patterns that were manually created.
The four methods above are based on context infor-
mation. In contrast, our method exploits the internal
structure of the semantic orientations of phrases.
Wilson et al. (2005) worked on phrase-level se-
mantic orientations. They introduced a polarity
shifter. They manually created the list of polarity
shifters. Inui (2004) also proposed a similar idea.
Takamura et al. (2006) proposed to use based on
latent variable models for sentiment classification of
noun-adjective pairs. Their model consists of vari-
ables respectively representing nouns, adjectives, se-
mantic orientations, and latent clusters, as well as
the edges between the nodes. The words that are
similar in terms of semantic orientations, such as
“risk” and “mortality” (i.e., the positive orientation
emerges when they are “low”), make a cluster in
their model, which can be an automated version of
Inui’s or Wilson et al.’s idea above. However, their
method cannot do anything for the words that did not
appear in the labeled training data. In this paper, we
call their method the latent variable method (LVM).
</bodyText>
<sectionHeader confidence="0.999109" genericHeader="method">
3 Potts Model
</sectionHeader>
<bodyText confidence="0.9999102">
If a variable can have more than two values and
there is no ordering relation between the values,
the network comprised of such variables is called
Potts model (Wu, 1982). In this section, we ex-
plain the simplified mathematical model of Potts
model, which is used for our task in Section 4.
The Potts system has been used as a mathematical
model in several applications such as image restora-
tion (Tanaka and Morita, 1996) and rumor transmis-
sion (Liu et al., 2001).
</bodyText>
<subsectionHeader confidence="0.992939">
3.1 Introduction to the Potts Model
</subsectionHeader>
<bodyText confidence="0.9792844">
Suppose a network consisting of nodes and weighted
edges is given. States of nodes are represented by c.
The weight between i and j is represented by wij.
Let H(c) denote an energy function, which indi-
cates a state of the whole network:
</bodyText>
<equation confidence="0.996861333333333">
� �
H(c) = −Q wijS(ci, cj)+α
ij iEL
</equation>
<bodyText confidence="0.998940583333333">
where Q is a constant called the inverse-temperature,
L is the set of the indices for the observed variables,
ai is the state of each observed variable indexed by i,
and α is a positive constant representing a weight on
labeled data. Function S returns 1 if two arguments
are equal to each other, 0 otherwise. The state is
penalized if ci (i E L) is different from ai. Using
H(c), the probability distribution of the network is
represented as P(c) = exp{−H(c)}/Z, where Z is
a normalization factor.
However, it is computationally difficult to exactly
estimate the state of this network. We resort to a
</bodyText>
<equation confidence="0.955117">
−S(ci, ai), (1)
</equation>
<page confidence="0.994895">
293
</page>
<bodyText confidence="0.9999028">
mean-field approximation method that is described
by Nishimori (2001). In the method, P(c) is re-
placed by factorized function p(c) = Hi pi(ci).
Then we can obtain the function with the smallest
value of the variational free energy:
We can find a similarity also to the PageRank al-
gorithm (Brin and Page, 1998), which has been ap-
plied also to natural language processing tasks (Mi-
halcea, 2004; Mihalcea, 2005). In the PageRank al-
gorithm, the pagerank score ri is updated as
</bodyText>
<equation confidence="0.999863142857143">
� � −P(c) log P(c) � wijrj, (4)
F (c) = P (c)H(c) − ri = (1 − d) + d
c c j
�= −α � ρi(ci)δ(ci, ai)
i ci
ρi(ci)ρj(cj)wijδ(ci, cj)
−ρi(ci) log ρi(ci). (2)
</equation>
<bodyText confidence="0.952758">
By minimizing F(c) under the condition that Vi,
Eci pi(ci) = 1, we obtain the following fixed point
equation for i E L:
</bodyText>
<equation confidence="0.9921945">
exp(αδ(c, ai) + β Ej wijρj(c))(3)
E. exp(αδ(n, ai) + β Ej wijρj (n))
</equation>
<bodyText confidence="0.999970416666667">
The fixed point equation for i E� L can be obtained
by removing aS(c, ai) from above.
This fixed point equation is solved by an itera-
tive computation. In the actual implementation, we
represent pi with a linear combination of the dis-
crete Tchebycheff polynomials (Tanaka and Morita,
1996). Details on the Potts model and its computa-
tion can be found in the literature (Nishimori, 2001).
After the computation, we obtain the function
Hi pi(ci). When the number of classes is 2, the Potts
model in this formulation is equivalent to the mean-
field Ising model (Nishimori, 2001).
</bodyText>
<subsectionHeader confidence="0.999645">
3.2 Relation to Other Models
</subsectionHeader>
<bodyText confidence="0.999824772727273">
This Potts model with the mean-field approximation
has relation to several other models.
As is often discussed (Mackay, 2003), the min-
imization of the variational free energy (Equa-
tion (2)) is equivalent to the obtaining the factorized
model that is most similar to the maximum likeli-
hood model in terms of the Kullback-Leibler diver-
gence.
The second term of Equation (2) is the entropy
of the factorized function. Hence the optimization
problem to be solved here is a kind of the maxi-
mum entropy model with a penalty term, which cor-
responds to the first term of Equation (2).
where d is a constant (0 &lt; d &lt; 1). This update
equation consists of the first term corresponding to
random jump from an arbitrary node and the sec-
ond term corresponding to the random walk from the
neighboring node.
Let us derive the first order Taylor expansion of
Equation (3). We use the equation for i E� L and
denote the denominator by Z,a, for simplicity. Since
exp x Pz� 1 + x, we obtain
</bodyText>
<equation confidence="0.9825555">
exp(β Ej wijρj(c))
Za
1 + β E j wijρj(c)
Za
�
= 1 +β
Za Za
j
</equation>
<bodyText confidence="0.9999721">
Equation (5) clearly has a quite similar form as
Equation (4). Thus, the PageRank algorithm can be
regarded as an approximation of our model. Let us
clarify the difference between the two algorithms.
The PageRank is designed for two-class classifica-
tion, while the Potts model can be used for an arbi-
trary number of classes. In this sense, the PageRank
is an approximated Ising model. The PageRank is
applicable to asymmetric graphs, while the theory
used in this paper is based on symmetric graphs.
</bodyText>
<sectionHeader confidence="0.9564385" genericHeader="method">
4 Potts Model for Phrasal Semantic
Orientations
</sectionHeader>
<bodyText confidence="0.999894">
In this section, we explain our classification method,
which is applicable also to the pairs consisting of an
adjective and an unseen noun.
</bodyText>
<subsectionHeader confidence="0.999888">
4.1 Construction of Lexical Networks
</subsectionHeader>
<bodyText confidence="0.9997785">
We construct a lexical network, which Takamura et
al. (2005) call the gloss network, by linking two
words if one word appears in the gloss of the other
word. Each link belongs to one of two groups:
</bodyText>
<equation confidence="0.863031461538461">
�
−β
ij
E
ci,cj
�
ci
�−
i
ρi(c) =
ρi(c) =
≈
wijρj(c). (5)
</equation>
<page confidence="0.988531">
294
</page>
<bodyText confidence="0.989880428571429">
the same-orientation links SL and the different-
orientation links DL.
If a negation word (e.g., nai, for Japanese) follows
a word in the gloss of the other word, the link is a
different-orientation link. Otherwise the links is a
same-orientation link1.
We next set weights W = (wid) to links:
</bodyText>
<table confidence="0.7985352">
/ &apos; (lid E SL) � (6)
y d(i)d(7)
− 1 (lid E DL)
y d(i)d(7)
0 otherwise
</table>
<bodyText confidence="0.9989754">
where lid denotes the link between word i and word
j, and d(i) denotes the degree of word i, which
means the number of words linked with word i. Two
words without connections are regarded as being
connected by a link of weight 0.
</bodyText>
<subsectionHeader confidence="0.999217">
4.2 Classification of Phrases
</subsectionHeader>
<bodyText confidence="0.999970851851852">
Takamura et al. (2005) used the Ising model to ex-
tract semantic orientations of words (not phrases).
We extend their idea and use the Potts model to ex-
tract semantic orientations of phrasal expressions.
Given an adjective, the decision remaining to be
made in classification of phrasal expressions con-
cerns nouns. We therefore estimate the state of the
nodes on the lexical network for each adjective. The
nouns paring with the given adjective in the train-
ing data are regarded as seed words, which we call
seen words, while the words that did not appear in
the training data are referred to as unseen words.
We use the mean-field method to estimate the
state of the system. If the probability pi(c) of a vari-
able being positive (negative, neutral) is the highest
of the three classes, then the word corresponding to
the variable is classified as a positive (negative, neu-
tral) word.
We explain the reason why we use the Potts model
instead of the Ising model. While only two classes
(i.e., positive and negative) can be modeled by the
Ising model, three classes (i.e., positive, negative
and neutral) can be modelled by the Potts model.
For the semantic orientations of words, all the words
are sorted in the order of the average orientation
value, equivalently the probability of the word be-
ing positive. Therefore, even if the neutral class is
</bodyText>
<footnote confidence="0.561609">
1For English data, a negation should precede a word, in or-
der for the corresponding link to be a different-orientation link.
</footnote>
<bodyText confidence="0.995901571428571">
not explicitly incorporated, we can manually deter-
mine two thresholds that define respectively the pos-
itive/neutral and negative/neutral boundaries. For
the semantic orientations of phrasal expressions,
however, it is impractical to manually determine
the thresholds for each of the numerous adjectives.
Therefore, we have to incorporate the neutral class
using the Potts model.
For some adjectives, the semantic orientation is
constant regardless of the nouns. We need not use
the Potts model for those unambiguous adjectives.
We thus propose the following two-step classifica-
tion procedure for a given noun-adjective pair &lt;
n, a &gt;.
</bodyText>
<listItem confidence="0.981054">
1. if the semantic orientation of all the instances
with a in L is c, then classify &lt; n, a &gt; into c.
2. otherwise, use the Potts model.
</listItem>
<bodyText confidence="0.977716">
We can also construct a probability model for
each noun to deal with unseen adjectives. However,
we focus on the unseen nouns in this paper, because
our dataset has many more nouns than adjectives.
</bodyText>
<subsectionHeader confidence="0.969136">
4.3 Hyper-parameter Prediction
</subsectionHeader>
<bodyText confidence="0.999992111111111">
The performance of the proposed method largely de-
pends on the value of hyper-parameter Q. In order to
make the method more practical, we propose a cri-
terion for determining its value.
Takamura et al. (2005) proposed two kinds of cri-
teria. One of the two criteria is an approximated
leave-one-out error rate and can be used only when a
large labeled dataset is available. The other is a no-
tion from statistical physics, that is, magnetization:
</bodyText>
<equation confidence="0.994004">
E� = xi/N. (7)
i
</equation>
<bodyText confidence="0.999861090909091">
At a high temperature, variables are randomly ori-
ented (paramagnetic phase, m Pz� 0). At a low
temperature, most of the variables have the same
direction (ferromagnetic phase, m =� 0). It is
known that at some intermediate temperature, ferro-
magnetic phase suddenly changes to paramagnetic
phase. This phenomenon is called phase transition.
Slightly before the phase transition, variables are lo-
cally polarized; strongly connected nodes have the
same polarity, but not in a global way. Intuitively,
the state of the lexical network is locally polarized.
</bodyText>
<equation confidence="0.861792">
wig = I
</equation>
<page confidence="0.990332">
295
</page>
<bodyText confidence="0.99998275">
Therefore, they calculate values of m with several
different values of Q and select the value just before
the phase transition.
Since we cannot expect a large labeled dataset
to be available for each adjective, we use not
the approximated leave-one-out error rate, but the
magnetization-like criterion. However, the magne-
tization above is defined for the Ising model. We
therefore consider that the phase transition has oc-
curred, if a certain class c begins to be favored all
over the system. In practice, when the maximum of
the spatial averages of the approximated probabil-
ities max, �i pi(c)/N exceeds a threshold during
increasing Q, we consider that the phase transition
has occurred. We select the value of Q slightly be-
fore the phase transition.
</bodyText>
<subsectionHeader confidence="0.994974">
4.4 Enlarging Seed Word Set
</subsectionHeader>
<bodyText confidence="0.999976142857143">
We usually have only a few seed words for a given
adjective. Enlarging the set of seed words will in-
crease the classification performance. Therefore, we
automatically classify unlabeled pairs by means of
an existing method and use the classified instances
as seeds.
As an existing classifier, we use LVM. Their
model can classify instances that consist of a seen
noun and a seen adjective, but are unseen as a pair.
Although we could classify and use all the nouns
that appeared in the training data (with an adjective
which is different from the given one), we do not
adopt such an alternative, because it will incorporate
even non-collocating pairs such as “green+idea” into
seeds, resulting in possible degradation of classifi-
cation performance. Therefore, we sample unseen
pairs consisting of a seen noun and a seen adjective
from a corpus, classify the pairs with the latent vari-
able model, and add them to the seed set. The en-
larged seed set consists of pairs used in newspaper
articles and does not include non-collocating pairs.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.78014">
5.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999973189189189">
We extracted pairs of a noun (subject) and an ad-
jective (predicate), from Mainichi newspaper arti-
cles (1995) written in Japanese, and annotated the
pairs with semantic orientation tags : positive, neu-
tral or negative. We thus obtained the labeled dataset
consisting of 12066 pair instances (7416 different
pairs). The dataset contains 4459 negative instances,
4252 neutral instances, and 3355 positive instances.
The number of distinct nouns is 4770 and the num-
ber of distinct adjectives is 384. To check the inter-
annotator agreement between two annotators, we
calculated r. statistics, which was 0.6402. This value
is allowable, but not quite high. However, positive-
negative disagreement is observed for only 0.7% of
the data. In other words, this statistics means that
the task of extracting neutral examples, which has
hardly been explored, is intrinsically difficult.
We should note that the judgment in annotation
depends on which perspective the annotator takes;
“high+salary” is positive from employee’s perspec-
tive, but negative from employer’s perspective. The
annotators are supposed to take a perspective subjec-
tively. Our attempt is to imitate annotator’s decision.
To construct a classifier that matches the decision of
the average person, we also have to address how to
create an average corpus. We do not pursue this is-
sue because it is out of the scope of the paper.
As unlabeled data, we extracted approximately
65,000 pairs for each iteration of the 10-fold cross-
validation, from the same news source.
The average number of seed nouns for each am-
biguous adjective was respectively 104 in the la-
beled seed set and 264 in the labeled+unlabeled seed
set. Please note that these figures are counted for
only ambiguous adjectives. Usually ambiguous ad-
jectives are more frequent than unambiguous adjec-
tives.
</bodyText>
<subsectionHeader confidence="0.996258">
5.2 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.9843619">
We employ 10-fold cross-validation to obtain the
averaged classification accuracy. We split the data
such that there is no overlapping pair (i.e., any pair
in the training data does not appear in the test data).
Hyperparameter α was set to 1000, which is very
large since we regard the labels in the seed set is
reliable. For the seed words added by the classifier,
lower α can be better. Determining a good value for
α is regarded as future work.
Hyperparameter Q is automatically selected from
</bodyText>
<footnote confidence="0.9686365">
2Although Kanayama and Nasukawa (2006) that r, for their
dataset similar to ours was 0.83, this value cannot be directly
compared with our value because their dataset includes both in-
dividual words and pairs of words.
</footnote>
<page confidence="0.997185">
296
</page>
<bodyText confidence="0.961979333333333">
10.1, 0.2, · · ·, 2.51 for each adjective and each fold
of the cross-validation using the prediction method
described in Section 4.3.
</bodyText>
<subsectionHeader confidence="0.87097">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999971333333333">
The results of the classification experiments are
summarized in Table 1.
The proposed method succeeded in classifying,
with approximately 65% in accuracy, those phrases
consisting of an ambiguous adjective and an unseen
noun, which could not be classified with existing
computational models such as LVM.
Incorporation of unlabeled data improves accu-
racy by 15.5 points for pairs consisting of a seen
noun and an ambiguous adjective, and by 3.5 points
for pairs consisting of an unseen noun and an am-
biguous adjective, approximately. The reason why
the former obtained high increase is that pairs with
an ambiguous adjective3 are usually frequent and
likely to be found in the added unlabeled dataset.
If we regard this classification task as binary clas-
sification problems where we are to classify in-
stances into one class or not, we obtain three accu-
racies: 90.76% for positive, 81.75% for neutral, and
86.85% for negative. This results suggests the iden-
tification of neutral instances is relatively difficult.
Next we compare the proposed method with
LVM. The latent variable method is applicable only
to instance pairs consisting of an adjective and a
seen noun. Therefore, we computed the accuracy
for 6586 instances using the latent variable method
and obtained 80.76 %. The corresponding accuracy
by our method was 80.93%. This comparison shows
that our method is better than or at least comparable
to the latent variable method. However, we have to
note that this accuracy of the proposed method was
computed using the unlabeled data classified by the
latent variable method.
</bodyText>
<subsectionHeader confidence="0.931322">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.948607875">
There are still 3320 (=12066-8746) word pairs
which could not be classified, because there are no
entries for those words in the dictionary. However,
the main cause of this problem is word segmenta-
3Seen nouns are observed in both the training and the test
datasets because they are frequent. Ambiguous adjectives are
often-used adjectives such as “large”, “small”, “high”, and
“low”.
tion, since many compound nouns and exceedingly-
subdivided morphemes are not in dictionaries. An
appropriate mapping from the words found in cor-
pus to entries of a dictionary will solve this problem.
We found a number of proper nouns, many of which
are not in the dictionary. By estimating a class of a
proper noun and finding the words that matches the
class in the dictionary, we can predict the semantic
orientations of the proper noun based on the orienta-
tions of the found words.
In order to see the overall tendency of errors, we
calculated the confusion matrices both for pairs of
an ambiguous adjective and a seen noun, and for
pairs of an ambiguous adjective and an unseen noun
(Table 2). The proposed method works quite well for
positive/negative classification, though it finds still
some difficulty in correctly classifying neutral in-
stances even after enhanced with the unlabeled data.
In order to qualitatively evaluate the method,
we list several word pairs below. These word
pairs are classified by the Potts model with the la-
beled+unlabeled seed set. All nouns are unseen;
they did not appear in the original training dataset.
Please note again that the actual data is Japanese.
</bodyText>
<figure confidence="0.972408416666667">
positive instances
noun adjective
cost low
basic price low
loss little
intelligence high
educational background high
contagion not-happening
version new
cafe many
salary high
commission low
negative instances
noun adjective
damage heavy
chance little
terrorist many
trouble many
variation little
capacity small
salary low
disaster many
disappointment big
knowledge little
</figure>
<bodyText confidence="0.837841">
For example, although both “salary” and “com-
mission” are kinds of money, our method captures
</bodyText>
<page confidence="0.997135">
297
</page>
<tableCaption confidence="0.984162">
Table 1: Classification accuracies (%) for various seed sets and test datasets. ‘Labeled’ seed set corresponds
</tableCaption>
<bodyText confidence="0.995175">
to the set of manually labeled pairs. ‘Labeled+unlabeled’ seed set corresponds to the union of ‘labeled’ seed
set and the set of pairs labeled by LVM. ‘Seen nouns’ for test are the nouns that appeared in the training
data, while ‘unseen nouns’ are the nouns that did not appear in the training dataset’. Please note that seen
pairs are excluded from the test data. ‘Unambiguous’ adjectives corresponds to the pairs with an adjective
which has a unique orientation in the original training dataset, while ‘ambiguous’ adjectives corresponds to
the pairs with an adjective which has more than one orientation in the original training dataset.
</bodyText>
<table confidence="0.998603727272727">
seed\test seen nouns unseen nouns total
labeled 68.24 73.70 69.59
(4494/6586) (1592/2160) (6086/8746)
unambiguous ambiguous unambiguous ambiguous
98.15 61.65 94.85 61.85
(1166/1188) (3328/5398) (736/776) (856/1384)
labeled+unlabeled 80.93 75.88 79.68
(5330/6586) (1639/2160) (6969/8746)
unambiguous ambiguous unambiguous ambiguous
98.15 77.14 94.85 65.25
(1166/1188) (4164/5398) (736/776) (903/1384)
</table>
<tableCaption confidence="0.985848">
Table 2: Confusion matrices of classification result with labeled+unlabeled seed set
</tableCaption>
<table confidence="0.603231571428571">
Potts model
seen nouns unseen nouns
positive neutral negative sum positive neutral negative sum
Gold standard positive 964 254 60 1278 126 84 30 240
neutral 198 1656 286 2140 60 427 104 591
negative 39 397 1544 1980 46 157 350 553
sum 1201 2307 1890 5398 232 668 484 1384
</table>
<bodyText confidence="0.999741333333333">
the difference between them; “high salary” is posi-
tive, while “low (cheap) commission” is also posi-
tive.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.964136166666667">
We proposed a method for extracting semantic ori-
entations of phrases (pairs of an adjective and a
noun). For each adjective, we constructed a Potts
system, which is actually a lexical network extracted
from glosses in a dictionary. We empirically showed
that the proposed method works well in terms of
classification accuracy.
Future work includes the following:
• We assumed that each word has a semantic ori-
entation. However, word senses and subjectiv-
ity have strong interaction (Wiebe and Mihal-
cea, 2006).
</bodyText>
<listItem confidence="0.698148833333333">
• The value of α must be properly set, because
lower α can be better for the seed words added
by the classifier,
• To address word-segmentation problem dis-
cussed in Section 5.3, we can utilize the fact
that the heads of compound nouns often inherit
the property determining the semantic orienta-
tion when combined with an adjective.
• The semantic orientations of pairs consisting of
a proper noun will be estimated from the named
entity classes of the proper nouns such as per-
son name and organization.
</listItem>
<page confidence="0.997159">
298
</page>
<sectionHeader confidence="0.98835" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999667693069307">
Faye Baron and Graeme Hirst. 2004. Collocations as
cues to semantic orientation. In AAAI Spring Sympo-
sium on Exploring Attitude and Affect in Text: Theo-
ries and Applications.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1–7):107–117.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determin-
ing the semantic orientation of terms through gloss
analysis. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management (CIKM’05), pages 617–624.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics and the
8th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 174–181.
Takashi Inui. 2004. Acquiring Causal Knowledge from
Text Using Connective Markers. Ph.D. thesis, Grad-
uate School of Information Science, Nara Institute of
Science and Technology.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientation of adjectives. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation (LREC’04), volume IV, pages
1115–1118.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP’06), pages 355–363.
Nozomi Kobayashi, Takashi Inui, and Kentaro Inui.
2001. Dictionary-based acquisition of the lexical
knowledge for p/n analysis (in Japanese). In Pro-
ceedings ofJapanese Societyfor Artificial Intelligence,
SLUD-33, pages 45–50.
Zhongzhu Liu, Jun Luo, and Chenggang Shao. 2001.
Potts model for exaggeration of a simple rumor trans-
mitted by recreant rumormongers. Physical Review E,
64:046134,1–046134,9.
David J. C. Mackay. 2003. Information Theory, Infer-
ence and Learning Algorithms. Cambridge University
Press.
Mainichi. 1995. Mainichi Shimbun CD-ROM version.
Rada Mihalcea. 2004. Graph-based ranking algorithms
for sentence extraction, applied to text summarization.
In The Companion Volume to the Proceedings of the
42nd Annual Meeting of the Association for Computa-
tional Linguistics, (ACL’04), pages 170–173.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
the Joint Conference on Human Language Technology
/Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 411–418.
Hidetoshi Nishimori. 2001. Statistical Physics of Spin
Glasses and Information Processing. Oxford Univer-
sity Press.
Frank Z. Smadja. 1993. Retrieving collocations from
text: Xtract. Computational Linguistics, 19(1):143–
177.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL’05), pages 133–140.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2006. Latent variable models for semantic orientations
of phrases. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL’06).
Kazuyuki Tanaka and Tohru Morita. 1996. Application
of cluster variation method to image restoration prob-
lem. In Theory and Applications of the Cluster Vari-
ation and Path Probability Methods, pages 353–373.
Plenum Press, New York.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation Systems, 21(4):315–346.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL’02), pages 417–424.
Janyce M. Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics (COLING-ACL’06), pages 1065–
1072.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of joint confer-
ence on Human Language Technology / Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP’05), pages 347–354.
Fa-Yueh Wu. 1982. The potts model. Reviews of Mod-
ern Physics, 54(1):235–268.
</reference>
<page confidence="0.998656">
299
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.086347">
<title confidence="0.998629">Extracting Semantic Orientations of Phrases from Dictionary</title>
<author confidence="0.847746">Hiroya Takamura Takashi Inui</author>
<affiliation confidence="0.98681">Precision and Intelligence Laboratory Integrated Research Institute Tokyo Institute of Technology Tokyo Institute of Technology</affiliation>
<abstract confidence="0.852203517241379">takamura@pi.titech.ac.jp inui@iri.titech.ac.jp Manabu Precision and Intelligence Tokyo Institute of oku@pi.titech.ac.jp Abstract We propose a method for extracting semantic orientations of phrases (pairs of an adjective and a noun): positive, negative, or neutral. Given an adjective, the semantic orientation classification of phrases can be reduced to the classification of words. We construct a lexical network by connecting similar/related words. In the network, each node has one of the three orientation values and the neighboring nodes tend to have the same value. We adopt the Potts model for the probability model of the lexical network. For each adjective, we estimate the states of the nodes, which indicate the semantic orientations of the adjective-noun pairs. Unlike existing methods for phrase classification, the proposed method can classify phrases consisting of unseen words. We also propose to use unlabeled data for a seed set of probability computation. Empirical evaluation shows the effectiveness of the proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Faye Baron</author>
<author>Graeme Hirst</author>
</authors>
<title>Collocations as cues to semantic orientation.</title>
<date>2004</date>
<booktitle>In AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.</booktitle>
<contexts>
<context position="4918" citStr="Baron and Hirst (2004)" startWordPosition="764" endWordPosition="767">by several researchers. Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words is used. In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created. The four methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Wilson et al. (2005) worked on phrase-level semantic orientations. Th</context>
</contexts>
<marker>Baron, Hirst, 2004</marker>
<rawString>Faye Baron and Graeme Hirst. 2004. Collocations as cues to semantic orientation. In AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual Web search engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<pages>30--1</pages>
<contexts>
<context position="8090" citStr="Brin and Page, 1998" startWordPosition="1297" endWordPosition="1300">he state is penalized if ci (i E L) is different from ai. Using H(c), the probability distribution of the network is represented as P(c) = exp{−H(c)}/Z, where Z is a normalization factor. However, it is computationally difficult to exactly estimate the state of this network. We resort to a −S(ci, ai), (1) 293 mean-field approximation method that is described by Nishimori (2001). In the method, P(c) is replaced by factorized function p(c) = Hi pi(ci). Then we can obtain the function with the smallest value of the variational free energy: We can find a similarity also to the PageRank algorithm (Brin and Page, 1998), which has been applied also to natural language processing tasks (Mihalcea, 2004; Mihalcea, 2005). In the PageRank algorithm, the pagerank score ri is updated as � � −P(c) log P(c) � wijrj, (4) F (c) = P (c)H(c) − ri = (1 − d) + d c c j �= −α � ρi(ci)δ(ci, ai) i ci ρi(ci)ρj(cj)wijδ(ci, cj) −ρi(ci) log ρi(ci). (2) By minimizing F(c) under the condition that Vi, Eci pi(ci) = 1, we obtain the following fixed point equation for i E L: exp(αδ(c, ai) + β Ej wijρj(c))(3) E. exp(αδ(n, ai) + β Ej wijρj (n)) The fixed point equation for i E� L can be obtained by removing aS(c, ai) from above. This fix</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems, 30(1–7):107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Determining the semantic orientation of terms through gloss analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM International Conference on Information and Knowledge Management (CIKM’05),</booktitle>
<pages>617--624</pages>
<contexts>
<context position="4536" citStr="Esuli and Sebastiani, 2005" startWordPosition="704" endWordPosition="707">rging the seed set by using the output of an existing method for the seed words of the probability computation. Empirical evaluation shows that our method works well both for seen and unseen nouns, and that the enlarged seed set significantly improves the classification performance of the proposed model. 2 Related Work The semantic orientation classification of words has been pursued by several researchers. Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence wi</context>
</contexts>
<marker>Esuli, Sebastiani, 2005</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2005. Determining the semantic orientation of terms through gloss analysis. In Proceedings of the 14th ACM International Conference on Information and Knowledge Management (CIKM’05), pages 617–624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>174--181</pages>
<contexts>
<context position="4381" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="680" endWordPosition="683">e the semantic orientations of the adjective-noun pairs. Information from seed words is diffused to unseen nouns on the network. We also propose a method for enlarging the seed set by using the output of an existing method for the seed words of the probability computation. Empirical evaluation shows that our method works well both for seen and unseen nouns, and that the enlarged seed set significantly improves the classification performance of the proposed model. 2 Related Work The semantic orientation classification of words has been pursued by several researchers. Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classifi</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th Conference of the European Chapter of the Association for Computational Linguistics, pages 174–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Inui</author>
</authors>
<title>Acquiring Causal Knowledge from Text Using Connective Markers.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Graduate School of Information Science, Nara Institute of Science and Technology.</institution>
<contexts>
<context position="5616" citStr="Inui (2004)" startWordPosition="872" endWordPosition="873">g the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words is used. In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created. The four methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Wilson et al. (2005) worked on phrase-level semantic orientations. They introduced a polarity shifter. They manually created the list of polarity shifters. Inui (2004) also proposed a similar idea. Takamura et al. (2006) proposed to use based on latent variable models for sentiment classification of noun-adjective pairs. Their model consists of variables respectively representing nouns, adjectives, semantic orientations, and latent clusters, as well as the edges between the nodes. The words that are similar in terms of semantic orientations, such as “risk” and “mortality” (i.e., the positive orientation emerges when they are “low”), make a cluster in their model, which can be an automated version of Inui’s or Wilson et al.’s idea above. However, their metho</context>
</contexts>
<marker>Inui, 2004</marker>
<rawString>Takashi Inui. 2004. Acquiring Causal Knowledge from Text Using Connective Markers. Ph.D. thesis, Graduate School of Information Science, Nara Institute of Science and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaap Kamps</author>
<author>Maarten Marx</author>
<author>Robert J Mokken</author>
<author>Maarten de Rijke</author>
</authors>
<title>Using wordnet to measure semantic orientation of adjectives.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04), volume IV,</booktitle>
<pages>1115--1118</pages>
<marker>Kamps, Marx, Mokken, de Rijke, 2004</marker>
<rawString>Jaap Kamps, Maarten Marx, Robert J. Mokken, and Maarten de Rijke. 2004. Using wordnet to measure semantic orientation of adjectives. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04), volume IV, pages 1115–1118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Kanayama</author>
<author>Tetsuya Nasukawa</author>
</authors>
<title>Fully automatic lexicon expansion for domain-oriented sentiment analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’06),</booktitle>
<pages>355--363</pages>
<contexts>
<context position="5225" citStr="Kanayama and Nasukawa (2006)" startWordPosition="811" endWordPosition="814">mantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words is used. In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created. The four methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Wilson et al. (2005) worked on phrase-level semantic orientations. They introduced a polarity shifter. They manually created the list of polarity shifters. Inui (2004) also proposed a similar idea. Takamura et al. (2006) proposed to use based on latent variable models for sentiment classification of noun-adjective pairs. Their model consists of variables respectively repres</context>
<context position="19559" citStr="Kanayama and Nasukawa (2006)" startWordPosition="3258" endWordPosition="3261">ambiguous adjectives are more frequent than unambiguous adjectives. 5.2 Experimental Settings We employ 10-fold cross-validation to obtain the averaged classification accuracy. We split the data such that there is no overlapping pair (i.e., any pair in the training data does not appear in the test data). Hyperparameter α was set to 1000, which is very large since we regard the labels in the seed set is reliable. For the seed words added by the classifier, lower α can be better. Determining a good value for α is regarded as future work. Hyperparameter Q is automatically selected from 2Although Kanayama and Nasukawa (2006) that r, for their dataset similar to ours was 0.83, this value cannot be directly compared with our value because their dataset includes both individual words and pairs of words. 296 10.1, 0.2, · · ·, 2.51 for each adjective and each fold of the cross-validation using the prediction method described in Section 4.3. 5.3 Results The results of the classification experiments are summarized in Table 1. The proposed method succeeded in classifying, with approximately 65% in accuracy, those phrases consisting of an ambiguous adjective and an unseen noun, which could not be classified with existing </context>
</contexts>
<marker>Kanayama, Nasukawa, 2006</marker>
<rawString>Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully automatic lexicon expansion for domain-oriented sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’06), pages 355–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nozomi Kobayashi</author>
<author>Takashi Inui</author>
<author>Kentaro Inui</author>
</authors>
<title>Dictionary-based acquisition of the lexical knowledge for p/n analysis (in Japanese).</title>
<date>2001</date>
<booktitle>In Proceedings ofJapanese Societyfor Artificial Intelligence, SLUD-33,</booktitle>
<pages>45--50</pages>
<contexts>
<context position="4464" citStr="Kobayashi et al., 2001" startWordPosition="692" endWordPosition="695">d to unseen nouns on the network. We also propose a method for enlarging the seed set by using the output of an existing method for the seed words of the probability computation. Empirical evaluation shows that our method works well both for seen and unseen nouns, and that the enlarged seed set significantly improves the classification performance of the proposed model. 2 Related Work The semantic orientation classification of words has been pursued by several researchers. Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentence</context>
</contexts>
<marker>Kobayashi, Inui, Inui, 2001</marker>
<rawString>Nozomi Kobayashi, Takashi Inui, and Kentaro Inui. 2001. Dictionary-based acquisition of the lexical knowledge for p/n analysis (in Japanese). In Proceedings ofJapanese Societyfor Artificial Intelligence, SLUD-33, pages 45–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongzhu Liu</author>
<author>Jun Luo</author>
<author>Chenggang Shao</author>
</authors>
<title>Potts model for exaggeration of a simple rumor transmitted by recreant rumormongers. Physical Review E,</title>
<date>2001</date>
<pages>64--046134</pages>
<contexts>
<context position="6848" citStr="Liu et al., 2001" startWordPosition="1076" endWordPosition="1079">thing for the words that did not appear in the labeled training data. In this paper, we call their method the latent variable method (LVM). 3 Potts Model If a variable can have more than two values and there is no ordering relation between the values, the network comprised of such variables is called Potts model (Wu, 1982). In this section, we explain the simplified mathematical model of Potts model, which is used for our task in Section 4. The Potts system has been used as a mathematical model in several applications such as image restoration (Tanaka and Morita, 1996) and rumor transmission (Liu et al., 2001). 3.1 Introduction to the Potts Model Suppose a network consisting of nodes and weighted edges is given. States of nodes are represented by c. The weight between i and j is represented by wij. Let H(c) denote an energy function, which indicates a state of the whole network: � � H(c) = −Q wijS(ci, cj)+α ij iEL where Q is a constant called the inverse-temperature, L is the set of the indices for the observed variables, ai is the state of each observed variable indexed by i, and α is a positive constant representing a weight on labeled data. Function S returns 1 if two arguments are equal to each</context>
</contexts>
<marker>Liu, Luo, Shao, 2001</marker>
<rawString>Zhongzhu Liu, Jun Luo, and Chenggang Shao. 2001. Potts model for exaggeration of a simple rumor transmitted by recreant rumormongers. Physical Review E, 64:046134,1–046134,9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C Mackay</author>
</authors>
<title>Information Theory, Inference and Learning Algorithms.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9325" citStr="Mackay, 2003" startWordPosition="1523" endWordPosition="1524">olved by an iterative computation. In the actual implementation, we represent pi with a linear combination of the discrete Tchebycheff polynomials (Tanaka and Morita, 1996). Details on the Potts model and its computation can be found in the literature (Nishimori, 2001). After the computation, we obtain the function Hi pi(ci). When the number of classes is 2, the Potts model in this formulation is equivalent to the meanfield Ising model (Nishimori, 2001). 3.2 Relation to Other Models This Potts model with the mean-field approximation has relation to several other models. As is often discussed (Mackay, 2003), the minimization of the variational free energy (Equation (2)) is equivalent to the obtaining the factorized model that is most similar to the maximum likelihood model in terms of the Kullback-Leibler divergence. The second term of Equation (2) is the entropy of the factorized function. Hence the optimization problem to be solved here is a kind of the maximum entropy model with a penalty term, which corresponds to the first term of Equation (2). where d is a constant (0 &lt; d &lt; 1). This update equation consists of the first term corresponding to random jump from an arbitrary node and the secon</context>
</contexts>
<marker>Mackay, 2003</marker>
<rawString>David J. C. Mackay. 2003. Information Theory, Inference and Learning Algorithms. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mainichi</author>
</authors>
<title>Mainichi Shimbun CD-ROM version.</title>
<date>1995</date>
<marker>Mainichi, 1995</marker>
<rawString>Mainichi. 1995. Mainichi Shimbun CD-ROM version.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Graph-based ranking algorithms for sentence extraction, applied to text summarization.</title>
<date>2004</date>
<booktitle>In The Companion Volume to the Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, (ACL’04),</booktitle>
<pages>170--173</pages>
<contexts>
<context position="8172" citStr="Mihalcea, 2004" startWordPosition="1312" endWordPosition="1314">stribution of the network is represented as P(c) = exp{−H(c)}/Z, where Z is a normalization factor. However, it is computationally difficult to exactly estimate the state of this network. We resort to a −S(ci, ai), (1) 293 mean-field approximation method that is described by Nishimori (2001). In the method, P(c) is replaced by factorized function p(c) = Hi pi(ci). Then we can obtain the function with the smallest value of the variational free energy: We can find a similarity also to the PageRank algorithm (Brin and Page, 1998), which has been applied also to natural language processing tasks (Mihalcea, 2004; Mihalcea, 2005). In the PageRank algorithm, the pagerank score ri is updated as � � −P(c) log P(c) � wijrj, (4) F (c) = P (c)H(c) − ri = (1 − d) + d c c j �= −α � ρi(ci)δ(ci, ai) i ci ρi(ci)ρj(cj)wijδ(ci, cj) −ρi(ci) log ρi(ci). (2) By minimizing F(c) under the condition that Vi, Eci pi(ci) = 1, we obtain the following fixed point equation for i E L: exp(αδ(c, ai) + β Ej wijρj(c))(3) E. exp(αδ(n, ai) + β Ej wijρj (n)) The fixed point equation for i E� L can be obtained by removing aS(c, ai) from above. This fixed point equation is solved by an iterative computation. In the actual implementat</context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>Rada Mihalcea. 2004. Graph-based ranking algorithms for sentence extraction, applied to text summarization. In The Companion Volume to the Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, (ACL’04), pages 170–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technology /Empirical Methods in Natural Language Processing (HLT/EMNLP),</booktitle>
<pages>411--418</pages>
<contexts>
<context position="8189" citStr="Mihalcea, 2005" startWordPosition="1315" endWordPosition="1316">e network is represented as P(c) = exp{−H(c)}/Z, where Z is a normalization factor. However, it is computationally difficult to exactly estimate the state of this network. We resort to a −S(ci, ai), (1) 293 mean-field approximation method that is described by Nishimori (2001). In the method, P(c) is replaced by factorized function p(c) = Hi pi(ci). Then we can obtain the function with the smallest value of the variational free energy: We can find a similarity also to the PageRank algorithm (Brin and Page, 1998), which has been applied also to natural language processing tasks (Mihalcea, 2004; Mihalcea, 2005). In the PageRank algorithm, the pagerank score ri is updated as � � −P(c) log P(c) � wijrj, (4) F (c) = P (c)H(c) − ri = (1 − d) + d c c j �= −α � ρi(ci)δ(ci, ai) i ci ρi(ci)ρj(cj)wijδ(ci, cj) −ρi(ci) log ρi(ci). (2) By minimizing F(c) under the condition that Vi, Eci pi(ci) = 1, we obtain the following fixed point equation for i E L: exp(αδ(c, ai) + β Ej wijρj(c))(3) E. exp(αδ(n, ai) + β Ej wijρj (n)) The fixed point equation for i E� L can be obtained by removing aS(c, ai) from above. This fixed point equation is solved by an iterative computation. In the actual implementation, we represent</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>Rada Mihalcea. 2005. Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling. In Proceedings of the Joint Conference on Human Language Technology /Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 411–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetoshi Nishimori</author>
</authors>
<date>2001</date>
<booktitle>Statistical Physics of Spin Glasses and Information Processing.</booktitle>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="7850" citStr="Nishimori (2001)" startWordPosition="1255" endWordPosition="1256">ndices for the observed variables, ai is the state of each observed variable indexed by i, and α is a positive constant representing a weight on labeled data. Function S returns 1 if two arguments are equal to each other, 0 otherwise. The state is penalized if ci (i E L) is different from ai. Using H(c), the probability distribution of the network is represented as P(c) = exp{−H(c)}/Z, where Z is a normalization factor. However, it is computationally difficult to exactly estimate the state of this network. We resort to a −S(ci, ai), (1) 293 mean-field approximation method that is described by Nishimori (2001). In the method, P(c) is replaced by factorized function p(c) = Hi pi(ci). Then we can obtain the function with the smallest value of the variational free energy: We can find a similarity also to the PageRank algorithm (Brin and Page, 1998), which has been applied also to natural language processing tasks (Mihalcea, 2004; Mihalcea, 2005). In the PageRank algorithm, the pagerank score ri is updated as � � −P(c) log P(c) � wijrj, (4) F (c) = P (c)H(c) − ri = (1 − d) + d c c j �= −α � ρi(ci)δ(ci, ai) i ci ρi(ci)ρj(cj)wijδ(ci, cj) −ρi(ci) log ρi(ci). (2) By minimizing F(c) under the condition that</context>
<context position="9169" citStr="Nishimori, 2001" startWordPosition="1499" endWordPosition="1500">(c))(3) E. exp(αδ(n, ai) + β Ej wijρj (n)) The fixed point equation for i E� L can be obtained by removing aS(c, ai) from above. This fixed point equation is solved by an iterative computation. In the actual implementation, we represent pi with a linear combination of the discrete Tchebycheff polynomials (Tanaka and Morita, 1996). Details on the Potts model and its computation can be found in the literature (Nishimori, 2001). After the computation, we obtain the function Hi pi(ci). When the number of classes is 2, the Potts model in this formulation is equivalent to the meanfield Ising model (Nishimori, 2001). 3.2 Relation to Other Models This Potts model with the mean-field approximation has relation to several other models. As is often discussed (Mackay, 2003), the minimization of the variational free energy (Equation (2)) is equivalent to the obtaining the factorized model that is most similar to the maximum likelihood model in terms of the Kullback-Leibler divergence. The second term of Equation (2) is the entropy of the factorized function. Hence the optimization problem to be solved here is a kind of the maximum entropy model with a penalty term, which corresponds to the first term of Equati</context>
</contexts>
<marker>Nishimori, 2001</marker>
<rawString>Hidetoshi Nishimori. 2001. Statistical Physics of Spin Glasses and Information Processing. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Z Smadja</author>
</authors>
<title>Retrieving collocations from text:</title>
<date>1993</date>
<journal>Xtract. Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>177</pages>
<contexts>
<context position="4968" citStr="Smadja, 1993" startWordPosition="773" endWordPosition="774">siloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words is used. In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created. The four methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Wilson et al. (2005) worked on phrase-level semantic orientations. They introduced a polarity shifter. They manually cr</context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>Frank Z. Smadja. 1993. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143– 177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Extracting semantic orientations of words using spin model.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>133--140</pages>
<contexts>
<context position="4507" citStr="Takamura et al., 2005" startWordPosition="700" endWordPosition="703">opose a method for enlarging the seed set by using the output of an existing method for the seed words of the probability computation. Empirical evaluation shows that our method works well both for seen and unseen nouns, and that the enlarged seed set significantly improves the classification performance of the proposed model. 2 Related Work The semantic orientation classification of words has been pursued by several researchers. Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in t</context>
<context position="11017" citStr="Takamura et al. (2005)" startWordPosition="1824" endWordPosition="1827">fference between the two algorithms. The PageRank is designed for two-class classification, while the Potts model can be used for an arbitrary number of classes. In this sense, the PageRank is an approximated Ising model. The PageRank is applicable to asymmetric graphs, while the theory used in this paper is based on symmetric graphs. 4 Potts Model for Phrasal Semantic Orientations In this section, we explain our classification method, which is applicable also to the pairs consisting of an adjective and an unseen noun. 4.1 Construction of Lexical Networks We construct a lexical network, which Takamura et al. (2005) call the gloss network, by linking two words if one word appears in the gloss of the other word. Each link belongs to one of two groups: � −β ij E ci,cj � ci �− i ρi(c) = ρi(c) = ≈ wijρj(c). (5) 294 the same-orientation links SL and the differentorientation links DL. If a negation word (e.g., nai, for Japanese) follows a word in the gloss of the other word, the link is a different-orientation link. Otherwise the links is a same-orientation link1. We next set weights W = (wid) to links: / &apos; (lid E SL) � (6) y d(i)d(7) − 1 (lid E DL) y d(i)d(7) 0 otherwise where lid denotes the link between wor</context>
<context position="14516" citStr="Takamura et al. (2005)" startWordPosition="2432" endWordPosition="2435">rocedure for a given noun-adjective pair &lt; n, a &gt;. 1. if the semantic orientation of all the instances with a in L is c, then classify &lt; n, a &gt; into c. 2. otherwise, use the Potts model. We can also construct a probability model for each noun to deal with unseen adjectives. However, we focus on the unseen nouns in this paper, because our dataset has many more nouns than adjectives. 4.3 Hyper-parameter Prediction The performance of the proposed method largely depends on the value of hyper-parameter Q. In order to make the method more practical, we propose a criterion for determining its value. Takamura et al. (2005) proposed two kinds of criteria. One of the two criteria is an approximated leave-one-out error rate and can be used only when a large labeled dataset is available. The other is a notion from statistical physics, that is, magnetization: E� = xi/N. (7) i At a high temperature, variables are randomly oriented (paramagnetic phase, m Pz� 0). At a low temperature, most of the variables have the same direction (ferromagnetic phase, m =� 0). It is known that at some intermediate temperature, ferromagnetic phase suddenly changes to paramagnetic phase. This phenomenon is called phase transition. Slight</context>
</contexts>
<marker>Takamura, Inui, Okumura, 2005</marker>
<rawString>Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientations of words using spin model. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 133–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Latent variable models for semantic orientations of phrases.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL’06).</booktitle>
<contexts>
<context position="2526" citStr="Takamura et al. (2006)" startWordPosition="380" endWordPosition="383">One of the next problems to be solved is to acquire semantic orientations of phrases, or multi-term expressions, such as “high+risk” and “light+laptop-computer”. Indeed the semantic orientations of phrases depend on context just as the semantic orientations of words do, but we would like to obtain the orientations of phrases as basic units for sentiment analysis. We believe that we can use the obtained basic orientations of phrases for affect analysis of higher linguistic units such as sentences and documents. A computational model for the semantic orientations of phrases has been proposed by Takamura et al. (2006). However, their method cannot deal with the words that did not appear in the training data. The purpose of this paper is to propose a method for extracting semantic orientations of phrases, which is applicable also to expressions consisting of unseen words. In our method, we regard this task as the noun classification problem for each adjective; the nouns that become respectively positive (negative, or neutral) when combined with a given adjective are distinguished from the other nouns. We create a lexical network with words being nodes, by connecting two words if one of the two appears in th</context>
<context position="5669" citStr="Takamura et al. (2006)" startWordPosition="879" endWordPosition="882">ghboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words is used. In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created. The four methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Wilson et al. (2005) worked on phrase-level semantic orientations. They introduced a polarity shifter. They manually created the list of polarity shifters. Inui (2004) also proposed a similar idea. Takamura et al. (2006) proposed to use based on latent variable models for sentiment classification of noun-adjective pairs. Their model consists of variables respectively representing nouns, adjectives, semantic orientations, and latent clusters, as well as the edges between the nodes. The words that are similar in terms of semantic orientations, such as “risk” and “mortality” (i.e., the positive orientation emerges when they are “low”), make a cluster in their model, which can be an automated version of Inui’s or Wilson et al.’s idea above. However, their method cannot do anything for the words that did not appea</context>
</contexts>
<marker>Takamura, Inui, Okumura, 2006</marker>
<rawString>Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2006. Latent variable models for semantic orientations of phrases. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuyuki Tanaka</author>
<author>Tohru Morita</author>
</authors>
<title>Application of cluster variation method to image restoration problem.</title>
<date>1996</date>
<booktitle>In Theory and Applications of the Cluster Variation and Path Probability Methods,</booktitle>
<pages>353--373</pages>
<publisher>Plenum Press,</publisher>
<location>New York.</location>
<contexts>
<context position="6806" citStr="Tanaka and Morita, 1996" startWordPosition="1068" endWordPosition="1071">s idea above. However, their method cannot do anything for the words that did not appear in the labeled training data. In this paper, we call their method the latent variable method (LVM). 3 Potts Model If a variable can have more than two values and there is no ordering relation between the values, the network comprised of such variables is called Potts model (Wu, 1982). In this section, we explain the simplified mathematical model of Potts model, which is used for our task in Section 4. The Potts system has been used as a mathematical model in several applications such as image restoration (Tanaka and Morita, 1996) and rumor transmission (Liu et al., 2001). 3.1 Introduction to the Potts Model Suppose a network consisting of nodes and weighted edges is given. States of nodes are represented by c. The weight between i and j is represented by wij. Let H(c) denote an energy function, which indicates a state of the whole network: � � H(c) = −Q wijS(ci, cj)+α ij iEL where Q is a constant called the inverse-temperature, L is the set of the indices for the observed variables, ai is the state of each observed variable indexed by i, and α is a positive constant representing a weight on labeled data. Function S re</context>
<context position="8884" citStr="Tanaka and Morita, 1996" startWordPosition="1448" endWordPosition="1451">(c) log P(c) � wijrj, (4) F (c) = P (c)H(c) − ri = (1 − d) + d c c j �= −α � ρi(ci)δ(ci, ai) i ci ρi(ci)ρj(cj)wijδ(ci, cj) −ρi(ci) log ρi(ci). (2) By minimizing F(c) under the condition that Vi, Eci pi(ci) = 1, we obtain the following fixed point equation for i E L: exp(αδ(c, ai) + β Ej wijρj(c))(3) E. exp(αδ(n, ai) + β Ej wijρj (n)) The fixed point equation for i E� L can be obtained by removing aS(c, ai) from above. This fixed point equation is solved by an iterative computation. In the actual implementation, we represent pi with a linear combination of the discrete Tchebycheff polynomials (Tanaka and Morita, 1996). Details on the Potts model and its computation can be found in the literature (Nishimori, 2001). After the computation, we obtain the function Hi pi(ci). When the number of classes is 2, the Potts model in this formulation is equivalent to the meanfield Ising model (Nishimori, 2001). 3.2 Relation to Other Models This Potts model with the mean-field approximation has relation to several other models. As is often discussed (Mackay, 2003), the minimization of the variational free energy (Equation (2)) is equivalent to the obtaining the factorized model that is most similar to the maximum likeli</context>
</contexts>
<marker>Tanaka, Morita, 1996</marker>
<rawString>Kazuyuki Tanaka and Tohru Morita. 1996. Application of cluster variation method to image restoration problem. In Theory and Applications of the Cluster Variation and Path Probability Methods, pages 353–373. Plenum Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="4408" citStr="Turney and Littman, 2003" startWordPosition="684" endWordPosition="687">djective-noun pairs. Information from seed words is diffused to unseen nouns on the network. We also propose a method for enlarging the seed set by using the output of an existing method for the seed words of the probability computation. Empirical evaluation shows that our method works well both for seen and unseen nouns, and that the enlarged seed set significantly improves the classification performance of the proposed model. 2 Related Work The semantic orientation classification of words has been pursued by several researchers. Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using t</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21(4):315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings 40th Annual Meeting of the Association for Computational Linguistics (ACL’02),</booktitle>
<pages>417--424</pages>
<contexts>
<context position="4551" citStr="Turney (2002)" startWordPosition="708" endWordPosition="709">he output of an existing method for the seed words of the probability computation. Empirical evaluation shows that our method works well both for seen and unseen nouns, and that the enlarged seed set significantly improves the classification performance of the proposed model. 2 Related Work The semantic orientation classification of words has been pursued by several researchers. Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words i</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proceedings 40th Annual Meeting of the Association for Computational Linguistics (ACL’02), pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
<author>Rada Mihalcea</author>
</authors>
<title>Word sense and subjectivity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics (COLING-ACL’06),</booktitle>
<pages>1065--1072</pages>
<marker>Wiebe, Mihalcea, 2006</marker>
<rawString>Janyce M. Wiebe and Rada Mihalcea. 2006. Word sense and subjectivity. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics (COLING-ACL’06), pages 1065– 1072.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of joint conference on Human Language Technology / Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP’05),</booktitle>
<pages>347--354</pages>
<contexts>
<context position="5469" citStr="Wilson et al. (2005)" startWordPosition="849" endWordPosition="852">R good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words is used. In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created. The four methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Wilson et al. (2005) worked on phrase-level semantic orientations. They introduced a polarity shifter. They manually created the list of polarity shifters. Inui (2004) also proposed a similar idea. Takamura et al. (2006) proposed to use based on latent variable models for sentiment classification of noun-adjective pairs. Their model consists of variables respectively representing nouns, adjectives, semantic orientations, and latent clusters, as well as the edges between the nodes. The words that are similar in terms of semantic orientations, such as “risk” and “mortality” (i.e., the positive orientation emerges w</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of joint conference on Human Language Technology / Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP’05), pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fa-Yueh Wu</author>
</authors>
<title>The potts model.</title>
<date>1982</date>
<journal>Reviews of Modern Physics,</journal>
<volume>54</volume>
<issue>1</issue>
<contexts>
<context position="6555" citStr="Wu, 1982" startWordPosition="1026" endWordPosition="1027">hat are similar in terms of semantic orientations, such as “risk” and “mortality” (i.e., the positive orientation emerges when they are “low”), make a cluster in their model, which can be an automated version of Inui’s or Wilson et al.’s idea above. However, their method cannot do anything for the words that did not appear in the labeled training data. In this paper, we call their method the latent variable method (LVM). 3 Potts Model If a variable can have more than two values and there is no ordering relation between the values, the network comprised of such variables is called Potts model (Wu, 1982). In this section, we explain the simplified mathematical model of Potts model, which is used for our task in Section 4. The Potts system has been used as a mathematical model in several applications such as image restoration (Tanaka and Morita, 1996) and rumor transmission (Liu et al., 2001). 3.1 Introduction to the Potts Model Suppose a network consisting of nodes and weighted edges is given. States of nodes are represented by c. The weight between i and j is represented by wij. Let H(c) denote an energy function, which indicates a state of the whole network: � � H(c) = −Q wijS(ci, cj)+α ij </context>
</contexts>
<marker>Wu, 1982</marker>
<rawString>Fa-Yueh Wu. 1982. The potts model. Reviews of Modern Physics, 54(1):235–268.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>