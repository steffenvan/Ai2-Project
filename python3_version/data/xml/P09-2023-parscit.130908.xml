<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009442">
<title confidence="0.9988585">
Predicting Barge-in Utterance Errors by using
Implicitly Supervised ASR Accuracy and Barge-in Rate per User
</title>
<author confidence="0.994498">
Kazunori Komatani
</author>
<affiliation confidence="0.9955015">
Graduate School of Informatics
Kyoto University
</affiliation>
<address confidence="0.908663">
Yoshida, Sakyo, Kyoto 606-8501, Japan
</address>
<email confidence="0.9996">
komatani@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999846052631579">
Modeling of individual users is a promis-
ing way of improving the performance of
spoken dialogue systems deployed for the
general public and utilized repeatedly. We
define “implicitly-supervised” ASR accu-
racy per user on the basis of responses
following the system’s explicit confirma-
tions. We combine the estimated ASR ac-
curacy with the user’s barge-in rate, which
represents how well the user is accus-
tomed to using the system, to predict in-
terpretation errors in barge-in utterances.
Experimental results showed that the es-
timated ASR accuracy improved predic-
tion performance. Since this ASR accu-
racy and the barge-in rate are obtainable
at runtime, they improve prediction perfor-
mance without the need for manual label-
ing.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999811736842106">
The automatic speech recognition (ASR) result
is the most important input information for spo-
ken dialogue systems, and therefore, its errors are
critical problems. Many researchers have tackled
this problem by developing ASR confidence mea-
sures based on utterance-level information and
dialogue-level information (Litman et al., 1999;
Walker et al., 2000). Especially in systems de-
ployed for the general public such as those of (Ko-
matani et al., 2005) and (Raux et al., 2006), the
systems need to correctly detect interpretation er-
rors caused by various utterances made by vari-
ous kinds of users including novices. Furthermore,
since some users access such systems repeatedly
(Komatani et al., 2007), error detection by using
individual user models would be a promising way
of improving performance.
In another aspect in dialogue systems, cer-
tain dialogue patterns indicate that ASR results
</bodyText>
<author confidence="0.618927">
Alexander I. Rudnicky
</author>
<affiliation confidence="0.928158666666667">
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213, U.S.A.
</affiliation>
<email confidence="0.984632">
air@cs.cmu.edu
</email>
<bodyText confidence="0.999970703703704">
in certain positions are reliable. For exam-
ple, Sudoh and Nakano (2005) proposed “post-
dialogue confidence scoring” in which ASR re-
sults corresponding to the user’s intention upon
dialogue completion are assumed to be correct
and are used for confidence scoring. Bohus and
Rudnicky (2007) proposed “implicitly-supervised
learning” in which users’ responses following the
system’s explicit confirmations are used for confi-
dence scoring. If ASR results can be regarded as
reliable after the dialogue, machine learning algo-
rithms can use such ASR results as teacher signals.
This approach enables the system to improve its
performance without any manual labeling or tran-
scription, a task which requires much time and la-
bor when spoken dialogue systems are developed.
We focus on users’ affirmative and negative re-
sponses to the system’s explicit confirmations as
in (Bohus and Rudnicky, 2007) and estimate the
user’s ASR accuracy on the basis of his or her his-
tory of responses. The estimated ASR accuracy is
combined with the user’s barge-in rate to predict
the interpretation error in the current barge-in ut-
terance. Because the estimated ASR accuracy and
the barge-in rate per user are obtainable at runtime,
it is possible to improve prediction performance
without any manual transcription or labeling.
</bodyText>
<sectionHeader confidence="0.998119" genericHeader="method">
2 Implicitly Supervised Estimation of
ASR Accuracy
</sectionHeader>
<subsectionHeader confidence="0.999933">
2.1 Predicting Errors in Barge-in Utterance
</subsectionHeader>
<bodyText confidence="0.999420777777778">
We aim to predict interpretation errors in barge-
in utterances at runtime. These errors are caused
by ASR errors, and barge-in utterances are more
prone to be misrecognized. A user study con-
ducted by Rose and Kim (2003) revealed that there
are many more disfluencies when users barge-in
compared with when users wait until the system
prompt ends. It is difficult to select the erroneous
utterances to be rejected by using a classifier that
</bodyText>
<page confidence="0.997995">
89
</page>
<note confidence="0.926115">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 89–92,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.99988316">
distinguishes speech from noise on the basis of the
Gaussian Mixture Model (Lee et al., 2004); such
disfluencies and resulting utterance fragments are
parts of human speech.
Barge-in utterances are, therefore, more diffi-
cult to recognize correctly, especially when novice
users barge-in. To detect their interpretation er-
rors, other features should be incorporated instead
of speech signals or ASR results. We predicted
the interpretation errors in barge-in utterances on
the basis of each user’s barge-in rate (Komatani et
al., 2008). This rate intuitively corresponds to how
well users are accustomed to using the system, es-
pecially to its barge-in function.
Furthermore, we utilize a user’s ASR accuracy
in his or her history of all utterances including
barge-ins. The ASR accuracy also indicates the
user’s habituation. However, it has been shown
that the user’s ASR accuracy and barge-in rate
do not improve simultaneously (Komatani et al.,
2007). In fact, some expert users have low barge-
in rates. We thus can predict whether a barge-in
utterance will be correctly interpreted or not by
integrating the user’s current ASR accuracy and
barge-in rate.
</bodyText>
<subsectionHeader confidence="0.993766">
2.2 Estimating ASR Accuracy by using
Implicitly Supervised Labels
</subsectionHeader>
<bodyText confidence="0.982878913043478">
To perform runtime prediction, we use informa-
tion derived from the dialogue patterns to estimate
the user’s ASR accuracy. We estimate the accu-
racy on the basis of the user’s history of responses
following the system’s explicit confirmations such
as “Leaving from Kyoto Station. Is that correct?”
Specifically, we assume that the ASR results
of affirmative or negative responses following ex-
plicit confirmations are correct and that the user
utterances corresponding to the content of the af-
firmative responses are also correct. We further
assume that the remaining utterances are incorrect
because users do not often respond with “no” for
explicit confirmations containing incorrect content
and instead repeat their original utterances. Con-
sequently, we regard that the ASR results of the
following utterances are correct: (1) affirmative
responses and their immediately preceding utter-
ances and (2) negative responses. Accordingly, all
other utterances are incorrect. We thus calculate
the user’s estimated ASR accuracy by using the
user’s utterance history, as follows:
(Estimated ASR accuracy)
</bodyText>
<equation confidence="0.99173775">
2 � (#affirmatives) + (#negatives)
=
(1)
(#all utterances)
</equation>
<subsectionHeader confidence="0.887765">
2.3 Predicting Errors by Using Barge-in Rate
and ASR Accuracy
</subsectionHeader>
<bodyText confidence="0.9997365">
We predict the errors in barge-in utterances by us-
ing a logistic regression function:
</bodyText>
<equation confidence="0.978172">
1 + exp(−(a1x1 + a2x2 + b))
</equation>
<bodyText confidence="0.999962380952381">
Its inputs x1 and x2 are the barge-in rate until the
current utterance and ASR accuracy until the pre-
vious utterance. To account for temporal changes
in barge-in rates, we set a window when calculat-
ing them (Komatani et al., 2008). That is, when
the window width is N, the rates are calculated by
using only the last N utterances, and the previous
utterances are discarded. When the window width
exceeds the total number of utterances by the user,
the barge-in rates are calculated by using all the
user’s utterances. Thus, when the width exceeds
2,838, the maximum number of utterances made
by one user in our data, the barge-in rates equal
the average rates of all previous utterances by the
user.
We calculate the estimated ASR accuracy every
time a user makes an affirmative or negative re-
sponse. When the user makes other utterances, we
take the estimated accuracy when the last affirma-
tive/negative response is made to be the accuracy
of those utterances.
</bodyText>
<sectionHeader confidence="0.999859" genericHeader="method">
3 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999744">
3.1 Target Data
</subsectionHeader>
<bodyText confidence="0.999949866666667">
We used data collected by the Kyoto City Bus In-
formation System (Komatani et al., 2005). This
system locates a bus that a user wants to ride and
tells the user how long it will be before the bus
arrives. The system was accessible to the public
by telephone. It used the safest strategy to prevent
erroneous responses, that is, to make explicit con-
firmations for all ASR results.
We used 27,519 utterances after removing calls
whose phone numbers were not recorded and
those the system developer called for debugging.
From that number, there were 7,193 barge-in ut-
terances, i.e., utterances that a user starts speaking
during a system prompt. The phone numbers of
the calls were recorded, and we assumed that each
</bodyText>
<equation confidence="0.5336785">
1
P=
</equation>
<page confidence="0.983372">
90
</page>
<tableCaption confidence="0.999523">
Table 1: ASR accuracy by response type
</tableCaption>
<table confidence="0.9917326">
Correct Incorrect Total (Acc.)
Affirmative 9,055 246 9,301 (97.4%)
Negative 2,006 289 2,295 (87.4%)
Other 8,914 7,009 15,923 (57.9%)
Total 19,975 7,544 27,519 (72.6%)
</table>
<figure confidence="0.981132461538462">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.2 0.4 0.6 0.8 1
Transcription-based ASR Accuracy
</figure>
<figureCaption confidence="0.997839">
Figure 1: Correlation between transcription-based
and estimated ASR accuracy
</figureCaption>
<bodyText confidence="0.999793">
number corresponded to one individual. Most of
the numbers were those of mobile phones, which
are usually not shared, so the assumption seems
reasonable.
Each utterance was transcribed and its interpre-
tation result, correct or not, was given manually.
We assumed that an interpretation result for an
utterance was correct if all content words in its
transcription were correctly included in the result.
The result was regarded as an error if any content
words were missed or misrecognized.
</bodyText>
<subsectionHeader confidence="0.99939">
3.2 Verifying Implicitly Supervised Labels
</subsectionHeader>
<bodyText confidence="0.999540777777778">
We confirmed our assumption that the ASR re-
sults of affirmative or negative responses follow-
ing explicit confirmations are correct. We clas-
sified the user utterances into affirmatives, nega-
tives, and other, and calculated the ASR accuracies
(precision rates) as shown in Table 1. Affirmatives
include hai (‘yes’), soudesu (‘that’s right’), OK,
etc; and negatives include iie (‘no’), chigaimasu
(‘I don’t agree’), dame (‘No good’), etc. The ta-
ble indicates that the ASR accuracies of affirma-
tives and negatives were high. One of the reasons
for the high accuracy was that these utterances are
much shorter than other content words, so they
were not confused with other content words. An-
other reason was that the system often gave help
messages such as “Please answer yes or no.”
We then analyzed the correlation between the
transcription-based ASR accuracy and the esti-
</bodyText>
<figure confidence="0.9461745">
1 10 100 1000 10000
Window width
</figure>
<figureCaption confidence="0.9715775">
Figure 2: Prediction accuracy with various win-
dow widths
</figureCaption>
<bodyText confidence="0.9995355">
mated ASR accuracy based on Equation 1. We
plotted the two ASR accuracies in Figure 1 for
26,231 utterances made after at least one affir-
mative/negative response by the user. The corre-
lation coefficient between them was 0.806. Al-
though the assumption that all ASR results of af-
firmative/negative responses are correct might be
strong, the estimated ASR accuracy had a high
correlation with the transcription-based ASR ac-
curacy.
</bodyText>
<subsectionHeader confidence="0.997946">
3.3 Prediction using Implicitly Supervised
Labels
</subsectionHeader>
<bodyText confidence="0.999989583333333">
We measured the prediction accuracy for 7,193
barge-in utterances under several conditions. We
did not set windows when calculating the ASR ac-
curacies and thus used all previous utterances of
the user, because the windows did not improve
prediction accuracy. One of the reasons for this
lack of improvement is that the ASR accuracies
did not change as significantly as the barge-in rates
because the accuracies of frequent users converged
earlier (Komatani et al., 2007).
We first confirmed the effect of the
transcription-based (“correct”, hereafter) ASR
accuracy. As shown in Figure 2 and Table 2,
the prediction accuracy improved by using the
ASR accuracy in addition to the barge-in rate.
The best prediction accuracy (78.6%) was when
the window width of the barge-in rate was 100,
and the accuracy converged when the width was
30. The prediction accuracy was 72.7% when
only the “correct” ASR accuracy was used, and
the prediction accuracy was 71.8% when only
the barge-in rate was used. Thus, the prediction
accuracy was better when both inputs were used
rather than when either input was used. This
</bodyText>
<figure confidence="0.979019363636364">
80
75
70
65
60
55
barge-in rate only
correct ASR acc. + barge-in rate
estimated ASR acc. + barge-in rate
Estimated ASR Accuracy.
Prediction Acc.
</figure>
<page confidence="0.980099">
91
</page>
<sectionHeader confidence="0.854761" genericHeader="evaluation">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.7442495">
We are grateful to Prof. Tatsuya Kawahara of Ky-
oto University who led the project of the Kyoto
City Bus Information System.
Table 2: Best prediction accuracies for each con-
dition and window width w
Conditions (Used inputs) Prediction acc. (%)
barge-in rate 71.8 (w=30)
correct ASR acc. 72.7
+ barge-in rate 78.6 (w=100)
estimated ASR acc. 59.4
+ barge-in rate 74.3 (w=30)
fact indicates that both the barge-in rate and
ASR accuracy have different information and
contribute to the prediction accuracy.
Next, we analyzed the prediction accuracy after
replacing the correct ASR accuracy with the esti-
mated one described in Section 2.2. The best ac-
curacy (74.3%) was when the window width was
30. This accuracy was higher than that of using
only barge-in rates. Hence, the estimated ASR ac-
curacy without manual labeling is effective in pre-
dicting the errors in barge-in utterances at runtime.
</bodyText>
<sectionHeader confidence="0.999533" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99997128">
We proposed a method to estimate the errors in
barge-in utterances by using a novel dialogue-level
feature obtainable at runtime. This method does
not require supervised manual labeling. The esti-
mated ASR accuracy based on the user’s utterance
history was dependable in predicting the errors in
the current utterance. We thus showed that ASR
accuracy can be estimated in an implicitly super-
vised manner.
The information obtained by our method can be
used for confidence scoring. Thus, our future work
will include integrating the proposed features with
bottom-up information such as acoustic-score-
based confidence measures. Additionally, we sim-
ply assumed in this study that all affirmative and
negative responses following the explicit confir-
mation are correct. By modeling this assumption
more precisely, prediction accuracy will improve.
Finally, we identified individuals on the basis of
their telephone numbers. If we utilize user identi-
fication techniques to account for situations when
no speaker information is available beforehand,
this method can be applied to systems other than
telephone-based ones, e.g., to human-robot inter-
action.
</bodyText>
<sectionHeader confidence="0.9984" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999543666666667">
Dan Bohus and Alexander Rudnicky. 2007. Implicitly-
supervised learning in spoken language interfaces: an ap-
plication to the confidence annotation problem. In Proc.
SIGdial Workshop on Discourse and Dialogue, pages
256–264.
Kazunori Komatani, Shinichi Ueno, Tatsuya Kawahara, and
Hiroshi G. Okuno. 2005. User modeling in spoken dia-
logue systems to generate flexible guidance. User Model-
ing and User-Adapted Interaction, 15(1):169–183.
Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G.
Okuno. 2007. Analyzing temporal transition of real user’s
behaviors in a spoken dialogue system. In Proc. INTER-
SPEECH, pages 142–145.
Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G.
Okuno. 2008. Predicting ASR errors by exploiting barge-
in rate of individual users for spoken dialogue systems. In
Proc. INTERSPEECH, pages 183–186.
Akinobu Lee, Keisuke Nakamura, Ryuichi Nisimura, Hiroshi
Saruwatari, and Kiyohiro Shikano. 2004. Noice robust
real world spoken dialogue system using GMM based re-
jection of unintended inputs. In Proc. Int’l Conf. Spoken
Language Processing (ICSLP), pages 173–176.
Diane J. Litman, Marilyn A. Walker, and Michael S. Kearns.
1999. Automatic detection of poor speech recognition at
the dialogue level. In Proc. Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages 309–
316.
Antoine Raux, Dan Bohus, Brian Langner, Alan W. Black,
and Maxine Eskenazi. 2006. Doing research on a de-
ployed spoken dialogue system: One year of Let’s Go!
experience. In Proc. INTERSPEECH.
Richard C. Rose and Hong Kook Kim. 2003. A hy-
brid barge-in procedure for more reliable turn-taking in
human-machine dialog systems. In Proc. IEEE Auto-
matic Speech Recognition and Understanding Workshop
(ASRU), pages 198–203.
Katsuhito Sudoh and Mikio Nanano. 2005. Post-dialogue
confidence scoring for unsupervised statistical language
model training. Speech Communication, 45:387–400.
Marilyn Walker, Irene Langkilde, Jerry Wright, Allen Gorin,
and Diane Litman. 2000. Learning to predict problematic
situations in a spoken dialogue system: Experiments with
How May I Help You? In Proc. North American Chapter
of Association for Computational Linguistics (NAACL),
pages 210–217.
</reference>
<page confidence="0.995943">
92
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.832036">
<title confidence="0.966083">Predicting Barge-in Utterance Errors by using Implicitly Supervised ASR Accuracy and Barge-in Rate per User</title>
<author confidence="0.999138">Kazunori Komatani</author>
<affiliation confidence="0.9998605">Graduate School of Informatics Kyoto University</affiliation>
<address confidence="0.999105">Yoshida, Sakyo, Kyoto 606-8501, Japan</address>
<email confidence="0.988569">komatani@i.kyoto-u.ac.jp</email>
<abstract confidence="0.9951766">Modeling of individual users is a promising way of improving the performance of spoken dialogue systems deployed for the general public and utilized repeatedly. We define “implicitly-supervised” ASR accuracy per user on the basis of responses following the system’s explicit confirmations. We combine the estimated ASR accuracy with the user’s barge-in rate, which represents how well the user is accustomed to using the system, to predict interpretation errors in barge-in utterances. Experimental results showed that the estimated ASR accuracy improved prediction performance. Since this ASR accuracy and the barge-in rate are obtainable at runtime, they improve prediction performance without the need for manual labeling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dan Bohus</author>
<author>Alexander Rudnicky</author>
</authors>
<title>Implicitlysupervised learning in spoken language interfaces: an application to the confidence annotation problem.</title>
<date>2007</date>
<booktitle>In Proc. SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>256--264</pages>
<contexts>
<context position="2286" citStr="Bohus and Rudnicky (2007)" startWordPosition="339" endWordPosition="342">y (Komatani et al., 2007), error detection by using individual user models would be a promising way of improving performance. In another aspect in dialogue systems, certain dialogue patterns indicate that ASR results Alexander I. Rudnicky Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213, U.S.A. air@cs.cmu.edu in certain positions are reliable. For example, Sudoh and Nakano (2005) proposed “postdialogue confidence scoring” in which ASR results corresponding to the user’s intention upon dialogue completion are assumed to be correct and are used for confidence scoring. Bohus and Rudnicky (2007) proposed “implicitly-supervised learning” in which users’ responses following the system’s explicit confirmations are used for confidence scoring. If ASR results can be regarded as reliable after the dialogue, machine learning algorithms can use such ASR results as teacher signals. This approach enables the system to improve its performance without any manual labeling or transcription, a task which requires much time and labor when spoken dialogue systems are developed. We focus on users’ affirmative and negative responses to the system’s explicit confirmations as in (Bohus and Rudnicky, 2007</context>
</contexts>
<marker>Bohus, Rudnicky, 2007</marker>
<rawString>Dan Bohus and Alexander Rudnicky. 2007. Implicitlysupervised learning in spoken language interfaces: an application to the confidence annotation problem. In Proc. SIGdial Workshop on Discourse and Dialogue, pages 256–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunori Komatani</author>
<author>Shinichi Ueno</author>
<author>Tatsuya Kawahara</author>
<author>Hiroshi G Okuno</author>
</authors>
<title>User modeling in spoken dialogue systems to generate flexible guidance. User Modeling and User-Adapted Interaction,</title>
<date>2005</date>
<contexts>
<context position="1438" citStr="Komatani et al., 2005" startWordPosition="211" endWordPosition="215">ormance. Since this ASR accuracy and the barge-in rate are obtainable at runtime, they improve prediction performance without the need for manual labeling. 1 Introduction The automatic speech recognition (ASR) result is the most important input information for spoken dialogue systems, and therefore, its errors are critical problems. Many researchers have tackled this problem by developing ASR confidence measures based on utterance-level information and dialogue-level information (Litman et al., 1999; Walker et al., 2000). Especially in systems deployed for the general public such as those of (Komatani et al., 2005) and (Raux et al., 2006), the systems need to correctly detect interpretation errors caused by various utterances made by various kinds of users including novices. Furthermore, since some users access such systems repeatedly (Komatani et al., 2007), error detection by using individual user models would be a promising way of improving performance. In another aspect in dialogue systems, certain dialogue patterns indicate that ASR results Alexander I. Rudnicky Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213, U.S.A. air@cs.cmu.edu in certain positions are reliable. For </context>
<context position="7612" citStr="Komatani et al., 2005" startWordPosition="1184" endWordPosition="1187">e calculated by using all the user’s utterances. Thus, when the width exceeds 2,838, the maximum number of utterances made by one user in our data, the barge-in rates equal the average rates of all previous utterances by the user. We calculate the estimated ASR accuracy every time a user makes an affirmative or negative response. When the user makes other utterances, we take the estimated accuracy when the last affirmative/negative response is made to be the accuracy of those utterances. 3 Experimental Evaluation 3.1 Target Data We used data collected by the Kyoto City Bus Information System (Komatani et al., 2005). This system locates a bus that a user wants to ride and tells the user how long it will be before the bus arrives. The system was accessible to the public by telephone. It used the safest strategy to prevent erroneous responses, that is, to make explicit confirmations for all ASR results. We used 27,519 utterances after removing calls whose phone numbers were not recorded and those the system developer called for debugging. From that number, there were 7,193 barge-in utterances, i.e., utterances that a user starts speaking during a system prompt. The phone numbers of the calls were recorded,</context>
</contexts>
<marker>Komatani, Ueno, Kawahara, Okuno, 2005</marker>
<rawString>Kazunori Komatani, Shinichi Ueno, Tatsuya Kawahara, and Hiroshi G. Okuno. 2005. User modeling in spoken dialogue systems to generate flexible guidance. User Modeling and User-Adapted Interaction, 15(1):169–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunori Komatani</author>
<author>Tatsuya Kawahara</author>
<author>Hiroshi G Okuno</author>
</authors>
<title>Analyzing temporal transition of real user’s behaviors in a spoken dialogue system.</title>
<date>2007</date>
<booktitle>In Proc. INTERSPEECH,</booktitle>
<pages>142--145</pages>
<contexts>
<context position="1686" citStr="Komatani et al., 2007" startWordPosition="251" endWordPosition="254">ormation for spoken dialogue systems, and therefore, its errors are critical problems. Many researchers have tackled this problem by developing ASR confidence measures based on utterance-level information and dialogue-level information (Litman et al., 1999; Walker et al., 2000). Especially in systems deployed for the general public such as those of (Komatani et al., 2005) and (Raux et al., 2006), the systems need to correctly detect interpretation errors caused by various utterances made by various kinds of users including novices. Furthermore, since some users access such systems repeatedly (Komatani et al., 2007), error detection by using individual user models would be a promising way of improving performance. In another aspect in dialogue systems, certain dialogue patterns indicate that ASR results Alexander I. Rudnicky Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213, U.S.A. air@cs.cmu.edu in certain positions are reliable. For example, Sudoh and Nakano (2005) proposed “postdialogue confidence scoring” in which ASR results corresponding to the user’s intention upon dialogue completion are assumed to be correct and are used for confidence scoring. Bohus and Rudnicky (2007)</context>
<context position="4918" citStr="Komatani et al., 2007" startWordPosition="751" endWordPosition="754">her features should be incorporated instead of speech signals or ASR results. We predicted the interpretation errors in barge-in utterances on the basis of each user’s barge-in rate (Komatani et al., 2008). This rate intuitively corresponds to how well users are accustomed to using the system, especially to its barge-in function. Furthermore, we utilize a user’s ASR accuracy in his or her history of all utterances including barge-ins. The ASR accuracy also indicates the user’s habituation. However, it has been shown that the user’s ASR accuracy and barge-in rate do not improve simultaneously (Komatani et al., 2007). In fact, some expert users have low bargein rates. We thus can predict whether a barge-in utterance will be correctly interpreted or not by integrating the user’s current ASR accuracy and barge-in rate. 2.2 Estimating ASR Accuracy by using Implicitly Supervised Labels To perform runtime prediction, we use information derived from the dialogue patterns to estimate the user’s ASR accuracy. We estimate the accuracy on the basis of the user’s history of responses following the system’s explicit confirmations such as “Leaving from Kyoto Station. Is that correct?” Specifically, we assume that the </context>
<context position="11061" citStr="Komatani et al., 2007" startWordPosition="1743" endWordPosition="1746"> strong, the estimated ASR accuracy had a high correlation with the transcription-based ASR accuracy. 3.3 Prediction using Implicitly Supervised Labels We measured the prediction accuracy for 7,193 barge-in utterances under several conditions. We did not set windows when calculating the ASR accuracies and thus used all previous utterances of the user, because the windows did not improve prediction accuracy. One of the reasons for this lack of improvement is that the ASR accuracies did not change as significantly as the barge-in rates because the accuracies of frequent users converged earlier (Komatani et al., 2007). We first confirmed the effect of the transcription-based (“correct”, hereafter) ASR accuracy. As shown in Figure 2 and Table 2, the prediction accuracy improved by using the ASR accuracy in addition to the barge-in rate. The best prediction accuracy (78.6%) was when the window width of the barge-in rate was 100, and the accuracy converged when the width was 30. The prediction accuracy was 72.7% when only the “correct” ASR accuracy was used, and the prediction accuracy was 71.8% when only the barge-in rate was used. Thus, the prediction accuracy was better when both inputs were used rather th</context>
</contexts>
<marker>Komatani, Kawahara, Okuno, 2007</marker>
<rawString>Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G. Okuno. 2007. Analyzing temporal transition of real user’s behaviors in a spoken dialogue system. In Proc. INTERSPEECH, pages 142–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunori Komatani</author>
<author>Tatsuya Kawahara</author>
<author>Hiroshi G Okuno</author>
</authors>
<title>Predicting ASR errors by exploiting bargein rate of individual users for spoken dialogue systems.</title>
<date>2008</date>
<booktitle>In Proc. INTERSPEECH,</booktitle>
<pages>183--186</pages>
<contexts>
<context position="4501" citStr="Komatani et al., 2008" startWordPosition="685" endWordPosition="688">rence Short Papers, pages 89–92, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP distinguishes speech from noise on the basis of the Gaussian Mixture Model (Lee et al., 2004); such disfluencies and resulting utterance fragments are parts of human speech. Barge-in utterances are, therefore, more difficult to recognize correctly, especially when novice users barge-in. To detect their interpretation errors, other features should be incorporated instead of speech signals or ASR results. We predicted the interpretation errors in barge-in utterances on the basis of each user’s barge-in rate (Komatani et al., 2008). This rate intuitively corresponds to how well users are accustomed to using the system, especially to its barge-in function. Furthermore, we utilize a user’s ASR accuracy in his or her history of all utterances including barge-ins. The ASR accuracy also indicates the user’s habituation. However, it has been shown that the user’s ASR accuracy and barge-in rate do not improve simultaneously (Komatani et al., 2007). In fact, some expert users have low bargein rates. We thus can predict whether a barge-in utterance will be correctly interpreted or not by integrating the user’s current ASR accura</context>
<context position="6751" citStr="Komatani et al., 2008" startWordPosition="1038" endWordPosition="1041">er utterances are incorrect. We thus calculate the user’s estimated ASR accuracy by using the user’s utterance history, as follows: (Estimated ASR accuracy) 2 � (#affirmatives) + (#negatives) = (1) (#all utterances) 2.3 Predicting Errors by Using Barge-in Rate and ASR Accuracy We predict the errors in barge-in utterances by using a logistic regression function: 1 + exp(−(a1x1 + a2x2 + b)) Its inputs x1 and x2 are the barge-in rate until the current utterance and ASR accuracy until the previous utterance. To account for temporal changes in barge-in rates, we set a window when calculating them (Komatani et al., 2008). That is, when the window width is N, the rates are calculated by using only the last N utterances, and the previous utterances are discarded. When the window width exceeds the total number of utterances by the user, the barge-in rates are calculated by using all the user’s utterances. Thus, when the width exceeds 2,838, the maximum number of utterances made by one user in our data, the barge-in rates equal the average rates of all previous utterances by the user. We calculate the estimated ASR accuracy every time a user makes an affirmative or negative response. When the user makes other utt</context>
</contexts>
<marker>Komatani, Kawahara, Okuno, 2008</marker>
<rawString>Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G. Okuno. 2008. Predicting ASR errors by exploiting bargein rate of individual users for spoken dialogue systems. In Proc. INTERSPEECH, pages 183–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akinobu Lee</author>
<author>Keisuke Nakamura</author>
<author>Ryuichi Nisimura</author>
<author>Hiroshi Saruwatari</author>
<author>Kiyohiro Shikano</author>
</authors>
<title>Noice robust real world spoken dialogue system using GMM based rejection of unintended inputs.</title>
<date>2004</date>
<booktitle>In Proc. Int’l Conf. Spoken Language Processing (ICSLP),</booktitle>
<pages>173--176</pages>
<contexts>
<context position="4060" citStr="Lee et al., 2004" startWordPosition="621" endWordPosition="624">gein utterances at runtime. These errors are caused by ASR errors, and barge-in utterances are more prone to be misrecognized. A user study conducted by Rose and Kim (2003) revealed that there are many more disfluencies when users barge-in compared with when users wait until the system prompt ends. It is difficult to select the erroneous utterances to be rejected by using a classifier that 89 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 89–92, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP distinguishes speech from noise on the basis of the Gaussian Mixture Model (Lee et al., 2004); such disfluencies and resulting utterance fragments are parts of human speech. Barge-in utterances are, therefore, more difficult to recognize correctly, especially when novice users barge-in. To detect their interpretation errors, other features should be incorporated instead of speech signals or ASR results. We predicted the interpretation errors in barge-in utterances on the basis of each user’s barge-in rate (Komatani et al., 2008). This rate intuitively corresponds to how well users are accustomed to using the system, especially to its barge-in function. Furthermore, we utilize a user’s</context>
</contexts>
<marker>Lee, Nakamura, Nisimura, Saruwatari, Shikano, 2004</marker>
<rawString>Akinobu Lee, Keisuke Nakamura, Ryuichi Nisimura, Hiroshi Saruwatari, and Kiyohiro Shikano. 2004. Noice robust real world spoken dialogue system using GMM based rejection of unintended inputs. In Proc. Int’l Conf. Spoken Language Processing (ICSLP), pages 173–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Marilyn A Walker</author>
<author>Michael S Kearns</author>
</authors>
<title>Automatic detection of poor speech recognition at the dialogue level.</title>
<date>1999</date>
<booktitle>In Proc. Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>309--316</pages>
<contexts>
<context position="1320" citStr="Litman et al., 1999" startWordPosition="190" endWordPosition="193">errors in barge-in utterances. Experimental results showed that the estimated ASR accuracy improved prediction performance. Since this ASR accuracy and the barge-in rate are obtainable at runtime, they improve prediction performance without the need for manual labeling. 1 Introduction The automatic speech recognition (ASR) result is the most important input information for spoken dialogue systems, and therefore, its errors are critical problems. Many researchers have tackled this problem by developing ASR confidence measures based on utterance-level information and dialogue-level information (Litman et al., 1999; Walker et al., 2000). Especially in systems deployed for the general public such as those of (Komatani et al., 2005) and (Raux et al., 2006), the systems need to correctly detect interpretation errors caused by various utterances made by various kinds of users including novices. Furthermore, since some users access such systems repeatedly (Komatani et al., 2007), error detection by using individual user models would be a promising way of improving performance. In another aspect in dialogue systems, certain dialogue patterns indicate that ASR results Alexander I. Rudnicky Computer Science Dep</context>
</contexts>
<marker>Litman, Walker, Kearns, 1999</marker>
<rawString>Diane J. Litman, Marilyn A. Walker, and Michael S. Kearns. 1999. Automatic detection of poor speech recognition at the dialogue level. In Proc. Annual Meeting of the Association for Computational Linguistics (ACL), pages 309– 316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Raux</author>
<author>Dan Bohus</author>
<author>Brian Langner</author>
<author>Alan W Black</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Doing research on a deployed spoken dialogue system: One year of Let’s Go! experience.</title>
<date>2006</date>
<booktitle>In Proc. INTERSPEECH.</booktitle>
<contexts>
<context position="1462" citStr="Raux et al., 2006" startWordPosition="217" endWordPosition="220">racy and the barge-in rate are obtainable at runtime, they improve prediction performance without the need for manual labeling. 1 Introduction The automatic speech recognition (ASR) result is the most important input information for spoken dialogue systems, and therefore, its errors are critical problems. Many researchers have tackled this problem by developing ASR confidence measures based on utterance-level information and dialogue-level information (Litman et al., 1999; Walker et al., 2000). Especially in systems deployed for the general public such as those of (Komatani et al., 2005) and (Raux et al., 2006), the systems need to correctly detect interpretation errors caused by various utterances made by various kinds of users including novices. Furthermore, since some users access such systems repeatedly (Komatani et al., 2007), error detection by using individual user models would be a promising way of improving performance. In another aspect in dialogue systems, certain dialogue patterns indicate that ASR results Alexander I. Rudnicky Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213, U.S.A. air@cs.cmu.edu in certain positions are reliable. For example, Sudoh and Nakan</context>
</contexts>
<marker>Raux, Bohus, Langner, Black, Eskenazi, 2006</marker>
<rawString>Antoine Raux, Dan Bohus, Brian Langner, Alan W. Black, and Maxine Eskenazi. 2006. Doing research on a deployed spoken dialogue system: One year of Let’s Go! experience. In Proc. INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard C Rose</author>
<author>Hong Kook Kim</author>
</authors>
<title>A hybrid barge-in procedure for more reliable turn-taking in human-machine dialog systems.</title>
<date>2003</date>
<booktitle>In Proc. IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),</booktitle>
<pages>198--203</pages>
<contexts>
<context position="3615" citStr="Rose and Kim (2003)" startWordPosition="549" endWordPosition="552">racy is combined with the user’s barge-in rate to predict the interpretation error in the current barge-in utterance. Because the estimated ASR accuracy and the barge-in rate per user are obtainable at runtime, it is possible to improve prediction performance without any manual transcription or labeling. 2 Implicitly Supervised Estimation of ASR Accuracy 2.1 Predicting Errors in Barge-in Utterance We aim to predict interpretation errors in bargein utterances at runtime. These errors are caused by ASR errors, and barge-in utterances are more prone to be misrecognized. A user study conducted by Rose and Kim (2003) revealed that there are many more disfluencies when users barge-in compared with when users wait until the system prompt ends. It is difficult to select the erroneous utterances to be rejected by using a classifier that 89 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 89–92, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP distinguishes speech from noise on the basis of the Gaussian Mixture Model (Lee et al., 2004); such disfluencies and resulting utterance fragments are parts of human speech. Barge-in utterances are, therefore, more difficult to recognize correctly,</context>
</contexts>
<marker>Rose, Kim, 2003</marker>
<rawString>Richard C. Rose and Hong Kook Kim. 2003. A hybrid barge-in procedure for more reliable turn-taking in human-machine dialog systems. In Proc. IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 198–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsuhito Sudoh</author>
<author>Mikio Nanano</author>
</authors>
<title>Post-dialogue confidence scoring for unsupervised statistical language model training. Speech Communication,</title>
<date>2005</date>
<pages>45--387</pages>
<marker>Sudoh, Nanano, 2005</marker>
<rawString>Katsuhito Sudoh and Mikio Nanano. 2005. Post-dialogue confidence scoring for unsupervised statistical language model training. Speech Communication, 45:387–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Irene Langkilde</author>
<author>Jerry Wright</author>
<author>Allen Gorin</author>
<author>Diane Litman</author>
</authors>
<title>Learning to predict problematic situations in a spoken dialogue system: Experiments with How May I Help You?</title>
<date>2000</date>
<booktitle>In Proc. North American Chapter of Association for Computational Linguistics (NAACL),</booktitle>
<pages>210--217</pages>
<contexts>
<context position="1342" citStr="Walker et al., 2000" startWordPosition="194" endWordPosition="197">terances. Experimental results showed that the estimated ASR accuracy improved prediction performance. Since this ASR accuracy and the barge-in rate are obtainable at runtime, they improve prediction performance without the need for manual labeling. 1 Introduction The automatic speech recognition (ASR) result is the most important input information for spoken dialogue systems, and therefore, its errors are critical problems. Many researchers have tackled this problem by developing ASR confidence measures based on utterance-level information and dialogue-level information (Litman et al., 1999; Walker et al., 2000). Especially in systems deployed for the general public such as those of (Komatani et al., 2005) and (Raux et al., 2006), the systems need to correctly detect interpretation errors caused by various utterances made by various kinds of users including novices. Furthermore, since some users access such systems repeatedly (Komatani et al., 2007), error detection by using individual user models would be a promising way of improving performance. In another aspect in dialogue systems, certain dialogue patterns indicate that ASR results Alexander I. Rudnicky Computer Science Department Carnegie Mello</context>
</contexts>
<marker>Walker, Langkilde, Wright, Gorin, Litman, 2000</marker>
<rawString>Marilyn Walker, Irene Langkilde, Jerry Wright, Allen Gorin, and Diane Litman. 2000. Learning to predict problematic situations in a spoken dialogue system: Experiments with How May I Help You? In Proc. North American Chapter of Association for Computational Linguistics (NAACL), pages 210–217.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>