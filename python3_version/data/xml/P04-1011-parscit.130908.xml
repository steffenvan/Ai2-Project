<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.999626">
Trainable Sentence Planning for Complex Information
Presentation in Spoken Dialog Systems
</title>
<author confidence="0.94608">
Amanda Stent
</author>
<affiliation confidence="0.910834">
Stony Brook University
</affiliation>
<address confidence="0.459363">
Stony Brook, NY 11794
U.S.A.
</address>
<email confidence="0.998193">
stent@cs.sunysb.edu
</email>
<author confidence="0.990227">
Rashmi Prasad
</author>
<affiliation confidence="0.995166">
University of Pennsylvania
</affiliation>
<address confidence="0.6076315">
Philadelphia, PA 19104
U.S.A.
</address>
<email confidence="0.998062">
rjprasad@linc.cis.upenn.edu
</email>
<author confidence="0.982738">
Marilyn Walker
</author>
<affiliation confidence="0.989472">
University of Sheffield
</affiliation>
<address confidence="0.5741635">
Sheffield S1 4DP
U.K.
</address>
<email confidence="0.994853">
M.A.Walker@sheffield.ac.uk
</email>
<sectionHeader confidence="0.993732" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999839923076923">
A challenging problem for spoken dialog sys-
tems is the design of utterance generation mod-
ules that are fast, flexible and general, yet pro-
duce high quality output in particular domains.
A promising approach is trainable generation,
which uses general-purpose linguistic knowledge
automatically adapted to the application do-
main. This paper presents a trainable sentence
planner for the MATCH dialog system. We
show that trainable sentence planning can pro-
duce output comparable to that of MATCH’s
template-based generator even for quite com-
plex information presentations.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999878071428572">
One very challenging problem for spoken dialog
systems is the design of the utterance genera-
tion module. This challenge arises partly from
the need for the generator to adapt to many
features of the dialog domain, user population,
and dialog context.
There are three possible approaches to gener-
ating system utterances. The first is template-
based generation, used in most dialog systems
today. Template-based generation enables a
programmer without linguistic training to pro-
gram a generator that can efficiently produce
high quality output specific to different dialog
situations. Its drawbacks include the need to
(1) create templates anew by hand for each ap-
plication; (2) design and maintain a set of tem-
plates that work well together in many dialog
contexts; and (3) repeatedly encode linguistic
constraints such as subject-verb agreement.
The second approach is natural language gen-
eration (NLG), which divides generation into:
(1) text (or content) planning, (2) sentence
planning, and (3) surface realization. NLG
promises portability across domains and dialog
contexts by using general rules for each genera-
tion module. However, the quality of the output
for a particular domain, or a particular dialog
context, may be inferior to that of a template-
based system unless domain-specific rules are
developed or general rules are tuned for the par-
ticular domain. Furthermore, full NLG may be
too slow for use in dialog systems.
A third, more recent, approach is trainable
generation: techniques for automatically train-
ing NLG modules, or hybrid techniques that
adapt NLG modules to particular domains or
user groups, e.g. (Langkilde, 2000; Mellish,
1998; Walker, Rambow and Rogati, 2002).
Open questions about the trainable approach
include (1) whether the output quality is high
enough, and (2) whether the techniques work
well across domains. For example, the training
method used in SPoT (Sentence Planner Train-
able), as described in (Walker, Rambow and Ro-
gati, 2002), was only shown to work in the travel
domain, for the information gathering phase of
the dialog, and with simple content plans in-
volving no rhetorical relations.
This paper describes trainable sentence
planning for information presentation in the
MATCH (Multimodal Access To City Help) di-
alog system (Johnston et al., 2002). We pro-
vide evidence that the trainable approach is
feasible by showing (1) that the training tech-
nique used for SPoT can be extended to a
new domain (restaurant information); (2) that
this technique, previously used for information-
gathering utterances, can be used for infor-
mation presentations, namely recommendations
and comparisons; and (3) that the quality
of the output is comparable to that of a
template-based generator previously developed
and experimentally evaluated with MATCH
users (Walker et al., 2002; Stent et al., 2002).
Section 2 describes SPaRKy (Sentence Plan-
ning with Rhetorical Knowledge), an extension
of SPoT that uses rhetorical relations. SPaRKy
consists of a randomized sentence plan gen-
erator (SPG) and a trainable sentence plan
ranker (SPR); these are described in Sections 3
</bodyText>
<note confidence="0.7539498">
strategy:recommend
items: Chanpen Thai
relations:justify(nuc:1;sat:2); justify(nuc:1;sat:3); jus-
tify(nuc:1;sat:4)
content: 1. assert(best(Chanpen Thai))
</note>
<sectionHeader confidence="0.562939" genericHeader="introduction">
2. assert(has-att(Chanpen Thai, decor(decent)))
3. assert(has-att(Chanpen Thai, service(good))
4. assert(has-att(Chanpen Thai, cuisine(Thai)))
</sectionHeader>
<figureCaption confidence="0.951395">
Figure 1: A content plan for a recommendation
for a restaurant in midtown Manhattan
</figureCaption>
<table confidence="0.3688545">
strategy:compare3
items: Above, Carmine’s
</table>
<tableCaption confidence="0.3767216">
relations:elaboration(1;2); elaboration(1;3); elabora-
tion(1,4); elaboration(1,5); elaboration(1,6);
elaboration(1,7); contrast(2;3); contrast(4;5);
contrast(6;7)
content: 1. assert(exceptional(Above, Carmine’s))
</tableCaption>
<reference confidence="0.981824142857143">
2. assert(has-att(Above, decor(good)))
3. assert(has-att(Carmine’s, decor(decent)))
4. assert(has-att(Above, service(good)))
5. assert(has-att(Carmine’s, service(good)))
6. assert(has-att(Above, cuisine(New Ameri-
can)))
7. assert(has-att(Carmine’s, cuisine(italian)))
</reference>
<figureCaption confidence="0.955597">
Figure 2: A content plan for a comparison be-
tween restaurants in midtown Manhattan
</figureCaption>
<bodyText confidence="0.990216083333333">
and 4. Section 5 presents the results of two
experiments. The first experiment shows that
given a content plan such as that in Figure 1,
SPaRKy can select sentence plans that commu-
nicate the desired rhetorical relations, are sig-
nificantly better than a randomly selected sen-
tence plan, and are on average less than 10%
worse than a sentence plan ranked highest by
human judges. The second experiment shows
that the quality of SPaRKy’s output is compa-
rable to that of MATCH’s template-based gen-
erator. We sum up in Section 6.
</bodyText>
<sectionHeader confidence="0.90091" genericHeader="method">
2 SPaRKy Architecture
</sectionHeader>
<bodyText confidence="0.991981642857143">
Information presentation in the MATCH sys-
tem focuses on user-tailored recommendations
and comparisons of restaurants (Walker et al.,
2002). Following the bottom-up approach to
text-planning described in (Marcu, 1997; Mel-
lish, 1998), each presentation consists of a set of
assertions about a set of restaurants and a spec-
ification of the rhetorical relations that hold be-
tween them. Example content plans are shown
in Figures 1 and 2. The job of the sentence
planner is to choose linguistic resources to real-
ize a content plan and then rank the resulting
alternative realizations. Figures 3 and 4 show
alternative realizations for the content plans in
Figures 1 and 2.
Alt Realization H SPR
2 Chanpen Thai, which is a Thai restau- 3 .28
rant, has decent decor. It has good
service. It has the best overall quality
among the selected restaurants.
5 Since Chanpen Thai is a Thai restau- 2.5 .14
rant, with good service, and it has de-
cent decor, it has the best overall qual-
ity among the selected restaurants.
6 Chanpen Thai, which is a Thai restau- 4 .70
rant, with decent decor and good ser-
vice, has the best overall quality among
the selected restaurants.
</bodyText>
<figureCaption confidence="0.735665">
Figure 3: Some alternative sentence plan real-
</figureCaption>
<bodyText confidence="0.959470125">
izations for the recommendation in Figure 1. H
= Humans’ score. SPR = SPR’s score.
Alt Realization H SPR
11 Above and Carmine’s offer exceptional 2 .73
value among the selected restaurants.
Above, which is a New American
restaurant, with good decor, has good
service. Carmine’s, which is an Italian
restaurant, with good service, has de-
cent decor.
12 Above and Carmine’s offer exceptional 2.5 .50
value among the selected restaurants.
Above has good decor, and Carmine’s
has decent decor. Above and Carmine’s
have good service. Above is a New
American restaurant. On the other
hand, Carmine’s is an Italian restau-
rant.
13 Above and Carmine’s offer exceptional 3 .67
value among the selected restaurants.
Above is a New American restaurant.
It has good decor. It has good service.
Carmine’s, which is an Italian restau-
rant, has decent decor and good service.
20 Above and Carmine’s offer exceptional 2.5 .49
value among the selected restaurants.
Carmine’s has decent decor but Above
has good decor, and Carmine’s and
Above have good service. Carmine’s is
an Italian restaurant. Above, however,
is a New American restaurant.
25 Above and Carmine’s offer exceptional NR NR
value among the selected restaurants.
Above has good decor. Carmine’s is
an Italian restaurant. Above has good
service. Carmine’s has decent decor.
Above is a New American restaurant.
Carmine’s has good service.
Figure 4: Some of the alternative sentence plan
realizations for the comparison in Figure 2. H
= Humans’ score. SPR = SPR’s score. NR =
Not generated or ranked
The architecture of the spoken language gen-
eration module in MATCH is shown in Figure 5.
The dialog manager sends a high-level commu-
nicative goal to the SPUR text planner, which
selects the content to be communicated using a
user model and brevity constraints (see (Walker
</bodyText>
<figure confidence="0.994864375">
DIALOGUE
MANAGER
Communicative
Goals
Speech
Synthesizer
SYSTEM
UTTERANCE
</figure>
<figureCaption confidence="0.965374">
Figure 5: A dialog system with a spoken lan-
guage generator
</figureCaption>
<bodyText confidence="0.9999010625">
et al., 2002)). The output is a content plan for
a recommendation or comparison such as those
in Figures 1 and 2.
SPaRKy, the sentence planner, gets the con-
tent plan, and then a sentence plan generator
(SPG) generates one or more sentence plans
(Figure 7) and a sentence plan ranker (SPR)
ranks the generated plans. In order for the
SPG to avoid generating sentence plans that are
clearly bad, a content-structuring module first
finds one or more ways to linearly order the in-
put content plan using principles of entity-based
coherence based on rhetorical relations (Knott
et al., 2001). It outputs a set of text plan
trees (tp-trees), consisting of a set of speech
acts to be communicated and the rhetorical re-
lations that hold between them. For example,
the two tp-trees in Figure 6 are generated for
the content plan in Figure 2. Sentence plans
such as alternative 25 in Figure 4 are avoided;
it is clearly worse than alternatives 12, 13 and
20 since it neither combines information based
on a restaurant entity (e.g Babbo) nor on an
attribute (e.g. decor).
The top ranked sentence plan output by the
SPR is input to the RealPro surface realizer
which produces a surface linguistic utterance
(Lavoie and Rambow, 1997). A prosody as-
signment module uses the prior levels of linguis-
tic representation to determine the appropriate
prosody for the utterance, and passes a marked-
up string to the text-to-speech module.
</bodyText>
<sectionHeader confidence="0.968455" genericHeader="method">
3 Sentence Plan Generation
</sectionHeader>
<bodyText confidence="0.995884471698113">
As in SPoT, the basis of the SPG is a set of
clause-combining operations that operate on tp-
trees and incrementally transform the elemen-
tary predicate-argument lexico-structural rep-
resentations (called DSyntS (Melcuk, 1988))
associated with the speech-acts on the leaves
of the tree. The operations are applied in a
bottom-up left-to-right fashion and the result-
ing representation may contain one or more sen-
tences. The application of the operations yields
two parallel structures: (1) a sentence plan
tree (sp-tree), a binary tree with leaves labeled
by the assertions from the input tp-tree, and in-
terior nodes labeled with clause-combining op-
erations; and (2) one or more DSyntS trees
(d-trees) which reflect the parallel operations
on the predicate-argument representations.
We generate a random sample of possible
sentence plans for each tp-tree, up to a pre-
specified number of sentence plans, by ran-
domly selecting among the operations accord-
ing to a probability distribution that favors pre-
ferred operations&apos;. The choice of operation is
further constrained by the rhetorical relation
that relates the assertions to be combined, as
in other work e.g. (Scott and de Souza, 1990).
In the current work, three RST rhetorical rela-
tions (Mann and Thompson, 1987) are used in
the content planning phase to express the rela-
tions between assertions: the JUSTIFY relation
for recommendations, and the CONTRAST and
ELABORATION relations for comparisons. We
added another relation to be used during the
content-structuring phase, called INFER, which
holds for combinations of speech acts for which
there is no rhetorical relation expressed in the
content plan, as in (Marcu, 1997). By explicitly
representing the discourse structure of the infor-
mation presentation, we can generate informa-
tion presentations with considerably more inter-
nal complexity than those generated in (Walker,
Rambow and Rogati, 2002) and eliminate those
that violate certain coherence principles, as de-
scribed in Section 2.
The clause-combining operations are general
operations similar to aggregation operations
used in other research (Rambow and Korelsky,
1992; Danlos, 2000). The operations and the
&apos;Although the probability distribution here is hand-
crafted based on assumed preferences for operations such
as MERGE, RELATIVE-CLAUSE and WITH-REDUCTION, it
might also be possible to learn this probability distribu-
tion from the data by training in two phases.
</bodyText>
<figure confidence="0.998028210526316">
SPUR
Text
Planner
What to Say
Sentence
Planner
Surface
Realizer
Prosody
Assigner
How to Say It
elaboration
infer
nucleus:&lt;1&gt;assert-com-list_exceptional
contrast contrast contrast
nucleus:&lt;2&gt;assert-com-decor nucleus:&lt;4&gt;assert-com-service nucleus:&lt;6&gt;assert-com-cuisine
nucleus:&lt;3&gt;assert-com-decor nucleus:&lt;5&gt;assert-com-service nucleus:&lt;7&gt;assert-com-cuisine
elaboration
nucleus:&lt;1&gt;assert-com-list_exceptional contrast
</figure>
<figureCaption confidence="0.966315">
Figure 6: Two tp-trees for alternative 13 in Figure 4.
</figureCaption>
<figure confidence="0.995014875">
nucleus:&lt;5&gt;assert-com-service
nucleus:&lt;4&gt;assert-com-service
nucleus:&lt;2&gt;assert-com-decor
nucleus:&lt;6&gt;assert-com-cuisine
nucleus:&lt;3&gt;assert-com-decor
nucleus:&lt;7&gt;assert-com-cuisine
infer
infer
</figure>
<bodyText confidence="0.992518725806452">
constraints on their use are described below.
MERGE applies to two clauses with identical
matrix verbs and all but one identical argu-
ments. The clauses are combined and the non-
identical arguments coordinated. For example,
MERGE(Above has good service;Carmine’s has
good service) yields Above and Carmine’s have
good service. MERGE applies only for the rela-
tions INFER and CONTRAST.
WITH-REDUCTION is treated as a kind of
“verbless” participial clause formation in which
the participial clause is interpreted with the
subject of the unreduced clause. For exam-
ple, WITH-REDUCTION(Above is a New Amer-
ican restaurant;Above has good decor) yields
Above is a New American restaurant, with good
decor. WITH-REDUCTION uses two syntactic
constraints: (a) the subjects of the clauses must
be identical, and (b) the clause that under-
goes the participial formation must have a have-
possession predicate. In the example above, for
instance, the Above is a New American restau-
rant clause cannot undergo participial forma-
tion since the predicate is not one of have-
possession. WITH-REDUCTION applies only for
the relations INFER and JUSTIFY.
RELATIVE-CLAUSE combines two clauses with
identical subjects, using the second clause to
relativize the first clause’s subject. For ex-
ample, RELATIVE-CLAUSE(Chanpen Thai is a
Thai restaurant, with decent decor and good ser-
vice;Chanpen Thai has the best overall quality
among the selected restaurants) yields Chanpen
Thai, which is a Thai restaurant, with decent
decor and good service, has the best overall qual-
ity among the selected restaurants. RELATIVE-
CLAUSE also applies only for the relations INFER
and JUSTIFY.
CUE-WORD inserts a discourse connective
(one of since, however, while, and, but, and on
the other hand), between the two clauses to be
combined. CUE-WORD CONJUNCTION combines
two distinct clauses into a single sentence with a
coordinating or subordinating conjunction (e.g.
Above has decent decor BUT Carmine’s has
good decor), while CUE-WORD INSERTION inserts
a cue word at the start of the second clause, pro-
ducing two separate sentences (e.g. Carmine’s
is an Italian restaurant. HOWEVER, Above
is a New American restaurant). The choice of
cue word is dependent on the rhetorical relation
holding between the clauses.
Finally, PERIOD applies to two clauses to be
treated as two independent sentences.
Note that a tp-tree can have very different
realizations, depending on the operations of the
SPG. For example, the second tp-tree in Fig-
ure 6 yields both Alt 11 and Alt 13 in Figure 4.
However, Alt 13 is more highly rated than Alt
11. The sp-tree and d-tree produced by the SPG
for Alt 13 are shown in Figures 7 and 8. The
composite labels on the interior nodes of the sp-
</bodyText>
<figure confidence="0.961688">
PERIOD_elaboration
&lt;1&gt;assert-com-list_exceptional PERIOD_contrast
PERIOD_infer RELATIVE_CLAUSE_infer
PERIOD_infer &lt;4&gt;assert-com-service &lt;7&gt;assert-com-cuisine MERGE_infer
&lt;6&gt;assert-com-cuisine &lt;2&gt;assert-com-decor
&lt;3&gt;assert-come-decor &lt;5&gt;assert-com-service
</figure>
<figureCaption confidence="0.996387">
Figure 7: Sentence plan tree (sp-tree) for alternative 13 in Figure 4
</figureCaption>
<figure confidence="0.81892">
PERIOD
</figure>
<figureCaption confidence="0.99055">
Figure 8: Dependency tree (d-tree) for alternative 13 in Figure 4
</figureCaption>
<figure confidence="0.998895535714286">
offer
PERIOD
value
among
exceptional restaurant
selected
Above_and_Carmine’s
BE3
PERIOD
HAVE1
Above
HAVE1
good
HAVE1
PERIOD
New_American
Above restaurant Above decor
Carmine’s
BE3
decor
decent AND2
service
good
service
Carmine’s
restaurant
good
Italian
</figure>
<bodyText confidence="0.9996339">
tree indicate the clause-combining relation se-
lected to communicate the specified rhetorical
relation. The d-tree for Alt 13 in Figure 8 shows
that the SPG treats the PERIOD operation as
part of the lexico-structural representation for
the d-tree. After sentence planning, the d-tree
is split into multiple d-trees at PERIOD nodes;
these are sent to the RealPro surface realizer.
Separately, the SPG also handles referring ex-
pression generation by converting proper names
to pronouns when they appear in the previous
utterance. The rules are applied locally, across
adjacent sequences of utterances (Brennan et
al., 1987). Referring expressions are manipu-
lated in the d-trees, either intrasententially dur-
ing the creation of the sp-tree, or intersenten-
tially, if the full sp-tree contains any PERIOD op-
erations. The third and fourth sentences for Alt
13 in Figure 4 show the conversion of a named
restaurant (Carmine’s) to a pronoun.
</bodyText>
<sectionHeader confidence="0.9917115" genericHeader="method">
4 Training the Sentence Plan
Ranker
</sectionHeader>
<bodyText confidence="0.98896916">
The SPR takes as input a set of sp-trees gener-
ated by the SPG and ranks them. The SPR’s
rules for ranking sp-trees are learned from a la-
beled set of sentence-plan training examples us-
ing the RankBoost algorithm (Schapire, 1999).
Examples and Feedback: To apply Rank-
Boost, a set of human-rated sp-trees are en-
coded in terms of a set of features. We started
with a set of 30 representative content plans for
each strategy. The SPG produced as many as 20
distinct sp-trees for each content plan. The sen-
tences, realized by RealPro from these sp-trees,
were then rated by two expert judges on a scale
from 1 to 5, and the ratings averaged. Each sp-
tree was an example input for RankBoost, with
each corresponding rating its feedback.
Features used by RankBoost: RankBoost
requires each example to be encoded as a set of
real-valued features (binary features have val-
ues 0 and 1). A strength of RankBoost is that
the set of features can be very large. We used
7024 features for training the SPR. These fea-
tures count the number of occurrences of certain
structural configurations in the sp-trees and the
d-trees, in order to capture declaratively de-
cisions made by the randomized SPG, as in
(Walker, Rambow and Rogati, 2002). The fea-
tures were automatically generated using fea-
ture templates. For this experiment, we use
two classes of feature: (1) Rule-features: These
features are derived from the sp-trees and repre-
sent the ways in which MERGE, INFER and CUE-
WORD operations are applied to the tp-trees.
These feature names start with “rule”. (2) Sent-
features: These features are derived from the
DSyntSs, and describe the deep-syntactic struc-
ture of the utterance, including the chosen lex-
emes. As a result, some may be domain specific.
These feature names are prefixed with “sent”.
We now describe the feature templates used
in the discovery process. Three templates were
used for both sp-tree and d-tree features; two
were used only for sp-tree features. Local feature
templates record structural configurations local
to a particular node (its ancestors, daughters
etc.). Global feature templates, which are used
only for sp-tree features, record properties of the
entire sp-tree. We discard features that occur
fewer than 10 times to avoid those specific to
particular text plans.
</bodyText>
<table confidence="0.9952869">
Strategy System Min Max Mean S.D.
Recommend SPaRKy 2.0 5.0 3.6 .71
HUMAN 2.5 5.0 3.9 .55
RANDOM 1.5 5.0 2.9 .88
Compare2 SPaRKy 2.5 5.0 3.9 .71
HUMAN 2.5 5.0 4.4 .54
RANDOM 1.0 5.0 2.9 1.3
Compare3 SPaRKy 1.5 4.5 3.4 .63
HUMAN 3.0 5.0 4.0 .49
RANDOM 1.0 4.5 2.7 1.0
</table>
<tableCaption confidence="0.8317805">
Table 1: Summary of Recommend, Compare2
and Compare3 results (N = 180)
</tableCaption>
<bodyText confidence="0.998907571428572">
There are four types of local feature
template: traversal features, sister features,
ancestor features and leaf features. Local
feature templates are applied to all nodes in a
sp-tree or d-tree (except that the leaf feature is
not used for d-trees); the value of the resulting
feature is the number of occurrences of the
described configuration in the tree. For each
node in the tree, traversal features record the
preorder traversal of the subtree rooted at
that node, for all subtrees of all depths. An
example is the feature “rule traversal assert-
com-list exceptional” (with value 1) of the
tree in Figure 7. Sister features record all
consecutive sister nodes. An example is the fea-
ture “rule sisters PERIOD infer RELATIVE
CLAUSE infer” (with value 1) of the
tree in Figure 7. For each node in the
tree, ancestor features record all the ini-
tial subpaths of the path from that node
to the root. An example is the feature
“rule ancestor PERIOD contrast*PERIOD
infer” (with value 1) of the tree in Figure 7.
Finally, leaf features record all initial substrings
of the frontier of the sp-tree. For example, the
sp-tree of Figure 7 has value 1 for the feature
“leaf #assert-com-list exceptional#assert-com-
cuisine”.
Global features apply only to the sp-
tree. They record, for each sp-tree and for
each clause-combining operation labeling a non-
frontier node, (1) the minimal number of leaves
dominated by a node labeled with that op-
eration in that tree (MIN); (2) the maximal
number of leaves dominated by a node la-
beled with that operation (MAX); and (3)
the average number of leaves dominated by
a node labeled with that operation (AVG).
For example, the sp-tree in Figure 7 has
value 3 for “PERIOD infer max”, value 2 for
“PERIOD infer min” and value 2.5 for “PE-
RIOD infer avg”.
</bodyText>
<sectionHeader confidence="0.986372" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999614">
We report two sets of experiments. The first ex-
periment tests the ability of the SPR to select a
high quality sentence plan from a population of
sentence plans randomly generated by the SPG.
Because the discriminatory power of the SPR is
best tested by the largest possible population of
sentence plans, we use 2-fold cross validation for
this experiment. The second experiment com-
pares SPaRKy to template-based generation.
</bodyText>
<subsectionHeader confidence="0.966113">
Cross Validation Experiment: We re-
</subsectionHeader>
<bodyText confidence="0.99306885106383">
peatedly tested SPaRKy on the half of the cor-
pus of 1756 sp-trees held out as test data for
each fold. The evaluation metric is the human-
assigned score for the variant that was rated
highest by SPaRKy for each text plan for each
task/user combination. We evaluated SPaRKy
on the test sets by comparing three data points
for each text plan: HUMAN (the score of the
top-ranked sentence plan); SPARKY (the score
of the SPR’s selected sentence); and RANDOM
(the score of a sentence plan randomly selected
from the alternate sentence plans).
We report results separately for comparisons
between two entities and among three or more
entities. These two types of comparison are gen-
erated using different strategies in the SPG, and
can produce text that is very different both in
terms of length and structure.
Table 1 summarizes the difference between
SPaRKy, HUMAN and RANDOM for recom-
mendations, comparisons between two entities
and comparisons between three or more enti-
ties. For all three presentation types, a paired
t-test comparing SPaRKy to HUMAN to RAN-
DOM showed that SPaRKy was significantly
better than RANDOM (df = 59, p &lt; .001) and
significantly worse than HUMAN (df = 59, p
&lt; .001). This demonstrates that the use of a
trainable sentence planner can lead to sentence
plans that are significantly better than baseline
(RANDOM), with less human effort than pro-
gramming templates.
Comparison with template generation:
For each content plan input to SPaRKy, the
judges also rated the output of a template-
based generator for MATCH. This template-
based generator performs text planning and sen-
tence planning (the focus of the current pa-
per), including some discourse cue insertion,
clause combining and referring expression gen-
eration; the templates themselves are described
in (Walker et al., 2002). Because the templates
are highly tailored to this domain, this genera-
tor can be expected to perform well. Example
template-based and SPaRKy outputs for a com-
parison between three or more items are shown
in Figure 9.
</bodyText>
<table confidence="0.994567">
Strategy System Min Max Mean S.D.
Recommend Template 2.5 5.0 4.22 0.74
SPaRKy 2.5 4.5 3.57 0.59
HUMAN 4.0 5.0 4.37 0.37
Compare2 Template 2.0 5.0 3.62 0.75
SPaRKy 2.5 4.75 3.87 0.52
HUMAN 4.0 5.0 4.62 0.39
Compare3 Template 1.0 5.0 4.08 1.23
SPaRKy 2.5 4.25 3.375 0.38
HUMAN 4.0 5.0 4.63 0.35
</table>
<tableCaption confidence="0.853792">
Table 2: Summary of template-based genera-
tion results. N = 180
</tableCaption>
<bodyText confidence="0.975046961538461">
Table 2 shows the mean HUMAN scores for
the template-based sentence planning. A paired
t-test comparing HUMAN and template-based
scores showed that HUMAN was significantly
better than template-based sentence planning
only for compare2 (df = 29, t = 6.2, p &lt; .001).
The judges evidently did not like the template
for comparisons between two items. A paired
t-test comparing SPaRKy and template-based
sentence planning showed that template-based
sentence planning was significantly better than
SPaRKy only for recommendations (df = 29, t
= 3.55, p &lt; .01). These results demonstrate
that trainable sentence planning shows promise
for producing output comparable to that of a
template-based generator, with less program-
ming effort and more flexibility.
The standard deviation for all three template-
based strategies was wider than for HUMAN
or SPaRKy, indicating that there may be
content-specific aspects to the sentence plan-
ning done by SPaRKy that contribute to out-
put variation. The data show this to be cor-
rect; SPaRKy learned content-specific prefer-
ences about clause combining and discourse cue
insertion that a template-based generator can-
</bodyText>
<table confidence="0.8650655625">
System Realization H
Template Among the selected restaurants, the fol- 4.5
lowing offer exceptional overall value.
Uguale’s price is 33 dollars. It has good
decor and very good service. It’s a
French, Italian restaurant. Da Andrea’s
price is 28 dollars. It has good decor and
very good service. It’s an Italian restau-
rant. John’s Pizzeria’s price is 20 dollars.
It has mediocre decor and decent service.
It’s an Italian, Pizza restaurant.
SPaRKy Da Andrea, Uguale, and John’s Pizze- 4
ria offer exceptional value among the se-
lected restaurants. Da Andrea is an Ital-
ian restaurant, with very good service, it
has good decor, and its price is 28 dol-
</table>
<figureCaption confidence="0.952335125">
lars. John’s Pizzeria is an Italian , Pizza
restaurant. It has decent service. It has
mediocre decor. Its price is 20 dollars.
Uguale is a French, Italian restaurant,
with very good service. It has good decor,
and its price is 33 dollars.
Figure 9: Comparisons between 3 or more
items, H = Humans’ score
</figureCaption>
<bodyText confidence="0.999774137931034">
not easily model, but that a trainable sentence
planner can. For example, Table 3 shows the
nine rules generated on the first test fold which
have the largest negative impact on the final
RankBoost score (above the double line) and
the largest positive impact on the final Rank-
Boost score (below the double line), for com-
parisons between three or more entities. The
rule with the largest positive impact shows that
SPaRKy learned to prefer that justifications in-
volving price be merged with other information
using a conjunction.
These rules are also specific to presentation
type. Averaging over both folds of the exper-
iment, the number of unique features appear-
ing in rules is 708, of which 66 appear in the
rule sets for two presentation types and 9 ap-
pear in the rule sets for all three presentation
types. There are on average 214 rule features,
428 sentence features and 26 leaf features. The
majority of the features are ancestor features
(319) followed by traversal features (264) and
sister features (60). The remainder of the fea-
tures (67) are for specific lexemes.
To sum up, this experiment shows that the
ability to model the interactions between do-
main content, task and presentation type is a
strength of the trainable approach to sentence
planning.
</bodyText>
<sectionHeader confidence="0.999294" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.907122">
This paper shows that the training technique
used in SPoT can be easily extended to a new
</bodyText>
<table confidence="0.99951065">
N Condition a3
1 sent anc PROPERNOUN RESTAURANT -0.859
*HAVE1 &gt; 16.5
2 sent anc II Upper East Side*ATTR IN1* -0.852
locate &gt; 4.5
3 sent anc PERIOD infer*PERIOD infer -0.542
*PERIOD elaboration &gt; -oo
4 rule anc assert-com-service*MERGE infer -0.356
&gt; 1.5
5 sent tvl depth 0 BE3 &gt; 4.5 -0.346
6 rule anc PERIOD infer*PERIOD infer -0.345
*PERIOD elaboration &gt; -oo
7 rule anc assert-com-decor*PERIOD infer -0.342
*PERIOD infer*PERIOD contrast *PE-
RIOD elaboration &gt; -oo
8 rule anc assert-com-food quality*MERGE 0.398
infer &gt; 1.5
9 rule anc assert-com-price*CW 0.527
CONJUNCTION infer*PERIOD justify
&gt; -oo
</table>
<tableCaption confidence="0.998876">
Table 3: The nine rules generated on the first
</tableCaption>
<bodyText confidence="0.985884652173913">
test fold which have the largest negative impact
on the final RankBoost score (above the dou-
ble line) and the largest positive impact on the
final RankBoost score (below the double line),
for Compare3. as represents the increment or
decrement associated with satisfying the condi-
tion.
domain and used for information presentation
as well as information gathering. Previous work
on SPoT also compared trainable sentence plan-
ning to a template-based generator that had
previously been developed for the same appli-
cation (Rambow et al., 2001). The evalua-
tion results for SPaRKy (1) support the results
for SPoT, by showing that trainable sentence
generation can produce output comparable to
template-based generation, even for complex in-
formation presentations such as extended com-
parisons; (2) show that trainable sentence gen-
eration is sensitive to variations in domain ap-
plication, presentation type, and even human
preferences about the arrangement of particu-
lar types of information.
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999580666666667">
We thank AT&amp;T for supporting this research,
and the anonymous reviewers for their helpful
comments on this paper.
</bodyText>
<sectionHeader confidence="0.999254" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889692307692">
I. Langkilde. Forest-based statistical sentence gen-
eration. In Proc. NAACL 2000, 2000.
S. E. Brennan, M. Walker Friedman, and C. J. Pol-
lard. A centering approach to pronouns. In Proc.
25th Annual Meeting of the ACL, Stanford, pages
155–162, 1987.
L. Danlos. 2000. G-TAG: A lexicalized formal-
ism for text generation inspired by tree ad-
joining grammar. In Tree Adjoining Grammars:
Formalisms, Linguistic Analysis, and Processing.
CSLI Publications.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker, and P. Mal-
oor. MATCH: An architecture for multimodal di-
alogue systems. In Annual Meeting of the ACL,
2002.
A. Knott, J. Oberlander, M. O’Donnell and C. Mel-
lish. Beyond Elaboration: the interaction of rela-
tions and focus in coherent text. In Text Repre-
sentation: linguistic and psycholinguistic aspects,
pages 181-196, 2001.
B. Lavoie and O. Rambow. A fast and portable re-
alizer for text generation systems. In Proc. of the
3rd Conference on Applied Natural Language Pro-
cessing, ANLP97, pages 265–268, 1997.
W.C. Mann and S.A. Thompson. Rhetorical struc-
ture theory: A framework for the analysis of texts.
Technical Report RS-87-190, USC/Information
Sciences Institute, 1987.
D. Marcu. From local to global coherence: a
bottom-up approach to text planning. In Proceed-
ings of the National Conference on Artificial In-
telligence (AAAI’97), 1997.
C. Mellish, A. Knott, J. Oberlander, and M.
O’Donnell. Experiments using stochastic search
for text planning. In Proceedings of INLG-98.
1998.
I. A. Melˇcuk. Dependency Syntax: Theory and Prac-
tice. SUNY, Albany, New York, 1988.
O. Rambow and T. Korelsky. Applied text genera-
tion. In Proceedings of the Third Conference on
Applied Natural Language Processing, ANLP92,
pages 40–47, 1992.
O. Rambow, M. Rogati and M. A. Walker. Evalu-
ating a Trainable Sentence Planner for a Spoken
Dialogue Travel System In Meeting of the ACL,
2001.
R. E. Schapire. A brief introduction to boosting. In
Proc. of the 16th IJCAI, 1999.
D. R. Scott and C. Sieckenius de Souza. Getting
the message across in RST-based text generation.
In Current Research in Natural Language Gener-
ation, pages 47–73, 1990.
A. Stent, M. Walker, S. Whittaker, and P. Maloor.
User-tailored generation for spoken dialogue: An
experiment. In Proceedings of ICSLP 2002., 2002.
M. A. Walker, S. J. Whittaker, A. Stent, P. Mal-
oor, J. D. Moore, M. Johnston, and G. Vasireddy.
Speech-Plans: Generating evaluative responses
in spoken dialogue. In Proceedings of INLG-02.,
2002.
M. Walker, O. Rambow, and M. Rogati. Training a
sentence planner for spoken dialogue using boost-
ing. Computer Speech and Language: Special Is-
sue on Spoken Language Generation, 2002.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.341377">
<title confidence="0.999337">Trainable Sentence Planning for Complex Information Presentation in Spoken Dialog Systems</title>
<author confidence="0.999536">Amanda Stent</author>
<affiliation confidence="0.997054">Stony Brook University</affiliation>
<address confidence="0.991866">Stony Brook, NY 11794 U.S.A.</address>
<email confidence="0.99981">stent@cs.sunysb.edu</email>
<author confidence="0.991201">Rashmi Prasad</author>
<affiliation confidence="0.999897">University of Pennsylvania</affiliation>
<address confidence="0.9920185">Philadelphia, PA 19104 U.S.A.</address>
<email confidence="0.999464">rjprasad@linc.cis.upenn.edu</email>
<author confidence="0.999532">Marilyn Walker</author>
<affiliation confidence="0.997669">University of Sheffield</affiliation>
<address confidence="0.631192">Sheffield S1 4DP</address>
<affiliation confidence="0.651027">U.K.</affiliation>
<email confidence="0.996093">M.A.Walker@sheffield.ac.uk</email>
<abstract confidence="0.998945428571429">A challenging problem for spoken dialog systems is the design of utterance generation modules that are fast, flexible and general, yet produce high quality output in particular domains. A promising approach is trainable generation, which uses general-purpose linguistic knowledge automatically adapted to the application domain. This paper presents a trainable sentence planner for the MATCH dialog system. We show that trainable sentence planning can produce output comparable to that of MATCH’s template-based generator even for quite complex information presentations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>assert</author>
</authors>
<title>service(good))) 6. assert(has-att(Above, cuisine(New Ameri- can))) 7. assert(has-att(Carmine’s, cuisine(italian</title>
<marker>assert, </marker>
<rawString>5. assert(has-att(Carmine’s, service(good))) 6. assert(has-att(Above, cuisine(New Ameri- can))) 7. assert(has-att(Carmine’s, cuisine(italian)))</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>