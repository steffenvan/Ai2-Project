<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.980874">
Word representations:
A simple and general method for semi-supervised learning
</title>
<author confidence="0.72354">
Joseph Turian
</author>
<affiliation confidence="0.418928333333333">
D´epartement d’Informatique et
Recherche Op´erationnelle (DIRO)
Universit´e de Montr´eal
</affiliation>
<address confidence="0.598837">
Montr´eal, Qu´ebec, Canada, H3T 1J4
</address>
<email confidence="0.993155">
lastname@iro.umontreal.ca
</email>
<author confidence="0.995904">
Lev Ratinov
</author>
<affiliation confidence="0.91841725">
Department of
Computer Science
University of Illinois at
Urbana-Champaign
</affiliation>
<address confidence="0.733157">
Urbana, IL 61801
</address>
<email confidence="0.995232">
ratinov2@uiuc.edu
</email>
<author confidence="0.522611">
Yoshua Bengio
</author>
<affiliation confidence="0.321589666666667">
D´epartement d’Informatique et
Recherche Op´erationnelle (DIRO)
Universit´e de Montr´eal
</affiliation>
<address confidence="0.573891">
Montr´eal, Qu´ebec, Canada, H3T 1J4
</address>
<email confidence="0.996504">
bengioy@iro.umontreal.ca
</email>
<sectionHeader confidence="0.993853" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998044333333333">
If we take an existing supervised NLP sys-
tem, a simple and general way to improve
accuracy is to use unsupervised word
representations as extra word features. We
evaluate Brown clusters, Collobert and
Weston (2008) embeddings, and HLBL
(Mnih &amp; Hinton, 2009) embeddings
of words on both NER and chunking.
We use near state-of-the-art supervised
baselines, and find that each of the three
word representations improves the accu-
racy of these baselines. We find further
improvements by combining different
word representations. You can download
our word features, for off-the-shelf use
in existing NLP systems, as well as our
code, here: http://metaoptimize.
com/projects/wordreprs/
</bodyText>
<sectionHeader confidence="0.998703" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999872944444445">
By using unlabelled data to reduce data sparsity
in the labeled training data, semi-supervised
approaches improve generalization accuracy.
Semi-supervised models such as Ando and Zhang
(2005), Suzuki and Isozaki (2008), and Suzuki
et al. (2009) achieve state-of-the-art accuracy.
However, these approaches dictate a particular
choice of model and training regime. It can be
tricky and time-consuming to adapt an existing su-
pervised NLP system to use these semi-supervised
techniques. It is preferable to use a simple and
general method to adapt existing supervised NLP
systems to be semi-supervised.
One approach that is becoming popular is
to use unsupervised methods to induce word
features—or to download word features that have
already been induced—plug these word features
into an existing system, and observe a significant
increase in accuracy. But which word features are
good for what tasks? Should we prefer certain
word features? Can we combine them?
A word representation is a mathematical object
associated with each word, often a vector. Each
dimension’s value corresponds to a feature and
might even have a semantic or grammatical
interpretation, so we call it a word feature.
Conventionally, supervised lexicalized NLP ap-
proaches take a word and convert it to a symbolic
ID, which is then transformed into a feature vector
using a one-hot representation: The feature vector
has the same length as the size of the vocabulary,
and only one dimension is on. However, the
one-hot representation of a word suffers from data
sparsity: Namely, for words that are rare in the
labeled training data, their corresponding model
parameters will be poorly estimated. Moreover,
at test time, the model cannot handle words that
do not appear in the labeled training data. These
limitations of one-hot word representations have
prompted researchers to investigate unsupervised
methods for inducing word representations over
large unlabeled corpora. Word features can be
hand-designed, but our goal is to learn them.
One common approach to inducing unsuper-
vised word representation is to use clustering,
perhaps hierarchical. This technique was used by
a variety of researchers (Miller et al., 2004; Liang,
2005; Koo et al., 2008; Ratinov &amp; Roth, 2009;
Huang &amp; Yates, 2009). This leads to a one-hot
representation over a smaller vocabulary size.
Neural language models (Bengio et al., 2001;
Schwenk &amp; Gauvain, 2002; Mnih &amp; Hinton,
2007; Collobert &amp; Weston, 2008), on the other
hand, induce dense real-valued low-dimensional
</bodyText>
<page confidence="0.981602">
384
</page>
<note confidence="0.9437055">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9995721875">
word embeddings using unsupervised approaches.
(See Bengio (2008) for a more complete list of
references on neural language models.)
Unsupervised word representations have
been used in previous NLP work, and have
demonstrated improvements in generalization
accuracy on a variety of tasks. But different word
representations have never been systematically
compared in a controlled way. In this work, we
compare different techniques for inducing word
representations, evaluating them on the tasks of
named entity recognition (NER) and chunking.
We retract former negative results published in
Turian et al. (2009) about Collobert and Weston
(2008) embeddings, given training improvements
that we describe in Section 7.1.
</bodyText>
<sectionHeader confidence="0.993742" genericHeader="introduction">
2 Distributional representations
</sectionHeader>
<bodyText confidence="0.999968207317073">
Distributional word representations are based
upon a cooccurrence matrix F of size WxC, where
W is the vocabulary size, each row Fw is the ini-
tial representation of word w, and each column Fc
is some context. Sahlgren (2006) and Turney and
Pantel (2010) describe a handful of possible de-
sign decisions in contructing F, including choice
of context types (left window? right window? size
of window?) and type of frequency count (raw?
binary? tf-idf?). Fw has dimensionality W, which
can be too large to use Fw as features for word w in
a supervised model. One can map F to matrix f of
size W x d, where d &lt;&lt; C, using some function g,
where f = g(F). fw represents word w as a vector
with d dimensions. The choice of g is another de-
sign decision, although perhaps not as important
as the statistics used to initially construct F.
The self-organizing semantic map (Ritter &amp;
Kohonen, 1989) is a distributional technique
that maps words to two dimensions, such that
syntactically and semantically related words are
nearby (Honkela et al., 1995; Honkela, 1997).
LSA (Dumais et al., 1988; Landauer et al.,
1998), LSI, and LDA (Blei et al., 2003) induce
distributional representations over F in which
each column is a document context. In most of the
other approaches discussed, the columns represent
word contexts. In LSA, g computes the SVD of F.
Hyperspace Analogue to Language (HAL) is
another early distributional approach (Lund et al.,
1995; Lund &amp; Burgess, 1996) to inducing word
representations. They compute F over a corpus of
160 million word tokens with a vocabulary size W
of 70K word types. There are 2·W types of context
(columns): The first or second W are counted if the
word c occurs within a window of 10 to the left or
right of the word w, respectively. f is chosen by
taking the 200 columns (out of 140K in F) with
the highest variances. ICA is another technique to
transform F into f. (V¨ayrynen &amp; Honkela, 2004;
V¨ayrynen &amp; Honkela, 2005; V¨ayrynen et al.,
2007). ICA is expensive, and the largest vocab-
ulary size used in these works was only 10K. As
far as we know, ICA methods have not been used
when the size of the vocab W is 100K or more.
Explicitly storing cooccurrence matrix F can be
memory-intensive, and transforming F to f can
be time-consuming. It is preferable that F never
be computed explicitly, and that f be constructed
incrementally. ˇReh˚uˇrek and Sojka (2010) describe
an incremental approach to inducing LSA and
LDA topic models over 270 millions word tokens
with a vocabulary of 315K word types. This is
similar in magnitude to our experiments.
Another incremental approach to constructing f
is using a random projection: Linear mapping g is
multiplying F by a random matrix chosen a pri-
ori. This random indexing method is motivated
by the Johnson-Lindenstrauss lemma, which states
that for certain choices of random matrix, if d is
sufficiently large, then the original distances be-
tween words in F will be preserved in f (Sahlgren,
2005). Kaski (1998) uses this technique to pro-
duce 100-dimensional representations of docu-
ments. Sahlgren (2001) was the first author to use
random indexing using narrow context. Sahlgren
(2006) does a battery of experiments exploring
different design decisions involved in construct-
ing F, prior to using random indexing. However,
like all the works cited above, Sahlgren (2006)
only uses distributional representation to improve
existing systems for one-shot classification tasks,
such as IR, WSD, semantic knowledge tests, and
text categorization. It is not well-understood
what settings are appropriate to induce distribu-
tional word representations for structured predic-
tion tasks (like parsing and MT) and sequence la-
beling tasks (like chunking and NER). Previous
research has achieved repeated successes on these
tasks using clustering representations (Section 3)
and distributed representations (Section 4), so we
focus on these representations in our work.
</bodyText>
<sectionHeader confidence="0.938053" genericHeader="method">
3 Clustering-based word representations
</sectionHeader>
<bodyText confidence="0.8923095">
Another type of word representation is to induce
a clustering over words. Clustering methods and
</bodyText>
<page confidence="0.99849">
385
</page>
<bodyText confidence="0.997590666666667">
distributional methods can overlap. For example,
Pereira et al. (1993) begin with a cooccurrence
matrix and transform this matrix into a clustering.
</bodyText>
<subsectionHeader confidence="0.999827">
3.1 Brown clustering
</subsectionHeader>
<bodyText confidence="0.998776384615385">
The Brown algorithm is a hierarchical clustering
algorithm which clusters words to maximize the
mutual information of bigrams (Brown et al.,
1992). So it is a class-based bigram language
model. It runs in time O(V·K2), where V is the size
of the vocabulary and K is the number of clusters.
The hierarchical nature of the clustering means
that we can choose the word class at several
levels in the hierarchy, which can compensate for
poor clusters of a small number of words. One
downside of Brown clustering is that it is based
solely on bigram statistics, and does not consider
word usage in a wider context.
Brown clusters have been used successfully in
a variety of NLP applications: NER (Miller et al.,
2004; Liang, 2005; Ratinov &amp; Roth, 2009), PCFG
parsing (Candito &amp; Crabb´e, 2009), dependency
parsing (Koo et al., 2008; Suzuki et al., 2009), and
semantic dependency parsing (Zhao et al., 2009).
Martin et al. (1998) presents algorithms for
inducing hierarchical clusterings based upon word
bigram and trigram statistics. Ushioda (1996)
presents an extension to the Brown clustering
algorithm, and learn hierarchical clusterings of
words as well as phrases, which they apply to
POS tagging.
</bodyText>
<subsectionHeader confidence="0.7079945">
3.2 Other work on cluster-based word
representations
</subsectionHeader>
<bodyText confidence="0.999812916666667">
Lin and Wu (2009) present a K-means-like
non-hierarchical clustering algorithm for phrases,
which uses MapReduce.
HMMs can be used to induce a soft clustering,
specifically a multinomial distribution over pos-
sible clusters (hidden states). Li and McCallum
(2005) use an HMM-LDA model to improve
POS tagging and Chinese Word Segmentation.
Huang and Yates (2009) induce a fully-connected
HMM, which emits a multinomial distribution
over possible vocabulary words. They perform
hard clustering using the Viterbi algorithm.
(Alternately, they could keep the soft clustering,
with the representation for a particular word token
being the posterior probability distribution over
the states.) However, the CRF chunker in Huang
and Yates (2009), which uses their HMM word
clusters as extra features, achieves F1 lower than
a baseline CRF chunker (Sha &amp; Pereira, 2003).
Goldberg et al. (2009) use an HMM to assign
POS tags to words, which in turns improves
the accuracy of the PCFG-based Hebrew parser.
Deschacht and Moens (2009) use a latent-variable
language model to improve semantic role labeling.
</bodyText>
<sectionHeader confidence="0.993308" genericHeader="method">
4 Distributed representations
</sectionHeader>
<bodyText confidence="0.999972916666667">
Another approach to word representation is to
learn a distributed representation. (Not to be
confused with distributional representations.)
A distributed representation is dense, low-
dimensional, and real-valued. Distributed word
representations are called word embeddings. Each
dimension of the embedding represents a latent
feature of the word, hopefully capturing useful
syntactic and semantic properties. A distributed
representation is compact, in the sense that it can
represent an exponential number of clusters in the
number of dimensions.
Word embeddings are typically induced us-
ing neural language models, which use neural
networks as the underlying predictive model
(Bengio, 2008). Historically, training and testing
of neural language models has been slow, scaling
as the size of the vocabulary for each model com-
putation (Bengio et al., 2001; Bengio et al., 2003).
However, many approaches have been proposed
in recent years to eliminate that linear dependency
on vocabulary size (Morin &amp; Bengio, 2005;
Collobert &amp; Weston, 2008; Mnih &amp; Hinton, 2009)
and allow scaling to very large training corpora.
</bodyText>
<subsectionHeader confidence="0.99772">
4.1 Collobert and Weston (2008) embeddings
</subsectionHeader>
<bodyText confidence="0.99991394117647">
Collobert and Weston (2008) presented a neural
language model that could be trained over billions
of words, because the gradient of the loss was
computed stochastically over a small sample of
possible outputs, in a spirit similar to Bengio and
S´en´ecal (2003). This neural model of Collobert
and Weston (2008) was refined and presented in
greater depth in Bengio et al. (2009).
The model is discriminative and non-
probabilistic. For each training update, we
read an n-gram x = (w1, ... , wn) from the corpus.
The model concatenates the learned embeddings
of the n words, giving e(w1) ® ... ® e(wn), where
e is the lookup table and ® is concatenation.
We also create a corrupted or noise n-gram
x˜ = (w1, ... , wn_q, ˜wn), where ˜wn # wn is chosen
uniformly from the vocabulary.1 For convenience,
</bodyText>
<footnote confidence="0.969529">
1In Collobert and Weston (2008), the middle word in the
</footnote>
<page confidence="0.998012">
386
</page>
<bodyText confidence="0.999942230769231">
we write e(x) to mean e(w1) ® ... ® e(wn). We
predict a score s(x) for x by passing e(x) through
a single hidden layer neural network. The training
criterion is that n-grams that are present in the
training corpus like x must have a score at least
some margin higher than corrupted n-grams like
˜x. Specifically: L(x) = max(0, 1− s(x) + s(˜x)). We
minimize this loss stochastically over the n-grams
in the corpus, doing gradient descent simultane-
ously over the neural network parameters and the
embedding lookup table.
We implemented the approach of Collobert and
Weston (2008), with the following differences:
</bodyText>
<listItem confidence="0.984174333333333">
• We did not achieve as low log-ranks on the
English Wikipedia as the authors reported in
Bengio et al. (2009), despite initially attempting
to have identical experimental conditions.
• We corrupt the last word of each n-gram.
• We had a separate learning rate for the em-
beddings and for the neural network weights.
We found that the embeddings should have a
learning rate generally 1000–32000 times higher
than the neural network weights. Otherwise, the
unsupervised training criterion drops slowly.
• Although their sampling technique makes train-
ing fast, testing is still expensive when the size of
the vocabulary is large. Instead of cross-validating
using the log-rank over the validation data as
they do, we instead used the moving average of
the training loss on training examples before the
weight update.
</listItem>
<subsectionHeader confidence="0.981983">
4.2 HLBL embeddings
</subsectionHeader>
<bodyText confidence="0.950461277777778">
The log-bilinear model (Mnih &amp; Hinton, 2007) is
a probabilistic and linear neural model. Given an
n-gram, the model concatenates the embeddings
of the n − 1 first words, and learns a linear model
to predict the embedding of the last word. The
similarity between the predicted embedding and
the current actual embedding is transformed
into a probability by exponentiating and then
normalizing. Mnih and Hinton (2009) speed up
model evaluation during training and testing by
using a hierarchy to exponentially filter down
the number of computations that are performed.
This hierarchical evaluation technique was first
proposed by Morin and Bengio (2005). The
model, combined with this optimization, is called
the hierarchical log-bilinear (HLBL) model.
n-gram is corrupted. In Bengio et al. (2009), the last word in
the n-gram is corrupted.
</bodyText>
<sectionHeader confidence="0.832548" genericHeader="method">
5 Supervised evaluation tasks
</sectionHeader>
<bodyText confidence="0.9999642">
We evaluate the hypothesis that one can take an
existing, near state-of-the-art, supervised NLP
system, and improve its accuracy by including
word representations as word features. This
technique for turning a supervised approach into a
semi-supervised one is general and task-agnostic.
However, we wish to find out if certain word
representations are preferable for certain tasks.
Lin and Wu (2009) finds that the representations
that are good for NER are poor for search query
classification, and vice-versa. We apply clus-
tering and distributed representations to NER
and chunking, which allows us to compare our
semi-supervised models to those of Ando and
Zhang (2005) and Suzuki and Isozaki (2008).
</bodyText>
<subsectionHeader confidence="0.991043">
5.1 Chunking
</subsectionHeader>
<bodyText confidence="0.99891075">
Chunking is a syntactic sequence labeling task.
We follow the conditions in the CoNLL-2000
shared task (Sang &amp; Buchholz, 2000).
The linear CRF chunker of Sha and Pereira
(2003) is a standard near-state-of-the-art baseline
chunker. In fact, many off-the-shelf CRF imple-
mentations now replicate Sha and Pereira (2003),
including their choice of feature set:
</bodyText>
<listItem confidence="0.992880333333333">
• CRF++ by Taku Kudo (http://crfpp.
sourceforge.net/)
• crfsgd by L´eon Bottou (http://leon.
bottou.org/projects/sgd)
• CRFsuite by by Naoaki Okazaki (http://
www.chokkan.org/software/crfsuite/)
</listItem>
<bodyText confidence="0.999807052631579">
We use CRFsuite because it makes it sim-
ple to modify the feature generation code,
so one can easily add new features. We
use SGD optimization, and enable negative
state features and negative transition fea-
tures. (“feature.possible transitions=1,
feature.possible states=1”)
Table 1 shows the features in the baseline chun-
ker. As you can see, the Brown and embedding
features are unigram features, and do not partici-
pate in conjunctions like the word features and tag
features do. Koo et al. (2008) sees further accu-
racy improvements on dependency parsing when
using word representations in compound features.
The data comes from the Penn Treebank, and
is newswire from the Wall Street Journal in 1989.
Of the 8936 training sentences, we used 1000
randomly sampled sentences (23615 words) for
development. We trained models on the 7936
</bodyText>
<page confidence="0.980671">
387
</page>
<listItem confidence="0.964930363636364">
• Word features: wi for i in 1−2, −1, 0, +1, +21,
wi n wi+1 for i in 1−1, 01.
• Tag features: wi for i in 1−2,−1,0,+1,+21,
ti n ti+1 for i in 1−2, −1, 0, +11. ti n ti+1 n ti+2
for i in 1−2, −1, 01.
• Embedding features [if applicable]: ei[d] for i
in 1−2, −1, 0, +1, +21, where d ranges over the
dimensions of the embedding ei.
• Brown features [if applicable]: substr(bi, 0, p)
for i in 1−2, −1, 0, +1, +21, where substr takes
the p-length prefix of the Brown cluster bi.
</listItem>
<tableCaption confidence="0.995116">
Table 1: Features templates used in the CRF chunker.
</tableCaption>
<bodyText confidence="0.999942111111111">
training partition sentences, and evaluated their
F1 on the development set. After choosing hy-
perparameters to maximize the dev F1, we would
retrain the model using these hyperparameters on
the full 8936 sentence training set, and evaluate
on test. One hyperparameter was l2-regularization
sigma, which for most models was optimal at 2 or
3.2. The word embeddings also required a scaling
hyperparameter, as described in Section 7.2.
</bodyText>
<subsectionHeader confidence="0.997495">
5.2 Named entity recognition
</subsectionHeader>
<bodyText confidence="0.999707346153846">
NER is typically treated as a sequence prediction
problem. Following Ratinov and Roth (2009), we
use the regularized averaged perceptron model.
Ratinov and Roth (2009) describe different
sequence encoding like BILOU and BIO, and
show that the BILOU encoding outperforms BIO,
and the greedy inference performs competitively
to Viterbi while being significantly faster. Ac-
cordingly, we use greedy inference and BILOU
text chunk representation. We use the publicly
available implementation from Ratinov and Roth
(2009) (see the end of this paper for the URL). In
our baseline experiments, we remove gazetteers
and non-local features (Krishnan &amp; Manning,
2006). However, we also run experiments that
include these features, to understand if the infor-
mation they provide mostly overlaps with that of
the word representations.
After each epoch over the training set, we
measured the accuracy of the model on the
development set. Training was stopped after the
accuracy on the development set did not improve
for 10 epochs, generally about 50–80 epochs
total. The epoch that performed best on the
development set was chosen as the final model.
We use the following baseline set of features
</bodyText>
<listItem confidence="0.93353">
from Zhang and Johnson (2003):
• Previous two predictions yi−1 and yi−2
• Current word xi
• xi word type information: all-capitalized,
is-capitalized, all-digits, alphanumeric, etc.
• Prefixes and suffixes of xi, if the word contains
hyphens, then the tokens between the hyphens
• Tokens in the window c =
(xi−2, xi−1, xi, xi+1, xi+2)
• Capitalization pattern in the window c
• Conjunction of c and yi−1.
Word representation features, if present, are used
the same way as in Table 1.
</listItem>
<bodyText confidence="0.999236060606061">
When using the lexical features, we normalize
dates and numbers. For example, 1980 becomes
*DDDD* and 212-325-4751 becomes *DDD*-
*DDD*-*DDDD*. This allows a degree of abstrac-
tion to years, phone numbers, etc. This delexi-
calization is performed separately from using the
word representation. That is, if we have induced
an embedding for 12/3/2008 , we will use the em-
bedding of 12/3/2008 , and *DD*/*D*/*DDDD*
in the baseline features listed above.
Unlike in our chunking experiments, after we
chose the best model on the development set, we
used that model on the test set too. (In chunking,
after finding the best hyperparameters on the
development set, we would combine the dev
and training set and training a model over this
combined set, and then evaluate on test.)
The standard evaluation benchmark for NER
is the CoNLL03 shared task dataset drawn from
the Reuters newswire. The training set contains
204K words (14K sentences, 946 documents), the
test set contains 46K words (3.5K sentences, 231
documents), and the development set contains
51K words (3.3K sentences, 216 documents).
We also evaluated on an out-of-domain (OOD)
dataset, the MUC7 formal run (59K words).
MUC7 has a different annotation standard than
the CoNLL03 data. It has several NE types that
don’t appear in CoNLL03: money, dates, and
numeric quantities. CoNLL03 has MISC, which
is not present in MUC7. To evaluate on MUC7,
we perform the following postprocessing steps
prior to evaluation:
</bodyText>
<listItem confidence="0.9875106">
1. In the gold-standard MUC7 data, discard
(label as ‘O’) all NEs with type NUM-
BER/MONEY/DATE.
2. In the predicted model output on MUC7 data,
discard (label as ‘O’) all NEs with type MISC.
</listItem>
<page confidence="0.995155">
388
</page>
<bodyText confidence="0.994577">
These postprocessing steps will adversely affect
all NER models across-the-board, nonetheless
allowing us to compare different models in a
controlled manner.
</bodyText>
<sectionHeader confidence="0.98684" genericHeader="method">
6 Unlabled Data
</sectionHeader>
<bodyText confidence="0.999708822222222">
Unlabeled data is used for inducing the word
representations. We used the RCV1 corpus, which
contains one year of Reuters English newswire,
from August 1996 to August 1997, about 63
millions words in 3.3 million sentences. We
left case intact in the corpus. By comparison,
Collobert and Weston (2008) downcases words
and delexicalizes numbers.
We use a preprocessing technique proposed
by Liang, (2005, p. 51), which was later used
by Koo et al. (2008): Remove all sentences that
are less than 90% lowercase a–z. We assume
that whitespace is not counted, although this
is not specified in Liang’s thesis. We call this
preprocessing step cleaning.
In Turian et al. (2009), we found that all
word representations performed better on the
supervised task when they were induced on the
clean unlabeled data, both embeddings and Brown
clusters. This is the case even though the cleaning
process was very aggressive, and discarded more
than half of the sentences. According to the
evidence and arguments presented in Bengio et al.
(2009), the non-convex optimization process for
Collobert and Weston (2008) embeddings might
be adversely affected by noise and the statistical
sparsity issues regarding rare words, especially
at the beginning of training. For this reason, we
hypothesize that learning representations over the
most frequent words first and gradually increasing
the vocabulary—a curriculum training strategy
(Elman, 1993; Bengio et al., 2009; Spitkovsky
et al., 2010)—would provide better results than
cleaning.
After cleaning, there are 37 million words (58%
of the original) in 1.3 million sentences (41% of
the original). The cleaned RCV1 corpus has 269K
word types. This is the vocabulary size, i.e. how
many word representations were induced. Note
that cleaning is applied only to the unlabeled data,
not to the labeled data used in the supervised tasks.
RCV1 is a superset of the CoNLL03 corpus.
For this reason, NER results that use RCV1
word representations are a form of transductive
learning.
</bodyText>
<sectionHeader confidence="0.993121" genericHeader="evaluation">
7 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.998283">
7.1 Details of inducing word representations
</subsectionHeader>
<bodyText confidence="0.999901208333333">
The Brown clusters took roughly 3 days to induce,
when we induced 1000 clusters, the baseline in
prior work (Koo et al., 2008; Ratinov &amp; Roth,
2009). We also induced 100, 320, and 3200
Brown clusters, for comparison. (Because Brown
clustering scales quadratically in the number of
clusters, inducing 10000 clusters would have
been prohibitive.) Because Brown clusters are
hierarchical, we can use cluster supersets as
features. We used clusters at path depth 4, 6, 10,
and 20 (Ratinov &amp; Roth, 2009). These are the
prefixes used in Table 1.
The Collobert and Weston (2008) (C&amp;W)
embeddings were induced over the course of a
few weeks, and trained for about 50 epochs. One
of the difficulties in inducing these embeddings is
that there is no stopping criterion defined, and that
the quality of the embeddings can keep improving
as training continues. Collobert (p.c.) simply
leaves one computer training his embeddings
indefinitely. We induced embeddings with 25, 50,
100, or 200 dimensions over 5-gram windows.
In comparison to Turian et al. (2009), we use
improved C&amp;W embeddings in this work:
</bodyText>
<listItem confidence="0.868783666666667">
• They were trained for 50 epochs, not just 20
epochs.
• We initialized all embedding dimensions uni-
</listItem>
<bodyText confidence="0.993076733333333">
formly in the range [-0.01, +0.01], not [-1,+1].
For rare words, which are typically updated only
143 times per epoch2, and given that our embed-
ding learning rate was typically 1e-6 or 1e-7, this
means that rare word embeddings will be concen-
trated around zero, instead of spread out randomly.
The HLBL embeddings were trained for 100
epochs (7 days).3 Unlike our Collobert and We-
ston (2008) embeddings, we did not extensively
tune the learning rates for HLBL. We used a learn-
ing rate of 1e-3 for both model parameters and
embedding parameters. We induced embeddings
with 100 dimensions over 5-gram windows, and
embeddings with 50 dimensions over 5-gram win-
dows. Embeddings were induced over one pass
</bodyText>
<footnote confidence="0.97358725">
2A rare word will appear 5 (window size) times per
epoch as a positive example, and 37M (training examples per
epoch) / 269K (vocabulary size) = 138 times per epoch as a
corruption example.
3The HLBL model updates require fewer matrix mul-
tiplies than Collobert and Weston (2008) model updates.
Additionally, HLBL models were trained on a GPGPU,
which is faster than conventional CPU arithmetic.
</footnote>
<page confidence="0.998969">
389
</page>
<bodyText confidence="0.999553">
approach using a random tree, not two passes with
an updated tree and embeddings re-estimation.
</bodyText>
<subsectionHeader confidence="0.998616">
7.2 Scaling of Word Embeddings
</subsectionHeader>
<bodyText confidence="0.986182636363636">
Like many NLP systems, the baseline system con-
tains only binary features. The word embeddings,
however, are real numbers that are not necessarily
in a bounded range. If the range of the word
embeddings is too large, they will exert more
influence than the binary features.
We generally found that embeddings had zero
mean. We can scale the embeddings by a hy-
perparameter, to control their standard deviation.
Assume that the embeddings are represented by a
matrix E:
</bodyText>
<equation confidence="0.999649">
E (-- c- · E/stddev(E) (1)
</equation>
<bodyText confidence="0.998551428571429">
c- is a scaling constant that sets the new standard
deviation after scaling the embeddings.
work. In Turian et al. (2009), we were not
able to prescribe a default value for scaling the
embeddings. However, these curves demonstrate
that a reasonable choice of scale factor is such that
the embeddings have a standard deviation of 0.1.
</bodyText>
<subsectionHeader confidence="0.87374">
7.3 Capacity of Word Representations
</subsectionHeader>
<table confidence="0.545956333333333">
# of embedding dimensions
# of Brown clusters
# of embedding dimensions
</table>
<figure confidence="0.992825">
25 50 100 200
100 320 1000 3200
(a)
Validation F1
94.7
94.6
94.5
94.4
94.3
94.2
94.1
C&amp;W
HLBL
Brown
baseline
25 50 100 200
C&amp;W, 50-dim
HLBL, 50-dim
C&amp;W, 200-dim
C&amp;W, 100-dim
HLBL, 100-dim
C&amp;W, 25-dim
baseline
92.5
91.5
90.5
92
91
90
C&amp;W
Brown
HLBL
baseline
100 320 1000 3200
# of Brown clusters
Validation F1
(b)
(a) Validation F1 94.8
94.6
94.4
94.2
94
93.8
93.6
0.001 0.01 0.1 1
Scaling factor σ
</figure>
<figureCaption confidence="0.999348769230769">
Figure 1: Effect as we vary the scaling factor a- (Equa-
tion 1) on the validation set F1. We experiment with
Collobert and Weston (2008) and HLBL embeddings of var-
ious dimensionality. (a) Chunking results. (b) NER results.
Figure 1 shows the effect of scaling factor c-
on both supervised tasks. We were surprised
to find that on both tasks, across Collobert and
Weston (2008) and HLBL embeddings of various
dimensionality, that all curves had similar shapes
and optima. This is one contributions of our
Figure 2: Effect as we vary the capacity of the word
representations on the validation set F1. (a) Chunking
results. (b) NER results.
</figureCaption>
<bodyText confidence="0.997589736842105">
There are capacity controls for the word
representations: number of Brown clusters, and
number of dimensions of the word embeddings.
Figure 2 shows the effect on the validation F1 as
we vary the capacity of the word representations.
In general, it appears that more Brown clusters
are better. We would like to induce 10000 Brown
clusters, however this would take several months.
In Turian et al. (2009), we hypothesized on
the basis of solely the HLBL NER curve that
higher-dimensional word embeddings would give
higher accuracy. Figure 2 shows that this hy-
pothesis is not true. For NER, the C&amp;W curve is
almost flat, and we were suprised to find the even
25-dimensional C&amp;W word embeddings work so
well. For chunking, 50-dimensional embeddings
had the highest validation F1 for both C&amp;W and
HLBL. These curves indicates that the optimal
capacity of the word embeddings is task-specific.
</bodyText>
<figure confidence="0.923470368421053">
0.001 0.01 0.1 1
Scaling factor σ
(b)
Validation F1
92.5
91.5
90.5
89.5
92
91
90
89
C&amp;W, 200-dim
C&amp;W, 100-dim
C&amp;W, 25-dim
C&amp;W, 50-dim
HLBL, 100-dim
HLBL, 50-dim
baseline
</figure>
<page confidence="0.927076">
390
</page>
<table confidence="0.988833083333333">
System Dev Test
Baseline 94.16 93.79
HLBL, 50-dim 94.63 94.00
C&amp;W, 50-dim 94.66 94.10
Brown, 3200 clusters 94.67 94.11
Brown+HLBL, 37M 94.62 94.13
C&amp;W+HLBL, 37M 94.68 94.25
Brown+C&amp;W+HLBL, 37M 94.72 94.15
Brown+C&amp;W, 37M 94.76 94.35
Ando and Zhang (2005), 15M - 94.39
Suzuki and Isozaki (2008), 15M - 94.67
Suzuki and Isozaki (2008), 1B - 95.15
</table>
<tableCaption confidence="0.9549765">
Table 2: Final chunking F1 results. In the last section, we
show how many unlabeled words were used.
</tableCaption>
<table confidence="0.999826190476191">
System Dev Test MUC7
Baseline 90.03 84.39 67.48
Baseline+Nonlocal 91.91 86.52 71.80
HLBL 100-dim 92.00 88.13 75.25
Gazetteers 92.09 87.36 77.76
C&amp;W 50-dim 92.27 87.93 75.74
Brown, 1000 clusters 92.32 88.52 78.84
C&amp;W 200-dim 92.46 87.96 75.51
C&amp;W+HLBL 92.52 88.56 78.64
Brown+HLBL 92.56 88.93 77.85
Brown+C&amp;W 92.79 89.31 80.13
HLBL+Gaz 92.91 89.35 79.29
C&amp;W+Gaz 92.98 88.88 81.44
Brown+Gaz 93.25 89.41 82.71
Lin and Wu (2009), 3.4B - 88.44 -
Ando and Zhang (2005), 27M 93.15 89.31 -
Suzuki and Isozaki (2008), 37M 93.66 89.36 -
Suzuki and Isozaki (2008), 1B 94.48 89.92 -
All (Brown+C&amp;W+HLBL+Gaz), 37M 93.17 90.04 82.50
All+Nonlocal, 37M 93.95 90.36 84.15
Lin and Wu (2009), 700B - 90.90 -
</table>
<tableCaption confidence="0.95854">
Table 3: Final NER F1 results, showing the cumulative
effect of adding word representations, non-local features, and
</tableCaption>
<bodyText confidence="0.8335564">
gazetteers to the baseline. To speed up training, in combined
experiments (C&amp;W plus another word representation),
we used the 50-dimensional C&amp;W embeddings, not the
200-dimensional ones. In the last section, we show how
many unlabeled words were used.
</bodyText>
<subsectionHeader confidence="0.99752">
7.4 Final results
</subsectionHeader>
<bodyText confidence="0.999713571428572">
Table 2 shows the final chunking results and Ta-
ble 3 shows the final NER F1 results. We compare
to the state-of-the-art methods of Ando and Zhang
(2005), Suzuki and Isozaki (2008), and—for
NER—Lin and Wu (2009). Tables 2 and 3 show
that accuracy can be increased further by combin-
ing the features from different types of word rep-
resentations. But, if only one word representation
is to be used, Brown clusters have the highest ac-
curacy. Given the improvements to the C&amp;W em-
beddings since Turian et al. (2009), C&amp;W em-
beddings outperform the HLBL embeddings. On
chunking, there is only a minute difference be-
tween Brown clusters and the embeddings. Com-
</bodyText>
<figure confidence="0.9625885">
0 1 10 100 1K 10K 100K 1M
Frequency of word in unlabeled data
0 1 10 100 1K 10K 100K 1M
Frequency of word in unlabeled data
</figure>
<figureCaption confidence="0.99224925">
Figure 3: For word tokens that have different frequency
in the unlabeled data, what is the total number of per-token
errors incurred on the test set? (a) Chunking results. (b) NER
results.
</figureCaption>
<bodyText confidence="0.99996288">
bining representations leads to small increases in
the test F1. In comparison to chunking, combin-
ing different word representations on NER seems
gives larger improvements on the test F1.
On NER, Brown clusters are superior to the
word embeddings. Since much of the NER F1
is derived from decisions made over rare words,
we suspected that Brown clustering has a superior
representation for rare words. Brown makes
a single hard clustering decision, whereas the
embedding for a rare word is close to its initial
value since it hasn’t received many training
updates (see Footnote 2). Figure 3 shows the total
number of per-token errors incurred on the test
set, depending upon the frequency of the word
token in the unlabeled data. For NER, Figure 3 (b)
shows that most errors occur on rare words, and
that Brown clusters do indeed incur fewer errors
for rare words. This supports our hypothesis
that, for rare words, Brown clustering produces
better representations than word embeddings that
haven’t received sufficient training updates. For
chunking, Brown clusters and C&amp;W embeddings
incur almost identical numbers of errors, and
errors are concentrated around the more common
</bodyText>
<figure confidence="0.99038565">
250
200
150
100
50
0
C&amp;W, 50-dim
Brown, 3200 clusters
(b)
# of per-token errors (test set)
250
200
150
100
50
0
C&amp;W, 50-dim
Brown, 1000 clusters
# of per-token errors (test set)
(a)
</figure>
<page confidence="0.995997">
391
</page>
<bodyText confidence="0.99998269047619">
words. We hypothesize that non-rare words have
good representations, regardless of the choice
of word representation technique. For tasks like
chunking in which a syntactic decision relies upon
looking at several token simultaneously, com-
pound features that use the word representations
might increase accuracy more (Koo et al., 2008).
Using word representations in NER brought
larger gains on the out-of-domain data than on the
in-domain data. We were surprised by this result,
because the OOD data was not even used during
the unsupervised word representation induction,
as was the in-domain data. We are curious to
investigate this phenomenon further.
Ando and Zhang (2005) present a semi-
supervised learning algorithm called alternating
structure optimization (ASO). They find a low-
dimensional projection of the input features that
gives good linear classifiers over auxiliary tasks.
These auxiliary tasks are sometimes specific
to the supervised task, and sometimes general
language modeling tasks like “predict the missing
word”. Suzuki and Isozaki (2008) present a semi-
supervised extension of CRFs. (In Suzuki et al.
(2009), they extend their semi-supervised ap-
proach to more general conditional models.) One
of the advantages of the semi-supervised learning
approach that we use is that it is simpler and more
general than that of Ando and Zhang (2005) and
Suzuki and Isozaki (2008). Their methods dictate
a particular choice of model and training regime
and could not, for instance, be used with an NLP
system based upon an SVM classifier.
Lin and Wu (2009) present a K-means-like
non-hierarchical clustering algorithm for phrases,
which uses MapReduce. Since they can scale
to millions of phrases, and they train over 800B
unlabeled words, they achieve state-of-the-art
accuracy on NER using their phrase clusters.
This suggests that extending word representa-
tions to phrase representations is worth further
investigation.
</bodyText>
<sectionHeader confidence="0.999359" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.989323255813954">
Word features can be learned in advance in an
unsupervised, task-inspecific, and model-agnostic
manner. These word features, once learned, are
easily disseminated with other researchers, and
easily integrated into existing supervised NLP
systems. The disadvantage, however, is that ac-
curacy might not be as high as a semi-supervised
method that includes task-specific information
and that jointly learns the supervised and unsu-
pervised tasks (Ando &amp; Zhang, 2005; Suzuki &amp;
Isozaki, 2008; Suzuki et al., 2009).
Unsupervised word representations have been
used in previous NLP work, and have demon-
strated improvements in generalization accuracy
on a variety of tasks. Ours is the first work to
systematically compare different word repre-
sentations in a controlled way. We found that
Brown clusters and word embeddings both can
improve the accuracy of a near-state-of-the-art
supervised NLP system. We also found that com-
bining different word representations can improve
accuracy further. Error analysis indicates that
Brown clustering induces better representations
for rare words than C&amp;W embeddings that have
not received many training updates.
Another contribution of our work is a default
method for setting the scaling parameter for
word embeddings. With this contribution, word
embeddings can now be used off-the-shelf as
word features, with no tuning.
Future work should explore methods for
inducing phrase representations, as well as tech-
niques for increasing in accuracy by using word
representations in compound features.
Replicating our experiments
You can visit http://metaoptimize.com/
projects/wordreprs/ to find: The word
representations we induced, which you can
download and use in your experiments; The code
for inducing the word representations, which you
can use to induce word representations on your
own data; The NER and chunking system, with
code for replicating our experiments.
</bodyText>
<sectionHeader confidence="0.997675" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999202714285714">
Thank you to Magnus Sahlgren, Bob Carpenter,
Percy Liang, Alexander Yates, and the anonymous
reviewers for useful discussion. Thank you to
Andriy Mnih for inducing his embeddings on
RCV1 for us. Joseph Turian and Yoshua Bengio
acknowledge the following agencies for re-
search funding and computing support: NSERC,
RQCHP, CIFAR. Lev Ratinov was supported by
the Air Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181. Any
opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the
author and do not necessarily reflect the view of
the Air Force Research Laboratory (AFRL).
</bodyText>
<page confidence="0.996317">
392
</page>
<sectionHeader confidence="0.989465" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.907410333333333">
Ando, R., &amp; Zhang, T. (2005). A high-
performance semi-supervised learning method
for text chunking. ACL.
Bengio, Y. (2008). Neural net language models.
Scholarpedia, 3, 3881.
Bengio, Y., Ducharme, R., &amp; Vincent, P. (2001).
A neural probabilistic language model. NIPS.
Bengio, Y., Ducharme, R., Vincent, P., &amp; Jauvin,
C. (2003). A neural probabilistic language
model. Journal of Machine Learning Research,
3, 1137–1155.
Bengio, Y., Louradour, J., Collobert, R., &amp;
Weston, J. (2009). Curriculum learning. ICML.
Bengio, Y., &amp; S´en´ecal, J.-S. (2003). Quick train-
ing of probabilistic neural nets by importance
sampling. AISTATS.
Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003).
Latent dirichlet allocation. Journal of Machine
Learning Research, 3, 993–1022.
Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra,
V. J. D., &amp; Lai, J. C. (1992). Class-based n-gram
models of natural language. Computational
Linguistics, 18, 467–479.
Candito, M., &amp; Crabb´e, B. (2009). Improving gen-
erative statistical parsing with semi-supervised
word clustering. IWPT (pp. 138–141).
Collobert, R., &amp; Weston, J. (2008). A unified
</reference>
<bodyText confidence="0.593816913043478">
architecture for natural language processing:
Deep neural networks with multitask learning.
ICML.
Deschacht, K., &amp; Moens, M.-F. (2009). Semi-
supervised semantic role labeling using the
Latent Words Language Model. EMNLP (pp.
21–29).
Dumais, S. T., Furnas, G. W., Landauer, T. K.,
Deerwester, S., &amp; Harshman, R. (1988). Using
latent semantic analysis to improve access to
textual information. SIGCHI Conference on
Human Factors in Computing Systems (pp.
281–285). ACM.
Elman, J. L. (1993). Learning and development
in neural networks: The importance of starting
small. Cognition, 48, 781–799.
Goldberg, Y., Tsarfaty, R., Adler, M., &amp; Elhadad,
M. (2009). Enhancing unlexicalized parsing
performance using a wide coverage lexicon,
fuzzy tag-set mapping, and EM-HMM-based
lexical probabilities. EACL.
Honkela, T. (1997). Self-organizing maps of
words for natural language processing applica-
</bodyText>
<reference confidence="0.997919318181818">
tions. Proceedings of the International ICSC
Symposium on Soft Computing.
Honkela, T., Pulkki, V., &amp; Kohonen, T. (1995).
Contextual relations of words in grimm tales,
analyzed by self-organizing map. ICANN.
Huang, F., &amp; Yates, A. (2009). Distributional rep-
resentations for handling sparsity in supervised
sequence labeling. ACL.
Kaski, S. (1998). Dimensionality reduction by
random mapping: Fast similarity computation
for clustering. IJCNN (pp. 413–418).
Koo, T., Carreras, X., &amp; Collins, M. (2008).
Simple semi-supervised dependency parsing.
ACL (pp. 595–603).
Krishnan, V., &amp; Manning, C. D. (2006). An
effective two-stage model for exploiting non-
local dependencies in named entity recognition.
COLING-ACL.
Landauer, T. K., Foltz, P. W., &amp; Laham, D. (1998).
An introduction to latent semantic analysis.
Discourse Processes, 259–284.
Li, W., &amp; McCallum, A. (2005). Semi-supervised
sequence modeling with syntactic topic models.
AAAI.
Liang, P. (2005). Semi-supervised learning
for natural language. Master’s thesis, Mas-
sachusetts Institute of Technology.
Lin, D., &amp; Wu, X. (2009). Phrase clustering
for discriminative learning. ACL-IJCNLP (pp.
1030–1038).
Lund, K., &amp; Burgess, C. (1996). Producing
highdimensional semantic spaces from lexical
co-occurrence. Behavior Research Methods,
Instrumentation, and Computers, 28, 203–208.
Lund, K., Burgess, C., &amp; Atchley, R. A. (1995).
Semantic and associative priming in high-
dimensional semantic space. Cognitive Science
Proceedings, LEA (pp. 660–665).
Martin, S., Liermann, J., &amp; Ney, H. (1998). Algo-
rithms for bigram and trigram word clustering.
Speech Communication, 24, 19–37.
Miller, S., Guinness, J., &amp; Zamanian, A. (2004).
Name tagging with word clusters and discrim-
inative training. HLT-NAACL (pp. 337–342).
</reference>
<page confidence="0.990432">
393
</page>
<reference confidence="0.999226621052631">
Mnih, A., &amp; Hinton, G. E. (2007). Three
new graphical models for statistical language
modelling. ICML.
Mnih, A., &amp; Hinton, G. E. (2009). A scalable
hierarchical distributed language model. NIPS
(pp. 1081–1088).
Morin, F., &amp; Bengio, Y. (2005). Hierarchical
probabilistic neural network language model.
AISTATS.
Pereira, F., Tishby, N., &amp; Lee, L. (1993). Distri-
butional clustering of english words. ACL (pp.
183–190).
Ratinov, L., &amp; Roth, D. (2009). Design chal-
lenges and misconceptions in named entity
recognition. CoNLL.
Ritter, H., &amp; Kohonen, T. (1989). Self-organizing
semantic maps. Biological Cybernetics,
241–254.
Sahlgren, M. (2001). Vector-based semantic
analysis: Representing word meanings based
on random labels. Proceedings of the Semantic
Knowledge Acquisition and Categorisation
Workshop, ESSLLI.
Sahlgren, M. (2005). An introduction to random
indexing. Methods and Applications of Seman-
tic Indexing Workshop at the 7th International
Conference on Terminology and Knowledge
Engineering (TKE).
Sahlgren, M. (2006). The word-space model:
Using distributional analysis to represent syn-
tagmatic and paradigmatic relations between
words in high-dimensional vector spaces.
Doctoral dissertation, Stockholm University.
Sang, E. T., &amp; Buchholz, S. (2000). Introduction
to the CoNLL-2000 shared task: Chunking.
CoNLL.
Schwenk, H., &amp; Gauvain, J.-L. (2002). Connec-
tionist language modeling for large vocabulary
continuous speech recognition. International
Conference on Acoustics, Speech and Signal
Processing (ICASSP) (pp. 765–768). Orlando,
Florida.
Sha, F., &amp; Pereira, F. C. N. (2003). Shal-
low parsing with conditional random fields.
HLT-NAACL.
Spitkovsky, V., Alshawi, H., &amp; Jurafsky, D.
(2010). From baby steps to leapfrog: How “less
is more” in unsupervised dependency parsing.
NAACL-HLT.
Suzuki, J., &amp; Isozaki, H. (2008). Semi-supervised
sequential labeling and segmentation using
giga-word scale unlabeled data. ACL-08: HLT
(pp. 665–673).
Suzuki, J., Isozaki, H., Carreras, X., &amp; Collins, M.
(2009). An empirical study of semi-supervised
structured conditional models for dependency
parsing. EMNLP.
Turian, J., Ratinov, L., Bengio, Y., &amp; Roth, D.
(2009). A preliminary evaluation of word
representations for named-entity recognition.
NIPS Workshop on Grammar Induction, Repre-
sentation of Language and Language Learning.
Turney, P. D., &amp; Pantel, P. (2010). From frequency
to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research.
Ushioda, A. (1996). Hierarchical clustering of
words. COLING (pp. 1159–1162).
V¨ayrynen, J., &amp; Honkela, T. (2005). Compar-
ison of independent component analysis and
singular value decomposition in word context
analysis. AKRR’05, International and Interdis-
ciplinary Conference on Adaptive Knowledge
Representation and Reasoning.
V¨ayrynen, J. J., &amp; Honkela, T. (2004). Word cat-
egory maps based on emergent features created
by ICA. Proceedings of the STeP’2004 Cogni-
tion + Cybernetics Symposium (pp. 173–185).
Finnish Artificial Intelligence Society.
V¨ayrynen, J. J., Honkela, T., &amp; Lindqvist, L.
(2007). Towards explicit semantic features
using independent component analysis. Pro-
ceedings of the Workshop Semantic Content
Acquisition and Representation (SCAR). Stock-
holm, Sweden: Swedish Institute of Computer
Science.
ˇReh˚uˇrek, R., &amp; Sojka, P. (2010). Software frame-
work for topic modelling with large corpora.
LREC.
Zhang, T., &amp; Johnson, D. (2003). A robust risk
minimization based named entity recognition
system. CoNLL.
Zhao, H., Chen, W., Kit, C., &amp; Zhou, G.
(2009). Multilingual dependency learning: a
huge feature engineering method to semantic
dependency parsing. CoNLL (pp. 55–60).
</reference>
<page confidence="0.998998">
394
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.262803">
<title confidence="0.9931275">Word representations: A simple and general method for semi-supervised learning</title>
<author confidence="0.997462">Joseph Turian</author>
<affiliation confidence="0.880282">D´epartement d’Informatique et Recherche Op´erationnelle (DIRO) Universit´e de Montr´eal</affiliation>
<address confidence="0.998673">Montr´eal, Qu´ebec, Canada, H3T 1J4</address>
<email confidence="0.994639">lastname@iro.umontreal.ca</email>
<author confidence="0.999441">Lev Ratinov</author>
<affiliation confidence="0.98828075">Department of Computer Science University of Illinois at Urbana-Champaign</affiliation>
<address confidence="0.951718">Urbana, IL 61801</address>
<email confidence="0.997984">ratinov2@uiuc.edu</email>
<author confidence="0.961943">Yoshua Bengio</author>
<affiliation confidence="0.875951333333333">D´epartement d’Informatique et Recherche Op´erationnelle (DIRO) Universit´e de Montr´eal</affiliation>
<address confidence="0.998772">Montr´eal, Qu´ebec, Canada, H3T 1J4</address>
<email confidence="0.995687">bengioy@iro.umontreal.ca</email>
<abstract confidence="0.993129611111111">If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih &amp; Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further by combining word representations. You can download word features, for use in existing NLP systems, as well as our here:</abstract>
<intro confidence="0.726515">com/projects/wordreprs/</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Ando</author>
<author>T Zhang</author>
</authors>
<title>A highperformance semi-supervised learning method for text chunking.</title>
<date>2005</date>
<publisher>ACL.</publisher>
<contexts>
<context position="1423" citStr="Ando and Zhang (2005)" startWordPosition="186" endWordPosition="189">ings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/ 1 Introduction By using unlabelled data to reduce data sparsity in the labeled training data, semi-supervised approaches improve generalization accuracy. Semi-supervised models such as Ando and Zhang (2005), Suzuki and Isozaki (2008), and Suzuki et al. (2009) achieve state-of-the-art accuracy. However, these approaches dictate a particular choice of model and training regime. It can be tricky and time-consuming to adapt an existing supervised NLP system to use these semi-supervised techniques. It is preferable to use a simple and general method to adapt existing supervised NLP systems to be semi-supervised. One approach that is becoming popular is to use unsupervised methods to induce word features—or to download word features that have already been induced—plug these word features into an exist</context>
<context position="16311" citStr="Ando and Zhang (2005)" startWordPosition="2549" endWordPosition="2552">isting, near state-of-the-art, supervised NLP system, and improve its accuracy by including word representations as word features. This technique for turning a supervised approach into a semi-supervised one is general and task-agnostic. However, we wish to find out if certain word representations are preferable for certain tasks. Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. We apply clustering and distributed representations to NER and chunking, which allows us to compare our semi-supervised models to those of Ando and Zhang (2005) and Suzuki and Isozaki (2008). 5.1 Chunking Chunking is a syntactic sequence labeling task. We follow the conditions in the CoNLL-2000 shared task (Sang &amp; Buchholz, 2000). The linear CRF chunker of Sha and Pereira (2003) is a standard near-state-of-the-art baseline chunker. In fact, many off-the-shelf CRF implementations now replicate Sha and Pereira (2003), including their choice of feature set: • CRF++ by Taku Kudo (http://crfpp. sourceforge.net/) • crfsgd by L´eon Bottou (http://leon. bottou.org/projects/sgd) • CRFsuite by by Naoaki Okazaki (http:// www.chokkan.org/software/crfsuite/) We u</context>
<context position="30024" citStr="Ando and Zhang (2005)" startWordPosition="4796" endWordPosition="4799">k so well. For chunking, 50-dimensional embeddings had the highest validation F1 for both C&amp;W and HLBL. These curves indicates that the optimal capacity of the word embeddings is task-specific. 0.001 0.01 0.1 1 Scaling factor σ (b) Validation F1 92.5 91.5 90.5 89.5 92 91 90 89 C&amp;W, 200-dim C&amp;W, 100-dim C&amp;W, 25-dim C&amp;W, 50-dim HLBL, 100-dim HLBL, 50-dim baseline 390 System Dev Test Baseline 94.16 93.79 HLBL, 50-dim 94.63 94.00 C&amp;W, 50-dim 94.66 94.10 Brown, 3200 clusters 94.67 94.11 Brown+HLBL, 37M 94.62 94.13 C&amp;W+HLBL, 37M 94.68 94.25 Brown+C&amp;W+HLBL, 37M 94.72 94.15 Brown+C&amp;W, 37M 94.76 94.35 Ando and Zhang (2005), 15M - 94.39 Suzuki and Isozaki (2008), 15M - 94.67 Suzuki and Isozaki (2008), 1B - 95.15 Table 2: Final chunking F1 results. In the last section, we show how many unlabeled words were used. System Dev Test MUC7 Baseline 90.03 84.39 67.48 Baseline+Nonlocal 91.91 86.52 71.80 HLBL 100-dim 92.00 88.13 75.25 Gazetteers 92.09 87.36 77.76 C&amp;W 50-dim 92.27 87.93 75.74 Brown, 1000 clusters 92.32 88.52 78.84 C&amp;W 200-dim 92.46 87.96 75.51 C&amp;W+HLBL 92.52 88.56 78.64 Brown+HLBL 92.56 88.93 77.85 Brown+C&amp;W 92.79 89.31 80.13 HLBL+Gaz 92.91 89.35 79.29 C&amp;W+Gaz 92.98 88.88 81.44 Brown+Gaz 93.25 89.41 82.71 L</context>
<context position="31444" citStr="Ando and Zhang (2005)" startWordPosition="5029" endWordPosition="5032">90.04 82.50 All+Nonlocal, 37M 93.95 90.36 84.15 Lin and Wu (2009), 700B - 90.90 - Table 3: Final NER F1 results, showing the cumulative effect of adding word representations, non-local features, and gazetteers to the baseline. To speed up training, in combined experiments (C&amp;W plus another word representation), we used the 50-dimensional C&amp;W embeddings, not the 200-dimensional ones. In the last section, we show how many unlabeled words were used. 7.4 Final results Table 2 shows the final chunking results and Table 3 shows the final NER F1 results. We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and—for NER—Lin and Wu (2009). Tables 2 and 3 show that accuracy can be increased further by combining the features from different types of word representations. But, if only one word representation is to be used, Brown clusters have the highest accuracy. Given the improvements to the C&amp;W embeddings since Turian et al. (2009), C&amp;W embeddings outperform the HLBL embeddings. On chunking, there is only a minute difference between Brown clusters and the embeddings. Com0 1 10 100 1K 10K 100K 1M Frequency of word in unlabeled data 0 1 10 100 1K 10K 100K 1M Frequency of w</context>
<context position="34294" citStr="Ando and Zhang (2005)" startWordPosition="5503" endWordPosition="5506">resentations, regardless of the choice of word representation technique. For tasks like chunking in which a syntactic decision relies upon looking at several token simultaneously, compound features that use the word representations might increase accuracy more (Koo et al., 2008). Using word representations in NER brought larger gains on the out-of-domain data than on the in-domain data. We were surprised by this result, because the OOD data was not even used during the unsupervised word representation induction, as was the in-domain data. We are curious to investigate this phenomenon further. Ando and Zhang (2005) present a semisupervised learning algorithm called alternating structure optimization (ASO). They find a lowdimensional projection of the input features that gives good linear classifiers over auxiliary tasks. These auxiliary tasks are sometimes specific to the supervised task, and sometimes general language modeling tasks like “predict the missing word”. Suzuki and Isozaki (2008) present a semisupervised extension of CRFs. (In Suzuki et al. (2009), they extend their semi-supervised approach to more general conditional models.) One of the advantages of the semi-supervised learning approach th</context>
<context position="36025" citStr="Ando &amp; Zhang, 2005" startWordPosition="5762" endWordPosition="5765">n NER using their phrase clusters. This suggests that extending word representations to phrase representations is worth further investigation. 8 Conclusions Word features can be learned in advance in an unsupervised, task-inspecific, and model-agnostic manner. These word features, once learned, are easily disseminated with other researchers, and easily integrated into existing supervised NLP systems. The disadvantage, however, is that accuracy might not be as high as a semi-supervised method that includes task-specific information and that jointly learns the supervised and unsupervised tasks (Ando &amp; Zhang, 2005; Suzuki &amp; Isozaki, 2008; Suzuki et al., 2009). Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. Ours is the first work to systematically compare different word representations in a controlled way. We found that Brown clusters and word embeddings both can improve the accuracy of a near-state-of-the-art supervised NLP system. We also found that combining different word representations can improve accuracy further. Error analysis indicates that Brown clustering induces better representation</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Ando, R., &amp; Zhang, T. (2005). A highperformance semi-supervised learning method for text chunking. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
</authors>
<title>Neural net language models.</title>
<date>2008</date>
<journal>Scholarpedia,</journal>
<volume>3</volume>
<pages>3881</pages>
<contexts>
<context position="4010" citStr="Bengio (2008)" startWordPosition="585" endWordPosition="586">esearchers (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Ratinov &amp; Roth, 2009; Huang &amp; Yates, 2009). This leads to a one-hot representation over a smaller vocabulary size. Neural language models (Bengio et al., 2001; Schwenk &amp; Gauvain, 2002; Mnih &amp; Hinton, 2007; Collobert &amp; Weston, 2008), on the other hand, induce dense real-valued low-dimensional 384 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics word embeddings using unsupervised approaches. (See Bengio (2008) for a more complete list of references on neural language models.) Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. But different word representations have never been systematically compared in a controlled way. In this work, we compare different techniques for inducing word representations, evaluating them on the tasks of named entity recognition (NER) and chunking. We retract former negative results published in Turian et al. (2009) about Collobert and Weston (2008) embeddings, given t</context>
<context position="12005" citStr="Bengio, 2008" startWordPosition="1851" endWordPosition="1852">ion. (Not to be confused with distributional representations.) A distributed representation is dense, lowdimensional, and real-valued. Distributed word representations are called word embeddings. Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. A distributed representation is compact, in the sense that it can represent an exponential number of clusters in the number of dimensions. Word embeddings are typically induced using neural language models, which use neural networks as the underlying predictive model (Bengio, 2008). Historically, training and testing of neural language models has been slow, scaling as the size of the vocabulary for each model computation (Bengio et al., 2001; Bengio et al., 2003). However, many approaches have been proposed in recent years to eliminate that linear dependency on vocabulary size (Morin &amp; Bengio, 2005; Collobert &amp; Weston, 2008; Mnih &amp; Hinton, 2009) and allow scaling to very large training corpora. 4.1 Collobert and Weston (2008) embeddings Collobert and Weston (2008) presented a neural language model that could be trained over billions of words, because the gradient of the</context>
</contexts>
<marker>Bengio, 2008</marker>
<rawString>Bengio, Y. (2008). Neural net language models. Scholarpedia, 3, 3881.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2001</date>
<publisher>NIPS.</publisher>
<contexts>
<context position="3619" citStr="Bengio et al., 2001" startWordPosition="530" endWordPosition="533">ta. These limitations of one-hot word representations have prompted researchers to investigate unsupervised methods for inducing word representations over large unlabeled corpora. Word features can be hand-designed, but our goal is to learn them. One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical. This technique was used by a variety of researchers (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Ratinov &amp; Roth, 2009; Huang &amp; Yates, 2009). This leads to a one-hot representation over a smaller vocabulary size. Neural language models (Bengio et al., 2001; Schwenk &amp; Gauvain, 2002; Mnih &amp; Hinton, 2007; Collobert &amp; Weston, 2008), on the other hand, induce dense real-valued low-dimensional 384 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics word embeddings using unsupervised approaches. (See Bengio (2008) for a more complete list of references on neural language models.) Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a var</context>
<context position="12168" citStr="Bengio et al., 2001" startWordPosition="1876" endWordPosition="1879">sentations are called word embeddings. Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. A distributed representation is compact, in the sense that it can represent an exponential number of clusters in the number of dimensions. Word embeddings are typically induced using neural language models, which use neural networks as the underlying predictive model (Bengio, 2008). Historically, training and testing of neural language models has been slow, scaling as the size of the vocabulary for each model computation (Bengio et al., 2001; Bengio et al., 2003). However, many approaches have been proposed in recent years to eliminate that linear dependency on vocabulary size (Morin &amp; Bengio, 2005; Collobert &amp; Weston, 2008; Mnih &amp; Hinton, 2009) and allow scaling to very large training corpora. 4.1 Collobert and Weston (2008) embeddings Collobert and Weston (2008) presented a neural language model that could be trained over billions of words, because the gradient of the loss was computed stochastically over a small sample of possible outputs, in a spirit similar to Bengio and S´en´ecal (2003). This neural model of Collobert and W</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2001</marker>
<rawString>Bengio, Y., Ducharme, R., &amp; Vincent, P. (2001). A neural probabilistic language model. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>1137--1155</pages>
<contexts>
<context position="12190" citStr="Bengio et al., 2003" startWordPosition="1880" endWordPosition="1883"> word embeddings. Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. A distributed representation is compact, in the sense that it can represent an exponential number of clusters in the number of dimensions. Word embeddings are typically induced using neural language models, which use neural networks as the underlying predictive model (Bengio, 2008). Historically, training and testing of neural language models has been slow, scaling as the size of the vocabulary for each model computation (Bengio et al., 2001; Bengio et al., 2003). However, many approaches have been proposed in recent years to eliminate that linear dependency on vocabulary size (Morin &amp; Bengio, 2005; Collobert &amp; Weston, 2008; Mnih &amp; Hinton, 2009) and allow scaling to very large training corpora. 4.1 Collobert and Weston (2008) embeddings Collobert and Weston (2008) presented a neural language model that could be trained over billions of words, because the gradient of the loss was computed stochastically over a small sample of possible outputs, in a spirit similar to Bengio and S´en´ecal (2003). This neural model of Collobert and Weston (2008) was refin</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Bengio, Y., Ducharme, R., Vincent, P., &amp; Jauvin, C. (2003). A neural probabilistic language model. Journal of Machine Learning Research, 3, 1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>J Louradour</author>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>Curriculum learning.</title>
<date>2009</date>
<publisher>ICML.</publisher>
<contexts>
<context position="12847" citStr="Bengio et al. (2009)" startWordPosition="1985" endWordPosition="1988">proposed in recent years to eliminate that linear dependency on vocabulary size (Morin &amp; Bengio, 2005; Collobert &amp; Weston, 2008; Mnih &amp; Hinton, 2009) and allow scaling to very large training corpora. 4.1 Collobert and Weston (2008) embeddings Collobert and Weston (2008) presented a neural language model that could be trained over billions of words, because the gradient of the loss was computed stochastically over a small sample of possible outputs, in a spirit similar to Bengio and S´en´ecal (2003). This neural model of Collobert and Weston (2008) was refined and presented in greater depth in Bengio et al. (2009). The model is discriminative and nonprobabilistic. For each training update, we read an n-gram x = (w1, ... , wn) from the corpus. The model concatenates the learned embeddings of the n words, giving e(w1) ® ... ® e(wn), where e is the lookup table and ® is concatenation. We also create a corrupted or noise n-gram x˜ = (w1, ... , wn_q, ˜wn), where ˜wn # wn is chosen uniformly from the vocabulary.1 For convenience, 1In Collobert and Weston (2008), the middle word in the 386 we write e(x) to mean e(w1) ® ... ® e(wn). We predict a score s(x) for x by passing e(x) through a single hidden layer ne</context>
<context position="15566" citStr="Bengio et al. (2009)" startWordPosition="2434" endWordPosition="2437">earns a linear model to predict the embedding of the last word. The similarity between the predicted embedding and the current actual embedding is transformed into a probability by exponentiating and then normalizing. Mnih and Hinton (2009) speed up model evaluation during training and testing by using a hierarchy to exponentially filter down the number of computations that are performed. This hierarchical evaluation technique was first proposed by Morin and Bengio (2005). The model, combined with this optimization, is called the hierarchical log-bilinear (HLBL) model. n-gram is corrupted. In Bengio et al. (2009), the last word in the n-gram is corrupted. 5 Supervised evaluation tasks We evaluate the hypothesis that one can take an existing, near state-of-the-art, supervised NLP system, and improve its accuracy by including word representations as word features. This technique for turning a supervised approach into a semi-supervised one is general and task-agnostic. However, we wish to find out if certain word representations are preferable for certain tasks. Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. We apply cluste</context>
<context position="23267" citStr="Bengio et al. (2009)" startWordPosition="3677" endWordPosition="3680"> which was later used by Koo et al. (2008): Remove all sentences that are less than 90% lowercase a–z. We assume that whitespace is not counted, although this is not specified in Liang’s thesis. We call this preprocessing step cleaning. In Turian et al. (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters. This is the case even though the cleaning process was very aggressive, and discarded more than half of the sentences. According to the evidence and arguments presented in Bengio et al. (2009), the non-convex optimization process for Collobert and Weston (2008) embeddings might be adversely affected by noise and the statistical sparsity issues regarding rare words, especially at the beginning of training. For this reason, we hypothesize that learning representations over the most frequent words first and gradually increasing the vocabulary—a curriculum training strategy (Elman, 1993; Bengio et al., 2009; Spitkovsky et al., 2010)—would provide better results than cleaning. After cleaning, there are 37 million words (58% of the original) in 1.3 million sentences (41% of the original)</context>
</contexts>
<marker>Bengio, Louradour, Collobert, Weston, 2009</marker>
<rawString>Bengio, Y., Louradour, J., Collobert, R., &amp; Weston, J. (2009). Curriculum learning. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>J-S S´en´ecal</author>
</authors>
<title>Quick training of probabilistic neural nets by importance sampling.</title>
<date>2003</date>
<publisher>AISTATS.</publisher>
<marker>Bengio, S´en´ecal, 2003</marker>
<rawString>Bengio, Y., &amp; S´en´ecal, J.-S. (2003). Quick training of probabilistic neural nets by importance sampling. AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="5835" citStr="Blei et al., 2003" startWordPosition="882" endWordPosition="885"> for word w in a supervised model. One can map F to matrix f of size W x d, where d &lt;&lt; C, using some function g, where f = g(F). fw represents word w as a vector with d dimensions. The choice of g is another design decision, although perhaps not as important as the statistics used to initially construct F. The self-organizing semantic map (Ritter &amp; Kohonen, 1989) is a distributional technique that maps words to two dimensions, such that syntactically and semantically related words are nearby (Honkela et al., 1995; Honkela, 1997). LSA (Dumais et al., 1988; Landauer et al., 1998), LSI, and LDA (Blei et al., 2003) induce distributional representations over F in which each column is a document context. In most of the other approaches discussed, the columns represent word contexts. In LSA, g computes the SVD of F. Hyperspace Analogue to Language (HAL) is another early distributional approach (Lund et al., 1995; Lund &amp; Burgess, 1996) to inducing word representations. They compute F over a corpus of 160 million word tokens with a vocabulary size W of 70K word types. There are 2·W types of context (columns): The first or second W are counted if the word c occurs within a window of 10 to the left or right of</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent dirichlet allocation. Journal of Machine Learning Research, 3, 993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V deSouza</author>
<author>R L Mercer</author>
<author>V J D Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<pages>467--479</pages>
<contexts>
<context position="9088" citStr="Brown et al., 1992" startWordPosition="1403" endWordPosition="1406">cesses on these tasks using clustering representations (Section 3) and distributed representations (Section 4), so we focus on these representations in our work. 3 Clustering-based word representations Another type of word representation is to induce a clustering over words. Clustering methods and 385 distributional methods can overlap. For example, Pereira et al. (1993) begin with a cooccurrence matrix and transform this matrix into a clustering. 3.1 Brown clustering The Brown algorithm is a hierarchical clustering algorithm which clusters words to maximize the mutual information of bigrams (Brown et al., 1992). So it is a class-based bigram language model. It runs in time O(V·K2), where V is the size of the vocabulary and K is the number of clusters. The hierarchical nature of the clustering means that we can choose the word class at several levels in the hierarchy, which can compensate for poor clusters of a small number of words. One downside of Brown clustering is that it is based solely on bigram statistics, and does not consider word usage in a wider context. Brown clusters have been used successfully in a variety of NLP applications: NER (Miller et al., 2004; Liang, 2005; Ratinov &amp; Roth, 2009</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra, V. J. D., &amp; Lai, J. C. (1992). Class-based n-gram models of natural language. Computational Linguistics, 18, 467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Candito</author>
<author>B Crabb´e</author>
</authors>
<title>Improving generative statistical parsing with semi-supervised word clustering. IWPT</title>
<date>2009</date>
<pages>138--141</pages>
<marker>Candito, Crabb´e, 2009</marker>
<rawString>Candito, M., &amp; Crabb´e, B. (2009). Improving generative statistical parsing with semi-supervised word clustering. IWPT (pp. 138–141).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified tions.</title>
<date>2008</date>
<booktitle>Proceedings of the International ICSC Symposium on Soft Computing.</booktitle>
<contexts>
<context position="752" citStr="Collobert and Weston (2008)" startWordPosition="91" endWordPosition="94">echerche Op´erationnelle (DIRO) Universit´e de Montr´eal Montr´eal, Qu´ebec, Canada, H3T 1J4 lastname@iro.umontreal.ca Lev Ratinov Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 ratinov2@uiuc.edu Yoshua Bengio D´epartement d’Informatique et Recherche Op´erationnelle (DIRO) Universit´e de Montr´eal Montr´eal, Qu´ebec, Canada, H3T 1J4 bengioy@iro.umontreal.ca Abstract If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih &amp; Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/ 1 Introduction By using unlabelled data to reduce data sparsity in the labeled training data, semi-supervised approaches improve genera</context>
<context position="4590" citStr="Collobert and Weston (2008)" startWordPosition="668" endWordPosition="671">ing unsupervised approaches. (See Bengio (2008) for a more complete list of references on neural language models.) Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. But different word representations have never been systematically compared in a controlled way. In this work, we compare different techniques for inducing word representations, evaluating them on the tasks of named entity recognition (NER) and chunking. We retract former negative results published in Turian et al. (2009) about Collobert and Weston (2008) embeddings, given training improvements that we describe in Section 7.1. 2 Distributional representations Distributional word representations are based upon a cooccurrence matrix F of size WxC, where W is the vocabulary size, each row Fw is the initial representation of word w, and each column Fc is some context. Sahlgren (2006) and Turney and Pantel (2010) describe a handful of possible design decisions in contructing F, including choice of context types (left window? right window? size of window?) and type of frequency count (raw? binary? tf-idf?). Fw has dimensionality W, which can be too </context>
<context position="12458" citStr="Collobert and Weston (2008)" startWordPosition="1922" endWordPosition="1925">s in the number of dimensions. Word embeddings are typically induced using neural language models, which use neural networks as the underlying predictive model (Bengio, 2008). Historically, training and testing of neural language models has been slow, scaling as the size of the vocabulary for each model computation (Bengio et al., 2001; Bengio et al., 2003). However, many approaches have been proposed in recent years to eliminate that linear dependency on vocabulary size (Morin &amp; Bengio, 2005; Collobert &amp; Weston, 2008; Mnih &amp; Hinton, 2009) and allow scaling to very large training corpora. 4.1 Collobert and Weston (2008) embeddings Collobert and Weston (2008) presented a neural language model that could be trained over billions of words, because the gradient of the loss was computed stochastically over a small sample of possible outputs, in a spirit similar to Bengio and S´en´ecal (2003). This neural model of Collobert and Weston (2008) was refined and presented in greater depth in Bengio et al. (2009). The model is discriminative and nonprobabilistic. For each training update, we read an n-gram x = (w1, ... , wn) from the corpus. The model concatenates the learned embeddings of the n words, giving e(w1) ® ..</context>
<context position="13903" citStr="Collobert and Weston (2008)" startWordPosition="2172" endWordPosition="2175">ert and Weston (2008), the middle word in the 386 we write e(x) to mean e(w1) ® ... ® e(wn). We predict a score s(x) for x by passing e(x) through a single hidden layer neural network. The training criterion is that n-grams that are present in the training corpus like x must have a score at least some margin higher than corrupted n-grams like ˜x. Specifically: L(x) = max(0, 1− s(x) + s(˜x)). We minimize this loss stochastically over the n-grams in the corpus, doing gradient descent simultaneously over the neural network parameters and the embedding lookup table. We implemented the approach of Collobert and Weston (2008), with the following differences: • We did not achieve as low log-ranks on the English Wikipedia as the authors reported in Bengio et al. (2009), despite initially attempting to have identical experimental conditions. • We corrupt the last word of each n-gram. • We had a separate learning rate for the embeddings and for the neural network weights. We found that the embeddings should have a learning rate generally 1000–32000 times higher than the neural network weights. Otherwise, the unsupervised training criterion drops slowly. • Although their sampling technique makes training fast, testing </context>
<context position="22537" citStr="Collobert and Weston (2008)" startWordPosition="3558" endWordPosition="3561">rd (label as ‘O’) all NEs with type NUMBER/MONEY/DATE. 2. In the predicted model output on MUC7 data, discard (label as ‘O’) all NEs with type MISC. 388 These postprocessing steps will adversely affect all NER models across-the-board, nonetheless allowing us to compare different models in a controlled manner. 6 Unlabled Data Unlabeled data is used for inducing the word representations. We used the RCV1 corpus, which contains one year of Reuters English newswire, from August 1996 to August 1997, about 63 millions words in 3.3 million sentences. We left case intact in the corpus. By comparison, Collobert and Weston (2008) downcases words and delexicalizes numbers. We use a preprocessing technique proposed by Liang, (2005, p. 51), which was later used by Koo et al. (2008): Remove all sentences that are less than 90% lowercase a–z. We assume that whitespace is not counted, although this is not specified in Liang’s thesis. We call this preprocessing step cleaning. In Turian et al. (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters. This is the case even though the cleaning process was very ag</context>
<context position="24889" citStr="Collobert and Weston (2008)" startWordPosition="3932" endWordPosition="3935">nd Results 7.1 Details of inducing word representations The Brown clusters took roughly 3 days to induce, when we induced 1000 clusters, the baseline in prior work (Koo et al., 2008; Ratinov &amp; Roth, 2009). We also induced 100, 320, and 3200 Brown clusters, for comparison. (Because Brown clustering scales quadratically in the number of clusters, inducing 10000 clusters would have been prohibitive.) Because Brown clusters are hierarchical, we can use cluster supersets as features. We used clusters at path depth 4, 6, 10, and 20 (Ratinov &amp; Roth, 2009). These are the prefixes used in Table 1. The Collobert and Weston (2008) (C&amp;W) embeddings were induced over the course of a few weeks, and trained for about 50 epochs. One of the difficulties in inducing these embeddings is that there is no stopping criterion defined, and that the quality of the embeddings can keep improving as training continues. Collobert (p.c.) simply leaves one computer training his embeddings indefinitely. We induced embeddings with 25, 50, 100, or 200 dimensions over 5-gram windows. In comparison to Turian et al. (2009), we use improved C&amp;W embeddings in this work: • They were trained for 50 epochs, not just 20 epochs. • We initialized all e</context>
<context position="26491" citStr="Collobert and Weston (2008)" startWordPosition="4199" endWordPosition="4202"> Unlike our Collobert and Weston (2008) embeddings, we did not extensively tune the learning rates for HLBL. We used a learning rate of 1e-3 for both model parameters and embedding parameters. We induced embeddings with 100 dimensions over 5-gram windows, and embeddings with 50 dimensions over 5-gram windows. Embeddings were induced over one pass 2A rare word will appear 5 (window size) times per epoch as a positive example, and 37M (training examples per epoch) / 269K (vocabulary size) = 138 times per epoch as a corruption example. 3The HLBL model updates require fewer matrix multiplies than Collobert and Weston (2008) model updates. Additionally, HLBL models were trained on a GPGPU, which is faster than conventional CPU arithmetic. 389 approach using a random tree, not two passes with an updated tree and embeddings re-estimation. 7.2 Scaling of Word Embeddings Like many NLP systems, the baseline system contains only binary features. The word embeddings, however, are real numbers that are not necessarily in a bounded range. If the range of the word embeddings is too large, they will exert more influence than the binary features. We generally found that embeddings had zero mean. We can scale the embeddings b</context>
<context position="28209" citStr="Collobert and Weston (2008)" startWordPosition="4496" endWordPosition="4499"> Word Representations # of embedding dimensions # of Brown clusters # of embedding dimensions 25 50 100 200 100 320 1000 3200 (a) Validation F1 94.7 94.6 94.5 94.4 94.3 94.2 94.1 C&amp;W HLBL Brown baseline 25 50 100 200 C&amp;W, 50-dim HLBL, 50-dim C&amp;W, 200-dim C&amp;W, 100-dim HLBL, 100-dim C&amp;W, 25-dim baseline 92.5 91.5 90.5 92 91 90 C&amp;W Brown HLBL baseline 100 320 1000 3200 # of Brown clusters Validation F1 (b) (a) Validation F1 94.8 94.6 94.4 94.2 94 93.8 93.6 0.001 0.01 0.1 1 Scaling factor σ Figure 1: Effect as we vary the scaling factor a- (Equation 1) on the validation set F1. We experiment with Collobert and Weston (2008) and HLBL embeddings of various dimensionality. (a) Chunking results. (b) NER results. Figure 1 shows the effect of scaling factor con both supervised tasks. We were surprised to find that on both tasks, across Collobert and Weston (2008) and HLBL embeddings of various dimensionality, that all curves had similar shapes and optima. This is one contributions of our Figure 2: Effect as we vary the capacity of the word representations on the validation set F1. (a) Chunking results. (b) NER results. There are capacity controls for the word representations: number of Brown clusters, and number of di</context>
<context position="3692" citStr="Collobert &amp; Weston, 2008" startWordPosition="542" endWordPosition="545"> researchers to investigate unsupervised methods for inducing word representations over large unlabeled corpora. Word features can be hand-designed, but our goal is to learn them. One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical. This technique was used by a variety of researchers (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Ratinov &amp; Roth, 2009; Huang &amp; Yates, 2009). This leads to a one-hot representation over a smaller vocabulary size. Neural language models (Bengio et al., 2001; Schwenk &amp; Gauvain, 2002; Mnih &amp; Hinton, 2007; Collobert &amp; Weston, 2008), on the other hand, induce dense real-valued low-dimensional 384 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics word embeddings using unsupervised approaches. (See Bengio (2008) for a more complete list of references on neural language models.) Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. But different word representations have never been systema</context>
<context position="12354" citStr="Collobert &amp; Weston, 2008" startWordPosition="1905" endWordPosition="1908">ibuted representation is compact, in the sense that it can represent an exponential number of clusters in the number of dimensions. Word embeddings are typically induced using neural language models, which use neural networks as the underlying predictive model (Bengio, 2008). Historically, training and testing of neural language models has been slow, scaling as the size of the vocabulary for each model computation (Bengio et al., 2001; Bengio et al., 2003). However, many approaches have been proposed in recent years to eliminate that linear dependency on vocabulary size (Morin &amp; Bengio, 2005; Collobert &amp; Weston, 2008; Mnih &amp; Hinton, 2009) and allow scaling to very large training corpora. 4.1 Collobert and Weston (2008) embeddings Collobert and Weston (2008) presented a neural language model that could be trained over billions of words, because the gradient of the loss was computed stochastically over a small sample of possible outputs, in a spirit similar to Bengio and S´en´ecal (2003). This neural model of Collobert and Weston (2008) was refined and presented in greater depth in Bengio et al. (2009). The model is discriminative and nonprobabilistic. For each training update, we read an n-gram x = (w1, ..</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Collobert, R., &amp; Weston, J. (2008). A unified tions. Proceedings of the International ICSC Symposium on Soft Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Honkela</author>
<author>V Pulkki</author>
<author>T Kohonen</author>
</authors>
<title>Contextual relations of words in grimm tales, analyzed by self-organizing map.</title>
<date>1995</date>
<publisher>ICANN.</publisher>
<contexts>
<context position="5735" citStr="Honkela et al., 1995" startWordPosition="864" endWordPosition="867">cy count (raw? binary? tf-idf?). Fw has dimensionality W, which can be too large to use Fw as features for word w in a supervised model. One can map F to matrix f of size W x d, where d &lt;&lt; C, using some function g, where f = g(F). fw represents word w as a vector with d dimensions. The choice of g is another design decision, although perhaps not as important as the statistics used to initially construct F. The self-organizing semantic map (Ritter &amp; Kohonen, 1989) is a distributional technique that maps words to two dimensions, such that syntactically and semantically related words are nearby (Honkela et al., 1995; Honkela, 1997). LSA (Dumais et al., 1988; Landauer et al., 1998), LSI, and LDA (Blei et al., 2003) induce distributional representations over F in which each column is a document context. In most of the other approaches discussed, the columns represent word contexts. In LSA, g computes the SVD of F. Hyperspace Analogue to Language (HAL) is another early distributional approach (Lund et al., 1995; Lund &amp; Burgess, 1996) to inducing word representations. They compute F over a corpus of 160 million word tokens with a vocabulary size W of 70K word types. There are 2·W types of context (columns): </context>
</contexts>
<marker>Honkela, Pulkki, Kohonen, 1995</marker>
<rawString>Honkela, T., Pulkki, V., &amp; Kohonen, T. (1995). Contextual relations of words in grimm tales, analyzed by self-organizing map. ICANN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Huang</author>
<author>A Yates</author>
</authors>
<title>Distributional representations for handling sparsity in supervised sequence labeling.</title>
<date>2009</date>
<publisher>ACL.</publisher>
<contexts>
<context position="10552" citStr="Huang and Yates (2009)" startWordPosition="1638" endWordPosition="1641">pon word bigram and trigram statistics. Ushioda (1996) presents an extension to the Brown clustering algorithm, and learn hierarchical clusterings of words as well as phrases, which they apply to POS tagging. 3.2 Other work on cluster-based word representations Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. HMMs can be used to induce a soft clustering, specifically a multinomial distribution over possible clusters (hidden states). Li and McCallum (2005) use an HMM-LDA model to improve POS tagging and Chinese Word Segmentation. Huang and Yates (2009) induce a fully-connected HMM, which emits a multinomial distribution over possible vocabulary words. They perform hard clustering using the Viterbi algorithm. (Alternately, they could keep the soft clustering, with the representation for a particular word token being the posterior probability distribution over the states.) However, the CRF chunker in Huang and Yates (2009), which uses their HMM word clusters as extra features, achieves F1 lower than a baseline CRF chunker (Sha &amp; Pereira, 2003). Goldberg et al. (2009) use an HMM to assign POS tags to words, which in turns improves the accuracy</context>
<context position="3503" citStr="Huang &amp; Yates, 2009" startWordPosition="512" endWordPosition="515">poorly estimated. Moreover, at test time, the model cannot handle words that do not appear in the labeled training data. These limitations of one-hot word representations have prompted researchers to investigate unsupervised methods for inducing word representations over large unlabeled corpora. Word features can be hand-designed, but our goal is to learn them. One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical. This technique was used by a variety of researchers (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Ratinov &amp; Roth, 2009; Huang &amp; Yates, 2009). This leads to a one-hot representation over a smaller vocabulary size. Neural language models (Bengio et al., 2001; Schwenk &amp; Gauvain, 2002; Mnih &amp; Hinton, 2007; Collobert &amp; Weston, 2008), on the other hand, induce dense real-valued low-dimensional 384 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics word embeddings using unsupervised approaches. (See Bengio (2008) for a more complete list of references on neural language models.) Unsupervised word represe</context>
</contexts>
<marker>Huang, Yates, 2009</marker>
<rawString>Huang, F., &amp; Yates, A. (2009). Distributional representations for handling sparsity in supervised sequence labeling. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kaski</author>
</authors>
<title>Dimensionality reduction by random mapping: Fast similarity computation for clustering. IJCNN</title>
<date>1998</date>
<pages>413--418</pages>
<contexts>
<context position="7686" citStr="Kaski (1998)" startWordPosition="1202" endWordPosition="1203">and Sojka (2010) describe an incremental approach to inducing LSA and LDA topic models over 270 millions word tokens with a vocabulary of 315K word types. This is similar in magnitude to our experiments. Another incremental approach to constructing f is using a random projection: Linear mapping g is multiplying F by a random matrix chosen a priori. This random indexing method is motivated by the Johnson-Lindenstrauss lemma, which states that for certain choices of random matrix, if d is sufficiently large, then the original distances between words in F will be preserved in f (Sahlgren, 2005). Kaski (1998) uses this technique to produce 100-dimensional representations of documents. Sahlgren (2001) was the first author to use random indexing using narrow context. Sahlgren (2006) does a battery of experiments exploring different design decisions involved in constructing F, prior to using random indexing. However, like all the works cited above, Sahlgren (2006) only uses distributional representation to improve existing systems for one-shot classification tasks, such as IR, WSD, semantic knowledge tests, and text categorization. It is not well-understood what settings are appropriate to induce dis</context>
</contexts>
<marker>Kaski, 1998</marker>
<rawString>Kaski, S. (1998). Dimensionality reduction by random mapping: Fast similarity computation for clustering. IJCNN (pp. 413–418).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<journal>ACL</journal>
<pages>595--603</pages>
<contexts>
<context position="3459" citStr="Koo et al., 2008" startWordPosition="504" endWordPosition="507"> corresponding model parameters will be poorly estimated. Moreover, at test time, the model cannot handle words that do not appear in the labeled training data. These limitations of one-hot word representations have prompted researchers to investigate unsupervised methods for inducing word representations over large unlabeled corpora. Word features can be hand-designed, but our goal is to learn them. One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical. This technique was used by a variety of researchers (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Ratinov &amp; Roth, 2009; Huang &amp; Yates, 2009). This leads to a one-hot representation over a smaller vocabulary size. Neural language models (Bengio et al., 2001; Schwenk &amp; Gauvain, 2002; Mnih &amp; Hinton, 2007; Collobert &amp; Weston, 2008), on the other hand, induce dense real-valued low-dimensional 384 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics word embeddings using unsupervised approaches. (See Bengio (2008) for a more complete list of references on neural</context>
<context position="9767" citStr="Koo et al., 2008" startWordPosition="1522" endWordPosition="1525"> O(V·K2), where V is the size of the vocabulary and K is the number of clusters. The hierarchical nature of the clustering means that we can choose the word class at several levels in the hierarchy, which can compensate for poor clusters of a small number of words. One downside of Brown clustering is that it is based solely on bigram statistics, and does not consider word usage in a wider context. Brown clusters have been used successfully in a variety of NLP applications: NER (Miller et al., 2004; Liang, 2005; Ratinov &amp; Roth, 2009), PCFG parsing (Candito &amp; Crabb´e, 2009), dependency parsing (Koo et al., 2008; Suzuki et al., 2009), and semantic dependency parsing (Zhao et al., 2009). Martin et al. (1998) presents algorithms for inducing hierarchical clusterings based upon word bigram and trigram statistics. Ushioda (1996) presents an extension to the Brown clustering algorithm, and learn hierarchical clusterings of words as well as phrases, which they apply to POS tagging. 3.2 Other work on cluster-based word representations Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. HMMs can be used to induce a soft clustering, specifically a </context>
<context position="17404" citStr="Koo et al. (2008)" startWordPosition="2713" endWordPosition="2716">tou (http://leon. bottou.org/projects/sgd) • CRFsuite by by Naoaki Okazaki (http:// www.chokkan.org/software/crfsuite/) We use CRFsuite because it makes it simple to modify the feature generation code, so one can easily add new features. We use SGD optimization, and enable negative state features and negative transition features. (“feature.possible transitions=1, feature.possible states=1”) Table 1 shows the features in the baseline chunker. As you can see, the Brown and embedding features are unigram features, and do not participate in conjunctions like the word features and tag features do. Koo et al. (2008) sees further accuracy improvements on dependency parsing when using word representations in compound features. The data comes from the Penn Treebank, and is newswire from the Wall Street Journal in 1989. Of the 8936 training sentences, we used 1000 randomly sampled sentences (23615 words) for development. We trained models on the 7936 387 • Word features: wi for i in 1−2, −1, 0, +1, +21, wi n wi+1 for i in 1−1, 01. • Tag features: wi for i in 1−2,−1,0,+1,+21, ti n ti+1 for i in 1−2, −1, 0, +11. ti n ti+1 n ti+2 for i in 1−2, −1, 01. • Embedding features [if applicable]: ei[d] for i in 1−2, −1</context>
<context position="22689" citStr="Koo et al. (2008)" startWordPosition="3583" endWordPosition="3586">tprocessing steps will adversely affect all NER models across-the-board, nonetheless allowing us to compare different models in a controlled manner. 6 Unlabled Data Unlabeled data is used for inducing the word representations. We used the RCV1 corpus, which contains one year of Reuters English newswire, from August 1996 to August 1997, about 63 millions words in 3.3 million sentences. We left case intact in the corpus. By comparison, Collobert and Weston (2008) downcases words and delexicalizes numbers. We use a preprocessing technique proposed by Liang, (2005, p. 51), which was later used by Koo et al. (2008): Remove all sentences that are less than 90% lowercase a–z. We assume that whitespace is not counted, although this is not specified in Liang’s thesis. We call this preprocessing step cleaning. In Turian et al. (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters. This is the case even though the cleaning process was very aggressive, and discarded more than half of the sentences. According to the evidence and arguments presented in Bengio et al. (2009), the non-convex optim</context>
<context position="24443" citStr="Koo et al., 2008" startWordPosition="3860" endWordPosition="3863"> million sentences (41% of the original). The cleaned RCV1 corpus has 269K word types. This is the vocabulary size, i.e. how many word representations were induced. Note that cleaning is applied only to the unlabeled data, not to the labeled data used in the supervised tasks. RCV1 is a superset of the CoNLL03 corpus. For this reason, NER results that use RCV1 word representations are a form of transductive learning. 7 Experiments and Results 7.1 Details of inducing word representations The Brown clusters took roughly 3 days to induce, when we induced 1000 clusters, the baseline in prior work (Koo et al., 2008; Ratinov &amp; Roth, 2009). We also induced 100, 320, and 3200 Brown clusters, for comparison. (Because Brown clustering scales quadratically in the number of clusters, inducing 10000 clusters would have been prohibitive.) Because Brown clusters are hierarchical, we can use cluster supersets as features. We used clusters at path depth 4, 6, 10, and 20 (Ratinov &amp; Roth, 2009). These are the prefixes used in Table 1. The Collobert and Weston (2008) (C&amp;W) embeddings were induced over the course of a few weeks, and trained for about 50 epochs. One of the difficulties in inducing these embeddings is th</context>
<context position="33952" citStr="Koo et al., 2008" startWordPosition="5449" endWordPosition="5452">ngs incur almost identical numbers of errors, and errors are concentrated around the more common 250 200 150 100 50 0 C&amp;W, 50-dim Brown, 3200 clusters (b) # of per-token errors (test set) 250 200 150 100 50 0 C&amp;W, 50-dim Brown, 1000 clusters # of per-token errors (test set) (a) 391 words. We hypothesize that non-rare words have good representations, regardless of the choice of word representation technique. For tasks like chunking in which a syntactic decision relies upon looking at several token simultaneously, compound features that use the word representations might increase accuracy more (Koo et al., 2008). Using word representations in NER brought larger gains on the out-of-domain data than on the in-domain data. We were surprised by this result, because the OOD data was not even used during the unsupervised word representation induction, as was the in-domain data. We are curious to investigate this phenomenon further. Ando and Zhang (2005) present a semisupervised learning algorithm called alternating structure optimization (ASO). They find a lowdimensional projection of the input features that gives good linear classifiers over auxiliary tasks. These auxiliary tasks are sometimes specific to</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Koo, T., Carreras, X., &amp; Collins, M. (2008). Simple semi-supervised dependency parsing. ACL (pp. 595–603).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Krishnan</author>
<author>C D Manning</author>
</authors>
<title>An effective two-stage model for exploiting nonlocal dependencies in named entity recognition.</title>
<date>2006</date>
<publisher>COLING-ACL.</publisher>
<contexts>
<context position="19390" citStr="Krishnan &amp; Manning, 2006" startWordPosition="3045" endWordPosition="3048">equence prediction problem. Following Ratinov and Roth (2009), we use the regularized averaged perceptron model. Ratinov and Roth (2009) describe different sequence encoding like BILOU and BIO, and show that the BILOU encoding outperforms BIO, and the greedy inference performs competitively to Viterbi while being significantly faster. Accordingly, we use greedy inference and BILOU text chunk representation. We use the publicly available implementation from Ratinov and Roth (2009) (see the end of this paper for the URL). In our baseline experiments, we remove gazetteers and non-local features (Krishnan &amp; Manning, 2006). However, we also run experiments that include these features, to understand if the information they provide mostly overlaps with that of the word representations. After each epoch over the training set, we measured the accuracy of the model on the development set. Training was stopped after the accuracy on the development set did not improve for 10 epochs, generally about 50–80 epochs total. The epoch that performed best on the development set was chosen as the final model. We use the following baseline set of features from Zhang and Johnson (2003): • Previous two predictions yi−1 and yi−2 •</context>
</contexts>
<marker>Krishnan, Manning, 2006</marker>
<rawString>Krishnan, V., &amp; Manning, C. D. (2006). An effective two-stage model for exploiting nonlocal dependencies in named entity recognition. COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>An introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>259--284</pages>
<contexts>
<context position="5801" citStr="Landauer et al., 1998" startWordPosition="875" endWordPosition="878">can be too large to use Fw as features for word w in a supervised model. One can map F to matrix f of size W x d, where d &lt;&lt; C, using some function g, where f = g(F). fw represents word w as a vector with d dimensions. The choice of g is another design decision, although perhaps not as important as the statistics used to initially construct F. The self-organizing semantic map (Ritter &amp; Kohonen, 1989) is a distributional technique that maps words to two dimensions, such that syntactically and semantically related words are nearby (Honkela et al., 1995; Honkela, 1997). LSA (Dumais et al., 1988; Landauer et al., 1998), LSI, and LDA (Blei et al., 2003) induce distributional representations over F in which each column is a document context. In most of the other approaches discussed, the columns represent word contexts. In LSA, g computes the SVD of F. Hyperspace Analogue to Language (HAL) is another early distributional approach (Lund et al., 1995; Lund &amp; Burgess, 1996) to inducing word representations. They compute F over a corpus of 160 million word tokens with a vocabulary size W of 70K word types. There are 2·W types of context (columns): The first or second W are counted if the word c occurs within a wi</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Landauer, T. K., Foltz, P. W., &amp; Laham, D. (1998). An introduction to latent semantic analysis. Discourse Processes, 259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Li</author>
<author>A McCallum</author>
</authors>
<title>Semi-supervised sequence modeling with syntactic topic models.</title>
<date>2005</date>
<publisher>AAAI.</publisher>
<contexts>
<context position="10454" citStr="Li and McCallum (2005)" startWordPosition="1622" endWordPosition="1625">al., 2009). Martin et al. (1998) presents algorithms for inducing hierarchical clusterings based upon word bigram and trigram statistics. Ushioda (1996) presents an extension to the Brown clustering algorithm, and learn hierarchical clusterings of words as well as phrases, which they apply to POS tagging. 3.2 Other work on cluster-based word representations Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. HMMs can be used to induce a soft clustering, specifically a multinomial distribution over possible clusters (hidden states). Li and McCallum (2005) use an HMM-LDA model to improve POS tagging and Chinese Word Segmentation. Huang and Yates (2009) induce a fully-connected HMM, which emits a multinomial distribution over possible vocabulary words. They perform hard clustering using the Viterbi algorithm. (Alternately, they could keep the soft clustering, with the representation for a particular word token being the posterior probability distribution over the states.) However, the CRF chunker in Huang and Yates (2009), which uses their HMM word clusters as extra features, achieves F1 lower than a baseline CRF chunker (Sha &amp; Pereira, 2003). G</context>
</contexts>
<marker>Li, McCallum, 2005</marker>
<rawString>Li, W., &amp; McCallum, A. (2005). Semi-supervised sequence modeling with syntactic topic models. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master’s thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="3441" citStr="Liang, 2005" startWordPosition="502" endWordPosition="503">g data, their corresponding model parameters will be poorly estimated. Moreover, at test time, the model cannot handle words that do not appear in the labeled training data. These limitations of one-hot word representations have prompted researchers to investigate unsupervised methods for inducing word representations over large unlabeled corpora. Word features can be hand-designed, but our goal is to learn them. One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical. This technique was used by a variety of researchers (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Ratinov &amp; Roth, 2009; Huang &amp; Yates, 2009). This leads to a one-hot representation over a smaller vocabulary size. Neural language models (Bengio et al., 2001; Schwenk &amp; Gauvain, 2002; Mnih &amp; Hinton, 2007; Collobert &amp; Weston, 2008), on the other hand, induce dense real-valued low-dimensional 384 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics word embeddings using unsupervised approaches. (See Bengio (2008) for a more complete list of re</context>
<context position="9666" citStr="Liang, 2005" startWordPosition="1508" endWordPosition="1509">n of bigrams (Brown et al., 1992). So it is a class-based bigram language model. It runs in time O(V·K2), where V is the size of the vocabulary and K is the number of clusters. The hierarchical nature of the clustering means that we can choose the word class at several levels in the hierarchy, which can compensate for poor clusters of a small number of words. One downside of Brown clustering is that it is based solely on bigram statistics, and does not consider word usage in a wider context. Brown clusters have been used successfully in a variety of NLP applications: NER (Miller et al., 2004; Liang, 2005; Ratinov &amp; Roth, 2009), PCFG parsing (Candito &amp; Crabb´e, 2009), dependency parsing (Koo et al., 2008; Suzuki et al., 2009), and semantic dependency parsing (Zhao et al., 2009). Martin et al. (1998) presents algorithms for inducing hierarchical clusterings based upon word bigram and trigram statistics. Ushioda (1996) presents an extension to the Brown clustering algorithm, and learn hierarchical clusterings of words as well as phrases, which they apply to POS tagging. 3.2 Other work on cluster-based word representations Lin and Wu (2009) present a K-means-like non-hierarchical clustering algor</context>
<context position="22638" citStr="Liang, (2005" startWordPosition="3574" endWordPosition="3575"> as ‘O’) all NEs with type MISC. 388 These postprocessing steps will adversely affect all NER models across-the-board, nonetheless allowing us to compare different models in a controlled manner. 6 Unlabled Data Unlabeled data is used for inducing the word representations. We used the RCV1 corpus, which contains one year of Reuters English newswire, from August 1996 to August 1997, about 63 millions words in 3.3 million sentences. We left case intact in the corpus. By comparison, Collobert and Weston (2008) downcases words and delexicalizes numbers. We use a preprocessing technique proposed by Liang, (2005, p. 51), which was later used by Koo et al. (2008): Remove all sentences that are less than 90% lowercase a–z. We assume that whitespace is not counted, although this is not specified in Liang’s thesis. We call this preprocessing step cleaning. In Turian et al. (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters. This is the case even though the cleaning process was very aggressive, and discarded more than half of the sentences. According to the evidence and arguments pres</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Liang, P. (2005). Semi-supervised learning for natural language. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>X Wu</author>
</authors>
<title>Phrase clustering for discriminative learning.</title>
<date>2009</date>
<journal>ACL-IJCNLP</journal>
<pages>1030--1038</pages>
<contexts>
<context position="10209" citStr="Lin and Wu (2009)" startWordPosition="1588" endWordPosition="1591">ly in a variety of NLP applications: NER (Miller et al., 2004; Liang, 2005; Ratinov &amp; Roth, 2009), PCFG parsing (Candito &amp; Crabb´e, 2009), dependency parsing (Koo et al., 2008; Suzuki et al., 2009), and semantic dependency parsing (Zhao et al., 2009). Martin et al. (1998) presents algorithms for inducing hierarchical clusterings based upon word bigram and trigram statistics. Ushioda (1996) presents an extension to the Brown clustering algorithm, and learn hierarchical clusterings of words as well as phrases, which they apply to POS tagging. 3.2 Other work on cluster-based word representations Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. HMMs can be used to induce a soft clustering, specifically a multinomial distribution over possible clusters (hidden states). Li and McCallum (2005) use an HMM-LDA model to improve POS tagging and Chinese Word Segmentation. Huang and Yates (2009) induce a fully-connected HMM, which emits a multinomial distribution over possible vocabulary words. They perform hard clustering using the Viterbi algorithm. (Alternately, they could keep the soft clustering, with the representation for a particular word </context>
<context position="16039" citStr="Lin and Wu (2009)" startWordPosition="2506" endWordPosition="2509">. The model, combined with this optimization, is called the hierarchical log-bilinear (HLBL) model. n-gram is corrupted. In Bengio et al. (2009), the last word in the n-gram is corrupted. 5 Supervised evaluation tasks We evaluate the hypothesis that one can take an existing, near state-of-the-art, supervised NLP system, and improve its accuracy by including word representations as word features. This technique for turning a supervised approach into a semi-supervised one is general and task-agnostic. However, we wish to find out if certain word representations are preferable for certain tasks. Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. We apply clustering and distributed representations to NER and chunking, which allows us to compare our semi-supervised models to those of Ando and Zhang (2005) and Suzuki and Isozaki (2008). 5.1 Chunking Chunking is a syntactic sequence labeling task. We follow the conditions in the CoNLL-2000 shared task (Sang &amp; Buchholz, 2000). The linear CRF chunker of Sha and Pereira (2003) is a standard near-state-of-the-art baseline chunker. In fact, many off-the-shelf CRF implementations now </context>
<context position="30640" citStr="Lin and Wu (2009)" startWordPosition="4896" endWordPosition="4899">), 15M - 94.39 Suzuki and Isozaki (2008), 15M - 94.67 Suzuki and Isozaki (2008), 1B - 95.15 Table 2: Final chunking F1 results. In the last section, we show how many unlabeled words were used. System Dev Test MUC7 Baseline 90.03 84.39 67.48 Baseline+Nonlocal 91.91 86.52 71.80 HLBL 100-dim 92.00 88.13 75.25 Gazetteers 92.09 87.36 77.76 C&amp;W 50-dim 92.27 87.93 75.74 Brown, 1000 clusters 92.32 88.52 78.84 C&amp;W 200-dim 92.46 87.96 75.51 C&amp;W+HLBL 92.52 88.56 78.64 Brown+HLBL 92.56 88.93 77.85 Brown+C&amp;W 92.79 89.31 80.13 HLBL+Gaz 92.91 89.35 79.29 C&amp;W+Gaz 92.98 88.88 81.44 Brown+Gaz 93.25 89.41 82.71 Lin and Wu (2009), 3.4B - 88.44 - Ando and Zhang (2005), 27M 93.15 89.31 - Suzuki and Isozaki (2008), 37M 93.66 89.36 - Suzuki and Isozaki (2008), 1B 94.48 89.92 - All (Brown+C&amp;W+HLBL+Gaz), 37M 93.17 90.04 82.50 All+Nonlocal, 37M 93.95 90.36 84.15 Lin and Wu (2009), 700B - 90.90 - Table 3: Final NER F1 results, showing the cumulative effect of adding word representations, non-local features, and gazetteers to the baseline. To speed up training, in combined experiments (C&amp;W plus another word representation), we used the 50-dimensional C&amp;W embeddings, not the 200-dimensional ones. In the last section, we show ho</context>
<context position="35183" citStr="Lin and Wu (2009)" startWordPosition="5643" endWordPosition="5646">d task, and sometimes general language modeling tasks like “predict the missing word”. Suzuki and Isozaki (2008) present a semisupervised extension of CRFs. (In Suzuki et al. (2009), they extend their semi-supervised approach to more general conditional models.) One of the advantages of the semi-supervised learning approach that we use is that it is simpler and more general than that of Ando and Zhang (2005) and Suzuki and Isozaki (2008). Their methods dictate a particular choice of model and training regime and could not, for instance, be used with an NLP system based upon an SVM classifier. Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. Since they can scale to millions of phrases, and they train over 800B unlabeled words, they achieve state-of-the-art accuracy on NER using their phrase clusters. This suggests that extending word representations to phrase representations is worth further investigation. 8 Conclusions Word features can be learned in advance in an unsupervised, task-inspecific, and model-agnostic manner. These word features, once learned, are easily disseminated with other researchers, and easily integrated into exist</context>
</contexts>
<marker>Lin, Wu, 2009</marker>
<rawString>Lin, D., &amp; Wu, X. (2009). Phrase clustering for discriminative learning. ACL-IJCNLP (pp. 1030–1038).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
</authors>
<title>Producing highdimensional semantic spaces from lexical co-occurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instrumentation, and Computers,</journal>
<volume>28</volume>
<pages>203--208</pages>
<contexts>
<context position="6158" citStr="Lund &amp; Burgess, 1996" startWordPosition="933" endWordPosition="936">anizing semantic map (Ritter &amp; Kohonen, 1989) is a distributional technique that maps words to two dimensions, such that syntactically and semantically related words are nearby (Honkela et al., 1995; Honkela, 1997). LSA (Dumais et al., 1988; Landauer et al., 1998), LSI, and LDA (Blei et al., 2003) induce distributional representations over F in which each column is a document context. In most of the other approaches discussed, the columns represent word contexts. In LSA, g computes the SVD of F. Hyperspace Analogue to Language (HAL) is another early distributional approach (Lund et al., 1995; Lund &amp; Burgess, 1996) to inducing word representations. They compute F over a corpus of 160 million word tokens with a vocabulary size W of 70K word types. There are 2·W types of context (columns): The first or second W are counted if the word c occurs within a window of 10 to the left or right of the word w, respectively. f is chosen by taking the 200 columns (out of 140K in F) with the highest variances. ICA is another technique to transform F into f. (V¨ayrynen &amp; Honkela, 2004; V¨ayrynen &amp; Honkela, 2005; V¨ayrynen et al., 2007). ICA is expensive, and the largest vocabulary size used in these works was only 10K.</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Lund, K., &amp; Burgess, C. (1996). Producing highdimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instrumentation, and Computers, 28, 203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
<author>R A Atchley</author>
</authors>
<title>Semantic and associative priming in highdimensional semantic space.</title>
<date>1995</date>
<booktitle>Cognitive Science Proceedings, LEA</booktitle>
<pages>660--665</pages>
<contexts>
<context position="6135" citStr="Lund et al., 1995" startWordPosition="929" endWordPosition="932">uct F. The self-organizing semantic map (Ritter &amp; Kohonen, 1989) is a distributional technique that maps words to two dimensions, such that syntactically and semantically related words are nearby (Honkela et al., 1995; Honkela, 1997). LSA (Dumais et al., 1988; Landauer et al., 1998), LSI, and LDA (Blei et al., 2003) induce distributional representations over F in which each column is a document context. In most of the other approaches discussed, the columns represent word contexts. In LSA, g computes the SVD of F. Hyperspace Analogue to Language (HAL) is another early distributional approach (Lund et al., 1995; Lund &amp; Burgess, 1996) to inducing word representations. They compute F over a corpus of 160 million word tokens with a vocabulary size W of 70K word types. There are 2·W types of context (columns): The first or second W are counted if the word c occurs within a window of 10 to the left or right of the word w, respectively. f is chosen by taking the 200 columns (out of 140K in F) with the highest variances. ICA is another technique to transform F into f. (V¨ayrynen &amp; Honkela, 2004; V¨ayrynen &amp; Honkela, 2005; V¨ayrynen et al., 2007). ICA is expensive, and the largest vocabulary size used in th</context>
</contexts>
<marker>Lund, Burgess, Atchley, 1995</marker>
<rawString>Lund, K., Burgess, C., &amp; Atchley, R. A. (1995). Semantic and associative priming in highdimensional semantic space. Cognitive Science Proceedings, LEA (pp. 660–665).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Martin</author>
<author>J Liermann</author>
<author>H Ney</author>
</authors>
<title>Algorithms for bigram and trigram word clustering.</title>
<date>1998</date>
<journal>Speech Communication,</journal>
<volume>24</volume>
<contexts>
<context position="9864" citStr="Martin et al. (1998)" startWordPosition="1538" endWordPosition="1541">ical nature of the clustering means that we can choose the word class at several levels in the hierarchy, which can compensate for poor clusters of a small number of words. One downside of Brown clustering is that it is based solely on bigram statistics, and does not consider word usage in a wider context. Brown clusters have been used successfully in a variety of NLP applications: NER (Miller et al., 2004; Liang, 2005; Ratinov &amp; Roth, 2009), PCFG parsing (Candito &amp; Crabb´e, 2009), dependency parsing (Koo et al., 2008; Suzuki et al., 2009), and semantic dependency parsing (Zhao et al., 2009). Martin et al. (1998) presents algorithms for inducing hierarchical clusterings based upon word bigram and trigram statistics. Ushioda (1996) presents an extension to the Brown clustering algorithm, and learn hierarchical clusterings of words as well as phrases, which they apply to POS tagging. 3.2 Other work on cluster-based word representations Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. HMMs can be used to induce a soft clustering, specifically a multinomial distribution over possible clusters (hidden states). Li and McCallum (2005) use an HM</context>
</contexts>
<marker>Martin, Liermann, Ney, 1998</marker>
<rawString>Martin, S., Liermann, J., &amp; Ney, H. (1998). Algorithms for bigram and trigram word clustering. Speech Communication, 24, 19–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>J Guinness</author>
<author>A Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training. HLT-NAACL</title>
<date>2004</date>
<pages>337--342</pages>
<contexts>
<context position="3428" citStr="Miller et al., 2004" startWordPosition="498" endWordPosition="501">n the labeled training data, their corresponding model parameters will be poorly estimated. Moreover, at test time, the model cannot handle words that do not appear in the labeled training data. These limitations of one-hot word representations have prompted researchers to investigate unsupervised methods for inducing word representations over large unlabeled corpora. Word features can be hand-designed, but our goal is to learn them. One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical. This technique was used by a variety of researchers (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Ratinov &amp; Roth, 2009; Huang &amp; Yates, 2009). This leads to a one-hot representation over a smaller vocabulary size. Neural language models (Bengio et al., 2001; Schwenk &amp; Gauvain, 2002; Mnih &amp; Hinton, 2007; Collobert &amp; Weston, 2008), on the other hand, induce dense real-valued low-dimensional 384 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics word embeddings using unsupervised approaches. (See Bengio (2008) for a more comple</context>
<context position="9653" citStr="Miller et al., 2004" startWordPosition="1504" endWordPosition="1507">the mutual information of bigrams (Brown et al., 1992). So it is a class-based bigram language model. It runs in time O(V·K2), where V is the size of the vocabulary and K is the number of clusters. The hierarchical nature of the clustering means that we can choose the word class at several levels in the hierarchy, which can compensate for poor clusters of a small number of words. One downside of Brown clustering is that it is based solely on bigram statistics, and does not consider word usage in a wider context. Brown clusters have been used successfully in a variety of NLP applications: NER (Miller et al., 2004; Liang, 2005; Ratinov &amp; Roth, 2009), PCFG parsing (Candito &amp; Crabb´e, 2009), dependency parsing (Koo et al., 2008; Suzuki et al., 2009), and semantic dependency parsing (Zhao et al., 2009). Martin et al. (1998) presents algorithms for inducing hierarchical clusterings based upon word bigram and trigram statistics. Ushioda (1996) presents an extension to the Brown clustering algorithm, and learn hierarchical clusterings of words as well as phrases, which they apply to POS tagging. 3.2 Other work on cluster-based word representations Lin and Wu (2009) present a K-means-like non-hierarchical clu</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Miller, S., Guinness, J., &amp; Zamanian, A. (2004). Name tagging with word clusters and discriminative training. HLT-NAACL (pp. 337–342).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G E Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<publisher>ICML.</publisher>
<contexts>
<context position="3665" citStr="Mnih &amp; Hinton, 2007" startWordPosition="538" endWordPosition="541">tations have prompted researchers to investigate unsupervised methods for inducing word representations over large unlabeled corpora. Word features can be hand-designed, but our goal is to learn them. One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical. This technique was used by a variety of researchers (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Ratinov &amp; Roth, 2009; Huang &amp; Yates, 2009). This leads to a one-hot representation over a smaller vocabulary size. Neural language models (Bengio et al., 2001; Schwenk &amp; Gauvain, 2002; Mnih &amp; Hinton, 2007; Collobert &amp; Weston, 2008), on the other hand, induce dense real-valued low-dimensional 384 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics word embeddings using unsupervised approaches. (See Bengio (2008) for a more complete list of references on neural language models.) Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. But different word representati</context>
<context position="14815" citStr="Mnih &amp; Hinton, 2007" startWordPosition="2318" endWordPosition="2321">te for the embeddings and for the neural network weights. We found that the embeddings should have a learning rate generally 1000–32000 times higher than the neural network weights. Otherwise, the unsupervised training criterion drops slowly. • Although their sampling technique makes training fast, testing is still expensive when the size of the vocabulary is large. Instead of cross-validating using the log-rank over the validation data as they do, we instead used the moving average of the training loss on training examples before the weight update. 4.2 HLBL embeddings The log-bilinear model (Mnih &amp; Hinton, 2007) is a probabilistic and linear neural model. Given an n-gram, the model concatenates the embeddings of the n − 1 first words, and learns a linear model to predict the embedding of the last word. The similarity between the predicted embedding and the current actual embedding is transformed into a probability by exponentiating and then normalizing. Mnih and Hinton (2009) speed up model evaluation during training and testing by using a hierarchy to exponentially filter down the number of computations that are performed. This hierarchical evaluation technique was first proposed by Morin and Bengio</context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Mnih, A., &amp; Hinton, G. E. (2007). Three new graphical models for statistical language modelling. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>G E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2009</date>
<journal>NIPS</journal>
<pages>1081--1088</pages>
<contexts>
<context position="15186" citStr="Mnih and Hinton (2009)" startWordPosition="2378" endWordPosition="2381">Instead of cross-validating using the log-rank over the validation data as they do, we instead used the moving average of the training loss on training examples before the weight update. 4.2 HLBL embeddings The log-bilinear model (Mnih &amp; Hinton, 2007) is a probabilistic and linear neural model. Given an n-gram, the model concatenates the embeddings of the n − 1 first words, and learns a linear model to predict the embedding of the last word. The similarity between the predicted embedding and the current actual embedding is transformed into a probability by exponentiating and then normalizing. Mnih and Hinton (2009) speed up model evaluation during training and testing by using a hierarchy to exponentially filter down the number of computations that are performed. This hierarchical evaluation technique was first proposed by Morin and Bengio (2005). The model, combined with this optimization, is called the hierarchical log-bilinear (HLBL) model. n-gram is corrupted. In Bengio et al. (2009), the last word in the n-gram is corrupted. 5 Supervised evaluation tasks We evaluate the hypothesis that one can take an existing, near state-of-the-art, supervised NLP system, and improve its accuracy by including word</context>
<context position="795" citStr="Mnih &amp; Hinton, 2009" startWordPosition="98" endWordPosition="101">tr´eal Montr´eal, Qu´ebec, Canada, H3T 1J4 lastname@iro.umontreal.ca Lev Ratinov Department of Computer Science University of Illinois at Urbana-Champaign Urbana, IL 61801 ratinov2@uiuc.edu Yoshua Bengio D´epartement d’Informatique et Recherche Op´erationnelle (DIRO) Universit´e de Montr´eal Montr´eal, Qu´ebec, Canada, H3T 1J4 bengioy@iro.umontreal.ca Abstract If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih &amp; Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/ 1 Introduction By using unlabelled data to reduce data sparsity in the labeled training data, semi-supervised approaches improve generalization accuracy. Semi-supervised models s</context>
<context position="12376" citStr="Mnih &amp; Hinton, 2009" startWordPosition="1909" endWordPosition="1912">ompact, in the sense that it can represent an exponential number of clusters in the number of dimensions. Word embeddings are typically induced using neural language models, which use neural networks as the underlying predictive model (Bengio, 2008). Historically, training and testing of neural language models has been slow, scaling as the size of the vocabulary for each model computation (Bengio et al., 2001; Bengio et al., 2003). However, many approaches have been proposed in recent years to eliminate that linear dependency on vocabulary size (Morin &amp; Bengio, 2005; Collobert &amp; Weston, 2008; Mnih &amp; Hinton, 2009) and allow scaling to very large training corpora. 4.1 Collobert and Weston (2008) embeddings Collobert and Weston (2008) presented a neural language model that could be trained over billions of words, because the gradient of the loss was computed stochastically over a small sample of possible outputs, in a spirit similar to Bengio and S´en´ecal (2003). This neural model of Collobert and Weston (2008) was refined and presented in greater depth in Bengio et al. (2009). The model is discriminative and nonprobabilistic. For each training update, we read an n-gram x = (w1, ... , wn) from the corpu</context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Mnih, A., &amp; Hinton, G. E. (2009). A scalable hierarchical distributed language model. NIPS (pp. 1081–1088).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Morin</author>
<author>Y Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<publisher>AISTATS.</publisher>
<contexts>
<context position="15422" citStr="Morin and Bengio (2005)" startWordPosition="2413" endWordPosition="2416"> &amp; Hinton, 2007) is a probabilistic and linear neural model. Given an n-gram, the model concatenates the embeddings of the n − 1 first words, and learns a linear model to predict the embedding of the last word. The similarity between the predicted embedding and the current actual embedding is transformed into a probability by exponentiating and then normalizing. Mnih and Hinton (2009) speed up model evaluation during training and testing by using a hierarchy to exponentially filter down the number of computations that are performed. This hierarchical evaluation technique was first proposed by Morin and Bengio (2005). The model, combined with this optimization, is called the hierarchical log-bilinear (HLBL) model. n-gram is corrupted. In Bengio et al. (2009), the last word in the n-gram is corrupted. 5 Supervised evaluation tasks We evaluate the hypothesis that one can take an existing, near state-of-the-art, supervised NLP system, and improve its accuracy by including word representations as word features. This technique for turning a supervised approach into a semi-supervised one is general and task-agnostic. However, we wish to find out if certain word representations are preferable for certain tasks. </context>
<context position="12328" citStr="Morin &amp; Bengio, 2005" startWordPosition="1901" endWordPosition="1904">ic properties. A distributed representation is compact, in the sense that it can represent an exponential number of clusters in the number of dimensions. Word embeddings are typically induced using neural language models, which use neural networks as the underlying predictive model (Bengio, 2008). Historically, training and testing of neural language models has been slow, scaling as the size of the vocabulary for each model computation (Bengio et al., 2001; Bengio et al., 2003). However, many approaches have been proposed in recent years to eliminate that linear dependency on vocabulary size (Morin &amp; Bengio, 2005; Collobert &amp; Weston, 2008; Mnih &amp; Hinton, 2009) and allow scaling to very large training corpora. 4.1 Collobert and Weston (2008) embeddings Collobert and Weston (2008) presented a neural language model that could be trained over billions of words, because the gradient of the loss was computed stochastically over a small sample of possible outputs, in a spirit similar to Bengio and S´en´ecal (2003). This neural model of Collobert and Weston (2008) was refined and presented in greater depth in Bengio et al. (2009). The model is discriminative and nonprobabilistic. For each training update, we </context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>Morin, F., &amp; Bengio, Y. (2005). Hierarchical probabilistic neural network language model. AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of english words.</title>
<date>1993</date>
<journal>ACL</journal>
<pages>183--190</pages>
<contexts>
<context position="8842" citStr="Pereira et al. (1993)" startWordPosition="1366" endWordPosition="1369">s not well-understood what settings are appropriate to induce distributional word representations for structured prediction tasks (like parsing and MT) and sequence labeling tasks (like chunking and NER). Previous research has achieved repeated successes on these tasks using clustering representations (Section 3) and distributed representations (Section 4), so we focus on these representations in our work. 3 Clustering-based word representations Another type of word representation is to induce a clustering over words. Clustering methods and 385 distributional methods can overlap. For example, Pereira et al. (1993) begin with a cooccurrence matrix and transform this matrix into a clustering. 3.1 Brown clustering The Brown algorithm is a hierarchical clustering algorithm which clusters words to maximize the mutual information of bigrams (Brown et al., 1992). So it is a class-based bigram language model. It runs in time O(V·K2), where V is the size of the vocabulary and K is the number of clusters. The hierarchical nature of the clustering means that we can choose the word class at several levels in the hierarchy, which can compensate for poor clusters of a small number of words. One downside of Brown clu</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Pereira, F., Tishby, N., &amp; Lee, L. (1993). Distributional clustering of english words. ACL (pp. 183–190).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<journal>CoNLL.</journal>
<contexts>
<context position="18826" citStr="Ratinov and Roth (2009)" startWordPosition="2961" endWordPosition="2964">own cluster bi. Table 1: Features templates used in the CRF chunker. training partition sentences, and evaluated their F1 on the development set. After choosing hyperparameters to maximize the dev F1, we would retrain the model using these hyperparameters on the full 8936 sentence training set, and evaluate on test. One hyperparameter was l2-regularization sigma, which for most models was optimal at 2 or 3.2. The word embeddings also required a scaling hyperparameter, as described in Section 7.2. 5.2 Named entity recognition NER is typically treated as a sequence prediction problem. Following Ratinov and Roth (2009), we use the regularized averaged perceptron model. Ratinov and Roth (2009) describe different sequence encoding like BILOU and BIO, and show that the BILOU encoding outperforms BIO, and the greedy inference performs competitively to Viterbi while being significantly faster. Accordingly, we use greedy inference and BILOU text chunk representation. We use the publicly available implementation from Ratinov and Roth (2009) (see the end of this paper for the URL). In our baseline experiments, we remove gazetteers and non-local features (Krishnan &amp; Manning, 2006). However, we also run experiments t</context>
<context position="3481" citStr="Ratinov &amp; Roth, 2009" startWordPosition="508" endWordPosition="511">el parameters will be poorly estimated. Moreover, at test time, the model cannot handle words that do not appear in the labeled training data. These limitations of one-hot word representations have prompted researchers to investigate unsupervised methods for inducing word representations over large unlabeled corpora. Word features can be hand-designed, but our goal is to learn them. One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical. This technique was used by a variety of researchers (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Ratinov &amp; Roth, 2009; Huang &amp; Yates, 2009). This leads to a one-hot representation over a smaller vocabulary size. Neural language models (Bengio et al., 2001; Schwenk &amp; Gauvain, 2002; Mnih &amp; Hinton, 2007; Collobert &amp; Weston, 2008), on the other hand, induce dense real-valued low-dimensional 384 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics word embeddings using unsupervised approaches. (See Bengio (2008) for a more complete list of references on neural language models.) Uns</context>
<context position="9689" citStr="Ratinov &amp; Roth, 2009" startWordPosition="1510" endWordPosition="1513">(Brown et al., 1992). So it is a class-based bigram language model. It runs in time O(V·K2), where V is the size of the vocabulary and K is the number of clusters. The hierarchical nature of the clustering means that we can choose the word class at several levels in the hierarchy, which can compensate for poor clusters of a small number of words. One downside of Brown clustering is that it is based solely on bigram statistics, and does not consider word usage in a wider context. Brown clusters have been used successfully in a variety of NLP applications: NER (Miller et al., 2004; Liang, 2005; Ratinov &amp; Roth, 2009), PCFG parsing (Candito &amp; Crabb´e, 2009), dependency parsing (Koo et al., 2008; Suzuki et al., 2009), and semantic dependency parsing (Zhao et al., 2009). Martin et al. (1998) presents algorithms for inducing hierarchical clusterings based upon word bigram and trigram statistics. Ushioda (1996) presents an extension to the Brown clustering algorithm, and learn hierarchical clusterings of words as well as phrases, which they apply to POS tagging. 3.2 Other work on cluster-based word representations Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which</context>
<context position="24466" citStr="Ratinov &amp; Roth, 2009" startWordPosition="3864" endWordPosition="3867"> (41% of the original). The cleaned RCV1 corpus has 269K word types. This is the vocabulary size, i.e. how many word representations were induced. Note that cleaning is applied only to the unlabeled data, not to the labeled data used in the supervised tasks. RCV1 is a superset of the CoNLL03 corpus. For this reason, NER results that use RCV1 word representations are a form of transductive learning. 7 Experiments and Results 7.1 Details of inducing word representations The Brown clusters took roughly 3 days to induce, when we induced 1000 clusters, the baseline in prior work (Koo et al., 2008; Ratinov &amp; Roth, 2009). We also induced 100, 320, and 3200 Brown clusters, for comparison. (Because Brown clustering scales quadratically in the number of clusters, inducing 10000 clusters would have been prohibitive.) Because Brown clusters are hierarchical, we can use cluster supersets as features. We used clusters at path depth 4, 6, 10, and 20 (Ratinov &amp; Roth, 2009). These are the prefixes used in Table 1. The Collobert and Weston (2008) (C&amp;W) embeddings were induced over the course of a few weeks, and trained for about 50 epochs. One of the difficulties in inducing these embeddings is that there is no stopping</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Ratinov, L., &amp; Roth, D. (2009). Design challenges and misconceptions in named entity recognition. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ritter</author>
<author>T Kohonen</author>
</authors>
<title>Self-organizing semantic maps.</title>
<date>1989</date>
<journal>Biological Cybernetics,</journal>
<pages>241--254</pages>
<contexts>
<context position="5582" citStr="Ritter &amp; Kohonen, 1989" startWordPosition="841" endWordPosition="844"> a handful of possible design decisions in contructing F, including choice of context types (left window? right window? size of window?) and type of frequency count (raw? binary? tf-idf?). Fw has dimensionality W, which can be too large to use Fw as features for word w in a supervised model. One can map F to matrix f of size W x d, where d &lt;&lt; C, using some function g, where f = g(F). fw represents word w as a vector with d dimensions. The choice of g is another design decision, although perhaps not as important as the statistics used to initially construct F. The self-organizing semantic map (Ritter &amp; Kohonen, 1989) is a distributional technique that maps words to two dimensions, such that syntactically and semantically related words are nearby (Honkela et al., 1995; Honkela, 1997). LSA (Dumais et al., 1988; Landauer et al., 1998), LSI, and LDA (Blei et al., 2003) induce distributional representations over F in which each column is a document context. In most of the other approaches discussed, the columns represent word contexts. In LSA, g computes the SVD of F. Hyperspace Analogue to Language (HAL) is another early distributional approach (Lund et al., 1995; Lund &amp; Burgess, 1996) to inducing word repres</context>
</contexts>
<marker>Ritter, Kohonen, 1989</marker>
<rawString>Ritter, H., &amp; Kohonen, T. (1989). Self-organizing semantic maps. Biological Cybernetics, 241–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
</authors>
<title>Vector-based semantic analysis: Representing word meanings based on random labels.</title>
<date>2001</date>
<booktitle>Proceedings of the Semantic Knowledge Acquisition and Categorisation Workshop, ESSLLI.</booktitle>
<contexts>
<context position="7779" citStr="Sahlgren (2001)" startWordPosition="1215" endWordPosition="1216">r 270 millions word tokens with a vocabulary of 315K word types. This is similar in magnitude to our experiments. Another incremental approach to constructing f is using a random projection: Linear mapping g is multiplying F by a random matrix chosen a priori. This random indexing method is motivated by the Johnson-Lindenstrauss lemma, which states that for certain choices of random matrix, if d is sufficiently large, then the original distances between words in F will be preserved in f (Sahlgren, 2005). Kaski (1998) uses this technique to produce 100-dimensional representations of documents. Sahlgren (2001) was the first author to use random indexing using narrow context. Sahlgren (2006) does a battery of experiments exploring different design decisions involved in constructing F, prior to using random indexing. However, like all the works cited above, Sahlgren (2006) only uses distributional representation to improve existing systems for one-shot classification tasks, such as IR, WSD, semantic knowledge tests, and text categorization. It is not well-understood what settings are appropriate to induce distributional word representations for structured prediction tasks (like parsing and MT) and se</context>
</contexts>
<marker>Sahlgren, 2001</marker>
<rawString>Sahlgren, M. (2001). Vector-based semantic analysis: Representing word meanings based on random labels. Proceedings of the Semantic Knowledge Acquisition and Categorisation Workshop, ESSLLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
</authors>
<title>An introduction to random indexing.</title>
<date>2005</date>
<booktitle>Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering (TKE).</booktitle>
<contexts>
<context position="7672" citStr="Sahlgren, 2005" startWordPosition="1200" endWordPosition="1201">ally. ˇReh˚uˇrek and Sojka (2010) describe an incremental approach to inducing LSA and LDA topic models over 270 millions word tokens with a vocabulary of 315K word types. This is similar in magnitude to our experiments. Another incremental approach to constructing f is using a random projection: Linear mapping g is multiplying F by a random matrix chosen a priori. This random indexing method is motivated by the Johnson-Lindenstrauss lemma, which states that for certain choices of random matrix, if d is sufficiently large, then the original distances between words in F will be preserved in f (Sahlgren, 2005). Kaski (1998) uses this technique to produce 100-dimensional representations of documents. Sahlgren (2001) was the first author to use random indexing using narrow context. Sahlgren (2006) does a battery of experiments exploring different design decisions involved in constructing F, prior to using random indexing. However, like all the works cited above, Sahlgren (2006) only uses distributional representation to improve existing systems for one-shot classification tasks, such as IR, WSD, semantic knowledge tests, and text categorization. It is not well-understood what settings are appropriate</context>
</contexts>
<marker>Sahlgren, 2005</marker>
<rawString>Sahlgren, M. (2005). An introduction to random indexing. Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering (TKE).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
</authors>
<title>The word-space model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces. Doctoral dissertation,</title>
<date>2006</date>
<institution>Stockholm University.</institution>
<contexts>
<context position="4921" citStr="Sahlgren (2006)" startWordPosition="722" endWordPosition="723">ed in a controlled way. In this work, we compare different techniques for inducing word representations, evaluating them on the tasks of named entity recognition (NER) and chunking. We retract former negative results published in Turian et al. (2009) about Collobert and Weston (2008) embeddings, given training improvements that we describe in Section 7.1. 2 Distributional representations Distributional word representations are based upon a cooccurrence matrix F of size WxC, where W is the vocabulary size, each row Fw is the initial representation of word w, and each column Fc is some context. Sahlgren (2006) and Turney and Pantel (2010) describe a handful of possible design decisions in contructing F, including choice of context types (left window? right window? size of window?) and type of frequency count (raw? binary? tf-idf?). Fw has dimensionality W, which can be too large to use Fw as features for word w in a supervised model. One can map F to matrix f of size W x d, where d &lt;&lt; C, using some function g, where f = g(F). fw represents word w as a vector with d dimensions. The choice of g is another design decision, although perhaps not as important as the statistics used to initially construct</context>
<context position="7861" citStr="Sahlgren (2006)" startWordPosition="1228" endWordPosition="1229">n magnitude to our experiments. Another incremental approach to constructing f is using a random projection: Linear mapping g is multiplying F by a random matrix chosen a priori. This random indexing method is motivated by the Johnson-Lindenstrauss lemma, which states that for certain choices of random matrix, if d is sufficiently large, then the original distances between words in F will be preserved in f (Sahlgren, 2005). Kaski (1998) uses this technique to produce 100-dimensional representations of documents. Sahlgren (2001) was the first author to use random indexing using narrow context. Sahlgren (2006) does a battery of experiments exploring different design decisions involved in constructing F, prior to using random indexing. However, like all the works cited above, Sahlgren (2006) only uses distributional representation to improve existing systems for one-shot classification tasks, such as IR, WSD, semantic knowledge tests, and text categorization. It is not well-understood what settings are appropriate to induce distributional word representations for structured prediction tasks (like parsing and MT) and sequence labeling tasks (like chunking and NER). Previous research has achieved repe</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Sahlgren, M. (2006). The word-space model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces. Doctoral dissertation, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task:</title>
<date>2000</date>
<journal>Chunking. CoNLL.</journal>
<contexts>
<context position="16482" citStr="Sang &amp; Buchholz, 2000" startWordPosition="2576" endWordPosition="2579"> approach into a semi-supervised one is general and task-agnostic. However, we wish to find out if certain word representations are preferable for certain tasks. Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. We apply clustering and distributed representations to NER and chunking, which allows us to compare our semi-supervised models to those of Ando and Zhang (2005) and Suzuki and Isozaki (2008). 5.1 Chunking Chunking is a syntactic sequence labeling task. We follow the conditions in the CoNLL-2000 shared task (Sang &amp; Buchholz, 2000). The linear CRF chunker of Sha and Pereira (2003) is a standard near-state-of-the-art baseline chunker. In fact, many off-the-shelf CRF implementations now replicate Sha and Pereira (2003), including their choice of feature set: • CRF++ by Taku Kudo (http://crfpp. sourceforge.net/) • crfsgd by L´eon Bottou (http://leon. bottou.org/projects/sgd) • CRFsuite by by Naoaki Okazaki (http:// www.chokkan.org/software/crfsuite/) We use CRFsuite because it makes it simple to modify the feature generation code, so one can easily add new features. We use SGD optimization, and enable negative state featur</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Sang, E. T., &amp; Buchholz, S. (2000). Introduction to the CoNLL-2000 shared task: Chunking. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
<author>J-L Gauvain</author>
</authors>
<title>Connectionist language modeling for large vocabulary continuous speech recognition.</title>
<date>2002</date>
<booktitle>International Conference on Acoustics, Speech and Signal Processing (ICASSP)</booktitle>
<pages>765--768</pages>
<location>Orlando, Florida.</location>
<contexts>
<context position="3644" citStr="Schwenk &amp; Gauvain, 2002" startWordPosition="534" endWordPosition="537"> of one-hot word representations have prompted researchers to investigate unsupervised methods for inducing word representations over large unlabeled corpora. Word features can be hand-designed, but our goal is to learn them. One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical. This technique was used by a variety of researchers (Miller et al., 2004; Liang, 2005; Koo et al., 2008; Ratinov &amp; Roth, 2009; Huang &amp; Yates, 2009). This leads to a one-hot representation over a smaller vocabulary size. Neural language models (Bengio et al., 2001; Schwenk &amp; Gauvain, 2002; Mnih &amp; Hinton, 2007; Collobert &amp; Weston, 2008), on the other hand, induce dense real-valued low-dimensional 384 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics word embeddings using unsupervised approaches. (See Bengio (2008) for a more complete list of references on neural language models.) Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. But differ</context>
</contexts>
<marker>Schwenk, Gauvain, 2002</marker>
<rawString>Schwenk, H., &amp; Gauvain, J.-L. (2002). Connectionist language modeling for large vocabulary continuous speech recognition. International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 765–768). Orlando, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F C N Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<publisher>HLT-NAACL.</publisher>
<contexts>
<context position="16532" citStr="Sha and Pereira (2003)" startWordPosition="2585" endWordPosition="2588">d task-agnostic. However, we wish to find out if certain word representations are preferable for certain tasks. Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. We apply clustering and distributed representations to NER and chunking, which allows us to compare our semi-supervised models to those of Ando and Zhang (2005) and Suzuki and Isozaki (2008). 5.1 Chunking Chunking is a syntactic sequence labeling task. We follow the conditions in the CoNLL-2000 shared task (Sang &amp; Buchholz, 2000). The linear CRF chunker of Sha and Pereira (2003) is a standard near-state-of-the-art baseline chunker. In fact, many off-the-shelf CRF implementations now replicate Sha and Pereira (2003), including their choice of feature set: • CRF++ by Taku Kudo (http://crfpp. sourceforge.net/) • crfsgd by L´eon Bottou (http://leon. bottou.org/projects/sgd) • CRFsuite by by Naoaki Okazaki (http:// www.chokkan.org/software/crfsuite/) We use CRFsuite because it makes it simple to modify the feature generation code, so one can easily add new features. We use SGD optimization, and enable negative state features and negative transition features. (“feature.pos</context>
<context position="11051" citStr="Sha &amp; Pereira, 2003" startWordPosition="1712" endWordPosition="1715"> Li and McCallum (2005) use an HMM-LDA model to improve POS tagging and Chinese Word Segmentation. Huang and Yates (2009) induce a fully-connected HMM, which emits a multinomial distribution over possible vocabulary words. They perform hard clustering using the Viterbi algorithm. (Alternately, they could keep the soft clustering, with the representation for a particular word token being the posterior probability distribution over the states.) However, the CRF chunker in Huang and Yates (2009), which uses their HMM word clusters as extra features, achieves F1 lower than a baseline CRF chunker (Sha &amp; Pereira, 2003). Goldberg et al. (2009) use an HMM to assign POS tags to words, which in turns improves the accuracy of the PCFG-based Hebrew parser. Deschacht and Moens (2009) use a latent-variable language model to improve semantic role labeling. 4 Distributed representations Another approach to word representation is to learn a distributed representation. (Not to be confused with distributional representations.) A distributed representation is dense, lowdimensional, and real-valued. Distributed word representations are called word embeddings. Each dimension of the embedding represents a latent feature of </context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Sha, F., &amp; Pereira, F. C. N. (2003). Shallow parsing with conditional random fields. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Spitkovsky</author>
<author>H Alshawi</author>
<author>D Jurafsky</author>
</authors>
<title>From baby steps to leapfrog: How “less is more” in unsupervised dependency parsing.</title>
<date>2010</date>
<publisher>NAACL-HLT.</publisher>
<contexts>
<context position="23711" citStr="Spitkovsky et al., 2010" startWordPosition="3739" endWordPosition="3742"> case even though the cleaning process was very aggressive, and discarded more than half of the sentences. According to the evidence and arguments presented in Bengio et al. (2009), the non-convex optimization process for Collobert and Weston (2008) embeddings might be adversely affected by noise and the statistical sparsity issues regarding rare words, especially at the beginning of training. For this reason, we hypothesize that learning representations over the most frequent words first and gradually increasing the vocabulary—a curriculum training strategy (Elman, 1993; Bengio et al., 2009; Spitkovsky et al., 2010)—would provide better results than cleaning. After cleaning, there are 37 million words (58% of the original) in 1.3 million sentences (41% of the original). The cleaned RCV1 corpus has 269K word types. This is the vocabulary size, i.e. how many word representations were induced. Note that cleaning is applied only to the unlabeled data, not to the labeled data used in the supervised tasks. RCV1 is a superset of the CoNLL03 corpus. For this reason, NER results that use RCV1 word representations are a form of transductive learning. 7 Experiments and Results 7.1 Details of inducing word represent</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2010</marker>
<rawString>Spitkovsky, V., Alshawi, H., &amp; Jurafsky, D. (2010). From baby steps to leapfrog: How “less is more” in unsupervised dependency parsing. NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>H Isozaki</author>
</authors>
<title>Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. ACL-08: HLT</title>
<date>2008</date>
<pages>665--673</pages>
<contexts>
<context position="1450" citStr="Suzuki and Isozaki (2008)" startWordPosition="190" endWordPosition="193">ER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/ 1 Introduction By using unlabelled data to reduce data sparsity in the labeled training data, semi-supervised approaches improve generalization accuracy. Semi-supervised models such as Ando and Zhang (2005), Suzuki and Isozaki (2008), and Suzuki et al. (2009) achieve state-of-the-art accuracy. However, these approaches dictate a particular choice of model and training regime. It can be tricky and time-consuming to adapt an existing supervised NLP system to use these semi-supervised techniques. It is preferable to use a simple and general method to adapt existing supervised NLP systems to be semi-supervised. One approach that is becoming popular is to use unsupervised methods to induce word features—or to download word features that have already been induced—plug these word features into an existing system, and observe a s</context>
<context position="16341" citStr="Suzuki and Isozaki (2008)" startWordPosition="2554" endWordPosition="2557">art, supervised NLP system, and improve its accuracy by including word representations as word features. This technique for turning a supervised approach into a semi-supervised one is general and task-agnostic. However, we wish to find out if certain word representations are preferable for certain tasks. Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. We apply clustering and distributed representations to NER and chunking, which allows us to compare our semi-supervised models to those of Ando and Zhang (2005) and Suzuki and Isozaki (2008). 5.1 Chunking Chunking is a syntactic sequence labeling task. We follow the conditions in the CoNLL-2000 shared task (Sang &amp; Buchholz, 2000). The linear CRF chunker of Sha and Pereira (2003) is a standard near-state-of-the-art baseline chunker. In fact, many off-the-shelf CRF implementations now replicate Sha and Pereira (2003), including their choice of feature set: • CRF++ by Taku Kudo (http://crfpp. sourceforge.net/) • crfsgd by L´eon Bottou (http://leon. bottou.org/projects/sgd) • CRFsuite by by Naoaki Okazaki (http:// www.chokkan.org/software/crfsuite/) We use CRFsuite because it makes i</context>
<context position="30063" citStr="Suzuki and Isozaki (2008)" startWordPosition="4803" endWordPosition="4806">onal embeddings had the highest validation F1 for both C&amp;W and HLBL. These curves indicates that the optimal capacity of the word embeddings is task-specific. 0.001 0.01 0.1 1 Scaling factor σ (b) Validation F1 92.5 91.5 90.5 89.5 92 91 90 89 C&amp;W, 200-dim C&amp;W, 100-dim C&amp;W, 25-dim C&amp;W, 50-dim HLBL, 100-dim HLBL, 50-dim baseline 390 System Dev Test Baseline 94.16 93.79 HLBL, 50-dim 94.63 94.00 C&amp;W, 50-dim 94.66 94.10 Brown, 3200 clusters 94.67 94.11 Brown+HLBL, 37M 94.62 94.13 C&amp;W+HLBL, 37M 94.68 94.25 Brown+C&amp;W+HLBL, 37M 94.72 94.15 Brown+C&amp;W, 37M 94.76 94.35 Ando and Zhang (2005), 15M - 94.39 Suzuki and Isozaki (2008), 15M - 94.67 Suzuki and Isozaki (2008), 1B - 95.15 Table 2: Final chunking F1 results. In the last section, we show how many unlabeled words were used. System Dev Test MUC7 Baseline 90.03 84.39 67.48 Baseline+Nonlocal 91.91 86.52 71.80 HLBL 100-dim 92.00 88.13 75.25 Gazetteers 92.09 87.36 77.76 C&amp;W 50-dim 92.27 87.93 75.74 Brown, 1000 clusters 92.32 88.52 78.84 C&amp;W 200-dim 92.46 87.96 75.51 C&amp;W+HLBL 92.52 88.56 78.64 Brown+HLBL 92.56 88.93 77.85 Brown+C&amp;W 92.79 89.31 80.13 HLBL+Gaz 92.91 89.35 79.29 C&amp;W+Gaz 92.98 88.88 81.44 Brown+Gaz 93.25 89.41 82.71 Lin and Wu (2009), 3.4B - 88.44 - Ando a</context>
<context position="31471" citStr="Suzuki and Isozaki (2008)" startWordPosition="5033" endWordPosition="5036">l, 37M 93.95 90.36 84.15 Lin and Wu (2009), 700B - 90.90 - Table 3: Final NER F1 results, showing the cumulative effect of adding word representations, non-local features, and gazetteers to the baseline. To speed up training, in combined experiments (C&amp;W plus another word representation), we used the 50-dimensional C&amp;W embeddings, not the 200-dimensional ones. In the last section, we show how many unlabeled words were used. 7.4 Final results Table 2 shows the final chunking results and Table 3 shows the final NER F1 results. We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and—for NER—Lin and Wu (2009). Tables 2 and 3 show that accuracy can be increased further by combining the features from different types of word representations. But, if only one word representation is to be used, Brown clusters have the highest accuracy. Given the improvements to the C&amp;W embeddings since Turian et al. (2009), C&amp;W embeddings outperform the HLBL embeddings. On chunking, there is only a minute difference between Brown clusters and the embeddings. Com0 1 10 100 1K 10K 100K 1M Frequency of word in unlabeled data 0 1 10 100 1K 10K 100K 1M Frequency of word in unlabeled data Figur</context>
<context position="34678" citStr="Suzuki and Isozaki (2008)" startWordPosition="5557" endWordPosition="5560">ain data. We were surprised by this result, because the OOD data was not even used during the unsupervised word representation induction, as was the in-domain data. We are curious to investigate this phenomenon further. Ando and Zhang (2005) present a semisupervised learning algorithm called alternating structure optimization (ASO). They find a lowdimensional projection of the input features that gives good linear classifiers over auxiliary tasks. These auxiliary tasks are sometimes specific to the supervised task, and sometimes general language modeling tasks like “predict the missing word”. Suzuki and Isozaki (2008) present a semisupervised extension of CRFs. (In Suzuki et al. (2009), they extend their semi-supervised approach to more general conditional models.) One of the advantages of the semi-supervised learning approach that we use is that it is simpler and more general than that of Ando and Zhang (2005) and Suzuki and Isozaki (2008). Their methods dictate a particular choice of model and training regime and could not, for instance, be used with an NLP system based upon an SVM classifier. Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce</context>
<context position="36049" citStr="Suzuki &amp; Isozaki, 2008" startWordPosition="5766" endWordPosition="5769">rase clusters. This suggests that extending word representations to phrase representations is worth further investigation. 8 Conclusions Word features can be learned in advance in an unsupervised, task-inspecific, and model-agnostic manner. These word features, once learned, are easily disseminated with other researchers, and easily integrated into existing supervised NLP systems. The disadvantage, however, is that accuracy might not be as high as a semi-supervised method that includes task-specific information and that jointly learns the supervised and unsupervised tasks (Ando &amp; Zhang, 2005; Suzuki &amp; Isozaki, 2008; Suzuki et al., 2009). Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. Ours is the first work to systematically compare different word representations in a controlled way. We found that Brown clusters and word embeddings both can improve the accuracy of a near-state-of-the-art supervised NLP system. We also found that combining different word representations can improve accuracy further. Error analysis indicates that Brown clustering induces better representations for rare words than C&amp;</context>
</contexts>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>Suzuki, J., &amp; Isozaki, H. (2008). Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. ACL-08: HLT (pp. 665–673).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>H Isozaki</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>An empirical study of semi-supervised structured conditional models for dependency parsing.</title>
<date>2009</date>
<publisher>EMNLP.</publisher>
<contexts>
<context position="1476" citStr="Suzuki et al. (2009)" startWordPosition="195" endWordPosition="198">ate-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/ 1 Introduction By using unlabelled data to reduce data sparsity in the labeled training data, semi-supervised approaches improve generalization accuracy. Semi-supervised models such as Ando and Zhang (2005), Suzuki and Isozaki (2008), and Suzuki et al. (2009) achieve state-of-the-art accuracy. However, these approaches dictate a particular choice of model and training regime. It can be tricky and time-consuming to adapt an existing supervised NLP system to use these semi-supervised techniques. It is preferable to use a simple and general method to adapt existing supervised NLP systems to be semi-supervised. One approach that is becoming popular is to use unsupervised methods to induce word features—or to download word features that have already been induced—plug these word features into an existing system, and observe a significant increase in acc</context>
<context position="9789" citStr="Suzuki et al., 2009" startWordPosition="1526" endWordPosition="1529">is the size of the vocabulary and K is the number of clusters. The hierarchical nature of the clustering means that we can choose the word class at several levels in the hierarchy, which can compensate for poor clusters of a small number of words. One downside of Brown clustering is that it is based solely on bigram statistics, and does not consider word usage in a wider context. Brown clusters have been used successfully in a variety of NLP applications: NER (Miller et al., 2004; Liang, 2005; Ratinov &amp; Roth, 2009), PCFG parsing (Candito &amp; Crabb´e, 2009), dependency parsing (Koo et al., 2008; Suzuki et al., 2009), and semantic dependency parsing (Zhao et al., 2009). Martin et al. (1998) presents algorithms for inducing hierarchical clusterings based upon word bigram and trigram statistics. Ushioda (1996) presents an extension to the Brown clustering algorithm, and learn hierarchical clusterings of words as well as phrases, which they apply to POS tagging. 3.2 Other work on cluster-based word representations Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. HMMs can be used to induce a soft clustering, specifically a multinomial distributi</context>
<context position="34747" citStr="Suzuki et al. (2009)" startWordPosition="5569" endWordPosition="5572">ven used during the unsupervised word representation induction, as was the in-domain data. We are curious to investigate this phenomenon further. Ando and Zhang (2005) present a semisupervised learning algorithm called alternating structure optimization (ASO). They find a lowdimensional projection of the input features that gives good linear classifiers over auxiliary tasks. These auxiliary tasks are sometimes specific to the supervised task, and sometimes general language modeling tasks like “predict the missing word”. Suzuki and Isozaki (2008) present a semisupervised extension of CRFs. (In Suzuki et al. (2009), they extend their semi-supervised approach to more general conditional models.) One of the advantages of the semi-supervised learning approach that we use is that it is simpler and more general than that of Ando and Zhang (2005) and Suzuki and Isozaki (2008). Their methods dictate a particular choice of model and training regime and could not, for instance, be used with an NLP system based upon an SVM classifier. Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. Since they can scale to millions of phrases, and they train over 80</context>
<context position="36071" citStr="Suzuki et al., 2009" startWordPosition="5770" endWordPosition="5773">ests that extending word representations to phrase representations is worth further investigation. 8 Conclusions Word features can be learned in advance in an unsupervised, task-inspecific, and model-agnostic manner. These word features, once learned, are easily disseminated with other researchers, and easily integrated into existing supervised NLP systems. The disadvantage, however, is that accuracy might not be as high as a semi-supervised method that includes task-specific information and that jointly learns the supervised and unsupervised tasks (Ando &amp; Zhang, 2005; Suzuki &amp; Isozaki, 2008; Suzuki et al., 2009). Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. Ours is the first work to systematically compare different word representations in a controlled way. We found that Brown clusters and word embeddings both can improve the accuracy of a near-state-of-the-art supervised NLP system. We also found that combining different word representations can improve accuracy further. Error analysis indicates that Brown clustering induces better representations for rare words than C&amp;W embeddings that have</context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>Suzuki, J., Isozaki, H., Carreras, X., &amp; Collins, M. (2009). An empirical study of semi-supervised structured conditional models for dependency parsing. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
<author>D Roth</author>
</authors>
<title>A preliminary evaluation of word representations for named-entity recognition.</title>
<date>2009</date>
<journal>NIPS Workshop on Grammar Induction, Representation of Language and Language Learning.</journal>
<contexts>
<context position="4556" citStr="Turian et al. (2009)" startWordPosition="663" endWordPosition="666">guistics word embeddings using unsupervised approaches. (See Bengio (2008) for a more complete list of references on neural language models.) Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. But different word representations have never been systematically compared in a controlled way. In this work, we compare different techniques for inducing word representations, evaluating them on the tasks of named entity recognition (NER) and chunking. We retract former negative results published in Turian et al. (2009) about Collobert and Weston (2008) embeddings, given training improvements that we describe in Section 7.1. 2 Distributional representations Distributional word representations are based upon a cooccurrence matrix F of size WxC, where W is the vocabulary size, each row Fw is the initial representation of word w, and each column Fc is some context. Sahlgren (2006) and Turney and Pantel (2010) describe a handful of possible design decisions in contructing F, including choice of context types (left window? right window? size of window?) and type of frequency count (raw? binary? tf-idf?). Fw has d</context>
<context position="22907" citStr="Turian et al. (2009)" startWordPosition="3619" endWordPosition="3622">sentations. We used the RCV1 corpus, which contains one year of Reuters English newswire, from August 1996 to August 1997, about 63 millions words in 3.3 million sentences. We left case intact in the corpus. By comparison, Collobert and Weston (2008) downcases words and delexicalizes numbers. We use a preprocessing technique proposed by Liang, (2005, p. 51), which was later used by Koo et al. (2008): Remove all sentences that are less than 90% lowercase a–z. We assume that whitespace is not counted, although this is not specified in Liang’s thesis. We call this preprocessing step cleaning. In Turian et al. (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters. This is the case even though the cleaning process was very aggressive, and discarded more than half of the sentences. According to the evidence and arguments presented in Bengio et al. (2009), the non-convex optimization process for Collobert and Weston (2008) embeddings might be adversely affected by noise and the statistical sparsity issues regarding rare words, especially at the beginning of training. For this reason, we hyp</context>
<context position="25365" citStr="Turian et al. (2009)" startWordPosition="4008" endWordPosition="4011"> We used clusters at path depth 4, 6, 10, and 20 (Ratinov &amp; Roth, 2009). These are the prefixes used in Table 1. The Collobert and Weston (2008) (C&amp;W) embeddings were induced over the course of a few weeks, and trained for about 50 epochs. One of the difficulties in inducing these embeddings is that there is no stopping criterion defined, and that the quality of the embeddings can keep improving as training continues. Collobert (p.c.) simply leaves one computer training his embeddings indefinitely. We induced embeddings with 25, 50, 100, or 200 dimensions over 5-gram windows. In comparison to Turian et al. (2009), we use improved C&amp;W embeddings in this work: • They were trained for 50 epochs, not just 20 epochs. • We initialized all embedding dimensions uniformly in the range [-0.01, +0.01], not [-1,+1]. For rare words, which are typically updated only 143 times per epoch2, and given that our embedding learning rate was typically 1e-6 or 1e-7, this means that rare word embeddings will be concentrated around zero, instead of spread out randomly. The HLBL embeddings were trained for 100 epochs (7 days).3 Unlike our Collobert and Weston (2008) embeddings, we did not extensively tune the learning rates fo</context>
<context position="27354" citStr="Turian et al. (2009)" startWordPosition="4342" endWordPosition="4345">ings Like many NLP systems, the baseline system contains only binary features. The word embeddings, however, are real numbers that are not necessarily in a bounded range. If the range of the word embeddings is too large, they will exert more influence than the binary features. We generally found that embeddings had zero mean. We can scale the embeddings by a hyperparameter, to control their standard deviation. Assume that the embeddings are represented by a matrix E: E (-- c- · E/stddev(E) (1) c- is a scaling constant that sets the new standard deviation after scaling the embeddings. work. In Turian et al. (2009), we were not able to prescribe a default value for scaling the embeddings. However, these curves demonstrate that a reasonable choice of scale factor is such that the embeddings have a standard deviation of 0.1. 7.3 Capacity of Word Representations # of embedding dimensions # of Brown clusters # of embedding dimensions 25 50 100 200 100 320 1000 3200 (a) Validation F1 94.7 94.6 94.5 94.4 94.3 94.2 94.1 C&amp;W HLBL Brown baseline 25 50 100 200 C&amp;W, 50-dim HLBL, 50-dim C&amp;W, 200-dim C&amp;W, 100-dim HLBL, 100-dim C&amp;W, 25-dim baseline 92.5 91.5 90.5 92 91 90 C&amp;W Brown HLBL baseline 100 320 1000 3200 # o</context>
<context position="29111" citStr="Turian et al. (2009)" startWordPosition="4646" endWordPosition="4649">ty, that all curves had similar shapes and optima. This is one contributions of our Figure 2: Effect as we vary the capacity of the word representations on the validation set F1. (a) Chunking results. (b) NER results. There are capacity controls for the word representations: number of Brown clusters, and number of dimensions of the word embeddings. Figure 2 shows the effect on the validation F1 as we vary the capacity of the word representations. In general, it appears that more Brown clusters are better. We would like to induce 10000 Brown clusters, however this would take several months. In Turian et al. (2009), we hypothesized on the basis of solely the HLBL NER curve that higher-dimensional word embeddings would give higher accuracy. Figure 2 shows that this hypothesis is not true. For NER, the C&amp;W curve is almost flat, and we were suprised to find the even 25-dimensional C&amp;W word embeddings work so well. For chunking, 50-dimensional embeddings had the highest validation F1 for both C&amp;W and HLBL. These curves indicates that the optimal capacity of the word embeddings is task-specific. 0.001 0.01 0.1 1 Scaling factor σ (b) Validation F1 92.5 91.5 90.5 89.5 92 91 90 89 C&amp;W, 200-dim C&amp;W, 100-dim C&amp;W,</context>
<context position="31800" citStr="Turian et al. (2009)" startWordPosition="5091" endWordPosition="5094">ot the 200-dimensional ones. In the last section, we show how many unlabeled words were used. 7.4 Final results Table 2 shows the final chunking results and Table 3 shows the final NER F1 results. We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and—for NER—Lin and Wu (2009). Tables 2 and 3 show that accuracy can be increased further by combining the features from different types of word representations. But, if only one word representation is to be used, Brown clusters have the highest accuracy. Given the improvements to the C&amp;W embeddings since Turian et al. (2009), C&amp;W embeddings outperform the HLBL embeddings. On chunking, there is only a minute difference between Brown clusters and the embeddings. Com0 1 10 100 1K 10K 100K 1M Frequency of word in unlabeled data 0 1 10 100 1K 10K 100K 1M Frequency of word in unlabeled data Figure 3: For word tokens that have different frequency in the unlabeled data, what is the total number of per-token errors incurred on the test set? (a) Chunking results. (b) NER results. bining representations leads to small increases in the test F1. In comparison to chunking, combining different word representations on NER seems </context>
</contexts>
<marker>Turian, Ratinov, Bengio, Roth, 2009</marker>
<rawString>Turian, J., Ratinov, L., Bengio, Y., &amp; Roth, D. (2009). A preliminary evaluation of word representations for named-entity recognition. NIPS Workshop on Grammar Induction, Representation of Language and Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research.</journal>
<contexts>
<context position="4950" citStr="Turney and Pantel (2010)" startWordPosition="725" endWordPosition="728">ay. In this work, we compare different techniques for inducing word representations, evaluating them on the tasks of named entity recognition (NER) and chunking. We retract former negative results published in Turian et al. (2009) about Collobert and Weston (2008) embeddings, given training improvements that we describe in Section 7.1. 2 Distributional representations Distributional word representations are based upon a cooccurrence matrix F of size WxC, where W is the vocabulary size, each row Fw is the initial representation of word w, and each column Fc is some context. Sahlgren (2006) and Turney and Pantel (2010) describe a handful of possible design decisions in contructing F, including choice of context types (left window? right window? size of window?) and type of frequency count (raw? binary? tf-idf?). Fw has dimensionality W, which can be too large to use Fw as features for word w in a supervised model. One can map F to matrix f of size W x d, where d &lt;&lt; C, using some function g, where f = g(F). fw represents word w as a vector with d dimensions. The choice of g is another design decision, although perhaps not as important as the statistics used to initially construct F. The self-organizing seman</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Turney, P. D., &amp; Pantel, P. (2010). From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ushioda</author>
</authors>
<title>Hierarchical clustering of words.</title>
<date>1996</date>
<journal>COLING</journal>
<pages>1159--1162</pages>
<contexts>
<context position="9984" citStr="Ushioda (1996)" startWordPosition="1555" endWordPosition="1556"> for poor clusters of a small number of words. One downside of Brown clustering is that it is based solely on bigram statistics, and does not consider word usage in a wider context. Brown clusters have been used successfully in a variety of NLP applications: NER (Miller et al., 2004; Liang, 2005; Ratinov &amp; Roth, 2009), PCFG parsing (Candito &amp; Crabb´e, 2009), dependency parsing (Koo et al., 2008; Suzuki et al., 2009), and semantic dependency parsing (Zhao et al., 2009). Martin et al. (1998) presents algorithms for inducing hierarchical clusterings based upon word bigram and trigram statistics. Ushioda (1996) presents an extension to the Brown clustering algorithm, and learn hierarchical clusterings of words as well as phrases, which they apply to POS tagging. 3.2 Other work on cluster-based word representations Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. HMMs can be used to induce a soft clustering, specifically a multinomial distribution over possible clusters (hidden states). Li and McCallum (2005) use an HMM-LDA model to improve POS tagging and Chinese Word Segmentation. Huang and Yates (2009) induce a fully-connected HMM, w</context>
</contexts>
<marker>Ushioda, 1996</marker>
<rawString>Ushioda, A. (1996). Hierarchical clustering of words. COLING (pp. 1159–1162).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J V¨ayrynen</author>
<author>T Honkela</author>
</authors>
<title>Comparison of independent component analysis and singular value decomposition in word context analysis.</title>
<date>2005</date>
<booktitle>AKRR’05, International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning.</booktitle>
<marker>V¨ayrynen, Honkela, 2005</marker>
<rawString>V¨ayrynen, J., &amp; Honkela, T. (2005). Comparison of independent component analysis and singular value decomposition in word context analysis. AKRR’05, International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J V¨ayrynen</author>
<author>T Honkela</author>
</authors>
<title>Word category maps based on emergent features created by ICA.</title>
<date>2004</date>
<booktitle>Proceedings of the STeP’2004 Cognition + Cybernetics Symposium (pp. 173–185). Finnish Artificial Intelligence Society.</booktitle>
<marker>V¨ayrynen, Honkela, 2004</marker>
<rawString>V¨ayrynen, J. J., &amp; Honkela, T. (2004). Word category maps based on emergent features created by ICA. Proceedings of the STeP’2004 Cognition + Cybernetics Symposium (pp. 173–185). Finnish Artificial Intelligence Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J V¨ayrynen</author>
<author>T Honkela</author>
<author>L Lindqvist</author>
</authors>
<title>Towards explicit semantic features using independent component analysis.</title>
<date>2007</date>
<booktitle>Proceedings of the Workshop Semantic Content Acquisition and Representation (SCAR).</booktitle>
<institution>Swedish Institute of Computer Science.</institution>
<location>Stockholm, Sweden:</location>
<marker>V¨ayrynen, Honkela, Lindqvist, 2007</marker>
<rawString>V¨ayrynen, J. J., Honkela, T., &amp; Lindqvist, L. (2007). Towards explicit semantic features using independent component analysis. Proceedings of the Workshop Semantic Content Acquisition and Representation (SCAR). Stockholm, Sweden: Swedish Institute of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R ˇReh˚uˇrek</author>
<author>P Sojka</author>
</authors>
<title>Software framework for topic modelling with large corpora.</title>
<date>2010</date>
<publisher>LREC.</publisher>
<marker>ˇReh˚uˇrek, Sojka, 2010</marker>
<rawString>ˇReh˚uˇrek, R., &amp; Sojka, P. (2010). Software framework for topic modelling with large corpora. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
<author>D Johnson</author>
</authors>
<title>A robust risk minimization based named entity recognition system.</title>
<date>2003</date>
<journal>CoNLL.</journal>
<contexts>
<context position="19946" citStr="Zhang and Johnson (2003)" startWordPosition="3137" endWordPosition="3140"> remove gazetteers and non-local features (Krishnan &amp; Manning, 2006). However, we also run experiments that include these features, to understand if the information they provide mostly overlaps with that of the word representations. After each epoch over the training set, we measured the accuracy of the model on the development set. Training was stopped after the accuracy on the development set did not improve for 10 epochs, generally about 50–80 epochs total. The epoch that performed best on the development set was chosen as the final model. We use the following baseline set of features from Zhang and Johnson (2003): • Previous two predictions yi−1 and yi−2 • Current word xi • xi word type information: all-capitalized, is-capitalized, all-digits, alphanumeric, etc. • Prefixes and suffixes of xi, if the word contains hyphens, then the tokens between the hyphens • Tokens in the window c = (xi−2, xi−1, xi, xi+1, xi+2) • Capitalization pattern in the window c • Conjunction of c and yi−1. Word representation features, if present, are used the same way as in Table 1. When using the lexical features, we normalize dates and numbers. For example, 1980 becomes *DDDD* and 212-325-4751 becomes *DDD*- *DDD*-*DDDD*. T</context>
</contexts>
<marker>Zhang, Johnson, 2003</marker>
<rawString>Zhang, T., &amp; Johnson, D. (2003). A robust risk minimization based named entity recognition system. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhao</author>
<author>W Chen</author>
<author>C Kit</author>
<author>G Zhou</author>
</authors>
<title>Multilingual dependency learning: a huge feature engineering method to semantic dependency parsing.</title>
<date>2009</date>
<journal>CoNLL</journal>
<pages>55--60</pages>
<contexts>
<context position="9842" citStr="Zhao et al., 2009" startWordPosition="1534" endWordPosition="1537">usters. The hierarchical nature of the clustering means that we can choose the word class at several levels in the hierarchy, which can compensate for poor clusters of a small number of words. One downside of Brown clustering is that it is based solely on bigram statistics, and does not consider word usage in a wider context. Brown clusters have been used successfully in a variety of NLP applications: NER (Miller et al., 2004; Liang, 2005; Ratinov &amp; Roth, 2009), PCFG parsing (Candito &amp; Crabb´e, 2009), dependency parsing (Koo et al., 2008; Suzuki et al., 2009), and semantic dependency parsing (Zhao et al., 2009). Martin et al. (1998) presents algorithms for inducing hierarchical clusterings based upon word bigram and trigram statistics. Ushioda (1996) presents an extension to the Brown clustering algorithm, and learn hierarchical clusterings of words as well as phrases, which they apply to POS tagging. 3.2 Other work on cluster-based word representations Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. HMMs can be used to induce a soft clustering, specifically a multinomial distribution over possible clusters (hidden states). Li and McC</context>
</contexts>
<marker>Zhao, Chen, Kit, Zhou, 2009</marker>
<rawString>Zhao, H., Chen, W., Kit, C., &amp; Zhou, G. (2009). Multilingual dependency learning: a huge feature engineering method to semantic dependency parsing. CoNLL (pp. 55–60).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>