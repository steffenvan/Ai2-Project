<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.987493">
Forest-to-String Statistical Translation Rules
</title>
<author confidence="0.998657">
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin
</author>
<affiliation confidence="0.986295666666667">
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.759847">
P.O. Box 2704, Beijing 100080, China
</address>
<email confidence="0.998442">
{yliu,huangyun,liuqun,sxlin}@ict.ac.cn
</email>
<sectionHeader confidence="0.995632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995364375">
In this paper, we propose forest-to-string
rules to enhance the expressive power of
tree-to-string translation models. A forest-
to-string rule is capable of capturing non-
syntactic phrase pairs by describing the cor-
respondence between multiple parse trees
and one string. To integrate these rules
into tree-to-string translation models, auxil-
iary rules are introduced to provide a gen-
eralization level. Experimental results show
that, on the NIST 2005 Chinese-English test
set, the tree-to-string model augmented with
forest-to-string rules achieves a relative im-
provement of 4.3% in terms of BLEU score
over the original model which allows tree-
to-string rules only.
</bodyText>
<sectionHeader confidence="0.999137" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998925846153846">
The past two years have witnessed the rapid de-
velopment of linguistically syntax-based translation
models (Quirk et al., 2005; Galley et al., 2006;
Marcu et al., 2006; Liu et al., 2006), which induce
tree-to-string translation rules from parallel texts
with linguistic annotations. They demonstrated very
promising results when compared with the state of
the art phrase-based system (Och and Ney, 2004)
in the NIST 2006 machine translation evaluation 1.
While Galley et al. (2006) and Marcu et al. (2006)
put emphasis on target language analysis, Quirk et
al. (2005) and Liu et al. (2006) show benefits from
modeling the syntax of source language.
</bodyText>
<footnote confidence="0.997636">
1See http://www.nist.gov/speech/tests/mt/
</footnote>
<page confidence="0.988478">
704
</page>
<bodyText confidence="0.999458705882353">
One major problem with linguistically syntax-
based models, however, is that tree-to-string rules
fail to syntactify non-syntactic phrase pairs because
they require a syntax tree fragment over the phrase
to be syntactified. Here, we distinguish between syn-
tactic and non-syntactic phrase pairs. By “syntactic”
we mean that the phrase pair is subsumed by some
syntax tree fragment. The phrase pairs without trees
over them are non-syntactic. Marcu et al. (2006)
report that approximately 28% of bilingual phrases
are non-syntactic on their English-Chinese corpus.
We believe that it is important to make available
to syntax-based models all the bilingual phrases that
are typically available to phrase-based models. On
one hand, phrases have been proven to be a simple
and powerful mechanism for machine translation.
They excel at capturing translations of short idioms,
providing local re-ordering decisions, and incorpo-
rating context information straightforwardly. Chi-
ang (2005) shows significant improvement by keep-
ing the strengths of phrases while incorporating syn-
tax into statistical translation. On the other hand,
the performance of linguistically syntax-based mod-
els can be hindered by making use of only syntac-
tic phrase pairs. Studies reveal that linguistically
syntax-based models are sensitive to syntactic anal-
ysis (Quirk and Corston-Oliver, 2006), which is still
not reliable enough to handle real-world texts due to
limited size and domain of training data.
Various solutions are proposed to tackle the prob-
lem. Galley et al. (2004) handle non-constituent
phrasal translation by traversing the tree upwards
until reaches a node that subsumes the phrase.
Marcu et al. (2006) argue that this choice is inap-
</bodyText>
<note confidence="0.9031445">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 704–711,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999653756756757">
propriate because large applicability contexts are re-
quired.
For a non-syntactic phrase pair, Marcu et al.
(2006) create a xRS rule headed by a pseudo, non-
syntactic nonterminal symbol that subsumes the
phrase and corresponding multi-headed syntactic
structure; and one sibling xRS rule that explains how
the non-syntactic nonterminal symbol can be com-
bined with other genuine nonterminals so as to ob-
tain genuine parse trees. The name of the pseudo
nonterminal is designed to reflect how the corre-
sponding rule can be fully realized. However, they
neglect alignment consistency when creating sibling
rules. In addition, it is hard for the naming mecha-
nism to deal with more complex phenomena.
Liu et al. (2006) treat bilingual phrases as lexi-
calized TATs (Tree-to-string Alignment Template).
A bilingual phrase can be used in decoding if the
source phrase is subsumed by the input parse tree.
Although this solution does help, only syntactic
bilingual phrases are available to the TAT-based
model. Moreover, it is problematic to combine
the translation probabilities of bilingual phrases and
TATs, which are estimated independently.
In this paper, we propose forest-to-string rules
which describe the correspondence between multi-
ple parse trees and a string. They can not only cap-
ture non-syntactic phrase pairs but also have the ca-
pability of generalization. To integrate these rules
into tree-to-string translation models, auxiliary rules
are introduced to provide a generalization level. As
there is no pseudo node or naming mechanism, the
integration of forest-to-string rules is flexible, rely-
ing only on their root nodes. The forest-to-string and
auxiliary rules enable tree-to-string models to derive
in a more general way, while the strengths of con-
ventional tree-to-string rules still remain.
</bodyText>
<sectionHeader confidence="0.877991" genericHeader="method">
2 Forest-to-String Translation Rules
</sectionHeader>
<bodyText confidence="0.997039666666667">
We define a tree-to-string rule r as a triple (˜T, ˜5, ˜A),
which describes the alignment A˜ between a source
parse tree T˜ = T(f��
</bodyText>
<footnote confidence="0.6176163">
1 ) and a target string 5˜ = e��
1 .
A source string f��
1 ,which is the sequence of leaf
nodes of T(f��
1 ), consists of both terminals (source
words) and nonterminals (phrasal categories). A tar-
get string e��
1 is also composed of both terminals
(target words) and nonterminals (placeholders). An
</footnote>
<note confidence="0.672812">
IP
</note>
<figureCaption confidence="0.9504355">
Figure 1: An English sentence aligned with a Chi-
nese parse tree.
</figureCaption>
<bodyText confidence="0.956952733333334">
alignment A˜ is defined as a subset of the Cartesian
product of source and target symbol positions:
A˜C {(j,i) : j = 1,..., J&apos;; i = 1,...,I&apos;}
A derivation θ = r1 o r2 o ... o r,,, is a left-
most composition of translation rules that explains
how a source parse tree T = T (f1 ), a target sen-
tence 5 = ei, and the word alignment A are syn-
chronously generated. For example, Table 1 demon-
strates a derivation composed of only tree-to-string
rules for the (T, 5, A) tuple in Figure 12.
As we mentioned before, tree-to-string rules can
not syntactify phrase pairs that are not subsumed
by any syntax tree fragments. For example, for the
phrase pair (“fie A”, “The gunman was”) in Fig-
ure 1, it is impossible to extract an equivalent tree-
to-string rule that subsumes the same phrase pair
because valid tree-to-string rules can not be multi-
headed.
To address this problem, we propose forest-to-
string rules3 to subsume the non-syntactic phrase
pairs. A forest-to-string rule r 4 is a triple ( F˜, ˜5, ˜A),
which describes the alignment A˜ between K source
parse trees F˜ = T˜1� and a target string ˜5. The
source string f��
1 is therefore the sequence of leaf
nodes of F˜.
Auxiliary rules are introduced to integrate forest-
to-string rules into tree-to-string translation models.
An auxiliary rule is a special unlexicalized tree-to-
string rule that allows multiple source nonterminals
</bodyText>
<footnote confidence="0.8927804">
2We use “X” to denote a nonterminal in the target string. If
there are more than one nonterminals, they are indexed.
3The term “forest” refers to an ordered and finite set of trees.
4We still use “r” to represent a forest-to-string rule to reduce
notational overhead.
</footnote>
<figure confidence="0.9831534">
NP
NN
VP
PU
SB
VP
NP
VV
NN
The gunman was killed by police .
</figure>
<page confidence="0.974991">
705
</page>
<table confidence="0.998185">
No. Rule
( IP ( NP ) ( VP ) ( PU ) ) X1 X2 X3 1:1 2:2 3:3
( NP ( NN ) ) The gunman 1:1 1:2
(VP(SB#)(VP(NP(NN))(VV7zt-k))) was killed by X 1:1 2:4 3:2
( NN ;� ) police 1:1
( PU . ) . 1:1
</table>
<tableCaption confidence="0.986764">
Table 1: A derivation composed of only tree-to-string rules for Figure 1.
</tableCaption>
<table confidence="0.9995184">
No. Rule
( IP ( NP ) ( VP ( SB ) ( VP ) ) ( PU ) ) X1 X2 1:1 2:1 3:2 4:2
( NP ( NN fie)) ( SB 4k) The gunman was 1:1 1:2 2:3
( VP ( NP ) ( VV 7J,-k ) ) ( PU . ) killed by X . 1:3 2:1 3:4
( NP ( NNS ) ) police 1:1
</table>
<tableCaption confidence="0.999033">
Table 2: A derivation composed of tree-to-string, forest-to-string, and auxiliary rules for Figure 1.
</tableCaption>
<bodyText confidence="0.999328071428571">
to correspond to one target nonterminal, suggesting
that the forest-to-string rules that are rooted at such
source nonterminals can be integrated.
For example, Table 2 shows a derivation com-
posed of tree-to-string, forest-to-string, and auxil-
iary rules for the (T, S, A) tuple in Figure 1. r1 is
an auxiliary rule, r2 and r3 are forest-to-string rules,
and r4 is a conventional tree-to-string rule.
Following Marcu et al. (2006), we define the
probability of a tuple (T, S, A) as the sum over all
derivations Oi E Θ that are consistent with the tuple,
c(Θ) = (T, S, A). The probability of each deriva-
tion Oi is given by the product of the probabilities of
all the rules p(rj) in the derivation.
</bodyText>
<equation confidence="0.9847845">
Pr(T, S, A) = � 11 p(rj) (1)
θiEΘ,c(Θ)=(T,S,A) rjEθi
</equation>
<sectionHeader confidence="0.988587" genericHeader="method">
3 Training
</sectionHeader>
<bodyText confidence="0.994126125">
We obtain tree-to-string and forest-to-string rules
from word-aligned, source side parsed bilingual cor-
pus. The extraction algorithm is shown in Figure 2.
Note that T� denotes either a tree or a forest.
For each span, the (tree/forest, string, alignment)
triples are identified first. If a triple is consistent with
the alignment, the skeleton of the triple is computed
then. A skeleton s is a rule satisfying the following:
</bodyText>
<listItem confidence="0.9883732">
1. s E R(t), s is induced from t.
2. node(T(s)) &gt; 2, the tree/forest of s contains
two or more nodes.
3. br E R(t) ∧ node(T (r)) &gt; 2, T (s) C T(r),
the tree/forest of s is the subgraph of that of any
r containing two or more nodes.
1: Input: a source tree T = T (fi ), a target string
S = el, and word alignment A between them
2: R := O
3: for u := 0 to J − 1 do
4: for v := 1 to J − u do
5: identify the triple set T corresponding to
span (v, v + u)
6: for each triple t = (T&apos;, S&apos;, A&apos;) E T do
7: if (T&apos;, S&apos;) is not consistent with A then
8: continue
9: end if
10: if u = 0 n node(T�) = 1 then
11: add t to R
12: add (root(T%“X”, 1:1) to R
13: else
14: compute the skeleton s of the triple t
15: register rules that are built on s using rules
extracted from the sub-triples of t:
R := R U build(s, R)
16: end if
17: end for
18: end for
19: end for
20: Output: rule set R
</listItem>
<figureCaption confidence="0.99231">
Figure 2: Rule extraction algorithm.
</figureCaption>
<bodyText confidence="0.880766166666667">
Given the skeleton and rules extracted from the
sub-triples, the rules for the triple can be acquired.
For example, the algorithm identifies the follow-
ing triple for span (1, 2) in Figure 1:
(( NP ( NN ) ) ( SB # ),“The gunman was”, 1:1 1:2 2:3)
The skeleton of the triple is:
</bodyText>
<equation confidence="0.860714">
(( NP ) ( SB ),“X1 X2”, 1:1 2:2)
</equation>
<bodyText confidence="0.998732">
As the algorithm proceeds bottom-up, five rules
have already been extracted from the sub-triples,
rooted at “NP” and “SB” respectively:
</bodyText>
<table confidence="0.850722666666667">
(( NP ),“X”, 1:1)
(( NP ( NN ) ),“X”, 1:1)
(( NP ( NN ) ),“The gunman”, 1:1 1:2)
706
(( SB ),“X”, 1:1)
(( SB # ),“was”, 1:1)
</table>
<bodyText confidence="0.9995346">
Hence, we can obtain new rules by replacing the
source and target symbols of the skeleton with corre-
sponding rules and also by modifying the alignment
information. For the above triple, the combination
of the five rules produces 2 × 3 = 6 new rules:
</bodyText>
<listItem confidence="0.990197333333333">
(( NP ) ( SB ),“X1 X2”, 1:1 2:2)
(( NP ) ( SB 4k ),“X was”, 1:1 2:2)
(( NP ( NN ) ) ( SB ),“X1 X2”, 1:1 2:2)
(( NP ( NN ) ) ( SB 4k ),“X was”, 1:1 2:2)
(( NP ( NN fie)) ( SB ),“The gunman X”, 1:1 1:2)
(( NP ( NN fie ) ) ( SB # ),“The gunman was”, 1:1 1:2 2:3)
</listItem>
<bodyText confidence="0.9969846">
Since we need only to check the alignment con-
sistency, in principle all phrase pairs can be captured
by tree-to-string and forest-to-string rules. To lower
the complexity for both training and decoding, we
impose four restrictions:
</bodyText>
<listItem confidence="0.986303714285714">
1. Both the first and the last symbols in the target
string must be aligned to some source symbols.
2. The height of a tree or forest is no greater than
h.
3. The number of direct descendants of a node is
no greater than c.
4. The number of leaf nodes is no greater than l.
</listItem>
<bodyText confidence="0.9983313">
Although possible, it is infeasible to learn aux-
iliary rules from training data. To extract an auxil-
iary rule which integrates at least one forest-to-string
rule, one need traverse the parse tree upwards until
one reaches a node that subsumes the entire forest
without violating the alignment consistency. This
usually results in very complex auxiliary rules, es-
pecially on real-world training data, making both
training and decoding very slow. As a result, we
construct auxiliary rules in decoding instead.
</bodyText>
<sectionHeader confidence="0.998505" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.999643666666667">
Given a source parse tree T(fJ 1 ), our decoder finds
the target yield of the single best derivation that has
source yield of T (fJ1 ):
</bodyText>
<listItem confidence="0.863776777777778">
1: Input: a source parse tree T = T (fi )
2: for u := 0 to J − 1 do
3: for v := 1 to J − u do
4: for each T&apos; spanning from v to v + u do
5: if T&apos; is a tree then
6: for each usable tree-to-string rule r do
7: for each derivation 0 inferred from r
and derivations in matrix do
8: add 0 to matrix[v, v + u, root(T&apos;)]
9: end for
10: end for
11: search subcell divisions D[v, v + u]
12: for each subcell division d E D[v, v + u] do
13: if d contains at least one forest cell then
14: construct auxiliary rule ra
15: for each derivation 0 inferred from ra
and derivations in matrix do
16: add 0 to matrix[v, v + u, root(T&apos;)]
17: end for
18: end if
19: end for
20: else
21: for each usable forest-to-string rule r do
22: for each derivation 0 inferred from r
and derivations in matrix do
23: add 0 to matrix[v, v + u, “”]
24: end for
25: end for
26: search subcell divisions D[v, v + u]
27: end if
28: end for
29: end for
30: end for
31: find the best derivation 0� in matrix[1, J, root(T)] and
get the best translation S = e(�0)
32: Output: a target string S
</listItem>
<figureCaption confidence="0.997457">
Figure 3: Decoding algorithm.
</figureCaption>
<equation confidence="0.968205666666667">
≈ argmax 11 p(rj) (2)
SAB
, , rjEB,c(B)=(T,S,A)
</equation>
<bodyText confidence="0.999828571428571">
Figure 3 demonstrates the decoding algorithm.
It organizes the derivations into an array matrix
whose cells matrix[j1, j2, X] are sets of derivations.
[j1, j2, X] represents a tree/forest rooted at X span-
ning from j1 to j2. We use the empty string “” to
denote the pseudo root of a forest.
Next, we will explain how to infer derivations for
a tree/forest provided a usable rule. If T (r) = T&apos;,
there is only one derivation which contains only the
rule r. This usually happens for leaf nodes. If
T (r) ⊂ T&apos;, the rule r resorts to derivations from
subcells to infer new derivations. Suppose that the
decoder is to translate the source tree in Figure 1
and finds a usable rule for [1, 5, “IP”]:
</bodyText>
<equation confidence="0.977684166666667">
(( IP ( NP ) ( VP ) ( PU ) ),“X1 X2 X3”, 1:1 2:2 3:3)
Sˆ = argmax Pr(T, S, A)
S,A
= argmax
S,A BjEΘ,c(Θ)=(T,S,A) rjEBj
E 11 p(rj)
</equation>
<page confidence="0.993236">
707
</page>
<table confidence="0.9864726">
Subcell Division Auxiliary Rule
[1, 1][2, 2][3, 5] ( IP ( NP ) ( VP ( SB ) ( VP ) ) ( PU ) ) X1 X2 X3 1:1 2:2 3:3 4:3
[1, 2][3, 4][5, 5] ( IP ( NP ) ( VP ( SB ) ( VP ) ) ( PU ) ) X1 X2 X3 1:1 2:1 3:2 4:3
[1, 3][4, 5] ( IP ( NP ) ( VP ( SB ) ( VP ( NP ) ( VV )) ) ( PU ) ) X1 X2 1:1 2:1 3:1 4:2 5:2
[1, 1][2, 5] (IP(NP)(VP)(PU)) X1 X2 1:1 2:2 3:2
</table>
<tableCaption confidence="0.999774">
Table 3: Subcell divisions and corresponding auxiliary rules for the source tree in Figure 1
</tableCaption>
<bodyText confidence="0.9977514">
Since the decoding algorithm proceeds in a
bottom-up fashion, the uncovered portions have al-
ready been translated.
For [1, 1, “NP”], suppose that we can find a
derivation in matrix:
</bodyText>
<equation confidence="0.810179428571429">
(( NP ( NN ) ),“The gunman”, 1:1 1:2)
For [2, 4, “VP”], we find a derivation in matrix:
(( VP ( SB 4 k) ( VP ( NP ( NN )) (VV 7J,-k) ) ),
“was killed by X”, 1:1 2:4 3:2)
(( NN P—ftp ),“police”, 1:1)
For [5, 5, “PU”], we find a derivation in matrix:
(( PU . ),“.”, 1:1)
</equation>
<bodyText confidence="0.9988665">
Henceforth, we get a derivation for [1, 5, “IP”],
shown in Table 1.
A translation rule r is said to be usable to an input
tree/forest T&apos; if and only if:
</bodyText>
<listItem confidence="0.9006375">
1. T (r) ⊆ T&apos;, the tree/forest of r is the subgraph
of T~.
2. root(T(r)) = root(T&apos;), the root sequence of
T (r) is identical to that of T~.
</listItem>
<bodyText confidence="0.7749805">
For example, the following rules are usable to the
tree “( NP ( NR t 1 ) ( NN L--%J&apos;j- ) )”:
</bodyText>
<equation confidence="0.869451857142857">
(( NP ( NR ) ( NN ) ),“X1 X2”, 1:2 2:1)
(( NP ( NR tX ) ( NN ) ),“China X”, 1:1 2:2)
(( NP ( NR tq ) ( NN ?-&apos;-;J,- ) ),“China economy”, 1:1 2:2)
Similarly, the forest-to-string rule
(( ( NP ( NR ) ( NN ) ) ( VP ) ),“X1 X2 X3”, 1:2 2:1 3:3)
is usable to the forest
( NP ( NR A7&apos; H-) ( NN .��tlk ) ) ( VP (VV )( NN ) )
</equation>
<bodyText confidence="0.999371857142857">
As we mentioned before, auxiliary rules are spe-
cial unlexicalized tree-to-string rules that are built in
decoding rather than learnt from real-world data. To
get an auxiliary rule for a cell, we need first identify
its subcell division.
A cell sequence c1, c2,. . . , cn is referred to as a
subcell division of a cell c if and only if:
</bodyText>
<listItem confidence="0.873661370370371">
1. c1.begin = c.begin
1: Input: a cell [j1, j2], the derivation array matrix,
the subcell division array D
2: if j1 = j2 then
3: p�:= 0
4: for each derivation 0 in matrix[j1, j2, ] do
5: p�:= max(p(0), P)
6: end for
7: add {[j1, j2]} : p� to D[j1, j2]
8: else
9: if [j1, j2] is a forest cell then
10: p� := 0
11: for each derivation 0 in matrix[j1, j2, ] do
12: p�:= max(p(0), P)
13: end for
14: add {[j1, j2]} : p� to D[j1, j2]
15: end if
16: for j := j1 to j2 − 1 do
17: for each division d1 E D[j1, j] do
18: for each division d2 E D[j + 1, j2] do
19: create a new division: d := d1 ® d2
20: add d to D[j1, j2]
21: end for
22: end for
23: end for
24: end if
25: Output: subcell divisions D[j1, j2]
</listItem>
<figureCaption confidence="0.825703">
Figure 4: Subcell division search algorithm.
</figureCaption>
<listItem confidence="0.9269115">
2. cn.end = c.end
3. cj.end + 1 = cj+1.begin,1 ≤ j &lt; n
</listItem>
<bodyText confidence="0.999815933333333">
Given a subcell division, it is easy to construct the
auxiliary rule for a cell. For each subcell, one need
transverse the parse tree upwards until one reaches
nodes that subsume it. All descendants of these
nodes are dropped. The target string consists of only
nonterminals, the number of which is identical to
that of subcells. To limit the search space, we as-
sume that the alignment between the source tree and
the target string is monotone.
Table 3 shows some subcell divisions and corre-
sponding auxiliary rules constructed for the source
tree in Figure 1. For simplicity, we ignore the root
node label.
There are 2n−1 subcell divisions for a cell which
has a length of n. We need only consider the sub-
</bodyText>
<page confidence="0.994999">
708
</page>
<bodyText confidence="0.984120708333333">
cell divisions which contain at least one forest cell node count of auxiliary rules of a derivation to pe-
because tree-to-string rules have already explored nalize the use of forest-to-string and auxiliary rules.
those contain only tree cells. 5 Experiments
The actual search algorithm for subcell divisions In this section, we report on experiments with
is shown in Figure 4. We use matrix[j1, j2, ·] to de- Chinese-to-English translation. The training corpus
note all trees or forests spanning from j1 to j2. The consists of 31,149 sentence pairs with 843, 256 Chi-
subcell divisions and their associated probabilities nese words and 949, 583 English words. For the
are stored in an array D. We define an operator ⊕ language model, we used SRI Language Modeling
between two divisions: their cell sequences are con- Toolkit (Stolcke, 2002) to train a trigram model with
catenated and the probabilities are accumulated. modified Kneser-Ney smoothing (Chen and Good-
As sometimes there are no usable rules available, man, 1998) on the 31,149 English sentences. We
we introduce default rules to ensure that we can al- selected 571 short sentences from the 2002 NIST
ways get a translation for any input parse tree. A de- MT Evaluation test set as our development corpus,
fault rule is a tree-to-string rule 5, built in two ways: and used the 2005 NIST MT Evaluation test set as
1. If the input tree contains only one node, the our test corpus. Our evaluation metric is BLEU-4
target string of the default rule is equal to the (Papineni et al., 2002), as calculated by the script
source string. mteval-v11b.pl with its default setting except that
2. If the height of the input tree is greater than we used case-sensitive matching of n-grams. To
one, the tree of the default rule contains only perform minimum error rate training (Och, 2003)
the root node and its direct descendants of the to tune the feature weights to maximize the sys-
input tree, the string contains only nontermi- tem’s BLEU score on development set, we used the
nals, and the alignment is monotone. script optimizeV5IBMBLEU.m (Venugopal and Vo-
To speed up the decoder, we limit the search space gel, 2005).
by reducing the number of rules used for each cell. We ran GIZA++ (Och and Ney, 2000) on the
There are two ways to limit the rule table size: by training corpus in both directions using its default
a fixed limit a of how many rules are retrieved for setting, and then applied the refinement rule “diag-
each cell, and by a probability threshold α that spec- and” described in (Koehn et al., 2003) to obtain a
ify that the rule probability has to be above some single many-to-many word alignment for each sen-
value. Also, instead of keeping the full list of deriva- tence pair. Next, we employed a Chinese parser
tions for a cell, we store a top-scoring subset of the written by Deyi Xiong (Xiong et al., 2005) to parse
derivations. This can also be done by a fixed limit all the 31,149 Chinese sentences. The parser was
b or a threshold Q. The subcell division array D, in trained on articles 1-270 of Penn Chinese Treebank
which divisions containing forest cells have priority version 1.0 and achieved 79.4% in terms of F1 mea-
over those composed of only tree cells, is pruned by sure.
keeping only a-best divisions. Given the word-aligned, source side parsed bilin-
Following Och and Ney (2002), we base our gual corpus, we obtained bilingual phrases using the
model on log-linear framework and adopt the seven training toolkits publicly released by Philipp Koehn
feature functions described in (Liu et al., 2006). It with its default setting. Then, we applied extrac-
is very important to balance the preference between tion algorithm described in Figure 2 to extract both
conventional tree-to-string rules and the newly- tree-to-string and forest-to-string rules by restricting
introduced forest-to-string and auxiliary rules. As h = 3, c = 5, and l = 7. All the rules, including
the probabilities of auxiliary rules are not learnt bilingual phrases, tree-to-string rules, and forest-to-
from training data, we add a feature that sums up the string rules, are filtered for the development and test
sets.
According to different levels of lexicalization, we
divide translation rules into three categories:
5There are no default rules for forests because only tree-to-
string rules are essential to tree-to-string translation models.
</bodyText>
<table confidence="0.9256926">
709
Rule L P U Total
BP 251,173 0 0 251,173
TR 56,983 41,027 3,529 101,539
FR 16,609 254,346 25,051 296,006
</table>
<tableCaption confidence="0.9581755">
Table 4: Number of rules used in experiments (BP:
bilingual phrase, TR: tree-to-string rule, FR: forest-
to-string rule; L: lexicalized, P: partial lexicalized,
U: unlexicalized).
</tableCaption>
<table confidence="0.999889333333333">
System Rule Set BLEU4
Pharaoh BP 0.2182 f 0.0089
Lynx BP 0.2059 f 0.0083
TR 0.2302 f 0.0089
TR + BP 0.2346 f 0.0088
TR + FR + AR 0.2402 f 0.0087
</table>
<tableCaption confidence="0.947191">
Table 5: Comparison of Pharaoh and Lynx with dif-
ferent rule sets.
</tableCaption>
<listItem confidence="0.9990042">
1. lexicalized: all symbols in both the source and
target strings are terminals
2. unlexicalized: all symbols in both the source
and target strings are nonterminals
3. partial lexicalized: otherwise
</listItem>
<bodyText confidence="0.998932666666667">
Table 4 shows the statistics of rules used in our ex-
periments. We find that even though forest-to-string
rules are introduced the total number (i.e. 73,592)
of lexicalized tree-to-string and forest-to-string rules
is still far less than that (i.e. 251,173) of bilingual
phrases. This difference results from the restriction
we impose in training that both the first and last sym-
bols in the target string must be aligned to some
source symbols. For the forest-to-string rules, par-
tial lexicalized ones are in the majority.
We compared our system Lynx against a freely
available phrase-based decoder Pharaoh (Koehn et
al., 2003). For Pharaoh, we set a = 20, α = 0,
b = 100, Q = 10−5, and distortion limit dl = 4. For
Lynx, we set a = 20, α = 0, b = 100, and Q = 0.
Two postprocessing procedures ran to improve the
outputs of both systems: OOVs removal and recapi-
talization.
Table 5 shows results on test set using Pharaoh
and Lynx with different rule sets. Note that Lynx
is capable of using only bilingual phrases plus de-
</bodyText>
<table confidence="0.9995375">
Forest-to-String Rule Set BLEU4
None 0.2225 f 0.0085
L 0.2297 f 0.0081
P 0.2279 f 0.0083
U 0.2270 f 0.0087
L + P + U 0.2312 f 0.0082
</table>
<tableCaption confidence="0.9080215">
Table 6: Effect of lexicalized, partial lexicalized,
and unlexicalized forest-to-string rules.
</tableCaption>
<bodyText confidence="0.99808062962963">
fault rules to perform monotone search. The 95%
confidence intervals were computed using Zhang’s
significance tester (Zhang et al., 2004). We mod-
ified it to conform to NIST’s current definition of
the BLEU brevity penalty. We find that Lynx out-
performs Pharaoh significantly. The integration of
forest-to-string rules achieves an absolute improve-
ment of 1.0% (4.3% relative) over using tree-to-
string rules only. This difference is statistically sig-
nificant (p &lt; 0.01). It also achieves better result
than treating bilingual phrases as lexicalized tree-to-
string rules. To produce the best result of 0.2402,
Lynx made use of 26, 082 tree-to-string rules, 9, 219
default rules, 5, 432 forest-to-string rules, and 2, 919
auxiliary rules. This suggests that tree-to-string
rules still play a central role, although the integra-
tion of forest-to-string and auxiliary rules is really
beneficial.
Table 6 demonstrates the effect of forest-to-string
rules with different lexicalization levels. We set
a = 3, α = 0, b = 10, and Q = 0. The second row
“None” shows the result of using only tree-to-string
rules. “L” denotes using tree-to-string rules and lex-
icalized forest-to-string rules. Similarly, “L+P+U”
denotes using tree-to-string rules and all forest-to-
string rules. We find that lexicalized forest-to-string
rules are more useful.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999898375">
In this paper, we introduce forest-to-string rules to
capture non-syntactic phrase pairs that are usually
unaccessible to traditional tree-to-string translation
models. With the help of auxiliary rules, forest-to-
string rules can be integrated into tree-to-string mod-
els to offer more general derivations. Experiment re-
sults show that the tree-to-string model augmented
with forest-to-string rules significantly outperforms
</bodyText>
<page confidence="0.982461">
710
</page>
<bodyText confidence="0.999301909090909">
the original model which allows tree-to-string rules
only.
Our current rule extraction algorithm attaches the
unaligned target words to the nearest ascendants that
subsume them. This constraint hampers the expres-
sive power of our model. We will try a more general
way as suggested in (Galley et al., 2006), making
no a priori assumption about assignment and using
EM training to learn the probability distribution. We
will also conduct experiments on large scale training
data to further examine our design philosophy.
</bodyText>
<sectionHeader confidence="0.972961" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.997607">
This work was supported by National Natural Sci-
ence Foundation of China, Contract No. 60603095
and 60573188.
</bodyText>
<sectionHeader confidence="0.999253" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999735373333334">
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University Center for
Research in Computing Technology.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings
of ACL 2005, pages 263–270, Ann Arbor, Michigan,
June.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In
Proceedings of HLT/NAACL 2004, pages 273–280,
Boston, Massachusetts, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL 2006, pages 961–968, Sydney,
Australia, July.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings ofHLT/NAACL 2003, pages 127–133, Edmonton,
Canada, May.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING/ACL 2006, pages
609–616, Sydney, Australia, July.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine trans-
lation with syntactified target language phrases. In
Proceedings of EMNLP 2006, pages 44–52, Sydney,
Australia, July.
Franz J. Och and Hermann Ney. 2000. Improved statis-
tical alignment models. In Proceedings of ACL 2000,
pages 440–447.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL 2002,
pages 295–302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417–449.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL
2003, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings ofACL
2002, pages 311–318, Philadephia, USA, July.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed statis-
tical machine translation. In Proceedings of EMNLP
2006, pages 62–69, Sydney, Australia, July.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of ACL 2005, pages
271–279, Ann Arbor, Michigan, June.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 30, pages 901–904.
Ashish Venugopal and Stephan Vogel. 2005. Consid-
erations in maximum mutual information and mini-
mum classification error training for statistical ma-
chine translation. In Proceedings of the Tenth Confer-
ence of the European Association for Machine Trans-
lation, pages 271–279.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin.
2005. Parsing the penn chinese treebank with seman-
tic knowledge. In Proceedings ofIJCNLP 2005, pages
70–81.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting bleu/nist scores how much improvement do
we need to have a better system? In Proceedings
of Fourth International Conference on Language Re-
sources and Evaluation, pages 2051–2054.
</reference>
<page confidence="0.997911">
711
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.572919">
<title confidence="0.999707">Forest-to-String Statistical Translation Rules</title>
<author confidence="0.982295">Yun Huang Liu</author>
<author confidence="0.982295">Qun Liu Lin</author>
<affiliation confidence="0.850648333333333">Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences</affiliation>
<address confidence="0.994494">P.O. Box 2704, Beijing 100080, China</address>
<abstract confidence="0.998177">In this paper, we propose forest-to-string rules to enhance the expressive power of tree-to-string translation models. A forestto-string rule is capable of capturing nonsyntactic phrase pairs by describing the correspondence between multiple parse trees and one string. To integrate these rules into tree-to-string translation models, auxiliary rules are introduced to provide a generalization level. Experimental results show that, on the NIST 2005 Chinese-English test set, the tree-to-string model augmented with forest-to-string rules achieves a relative imof terms of BLEU score over the original model which allows treeto-string rules only.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Harvard University Center for Research in Computing Technology.</institution>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report, Harvard University Center for Research in Computing Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="2643" citStr="Chiang (2005)" startWordPosition="383" endWordPosition="385">tree fragment. The phrase pairs without trees over them are non-syntactic. Marcu et al. (2006) report that approximately 28% of bilingual phrases are non-syntactic on their English-Chinese corpus. We believe that it is important to make available to syntax-based models all the bilingual phrases that are typically available to phrase-based models. On one hand, phrases have been proven to be a simple and powerful mechanism for machine translation. They excel at capturing translations of short idioms, providing local re-ordering decisions, and incorporating context information straightforwardly. Chiang (2005) shows significant improvement by keeping the strengths of phrases while incorporating syntax into statistical translation. On the other hand, the performance of linguistically syntax-based models can be hindered by making use of only syntactic phrase pairs. Studies reveal that linguistically syntax-based models are sensitive to syntactic analysis (Quirk and Corston-Oliver, 2006), which is still not reliable enough to handle real-world texts due to limited size and domain of training data. Various solutions are proposed to tackle the problem. Galley et al. (2004) handle non-constituent phrasal</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL 2005, pages 263–270, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL 2004,</booktitle>
<pages>273--280</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="3212" citStr="Galley et al. (2004)" startWordPosition="470" endWordPosition="473">ontext information straightforwardly. Chiang (2005) shows significant improvement by keeping the strengths of phrases while incorporating syntax into statistical translation. On the other hand, the performance of linguistically syntax-based models can be hindered by making use of only syntactic phrase pairs. Studies reveal that linguistically syntax-based models are sensitive to syntactic analysis (Quirk and Corston-Oliver, 2006), which is still not reliable enough to handle real-world texts due to limited size and domain of training data. Various solutions are proposed to tackle the problem. Galley et al. (2004) handle non-constituent phrasal translation by traversing the tree upwards until reaches a node that subsumes the phrase. Marcu et al. (2006) argue that this choice is inapProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 704–711, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics propriate because large applicability contexts are required. For a non-syntactic phrase pair, Marcu et al. (2006) create a xRS rule headed by a pseudo, nonsyntactic nonterminal symbol that subsumes the phrase and corresponding multi-headed s</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of HLT/NAACL 2004, pages 273–280, Boston, Massachusetts, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1118" citStr="Galley et al., 2006" startWordPosition="155" endWordPosition="158">he correspondence between multiple parse trees and one string. To integrate these rules into tree-to-string translation models, auxiliary rules are introduced to provide a generalization level. Experimental results show that, on the NIST 2005 Chinese-English test set, the tree-to-string model augmented with forest-to-string rules achieves a relative improvement of 4.3% in terms of BLEU score over the original model which allows treeto-string rules only. 1 Introduction The past two years have witnessed the rapid development of linguistically syntax-based translation models (Quirk et al., 2005; Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006), which induce tree-to-string translation rules from parallel texts with linguistic annotations. They demonstrated very promising results when compared with the state of the art phrase-based system (Och and Ney, 2004) in the NIST 2006 machine translation evaluation 1. While Galley et al. (2006) and Marcu et al. (2006) put emphasis on target language analysis, Quirk et al. (2005) and Liu et al. (2006) show benefits from modeling the syntax of source language. 1See http://www.nist.gov/speech/tests/mt/ 704 One major problem with linguistically syntaxbased mo</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING/ACL 2006, pages 961–968, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT/NAACL 2003,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="20484" citStr="Koehn et al., 2003" startWordPosition="3789" endWordPosition="3792"> string contains only nontermi- tem’s BLEU score on development set, we used the nals, and the alignment is monotone. script optimizeV5IBMBLEU.m (Venugopal and VoTo speed up the decoder, we limit the search space gel, 2005). by reducing the number of rules used for each cell. We ran GIZA++ (Och and Ney, 2000) on the There are two ways to limit the rule table size: by training corpus in both directions using its default a fixed limit a of how many rules are retrieved for setting, and then applied the refinement rule “diageach cell, and by a probability threshold α that spec- and” described in (Koehn et al., 2003) to obtain a ify that the rule probability has to be above some single many-to-many word alignment for each senvalue. Also, instead of keeping the full list of deriva- tence pair. Next, we employed a Chinese parser tions for a cell, we store a top-scoring subset of the written by Deyi Xiong (Xiong et al., 2005) to parse derivations. This can also be done by a fixed limit all the 31,149 Chinese sentences. The parser was b or a threshold Q. The subcell division array D, in trained on articles 1-270 of Penn Chinese Treebank which divisions containing forest cells have priority version 1.0 and ach</context>
<context position="23635" citStr="Koehn et al., 2003" startWordPosition="4307" endWordPosition="4310"> shows the statistics of rules used in our experiments. We find that even though forest-to-string rules are introduced the total number (i.e. 73,592) of lexicalized tree-to-string and forest-to-string rules is still far less than that (i.e. 251,173) of bilingual phrases. This difference results from the restriction we impose in training that both the first and last symbols in the target string must be aligned to some source symbols. For the forest-to-string rules, partial lexicalized ones are in the majority. We compared our system Lynx against a freely available phrase-based decoder Pharaoh (Koehn et al., 2003). For Pharaoh, we set a = 20, α = 0, b = 100, Q = 10−5, and distortion limit dl = 4. For Lynx, we set a = 20, α = 0, b = 100, and Q = 0. Two postprocessing procedures ran to improve the outputs of both systems: OOVs removal and recapitalization. Table 5 shows results on test set using Pharaoh and Lynx with different rule sets. Note that Lynx is capable of using only bilingual phrases plus deForest-to-String Rule Set BLEU4 None 0.2225 f 0.0085 L 0.2297 f 0.0081 P 0.2279 f 0.0083 U 0.2270 f 0.0087 L + P + U 0.2312 f 0.0082 Table 6: Effect of lexicalized, partial lexicalized, and unlexicalized fo</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings ofHLT/NAACL 2003, pages 127–133, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL 2006,</booktitle>
<pages>609--616</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1157" citStr="Liu et al., 2006" startWordPosition="163" endWordPosition="166">trees and one string. To integrate these rules into tree-to-string translation models, auxiliary rules are introduced to provide a generalization level. Experimental results show that, on the NIST 2005 Chinese-English test set, the tree-to-string model augmented with forest-to-string rules achieves a relative improvement of 4.3% in terms of BLEU score over the original model which allows treeto-string rules only. 1 Introduction The past two years have witnessed the rapid development of linguistically syntax-based translation models (Quirk et al., 2005; Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006), which induce tree-to-string translation rules from parallel texts with linguistic annotations. They demonstrated very promising results when compared with the state of the art phrase-based system (Och and Ney, 2004) in the NIST 2006 machine translation evaluation 1. While Galley et al. (2006) and Marcu et al. (2006) put emphasis on target language analysis, Quirk et al. (2005) and Liu et al. (2006) show benefits from modeling the syntax of source language. 1See http://www.nist.gov/speech/tests/mt/ 704 One major problem with linguistically syntaxbased models, however, is that tree-to-string r</context>
<context position="4281" citStr="Liu et al. (2006)" startWordPosition="635" endWordPosition="638"> Marcu et al. (2006) create a xRS rule headed by a pseudo, nonsyntactic nonterminal symbol that subsumes the phrase and corresponding multi-headed syntactic structure; and one sibling xRS rule that explains how the non-syntactic nonterminal symbol can be combined with other genuine nonterminals so as to obtain genuine parse trees. The name of the pseudo nonterminal is designed to reflect how the corresponding rule can be fully realized. However, they neglect alignment consistency when creating sibling rules. In addition, it is hard for the naming mechanism to deal with more complex phenomena. Liu et al. (2006) treat bilingual phrases as lexicalized TATs (Tree-to-string Alignment Template). A bilingual phrase can be used in decoding if the source phrase is subsumed by the input parse tree. Although this solution does help, only syntactic bilingual phrases are available to the TAT-based model. Moreover, it is problematic to combine the translation probabilities of bilingual phrases and TATs, which are estimated independently. In this paper, we propose forest-to-string rules which describe the correspondence between multiple parse trees and a string. They can not only capture non-syntactic phrase pair</context>
<context position="21499" citStr="Liu et al., 2006" startWordPosition="3963" endWordPosition="3966">entences. The parser was b or a threshold Q. The subcell division array D, in trained on articles 1-270 of Penn Chinese Treebank which divisions containing forest cells have priority version 1.0 and achieved 79.4% in terms of F1 meaover those composed of only tree cells, is pruned by sure. keeping only a-best divisions. Given the word-aligned, source side parsed bilinFollowing Och and Ney (2002), we base our gual corpus, we obtained bilingual phrases using the model on log-linear framework and adopt the seven training toolkits publicly released by Philipp Koehn feature functions described in (Liu et al., 2006). It with its default setting. Then, we applied extracis very important to balance the preference between tion algorithm described in Figure 2 to extract both conventional tree-to-string rules and the newly- tree-to-string and forest-to-string rules by restricting introduced forest-to-string and auxiliary rules. As h = 3, c = 5, and l = 7. All the rules, including the probabilities of auxiliary rules are not learnt bilingual phrases, tree-to-string rules, and forest-tofrom training data, we add a feature that sums up the string rules, are filtered for the development and test sets. According t</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of COLING/ACL 2006, pages 609–616, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>Spmt: Statistical machine translation with syntactified target language phrases.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>44--52</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1138" citStr="Marcu et al., 2006" startWordPosition="159" endWordPosition="162">ween multiple parse trees and one string. To integrate these rules into tree-to-string translation models, auxiliary rules are introduced to provide a generalization level. Experimental results show that, on the NIST 2005 Chinese-English test set, the tree-to-string model augmented with forest-to-string rules achieves a relative improvement of 4.3% in terms of BLEU score over the original model which allows treeto-string rules only. 1 Introduction The past two years have witnessed the rapid development of linguistically syntax-based translation models (Quirk et al., 2005; Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006), which induce tree-to-string translation rules from parallel texts with linguistic annotations. They demonstrated very promising results when compared with the state of the art phrase-based system (Och and Ney, 2004) in the NIST 2006 machine translation evaluation 1. While Galley et al. (2006) and Marcu et al. (2006) put emphasis on target language analysis, Quirk et al. (2005) and Liu et al. (2006) show benefits from modeling the syntax of source language. 1See http://www.nist.gov/speech/tests/mt/ 704 One major problem with linguistically syntaxbased models, however, is th</context>
<context position="3353" citStr="Marcu et al. (2006)" startWordPosition="491" endWordPosition="494">ax into statistical translation. On the other hand, the performance of linguistically syntax-based models can be hindered by making use of only syntactic phrase pairs. Studies reveal that linguistically syntax-based models are sensitive to syntactic analysis (Quirk and Corston-Oliver, 2006), which is still not reliable enough to handle real-world texts due to limited size and domain of training data. Various solutions are proposed to tackle the problem. Galley et al. (2004) handle non-constituent phrasal translation by traversing the tree upwards until reaches a node that subsumes the phrase. Marcu et al. (2006) argue that this choice is inapProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 704–711, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics propriate because large applicability contexts are required. For a non-syntactic phrase pair, Marcu et al. (2006) create a xRS rule headed by a pseudo, nonsyntactic nonterminal symbol that subsumes the phrase and corresponding multi-headed syntactic structure; and one sibling xRS rule that explains how the non-syntactic nonterminal symbol can be combined with other genuine nonter</context>
<context position="8619" citStr="Marcu et al. (2006)" startWordPosition="1415" endWordPosition="1418"> ( VP ( NP ) ( VV 7J,-k ) ) ( PU . ) killed by X . 1:3 2:1 3:4 ( NP ( NNS ) ) police 1:1 Table 2: A derivation composed of tree-to-string, forest-to-string, and auxiliary rules for Figure 1. to correspond to one target nonterminal, suggesting that the forest-to-string rules that are rooted at such source nonterminals can be integrated. For example, Table 2 shows a derivation composed of tree-to-string, forest-to-string, and auxiliary rules for the (T, S, A) tuple in Figure 1. r1 is an auxiliary rule, r2 and r3 are forest-to-string rules, and r4 is a conventional tree-to-string rule. Following Marcu et al. (2006), we define the probability of a tuple (T, S, A) as the sum over all derivations Oi E Θ that are consistent with the tuple, c(Θ) = (T, S, A). The probability of each derivation Oi is given by the product of the probabilities of all the rules p(rj) in the derivation. Pr(T, S, A) = � 11 p(rj) (1) θiEΘ,c(Θ)=(T,S,A) rjEθi 3 Training We obtain tree-to-string and forest-to-string rules from word-aligned, source side parsed bilingual corpus. The extraction algorithm is shown in Figure 2. Note that T� denotes either a tree or a forest. For each span, the (tree/forest, string, alignment) triples are id</context>
</contexts>
<marker>Marcu, Wang, Echihabi, Knight, 2006</marker>
<rawString>Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. Spmt: Statistical machine translation with syntactified target language phrases. In Proceedings of EMNLP 2006, pages 44–52, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>440--447</pages>
<contexts>
<context position="20175" citStr="Och and Ney, 2000" startWordPosition="3732" endWordPosition="3735">hat 2. If the height of the input tree is greater than we used case-sensitive matching of n-grams. To one, the tree of the default rule contains only perform minimum error rate training (Och, 2003) the root node and its direct descendants of the to tune the feature weights to maximize the sysinput tree, the string contains only nontermi- tem’s BLEU score on development set, we used the nals, and the alignment is monotone. script optimizeV5IBMBLEU.m (Venugopal and VoTo speed up the decoder, we limit the search space gel, 2005). by reducing the number of rules used for each cell. We ran GIZA++ (Och and Ney, 2000) on the There are two ways to limit the rule table size: by training corpus in both directions using its default a fixed limit a of how many rules are retrieved for setting, and then applied the refinement rule “diageach cell, and by a probability threshold α that spec- and” described in (Koehn et al., 2003) to obtain a ify that the rule probability has to be above some single many-to-many word alignment for each senvalue. Also, instead of keeping the full list of deriva- tence pair. Next, we employed a Chinese parser tions for a cell, we store a top-scoring subset of the written by Deyi Xiong</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz J. Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of ACL 2000, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>295--302</pages>
<contexts>
<context position="21280" citStr="Och and Ney (2002)" startWordPosition="3929" endWordPosition="3932">. Next, we employed a Chinese parser tions for a cell, we store a top-scoring subset of the written by Deyi Xiong (Xiong et al., 2005) to parse derivations. This can also be done by a fixed limit all the 31,149 Chinese sentences. The parser was b or a threshold Q. The subcell division array D, in trained on articles 1-270 of Penn Chinese Treebank which divisions containing forest cells have priority version 1.0 and achieved 79.4% in terms of F1 meaover those composed of only tree cells, is pruned by sure. keeping only a-best divisions. Given the word-aligned, source side parsed bilinFollowing Och and Ney (2002), we base our gual corpus, we obtained bilingual phrases using the model on log-linear framework and adopt the seven training toolkits publicly released by Philipp Koehn feature functions described in (Liu et al., 2006). It with its default setting. Then, we applied extracis very important to balance the preference between tion algorithm described in Figure 2 to extract both conventional tree-to-string rules and the newly- tree-to-string and forest-to-string rules by restricting introduced forest-to-string and auxiliary rules. As h = 3, c = 5, and l = 7. All the rules, including the probabilit</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of ACL 2002, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1374" citStr="Och and Ney, 2004" startWordPosition="193" endWordPosition="196">h test set, the tree-to-string model augmented with forest-to-string rules achieves a relative improvement of 4.3% in terms of BLEU score over the original model which allows treeto-string rules only. 1 Introduction The past two years have witnessed the rapid development of linguistically syntax-based translation models (Quirk et al., 2005; Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006), which induce tree-to-string translation rules from parallel texts with linguistic annotations. They demonstrated very promising results when compared with the state of the art phrase-based system (Och and Ney, 2004) in the NIST 2006 machine translation evaluation 1. While Galley et al. (2006) and Marcu et al. (2006) put emphasis on target language analysis, Quirk et al. (2005) and Liu et al. (2006) show benefits from modeling the syntax of source language. 1See http://www.nist.gov/speech/tests/mt/ 704 One major problem with linguistically syntaxbased models, however, is that tree-to-string rules fail to syntactify non-syntactic phrase pairs because they require a syntax tree fragment over the phrase to be syntactified. Here, we distinguish between syntactic and non-syntactic phrase pairs. By “syntactic” </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="19754" citStr="Och, 2003" startWordPosition="3660" endWordPosition="3661">ation test set as our development corpus, fault rule is a tree-to-string rule 5, built in two ways: and used the 2005 NIST MT Evaluation test set as 1. If the input tree contains only one node, the our test corpus. Our evaluation metric is BLEU-4 target string of the default rule is equal to the (Papineni et al., 2002), as calculated by the script source string. mteval-v11b.pl with its default setting except that 2. If the height of the input tree is greater than we used case-sensitive matching of n-grams. To one, the tree of the default rule contains only perform minimum error rate training (Och, 2003) the root node and its direct descendants of the to tune the feature weights to maximize the sysinput tree, the string contains only nontermi- tem’s BLEU score on development set, we used the nals, and the alignment is monotone. script optimizeV5IBMBLEU.m (Venugopal and VoTo speed up the decoder, we limit the search space gel, 2005). by reducing the number of rules used for each cell. We ran GIZA++ (Och and Ney, 2000) on the There are two ways to limit the rule table size: by training corpus in both directions using its default a fixed limit a of how many rules are retrieved for setting, and t</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL 2003, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL 2002,</booktitle>
<pages>311--318</pages>
<location>Philadephia, USA,</location>
<contexts>
<context position="19464" citStr="Papineni et al., 2002" startWordPosition="3610" endWordPosition="3613">ified Kneser-Ney smoothing (Chen and GoodAs sometimes there are no usable rules available, man, 1998) on the 31,149 English sentences. We we introduce default rules to ensure that we can al- selected 571 short sentences from the 2002 NIST ways get a translation for any input parse tree. A de- MT Evaluation test set as our development corpus, fault rule is a tree-to-string rule 5, built in two ways: and used the 2005 NIST MT Evaluation test set as 1. If the input tree contains only one node, the our test corpus. Our evaluation metric is BLEU-4 target string of the default rule is equal to the (Papineni et al., 2002), as calculated by the script source string. mteval-v11b.pl with its default setting except that 2. If the height of the input tree is greater than we used case-sensitive matching of n-grams. To one, the tree of the default rule contains only perform minimum error rate training (Och, 2003) the root node and its direct descendants of the to tune the feature weights to maximize the sysinput tree, the string contains only nontermi- tem’s BLEU score on development set, we used the nals, and the alignment is monotone. script optimizeV5IBMBLEU.m (Venugopal and VoTo speed up the decoder, we limit the</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings ofACL 2002, pages 311–318, Philadephia, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Simon Corston-Oliver</author>
</authors>
<title>The impact of parse quality on syntactically-informed statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP 2006,</booktitle>
<pages>62--69</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="3025" citStr="Quirk and Corston-Oliver, 2006" startWordPosition="438" endWordPosition="441">rases have been proven to be a simple and powerful mechanism for machine translation. They excel at capturing translations of short idioms, providing local re-ordering decisions, and incorporating context information straightforwardly. Chiang (2005) shows significant improvement by keeping the strengths of phrases while incorporating syntax into statistical translation. On the other hand, the performance of linguistically syntax-based models can be hindered by making use of only syntactic phrase pairs. Studies reveal that linguistically syntax-based models are sensitive to syntactic analysis (Quirk and Corston-Oliver, 2006), which is still not reliable enough to handle real-world texts due to limited size and domain of training data. Various solutions are proposed to tackle the problem. Galley et al. (2004) handle non-constituent phrasal translation by traversing the tree upwards until reaches a node that subsumes the phrase. Marcu et al. (2006) argue that this choice is inapProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 704–711, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics propriate because large applicability contexts are req</context>
</contexts>
<marker>Quirk, Corston-Oliver, 2006</marker>
<rawString>Chris Quirk and Simon Corston-Oliver. 2006. The impact of parse quality on syntactically-informed statistical machine translation. In Proceedings of EMNLP 2006, pages 62–69, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL 2005,</booktitle>
<pages>271--279</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1097" citStr="Quirk et al., 2005" startWordPosition="151" endWordPosition="154">airs by describing the correspondence between multiple parse trees and one string. To integrate these rules into tree-to-string translation models, auxiliary rules are introduced to provide a generalization level. Experimental results show that, on the NIST 2005 Chinese-English test set, the tree-to-string model augmented with forest-to-string rules achieves a relative improvement of 4.3% in terms of BLEU score over the original model which allows treeto-string rules only. 1 Introduction The past two years have witnessed the rapid development of linguistically syntax-based translation models (Quirk et al., 2005; Galley et al., 2006; Marcu et al., 2006; Liu et al., 2006), which induce tree-to-string translation rules from parallel texts with linguistic annotations. They demonstrated very promising results when compared with the state of the art phrase-based system (Och and Ney, 2004) in the NIST 2006 machine translation evaluation 1. While Galley et al. (2006) and Marcu et al. (2006) put emphasis on target language analysis, Quirk et al. (2005) and Liu et al. (2006) show benefits from modeling the syntax of source language. 1See http://www.nist.gov/speech/tests/mt/ 704 One major problem with linguist</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of ACL 2005, pages 271–279, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Spoken Language Processing,</booktitle>
<volume>30</volume>
<pages>901--904</pages>
<contexts>
<context position="18759" citStr="Stolcke, 2002" startWordPosition="3488" endWordPosition="3489">in only tree cells. 5 Experiments The actual search algorithm for subcell divisions In this section, we report on experiments with is shown in Figure 4. We use matrix[j1, j2, ·] to de- Chinese-to-English translation. The training corpus note all trees or forests spanning from j1 to j2. The consists of 31,149 sentence pairs with 843, 256 Chisubcell divisions and their associated probabilities nese words and 949, 583 English words. For the are stored in an array D. We define an operator ⊕ language model, we used SRI Language Modeling between two divisions: their cell sequences are con- Toolkit (Stolcke, 2002) to train a trigram model with catenated and the probabilities are accumulated. modified Kneser-Ney smoothing (Chen and GoodAs sometimes there are no usable rules available, man, 1998) on the 31,149 English sentences. We we introduce default rules to ensure that we can al- selected 571 short sentences from the 2002 NIST ways get a translation for any input parse tree. A de- MT Evaluation test set as our development corpus, fault rule is a tree-to-string rule 5, built in two ways: and used the 2005 NIST MT Evaluation test set as 1. If the input tree contains only one node, the our test corpus. </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of International Conference on Spoken Language Processing, volume 30, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Stephan Vogel</author>
</authors>
<title>Considerations in maximum mutual information and minimum classification error training for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth Conference of the European Association for Machine Translation,</booktitle>
<pages>271--279</pages>
<marker>Venugopal, Vogel, 2005</marker>
<rawString>Ashish Venugopal and Stephan Vogel. 2005. Considerations in maximum mutual information and minimum classification error training for statistical machine translation. In Proceedings of the Tenth Conference of the European Association for Machine Translation, pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Shuanglong Li</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Parsing the penn chinese treebank with semantic knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings ofIJCNLP</booktitle>
<pages>70--81</pages>
<contexts>
<context position="20796" citStr="Xiong et al., 2005" startWordPosition="3846" endWordPosition="3849">n the There are two ways to limit the rule table size: by training corpus in both directions using its default a fixed limit a of how many rules are retrieved for setting, and then applied the refinement rule “diageach cell, and by a probability threshold α that spec- and” described in (Koehn et al., 2003) to obtain a ify that the rule probability has to be above some single many-to-many word alignment for each senvalue. Also, instead of keeping the full list of deriva- tence pair. Next, we employed a Chinese parser tions for a cell, we store a top-scoring subset of the written by Deyi Xiong (Xiong et al., 2005) to parse derivations. This can also be done by a fixed limit all the 31,149 Chinese sentences. The parser was b or a threshold Q. The subcell division array D, in trained on articles 1-270 of Penn Chinese Treebank which divisions containing forest cells have priority version 1.0 and achieved 79.4% in terms of F1 meaover those composed of only tree cells, is pruned by sure. keeping only a-best divisions. Given the word-aligned, source side parsed bilinFollowing Och and Ney (2002), we base our gual corpus, we obtained bilingual phrases using the model on log-linear framework and adopt the seven</context>
</contexts>
<marker>Xiong, Li, Liu, Lin, 2005</marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin. 2005. Parsing the penn chinese treebank with semantic knowledge. In Proceedings ofIJCNLP 2005, pages 70–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhang</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Interpreting bleu/nist scores how much improvement do we need to have a better system?</title>
<date>2004</date>
<booktitle>In Proceedings of Fourth International Conference on Language Resources and Evaluation,</booktitle>
<pages>2051--2054</pages>
<contexts>
<context position="24394" citStr="Zhang et al., 2004" startWordPosition="4448" endWordPosition="4451"> postprocessing procedures ran to improve the outputs of both systems: OOVs removal and recapitalization. Table 5 shows results on test set using Pharaoh and Lynx with different rule sets. Note that Lynx is capable of using only bilingual phrases plus deForest-to-String Rule Set BLEU4 None 0.2225 f 0.0085 L 0.2297 f 0.0081 P 0.2279 f 0.0083 U 0.2270 f 0.0087 L + P + U 0.2312 f 0.0082 Table 6: Effect of lexicalized, partial lexicalized, and unlexicalized forest-to-string rules. fault rules to perform monotone search. The 95% confidence intervals were computed using Zhang’s significance tester (Zhang et al., 2004). We modified it to conform to NIST’s current definition of the BLEU brevity penalty. We find that Lynx outperforms Pharaoh significantly. The integration of forest-to-string rules achieves an absolute improvement of 1.0% (4.3% relative) over using tree-tostring rules only. This difference is statistically significant (p &lt; 0.01). It also achieves better result than treating bilingual phrases as lexicalized tree-tostring rules. To produce the best result of 0.2402, Lynx made use of 26, 082 tree-to-string rules, 9, 219 default rules, 5, 432 forest-to-string rules, and 2, 919 auxiliary rules. Thi</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2004</marker>
<rawString>Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Interpreting bleu/nist scores how much improvement do we need to have a better system? In Proceedings of Fourth International Conference on Language Resources and Evaluation, pages 2051–2054.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>