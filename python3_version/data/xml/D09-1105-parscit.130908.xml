<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.976345">
Learning Linear Ordering Problems for Better Translation∗
</title>
<author confidence="0.929484">
Roy Tromble
</author>
<affiliation confidence="0.728944">
Google, Inc.
</affiliation>
<address confidence="0.651311">
4720 Forbes Ave.
Pittsburgh, PA 15213
</address>
<email confidence="0.991833">
royt@google.com
</email>
<author confidence="0.997978">
Jason Eisner
</author>
<affiliation confidence="0.9207765">
Department of Computer Science
Johns Hopkins University
</affiliation>
<address confidence="0.940068">
Baltimore, MD 21218
</address>
<email confidence="0.998529">
jason@cs.jhu.edu
</email>
<sectionHeader confidence="0.993887" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964875">
We apply machine learning to the Lin-
ear Ordering Problem in order to learn
sentence-specific reordering models for
machine translation. We demonstrate that
even when these models are used as a mere
preprocessing step for German-English
translation, they significantly outperform
Moses’ integrated lexicalized reordering
model.
Our models are trained on automatically
aligned bitext. Their form is simple but
novel. They assess, based on features of
the input sentence, how strongly each pair
of input word tokens wi, wj would like
to reverse their relative order. Combining
all these pairwise preferences to find the
best global reordering is NP-hard. How-
ever, we present a non-trivial O(n3) al-
gorithm, based on chart parsing, that at
least finds the best reordering within a cer-
tain exponentially large neighborhood. We
show how to iterate this reordering process
within a local search algorithm, which we
use in training.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999705714285714">
Machine translation is an important but difficult
problem. One of the properties that makes it dif-
ficult is the fact that different languages express
the same concepts in different orders. A ma-
chine translation system must therefore rearrange
the source language concepts to produce a fluent
translation in the target language.
</bodyText>
<footnote confidence="0.981659">
1This work is excerpted and adapted from the first au-
thor’s Ph.D. thesis (Tromble, 2009). Some of the ideas here
appeared in (Eisner and Tromble, 2006) without empirical
validation. The material is based in part upon work sup-
ported by the National Science Foundation under Grant No.
0347822.
</footnote>
<bodyText confidence="0.999807292682927">
Phrase-based translation systems rely heavily
on the target language model to ensure a fluent
output order. However, a target n-gram language
model alone is known to be inadequate. Thus,
translation systems should also look at how the
source sentence prefers to reorder. Yet past sys-
tems have traditionally used rather weak models of
the reordering process. They may look only at the
distance between neighboring phrases, or depend
only on phrase unigrams. The decoders also rely
on search error, in the form of limited reordering
windows, for both efficiency and translation qual-
ity.
Demonstrating the inadequacy of such ap-
proaches, Al-Onaizan and Papineni (2006)
showed that even given the words in the reference
translation, and their alignment to the source
words, a decoder of this sort charged with merely
rearranging them into the correct target-language
order could achieve a BLEU score (Papineni et
al., 2002) of at best 69%—and that only when
restricted to keep most words very close to their
source positions.
This paper introduces a more sophisticated
model of reordering based on the Linear Order-
ing Problem (LOP), itself an NP-hard permutation
problem. We apply machine learning, in the form
of a modified perceptron algorithm, to learn pa-
rameters of a linear model that constructs a matrix
of weights from each source language sentence.
We train the parameters on orderings derived from
automatic word alignments of parallel sentences.
The LOP model of reordering is a complete
ordering model, capable of assigning a different
score to every possible permutation of the source-
language sentence. Unlike the target language
model, it uses information about the relative posi-
tions of the words in the source language, as well
as the source words themselves and their parts of
speech and contexts. It is therefore a language-pair
specific model.
</bodyText>
<page confidence="0.95022">
1007
</page>
<note confidence="0.9965825">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1007–1016,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.99995005">
We apply the learned LOP model as a prepro-
cessing step before both training and evaluation of
a phrase-based translation system, namely Moses.
Our methods for finding a good reordering un-
der the NP-hard LOP are themselves of interest,
adapting algorithms from natural language parsing
and developing novel dynamic programs.
Our results demonstrate a significant improve-
ment over translation using unreordered German.
Using Moses with only distance-based reordering
and a distortion limit of 6, our preprocessing im-
proves BLEU from 25.27 to 26.40. Furthermore,
that improvement is significantly greater than the
improvement Moses achieves with its lexicalized
reordering model, 25.55.
Collins et al. (2005) improved German-English
translation using a statistical parser and several
hand-written rules for preprocessing the German
sentences. This paper presents a similar improve-
ment using fully automatic methods.
</bodyText>
<sectionHeader confidence="0.983786" genericHeader="method">
2 A Linear Ordering Model
</sectionHeader>
<bodyText confidence="0.999242333333333">
This section introduces a model of word reorder-
ing for machine translation based on the Linear
Ordering Problem.
</bodyText>
<subsectionHeader confidence="0.963233">
2.1 Formalization
</subsectionHeader>
<bodyText confidence="0.9993789375">
The input sentence is w = w1w2 ... wn. To dis-
tinguish duplicate tokens of the same word, we as-
sume that each token is superscripted by its input
position, e.g., w = die1 Katze2 hat3 die Frau5
gekauft6 (gloss: “the cat has the woman bought”).
For a fixed w, a permutation π = 7172 ... 7n is
any reordering of the tokens in w. The set Hn of
all such permutations has size n!. We would like to
define a scoring model that assigns a high score to
the permutation π = die Frau5 hat3 gekauft6 die1
Katze2 (gloss: “the woman has bought the cat”),
since that corresponds well to the desired English
order.
To construct a function that scores permutations
of w, we first construct a pairwise preference ma-
trix Bw ∈ Rn×n, whose entries are
</bodyText>
<equation confidence="0.765015">
Bw[E, r] def = θ · φ(w, E, r), (1)
</equation>
<bodyText confidence="0.970974533333333">
Here θ is a vector of weights. φ is a vector of
feature functions, each considering the entire word
sequence w, as well as any functions thereof, such
as part of speech tags.
We will hereafter abbreviate Bw as B. Its inte-
ger indices E and r are identified with the input to-
kens wt and wr, and it can be helpful to write them
that way; e.g., we will sometimes write B[2, 5] as
B[Katze2, Frau5].
The idea behind our reordering model is
that B[Katze2, Frau5] &gt; B[Katze5,Frau2] ex-
presses a preference to keep Katze2 before Frau5,
whereas the opposite inequality would express a
preference—other things equal—for permutations
in which their order is reversed. Thus, we define1
</bodyText>
<equation confidence="0.956591">
score(π) def E B[7i, 7j]
p(π) = i,j: 1&lt;i&lt;j&lt;n
π =def 1Z exp(γ · score(π))
def argmax score(π)
=
π∈Πn
</equation>
<bodyText confidence="0.998294333333333">
Note that i and j denote positions in π, whereas
7i, 7j, E, and r denote particular input tokens such
as Katze2 and Frau5.
</bodyText>
<subsectionHeader confidence="0.977888">
2.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999960043478261">
To the extent that the costs B generally discour-
age reordering, they will particularly discourage
long-distance movement, as it swaps more pairs
of words.
We point out that our model is somewhat pecu-
liar, since it does not directly consider whether the
permutation π keeps die and Frau5 adjacent or
even close together, but only whether their order
is reversed.
Of course, the model could be extended to con-
sider adjacency, or more generally, the three-way
cost of interposing k between i and j. See (Eis-
ner and Tromble, 2006; Tromble, 2009) for such
extensions and associated algorithms.
However, in the present paper we focus on the
model in the simple form (2) that only considers
pairwise reordering costs for all pairs in the sen-
tence. Our goal is to show that these unfamiliar
pairwise reordering costs are useful, when mod-
eled with a rich feature set via equation (1). Even
in isolation (as a preprocessing step), without con-
sidering any other kinds of reordering costs or lan-
guage model, they can achieve useful reorderings
</bodyText>
<footnote confidence="0.938922444444445">
1For any f &lt; r, we may assume without loss of gener-
ality that B[r, f] = 0, since if not, subtracting B[r, f] from
both B[f, r] and B[r, f] (exactly one of which appears in each
score(π)) will merely reduce the scores of all permutations
by this amount, leaving equations (3) and (4) unchanged.
Thus, in practice, we take B to be an upper triangular ma-
trix. We use equation (1) only to define B[f, r] for f &lt; r, and
train θ accordingly. However, we will ignore this point in our
exposition.
</footnote>
<page confidence="0.996254">
1008
</page>
<bodyText confidence="0.999919461538461">
of German that complement existing techniques
and thus improve state-of-the-art systems. Our
positive results in even this situation suggest that
in future, pairwise reordering costs should proba-
bly be integrated into MT systems.
The probabilistic interpretation (3) of the
score (2) may be useful when thus integrating our
model with language models or other reordering
models during translation, or simply when train-
ing our model to maximize likelihood or minimize
expected error. However, in the present paper we
will stick to purely discriminative training and de-
coding methods that simply try to maximize (2).
</bodyText>
<subsectionHeader confidence="0.997824">
2.3 The Linear Ordering Problem
</subsectionHeader>
<bodyText confidence="0.999973315789474">
In the combinatorial optimization literature, the
maximization problem (4) (with input B) is known
as the Linear Ordering Problem. It has numer-
ous practical applications in fields including eco-
nomics, sociology, graph theory, graph drawing,
archaeology, and task scheduling (Gr¨otschel et
al., 1984). Computational studies on real data
have often used “input-output” matrices represent-
ing resource flows among economic sectors (Schi-
avinotto and St¨utzle, 2004).
Unfortunately, the problem is NP-hard. Further-
more, it is known to be APX-complete, meaning
that there is no polynomial time approximation
scheme unless P=NP (Mishra and Sikdar, 2004).
However, there are various heuristic procedures
for approximating it (Tromble, 2009). We now
give an attractive, novel procedure, which uses a
CKY-parsing-like algorithm to search various sub-
sets of IIn in polynomial time.
</bodyText>
<sectionHeader confidence="0.994025" genericHeader="method">
3 Local Search
</sectionHeader>
<bodyText confidence="0.965667666666667">
“Local search” refers to any hill-climbing proce-
dure that iteratively improves a solution by mak-
ing an optimal “local” change at each iteration.2
In this case, we start with the identity permutation,
find a “nearby” permutation with a better score (2),
and repeat until we have reached a local maximum
of the scoring objective.
This section describes a local search procedure
that uses a very generous definition of “local.” At
each iteration, it finds the optimal permutation in
a certain exponentially large neighborhood N(π)
of the current permutation π.
2One can introduce randomness to obtain MCMC sam-
pling or simulated annealing algorithms. Our algorithms ex-
tend naturally to allow this (cf. Tromble (2009)).
</bodyText>
<equation confidence="0.999317666666667">
S — S0,n
Si,k — Si,j Sj,k
Si−1,i — 7ri
</equation>
<figureCaption confidence="0.974317">
Figure 1: A grammar for a large neighborhood of
</figureCaption>
<bodyText confidence="0.989899844444445">
permutations, given one permutation π of length
n. The Si,k rules are instantiated for each 0 &lt;
i &lt; j &lt; k &lt; n, and the Si−1,i rules for each
0 &lt; i &lt; n.
We say that two permutations are neighbors iff
they can be aligned by an Inversion Transduction
Grammar (ITG) (Wu, 1997), which is a familiar
reordering device in machine translation. Equiva-
lently, π&apos; E N(π) iff π can be transformed into
π&apos; by swapping various adjacent substrings of π,
as long as these swaps are properly nested. Zens
and Ney (2003) used a normal form to show that
the size of the ITG neighborhood N(π) is a large
Schr¨oder number, which grows exponentially in
n. Asymptotically, the ratio between the size of
the neighborhood for n + 1 and the size for n ap-
proaches 3 + 2.\/2 pt� 5.8.
We show that equation (2) can be optimized
within N(π) in O(n3) time, using dynamic pro-
gramming. The algorithm is based on CKY pars-
ing. However, a novelty is that the grammar
weights must themselves be computed by O(n3)
dynamic programming.
Our grammar is shown in Figure 1. Parsing
the “input sentence” π with this grammar simply
constructs all binary trees that yield the string π.
There is essentially only one nonterminal, S, but
we split it into O(n2) position-specific nontermi-
nals such as Si,j, which can only yield the span
7ri+17ri+2 . . . 7rj. An example parse is shown in
Figure 2.
The important point is that we will place a
score on each binary grammar rule. The score
of the rule Si,k — Si,j Sj,k is max(0, Ai,j,k),
where Ai,j,k is the benefit to swapping the sub-
strings 7ri+17ri+2 ... 7rj and 7rj+17rj+2 ... 7rk. The
rule is considered to be a “swap rule” if its
score is positive, showing that a swap will be
beneficial (independent of the rest of the tree).
If the parse in Figure 2 is the parse with
the highest total score, and its swap rules are
S0,5 — S0,1 S1,5 and S3,5 — S3,4 S4,5, then
our best permutation in the neighborhood of π
must be the (linguistically desirable) permutation
die4Frau5hat3gekauft6die1Katze2, obtained from
</bodyText>
<page confidence="0.717283">
1009
</page>
<equation confidence="0.970487375">
S
S0,6
❍
✟✟✟✟✟✟ ❍ ❍ ❍ ❍ ❍
S0,5
✟✟✟✟ ❍ ❍ ❍ ❍
S1,5
✟✟✟✟ ❍❍ ❍ ❍
</equation>
<bodyText confidence="0.859035666666667">
Figure 2: One parse of the current permutation 7r.
In this example, 7r has somehow gotten the input
words into alphabetical order (owing to previous
hill-climbing steps). We are now trying to further
improve this order.
7r by two swaps.
How do we find this solution? Clearly
the benefit (positive or negative) to swapping
7ri+17ri+2 ... 7rj with 7rj+17rj+2 ... 7rk is
</bodyText>
<equation confidence="0.980075">
B[7rr,7rP] − B[7rP,7rr] (5)
</equation>
<bodyText confidence="0.998965666666667">
We can evaluate all O(n3) possible swaps in to-
tal time O(n3), using the dynamic programming
recurrence
</bodyText>
<equation confidence="0.9981885">
Di,j,k = Di,j,k−1 + Di+1,j,k − Di+1,j,k−1 (6)
+B[7rk, 7ri+1] − B[7ri+1, 7rk]
</equation>
<bodyText confidence="0.994270230769231">
with the base case Di,j,k = 0 if i = j or j = k.
This gives us the weights for the grammar rules,
and then we can use weighted CKY parsing to
find the highest-scoring (Viterbi) parse in O(n3)
time. Extracting our new and improved permuta-
tion 7r&apos; E N(7r) from this parse is a simple O(n)-
time algorithm.
Figure 3 gives pseudocode for our local search
algorithm, showing how to compute the quan-
tities (6) during parsing rather than beforehand.
β[i, k] holds the weight of the best permuta-
tion (in the neighborhood) of the subsequence
7ri+17ri+1 ... 7rk.3
</bodyText>
<footnote confidence="0.88112">
3The use of β is intended to suggest an analogy to inside
probability—or more precisely, the Viterbi approximation to
inside probability (since we are maximizing rather than sum-
ming over parses).
</footnote>
<bodyText confidence="0.99991325">
The next two sections describe how to use our
local search algorithm to discriminatively learn the
weights of the parameters from Section 2, equa-
tion (1).
</bodyText>
<sectionHeader confidence="0.998704" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.999949125">
Our objective function (2) works only to the extent
that we can derive a good pairwise preference ma-
trix B.,,,. We do this by using a rich feature set in
equation (1).
We adapt the features of McDonald et al.
(2005), introduced there for dependency parsing,
to the task of machine translation reordering. Be-
cause both models construct features for pairs of
words given the entire sentence, there is a close
correspondence between the two tasks, although
the output is quite different.
Each feature φ(w, E, r) in equation (1) is a bi-
nary feature that fires when (w, E, r) has some
conjunction of properties. The properties that are
considered include the words wP and wr, the parts
of speech of {wP−1, . . . , wr+11, and the distance
r − E. Table 1 shows the feature templates.
We also tried features based on a dependency
parse of the German, with the idea of using LOP
features to reorder the dependents of each word,
and thus model syntactic movement. This did
produce better monolingual reorderings (as in Ta-
ble 2), but it did not help final translation into En-
glish (Table 3), so we do not report the details here.
</bodyText>
<sectionHeader confidence="0.945071" genericHeader="method">
5 Learning to Reorder
</sectionHeader>
<bodyText confidence="0.999795222222222">
Ideally, we would have a large corpus of desir-
able reorderings of source sentences—in our case,
German sentences permuted into target English
word order—from which to train the parameters of
our model. Unfortunately, the alignments between
German and English sentences are only infre-
quently one-to-one. Furthermore, human-aligned
parallel sentences are hard to come by, and never
in the quantity we would like.
Instead, we make do with automatically-
generated word alignments, and we heuristi-
cally derive an English-like word order for
the German sentence based on the alignment.
We used GIZA++ (Och and Ney, 2003) to
align approximately 751,000 sentences from the
German-English portion of the Europarl corpus
(Koehn, 2005), in both the German-to-English and
English-to-German directions. We combined the
</bodyText>
<figure confidence="0.996460944444445">
S1,3
✟✟ ❍❍
S1,2
die4
S3,5
✟✟❍❍
S3,4
gekauft6
S4,5
hat3
S2,3
Frau5
S5,6
Katze2
S0,1
die1
Di,j,k = � j k
P=i+1 r=j+1
</figure>
<page confidence="0.720656">
1010
</page>
<listItem confidence="0.944729">
1: procedure LOCALSEARCHSTEP(B, π, n)
2: for i ← 0 to n − 1 do
3: 3[i, i + 1] ← 0
4: fork ← i + 1 to n do
5: Δ[i, i, k] ← Δ[i, k, k] ← 0
6: end for
7: end for
8: for w ← 2 to n do
9: for i ← 0 to n − w do
10: k ← i + w
11: 3[i, k] ← −∞
12: for j ← i + 1 to k − 1 do
13: Δ[i, j, k] ← Δ[i, j, k − 1] + Δ[i + 1, j, k] − Δ[i + 1, j, k − 1] + B[Irk, Iri+1] − B[Iri+1, Irk]
</listItem>
<figure confidence="0.823959">
14: 3[i, k] ← max(3[i, k], 3[i, j] + 3[j, k] + max(0, Δ[i, j, k]))
15: end for
16: end for
17: end for
18: return 3[0, n]
19: end procedure
</figure>
<figureCaption confidence="0.979829">
Figure 3: Pseudocode for computing the score of the best permutation in the neighborhood of π under
the Linear Ordering Problem specified by the matrix B. Computing the best neighbor is a simple matter
of keeping back pointers to the choices of max and ordering them as implied.
</figureCaption>
<bodyText confidence="0.998804803030303">
alignments using the “grow-diag-final-and” proce-
dure provided with Moses (Koehn et al., 2007).
For each of these German sentences, we derived
the English-like reordering of it, which we call
German&apos;, by the following procedure. Each Ger-
man token was assigned an integer key, namely
the position of the leftmost of the English tokens
to which it was aligned, or 0 if it was not aligned
to any English tokens. We then did a stable sort of
the German tokens based on these keys, meaning
that if two German tokens had the same key, their
order was preserved.
This is similar to the oracle ordering used by
Al-Onaizan and Papineni (2006), but differs in the
handling of unaligned words. They kept unaligned
words with the closest preceding aligned word.4
Having found the German&apos; corresponding to
each German sentence, we randomly divided
the sentences into 2,000 each for development
and evaluation, and the remaining approximately
747,000 for training.
We used the averaged perceptron algorithm
(Freund and Schapire, 1998; Collins, 2002) to
train the parameters of the model. We ran the al-
gorithm multiple times over the training sentences,
4We tried two other methods for deriving English word
order from word alignments. The first alternative was to
align only in one direction, from English to German, with
null alignments disallowed, so that every German word was
aligned to a single English word. The second alternative
used BerkeleyAligner (Liang et al., 2006; DeNero and Klein,
2007), which shares information between the two alignment
directions to improve alignment quality. Neither alternative
produced improvements in our ultimate translation quality.
measuring the quality of the learned parameters by
reordering the held-out development set after each
iteration. We stopped when the BLEU score on
the development set failed to improve for two con-
secutive iterations, which occurred after fourteen
passes over the data.
Each perceptron update should compare the true
German&apos; to the German&apos; that would be predicted
by the model (2). As the latter is NP-hard to find,
we instead substitute the local maximum found by
local search as described in Section 3, starting at
the identity permutation, which corresponds to the
original German word order.
During training, we iterate the local search as
described earlier. However, for decoding, we only
do a single step of local search, thus restricting re-
orderings to the ITG neighborhood of the origi-
nal German. This restriction turns out to improve
performance slightly, even though it reduces the
quality of our approximation to the LOP prob-
lem (4). In other words, it turns out that reorder-
ings found outside the ITG neighborhood tend to
be poor German&apos; even if our LOP-based objective
function thinks that they are good German&apos;.
This is not to say that the gold standard German&apos;
is always in the ITG neighborhood of the original
German—often it is not. Thus, it might be bet-
ter in future work to still allow the local search to
take more than one step, but to penalize the second
step. In effect, score(π) would then include a fea-
ture indicating whether π is in the neighborhood
of the original German.
</bodyText>
<page confidence="0.834508">
1011
</page>
<equation confidence="0.887934375">
t`−1 w` t` t`+1 tb tr−1 wr tr tr+1
x x x x x x x x x
x x x x x x x x
x x x x x x x x
x x x x
x x x x
x x x x
x x x x
</equation>
<tableCaption confidence="0.6910569">
x x
x x
x x
x x
x x
x x
x x
x x
x x
Table 1: Feature templates for B[E, r] (w` is the Eth
</tableCaption>
<bodyText confidence="0.9850477">
word, t` its part of speech tag, and b matches any
index such that E &lt; b &lt; r). Each of the above
is also conjoined with the distance between the
words, r − E, to form an additional feature tem-
plate. Distances are binned into 1, 2, 3, 4, 5, &gt; 5,
and &gt; 10.
The model is initialized at the start of train-
ing using log-odds of the parameters. Let Φm =
{(w, E, r) 1 φm(w, E, r) = 11 be the set of word
pairs in the training data for which feature m fires.
</bodyText>
<equation confidence="0.513604">
→
Let Φm be the subset of Φm for which the words
←
</equation>
<bodyText confidence="0.965459538461538">
stay in order, and Φm the subset for which the
words reverse order. Then in this model,
θm = log~ Φm + 2 I −log ~ Φm + 1 I . (7) 2
This model is equivalent to smoothed naive Bayes
if converted to probabilities. The learned model
significantly outperforms it on the monolingual re-
ordering task.
Table 2 compares the model after perceptron
training to the model at the start of training,
measuring BLEU score of the predicted German&apos;
against the observed German&apos;. In addition to these
BLEU scores, we can measure precision and re-
call of pairs of reordered words against the ob-
</bodyText>
<table confidence="0.99935325">
Ordering p2 p3 p4 BLEU
German 57.4 38.3 27.7 49.65
Log-odds 57.4 38.4 27.8 49.75
Perceptron 58.6 40.3 29.8 51.51
</table>
<tableCaption confidence="0.980654">
Table 2: Monolingual BLEU score on develop-
</tableCaption>
<bodyText confidence="0.865332133333333">
ment data, measured against the “true” German&apos;
ordering that was derived from automatic align-
ments to known English translations. The table
evaluates three candidate orderings: the original
German, German reordered using the log-odds
initialized model, and German reordered using
the perceptron-learned model. In addition to the
BLEU score, the table shows bigram, trigram, and
4-gram precisions. The unigram precisions are al-
ways 100%, because the correct words are given.
served German&apos;. On the held out test set, the pre-
dicted German&apos; achieves a recall of only 21%, but
a precision of 64%. Thus, the learned model is
too conservative, but makes moderately good de-
cisions when it does reorder.
</bodyText>
<sectionHeader confidence="0.946202" genericHeader="method">
6 Reordering as Preprocessing
</sectionHeader>
<bodyText confidence="0.999930518518519">
This section describes experiments using the
model introduced in Section 2 and learned in Sec-
tion 5 to preprocess German sentences for trans-
lation into English. These experiments are similar
to those of Collins et al. (2005).
We used the model learned in Section 5 to gen-
erate a German&apos; ordering of the training, develop-
ment, and test sets. The training sentences are the
same that the model was trained on, and the devel-
opment set is the same that was used as the stop-
ping criterion for the perceptron. The test set was
unused in training.
We used the resulting German&apos; as the input to
the Moses training pipeline. That is, Moses re-
computed alignments of the German&apos; training data
to the English sentences using GIZA++, then con-
structed a phrase table. Moses used the develop-
ment data for minimum error-rate training (Och,
2003) of its small number of parameters. Finally,
Moses translated the test sentences, and we mea-
sured performance against the English reference
sentences. This is the standard Moses pipeline, ex-
cept German has been replaced by German&apos;.
Table 3 shows the results of translation, both
starting with unreordered German, and starting
with German&apos;, reordered using the learned Linear
Ordering Problems. Note that Moses may itself re-
</bodyText>
<page confidence="0.978423">
1012
</page>
<table confidence="0.9990548">
System Input Moses Reord. p1 p2 p3 p4 BLEU METEOR TER
baseline German Distance 59.6 31.4 18.8 11.6 25.27 54.03 60.60
German Lexical 60.0 32.0 19.3 12.1 25.55 54.18 59.76
German&apos; Distance 60.4 32.7 20.2 12.8 26.40 54.91 58.63
(a)+(b) German&apos; Lexical 59.9 32.4 20.0 12.8 26.44 54.61 59.23
</table>
<tableCaption confidence="0.996362">
Table 3: Machine translation performance of several systems, measured against a single English refer-
</tableCaption>
<bodyText confidence="0.993742372093023">
ence translation. The results vary both the preprocessing—either none, or reordered using the learned
Linear Ordering Problems—and the reordering model used in Moses. Performance is measured using
BLEU, METEOR (Lavie et al., 2004), and TER (Snover et al., 2006). (For TER, smaller values are
better.)
order whatever input that it receives, during trans-
lation into English. Thus, the results in the table
also vary the reordering model used in Moses, set
to either a single-parameter distance-based model,
or to the lexicalized bidirectional msd model. The
latter model has six parameters for each phrase
in the phrase table, corresponding to monotone,
swapped, or discontinuous ordering relative to the
previous phrase in either the source or target lan-
guage.
How should we understand the results? The
baseline system is Moses phrase-based translation
with no preprocessing and only a simple distance-
based reordering model. There are two ways to
improve this: (a) ask Moses to use the lexicalized
bidirectional msd reordering model that is pro-
vided with Moses and is integrated with the rest of
translation, or (b) keep the simple distance-based
model within Moses, but preprocess its training
and test data with our linear reordering model.
Note that the preprocessing in (b) will obviously
change the phrasal substrings that are learned by
Moses, for better or for worse.
First, remarkably, (b) is significantly better than
(a) on BLEU, with p &lt; 0.0001 according to a
paired permutation test.
Second, combining (a) with (b) produced no im-
provement over (b) in BLEU score (the difference
between 26.40 and 26.44 is not significant, even
at p &lt; 0.2, according to the same paired per-
mutation test). Lexicalized reordering in Moses
even degraded translation performance according
to METEOR and TER. The TER change is sig-
nificant according to the paired permutation test at
p &lt; 0.001. (We did not perform a significance test
for METEOR.)
Our word-based model surpasses the lexical-
ized reordering in Moses largely because of long-
distance movement. The 518 sentences (26%) in
</bodyText>
<figure confidence="0.90341">
BLEU Improvement Aggregated by Amount of Reordering
0 10 20 30 40 50
Word Pairs Reordered
</figure>
<figureCaption confidence="0.849973">
Figure 4: Cumulative change in BLEU score of
(b) relative to the baseline and (a), aggregated by
</figureCaption>
<bodyText confidence="0.95689805">
the number of reordered word pairs in each sen-
tence. For those sentences where our model re-
orders fewer than five word pairs, the BLEU score
of translation degrades.
the test set for which our model moves a word
more than six words away from its starting posi-
tion account for more than 67% of the improve-
ment in BLEU from (a) to (b).
Figure 4 shows another view of the BLEU im-
provement. It shows that, compared to the base-
line, our preprocessing has basically no effect for
sentences where it does only a little reordering,
changing the relative order of fewer than five pairs
of words. Compared to Moses with lexicalized re-
ordering, these same sentences actually hurt per-
formance. This more than accounts for the differ-
ence between the BLEU scores of (b) and (a)+(b).
Going beyond preprocessing, our model could
also be integrated into a phrase-based decoder. We
briefly sketch that possibility here.
</bodyText>
<figure confidence="0.992605266666667">
−0.002 0.000 0.002 0.004 0.006 0.008 0.010
●●●●
●
●
●
Cumulative BLEU Change
●●●
●●●
●●
●
●●
●●
●
●●●
●
●
●●
●●
●
●
●
●
●●
●●●●●●
●
●●
●●●●
●
vs. baseline
vs. (a)
</figure>
<page confidence="0.986507">
1013
</page>
<bodyText confidence="0.9826044">
Phrase-based decoders keep a source coverage
vector with every partial translation hypothesis.
That coverage vector allows us to incorporate the
scores from a LOP matrix B directly. Whenever
the decoder extends the hypothesis with a new
source phrase, covering wi+1wi+2 ... wj, it adds
The first term represents the phrase-internal score,
and the second the score of putting the words in the
phrase before all the remaining uncovered words
U.
</bodyText>
<sectionHeader confidence="0.844937" genericHeader="method">
7 Comparison to Prior Work
</sectionHeader>
<bodyText confidence="0.999962075">
Preprocessing the source language to improve
translation is a common technique. Xia and Mc-
Cord (2004) improved English-French translation
using syntactic rewrite rules derived from Slot
Grammar parses. Collins et al. (2005) reported
an improvement from 25.2% to 26.8% BLEU
on German-English translation using six hand-
written rules to reorder the German sentences
based on automatically-generated phrase-structure
trees. Our work differs from these approaches in
providing an explicit model that scores all pos-
sible reorderings. In this paper, our model was
trained and used only for 1-best preprocessing, but
it could potentially be integrated into decoding as
well, where it would work together with the trans-
lation model and target language model to find a
congenial translation.
Costa-juss`a and Fonollosa (2006) improved
Spanish-English and Chinese-English translation
using a two-step process, first reordering the
source language, then translating it, both using dif-
ferent versions of a phrase-based translation sys-
tem. Many others have proposed more explicit
reordering models (Tillmann, 2004; Kumar and
Byrne, 2005; Koehn et al., 2005; Al-Onaizan and
Papineni, 2006). The primary advantage of our
model is that it directly accounts for interactions
between distant words, leading to better treatment
of long-distance movement.
Xiong et al. (2006) proposed a constituent
reordering model for a bracketing transduction
grammar (BTG) (Wu, 1995), which predicts the
probability that a pair of subconstituents will re-
order when combined to form a new constituent.
The features of their model look only at the first
source and target word of each constituent, mak-
ing it something like a sparse version of our model.
However, because of the target word features, their
reordering model cannot be separated from their
translation model.
</bodyText>
<sectionHeader confidence="0.987637" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99997475">
We have presented an entirely new model of re-
ordering for statistical machine translation, based
on the Linear Ordering Problem, and shown that
it can substantially improve translation from Ger-
man to English.
The model is demonstrably useful in this pre-
processing setting—which means that it can be
very simply added as a preprocessing step to any
MT system. German-to-English is a particularly
attractive use case, because the word orders are
sufficiently different as to require a good reorder-
ing model that requires long-distance reordering.
Our preprocessing here gave us a BLEU gain
of 0.9 point over the best Moses-based result.
English-to-German would obviously be another
potential win, as would translating between En-
glish and Japanese, for example.
As mentioned in Section 6, our model could
also be integrated into a phrase-based, or a syntax-
based decoder. That possibility remains future
work, but it is likely to lead to further improve-
ments, because it allows the translation system to
consider multiple possible reorderings under the
model, as well as to tune the weight of the model
relative to the other parts of the system during
MERT.
Tromble (2009) covers this integration in more
detail, and proposes several other ways of integrat-
ing our reordering model into machine translation.
It also experiments with numerous other param-
eter estimation procedures, including some that
use the probabilistic interpretation of our model
from (3). It presents numerous additional neigh-
borhoods for search in the Linear Ordering Prob-
lem.
We mentioned several possible extensions to the
model, such as going beyond the scoring model
of equation (2), or considering syntax-based fea-
tures. Another extension would try to reorder not
words but phrases, following (Xiong et al., 2006),
or segment choice models (Kuhn et al., 2006),
which assume a single segmentation of the words
into phrases. We would have to define the pair-
wise preference matrix B over phrases rather than
</bodyText>
<equation confidence="0.389823">
j−1 L j B[E, r] + L j L B[E, r].
`=i+1 r=`+1 `=i+1 r∈U
</equation>
<page confidence="0.984571">
1014
</page>
<bodyText confidence="0.99986744">
words (Eisner and Tromble, 2006). This would
have the disadvantage of complicating the feature
space, but might be a better fit for integration with
a phrase-based decoder.
Finally, we gave a novel algorithm for ap-
proximately solving the Linear Ordering Prob-
lem, interestingly combining dynamic program-
ming with local search. Another novel contri-
bution is that we showed how to parameterize a
function that constructs a specific Linear Order-
ing Problem instance from an input sentence w,
and showed how to learn those parameters from
a corpus of parallel sentences, using the percep-
tron algorithm. Likelihood-based training using
equation (3) would also be possible, with modifi-
cations to our algorithm, notably the use of normal
forms to avoid counting some permutations multi-
ple times (Tromble, 2009).
It would be interesting to compare the speed
and accuracy of our dynamic-programming local-
search method with an exact algorithm for solving
the LOP, such as integer linear programming with
branch and bound (cf. Charon and Hudry (2006)).
Exact solutions can generally be found in practice
for n ≤ 100.
</bodyText>
<sectionHeader confidence="0.997478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997743631578948">
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
COLING-ACL, pages 529–536, Sydney, July.
Ir`ene Charon and Olivier Hudry. 2006. A branch-and-
bound algorithm to solve the linear ordering problem
for weighted tournaments. Discrete Applied Mathe-
matics, 154(15):2097–2116, October.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In ACL, pages 531–540, Ann Arbor,
Michigan, June.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In EMNLP,
pages 1–8, Philadelphia, July.
Marta R. Costa-juss`a and Jos´e A. R. Fonollosa. 2006.
Statistical machine reordering. In EMNLP, pages
70–76, Sydney, July.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL,
pages 17–24, Prague, June.
Jason Eisner and Roy W. Tromble. 2006. Local search
with very large-scale neighborhoods for optimal per-
mutations in machine translation. In Workshop on
computationally hard problems and joint inference
in speech and language processing, New York, June.
Yoav Freund and Robert E. Schapire. 1998. Large
margin classification using the perceptron algorithm.
In COLT, pages 209–217, New York. ACM Press.
Martin Gr¨otschel, Michael J¨unger, and Gerhard
Reinelt. 1984. A cutting plane algorithm for
the linear ordering problem. Operations Research,
32(6):1195–1220, November–December.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In IWSLT, Pittsburgh, October.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In ACL Demo and Poster Sessions, pages 177–
180, Prague, June.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In MT Summit
X, pages 79–86, Phuket, Thailand, September.
Roland Kuhn, Denis Yuen, Michel Simard, Patrick
Paul, George Foster, Eric Joanis, and Howard John-
son. 2006. Segment choice models: Feature-rich
models for global distortion in statistical machine
translation. In HLT-NAACL, pages 25–32, New
York, June.
Shankar Kumar and William Byrne. 2005. Lo-
cal phrase reordering models for statistical machine
translation. In HLT-EMNLP, pages 161–168, Van-
couver, October.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-
man. 2004. The signicance of recall in automatic
metrics for MT evaluation. In Robert E. Frederking
and Kathryn B. Taylor, editors, Machine Transla-
tion: From Real Users to Research, pages 134–143.
AMTA, Springer, September–October.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL, pages 104–
111, New York, June.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Spanning tree methods for discrim-
inative training of dependency parsers. Technical
Report MS-CIS-05-11, UPenn CIS.
Sounaka Mishra and Kripasindhu Sikdar. 2004. On
approximability of linear ordering and related NP-
optimization problems on graphs. Discrete Applied
Mathematics, 136(2–3):249–269, February.
</reference>
<page confidence="0.798849">
1015
</page>
<reference confidence="0.999778304347826">
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51,
March.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160–
167, Sapporo, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL, pages
311–318, Philadelphia, July.
Tommaso Schiavinotto and Thomas St¨utzle. 2004.
The linear ordering problem: Instances, search
space analysis and algorithms. Journal of Math-
ematical Modelling and Algorithms, 3(4):367–402,
December.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In HLT-
NAACL Short Papers, pages 101–104, Boston, May.
Roy Wesley Tromble. 2009. Search and Learning for
the Linear Ordering Problem with an Application
to Machine Translation. Ph.D. thesis, Johns Hop-
kins University, Baltimore, April. http://nlp.
cs.jhu.edu/˜royt/
Dekai Wu. 1995. An algorithm for simultaneously
bracketing parallel texts by aligning words. In ACL,
pages 244–251, Cambridge, Massachusetts, June.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404, Septem-
ber.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In COLING, pages 508–514,
Geneva, August.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In COLING-ACL,
pages 521–528, Sydney, July.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In ACL, pages 144–151, Sapporo, July.
</reference>
<page confidence="0.992979">
1016
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.192802">
<title confidence="0.989304">Linear Ordering Problems for Better</title>
<address confidence="0.75022575">Roy Google, 4720 Forbes Pittsburgh, PA</address>
<email confidence="0.999458">royt@google.com</email>
<author confidence="0.996722">Jason</author>
<affiliation confidence="0.7807855">Department of Computer Johns Hopkins</affiliation>
<address confidence="0.949979">Baltimore, MD</address>
<email confidence="0.999432">jason@cs.jhu.edu</email>
<abstract confidence="0.99971252">We apply machine learning to the Linear Ordering Problem in order to learn sentence-specific reordering models for machine translation. We demonstrate that even when these models are used as a mere preprocessing step for German-English translation, they significantly outperform Moses’ integrated lexicalized reordering model. Our models are trained on automatically aligned bitext. Their form is simple but novel. They assess, based on features of the input sentence, how strongly each pair input word tokens like to reverse their relative order. Combining all these pairwise preferences to find the best global reordering is NP-hard. Howwe present a non-trivial algorithm, based on chart parsing, that at least finds the best reordering within a certain exponentially large neighborhood. We show how to iterate this reordering process within a local search algorithm, which we use in training.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kishore Papineni</author>
</authors>
<title>Distortion models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>529--536</pages>
<location>Sydney,</location>
<contexts>
<context position="2475" citStr="Al-Onaizan and Papineni (2006)" startWordPosition="376" endWordPosition="379">ely heavily on the target language model to ensure a fluent output order. However, a target n-gram language model alone is known to be inadequate. Thus, translation systems should also look at how the source sentence prefers to reorder. Yet past systems have traditionally used rather weak models of the reordering process. They may look only at the distance between neighboring phrases, or depend only on phrase unigrams. The decoders also rely on search error, in the form of limited reordering windows, for both efficiency and translation quality. Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni (2006) showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score (Papineni et al., 2002) of at best 69%—and that only when restricted to keep most words very close to their source positions. This paper introduces a more sophisticated model of reordering based on the Linear Ordering Problem (LOP), itself an NP-hard permutation problem. We apply machine learning, in the form of a modified perceptron algorithm, to learn parameters of</context>
<context position="17486" citStr="Al-Onaizan and Papineni (2006)" startWordPosition="2992" endWordPosition="2995">ignments using the “grow-diag-final-and” procedure provided with Moses (Koehn et al., 2007). For each of these German sentences, we derived the English-like reordering of it, which we call German&apos;, by the following procedure. Each German token was assigned an integer key, namely the position of the leftmost of the English tokens to which it was aligned, or 0 if it was not aligned to any English tokens. We then did a stable sort of the German tokens based on these keys, meaning that if two German tokens had the same key, their order was preserved. This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. They kept unaligned words with the closest preceding aligned word.4 Having found the German&apos; corresponding to each German sentence, we randomly divided the sentences into 2,000 each for development and evaluation, and the remaining approximately 747,000 for training. We used the averaged perceptron algorithm (Freund and Schapire, 1998; Collins, 2002) to train the parameters of the model. We ran the algorithm multiple times over the training sentences, 4We tried two other methods for deriving English word order from word alignments. The first al</context>
<context position="28798" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="4918" endWordPosition="4921">ur model was trained and used only for 1-best preprocessing, but it could potentially be integrated into decoding as well, where it would work together with the translation model and target language model to find a congenial translation. Costa-juss`a and Fonollosa (2006) improved Spanish-English and Chinese-English translation using a two-step process, first reordering the source language, then translating it, both using different versions of a phrase-based translation system. Many others have proposed more explicit reordering models (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005; Al-Onaizan and Papineni, 2006). The primary advantage of our model is that it directly accounts for interactions between distant words, leading to better treatment of long-distance movement. Xiong et al. (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. The features of their model look only at the first source and target word of each constituent, making it something like a sparse version of our model. However, because of the target word features, their reor</context>
</contexts>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In COLING-ACL, pages 529–536, Sydney, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ir`ene Charon</author>
<author>Olivier Hudry</author>
</authors>
<title>A branch-andbound algorithm to solve the linear ordering problem for weighted tournaments.</title>
<date>2006</date>
<journal>Discrete Applied Mathematics,</journal>
<volume>154</volume>
<issue>15</issue>
<marker>Charon, Hudry, 2006</marker>
<rawString>Ir`ene Charon and Olivier Hudry. 2006. A branch-andbound algorithm to solve the linear ordering problem for weighted tournaments. Discrete Applied Mathematics, 154(15):2097–2116, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>531--540</pages>
<location>Ann Arbor, Michigan,</location>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In ACL, pages 531–540, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<pages>1--8</pages>
<location>Philadelphia,</location>
<contexts>
<context position="17888" citStr="Collins, 2002" startWordPosition="3052" endWordPosition="3053">a stable sort of the German tokens based on these keys, meaning that if two German tokens had the same key, their order was preserved. This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. They kept unaligned words with the closest preceding aligned word.4 Having found the German&apos; corresponding to each German sentence, we randomly divided the sentences into 2,000 each for development and evaluation, and the remaining approximately 747,000 for training. We used the averaged perceptron algorithm (Freund and Schapire, 1998; Collins, 2002) to train the parameters of the model. We ran the algorithm multiple times over the training sentences, 4We tried two other methods for deriving English word order from word alignments. The first alternative was to align only in one direction, from English to German, with null alignments disallowed, so that every German word was aligned to a single English word. The second alternative used BerkeleyAligner (Liang et al., 2006; DeNero and Klein, 2007), which shares information between the two alignment directions to improve alignment quality. Neither alternative produced improvements in our ulti</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In EMNLP, pages 1–8, Philadelphia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta R Costa-juss`a</author>
<author>Jos´e A R Fonollosa</author>
</authors>
<title>Statistical machine reordering.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<pages>70--76</pages>
<location>Sydney,</location>
<marker>Costa-juss`a, Fonollosa, 2006</marker>
<rawString>Marta R. Costa-juss`a and Jos´e A. R. Fonollosa. 2006. Statistical machine reordering. In EMNLP, pages 70–76, Sydney, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Tailoring word alignments to syntactic machine translation.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>17--24</pages>
<location>Prague,</location>
<contexts>
<context position="18341" citStr="DeNero and Klein, 2007" startWordPosition="3124" endWordPosition="3127">for development and evaluation, and the remaining approximately 747,000 for training. We used the averaged perceptron algorithm (Freund and Schapire, 1998; Collins, 2002) to train the parameters of the model. We ran the algorithm multiple times over the training sentences, 4We tried two other methods for deriving English word order from word alignments. The first alternative was to align only in one direction, from English to German, with null alignments disallowed, so that every German word was aligned to a single English word. The second alternative used BerkeleyAligner (Liang et al., 2006; DeNero and Klein, 2007), which shares information between the two alignment directions to improve alignment quality. Neither alternative produced improvements in our ultimate translation quality. measuring the quality of the learned parameters by reordering the held-out development set after each iteration. We stopped when the BLEU score on the development set failed to improve for two consecutive iterations, which occurred after fourteen passes over the data. Each perceptron update should compare the true German&apos; to the German&apos; that would be predicted by the model (2). As the latter is NP-hard to find, we instead s</context>
</contexts>
<marker>DeNero, Klein, 2007</marker>
<rawString>John DeNero and Dan Klein. 2007. Tailoring word alignments to syntactic machine translation. In ACL, pages 17–24, Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Roy W Tromble</author>
</authors>
<title>Local search with very large-scale neighborhoods for optimal permutations in machine translation. In Workshop on computationally hard problems and joint inference in speech and language processing,</title>
<date>2006</date>
<location>New York,</location>
<contexts>
<context position="1670" citStr="Eisner and Tromble, 2006" startWordPosition="250" endWordPosition="253">ally large neighborhood. We show how to iterate this reordering process within a local search algorithm, which we use in training. 1 Introduction Machine translation is an important but difficult problem. One of the properties that makes it difficult is the fact that different languages express the same concepts in different orders. A machine translation system must therefore rearrange the source language concepts to produce a fluent translation in the target language. 1This work is excerpted and adapted from the first author’s Ph.D. thesis (Tromble, 2009). Some of the ideas here appeared in (Eisner and Tromble, 2006) without empirical validation. The material is based in part upon work supported by the National Science Foundation under Grant No. 0347822. Phrase-based translation systems rely heavily on the target language model to ensure a fluent output order. However, a target n-gram language model alone is known to be inadequate. Thus, translation systems should also look at how the source sentence prefers to reorder. Yet past systems have traditionally used rather weak models of the reordering process. They may look only at the distance between neighboring phrases, or depend only on phrase unigrams. Th</context>
<context position="7097" citStr="Eisner and Tromble, 2006" startWordPosition="1148" endWordPosition="1152">, whereas 7i, 7j, E, and r denote particular input tokens such as Katze2 and Frau5. 2.2 Discussion To the extent that the costs B generally discourage reordering, they will particularly discourage long-distance movement, as it swaps more pairs of words. We point out that our model is somewhat peculiar, since it does not directly consider whether the permutation π keeps die and Frau5 adjacent or even close together, but only whether their order is reversed. Of course, the model could be extended to consider adjacency, or more generally, the three-way cost of interposing k between i and j. See (Eisner and Tromble, 2006; Tromble, 2009) for such extensions and associated algorithms. However, in the present paper we focus on the model in the simple form (2) that only considers pairwise reordering costs for all pairs in the sentence. Our goal is to show that these unfamiliar pairwise reordering costs are useful, when modeled with a rich feature set via equation (1). Even in isolation (as a preprocessing step), without considering any other kinds of reordering costs or language model, they can achieve useful reorderings 1For any f &lt; r, we may assume without loss of generality that B[r, f] = 0, since if not, subt</context>
<context position="31561" citStr="Eisner and Tromble, 2006" startWordPosition="5363" endWordPosition="5366"> from (3). It presents numerous additional neighborhoods for search in the Linear Ordering Problem. We mentioned several possible extensions to the model, such as going beyond the scoring model of equation (2), or considering syntax-based features. Another extension would try to reorder not words but phrases, following (Xiong et al., 2006), or segment choice models (Kuhn et al., 2006), which assume a single segmentation of the words into phrases. We would have to define the pairwise preference matrix B over phrases rather than j−1 L j B[E, r] + L j L B[E, r]. `=i+1 r=`+1 `=i+1 r∈U 1014 words (Eisner and Tromble, 2006). This would have the disadvantage of complicating the feature space, but might be a better fit for integration with a phrase-based decoder. Finally, we gave a novel algorithm for approximately solving the Linear Ordering Problem, interestingly combining dynamic programming with local search. Another novel contribution is that we showed how to parameterize a function that constructs a specific Linear Ordering Problem instance from an input sentence w, and showed how to learn those parameters from a corpus of parallel sentences, using the perceptron algorithm. Likelihood-based training using eq</context>
</contexts>
<marker>Eisner, Tromble, 2006</marker>
<rawString>Jason Eisner and Roy W. Tromble. 2006. Local search with very large-scale neighborhoods for optimal permutations in machine translation. In Workshop on computationally hard problems and joint inference in speech and language processing, New York, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1998</date>
<booktitle>In COLT,</booktitle>
<pages>209--217</pages>
<publisher>ACM Press.</publisher>
<location>New York.</location>
<contexts>
<context position="17872" citStr="Freund and Schapire, 1998" startWordPosition="3048" endWordPosition="3051">nglish tokens. We then did a stable sort of the German tokens based on these keys, meaning that if two German tokens had the same key, their order was preserved. This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. They kept unaligned words with the closest preceding aligned word.4 Having found the German&apos; corresponding to each German sentence, we randomly divided the sentences into 2,000 each for development and evaluation, and the remaining approximately 747,000 for training. We used the averaged perceptron algorithm (Freund and Schapire, 1998; Collins, 2002) to train the parameters of the model. We ran the algorithm multiple times over the training sentences, 4We tried two other methods for deriving English word order from word alignments. The first alternative was to align only in one direction, from English to German, with null alignments disallowed, so that every German word was aligned to a single English word. The second alternative used BerkeleyAligner (Liang et al., 2006; DeNero and Klein, 2007), which shares information between the two alignment directions to improve alignment quality. Neither alternative produced improvem</context>
</contexts>
<marker>Freund, Schapire, 1998</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1998. Large margin classification using the perceptron algorithm. In COLT, pages 209–217, New York. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Gr¨otschel</author>
<author>Michael J¨unger</author>
<author>Gerhard Reinelt</author>
</authors>
<title>A cutting plane algorithm for the linear ordering problem.</title>
<date>1984</date>
<journal>Operations Research,</journal>
<volume>32</volume>
<issue>6</issue>
<pages>November–December.</pages>
<marker>Gr¨otschel, J¨unger, Reinelt, 1984</marker>
<rawString>Martin Gr¨otschel, Michael J¨unger, and Gerhard Reinelt. 1984. A cutting plane algorithm for the linear ordering problem. Operations Research, 32(6):1195–1220, November–December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In IWSLT,</booktitle>
<location>Pittsburgh,</location>
<contexts>
<context position="28766" citStr="Koehn et al., 2005" startWordPosition="4914" endWordPosition="4917">gs. In this paper, our model was trained and used only for 1-best preprocessing, but it could potentially be integrated into decoding as well, where it would work together with the translation model and target language model to find a congenial translation. Costa-juss`a and Fonollosa (2006) improved Spanish-English and Chinese-English translation using a two-step process, first reordering the source language, then translating it, both using different versions of a phrase-based translation system. Many others have proposed more explicit reordering models (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005; Al-Onaizan and Papineni, 2006). The primary advantage of our model is that it directly accounts for interactions between distant words, leading to better treatment of long-distance movement. Xiong et al. (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. The features of their model look only at the first source and target word of each constituent, making it something like a sparse version of our model. However, because of the </context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In IWSLT, Pittsburgh, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="16947" citStr="Koehn et al., 2007" startWordPosition="2895" endWordPosition="2898">1 do 13: Δ[i, j, k] ← Δ[i, j, k − 1] + Δ[i + 1, j, k] − Δ[i + 1, j, k − 1] + B[Irk, Iri+1] − B[Iri+1, Irk] 14: 3[i, k] ← max(3[i, k], 3[i, j] + 3[j, k] + max(0, Δ[i, j, k])) 15: end for 16: end for 17: end for 18: return 3[0, n] 19: end procedure Figure 3: Pseudocode for computing the score of the best permutation in the neighborhood of π under the Linear Ordering Problem specified by the matrix B. Computing the best neighbor is a simple matter of keeping back pointers to the choices of max and ordering them as implied. alignments using the “grow-diag-final-and” procedure provided with Moses (Koehn et al., 2007). For each of these German sentences, we derived the English-like reordering of it, which we call German&apos;, by the following procedure. Each German token was assigned an integer key, namely the position of the leftmost of the English tokens to which it was aligned, or 0 if it was not aligned to any English tokens. We then did a stable sort of the German tokens based on these keys, meaning that if two German tokens had the same key, their order was preserved. This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. They kept u</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL Demo and Poster Sessions, pages 177– 180, Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit X,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand,</location>
<contexts>
<context position="15865" citStr="Koehn, 2005" startWordPosition="2651" endWordPosition="2652">s permuted into target English word order—from which to train the parameters of our model. Unfortunately, the alignments between German and English sentences are only infrequently one-to-one. Furthermore, human-aligned parallel sentences are hard to come by, and never in the quantity we would like. Instead, we make do with automaticallygenerated word alignments, and we heuristically derive an English-like word order for the German sentence based on the alignment. We used GIZA++ (Och and Ney, 2003) to align approximately 751,000 sentences from the German-English portion of the Europarl corpus (Koehn, 2005), in both the German-to-English and English-to-German directions. We combined the S1,3 ✟✟ ❍❍ S1,2 die4 S3,5 ✟✟❍❍ S3,4 gekauft6 S4,5 hat3 S2,3 Frau5 S5,6 Katze2 S0,1 die1 Di,j,k = � j k P=i+1 r=j+1 1010 1: procedure LOCALSEARCHSTEP(B, π, n) 2: for i ← 0 to n − 1 do 3: 3[i, i + 1] ← 0 4: fork ← i + 1 to n do 5: Δ[i, i, k] ← Δ[i, k, k] ← 0 6: end for 7: end for 8: for w ← 2 to n do 9: for i ← 0 to n − w do 10: k ← i + w 11: 3[i, k] ← −∞ 12: for j ← i + 1 to k − 1 do 13: Δ[i, j, k] ← Δ[i, j, k − 1] + Δ[i + 1, j, k] − Δ[i + 1, j, k − 1] + B[Irk, Iri+1] − B[Iri+1, Irk] 14: 3[i, k] ← max(3[i, k], 3[i</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit X, pages 79–86, Phuket, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Kuhn</author>
<author>Denis Yuen</author>
<author>Michel Simard</author>
<author>Patrick Paul</author>
<author>George Foster</author>
<author>Eric Joanis</author>
<author>Howard Johnson</author>
</authors>
<title>Segment choice models: Feature-rich models for global distortion in statistical machine translation.</title>
<date>2006</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>25--32</pages>
<location>New York,</location>
<contexts>
<context position="31323" citStr="Kuhn et al., 2006" startWordPosition="5317" endWordPosition="5320">roposes several other ways of integrating our reordering model into machine translation. It also experiments with numerous other parameter estimation procedures, including some that use the probabilistic interpretation of our model from (3). It presents numerous additional neighborhoods for search in the Linear Ordering Problem. We mentioned several possible extensions to the model, such as going beyond the scoring model of equation (2), or considering syntax-based features. Another extension would try to reorder not words but phrases, following (Xiong et al., 2006), or segment choice models (Kuhn et al., 2006), which assume a single segmentation of the words into phrases. We would have to define the pairwise preference matrix B over phrases rather than j−1 L j B[E, r] + L j L B[E, r]. `=i+1 r=`+1 `=i+1 r∈U 1014 words (Eisner and Tromble, 2006). This would have the disadvantage of complicating the feature space, but might be a better fit for integration with a phrase-based decoder. Finally, we gave a novel algorithm for approximately solving the Linear Ordering Problem, interestingly combining dynamic programming with local search. Another novel contribution is that we showed how to parameterize a f</context>
</contexts>
<marker>Kuhn, Yuen, Simard, Paul, Foster, Joanis, Johnson, 2006</marker>
<rawString>Roland Kuhn, Denis Yuen, Michel Simard, Patrick Paul, George Foster, Eric Joanis, and Howard Johnson. 2006. Segment choice models: Feature-rich models for global distortion in statistical machine translation. In HLT-NAACL, pages 25–32, New York, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In HLT-EMNLP,</booktitle>
<pages>161--168</pages>
<location>Vancouver,</location>
<contexts>
<context position="28746" citStr="Kumar and Byrne, 2005" startWordPosition="4910" endWordPosition="4913"> all possible reorderings. In this paper, our model was trained and used only for 1-best preprocessing, but it could potentially be integrated into decoding as well, where it would work together with the translation model and target language model to find a congenial translation. Costa-juss`a and Fonollosa (2006) improved Spanish-English and Chinese-English translation using a two-step process, first reordering the source language, then translating it, both using different versions of a phrase-based translation system. Many others have proposed more explicit reordering models (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005; Al-Onaizan and Papineni, 2006). The primary advantage of our model is that it directly accounts for interactions between distant words, leading to better treatment of long-distance movement. Xiong et al. (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. The features of their model look only at the first source and target word of each constituent, making it something like a sparse version of our model. Howe</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Shankar Kumar and William Byrne. 2005. Local phrase reordering models for statistical machine translation. In HLT-EMNLP, pages 161–168, Vancouver, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Kenji Sagae</author>
<author>Shyamsundar Jayaraman</author>
</authors>
<title>The signicance of recall in automatic metrics for MT evaluation.</title>
<date>2004</date>
<booktitle>Machine Translation: From Real Users to Research,</booktitle>
<pages>134--143</pages>
<editor>In Robert E. Frederking and Kathryn B. Taylor, editors,</editor>
<publisher>AMTA, Springer, September–October.</publisher>
<contexts>
<context position="24056" citStr="Lavie et al., 2004" startWordPosition="4143" endWordPosition="4146">2 System Input Moses Reord. p1 p2 p3 p4 BLEU METEOR TER baseline German Distance 59.6 31.4 18.8 11.6 25.27 54.03 60.60 German Lexical 60.0 32.0 19.3 12.1 25.55 54.18 59.76 German&apos; Distance 60.4 32.7 20.2 12.8 26.40 54.91 58.63 (a)+(b) German&apos; Lexical 59.9 32.4 20.0 12.8 26.44 54.61 59.23 Table 3: Machine translation performance of several systems, measured against a single English reference translation. The results vary both the preprocessing—either none, or reordered using the learned Linear Ordering Problems—and the reordering model used in Moses. Performance is measured using BLEU, METEOR (Lavie et al., 2004), and TER (Snover et al., 2006). (For TER, smaller values are better.) order whatever input that it receives, during translation into English. Thus, the results in the table also vary the reordering model used in Moses, set to either a single-parameter distance-based model, or to the lexicalized bidirectional msd model. The latter model has six parameters for each phrase in the phrase table, corresponding to monotone, swapped, or discontinuous ordering relative to the previous phrase in either the source or target language. How should we understand the results? The baseline system is Moses phr</context>
</contexts>
<marker>Lavie, Sagae, Jayaraman, 2004</marker>
<rawString>Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman. 2004. The signicance of recall in automatic metrics for MT evaluation. In Robert E. Frederking and Kathryn B. Taylor, editors, Machine Translation: From Real Users to Research, pages 134–143. AMTA, Springer, September–October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>104--111</pages>
<location>New York,</location>
<contexts>
<context position="18316" citStr="Liang et al., 2006" startWordPosition="3120" endWordPosition="3123">ces into 2,000 each for development and evaluation, and the remaining approximately 747,000 for training. We used the averaged perceptron algorithm (Freund and Schapire, 1998; Collins, 2002) to train the parameters of the model. We ran the algorithm multiple times over the training sentences, 4We tried two other methods for deriving English word order from word alignments. The first alternative was to align only in one direction, from English to German, with null alignments disallowed, so that every German word was aligned to a single English word. The second alternative used BerkeleyAligner (Liang et al., 2006; DeNero and Klein, 2007), which shares information between the two alignment directions to improve alignment quality. Neither alternative produced improvements in our ultimate translation quality. measuring the quality of the learned parameters by reordering the held-out development set after each iteration. We stopped when the BLEU score on the development set failed to improve for two consecutive iterations, which occurred after fourteen passes over the data. Each perceptron update should compare the true German&apos; to the German&apos; that would be predicted by the model (2). As the latter is NP-h</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In HLT-NAACL, pages 104– 111, New York, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Spanning tree methods for discriminative training of dependency parsers.</title>
<date>2005</date>
<tech>Technical Report MS-CIS-05-11, UPenn CIS.</tech>
<contexts>
<context position="14216" citStr="McDonald et al. (2005)" startWordPosition="2374" endWordPosition="2377">ood) of the subsequence 7ri+17ri+1 ... 7rk.3 3The use of β is intended to suggest an analogy to inside probability—or more precisely, the Viterbi approximation to inside probability (since we are maximizing rather than summing over parses). The next two sections describe how to use our local search algorithm to discriminatively learn the weights of the parameters from Section 2, equation (1). 4 Features Our objective function (2) works only to the extent that we can derive a good pairwise preference matrix B.,,,. We do this by using a rich feature set in equation (1). We adapt the features of McDonald et al. (2005), introduced there for dependency parsing, to the task of machine translation reordering. Because both models construct features for pairs of words given the entire sentence, there is a close correspondence between the two tasks, although the output is quite different. Each feature φ(w, E, r) in equation (1) is a binary feature that fires when (w, E, r) has some conjunction of properties. The properties that are considered include the words wP and wr, the parts of speech of {wP−1, . . . , wr+11, and the distance r − E. Table 1 shows the feature templates. We also tried features based on a depe</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Spanning tree methods for discriminative training of dependency parsers. Technical Report MS-CIS-05-11, UPenn CIS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sounaka Mishra</author>
<author>Kripasindhu Sikdar</author>
</authors>
<title>On approximability of linear ordering and related NPoptimization problems on graphs.</title>
<date>2004</date>
<journal>Discrete Applied Mathematics,</journal>
<pages>136--2</pages>
<contexts>
<context position="9391" citStr="Mishra and Sikdar, 2004" startWordPosition="1522" endWordPosition="1525">optimization literature, the maximization problem (4) (with input B) is known as the Linear Ordering Problem. It has numerous practical applications in fields including economics, sociology, graph theory, graph drawing, archaeology, and task scheduling (Gr¨otschel et al., 1984). Computational studies on real data have often used “input-output” matrices representing resource flows among economic sectors (Schiavinotto and St¨utzle, 2004). Unfortunately, the problem is NP-hard. Furthermore, it is known to be APX-complete, meaning that there is no polynomial time approximation scheme unless P=NP (Mishra and Sikdar, 2004). However, there are various heuristic procedures for approximating it (Tromble, 2009). We now give an attractive, novel procedure, which uses a CKY-parsing-like algorithm to search various subsets of IIn in polynomial time. 3 Local Search “Local search” refers to any hill-climbing procedure that iteratively improves a solution by making an optimal “local” change at each iteration.2 In this case, we start with the identity permutation, find a “nearby” permutation with a better score (2), and repeat until we have reached a local maximum of the scoring objective. This section describes a local s</context>
</contexts>
<marker>Mishra, Sikdar, 2004</marker>
<rawString>Sounaka Mishra and Kripasindhu Sikdar. 2004. On approximability of linear ordering and related NPoptimization problems on graphs. Discrete Applied Mathematics, 136(2–3):249–269, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="15755" citStr="Och and Ney, 2003" startWordPosition="2634" endWordPosition="2637">rder Ideally, we would have a large corpus of desirable reorderings of source sentences—in our case, German sentences permuted into target English word order—from which to train the parameters of our model. Unfortunately, the alignments between German and English sentences are only infrequently one-to-one. Furthermore, human-aligned parallel sentences are hard to come by, and never in the quantity we would like. Instead, we make do with automaticallygenerated word alignments, and we heuristically derive an English-like word order for the German sentence based on the alignment. We used GIZA++ (Och and Ney, 2003) to align approximately 751,000 sentences from the German-English portion of the Europarl corpus (Koehn, 2005), in both the German-to-English and English-to-German directions. We combined the S1,3 ✟✟ ❍❍ S1,2 die4 S3,5 ✟✟❍❍ S3,4 gekauft6 S4,5 hat3 S2,3 Frau5 S5,6 Katze2 S0,1 die1 Di,j,k = � j k P=i+1 r=j+1 1010 1: procedure LOCALSEARCHSTEP(B, π, n) 2: for i ← 0 to n − 1 do 3: 3[i, i + 1] ← 0 4: fork ← i + 1 to n do 5: Δ[i, i, k] ← Δ[i, k, k] ← 0 6: end for 7: end for 8: for w ← 2 to n do 9: for i ← 0 to n − w do 10: k ← i + w 11: 3[i, k] ← −∞ 12: for j ← i + 1 to k − 1 do 13: Δ[i, j, k] ← Δ[i, </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>160--167</pages>
<location>Sapporo,</location>
<contexts>
<context position="23011" citStr="Och, 2003" startWordPosition="3983" endWordPosition="3984">2005). We used the model learned in Section 5 to generate a German&apos; ordering of the training, development, and test sets. The training sentences are the same that the model was trained on, and the development set is the same that was used as the stopping criterion for the perceptron. The test set was unused in training. We used the resulting German&apos; as the input to the Moses training pipeline. That is, Moses recomputed alignments of the German&apos; training data to the English sentences using GIZA++, then constructed a phrase table. Moses used the development data for minimum error-rate training (Och, 2003) of its small number of parameters. Finally, Moses translated the test sentences, and we measured performance against the English reference sentences. This is the standard Moses pipeline, except German has been replaced by German&apos;. Table 3 shows the results of translation, both starting with unreordered German, and starting with German&apos;, reordered using the learned Linear Ordering Problems. Note that Moses may itself re1012 System Input Moses Reord. p1 p2 p3 p4 BLEU METEOR TER baseline German Distance 59.6 31.4 18.8 11.6 25.27 54.03 60.60 German Lexical 60.0 32.0 19.3 12.1 25.55 54.18 59.76 Ge</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL, pages 160– 167, Sapporo, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia,</location>
<contexts>
<context position="2729" citStr="Papineni et al., 2002" startWordPosition="417" endWordPosition="420">ditionally used rather weak models of the reordering process. They may look only at the distance between neighboring phrases, or depend only on phrase unigrams. The decoders also rely on search error, in the form of limited reordering windows, for both efficiency and translation quality. Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni (2006) showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score (Papineni et al., 2002) of at best 69%—and that only when restricted to keep most words very close to their source positions. This paper introduces a more sophisticated model of reordering based on the Linear Ordering Problem (LOP), itself an NP-hard permutation problem. We apply machine learning, in the form of a modified perceptron algorithm, to learn parameters of a linear model that constructs a matrix of weights from each source language sentence. We train the parameters on orderings derived from automatic word alignments of parallel sentences. The LOP model of reordering is a complete ordering model, capable o</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL, pages 311–318, Philadelphia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tommaso Schiavinotto</author>
<author>Thomas St¨utzle</author>
</authors>
<title>The linear ordering problem: Instances, search space analysis and algorithms.</title>
<date>2004</date>
<journal>Journal of Mathematical Modelling and Algorithms,</journal>
<volume>3</volume>
<issue>4</issue>
<marker>Schiavinotto, St¨utzle, 2004</marker>
<rawString>Tommaso Schiavinotto and Thomas St¨utzle. 2004. The linear ordering problem: Instances, search space analysis and algorithms. Journal of Mathematical Modelling and Algorithms, 3(4):367–402, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In AMTA.</booktitle>
<contexts>
<context position="24087" citStr="Snover et al., 2006" startWordPosition="4149" endWordPosition="4152"> p2 p3 p4 BLEU METEOR TER baseline German Distance 59.6 31.4 18.8 11.6 25.27 54.03 60.60 German Lexical 60.0 32.0 19.3 12.1 25.55 54.18 59.76 German&apos; Distance 60.4 32.7 20.2 12.8 26.40 54.91 58.63 (a)+(b) German&apos; Lexical 59.9 32.4 20.0 12.8 26.44 54.61 59.23 Table 3: Machine translation performance of several systems, measured against a single English reference translation. The results vary both the preprocessing—either none, or reordered using the learned Linear Ordering Problems—and the reordering model used in Moses. Performance is measured using BLEU, METEOR (Lavie et al., 2004), and TER (Snover et al., 2006). (For TER, smaller values are better.) order whatever input that it receives, during translation into English. Thus, the results in the table also vary the reordering model used in Moses, set to either a single-parameter distance-based model, or to the lexicalized bidirectional msd model. The latter model has six parameters for each phrase in the phrase table, corresponding to monotone, swapped, or discontinuous ordering relative to the previous phrase in either the source or target language. How should we understand the results? The baseline system is Moses phrase-based translation with no p</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLTNAACL Short Papers,</booktitle>
<pages>101--104</pages>
<location>Boston,</location>
<contexts>
<context position="28723" citStr="Tillmann, 2004" startWordPosition="4908" endWordPosition="4909">odel that scores all possible reorderings. In this paper, our model was trained and used only for 1-best preprocessing, but it could potentially be integrated into decoding as well, where it would work together with the translation model and target language model to find a congenial translation. Costa-juss`a and Fonollosa (2006) improved Spanish-English and Chinese-English translation using a two-step process, first reordering the source language, then translating it, both using different versions of a phrase-based translation system. Many others have proposed more explicit reordering models (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005; Al-Onaizan and Papineni, 2006). The primary advantage of our model is that it directly accounts for interactions between distant words, leading to better treatment of long-distance movement. Xiong et al. (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. The features of their model look only at the first source and target word of each constituent, making it something like a sparse ver</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A unigram orientation model for statistical machine translation. In HLTNAACL Short Papers, pages 101–104, Boston, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Wesley Tromble</author>
</authors>
<title>Search and Learning for the Linear Ordering Problem with an Application to Machine Translation.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University,</institution>
<location>Baltimore,</location>
<note>http://nlp. cs.jhu.edu/˜royt/</note>
<contexts>
<context position="1607" citStr="Tromble, 2009" startWordPosition="241" endWordPosition="242">finds the best reordering within a certain exponentially large neighborhood. We show how to iterate this reordering process within a local search algorithm, which we use in training. 1 Introduction Machine translation is an important but difficult problem. One of the properties that makes it difficult is the fact that different languages express the same concepts in different orders. A machine translation system must therefore rearrange the source language concepts to produce a fluent translation in the target language. 1This work is excerpted and adapted from the first author’s Ph.D. thesis (Tromble, 2009). Some of the ideas here appeared in (Eisner and Tromble, 2006) without empirical validation. The material is based in part upon work supported by the National Science Foundation under Grant No. 0347822. Phrase-based translation systems rely heavily on the target language model to ensure a fluent output order. However, a target n-gram language model alone is known to be inadequate. Thus, translation systems should also look at how the source sentence prefers to reorder. Yet past systems have traditionally used rather weak models of the reordering process. They may look only at the distance bet</context>
<context position="7113" citStr="Tromble, 2009" startWordPosition="1153" endWordPosition="1154"> denote particular input tokens such as Katze2 and Frau5. 2.2 Discussion To the extent that the costs B generally discourage reordering, they will particularly discourage long-distance movement, as it swaps more pairs of words. We point out that our model is somewhat peculiar, since it does not directly consider whether the permutation π keeps die and Frau5 adjacent or even close together, but only whether their order is reversed. Of course, the model could be extended to consider adjacency, or more generally, the three-way cost of interposing k between i and j. See (Eisner and Tromble, 2006; Tromble, 2009) for such extensions and associated algorithms. However, in the present paper we focus on the model in the simple form (2) that only considers pairwise reordering costs for all pairs in the sentence. Our goal is to show that these unfamiliar pairwise reordering costs are useful, when modeled with a rich feature set via equation (1). Even in isolation (as a preprocessing step), without considering any other kinds of reordering costs or language model, they can achieve useful reorderings 1For any f &lt; r, we may assume without loss of generality that B[r, f] = 0, since if not, subtracting B[r, f] </context>
<context position="9477" citStr="Tromble, 2009" startWordPosition="1535" endWordPosition="1536">ring Problem. It has numerous practical applications in fields including economics, sociology, graph theory, graph drawing, archaeology, and task scheduling (Gr¨otschel et al., 1984). Computational studies on real data have often used “input-output” matrices representing resource flows among economic sectors (Schiavinotto and St¨utzle, 2004). Unfortunately, the problem is NP-hard. Furthermore, it is known to be APX-complete, meaning that there is no polynomial time approximation scheme unless P=NP (Mishra and Sikdar, 2004). However, there are various heuristic procedures for approximating it (Tromble, 2009). We now give an attractive, novel procedure, which uses a CKY-parsing-like algorithm to search various subsets of IIn in polynomial time. 3 Local Search “Local search” refers to any hill-climbing procedure that iteratively improves a solution by making an optimal “local” change at each iteration.2 In this case, we start with the identity permutation, find a “nearby” permutation with a better score (2), and repeat until we have reached a local maximum of the scoring objective. This section describes a local search procedure that uses a very generous definition of “local.” At each iteration, it</context>
<context position="30659" citStr="Tromble (2009)" startWordPosition="5217" endWordPosition="5218">ocessing here gave us a BLEU gain of 0.9 point over the best Moses-based result. English-to-German would obviously be another potential win, as would translating between English and Japanese, for example. As mentioned in Section 6, our model could also be integrated into a phrase-based, or a syntaxbased decoder. That possibility remains future work, but it is likely to lead to further improvements, because it allows the translation system to consider multiple possible reorderings under the model, as well as to tune the weight of the model relative to the other parts of the system during MERT. Tromble (2009) covers this integration in more detail, and proposes several other ways of integrating our reordering model into machine translation. It also experiments with numerous other parameter estimation procedures, including some that use the probabilistic interpretation of our model from (3). It presents numerous additional neighborhoods for search in the Linear Ordering Problem. We mentioned several possible extensions to the model, such as going beyond the scoring model of equation (2), or considering syntax-based features. Another extension would try to reorder not words but phrases, following (X</context>
</contexts>
<marker>Tromble, 2009</marker>
<rawString>Roy Wesley Tromble. 2009. Search and Learning for the Linear Ordering Problem with an Application to Machine Translation. Ph.D. thesis, Johns Hopkins University, Baltimore, April. http://nlp. cs.jhu.edu/˜royt/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>An algorithm for simultaneously bracketing parallel texts by aligning words.</title>
<date>1995</date>
<booktitle>In ACL,</booktitle>
<pages>244--251</pages>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="29073" citStr="Wu, 1995" startWordPosition="4960" endWordPosition="4961"> and Chinese-English translation using a two-step process, first reordering the source language, then translating it, both using different versions of a phrase-based translation system. Many others have proposed more explicit reordering models (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005; Al-Onaizan and Papineni, 2006). The primary advantage of our model is that it directly accounts for interactions between distant words, leading to better treatment of long-distance movement. Xiong et al. (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. The features of their model look only at the first source and target word of each constituent, making it something like a sparse version of our model. However, because of the target word features, their reordering model cannot be separated from their translation model. 8 Conclusions and Future Work We have presented an entirely new model of reordering for statistical machine translation, based on the Linear Ordering Problem, and shown that it can substantially improve translati</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. 1995. An algorithm for simultaneously bracketing parallel texts by aligning words. In ACL, pages 244–251, Cambridge, Massachusetts, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="10705" citStr="Wu, 1997" startWordPosition="1747" endWordPosition="1748">permutation in a certain exponentially large neighborhood N(π) of the current permutation π. 2One can introduce randomness to obtain MCMC sampling or simulated annealing algorithms. Our algorithms extend naturally to allow this (cf. Tromble (2009)). S — S0,n Si,k — Si,j Sj,k Si−1,i — 7ri Figure 1: A grammar for a large neighborhood of permutations, given one permutation π of length n. The Si,k rules are instantiated for each 0 &lt; i &lt; j &lt; k &lt; n, and the Si−1,i rules for each 0 &lt; i &lt; n. We say that two permutations are neighbors iff they can be aligned by an Inversion Transduction Grammar (ITG) (Wu, 1997), which is a familiar reordering device in machine translation. Equivalently, π&apos; E N(π) iff π can be transformed into π&apos; by swapping various adjacent substrings of π, as long as these swaps are properly nested. Zens and Ney (2003) used a normal form to show that the size of the ITG neighborhood N(π) is a large Schr¨oder number, which grows exponentially in n. Asymptotically, the ratio between the size of the neighborhood for n + 1 and the size for n approaches 3 + 2.\/2 pt� 5.8. We show that equation (2) can be optimized within N(π) in O(n3) time, using dynamic programming. The algorithm is ba</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In COLING,</booktitle>
<pages>508--514</pages>
<location>Geneva,</location>
<contexts>
<context position="27725" citStr="Xia and McCord (2004)" startWordPosition="4762" endWordPosition="4766">● ●● ●●●●●● ● ●● ●●●● ● vs. baseline vs. (a) 1013 Phrase-based decoders keep a source coverage vector with every partial translation hypothesis. That coverage vector allows us to incorporate the scores from a LOP matrix B directly. Whenever the decoder extends the hypothesis with a new source phrase, covering wi+1wi+2 ... wj, it adds The first term represents the phrase-internal score, and the second the score of putting the words in the phrase before all the remaining uncovered words U. 7 Comparison to Prior Work Preprocessing the source language to improve translation is a common technique. Xia and McCord (2004) improved English-French translation using syntactic rewrite rules derived from Slot Grammar parses. Collins et al. (2005) reported an improvement from 25.2% to 26.8% BLEU on German-English translation using six handwritten rules to reorder the German sentences based on automatically-generated phrase-structure trees. Our work differs from these approaches in providing an explicit model that scores all possible reorderings. In this paper, our model was trained and used only for 1-best preprocessing, but it could potentially be integrated into decoding as well, where it would work together with </context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In COLING, pages 508–514, Geneva, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>521--528</pages>
<location>Sydney,</location>
<contexts>
<context position="28978" citStr="Xiong et al. (2006)" startWordPosition="4945" endWordPosition="4948">anguage model to find a congenial translation. Costa-juss`a and Fonollosa (2006) improved Spanish-English and Chinese-English translation using a two-step process, first reordering the source language, then translating it, both using different versions of a phrase-based translation system. Many others have proposed more explicit reordering models (Tillmann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005; Al-Onaizan and Papineni, 2006). The primary advantage of our model is that it directly accounts for interactions between distant words, leading to better treatment of long-distance movement. Xiong et al. (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. The features of their model look only at the first source and target word of each constituent, making it something like a sparse version of our model. However, because of the target word features, their reordering model cannot be separated from their translation model. 8 Conclusions and Future Work We have presented an entirely new model of reordering for statistical machine translati</context>
<context position="31277" citStr="Xiong et al., 2006" startWordPosition="5309" endWordPosition="5312">) covers this integration in more detail, and proposes several other ways of integrating our reordering model into machine translation. It also experiments with numerous other parameter estimation procedures, including some that use the probabilistic interpretation of our model from (3). It presents numerous additional neighborhoods for search in the Linear Ordering Problem. We mentioned several possible extensions to the model, such as going beyond the scoring model of equation (2), or considering syntax-based features. Another extension would try to reorder not words but phrases, following (Xiong et al., 2006), or segment choice models (Kuhn et al., 2006), which assume a single segmentation of the words into phrases. We would have to define the pairwise preference matrix B over phrases rather than j−1 L j B[E, r] + L j L B[E, r]. `=i+1 r=`+1 `=i+1 r∈U 1014 words (Eisner and Tromble, 2006). This would have the disadvantage of complicating the feature space, but might be a better fit for integration with a phrase-based decoder. Finally, we gave a novel algorithm for approximately solving the Linear Ordering Problem, interestingly combining dynamic programming with local search. Another novel contribu</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In COLING-ACL, pages 521–528, Sydney, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>A comparative study on reordering constraints in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>144--151</pages>
<location>Sapporo,</location>
<contexts>
<context position="10935" citStr="Zens and Ney (2003)" startWordPosition="1785" endWordPosition="1788">ow this (cf. Tromble (2009)). S — S0,n Si,k — Si,j Sj,k Si−1,i — 7ri Figure 1: A grammar for a large neighborhood of permutations, given one permutation π of length n. The Si,k rules are instantiated for each 0 &lt; i &lt; j &lt; k &lt; n, and the Si−1,i rules for each 0 &lt; i &lt; n. We say that two permutations are neighbors iff they can be aligned by an Inversion Transduction Grammar (ITG) (Wu, 1997), which is a familiar reordering device in machine translation. Equivalently, π&apos; E N(π) iff π can be transformed into π&apos; by swapping various adjacent substrings of π, as long as these swaps are properly nested. Zens and Ney (2003) used a normal form to show that the size of the ITG neighborhood N(π) is a large Schr¨oder number, which grows exponentially in n. Asymptotically, the ratio between the size of the neighborhood for n + 1 and the size for n approaches 3 + 2.\/2 pt� 5.8. We show that equation (2) can be optimized within N(π) in O(n3) time, using dynamic programming. The algorithm is based on CKY parsing. However, a novelty is that the grammar weights must themselves be computed by O(n3) dynamic programming. Our grammar is shown in Figure 1. Parsing the “input sentence” π with this grammar simply constructs all </context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>Richard Zens and Hermann Ney. 2003. A comparative study on reordering constraints in statistical machine translation. In ACL, pages 144–151, Sapporo, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>