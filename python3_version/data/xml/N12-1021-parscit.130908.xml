<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006083">
<title confidence="0.9980375">
Towards Effective Tutorial Feedback for Explanation Questions:
A Dataset and Baselines
</title>
<author confidence="0.882001">
Myroslava O. Dzikovska* and Rodney D. Nielsen† and Chris Brew$
</author>
<affiliation confidence="0.990696">
*School of Informatics, University of Edinburgh, Edinburgh, EH8 9AB, UK
†Computational Language &amp; Education Research Center
University of Colorado at Boulder, Boulder, CO 80309-0594, USA
$Educational Testing Service, Princeton, NJ 08451, USA
</affiliation>
<email confidence="0.995657">
m.dzikovska@ed.ac.uk,rodney.nielsen@colorado.edu,cbrew@ets.org
</email>
<sectionHeader confidence="0.995599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998353588235294">
We propose a new shared task on grading stu-
dent answers with the goal of enabling well-
targeted and flexible feedback in a tutorial di-
alogue setting. We provide an annotated cor-
pus designed for the purpose, a precise speci-
fication for a prediction task and an associated
evaluation methodology. The task is feasible
but non-trivial, which is demonstrated by cre-
ating and comparing three alternative baseline
systems. We believe that this corpus will be
of interest to the researchers working in tex-
tual entailment and will stimulate new devel-
opments both in natural language processing
in tutorial dialogue systems and textual entail-
ment, contradiction detection and other tech-
niques of interest for a variety of computa-
tional linguistics tasks.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999866458333334">
In human-human tutoring, it is an effective strategy
to ask students to explain instructional material in
their own words. Self-explanation (Chi et al., 1994)
and contentful talk focused on the domain are cor-
related with better learning outcomes (Litman et al.,
2009; Chi et al., 1994). There has therefore been
much interest in developing automated tutorial dia-
logue systems that ask students open-ended expla-
nation questions (Graesser et al., 1999; Aleven et
al., 2001; Jordan et al., 2006; VanLehn et al., 2007;
Nielsen et al., 2009; Dzikovska et al., 2010a). In
order to do this well, it is not enough to simply
ask the initiating question, because students need
the experience of engaging in meaningful dialogue
about the instructional content. Thus, systems must
respond appropriately to student explanations, and
must provide detailed, flexible and appropriate feed-
back (Aleven et al., 2002; Jordan et al., 2004).
In simple domains, we can adopt a knowledge en-
gineering approach and build a domain model and a
diagnoser, together with a natural language parser to
produce detailed semantic representations of student
input (Glass, 2000; Aleven et al., 2002; Pon-Barry
et al., 2004; Callaway et al., 2006; Dzikovska et al.,
2010a). The advantage of this approach is that it
allows for flexible adaptation of feedback to a va-
riety of factors such as student performance. For
example, it is easy for the system to know if the
student made the same error before, and adjust its
feedback to reflect it. Moreover, this approach al-
lows for easy addition of new exercises : as long as
an exercise relies on the concepts covered by the do-
main model, the system can apply standard instruc-
tional strategies to each new question automatically.
However, this approach is significantly limited by
the requirement that the domain be small enough to
allow comprehensive knowledge engineering, and it
is very labor-intensive even for small domains.
Alternatively, we can adopt a data-driven ap-
proach, asking human tutors to anticipate in ad-
vance a range of possible correct and incorrect an-
swers, and associating each answer with an appro-
priate remediation (Graesser et al., 1999; Jordan et
al., 2004; VanLehn et al., 2007). The advantage
of this approach is that it allows more complex and
interesting domains and provides a good framework
for eliciting the necessary information from the hu-
man experts. A weakness of this approach, which
</bodyText>
<page confidence="0.642929">
200
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 200–210,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999605902439024">
also arises in content-scoring applications such as
ETS’s c-rater (Leacock and Chodorow, 2003), is that
human experts find it extremely difficult to predict
with any certainty what the full range of student re-
sponses will be. This leads to a lack of adaptivity
and generality – if the system designers have failed
to predict the full range of possibilities, students will
often receive the default feedback. It is frustrating
and confusing for students to repeatedly receive the
same feedback, regardless of their past performance
or dialogue context (Jordan, 2004).
Our goal is to address the weaknesses of the data-
driven approach by creating a framework for sup-
porting more flexible and systematic feedback. Our
approach identifies general classes of error, such as
omissions, incorrect statements and off-topic state-
ments, then aims to develop general remediation
strategies for each error type. This has the potential
to free system designers from the need to pre-author
separate remediations for each individual question.
A precondition for the success of this approach is
that the system be able to identify error types based
on the student response and the model answers.
A contribution of this paper is to provide a new
dataset that will enable researchers to develop clas-
sifiers specifically for this purpose. The hope is that
with an appropriate dataset the data-driven approach
will be flexible and responsive enough to maintain
student engagement. We provide a corpus that is la-
beled for a set of five student response types, develop
a precise definition of the corresponding supervised
classification task, and report results for a variety of
simple baseline classifiers. This will provide a ba-
sis for the development, comparison and evaluation
of alternative approaches to the error classification
task. We believe that the natural language capabil-
ities needed for this task will be directly applicable
to a far wider range of tasks in educational assess-
ment, information extraction and computational se-
mantics. This dataset is publicly available and will
be used in a community-wide shared task.
</bodyText>
<sectionHeader confidence="0.983623" genericHeader="introduction">
2 Corpus
</sectionHeader>
<bodyText confidence="0.999963307692308">
The data set we developed draws on two established
sources – a data set collected and annotated during
an evaluation of the BEETLE II tutorial dialogue sys-
tem (Dzikovska et al., 2010a) (henceforth, BEETLE
corpus) and a set of student answers to questions
from 16 science modules in the Assessing Science
Knowledge (ASK) assessment inventory (Lawrence
Hall of Science, 2006) (henceforth, the Science En-
tailments Bank or SCIENTSBANK).
In both corpora, each question was associated
with one or more reference answers provided by
the experts. Student answers were evaluated against
these reference answers and, using corpus-specific
annotation schemes, assigned labels for correct-
ness. In order to reconcile the two different schemes
and to cast the task in terms of standard supervised
machine classification at the sentence level, we de-
rived a new set of annotations, using the annotation
scheme shown in Figure 1.
Our label set has some similarity to the RTE5 3-
way task (Bentivogli et al., 2009), which used “en-
tailment”, “contradiction” and “unknown” labels.
The additional distinctions in our labels reflect typi-
cal distinctions made by tutorial dialogue systems.
They match our human tutors’ intuitions about
the general error types observed in student answers
and corresponding teaching tactics. For example,
a likely response to “partially correct incomplete”
would be to tell the student that what they said so far
was correct but it had some gaps, and to encourage
them to fill in those gaps. In contrast, the response
to “contradictory” would emphasize that there is a
mistake and the student needs to change their an-
swer rather than just expand it. Finally, the response
to “irrelevant” may encourage the student to address
relevant concepts. The “non domain” content could
be an indicator that the student is frustrated or con-
fused, and may require special attention.
The annotations in the source corpora make some
more fine-grained distinctions based on the needs of
the corresponding systems. In principle, it is possi-
ble to have answers that have both correct and con-
tradictory parts, and acknowledge correct parts be-
fore pointing out mistakes. There are also distinct
classes of “non domain” utterances, e.g., social and
metacognitive statements, to which an ITS may want
to react differently (described in Section 2.1). How-
ever, these situations were rare in our corpora, and
we decided to use a single class for all contradictory
answers and a single non-domain class. This may be
expanded in the future as more data becomes avail-
able for new versions of this challenge task.
</bodyText>
<page confidence="0.972044">
201
</page>
<bodyText confidence="0.993155857142857">
Label Definition
non domain does not contain domain content, e.g., a help request or “I don’t know”
correct the student answer is correct
partially correct incomplete the answer does not contradict the reference answer and includes some
contradictory correct nuggets, but parts are missing
irrelevant an answer that contradicts some part of the reference answer
contains domain content, but does not answer the question
</bodyText>
<figureCaption confidence="0.999712">
Figure 1: The set of answer labels used in our task
</figureCaption>
<bodyText confidence="0.9999804">
We further discuss the relationship with the task
of recognizing textual entailment in Section 5. In
the rest of this section, we describe our corpora and
discuss how we obtained these labels from the raw
data available in our datasets.
</bodyText>
<subsectionHeader confidence="0.957015">
2.1 BEETLE II data
</subsectionHeader>
<bodyText confidence="0.999944107692308">
The BEETLE corpus consists of the interactions be-
tween students and the BEETLE II tutorial dialogue
system (Dzikovska et al., 2010b). The BEETLE II
system is an intelligent tutoring system that teaches
students with no knowledge of high-school physics
concepts in basic electricity and electronics. In the
first system evaluation, students spend 3-5 hours go-
ing through prepared reading material, building and
observing circuits in the simulator and interacting
with a dialogue-based tutor. The interaction was
by keyboard, with the computer tutor asking ques-
tions, receiving replies and providing feedback via a
text-based chat interface. The data from 73 under-
graduate volunteer participants at southeastern US
university were recorded and annotated to form the
BEETLE human-computer dialogue corpus.
The BEETLE II lesson material contains two types
of questions. Factual questions require them to name
a set of objects or a simple property, e.g., “Which
components in circuit 1 are in a closed path?” or
“Are bulbs A and B wired in series or in parallel”.
Explanation and definition questions require longer
answers that consist of 1-2 sentences, e.g., “Why
was bulb A on when switch Z was open?” (expected
answer “Because it was still in a closed path with the
battery”) or “What is voltage?” (expected answer
“Voltage is the difference in states between two ter-
minals”). From the full BEETLE evaluation corpus,
we automatically extracted only the students’ an-
swers to explanation and definition questions, since
reacting to them appropriately requires processing
more complex input than factual questions.
The extracted answers were filtered to remove du-
plicates. In the BEETLE II lesson material there
are a number of similar questions and the tutor ef-
fectively had a template answer such as ”Terminal
X is connected to the negative/positive battery ter-
minal”. A number of students picked up on this
and used the same pattern in their responses (Stein-
hauser et al., 2011). This resulted in a number of an-
swers to certain questions that came from different
speakers but which were exact copies of each other.
We removed such answers from the data set, since
they were likely to be in both the training and test
set, thus inflating our results. Note that only exact
matches were removed: for example, answers that
were nearly identical but contained spelling errors
were retained, since they would need to be handled
in a practical system.
Student utterances were manually labeled using a
simplified version of the DEMAND coding scheme
(Campbell et al., 2009) shown in Figure 2. The utter-
ances were first classified as related to domain con-
tent, student’s metacognitive state, or social inter-
action. Utterances addressing domain content were
further classified with respect to their correctness as
described in the table. The Kappa value for this
annotation effort was r. = 0.69.
This annotation maps straightforwardly into our
set of labels. The social and metacognitive state-
ments are mapped to the “non domain” label;
“pc some error”, “pc” and “incorrect” are mapped
to the “contradictory” label; and the other classes
have a one-to-one correspondence with our task la-
bels.
</bodyText>
<subsectionHeader confidence="0.988327">
2.2 SCIENTSBANK data
</subsectionHeader>
<bodyText confidence="0.9995475">
The SCIENTSBANK corpus (Nielsen et al., 2008)
consists of student responses to science assessment
</bodyText>
<page confidence="0.941741">
202
</page>
<bodyText confidence="0.959345857142857">
Category Subcategory Description
Metacognitive positive content-free expressions describing student knowledge, e.g., “I don’t
negative know”
Social positive expressions describing student’s attitudes towards themselves and
negative the computer (mostly negative in this data, e.g., “You are stupid”)
neutral
Content correct the utterance addresses domain content.
pc some missing the student answer is fully correct
incorrect the student said something correct, but incomplete
pc some error the student’s answer is completely incorrect
pc the student’s answer contains correct parts, but some errors as well
irrelevant the answer contains a mixture of correct, incorrect and missing parts
the answer may be correct or incorrect, but it is not answering the
question.
</bodyText>
<figureCaption confidence="0.996007">
Figure 2: Annotation scheme used in the BEETLE corpus
</figureCaption>
<bodyText confidence="0.98258715">
questions. Specifically, around 16k answers were
collected spanning 16 distinct science subject ar-
eas within physical sciences, life sciences, earth
sciences, space sciences, scientific reasoning and
technology. The tests were part of the Berke-
ley Lawrence Hall of Science Assessing Science
Knowledge (ASK) standardized assessments cover-
ing material from their Full Option Science System
(FOSS) (Lawrence Hall of Science, 2011). The an-
swers came from students in grades 3-6 in schools
across North America.
The tests included a variety of questions includ-
ing “fill in the blank” and multiple choice, but the
SCIENTSBANK corpus only used a subset that re-
quired students to explain their beliefs about top-
ics, typically in one to two sentences. We reviewed
the questions and a sample of the responses and
decided to filter the following types of questions
from the corpus, because they did not mesh with
our goals. First, we removed questions whose ex-
pected answer was more than two full sentences
(typically multi-step procedures), which were be-
yond the scope of our task. Second, we removed
questions where the expected answer was ill-defined
or very open-ended. Finally, the most frequent rea-
son for removing questions was an extreme imbal-
ance in the answer classifications (e.g., for many
questions, almost all of the answers were labeled
“partially correct incomplete”). Specifically, we re-
moved questions where more than 80% of the an-
swers had the same label and questions with fewer
than three correct answers, since these questions
were unlikely to be useful in differentiating between
the quality of assessment systems.
The SCIENTSBANK corpus was developed for the
purpose of assessing student responses at a very fine-
grained level. The reference answers were broken
down into several facets, which consisted roughly
of two key terms and the relation connecting them.
Nielsen et al. annotated student responses to indicate
for each reference answer facet whether the response
1) implied the student understood the facet, 2) im-
plied they held a contradictory belief, 3) included a
related, non-contradicting facet, or 4) left the facet
unaddressed. Reported agreement was 86.2% with
a kappa statistic (Cohen, 1960) of 0.728, which is in
the range of substantial agreement.1
Because our task focuses on answer classifica-
tion rather than facet classification, we developed a
set of rules indicating which combinations of facets
constituted a correct answer. We were then able
to compute an answer label from the gold-standard
facet annotations, as follows. First, if any facet
was annotated as contradictory, the answer was also
labeled “contradictory”. Second, if all of the ex-
pected facets for any valid answer were annotated
as being understood, the answer was labeled “cor-
1These statistics were actually based on five labels, but we
chose to combine the fifth, a self-contradiction, with other con-
tradictions for the purposes of our task.
</bodyText>
<page confidence="0.995249">
203
</page>
<bodyText confidence="0.999982909090909">
rect”. Third, the remaining answers that included
some but not all of the expected facets were la-
beled “partially correct incomplete”. Fourth, if an
answer matched none of the expected facets, and
had not been previously labeled as “contradictory” it
was given the label “irrelevant”. Finally, all “irrele-
vant” answers were reviewed manually to determine
whether they should be relabeled as “non domain”.
However, since Nielsen et al. had already removed
most of the responses that originally fell into this
category, we only found 24 “non domain” answers.
</bodyText>
<sectionHeader confidence="0.99644" genericHeader="method">
3 Baselines
</sectionHeader>
<bodyText confidence="0.999992125">
We established three baselines for our data set – a
straightforward majority class baseline, an existing
system baseline (BEETLE II system performance,
which we report only for the BEETLE portion of the
dataset), and the performance of a simple classifier
based on lexical similarity, which we report in order
to offer a substantial example of applying the same
classifier to both portions of the dataset.
</bodyText>
<subsectionHeader confidence="0.969494">
3.1 BEETLE II system baseline
</subsectionHeader>
<bodyText confidence="0.999995444444445">
The interpretation component of the BEETLE II
system uses a syntactic parser and a set of hand-
authored rules to extract the domain-specific se-
mantic representations of student utterances from
the text. These representations were then matched
against the semantic representations of expected cor-
rect answers supplied by tutors. The resulting sys-
tem output was automatically mapped into our target
labels as discussed in (Dzikovska et al., 2012).
</bodyText>
<subsectionHeader confidence="0.999739">
3.2 Lexical similarity baseline
</subsectionHeader>
<bodyText confidence="0.999891416666667">
To provide a higher baseline that is compara-
ble across both subsets of the data, we built
a simple decision tree classifier using the Weka
3.6.2 implementation of C4.5 pruned decision trees
(weka.classifiers.trees.J48 class), with default pa-
rameters. As features, we used lexical similar-
ity scores computed by the Text::Similarity
package with default parameters2. The code com-
putes four similarity metrics – the raw number of
overlapping words, F1 score, Lesk score and cosine
score. We compared the learner response to the ex-
pected answer(s) and the question, resulting in eight
</bodyText>
<footnote confidence="0.843446">
2http://search.cpan.org/dist/Text-Similarity/
</footnote>
<bodyText confidence="0.9997885">
total features (the four values indicated above for
the comparison with the question and the highest of
each value from the comparisons with each possible
expected answer).
This baseline is based on the lexical overlap base-
line used in RTE tasks (Bentivogli et al., 2009).
However, we measured overlap with the question
text in addition to the overlap with the expected
answers. Students often repeat parts of the ques-
tion in their answer and this needs to be taken
into account to differentiate, for example, “par-
tially correct incomplete” and “correct” answers.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.988965">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99617703030303">
We held back part of the data set for use as standard
test data in the future challenge tasks. For BEETLE,
this consisted of all student answers to 9 out of 56
explanation questions asked by the system, plus ap-
proximately 15% of the student answers to the re-
maining 47 questions, sampling so that the distribu-
tion of labels in test data was similar to the training
data. For SCIENTSBANK, we used a previous train-
test split (Nielsen et al., 2009). For both data sets,
the data was split so that in the future we can test
how well the different systems generalize: i.e., how
well they perform on answers to questions for which
they have some sample student answers vs. how
well they perform on answers to questions that were
not in the training data (e.g., newly created questions
in a deployed system). We discuss this in more detail
in Section 5.
In this paper, we report baseline performance on
the training set to demonstrate that the task is suf-
ficiently challenging to be interesting and that sys-
tems can be compared using our evaluation met-
rics. We preserve the true test data for use in the
planned large-scale system comparisons in a com-
munity shared task.
For the lexical similarity baseline, we use 10-fold
cross-validation.3 For the BEETLE II system base-
line, the language understanding module was de-
3We did not take the student id into account explicitly during
cross-validation. While there is some risk that the classifiers
will learn features specific to the student, we concluded (based
on our understanding of data collection specifics for both data
sets) that there is little enough overlap in cross-validation on the
training data that this should not have a big effect on the results.
</bodyText>
<page confidence="0.993471">
204
</page>
<bodyText confidence="0.9998925">
veloped based on eight transcripts, each taken from
the interaction of a different student with an earlier
version of the system. These sessions were com-
pleted prior to the beginning of the experiment dur-
ing which the BEETLE corpus was collected, and are
not included in the corpus presented here. Thus, the
dataset used in the paper constitutes unseen data for
the BEETLE II system.
We process the two corpora separately because
the additional system baseline is available for bee-
tle, and because the corpora may be different enough
that it will be helpful for shared task participants to
devise processing strategies that are sensitive to the
provenance of the data.
</bodyText>
<subsectionHeader confidence="0.936555">
4.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999842444444444">
Table 1 shows the distribution of codes in the anno-
tated data. The distribution is unbalanced, and there-
fore in our evaluation results we report per-class pre-
cision, recall and F1 scores, plus the averaged scores
using two different ways to average over per-class
evaluation scores, micro- and macro- averaging.
For a set of classes C, each represented with Nc
instances in the test set, the macro-averaged recall is
defined as
</bodyText>
<equation confidence="0.948554666666667">
1
Rmacro = |C |E R(c)
c�C
</equation>
<bodyText confidence="0.659413">
and the micro-averaged recall as
</bodyText>
<equation confidence="0.9724025">
�Rmicro =
cfC
</equation>
<bodyText confidence="0.997898">
Micro- and macro-averaged precision and F1 are de-
fined similarly.
Micro-averaging takes class sizes into account, so
a system that performs well on the most common
classes will have a high micro-average score. This is
the most commonly used classifier evaluation met-
ric. Note that, in particular, overall classification
accuracy (defined as the number of correctly clas-
sified instances out of all instances) is mathemat-
ically equivalent to micro-averaged recall (Abuda-
wood and Flach, 2011). However, macro-averaging
better reflects performance on small classes, and is
commonly used for unbalanced classification prob-
lems (see, e.g., (Lewis, 1991)). We report both val-
ues in our results.
</bodyText>
<table confidence="0.999023777777778">
BEETLE SCIENTSBANK
Label Count Freq. Count Freq.
correct 1157 0.42 2095 0.40
partially correct 626 0.23 1431 0.27
incomplete
contradictory 656 0.24 526 0.10
irrelevant 86 0.03 1175 0.22
non domain 204 0.07 24 0.005
total 2729 5251
</table>
<tableCaption confidence="0.999656">
Table 1: Distribution of annotated labels in the data
</tableCaption>
<bodyText confidence="0.9999884375">
In addition, we report the system scores on the bi-
nary decision of whether or not the corrective feed-
back should be issued (denoted “corrective feed-
back” in the results table). It assumes that a tutoring
system using a classifier will give corrective feed-
back if the classifiers returns any label other than
“correct”. Thus, every instance classified as “par-
tially correct incomplete”, “contradictory”, “irrele-
vant” or “non domain” is counted as true positive
if the hand-annotated label also belongs to this set
(even if the classifier disagrees with the annotation);
and as false positive if the hand-annotated label is
“correct”. This reflects the idea that students are
likely to be frustrated if the system gives corrective
feedback when their answer is in fact a fully accurate
paraphrase of a correct answer.
</bodyText>
<subsectionHeader confidence="0.996731">
4.3 BEETLE baseline performance
</subsectionHeader>
<bodyText confidence="0.999054111111111">
The detailed evaluation results for all baselines are
presented in Table 2.
The majority class baseline is to assign “correct”
to every test instance. It achieves 42% overall ac-
curacy. However, this is obviously at the expense
of serious errors; for example, such a system would
tell the students that they are correct if they are say-
ing something contradictory. This is reflected in a
much lower macro-averaged F1 score.
The BEETLE II system performs only slightly bet-
ter than the baseline on the overall accuracy (0.44
vs. 0.42 micro-averaged recall). However, the
macro-averaged F1 score of the BEETLE II system
is substantially higher (0.46 vs. 0.12). The micro-
averaged results show a similar pattern, although the
majority-class baseline performs slightly better than
in the macro-averaged case, as expected.
Comparing the BEETLE II parser to our lexical
</bodyText>
<equation confidence="0.454339">
1 R(c)
Nc
</equation>
<page confidence="0.99622">
205
</page>
<bodyText confidence="0.999867521739131">
similarity baseline, BEETLE II has lower overall ac-
curacy, but performs similarly on micro- and macro-
averaged scores. BEETLE II precision is higher than
that of the classifier in all cases except for the binary
decision as to whether corrective feedback should
be issued. This is not unexpected given how the sys-
tem was designed – since misunderstandings caused
dialogue breakdown in pilot tests, the parser was
built to prefer rejecting utterances as uninterpretable
rather than assigning them an incorrect class, lead-
ing to a considerably lower recall. Around 31% of
utterances could not be interpreted.
Our recent analysis shows that both incorrect
interpretations (in particular, confusions between
“partially correct incomplete” and “contradictory”)
and rejections have significant negative effects on
learning gain (Dzikovska et al., 2012). Classifiers
can be tuned to reject examples where classification
confidence falls below a given threshold, resulting
in precision-recall trade-offs. Our baseline classifier
classified all answer instances; exploring the possi-
bilities for rejecting some low-confidence answers is
planned for future work.
</bodyText>
<subsectionHeader confidence="0.970635">
4.4 SCIENTSBANK baseline performance
</subsectionHeader>
<bodyText confidence="0.999989086956521">
The accuracy of the majority class baseline (which
assumes all answers are “correct”) is 40% for SCI-
ENTSBANK, about the same as it was for BEE-
TLE. The evaluation results, based on 10-fold cross-
validation, for our simple lexical similarity classi-
fier are presented in Table 3. The lexical similar-
ity based classifier outperforms the majority class
baseline by 0.18 and 3% on the macro-averaged
Fl-measure and accuracy, respectively. The Fi-
measure for the two-way classification detecting an-
swers which need corrective feedback is 0.66.
The scores on SCIENTSBANK are noticeably
lower than those for BEETLE. The SCIENTSBANK
includes questions from 12 distinct science subject
areas, rather than a single area as in BEETLE. This
decision tree classifier learns a function from the
eight text similarity features to the desired answer
label. Because the features do not mention particular
words, the model can be applied to items other than
the ones on which it was trained, and even to items
from different subject areas. However, the correct
weighting of the textual similarity features depends
on the extent and nature of the expected textual over-
</bodyText>
<table confidence="0.8503445">
Predictn correct pc inc contra irrlvnt nondom
correct 1213 553 209 392 2
pc inc 432 497 128 241 2
contra 115 109 58 74 3
irrlvnt 335 272 131 468 17
nondom 0 0 0 0 0
</table>
<figureCaption confidence="0.9894505">
Figure 4: Confusion matrix for lexical classifier on SCI-
ENTSBANK. Predictions in rows, gold labels in columns
</figureCaption>
<bodyText confidence="0.999865083333333">
lap, which does vary from subject-area to subject-
area. We suspect that the differences between sub-
ject areas made it hard for the decision-tree classi-
fier to find a single, globally appropriate strategy.
Nielsen (2009) reported the best results for classify-
ing facets when training separate question-specific
or even facet-specific classifiers. Although separate
training for each item reduces the amount of relevant
training data for each classifier, it allows each clas-
sifier to learn the specifics of how that item works.
A comparison using this style of training would be a
reasonable next step,
</bodyText>
<sectionHeader confidence="0.987938" genericHeader="conclusions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999973541666667">
The results presented satisfy two critical require-
ments for a challenge task. First, we have shown that
it is feasible to develop a system that performs sig-
nificantly better than the majority class baseline. On
the macro-averaged Fl-measure, our lexical clas-
sifier outperformed the majority-class baseline by
0.33 (on BEETLE) and 0.18 (on SCIENTSBANK)
and by 13% and 3% on accuracy. Second, we have
also shown, as is desired for a challenge task, that
the task is not trivial. With a system specifically
designed to parse the BEETLE corpus answers, the
macro-averaged Fl-measure was just 0.46 and on
the binary decision regarding whether the response
needed corrective feedback, it achieved just 0.63.
One contribution of this work was to define a gen-
eral classification scheme for student responses that
allows more specific learner feedback. Another key
contribution was to unify two, previously incom-
patible, large student response corpora under this
common annotation scheme. The resultant corpus
will enable researchers to train learning algorithms
to classify student responses. These classifications
can then be used by a dialogue manager to generate
targeted learner feedback. The corpus is available
</bodyText>
<page confidence="0.996156">
206
</page>
<table confidence="0.9998501">
Classifier: majority lexical similarity BEETLE II
Predicted label prec. recall F1 prec. recall F1 prec. recall F1
correct 0.42 1.00 0.60 0.68 0.75 0.72 0.93 0.53 0.68
partially correct incomplete 0.00 0.00 0.00 0.41 0.38 0.39 0.43 0.53 0.47
contradictory 0.00 0.00 0.00 0.39 0.34 0.36 0.58 0.23 0.33
irrelevant 0.00 0.00 0.00 0.05 0.02 0.03 0.23 0.17 0.20
non domain 0.00 0.00 0.00 0.66 0.82 0.73 0.92 0.46 0.61
macroaverage 0.09 0.20 0.12 0.44 0.46 0.45 0.62 0.39 0.46
microaverage 0.18 0.42 0.25 0.53 0.55 0.54 0.71 0.44 0.53
corrective feedback 0.00 0.00 0.00 0.80 0.74 0.77 0.73 0.56 0.63
</table>
<tableCaption confidence="0.978059">
Table 2: Evaluation results for BEETLE corpus
</tableCaption>
<table confidence="0.997162875">
Classifier: lexical similarity BEETLE II
Predicted label corrct pc inc contra irrlvnt nondom corrct pc inc contra irrlvnt nondom
correct 870 187 199 20 2 617 20 23 0 3
part corr incmp 138 239 178 24 11 249 332 146 29 20
contradictory 139 153 221 33 22 68 38 149 3 0
irrelevant 3 20 12 2 1 4 22 23 15 1
non domain 7 27 46 7 168 3 3 1 1 94
uninterpretable n/a n/a n/a n/a n/a 216 211 314 38 86
</table>
<figureCaption confidence="0.511826">
Figure 3: Confusion matrix for BEETLE corpus. Predictions in rows, gold labels in columns
</figureCaption>
<table confidence="0.9999536">
Classifier: baseline lexical similarity
Predicted label prec. recall F1 prec. recall F1
correct 0.40 1.00 0.57 0.51 0.58 0.54
partially correct incomplete 0.00 0.00 0.00 0.38 0.35 0.36
contradictory 0.00 0.00 0.00 0.16 0.11 0.13
irrelevant 0.00 0.00 0.00 0.38 0.40 0.39
non domain 0.00 0.00 0.00 0.00 0.00 0.00
macroaverage 0.08 0.20 0.11 0.29 0.29 0.29
microaverage 0.16 0.40 0.23 0.41 0.43 0.42
corrective feedback 0.00 0.00 0.00 0.69 0.63 0.66
</table>
<tableCaption confidence="0.999831">
Table 3: Evaluation results for SCIENTSBANK baselines
</tableCaption>
<page confidence="0.997108">
207
</page>
<bodyText confidence="0.999946782608696">
for general research purposes and forms the basis
of SEMEVAL-2013 shared task “Textual entailment
and paraphrasing for student input assessment”.4
A third contribution of this work was to provide
basic evaluation benchmark metrics and the corre-
sponding evaluation scripts (downloadable from the
site above) for other researchers, including shared
task participants. This will facilitate the comparison
and, hence, the progress, of research.
The work reported here is based on approximately
8000 student responses to questions covering 12 dis-
tinct science subjects and coming from a wide range
of student ages. These responses comprise the train-
ing data for our task. The vast majority of prior
work, including BEETLE II, which was included as
a benchmark here, has been designed to provide ITS
feedback for relatively small, well-defined domains.
The corpus presented in this paper is intended to en-
courage research into more generalizable, domain-
independent techniques. Following Nielsen (2009),
from whom the SCIENTSBANK corpus was adapted,
our shared task evaluation corpus will be composed
of three types of data: additional student responses
for all of the questions in the training data (Un-
seen Answers), student responses to questions that
were not seen in the training data, but that are from
the same subject areas (Unseen Questions), and re-
sponses to questions from three entirely different
subject areas (Unseen Domains), though in this case
the questions are still from the same general domain
– science. Unseen Answers is the typical scenario
for the vast majority of prior work – training and
testing on responses to the same questions. Unseen
Questions and Unseen Domains allow researchers to
evaluate how well their systems generalize to near
and far domains, respectively.
The primary target application for this work is in-
telligent tutoring systems, where the classification of
responses is intended to facilitate specific pedagogic
feedback. Beneath the surface, the baseline systems
reported here are more similar to grading systems
that use the approach of (Leacock and Chodorow,
2003), which uses classifier technology to detect ex-
pressions of facet-like concepts, then converts the
result to a numerical score, than to grading systems
like (Mohler et al., 2011), which directly produces a
</bodyText>
<footnote confidence="0.981168">
4See http://www.cs.york.ac.uk/semeval-2013/task4/
</footnote>
<bodyText confidence="0.99987884375">
numerical score, using support vector regression and
similar techniques. Either approach is reasonable,
but we think that feedback is the more challeng-
ing test of a system’s ultimate abilities, and there-
fore a better candidate for the shared task. The cor-
pora from those systems, alongside with new cor-
pora currently being collected in BEETLE and SCI-
ENTSBANK domains, can serve as sources of data
for future tasks extensions.
Future systems developed for this task can benefit
from the large amount of existing work on recog-
nizing textual entailment (Giampiccolo et al., 2007;
Giampiccolo et al., 2008; Bentivogli et al., 2009)
and on detecting contradiction (Ritter et al., 2008;
De Marneffe et al., 2008). However, there are sub-
stantial challenges in applying the RTE tools directly
to this data set. Our set of labels is more fine-grained
than RTE labels to reflect the needs of intelligent tu-
toring systems (see Section 2). In addition, the top-
performing systems in RTE5 3-way task, as well as
contradiction detection methods, rely on NLP tools
such as dependency parsers and semantic role la-
belers; these do not perform well on specialized
terminology and language constructs coming from
(typed) dialogue context. We chose to use lexical
similarity as a baseline specifically because a simi-
lar measure was used as a standard baseline in RTE
tasks, and we expect that adapting the more complex
RTE approaches for purposes of this task will result
in both improved results on our data set and new de-
velopments in computational linguistics algorithms
used for RTE and related tasks.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999942363636364">
We thank Natalie Steinhauser, Gwendolyn Camp-
bell, Charlie Scott, Simon Caine, Leanne Taylor,
Katherine Harrison and Jonathan Kilgour for help
with data collection and preparation. The research
reported here was supported by the US ONR award
N000141010085 and by the Institute of Education
Sciences, U.S. Department of Education, through
Grant R305A110811 to Boulder Language Tech-
nologies Inc. The opinions expressed are those of
the authors and do not represent views of the Insti-
tute or the U.S. Department of Education.
</bodyText>
<page confidence="0.997517">
208
</page>
<sectionHeader confidence="0.989152" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998031533980583">
Tarek Abudawood and Peter Flach. 2011. Learn-
ing multi-class theories in ilp. In The 20th Interna-
tional Conference on Inductive Logic Programming
(ILP’10). Springer, June.
V. Aleven, O. Popescu, and K. R. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cogni-
tive tutor. In Proceedings of the 101h International
Conference on Artificial Intelligence in Education
(AIED ’01)”.
Vincent Aleven, Octav Popescu, and Koedinger
Koedinger. 2002. Pilot-testing a tutorial dialogue
system that supports self-explanation. Lecture Notes
in Computer Science, 2363:344–354.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Charles Callaway, Myroslava Dzikovska, Colin Mathe-
son, Johanna Moore, and Claus Zinn. 2006. Using
dialogue to learn math in the LeActiveMath project.
In Proceedings of the ECAI Workshop on Language-
Enhanced Educational Technology, pages 1–8, Au-
gust.
Gwendolyn C. Campbell, Natalie B. Steinhauser, My-
roslava O. Dzikovska, Johanna D. Moore, Charles B.
Callaway, and Elaine Farrow. 2009. The DeMAND
coding scheme: A “common language” for represent-
ing and analyzing student discourse. In Proceedings
of 14th International Conference on Artificial Intelli-
gence in Education (AIED), poster session, Brighton,
UK, July.
Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung
Chiu, and Christian LaVancher. 1994. Eliciting self-
explanations improves understanding. Cognitive Sci-
ence, 18(3):439–477.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measure-
ment, 20(1):3746.
M.C. De Marneffe, A.N. Rafferty, and C.D. Manning.
2008. Finding contradictions in text. Proceedings of
ACL-08: HLT, pages 1039–1047.
Myroslava Dzikovska, Diana Bental, Johanna D. Moore,
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Elaine Farrow, and Charles B. Callaway. 2010a. In-
telligent tutoring with natural language support in the
Beetle II system. In Sustaining TEL: From Innovation
to Learning and Practice - 5th European Conference
on Technology Enhanced Learning, (EC-TEL 2010),
Barcelona, Spain, October.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010b. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL-
2010) demo session, Uppsala, Sweden, July.
Myroslava O. Dzikovska, Peter Bell, Amy Isard, and Jo-
hanna D. Moore. 2012. Evaluating language under-
standing accuracy with respect to objective outcomes
in a dialogue system. In Proceedings of the 13th Con-
ference of the European Chapter of the Association for
computational Linguistics, Avignon, France, April.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third PASCAL recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 1–9, Prague, June.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth PASCAL recognizing textual entail-
ment challenge. In Proceedings of Text Analysis Con-
ference (TAC) 2008, Gaithersburg, MD, November.
Michael Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Pro-
ceedings of the AAAI Fall Symposium on Building Di-
alogue Systems for Tutorial Applications.
A. C. Graesser, P. Wiemer-Hastings, K. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35–51.
Pamela W. Jordan, Maxim Makatchev, and Kurt Van-
Lehn. 2004. Combining competing language un-
derstanding approaches in an intelligent tutoring sys-
tem. In James C. Lester, Rosa Maria Vicari, and
F´abio Paraguac¸u, editors, Intelligent Tutoring Systems,
volume 3220 of Lecture Notes in Computer Science,
pages 346–357. Springer.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system
for physics. In Proceedings of the 19th International
FLAIRS conference.
Pamela W. Jordan. 2004. Using student explanations
as models for adapting tutorial dialogue. In Valerie
Barr and Zdravko Markov, editors, FLAIRS Confer-
ence. AAAI Press.
Lawrence Hall of Science. 2006. Assessing Science
Knowledge (ask). University of California at Berke-
ley, NSF-0242510.
Lawrence Hall of Science. 2011. Full option science
system.
</reference>
<page confidence="0.982233">
209
</page>
<reference confidence="0.999570173076923">
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389–405.
David D. Lewis. 1991. Evaluating text categorization. In
Proceedings of the workshop on Speech and Natural
Language, HLT ’91, pages 312–318, Stroudsburg, PA,
USA.
Diane Litman, Johanna Moore, Myroslava Dzikovska,
and Elaine Farrow. 2009. Using natural language pro-
cessing to analyze tutorial dialogue corpora across do-
mains and modalities. In Proceedings of 14th Interna-
tional Conference on Artificial Intelligence in Educa-
tion (AIED), Brighton, UK, July.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752–762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008. Annotating students’ under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2009. Recognizing entailment in intelligent tutoring
systems. The Journal of Natural Language Engineer-
ing, 15:479–501.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proceedings of ITS-
2004, pages 390–400.
A. Ritter, D. Downey, S. Soderland, and O. Etzioni.
2008. It’s a contradiction—no, it’s not: a case study
using functional relations. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 11–20.
Natalie B. Steinhauser, Gwendolyn E. Campbell,
Leanne S. Taylor, Simon Caine, Charlie Scott, My-
roslava O. Dzikovska, and Johanna D. Moore. 2011.
Talk like an electrician: Student dialogue mimicking
behavior in an intelligent tutoring system. In Proceed-
ings of the 15th International Conference on Artificial
Intelligence in Education (AIED-2011).
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proceedings of
SLaTE Workshop on Speech and Language Technol-
ogy in Education, Farmington, PA, October.
</reference>
<page confidence="0.998958">
210
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.428871">
<title confidence="0.9936915">Towards Effective Tutorial Feedback for Explanation A Dataset and Baselines</title>
<author confidence="0.490503">O</author>
<author confidence="0.490503">D</author>
<affiliation confidence="0.915163">of Informatics, University of Edinburgh, Edinburgh, EH8 9AB, Language &amp; Education Research University of Colorado at Boulder, Boulder, CO 80309-0594,</affiliation>
<address confidence="0.970578">Testing Service, Princeton, NJ 08451,</address>
<abstract confidence="0.998970833333333">We propose a new shared task on grading student answers with the goal of enabling welltargeted and flexible feedback in a tutorial dialogue setting. We provide an annotated corpus designed for the purpose, a precise specification for a prediction task and an associated evaluation methodology. The task is feasible but non-trivial, which is demonstrated by creating and comparing three alternative baseline systems. We believe that this corpus will be of interest to the researchers working in textual entailment and will stimulate new developments both in natural language processing in tutorial dialogue systems and textual entailment, contradiction detection and other techniques of interest for a variety of computational linguistics tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tarek Abudawood</author>
<author>Peter Flach</author>
</authors>
<title>Learning multi-class theories in ilp.</title>
<date>2011</date>
<booktitle>In The 20th International Conference on Inductive Logic Programming (ILP’10).</booktitle>
<publisher>Springer,</publisher>
<contexts>
<context position="22500" citStr="Abudawood and Flach, 2011" startWordPosition="3570" endWordPosition="3574">tances in the test set, the macro-averaged recall is defined as 1 Rmacro = |C |E R(c) c�C and the micro-averaged recall as �Rmicro = cfC Micro- and macro-averaged precision and F1 are defined similarly. Micro-averaging takes class sizes into account, so a system that performs well on the most common classes will have a high micro-average score. This is the most commonly used classifier evaluation metric. Note that, in particular, overall classification accuracy (defined as the number of correctly classified instances out of all instances) is mathematically equivalent to micro-averaged recall (Abudawood and Flach, 2011). However, macro-averaging better reflects performance on small classes, and is commonly used for unbalanced classification problems (see, e.g., (Lewis, 1991)). We report both values in our results. BEETLE SCIENTSBANK Label Count Freq. Count Freq. correct 1157 0.42 2095 0.40 partially correct 626 0.23 1431 0.27 incomplete contradictory 656 0.24 526 0.10 irrelevant 86 0.03 1175 0.22 non domain 204 0.07 24 0.005 total 2729 5251 Table 1: Distribution of annotated labels in the data In addition, we report the system scores on the binary decision of whether or not the corrective feedback should be </context>
</contexts>
<marker>Abudawood, Flach, 2011</marker>
<rawString>Tarek Abudawood and Peter Flach. 2011. Learning multi-class theories in ilp. In The 20th International Conference on Inductive Logic Programming (ILP’10). Springer, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Aleven</author>
<author>O Popescu</author>
<author>K R Koedinger</author>
</authors>
<title>Towards tutorial dialog to support self-explanation: Adding natural language understanding to a cognitive tutor.</title>
<date>2001</date>
<booktitle>In Proceedings of the 101h International Conference on Artificial Intelligence in Education (AIED ’01)”.</booktitle>
<contexts>
<context position="1694" citStr="Aleven et al., 2001" startWordPosition="247" endWordPosition="250">ms and textual entailment, contradiction detection and other techniques of interest for a variety of computational linguistics tasks. 1 Introduction In human-human tutoring, it is an effective strategy to ask students to explain instructional material in their own words. Self-explanation (Chi et al., 1994) and contentful talk focused on the domain are correlated with better learning outcomes (Litman et al., 2009; Chi et al., 1994). There has therefore been much interest in developing automated tutorial dialogue systems that ask students open-ended explanation questions (Graesser et al., 1999; Aleven et al., 2001; Jordan et al., 2006; VanLehn et al., 2007; Nielsen et al., 2009; Dzikovska et al., 2010a). In order to do this well, it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue about the instructional content. Thus, systems must respond appropriately to student explanations, and must provide detailed, flexible and appropriate feedback (Aleven et al., 2002; Jordan et al., 2004). In simple domains, we can adopt a knowledge engineering approach and build a domain model and a diagnoser, together with a natural language parser to</context>
</contexts>
<marker>Aleven, Popescu, Koedinger, 2001</marker>
<rawString>V. Aleven, O. Popescu, and K. R. Koedinger. 2001. Towards tutorial dialog to support self-explanation: Adding natural language understanding to a cognitive tutor. In Proceedings of the 101h International Conference on Artificial Intelligence in Education (AIED ’01)”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Aleven</author>
<author>Octav Popescu</author>
<author>Koedinger Koedinger</author>
</authors>
<title>Pilot-testing a tutorial dialogue system that supports self-explanation.</title>
<date>2002</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>2363--344</pages>
<contexts>
<context position="2121" citStr="Aleven et al., 2002" startWordPosition="314" endWordPosition="317">, 1994). There has therefore been much interest in developing automated tutorial dialogue systems that ask students open-ended explanation questions (Graesser et al., 1999; Aleven et al., 2001; Jordan et al., 2006; VanLehn et al., 2007; Nielsen et al., 2009; Dzikovska et al., 2010a). In order to do this well, it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue about the instructional content. Thus, systems must respond appropriately to student explanations, and must provide detailed, flexible and appropriate feedback (Aleven et al., 2002; Jordan et al., 2004). In simple domains, we can adopt a knowledge engineering approach and build a domain model and a diagnoser, together with a natural language parser to produce detailed semantic representations of student input (Glass, 2000; Aleven et al., 2002; Pon-Barry et al., 2004; Callaway et al., 2006; Dzikovska et al., 2010a). The advantage of this approach is that it allows for flexible adaptation of feedback to a variety of factors such as student performance. For example, it is easy for the system to know if the student made the same error before, and adjust its feedback to refl</context>
</contexts>
<marker>Aleven, Popescu, Koedinger, 2002</marker>
<rawString>Vincent Aleven, Octav Popescu, and Koedinger Koedinger. 2002. Pilot-testing a tutorial dialogue system that supports self-explanation. Lecture Notes in Computer Science, 2363:344–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Ido Dagan</author>
<author>Hoa Trang Dang</author>
<author>Danilo Giampiccolo</author>
<author>Bernando Magnini</author>
</authors>
<title>The fifth PASCAL recognizing textual entailment challenge.</title>
<date>2009</date>
<booktitle>In Notebook papers and results, Text Analysis Conference (TAC).</booktitle>
<contexts>
<context position="6998" citStr="Bentivogli et al., 2009" startWordPosition="1091" endWordPosition="1094">) (henceforth, the Science Entailments Bank or SCIENTSBANK). In both corpora, each question was associated with one or more reference answers provided by the experts. Student answers were evaluated against these reference answers and, using corpus-specific annotation schemes, assigned labels for correctness. In order to reconcile the two different schemes and to cast the task in terms of standard supervised machine classification at the sentence level, we derived a new set of annotations, using the annotation scheme shown in Figure 1. Our label set has some similarity to the RTE5 3- way task (Bentivogli et al., 2009), which used “entailment”, “contradiction” and “unknown” labels. The additional distinctions in our labels reflect typical distinctions made by tutorial dialogue systems. They match our human tutors’ intuitions about the general error types observed in student answers and corresponding teaching tactics. For example, a likely response to “partially correct incomplete” would be to tell the student that what they said so far was correct but it had some gaps, and to encourage them to fill in those gaps. In contrast, the response to “contradictory” would emphasize that there is a mistake and the st</context>
<context position="18784" citStr="Bentivogli et al., 2009" startWordPosition="2948" endWordPosition="2951">xical similarity scores computed by the Text::Similarity package with default parameters2. The code computes four similarity metrics – the raw number of overlapping words, F1 score, Lesk score and cosine score. We compared the learner response to the expected answer(s) and the question, resulting in eight 2http://search.cpan.org/dist/Text-Similarity/ total features (the four values indicated above for the comparison with the question and the highest of each value from the comparisons with each possible expected answer). This baseline is based on the lexical overlap baseline used in RTE tasks (Bentivogli et al., 2009). However, we measured overlap with the question text in addition to the overlap with the expected answers. Students often repeat parts of the question in their answer and this needs to be taken into account to differentiate, for example, “partially correct incomplete” and “correct” answers. 4 Results 4.1 Experimental Setup We held back part of the data set for use as standard test data in the future challenge tasks. For BEETLE, this consisted of all student answers to 9 out of 56 explanation questions asked by the system, plus approximately 15% of the student answers to the remaining 47 quest</context>
<context position="33771" citStr="Bentivogli et al., 2009" startWordPosition="5368" endWordPosition="5371">erical score, using support vector regression and similar techniques. Either approach is reasonable, but we think that feedback is the more challenging test of a system’s ultimate abilities, and therefore a better candidate for the shared task. The corpora from those systems, alongside with new corpora currently being collected in BEETLE and SCIENTSBANK domains, can serve as sources of data for future tasks extensions. Future systems developed for this task can benefit from the large amount of existing work on recognizing textual entailment (Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009) and on detecting contradiction (Ritter et al., 2008; De Marneffe et al., 2008). However, there are substantial challenges in applying the RTE tools directly to this data set. Our set of labels is more fine-grained than RTE labels to reflect the needs of intelligent tutoring systems (see Section 2). In addition, the topperforming systems in RTE5 3-way task, as well as contradiction detection methods, rely on NLP tools such as dependency parsers and semantic role labelers; these do not perform well on specialized terminology and language constructs coming from (typed) dialogue context. We chose</context>
</contexts>
<marker>Bentivogli, Dagan, Dang, Giampiccolo, Magnini, 2009</marker>
<rawString>Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernando Magnini. 2009. The fifth PASCAL recognizing textual entailment challenge. In Notebook papers and results, Text Analysis Conference (TAC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Callaway</author>
<author>Myroslava Dzikovska</author>
<author>Colin Matheson</author>
<author>Johanna Moore</author>
<author>Claus Zinn</author>
</authors>
<title>Using dialogue to learn math in the LeActiveMath project.</title>
<date>2006</date>
<booktitle>In Proceedings of the ECAI Workshop on LanguageEnhanced Educational Technology,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2434" citStr="Callaway et al., 2006" startWordPosition="365" endWordPosition="368">it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue about the instructional content. Thus, systems must respond appropriately to student explanations, and must provide detailed, flexible and appropriate feedback (Aleven et al., 2002; Jordan et al., 2004). In simple domains, we can adopt a knowledge engineering approach and build a domain model and a diagnoser, together with a natural language parser to produce detailed semantic representations of student input (Glass, 2000; Aleven et al., 2002; Pon-Barry et al., 2004; Callaway et al., 2006; Dzikovska et al., 2010a). The advantage of this approach is that it allows for flexible adaptation of feedback to a variety of factors such as student performance. For example, it is easy for the system to know if the student made the same error before, and adjust its feedback to reflect it. Moreover, this approach allows for easy addition of new exercises : as long as an exercise relies on the concepts covered by the domain model, the system can apply standard instructional strategies to each new question automatically. However, this approach is significantly limited by the requirement that</context>
</contexts>
<marker>Callaway, Dzikovska, Matheson, Moore, Zinn, 2006</marker>
<rawString>Charles Callaway, Myroslava Dzikovska, Colin Matheson, Johanna Moore, and Claus Zinn. 2006. Using dialogue to learn math in the LeActiveMath project. In Proceedings of the ECAI Workshop on LanguageEnhanced Educational Technology, pages 1–8, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gwendolyn C Campbell</author>
<author>Natalie B Steinhauser</author>
<author>Myroslava O Dzikovska</author>
<author>Johanna D Moore</author>
<author>Charles B Callaway</author>
<author>Elaine Farrow</author>
</authors>
<title>The DeMAND coding scheme: A “common language” for representing and analyzing student discourse.</title>
<date>2009</date>
<booktitle>In Proceedings of 14th International Conference on Artificial Intelligence in Education (AIED), poster session,</booktitle>
<location>Brighton, UK,</location>
<contexts>
<context position="11891" citStr="Campbell et al., 2009" startWordPosition="1880" endWordPosition="1883">responses (Steinhauser et al., 2011). This resulted in a number of answers to certain questions that came from different speakers but which were exact copies of each other. We removed such answers from the data set, since they were likely to be in both the training and test set, thus inflating our results. Note that only exact matches were removed: for example, answers that were nearly identical but contained spelling errors were retained, since they would need to be handled in a practical system. Student utterances were manually labeled using a simplified version of the DEMAND coding scheme (Campbell et al., 2009) shown in Figure 2. The utterances were first classified as related to domain content, student’s metacognitive state, or social interaction. Utterances addressing domain content were further classified with respect to their correctness as described in the table. The Kappa value for this annotation effort was r. = 0.69. This annotation maps straightforwardly into our set of labels. The social and metacognitive statements are mapped to the “non domain” label; “pc some error”, “pc” and “incorrect” are mapped to the “contradictory” label; and the other classes have a one-to-one correspondence with</context>
</contexts>
<marker>Campbell, Steinhauser, Dzikovska, Moore, Callaway, Farrow, 2009</marker>
<rawString>Gwendolyn C. Campbell, Natalie B. Steinhauser, Myroslava O. Dzikovska, Johanna D. Moore, Charles B. Callaway, and Elaine Farrow. 2009. The DeMAND coding scheme: A “common language” for representing and analyzing student discourse. In Proceedings of 14th International Conference on Artificial Intelligence in Education (AIED), poster session, Brighton, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelene T H Chi</author>
<author>Nicholas de Leeuw</author>
<author>Mei-Hung Chiu</author>
<author>Christian LaVancher</author>
</authors>
<title>Eliciting selfexplanations improves understanding.</title>
<date>1994</date>
<journal>Cognitive Science,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>Chi, de Leeuw, Chiu, LaVancher, 1994</marker>
<rawString>Michelene T. H. Chi, Nicholas de Leeuw, Mei-Hung Chiu, and Christian LaVancher. 1994. Eliciting selfexplanations improves understanding. Cognitive Science, 18(3):439–477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="15673" citStr="Cohen, 1960" startWordPosition="2464" endWordPosition="2465">ssessment systems. The SCIENTSBANK corpus was developed for the purpose of assessing student responses at a very finegrained level. The reference answers were broken down into several facets, which consisted roughly of two key terms and the relation connecting them. Nielsen et al. annotated student responses to indicate for each reference answer facet whether the response 1) implied the student understood the facet, 2) implied they held a contradictory belief, 3) included a related, non-contradicting facet, or 4) left the facet unaddressed. Reported agreement was 86.2% with a kappa statistic (Cohen, 1960) of 0.728, which is in the range of substantial agreement.1 Because our task focuses on answer classification rather than facet classification, we developed a set of rules indicating which combinations of facets constituted a correct answer. We were then able to compute an answer label from the gold-standard facet annotations, as follows. First, if any facet was annotated as contradictory, the answer was also labeled “contradictory”. Second, if all of the expected facets for any valid answer were annotated as being understood, the answer was labeled “cor1These statistics were actually based on</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):3746.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C De Marneffe</author>
<author>A N Rafferty</author>
<author>C D Manning</author>
</authors>
<title>Finding contradictions in text.</title>
<date>2008</date>
<booktitle>Proceedings of ACL-08: HLT,</booktitle>
<pages>1039--1047</pages>
<marker>De Marneffe, Rafferty, Manning, 2008</marker>
<rawString>M.C. De Marneffe, A.N. Rafferty, and C.D. Manning. 2008. Finding contradictions in text. Proceedings of ACL-08: HLT, pages 1039–1047.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava Dzikovska</author>
<author>Diana Bental</author>
<author>Johanna D Moore</author>
<author>Natalie B Steinhauser</author>
<author>Gwendolyn E Campbell</author>
<author>Elaine Farrow</author>
<author>Charles B Callaway</author>
</authors>
<title>Intelligent tutoring with natural language support in the Beetle II system.</title>
<date>2010</date>
<booktitle>In Sustaining TEL: From Innovation to Learning and Practice - 5th European Conference on Technology Enhanced Learning, (EC-TEL 2010),</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1783" citStr="Dzikovska et al., 2010" startWordPosition="263" endWordPosition="266">r a variety of computational linguistics tasks. 1 Introduction In human-human tutoring, it is an effective strategy to ask students to explain instructional material in their own words. Self-explanation (Chi et al., 1994) and contentful talk focused on the domain are correlated with better learning outcomes (Litman et al., 2009; Chi et al., 1994). There has therefore been much interest in developing automated tutorial dialogue systems that ask students open-ended explanation questions (Graesser et al., 1999; Aleven et al., 2001; Jordan et al., 2006; VanLehn et al., 2007; Nielsen et al., 2009; Dzikovska et al., 2010a). In order to do this well, it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue about the instructional content. Thus, systems must respond appropriately to student explanations, and must provide detailed, flexible and appropriate feedback (Aleven et al., 2002; Jordan et al., 2004). In simple domains, we can adopt a knowledge engineering approach and build a domain model and a diagnoser, together with a natural language parser to produce detailed semantic representations of student input (Glass, 2000; Aleven et al., </context>
<context position="6184" citStr="Dzikovska et al., 2010" startWordPosition="964" endWordPosition="967">ers. This will provide a basis for the development, comparison and evaluation of alternative approaches to the error classification task. We believe that the natural language capabilities needed for this task will be directly applicable to a far wider range of tasks in educational assessment, information extraction and computational semantics. This dataset is publicly available and will be used in a community-wide shared task. 2 Corpus The data set we developed draws on two established sources – a data set collected and annotated during an evaluation of the BEETLE II tutorial dialogue system (Dzikovska et al., 2010a) (henceforth, BEETLE corpus) and a set of student answers to questions from 16 science modules in the Assessing Science Knowledge (ASK) assessment inventory (Lawrence Hall of Science, 2006) (henceforth, the Science Entailments Bank or SCIENTSBANK). In both corpora, each question was associated with one or more reference answers provided by the experts. Student answers were evaluated against these reference answers and, using corpus-specific annotation schemes, assigned labels for correctness. In order to reconcile the two different schemes and to cast the task in terms of standard supervised</context>
<context position="9458" citStr="Dzikovska et al., 2010" startWordPosition="1490" endWordPosition="1493"> contradictory correct nuggets, but parts are missing irrelevant an answer that contradicts some part of the reference answer contains domain content, but does not answer the question Figure 1: The set of answer labels used in our task We further discuss the relationship with the task of recognizing textual entailment in Section 5. In the rest of this section, we describe our corpora and discuss how we obtained these labels from the raw data available in our datasets. 2.1 BEETLE II data The BEETLE corpus consists of the interactions between students and the BEETLE II tutorial dialogue system (Dzikovska et al., 2010b). The BEETLE II system is an intelligent tutoring system that teaches students with no knowledge of high-school physics concepts in basic electricity and electronics. In the first system evaluation, students spend 3-5 hours going through prepared reading material, building and observing circuits in the simulator and interacting with a dialogue-based tutor. The interaction was by keyboard, with the computer tutor asking questions, receiving replies and providing feedback via a text-based chat interface. The data from 73 undergraduate volunteer participants at southeastern US university were r</context>
</contexts>
<marker>Dzikovska, Bental, Moore, Steinhauser, Campbell, Farrow, Callaway, 2010</marker>
<rawString>Myroslava Dzikovska, Diana Bental, Johanna D. Moore, Natalie B. Steinhauser, Gwendolyn E. Campbell, Elaine Farrow, and Charles B. Callaway. 2010a. Intelligent tutoring with natural language support in the Beetle II system. In Sustaining TEL: From Innovation to Learning and Practice - 5th European Conference on Technology Enhanced Learning, (EC-TEL 2010), Barcelona, Spain, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Johanna D Moore</author>
<author>Natalie Steinhauser</author>
<author>Gwendolyn Campbell</author>
<author>Elaine Farrow</author>
<author>Charles B Callaway</author>
</authors>
<title>Beetle II: a system for tutoring and computational linguistics experimentation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL2010) demo session,</booktitle>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1783" citStr="Dzikovska et al., 2010" startWordPosition="263" endWordPosition="266">r a variety of computational linguistics tasks. 1 Introduction In human-human tutoring, it is an effective strategy to ask students to explain instructional material in their own words. Self-explanation (Chi et al., 1994) and contentful talk focused on the domain are correlated with better learning outcomes (Litman et al., 2009; Chi et al., 1994). There has therefore been much interest in developing automated tutorial dialogue systems that ask students open-ended explanation questions (Graesser et al., 1999; Aleven et al., 2001; Jordan et al., 2006; VanLehn et al., 2007; Nielsen et al., 2009; Dzikovska et al., 2010a). In order to do this well, it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue about the instructional content. Thus, systems must respond appropriately to student explanations, and must provide detailed, flexible and appropriate feedback (Aleven et al., 2002; Jordan et al., 2004). In simple domains, we can adopt a knowledge engineering approach and build a domain model and a diagnoser, together with a natural language parser to produce detailed semantic representations of student input (Glass, 2000; Aleven et al., </context>
<context position="6184" citStr="Dzikovska et al., 2010" startWordPosition="964" endWordPosition="967">ers. This will provide a basis for the development, comparison and evaluation of alternative approaches to the error classification task. We believe that the natural language capabilities needed for this task will be directly applicable to a far wider range of tasks in educational assessment, information extraction and computational semantics. This dataset is publicly available and will be used in a community-wide shared task. 2 Corpus The data set we developed draws on two established sources – a data set collected and annotated during an evaluation of the BEETLE II tutorial dialogue system (Dzikovska et al., 2010a) (henceforth, BEETLE corpus) and a set of student answers to questions from 16 science modules in the Assessing Science Knowledge (ASK) assessment inventory (Lawrence Hall of Science, 2006) (henceforth, the Science Entailments Bank or SCIENTSBANK). In both corpora, each question was associated with one or more reference answers provided by the experts. Student answers were evaluated against these reference answers and, using corpus-specific annotation schemes, assigned labels for correctness. In order to reconcile the two different schemes and to cast the task in terms of standard supervised</context>
<context position="9458" citStr="Dzikovska et al., 2010" startWordPosition="1490" endWordPosition="1493"> contradictory correct nuggets, but parts are missing irrelevant an answer that contradicts some part of the reference answer contains domain content, but does not answer the question Figure 1: The set of answer labels used in our task We further discuss the relationship with the task of recognizing textual entailment in Section 5. In the rest of this section, we describe our corpora and discuss how we obtained these labels from the raw data available in our datasets. 2.1 BEETLE II data The BEETLE corpus consists of the interactions between students and the BEETLE II tutorial dialogue system (Dzikovska et al., 2010b). The BEETLE II system is an intelligent tutoring system that teaches students with no knowledge of high-school physics concepts in basic electricity and electronics. In the first system evaluation, students spend 3-5 hours going through prepared reading material, building and observing circuits in the simulator and interacting with a dialogue-based tutor. The interaction was by keyboard, with the computer tutor asking questions, receiving replies and providing feedback via a text-based chat interface. The data from 73 undergraduate volunteer participants at southeastern US university were r</context>
</contexts>
<marker>Dzikovska, Moore, Steinhauser, Campbell, Farrow, Callaway, 2010</marker>
<rawString>Myroslava O. Dzikovska, Johanna D. Moore, Natalie Steinhauser, Gwendolyn Campbell, Elaine Farrow, and Charles B. Callaway. 2010b. Beetle II: a system for tutoring and computational linguistics experimentation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL2010) demo session, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myroslava O Dzikovska</author>
<author>Peter Bell</author>
<author>Amy Isard</author>
<author>Johanna D Moore</author>
</authors>
<title>Evaluating language understanding accuracy with respect to objective outcomes in a dialogue system.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for computational Linguistics,</booktitle>
<location>Avignon, France,</location>
<contexts>
<context position="17852" citStr="Dzikovska et al., 2012" startWordPosition="2807" endWordPosition="2810">based on lexical similarity, which we report in order to offer a substantial example of applying the same classifier to both portions of the dataset. 3.1 BEETLE II system baseline The interpretation component of the BEETLE II system uses a syntactic parser and a set of handauthored rules to extract the domain-specific semantic representations of student utterances from the text. These representations were then matched against the semantic representations of expected correct answers supplied by tutors. The resulting system output was automatically mapped into our target labels as discussed in (Dzikovska et al., 2012). 3.2 Lexical similarity baseline To provide a higher baseline that is comparable across both subsets of the data, we built a simple decision tree classifier using the Weka 3.6.2 implementation of C4.5 pruned decision trees (weka.classifiers.trees.J48 class), with default parameters. As features, we used lexical similarity scores computed by the Text::Similarity package with default parameters2. The code computes four similarity metrics – the raw number of overlapping words, F1 score, Lesk score and cosine score. We compared the learner response to the expected answer(s) and the question, resu</context>
<context position="25551" citStr="Dzikovska et al., 2012" startWordPosition="4050" endWordPosition="4053"> to whether corrective feedback should be issued. This is not unexpected given how the system was designed – since misunderstandings caused dialogue breakdown in pilot tests, the parser was built to prefer rejecting utterances as uninterpretable rather than assigning them an incorrect class, leading to a considerably lower recall. Around 31% of utterances could not be interpreted. Our recent analysis shows that both incorrect interpretations (in particular, confusions between “partially correct incomplete” and “contradictory”) and rejections have significant negative effects on learning gain (Dzikovska et al., 2012). Classifiers can be tuned to reject examples where classification confidence falls below a given threshold, resulting in precision-recall trade-offs. Our baseline classifier classified all answer instances; exploring the possibilities for rejecting some low-confidence answers is planned for future work. 4.4 SCIENTSBANK baseline performance The accuracy of the majority class baseline (which assumes all answers are “correct”) is 40% for SCIENTSBANK, about the same as it was for BEETLE. The evaluation results, based on 10-fold crossvalidation, for our simple lexical similarity classifier are pre</context>
</contexts>
<marker>Dzikovska, Bell, Isard, Moore, 2012</marker>
<rawString>Myroslava O. Dzikovska, Peter Bell, Amy Isard, and Johanna D. Moore. 2012. Evaluating language understanding accuracy with respect to objective outcomes in a dialogue system. In Proceedings of the 13th Conference of the European Chapter of the Association for computational Linguistics, Avignon, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The third PASCAL recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>1--9</pages>
<location>Prague,</location>
<contexts>
<context position="33719" citStr="Giampiccolo et al., 2007" startWordPosition="5360" endWordPosition="5363">See http://www.cs.york.ac.uk/semeval-2013/task4/ numerical score, using support vector regression and similar techniques. Either approach is reasonable, but we think that feedback is the more challenging test of a system’s ultimate abilities, and therefore a better candidate for the shared task. The corpora from those systems, alongside with new corpora currently being collected in BEETLE and SCIENTSBANK domains, can serve as sources of data for future tasks extensions. Future systems developed for this task can benefit from the large amount of existing work on recognizing textual entailment (Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009) and on detecting contradiction (Ritter et al., 2008; De Marneffe et al., 2008). However, there are substantial challenges in applying the RTE tools directly to this data set. Our set of labels is more fine-grained than RTE labels to reflect the needs of intelligent tutoring systems (see Section 2). In addition, the topperforming systems in RTE5 3-way task, as well as contradiction detection methods, rely on NLP tools such as dependency parsers and semantic role labelers; these do not perform well on specialized terminology and language const</context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing, pages 1–9, Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Hoa Trang Dang</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Elena Cabrio</author>
<author>Bill Dolan</author>
</authors>
<title>The fourth PASCAL recognizing textual entailment challenge.</title>
<date>2008</date>
<booktitle>In Proceedings of Text Analysis Conference (TAC)</booktitle>
<location>Gaithersburg, MD,</location>
<contexts>
<context position="33745" citStr="Giampiccolo et al., 2008" startWordPosition="5364" endWordPosition="5367">uk/semeval-2013/task4/ numerical score, using support vector regression and similar techniques. Either approach is reasonable, but we think that feedback is the more challenging test of a system’s ultimate abilities, and therefore a better candidate for the shared task. The corpora from those systems, alongside with new corpora currently being collected in BEETLE and SCIENTSBANK domains, can serve as sources of data for future tasks extensions. Future systems developed for this task can benefit from the large amount of existing work on recognizing textual entailment (Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009) and on detecting contradiction (Ritter et al., 2008; De Marneffe et al., 2008). However, there are substantial challenges in applying the RTE tools directly to this data set. Our set of labels is more fine-grained than RTE labels to reflect the needs of intelligent tutoring systems (see Section 2). In addition, the topperforming systems in RTE5 3-way task, as well as contradiction detection methods, rely on NLP tools such as dependency parsers and semantic role labelers; these do not perform well on specialized terminology and language constructs coming from (typed) </context>
</contexts>
<marker>Giampiccolo, Dang, Magnini, Dagan, Cabrio, Dolan, 2008</marker>
<rawString>Danilo Giampiccolo, Hoa Trang Dang, Bernardo Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan. 2008. The fourth PASCAL recognizing textual entailment challenge. In Proceedings of Text Analysis Conference (TAC) 2008, Gaithersburg, MD, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Glass</author>
</authors>
<title>Processing language input in the CIRCSIM-Tutor intelligent tutoring system.</title>
<date>2000</date>
<booktitle>In Proceedings of the AAAI Fall Symposium on Building Dialogue Systems for Tutorial Applications.</booktitle>
<contexts>
<context position="2366" citStr="Glass, 2000" startWordPosition="355" endWordPosition="356">2009; Dzikovska et al., 2010a). In order to do this well, it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue about the instructional content. Thus, systems must respond appropriately to student explanations, and must provide detailed, flexible and appropriate feedback (Aleven et al., 2002; Jordan et al., 2004). In simple domains, we can adopt a knowledge engineering approach and build a domain model and a diagnoser, together with a natural language parser to produce detailed semantic representations of student input (Glass, 2000; Aleven et al., 2002; Pon-Barry et al., 2004; Callaway et al., 2006; Dzikovska et al., 2010a). The advantage of this approach is that it allows for flexible adaptation of feedback to a variety of factors such as student performance. For example, it is easy for the system to know if the student made the same error before, and adjust its feedback to reflect it. Moreover, this approach allows for easy addition of new exercises : as long as an exercise relies on the concepts covered by the domain model, the system can apply standard instructional strategies to each new question automatically. How</context>
</contexts>
<marker>Glass, 2000</marker>
<rawString>Michael Glass. 2000. Processing language input in the CIRCSIM-Tutor intelligent tutoring system. In Proceedings of the AAAI Fall Symposium on Building Dialogue Systems for Tutorial Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Graesser</author>
<author>P Wiemer-Hastings</author>
<author>K WiemerHastings</author>
<author>R Kreuz</author>
</authors>
<title>Autotutor: A simulation of a human tutor. Cognitive Systems Research,</title>
<date>1999</date>
<pages>1--35</pages>
<contexts>
<context position="1673" citStr="Graesser et al., 1999" startWordPosition="243" endWordPosition="246">tutorial dialogue systems and textual entailment, contradiction detection and other techniques of interest for a variety of computational linguistics tasks. 1 Introduction In human-human tutoring, it is an effective strategy to ask students to explain instructional material in their own words. Self-explanation (Chi et al., 1994) and contentful talk focused on the domain are correlated with better learning outcomes (Litman et al., 2009; Chi et al., 1994). There has therefore been much interest in developing automated tutorial dialogue systems that ask students open-ended explanation questions (Graesser et al., 1999; Aleven et al., 2001; Jordan et al., 2006; VanLehn et al., 2007; Nielsen et al., 2009; Dzikovska et al., 2010a). In order to do this well, it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue about the instructional content. Thus, systems must respond appropriately to student explanations, and must provide detailed, flexible and appropriate feedback (Aleven et al., 2002; Jordan et al., 2004). In simple domains, we can adopt a knowledge engineering approach and build a domain model and a diagnoser, together with a natur</context>
<context position="3393" citStr="Graesser et al., 1999" startWordPosition="525" endWordPosition="528">ition of new exercises : as long as an exercise relies on the concepts covered by the domain model, the system can apply standard instructional strategies to each new question automatically. However, this approach is significantly limited by the requirement that the domain be small enough to allow comprehensive knowledge engineering, and it is very labor-intensive even for small domains. Alternatively, we can adopt a data-driven approach, asking human tutors to anticipate in advance a range of possible correct and incorrect answers, and associating each answer with an appropriate remediation (Graesser et al., 1999; Jordan et al., 2004; VanLehn et al., 2007). The advantage of this approach is that it allows more complex and interesting domains and provides a good framework for eliciting the necessary information from the human experts. A weakness of this approach, which 200 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 200–210, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics also arises in content-scoring applications such as ETS’s c-rater (Leacock and Chodorow, 2003), is that human e</context>
</contexts>
<marker>Graesser, Wiemer-Hastings, WiemerHastings, Kreuz, 1999</marker>
<rawString>A. C. Graesser, P. Wiemer-Hastings, K. WiemerHastings, and R. Kreuz. 1999. Autotutor: A simulation of a human tutor. Cognitive Systems Research, 1:35–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela W Jordan</author>
<author>Maxim Makatchev</author>
<author>Kurt VanLehn</author>
</authors>
<title>Combining competing language understanding approaches in an intelligent tutoring system. In</title>
<date>2004</date>
<booktitle>Intelligent Tutoring Systems,</booktitle>
<volume>3220</volume>
<pages>346--357</pages>
<editor>James C. Lester, Rosa Maria Vicari, and F´abio Paraguac¸u, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="2143" citStr="Jordan et al., 2004" startWordPosition="318" endWordPosition="321">erefore been much interest in developing automated tutorial dialogue systems that ask students open-ended explanation questions (Graesser et al., 1999; Aleven et al., 2001; Jordan et al., 2006; VanLehn et al., 2007; Nielsen et al., 2009; Dzikovska et al., 2010a). In order to do this well, it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue about the instructional content. Thus, systems must respond appropriately to student explanations, and must provide detailed, flexible and appropriate feedback (Aleven et al., 2002; Jordan et al., 2004). In simple domains, we can adopt a knowledge engineering approach and build a domain model and a diagnoser, together with a natural language parser to produce detailed semantic representations of student input (Glass, 2000; Aleven et al., 2002; Pon-Barry et al., 2004; Callaway et al., 2006; Dzikovska et al., 2010a). The advantage of this approach is that it allows for flexible adaptation of feedback to a variety of factors such as student performance. For example, it is easy for the system to know if the student made the same error before, and adjust its feedback to reflect it. Moreover, this</context>
<context position="3414" citStr="Jordan et al., 2004" startWordPosition="529" endWordPosition="532">: as long as an exercise relies on the concepts covered by the domain model, the system can apply standard instructional strategies to each new question automatically. However, this approach is significantly limited by the requirement that the domain be small enough to allow comprehensive knowledge engineering, and it is very labor-intensive even for small domains. Alternatively, we can adopt a data-driven approach, asking human tutors to anticipate in advance a range of possible correct and incorrect answers, and associating each answer with an appropriate remediation (Graesser et al., 1999; Jordan et al., 2004; VanLehn et al., 2007). The advantage of this approach is that it allows more complex and interesting domains and provides a good framework for eliciting the necessary information from the human experts. A weakness of this approach, which 200 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 200–210, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics also arises in content-scoring applications such as ETS’s c-rater (Leacock and Chodorow, 2003), is that human experts find it extrem</context>
</contexts>
<marker>Jordan, Makatchev, VanLehn, 2004</marker>
<rawString>Pamela W. Jordan, Maxim Makatchev, and Kurt VanLehn. 2004. Combining competing language understanding approaches in an intelligent tutoring system. In James C. Lester, Rosa Maria Vicari, and F´abio Paraguac¸u, editors, Intelligent Tutoring Systems, volume 3220 of Lecture Notes in Computer Science, pages 346–357. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela Jordan</author>
<author>Maxim Makatchev</author>
<author>Umarani Pappuswamy</author>
<author>Kurt VanLehn</author>
<author>Patricia Albacete</author>
</authors>
<title>A natural language tutorial dialogue system for physics.</title>
<date>2006</date>
<booktitle>In Proceedings of the 19th International FLAIRS conference.</booktitle>
<contexts>
<context position="1715" citStr="Jordan et al., 2006" startWordPosition="251" endWordPosition="254">ment, contradiction detection and other techniques of interest for a variety of computational linguistics tasks. 1 Introduction In human-human tutoring, it is an effective strategy to ask students to explain instructional material in their own words. Self-explanation (Chi et al., 1994) and contentful talk focused on the domain are correlated with better learning outcomes (Litman et al., 2009; Chi et al., 1994). There has therefore been much interest in developing automated tutorial dialogue systems that ask students open-ended explanation questions (Graesser et al., 1999; Aleven et al., 2001; Jordan et al., 2006; VanLehn et al., 2007; Nielsen et al., 2009; Dzikovska et al., 2010a). In order to do this well, it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue about the instructional content. Thus, systems must respond appropriately to student explanations, and must provide detailed, flexible and appropriate feedback (Aleven et al., 2002; Jordan et al., 2004). In simple domains, we can adopt a knowledge engineering approach and build a domain model and a diagnoser, together with a natural language parser to produce detailed sem</context>
</contexts>
<marker>Jordan, Makatchev, Pappuswamy, VanLehn, Albacete, 2006</marker>
<rawString>Pamela Jordan, Maxim Makatchev, Umarani Pappuswamy, Kurt VanLehn, and Patricia Albacete. 2006. A natural language tutorial dialogue system for physics. In Proceedings of the 19th International FLAIRS conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela W Jordan</author>
</authors>
<title>Using student explanations as models for adapting tutorial dialogue.</title>
<date>2004</date>
<editor>In Valerie Barr and Zdravko Markov, editors, FLAIRS Conference.</editor>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="4447" citStr="Jordan, 2004" startWordPosition="688" endWordPosition="689">�2012 Association for Computational Linguistics also arises in content-scoring applications such as ETS’s c-rater (Leacock and Chodorow, 2003), is that human experts find it extremely difficult to predict with any certainty what the full range of student responses will be. This leads to a lack of adaptivity and generality – if the system designers have failed to predict the full range of possibilities, students will often receive the default feedback. It is frustrating and confusing for students to repeatedly receive the same feedback, regardless of their past performance or dialogue context (Jordan, 2004). Our goal is to address the weaknesses of the datadriven approach by creating a framework for supporting more flexible and systematic feedback. Our approach identifies general classes of error, such as omissions, incorrect statements and off-topic statements, then aims to develop general remediation strategies for each error type. This has the potential to free system designers from the need to pre-author separate remediations for each individual question. A precondition for the success of this approach is that the system be able to identify error types based on the student response and the m</context>
</contexts>
<marker>Jordan, 2004</marker>
<rawString>Pamela W. Jordan. 2004. Using student explanations as models for adapting tutorial dialogue. In Valerie Barr and Zdravko Markov, editors, FLAIRS Conference. AAAI Press.</rawString>
</citation>
<citation valid="true">
<title>Assessing Science Knowledge (ask).</title>
<date>2006</date>
<institution>Lawrence Hall of Science.</institution>
<marker>2006</marker>
<rawString>Lawrence Hall of Science. 2006. Assessing Science Knowledge (ask). University of California at Berkeley, NSF-0242510.</rawString>
</citation>
<citation valid="true">
<title>Full option science system.</title>
<date>2011</date>
<institution>Lawrence Hall of Science.</institution>
<marker>2011</marker>
<rawString>Lawrence Hall of Science. 2011. Full option science system.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>C-rater: Automated scoring of short-answer questions.</title>
<date>2003</date>
<journal>Computers and the Humanities,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="3976" citStr="Leacock and Chodorow, 2003" startWordPosition="610" endWordPosition="613">propriate remediation (Graesser et al., 1999; Jordan et al., 2004; VanLehn et al., 2007). The advantage of this approach is that it allows more complex and interesting domains and provides a good framework for eliciting the necessary information from the human experts. A weakness of this approach, which 200 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 200–210, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics also arises in content-scoring applications such as ETS’s c-rater (Leacock and Chodorow, 2003), is that human experts find it extremely difficult to predict with any certainty what the full range of student responses will be. This leads to a lack of adaptivity and generality – if the system designers have failed to predict the full range of possibilities, students will often receive the default feedback. It is frustrating and confusing for students to repeatedly receive the same feedback, regardless of their past performance or dialogue context (Jordan, 2004). Our goal is to address the weaknesses of the datadriven approach by creating a framework for supporting more flexible and syste</context>
<context position="32888" citStr="Leacock and Chodorow, 2003" startWordPosition="5233" endWordPosition="5236">om the same general domain – science. Unseen Answers is the typical scenario for the vast majority of prior work – training and testing on responses to the same questions. Unseen Questions and Unseen Domains allow researchers to evaluate how well their systems generalize to near and far domains, respectively. The primary target application for this work is intelligent tutoring systems, where the classification of responses is intended to facilitate specific pedagogic feedback. Beneath the surface, the baseline systems reported here are more similar to grading systems that use the approach of (Leacock and Chodorow, 2003), which uses classifier technology to detect expressions of facet-like concepts, then converts the result to a numerical score, than to grading systems like (Mohler et al., 2011), which directly produces a 4See http://www.cs.york.ac.uk/semeval-2013/task4/ numerical score, using support vector regression and similar techniques. Either approach is reasonable, but we think that feedback is the more challenging test of a system’s ultimate abilities, and therefore a better candidate for the shared task. The corpora from those systems, alongside with new corpora currently being collected in BEETLE a</context>
</contexts>
<marker>Leacock, Chodorow, 2003</marker>
<rawString>Claudia Leacock and Martin Chodorow. 2003. C-rater: Automated scoring of short-answer questions. Computers and the Humanities, 37(4):389–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>Evaluating text categorization.</title>
<date>1991</date>
<booktitle>In Proceedings of the workshop on Speech and Natural Language, HLT ’91,</booktitle>
<pages>312--318</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="22658" citStr="Lewis, 1991" startWordPosition="3594" endWordPosition="3595"> and F1 are defined similarly. Micro-averaging takes class sizes into account, so a system that performs well on the most common classes will have a high micro-average score. This is the most commonly used classifier evaluation metric. Note that, in particular, overall classification accuracy (defined as the number of correctly classified instances out of all instances) is mathematically equivalent to micro-averaged recall (Abudawood and Flach, 2011). However, macro-averaging better reflects performance on small classes, and is commonly used for unbalanced classification problems (see, e.g., (Lewis, 1991)). We report both values in our results. BEETLE SCIENTSBANK Label Count Freq. Count Freq. correct 1157 0.42 2095 0.40 partially correct 626 0.23 1431 0.27 incomplete contradictory 656 0.24 526 0.10 irrelevant 86 0.03 1175 0.22 non domain 204 0.07 24 0.005 total 2729 5251 Table 1: Distribution of annotated labels in the data In addition, we report the system scores on the binary decision of whether or not the corrective feedback should be issued (denoted “corrective feedback” in the results table). It assumes that a tutoring system using a classifier will give corrective feedback if the classif</context>
</contexts>
<marker>Lewis, 1991</marker>
<rawString>David D. Lewis. 1991. Evaluating text categorization. In Proceedings of the workshop on Speech and Natural Language, HLT ’91, pages 312–318, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>Johanna Moore</author>
<author>Myroslava Dzikovska</author>
<author>Elaine Farrow</author>
</authors>
<title>Using natural language processing to analyze tutorial dialogue corpora across domains and modalities.</title>
<date>2009</date>
<booktitle>In Proceedings of 14th International Conference on Artificial Intelligence in Education (AIED),</booktitle>
<location>Brighton, UK,</location>
<contexts>
<context position="1490" citStr="Litman et al., 2009" startWordPosition="215" endWordPosition="218">systems. We believe that this corpus will be of interest to the researchers working in textual entailment and will stimulate new developments both in natural language processing in tutorial dialogue systems and textual entailment, contradiction detection and other techniques of interest for a variety of computational linguistics tasks. 1 Introduction In human-human tutoring, it is an effective strategy to ask students to explain instructional material in their own words. Self-explanation (Chi et al., 1994) and contentful talk focused on the domain are correlated with better learning outcomes (Litman et al., 2009; Chi et al., 1994). There has therefore been much interest in developing automated tutorial dialogue systems that ask students open-ended explanation questions (Graesser et al., 1999; Aleven et al., 2001; Jordan et al., 2006; VanLehn et al., 2007; Nielsen et al., 2009; Dzikovska et al., 2010a). In order to do this well, it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue about the instructional content. Thus, systems must respond appropriately to student explanations, and must provide detailed, flexible and appropriat</context>
</contexts>
<marker>Litman, Moore, Dzikovska, Farrow, 2009</marker>
<rawString>Diane Litman, Johanna Moore, Myroslava Dzikovska, and Elaine Farrow. 2009. Using natural language processing to analyze tutorial dialogue corpora across domains and modalities. In Proceedings of 14th International Conference on Artificial Intelligence in Education (AIED), Brighton, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mohler</author>
<author>Razvan Bunescu</author>
<author>Rada Mihalcea</author>
</authors>
<title>Learning to grade short answer questions using semantic similarity measures and dependency graph alignments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>752--762</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="33066" citStr="Mohler et al., 2011" startWordPosition="5261" endWordPosition="5264">ons and Unseen Domains allow researchers to evaluate how well their systems generalize to near and far domains, respectively. The primary target application for this work is intelligent tutoring systems, where the classification of responses is intended to facilitate specific pedagogic feedback. Beneath the surface, the baseline systems reported here are more similar to grading systems that use the approach of (Leacock and Chodorow, 2003), which uses classifier technology to detect expressions of facet-like concepts, then converts the result to a numerical score, than to grading systems like (Mohler et al., 2011), which directly produces a 4See http://www.cs.york.ac.uk/semeval-2013/task4/ numerical score, using support vector regression and similar techniques. Either approach is reasonable, but we think that feedback is the more challenging test of a system’s ultimate abilities, and therefore a better candidate for the shared task. The corpora from those systems, alongside with new corpora currently being collected in BEETLE and SCIENTSBANK domains, can serve as sources of data for future tasks extensions. Future systems developed for this task can benefit from the large amount of existing work on rec</context>
</contexts>
<marker>Mohler, Bunescu, Mihalcea, 2011</marker>
<rawString>Michael Mohler, Razvan Bunescu, and Rada Mihalcea. 2011. Learning to grade short answer questions using semantic similarity measures and dependency graph alignments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 752–762, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney D Nielsen</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Martha Palmer</author>
</authors>
<title>Annotating students’ understanding of science concepts.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Language Resources and Evaluation Conference, (LREC08),</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="12575" citStr="Nielsen et al., 2008" startWordPosition="1988" endWordPosition="1991">ated to domain content, student’s metacognitive state, or social interaction. Utterances addressing domain content were further classified with respect to their correctness as described in the table. The Kappa value for this annotation effort was r. = 0.69. This annotation maps straightforwardly into our set of labels. The social and metacognitive statements are mapped to the “non domain” label; “pc some error”, “pc” and “incorrect” are mapped to the “contradictory” label; and the other classes have a one-to-one correspondence with our task labels. 2.2 SCIENTSBANK data The SCIENTSBANK corpus (Nielsen et al., 2008) consists of student responses to science assessment 202 Category Subcategory Description Metacognitive positive content-free expressions describing student knowledge, e.g., “I don’t negative know” Social positive expressions describing student’s attitudes towards themselves and negative the computer (mostly negative in this data, e.g., “You are stupid”) neutral Content correct the utterance addresses domain content. pc some missing the student answer is fully correct incorrect the student said something correct, but incomplete pc some error the student’s answer is completely incorrect pc the </context>
</contexts>
<marker>Nielsen, Ward, Martin, Palmer, 2008</marker>
<rawString>Rodney D. Nielsen, Wayne Ward, James H. Martin, and Martha Palmer. 2008. Annotating students’ understanding of science concepts. In Proceedings of the Sixth International Language Resources and Evaluation Conference, (LREC08), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney D Nielsen</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
</authors>
<title>Recognizing entailment in intelligent tutoring systems.</title>
<date>2009</date>
<journal>The Journal of Natural Language Engineering,</journal>
<pages>15--479</pages>
<contexts>
<context position="1759" citStr="Nielsen et al., 2009" startWordPosition="259" endWordPosition="262">hniques of interest for a variety of computational linguistics tasks. 1 Introduction In human-human tutoring, it is an effective strategy to ask students to explain instructional material in their own words. Self-explanation (Chi et al., 1994) and contentful talk focused on the domain are correlated with better learning outcomes (Litman et al., 2009; Chi et al., 1994). There has therefore been much interest in developing automated tutorial dialogue systems that ask students open-ended explanation questions (Graesser et al., 1999; Aleven et al., 2001; Jordan et al., 2006; VanLehn et al., 2007; Nielsen et al., 2009; Dzikovska et al., 2010a). In order to do this well, it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue about the instructional content. Thus, systems must respond appropriately to student explanations, and must provide detailed, flexible and appropriate feedback (Aleven et al., 2002; Jordan et al., 2004). In simple domains, we can adopt a knowledge engineering approach and build a domain model and a diagnoser, together with a natural language parser to produce detailed semantic representations of student input (Glas</context>
<context position="19555" citStr="Nielsen et al., 2009" startWordPosition="3082" endWordPosition="3085">n in their answer and this needs to be taken into account to differentiate, for example, “partially correct incomplete” and “correct” answers. 4 Results 4.1 Experimental Setup We held back part of the data set for use as standard test data in the future challenge tasks. For BEETLE, this consisted of all student answers to 9 out of 56 explanation questions asked by the system, plus approximately 15% of the student answers to the remaining 47 questions, sampling so that the distribution of labels in test data was similar to the training data. For SCIENTSBANK, we used a previous traintest split (Nielsen et al., 2009). For both data sets, the data was split so that in the future we can test how well the different systems generalize: i.e., how well they perform on answers to questions for which they have some sample student answers vs. how well they perform on answers to questions that were not in the training data (e.g., newly created questions in a deployed system). We discuss this in more detail in Section 5. In this paper, we report baseline performance on the training set to demonstrate that the task is sufficiently challenging to be interesting and that systems can be compared using our evaluation met</context>
</contexts>
<marker>Nielsen, Ward, Martin, 2009</marker>
<rawString>Rodney D. Nielsen, Wayne Ward, and James H. Martin. 2009. Recognizing entailment in intelligent tutoring systems. The Journal of Natural Language Engineering, 15:479–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heather Pon-Barry</author>
<author>Brady Clark</author>
<author>Karl Schultz</author>
<author>Elizabeth Owen Bratt</author>
<author>Stanley Peters</author>
</authors>
<title>Advantages of spoken language interaction in dialogue-based intelligent tutoring systems.</title>
<date>2004</date>
<booktitle>In Proceedings of ITS2004,</booktitle>
<pages>390--400</pages>
<contexts>
<context position="2411" citStr="Pon-Barry et al., 2004" startWordPosition="361" endWordPosition="364"> order to do this well, it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue about the instructional content. Thus, systems must respond appropriately to student explanations, and must provide detailed, flexible and appropriate feedback (Aleven et al., 2002; Jordan et al., 2004). In simple domains, we can adopt a knowledge engineering approach and build a domain model and a diagnoser, together with a natural language parser to produce detailed semantic representations of student input (Glass, 2000; Aleven et al., 2002; Pon-Barry et al., 2004; Callaway et al., 2006; Dzikovska et al., 2010a). The advantage of this approach is that it allows for flexible adaptation of feedback to a variety of factors such as student performance. For example, it is easy for the system to know if the student made the same error before, and adjust its feedback to reflect it. Moreover, this approach allows for easy addition of new exercises : as long as an exercise relies on the concepts covered by the domain model, the system can apply standard instructional strategies to each new question automatically. However, this approach is significantly limited </context>
</contexts>
<marker>Pon-Barry, Clark, Schultz, Bratt, Peters, 2004</marker>
<rawString>Heather Pon-Barry, Brady Clark, Karl Schultz, Elizabeth Owen Bratt, and Stanley Peters. 2004. Advantages of spoken language interaction in dialogue-based intelligent tutoring systems. In Proceedings of ITS2004, pages 390–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>D Downey</author>
<author>S Soderland</author>
<author>O Etzioni</author>
</authors>
<title>It’s a contradiction—no, it’s not: a case study using functional relations.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>11--20</pages>
<contexts>
<context position="33823" citStr="Ritter et al., 2008" startWordPosition="5376" endWordPosition="5379"> techniques. Either approach is reasonable, but we think that feedback is the more challenging test of a system’s ultimate abilities, and therefore a better candidate for the shared task. The corpora from those systems, alongside with new corpora currently being collected in BEETLE and SCIENTSBANK domains, can serve as sources of data for future tasks extensions. Future systems developed for this task can benefit from the large amount of existing work on recognizing textual entailment (Giampiccolo et al., 2007; Giampiccolo et al., 2008; Bentivogli et al., 2009) and on detecting contradiction (Ritter et al., 2008; De Marneffe et al., 2008). However, there are substantial challenges in applying the RTE tools directly to this data set. Our set of labels is more fine-grained than RTE labels to reflect the needs of intelligent tutoring systems (see Section 2). In addition, the topperforming systems in RTE5 3-way task, as well as contradiction detection methods, rely on NLP tools such as dependency parsers and semantic role labelers; these do not perform well on specialized terminology and language constructs coming from (typed) dialogue context. We chose to use lexical similarity as a baseline specificall</context>
</contexts>
<marker>Ritter, Downey, Soderland, Etzioni, 2008</marker>
<rawString>A. Ritter, D. Downey, S. Soderland, and O. Etzioni. 2008. It’s a contradiction—no, it’s not: a case study using functional relations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 11–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalie B Steinhauser</author>
<author>Gwendolyn E Campbell</author>
<author>Leanne S Taylor</author>
<author>Simon Caine</author>
<author>Charlie Scott</author>
<author>Myroslava O Dzikovska</author>
<author>Johanna D Moore</author>
</authors>
<title>Talk like an electrician: Student dialogue mimicking behavior in an intelligent tutoring system.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th International Conference on Artificial Intelligence in Education (AIED-2011).</booktitle>
<contexts>
<context position="11305" citStr="Steinhauser et al., 2011" startWordPosition="1781" endWordPosition="1785">etween two terminals”). From the full BEETLE evaluation corpus, we automatically extracted only the students’ answers to explanation and definition questions, since reacting to them appropriately requires processing more complex input than factual questions. The extracted answers were filtered to remove duplicates. In the BEETLE II lesson material there are a number of similar questions and the tutor effectively had a template answer such as ”Terminal X is connected to the negative/positive battery terminal”. A number of students picked up on this and used the same pattern in their responses (Steinhauser et al., 2011). This resulted in a number of answers to certain questions that came from different speakers but which were exact copies of each other. We removed such answers from the data set, since they were likely to be in both the training and test set, thus inflating our results. Note that only exact matches were removed: for example, answers that were nearly identical but contained spelling errors were retained, since they would need to be handled in a practical system. Student utterances were manually labeled using a simplified version of the DEMAND coding scheme (Campbell et al., 2009) shown in Figu</context>
</contexts>
<marker>Steinhauser, Campbell, Taylor, Caine, Scott, Dzikovska, Moore, 2011</marker>
<rawString>Natalie B. Steinhauser, Gwendolyn E. Campbell, Leanne S. Taylor, Simon Caine, Charlie Scott, Myroslava O. Dzikovska, and Johanna D. Moore. 2011. Talk like an electrician: Student dialogue mimicking behavior in an intelligent tutoring system. In Proceedings of the 15th International Conference on Artificial Intelligence in Education (AIED-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt VanLehn</author>
<author>Pamela Jordan</author>
<author>Diane Litman</author>
</authors>
<title>Developing pedagogically effective tutorial dialogue tactics: Experiments and a testbed.</title>
<date>2007</date>
<booktitle>In Proceedings of SLaTE Workshop on Speech and Language Technology in Education,</booktitle>
<location>Farmington, PA,</location>
<contexts>
<context position="1737" citStr="VanLehn et al., 2007" startWordPosition="255" endWordPosition="258">etection and other techniques of interest for a variety of computational linguistics tasks. 1 Introduction In human-human tutoring, it is an effective strategy to ask students to explain instructional material in their own words. Self-explanation (Chi et al., 1994) and contentful talk focused on the domain are correlated with better learning outcomes (Litman et al., 2009; Chi et al., 1994). There has therefore been much interest in developing automated tutorial dialogue systems that ask students open-ended explanation questions (Graesser et al., 1999; Aleven et al., 2001; Jordan et al., 2006; VanLehn et al., 2007; Nielsen et al., 2009; Dzikovska et al., 2010a). In order to do this well, it is not enough to simply ask the initiating question, because students need the experience of engaging in meaningful dialogue about the instructional content. Thus, systems must respond appropriately to student explanations, and must provide detailed, flexible and appropriate feedback (Aleven et al., 2002; Jordan et al., 2004). In simple domains, we can adopt a knowledge engineering approach and build a domain model and a diagnoser, together with a natural language parser to produce detailed semantic representations </context>
<context position="3437" citStr="VanLehn et al., 2007" startWordPosition="533" endWordPosition="536">ise relies on the concepts covered by the domain model, the system can apply standard instructional strategies to each new question automatically. However, this approach is significantly limited by the requirement that the domain be small enough to allow comprehensive knowledge engineering, and it is very labor-intensive even for small domains. Alternatively, we can adopt a data-driven approach, asking human tutors to anticipate in advance a range of possible correct and incorrect answers, and associating each answer with an appropriate remediation (Graesser et al., 1999; Jordan et al., 2004; VanLehn et al., 2007). The advantage of this approach is that it allows more complex and interesting domains and provides a good framework for eliciting the necessary information from the human experts. A weakness of this approach, which 200 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 200–210, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics also arises in content-scoring applications such as ETS’s c-rater (Leacock and Chodorow, 2003), is that human experts find it extremely difficult to predic</context>
</contexts>
<marker>VanLehn, Jordan, Litman, 2007</marker>
<rawString>Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007. Developing pedagogically effective tutorial dialogue tactics: Experiments and a testbed. In Proceedings of SLaTE Workshop on Speech and Language Technology in Education, Farmington, PA, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>