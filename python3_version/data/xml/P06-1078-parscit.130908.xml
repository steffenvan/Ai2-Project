<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000087">
<title confidence="0.9913275">
Incorporating speech recognition confidence into
discriminative named entity recognition of speech data
</title>
<author confidence="0.801171">
Katsuhito Sudoh Hajime Tsukada Hideki Isozaki
</author>
<affiliation confidence="0.658333">
NTT Communication Science Laboratories
Nippon Telegraph and Telephone Corporation
</affiliation>
<address confidence="0.975404">
2-4 Hikaridai, Seika-cho, Keihanna Science City, Kyoto 619-0237, Japan
</address>
<email confidence="0.999703">
{sudoh,tsukada,isozaki}@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.997401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940166666667">
This paper proposes a named entity recog-
nition (NER) method for speech recogni-
tion results that uses confidence on auto-
matic speech recognition (ASR) as a fea-
ture. The ASR confidence feature indi-
cates whether each word has been cor-
rectly recognized. The NER model is
trained using ASR results with named en-
tity (NE) labels as well as the correspond-
ing transcriptions with NE labels. In ex-
periments using support vector machines
(SVMs) and speech data from Japanese
newspaper articles, the proposed method
outperformed a simple application of text-
based NER to ASR results in NER F-
measure by improving precision. These
results show that the proposed method is
effective in NER for noisy inputs.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997210611111111">
As network bandwidths and storage capacities
continue to grow, a large volume of speech data
including broadcast news and PodCasts is becom-
ing available. These data are important informa-
tion sources as well as such text data as newspaper
articles and WWW pages. Speech data as infor-
mation sources are attracting a great deal of inter-
est, such as DARPA’s global autonomous language
exploitation (GALE) program. We also aim to use
them for information extraction (IE), question an-
swering, and indexing.
Named entity recognition (NER) is a key tech-
nique for IE and other natural language process-
ing tasks. Named entities (NEs) are the proper ex-
pressions for things such as peoples’ names, loca-
tions’ names, and dates, and NER identifies those
expressions and their categories. Unlike text data,
speech data introduce automatic speech recogni-
tion (ASR) error problems to NER. Although im-
provements to ASR are needed, developing a ro-
bust NER for noisy word sequences is also impor-
tant. In this paper, we focus on the NER of ASR
results and discuss the suppression of ASR error
problems in NER.
Most previous studies of the NER of speech
data used generative models such as hidden
Markov models (HMMs) (Miller et al., 1999;
Palmer and Ostendorf, 2001; Horlock and King,
2003b; B´echet et al., 2004; Favre et al., 2005).
On the other hand, in text-based NER, better re-
sults are obtained using discriminative schemes
such as maximum entropy (ME) models (Borth-
wick, 1999; Chieu and Ng, 2003), support vec-
tor machines (SVMs) (Isozaki and Kazawa, 2002),
and conditional random fields (CRFs) (McCal-
lum and Li, 2003). Zhai et al. (2004) applied a
text-level ME-based NER to ASR results. These
models have an advantage in utilizing various fea-
tures, such as part-of-speech information, charac-
ter types, and surrounding words, which may be
overlapped, while overlapping features are hard to
use in HMM-based models.
To deal with ASR error problems in NER,
Palmer and Ostendorf (2001) proposed an HMM-
based NER method that explicitly models ASR er-
rors using ASR confidence and rejects erroneous
word hypotheses in the ASR results. Such rejec-
tion is especially effective when ASR accuracy is
relatively low because many misrecognized words
may be extracted as NEs, which would decrease
NER precision.
Motivated by these issues, we extended their ap-
proach to discriminative models and propose an
NER method that deals with ASR errors as fea-
</bodyText>
<page confidence="0.955911">
617
</page>
<note confidence="0.867138">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 617–624,
Sydney, July 2006. c�2006 Association for Computational Linguistics
Speech data
ASR
</note>
<bodyText confidence="0.9999352">
tures. We use NE-labeled ASR results for training
to incorporate the features into the NER model as
well as the corresponding transcriptions with NE
labels. In testing, ASR errors are identified by
ASR confidence scores and are used for the NER.
In experiments using SVM-based NER and speech
data from Japanese newspaper articles, the pro-
posed method increased the NER F-measure, es-
pecially in precision, compared to simply applying
text-based NER to the ASR results.
</bodyText>
<sectionHeader confidence="0.99611" genericHeader="method">
2 SVM-based NER
</sectionHeader>
<bodyText confidence="0.998813846153846">
NER is a kind of chunking problem that can
be solved by classifying words into NE classes
that consist of name categories and such chunk-
ing states as PERSON-BEGIN (the beginning of
a person’s name) and LOCATION-MIDDLE (the
middle of a location’s name). Many discrimi-
native methods have been applied to NER, such
as decision trees (Sekine et al., 1998), ME mod-
els (Borthwick, 1999; Chieu and Ng, 2003), and
CRFs (McCallum and Li, 2003). In this paper, we
employ an SVM-based NER method in the follow-
ing way that showed good NER performance in
Japanese (Isozaki and Kazawa, 2002).
We define three features for each word: the
word itself, its part-of-speech tag, and its charac-
ter type. We also use those features for the two
preceding and succeeding words for context de-
pendence and use 15 features when classifying a
word. Each feature is represented by a binary
value (1 or 0), for example, “whether the previous
word is Japan,” and each word is classified based
on a long binary vector where only 15 elements
are 1.
We have two problems when solving NER
using SVMs. One, SVMs can solve only a
two-class problem. We reduce multi-class prob-
lems of NER to a group of two-class problems
using the one-against-all approach, where each
SVM is trained to distinguish members of a
class (e.g., PERSON-BEGIN) from non-members
(PERSON-MIDDLE,MONEY-BEGIN,... ). Inthis
approach, two or more classes may be assigned to
a word or no class may be assigned to a word. To
avoid these situations, we choose class c that has
the largest SVM output score g,(x) among all oth-
ers.
The other is that the NE label sequence must be
consistent; for example, ARTIFACT-END
must follow ARTIFACT-BEGIN or
</bodyText>
<figure confidence="0.953820636363636">
Manual
transcription
Transcriptions
NE labeling
NE-labeled
transcriptions
Setting ASR
confidence
feature to 1
Text-based
training data
</figure>
<figureCaption confidence="0.999914">
Figure 1: Procedure for preparing training data.
</figureCaption>
<bodyText confidence="0.999452833333333">
ARTIFACT-MIDDLE. We use a Viterbi search to
obtain the best and consistent NE label sequence
after classifying all words in a sentence, based
on probability-like values obtained by applying
sigmoid function s,,,(x) = 1/(1 + exp(−Q,,,x)) to
SVM output score g,(x).
</bodyText>
<sectionHeader confidence="0.999147" genericHeader="method">
3 Proposed method
</sectionHeader>
<subsectionHeader confidence="0.999883">
3.1 Incorporating ASR confidence into NER
</subsectionHeader>
<bodyText confidence="0.999989904761905">
In the NER of ASR results, ASR errors cause NEs
to be missed and erroneous NEs to be recognized.
If one or more words constituting an NE are mis-
recognized, we cannot recognize the correct NE.
Even if all words constituting an NE are correctly
recognized, we may not recognize the correct NE
due to ASR errors on context words. To avoid
this problem, we model ASR errors using addi-
tional features that indicate whether each word is
correctly recognized. Our NER model is trained
using ASR results with a feature, where feature
values are obtained through alignment to the cor-
responding transcriptions. In testing, we estimate
feature values using ASR confidence scores. In
this paper, this feature is called the ASR confidence
feature.
Note that we only aim to identify NEs that are
correctly recognized by ASR, and NEs containing
ASR errors are not regarded as NEs. Utilizing er-
roneous NEs is a more difficult problem that is be-
yond the scope of this paper.
</bodyText>
<subsectionHeader confidence="0.966778">
3.2 Training NER model
</subsectionHeader>
<figureCaption confidence="0.628788">
Figure 1 illustrates the procedure for preparing
training data from speech data. First, the speech
</figureCaption>
<figure confidence="0.995395875">
ASR results
ASR-based
training data
Alignment
&amp;
identifying
ASR errors
and NEs
</figure>
<page confidence="0.99345">
618
</page>
<bodyText confidence="0.99998247826087">
data are manually transcribed and automatically
recognized by the ASR. Second, we label NEs
in the transcriptions and then set the ASR con-
fidence feature values to 1 because the words in
the transcriptions are regarded as correctly recog-
nized words. Finally, we align the ASR results to
the transcriptions to identify ASR errors for the
ASR confidence feature values and to label cor-
rectly recognized NEs in the ASR results. Note
that we label the NEs in the ASR results that exist
in the same positions as the transcriptions. If a part
of an NE is misrecognized, the NE is ignored, and
all words for the NE are labeled as non-NE words
(OTHER). Examples of text-based and ASR-based
training data are shown in Tables 1 and 2. Since
the name Murayama Tomiichi in Table 1 is mis-
recognized in ASR, the correctly recognized word
Murayama is also labeled OTHER in Table 2. An-
other approach can be considered, where misrec-
ognized words are replaced by word error symbols
such as those shown in Table 3. In this case, those
words are rejected, and those part-of-speech and
character type features are not used in NER.
</bodyText>
<subsectionHeader confidence="0.63388">
3.3 ASR confidence scoring for using the
</subsectionHeader>
<bodyText confidence="0.9837232">
proposed NER model
ASR confidence scoring is an important technique
in many ASR applications, and many methods
have been proposed including using word poste-
rior probabilities on word graphs (Wessel et al.,
2001), integrating several confidence measures us-
ing neural networks (Schaaf and Kemp, 1997),
using linear discriminant analysis (Kamppari and
Hazen, 2000), and using SVMs (Zhang and Rud-
nicky, 2001).
Word posterior probability is a commonly used
and effective ASR confidence measure. Word pos-
terior probability p([w; T, t]|X) of word w at time
interval [T, t] for speech signal X is calculated as
follows (Wessel et al., 2001):
</bodyText>
<equation confidence="0.996922">
p([w;T,t]|X)
{p(X|W) (p(W))&amp;quot;J α
, (1)
p(X)
</equation>
<bodyText confidence="0.9999435">
where W is a sentence hypothesis, W [w; T, t] is
the set of sentence hypotheses that include w in
[T, t], p(X|W) is a acoustic model score, p(W )
is a language model score, α is a scaling param-
eter (α&lt;1), and Q is a language model weight.
α is used for scaling the large dynamic range of
</bodyText>
<equation confidence="0.915026666666667">
Word Confidence NE label
Murayama 1 PERSON-BEGIN
Tomiichi 1 PERSON-END
shusho 1 OTHER
wa 1 OTHER
nento 1 DATE-SINGLE
</equation>
<tableCaption confidence="0.7771385">
Table 1: An example of text-based training data.
Word Confidence NE label
</tableCaption>
<bodyText confidence="0.8132738">
Murayama 1 OTHER
shi 0 OTHER
ni 0 OTHER
ichi 0 OTHER
shiyo 0 OTHER
</bodyText>
<equation confidence="0.815714636363636">
wa 1 OTHER
nento 1 DATE-SINGLE
Table 2: An example of ASR-based training data.
Word Confidence NE label
Murayama 1 OTHER
(error) 0 OTHER
(error) 0 OTHER
(error) 0 OTHER
(error) 0 OTHER
wa 1 OTHER
nento 1 DATE-SINGLE
</equation>
<tableCaption confidence="0.7656975">
Table 3: An example of ASR-based training data
with word error symbols.
</tableCaption>
<bodyText confidence="0.99917925">
p(X|W)(p(W))&amp;quot; to avoid a few of the top hy-
potheses dominating posterior probabilities. p(X)
is approximated by the sum over all sentence hy-
potheses and is denoted as
</bodyText>
<equation confidence="0.9989255">
p(X) = E {p(X|W) (p(W))&amp;quot;J α . (2)
W
</equation>
<bodyText confidence="0.992433615384615">
p([w; T, t]|X) can be efficiently calculated using a
forward-backward algorithm.
In this paper, we use SVMs for ASR confidence
scoring to achieve a better performance than when
using word posterior probabilities as ASR confi-
dence scores. SVMs are trained using ASR re-
sults, whose errors are known through their align-
ment to their reference transcriptions. The follow-
ing features are used for confidence scoring: the
word itself, its part-of-speech tag, and its word
posterior probability; those of the two preceding
and succeeding words are also used. The word
itself and its part-of-speech are also represented
</bodyText>
<equation confidence="0.642641">
E=
W EW [w;r,t]
</equation>
<page confidence="0.980271">
619
</page>
<bodyText confidence="0.999936210526316">
by a set of binary values, the same as with an
SVM-based NER. Since all other features are bi-
nary, we reduce real-valued word posterior prob-
ability p to ten binary features for simplicity: (if
0 &lt; p &lt; 0.1, if 0.1 &lt; p &lt; 0.2, ... , and if
0.9 &lt; p &lt; 1.0). To normalize SVMs’ output
scores for ASR confidence, we use a sigmoid func-
tion s,,,(x) = 1/(1 + exp(−O,,,x)). We use these
normalized scores as ASR confidence scores. Al-
though a large variety of features have been pro-
posed in previous studies, we use only these sim-
ple features and reserve the other features for fur-
ther studies.
Using the ASR confidence scores, we estimate
whether each word is correctly recognized. If the
ASR confidence score of a word is greater than
threshold t,,,, the word is estimated as correct, and
we set the ASR confidence feature value to 1; oth-
erwise we set it to 0.
</bodyText>
<subsectionHeader confidence="0.96484">
3.4 Rejection at the NER level
</subsectionHeader>
<bodyText confidence="0.999977117647059">
We use the ASR confidence feature to suppress
ASR error problems; however, even text-based
NERs sometimes make errors. NER performance
is a trade-off between missing correct NEs and
accepting erroneous NEs, and requirements dif-
fer by task. Although we can tune the parame-
ters in training SVMs to control the trade-off, it
seems very hard to find appropriate values for all
the SVMs. We use a simple NER-level rejection
by modifying the SVM output scores for the non-
NE class (OTHER). We add constant offset value to
to each SVM output score for OTHER. With a large
to, OTHER becomes more desirable than the other
NE classes, and many words are classified as non-
NE words and vice versa. Therefore, to works as a
parameter for NER-level rejection. This approach
can also be applied to text-based NER.
</bodyText>
<sectionHeader confidence="0.999819" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999994666666667">
We conducted the following experiments related
to the NER of speech data to investigate the per-
formance of the proposed method.
</bodyText>
<subsectionHeader confidence="0.961572">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999939636363637">
In the experiment, we simulated the procedure
shown in Figure 1 using speech data from the
NE-labeled text corpus. We used the training
data of the Information Retrieval and Extraction
Exercise (IREX) workshop (Sekine and Eriguchi,
2000) as the text corpus, which consisted of 1,174
Japanese newspaper articles (10,718 sentences)
and 18,200 NEs in eight categories (artifact, or-
ganization, location, person, date, time, money,
and percent). The sentences were read by 106
speakers (about 100 sentences per speaker), and
the recorded speech data were used for the exper-
iments. The experiments were conducted with 5-
fold cross validation, using 80% of the 1,174 ar-
ticles and the ASR results of the corresponding
speech data for training SVMs (both for ASR con-
fidence scoring and for NER) and the rest for the
test.
We tokenized the sentences into words and
tagged the part-of-speech information using the
Japanese morphological analyzer ChaSen 1 2.3.3
and then labeled the NEs. Unreadable to-
kens such as parentheses were removed in to-
kenization. After tokenization, the text cor-
pus had 264,388 words of 60 part-of-speech
types. Since three different kinds of charac-
ters are used in Japanese, the character types
used as features included: single-kanji
(words written in a single Chinese charac-
ter), all-kanji (longer words written in Chi-
nese characters), hiragana (words written
in hiragana Japanese phonograms), katakana
(words written in katakana Japanese phono-
grams), number, single-capital (words
with a single capitalized letter), all-capital,
capitalized (only the first letter is capital-
ized), roman (other roman character words), and
others (all other words). We used all the fea-
tures that appeared in each training set (no feature
selection was performed). The chunking states in-
cluded in the NE classes were: BEGIN (beginning
of a NE), MIDDLE (middle of a NE), END (ending
of a NE), and SINGLE (a single-word NE). There
were 33 NE classes (eight categories * four chunk-
ing states + OTHER), and therefore we trained 33
SVMs to distinguish words of a class from words
of other classes. For NER, we used an SVM-based
chunk annotator YamCha 2 0.33 with a quadratic
kernel (1 + j5�· y)2 and a soft margin parameter
of SVMs C=0.1 for training and applied sigmoid
function s,(x) with 0,=1.0 and Viterbi search to
the SVMs’ outputs. These parameters were exper-
imentally chosen using the test set.
We used an ASR engine (Hori et al., 2004) with
a speaker-independent acoustic model. The lan-
</bodyText>
<footnote confidence="0.999968">
1http://chasen.naist.jp/hiki/ChaSen/ (in Japanese)
2http://www.chasen.org/˜taku/software/yamcha/
</footnote>
<page confidence="0.997451">
620
</page>
<bodyText confidence="0.975506146341463">
guage model was a word 3-gram model, trained
using other Japanese newspaper articles (about
340 M words) that were also tokenized using
ChaSen. The vocabulary size of the word 3-gram
model was 426,023. The test-set perplexity over
the text corpus was 76.928. The number of out-
of-vocabulary words was 1,551 (0.587%). 223
(1.23%) NEs in the text corpus contained such out-
of-vocabulary words, so those NEs could not be
correctly recognized by ASR. The scaling param-
eter α was set to 0.01, which showed the best ASR
error estimation results using word posterior prob-
abilities in the test set in terms of receiver operator
characteristic (ROC) curves. The language model
weight Q was set to 15, which is a commonly used
value in our ASR system. The word accuracy ob-
tained using our ASR engine for the overall dataset
was 79.45%. In the ASR results, 82.00% of the
NEs in the text corpus remained. Figure 2 shows
the ROC curves of ASR error estimation for the
overall five cross-validation test sets, using SVM-
based ASR confidence scoring and word posterior
probabilities as ASR confidence scores, where
True positive rate
# correctly recognized words estimated as correct
=
# correctly recognized words
False positive rate
# misrecognized words estimated as correct
=
In SVM-based ASR confidence scoring, we used
the quadratic kernel and C=0.01. Parameter 0,,, of
sigmoid function s,,,(x) was set to 1.0. These pa-
rameters were also experimentally chosen. SVM-
based ASR confidence scoring showed better per-
formance in ASR error estimation than simple
word posterior probabilities by integrating mul-
tiple features. Five values of ASR confidence
threshold t,,, were tested in the following experi-
ments: 0.2, 0.3, 0.4, 0.5, and 0.6 (shown by black
dots in Figure 2).
</bodyText>
<subsectionHeader confidence="0.956674">
4.2 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.959307714285714">
Evaluation was based on an averaged NER F-
measure, which is the harmonic mean of NER pre-
cision and recall:
NER precision =
# correctly recognized NEs
# recognized NEs
# correctly recognized NEs
</bodyText>
<equation confidence="0.792202">
NER recall = �
# NEs in original text
</equation>
<figureCaption confidence="0.814330333333333">
Figure 2: SVM-based confidence scoring outper-
forms word posterior probability for ASR error es-
timation.
</figureCaption>
<bodyText confidence="0.999971375">
A recognized NE was accepted as correct if and
only if it appeared in the same position as its refer-
ence NE through alignment, in addition to having
the correct NE surface and category, because the
same NEs might appear more than once. Compar-
isons of NE surfaces did not include differences
in word segmentation because of the segmentation
ambiguity in Japanese. Note that NER recall with
ASR results could not exceed the rate of the re-
maining NEs after ASR (about 82%) because NEs
containing ASR errors were always lost.
In addition, we also evaluated the NER perfor-
mance in NER precision and recall with NER-
level rejection using the procedure in Section 3.4,
by modifying the non-NE class scores using offset
value to.
</bodyText>
<subsectionHeader confidence="0.868074">
4.3 Compared methods
</subsectionHeader>
<bodyText confidence="0.999795272727273">
We compared several combinations of features
and training conditions for evaluating the effect of
incorporating the ASR confidence feature and in-
vestigating differences among training data: text-
based, ASR-based, and both.
Baseline does not use the ASR confidence fea-
ture and is trained using text-based training data
only.
NoConf-A does not use the ASR confidence
feature and is trained using ASR-based training
data only.
</bodyText>
<figure confidence="0.998477">
0 20 40 60 80 100
False positive rate (%)
True positve rate (%)
100
80
40
20
60
0
SVM-based
confidence
scoring
Word posterior probability
tw
=0.6
tw
=0.5
tw
=0.4
tw
=0.3
tw=0.2
�
# misrecognized words
</figure>
<page confidence="0.981908">
621
</page>
<table confidence="0.999182888888889">
Method Confidence Training Test F-measure (%) Precision (%) Recall (%)
Baseline Not used Text ASR 67.00 70.67 63.70
NoConf-A ASR ASR 65.52 78.86 56.05
NoConf-TA Text+ASR ASR 66.95 77.55 58.91
Conf-A Used ASR ASR* 67.69 76.69 60.59
Proposed Text+ASR ASR* 69.02 78.13 61.81
Conf-Reject Used† Text+ASR ASR* 68.77 77.57 61.78
Conf-UB Used Text+ASR ASR** 73.14 87.51 62.83
Transcription Not used Text Text 84.04 86.27 81.93
</table>
<tableCaption confidence="0.975252">
Table 4: NER results in averaged NER F-measure, precision, and recall without considering NER-level
</tableCaption>
<bodyText confidence="0.982563391304348">
rejection (to = 0). ASR word accuracy was 79.45%, and 82.00% of NEs remained in ASR results.
(†Unconfident words were rejected and replaced by word error symbols, *t,,, = 0.4, **ASR errors were
known.)
NoConf-TA does not use the ASR confidence
feature and is trained using both text-based and
ASR-based training data.
Conf-A uses the ASR confidence feature and is
trained using ASR-based training data only.
Proposed uses the ASR confidence feature and
is trained using both text-based and ASR-based
training data.
Conf-Reject is almost the same as Proposed,
but misrecognized words are rejected and replaced
with word error symbols, as described at the end
of Section 3.2.
The following two methods are for reference.
Conf-UB assumes perfect ASR confidence scor-
ing, so the ASR errors in the test set are known.
The NER model, which is identical to Proposed,
is regarded as the upper-boundary of Proposed.
Transcription applies the same model as Base-
line to reference transcriptions, assuming word ac-
curacy is 100%.
</bodyText>
<subsectionHeader confidence="0.997917">
4.4 NER Results
</subsectionHeader>
<bodyText confidence="0.992614636363636">
In the NER experiments, Proposed achieved the
best results among the above methods. Table
4 shows the NER results obtained by the meth-
ods without considering NER-level rejection (i.e.,
to = 0), using threshold t,,, = 0.4 for Conf-A,
Proposed, and Conf-Reject, which resulted in the
best NER F-measures (see Table 5). Proposed
showed the best F-measure, 69.02%. It outper-
formed Baseline by 2.0%, with a 7.5% improve-
ment in precision, instead of a recall decrease of
1.9%. Conf-Reject showed slightly worse results
</bodyText>
<table confidence="0.999828">
Method t,,, F (%) P (%) R (%)
0.2 66.72 71.28 62.71
0.3 67.32 73.68 61.98
Conf-A 0.4 67.69 76.69 60.59
0.5 67.04 79.64 57.89
0.6 64.48 81.90 53.14
0.2 68.08 72.54 64.14
0.3 68.70 75.11 63.31
Proposed 0.4 69.02 78.13 61.81
0.5 68.17 80.88 58.93
0.6 65.39 83.00 53.96
0.2 68.06 72.49 64.14
0.3 68.61 74.88 63.31
Conf-Reject 0.4 68.77 77.57 61.78
0.5 67.93 80.23 58.91
0.6 64.93 82.05 53.73
</table>
<tableCaption confidence="0.923923">
Table 5: NER results with varying ASR confi-
</tableCaption>
<bodyText confidence="0.9769401875">
dence score threshold t,,, for Conf-A, Proposed,
and Conf-Reject. (F: F-measure, P: precision, R:
recall)
than Proposed. Conf-A resulted in 1.3% worse F-
measure than Proposed. NoConf-A and NoConf-
TA achieved 7-8% higher precision than Base-
line; however, their F-measure results were worse
than Baseline because of the large drop of recall.
The upper-bound results of the proposed method
(Conf-UB) in F-measure was 73.14%, which was
4% higher than Proposed.
Figure 3 shows NER precision and recall with
NER-level rejection by to for Baseline, NoConf-
TA, Proposed, Conf-UB, and Transcription. In the
figure, black dots represent results with to = 0,
as shown in Table 4. By all five methods, we
</bodyText>
<page confidence="0.984768">
622
</page>
<figure confidence="0.717933">
Precision (%)
</figure>
<figureCaption confidence="0.930203">
Figure 3: NER precision and recall with NER-
</figureCaption>
<bodyText confidence="0.999337333333333">
level rejection by to
obtained higher precision with to &gt; 0. Pro-
posed achieved more than 5% higher precision
than Baseline on most recall ranges and showed
higher precision than NoConf-TA on recall ranges
higher than about 35%.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999982090909091">
The proposed method effectively improves NER
performance, as shown by the difference between
Proposed and Baseline in Tables 4 and 5. Improve-
ment comes from two factors: using both text-
based and ASR-based training data and incorpo-
rating ASR confidence feature. As shown by the
difference between Baseline and the methods us-
ing ASR-based training data (NoConf-A, NoConf-
TA, Conf-A, Proposed, Conf-Reject), ASR-based
training data increases precision and decreases
recall. In ASR-based training data, all words
constituting NEs that contain ASR errors are re-
garded as non-NE words, and those NE exam-
ples are lost in training, which emphasizes NER
precision. When text-based training data are also
available, they compensate for the loss of NE
examples and recover NER recall, as shown by
the difference between the methods without text-
based training data (NoConf-A, Conf-A) and those
with (NoConf-TA, Proposed). The ASR confi-
dence feature also increases NER recall, as shown
by the difference between the methods without
it (NoConf-A, NoConf-TA) and with it (Conf-A,
Proposed). This suggests that the ASR confidence
feature helps distinguish whether ASR error influ-
ences NER and suppresses excessive rejection of
NEs around ASR errors.
With respect to the ASR confidence feature, the
small difference between Conf-Reject and Pro-
posed suggests that ASR confidence is a more
dominant feature in misrecognized words than the
other features: the word itself, its part-of-speech
tag, and its character type. In addition, the dif-
ference between Conf-UB and Proposed indicated
that there is room to improve NER performance
with better ASR confidence scoring.
NER-level rejection also increased precision, as
shown in Figure 3. We can control the trade-
off between precision and recall with to accord-
ing to the task requirements, even in text-based
NER. In the NER of speech data, we can ob-
tain much higher precision using both ASR-based
training data and NER-level rejection than using
either one.
</bodyText>
<sectionHeader confidence="0.999985" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999954208333333">
Recent studies on the NER of speech data consider
more than 1-best ASR results in the form of N-best
lists and word lattices. Using many ASR hypothe-
ses helps recover the ASR errors of NE words in
1-best ASR results and improves NER accuracy.
Our method can be extended to multiple ASR hy-
potheses.
Generative NER models were used for multi-
pass ASR and NER searches using word lattices
(Horlock and King, 2003b; B´echet et al., 2004;
Favre et al., 2005). Horlock and King (2003a)
also proposed discriminative training of their NER
models. These studies showed the advantage of
using multiple ASR hypotheses, but they do not
use overlapping features.
Discriminative NER models were also applied
to multiple ASR hypotheses. Zhai et al. (2004) ap-
plied text-based NER to N-best ASR results, and
merged the N-best NER results by weighted vot-
ing based on several sentence-level results such as
ASR and NER scores. Using the ASR confidence
feature does not depend on SVMs and can be used
with their method and other discriminative mod-
els.
</bodyText>
<sectionHeader confidence="0.999284" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999540333333333">
We proposed a method for NER of speech data
that incorporates ASR confidence as a feature
of discriminative NER, where the NER model
</bodyText>
<figure confidence="0.998514384615385">
Recall (%)
100
80
40
20
60
0
50 60 70 80 90 100
Baseline
NoConf-TA
Proposed
Conf-UB
Transcription
</figure>
<page confidence="0.997379">
623
</page>
<bodyText confidence="0.999786428571429">
is trained using both text-based and ASR-based
training data. In experiments using SVMs,
the proposed method showed a higher NER F-
measure, especially in terms of improving pre-
cision, than simply applying text-based NER to
ASR results. The method effectively rejected erro-
neous NEs due to ASR errors with a small drop of
recall, thanks to both the ASR confidence feature
and ASR-based training data. NER-level rejection
also effectively increased precision.
Our approach can also be used in other tasks
in spoken language processing, and we expect it
to be effective. Since confidence itself is not lim-
ited to speech, our approach can also be applied to
other noisy inputs, such as optical character recog-
nition (OCR). For further improvement, we will
consider N-best ASR results or word lattices as in-
puts and introduce more speech-specific features
such as word durations and prosodic features.
Acknowledgments We would like to thank
anonymous reviewers for their helpful comments.
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999071528571429">
Fr´ed´eric B´echet, Allen L. Gorin, Jeremy H. Wright,
and Dilek Hakkani-T¨ur. 2004. Detecting and ex-
tracting named entities from spontaneous speech in a
mixed-initiative spoken dialogue context: How May
I Help You? Speech Communication, 42(2):207–
225.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach.
In Proc. CoNLL, pages 160–163.
Benoit Favre, Fr´ed´eric B´echet, and Pascal Noc´era.
2005. Robust named entity extraction from large
spoken archives. In Proc. HLT-EMNLP, pages 491–
498.
Takaaki Hori, Chiori Hori, and Yasuhiro Minami.
2004. Fast on-the-fly composition for weighted
finite-state transducers in 1.8 million-word vocab-
ulary continuous-speech recognition. In Proc. IC-
SLP, volume 1, pages 289–292.
James Horlock and Simon King. 2003a. Discrimi-
native methods for improving named entity extrac-
tion on speech data. In Proc. EUROSPEECH, pages
2765–2768.
James Horlock and Simon King. 2003b. Named en-
tity extraction from word lattices. In Proc. EU-
ROSPEECH, pages 1265–1268.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recogni-
tion. In Proc. COLING, pages 390–396.
Simo O. Kamppari and Timothy J. Hazen. 2000. Word
and phone level acoustic confidence scoring. In
Proc. ICASSP, volume 3, pages 1799–1802.
Andrew McCallum and Wei Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proc. CoNLL, pages 188–191.
David Miller, Richard Schwartz, Ralph Weischedel,
and Rebecca Stone. 1999. Named entity extraction
from broadcast news. In Proceedings of the DARPA
Broadcast News Workshop, pages 37–40.
David D. Palmer and Mari Ostendorf. 2001. Im-
proving information extraction by modeling errors
in speech recognizer output. In Proc. HLT, pages
156–160.
Thomas Schaaf and Thomas Kemp. 1997. Confidence
measures for spontaneous speech recognition. In
Proc. ICASSP, volume II, pages 875–878.
Satoshi Sekine and Yoshio Eriguchi. 2000. Japanese
named entity extraction evaluation - analysis of re-
sults. In Proc. COLING, pages 25–30.
Satoshi Sekine, Ralph Grishman, and Hiroyuki Shin-
nou. 1998. A decision tree method for finding and
classifying names in Japanese texts. In Proc. the
Sixth Workshop on Very Large Corpora, pages 171–
178.
Frank Wessel, Ralf Schl¨uter, Klaus Macherey, and
Hermann Ney. 2001. Confidence measures for
large vocabulary continuous speech recognition.
IEEE Transactions on Speech and Audio Process-
ing, 9(3):288–298.
Lufeng Zhai, Pascale Fung, Richard Schwartz, Marine
Carpuat, and Dekai Wu. 2004. Using N-best lists
for named entity recognition from chinese speech.
In Proc. HLT-NAACL, pages 37–40.
Rong Zhang and Alexander I. Rudnicky. 2001. Word
level confidence annotation using combinations of
features. In Proc. EUROSPEECH, pages 2105–
2108.
</reference>
<page confidence="0.998642">
624
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.891920">
<title confidence="0.975544">Incorporating speech recognition confidence into discriminative named entity recognition of speech data</title>
<author confidence="0.99298">Katsuhito Sudoh Hajime Tsukada Hideki Isozaki</author>
<affiliation confidence="0.994026">NTT Communication Science Laboratories Nippon Telegraph and Telephone Corporation</affiliation>
<address confidence="0.958777">2-4 Hikaridai, Seika-cho, Keihanna Science City, Kyoto 619-0237, Japan</address>
<abstract confidence="0.998989263157895">This paper proposes a named entity recognition (NER) method for speech recognition results that uses confidence on automatic speech recognition (ASR) as a feature. The ASR confidence feature indicates whether each word has been correctly recognized. The NER model is trained using ASR results with named entity (NE) labels as well as the corresponding transcriptions with NE labels. In experiments using support vector machines (SVMs) and speech data from Japanese newspaper articles, the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure by improving precision. These results show that the proposed method is effective in NER for noisy inputs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fr´ed´eric B´echet</author>
<author>Allen L Gorin</author>
<author>Jeremy H Wright</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Detecting and extracting named entities from spontaneous speech in a mixed-initiative spoken dialogue context: How May I Help You? Speech Communication,</title>
<date>2004</date>
<volume>42</volume>
<issue>2</issue>
<pages>225</pages>
<marker>B´echet, Gorin, Wright, Hakkani-T¨ur, 2004</marker>
<rawString>Fr´ed´eric B´echet, Allen L. Gorin, Jeremy H. Wright, and Dilek Hakkani-T¨ur. 2004. Detecting and extracting named entities from spontaneous speech in a mixed-initiative spoken dialogue context: How May I Help You? Speech Communication, 42(2):207– 225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>New York University.</institution>
<contexts>
<context position="2526" citStr="Borthwick, 1999" startWordPosition="397" endWordPosition="399">R) error problems to NER. Although improvements to ASR are needed, developing a robust NER for noisy word sequences is also important. In this paper, we focus on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while overlapping features are hard to use in HMM-based models. To deal with ASR error problems in NER, Palmer and Ostendorf (2001) proposed an HMMbased NER method that explicitly models ASR errors using ASR confidence and r</context>
<context position="4566" citStr="Borthwick, 1999" startWordPosition="726" endWordPosition="727"> experiments using SVM-based NER and speech data from Japanese newspaper articles, the proposed method increased the NER F-measure, especially in precision, compared to simply applying text-based NER to the ASR results. 2 SVM-based NER NER is a kind of chunking problem that can be solved by classifying words into NE classes that consist of name categories and such chunking states as PERSON-BEGIN (the beginning of a person’s name) and LOCATION-MIDDLE (the middle of a location’s name). Many discriminative methods have been applied to NER, such as decision trees (Sekine et al., 1998), ME models (Borthwick, 1999; Chieu and Ng, 2003), and CRFs (McCallum and Li, 2003). In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese (Isozaki and Kazawa, 2002). We define three features for each word: the word itself, its part-of-speech tag, and its character type. We also use those features for the two preceding and succeeding words for context dependence and use 15 features when classifying a word. Each feature is represented by a binary value (1 or 0), for example, “whether the previous word is Japan,” and each word is classified based on a long binary</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Andrew Borthwick. 1999. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. thesis, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Named entity recognition with a maximum entropy approach.</title>
<date>2003</date>
<booktitle>In Proc. CoNLL,</booktitle>
<pages>160--163</pages>
<contexts>
<context position="2547" citStr="Chieu and Ng, 2003" startWordPosition="400" endWordPosition="403"> to NER. Although improvements to ASR are needed, developing a robust NER for noisy word sequences is also important. In this paper, we focus on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while overlapping features are hard to use in HMM-based models. To deal with ASR error problems in NER, Palmer and Ostendorf (2001) proposed an HMMbased NER method that explicitly models ASR errors using ASR confidence and rejects erroneous word</context>
<context position="4587" citStr="Chieu and Ng, 2003" startWordPosition="728" endWordPosition="731">g SVM-based NER and speech data from Japanese newspaper articles, the proposed method increased the NER F-measure, especially in precision, compared to simply applying text-based NER to the ASR results. 2 SVM-based NER NER is a kind of chunking problem that can be solved by classifying words into NE classes that consist of name categories and such chunking states as PERSON-BEGIN (the beginning of a person’s name) and LOCATION-MIDDLE (the middle of a location’s name). Many discriminative methods have been applied to NER, such as decision trees (Sekine et al., 1998), ME models (Borthwick, 1999; Chieu and Ng, 2003), and CRFs (McCallum and Li, 2003). In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese (Isozaki and Kazawa, 2002). We define three features for each word: the word itself, its part-of-speech tag, and its character type. We also use those features for the two preceding and succeeding words for context dependence and use 15 features when classifying a word. Each feature is represented by a binary value (1 or 0), for example, “whether the previous word is Japan,” and each word is classified based on a long binary vector where only 15</context>
</contexts>
<marker>Chieu, Ng, 2003</marker>
<rawString>Hai Leong Chieu and Hwee Tou Ng. 2003. Named entity recognition with a maximum entropy approach. In Proc. CoNLL, pages 160–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Favre</author>
<author>Fr´ed´eric B´echet</author>
<author>Pascal Noc´era</author>
</authors>
<title>Robust named entity extraction from large spoken archives.</title>
<date>2005</date>
<booktitle>In Proc. HLT-EMNLP,</booktitle>
<pages>491--498</pages>
<marker>Favre, B´echet, Noc´era, 2005</marker>
<rawString>Benoit Favre, Fr´ed´eric B´echet, and Pascal Noc´era. 2005. Robust named entity extraction from large spoken archives. In Proc. HLT-EMNLP, pages 491– 498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hori</author>
<author>Chiori Hori</author>
<author>Yasuhiro Minami</author>
</authors>
<title>Fast on-the-fly composition for weighted finite-state transducers in 1.8 million-word vocabulary continuous-speech recognition.</title>
<date>2004</date>
<booktitle>In Proc. ICSLP,</booktitle>
<volume>1</volume>
<pages>289--292</pages>
<contexts>
<context position="15302" citStr="Hori et al., 2004" startWordPosition="2548" endWordPosition="2551">IN (beginning of a NE), MIDDLE (middle of a NE), END (ending of a NE), and SINGLE (a single-word NE). There were 33 NE classes (eight categories * four chunking states + OTHER), and therefore we trained 33 SVMs to distinguish words of a class from words of other classes. For NER, we used an SVM-based chunk annotator YamCha 2 0.33 with a quadratic kernel (1 + j5�· y)2 and a soft margin parameter of SVMs C=0.1 for training and applied sigmoid function s,(x) with 0,=1.0 and Viterbi search to the SVMs’ outputs. These parameters were experimentally chosen using the test set. We used an ASR engine (Hori et al., 2004) with a speaker-independent acoustic model. The lan1http://chasen.naist.jp/hiki/ChaSen/ (in Japanese) 2http://www.chasen.org/˜taku/software/yamcha/ 620 guage model was a word 3-gram model, trained using other Japanese newspaper articles (about 340 M words) that were also tokenized using ChaSen. The vocabulary size of the word 3-gram model was 426,023. The test-set perplexity over the text corpus was 76.928. The number of outof-vocabulary words was 1,551 (0.587%). 223 (1.23%) NEs in the text corpus contained such outof-vocabulary words, so those NEs could not be correctly recognized by ASR. The</context>
</contexts>
<marker>Hori, Hori, Minami, 2004</marker>
<rawString>Takaaki Hori, Chiori Hori, and Yasuhiro Minami. 2004. Fast on-the-fly composition for weighted finite-state transducers in 1.8 million-word vocabulary continuous-speech recognition. In Proc. ICSLP, volume 1, pages 289–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Horlock</author>
<author>Simon King</author>
</authors>
<title>Discriminative methods for improving named entity extraction on speech data.</title>
<date>2003</date>
<booktitle>In Proc. EUROSPEECH,</booktitle>
<pages>2765--2768</pages>
<contexts>
<context position="2333" citStr="Horlock and King, 2003" startWordPosition="364" endWordPosition="367">ions for things such as peoples’ names, locations’ names, and dates, and NER identifies those expressions and their categories. Unlike text data, speech data introduce automatic speech recognition (ASR) error problems to NER. Although improvements to ASR are needed, developing a robust NER for noisy word sequences is also important. In this paper, we focus on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while overlapping features are </context>
<context position="24834" citStr="Horlock and King, 2003" startWordPosition="4100" endWordPosition="4103">ding to the task requirements, even in text-based NER. In the NER of speech data, we can obtain much higher precision using both ASR-based training data and NER-level rejection than using either one. 6 Related work Recent studies on the NER of speech data consider more than 1-best ASR results in the form of N-best lists and word lattices. Using many ASR hypotheses helps recover the ASR errors of NE words in 1-best ASR results and improves NER accuracy. Our method can be extended to multiple ASR hypotheses. Generative NER models were used for multipass ASR and NER searches using word lattices (Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). Horlock and King (2003a) also proposed discriminative training of their NER models. These studies showed the advantage of using multiple ASR hypotheses, but they do not use overlapping features. Discriminative NER models were also applied to multiple ASR hypotheses. Zhai et al. (2004) applied text-based NER to N-best ASR results, and merged the N-best NER results by weighted voting based on several sentence-level results such as ASR and NER scores. Using the ASR confidence feature does not depend on SVMs and can be used with their method and other </context>
</contexts>
<marker>Horlock, King, 2003</marker>
<rawString>James Horlock and Simon King. 2003a. Discriminative methods for improving named entity extraction on speech data. In Proc. EUROSPEECH, pages 2765–2768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Horlock</author>
<author>Simon King</author>
</authors>
<title>Named entity extraction from word lattices.</title>
<date>2003</date>
<booktitle>In Proc. EUROSPEECH,</booktitle>
<pages>1265--1268</pages>
<contexts>
<context position="2333" citStr="Horlock and King, 2003" startWordPosition="364" endWordPosition="367">ions for things such as peoples’ names, locations’ names, and dates, and NER identifies those expressions and their categories. Unlike text data, speech data introduce automatic speech recognition (ASR) error problems to NER. Although improvements to ASR are needed, developing a robust NER for noisy word sequences is also important. In this paper, we focus on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while overlapping features are </context>
<context position="24834" citStr="Horlock and King, 2003" startWordPosition="4100" endWordPosition="4103">ding to the task requirements, even in text-based NER. In the NER of speech data, we can obtain much higher precision using both ASR-based training data and NER-level rejection than using either one. 6 Related work Recent studies on the NER of speech data consider more than 1-best ASR results in the form of N-best lists and word lattices. Using many ASR hypotheses helps recover the ASR errors of NE words in 1-best ASR results and improves NER accuracy. Our method can be extended to multiple ASR hypotheses. Generative NER models were used for multipass ASR and NER searches using word lattices (Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). Horlock and King (2003a) also proposed discriminative training of their NER models. These studies showed the advantage of using multiple ASR hypotheses, but they do not use overlapping features. Discriminative NER models were also applied to multiple ASR hypotheses. Zhai et al. (2004) applied text-based NER to N-best ASR results, and merged the N-best NER results by weighted voting based on several sentence-level results such as ASR and NER scores. Using the ASR confidence feature does not depend on SVMs and can be used with their method and other </context>
</contexts>
<marker>Horlock, King, 2003</marker>
<rawString>James Horlock and Simon King. 2003b. Named entity extraction from word lattices. In Proc. EUROSPEECH, pages 1265–1268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Hideto Kazawa</author>
</authors>
<title>Efficient support vector classifiers for named entity recognition.</title>
<date>2002</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>390--396</pages>
<contexts>
<context position="2606" citStr="Isozaki and Kazawa, 2002" startWordPosition="409" endWordPosition="412">eloping a robust NER for noisy word sequences is also important. In this paper, we focus on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while overlapping features are hard to use in HMM-based models. To deal with ASR error problems in NER, Palmer and Ostendorf (2001) proposed an HMMbased NER method that explicitly models ASR errors using ASR confidence and rejects erroneous word hypotheses in the ASR results. Such rejection is especiall</context>
<context position="4764" citStr="Isozaki and Kazawa, 2002" startWordPosition="759" endWordPosition="762">t-based NER to the ASR results. 2 SVM-based NER NER is a kind of chunking problem that can be solved by classifying words into NE classes that consist of name categories and such chunking states as PERSON-BEGIN (the beginning of a person’s name) and LOCATION-MIDDLE (the middle of a location’s name). Many discriminative methods have been applied to NER, such as decision trees (Sekine et al., 1998), ME models (Borthwick, 1999; Chieu and Ng, 2003), and CRFs (McCallum and Li, 2003). In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese (Isozaki and Kazawa, 2002). We define three features for each word: the word itself, its part-of-speech tag, and its character type. We also use those features for the two preceding and succeeding words for context dependence and use 15 features when classifying a word. Each feature is represented by a binary value (1 or 0), for example, “whether the previous word is Japan,” and each word is classified based on a long binary vector where only 15 elements are 1. We have two problems when solving NER using SVMs. One, SVMs can solve only a two-class problem. We reduce multi-class problems of NER to a group of two-class pr</context>
</contexts>
<marker>Isozaki, Kazawa, 2002</marker>
<rawString>Hideki Isozaki and Hideto Kazawa. 2002. Efficient support vector classifiers for named entity recognition. In Proc. COLING, pages 390–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simo O Kamppari</author>
<author>Timothy J Hazen</author>
</authors>
<title>Word and phone level acoustic confidence scoring.</title>
<date>2000</date>
<booktitle>In Proc. ICASSP,</booktitle>
<volume>3</volume>
<pages>1799--1802</pages>
<contexts>
<context position="9047" citStr="Kamppari and Hazen, 2000" startWordPosition="1473" endWordPosition="1476"> be considered, where misrecognized words are replaced by word error symbols such as those shown in Table 3. In this case, those words are rejected, and those part-of-speech and character type features are not used in NER. 3.3 ASR confidence scoring for using the proposed NER model ASR confidence scoring is an important technique in many ASR applications, and many methods have been proposed including using word posterior probabilities on word graphs (Wessel et al., 2001), integrating several confidence measures using neural networks (Schaaf and Kemp, 1997), using linear discriminant analysis (Kamppari and Hazen, 2000), and using SVMs (Zhang and Rudnicky, 2001). Word posterior probability is a commonly used and effective ASR confidence measure. Word posterior probability p([w; T, t]|X) of word w at time interval [T, t] for speech signal X is calculated as follows (Wessel et al., 2001): p([w;T,t]|X) {p(X|W) (p(W))&amp;quot;J α , (1) p(X) where W is a sentence hypothesis, W [w; T, t] is the set of sentence hypotheses that include w in [T, t], p(X|W) is a acoustic model score, p(W ) is a language model score, α is a scaling parameter (α&lt;1), and Q is a language model weight. α is used for scaling the large dynamic range</context>
</contexts>
<marker>Kamppari, Hazen, 2000</marker>
<rawString>Simo O. Kamppari and Timothy J. Hazen. 2000. Word and phone level acoustic confidence scoring. In Proc. ICASSP, volume 3, pages 1799–1802.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proc. CoNLL,</booktitle>
<pages>188--191</pages>
<contexts>
<context position="2668" citStr="McCallum and Li, 2003" startWordPosition="418" endWordPosition="422">In this paper, we focus on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while overlapping features are hard to use in HMM-based models. To deal with ASR error problems in NER, Palmer and Ostendorf (2001) proposed an HMMbased NER method that explicitly models ASR errors using ASR confidence and rejects erroneous word hypotheses in the ASR results. Such rejection is especially effective when ASR accuracy is relatively low because many m</context>
<context position="4621" citStr="McCallum and Li, 2003" startWordPosition="734" endWordPosition="737"> from Japanese newspaper articles, the proposed method increased the NER F-measure, especially in precision, compared to simply applying text-based NER to the ASR results. 2 SVM-based NER NER is a kind of chunking problem that can be solved by classifying words into NE classes that consist of name categories and such chunking states as PERSON-BEGIN (the beginning of a person’s name) and LOCATION-MIDDLE (the middle of a location’s name). Many discriminative methods have been applied to NER, such as decision trees (Sekine et al., 1998), ME models (Borthwick, 1999; Chieu and Ng, 2003), and CRFs (McCallum and Li, 2003). In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese (Isozaki and Kazawa, 2002). We define three features for each word: the word itself, its part-of-speech tag, and its character type. We also use those features for the two preceding and succeeding words for context dependence and use 15 features when classifying a word. Each feature is represented by a binary value (1 or 0), for example, “whether the previous word is Japan,” and each word is classified based on a long binary vector where only 15 elements are 1. We have two probl</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In Proc. CoNLL, pages 188–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Miller</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
<author>Rebecca Stone</author>
</authors>
<title>Named entity extraction from broadcast news.</title>
<date>1999</date>
<booktitle>In Proceedings of the DARPA Broadcast News Workshop,</booktitle>
<pages>37--40</pages>
<contexts>
<context position="2281" citStr="Miller et al., 1999" startWordPosition="356" endWordPosition="359">asks. Named entities (NEs) are the proper expressions for things such as peoples’ names, locations’ names, and dates, and NER identifies those expressions and their categories. Unlike text data, speech data introduce automatic speech recognition (ASR) error problems to NER. Although improvements to ASR are needed, developing a robust NER for noisy word sequences is also important. In this paper, we focus on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, whic</context>
</contexts>
<marker>Miller, Schwartz, Weischedel, Stone, 1999</marker>
<rawString>David Miller, Richard Schwartz, Ralph Weischedel, and Rebecca Stone. 1999. Named entity extraction from broadcast news. In Proceedings of the DARPA Broadcast News Workshop, pages 37–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Palmer</author>
<author>Mari Ostendorf</author>
</authors>
<title>Improving information extraction by modeling errors in speech recognizer output.</title>
<date>2001</date>
<booktitle>In Proc. HLT,</booktitle>
<pages>156--160</pages>
<contexts>
<context position="2309" citStr="Palmer and Ostendorf, 2001" startWordPosition="360" endWordPosition="363">(NEs) are the proper expressions for things such as peoples’ names, locations’ names, and dates, and NER identifies those expressions and their categories. Unlike text data, speech data introduce automatic speech recognition (ASR) error problems to NER. Although improvements to ASR are needed, developing a robust NER for noisy word sequences is also important. In this paper, we focus on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while o</context>
</contexts>
<marker>Palmer, Ostendorf, 2001</marker>
<rawString>David D. Palmer and Mari Ostendorf. 2001. Improving information extraction by modeling errors in speech recognizer output. In Proc. HLT, pages 156–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Schaaf</author>
<author>Thomas Kemp</author>
</authors>
<title>Confidence measures for spontaneous speech recognition.</title>
<date>1997</date>
<booktitle>In Proc. ICASSP, volume II,</booktitle>
<pages>875--878</pages>
<contexts>
<context position="8984" citStr="Schaaf and Kemp, 1997" startWordPosition="1465" endWordPosition="1468">ayama is also labeled OTHER in Table 2. Another approach can be considered, where misrecognized words are replaced by word error symbols such as those shown in Table 3. In this case, those words are rejected, and those part-of-speech and character type features are not used in NER. 3.3 ASR confidence scoring for using the proposed NER model ASR confidence scoring is an important technique in many ASR applications, and many methods have been proposed including using word posterior probabilities on word graphs (Wessel et al., 2001), integrating several confidence measures using neural networks (Schaaf and Kemp, 1997), using linear discriminant analysis (Kamppari and Hazen, 2000), and using SVMs (Zhang and Rudnicky, 2001). Word posterior probability is a commonly used and effective ASR confidence measure. Word posterior probability p([w; T, t]|X) of word w at time interval [T, t] for speech signal X is calculated as follows (Wessel et al., 2001): p([w;T,t]|X) {p(X|W) (p(W))&amp;quot;J α , (1) p(X) where W is a sentence hypothesis, W [w; T, t] is the set of sentence hypotheses that include w in [T, t], p(X|W) is a acoustic model score, p(W ) is a language model score, α is a scaling parameter (α&lt;1), and Q is a langu</context>
</contexts>
<marker>Schaaf, Kemp, 1997</marker>
<rawString>Thomas Schaaf and Thomas Kemp. 1997. Confidence measures for spontaneous speech recognition. In Proc. ICASSP, volume II, pages 875–878.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Yoshio Eriguchi</author>
</authors>
<title>Japanese named entity extraction evaluation - analysis of results.</title>
<date>2000</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>25--30</pages>
<contexts>
<context position="13100" citStr="Sekine and Eriguchi, 2000" startWordPosition="2188" endWordPosition="2191">ge to, OTHER becomes more desirable than the other NE classes, and many words are classified as nonNE words and vice versa. Therefore, to works as a parameter for NER-level rejection. This approach can also be applied to text-based NER. 4 Experiments We conducted the following experiments related to the NER of speech data to investigate the performance of the proposed method. 4.1 Setup In the experiment, we simulated the procedure shown in Figure 1 using speech data from the NE-labeled text corpus. We used the training data of the Information Retrieval and Extraction Exercise (IREX) workshop (Sekine and Eriguchi, 2000) as the text corpus, which consisted of 1,174 Japanese newspaper articles (10,718 sentences) and 18,200 NEs in eight categories (artifact, organization, location, person, date, time, money, and percent). The sentences were read by 106 speakers (about 100 sentences per speaker), and the recorded speech data were used for the experiments. The experiments were conducted with 5- fold cross validation, using 80% of the 1,174 articles and the ASR results of the corresponding speech data for training SVMs (both for ASR confidence scoring and for NER) and the rest for the test. We tokenized the senten</context>
</contexts>
<marker>Sekine, Eriguchi, 2000</marker>
<rawString>Satoshi Sekine and Yoshio Eriguchi. 2000. Japanese named entity extraction evaluation - analysis of results. In Proc. COLING, pages 25–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
<author>Hiroyuki Shinnou</author>
</authors>
<title>A decision tree method for finding and classifying names in Japanese texts.</title>
<date>1998</date>
<booktitle>In Proc. the Sixth Workshop on Very Large Corpora,</booktitle>
<pages>171--178</pages>
<contexts>
<context position="4538" citStr="Sekine et al., 1998" startWordPosition="719" endWordPosition="722">ores and are used for the NER. In experiments using SVM-based NER and speech data from Japanese newspaper articles, the proposed method increased the NER F-measure, especially in precision, compared to simply applying text-based NER to the ASR results. 2 SVM-based NER NER is a kind of chunking problem that can be solved by classifying words into NE classes that consist of name categories and such chunking states as PERSON-BEGIN (the beginning of a person’s name) and LOCATION-MIDDLE (the middle of a location’s name). Many discriminative methods have been applied to NER, such as decision trees (Sekine et al., 1998), ME models (Borthwick, 1999; Chieu and Ng, 2003), and CRFs (McCallum and Li, 2003). In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese (Isozaki and Kazawa, 2002). We define three features for each word: the word itself, its part-of-speech tag, and its character type. We also use those features for the two preceding and succeeding words for context dependence and use 15 features when classifying a word. Each feature is represented by a binary value (1 or 0), for example, “whether the previous word is Japan,” and each word is class</context>
</contexts>
<marker>Sekine, Grishman, Shinnou, 1998</marker>
<rawString>Satoshi Sekine, Ralph Grishman, and Hiroyuki Shinnou. 1998. A decision tree method for finding and classifying names in Japanese texts. In Proc. the Sixth Workshop on Very Large Corpora, pages 171– 178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Wessel</author>
<author>Ralf Schl¨uter</author>
<author>Klaus Macherey</author>
<author>Hermann Ney</author>
</authors>
<title>Confidence measures for large vocabulary continuous speech recognition.</title>
<date>2001</date>
<booktitle>IEEE Transactions on Speech and Audio Processing,</booktitle>
<pages>9--3</pages>
<marker>Wessel, Schl¨uter, Macherey, Ney, 2001</marker>
<rawString>Frank Wessel, Ralf Schl¨uter, Klaus Macherey, and Hermann Ney. 2001. Confidence measures for large vocabulary continuous speech recognition. IEEE Transactions on Speech and Audio Processing, 9(3):288–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lufeng Zhai</author>
<author>Pascale Fung</author>
<author>Richard Schwartz</author>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Using N-best lists for named entity recognition from chinese speech.</title>
<date>2004</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>37--40</pages>
<contexts>
<context position="2688" citStr="Zhai et al. (2004)" startWordPosition="423" endWordPosition="426">on the NER of ASR results and discuss the suppression of ASR error problems in NER. Most previous studies of the NER of speech data used generative models such as hidden Markov models (HMMs) (Miller et al., 1999; Palmer and Ostendorf, 2001; Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). Zhai et al. (2004) applied a text-level ME-based NER to ASR results. These models have an advantage in utilizing various features, such as part-of-speech information, character types, and surrounding words, which may be overlapped, while overlapping features are hard to use in HMM-based models. To deal with ASR error problems in NER, Palmer and Ostendorf (2001) proposed an HMMbased NER method that explicitly models ASR errors using ASR confidence and rejects erroneous word hypotheses in the ASR results. Such rejection is especially effective when ASR accuracy is relatively low because many misrecognized words m</context>
<context position="25165" citStr="Zhai et al. (2004)" startWordPosition="4151" endWordPosition="4154">ices. Using many ASR hypotheses helps recover the ASR errors of NE words in 1-best ASR results and improves NER accuracy. Our method can be extended to multiple ASR hypotheses. Generative NER models were used for multipass ASR and NER searches using word lattices (Horlock and King, 2003b; B´echet et al., 2004; Favre et al., 2005). Horlock and King (2003a) also proposed discriminative training of their NER models. These studies showed the advantage of using multiple ASR hypotheses, but they do not use overlapping features. Discriminative NER models were also applied to multiple ASR hypotheses. Zhai et al. (2004) applied text-based NER to N-best ASR results, and merged the N-best NER results by weighted voting based on several sentence-level results such as ASR and NER scores. Using the ASR confidence feature does not depend on SVMs and can be used with their method and other discriminative models. 7 Conclusion We proposed a method for NER of speech data that incorporates ASR confidence as a feature of discriminative NER, where the NER model Recall (%) 100 80 40 20 60 0 50 60 70 80 90 100 Baseline NoConf-TA Proposed Conf-UB Transcription 623 is trained using both text-based and ASR-based training data</context>
</contexts>
<marker>Zhai, Fung, Schwartz, Carpuat, Wu, 2004</marker>
<rawString>Lufeng Zhai, Pascale Fung, Richard Schwartz, Marine Carpuat, and Dekai Wu. 2004. Using N-best lists for named entity recognition from chinese speech. In Proc. HLT-NAACL, pages 37–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Zhang</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Word level confidence annotation using combinations of features.</title>
<date>2001</date>
<booktitle>In Proc. EUROSPEECH,</booktitle>
<pages>2105--2108</pages>
<contexts>
<context position="9090" citStr="Zhang and Rudnicky, 2001" startWordPosition="1480" endWordPosition="1484">re replaced by word error symbols such as those shown in Table 3. In this case, those words are rejected, and those part-of-speech and character type features are not used in NER. 3.3 ASR confidence scoring for using the proposed NER model ASR confidence scoring is an important technique in many ASR applications, and many methods have been proposed including using word posterior probabilities on word graphs (Wessel et al., 2001), integrating several confidence measures using neural networks (Schaaf and Kemp, 1997), using linear discriminant analysis (Kamppari and Hazen, 2000), and using SVMs (Zhang and Rudnicky, 2001). Word posterior probability is a commonly used and effective ASR confidence measure. Word posterior probability p([w; T, t]|X) of word w at time interval [T, t] for speech signal X is calculated as follows (Wessel et al., 2001): p([w;T,t]|X) {p(X|W) (p(W))&amp;quot;J α , (1) p(X) where W is a sentence hypothesis, W [w; T, t] is the set of sentence hypotheses that include w in [T, t], p(X|W) is a acoustic model score, p(W ) is a language model score, α is a scaling parameter (α&lt;1), and Q is a language model weight. α is used for scaling the large dynamic range of Word Confidence NE label Murayama 1 PER</context>
</contexts>
<marker>Zhang, Rudnicky, 2001</marker>
<rawString>Rong Zhang and Alexander I. Rudnicky. 2001. Word level confidence annotation using combinations of features. In Proc. EUROSPEECH, pages 2105– 2108.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>