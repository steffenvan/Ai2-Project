<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9734465">
A Parser That Doesn&apos;t
S. G. Pulman
University of Cambridge
Computer Laboratory
Corn Exchange Street
Cambridge CB2 3QC, UK.
</title>
<sectionHeader confidence="0.961868" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997468">
This paper describes an implemented parser-interpreter which
is intended as an abstract formal model of part of the process of
sentence comprehension. It is illustrated here for Phrase Structure
Grammars with a translation into a familiar type of logical form,
although the general principles are intended to apply to any gram-
matical theory sharing certain basic assumptions, which are dis-
cussed in the paper. The procedure allows for incremental seman-
tic interpretation as a sentence is parsed, and provides a principled
explanation for some familiar observations concerning properties of
deeply recursive constructions.
</bodyText>
<subsectionHeader confidence="0.770634">
Back ground
</subsectionHeader>
<bodyText confidence="0.999923">
The starting point for the present work is a set of familiar
and, for the most part, uncontroversial claims about the nature of
grammatical description and of human parsing of natural language.
These claims and assumptions can be briefly summarised as follows:
</bodyText>
<sectionHeader confidence="0.561688" genericHeader="method">
A Hierarchical Structure
</sectionHeader>
<bodyText confidence="0.9996675">
Linguists assign constituent structures to sentences on the ba-
sis of distributional tests of various kinds. On the basis of these
tests, the &apos;correct&apos; structures are always hierarchical and often
deeply nested. The tree representing a sentence may impose a
great deal of structure on it, with string-adjacent items often ap-
pearing at very different levels in the tree. In general, shallow,
&apos;fiat&apos; structures are not generated by grammars, nor warranted on
distributional grounds. However, as we shall see, it is likely that
these deeply nested structures may be somewhat remote from any
that are actually computed during parsing.
</bodyText>
<subsectionHeader confidence="0.403717">
B Semantics is (i) compositional and (ii) syntax-driven.
</subsectionHeader>
<bodyText confidence="0.999643636363636">
Both of these claims can be made in a variety of versions
of different strengths, from the trivially true to the fairly clearly
false. What is intended here is the assumption sometimes called
the &apos;rule to rule&apos; hypothesis. shared by almost all current grammat-
ical frameworks, that to each syntactic rule of a grammar (or for
each subtree induced by such a rule) there is an associated seman-
tic rule, either producing an interpretation directly, or translating
into some formal language. Interpretations for whole sentences are
built up from the constituent parts in ways specified by these rules,
in a fashion which mimics and uses the syntactic structure of the
sentence.
</bodyText>
<sectionHeader confidence="0.994614" genericHeader="method">
C Incremental interpretation
</sectionHeader>
<bodyText confidence="0.999554333333333">
As a sentence is parsed, its interpretation is built up word by
word: there is little or no delay in interpreting it. In particular,
we do not wait until all syntactic constituents have been completed
before beginning to integrate then into some non-syntactic repre-
sentation. Ample intuitive and experimental evidence supports this
uncontroversial observation.
</bodyText>
<sectionHeader confidence="0.821275" genericHeader="method">
D Limited recursion.
</sectionHeader>
<bodyText confidence="0.994559391304348">
One of the most firmly established facts about human syntac-
tic processing is that constructions which are ineliminably deeply
recursive (such as central self-embeddings) are difficult or impossi-
ble to parse. A sentence like:
1 The boy who the girl that the dog bit liked ran away
is clumsy at best, and one like:
2 The boy the girl the dog the cat scratched bit saw left
is utterly unmanageable under normal circumstances.
Under the further assumption, recently more controversial (Katz
1081), that grammars have some kind of mental reality as repre-
sentations of linguistic knowledge, it is clear that A to D, although
simple and generally agreed upon observations, by no means obvi-
ously consistent with each other. Consider, for example, the nat-
ural way in which one might set about implementing a system
which observed B, a principle which, in itself, is a computation-
ally natural principle. Such a system might first parse a sentence,
annotating nodes in the resulting tree with an indication of the syn-
tactic rules used. This annotated tree would then be passed to an
interpretation routine which applied the appropriate semantic op-
eration to the topmost node (guided by the syntactic information
found there, in particular a pointer to the semantic information
necessary), calling itself recursively on each subtree to build up the
complete interpretation. (Systems operating in more or less this
manner are described in Rosenschein and Shieber 1982, Gawron et
al. 1982 and Schubert and Pelletier 1982. They are not intended as
psychological models in any but the most abstract sense, of course.)
Such a system would, in observing B, also naturally be con-
sistent with A. Obviously, though, this type of system requires a
complete syntactic analysis to be available before it can even be-
giu the process of interpretation, thus conflicting straightforwardly
with C.
Consider next A and D. The structures which linguists postu-
late in accordance with A are often recursive, and it is in the nature
of hierarchical structures that this should be a possibility. This is
rather puzzling in the light of D, for if D is correct, it seems to show
that a class of structures which are natural from one point of view
(i.e. centre embeddings) are extremely unnatural from another.
It is not necessarily to be expected that human linguistic abili-
ties have evolved in a harmonious and homogeneous manner, but
other things being equal, we would not expect to find two appar-
ently co-operating modules so ill-suited to each other. Why should
grammars be able to generate things that parsers can&apos;t parse?
When we consider left and right recursions, some further ten-
sion between A. B and D emerges. Multiple left recursions in En-
glish are most clearly illustrated by possessive determiner phrases,
which are generally assumed to have a structure something like 3:
</bodyText>
<page confidence="0.989272">
128
</page>
<figure confidence="0.949728375">
3
HP VP
VP
Det
Dot
HP pose :
Det
NP pone
</figure>
<subsectionHeader confidence="0.73993">
Jane s mother e eat • fur
</subsectionHeader>
<bodyText confidence="0.993193583333334">
and multiple right recursions by a variety of structures, for
example, relative clauses:
4 That&apos;s [the company that manufactures (the drugs that have
[the side effects that made her come out in a rashill
There are several facts which suggest that the structures as-
signed to these examples by a grammar in accordance with A can-
not realistically be assumed to play any very direct role in the
actual processing of them in performance. Firstly, there is the fa-
miliar observation (Chomsky 1965: 13-14, Langendoen 1975: 544),
that examples like 3 and 4 do not have the intonation contours
that would be predicted for them on the basis of the constituent
structures assigned to them by a grammar. For example, in 3, the
intonation of the sequence of possessives is not defined over the
whole constituent, as might be expected, but is more like a &apos;list&apos;
intonation. In sentences like 4, the intonation contour sometimes
breaks the sentence up in the way indicated informally here:
5 (That&apos;s the company( (that manufactures the drugs) (that
have the side effects] [that made her come out in a rash)
This chunking of the sentence does not respect its syntactic
structure, splitting the head NP of the relative clause from its
modifier and grouping it with the main clause instead. The condi-
tions under which this happens are clearly connected with matters
of length and so on, so the actual examples here are also capable of
receiving the &apos;correct&apos; contour, but the effect is clearly to be seen in
longer and more complex sequences. This observation is generally
taken to indicate that, whatever else is happening in the produc-
tion and comprehension of such examples, it is not the case that
complete syntactic structures of the type assigned by a grammar
are being computed.
A further argument that this is so derives from the fact that
although 4 was displayed as a right branching structure, it would
also receive a left branching analysis, and if sufficiently complex.
all possible combinmions of the two. This means that the number
of parses such a structure would receive goes up massively with
the number of clauses involved (see Church and Patil 1982 for dis-
cussion of this. Analogous comments hold for PP modifiers and
conjunctions on most analyses). It is clearly stretching credibility
to assume that a parsing procedure follows very faithfully what a
grammar says about such cases.
While difficult to reconcile with A (and hence B) these obser-
vations are consistent with D. This perhaps needs some elaboration:
it is a reasonable conjecture, given what we know about short term
linguistic memory, that the human parsing mechanism operates in
a way which has the formal properties of a finite state device (see
e.g. Chomsky 1963, Chomsky and Miller 1963, or, more recently,
Langendoeu and Langsam 1984). The fact that unlimited right or
left recursion can be recognised, whereas centre recursion cannot,
is consistent with this, for any (CF) language with bounded centre
embedding is also a finite state language. However, when we turn to
full parsing, as opposed to recognition, it turns out that the proper
analysis even of left and right recursion demands non-finite-state
resources (Langendoen 1975). Intuitively, this is easily seen: pars-
ing a language can be regarded, abstractly, as a transduction from
strings of terminal items to labelled bracketings representing struc-
tural descriptions. For the labelled bracketings to be well formed,
left and right brackets bearing the same label must be paired up
correctly. In the case of recursion, this means that the bracket lan-
guage contains cases where some number of left brackets of type X
must be paired up with the same number of right brackets of type
X, for any number. This is a classic non-finite state language, and
thus even if the input to the transducer is finite state, the overall
transduction must be at least of context-free power, given no finite
bound on recursion. Full parsing, therefore, of structures like 3 and
4, will demand resources of at least this power.
Let us now assume that D should be taken to apply, not just
to cases of centre embedding, but to all types of recursion (as in
Miller and lsard&apos;s original (1963) discussion of centre embedding).
This is, in effect, a conjecture that the human parsing mechanism
is forced to operate with no more than finite state resources, even
though the class of languages generated by the grammars found
natural by human beings might lie far outside the finite state class.
Under such circumstances it would be expected that in the left and
right recursive cases, a full parsing would not always be available,
an expectation that we may take to be supported by the intona-
tional evidence, and by the combinatorial explosion considerations
alluded to above.
If this is a plausible line of reasoning, it nevertheless presents
us with a further difficulty in the light of observation B. For if se-
mantles is driven by syntax, it would seem to follow that structures
which are not properly parsed should not be fully interpretable ei-
ther. While this is clearly the case for centre embeddings, it is
not the case for either left or right recursion: semantically speak-
ing they are completely unproblematic. This is a further conflict
which our model of parsing will have to resolve.
</bodyText>
<page confidence="0.997506">
129
</page>
<subsectionHeader confidence="0.884139">
An Incremental Parser-Interpreter
</subsectionHeader>
<bodyText confidence="0.999527743589744">
My aim was to develop s parser and interpreter which was
compatible with A to D, resolving the apparent conflicts between
them, and which also incorporated in a fairly concrete form the
assumption that grammars have some status, independently of
parsers. as mental objects. That is to say, it was assumed that
what linguists say about natural language in the form of a gram-
mar (including semantic interpretation rules) is available to the
parser-interpreter as some kind of data structure having roughly
the form that the linguist&apos;s pencil and paper description would
suggest. The aim was also to demonstrate a serious commitment
to C by getting the parser to build up explicit representations of the
meaning of a sentence piece by piece during the course of a parse.
To my knowledge, the only other work which takes this commit-
ment seriously at the appropriate lev-el of formal detail (there is
no shortage of well intentioned hand-waving) is that of Ades and
Steedman (1082). In Pulman (forthcoming), I discuss some of the
similarities and differences between these two approaches.
For purposes of illustration, I will assume that the underly-
ing grammatical theory involved is some form of Phrase Structure
Grammar, where semantic interpretation consists of translation
into a simple form of higher order logic. Neither of these assump-
tions is crucial: the parsing procedure can be adapted to certain
types of transformational grammar, and the associated process of
semantic interpretation requires only that the semantic theory can
he driven by syntactic structures, and that there is some way of
(king function application and composition. It is unlikely that this
this rules out any candidates at all.
The procedure is best thought of as a type of stack-based
shift-reduce algorithm, though with the ability to deal with in-
complete constituents. In the current implementation it operates
non-deterministically: I (and others) have argued elsewhere (Pul-
man, forthcoming) that there is no good reason to suppose that
parsing (as opposed to a more global process of comprehension) is
deterministic. (Contra Marcus 1980, Berwick and Weinberg 1984.
See also CraM and Steedman, forthcoming., Briscoe 1984).
The driving mechanism of the parser-interpreter maintains an
agenda of configurations, each representing a particular state of a
pane. A configuration is a pair consisting of a representation of
the state of the stack, and the current position in the input string.
The stack is a list of entries, of which (usually) only the top two
are accessible to the basic operations of the parser. Each entry
repri•sents a wholly or partially recognised constituent, along with
its interpretation in terms of a translation into a logical expres-
sion. An entry is a triple, consisting of a category label, indicating
what type of constituent is being recognised, a &apos;needed&apos; list of con-
stituents which must be found before the category is complete, and
the interpretation so far. The parser starts with an initial configu-
ration and proceeds by trying to produce new ones from that until
either no more alternatives aee left, and the parse has failed, or one
or more complete parses are produced.
There are four basic operations which produce a new config-
uration from an old one. Which one is performed depends on the
state of the stack. If there is a choice between two, both are per-
formed, producing two new configurations.
SHIFT: takes the next word from the input and creates a new
stack entry for it (for each lexical entry it has in the dictionary).
For example, given a lexical entry like
(every, Det, AP AQ Ax Px —• Qx)
Shift produces a stack entry like:
IDet, nil, AP AQ Ax Px Qx)
The interpretation of non-logical words is assumed to be the
associated constant, as is customary. Since lexical categories are
always complete the second &apos;needed&apos; element in a stack entry will
always be empty. Having created a new stack entry, Shift records
a new configuration with that entry on top of the stack, and an
updated input pointer.
INVOKE-RULE: applies when there is a completed entry on
top of the stack. Essentially, it checks the rules in the grammar
to see whether the category represented by that entry could begin
some higher level constituent. Although this is not strictly neces-
sary, a one-word lookahead is incorporated for efficiency.
If Invoke-rule succeeds in matching a category of an entry with
the first member of the right hand side of a rule, it creates a new en-
try from them. Logically speaking, this process happens as follows:
assume, for illustration, an entry of the form
(Bet, nil, every)
(where the interpretation of &apos;every&apos; might actually be as above)
and a example rule of the form:
</bodyText>
<equation confidence="0.461767">
NP Det N ; Det&apos; (N&apos;)
</equation>
<bodyText confidence="0.999695375">
where the part ofter the semi-colon is the semantic component.
The entry matches the beginning of the right hand side of the rule
and so could begin an NP constituent. Now assume a function, call
it Abstract, which when applied to a rule of this form produces from
its right hand side and semantic component the result of lambda
abstracting over all the right hand side symbols (in the order spec-
ified in the rule) which appear in the semantic component. Thus
Abstract applied to the rule above would produce
</bodyText>
<figure confidence="0.9405578">
A det A n det (n)}
If applied to a rule like
S NP VP ; VP&apos; (NP&apos;)
it would produce
A np A vp vp (rip))
</figure>
<bodyText confidence="0.99189505">
This is simply a more literal rendering of what the rule actually
says, in fact: making explicit the fact that the items occurring in
the semantic part of the rule are to be interpreted as variables.
When Invoke-rule has matched an entry to a rule it produces
a new entry where the category is the left hand side of the rule,
the &apos;needed` list is all but the first of the right hand side, and the
interpretation is the result of applying Abstract to the rule and
then applying that to the interpretation of the original entry. In
the example above the result of all this would be:
(NP, N, A a ( every (n)) )
In other words, the interpretation is simply that of the whole
rule with that of the existing entry put in the appropriate place: a
semantic equivalent of the &apos;needed&apos; field. In general, the interpreta-
tion of an incomplete constituent is that it is a function expecting
to find the needed items as arguments.
COMBINE: combines a complete entry on top of the stack
with an incomplete one below it, if the category label of the for-
mer matches the first &apos;needed&apos; item of the latter. For example, if
the stack contained an entry like the one just described, with a
complete entry on top:
</bodyText>
<equation confidence="0.322044">
(N, nil, man)
(NP, N, A n { every (n)} }
</equation>
<bodyText confidence="0.2208756">
then Combine would produce a new entry with the category of
the incomplete one, the remainder, if any, of the needed list, and an
interpretation which is the result of applying that of the incomplete
entry to that of the complete one. Here the result would be:
{NI&apos;, nil, every (man) }
</bodyText>
<page confidence="0.992503">
130
</page>
<bodyText confidence="0.979822666666667">
when beta reduction of the lambda expressions has taken place,
which is a complete constituent, in this instance, although this need
not be the case. If the needed field is not nil, the interpretation
will always reflect this.
These three operations are in fact sufficient to allow the parser
to operate. However, a further operation is also necessary if we are
to maintain consistency with our original assumptions.
CLEAR: Clear is intended to correspond to the intuition that
once a complete or completable representation of a proposition has
beeu built up. the syntactic information needed to do this is no
longer required. under normal circumstances. The conditions under
which Clear operates in the present implementation ensures that
this type of syntactic information is discarded as soon as possible:
although this is probably not a realistic claim about human parsing.
Clear operates when:
</bodyText>
<listItem confidence="0.933772166666667">
(i) there are only two items on the stack (in a less enthusiastic
version. Clear would be constrained to operate only on the bottom
two items on the stack)
(ii) the topmost one potentially contains everything needed to
complete the bottom one
(iii) the topmost one is a VP or S
</listItem>
<bodyText confidence="0.9996926">
The first two conditions correspond to the obvious truth that
you can only get rid of syntactic information when it is safe to
do so, and that &apos;selective forgetting&apos; is not possible: either all the
syntactic information relevant to the earlier portion of the sentence
is discarded, or none of it is. Otherwise, the claim, and the later
explanations which depend on it, would be vacuous. The third is
intended to capture the intuition that it is the main predicate of a
sentence which when encountered provides enough information to
be able to continue parsing safely after that point with no reference
to anything before. For example, when a verb is encountered, the
number and type of (obligatory) arguments will be known.
When the conditions for Clear are met, the effect is that the
interpretation of the bottommost entry is composed with that of
the topmost, the bottom one then being erased. For example, in a
situation like:
</bodyText>
<listItem confidence="0.560804">
{VP, NP, A np (likes (up))
{S, VP, A vp {vp (some (man))) )
</listItem>
<bodyText confidence="0.9933476">
where the topmost entry is of the type that the one underneath
is looking for, the result of Clear is that the stack will contain just:
(VP, NP, A x (A vp {vp (some (man))) {A np (likes (np))
kW}
When this VP finds the NP it is looking for, the interpretation
will reduce to what we would have had more directly if Clear had
not operated.
Here is a trace of the parser to show how all these operations
work together. The meanings of the individual lexical items have
been suppressed in the interests of readability.
</bodyText>
<table confidence="0.437766414634146">
S NP VP; VP (NP)
VP --• V NP ; V (NP)
NP Det N ; Det (N)
Input: The farmer killed the duckling
Shift:
{Det, nil, the)
Invoke:
N, A n {the (n)}
Shift:
(N, nil, farmer)
{NP, N, A a (the (a)) }
Combine:
{NP, N, the (farmer) I
Invoke:
{S, VP, A vp (vp (the (farmer))) }
Shift:
(V, nil. killed)
{S, VP, A vp vp (the (farmer)))
Invoke:
(VP, NP, A np { killed (np)) )
{S, VP, A vp { vp (the (farmer))) I
Clear:
{VP, NP, A x vp {vp (the (farmer))) {A np {killed (up)) (x)
}H
Shift:
(Det, nil, the)
(VP, NP, A x (A vp {vp (the (farmer))) (A np (killed (np)) (x)
)
Invoke:
(NP, N, A n (the (n)} }
{VP, NP, A x {A vp {vp (the (farmer))) {A np (np)) (x)
Shift:
(N, nil, duckling)
{NP, N, A n (the (a)) )
{VP, NP, A x {A vp {vp (the (farmer))) {A np (np)) (x)
1) )
Combine:
(NP, nil, the (duckling))
{VP, NP, A x {A vp {vp (the (farmer))) np (killed (np)) (x)
}}
Combine: (VP, nil, A x {A vp {vp (the (farmer))) {A np
</table>
<equation confidence="0.822884">
(np)) (x) }} (the (duckling)) )
</equation>
<bodyText confidence="0.9964666">
At this point the parse is complete, and the complex interpre-
tation beta-reduces to:
(killed (the (duckling))) (the (farmer))
The resulting interpretation is exactly what would have been
obtained by a &apos;classical&apos; system operating as described earlier.
</bodyText>
<sectionHeader confidence="0.486901" genericHeader="method">
Modelling Incremental Interpretation
</sectionHeader>
<bodyText confidence="0.999952647058824">
How does the parsing procedure manage to remain faithful to
A to D simultaneously? Let us begin with B: the compositional,
syntax-driven nature of semantics. The parser assumes that se-
mantic information can be associated with syntactic rules in some
way (though it is not ruled out - in fact, it is assumed - that some
extra aspects of interpretation may need to be computed by sep-
arate procedures: for example, identification of variables for the
purposes of indicating coreference; cases of wide scope of quantifier
phrases in syntactic narrow scope positions, etc.). Once the rule in
question has been identified by Invoke-rule, the semantic informa-
tion involved is extracted and used to form the next stack entry.
The syntactic information is also used to form expectations about
what constituents must come next, although it is conceivable that
if semantic type is entirely predictable from syntactic category and
vice versa this information is actually redundant. No other mech-
anisms for linking syntax with semantics are required. Hence the
parser obeys condition B absolutely literally and faithfully.
</bodyText>
<page confidence="0.996926">
131
</page>
<bodyText confidence="0.993403346153846">
The important thing to notice is that this is achieved without
building any explicit syntax trees during the course of parsing a
sentence. Syntactic information is used to build up the interpreta-
tion and to guide the parse, but does not result in the construction
of an independent level of representation. AS the title of the paper
indicates, there is no parse tree built for a sentence at all. While
it is true that in some sense trees are implicit in the sequence of
operations of the parser, this is an inevitable consequence of the
fact that the rules used themselves define trees, and as we shall
see, even in this weak sense the tree structures implicit for certain
types of recursive construction are not isomorphic to those which
would be defined by the grammar.
I like to think of this aspect of the operation of the parser as
embodying the intuition often expressed (most often in the oral
traditiou, though explicit in Isard 1974), that syntax is a &apos;control
structure for semantics. It also has the merit of being consistent
both with the widespread agreement among linguists that syntax
plays a central role in language understanding, and with the ap-
parently equally widespread failure of psycholinguists to find any
evidence that purely syntactic representations are computed at any
stage during normal comprehension.
Turning now to C, the observation that sentences are under-
stood on (at least) a word by word basis on a pass through from
left to right, it should be clear that our procedure provides a direct
model of this process, on the assumption that at least a central
part of the meaning of a sentence is given by a translation into
a logical form of this kind. As soon as a word is encountered, it
is integrated into the logical form being built up. At every stage,
this logical form, though possibly not yet complete, is a perfectly
meaningful object (within the higher order logic assumed here it is
just a term like any other): it can be used to perform inferences,
be the antecedent for anaphora or ellipsis, be integrated with the
context so as to assess and discard alternative interpretations cor-
responding to different parsings, and in general perform any of the
functions we expect the meaning of a sentence or sentence fragment
to be able to do.
The satisfying of A is in a sense automatic but trivial, given
that the parser uses ordinary grammatical rules, rather than some
preprocessed version altering the output of the rules to produce
flat structures (as, for example, in Langendoen 1975, Langendoen
and Langsam 1984, and also - wrongly, on the present approach -
in Putman 1983). More interesting is the way the parser produces
a similar effect to that achieved with these preprocessings, without
altering the rules themselves, as a side effect of its observance of D
- the limitation on recursion.
Recursion Limitations
I have argued elsewhere (Putman, forthcoming) that attempts
to explain the difficulty of centre embedded sentences as a conse-
quence of parsing strategies are unsuccessful, and that the simplest
explanation is the original one (Miller and Isard 1963): that the
human parsing mechanism is fundamentally incapable of operating
recursively. To be more precise: if (in the worst case) the parser
encounters an instance of a construction in the course of trying to
parse an earlier instance of it, the record of the earlier instance will
be erased and &apos;forgotten&apos;, causing confusion in those cases where
the information is needed to complete a parse successfully, as in
the centre embedding cases. Clearly this is not absolute: some in-
stances of centre embedding can be found to a depth of 4 or 5, but
for simplicity we will assume that there is some small fixed limit,
L.
The present procedure implements this restriction quite lit-
erally: if Invoke-rule attempts to put on the stack an incomplete
constituent of category X, when there are already L instances of
such incomplete Xs on the stack, then the earliest instance is erased
before Invoke-rule can succeed. The interesting and striking thing
about this restriction is that as stated, it applies to all types of
recursion, and thus might be expected to result in parsing failures
not just for centre embedded examples of a depth greater than L,
but for left and right recursions deeper than L too. However, this
does not happen: the basic operations of the parser in fact conspire
to bring it about that both left and right recursions can be parsed,
the former fully, and the latter to just the extent, apparently, that
is needed to be able to provide them with an appropriate inter-
pretation. Thus a perfectly general and simple restriction can be
imposed, rather than some version (implausibly) qualified so as to
distinguish between different types of recursion.
The simplest case is that of left recursion, which we will illus-
trate with an artifical example grammar:
</bodyText>
<figure confidence="0.864706625">
A Aa A (a)
A — a; a
When processing a string `aaa...&apos;, the parser operates as in the
following trace (`11&apos; is the interpretation of &apos;al:
Shift:
(a, nil, b)
Invoke:
(A, nil, b}
Invoke: (A, a, Aa (b (a)))
Shift:
(a, nil, b)
(A, a, Aa (b (a)))
Combine:
(A, nil, b (b) )
Invoke:
(A, a, Aa (b (b (a))) }
</figure>
<bodyText confidence="0.9999527">
At this point the cycle of operations has become evident: at
no point is there ever more than one occurrence of an incomplete
A constituent on the stack, and so there is never any situation in
which the recursion limitation would come into effect. In other
words, like any shift-reduce mechanism, this parser can process
unbounded left recursion without the stack growing beyond a con-
stant depth.
Centre embeddings of a depth greater than L will not be parsed
correctly. To see how this might work out in detail we will assume
some simple rules for relatives:
</bodyText>
<sectionHeader confidence="0.9318545" genericHeader="method">
NP NP REL : REL&apos;(NP&apos;)
REL NP VP : NP1VP&apos;)
</sectionHeader>
<bodyText confidence="0.986667909090909">
and we will ignore the question of how wh words are linked
with gaps appropriately, other than the assumption that this infor-
mation is contained somewhere in the trees defined by these rules.
Notice that we are assuming for simplicity that relative clauses are
a distinct constituent from S, and also, that no recursion at all is
allowed. For clarity, rather than build the incremental semantic in-
terpretations yielded by the parser we will display the partial tree
that a more conventional parser might build.
For a sentence like:
7 The woman the boy the child knew waved to laughed
we ought to build a tree like:
</bodyText>
<page confidence="0.987287">
132
</page>
<figure confidence="0.726441833333333">
NP VP
&amp;EL V
NP VP
REL V
NP VP
V
</figure>
<bodyText confidence="0.8758495">
the woman the boy the child knew waved to laughed
Things proceed as follows, ignoring some obvious steps:
</bodyText>
<listItem confidence="0.5432195">
(i) ( NP, nil, (NP the woman))
(ii) REL, (NP (NP the woman} {REL _4))
(iii) (NP, nil, (NP the boy))
( NP, REL, INP (NP the woman)(REL
</listItem>
<bodyText confidence="0.977425419354838">
At this point, if we are to find the correct interpretation or build
the appropriate parse tree Invoke must recognise the NP &apos;the girl&apos;
as the beginning of another relative clause, and place on the stack
an entry like:
(NP, REL, (NP {NP the girl} (REL ...)))
But of course this will violate the recursion restriction, for
there is already an (NP, REL...) on the stack. Let as assume that
this earlier one is thus &apos;forgotten&apos;, or at least rendered inaccessible
to the parsing procedure in some way. Things now proceed - again
ignoring obvious details - until we have recognised the sentence as
far as the word &apos;knew&apos;:
(iv) (NP, nil, (NP (NP the girI){REL (NP the boy)(VP
knew))))
At this point the procedure runs into trouble. If the parser
merely continues with `waved to&apos; it will be stuck: &apos;waved to&apos; in its
own is not a complete VP, for it is missing an object. So a possible
parse in which what is on the stack is the subject of &apos;waved to&apos; will
fail. But there is no other option available for it. In order to treat
&apos;waved to&apos; correctly, the parser needs to know that it is part of
a relative clause and thus can legitimately have a missing object.
But this of course is precisely the information that is no longer
available to it, for the REL entry which would have signalled this
has been erased. So the parser cannot proceed beyond this point
coherently. It is reassuring that this is exactly the point - after the
first verb of the sequence stacked up - where both intuitive and
experimental evidence (Miller and Isard 1963) suggest the onset of
difficulty with these constructions. Our parsing procedure seems
to get stuck at exactly the same point people do in these centre
embedded constructions.
With right recursion there are two cases of interest. With
multiple sentential complementation like
</bodyText>
<footnote confidence="0.71445">
9 Joe thought that Bill expected that Mary knew ....
then the operation of Clear means that the recursion limit will
never be exceeded. Whenever we have a stack of the form:
</footnote>
<table confidence="0.881729888888889">
(VP, S, beta)
{S, VP. alpha }
Clear will erase the bottom entry leaving:
(VP, S, Ax {alpha (beta (x))) }
Whenever there is a stack of the form:
(S, VP. beta)
(VP, S. alpha)
Clear will likewise produce:
(S, VP, Ax {alpha (beta (x))) }
</table>
<bodyText confidence="0.941129555555556">
Thus neither recursive category will ever have more than one
instance on the stack at a time. As in the earlier illustrative ex-
amples, the process of function composition means that, when the
final constituent is encountered, the whole complex logical expres-
sion reduces down to exactly what we would have had under the
&apos;classical&apos; view: the difference here is that we do not depend on the
whole syntactic tree being explicitly constructed first in order to
get the correct results.
While the general idea here seems correct, the details are
not entirely satisfactory, however. In the current implementation,
Clear operates whenever it can, which, as remarked above, does
not seem very plausible. Since the motivation for Clear is partly
via considerations of short term memory load, in a more realistic
model some extra parameter to reflect this transient load should
clearly be involved, such that Clear only operates when a certain
threshold is exceeded. This would mean that there was room for
some decoupling of the recursion limitation from the conditions on
Clear: at present, with a recursion limit of 1, even a sentence like
</bodyText>
<sectionHeader confidence="0.812023" genericHeader="method">
10 John expected that Bill would leave
</sectionHeader>
<bodyText confidence="0.696514307692308">
could not be parsed unless Clear had operated. But it seems
unlikely that such a short sentence imposes any very great strain
on syntactic short term memory. Furthermore, in the present im-
plementation, Clear will prevent sentential conjunctions from being
parsed at all, for by the time the conjunction is reached, the only
constituent left on the stack is labelled as a VP, not an S, and
so Invoke-rule cannot find an appropriate candidate to continue.
Fortunately, both of these wrinkles are easily amended by mak-
ing Clear more conservative in its operation, while preserving the
present type of explanation for why this type of right recursive
construction can still be parsed with little apparent effort.
Not all cases of right recursion need be &apos;rescued&apos; by Clear,
however. Given multiple PP modifiers, introduced by a rule:
</bodyText>
<sectionHeader confidence="0.775188" genericHeader="method">
NP NP PP
</sectionHeader>
<bodyText confidence="0.99958025">
we have the potential for the type of situation described earlier,
where there may be many distinct parse trees, only one of which
may accurately reflect the actual pattern of attachment of PPs to
the NP they modify.
</bodyText>
<page confidence="0.502648">
11
</page>
<bodyText confidence="0.9943333">
The house in the woods by the river
The book on rock climbing by the writer from
Scotland
The bird in the tree near the flowerbed with a red
beak
Assuming a recursion limit of 1. there is only one &apos;parse&apos; of
such structures that will succeed, since Clear - applying only to
projections of +V, recall - cannot be involved. The parsing proce-
dure will process these cases in a way which corresponds to a left
branching or stacked analysis:
</bodyText>
<figure confidence="0.732933">
NP
NP
</figure>
<page confidence="0.858989">
133
</page>
<table confidence="0.688924">
NP NP
NP PP
NP PP : NP REL
NP PP NP REt
------
</table>
<sectionHeader confidence="0.553469" genericHeader="method">
NP REL
</sectionHeader>
<bodyText confidence="0.995737138888889">
This might seem to be a serious disadvantage, for there are
clearly readings of the above examples which appear not to be
those suggested by such a `parse&apos;. However, it is actually a good
result: when there is more than one parse of a sequence like this, the
`correct&apos; one - i.e. that consistent with the preferred attachments -
must be decided on by a mixture of semantic and contextual con-
straints on what can modify what. A full and exhaustive parse is
thus still not sufficient to arrive at a unique interpretation. But if
the real work of deciding what attachments are to be made is done
by these non-syntactic procedures, then all but the lowest level of
syntactic analysis, (into non-recursive NP and PP constituents),
is entirely redundant. All but one of the more complex analyses
will be thrown away, and all of the semantic information to be
gained from that analysis has already been computed in the course
of deciding that it is the &apos;correct&apos; one. (As everyone who has ever
written a practical parser has discovered, this is in any case an
extremely silly way to do things). Thus an exhaustive syntactic
analysis is neither necessary nor sufficient for the correct handling
of these sequences. All that is required is that the low level con-
stituent structure be recognised: thereafter, the meaning of a mod-
ifier can he assumed to be a function which seeks an appropriate
argument to modify, and is thus just applied to the representation
of the meaning of the sentence that has already been built up. In-
cidentally. notice that this latter assumption is almost forced on
Its independently by the existence of rightward extraposed nomi-
nal modifiers which may be encountered without warning after an
apparently complete sentence meaning has been assembled:
12 I gave the hook back to the girl in the library that you
asked me to photocopy
The level of analysis provided by our treatment appears to be
exactly what is needed for the attachment of these modifiers to be
accommodated appropriately.
Sequences of ordinary relative clauses, and multiple conjunc-
tions will be treated in a similar way, and similar arguments apply
to them. In the case of conjunctions, of course, the fact that no
information is lost by not computing massive parse trees is even
</bodyText>
<subsectionHeader confidence="0.477899">
Mort. obVil1114.
</subsectionHeader>
<bodyText confidence="0.989617166666667">
It is interesting to note, in connection with sequences of rela-
tives. that the stacked &apos;parse&apos; which the operation of the procedure
mimics is actually the one which corresponds almost exactly to the
unexpected intonation patterns noted by Chomsky and Langen-
doen:
13 (That&apos;s the company) (that manufactures the drugs) (that
have the side effects) (that made her come out in a rash)
the company that.., that... that...
In general, then, the recursion limitation and the basic opera-
tions of the parser-interpreter seem to combine to provide a fairly
satisfactory model of the parsing and understanding of these dif-
ferent types of recursive constructions.
</bodyText>
<sectionHeader confidence="0.942844" genericHeader="conclusions">
Summary
</sectionHeader>
<bodyText confidence="0.979909333333333">
I have presented an algorithm for parsing and interpreting
grammars and semantic descriptions of a certain formal type, which
is consistent with a set of clear and uncontroversial facts about
human linguistic performance. In particular, I hope to have show
that a (partial) theory of competence can be literally embedded
within a model of performance, in such a way that simple principles
belonging to the latter (recursion limitations) explain phenomena
that have sometimes been taken to pertain to the former.
There are some further practical consequences arising from
this work: there is not space to go into the details here, but there
is an interpretation of the parsing algorithm above - as one might
suspect, given its formal properties - as a finite state transducer
mapping strings of (labelled) terminal items directly into logical
forms. While the construction of such a device from a grammar of
the original type is rather complex, the result would be a &apos;linguistic
engine&apos; (sentences in, logical forms out) of formidable efficency.
Footnote
The parser-interpreter is written in Franz Lisp under 4.2 Unix
on a Sun workstation. The current grammar provides syntactic
and semantic coverage for simple complement types, phrasal and
sentential conjunction, relative clauses, and questions.
</bodyText>
<page confidence="0.998239">
134
</page>
<sectionHeader confidence="0.996931" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993130103896104">
Ades, A. E. and Steedman, M. J. (1982)
On the Order of Words. Linguistics and Phi-
losophy 4, 517-558
Berwick, It. C. and Weinberg, A. S. (1984)
The Grammatical Basis of Linguistic Per-
formance: Language Use and Acquisition.
Cambridge, Mass: MIT Press.
13riscoe, E. J. (1984)
Towards an Understanding of Spoken Speech
Comprehension: the Interactive Determin-
ism Hypothesis. Ph. D. Diss., Dept. of Lin-
guistics, Univ. of Cambridge.
Chomsky, N. (1963)
Formal Properties of Grammars. In R.
Luce, R. Bush and E. Galanter (eds) Handbook
of Mathematical Psychology Vol II, New York:
John Wiley.
Chomsky, N. (1965)
Aspects of the Theory of Syntax. Cam-
bridge, Mass: MIT Press.
Chomsky, N. and Miller, G. (1963)
Finitary Models of Language Users. In R.
Luce, R. Bush and E. Galanter (eds) Handbook
of Mathematical Psychology Vol II, New York:
John Wiley.
Church, K. W. and Path, B.. (1982)
Coping with Syntactic Ambiguity, American
Journal of Computational Linguistics, 8, 139-149
Crab*, S. and Steedman, M. J. (forthcoming)
On Not Being Led Up The Garden Path:
The Use of Context by the Psychological
Parser. In A. Zwicky, L. Kartunnen, and
D. Dowty (eds) Natural Language Parsing: Psy-
choling-uistic, Theoretical, and Computational Per-
spectives, Cambridge: Cambridge Univ. Press.
Gawron, J. M. et al (1982)
The GPSG Linguistics System, Proceedings
of the 20th annual meeting, Association for Com-
putational Linguistics.
Isard, S. (1974)
What would you have done if...? Theoretical
Linguistics, Vol 1, 233-256
Katz, J. J. (1981)
Language and Other Abstract Objects Ox-
ford: Basil Blackwell.
Laugendoen, D. T. (1975)
Finite State Parsing of Phrase Structure
Languages and the Status of Readjustment Rules
in Grammar. Linguistic Inquiry 6, 533-554.
Langeudoen, D. T. and Langsam, Y. (1984)
The Representation of Constituent Struc-
tures for Finite State Parsing in Proceedings
of Coling 84, Association for Computational Lin-
guistics.
Miller, G. and Isard, S. D. (1964)
Free Recall of Self Embedded English Sen-
tences. Information and Control 7, 292-303.
Pulman, S. G. (1983)
Generalised Phrase Structure Grammar, Ear-
ley&apos;s Algorithm, and the Minimisation of
Recursion in Sparck Jones and Wilks eds.
Pulman, S. G. (forthcoming)
Computational Models of Parsing in A. Ellis
(ed) Progress in the Psychology of Language, Vol
2, Lawrence Erlbaum Associates Ltd.
Rosenschein, S. J. and Shieber, S. M. (1982)
Translating English Into Logical Form Pro-
ceedings of the 20th annual meeting, Asssociation
for Computational Linguistics.
Schubert, L. K. and Pelletier, F. J. (1982)
From English to Logic: Context Free Com-
putation of &apos;Conventional&apos; Logical Trans-
lation, American Journal of Computational Lin-
guistics, 8, 27-44.
Sparck Jones, K., and Wilks, Y. (eds) (1983)
Automatic Natural Language Parsing, Chich-
ester: Ellis Horwood Ltd.
</reference>
<page confidence="0.998787">
135
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999423">A Parser That Doesn&apos;t</title>
<author confidence="0.999897">S G Pulman</author>
<affiliation confidence="0.904366666666667">University of Cambridge Computer Laboratory Corn Exchange Street</affiliation>
<address confidence="0.996543">Cambridge CB2 3QC, UK.</address>
<abstract confidence="0.987801306179775">This paper describes an implemented parser-interpreter which is intended as an abstract formal model of part of the process of sentence comprehension. It is illustrated here for Phrase Structure Grammars with a translation into a familiar type of logical form, although the general principles are intended to apply to any grammatical theory sharing certain basic assumptions, which are discussed in the paper. The procedure allows for incremental semantic interpretation as a sentence is parsed, and provides a principled explanation for some familiar observations concerning properties of deeply recursive constructions. Back ground The starting point for the present work is a set of familiar and, for the most part, uncontroversial claims about the nature of grammatical description and of human parsing of natural language. These claims and assumptions can be briefly summarised as follows: A Hierarchical Structure Linguists assign constituent structures to sentences on the basis of distributional tests of various kinds. On the basis of these tests, the &apos;correct&apos; structures are always hierarchical and often deeply nested. The tree representing a sentence may impose a great deal of structure on it, with string-adjacent items often appearing at very different levels in the tree. In general, shallow, &apos;fiat&apos; structures are not generated by grammars, nor warranted on distributional grounds. However, as we shall see, it is likely that these deeply nested structures may be somewhat remote from any that are actually computed during parsing. Semantics is (i) compositional and (ii) Both of these claims can be made in a variety of versions of different strengths, from the trivially true to the fairly clearly false. What is intended here is the assumption sometimes called the &apos;rule to rule&apos; hypothesis. shared by almost all current grammatical frameworks, that to each syntactic rule of a grammar (or for each subtree induced by such a rule) there is an associated semantic rule, either producing an interpretation directly, or translating into some formal language. Interpretations for whole sentences are built up from the constituent parts in ways specified by these rules, in a fashion which mimics and uses the syntactic structure of the sentence. C Incremental interpretation As a sentence is parsed, its interpretation is built up word by word: there is little or no delay in interpreting it. In particular, we do not wait until all syntactic constituents have been completed before beginning to integrate then into some non-syntactic representation. Ample intuitive and experimental evidence supports this uncontroversial observation. D Limited recursion. One of the most firmly established facts about human syntactic processing is that constructions which are ineliminably deeply recursive (such as central self-embeddings) are difficult or impossible to parse. A sentence like: 1 The boy who the girl that the dog bit liked ran away is clumsy at best, and one like: 2 The boy the girl the dog the cat scratched bit saw left is utterly unmanageable under normal circumstances. Under the further assumption, recently more controversial (Katz 1081), that grammars have some kind of mental reality as representations of linguistic knowledge, it is clear that A to D, although simple and generally agreed upon observations, by no means obviously consistent with each other. Consider, for example, the natural way in which one might set about implementing a system which observed B, a principle which, in itself, is a computationally natural principle. Such a system might first parse a sentence, annotating nodes in the resulting tree with an indication of the syntactic rules used. This annotated tree would then be passed to an interpretation routine which applied the appropriate semantic operation to the topmost node (guided by the syntactic information found there, in particular a pointer to the semantic information necessary), calling itself recursively on each subtree to build up the complete interpretation. (Systems operating in more or less this manner are described in Rosenschein and Shieber 1982, Gawron et al. 1982 and Schubert and Pelletier 1982. They are not intended as psychological models in any but the most abstract sense, of course.) Such a system would, in observing B, also naturally be consistent with A. Obviously, though, this type of system requires a complete syntactic analysis to be available before it can even begiu the process of interpretation, thus conflicting straightforwardly with C. Consider next A and D. The structures which linguists postulate in accordance with A are often recursive, and it is in the nature of hierarchical structures that this should be a possibility. This is rather puzzling in the light of D, for if D is correct, it seems to show that a class of structures which are natural from one point of view (i.e. centre embeddings) are extremely unnatural from another. It is not necessarily to be expected that human linguistic abilities have evolved in a harmonious and homogeneous manner, but other things being equal, we would not expect to find two apparently co-operating modules so ill-suited to each other. Why should grammars be able to generate things that parsers can&apos;t parse? When we consider left and right recursions, some further tension between A. B and D emerges. Multiple left recursions in English are most clearly illustrated by possessive determiner phrases, which are generally assumed to have a structure something like 3: 128 3 HP VP VP Det Dot HP pose : Det NP pone Jane s mother e eat • fur and multiple right recursions by a variety of structures, for example, relative clauses: [the company that (the drugs that have side that her come out in a rashill several facts which suggest that the structures assigned to these examples by a grammar in accordance with A cannot realistically be assumed to play any very direct role in the actual processing of them in performance. Firstly, there is the faobservation (Chomsky 1965: 13-14, Langendoen 544), examples like and 4 do not have the intonation contours that would be predicted for them on the basis of the constituent structures assigned to them by a grammar. For example, in 3, the intonation of the sequence of possessives is not defined over the whole constituent, as might be expected, but is more like a &apos;list&apos; intonation. In sentences like 4, the intonation contour sometimes breaks the sentence up in the way indicated informally here: 5 (That&apos;s the company( (that manufactures the drugs) (that have the side effects] [that made her come out in a rash) This chunking of the sentence does not respect its syntactic structure, splitting the head NP of the relative clause from its modifier and grouping it with the main clause instead. The conditions under which this happens are clearly connected with matters and so on, so the actual examples here are also capable of receiving the &apos;correct&apos; contour, but the effect is clearly to be seen in longer and more complex sequences. This observation is generally taken to indicate that, whatever else is happening in the production and comprehension of such examples, it is not the case that complete syntactic structures of the type assigned by a grammar A further argument that this is so derives from the fact that although 4 was displayed as a right branching structure, it would also receive a left branching analysis, and if sufficiently complex. all possible combinmions of the two. This means that the number of parses such a structure would receive goes up massively with the number of clauses involved (see Church and Patil 1982 for discussion of this. Analogous comments hold for PP modifiers and conjunctions on most analyses). It is clearly stretching credibility to assume that a parsing procedure follows very faithfully what a grammar says about such cases. difficult to reconcile with A (and hence obserare consistent with This perhaps needs some elaboration: is reasonable conjecture, given what we know about short term linguistic memory, that the human parsing mechanism operates in a way which has the formal properties of a finite state device (see e.g. Chomsky 1963, Chomsky and Miller 1963, or, more recently, Langendoeu and Langsam 1984). The fact that unlimited right or left recursion can be recognised, whereas centre recursion cannot, is consistent with this, for any (CF) language with bounded centre embedding is also a finite state language. However, when we turn to full parsing, as opposed to recognition, it turns out that the proper left and right recursion demands non-finite-state resources (Langendoen 1975). Intuitively, this is easily seen: parsing a language can be regarded, abstractly, as a transduction from strings of terminal items to labelled bracketings representing structural descriptions. For the labelled bracketings to be well formed, left and right brackets bearing the same label must be paired up correctly. In the case of recursion, this means that the bracket language contains cases where some number of left brackets of type X must be paired up with the same number of right brackets of type X, for any number. This is a classic non-finite state language, and thus even if the input to the transducer is finite state, the overall transduction must be at least of context-free power, given no finite bound on recursion. Full parsing, therefore, of structures like 3 and 4, will demand resources of at least this power. us now assume that should be taken to apply, not just to cases of centre embedding, but to all types of recursion (as in Miller and lsard&apos;s original (1963) discussion of centre embedding). This is, in effect, a conjecture that the human parsing mechanism is forced to operate with no more than finite state resources, even though the class of languages generated by the grammars found natural by human beings might lie far outside the finite state class. Under such circumstances it would be expected that in the left and right recursive cases, a full parsing would not always be available, an expectation that we may take to be supported by the intonational evidence, and by the combinatorial explosion considerations alluded to above. If this is a plausible line of reasoning, it nevertheless presents with a further difficulty in the light of observation For if semantles is driven by syntax, it would seem to follow that structures which are not properly parsed should not be fully interpretable either. While this is clearly the case for centre embeddings, it is the case for either left or right recursion: semantically speaking they are completely unproblematic. This is a further conflict which our model of parsing will have to resolve. 129 An Incremental Parser-Interpreter My aim was to develop s parser and interpreter which was compatible with A to D, resolving the apparent conflicts between them, and which also incorporated in a fairly concrete form the assumption that grammars have some status, independently of parsers. as mental objects. That is to say, it was assumed that what linguists say about natural language in the form of a grammar (including semantic interpretation rules) is available to the parser-interpreter as some kind of data structure having roughly the form that the linguist&apos;s pencil and paper description would suggest. The aim was also to demonstrate a serious commitment to C by getting the parser to build up explicit representations of the meaning of a sentence piece by piece during the course of a parse. To my knowledge, the only other work which takes this commitseriously at the appropriate of formal detail (there is no shortage of well intentioned hand-waving) is that of Ades and Steedman (1082). In Pulman (forthcoming), I discuss some of the similarities and differences between these two approaches. For purposes of illustration, I will assume that the underlying grammatical theory involved is some form of Phrase Structure Grammar, where semantic interpretation consists of translation into a simple form of higher order logic. Neither of these assumptions is crucial: the parsing procedure can be adapted to certain of grammar, and the associated process of semantic interpretation requires only that the semantic theory can driven syntactic structures, and that there some way of function application and composition. unlikely that this rules out any at all. procedure is best of as a type of stack-based algorithm, with the ability to deal with inconstituents. the current implementation it operates others) have argued elsewhere (Pulman, forthcoming) that there is no good reason to suppose that (as opposed a more global process of comprehension) is Marcus 1980, Berwick and Weinberg 1984. also CraM Steedman, forthcoming., Briscoe 1984). driving of the parser-interpreter maintains an representing a particular state of a configuration is pair consisting of a representation of stack, the current position in the input string. stack is a list of entries, which (usually) only the top two the operations of the parser. Each entry a wholly partially recognised constituent, along with its interpretation in terms of a translation into a logical expression. An entry is a triple, consisting of a category label, indicating type of constituent is being recognised, a &apos;needed&apos; list of conwhich must found before the category is complete, and the interpretation so far. The parser starts with an initial configuby trying to produce new ones from that no more alternatives left, and the parse has failed, or one more parses are produced. are four basic operations which produce a new configfrom an one. Which one is performed depends on the of the stack. there is a choice between two, both are performed, producing two new configurations. SHIFT: takes the next word from the input and creates a new entry it (for each lexical entry it has in the dictionary). For example, given a lexical entry like (every, Det, AP AQ Ax Px —• Qx) Shift produces a stack entry like: IDet, nil, AP AQ Ax Px Qx) interpretation of non-logical is assumed to be the constant, as is Since lexical categories are complete second &apos;needed&apos; element in a stack entry will be empty. created a new stack entry, Shift records a new configuration with that entry on top of the stack, and an updated input pointer. INVOKE-RULE: applies when there is a completed entry on top of the stack. Essentially, it checks the rules in the grammar see whether category represented by that entry could higher level Although this is not strictly necesone-word lookahead incorporated for efficiency. If Invoke-rule succeeds in matching a category of an entry with the first member of the right hand side of a rule, it creates a new entry from them. Logically speaking, this process happens as follows: assume, for illustration, an entry of the form (Bet, nil, every) (where the interpretation of &apos;every&apos; might actually be as above) and a example rule of the form: NP Det N ; Det&apos; (N&apos;) where the part ofter the semi-colon is the semantic component. The entry matches the beginning of the right hand side of the rule and so could begin an NP constituent. Now assume a function, call it Abstract, which when applied to a rule of this form produces from its right hand side and semantic component the result of lambda abstracting over all the right hand side symbols (in the order specified in the rule) which appear in the semantic component. Thus Abstract applied to the rule above would produce A det A n det (n)} If applied to a rule like S NP VP ; VP&apos; (NP&apos;) it would produce np A vp (rip)) is simply a more literal rendering of what the rule says, in fact: making explicit the fact that the items occurring in the semantic part of the rule are to be interpreted as variables. When Invoke-rule has matched an entry to a rule it produces a new entry where the category is the left hand side of the rule, the &apos;needed` list is all but the first of the right hand side, and the interpretation is the result of applying Abstract to the rule and applying that to the interpretation of original entry. In example above the result all this would be: N, A ( (n)) ) In other words, the interpretation is simply that of the whole rule with that of the existing entry put in the appropriate place: a equivalent of the &apos;needed&apos; field. the interpretation of an incomplete constituent is that it is a function expecting to find the needed items as arguments. COMBINE: combines a complete entry on top of the stack with an incomplete one below it, if the category label of the former matches the first &apos;needed&apos; item of the latter. For example, if the stack contained an entry like the one just described, with a complete entry on top: (N, nil, man) (NP, N, A n { every (n)} } would produce a new entry with the category of incomplete one, remainder, if any, of the needed list, and an which is the result applying that of the incomplete to that of one. Here the result would {NI&apos;, nil, every (man) } 130 when beta reduction of the lambda expressions has taken place, which is a complete constituent, in this instance, although this need be the case. If the is not nil, the interpretation will always reflect this. These three operations are in fact sufficient to allow the parser to operate. However, a further operation is also necessary if we are to maintain consistency with our original assumptions. CLEAR: Clear is intended to correspond to the intuition that complete or completable representation of a proposition has beeu built up. the syntactic information needed to do this is no longer required. under normal circumstances. The conditions under which Clear operates in the present implementation ensures that this type of syntactic information is discarded as soon as possible: although this is probably not a realistic claim about human parsing. Clear operates when: (i) there are only two items on the stack (in a less enthusiastic version. Clear would be constrained to operate only on the bottom two items on the stack) (ii) the topmost one potentially contains everything needed to complete the bottom one (iii) the topmost one is a VP or S The first two conditions correspond to the obvious truth that you can only get rid of syntactic information when it is safe to do so, and that &apos;selective forgetting&apos; is not possible: either all the syntactic information relevant to the earlier portion of the sentence is discarded, or none of it is. Otherwise, the claim, and the later explanations which depend on it, would be vacuous. The third is intended to capture the intuition that it is the main predicate of a sentence which when encountered provides enough information to be able to continue parsing safely after that point with no reference to anything before. For example, when a verb is encountered, the number and type of (obligatory) arguments will be known. When the conditions for Clear are met, the effect is that the interpretation of the bottommost entry is composed with that of the topmost, the bottom one then being erased. For example, in a situation like: {VP, NP, A np (likes (up)) {S, VP, A vp {vp (some (man))) ) where the topmost entry is of the type that the one underneath is looking for, the result of Clear is that the stack will contain just: (VP, NP, A x (A vp {vp (some (man))) {A np (likes (np)) kW} When this VP finds the NP it is looking for, the interpretation will reduce to what we would have had more directly if Clear had not operated. Here is a trace of the parser to show how all these operations work together. The meanings of the individual lexical items have been suppressed in the interests of readability.</abstract>
<degree confidence="0.19834675">S NP VP; VP (NP) VP --• V NP ; V (NP) NP Det N ; Det (N) Input: The farmer killed the duckling</degree>
<note confidence="0.952587411764706">Shift: {Det, nil, the) Invoke: N, A n {the (n)} Shift: (N, nil, farmer) {NP, N, A a (the (a)) } Combine: {NP, N, the (farmer) I Invoke: {S, VP, A vp (vp (the (farmer))) } Shift: (V, nil. killed) {S, VP, A vp vp (the (farmer))) Invoke: (VP, NP, A np { killed (np)) ) {S, VP, A vp { vp (the (farmer))) I Clear: {VP, NP, A x vp {vp (the (farmer))) {A np {killed (up)) (x) }H Shift: (Det, nil, the) (VP, NP, A x (A vp {vp (the (farmer))) (A np (killed (np)) (x) ) Invoke: N, A (the (n)} } {VP, NP, A x {A vp {vp (the (farmer))) {A np (np)) (x) Shift: (N, nil, duckling) {NP, N, A n (the (a)) ) {VP, NP, A x {A vp {vp (the (farmer))) {A np (np)) (x) 1) ) Combine: (NP, nil, the (duckling</note>
<abstract confidence="0.963377490909091">VP, NP, A x {A vp {vp (the (farmer))) np (killed (np)) (x) }} Combine: (VP, nil, A x {A vp {vp (the (farmer))) {A np (np)) (x) }} (the (duckling)) ) At this point the parse is complete, and the complex interpretation beta-reduces to: (killed (the (duckling))) (the (farmer)) The resulting interpretation is exactly what would have been obtained by a &apos;classical&apos; system operating as described earlier. Modelling Incremental Interpretation How does the parsing procedure manage to remain faithful to to Let us begin with B: the compositional, syntax-driven nature of semantics. The parser assumes that semantic information can be associated with syntactic rules in some way (though it is not ruled out in fact, it is assumed that some extra aspects of interpretation may need to be computed by separate procedures: for example, identification of variables for the purposes of indicating coreference; cases of wide scope of quantifier phrases in syntactic narrow scope positions, etc.). Once the rule in question has been identified by Invoke-rule, the semantic information involved is extracted and used to form the next stack entry. The syntactic information is also used to form expectations about what constituents must come next, although it is conceivable that if semantic type is entirely predictable from syntactic category and vice versa this information is actually redundant. No other mechanisms for linking syntax with semantics are required. Hence the parser obeys condition B absolutely literally and faithfully. 131 The important thing to notice is that this is achieved without building any explicit syntax trees during the course of parsing a sentence. Syntactic information is used to build up the interpretation and to guide the parse, but does not result in the construction of an independent level of representation. AS the title of the paper indicates, there is no parse tree built for a sentence at all. While it is true that in some sense trees are implicit in the sequence of operations of the parser, this is an inevitable consequence of the fact that the rules used themselves define trees, and as we shall see, even in this weak sense the tree structures implicit for certain types of recursive construction are not isomorphic to those which would be defined by the grammar. I like to think of this aspect of the operation of the parser as embodying the intuition often expressed (most often in the oral traditiou, though explicit in Isard 1974), that syntax is a &apos;control structure for semantics. It also has the merit of being consistent the widespread agreement among linguists that syntax plays a central role in language understanding, and with the apparently equally widespread failure of psycholinguists to find any evidence that purely syntactic representations are computed at any stage during normal comprehension. Turning now to C, the observation that sentences are understood on (at least) a word by word basis on a pass through from left to right, it should be clear that our procedure provides a direct model of this process, on the assumption that at least a central part of the meaning of a sentence is given by a translation into a logical form of this kind. As soon as a word is encountered, it is integrated into the logical form being built up. At every stage, this logical form, though possibly not yet complete, is a perfectly meaningful object (within the higher order logic assumed here it is just a term like any other): it can be used to perform inferences, be the antecedent for anaphora or ellipsis, be integrated with the so as to assess and discard alternative interpretations corresponding to different parsings, and in general perform any of the functions we expect the meaning of a sentence or sentence fragment to be able to do. The satisfying of A is in a sense automatic but trivial, given that the parser uses ordinary grammatical rules, rather than some preprocessed version altering the output of the rules to produce flat structures (as, for example, in Langendoen 1975, Langendoen and Langsam 1984, and also wrongly, on the present approach in Putman 1983). More interesting is the way the parser produces a similar effect to that achieved with these preprocessings, without altering the rules themselves, as a side effect of its observance of D the limitation on recursion. Recursion Limitations I have argued elsewhere (Putman, forthcoming) that attempts to explain the difficulty of centre embedded sentences as a conseparsing strategies unsuccessful, and that the simplest explanation is the original one (Miller and Isard 1963): that the human parsing mechanism is fundamentally incapable of operating recursively. To be more precise: if (in the worst case) the parser encounters an instance of a construction in the course of trying to parse an earlier instance of it, the record of the earlier instance will erased and &apos;forgotten&apos;, causing confusion cases where the information is needed to complete a parse successfully, as in the centre embedding cases. Clearly this is not absolute: some instances of centre embedding can be found to a depth of 4 or 5, but for simplicity we will assume that there is some small fixed limit, L. The present procedure implements this restriction quite literally: if Invoke-rule attempts to put on the stack an incomplete of category X, are already L instances of such incomplete Xs on the stack, then the earliest instance is erased Invoke-rule can succeed. thing this restriction is as stated, it applies to all types and thus be expected to result in parsing failures for centre embedded of a depth greater than L, left and right recursions than L too. However, this does not happen: the basic operations of the parser in fact conspire to bring it about that both left and right recursions can be parsed, the former fully, and the latter to just the extent, apparently, that be able to provide them with an appropriate intera perfectly and simple restriction can be imposed, rather than some version (implausibly) qualified so as to distinguish between different types of recursion. simplest case that of left recursion, which we will illuswith an artifical grammar: A Aa A (a) A — a; a processing a string `aaa...&apos;, the parser operates as in trace (`11&apos; is the of &apos;al:</abstract>
<note confidence="0.961992916666667">Shift: (a, nil, b) Invoke: (A, nil, b} Invoke: (A, a, Aa (b (a))) Shift: (a, nil, b) (A, a, Aa (b (a))) Combine: (A, nil, b (b) ) Invoke: (A, a, Aa (b (b (a))) </note>
<abstract confidence="0.954546916666667">At this point the cycle of operations has become evident: at no point is there ever more than one occurrence of an incomplete A constituent on the stack, and so there is never any situation in which the recursion limitation would come into effect. In other words, like any shift-reduce mechanism, this parser can process left without the stack growing beyond a constant depth. Centre embeddings of a depth greater than L will not be parsed correctly. To see how this might work out in detail we will assume some simple rules for relatives: NP NP REL : REL&apos;(NP&apos;) REL NP VP : NP1VP&apos;) and we will ignore the question of how wh words are linked with gaps appropriately, other than the assumption that this inforcontained somewhere in trees defined by these that we are assuming simplicity that relative clauses are a distinct constituent from S, and also, that no recursion at all is clarity, than build the incremental semantic inby the parser we will display the partial tree that a more conventional parser might build. For a sentence like: 7 The woman the boy the child knew waved to laughed we ought to build a tree like: 132</abstract>
<title confidence="0.791326">amp;EL V NP VP REL V NP VP</title>
<abstract confidence="0.978289603960396">V the woman the boy the child knew waved to laughed Things proceed as follows, ignoring some obvious steps: (i) ( NP, nil, (NP the woman)) (ii) REL, (NP (NP the woman} {REL _4)) (NP, nil, the boy)) ( NP, REL, INP (NP the woman)(REL At this point, if we are to find the correct interpretation or build appropriate parse tree Invoke must recognise &apos;the girl&apos; as the beginning of another relative clause, and place on the stack entry (NP, REL, (NP {NP the girl} (REL ...))) But of course this will violate the recursion restriction, for there is already an (NP, REL...) on the stack. Let as assume that this earlier one is thus &apos;forgotten&apos;, or at least rendered inaccessible to the parsing procedure in some way. Things now proceed again ignoring obvious details until we have recognised the sentence as far as the word &apos;knew&apos;: (iv) (NP, nil, (NP (NP the girI){REL (NP the boy)(VP knew)))) At this point the procedure runs into trouble. If the parser merely continues with `waved to&apos; it will be stuck: &apos;waved to&apos; in its own is not a complete VP, for it is missing an object. So a possible parse in which what is on the stack is the subject of &apos;waved to&apos; will fail. But there is no other option available for it. In order to treat &apos;waved to&apos; correctly, the parser needs to know that it is part of a relative clause and thus can legitimately have a missing object. But this of course is precisely the information that is no longer to for the REL entry which would have signalled this erased. So the parser proceed beyond this point is reassuring that this is exactly the point after the sequence stacked up both intuitive and evidence (Miller and Isard suggest the onset of difficulty with these constructions. Our parsing procedure seems to get stuck at exactly the same point people do in these centre embedded constructions. With right recursion there are two cases of interest. With multiple sentential complementation like that Bill expected that knew .... then the operation of Clear means that the recursion limit will we have a stack of the form: (VP, S, beta) {S, VP. alpha } will erase the entry leaving: (VP, S, Ax {alpha (beta (x))) } Whenever there is a stack of the form: (S, VP. beta) (VP, S. alpha) Clear will likewise produce: (S, VP, Ax {alpha (beta (x))) } Thus neither recursive category will ever have more than one instance on the stack at a time. As in the earlier illustrative examples, the process of function composition means that, when the final constituent is encountered, the whole complex logical expression reduces down to exactly what we would have had under the view: the difference here that we do not depend on the whole syntactic tree being explicitly constructed first in order to get the correct results. While the general idea here seems correct, the details are not entirely satisfactory, however. In the current implementation, Clear operates whenever it can, which, as remarked above, does not seem very plausible. Since the motivation for Clear is partly via considerations of short term memory load, in a more realistic model some extra parameter to reflect this transient load should clearly be involved, such that Clear only operates when a certain is exceeded. This would mean that there for some decoupling of the recursion limitation from the conditions on Clear: at present, with a recursion limit of 1, even a sentence like 10 John expected that Bill would leave could not be parsed unless Clear had operated. But it seems that such a short sentence imposes any very great syntactic term memory. Furthermore, in the present implementation, Clear will prevent sentential conjunctions from being parsed at all, for by the time the conjunction is reached, the only constituent left on the stack is labelled as a VP, not an S, and so Invoke-rule cannot find an appropriate candidate to continue. Fortunately, both of these wrinkles are easily amended by making Clear more conservative in its operation, while preserving the present type of explanation for why this type of right recursive construction can still be parsed with little apparent effort. Not all cases of right recursion need be &apos;rescued&apos; by Clear, however. Given multiple PP modifiers, introduced by a rule: NP NP PP we have the potential for the type of situation described earlier, there may many distinct parse trees, only one of which the actual pattern of attachment of PPs to the NP they modify. 11 The house in the woods by the river The book on rock climbing by the writer from Scotland The bird in the tree near the flowerbed with a red beak Assuming a recursion limit of 1. there is only one &apos;parse&apos; of such structures that will succeed, since Clear applying only to of +V, recall cannot involved. parsing procewill process these a way which corresponds to a left branching or stacked analysis: NP NP 133</abstract>
<title confidence="0.911032333333333">NP NP NP PP NP PP : NP REL NP PP NP ------ NP REL</title>
<abstract confidence="0.995491471428572">This might seem to be a serious disadvantage, for there are clearly readings of the above examples which appear not to be those suggested by such a `parse&apos;. However, it is actually a good result: when there is more than one parse of a sequence like this, the `correct&apos; one i.e. that consistent with the preferred attachments must be decided on by a mixture of semantic and contextual constraints on what can modify what. A full and exhaustive parse is thus still not sufficient to arrive at a unique interpretation. But if the real work of deciding what attachments are to be made is done by these non-syntactic procedures, then all but the lowest level of syntactic analysis, (into non-recursive NP and PP constituents), is entirely redundant. All but one of the more complex analyses be away, and all of the semantic information to be from that analysis has been computed in the course that it is the &apos;correct&apos; one. (As everyone has ever written a practical parser has discovered, this is in any case an silly way to do an exhaustive syntactic is necessary nor sufficient for the correct handling of these sequences. All that is required is that the low level constituent structure be recognised: thereafter, the meaning of a modifier can he assumed to be a function which seeks an appropriate argument to modify, and is thus just applied to the representation of the meaning of the sentence that has already been built up. Inthis latter assumption is almost forced on independently by the existence of rightward nomimodifiers which be encountered without warning after an complete meaning has been assembled: I the hook to the girl in the library that you me photocopy analysis provided by our treatment appears to be what is for attachment of these modifiers to be relative clauses, and multiple conjuncbe treated in similar way, and similar arguments apply them. the of of course, the fact that no is lost not computing massive parse trees is even Mort. obVil1114. is interesting to note, connection with sequences of relatives. that the stacked &apos;parse&apos; which the operation of the procedure mimics is actually the one which corresponds almost exactly to the intonation patterns noted by and Langendoen: the company) (that manufactures the drugs) (that side effects) (that made her come out in a rash) the company that.., that... that... In general, then, the recursion limitation and the basic operaparser-interpreter seem to combine to provide a fairly model the parsing and understanding of these different types of recursive constructions. Summary I have presented an algorithm for parsing and interpreting grammars and semantic descriptions of a certain formal type, which is consistent with a set of clear and uncontroversial facts about human linguistic performance. In particular, I hope to have show that a (partial) theory of competence can be literally embedded within a model of performance, in such a way that simple principles belonging to the latter (recursion limitations) explain phenomena that have sometimes been taken to pertain to the former. There are some further practical consequences arising from this work: there is not space to go into the details here, but there is an interpretation of the parsing algorithm above as one might suspect, given its formal properties as a finite state transducer of (labelled) terminal items directly into logical forms. While the construction of such a device from a grammar of the original type is rather complex, the result would be a &apos;linguistic engine&apos; (sentences in, logical forms out) of formidable efficency. Footnote The parser-interpreter is written in Franz Lisp under 4.2 Unix on a Sun workstation. The current grammar provides syntactic and semantic coverage for simple complement types, phrasal and sentential conjunction, relative clauses, and questions.</abstract>
<note confidence="0.7365575">134 References A. E. and Steedman, M. (1982) the Order of Words. and Philosophy 4, 517-558 Berwick, It. C. and Weinberg, A. S. (1984) The Grammatical Basis of Linguistic Performance: Language Use and Acquisition. Mass: 13riscoe, E. J. (1984)</note>
<title confidence="0.912786">Towards an Understanding of Spoken Speech Comprehension: the Interactive Determin-</title>
<author confidence="0.670262">D Diss</author>
<author confidence="0.670262">Dept of Lin-</author>
<affiliation confidence="0.678525">guistics, Univ. of Cambridge.</affiliation>
<degree confidence="0.957645230769231">Chomsky, N. (1963) Properties of Grammars. In Luce, R. Bush and E. Galanter (eds) Handbook of Mathematical Psychology Vol II, New York: John Wiley. Chomsky, N. (1965) of the Theory of Syntax. Cambridge, Mass: MIT Press. Chomsky, N. and Miller, G. (1963) Models of Language Users. In Luce, R. Bush and E. Galanter (eds) Handbook of Mathematical Psychology Vol II, New York: John Wiley.</degree>
<note confidence="0.658463666666666">Church, K. W. and Path, B.. (1982) with Syntactic Ambiguity, Journal of Computational Linguistics, 8, 139-149 S. and Steedman, M. On Not Being Led Up The Garden Path: The Use of Context by the Psychological In Zwicky, L. Kartunnen, and D. Dowty (eds) Natural Language Parsing: Psy- Theoretical, and Computational Perspectives, Cambridge: Cambridge Univ. Press. Gawron, J. M. et al (1982) GPSG Linguistics System, of the 20th annual meeting, Association for Computational Linguistics. Isard, S. (1974) would you have done if...? Linguistics, Vol 1, 233-256 Katz, J. J. (1981) and Other Abstract Objects Oxford: Basil Blackwell. Laugendoen, D. T. (1975)</note>
<title confidence="0.7268055">Finite State Parsing of Phrase Structure the Status of Readjustment Rules</title>
<note confidence="0.7193565">in Grammar. Linguistic Inquiry 6, 533-554. Langeudoen, D. T. and Langsam, Y. (1984)</note>
<title confidence="0.927893">The Representation of Constituent Strucfor Finite State Parsing Proceedings</title>
<note confidence="0.921162384615385">of Coling 84, Association for Computational Linguistics. G. and Isard, S. (1964) Free Recall of Self Embedded English Senand Control 7, 292-303. Pulman, S. G. (1983) Generalised Phrase Structure Grammar, Earley&apos;s Algorithm, and the Minimisation of Sparck Jones and Wilks eds. Pulman, S. G. (forthcoming) Models of Parsing A. Ellis (ed) Progress in the Psychology of Language, Vol 2, Lawrence Erlbaum Associates Ltd. S. J. and Shieber, S. M. English Into Logical Form Proceedings of the 20th annual meeting, Asssociation for Computational Linguistics. Schubert, L. K. and Pelletier, F. J. (1982) From English to Logic: Context Free Computation of &apos;Conventional&apos; Logical Trans- Journal of Computational Linguistics, 8, 27-44. Sparck Jones, K., and Wilks, Y. (eds) (1983) Natural Language Parsing, Chichester: Ellis Horwood Ltd. 135</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A E Ades</author>
<author>M J Steedman</author>
</authors>
<title>On the Order of Words.</title>
<date>1982</date>
<journal>Linguistics and Philosophy</journal>
<volume>4</volume>
<pages>517--558</pages>
<marker>Ades, Steedman, 1982</marker>
<rawString>Ades, A. E. and Steedman, M. J. (1982) On the Order of Words. Linguistics and Philosophy 4, 517-558</rawString>
</citation>
<citation valid="true">
<authors>
<author>C</author>
<author>A S Weinberg</author>
</authors>
<title>The Grammatical Basis of Linguistic Performance: Language Use and Acquisition.</title>
<date>1984</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass:</location>
<marker>C, Weinberg, 1984</marker>
<rawString>Berwick, It. C. and Weinberg, A. S. (1984) The Grammatical Basis of Linguistic Performance: Language Use and Acquisition. Cambridge, Mass: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J 13riscoe</author>
</authors>
<title>Towards an Understanding of Spoken Speech Comprehension: the Interactive Determinism Hypothesis.</title>
<date>1984</date>
<tech>Ph. D. Diss.,</tech>
<institution>Dept. of Linguistics, Univ. of Cambridge.</institution>
<marker>13riscoe, 1984</marker>
<rawString>13riscoe, E. J. (1984) Towards an Understanding of Spoken Speech Comprehension: the Interactive Determinism Hypothesis. Ph. D. Diss., Dept. of Linguistics, Univ. of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Formal Properties of Grammars. In</title>
<date>1963</date>
<booktitle>Galanter (eds) Handbook of Mathematical Psychology Vol II,</booktitle>
<publisher>John Wiley.</publisher>
<location>New York:</location>
<contexts>
<context position="8473" citStr="Chomsky 1963" startWordPosition="1387" endWordPosition="1388">lved (see Church and Patil 1982 for discussion of this. Analogous comments hold for PP modifiers and conjunctions on most analyses). It is clearly stretching credibility to assume that a parsing procedure follows very faithfully what a grammar says about such cases. While difficult to reconcile with A (and hence B) these observations are consistent with D. This perhaps needs some elaboration: it is a reasonable conjecture, given what we know about short term linguistic memory, that the human parsing mechanism operates in a way which has the formal properties of a finite state device (see e.g. Chomsky 1963, Chomsky and Miller 1963, or, more recently, Langendoeu and Langsam 1984). The fact that unlimited right or left recursion can be recognised, whereas centre recursion cannot, is consistent with this, for any (CF) language with bounded centre embedding is also a finite state language. However, when we turn to full parsing, as opposed to recognition, it turns out that the proper analysis even of left and right recursion demands non-finite-state resources (Langendoen 1975). Intuitively, this is easily seen: parsing a language can be regarded, abstractly, as a transduction from strings of termina</context>
</contexts>
<marker>Chomsky, 1963</marker>
<rawString>Chomsky, N. (1963) Formal Properties of Grammars. In R. Luce, R. Bush and E. Galanter (eds) Handbook of Mathematical Psychology Vol II, New York: John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax.</title>
<date>1965</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass:</location>
<contexts>
<context position="6232" citStr="Chomsky 1965" startWordPosition="1010" endWordPosition="1011">rally assumed to have a structure something like 3: 128 3 HP VP VP Det Dot HP pose : Det NP pone Jane s mother e eat • fur and multiple right recursions by a variety of structures, for example, relative clauses: 4 That&apos;s [the company that manufactures (the drugs that have [the side effects that made her come out in a rashill There are several facts which suggest that the structures assigned to these examples by a grammar in accordance with A cannot realistically be assumed to play any very direct role in the actual processing of them in performance. Firstly, there is the familiar observation (Chomsky 1965: 13-14, Langendoen 1975: 544), that examples like 3 and 4 do not have the intonation contours that would be predicted for them on the basis of the constituent structures assigned to them by a grammar. For example, in 3, the intonation of the sequence of possessives is not defined over the whole constituent, as might be expected, but is more like a &apos;list&apos; intonation. In sentences like 4, the intonation contour sometimes breaks the sentence up in the way indicated informally here: 5 (That&apos;s the company( (that manufactures the drugs) (that have the side effects] [that made her come out in a rash</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Chomsky, N. (1965) Aspects of the Theory of Syntax. Cambridge, Mass: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
<author>G Miller</author>
</authors>
<title>Finitary Models of Language Users. In</title>
<date>1963</date>
<booktitle>Galanter (eds) Handbook of Mathematical Psychology Vol II,</booktitle>
<publisher>John Wiley.</publisher>
<location>New York:</location>
<contexts>
<context position="8498" citStr="Chomsky and Miller 1963" startWordPosition="1389" endWordPosition="1392">ch and Patil 1982 for discussion of this. Analogous comments hold for PP modifiers and conjunctions on most analyses). It is clearly stretching credibility to assume that a parsing procedure follows very faithfully what a grammar says about such cases. While difficult to reconcile with A (and hence B) these observations are consistent with D. This perhaps needs some elaboration: it is a reasonable conjecture, given what we know about short term linguistic memory, that the human parsing mechanism operates in a way which has the formal properties of a finite state device (see e.g. Chomsky 1963, Chomsky and Miller 1963, or, more recently, Langendoeu and Langsam 1984). The fact that unlimited right or left recursion can be recognised, whereas centre recursion cannot, is consistent with this, for any (CF) language with bounded centre embedding is also a finite state language. However, when we turn to full parsing, as opposed to recognition, it turns out that the proper analysis even of left and right recursion demands non-finite-state resources (Langendoen 1975). Intuitively, this is easily seen: parsing a language can be regarded, abstractly, as a transduction from strings of terminal items to labelled brack</context>
</contexts>
<marker>Chomsky, Miller, 1963</marker>
<rawString>Chomsky, N. and Miller, G. (1963) Finitary Models of Language Users. In R. Luce, R. Bush and E. Galanter (eds) Handbook of Mathematical Psychology Vol II, New York: John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>B Path</author>
</authors>
<title>Coping with Syntactic Ambiguity,</title>
<date>1982</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>8</volume>
<pages>139--149</pages>
<marker>Church, Path, 1982</marker>
<rawString>Church, K. W. and Path, B.. (1982) Coping with Syntactic Ambiguity, American Journal of Computational Linguistics, 8, 139-149</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Crab</author>
<author>M J Steedman</author>
</authors>
<title>On Not Being Led Up The Garden Path: The Use of Context by the Psychological Parser. In</title>
<publisher>Cambridge Univ. Press.</publisher>
<marker>Crab, Steedman, </marker>
<rawString>Crab*, S. and Steedman, M. J. (forthcoming) On Not Being Led Up The Garden Path: The Use of Context by the Psychological Parser. In A. Zwicky, L. Kartunnen, and D. Dowty (eds) Natural Language Parsing: Psycholing-uistic, Theoretical, and Computational Perspectives, Cambridge: Cambridge Univ. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Gawron</author>
</authors>
<title>The GPSG Linguistics System,</title>
<date>1982</date>
<booktitle>Proceedings of the 20th annual meeting, Association for Computational Linguistics.</booktitle>
<marker>Gawron, 1982</marker>
<rawString>Gawron, J. M. et al (1982) The GPSG Linguistics System, Proceedings of the 20th annual meeting, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Isard</author>
</authors>
<title>What would you have done if...?</title>
<date>1974</date>
<journal>Theoretical Linguistics, Vol</journal>
<volume>1</volume>
<pages>233--256</pages>
<contexts>
<context position="23906" citStr="Isard 1974" startWordPosition="4059" endWordPosition="4060">r indicates, there is no parse tree built for a sentence at all. While it is true that in some sense trees are implicit in the sequence of operations of the parser, this is an inevitable consequence of the fact that the rules used themselves define trees, and as we shall see, even in this weak sense the tree structures implicit for certain types of recursive construction are not isomorphic to those which would be defined by the grammar. I like to think of this aspect of the operation of the parser as embodying the intuition often expressed (most often in the oral traditiou, though explicit in Isard 1974), that syntax is a &apos;control structure for semantics. It also has the merit of being consistent both with the widespread agreement among linguists that syntax plays a central role in language understanding, and with the apparently equally widespread failure of psycholinguists to find any evidence that purely syntactic representations are computed at any stage during normal comprehension. Turning now to C, the observation that sentences are understood on (at least) a word by word basis on a pass through from left to right, it should be clear that our procedure provides a direct model of this pro</context>
</contexts>
<marker>Isard, 1974</marker>
<rawString>Isard, S. (1974) What would you have done if...? Theoretical Linguistics, Vol 1, 233-256</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Katz</author>
</authors>
<title>Language and Other Abstract Objects Oxford:</title>
<date>1981</date>
<publisher>Basil Blackwell.</publisher>
<marker>Katz, 1981</marker>
<rawString>Katz, J. J. (1981) Language and Other Abstract Objects Oxford: Basil Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D T Laugendoen</author>
</authors>
<title>Finite State Parsing of Phrase Structure Languages and the Status of Readjustment Rules in Grammar.</title>
<date>1975</date>
<journal>Linguistic Inquiry</journal>
<volume>6</volume>
<pages>533--554</pages>
<marker>Laugendoen, 1975</marker>
<rawString>Laugendoen, D. T. (1975) Finite State Parsing of Phrase Structure Languages and the Status of Readjustment Rules in Grammar. Linguistic Inquiry 6, 533-554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D T Langeudoen</author>
<author>Y Langsam</author>
</authors>
<title>The Representation of Constituent Structures for Finite State Parsing in</title>
<date>1984</date>
<booktitle>Proceedings of Coling 84, Association for Computational Linguistics.</booktitle>
<marker>Langeudoen, Langsam, 1984</marker>
<rawString>Langeudoen, D. T. and Langsam, Y. (1984) The Representation of Constituent Structures for Finite State Parsing in Proceedings of Coling 84, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>S D Isard</author>
</authors>
<title>Free Recall of Self Embedded English Sentences.</title>
<date>1964</date>
<journal>Information and Control</journal>
<volume>7</volume>
<pages>292--303</pages>
<marker>Miller, Isard, 1964</marker>
<rawString>Miller, G. and Isard, S. D. (1964) Free Recall of Self Embedded English Sentences. Information and Control 7, 292-303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S G Pulman</author>
</authors>
<title>Generalised Phrase Structure Grammar, Earley&apos;s Algorithm, and the Minimisation of Recursion</title>
<date>1983</date>
<booktitle>in Sparck Jones and Wilks eds. Pulman, S. G. (forthcoming) Computational Models of Parsing in A. Ellis (ed) Progress in the Psychology of Language, Vol 2, Lawrence Erlbaum Associates Ltd.</booktitle>
<marker>Pulman, 1983</marker>
<rawString>Pulman, S. G. (1983) Generalised Phrase Structure Grammar, Earley&apos;s Algorithm, and the Minimisation of Recursion in Sparck Jones and Wilks eds. Pulman, S. G. (forthcoming) Computational Models of Parsing in A. Ellis (ed) Progress in the Psychology of Language, Vol 2, Lawrence Erlbaum Associates Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Rosenschein</author>
<author>S M Shieber</author>
</authors>
<title>Translating English Into Logical Form</title>
<date>1982</date>
<booktitle>Proceedings of the 20th annual meeting, Asssociation for Computational Linguistics.</booktitle>
<contexts>
<context position="4277" citStr="Rosenschein and Shieber 1982" startWordPosition="670" endWordPosition="673">principle which, in itself, is a computationally natural principle. Such a system might first parse a sentence, annotating nodes in the resulting tree with an indication of the syntactic rules used. This annotated tree would then be passed to an interpretation routine which applied the appropriate semantic operation to the topmost node (guided by the syntactic information found there, in particular a pointer to the semantic information necessary), calling itself recursively on each subtree to build up the complete interpretation. (Systems operating in more or less this manner are described in Rosenschein and Shieber 1982, Gawron et al. 1982 and Schubert and Pelletier 1982. They are not intended as psychological models in any but the most abstract sense, of course.) Such a system would, in observing B, also naturally be consistent with A. Obviously, though, this type of system requires a complete syntactic analysis to be available before it can even begiu the process of interpretation, thus conflicting straightforwardly with C. Consider next A and D. The structures which linguists postulate in accordance with A are often recursive, and it is in the nature of hierarchical structures that this should be a possib</context>
</contexts>
<marker>Rosenschein, Shieber, 1982</marker>
<rawString>Rosenschein, S. J. and Shieber, S. M. (1982) Translating English Into Logical Form Proceedings of the 20th annual meeting, Asssociation for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L K Schubert</author>
<author>F J Pelletier</author>
</authors>
<title>From English to Logic: Context Free Computation of &apos;Conventional&apos; Logical Translation,</title>
<date>1982</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>8</volume>
<pages>27--44</pages>
<contexts>
<context position="4329" citStr="Schubert and Pelletier 1982" startWordPosition="679" endWordPosition="682">ral principle. Such a system might first parse a sentence, annotating nodes in the resulting tree with an indication of the syntactic rules used. This annotated tree would then be passed to an interpretation routine which applied the appropriate semantic operation to the topmost node (guided by the syntactic information found there, in particular a pointer to the semantic information necessary), calling itself recursively on each subtree to build up the complete interpretation. (Systems operating in more or less this manner are described in Rosenschein and Shieber 1982, Gawron et al. 1982 and Schubert and Pelletier 1982. They are not intended as psychological models in any but the most abstract sense, of course.) Such a system would, in observing B, also naturally be consistent with A. Obviously, though, this type of system requires a complete syntactic analysis to be available before it can even begiu the process of interpretation, thus conflicting straightforwardly with C. Consider next A and D. The structures which linguists postulate in accordance with A are often recursive, and it is in the nature of hierarchical structures that this should be a possibility. This is rather puzzling in the light of D, fo</context>
</contexts>
<marker>Schubert, Pelletier, 1982</marker>
<rawString>Schubert, L. K. and Pelletier, F. J. (1982) From English to Logic: Context Free Computation of &apos;Conventional&apos; Logical Translation, American Journal of Computational Linguistics, 8, 27-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparck Jones</author>
<author>K</author>
<author>Y Wilks</author>
</authors>
<title>Automatic Natural Language Parsing,</title>
<date>1983</date>
<publisher>Ellis Horwood Ltd.</publisher>
<location>Chichester:</location>
<marker>Jones, K, Wilks, 1983</marker>
<rawString>Sparck Jones, K., and Wilks, Y. (eds) (1983) Automatic Natural Language Parsing, Chichester: Ellis Horwood Ltd.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>