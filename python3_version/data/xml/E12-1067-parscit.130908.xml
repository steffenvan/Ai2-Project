<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9987725">
Probabilistic Hierarchical Clustering of
Morphological Paradigms
</title>
<author confidence="0.996457">
Burcu Can
</author>
<affiliation confidence="0.997967">
Department of Computer Science
University of York
</affiliation>
<address confidence="0.936702">
Heslington, York, YO10 5GH, UK
</address>
<email confidence="0.998227">
burcucan@gmail.com
</email>
<author confidence="0.990169">
Suresh Manandhar
</author>
<affiliation confidence="0.9979595">
Department of Computer Science
University of York
</affiliation>
<address confidence="0.936217">
Heslington, York, YO10 5GH, UK
</address>
<email confidence="0.998078">
suresh@cs.york.ac.uk
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999736461538462">
We propose a novel method for learning
morphological paradigms that are struc-
tured within a hierarchy. The hierarchi-
cal structuring of paradigms groups mor-
phologically similar words close to each
other in a tree structure. This allows detect-
ing morphological similarities easily lead-
ing to improved morphological segmen-
tation. Our evaluation using (Kurimo et
al., 2011a; Kurimo et al., 2011b) dataset
shows that our method performs competi-
tively when compared with current state-of-
art systems.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999800754716981">
Unsupervised morphological segmentation of a
text involves learning rules for segmenting words
into their morphemes. Morphemes are the small-
est meaning bearing units of words. The learn-
ing process is fully unsupervised, using only raw
text as input to the learning system. For example,
the word respectively is split into morphemes re-
spect, ive and ly. Many fields, such as machine
translation, information retrieval, speech recog-
nition etc., require morphological segmentation
since new words are always created and storing
all the word forms will require a massive dictio-
nary. The task is even more complex, when mor-
phologically complicated languages (i.e. agglu-
tinative languages) are considered. The sparsity
problem is more severe for more morphologically
complex languages. Applying morphological seg-
mentation mitigates data sparsity by tackling the
issue with out-of-vocabulary (OOV) words.
In this paper, we propose a paradigmatic ap-
proach. A morphological paradigm is a pair
(StemList, SuffixList) such that each concatena-
tion of Stem+Suffix (where Stem E StemList and
Suffix E SuffixList) is a valid word form. The
learning of morphological paradigms is not novel
as there has already been existing work in this area
such as Goldsmith (2001), Snover et al. (2002),
Monson et al. (2009), Can and Manandhar (2009)
and Dreyer and Eisner (2011). However, none of
these existing approaches address learning of the
hierarchical structure of paradigms.
Hierarchical organisation of words help cap-
ture morphological similarities between words in
a compact structure by factoring these similarities
through stems, suffixes or prefixes. Our inference
algorithm simultaneously infers latent variables
(i.e. the morphemes) along with their hierarchical
organisation. Most hierarchical clustering algo-
rithms are single-pass, where once the hierarchi-
cal structure is built, the structure does not change
further.
The paper is structured as follows: section 2
gives the related work, section 3 describes the
probabilistic hierarchical clustering scheme, sec-
tion 4 explains the morphological segmenta-
tion model by embedding it into the clustering
scheme and describes the inference algorithm
along with how the morphological segmentation
is performed, section 5 presents the experiment
settings along with the evaluation scores, and fi-
nally section 6 presents a discussion with a com-
parison with other systems that participated in
Morpho Challenge 2009 and 2010 .
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999743">
We propose a Bayesian approach for learning of
paradigms in a hierarchy. If we ignore the hierar-
chical aspect of our learning algorithm, then our
</bodyText>
<page confidence="0.985786">
654
</page>
<note confidence="0.931514666666667">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 654–663,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
{walk, talk, quick}{0,ed,ing,ly, s}
</note>
<figure confidence="0.893359">
{walk, talk}{0,ed,ing,s}
{walk}{0,ing}
{talk}{ed,s}
{quick}{0,ly}
walk walking talked talks quick quickly
</figure>
<figureCaption confidence="0.999987">
Figure 1: A sample tree structure.
</figureCaption>
<bodyText confidence="0.99992775">
method is similar to the Dirichlet Process (DP)
based model of Goldwater et al. (2006). From
this perspective, our method can be understood
as adding a hierarchical structure learning layer
on top of the DP based learning method proposed
in Goldwater et al. (2006). Dreyer and Eisner
(2011) propose an infinite Diriclet mixture model
for capturing paradigms. However, they do not
address learning of hierarchy.
The method proposed in Chan (2006) also
learns within a hierarchical structure where La-
tent Dirichlet Allocation (LDA) is used to find
stem-suffix matrices. However, their work is su-
pervised, as true morphological analyses of words
are provided to the system. In contrast, our pro-
posed method is fully unsupervised.
</bodyText>
<sectionHeader confidence="0.997973" genericHeader="method">
3 Probabilistic Hierarchical Model
</sectionHeader>
<bodyText confidence="0.999641">
The hierarchical clustering proposed in this work
is different from existing hierarchical clustering
algorithms in two aspects:
</bodyText>
<listItem confidence="0.99824275">
• It is not single-pass as the hierarchical struc-
ture changes.
• It is probabilistic and is not dependent on a
distance metric.
</listItem>
<subsectionHeader confidence="0.995333">
3.1 Mathematical Definition
</subsectionHeader>
<bodyText confidence="0.995542333333333">
In this paper, a hierarchical structure is a binary
tree in which each internal node represents a clus-
ter.
Let a data set be D = {x1, x2, ... , xn} and
T be the entire tree, where each data point xi is
located at one of the leaf nodes (see Figure 2).
Here, Dk denotes the data points in the branch
Tk. Each node defines a probabilistic model for
words that the cluster acquires. The probabilistic
</bodyText>
<figureCaption confidence="0.895667">
Figure 2: A segment of a tree with with internal nodes
Di, Dj, Dk having data points {x1, x2, x3, x4}. The
subtree below the internal node Di is called Ti, the
subtree below the internal node Dj is Tj, and the sub-
tree below the internal node Dk is Tk.
</figureCaption>
<bodyText confidence="0.9912905">
model can be denoted as p(xi|0) where 0 denotes
the parameters of the probabilistic model.
The marginal probability of data in any node
can be calculated as:
</bodyText>
<equation confidence="0.99984">
�p(Dk) = p(Dk|e)p(e|)3)de (1)
</equation>
<bodyText confidence="0.9797995">
The likelihood of data under any subtree is de-
fined as follows:
</bodyText>
<equation confidence="0.999823">
p(Dk|Tk) = p(Dk)p(Dl|Tl)p(Dr|Tr) (2)
</equation>
<bodyText confidence="0.999985111111111">
where the probability is defined in terms of left Tl
and right Tr subtrees. Equation 2 provides a re-
cursive decomposition of the likelihood in terms
of the likelihood of the left and the right sub-
trees until the leaf nodes are reached. We use the
marginal probability (Equation 1) as prior infor-
mation since the marginal probability bears the
probability of having the data from the left and
right subtrees within a single cluster.
</bodyText>
<sectionHeader confidence="0.981157" genericHeader="method">
4 Morphological Segmentation
</sectionHeader>
<bodyText confidence="0.999897090909091">
In our model, data points are words to be clus-
tered and each cluster represents a paradigm. In
the hierarchical structure, words will be organised
in such a way that morphologically similar words
will be located close to each other to be grouped
in the same paradigms. Morphological similarity
refers to at least one common morpheme between
words. However, we do not make a distinction be-
tween morpheme types. Instead, we assume that
each word is organised as a stem+suffix combina-
tion.
</bodyText>
<subsectionHeader confidence="0.995229">
4.1 Model Definition
</subsectionHeader>
<bodyText confidence="0.999993">
Let a dataset D consist of words to be analysed,
where each word wi has a latent variable which is
</bodyText>
<equation confidence="0.97779875">
Dk
Di
DI
X1 X2 X3 X4
</equation>
<page confidence="0.992338">
655
</page>
<bodyText confidence="0.999784">
the split point that analyses the word into its stem
si and suffix mi:
</bodyText>
<equation confidence="0.993308">
D = {w1 = s1 + m1,...,wn = sn + mn}
</equation>
<bodyText confidence="0.8503085">
The marginal likelihood of words in the node k
is defined such that:
</bodyText>
<equation confidence="0.99931">
p(Dk) = p(Sk)p(Mk)
= p(s1, s2, ... , sn)p(m1, m2, ... , mn)
</equation>
<bodyText confidence="0.999805785714286">
The words in each cluster represents a
paradigm that consists of stems and suffixes. The
hierarchical model puts words sharing the same
stems or suffixes close to each other in the tree.
Each word is part of all the paradigms on the
path from the leaf node having that word to the
root. The word can share either its stem or suffix
with other words in the same paradigm. Hence,
a considerable number of words can be generated
through this approach that may not be seen in the
corpus.
We postulate that stems and suffixes are gen-
erated independently from each other. Thus, the
probability of a word becomes:
</bodyText>
<equation confidence="0.984155">
p(w = s + m) = p(s)p(m) (3)
</equation>
<bodyText confidence="0.9996925">
We define two Dirichlet processes to generate
stems and suffixes independently:
</bodyText>
<equation confidence="0.95389175">
Gs|βs, Ps — DP(βs, Ps)
Gm|βm, Pm — DP(βm, Pm)
s|Gs — Gs
m|Gm — Gm
</equation>
<bodyText confidence="0.9996270625">
where DP(βs, Ps) denotes a Dirichlet process
that generates stems. Here, βs is the concentration
parameter, which determines the number of stem
types generated by the Dirichlet process. The
smaller the value of the concentration parameter,
the less likely to generate new stem types the pro-
cess is. In contrast, the larger the value of concen-
tration parameter, the more likely it is to generate
new stem types, yielding a more uniform distribu-
tion over stem types. If βs &lt; 1, sparse stems are
supported, it yields a more skewed distribution.
To support a small number of stem types in each
cluster, we chose βs &lt; 1.
Here, Ps is the base distribution. We use the
base distribution as a prior probability distribu-
tion for morpheme lengths. We model morpheme
</bodyText>
<figureCaption confidence="0.944203166666667">
Figure 3: The plate diagram of the model, representing
the generation of a word wi from the stem si and the
suffix mi that are generated from Dirichlet processes.
In the representation, solid-boxes denote that the pro-
cess is repeated with the number given on the corner
of each box.
</figureCaption>
<bodyText confidence="0.500788">
lengths implicitly through the morpheme letters:
</bodyText>
<equation confidence="0.994291">
Ps(si) = � p(ci) (4)
ciEsi
</equation>
<bodyText confidence="0.97808619047619">
where ci denotes the letters, which are distributed
uniformly. Modelling morpheme letters is a way
of modelling the morpheme length since shorter
morphemes are favoured in order to have fewer
factors in Equation 4 (Creutz and Lagus, 2005b).
The Dirichlet process, DP(βm, Pm), is defined
for suffixes analogously. The graphical represen-
tation of the entire model is given in Figure 3.
Once the probability distributions G =
{Gs, Gm} are drawn from both Dirichlet pro-
cesses, words can be generated by drawing a stem
from Gs and a suffix from Gm. However, we do
not attempt to estimate the probability distribu-
tions G; instead, G is integrated out. The joint
probability of stems is calculated by integrating
out Gs:
where L denotes the number of stem tokens. The
joint probability distribution of stems can be tack-
led as a Chinese restaurant process. The Chi-
nese restaurant process introduces dependencies
between stems. Hence, the joint probability of
</bodyText>
<equation confidence="0.831732">
βs βm
Ps Gs Gm Pm
si mi
L N
wi
n
p(s1, s2, ... , sM)
L (5)
� �
= p(Gs) p(si|Gs)dGs
i=1
656
stems S = {s1, ... , sL} becomes:
p(s1, s2, ... , sL)
= p(s1)p(s2|s1) ... p(sM|s1, ... ,sM−1)
Γ(βs) βK−1
Γ(L + βs) s
</equation>
<bodyText confidence="0.987249416666667">
where K denotes the number of stem types. In
the equation, the second and the third factor corre-
spond to the case where novel stems are generated
for the first time; the last factor corresponds to the
case in which stems that have already been gener-
ated for nsi times previously are being generated
again. The first factor consists of all denominators
from both cases.
The integration process is applied for proba-
bility distributions Gm for suffixes analogously.
Hence, the joint probability of suffixes M =
{m1, ... , mN} becomes:
</bodyText>
<equation confidence="0.98491175">
p(m1, m2,... , mN)
= p(m1)p(m2|m1) ... p(mN|m1, ... , mN−1)
Γ(α) αT
Γ(N + α)
</equation>
<bodyText confidence="0.999973666666667">
where T denotes the number of suffix types and
nmi is the number of stem types mi which have
been already generated.
Following the joint probability distribution of
stems, the conditional probability of a stem given
previously generated stems can be derived as:
</bodyText>
<equation confidence="0.976778">
p(si|S−si, βs, Ps)
{ L 3+βs if si E S−si
Si−si
βs∗Ps(si) otherwise
L−1+βs
</equation>
<bodyText confidence="0.999752625">
where nS−si
si denotes the number of stem in-
stances si that have been previously generated,
where S−si denotes the stem set excluding the
new instance of the stem si.
The conditional probability of a suffix given the
other suffixes that have been previously generated
is defined similarly:
</bodyText>
<equation confidence="0.548407">
(9)
M−i
</equation>
<bodyText confidence="0.941419">
where nmi is the number of instances mi that
k
have been generated previously where M−mi is
</bodyText>
<figureCaption confidence="0.999359">
Figure 4: A portion of a sample tree.
</figureCaption>
<bodyText confidence="0.999042">
the set of suffixes, excluding the new instance of
the suffix mi.
A portion of a tree is given in Figure 4. As
can be seen on the figure, all words are lo-
cated at leaf nodes. Therefore, the root node
of this subtree consists of words {plugg+ed,
skew+ed, exclaim+ed, borrow+s, borrow+ed,
liken+s, liken+ed, consist+s, consist+ed}.
</bodyText>
<subsectionHeader confidence="0.780558">
4.2 Inference
</subsectionHeader>
<bodyText confidence="0.9999878">
The initial tree is constructed by randomly choos-
ing a word from the corpus and adding this into a
randomly chosen position in the tree. When con-
structing the initial tree, latent variables are also
assigned randomly, i.e. each word is split at a ran-
dom position (see Algorithm 1).
We use Metropolis Hastings algorithm (Hast-
ings, 1970), an instance of Markov Chain Monte
Carlo (MCMC) algorithms, to infer the optimal
hierarchical structure along with the morphologi-
cal segmentation of words (given in Algorithm 2).
During each iteration i, a leaf node Di = {wi =
si + mi} is drawn from the current tree structure.
The drawn leaf node is removed from the tree.
Next, a node Dk is drawn uniformly from the tree
</bodyText>
<figure confidence="0.866637076923077">
p(mi|M−mi,βm, Pm)
nM−mi
mi if mi E M−mi
N−1+β
{
m
βm∗Pm(mi) otherwise
N−1+βm
plugg+ed skew+ed
borrow+s borrow+ed
exclaim+ed
liken+s liken+ed
consist+s consist+ed
</figure>
<equation confidence="0.970189166666667">
K K
Ps(si) (nsi − 1)!
i=1 i=1
T T
Pm(mi) (nmi − 1)!
i=1 i=1
</equation>
<page confidence="0.986143">
657
</page>
<construct confidence="0.640105">
Algorithm 1 Creating initial tree.
</construct>
<listItem confidence="0.943264117647059">
1: input: data D = {w1 = s1 + m1, ... , wn =
sn + mn},
2: initialise: root ← D1 where
D1 = {w1 = s1 + m1}
3: initialise: c ← n − 1
4: while c &gt;= 1 do
5: Draw a word wj from the corpus.
6: Split the word randomly such that wj =
sj + mj
7: Create a new node Dj where Dj =
{wj = sj + mj}
8: Choose a sibling node Dk for Dj
9: Merge DneTAJ ← Dj ⊎ Dk
10: Remove wj from the corpus
11: c ← c − 1
12: end while
13: output: Initial tree
</listItem>
<equation confidence="0.9076925">
to make it a sibling node to Di. In addition to a
� �
</equation>
<bodyText confidence="0.9997307">
sibling node, a split point wi = si + mi is drawn
uniformly. Next, the node Di = {wi = s�i + m�i}
is inserted as a sibling node to Dk. After updating
all probabilities along the path to the root, the new
tree structure is either accepted or rejected by ap-
plying the Metropolis-Hastings update rule. The
likelihood of data under the given tree structure is
used as the sampling probability.
We use a simulated annealing schedule to up-
date PAcc:
</bodyText>
<equation confidence="0.9748405">
�pnext(D|T ) �
pcur(D|T)
</equation>
<bodyText confidence="0.999894">
where γ denotes the current temperature,
pnext(D|T) denotes the marginal likelihood
of the data under the new tree structure, and
pcur(D|T) denotes the marginal likelihood of
data under the latest accepted tree structure. If
(pnext(D|T) &gt; pcur(D|T)) then the update is
accepted (see line 9, Algorithm 2), otherwise, the
tree structure is still accepted with a probability
of pAcc (see line 14, Algorithm 2). In our
experiments (see section 5) we set γ to 2. The
system temperature is reduced in each iteration
of the Metropolis Hastings algorithm:
</bodyText>
<equation confidence="0.949909">
γ ← γ − η (11)
</equation>
<bodyText confidence="0.799998333333333">
Most tree structures are accepted in the earlier
stages of the algorithm, however, as the tempera-
Algorithm 2 Inference algorithm
</bodyText>
<listItem confidence="0.9944442">
1: input: data D = {w1 = s1 + m1, ... , wn =
sn + mn}, initial tree T, initial temperature
of the system γ, the target temperature of the
system κ, temperature decrement η
2: initialise: i ← 1, w ← wi = si + mi,
pcur(D|T) ← p(D|T)
3: while γ &gt; κ do
4: Remove the leaf node Di that has the
word wi = si + mi
5: Draw a split point for the word such that
</listItem>
<equation confidence="0.775569">
�
wi = si + m
</equation>
<listItem confidence="0.69271425">
6: Draw a sibling node Dj
7: Dryn ← Di ⊎ Dj
8: Update pnext(D|T)
9: if pnext(D|T) &gt;= pcur(D|T) then
10: Accept the new tree structure
11: pcur(D|T) ← pnext(D|T)
12: else
13: random ∼ Normal(0,1)
</listItem>
<equation confidence="0.444051">
(������D T ) )
</equation>
<listItem confidence="0.905867266666667">
14: if random &lt; �����D T )
15: Accept the new tree structure
16: pcur(D|T) ← pnext(D|T)
17: else
18: Reject the new tree structure
19: Re-insert the node Di at its pre-
vious position with the previous
split point
20: end if
21: end if
22: w ← wi+1 = si+1 + mi+1
23: γ ← γ − η
24: end while
25: output: A tree structure where each node
corresponds to a paradigm.
</listItem>
<bodyText confidence="0.999258846153846">
ture decreases only tree structures that lead lead to
a considerable improvement in the marginal prob-
ability p(D|T) are accepted.
An illustration of sampling a new tree structure
is given in Figure 5 and 6. Figure 5 shows that
D0 will be removed from the tree in order to sam-
ple a new position on the tree, along with a new
split point of the word. Once the leaf node is re-
moved from the tree, the parent node is removed
from the tree, as the parent node D5 will consist
of only one child. Figure 6 shows that D8 is sam-
pled to be the sibling node of D0. Subsequently,
the two nodes are merged within a new cluster that
</bodyText>
<figure confidence="0.902411384615385">
PAcc =
1
7
(10)
�
i
1
7 then
658
D5
D8
Ds
D7
</figure>
<equation confidence="0.915230666666667">
D0 D1 D2 D3 D4
p(sj|Sroot, βs, Ps) p(mj|Mroot, βm, Pm)
(13)
</equation>
<bodyText confidence="0.870736666666667">
where Sroot denotes all the stems in Droot and
Mroot denotes all the suffixes in Droot. Here
p(sj|Sroot, βs, Ps) is calculated as given below:
</bodyText>
<figure confidence="0.7410188">
p(si|Sroot, βs, Ps) =
nSroot
si
{
(14)
</figure>
<figureCaption confidence="0.888796">
Figure 5: D0 will be removed from the tree.
</figureCaption>
<figure confidence="0.98963825">
De
L +C3 if si E Sroot
βA(si)
L+βs otherwise
D7
D9
D8
D2 D3 D4 D0
</figure>
<figureCaption confidence="0.998206">
Figure 6: D8 is sampled to be the sibling of D0.
</figureCaption>
<bodyText confidence="0.921755">
introduces a new node D9.
</bodyText>
<subsectionHeader confidence="0.999088">
4.3 Morphological Segmentation
</subsectionHeader>
<bodyText confidence="0.998587777777778">
Once the optimal tree structure is inferred, along
with the morphological segmentation of words,
any novel word can be analysed. For the segmen-
tation of novel words, the root node is used as it
contains all stems and suffixes which are already
extracted from the training data. Morphological
segmentation is performed in two ways: segmen-
tation at a single point and segmentation at multi-
ple points.
</bodyText>
<subsectionHeader confidence="0.987224">
4.3.1 Single Split Point
</subsectionHeader>
<bodyText confidence="0.9999902">
In order to find single split point for the mor-
phological segmentation of a word, the split point
yielding the maximum probability given inferred
stems and suffixes is chosen to be the final analy-
sis of the word:
</bodyText>
<equation confidence="0.930004333333333">
arg max p(wi = sj + mj|Droot, βm, Pm, βs, Ps)
i
(12)
</equation>
<bodyText confidence="0.99829075">
where Droot refers to the root of the entire tree.
Here, the probability of a segmentation of a
given word given Droot is calculated as given be-
low:
</bodyText>
<equation confidence="0.698511666666667">
p(wi = sj + mj|Droot, βm, Pm, βs, Ps) =
Similarly, p(mj|Mroot, βm, Pm) is calculated
as:
p(mi|Mroot,βm, Pm) =
{ nMroot (15)
mi if mi E Mroot
N+βm
βm�Pm(mi) otherwise
N+βm
</equation>
<subsectionHeader confidence="0.955873">
4.3.2 Multiple Split Points
</subsectionHeader>
<bodyText confidence="0.9999598">
In order to discover words with multiple split
points, we propose a hierarchical segmentation
where each segment is split further. The rules for
generating multiple split points is given by the fol-
lowing context free grammar:
</bodyText>
<equation confidence="0.9997436">
w s1 m1|s2 m2 (16)
s1 s m|s s (17)
s2 s (18)
m1 m m (19)
m2 s m|m m (20)
</equation>
<bodyText confidence="0.997525266666667">
Here, s is a pre-terminal node that generates all
the stems from the root node. And similarly, m is
a pre-terminal node that generates all the suffixes
from the root node. First, using Equation 16, the
word (e.g. housekeeper) is split into s1 m1 (e.g.
housekeep+er) or s2 m2 (house+keeper). The first
segment is regarded as a stem, and the second
segment is either a stem or a suffix, consider-
ing the probability of having a compound word.
Equation 12 is used to decide whether the sec-
ond segment is a stem or a suffix. At the sec-
ond segmentation level, each segment is split once
more. If the first production rule is followed in
the first segmentation level, the first segment s1
can be analysed as s m (e.g. housekeep+O) or s s
</bodyText>
<page confidence="0.7029185">
D1
659
</page>
<figure confidence="0.694349279069767">
Marginal likelihood
-2.00E+006
-4.00E+006
-6.00E+006
-8.00E+006
-1.00E+007
-1.20E+007
-1.40E+007
-1.60E+007
-1.80E+007
0.00E+000
4
72
140
208
276
344
412
480
548
616
684
752
820
888
956
1024
1092
1160
1228
1296
1364
1432
1500
1568
1636
1704
16K
22K
house 0 keep er
house keeper
housekeeper
Iterations
</figure>
<figureCaption confidence="0.9998488">
Figure 7: An example that depicts how the word
housekeeper can be analysed further to find more split
points.
Figure 8: Marginal likelihood convergence for datasets
of size 16K and 22K words.
</figureCaption>
<bodyText confidence="0.840119666666667">
(e.g. house+keep) (Equation 17). The decision
to choose which production rule to apply is made
using:
</bodyText>
<equation confidence="0.931332333333333">
{ s s if p(s|S, βs, Ps) &gt; p(m|M, βm, Pm)
s1 ←
s m otherwise
</equation>
<bodyText confidence="0.994052681818182">
where S and M denote all the stems and suffixes
in the root node.
Following the same production rule, the second
segment m1 can only be analysed as m m (er+O).
We postulate that words cannot have more than
two stems and suffixes always follow stems. We
do not allow any prefixes, circumfixes, or infixes.
Therefore, the first production rule can output two
different analyses: s m m m and s s m m (e.g.
housekeep+er and house+keep+er).
On the other hand, if the word is analysed as
s2 m2 (e.g. house+keeper), then s2 cannot be
analysed further. (e.g. house). The second seg-
ment m2 can be analysed further, such that s m
(stem+suffix) (e.g. keep+er, keeper+O) or m m
(suffix+suffix). The decision to choose which pro-
duction rule to apply is made as follows:
{s m if p(s|S, βs, Ps) &gt; p(m|M, βm, Pm)
m m otherwise
Thus, the second production rule yields two
different analyses: s s m and s m m (e.g.
house+keep+er or house+keeper).
</bodyText>
<sectionHeader confidence="0.993238" genericHeader="method">
5 Experiments &amp; Results
</sectionHeader>
<bodyText confidence="0.995773113636364">
Two sets of experiments were performed for the
evaluation of the model. In the first set of exper-
iments, each word is split at single point giving a
single stem and a single suffix. In the second set
of experiments, potentially multiple split points
are generated, by splitting each stem and suffix
once more, if it is possible to do so.
Morpho Challenge (Kurimo et al., 2011b) pro-
vides a well established evaluation framework
that additionally allows comparing our model in
a range of languages. In both sets of experiments,
the Morpho Challenge 2010 dataset is used (Ku-
rimo et al., 2011b). Experiments are performed
for English, where the dataset consists of 878,034
words. Although the dataset provides word fre-
quencies, we have not used any frequency infor-
mation. However, for training our model, we only
chose words with frequency greater than 200.
�a�e
In our experiments, we used dataset sizes of
10K, 16K, 22K words. However, for final eval-
uation, we trained our models on 22K words. We
were unable to complete the experiments with
larger training datasets due to memory limita-
tions. We plan to report this in future work. Once
the tree is learned by the inference algorithm, the
final tree is used for the segmentation of the entire
dataset. Several experiments are performed for
each setting where the setting varies with the tree
size and the model parameters. Model parameters
are the concentration parameters β = 1β, βm}
of the Dirichlet processes. The concentration pa-
rameters, which are set for the experiments, are
0.1, 0.2, 0.02, 0.001, 0.002.
In all experiments, the initial temperature of the
system is assigned as γ = 2 and it is reduced to
the temperature γ = 0.01 with decrements η =
0.0001. Figure 8 shows how the log likelihoods of
trees of size 16K and 22K converge in time (where
the time axis refers to sampling iterations).
Since different training sets will lead to differ-
ent tree structures, each experiment is repeated
three times keeping the experiment setting the
same.
</bodyText>
<equation confidence="0.890728">
m2 ←
</equation>
<page confidence="0.978825">
660
</page>
<table confidence="0.99955075">
Data Size P(%) R(%) F(%) 0,1,01.
10K 81.48 33.03 47.01 0.1, 0.1
16K 86.48 35.13 50.02 0.002, 0.002
22K 89.04 36.01 51.28 0.002, 0.002
</table>
<tableCaption confidence="0.822465666666667">
Table 1: Highest evaluation scores of single split point
experiments obtained from the trees with 10K, 16K,
and 22K words.
</tableCaption>
<table confidence="0.999876">
Data Size P(%) R(%) F(%) 0,1, 01.
10K 62.45 57.62 59.98 0.1, 0.1
16K 67.80 57.72 62.36 0.002, 0.002
22K 68.71 62.56 62.56 0.001 0.001
</table>
<tableCaption confidence="0.981072">
Table 2: Evaluation scores of multiple split point ex-
periments obtained from the trees with 10K, 16K, and
22K words.
</tableCaption>
<subsectionHeader confidence="0.996174">
5.1 Experiments with Single Split Points
</subsectionHeader>
<bodyText confidence="0.999986142857143">
In the first set of experiments, words are split into
a single stem and suffix. During the segmentation,
Equation 12 is used to determine the split position
of each word. Evaluation scores are given in Ta-
ble 1. The highest F-measure obtained is 51.28%
with the dataset of 22K words. The scores are no-
ticeably higher with the largest training set.
</bodyText>
<subsectionHeader confidence="0.999668">
5.2 Experiments with Multiple Split Points
</subsectionHeader>
<bodyText confidence="0.999979375">
The evaluation scores of experiments with mul-
tiple split points are given in Table 2. The high-
est F-measure obtained is 62.56% with the dataset
with 22K words. As for single split points, the
scores are noticeably higher with the largest train-
ing set.
For both, single and multiple segmentation, the
same inferred tree has been used.
</bodyText>
<subsectionHeader confidence="0.999865">
5.3 Comparison with Other Systems
</subsectionHeader>
<bodyText confidence="0.999676214285714">
For all our evaluation experiments using Mor-
pho Challenge 2010 (English and Turkish) and
Morpho Challenge 2009 (English), we used 22k
words for training. For each evaluation, we ran-
domly chose 22k words for training and ran our
MCMC inference procedure to learn our model.
We generated 3 different models by choosing 3
different randomly generated training sets each
consisting of 22k words. The results are the best
results over these 3 models. We are reporting the
best results out of the 3 models due to the small
(22k word) datasets used. Use of larger datasets
would have resulted in less variation and better
results.
</bodyText>
<table confidence="0.9991369">
System P(%) R(%) F(%)
Allomorf1 68.98 56.82 62.31
Morf. Base.2 74.93 49.81 59.84
PM-Union3 55.68 62.33 58.82
Lignos4 83.49 45.00 58.48
Prob. Clustering (multiple) 57.08 57.58 57.33
PM-mimic3 53.13 59.01 55.91
MorphoNet5 65.08 47.82 55.13
Rali-cof6 68.32 46.45 55.30
CanMan7 58.52 44.82 50.76
</table>
<footnote confidence="0.619235571428571">
1 Virpioja et al. (2009)
2 Creutz and Lagus (2002)
3 Monson et al. (2009)
4 Lignos et al. (2009)
5 Bernhard (2009)
6 Lavall´ee and Langlais (2009)
7 Can and Manandhar (2009)
</footnote>
<tableCaption confidence="0.980640333333333">
Table 3: Comparison with other unsupervised systems
that participated in Morpho Challenge 2009 for En-
glish.
</tableCaption>
<bodyText confidence="0.99684">
We compare our system with the other partici-
pant systems in Morpho Challenge 2010. Results
are given in Table 6 (Virpioja et al., 2011). Since
the model is evaluated using the official (hidden)
Morpho Challenge 2010 evaluation dataset where
we submit our system for evaluation to the organ-
isers, the scores are different from the ones that
we presented Table 1 and Table 2.
We also demonstrate experiments with Morpho
Challenge 2009 English dataset. The dataset con-
sists of 384,904 words. Our results and the re-
sults of other participant systems in Morpho Chal-
lenge 2009 are given in Table 3 (Kurimo et al.,
2009). It should be noted that we only present
the top systems that participated in Morpho Chal-
lenge 2009. If all the systems are considered, our
system comes 5th out of 16 systems.
The problem of morphologically rich lan-
guages is not our priority within this research.
Nevertheless, we provide evaluation scores on
Turkish. The Turkish dataset consists of 617,298
words. We chose words with frequency greater
than 50 for Turkish since the Turkish dataset is not
large enough. The results for Turkish are given in
Table 4. Our system comes 3rd out of 7 systems.
</bodyText>
<sectionHeader confidence="0.999807" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.995489">
The model can easily capture common suffixes
such as -less, -s, -ed, -ment, etc. Some sample tree
nodes obtained from trees are given in Table 6.
</bodyText>
<page confidence="0.993377">
661
</page>
<table confidence="0.999939875">
System P(%) R(%) F(%)
Morf. CatMAP 79.38 31.88 45.49
Aggressive Comp. 55.51 34.36 42.45
Prob. Clustering (multiple) 72.36 25.81 38.04
Iterative Comp. 68.69 21.44 32.68
Nicolas 79.02 19.78 31.64
Morf. Base. 89.68 17.78 29.67
Base Inference 72.81 16.11 26.38
</table>
<tableCaption confidence="0.980851">
Table 4: Comparison with other unsupervised systems
that participated in Morpho Challenge 2010 for Turk-
ish.
</tableCaption>
<table confidence="0.990122941176471">
regard+less, base+less, shame+less, bound+less,
harm+less, regard+ed, relent+less
solve+d, high+-priced, lower+s, lower+-level,
high+-level, lower+-income, histor+ians
pre+mise, pre+face, pre+sumed, pre+, pre+gnant
base+ment, ail+ment, over+looked, predica+ment,
deploy+ment, compart+ment, embodi+ment
anti+-fraud, anti+-war, anti+-tank, anti+-nuclear,
anti+-terrorism, switzer+, anti+gua, switzer+land
sharp+ened, strength+s, tight+ened, strength+ened,
black+ened
inspir+e, inspir+ing, inspir+ed, inspir+es, earn+ing,
ponder+ing
downgrade+s, crash+ed, crash+ing, lack+ing,
blind+ing, blind+, crash+, compris+ing, com-
pris+es, stifl+ing, compris+ed, lack+s, assist+ing,
blind+ed, blind+er,
</table>
<tableCaption confidence="0.983342">
Table 5: Sample tree nodes obtained from various
trees.
</tableCaption>
<bodyText confidence="0.9999706">
As seen from the table, morphologically similar
words are grouped together. Morphological sim-
ilarity refers to at least one common morpheme
between words. For example, the words high-
priced and lower-level are grouped in the same
node through the word high-level which shares
the same stem with high-priced and the same end-
ing with lower-level.
As seen from the sample nodes, prefixes
can also be identified, for example anti+fraud,
anti+war, anti+tank, anti+nuclear. This illus-
trates the flexibility in the model by capturing the
similarities through either stems, suffixes or pre-
fixes. However, as mentioned above, the model
does not consider any discrimination between dif-
ferent types of morphological forms during train-
ing. As the prefix pre- appears at the beginning of
words, it is identified as a stem. However, identi-
fying pre- as a stem does not yield a change in the
morphological analysis of the word.
</bodyText>
<table confidence="0.957811">
System P(%) R(%) F(%)
Base Inference1 80.77 53.76 64.55
Iterative Comp.1 80.27 52.76 63.67
Aggressive Comp.1 71.45 52.31 60.40
Nicolas2 67.83 53.43 59.78
Prob. Clustering (multiple) 57.08 57.58 57.33
Morf. Baseline3 81.39 41.70 55.14
Prob. Clustering (single) 70.76 36.51 48.17
Morf. CatMAP4 86.84 30.03 44.63
1 Lignos (2010)
2 Nicolas et al. (2010)
3 Creutz and Lagus (2002)
4 Creutz and Lagus (2005a)
</table>
<tableCaption confidence="0.991257666666667">
Table 6: Comparison of our model with other unsuper-
vised systems that participated in Morpho Challenge
2010 for English.
</tableCaption>
<bodyText confidence="0.998511">
Sometimes similarities may not yield a valid
analysis of words. For example, the prefix pre-
leads the words pre+mise, pre+sumed, pre+gnant
to be analysed wrongly, whereas pre- is a valid
prefix for the word pre+face. Another nice fea-
ture about the model is that compounds are easily
captured through common stems: e.g. doubt+fire,
bon+fire, gun+fire, clear+cut.
</bodyText>
<sectionHeader confidence="0.994189" genericHeader="conclusions">
7 Conclusion &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999220363636364">
In this paper, we present a novel probabilis-
tic model for unsupervised morphology learn-
ing. The model adopts a hierarchical structure
in which words are organised in a tree so that
morphologically similar words are located close
to each other.
In hierarchical clustering, tree-cutting would be
a very useful thing to do but it is not addressed
in the current paper. We used just the root node
as a morpheme lexicon to apply segmentation.
Clearly, adding tree cutting would improve the ac-
curacy of the segmentation and will help us iden-
tify paradigms with higher accuracy. However,
the segmentation accuracy obtained without us-
ing tree cutting provides a very useful indicator
to show whether this approach is promising. And
experimental results show that this is indeed the
case.
In the current model, we did not use any syn-
tactic information, only words. POS tags can be
utilised to group words which are both morpho-
logically and syntactically similar.
</bodyText>
<page confidence="0.996983">
662
</page>
<sectionHeader confidence="0.995879" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999858745283019">
Delphine Bernhard. 2009. Morphonet: Exploring the
use of community structure for unsupervised mor-
pheme analysis. In Working Notes for the CLEF
2009 Workshop, September.
Burcu Can and Suresh Manandhar. 2009. Cluster-
ing morphological paradigms using syntactic cate-
gories. In Working Notes for the CLEF 2009 Work-
shop, September.
Erwin Chan. 2006. Learning probabilistic paradigms
for morphology in a latent class model. In Proceed-
ings of the Eighth Meeting of the ACL Special Inter-
est Group on Computational Phonology and Mor-
phology, SIGPHON ’06, pages 69–78, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Mathias Creutz and Krista Lagus. 2002. Unsu-
pervised discovery of morphemes. In Proceed-
ings of the ACL-02 workshop on Morphological
and phonological learning - Volume 6, MPL ’02,
pages 21–30, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mathias Creutz and Krista Lagus. 2005a. Induc-
ing the morphological lexicon of a natural language
from unannotated text. In In Proceedings of the
International and Interdisciplinary Conference on
Adaptive Knowledge Representation and Reasoning
(AKRR 2005, pages 106–113.
Mathias Creutz and Krista Lagus. 2005b. Unsu-
pervised morpheme segmentation and morphology
induction from text corpora using morfessor 1.0.
Technical Report A81.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using
a dirichlet process mixture model. In Proceedings
of the 2011 Conference on Empirical Methods in
Natural Language Processing, pages 616–627, Ed-
inburgh, Scotland, UK., July. Association for Com-
putational Linguistics.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153–198.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Interpolating between types and to-
kens by estimating power-law generators. In In Ad-
vances in Neural Information Processing Systems
18, page 18.
W. K. Hastings. 1970. Monte carlo sampling meth-
ods using markov chains and their applications.
Biometrika, 57:97–109.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of morpho challenge 2009.
In Proceedings of the 10th cross-language eval-
uation forum conference on Multilingual infor-
mation access evaluation: text retrieval experi-
ments, CLEF’09, pages 578–597, Berlin, Heidel-
berg. Springer-Verlag.
Mikko Kurimo, Krista Lagus, Sami Virpioja, and
Ville Turunen. 2011a. Morpho challenge
2009. http://research.ics.tkk.fi/
events/morphochallenge2009/, June.
Mikko Kurimo, Krista Lagus, Sami Virpioja, and
Ville Turunen. 2011b. Morpho challenge
2010. http://research.ics.tkk.fi/
events/morphochallenge2010/, June.
Jean Franc¸ois Lavall´ee and Philippe Langlais. 2009.
Morphological acquisition by formal analogy. In
Working Notes for the CLEF 2009 Workshop,
September.
Constantine Lignos, Erwin Chan, Mitchell P. Marcus,
and Charles Yang. 2009. A rule-based unsuper-
vised morphology learning framework. In Working
Notes for the CLEF 2009 Workshop, September.
Constantine Lignos. 2010. Learning from unseen
data. In Mikko Kurimo, Sami Virpioja, Ville Tu-
runen, and Krista Lagus, editors, Proceedings of the
Morpho Challenge 2010 Workshop, pages 35–38,
Aalto University, Espoo, Finland.
Christian Monson, Kristy Hollingshead, and Brian
Roark. 2009. Probabilistic paramor. In Pro-
ceedings of the 10th cross-language evaluation fo-
rum conference on Multilingual information access
evaluation: text retrieval experiments, CLEF’09,
September.
Lionel Nicolas, Jacques Farr´e, and Miguel A. Mo-
linero. 2010. Unsupervised learning of concate-
native morphology based on frequency-related form
occurrence. In Mikko Kurimo, Sami Virpioja, Ville
Turunen, and Krista Lagus, editors, Proceedings of
the Morpho Challenge 2010 Workshop, pages 39–
43, Aalto University, Espoo, Finland.
Matthew G. Snover, Gaja E. Jarosz, and Michael R.
Brent. 2002. Unsupervised learning of morphol-
ogy using a novel directed search algorithm: Taking
the first step. In Proceedings of the ACL-02 Work-
shop on Morphological and Phonological Learn-
ing, pages 11–20, Morristown, NJ, USA. ACL.
Sami Virpioja, Oskar Kohonen, and Krista Lagus.
2009. Unsupervised morpheme discovery with al-
lomorfessor. In Working Notes for the CLEF 2009
Workshop. September.
Sami Virpioja, Ville T. Turunen, Sebastian Spiegler,
Oskar Kohonen, and Mikko Kurimo. 2011. Em-
pirical comparison of evaluation methods for unsu-
pervised learning of morphology. In TraitementAu-
tomatique des Langues.
</reference>
<page confidence="0.99889">
663
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.553630">
<title confidence="0.9917">Probabilistic Hierarchical Clustering Morphological Paradigms</title>
<author confidence="0.907274">Burcu</author>
<affiliation confidence="0.999664">Department of Computer University of</affiliation>
<address confidence="0.884012">Heslington, York, YO10 5GH,</address>
<email confidence="0.999719">burcucan@gmail.com</email>
<author confidence="0.742031">Suresh</author>
<affiliation confidence="0.9996885">Department of Computer University of</affiliation>
<address confidence="0.961849">Heslington, York, YO10 5GH,</address>
<email confidence="0.999146">suresh@cs.york.ac.uk</email>
<abstract confidence="0.998432214285714">We propose a novel method for learning morphological paradigms that are structured within a hierarchy. The hierarchical structuring of paradigms groups morphologically similar words close to each other in a tree structure. This allows detecting morphological similarities easily leading to improved morphological segmentation. Our evaluation using (Kurimo et al., 2011a; Kurimo et al., 2011b) dataset shows that our method performs competitively when compared with current state-ofart systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Delphine Bernhard</author>
</authors>
<title>Morphonet: Exploring the use of community structure for unsupervised morpheme analysis.</title>
<date>2009</date>
<booktitle>In Working Notes for the CLEF 2009 Workshop,</booktitle>
<contexts>
<context position="24612" citStr="Bernhard (2009)" startWordPosition="4302" endWordPosition="4303">t results over these 3 models. We are reporting the best results out of the 3 models due to the small (22k word) datasets used. Use of larger datasets would have resulted in less variation and better results. System P(%) R(%) F(%) Allomorf1 68.98 56.82 62.31 Morf. Base.2 74.93 49.81 59.84 PM-Union3 55.68 62.33 58.82 Lignos4 83.49 45.00 58.48 Prob. Clustering (multiple) 57.08 57.58 57.33 PM-mimic3 53.13 59.01 55.91 MorphoNet5 65.08 47.82 55.13 Rali-cof6 68.32 46.45 55.30 CanMan7 58.52 44.82 50.76 1 Virpioja et al. (2009) 2 Creutz and Lagus (2002) 3 Monson et al. (2009) 4 Lignos et al. (2009) 5 Bernhard (2009) 6 Lavall´ee and Langlais (2009) 7 Can and Manandhar (2009) Table 3: Comparison with other unsupervised systems that participated in Morpho Challenge 2009 for English. We compare our system with the other participant systems in Morpho Challenge 2010. Results are given in Table 6 (Virpioja et al., 2011). Since the model is evaluated using the official (hidden) Morpho Challenge 2010 evaluation dataset where we submit our system for evaluation to the organisers, the scores are different from the ones that we presented Table 1 and Table 2. We also demonstrate experiments with Morpho Challenge 2009</context>
</contexts>
<marker>Bernhard, 2009</marker>
<rawString>Delphine Bernhard. 2009. Morphonet: Exploring the use of community structure for unsupervised morpheme analysis. In Working Notes for the CLEF 2009 Workshop, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burcu Can</author>
<author>Suresh Manandhar</author>
</authors>
<title>Clustering morphological paradigms using syntactic categories.</title>
<date>2009</date>
<booktitle>In Working Notes for the CLEF 2009 Workshop,</booktitle>
<contexts>
<context position="2133" citStr="Can and Manandhar (2009)" startWordPosition="313" endWordPosition="316">d. The sparsity problem is more severe for more morphologically complex languages. Applying morphological segmentation mitigates data sparsity by tackling the issue with out-of-vocabulary (OOV) words. In this paper, we propose a paradigmatic approach. A morphological paradigm is a pair (StemList, SuffixList) such that each concatenation of Stem+Suffix (where Stem E StemList and Suffix E SuffixList) is a valid word form. The learning of morphological paradigms is not novel as there has already been existing work in this area such as Goldsmith (2001), Snover et al. (2002), Monson et al. (2009), Can and Manandhar (2009) and Dreyer and Eisner (2011). However, none of these existing approaches address learning of the hierarchical structure of paradigms. Hierarchical organisation of words help capture morphological similarities between words in a compact structure by factoring these similarities through stems, suffixes or prefixes. Our inference algorithm simultaneously infers latent variables (i.e. the morphemes) along with their hierarchical organisation. Most hierarchical clustering algorithms are single-pass, where once the hierarchical structure is built, the structure does not change further. The paper is</context>
<context position="24671" citStr="Can and Manandhar (2009)" startWordPosition="4310" endWordPosition="4313">e best results out of the 3 models due to the small (22k word) datasets used. Use of larger datasets would have resulted in less variation and better results. System P(%) R(%) F(%) Allomorf1 68.98 56.82 62.31 Morf. Base.2 74.93 49.81 59.84 PM-Union3 55.68 62.33 58.82 Lignos4 83.49 45.00 58.48 Prob. Clustering (multiple) 57.08 57.58 57.33 PM-mimic3 53.13 59.01 55.91 MorphoNet5 65.08 47.82 55.13 Rali-cof6 68.32 46.45 55.30 CanMan7 58.52 44.82 50.76 1 Virpioja et al. (2009) 2 Creutz and Lagus (2002) 3 Monson et al. (2009) 4 Lignos et al. (2009) 5 Bernhard (2009) 6 Lavall´ee and Langlais (2009) 7 Can and Manandhar (2009) Table 3: Comparison with other unsupervised systems that participated in Morpho Challenge 2009 for English. We compare our system with the other participant systems in Morpho Challenge 2010. Results are given in Table 6 (Virpioja et al., 2011). Since the model is evaluated using the official (hidden) Morpho Challenge 2010 evaluation dataset where we submit our system for evaluation to the organisers, the scores are different from the ones that we presented Table 1 and Table 2. We also demonstrate experiments with Morpho Challenge 2009 English dataset. The dataset consists of 384,904 words. Ou</context>
</contexts>
<marker>Can, Manandhar, 2009</marker>
<rawString>Burcu Can and Suresh Manandhar. 2009. Clustering morphological paradigms using syntactic categories. In Working Notes for the CLEF 2009 Workshop, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erwin Chan</author>
</authors>
<title>Learning probabilistic paradigms for morphology in a latent class model.</title>
<date>2006</date>
<booktitle>In Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology and Morphology, SIGPHON ’06,</booktitle>
<pages>69--78</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4267" citStr="Chan (2006)" startWordPosition="628" endWordPosition="629">, quick}{0,ed,ing,ly, s} {walk, talk}{0,ed,ing,s} {walk}{0,ing} {talk}{ed,s} {quick}{0,ly} walk walking talked talks quick quickly Figure 1: A sample tree structure. method is similar to the Dirichlet Process (DP) based model of Goldwater et al. (2006). From this perspective, our method can be understood as adding a hierarchical structure learning layer on top of the DP based learning method proposed in Goldwater et al. (2006). Dreyer and Eisner (2011) propose an infinite Diriclet mixture model for capturing paradigms. However, they do not address learning of hierarchy. The method proposed in Chan (2006) also learns within a hierarchical structure where Latent Dirichlet Allocation (LDA) is used to find stem-suffix matrices. However, their work is supervised, as true morphological analyses of words are provided to the system. In contrast, our proposed method is fully unsupervised. 3 Probabilistic Hierarchical Model The hierarchical clustering proposed in this work is different from existing hierarchical clustering algorithms in two aspects: • It is not single-pass as the hierarchical structure changes. • It is probabilistic and is not dependent on a distance metric. 3.1 Mathematical Definition</context>
</contexts>
<marker>Chan, 2006</marker>
<rawString>Erwin Chan. 2006. Learning probabilistic paradigms for morphology in a latent class model. In Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology and Morphology, SIGPHON ’06, pages 69–78, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised discovery of morphemes.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Morphological and phonological learning - Volume 6, MPL ’02,</booktitle>
<pages>21--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="24548" citStr="Creutz and Lagus (2002)" startWordPosition="4287" endWordPosition="4290">ated training sets each consisting of 22k words. The results are the best results over these 3 models. We are reporting the best results out of the 3 models due to the small (22k word) datasets used. Use of larger datasets would have resulted in less variation and better results. System P(%) R(%) F(%) Allomorf1 68.98 56.82 62.31 Morf. Base.2 74.93 49.81 59.84 PM-Union3 55.68 62.33 58.82 Lignos4 83.49 45.00 58.48 Prob. Clustering (multiple) 57.08 57.58 57.33 PM-mimic3 53.13 59.01 55.91 MorphoNet5 65.08 47.82 55.13 Rali-cof6 68.32 46.45 55.30 CanMan7 58.52 44.82 50.76 1 Virpioja et al. (2009) 2 Creutz and Lagus (2002) 3 Monson et al. (2009) 4 Lignos et al. (2009) 5 Bernhard (2009) 6 Lavall´ee and Langlais (2009) 7 Can and Manandhar (2009) Table 3: Comparison with other unsupervised systems that participated in Morpho Challenge 2009 for English. We compare our system with the other participant systems in Morpho Challenge 2010. Results are given in Table 6 (Virpioja et al., 2011). Since the model is evaluated using the official (hidden) Morpho Challenge 2010 evaluation dataset where we submit our system for evaluation to the organisers, the scores are different from the ones that we presented Table 1 and Tab</context>
<context position="28510" citStr="Creutz and Lagus (2002)" startWordPosition="4885" endWordPosition="4888">etween different types of morphological forms during training. As the prefix pre- appears at the beginning of words, it is identified as a stem. However, identifying pre- as a stem does not yield a change in the morphological analysis of the word. System P(%) R(%) F(%) Base Inference1 80.77 53.76 64.55 Iterative Comp.1 80.27 52.76 63.67 Aggressive Comp.1 71.45 52.31 60.40 Nicolas2 67.83 53.43 59.78 Prob. Clustering (multiple) 57.08 57.58 57.33 Morf. Baseline3 81.39 41.70 55.14 Prob. Clustering (single) 70.76 36.51 48.17 Morf. CatMAP4 86.84 30.03 44.63 1 Lignos (2010) 2 Nicolas et al. (2010) 3 Creutz and Lagus (2002) 4 Creutz and Lagus (2005a) Table 6: Comparison of our model with other unsupervised systems that participated in Morpho Challenge 2010 for English. Sometimes similarities may not yield a valid analysis of words. For example, the prefix preleads the words pre+mise, pre+sumed, pre+gnant to be analysed wrongly, whereas pre- is a valid prefix for the word pre+face. Another nice feature about the model is that compounds are easily captured through common stems: e.g. doubt+fire, bon+fire, gun+fire, clear+cut. 7 Conclusion &amp; Future Work In this paper, we present a novel probabilistic model for unsup</context>
</contexts>
<marker>Creutz, Lagus, 2002</marker>
<rawString>Mathias Creutz and Krista Lagus. 2002. Unsupervised discovery of morphemes. In Proceedings of the ACL-02 workshop on Morphological and phonological learning - Volume 6, MPL ’02, pages 21–30, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Inducing the morphological lexicon of a natural language from unannotated text. In</title>
<date>2005</date>
<booktitle>In Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR</booktitle>
<pages>106--113</pages>
<contexts>
<context position="9268" citStr="Creutz and Lagus, 2005" startWordPosition="1508" endWordPosition="1511">engths. We model morpheme Figure 3: The plate diagram of the model, representing the generation of a word wi from the stem si and the suffix mi that are generated from Dirichlet processes. In the representation, solid-boxes denote that the process is repeated with the number given on the corner of each box. lengths implicitly through the morpheme letters: Ps(si) = � p(ci) (4) ciEsi where ci denotes the letters, which are distributed uniformly. Modelling morpheme letters is a way of modelling the morpheme length since shorter morphemes are favoured in order to have fewer factors in Equation 4 (Creutz and Lagus, 2005b). The Dirichlet process, DP(βm, Pm), is defined for suffixes analogously. The graphical representation of the entire model is given in Figure 3. Once the probability distributions G = {Gs, Gm} are drawn from both Dirichlet processes, words can be generated by drawing a stem from Gs and a suffix from Gm. However, we do not attempt to estimate the probability distributions G; instead, G is integrated out. The joint probability of stems is calculated by integrating out Gs: where L denotes the number of stem tokens. The joint probability distribution of stems can be tackled as a Chinese restaura</context>
<context position="28535" citStr="Creutz and Lagus (2005" startWordPosition="4890" endWordPosition="4893">morphological forms during training. As the prefix pre- appears at the beginning of words, it is identified as a stem. However, identifying pre- as a stem does not yield a change in the morphological analysis of the word. System P(%) R(%) F(%) Base Inference1 80.77 53.76 64.55 Iterative Comp.1 80.27 52.76 63.67 Aggressive Comp.1 71.45 52.31 60.40 Nicolas2 67.83 53.43 59.78 Prob. Clustering (multiple) 57.08 57.58 57.33 Morf. Baseline3 81.39 41.70 55.14 Prob. Clustering (single) 70.76 36.51 48.17 Morf. CatMAP4 86.84 30.03 44.63 1 Lignos (2010) 2 Nicolas et al. (2010) 3 Creutz and Lagus (2002) 4 Creutz and Lagus (2005a) Table 6: Comparison of our model with other unsupervised systems that participated in Morpho Challenge 2010 for English. Sometimes similarities may not yield a valid analysis of words. For example, the prefix preleads the words pre+mise, pre+sumed, pre+gnant to be analysed wrongly, whereas pre- is a valid prefix for the word pre+face. Another nice feature about the model is that compounds are easily captured through common stems: e.g. doubt+fire, bon+fire, gun+fire, clear+cut. 7 Conclusion &amp; Future Work In this paper, we present a novel probabilistic model for unsupervised morphology learni</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>Mathias Creutz and Krista Lagus. 2005a. Inducing the morphological lexicon of a natural language from unannotated text. In In Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR 2005, pages 106–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised morpheme segmentation and morphology induction from text corpora using morfessor 1.0.</title>
<date>2005</date>
<tech>Technical Report A81.</tech>
<contexts>
<context position="9268" citStr="Creutz and Lagus, 2005" startWordPosition="1508" endWordPosition="1511">engths. We model morpheme Figure 3: The plate diagram of the model, representing the generation of a word wi from the stem si and the suffix mi that are generated from Dirichlet processes. In the representation, solid-boxes denote that the process is repeated with the number given on the corner of each box. lengths implicitly through the morpheme letters: Ps(si) = � p(ci) (4) ciEsi where ci denotes the letters, which are distributed uniformly. Modelling morpheme letters is a way of modelling the morpheme length since shorter morphemes are favoured in order to have fewer factors in Equation 4 (Creutz and Lagus, 2005b). The Dirichlet process, DP(βm, Pm), is defined for suffixes analogously. The graphical representation of the entire model is given in Figure 3. Once the probability distributions G = {Gs, Gm} are drawn from both Dirichlet processes, words can be generated by drawing a stem from Gs and a suffix from Gm. However, we do not attempt to estimate the probability distributions G; instead, G is integrated out. The joint probability of stems is calculated by integrating out Gs: where L denotes the number of stem tokens. The joint probability distribution of stems can be tackled as a Chinese restaura</context>
<context position="28535" citStr="Creutz and Lagus (2005" startWordPosition="4890" endWordPosition="4893">morphological forms during training. As the prefix pre- appears at the beginning of words, it is identified as a stem. However, identifying pre- as a stem does not yield a change in the morphological analysis of the word. System P(%) R(%) F(%) Base Inference1 80.77 53.76 64.55 Iterative Comp.1 80.27 52.76 63.67 Aggressive Comp.1 71.45 52.31 60.40 Nicolas2 67.83 53.43 59.78 Prob. Clustering (multiple) 57.08 57.58 57.33 Morf. Baseline3 81.39 41.70 55.14 Prob. Clustering (single) 70.76 36.51 48.17 Morf. CatMAP4 86.84 30.03 44.63 1 Lignos (2010) 2 Nicolas et al. (2010) 3 Creutz and Lagus (2002) 4 Creutz and Lagus (2005a) Table 6: Comparison of our model with other unsupervised systems that participated in Morpho Challenge 2010 for English. Sometimes similarities may not yield a valid analysis of words. For example, the prefix preleads the words pre+mise, pre+sumed, pre+gnant to be analysed wrongly, whereas pre- is a valid prefix for the word pre+face. Another nice feature about the model is that compounds are easily captured through common stems: e.g. doubt+fire, bon+fire, gun+fire, clear+cut. 7 Conclusion &amp; Future Work In this paper, we present a novel probabilistic model for unsupervised morphology learni</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>Mathias Creutz and Krista Lagus. 2005b. Unsupervised morpheme segmentation and morphology induction from text corpora using morfessor 1.0. Technical Report A81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Discovering morphological paradigms from plain text using a dirichlet process mixture model.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>616--627</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="2162" citStr="Dreyer and Eisner (2011)" startWordPosition="318" endWordPosition="321">re severe for more morphologically complex languages. Applying morphological segmentation mitigates data sparsity by tackling the issue with out-of-vocabulary (OOV) words. In this paper, we propose a paradigmatic approach. A morphological paradigm is a pair (StemList, SuffixList) such that each concatenation of Stem+Suffix (where Stem E StemList and Suffix E SuffixList) is a valid word form. The learning of morphological paradigms is not novel as there has already been existing work in this area such as Goldsmith (2001), Snover et al. (2002), Monson et al. (2009), Can and Manandhar (2009) and Dreyer and Eisner (2011). However, none of these existing approaches address learning of the hierarchical structure of paradigms. Hierarchical organisation of words help capture morphological similarities between words in a compact structure by factoring these similarities through stems, suffixes or prefixes. Our inference algorithm simultaneously infers latent variables (i.e. the morphemes) along with their hierarchical organisation. Most hierarchical clustering algorithms are single-pass, where once the hierarchical structure is built, the structure does not change further. The paper is structured as follows: secti</context>
<context position="4112" citStr="Dreyer and Eisner (2011)" startWordPosition="603" endWordPosition="606">apter of the Association for Computational Linguistics, pages 654–663, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics {walk, talk, quick}{0,ed,ing,ly, s} {walk, talk}{0,ed,ing,s} {walk}{0,ing} {talk}{ed,s} {quick}{0,ly} walk walking talked talks quick quickly Figure 1: A sample tree structure. method is similar to the Dirichlet Process (DP) based model of Goldwater et al. (2006). From this perspective, our method can be understood as adding a hierarchical structure learning layer on top of the DP based learning method proposed in Goldwater et al. (2006). Dreyer and Eisner (2011) propose an infinite Diriclet mixture model for capturing paradigms. However, they do not address learning of hierarchy. The method proposed in Chan (2006) also learns within a hierarchical structure where Latent Dirichlet Allocation (LDA) is used to find stem-suffix matrices. However, their work is supervised, as true morphological analyses of words are provided to the system. In contrast, our proposed method is fully unsupervised. 3 Probabilistic Hierarchical Model The hierarchical clustering proposed in this work is different from existing hierarchical clustering algorithms in two aspects: </context>
</contexts>
<marker>Dreyer, Eisner, 2011</marker>
<rawString>Markus Dreyer and Jason Eisner. 2011. Discovering morphological paradigms from plain text using a dirichlet process mixture model. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 616–627, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="2063" citStr="Goldsmith (2001)" startWordPosition="303" endWordPosition="304">licated languages (i.e. agglutinative languages) are considered. The sparsity problem is more severe for more morphologically complex languages. Applying morphological segmentation mitigates data sparsity by tackling the issue with out-of-vocabulary (OOV) words. In this paper, we propose a paradigmatic approach. A morphological paradigm is a pair (StemList, SuffixList) such that each concatenation of Stem+Suffix (where Stem E StemList and Suffix E SuffixList) is a valid word form. The learning of morphological paradigms is not novel as there has already been existing work in this area such as Goldsmith (2001), Snover et al. (2002), Monson et al. (2009), Can and Manandhar (2009) and Dreyer and Eisner (2011). However, none of these existing approaches address learning of the hierarchical structure of paradigms. Hierarchical organisation of words help capture morphological similarities between words in a compact structure by factoring these similarities through stems, suffixes or prefixes. Our inference algorithm simultaneously infers latent variables (i.e. the morphemes) along with their hierarchical organisation. Most hierarchical clustering algorithms are single-pass, where once the hierarchical s</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators. In</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 18,</booktitle>
<pages>18</pages>
<contexts>
<context position="3908" citStr="Goldwater et al. (2006)" startWordPosition="570" endWordPosition="573">We propose a Bayesian approach for learning of paradigms in a hierarchy. If we ignore the hierarchical aspect of our learning algorithm, then our 654 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 654–663, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics {walk, talk, quick}{0,ed,ing,ly, s} {walk, talk}{0,ed,ing,s} {walk}{0,ing} {talk}{ed,s} {quick}{0,ly} walk walking talked talks quick quickly Figure 1: A sample tree structure. method is similar to the Dirichlet Process (DP) based model of Goldwater et al. (2006). From this perspective, our method can be understood as adding a hierarchical structure learning layer on top of the DP based learning method proposed in Goldwater et al. (2006). Dreyer and Eisner (2011) propose an infinite Diriclet mixture model for capturing paradigms. However, they do not address learning of hierarchy. The method proposed in Chan (2006) also learns within a hierarchical structure where Latent Dirichlet Allocation (LDA) is used to find stem-suffix matrices. However, their work is supervised, as true morphological analyses of words are provided to the system. In contrast, ou</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. In In Advances in Neural Information Processing Systems 18, page 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W K Hastings</author>
</authors>
<title>Monte carlo sampling methods using markov chains and their applications.</title>
<date>1970</date>
<journal>Biometrika,</journal>
<pages>57--97</pages>
<contexts>
<context position="12242" citStr="Hastings, 1970" startWordPosition="2033" endWordPosition="2035">. A portion of a tree is given in Figure 4. As can be seen on the figure, all words are located at leaf nodes. Therefore, the root node of this subtree consists of words {plugg+ed, skew+ed, exclaim+ed, borrow+s, borrow+ed, liken+s, liken+ed, consist+s, consist+ed}. 4.2 Inference The initial tree is constructed by randomly choosing a word from the corpus and adding this into a randomly chosen position in the tree. When constructing the initial tree, latent variables are also assigned randomly, i.e. each word is split at a random position (see Algorithm 1). We use Metropolis Hastings algorithm (Hastings, 1970), an instance of Markov Chain Monte Carlo (MCMC) algorithms, to infer the optimal hierarchical structure along with the morphological segmentation of words (given in Algorithm 2). During each iteration i, a leaf node Di = {wi = si + mi} is drawn from the current tree structure. The drawn leaf node is removed from the tree. Next, a node Dk is drawn uniformly from the tree p(mi|M−mi,βm, Pm) nM−mi mi if mi E M−mi N−1+β { m βm∗Pm(mi) otherwise N−1+βm plugg+ed skew+ed borrow+s borrow+ed exclaim+ed liken+s liken+ed consist+s consist+ed K K Ps(si) (nsi − 1)! i=1 i=1 T T Pm(mi) (nmi − 1)! i=1 i=1 657 </context>
</contexts>
<marker>Hastings, 1970</marker>
<rawString>W. K. Hastings. 1970. Monte carlo sampling methods using markov chains and their applications. Biometrika, 57:97–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Sami Virpioja</author>
<author>Ville T Turunen</author>
<author>Graeme W Blackwood</author>
<author>William Byrne</author>
</authors>
<title>Overview and results of morpho challenge</title>
<date>2009</date>
<booktitle>In Proceedings of the 10th cross-language evaluation forum conference on Multilingual information access evaluation: text retrieval experiments, CLEF’09,</booktitle>
<pages>578--597</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="25393" citStr="Kurimo et al., 2009" startWordPosition="4431" endWordPosition="4434">glish. We compare our system with the other participant systems in Morpho Challenge 2010. Results are given in Table 6 (Virpioja et al., 2011). Since the model is evaluated using the official (hidden) Morpho Challenge 2010 evaluation dataset where we submit our system for evaluation to the organisers, the scores are different from the ones that we presented Table 1 and Table 2. We also demonstrate experiments with Morpho Challenge 2009 English dataset. The dataset consists of 384,904 words. Our results and the results of other participant systems in Morpho Challenge 2009 are given in Table 3 (Kurimo et al., 2009). It should be noted that we only present the top systems that participated in Morpho Challenge 2009. If all the systems are considered, our system comes 5th out of 16 systems. The problem of morphologically rich languages is not our priority within this research. Nevertheless, we provide evaluation scores on Turkish. The Turkish dataset consists of 617,298 words. We chose words with frequency greater than 50 for Turkish since the Turkish dataset is not large enough. The results for Turkish are given in Table 4. Our system comes 3rd out of 7 systems. 6 Discussion The model can easily capture c</context>
</contexts>
<marker>Kurimo, Virpioja, Turunen, Blackwood, Byrne, 2009</marker>
<rawString>Mikko Kurimo, Sami Virpioja, Ville T. Turunen, Graeme W. Blackwood, and William Byrne. 2009. Overview and results of morpho challenge 2009. In Proceedings of the 10th cross-language evaluation forum conference on Multilingual information access evaluation: text retrieval experiments, CLEF’09, pages 578–597, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
</authors>
<title>Krista Lagus, Sami Virpioja, and Ville Turunen.</title>
<date></date>
<booktitle>2011a. Morpho challenge 2009. http://research.ics.tkk.fi/ events/morphochallenge2009/,</booktitle>
<institution>Mikko Kurimo, Krista Lagus,</institution>
<location>Sami Virpioja, and</location>
<marker>Kurimo, </marker>
<rawString>Mikko Kurimo, Krista Lagus, Sami Virpioja, and Ville Turunen. 2011a. Morpho challenge 2009. http://research.ics.tkk.fi/ events/morphochallenge2009/, June. Mikko Kurimo, Krista Lagus, Sami Virpioja, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ville Turunen</author>
</authors>
<date>2011</date>
<booktitle>Morpho challenge 2010. http://research.ics.tkk.fi/ events/morphochallenge2010/,</booktitle>
<marker>Turunen, 2011</marker>
<rawString>Ville Turunen. 2011b. Morpho challenge 2010. http://research.ics.tkk.fi/ events/morphochallenge2010/, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Franc¸ois Lavall´ee</author>
<author>Philippe Langlais</author>
</authors>
<title>Morphological acquisition by formal analogy.</title>
<date>2009</date>
<booktitle>In Working Notes for the CLEF 2009 Workshop,</booktitle>
<marker>Lavall´ee, Langlais, 2009</marker>
<rawString>Jean Franc¸ois Lavall´ee and Philippe Langlais. 2009. Morphological acquisition by formal analogy. In Working Notes for the CLEF 2009 Workshop, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantine Lignos</author>
<author>Erwin Chan</author>
<author>Mitchell P Marcus</author>
<author>Charles Yang</author>
</authors>
<title>A rule-based unsupervised morphology learning framework.</title>
<date>2009</date>
<booktitle>In Working Notes for the CLEF 2009 Workshop,</booktitle>
<contexts>
<context position="24594" citStr="Lignos et al. (2009)" startWordPosition="4297" endWordPosition="4300">The results are the best results over these 3 models. We are reporting the best results out of the 3 models due to the small (22k word) datasets used. Use of larger datasets would have resulted in less variation and better results. System P(%) R(%) F(%) Allomorf1 68.98 56.82 62.31 Morf. Base.2 74.93 49.81 59.84 PM-Union3 55.68 62.33 58.82 Lignos4 83.49 45.00 58.48 Prob. Clustering (multiple) 57.08 57.58 57.33 PM-mimic3 53.13 59.01 55.91 MorphoNet5 65.08 47.82 55.13 Rali-cof6 68.32 46.45 55.30 CanMan7 58.52 44.82 50.76 1 Virpioja et al. (2009) 2 Creutz and Lagus (2002) 3 Monson et al. (2009) 4 Lignos et al. (2009) 5 Bernhard (2009) 6 Lavall´ee and Langlais (2009) 7 Can and Manandhar (2009) Table 3: Comparison with other unsupervised systems that participated in Morpho Challenge 2009 for English. We compare our system with the other participant systems in Morpho Challenge 2010. Results are given in Table 6 (Virpioja et al., 2011). Since the model is evaluated using the official (hidden) Morpho Challenge 2010 evaluation dataset where we submit our system for evaluation to the organisers, the scores are different from the ones that we presented Table 1 and Table 2. We also demonstrate experiments with Mor</context>
</contexts>
<marker>Lignos, Chan, Marcus, Yang, 2009</marker>
<rawString>Constantine Lignos, Erwin Chan, Mitchell P. Marcus, and Charles Yang. 2009. A rule-based unsupervised morphology learning framework. In Working Notes for the CLEF 2009 Workshop, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantine Lignos</author>
</authors>
<title>Learning from unseen data. In</title>
<date>2010</date>
<booktitle>Proceedings of the Morpho Challenge 2010 Workshop,</booktitle>
<pages>35--38</pages>
<editor>Mikko Kurimo, Sami Virpioja, Ville Turunen, and Krista Lagus, editors,</editor>
<institution>Aalto University,</institution>
<location>Espoo, Finland.</location>
<contexts>
<context position="28460" citStr="Lignos (2010)" startWordPosition="4877" endWordPosition="4878">l does not consider any discrimination between different types of morphological forms during training. As the prefix pre- appears at the beginning of words, it is identified as a stem. However, identifying pre- as a stem does not yield a change in the morphological analysis of the word. System P(%) R(%) F(%) Base Inference1 80.77 53.76 64.55 Iterative Comp.1 80.27 52.76 63.67 Aggressive Comp.1 71.45 52.31 60.40 Nicolas2 67.83 53.43 59.78 Prob. Clustering (multiple) 57.08 57.58 57.33 Morf. Baseline3 81.39 41.70 55.14 Prob. Clustering (single) 70.76 36.51 48.17 Morf. CatMAP4 86.84 30.03 44.63 1 Lignos (2010) 2 Nicolas et al. (2010) 3 Creutz and Lagus (2002) 4 Creutz and Lagus (2005a) Table 6: Comparison of our model with other unsupervised systems that participated in Morpho Challenge 2010 for English. Sometimes similarities may not yield a valid analysis of words. For example, the prefix preleads the words pre+mise, pre+sumed, pre+gnant to be analysed wrongly, whereas pre- is a valid prefix for the word pre+face. Another nice feature about the model is that compounds are easily captured through common stems: e.g. doubt+fire, bon+fire, gun+fire, clear+cut. 7 Conclusion &amp; Future Work In this paper</context>
</contexts>
<marker>Lignos, 2010</marker>
<rawString>Constantine Lignos. 2010. Learning from unseen data. In Mikko Kurimo, Sami Virpioja, Ville Turunen, and Krista Lagus, editors, Proceedings of the Morpho Challenge 2010 Workshop, pages 35–38, Aalto University, Espoo, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Monson</author>
<author>Kristy Hollingshead</author>
<author>Brian Roark</author>
</authors>
<title>Probabilistic paramor.</title>
<date>2009</date>
<booktitle>In Proceedings of the 10th cross-language evaluation forum conference on Multilingual information access evaluation: text retrieval experiments, CLEF’09,</booktitle>
<contexts>
<context position="2107" citStr="Monson et al. (2009)" startWordPosition="309" endWordPosition="312">nguages) are considered. The sparsity problem is more severe for more morphologically complex languages. Applying morphological segmentation mitigates data sparsity by tackling the issue with out-of-vocabulary (OOV) words. In this paper, we propose a paradigmatic approach. A morphological paradigm is a pair (StemList, SuffixList) such that each concatenation of Stem+Suffix (where Stem E StemList and Suffix E SuffixList) is a valid word form. The learning of morphological paradigms is not novel as there has already been existing work in this area such as Goldsmith (2001), Snover et al. (2002), Monson et al. (2009), Can and Manandhar (2009) and Dreyer and Eisner (2011). However, none of these existing approaches address learning of the hierarchical structure of paradigms. Hierarchical organisation of words help capture morphological similarities between words in a compact structure by factoring these similarities through stems, suffixes or prefixes. Our inference algorithm simultaneously infers latent variables (i.e. the morphemes) along with their hierarchical organisation. Most hierarchical clustering algorithms are single-pass, where once the hierarchical structure is built, the structure does not ch</context>
<context position="24571" citStr="Monson et al. (2009)" startWordPosition="4292" endWordPosition="4295">nsisting of 22k words. The results are the best results over these 3 models. We are reporting the best results out of the 3 models due to the small (22k word) datasets used. Use of larger datasets would have resulted in less variation and better results. System P(%) R(%) F(%) Allomorf1 68.98 56.82 62.31 Morf. Base.2 74.93 49.81 59.84 PM-Union3 55.68 62.33 58.82 Lignos4 83.49 45.00 58.48 Prob. Clustering (multiple) 57.08 57.58 57.33 PM-mimic3 53.13 59.01 55.91 MorphoNet5 65.08 47.82 55.13 Rali-cof6 68.32 46.45 55.30 CanMan7 58.52 44.82 50.76 1 Virpioja et al. (2009) 2 Creutz and Lagus (2002) 3 Monson et al. (2009) 4 Lignos et al. (2009) 5 Bernhard (2009) 6 Lavall´ee and Langlais (2009) 7 Can and Manandhar (2009) Table 3: Comparison with other unsupervised systems that participated in Morpho Challenge 2009 for English. We compare our system with the other participant systems in Morpho Challenge 2010. Results are given in Table 6 (Virpioja et al., 2011). Since the model is evaluated using the official (hidden) Morpho Challenge 2010 evaluation dataset where we submit our system for evaluation to the organisers, the scores are different from the ones that we presented Table 1 and Table 2. We also demonstra</context>
</contexts>
<marker>Monson, Hollingshead, Roark, 2009</marker>
<rawString>Christian Monson, Kristy Hollingshead, and Brian Roark. 2009. Probabilistic paramor. In Proceedings of the 10th cross-language evaluation forum conference on Multilingual information access evaluation: text retrieval experiments, CLEF’09, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lionel Nicolas</author>
<author>Jacques Farr´e</author>
<author>Miguel A Molinero</author>
</authors>
<title>Unsupervised learning of concatenative morphology based on frequency-related form occurrence.</title>
<date>2010</date>
<booktitle>In Mikko Kurimo, Sami Virpioja, Ville Turunen, and Krista Lagus, editors, Proceedings of the Morpho Challenge 2010 Workshop,</booktitle>
<pages>39--43</pages>
<institution>Aalto University,</institution>
<location>Espoo, Finland.</location>
<marker>Nicolas, Farr´e, Molinero, 2010</marker>
<rawString>Lionel Nicolas, Jacques Farr´e, and Miguel A. Molinero. 2010. Unsupervised learning of concatenative morphology based on frequency-related form occurrence. In Mikko Kurimo, Sami Virpioja, Ville Turunen, and Krista Lagus, editors, Proceedings of the Morpho Challenge 2010 Workshop, pages 39– 43, Aalto University, Espoo, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew G Snover</author>
<author>Gaja E Jarosz</author>
<author>Michael R Brent</author>
</authors>
<title>Unsupervised learning of morphology using a novel directed search algorithm: Taking the first step.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning,</booktitle>
<pages>11--20</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2085" citStr="Snover et al. (2002)" startWordPosition="305" endWordPosition="308">(i.e. agglutinative languages) are considered. The sparsity problem is more severe for more morphologically complex languages. Applying morphological segmentation mitigates data sparsity by tackling the issue with out-of-vocabulary (OOV) words. In this paper, we propose a paradigmatic approach. A morphological paradigm is a pair (StemList, SuffixList) such that each concatenation of Stem+Suffix (where Stem E StemList and Suffix E SuffixList) is a valid word form. The learning of morphological paradigms is not novel as there has already been existing work in this area such as Goldsmith (2001), Snover et al. (2002), Monson et al. (2009), Can and Manandhar (2009) and Dreyer and Eisner (2011). However, none of these existing approaches address learning of the hierarchical structure of paradigms. Hierarchical organisation of words help capture morphological similarities between words in a compact structure by factoring these similarities through stems, suffixes or prefixes. Our inference algorithm simultaneously infers latent variables (i.e. the morphemes) along with their hierarchical organisation. Most hierarchical clustering algorithms are single-pass, where once the hierarchical structure is built, the</context>
</contexts>
<marker>Snover, Jarosz, Brent, 2002</marker>
<rawString>Matthew G. Snover, Gaja E. Jarosz, and Michael R. Brent. 2002. Unsupervised learning of morphology using a novel directed search algorithm: Taking the first step. In Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning, pages 11–20, Morristown, NJ, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sami Virpioja</author>
<author>Oskar Kohonen</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised morpheme discovery with allomorfessor.</title>
<date>2009</date>
<booktitle>In Working Notes for the CLEF</booktitle>
<location>Workshop.</location>
<contexts>
<context position="24522" citStr="Virpioja et al. (2009)" startWordPosition="4282" endWordPosition="4285"> different randomly generated training sets each consisting of 22k words. The results are the best results over these 3 models. We are reporting the best results out of the 3 models due to the small (22k word) datasets used. Use of larger datasets would have resulted in less variation and better results. System P(%) R(%) F(%) Allomorf1 68.98 56.82 62.31 Morf. Base.2 74.93 49.81 59.84 PM-Union3 55.68 62.33 58.82 Lignos4 83.49 45.00 58.48 Prob. Clustering (multiple) 57.08 57.58 57.33 PM-mimic3 53.13 59.01 55.91 MorphoNet5 65.08 47.82 55.13 Rali-cof6 68.32 46.45 55.30 CanMan7 58.52 44.82 50.76 1 Virpioja et al. (2009) 2 Creutz and Lagus (2002) 3 Monson et al. (2009) 4 Lignos et al. (2009) 5 Bernhard (2009) 6 Lavall´ee and Langlais (2009) 7 Can and Manandhar (2009) Table 3: Comparison with other unsupervised systems that participated in Morpho Challenge 2009 for English. We compare our system with the other participant systems in Morpho Challenge 2010. Results are given in Table 6 (Virpioja et al., 2011). Since the model is evaluated using the official (hidden) Morpho Challenge 2010 evaluation dataset where we submit our system for evaluation to the organisers, the scores are different from the ones that we</context>
</contexts>
<marker>Virpioja, Kohonen, Lagus, 2009</marker>
<rawString>Sami Virpioja, Oskar Kohonen, and Krista Lagus. 2009. Unsupervised morpheme discovery with allomorfessor. In Working Notes for the CLEF 2009 Workshop. September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sami Virpioja</author>
<author>Ville T Turunen</author>
<author>Sebastian Spiegler</author>
<author>Oskar Kohonen</author>
<author>Mikko Kurimo</author>
</authors>
<title>Empirical comparison of evaluation methods for unsupervised learning of morphology.</title>
<date>2011</date>
<booktitle>In TraitementAutomatique des Langues.</booktitle>
<contexts>
<context position="24915" citStr="Virpioja et al., 2011" startWordPosition="4350" endWordPosition="4353">on3 55.68 62.33 58.82 Lignos4 83.49 45.00 58.48 Prob. Clustering (multiple) 57.08 57.58 57.33 PM-mimic3 53.13 59.01 55.91 MorphoNet5 65.08 47.82 55.13 Rali-cof6 68.32 46.45 55.30 CanMan7 58.52 44.82 50.76 1 Virpioja et al. (2009) 2 Creutz and Lagus (2002) 3 Monson et al. (2009) 4 Lignos et al. (2009) 5 Bernhard (2009) 6 Lavall´ee and Langlais (2009) 7 Can and Manandhar (2009) Table 3: Comparison with other unsupervised systems that participated in Morpho Challenge 2009 for English. We compare our system with the other participant systems in Morpho Challenge 2010. Results are given in Table 6 (Virpioja et al., 2011). Since the model is evaluated using the official (hidden) Morpho Challenge 2010 evaluation dataset where we submit our system for evaluation to the organisers, the scores are different from the ones that we presented Table 1 and Table 2. We also demonstrate experiments with Morpho Challenge 2009 English dataset. The dataset consists of 384,904 words. Our results and the results of other participant systems in Morpho Challenge 2009 are given in Table 3 (Kurimo et al., 2009). It should be noted that we only present the top systems that participated in Morpho Challenge 2009. If all the systems a</context>
</contexts>
<marker>Virpioja, Turunen, Spiegler, Kohonen, Kurimo, 2011</marker>
<rawString>Sami Virpioja, Ville T. Turunen, Sebastian Spiegler, Oskar Kohonen, and Mikko Kurimo. 2011. Empirical comparison of evaluation methods for unsupervised learning of morphology. In TraitementAutomatique des Langues.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>