<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000044">
<title confidence="0.884793">
How to Produce Unseen Teddy Bears:
Improved Morphological Processing of Compounds in SMT
</title>
<author confidence="0.990337">
Fabienne Cap, Alexander Fraser
</author>
<affiliation confidence="0.993547">
CIS, University of Munich
</affiliation>
<email confidence="0.986643">
{cap|fraser}@cis.uni-muenchen.de
</email>
<author confidence="0.996235">
Marion Weller Aoife Cahill
</author>
<affiliation confidence="0.999635">
IMS, University of Stuttgart Educational Testing Service
</affiliation>
<email confidence="0.993422">
wellermn@ims.uni-stuttgart.de acahill@ets.org
</email>
<sectionHeader confidence="0.99465" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920263157895">
Compounding in morphologically rich
languages is a highly productive process
which often causes SMT approaches to
fail because of unseen words. We present
an approach for translation into a com-
pounding language that splits compounds
into simple words for training and, due
to an underspecified representation, allows
for free merging of simple words into
compounds after translation. In contrast to
previous approaches, we use features pro-
jected from the source language to predict
compound mergings. We integrate our ap-
proach into end-to-end SMT and show that
many compounds matching the reference
translation are produced which did not ap-
pear in the training data. Additional man-
ual evaluations support the usefulness of
generalizing compound formation in SMT.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973166666667">
Productive processes like compounding or inflec-
tion are problematic for traditional phrase-based
statistical machine translation (SMT) approaches,
because words can only be translated as they have
occurred in the parallel training data. As paral-
lel training data is limited, it is desirable to ex-
tract as much information from it as possible. We
present an approach for compound processing in
SMT, translating from English to German, that
splits compounds prior to training (in order to ac-
cess the individual words which together form the
compound) and recombines them after translation.
While compound splitting is a well-studied task,
compound merging has not received as much at-
tention in the past. We start from Stymne and Can-
cedda (2011), who used sequence models to pre-
dict compound merging and Fraser et al. (2012)
who, in addition, generalise over German inflec-
tion. Our new contributions are: (i) We project
features from the source language to support com-
pound merging predictions. As the source lan-
guage input is fluent, these features are more re-
liable than features derived from target language
SMT output. (ii) We reduce compound parts to
an underspecified representation which allows for
maximal generalisation. (iii) We present a detailed
manual evaluation methodology which shows that
we obtain improved compound translations.
We evaluated compound processing both on
held-out split data and in end-to-end SMT. We
show that using source language features increases
the accuracy of compound generation. Moreover,
we find more correct compounds than the base-
lines, and a considerable number of these com-
pounds are unseen in the training data. This is
largely due to the underspecified representation we
are using. Finally, we show that our approach im-
proves upon the previous work.
We discuss compound processing in SMT in
Section 2, and summarise related work in Sec-
tion 3. In Section 4 we present our method for
splitting compounds and reducing the component
words to an underspecified representation. The
merging to obtain German compounds is the sub-
ject of Section 5. We evaluate the accuracy of
compound prediction on held-out data in Section 6
and in end-to-end SMT experiments in Section 7.
We conclude in Section 8.
</bodyText>
<sectionHeader confidence="0.85801" genericHeader="introduction">
2 Dealing with Compounds in SMT
</sectionHeader>
<bodyText confidence="0.999739888888889">
In German, two (or more) single words (usually
nouns or adjectives) are combined to form a
compound which is considered a semantic unit.
The rightmost part is referred to as the head while
all other parts are called modifiers. EXAMPLE (1)
lists different ways of joining simple words into
compounds: mostly, no modification is required
(A) or a filler letter is introduced (B). More rarely,
a letter is deleted (C), or transformed (D).
</bodyText>
<page confidence="0.977327">
579
</page>
<note confidence="0.997456">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 579–587,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<figure confidence="0.994710727272727">
splitting training testing re−combination
Obsthandel
Werkzeugkiste
Obst
Handel
Werkzeug
Kiste
fruit
trading
tool
box
trading
tool
fruit
box
Handel
Werkzeug
Obst
Kiste
Handelswerkzeug
Obstkiste
splitting training testing re−combination
</figure>
<figureCaption confidence="0.999978">
Figure 1: Compound processing in SMT allows the synthesis of compounds unseen in the training data.
</figureCaption>
<equation confidence="0.9927856">
EXAMPLE (1)
|XML |xmlLoc_1 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_yes bi_xmlSFBIA_new bi_xmlPara_new
|XML |xmlLoc_1 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_yes bi_xmlSFBIA_continue bi_xmlPara_new
|XML |xmlLoc_1 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_yes bi_xmlSFBIA_continue bi_xmlPara_new
|XML |xmlLoc_1 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_yes bi_xmlSFBIA_continue bi_xmlPara_new
</equation>
<bodyText confidence="0.999947633333333">
German compounds are highly productive,1 and
traditional SMT approaches often fail in the face
of such productivity. Therefore, special process-
ing of compounds is required for translation into
German, as many compounds will not (e.g. Haus-
boot, “house boat”) or only rarely have been seen
in the training data.2 In contrast, most compounds
consist of two (or more) simple words that occur
more frequently in the data than the compound as
a whole (e.g. Haus (7,975) and Boot (162)) and of-
ten, these compound parts can be translated 1-to-
1 into simple English words. Figure 1 illustrates
the basic idea of compound processing in SMT:
imagine, “Werkzeug” (“tool”) occurred only as a
modifier of e.g. “Kiste” (“box”) in the training
data, but the test set contains “tool” as a simple
word or as the head of a compound. Splitting com-
pounds prior to translation model training enables
better access to the component translations and al-
lows for a high degree of generalisation. At test-
ing time, the English text is translated into the split
German representation, and only afterwards, some
sequences of simple words are (re-)combined into
(possibly unseen) compounds where appropriate.
This merging of compounds is much more chal-
lenging than the splitting, as it has to be applied
to disfluent MT output: i.e., compound parts may
not occur in the correct word order and even if they
do, not all sequences of German words that could
form a compound should be merged.
</bodyText>
<sectionHeader confidence="0.999937" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.9853335">
Compound processing for translation into a com-
pounding language includes both compound split-
</bodyText>
<footnote confidence="0.995297666666667">
1Most newly appearing words in German are compounds.
2~30% of the word types and ~77% of the compound
types we identified in our training data occurred ≤ 3 times.
</footnote>
<bodyText confidence="0.998436048780488">
ting and merging, we thus report on previous ap-
proaches for both of these tasks.
In the past, there have been numerous attempts
to split compounds, all improving translation qual-
ity when translating from a compounding to a non-
compounding language. Several compound split-
ting approaches make use of substring corpus fre-
quencies in order to find the optimal split points of
a compound (e.g. Koehn and Knight (2003), who
allowed only “(e)s” as filler letters). Stymne et al.
(2008) use Koehn and Knight’s technique, include
a larger list of possible modifier transformations
and apply POS restrictions on the substrings, while
Fritzinger and Fraser (2010) use a morphological
analyser to find only linguistically motivated sub-
strings. In contrast, Dyer (2010) presents a lattice-
based approach to encode different segmentations
of words (instead of finding the one-best split).
More recently, Macherey et al. (2011) presented
a language-independent unsupervised approach in
which filler letters and a list of words not to be split
(e.g., named entities) are learned using phrase ta-
bles and Levenshtein distance.
In contrast to splitting, the merging of com-
pounds has received much less attention in the
past. An early approach by Popovi´c et al. (2006)
recombines compounds using a list of compounds
and their parts. It thus never creates invalid Ger-
man compounds, but on the other hand it is limited
to the coverage of the list. Moreover, in some con-
texts a merging in the list may still be wrong, cf.
EXAMPLE (3) in Section 5 below. The approach
of Stymne (2009) makes use of a factored model,
with a special POS-markup for compound mod-
ifiers, derived from the POS of the whole com-
pound. This markup enables sound mergings of
compound parts after translation if the POS of the
candidate modifier (X-Part) matches the POS of
the candidate compound head (X): InflationsIN-
Part + Rate|N = Inflationsrate|N (“inflation rate”).
In Stymne and Cancedda (2011) the factored ap-
</bodyText>
<figure confidence="0.953275761904762">
Haus+Boot = Hausboot (“house boat”)
Ort+s+Zeit = Ortszeit (“local time”)
Kirche-e+Turm = Kirchturm (“church tower”)
Kriterium+Liste = Kriterienliste (“criteria list”)
580
0) Original Text
1) Bitpar Parsed Text
2) True Casing
3) SMOR Analysis
4) Disambiguation
Amerikanische Medien ...
Der Gastraum des ...
Tim Baumeister besiegt ...
...
...
...
(S(NP(ADJA Amerikanische) (NN Medien)...))
...
(S(NP(ART Der) (NN Gastraum) (ART des)...))
...
(S(NP(PN(NE Tim)(NE Baumeister))(VV besiegt)...))
...
amerikanische Medien ...
ADJA NN
der Gastraum des ...
ART NN ART
Tim Baumeister besiegt ...
NE NE VV
...
...
...
&gt; amerikanische
amerikanisch&lt;+ADJ&gt;
&gt; Baumeister
Bau&lt;NN&gt;Meister&lt;+NN&gt;
Baumeister&lt;+NPROP&gt;
&gt; Gastraum
Gas&lt;NN&gt;Traum&lt;+NN&gt;
Gast&lt;NN&gt;Raum&lt;+NN&gt;
Gastraum 3.74
Gas|Traum 8.34
Gast|Raum 8.59
</figure>
<figureCaption confidence="0.978191">
Figure 2: Compound splitting pipeline 1) The original text is parsed with BITPAR to get unambiguous POS tags,
</figureCaption>
<listItem confidence="0.883255333333333">
2) The original text is then true-cased using the most frequent casing for each word and BITPAR tags are added,
3) All words are analysed with SMOR, analyses are filtered using BITPAR tags (only bold-faced analyses are kept),
4) If several splitting options remain, the geometric mean of the word (part) frequencies is used to disambiguate them.
</listItem>
<bodyText confidence="0.999952571428571">
proach was extended to make use of a CRF se-
quence labeller (Lafferty et al., 2001) in order
to find reasonable merging points. Besides the
words and their POS, many different target lan-
guage frequency features were defined to train the
CRF. This approach can even produce new com-
pounds unseen in the training data, provided that
the modifiers occurred in modifier position of a
compound and heads occurred as heads or even as
simple words with the same inflectional endings.
However, as former compound modifiers were left
with their filler letters (cf. “Inflations”), they can
not be generalised to compound heads or simple
words, nor can inflectional variants of compound
heads or simple words be created (e.g. if “Rate”
had only been observed in nominative form in the
training data, the genitive “Raten” could not be
produced). The underspecified representation we
are using allows for maximal generalisation over
word parts independent of their position of oc-
currence or inflectional realisations. Moreover,
their experiments were limited to predicting com-
pounds on held-out data; no results were reported
for using their approach in translation. In Fraser
et al. (2012) we re-implemented the approach of
Stymne and Cancedda (2011), combined it with
inflection prediction and applied it to a transla-
tion task. However, compound merging was re-
stricted to a list of compounds and parts. Our
present work facilitates more independent com-
bination. Toutanova et al. (2008) and Weller et
al. (2013) used source language features for target
language inflection, but to our knowledge, none of
these works applied source language features for
compound merging.
</bodyText>
<sectionHeader confidence="0.957234" genericHeader="method">
4 Step 1: Underspecified Representation
</sectionHeader>
<bodyText confidence="0.9999716">
In order to enhance translation model accuracy,
it is reasonable to have similar degrees of mor-
phological richness between source and target lan-
guage. We thus reduce the German target lan-
guage training data to an underspecified represen-
tation: we split compounds, and lemmatise all
words (except verbs). All occurrences of simple
words, former compound modifiers or heads have
the same representation and can thus be freely
merged into “old” and “new” compounds after
translation, cf. Figure 1 above. So that we can later
predict the merging of simple words into com-
pounds and the inflection of the words, we store
all of the morphological information stripped from
the underspecified representation.
Note that erroneous over-splitting might make
the correct merging of compounds difficult3
(or even impossible), due to the number of
correct decisions required. For example, it
requires only 1 correct prediction to recom-
bine “Niederschlag|Menge” into “Niederschlags-
menge” (“amount of precipitation”) but 3 for
the wrong split into “nie|der|Schlag|Menge”
(“never|the|hit|amount”). We use the compound
splitter of Fritzinger and Fraser (2010), who have
shown that using a rule-based morphological anal-
yser (SMOR, Schmid et al. (2004)) drastically re-
duced the number of erroneous splits when com-
pared to the frequency-based approach of Koehn
and Knight (2003). However, we adapted it to
work on tokens: some words can, depending on
their context, either be interpreted as named enti-
ties or common nouns, e.g., “Dinkelacker” (a Ger-
man beer brand or “spelt|field”).4 We parsed the
training data and use the parser’s decisions to iden-
tify proper names, see “Baumeister” in Figure 2.
After splitting, we use SMOR to reduce words to
lemmas, keeping morphological features like gen-
der or number, and stripping features like case, as
illustrated for “ ¨Olexporteure” (“oil exporters”):
</bodyText>
<footnote confidence="0.9298095">
3In contrast, they may not hurt translation quality in the
other direction, where phrase-based SMT is likely to learn
the split words as a phrase and thus recover from that error.
4Note that Macherey et al. (2011) blocked splitting of
words which can be used as named entities, independent of
context, which is less general than our solution.
</footnote>
<page confidence="0.987526">
581
</page>
<table confidence="0.869479923076923">
No. Feature Description Example Experiment
SC T TR
1SC surface form of the word string: Arbeit&lt;+NN&gt;&lt;Fem&gt;&lt;Sg&gt; X X
2SC main part of speech of the word (from the parser) string: +NN X X
3SC word occurs in a bigram with the next word frequency: 0 X X
4SC word combined to a compound with the next word frequency: 10,000 X X X
5SC word occurs in modifier position of a compound frequency: 100,000 X X
6SC word occurs in a head position of a compound frequency: 10,000 X X
7SC word occurs in modifier position vs. simplex string: P&gt;W (P= 5SC, W= 100,000) X
8SC word occurs in head position vs. simplex string: S&lt;W (S= 6SC, W= 100,000) X
7SC+ word occurs in modifier position vs. simplex ratio: 10 (10**ceil(log10(5SC/W))) X X
8SC+ word occurs in head position vs. simplex ratio: 1 (10**ceil(log10(6SC/W))) X X
9N different head types the word can combine with number: 10,000 X X
</table>
<tableCaption confidence="0.999552">
Table 1: Target language CRF features for compound merging. SC = features taken from Stymne and Cancedda
</tableCaption>
<equation confidence="0.809609833333333">
(2011), SC+ = improved versions, N = new feature. Experiments: SC = re-implementation of Stymne and Cancedda (2011),
T= use full Target feature set, TR = use Target features, but only a Reduced set.
EXAMPLE (2)
compound
Öl&lt;NN&gt;Exporteur&lt;+NN&gt;&lt;Masc&gt;&lt;Nom&gt;&lt;Pl&gt;
Öl&lt;+NN&gt;&lt;Neut&gt;&lt;Sg&gt; Exporteur&lt;+NN&gt; &lt;Masc&gt;&lt;Pl&gt;
</equation>
<bodyText confidence="0.944712555555555">
modifier head
While the former compound head (“Exporteure”)
automatically inherits all morphological features
of the compound as a whole, the features of the
modifier need to be derived from SMOR in an ad-
ditional step. We need to ensure that the repre-
sentation of the modifier is identical to the same
word when it occurs independently in order to ob-
tain full generalisation over compound parts.
</bodyText>
<sectionHeader confidence="0.96859" genericHeader="method">
5 Step 2: Compound Merging
</sectionHeader>
<bodyText confidence="0.995079833333333">
After translation from English into the underspec-
ified German representation, post-processing is re-
quired to transform the output back into fluent,
morphologically fully specified German. First,
compounds need to be merged where appropriate,
e.g., “Hausboote” (“house boats”):
</bodyText>
<figure confidence="0.4224748">
Haus&lt;+NN&gt;&lt;Neut&gt;&lt;Sg&gt; + Boot&lt;+NN&gt;&lt;Neut&gt;&lt;Pl&gt;
→ Haus&lt;NN&gt;Boot&lt;+NN&gt;&lt;Neut&gt;&lt;Pl&gt; (merged)
and second, all words need to be inflected:
Haus&lt;NN&gt;Boot&lt;+NN&gt;&lt;Neut&gt;&lt;Acc&gt;&lt;Pl&gt;
→ Hausbooten (inflected)
</figure>
<subsectionHeader confidence="0.979108">
5.1 Target Language Features
</subsectionHeader>
<bodyText confidence="0.9999845">
To decide which words should be combined, we
follow Stymne and Cancedda (2011) who used
CRFs for this task. The features we derived from
the target language to train CRF models are listed
in Table 1. We adapted features No. 1-8 from
Stymne and Cancedda (2011). Then, we modi-
fied two features (7+8) and created a new feature
indicating the productivity of a modifier (9N).
</bodyText>
<subsectionHeader confidence="0.997756">
5.2 Projecting Source Language Features
</subsectionHeader>
<bodyText confidence="0.998447166666667">
We also use new features derived from the English
source language input, which is coherent and flu-
ent. This makes features derived from it more reli-
able than the target language features derived from
disfluent SMT output. Moreover, source language
features might support or block merging decisions
in unclear cases, i.e., where target language fre-
quencies are not helpful, either because they are
very low or they have roughly equal frequency dis-
tributions when occurring in a compound (as mod-
ifier or head) vs. as a simple word.
In Table 2, we list three types of features:
</bodyText>
<listItem confidence="0.973549">
1. Syntactic features: different English noun
phrase patterns that are aligned to German
compound candidate words (cf. 10E-13E)
2. The POS tag of the English word (cf. 14E)
3. Alignment features, derived from word
alignments (cf. 15E-18E)
</listItem>
<bodyText confidence="0.999964857142857">
The examples given in Table 2 (10E-13E) show
that English compounds often have 1-to-1 corre-
spondences to the parts of a German compound.
Knowing that two consecutive German simple
words are aligned to two English words of the
same noun phrase is a strong indicator that the
German words should be merged:
</bodyText>
<equation confidence="0.732342">
EXAMPLE (3)
</equation>
<bodyText confidence="0.7008815">
should be merged:
ein erh¨ohtes verkehrs aufkommen sorgt f¨ur chaos
“an increased traffic volume causes chaos”
(S...(NP(DT An)(VN increased)(NN traffic)(NN volume))..)))
should not be merged:
f¨ur die finanzierung des verkehrs aufkommen
“pay for the financing of transport”
(VP(V pay)(PP(IN for)(NP(NP(DT the)(NN financing))
(PP(IN of)(NP(NN transport)..))
In the compound reading of “verkehr + aufkom-
men”, the English parse structure indicates that
the words aligned to “verkehr” (“traffic”) and
</bodyText>
<page confidence="0.980324">
582
</page>
<table confidence="0.999640555555555">
No. Feature Description Type
10E word and next word are aligned from a noun phrase in the English source sentence: true/false
(NP(NN traffic)(NN accident)) → Verkehr (“traffic”) + Unfall (“accident”)
11E word and next word are aligned from a gerund construction in the English source sentence: true/false
(NP(VBG developing)(NNS nations)) → Entwicklung (“development”) + L¨ander (“countries”)
12E word and next word are aligned from a genitive construction in the English source sentence: true/false
(NP(NP(DT the)(NN end))(PP(IN of)(NP(DT the)(NN year)) → Jahr (“year”) + Ende(“end”)
13E word and next word are aligned from an adjective noun construction in the English source sentence: true/false
(NP (ADJ protective)(NNS measures)) → Schutz (“protection”) + Maßnahmen (“measures”)
14E print the POS of the corresponding aligned English word string
15E word and next word are aligned 1-to-1 from the same word in the English source sentence, e.g., true/false
beefZ Rind(“cow”)
`-, Fleisch(“meat”)
16E Nobel(“Nobel”) true/false
like 15E, but the English dash, Nobel − Prize
word contains a e.g., `-, Preis(“prize”)
17E like 15E, but also considering 1-to-n and n-to-1 links true/false
18E like 16E, but also considering 1-to-n and n-to-1 links true/false
</table>
<tableCaption confidence="0.9991">
Table 2: List of new source language CRF features for compound merging.
</tableCaption>
<bodyText confidence="0.9999094375">
“aufkommen” (“volume”), are both nouns and
part of one common noun phrase, which is a strong
indicator that the two words should be merged
in German. In contrast, the syntactic relation-
ship between “pay” (aligned to “aufkommen”)
and “transport” (aligned to “verkehr”) is more dis-
tant5: merging is not indicated.
We also use the POS of the English words to
learn (un)usual combinations of POS, indepen-
dent of their exact syntactic structure (14E). Re-
consider EXAMPLE (3): NN+NN is a more com-
mon POS pair for compounds than V+NN.
Finally, the alignment features (15E-18E) pro-
mote the merging into compounds whose align-
ments indicate that they should not have been split
in the first place (e.g., Rindfleisch, 15E).
</bodyText>
<subsectionHeader confidence="0.998913">
5.3 Compound Generation and Inflection
</subsectionHeader>
<bodyText confidence="0.98424704">
So far, we reported on how to decide which sim-
ple words are to be merged into compounds, but
not how to recombine them. Recall from EXAM-
PLE (1) that the modifier of a compound some-
times needs to be transformed, before it can be
combined with the head word (or next modifier),
e.g., “Ort”+“Zeit” = “Ortszeit” (“local time”).
We use SMOR to generate compounds from a
combination of simple words. This allows us to
create compounds with modifiers that never oc-
curred as such in the training data. Imagine that
“Ort” occurred only as compound head or as a
single word in the training data. Using SMOR, we
are still able to create the correct form of the mod-
ifier, including the required filler letter: “Orts”.
This ability distinguishes our approach from pre-
5Note that “f¨ur etwas aufkommen” (lit. “for sth. arise”,
idiom.: “to pay for sth.”) is an idiomatic expression.
vious approaches: Stymne and Cancedda (2011)
do not reduce modifiers to their base forms6 (they
can only create new compounds when the modifier
occurred as such in the training data) and Fraser et
al. (2012) use a list for merging.
Finally, we use the system described in Fraser
et al. (2012) to inflect the entire text.
</bodyText>
<sectionHeader confidence="0.885672" genericHeader="method">
6 Accuracy of Compound Prediction
</sectionHeader>
<bodyText confidence="0.999994714285714">
We trained CRF models on the parallel training
data (~40 million words)7 of the EACL 2009
workshop on statistical machine translation8 us-
ing different feature (sub)sets, cf. the “Exper-
iment” column in Table 1 above. We exam-
ined the reliability of the CRF compound predic-
tion models by applying them to held-out data:
</bodyText>
<listItem confidence="0.99935">
1. split the German wmt2009 tuning data set
2. remember compound split points
3. predict merging with CRF models
4. combine predicted words into compounds
5. calculate f-scores on how properly the
</listItem>
<bodyText confidence="0.992457545454545">
compounds were merged
Table 3 lists the CRF models we trained, together
with their compound merging accuracies on held-
out data. It can be seen that using more features
(SC→T→ST) is favourable in terms of precision
and overall accuracy and the positive impact of us-
ing source language features is clearer when only
reduced feature sets are used (TR vs. STR).
However, these accuracies only somewhat cor-
relate with SMT performance: while being trained
and tested on clean, fluent German language, the
</bodyText>
<footnote confidence="0.9984428">
6They account for modifier transformations by using char-
acter n-gram features (cf.EXAMPLE (1)).
7However, target language feature frequencies are derived
from the monolingual training data, ~146 million words.
8http://www.statmt.org/wmt09
</footnote>
<page confidence="0.996252">
583
</page>
<table confidence="0.934234">
exp to be all correct wrong wrong not merging precision recall f-score
merged merged merged merged merged wrong
SC 1,047 997 921 73 121 3 92.38% 88.13% 90.21%
T 1,047 979 916 59 128 4 93.56% 87.40% 90.38%
ST 1,047 976 917 55 126 4 93.95% 87.58% 90.66%
TR 1,047 893 836 52 204 5 93.62% 80.00% 86.27%
STR 1,047 930 866 58 172 6 93.12% 82.95% 87.74%
</table>
<tableCaption confidence="0.991670666666667">
Table 3: Compound production accuracies of CRF models on held-out data: SC: re-implementation of Stymne
and Cancedda (2011); T: all target language features, including a new one (cf. Table 1); ST = all Source and Target language
features; TR: only a reduced set of target language features; STR: TR, plus all source language features given in Table 2.
</tableCaption>
<table confidence="0.9999159">
exp BLEU SCORES #compounds found
mert.log BLEU RTS all ref new new*
RAW 14.88 14.25 1.0054 646 175 n.a. n.a.
UNSPLIT 15.86 14.74 0.9964 661 185 n.a. n.a.
SC 15.44 14.45 0.9870 882 241 47 8
T 15.56 14.32 0.9634 845 251 47 8
ST 15.33 14.51 0.9760 820 248 46 9
TR 15.24 14.26 0.9710 753 234 44 5
STR 15.37 14.61 0.9884 758 239 43 7
#compounds in reference text: 1,105 1,105 396 193
</table>
<tableCaption confidence="0.999789">
Table 4: SMT results. Tuning scores (mert.log) are on merged but uninflected data (except RAW).
</tableCaption>
<bodyText confidence="0.980808714285714">
RTS: length ratio; all: #compounds produced; ref: reference matches; new: unknown to parallel data; new*: unknown to
target language data. bold face indicates statistical significance wrt. the RAW baseline, SC, T and TR.
models will later be applied to disfluent SMT out-
put and might thus lead to different results there.
Stymne and Cancedda (2011) dealt with this by
noisifying the CRF training data: they translated
the whole data set using an SMT system that was
trained on the same data set. This way, the train-
ing data was less fluent than in its original format,
but still of higher quality than SMT output of un-
seen data. In contrast, we left the training data as
it was, but strongly reduced the feature set for CRF
model training (e.g., no more use of surface words
and POS tags, cf. TR and STR in Table 3) instead.
</bodyText>
<sectionHeader confidence="0.98105" genericHeader="method">
7 Translation Performance
</sectionHeader>
<bodyText confidence="0.99497545">
We integrated our compound processing pipeline
into an end-to-end SMT system. Models were
trained with the default settings of the Moses SMT
toolkit, v1.0 (Koehn et al., 2007) using the data
from the EACL 2009 workshop on statistical ma-
chine translation. All compound processing sys-
tems are trained and tuned identically, except us-
ing different CRF models for compound predic-
tion. All training data was split and reduced
to the underspecified representation described in
Section 4. We used KenLM (Heafield, 2011) with
SRILM (Stolcke, 2002) to train a 5-gram language
model based on all available target language train-
ing data. For tuning, we used batch-mira with ‘-
safe-hope’ (Cherry and Foster, 2012) and ran it
separately for every experiment. We integrated the
CRF-based merging of compounds into each itera-
tion of tuning and scored each output with respect
to an unsplit and lemmatised version of the tuning
reference. Testing consists of:
</bodyText>
<listItem confidence="0.9988964">
1. translation into the split, underspecified
German representation
2. compound merging using CRF models
to predict recombination points
3. inflection of all words
</listItem>
<subsectionHeader confidence="0.997504">
7.1 SMT Results
</subsectionHeader>
<bodyText confidence="0.999717941176471">
We use 1,025 sentences for tuning and 1,026 sen-
tences for testing. The results are given in Table 4.
We calculate BLEU scores (Papineni et al., 2002)
and compare our systems to a RAW baseline (built
following the instructions of the shared task) and a
baseline very similar to Fraser et al. (2012), using
a lemmatised representation of words for decod-
ing, re-inflecting them after translation, but with-
out compound processing (UNSPLIT). Table 4
shows that only UNSPLIT and STR (source lan-
guage and a reduced set of target language fea-
tures) are significantly9 improving over the RAW
baseline. They also significantly outperform all
other systems, except ST (full source and target
language feature set). The difference between STR
(14.61) and the UNSPLIT baseline (14.74) is not
statistically significant.
</bodyText>
<footnote confidence="0.998692">
9We used pair-wise bootstrap resampling with sample size
1000 and p-value 0.05, from: http://www.ark.cs.cmu.edu/MT
</footnote>
<page confidence="0.990099">
584
</page>
<table confidence="0.9996628">
group ID example reference english UNSPLIT STR
lexically 1a: perfect match Inflationsrate Inflationsrate inflation rate 185 239
matches 1b: inflection wrong Rohstoffpreisen Rohstoffpreise raw material prices 40 44
the 2a: merging wrong Anwaltsbewegung Anw¨altebewegung lawyers movement 5 9
reference 2b: no merging Polizei Chef Polizeichef police chief 101 54
correct 3a: compound Zentralbanken Notenbank central banks 92 171
translation 3b: no compound pflanzliche ¨Ole Speise¨ol vegetable oils 345 291
wrong 4a: compound Haushaltsdefizite Staatshaushalts state budget 12 42
translation 4b: no compound Ansporn Linien Nebenlinien spur lines 325 255
Total number of compounds in reference text: 1,105 1,105
</table>
<tableCaption confidence="0.752645">
Table 5: Groups for detailed manual compound evaluation and results for UNSPLIT and STR.
</tableCaption>
<table confidence="0.999764076923077">
reference English source UNSPLIT baseline STR
Teddyb¨aren teddy bear 4b Teddy tragen 1a Teddyb¨aren
(Teddy, to bear) (teddy bear)
Emissionsreduktion emissions reduction 3b Emissionen Reduzierung 3a Emissionsverringerung
(emissions, reducing) (emission decrease)
Geldstrafe fine 4b sch¨onen 3a Bußgeld
(fine/nice) (monetary fine)
Tischtennis table tennis 2b Tisch Tennis 4a Spieltischtennis
(table, tennis) (play table tennis)
Kreditkartenmarkt credit-card market 2b Kreditkarte Markt (credit-card, market) 4a Kreditmarkt
(credit market)
Rotationstempo rotation rate 2b Tempo Rotation 4a Temporotation
(rate, rotation) (rate rotation)
</table>
<tableCaption confidence="0.999314">
Table 6: Examples of the detailed manual compound analysis for UNSPLIT and STR.
</tableCaption>
<bodyText confidence="0.9999775">
Compound processing leads to improvements at
the level of unigrams and as BLEU is dominated
by four-gram precision and length penalty, it does
not adequately reflect compound related improve-
ments. We thus calculated the number of com-
pounds matching the reference for each experi-
ment and verified whether these were known to
the training data. The numbers in Table 4 show
that all compound processing systems outperform
both baselines in terms of finding more exact refer-
ence matches and also more compounds unknown
to the training data. Note that STR finds less ref-
erence matches than e.g. T or ST, but it also pro-
duces less compounds overall, i.e. it is more pre-
cise when producing compounds.
However, as compounds that are correctly com-
bined but poorly inflected are not counted, this is
only a lower bound on true compounding perfor-
mance. We thus performed two additional manual
evaluations and show that the quality of the com-
pounds (Section 7.2), and the human perception of
translation quality is improving (Section 7.3).
</bodyText>
<subsectionHeader confidence="0.995312">
7.2 Detailed Evaluation of Compounds
</subsectionHeader>
<bodyText confidence="0.9994805">
This evaluation focuses on how compounds in the
the reference text have been translated.10 We:
</bodyText>
<footnote confidence="0.793480333333333">
10In another evaluation, we investigated the 519 com-
pounds that our system produced but which did not match
the reference: 367 were correct translations of the English,
</footnote>
<listItem confidence="0.956659777777778">
1. manually identify compounds in German
reference text (1,105 found)
2. manually perform word alignment of these
compounds to the English source text
3. project these English counterparts of com-
pounds in the reference text to the decoded
text using the “–print-alignment-info” flag
4. manually annotate the resulting tuples, us-
ing the categories given in Table 5
</listItem>
<bodyText confidence="0.999901555555555">
The results are given in the two rightmost columns
of Table 5: besides a higher number of reference
matches (cf. row 1a), STR overall produces more
compounds than the UNSPLIT baseline, cf. rows
2a, 3a and 4a. Indirectly, this can also be seen from
the low numbers of STR in category 2b), where
the UNSPLIT baseline produces much more (101
vs. 54) translations that lexically match the refer-
ence without being a compound. While the 171
compounds of STR of category 3a) show that our
system produces many compounds that are correct
translations of the English, even though not match-
ing the reference (and thus not credited by BLEU),
the compounds of categories 2a) and 4a) contain
examples where we either fail to reproduce the
correct compound or over-generate compounds.
We give some examples in Table 6: for “teddy
bear”, the correct German word “Teddyb¨aren” is
</bodyText>
<page confidence="0.7910805">
87 contained erroneous lexemes and 65 were over-mergings.
585
</page>
<bodyText confidence="0.980039153846154">
missing in the parallel training data and instead
of “B¨ar” (“bear”), the baseline selected “tragen”
(“to bear”). Extracting all words containing the
substring “b¨ar” (“bear”) from the original parallel
training data and from its underspecified split
version demonstrates that our approach is able
to access all occurrences of the word. This leads
to higher frequency counts and thus enhances
the probabilities for correct translations. We can
generalise over 18 different word types containing
“bear” (e.g. “polar bears”, “brown bears”, “bear
skin”, “bear fur”) to obtain only 2:
occurrences in raw training data: B¨ar (19), B¨aren
</bodyText>
<table confidence="0.652578">
(26), B¨arendienst (42), B¨arenfarmen (1), B¨arenfell (2),
B¨arengalle(1), B¨arenhaut (1), B¨arenmarkt (1), Braunb¨ar
(1), Braunb¨aren (3), Braunb¨arengebiete (1), Braunb¨ar-
Population (1), Eisb¨aren(18), Eisb¨arenpopulation (2),
</table>
<equation confidence="0.745488">
Eisb¨arenpopulationen (1), Schwarzb¨ar (1), Schwarzb¨aren (1)
“bar” occurring in underspecified split data:
B¨ar&lt;+NN&gt;&lt;Masc&gt;&lt;Sg&gt; (94)
B¨ar&lt;+NN&gt;&lt;Masc&gt;&lt;Pl&gt; (29)
</equation>
<bodyText confidence="0.999453642857143">
“Emissionsverringerung” (cf. Table 6) is a typ-
ical example of group 3a): a correctly translated
compound that does not lexically match the ref-
erence, but which is semantically very similar to
the reference. The same applies for “Bußgeld”,
a synonym of “Geldstrafe”, for which the UN-
SPLIT baseline selected “sch¨onen” (“fine, nice”)
instead. Consider also the wrong compound pro-
ductions, e.g. “Tischtennis” is combined with
the verb “spielen” (“to play”) into “Spieltischten-
nis”. In contrast, “Kreditmarkt” dropped the mid-
dle part “Karte” (“card”), and in the case of “Tem-
porotation”, the head and modifier of the com-
pound are switched.
</bodyText>
<subsectionHeader confidence="0.986985">
7.3 Human perception of translation quality
</subsectionHeader>
<bodyText confidence="0.999963357142857">
We presented sentences of the UNSPLIT baseline
and of STR in random order to two native speak-
ers of German and asked them to rank the sen-
tences according to preference. In order to pre-
vent them from being biased towards compound-
bearing sentences, we asked them to select sen-
tences based on their native intuition, without re-
vealing our focus on compound processing.
Sentences were selected based on source lan-
guage sentence length: 10-15 words (178 sen-
tences), of which either the reference or our
system had to contain a compound (95 sen-
tences). After removing duplicates, we ended up
with 84 sentences to be annotated in two subse-
</bodyText>
<table confidence="0.991023571428571">
(a) Fluency: without reference sentence
κ = 0.3631 person 1
STR UNSPLIT equal
person 2 STR 24 6 7 37
UNSPLIT 5 16 9 30
equal 6 2 9 17
35 24 25 84
(b) Adequacy: with reference sentence
κ = 0.4948 person 1
STR UNSPLIT equal
person 2 STR 23 4 5 32
UNSPLIT 4 21 7 32
equal 5 3 12 20
32 28 24 84
</table>
<tableCaption confidence="0.999583">
Table 7: Human perception of translation quality.
</tableCaption>
<bodyText confidence="0.9915325">
quent passes: first, without being given the refer-
ence sentence (approximating fluency), then, with
the reference sentence (approximating adequacy).
The results are given in Table 7. Both annotators
preferred more sentences of our system overall,
but the difference is clearer for the fluency task.
</bodyText>
<sectionHeader confidence="0.997388" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99996835">
Compounds require special attention in SMT, es-
pecially when translating into a compounding lan-
guage. Compared with the baselines, all of our ex-
periments that included compound processing pro-
duced not only many more compounds matching
the reference exactly, but also many compounds
that did not occur in the training data. Taking
a closer look, we found that some of these new
compounds could only be produced due to the un-
derspecified representation we are using, which al-
lows us to generalise over occurrences of simple
words, compound modifiers and heads. Moreover,
we demonstrated that features derived from the
source language are a valuable source of informa-
tion for compound prediction: experiments were
significantly better compared with contrastive ex-
periments without these features. Additional man-
ual evaluations showed that compound processing
leads to improved translations where the improve-
ment is not captured by BLEU.
</bodyText>
<sectionHeader confidence="0.996178" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999601">
This work was supported by Deutsche For-
schungsgemeinschaft grants Models of Mor-
phosyntax for Statistical Machine Translation
(Phase 2) and Distributional Approaches to Se-
mantic Relatedness. We thank the anonymous re-
viewers for their comments and the annotators.
</bodyText>
<page confidence="0.997477">
586
</page>
<sectionHeader confidence="0.982111" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999856205357143">
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
HLT-NAACL’12: Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, volume 12, pages 34–35. Association
for Computational Linguistics.
Chris Dyer. 2010. A Formal Model of Ambiguity and
its Applications in Machine Translation. Phd disser-
tation, University of Maryland, USA.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word
Formation in SMT. In EACL’12: Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 664–
674. Association for Computational Linguistics.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the Fifth Work-
shop on Statistical Machine Translation, pages 224–
234. Association for Computational Linguistics.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
Edinburgh, UK, July. Association for Computational
Linguistics.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL ’03:
Proceedings of the 10th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 187–193, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL’07: Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics, Demonstration Session, pages 177–180. Asso-
ciation for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In ICML’01: Proceedings of the 18th
International Conference on Machine Learning.
Klaus Macherey, Andrew M. Dai, David Talbot,
Ashok C. Popat, and Franz Och. 2011. Language-
independent Compound Splitting with Morpholog-
ical Operations. In ACL ’11: Proceedings of the
49th annual meeting of the Association for Compu-
tational Linguistics, pages 1395–1404. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A Method for Automatic
Evaluation of Machine Translation. In ACL’02:
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 311–
318. Association for Computational Linguistics.
Maja Popovi´c, Daniel Stein, and Hermann Ney. 2006.
Statistical Machine Translation of German Com-
pound Words. In FinTAL’06: Proceedings of the
5th International Conference on Natural Language
Processing, pages 616–624. Springer Verlag.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition and Inflec-
tion. In LREC ’04: Proceedings of the 4th Confer-
ence on Language Resources and Evaluation, pages
1263–1266.
Andreas Stolcke. 2002. SRILM – an Extensible Lan-
guage Modelling Toolkit. In ICSLN’02: Proceed-
ings of the international conference on spoken lan-
guage processing, pages 901–904.
Sara Stymne and Nicola Cancedda. 2011. Productive
Generation of Compound Words in Statistical Ma-
chine Translation. In EMNLP’11: Proceedings of
the 6th Workshop on Statistical Machine Transla-
tion and Metrics MATR of the conference on Em-
pirical Methods in Natural Language Processing,
pages 250–260. Association for Computational Lin-
guistics.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of Morphological Analysis in Transla-
tion between German and English. In ACL’08: Pro-
ceedings of the 3rd workshop on statistical machine
translation of the 46th annual meeting of the Associ-
ation for Compuational Linguistics, pages 135–138.
Association for Computational Linguistics,.
Sara Stymne. 2009. A Comparison of Merging Strate-
gies for Translation of German Compounds. In
EACL ’09: Proceedings of the Student Research
Workshop of the 12th conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 61–69. Association for Computa-
tional Linguistics.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying Morphology Generation Models to
Machine Translation. In ACL’08: Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 514–522. Association for Computational
Linguistics.
Marion Weller, Alexander Fraser, and Sabine
Schulte im Walde. 2013. Using Subcatego-
rization Knowledge to Improve Case Prediction for
Translation to German. In ACL’13: Proceedings
of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 593–603.
Association for Computational Linguistics.
</reference>
<page confidence="0.997798">
587
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.797349">
<title confidence="0.9922955">How to Produce Unseen Teddy Bears: Improved Morphological Processing of Compounds in SMT</title>
<author confidence="0.995203">Fabienne Cap</author>
<author confidence="0.995203">Alexander</author>
<affiliation confidence="0.999005">CIS, University of Munich</affiliation>
<author confidence="0.966005">Marion Weller Aoife Cahill</author>
<affiliation confidence="0.97669">IMS, University of Stuttgart Educational Testing Service</affiliation>
<email confidence="0.860904">wellermn@ims.uni-stuttgart.deacahill@ets.org</email>
<abstract confidence="0.99901165">Compounding in morphologically rich languages is a highly productive process which often causes SMT approaches to fail because of unseen words. We present an approach for translation into a compounding language that splits compounds into simple words for training and, due to an underspecified representation, allows for free merging of simple words into compounds after translation. In contrast to previous approaches, we use features projected from the source language to predict compound mergings. We integrate our approach into end-to-end SMT and show that many compounds matching the reference translation are produced which did not appear in the training data. Additional manual evaluations support the usefulness of generalizing compound formation in SMT.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch Tuning Strategies for Statistical Machine Translation. In</title>
<date>2012</date>
<booktitle>HLT-NAACL’12: Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<volume>12</volume>
<pages>34--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25257" citStr="Cherry and Foster, 2012" startWordPosition="3990" endWordPosition="3993">Models were trained with the default settings of the Moses SMT toolkit, v1.0 (Koehn et al., 2007) using the data from the EACL 2009 workshop on statistical machine translation. All compound processing systems are trained and tuned identically, except using different CRF models for compound prediction. All training data was split and reduced to the underspecified representation described in Section 4. We used KenLM (Heafield, 2011) with SRILM (Stolcke, 2002) to train a 5-gram language model based on all available target language training data. For tuning, we used batch-mira with ‘- safe-hope’ (Cherry and Foster, 2012) and ran it separately for every experiment. We integrated the CRF-based merging of compounds into each iteration of tuning and scored each output with respect to an unsplit and lemmatised version of the tuning reference. Testing consists of: 1. translation into the split, underspecified German representation 2. compound merging using CRF models to predict recombination points 3. inflection of all words 7.1 SMT Results We use 1,025 sentences for tuning and 1,026 sentences for testing. The results are given in Table 4. We calculate BLEU scores (Papineni et al., 2002) and compare our systems to </context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In HLT-NAACL’12: Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, volume 12, pages 34–35. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
</authors>
<title>A Formal Model of Ambiguity and its Applications in Machine Translation. Phd dissertation,</title>
<date>2010</date>
<institution>University of Maryland, USA.</institution>
<contexts>
<context position="7367" citStr="Dyer (2010)" startWordPosition="1111" endWordPosition="1112">s, all improving translation quality when translating from a compounding to a noncompounding language. Several compound splitting approaches make use of substring corpus frequencies in order to find the optimal split points of a compound (e.g. Koehn and Knight (2003), who allowed only “(e)s” as filler letters). Stymne et al. (2008) use Koehn and Knight’s technique, include a larger list of possible modifier transformations and apply POS restrictions on the substrings, while Fritzinger and Fraser (2010) use a morphological analyser to find only linguistically motivated substrings. In contrast, Dyer (2010) presents a latticebased approach to encode different segmentations of words (instead of finding the one-best split). More recently, Macherey et al. (2011) presented a language-independent unsupervised approach in which filler letters and a list of words not to be split (e.g., named entities) are learned using phrase tables and Levenshtein distance. In contrast to splitting, the merging of compounds has received much less attention in the past. An early approach by Popovi´c et al. (2006) recombines compounds using a list of compounds and their parts. It thus never creates invalid German compou</context>
</contexts>
<marker>Dyer, 2010</marker>
<rawString>Chris Dyer. 2010. A Formal Model of Ambiguity and its Applications in Machine Translation. Phd dissertation, University of Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Marion Weller</author>
<author>Aoife Cahill</author>
<author>Fabienne Cap</author>
</authors>
<title>Modeling Inflection and Word Formation in SMT.</title>
<date>2012</date>
<booktitle>In EACL’12: Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>664--674</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1917" citStr="Fraser et al. (2012)" startWordPosition="281" endWordPosition="284">rred in the parallel training data. As parallel training data is limited, it is desirable to extract as much information from it as possible. We present an approach for compound processing in SMT, translating from English to German, that splits compounds prior to training (in order to access the individual words which together form the compound) and recombines them after translation. While compound splitting is a well-studied task, compound merging has not received as much attention in the past. We start from Stymne and Cancedda (2011), who used sequence models to predict compound merging and Fraser et al. (2012) who, in addition, generalise over German inflection. Our new contributions are: (i) We project features from the source language to support compound merging predictions. As the source language input is fluent, these features are more reliable than features derived from target language SMT output. (ii) We reduce compound parts to an underspecified representation which allows for maximal generalisation. (iii) We present a detailed manual evaluation methodology which shows that we obtain improved compound translations. We evaluated compound processing both on held-out split data and in end-to-en</context>
<context position="10992" citStr="Fraser et al. (2012)" startWordPosition="1680" endWordPosition="1683">Inflations”), they can not be generalised to compound heads or simple words, nor can inflectional variants of compound heads or simple words be created (e.g. if “Rate” had only been observed in nominative form in the training data, the genitive “Raten” could not be produced). The underspecified representation we are using allows for maximal generalisation over word parts independent of their position of occurrence or inflectional realisations. Moreover, their experiments were limited to predicting compounds on held-out data; no results were reported for using their approach in translation. In Fraser et al. (2012) we re-implemented the approach of Stymne and Cancedda (2011), combined it with inflection prediction and applied it to a translation task. However, compound merging was restricted to a list of compounds and parts. Our present work facilitates more independent combination. Toutanova et al. (2008) and Weller et al. (2013) used source language features for target language inflection, but to our knowledge, none of these works applied source language features for compound merging. 4 Step 1: Underspecified Representation In order to enhance translation model accuracy, it is reasonable to have simil</context>
<context position="21126" citStr="Fraser et al. (2012)" startWordPosition="3289" endWordPosition="3292"> occurred as such in the training data. Imagine that “Ort” occurred only as compound head or as a single word in the training data. Using SMOR, we are still able to create the correct form of the modifier, including the required filler letter: “Orts”. This ability distinguishes our approach from pre5Note that “f¨ur etwas aufkommen” (lit. “for sth. arise”, idiom.: “to pay for sth.”) is an idiomatic expression. vious approaches: Stymne and Cancedda (2011) do not reduce modifiers to their base forms6 (they can only create new compounds when the modifier occurred as such in the training data) and Fraser et al. (2012) use a list for merging. Finally, we use the system described in Fraser et al. (2012) to inflect the entire text. 6 Accuracy of Compound Prediction We trained CRF models on the parallel training data (~40 million words)7 of the EACL 2009 workshop on statistical machine translation8 using different feature (sub)sets, cf. the “Experiment” column in Table 1 above. We examined the reliability of the CRF compound prediction models by applying them to held-out data: 1. split the German wmt2009 tuning data set 2. remember compound split points 3. predict merging with CRF models 4. combine predicted w</context>
<context position="25977" citStr="Fraser et al. (2012)" startWordPosition="4108" endWordPosition="4111">ach iteration of tuning and scored each output with respect to an unsplit and lemmatised version of the tuning reference. Testing consists of: 1. translation into the split, underspecified German representation 2. compound merging using CRF models to predict recombination points 3. inflection of all words 7.1 SMT Results We use 1,025 sentences for tuning and 1,026 sentences for testing. The results are given in Table 4. We calculate BLEU scores (Papineni et al., 2002) and compare our systems to a RAW baseline (built following the instructions of the shared task) and a baseline very similar to Fraser et al. (2012), using a lemmatised representation of words for decoding, re-inflecting them after translation, but without compound processing (UNSPLIT). Table 4 shows that only UNSPLIT and STR (source language and a reduced set of target language features) are significantly9 improving over the RAW baseline. They also significantly outperform all other systems, except ST (full source and target language feature set). The difference between STR (14.61) and the UNSPLIT baseline (14.74) is not statistically significant. 9We used pair-wise bootstrap resampling with sample size 1000 and p-value 0.05, from: http:</context>
</contexts>
<marker>Fraser, Weller, Cahill, Cap, 2012</marker>
<rawString>Alexander Fraser, Marion Weller, Aoife Cahill, and Fabienne Cap. 2012. Modeling Inflection and Word Formation in SMT. In EACL’12: Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 664– 674. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Fritzinger</author>
<author>Alexander Fraser</author>
</authors>
<title>How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fifth Workshop on Statistical Machine Translation,</booktitle>
<pages>224--234</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7263" citStr="Fritzinger and Fraser (2010)" startWordPosition="1094" endWordPosition="1097">s report on previous approaches for both of these tasks. In the past, there have been numerous attempts to split compounds, all improving translation quality when translating from a compounding to a noncompounding language. Several compound splitting approaches make use of substring corpus frequencies in order to find the optimal split points of a compound (e.g. Koehn and Knight (2003), who allowed only “(e)s” as filler letters). Stymne et al. (2008) use Koehn and Knight’s technique, include a larger list of possible modifier transformations and apply POS restrictions on the substrings, while Fritzinger and Fraser (2010) use a morphological analyser to find only linguistically motivated substrings. In contrast, Dyer (2010) presents a latticebased approach to encode different segmentations of words (instead of finding the one-best split). More recently, Macherey et al. (2011) presented a language-independent unsupervised approach in which filler letters and a list of words not to be split (e.g., named entities) are learned using phrase tables and Levenshtein distance. In contrast to splitting, the merging of compounds has received much less attention in the past. An early approach by Popovi´c et al. (2006) rec</context>
<context position="12655" citStr="Fritzinger and Fraser (2010)" startWordPosition="1931" endWordPosition="1934">an later predict the merging of simple words into compounds and the inflection of the words, we store all of the morphological information stripped from the underspecified representation. Note that erroneous over-splitting might make the correct merging of compounds difficult3 (or even impossible), due to the number of correct decisions required. For example, it requires only 1 correct prediction to recombine “Niederschlag|Menge” into “Niederschlagsmenge” (“amount of precipitation”) but 3 for the wrong split into “nie|der|Schlag|Menge” (“never|the|hit|amount”). We use the compound splitter of Fritzinger and Fraser (2010), who have shown that using a rule-based morphological analyser (SMOR, Schmid et al. (2004)) drastically reduced the number of erroneous splits when compared to the frequency-based approach of Koehn and Knight (2003). However, we adapted it to work on tokens: some words can, depending on their context, either be interpreted as named entities or common nouns, e.g., “Dinkelacker” (a German beer brand or “spelt|field”).4 We parsed the training data and use the parser’s decisions to identify proper names, see “Baumeister” in Figure 2. After splitting, we use SMOR to reduce words to lemmas, keeping</context>
</contexts>
<marker>Fritzinger, Fraser, 2010</marker>
<rawString>Fabienne Fritzinger and Alexander Fraser. 2010. How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing. In Proceedings of the Fifth Workshop on Statistical Machine Translation, pages 224– 234. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and Smaller Language Model Queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, UK,</location>
<contexts>
<context position="25067" citStr="Heafield, 2011" startWordPosition="3961" endWordPosition="3962"> more use of surface words and POS tags, cf. TR and STR in Table 3) instead. 7 Translation Performance We integrated our compound processing pipeline into an end-to-end SMT system. Models were trained with the default settings of the Moses SMT toolkit, v1.0 (Koehn et al., 2007) using the data from the EACL 2009 workshop on statistical machine translation. All compound processing systems are trained and tuned identically, except using different CRF models for compound prediction. All training data was split and reduced to the underspecified representation described in Section 4. We used KenLM (Heafield, 2011) with SRILM (Stolcke, 2002) to train a 5-gram language model based on all available target language training data. For tuning, we used batch-mira with ‘- safe-hope’ (Cherry and Foster, 2012) and ran it separately for every experiment. We integrated the CRF-based merging of compounds into each iteration of tuning and scored each output with respect to an unsplit and lemmatised version of the tuning reference. Testing consists of: 1. translation into the split, underspecified German representation 2. compound merging using CRF models to predict recombination points 3. inflection of all words 7.1</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, UK, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical Methods for Compound Splitting. In</title>
<date>2003</date>
<booktitle>EACL ’03: Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>187--193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7023" citStr="Koehn and Knight (2003)" startWordPosition="1058" endWordPosition="1061">ompounding language includes both compound split1Most newly appearing words in German are compounds. 2~30% of the word types and ~77% of the compound types we identified in our training data occurred ≤ 3 times. ting and merging, we thus report on previous approaches for both of these tasks. In the past, there have been numerous attempts to split compounds, all improving translation quality when translating from a compounding to a noncompounding language. Several compound splitting approaches make use of substring corpus frequencies in order to find the optimal split points of a compound (e.g. Koehn and Knight (2003), who allowed only “(e)s” as filler letters). Stymne et al. (2008) use Koehn and Knight’s technique, include a larger list of possible modifier transformations and apply POS restrictions on the substrings, while Fritzinger and Fraser (2010) use a morphological analyser to find only linguistically motivated substrings. In contrast, Dyer (2010) presents a latticebased approach to encode different segmentations of words (instead of finding the one-best split). More recently, Macherey et al. (2011) presented a language-independent unsupervised approach in which filler letters and a list of words n</context>
<context position="12871" citStr="Koehn and Knight (2003)" startWordPosition="1966" endWordPosition="1969">g might make the correct merging of compounds difficult3 (or even impossible), due to the number of correct decisions required. For example, it requires only 1 correct prediction to recombine “Niederschlag|Menge” into “Niederschlagsmenge” (“amount of precipitation”) but 3 for the wrong split into “nie|der|Schlag|Menge” (“never|the|hit|amount”). We use the compound splitter of Fritzinger and Fraser (2010), who have shown that using a rule-based morphological analyser (SMOR, Schmid et al. (2004)) drastically reduced the number of erroneous splits when compared to the frequency-based approach of Koehn and Knight (2003). However, we adapted it to work on tokens: some words can, depending on their context, either be interpreted as named entities or common nouns, e.g., “Dinkelacker” (a German beer brand or “spelt|field”).4 We parsed the training data and use the parser’s decisions to identify proper names, see “Baumeister” in Figure 2. After splitting, we use SMOR to reduce words to lemmas, keeping morphological features like gender or number, and stripping features like case, as illustrated for “ ¨Olexporteure” (“oil exporters”): 3In contrast, they may not hurt translation quality in the other direction, wher</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical Methods for Compound Splitting. In EACL ’03: Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 187–193, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation. In</title>
<date>2007</date>
<booktitle>ACL’07: Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Demonstration Session,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24730" citStr="Koehn et al., 2007" startWordPosition="3906" endWordPosition="3909">translated the whole data set using an SMT system that was trained on the same data set. This way, the training data was less fluent than in its original format, but still of higher quality than SMT output of unseen data. In contrast, we left the training data as it was, but strongly reduced the feature set for CRF model training (e.g., no more use of surface words and POS tags, cf. TR and STR in Table 3) instead. 7 Translation Performance We integrated our compound processing pipeline into an end-to-end SMT system. Models were trained with the default settings of the Moses SMT toolkit, v1.0 (Koehn et al., 2007) using the data from the EACL 2009 workshop on statistical machine translation. All compound processing systems are trained and tuned identically, except using different CRF models for compound prediction. All training data was split and reduced to the underspecified representation described in Section 4. We used KenLM (Heafield, 2011) with SRILM (Stolcke, 2002) to train a 5-gram language model based on all available target language training data. For tuning, we used batch-mira with ‘- safe-hope’ (Cherry and Foster, 2012) and ran it separately for every experiment. We integrated the CRF-based </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL’07: Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Demonstration Session, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In</title>
<date>2001</date>
<booktitle>ICML’01: Proceedings of the 18th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="9899" citStr="Lafferty et al., 2001" startWordPosition="1508" endWordPosition="1511">aum Gas&lt;NN&gt;Traum&lt;+NN&gt; Gast&lt;NN&gt;Raum&lt;+NN&gt; Gastraum 3.74 Gas|Traum 8.34 Gast|Raum 8.59 Figure 2: Compound splitting pipeline 1) The original text is parsed with BITPAR to get unambiguous POS tags, 2) The original text is then true-cased using the most frequent casing for each word and BITPAR tags are added, 3) All words are analysed with SMOR, analyses are filtered using BITPAR tags (only bold-faced analyses are kept), 4) If several splitting options remain, the geometric mean of the word (part) frequencies is used to disambiguate them. proach was extended to make use of a CRF sequence labeller (Lafferty et al., 2001) in order to find reasonable merging points. Besides the words and their POS, many different target language frequency features were defined to train the CRF. This approach can even produce new compounds unseen in the training data, provided that the modifiers occurred in modifier position of a compound and heads occurred as heads or even as simple words with the same inflectional endings. However, as former compound modifiers were left with their filler letters (cf. “Inflations”), they can not be generalised to compound heads or simple words, nor can inflectional variants of compound heads or</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML’01: Proceedings of the 18th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Macherey</author>
<author>Andrew M Dai</author>
<author>David Talbot</author>
<author>Ashok C Popat</author>
<author>Franz Och</author>
</authors>
<title>Languageindependent Compound Splitting with Morphological Operations.</title>
<date>2011</date>
<booktitle>In ACL ’11: Proceedings of the 49th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>1395--1404</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7522" citStr="Macherey et al. (2011)" startWordPosition="1132" endWordPosition="1135">e use of substring corpus frequencies in order to find the optimal split points of a compound (e.g. Koehn and Knight (2003), who allowed only “(e)s” as filler letters). Stymne et al. (2008) use Koehn and Knight’s technique, include a larger list of possible modifier transformations and apply POS restrictions on the substrings, while Fritzinger and Fraser (2010) use a morphological analyser to find only linguistically motivated substrings. In contrast, Dyer (2010) presents a latticebased approach to encode different segmentations of words (instead of finding the one-best split). More recently, Macherey et al. (2011) presented a language-independent unsupervised approach in which filler letters and a list of words not to be split (e.g., named entities) are learned using phrase tables and Levenshtein distance. In contrast to splitting, the merging of compounds has received much less attention in the past. An early approach by Popovi´c et al. (2006) recombines compounds using a list of compounds and their parts. It thus never creates invalid German compounds, but on the other hand it is limited to the coverage of the list. Moreover, in some contexts a merging in the list may still be wrong, cf. EXAMPLE (3) </context>
<context position="13604" citStr="Macherey et al. (2011)" startWordPosition="2087" endWordPosition="2090">s named entities or common nouns, e.g., “Dinkelacker” (a German beer brand or “spelt|field”).4 We parsed the training data and use the parser’s decisions to identify proper names, see “Baumeister” in Figure 2. After splitting, we use SMOR to reduce words to lemmas, keeping morphological features like gender or number, and stripping features like case, as illustrated for “ ¨Olexporteure” (“oil exporters”): 3In contrast, they may not hurt translation quality in the other direction, where phrase-based SMT is likely to learn the split words as a phrase and thus recover from that error. 4Note that Macherey et al. (2011) blocked splitting of words which can be used as named entities, independent of context, which is less general than our solution. 581 No. Feature Description Example Experiment SC T TR 1SC surface form of the word string: Arbeit&lt;+NN&gt;&lt;Fem&gt;&lt;Sg&gt; X X 2SC main part of speech of the word (from the parser) string: +NN X X 3SC word occurs in a bigram with the next word frequency: 0 X X 4SC word combined to a compound with the next word frequency: 10,000 X X X 5SC word occurs in modifier position of a compound frequency: 100,000 X X 6SC word occurs in a head position of a compound frequency: 10,000 X X</context>
</contexts>
<marker>Macherey, Dai, Talbot, Popat, Och, 2011</marker>
<rawString>Klaus Macherey, Andrew M. Dai, David Talbot, Ashok C. Popat, and Franz Och. 2011. Languageindependent Compound Splitting with Morphological Operations. In ACL ’11: Proceedings of the 49th annual meeting of the Association for Computational Linguistics, pages 1395–1404. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: A Method for Automatic Evaluation of Machine Translation. In</title>
<date>2002</date>
<booktitle>ACL’02: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25829" citStr="Papineni et al., 2002" startWordPosition="4082" endWordPosition="4085">-mira with ‘- safe-hope’ (Cherry and Foster, 2012) and ran it separately for every experiment. We integrated the CRF-based merging of compounds into each iteration of tuning and scored each output with respect to an unsplit and lemmatised version of the tuning reference. Testing consists of: 1. translation into the split, underspecified German representation 2. compound merging using CRF models to predict recombination points 3. inflection of all words 7.1 SMT Results We use 1,025 sentences for tuning and 1,026 sentences for testing. The results are given in Table 4. We calculate BLEU scores (Papineni et al., 2002) and compare our systems to a RAW baseline (built following the instructions of the shared task) and a baseline very similar to Fraser et al. (2012), using a lemmatised representation of words for decoding, re-inflecting them after translation, but without compound processing (UNSPLIT). Table 4 shows that only UNSPLIT and STR (source language and a reduced set of target language features) are significantly9 improving over the RAW baseline. They also significantly outperform all other systems, except ST (full source and target language feature set). The difference between STR (14.61) and the UN</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: A Method for Automatic Evaluation of Machine Translation. In ACL’02: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311– 318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Daniel Stein</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical Machine Translation of German Compound Words. In</title>
<date>2006</date>
<booktitle>FinTAL’06: Proceedings of the 5th International Conference on Natural Language Processing,</booktitle>
<pages>616--624</pages>
<publisher>Springer Verlag.</publisher>
<marker>Popovi´c, Stein, Ney, 2006</marker>
<rawString>Maja Popovi´c, Daniel Stein, and Hermann Ney. 2006. Statistical Machine Translation of German Compound Words. In FinTAL’06: Proceedings of the 5th International Conference on Natural Language Processing, pages 616–624. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Arne Fitschen</author>
<author>Ulrich Heid</author>
</authors>
<title>SMOR: A German Computational Morphology Covering Derivation, Composition and Inflection.</title>
<date>2004</date>
<booktitle>In LREC ’04: Proceedings of the 4th Conference on Language Resources and Evaluation,</booktitle>
<pages>1263--1266</pages>
<contexts>
<context position="12746" citStr="Schmid et al. (2004)" startWordPosition="1946" endWordPosition="1949">re all of the morphological information stripped from the underspecified representation. Note that erroneous over-splitting might make the correct merging of compounds difficult3 (or even impossible), due to the number of correct decisions required. For example, it requires only 1 correct prediction to recombine “Niederschlag|Menge” into “Niederschlagsmenge” (“amount of precipitation”) but 3 for the wrong split into “nie|der|Schlag|Menge” (“never|the|hit|amount”). We use the compound splitter of Fritzinger and Fraser (2010), who have shown that using a rule-based morphological analyser (SMOR, Schmid et al. (2004)) drastically reduced the number of erroneous splits when compared to the frequency-based approach of Koehn and Knight (2003). However, we adapted it to work on tokens: some words can, depending on their context, either be interpreted as named entities or common nouns, e.g., “Dinkelacker” (a German beer brand or “spelt|field”).4 We parsed the training data and use the parser’s decisions to identify proper names, see “Baumeister” in Figure 2. After splitting, we use SMOR to reduce words to lemmas, keeping morphological features like gender or number, and stripping features like case, as illustr</context>
</contexts>
<marker>Schmid, Fitschen, Heid, 2004</marker>
<rawString>Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004. SMOR: A German Computational Morphology Covering Derivation, Composition and Inflection. In LREC ’04: Proceedings of the 4th Conference on Language Resources and Evaluation, pages 1263–1266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an Extensible Language Modelling Toolkit. In</title>
<date>2002</date>
<booktitle>ICSLN’02: Proceedings of the international conference on spoken language processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="25094" citStr="Stolcke, 2002" startWordPosition="3965" endWordPosition="3966">nd POS tags, cf. TR and STR in Table 3) instead. 7 Translation Performance We integrated our compound processing pipeline into an end-to-end SMT system. Models were trained with the default settings of the Moses SMT toolkit, v1.0 (Koehn et al., 2007) using the data from the EACL 2009 workshop on statistical machine translation. All compound processing systems are trained and tuned identically, except using different CRF models for compound prediction. All training data was split and reduced to the underspecified representation described in Section 4. We used KenLM (Heafield, 2011) with SRILM (Stolcke, 2002) to train a 5-gram language model based on all available target language training data. For tuning, we used batch-mira with ‘- safe-hope’ (Cherry and Foster, 2012) and ran it separately for every experiment. We integrated the CRF-based merging of compounds into each iteration of tuning and scored each output with respect to an unsplit and lemmatised version of the tuning reference. Testing consists of: 1. translation into the split, underspecified German representation 2. compound merging using CRF models to predict recombination points 3. inflection of all words 7.1 SMT Results We use 1,025 s</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an Extensible Language Modelling Toolkit. In ICSLN’02: Proceedings of the international conference on spoken language processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Nicola Cancedda</author>
</authors>
<title>Productive Generation of Compound Words in Statistical Machine Translation. In</title>
<date>2011</date>
<booktitle>EMNLP’11: Proceedings of the 6th Workshop on Statistical Machine Translation and Metrics MATR of the conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>250--260</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1838" citStr="Stymne and Cancedda (2011)" startWordPosition="266" endWordPosition="270"> translation (SMT) approaches, because words can only be translated as they have occurred in the parallel training data. As parallel training data is limited, it is desirable to extract as much information from it as possible. We present an approach for compound processing in SMT, translating from English to German, that splits compounds prior to training (in order to access the individual words which together form the compound) and recombines them after translation. While compound splitting is a well-studied task, compound merging has not received as much attention in the past. We start from Stymne and Cancedda (2011), who used sequence models to predict compound merging and Fraser et al. (2012) who, in addition, generalise over German inflection. Our new contributions are: (i) We project features from the source language to support compound merging predictions. As the source language input is fluent, these features are more reliable than features derived from target language SMT output. (ii) We reduce compound parts to an underspecified representation which allows for maximal generalisation. (iii) We present a detailed manual evaluation methodology which shows that we obtain improved compound translations</context>
<context position="8560" citStr="Stymne and Cancedda (2011)" startWordPosition="1308" endWordPosition="1311">ver creates invalid German compounds, but on the other hand it is limited to the coverage of the list. Moreover, in some contexts a merging in the list may still be wrong, cf. EXAMPLE (3) in Section 5 below. The approach of Stymne (2009) makes use of a factored model, with a special POS-markup for compound modifiers, derived from the POS of the whole compound. This markup enables sound mergings of compound parts after translation if the POS of the candidate modifier (X-Part) matches the POS of the candidate compound head (X): InflationsINPart + Rate|N = Inflationsrate|N (“inflation rate”). In Stymne and Cancedda (2011) the factored apHaus+Boot = Hausboot (“house boat”) Ort+s+Zeit = Ortszeit (“local time”) Kirche-e+Turm = Kirchturm (“church tower”) Kriterium+Liste = Kriterienliste (“criteria list”) 580 0) Original Text 1) Bitpar Parsed Text 2) True Casing 3) SMOR Analysis 4) Disambiguation Amerikanische Medien ... Der Gastraum des ... Tim Baumeister besiegt ... ... ... ... (S(NP(ADJA Amerikanische) (NN Medien)...)) ... (S(NP(ART Der) (NN Gastraum) (ART des)...)) ... (S(NP(PN(NE Tim)(NE Baumeister))(VV besiegt)...)) ... amerikanische Medien ... ADJA NN der Gastraum des ... ART NN ART Tim Baumeister besiegt ..</context>
<context position="11053" citStr="Stymne and Cancedda (2011)" startWordPosition="1689" endWordPosition="1692">eads or simple words, nor can inflectional variants of compound heads or simple words be created (e.g. if “Rate” had only been observed in nominative form in the training data, the genitive “Raten” could not be produced). The underspecified representation we are using allows for maximal generalisation over word parts independent of their position of occurrence or inflectional realisations. Moreover, their experiments were limited to predicting compounds on held-out data; no results were reported for using their approach in translation. In Fraser et al. (2012) we re-implemented the approach of Stymne and Cancedda (2011), combined it with inflection prediction and applied it to a translation task. However, compound merging was restricted to a list of compounds and parts. Our present work facilitates more independent combination. Toutanova et al. (2008) and Weller et al. (2013) used source language features for target language inflection, but to our knowledge, none of these works applied source language features for compound merging. 4 Step 1: Underspecified Representation In order to enhance translation model accuracy, it is reasonable to have similar degrees of morphological richness between source and targe</context>
<context position="14722" citStr="Stymne and Cancedda (2011)" startWordPosition="2285" endWordPosition="2288"> of a compound frequency: 100,000 X X 6SC word occurs in a head position of a compound frequency: 10,000 X X 7SC word occurs in modifier position vs. simplex string: P&gt;W (P= 5SC, W= 100,000) X 8SC word occurs in head position vs. simplex string: S&lt;W (S= 6SC, W= 100,000) X 7SC+ word occurs in modifier position vs. simplex ratio: 10 (10**ceil(log10(5SC/W))) X X 8SC+ word occurs in head position vs. simplex ratio: 1 (10**ceil(log10(6SC/W))) X X 9N different head types the word can combine with number: 10,000 X X Table 1: Target language CRF features for compound merging. SC = features taken from Stymne and Cancedda (2011), SC+ = improved versions, N = new feature. Experiments: SC = re-implementation of Stymne and Cancedda (2011), T= use full Target feature set, TR = use Target features, but only a Reduced set. EXAMPLE (2) compound Öl&lt;NN&gt;Exporteur&lt;+NN&gt;&lt;Masc&gt;&lt;Nom&gt;&lt;Pl&gt; Öl&lt;+NN&gt;&lt;Neut&gt;&lt;Sg&gt; Exporteur&lt;+NN&gt; &lt;Masc&gt;&lt;Pl&gt; modifier head While the former compound head (“Exporteure”) automatically inherits all morphological features of the compound as a whole, the features of the modifier need to be derived from SMOR in an additional step. We need to ensure that the representation of the modifier is identical to the same word</context>
<context position="16006" citStr="Stymne and Cancedda (2011)" startWordPosition="2468" endWordPosition="2471">ralisation over compound parts. 5 Step 2: Compound Merging After translation from English into the underspecified German representation, post-processing is required to transform the output back into fluent, morphologically fully specified German. First, compounds need to be merged where appropriate, e.g., “Hausboote” (“house boats”): Haus&lt;+NN&gt;&lt;Neut&gt;&lt;Sg&gt; + Boot&lt;+NN&gt;&lt;Neut&gt;&lt;Pl&gt; → Haus&lt;NN&gt;Boot&lt;+NN&gt;&lt;Neut&gt;&lt;Pl&gt; (merged) and second, all words need to be inflected: Haus&lt;NN&gt;Boot&lt;+NN&gt;&lt;Neut&gt;&lt;Acc&gt;&lt;Pl&gt; → Hausbooten (inflected) 5.1 Target Language Features To decide which words should be combined, we follow Stymne and Cancedda (2011) who used CRFs for this task. The features we derived from the target language to train CRF models are listed in Table 1. We adapted features No. 1-8 from Stymne and Cancedda (2011). Then, we modified two features (7+8) and created a new feature indicating the productivity of a modifier (9N). 5.2 Projecting Source Language Features We also use new features derived from the English source language input, which is coherent and fluent. This makes features derived from it more reliable than the target language features derived from disfluent SMT output. Moreover, source language features might sup</context>
<context position="20963" citStr="Stymne and Cancedda (2011)" startWordPosition="3260" endWordPosition="3263">t”+“Zeit” = “Ortszeit” (“local time”). We use SMOR to generate compounds from a combination of simple words. This allows us to create compounds with modifiers that never occurred as such in the training data. Imagine that “Ort” occurred only as compound head or as a single word in the training data. Using SMOR, we are still able to create the correct form of the modifier, including the required filler letter: “Orts”. This ability distinguishes our approach from pre5Note that “f¨ur etwas aufkommen” (lit. “for sth. arise”, idiom.: “to pay for sth.”) is an idiomatic expression. vious approaches: Stymne and Cancedda (2011) do not reduce modifiers to their base forms6 (they can only create new compounds when the modifier occurred as such in the training data) and Fraser et al. (2012) use a list for merging. Finally, we use the system described in Fraser et al. (2012) to inflect the entire text. 6 Accuracy of Compound Prediction We trained CRF models on the parallel training data (~40 million words)7 of the EACL 2009 workshop on statistical machine translation8 using different feature (sub)sets, cf. the “Experiment” column in Table 1 above. We examined the reliability of the CRF compound prediction models by appl</context>
<context position="23000" citStr="Stymne and Cancedda (2011)" startWordPosition="3598" endWordPosition="3601">LE (1)). 7However, target language feature frequencies are derived from the monolingual training data, ~146 million words. 8http://www.statmt.org/wmt09 583 exp to be all correct wrong wrong not merging precision recall f-score merged merged merged merged merged wrong SC 1,047 997 921 73 121 3 92.38% 88.13% 90.21% T 1,047 979 916 59 128 4 93.56% 87.40% 90.38% ST 1,047 976 917 55 126 4 93.95% 87.58% 90.66% TR 1,047 893 836 52 204 5 93.62% 80.00% 86.27% STR 1,047 930 866 58 172 6 93.12% 82.95% 87.74% Table 3: Compound production accuracies of CRF models on held-out data: SC: re-implementation of Stymne and Cancedda (2011); T: all target language features, including a new one (cf. Table 1); ST = all Source and Target language features; TR: only a reduced set of target language features; STR: TR, plus all source language features given in Table 2. exp BLEU SCORES #compounds found mert.log BLEU RTS all ref new new* RAW 14.88 14.25 1.0054 646 175 n.a. n.a. UNSPLIT 15.86 14.74 0.9964 661 185 n.a. n.a. SC 15.44 14.45 0.9870 882 241 47 8 T 15.56 14.32 0.9634 845 251 47 8 ST 15.33 14.51 0.9760 820 248 46 9 TR 15.24 14.26 0.9710 753 234 44 5 STR 15.37 14.61 0.9884 758 239 43 7 #compounds in reference text: 1,105 1,105 </context>
</contexts>
<marker>Stymne, Cancedda, 2011</marker>
<rawString>Sara Stymne and Nicola Cancedda. 2011. Productive Generation of Compound Words in Statistical Machine Translation. In EMNLP’11: Proceedings of the 6th Workshop on Statistical Machine Translation and Metrics MATR of the conference on Empirical Methods in Natural Language Processing, pages 250–260. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Maria Holmqvist</author>
<author>Lars Ahrenberg</author>
</authors>
<title>Effects of Morphological Analysis in Translation between German and English.</title>
<date>2008</date>
<booktitle>In ACL’08: Proceedings of the 3rd workshop on statistical machine translation of the 46th annual meeting of the Association for Compuational Linguistics,</booktitle>
<pages>135--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,.</institution>
<contexts>
<context position="7089" citStr="Stymne et al. (2008)" startWordPosition="1069" endWordPosition="1072"> words in German are compounds. 2~30% of the word types and ~77% of the compound types we identified in our training data occurred ≤ 3 times. ting and merging, we thus report on previous approaches for both of these tasks. In the past, there have been numerous attempts to split compounds, all improving translation quality when translating from a compounding to a noncompounding language. Several compound splitting approaches make use of substring corpus frequencies in order to find the optimal split points of a compound (e.g. Koehn and Knight (2003), who allowed only “(e)s” as filler letters). Stymne et al. (2008) use Koehn and Knight’s technique, include a larger list of possible modifier transformations and apply POS restrictions on the substrings, while Fritzinger and Fraser (2010) use a morphological analyser to find only linguistically motivated substrings. In contrast, Dyer (2010) presents a latticebased approach to encode different segmentations of words (instead of finding the one-best split). More recently, Macherey et al. (2011) presented a language-independent unsupervised approach in which filler letters and a list of words not to be split (e.g., named entities) are learned using phrase tab</context>
</contexts>
<marker>Stymne, Holmqvist, Ahrenberg, 2008</marker>
<rawString>Sara Stymne, Maria Holmqvist, and Lars Ahrenberg. 2008. Effects of Morphological Analysis in Translation between German and English. In ACL’08: Proceedings of the 3rd workshop on statistical machine translation of the 46th annual meeting of the Association for Compuational Linguistics, pages 135–138. Association for Computational Linguistics,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
</authors>
<title>A Comparison of Merging Strategies for Translation of German Compounds.</title>
<date>2009</date>
<booktitle>In EACL ’09: Proceedings of the Student Research Workshop of the 12th conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>61--69</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8171" citStr="Stymne (2009)" startWordPosition="1247" endWordPosition="1248">supervised approach in which filler letters and a list of words not to be split (e.g., named entities) are learned using phrase tables and Levenshtein distance. In contrast to splitting, the merging of compounds has received much less attention in the past. An early approach by Popovi´c et al. (2006) recombines compounds using a list of compounds and their parts. It thus never creates invalid German compounds, but on the other hand it is limited to the coverage of the list. Moreover, in some contexts a merging in the list may still be wrong, cf. EXAMPLE (3) in Section 5 below. The approach of Stymne (2009) makes use of a factored model, with a special POS-markup for compound modifiers, derived from the POS of the whole compound. This markup enables sound mergings of compound parts after translation if the POS of the candidate modifier (X-Part) matches the POS of the candidate compound head (X): InflationsINPart + Rate|N = Inflationsrate|N (“inflation rate”). In Stymne and Cancedda (2011) the factored apHaus+Boot = Hausboot (“house boat”) Ort+s+Zeit = Ortszeit (“local time”) Kirche-e+Turm = Kirchturm (“church tower”) Kriterium+Liste = Kriterienliste (“criteria list”) 580 0) Original Text 1) Bitp</context>
</contexts>
<marker>Stymne, 2009</marker>
<rawString>Sara Stymne. 2009. A Comparison of Merging Strategies for Translation of German Compounds. In EACL ’09: Proceedings of the Student Research Workshop of the 12th conference of the European Chapter of the Association for Computational Linguistics, pages 61–69. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
<author>Achim Ruopp</author>
</authors>
<title>Applying Morphology Generation Models to Machine Translation. In</title>
<date>2008</date>
<booktitle>ACL’08: Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>514--522</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11289" citStr="Toutanova et al. (2008)" startWordPosition="1727" endWordPosition="1730">d representation we are using allows for maximal generalisation over word parts independent of their position of occurrence or inflectional realisations. Moreover, their experiments were limited to predicting compounds on held-out data; no results were reported for using their approach in translation. In Fraser et al. (2012) we re-implemented the approach of Stymne and Cancedda (2011), combined it with inflection prediction and applied it to a translation task. However, compound merging was restricted to a list of compounds and parts. Our present work facilitates more independent combination. Toutanova et al. (2008) and Weller et al. (2013) used source language features for target language inflection, but to our knowledge, none of these works applied source language features for compound merging. 4 Step 1: Underspecified Representation In order to enhance translation model accuracy, it is reasonable to have similar degrees of morphological richness between source and target language. We thus reduce the German target language training data to an underspecified representation: we split compounds, and lemmatise all words (except verbs). All occurrences of simple words, former compound modifiers or heads hav</context>
</contexts>
<marker>Toutanova, Suzuki, Ruopp, 2008</marker>
<rawString>Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. 2008. Applying Morphology Generation Models to Machine Translation. In ACL’08: Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 514–522. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marion Weller</author>
<author>Alexander Fraser</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Using Subcategorization Knowledge to Improve Case Prediction for Translation to German. In</title>
<date>2013</date>
<booktitle>ACL’13: Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>593--603</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11314" citStr="Weller et al. (2013)" startWordPosition="1732" endWordPosition="1735">g allows for maximal generalisation over word parts independent of their position of occurrence or inflectional realisations. Moreover, their experiments were limited to predicting compounds on held-out data; no results were reported for using their approach in translation. In Fraser et al. (2012) we re-implemented the approach of Stymne and Cancedda (2011), combined it with inflection prediction and applied it to a translation task. However, compound merging was restricted to a list of compounds and parts. Our present work facilitates more independent combination. Toutanova et al. (2008) and Weller et al. (2013) used source language features for target language inflection, but to our knowledge, none of these works applied source language features for compound merging. 4 Step 1: Underspecified Representation In order to enhance translation model accuracy, it is reasonable to have similar degrees of morphological richness between source and target language. We thus reduce the German target language training data to an underspecified representation: we split compounds, and lemmatise all words (except verbs). All occurrences of simple words, former compound modifiers or heads have the same representation</context>
</contexts>
<marker>Weller, Fraser, Walde, 2013</marker>
<rawString>Marion Weller, Alexander Fraser, and Sabine Schulte im Walde. 2013. Using Subcategorization Knowledge to Improve Case Prediction for Translation to German. In ACL’13: Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 593–603. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>