<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001474">
<title confidence="0.9991765">
A Stacked Sub-Word Model for Joint Chinese Word Segmentation and
Part-of-Speech Tagging
</title>
<author confidence="0.998563">
Weiwei Sun
</author>
<affiliation confidence="0.8780725">
Department of Computational Linguistics, Saarland University
German Research Center for Artificial Intelligence (DFKI)
</affiliation>
<address confidence="0.643324">
D-66123, Saarbr¨ucken, Germany
</address>
<email confidence="0.99299">
wsun@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.997336" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999345391304348">
The large combined search space of joint word
segmentation and Part-of-Speech (POS) tag-
ging makes efficient decoding very hard. As a
result, effective high order features represent-
ing rich contexts are inconvenient to use. In
this work, we propose a novel stacked sub-
word model for this task, concerning both ef-
ficiency and effectiveness. Our solution is
a two step process. First, one word-based
segmenter, one character-based segmenter and
one local character classifier are trained to pro-
duce coarse segmentation and POS informa-
tion. Second, the outputs of the three pre-
dictors are merged into sub-word sequences,
which are further bracketed and labeled with
POS tags by a fine-grained sub-word tag-
ger. The coarse-to-fine search scheme is effi-
cient, while in the sub-word tagging step rich
contextual features can be approximately de-
rived. Evaluation on the Penn Chinese Tree-
bank shows that our model yields improve-
ments over the best system reported in the lit-
erature.
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936510638298">
Word segmentation and part-of-speech (POS) tag-
ging are necessary initial steps for more advanced
Chinese language processing tasks, such as pars-
ing and semantic role labeling. Joint approaches
that resolve the two tasks simultaneously have re-
ceived much attention in recent research. Previous
work has shown that joint solutions led to accu-
racy improvements over pipelined systems by avoid-
ing segmentation error propagation and exploiting
POS information to help segmentation. A challenge
for joint approaches is the large combined search
space, which makes efficient decoding and struc-
tured learning of parameters very hard. Moreover,
the representation ability of models is limited since
using rich contextual word features makes the search
intractable. To overcome such efficiency and effec-
tiveness limitations, the approximate inference and
reranking techniques have been explored in previous
work (Zhang and Clark, 2010; Jiang et al., 2008b).
In this paper, we present an effective and effi-
cient solution for joint Chinese word segmentation
and POS tagging. Our work is motivated by several
characteristics of this problem. First of all, a major-
ity of words are easy to identify in the segmentation
problem. For example, a simple maximum match-
ing segmenter can achieve an f-score of about 90.
We will show that it is possible to improve the ef-
ficiency and accuracy by using different strategies
for different words. Second, segmenters designed
with different views have complementary strength.
We argue that the agreements and disagreements of
different solvers can be used to construct an inter-
mediate sub-word structure for joint segmentation
and tagging. Since the sub-words are large enough
in practice, the decoding for POS tagging over sub-
words is efficient. Finally, the Chinese language is
characterized by the lack of morphology that often
provides important clues for POS tagging, and the
POS tags contain much syntactic information, which
need context information within a large window for
disambiguation. For example, Huang et al. (2007)
showed the effectiveness of utilizing syntactic infor-
mation to rerank POS tagging results. As a result,
the capability to represent rich contextual features
is crucial to a POS tagger. In this work, we use
a representation-efficiency tradeoff through stacked
learning, a way of approximating rich non-local fea-
</bodyText>
<page confidence="0.932445">
1385
</page>
<note confidence="0.9793775">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1385–1394,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999287476190476">
tures.
This paper describes a novel stacked sub-word
model. Given multiple word segmentations of one
sentence, we formally define a sub-word structure
that maximizes the agreement of non-word-break
positions. Based on the sub-word structure, joint
word segmentation and POS tagging is addressed as
a two step process. In the first step, one word-based
segmenter, one character-based segmenter and one
local character classifier are used to produce coarse
segmentation and POS information. The results of
the three predictors are then merged into sub-word
sequences, which are further bracketed and labeled
with POS tags by a fine-grained sub-word tagger. If
a string is consistently segmented as a word by the
three segmenters, it will be a correct word prediction
with a very high probability. In the sub-word tag-
ging phase, the fine-grained tagger mainly considers
its POS tag prediction problem. For the words that
are not consistently predicted, the fine-grained tag-
ger will also consider their bracketing problem. The
coarse-to-fine scheme significantly improves the ef-
ficiency of decoding. Furthermore, in the sub-word
tagging step, word features in a large window can be
approximately derived from the coarse segmentation
and tagging results. To train a good sub-word tagger,
we use the stacked learning technique, which can ef-
fectively correct the training/test mismatch problem.
We conduct our experiments on the Penn Chinese
Treebank and compare our system with the state-
of-the-art systems. We present encouraging results.
Our system achieves an f-score of 98.17 for the word
segmentation task and an f-score of 94.02 for the
whole task, resulting in relative error reductions of
14.1% and 5.5% respectively over the best system
reported in the literature.
The remaining part of the paper is organized as
follows. Section 2 gives a brief introduction to the
problem and reviews the relevant previous research.
Section 3 describes the details of our method. Sec-
tion 4 presents experimental results and empirical
analyses. Section 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.998574" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.998355">
2.1 Problem Definition
</subsectionHeader>
<bodyText confidence="0.999762">
Given a sequence of characters c = (c1, ..., c#�),
the task of word segmentation and POS tagging is
to predict a sequence of word and POS tag pairs
y = ((w1, p1), (w#y, p#y)), where wi is a word, pi
is its POS tag, and a “#” symbol denotes the number
of elements in each variable. In order to avoid error
propagation and make use of POS information for
word segmentation, the two tasks should resolved
jointly. Previous research has shown that the inte-
grated methods outperformed pipelined systems (Ng
and Low, 2004; Jiang et al., 2008a; Zhang and Clark,
2008).
</bodyText>
<subsectionHeader confidence="0.8135775">
2.2 Character-Based and Word-Based
Methods
</subsectionHeader>
<bodyText confidence="0.999981028571429">
Two kinds of approaches are popular for joint word
segmentation and POS tagging. The first is the
“character-based” approach, where basic process-
ing units are characters which compose words. In
this kind of approach, the task is formulated as
the classification of characters into POS tags with
boundary information. Both the IOB2 representa-
tion (Ramshaw and Marcus, 1995) and the Start/End
representation (Kudo and Matsumoto, 2001) are
popular. For example, the label B-NN indicates that
a character is located at the begging of a noun. Using
this method, POS information is allowed to inter-
act with segmentation. Note that word segmentation
can also be formulated as a sequential classification
problem to predict whether a character is located at
the beginning of, inside or at the end of a word. This
character-by-character method for segmentation was
first proposed in (Xue, 2003), and was then further
used in POS tagging in (Ng and Low, 2004). One
main disadvantage of this model is the difficulty in
incorporating the whole word information.
The second kind of solution is the “word-based”
method, where the basic predicting units are words
themselves. This kind of solver sequentially decides
whether the local sequence of characters makes up
a word as well as its possible POS tag. In partic-
ular, a word-based solver reads the input sentence
from left to right, predicts whether the current piece
of continuous characters is a word token and which
class it belongs to. Solvers may use previously pre-
dicted words and their POS information as clues to
find a new word. After one word is found and classi-
fied, solvers move on and search for the next possi-
ble word. This word-by-word method for segmenta-
tion was first proposed in (Zhang and Clark, 2007),
</bodyText>
<page confidence="0.986735">
1386
</page>
<bodyText confidence="0.999897470588235">
and was then further used in POS tagging in (Zhang
and Clark, 2008).
In our previous work(Sun, 2010), we presented
a theoretical and empirical comparative analysis of
character-based and word-based methods for Chi-
nese word segmentation. We showed that the two
methods produced different distributions of segmen-
tation errors in a way that could be explained by
theoretical properties of the two models. A system
combination method that leverages the complemen-
tary strength of word-based and character-based seg-
mentation models was also successfully explored in
their work. Different from our previous focus, the
diversity of different models designed with different
views is utilized to construct sub-word structures in
this work. We will discuss the details in the next
section.
</bodyText>
<subsectionHeader confidence="0.998313">
2.3 Stacked Learning
</subsectionHeader>
<bodyText confidence="0.99968396875">
Stacked generalization is a meta-learning algorithm
that was first proposed in (Wolpert, 1992) and
(Breiman, 1996). The idea is to include two “levels”
of predictors. The first level includes one or more
predictors g1, ...gK : Rd —* R; each receives input
x E Rd and outputs a prediction gk(x). The second
level consists of a single function h : Rd+K —* R
that takes as input (x, g1(x), ..., gK(x)) and outputs
a final prediction y� = h(x, g1(x), ..., gK(x)).
Training is done as follows. The training data S =
{(xt, yt) : t E [1, T]} is split into L equal-sized dis-
joint subsets S1, ..., SL. Then functions g1, ..., gL
(where gl = (gl1, ..., glK)) are seperately trained on
S − Sl, and are used to construct the augmented
dataset S� = {((xt, k1t , ..., kKt ), yt) : kkt = glk(xt)
and xt E Sl}. Finally, each gk is trained on the origi-
nal dataset and the second level predictor h is trained
on S. The intent of the cross-validation scheme is
that ykt is similar to the prediction produced by a
predictor which is learned on a sample that does not
include xt.
Stacked learning has been applied as a system en-
semble method in several NLP tasks, such as named
entity recognition (Wu et al., 2003) and dependency
parsing (Nivre and McDonald, 2008). This frame-
work is also explored as a solution for learning non-
local features in (Torres Martins et al., 2008). In
the machine learning research, stacked learning has
been applied to structured prediction (Cohen and
Carvalho, 2005). In this work, stacked learning is
used to acquire extended training data for sub-word
tagging.
</bodyText>
<sectionHeader confidence="0.996937" genericHeader="method">
3 Method
</sectionHeader>
<subsectionHeader confidence="0.999902">
3.1 Architecture
</subsectionHeader>
<bodyText confidence="0.9999884">
In our stacked sub-word model, joint word segmen-
tation and POS tagging is decomposed into two
steps: (1) coarse-grained word segmentation and
tagging, and (2) fine-grained sub-word tagging. The
workflow is shown in Figure 1. In the first phase, one
word-based segmenter (SegW) and one character-
based segmenter (SegC) are trained to produce word
boundaries. Additionally, a local character-based
joint segmentation and tagging solver (SegTagL) is
used to provide word boundaries as well as inaccu-
rate POS information. Here, the word local means
the labels of nearby characters are not used as fea-
tures. In other words, the local character classi-
fier assumes that the tags of characters are indepen-
dent of each other. In the second phase, our system
first combines the three segmentation and tagging
results to get sub-words which maximize the agree-
ment about word boundaries. Finally, a fine-grained
sub-word tagger (SubTag) is applied to bracket sub-
words into words and also to obtain their POS tags.
</bodyText>
<subsectionHeader confidence="0.725225">
Merging
</subsectionHeader>
<bodyText confidence="0.990562666666667">
In our model, segmentation and POS tagging in-
teract with each other in two processes. First, al-
though SegTagL is locally trained, it resolves the
</bodyText>
<figure confidence="0.993932166666667">
Word-based
Segmenter SegW
Segmented
sentences
Character-based
segmenter SegC
Raw sentences
Segmented
sentences
Local character
classifier
SegTagL
Segmented
sentences
Sub-word tag-
ger SubTag
Sub-word
sequences
</figure>
<figureCaption confidence="0.99978">
Figure 1: Workflow of the stacked sub-word model.
</figureCaption>
<page confidence="0.943109">
1387
</page>
<bodyText confidence="0.99998646875">
two sub-tasks simultaneously. Therefore, in the sub-
word generating stage, segmentation and POS tag-
ging help each other. Second, in the sub-word tag-
ging stage, the bracketing and the classification of
sub-words are jointly resolved as one sequence la-
beling problem.
Our experiments on the Penn Chinese Treebank
will show that the word-based and character-based
segmenters and the local tagger on their own pro-
duce high quality word boundaries. As a result, the
oracle performance to recover words from a sub-
word sequence is very high. The quality of the fi-
nal tagger relies on the quality of the sub-word tag-
ger. If a high performance sub-word tagger can be
constructed, the whole task can be well resolved.
The statistics will also empirically show that sub-
words are significantly larger than characters and
only slightly smaller than words. As a result, the
search space of the sub-word tagging is significantly
shrunken, and exact Viterbi decoding without ap-
proximately pruning can be efficiently processed.
This property makes nearly all popular sequence la-
beling algorithms applicable.
Zhang et al. (2006) described a sub-word based
tagging model to resolve word segmentation. To
get the pieces which are larger than characters but
smaller than words, they combine a character-based
segmenter and a dictionary matching segmenter.
Our contributions include (1) providing a formal
definition of our sub-word structure that is based on
multiple segmentations and (2) proposing a stacking
method to acquire sub-words.
</bodyText>
<subsectionHeader confidence="0.999902">
3.2 The Coarse-grained Solvers
</subsectionHeader>
<bodyText confidence="0.99999155">
We systematically described the implementation of
two state-of-the-art Chinese word segmenters in
word-based and character-based architectures, re-
spectively (Sun, 2010). Our word-based segmenter
is based on a discriminative joint model with a
first order semi-Markov structure, and the other seg-
menter is based on a first order Markov model. Ex-
act Viterbi-style search algorithms are used for de-
coding. Limited to the document length, we do not
give the description of the features. We refer readers
to read the above paper for details. For parameter
estimation, our work adopt the Passive-Aggressive
(PA) framework (Crammer et al., 2006), a family
of margin based online learning algorithms. In this
work, we introduce two simple but important refine-
ments: (1) to shuffle the sample orders in each itera-
tion and (2) to average the parameters in each itera-
tion as the final parameters.
Idiom In linguistics, idioms are usually presumed
to be figures of speech contradicting the principle
of compositionality. As a result, it is very hard to
recognize out-of-vocabulary idioms for word seg-
mentation. However, the lexicon of idioms can be
taken as a close set, which helps resolve the problem
well. We collect 12992 idioms1 from several on-
line Chinese dictionaries. For both word-based and
character-based segmentation, we first match every
string of a given sentence with idioms. Every sen-
tence is then splitted into smaller pieces which are
seperated by idioms. Statistical segmentation mod-
els are then performed on these smaller character se-
quences.
We use a local classifier to predict the POS
tag with positional information for each character.
Each character can be assigned one of two possi-
ble boundary tags: “B” for a character that begins a
word and “I” for a character that occurs in the mid-
dle of a word. We denote a candidate character to-
ken ci with a fixed window ci_2ci_1cici+1ci+2. The
following features are used:
</bodyText>
<listItem confidence="0.9997505">
• character uni-grams: ck (i − 2 G k G i + 2)
• character bi-grams: ckck+1 (i − 2 G k G i + 1)
</listItem>
<bodyText confidence="0.9971685">
To resolve the classification problem, we use the lin-
ear SVM classifier LIBLINEAR2.
</bodyText>
<subsectionHeader confidence="0.996265">
3.3 Merging Multiple Segmentation Results
into Sub-Word Sequences
</subsectionHeader>
<bodyText confidence="0.999982111111111">
A majority of words are easy to identify in the seg-
mentation problem. We favor the idea treating dif-
ferent words using different strategies. In this work
we try to identify simple and difficult words first and
to integrate them into a sub-word level. Inspired by
previous work, we constructed this sub-word struc-
ture by using multiple solvers designed from differ-
ent views. If a piece of continuous characters is con-
sistently segmented by multiple segmenters, it will
</bodyText>
<footnote confidence="0.99626575">
1This resource is publicly available at http://www.
coli.uni-saarland.de/˜wsun/idioms.txt.
2Available at http://www.csie.ntu.edu.tw/
˜cjlin/liblinear/.
</footnote>
<page confidence="0.840301">
1388
</page>
<table confidence="0.74786675">
以 总 成绩 3 5 5
Answer: [P] [JJ] [ NN ] [
SegW: [] [] [ ] [ ]
SegC: [] [] [ ] [
SegTagL: [P] [JJ] [ NN ] [ CD
Sub-words: [P] [JJ] [ NN ] [ B-CD ]
. 3 5 分 居 领 先 地 位
CD ] [M] [VV] [ JJ ] [ NN ]
[ ] [ ] [ ] [ ]
] [] [ ] [ ]
] [NT] [CD] [NT] [VV] [ VV ] [ NN ]
[I-CD] [NT] [CD] [NT] [VV] [ VV ] [ NN ]
</table>
<figureCaption confidence="0.995162">
Figure 2: An example phrase: 以总成绩355.35分居领先地位 (Being in front with a total score of 355.35
points).
</figureCaption>
<bodyText confidence="0.996811166666667">
not be separated in the sub-word tagging step. The
intuition is that strings which are consistently seg-
mented by the different segmenters tend to be cor-
rect predictions. In our experiment on the Penn Chi-
nese Treebank (Xue et al., 2005), the accuracy is
98.59% on the development data which is defined
in the next section. The key point for the interme-
diate sub-word structures is to maximize the agree-
ment of the three coarse-grained systems. In other
words, the goal is to make merged sub-words as
large as possible but not overlap with any predicted
word produced by the three coarse-grained solvers.
In particular, if the position between two continu-
ous characters is predicted as a word boundary by
any segmenter, this position is taken as a separation
position of the sub-word sequence. This strategy
makes sure that it is still possible to re-segment the
strings of which the boundaries are disagreed with
by the coarse-grained segmenters in the fine-grained
tagging stage.
The formal definition is as follows. Given a se-
quence of characters c = (c1, ..., c#�), let c[i : j]
denote a string that is made up of characters between
ci and cj (including ci and cj), then a partition of
the sentence can be written as c[0 : e1], c[e1 + 1 :
e2], ..., c[em : #c]. Let sk = {c[i : j]} denote the
set of all segments of a partition. Given multiple
partitions of a character sequence S = {sk}, there
is one and only one merged partition sS = {c[i : j]}
s.t.
</bodyText>
<listItem confidence="0.99586">
1. ∀c[i : j] ∈ sS, ∀sk ∈ S, ∃c[s : e] ∈ sk, s ≤
i ≤ j≤ e.
2. ∀C&apos; satisfies the above condition, |C&apos; |&gt; |C|.
</listItem>
<bodyText confidence="0.999354173913044">
The first condition makes sure that all segments in
the merged partition can be only embedded in but do
not overlap with any segment of any partition from
S. The second condition promises that segments of
the merged partition achieve maximum length.
Figure 2 is an example to illustrate the proce-
dure of our method. The lines SegW, SegC and
SegTagL are the predictions of the three coarse-
grained solvers. For the three words at the begin-
ning and the two words at the end, the three predic-
tors agree with each other. And these five words are
kept as sub-words. For the character sequence “3
55.35分居”, the predictions are very differ-
ent. Because there are no word break predictions
among the first three characters “355”, it is as
a whole taken as one sub-word. For the other five
characters, either the left position or the right po-
sition is segmented as a word break by some pre-
dictor, so the merging processor seperates them and
takes each one as a single sub-word. The last line
shows the merged sub-word sequence. The coarse-
grained POS tags with positional information are de-
rived from the labels provided by SegTagL.
</bodyText>
<subsectionHeader confidence="0.995485">
3.4 The Fine-grained Sub-Word Tagger
</subsectionHeader>
<bodyText confidence="0.9999745625">
Bracketing sub-words into words is formulated as
a IOB-style sequential classification problem. Each
sub-word may be assigned with one POS tag as well
as two possible boundary tags: “B” for the begin-
ning position and “I” for the middle position. A
tagger is trained to classify sub-word by using the
features derived from its contexts.
The sub-word level allows our system to utilize
features in a large context, which is very important
for POS tagging of the morphologically poor lan-
guage. Features are formed making use of sub-word
contents, their IOB-style inaccurate POS tags. In
the following description, “C” refers to the content
of the sub-word, while “T” refers to the IOB-style
POS tags. For convenience, we denote a sub-word
with its context ...si_2si_1sisi+1si+2..., where si is
</bodyText>
<page confidence="0.993509">
1389
</page>
<table confidence="0.999476363636364">
C(si−1)=“)A绩”; T(si−1)=“NN”
C(si)=“355”; T(si)=“B-CD”
C(si+1)=“.”; T(si+1)=“I-CD”
C(si−1)C(si)=“)A绩 355”
T(si−1)T(si)=“NN B-CD”
C(si)C(si+1)=“355 .”
T(si)T(si+1)=“B-CD I-CD”
C(si−1)C(si+1)=“)A绩 .”
T(si−1)T(si+1)=“B-NN I-CD”
Prefix(1)=“3”; Prefix(2)=“3 5”; Prefix(3)=“355”
Suffix(1)=“5”; Suffix(2)=“5 5”; Suffix(3)=“355”
</table>
<tableCaption confidence="0.9801255">
Table 1: An example of features used in the sub-word
tagging.
</tableCaption>
<listItem confidence="0.9436056">
the current token. We denote lC, lT as the sizes of
the window.
• Uni-gram features: C(sk) (−lC ≤ k ≤ lC),
T(sk) (−lT ≤ k ≤ lT)
• Bi-gram features: C(sk)C(sk+1) (−lC ≤ k ≤
lC − 1), T(sk)T(sk+1) (−lT ≤ k ≤ lT − 1)
• C(si−1)C(si+1) (if lC ≥ 1), T(si−1)T(si+1) (if
lT ≥ 1)
• T(si−2)T(si+1) (if lT ≥ 2)
• In order to better handle unknown words, we
</listItem>
<bodyText confidence="0.9844842">
also extract morphological features: character
n-gram prefixes and suffixes for n up to 3.
These features have been shown useful in pre-
vious research (Huang et al., 2007).
Take the sub-word “355” in Figure 2 for ex-
ample, when lC and lT are both set to 1, all features
used are listed in Table 1.
In the following experiments, we will vary win-
dow sizes lC and lT to find out the contribution of
context information for the disambiguation. A first
order Max-Margin Markov Networks model is used
to resolve the sequence tagging problem. We use the
SVM-HMM3 implementation for the experiments in
this work. We use the basic linear model without
applying any kernel function.
</bodyText>
<footnote confidence="0.974551">
3Available at http://www.cs.cornell.edu/
People/tj/svm—light/svm—hmm.html.
</footnote>
<figureCaption confidence="0.9087835">
Algorithm 1: The stacked learning procedure
for the sub-word tagger.
</figureCaption>
<figure confidence="0.860024272727273">
input : Data S = {(ct, yt), t = 1, 2,..., n}
Split S into L partitions {S1, ...SL}
for l = 1, ..., L do
Train SegWl, SegCl and SegTagLl using
S − Sl.
Predict Sl using SegWl, SegCl and
SegTagLl.
Merge the predictions to get sub-words
training sample S0l.
end
Train the sub-word tagger SubTag using S0.
</figure>
<subsectionHeader confidence="0.906134">
3.5 Stacked Learning for the Sub-Word Tagger
</subsectionHeader>
<bodyText confidence="0.99992812">
The three coarse-grained solvers SegW, SegC and
SegTagL are directly trained on the original train-
ing data. When these three predictors are used to
produce the training data, the performance is per-
fect. However, this does not hold when these mod-
els are applied to the test data. If we directly apply
SegW, SegC and SegTagL to extend the training data
to generate sub-word samples, the extended training
data for the sub-word tagger will be very different
from the data in the run time, resulting in poor per-
formance.
One way to correct the training/test mismatch is
to use the stacking method, where a K-fold cross-
validation on the original data is performed to con-
struct the training data for sub-word tagging. Algo-
rithm 1 illustrates the learning procedure. First, the
training data S = {(ct, yt)} is split into L equal-
sized disjoint subsets S1,..., SL. For each subset Sl,
the complementary set S − Sl is used to train three
coarse solvers SegWl, SegCl and SegTagLl, which
process the Sl and provide inaccurate predictions.
Then the inaccurate predictions are merged into sub-
word sequences and Sl is extended to S0l. Finally,
the sub-word tagger is trained on the whole extended
data set S0.
</bodyText>
<sectionHeader confidence="0.999712" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.990086">
4.1 Setting
</subsectionHeader>
<bodyText confidence="0.998669">
Previous studies on joint Chinese word segmenta-
tion and POS tagging have used the Penn Chinese
Treebank (CTB) in experiments. We follow this set-
</bodyText>
<page confidence="0.978875">
1390
</page>
<bodyText confidence="0.999735">
ting in this paper. We use CTB 5.0 as our main
corpus and define the training, development and test
sets according to (Jiang et al., 2008a; Jiang et al.,
2008b; Kruengkrai et al., 2009; Zhang and Clark,
2010). Table 2 shows the statistics of our experi-
mental settings.
</bodyText>
<table confidence="0.999496333333333">
Data set CTB files # of sent. # of words
Training 1-270 18,089 493,939
400-931
1001-1151
Devel. 301-325 350 6821
Test 271-300 348 8008
</table>
<tableCaption confidence="0.999768">
Table 2: Training, development and test data on CTB 5.0
</tableCaption>
<bodyText confidence="0.9995924">
Three metrics are used for evaluation: precision
(P), recall (R) and balanced f-score (F) defined by
2PR/(P+R). Precision is the relative amount of cor-
rect words in the system output. Recall is the rela-
tive amount of correct words compared to the gold
standard annotations. For segmentation, a token is
considered to be correct if its boundaries match the
boundaries of a word in the gold standard. For the
whole task, both the boundaries and the POS tag
have to be correctly identified.
</bodyText>
<subsectionHeader confidence="0.999223">
4.2 Performance of the Coarse-grained Solvers
</subsectionHeader>
<bodyText confidence="0.999628222222222">
Table 3 shows the performance on the development
data set of the three coarse-grained solvers. In this
paper, we use 20 iterations to train Segal and SegC
for all experiments. Even only locally trained, the
character classifier SegTagL still significantly out-
performs the two state-of-the-art segmenters Segal
and SegC. This good performance indicates that the
POS information is very important for word segmen-
tation.
</bodyText>
<table confidence="0.9985176">
Devel. Task P(%) R(%) F
Segal Seg 94.55 94.84 94.69
SegC Seg 95.10 94.38 94.73
SegTagL Seg 95.67 95.98 95.83
Seg&amp;Tag 87.54 91.29 89.38
</table>
<tableCaption confidence="0.9963355">
Table 3: Performance of the coarse-grained solvers on the
development data.
</tableCaption>
<subsectionHeader confidence="0.998963">
4.3 Statistics of Sub-Words
</subsectionHeader>
<bodyText confidence="0.999921">
Since the base predictors to generate coarse infor-
mation are two word segmenters and a local charac-
ter classifier, the coarse decoding is efficient. If the
length of sub-words is too short, i.e. the decoding
path for sub-word sequences are too long, the decod-
ing of the fine-grained stage is still hard. Although
we cannot give a theoretical average length of sub-
words, we can still show the empirical one. The av-
erage length of sub-words on the development set is
1.64, while the average length of words is 1.69. The
number of all IOB-style POS tags is 59 (when using
5-fold cross-validation to generate stacked training
samples). The number of all POS tags is 35. Empir-
ically, the decoding over sub-words is 1.69 X (59 n+1
</bodyText>
<equation confidence="0.586867">
1.64 35
</equation>
<bodyText confidence="0.999381571428571">
times as slow as the decoding over words, where n
is the order of the markov model. When a first order
markov model is used, this number is 2.93. These
statistics empirically suggest that the decoding over
sub-word sequence can be efficient.
On the other hand, the sub-word sequences are
not perfect in the sense that they do not promise
to recover all words because of the errors made in
the first step. Similarly, we can only show the em-
pirical upper bound of the sub-word tagging. The
oracle performance of the final POS tagging on the
development data set is shown in Table 4. The up-
per bound indicates that the coarse search procedure
does not lose too much.
</bodyText>
<table confidence="0.930751">
Task P(%) R(%) F
Seg&amp;Tag 99.50% 99.09% 99.29
</table>
<tableCaption confidence="0.9907195">
Table 4: Upper bound of the sub-word tagging on the
development data.
</tableCaption>
<bodyText confidence="0.9998678">
One main disadvantage of character-based ap-
proach is the difficulty to incorporate word features.
Since the sub-words are on average close to words,
sub-word features are good approximations of word
features.
</bodyText>
<subsectionHeader confidence="0.99967">
4.4 Rich Contextual Features Are Useful
</subsectionHeader>
<bodyText confidence="0.998718">
Table 5 shows the effect that features within differ-
ent window size has on the sub-word tagging task.
In this table, the symbol “C” means sub-word con-
tent features while the symbol “T” means IOB-style
POS tag features. The number indicates the length
</bodyText>
<page confidence="0.98192">
1391
</page>
<table confidence="0.9997071">
Devel. P(%) R(%) F
C:±0 T:±0 92.52 92.83 92.67
C:±1 T:±0 92.63 93.27 92.95
C:±1 T:±1 92.62 93.05 92.83
C:±2 T:±0 93.17 93.86 93.51
C:±2 T:±1 93.27 93.64 93.45
C:±2 T:±2 93.08 93.61 93.34
C:±3 T:±0 93.12 93.86 93.49
C:±3 T:±1 93.34 93.96 93.65
C:±3 T:±2 93.34 93.96 93.65
</table>
<tableCaption confidence="0.8792615">
Table 5: Performance of the stacked sub-word model
(K = 5) with features in different window sizes.
</tableCaption>
<bodyText confidence="0.99997178125">
of the window. For example, “C:±1” means that the
tagger uses one preceding sub-word and one suc-
ceeding sub-word as features. From this table, we
can clearly see the impact of features derived from
neighboring sub-words. There is a significant in-
crease between “C:±2” and “C:±1” models. This
confirms our motivation that longer history and fu-
ture features are crucial to the Chinese POS tagging
problem. It is the main advantage of our model that
making rich contextual features applicable. In all
previous solutions, only features within a short his-
tory can be used due to the efficiency limitation.
The performance is further slightly improved
when the window size is increased to 3. Using the
labeled bracketing f-score, the evaluation shows that
the “C:±3 T:±1” model performs the same as the
“C:±3 T:±2” model. However, the sub-word clas-
sification accuracy of the “C:±3 T:±1” model is
higher, so in the following experiments and the fi-
nal results reported on the test data set, we choose
this setting.
This table also suggests that the IOB-style POS
information of sub-words does not contribute. We
think there are two main reasons: (1) The POS infor-
mation provided by the local classifier is inaccurate;
(2) The structured learning of the sub-word tagger
can use real predicted sub-word labels during its de-
coding time, since this learning algorithm does in-
ference during the training time. It is still an open
question whether more accurate POS information in
rich contexts can help this task. If the answer is YES,
how can we efficiently incorporate these features?
</bodyText>
<subsectionHeader confidence="0.979486">
4.5 Stacked Learning Is Useful
</subsectionHeader>
<bodyText confidence="0.9999106">
Table 6 compares the performance of “C:±3 T:±1”
models trained with no stacking as well as differ-
ent folds of cross-validation. We can see that al-
though it is still possible to improve the segmenta-
tion and POS tagging performance compared to the
local character classifier, the whole task just benefits
only a little from the sub-word tagging procedure if
the stacking technique is not applied. The stacking
technique can significantly improve the system per-
formance, both for segmentation and POS tagging.
This experiment confirms the theoretical motivation
of using stacked learning: simulating the test-time
setting when a sub-word tagger is applied to a new
instance. There is not much difference between the
5-fold and the 10-fold cross-validation.
</bodyText>
<table confidence="0.998746571428571">
Devel. Task P(%) R(%) F
No stacking Seg 95.75 96.48 96.12
Seg&amp;Tag 91.42 92.13 91.77
K = 5 Seg 96.42 97.04 96.73
Seg&amp;Tag 93.34 93.96 93.65
K = 10 Seg 96.67 97.11 96.89
Seg&amp;Tag 93.50 94.06 93.78
</table>
<tableCaption confidence="0.978788">
Table 6: Performance on the development data. No stack-
ing and different folds of cross-validation are separately
applied.
</tableCaption>
<subsectionHeader confidence="0.981802">
4.6 Final Results
</subsectionHeader>
<bodyText confidence="0.999886214285714">
Table 7 summarizes the performance of our final
system on the test data and other systems reported
in a majority of previous work. The final results
of our system are achieved by using 10-fold cross-
validation “C:±3 T:±1” models. The left most col-
umn indicates the reference of previous systems that
represent state-of-the-art results. The comparison of
the accuracy between our stacked sub-word system
and the state-of-the-art systems in the literature in-
dicates that our method is competitive with the best
systems. Our system obtains the highest f-score per-
formance on both segmentation and the whole task,
resulting in error reductions of 14.1% and 5.5% re-
spectively.
</bodyText>
<page confidence="0.966688">
1392
</page>
<table confidence="0.985027666666667">
Test Seg Seg&amp;Tag
(Jiang et al., 2008a) 97.85 93.41
(Jiang et al., 2008b) 97.74 93.37
(Kruengkrai et al., 2009) 97.87 93.67
(Zhang and Clark, 2010) 97.78 93.67
Our system 98.17 94.02
</table>
<tableCaption confidence="0.999569">
Table 7: F-score performance on the test data.
</tableCaption>
<sectionHeader confidence="0.99515" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999965222222222">
This paper has described a stacked sub-word model
for joint Chinese word segmentation and POS tag-
ging. We defined a sub-word structure which maxi-
mizes the agreement of multiple segmentations pro-
vided by different segmenters. We showed that this
sub-word structure could explore the complemen-
tary strength of different systems designed with dif-
ferent views. Moreover, the POS tagging could be
efficiently and effectively resolved over sub-word
sequences. To train a good sub-word tagger, we in-
troduced a stacked learning procedure. Experiments
showed that our approach was superior to the exist-
ing approaches reported in the literature.
Machine learning and statistical approaches en-
counter difficulties when the input/output data have
a structured and relational form. Research in em-
pirical Natural Language Processing has been tack-
ling these complexities since the early work in the
field. Recent work in machine learning has pro-
vided several paradigms to globally represent and
process such data: linear models for structured pre-
diction, graphical models, constrained conditional
models, and reranking, among others. A general
expressivity-efficiency trade off is observed. Al-
though the stacked sub-word model is an ad hoc so-
lution for a particular problem, namely joint word
segmentation and POS tagging, the idea to em-
ploy system ensemble and stacked learning in gen-
eral provides an alternative for structured problems.
Multiple “cheap” coarse systems are used to provide
diverse outputs, which may be inaccurate. These
outputs are further merged into an intermediate rep-
resentation, which allows an extractive system to use
rich contexts to predict the final results. A natu-
ral avenue for future work is the extension of our
method to other NLP tasks.
</bodyText>
<sectionHeader confidence="0.997714" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999941222222222">
The work is supported by the project TAKE (Tech-
nologies for Advanced Knowledge Extraction),
funded under contract 01IW08003 by the German
Federal Ministry of Education and Research. The
author is also funded by German Academic Ex-
change Service (DAAD).
The author would would like to thank Dr. Jia
Xu for her helpful discussion, and Regine Bader for
proofreading this paper.
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999484921052631">
Leo Breiman. 1996. Stacked regressions. Mach. Learn.,
24:49–64, July.
William W. Cohen and Vitor R. Carvalho. 2005. Stacked
sequential learning. In Proceedings of the 19th in-
ternational joint conference on Artificial intelligence,
pages 671–676, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JOURNAL OF MACHINE
LEARNING RESEARCH, 7:551–585.
Zhongqiang Huang, Mary Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and discrim-
inative reranking. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1093–1102,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u.
2008a. A cascaded linear model for joint Chinese
word segmentation and part-of-speech tagging. In
Proceedings ofACL-08: HLT, pages 897–904, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word
lattice reranking for Chinese word segmentation and
part-of-speech tagging. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 385–392, Manchester, UK, Au-
gust. Coling 2008 Organizing Committee.
Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and pos
tagging. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
</reference>
<page confidence="0.579774">
1393
</page>
<reference confidence="0.999873054054054">
ing of the AFNLP, pages 513–521, Suntec, Singapore,
August. Association for Computational Linguistics.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In NAACL ’01: Second
meeting of the North American Chapter of the Associa-
tion for Computational Linguistics on Language tech-
nologies 2001, pages 1–8, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages 277–
284, Barcelona, Spain, July. Association for Computa-
tional Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950–958,
Columbus, Ohio, June. Association for Computational
Linguistics.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of the 3rd ACL/SIGDAT Workshop on Very Large Cor-
pora, Cambridge, Massachusetts, USA, pages 82–94.
Weiwei Sun. 2010. Word-based and character-based
word segmentation models: Comparison and combi-
nation. In Coling 2010: Posters, pages 1211–1219,
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Andr´e Filipe Torres Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking dependency
parsers. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 157–166, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
David H. Wolpert. 1992. Original contribution: Stacked
generalization. Neural Netw., 5:241–259, February.
Dekai Wu, Grace Ngai, and Marine Carpuat. 2003. A
stacked, voted, stacked model for named entity recog-
nition. In Walter Daelemans and Miles Osborne, ed-
itors, Proceedings of the Seventh Conference on Nat-
ural Language Learning at HLT-NAACL 2003, pages
200–203.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. In International Journal of Com-
putational Linguistics and Chinese Language Process-
ing.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 840–847, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Yue Zhang and Stephen Clark. 2008. Joint word segmen-
tation and POS tagging using a single perceptron. In
Proceedings ofACL-08: HLT, pages 888–896, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Yue Zhang and Stephen Clark. 2010. A fast decoder for
joint word segmentation and POS-tagging using a sin-
gle discriminative model. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 843–852, Cambridge, MA,
October. Association for Computational Linguistics.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for Chinese word segmentation. In Proceedings
of the Human Language Technology Conference of
the NAACL, Companion Volume: Short Papers, pages
193–196, New York City, USA, June. Association for
Computational Linguistics.
</reference>
<page confidence="0.994944">
1394
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.646503">
<title confidence="0.9934515">A Stacked Sub-Word Model for Joint Chinese Word Segmentation Part-of-Speech Tagging</title>
<author confidence="0.983429">Weiwei</author>
<affiliation confidence="0.960051">Department of Computational Linguistics, Saarland German Research Center for Artificial Intelligence</affiliation>
<address confidence="0.917796">D-66123, Saarbr¨ucken,</address>
<email confidence="0.996615">wsun@coli.uni-saarland.de</email>
<abstract confidence="0.988766083333333">The large combined search space of joint word segmentation and Part-of-Speech (POS) tagging makes efficient decoding very hard. As a result, effective high order features representing rich contexts are inconvenient to use. In this work, we propose a novel stacked subword model for this task, concerning both efficiency and effectiveness. Our solution is a two step process. First, one word-based segmenter, one character-based segmenter and one local character classifier are trained to produce coarse segmentation and POS information. Second, the outputs of the three predictors are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a fine-grained sub-word tagger. The coarse-to-fine search scheme is efficient, while in the sub-word tagging step rich contextual features can be approximately derived. Evaluation on the Penn Chinese Treebank shows that our model yields improvements over the best system reported in the literature.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Stacked regressions.</title>
<date>1996</date>
<location>Mach. Learn., 24:49–64,</location>
<contexts>
<context position="9216" citStr="Breiman, 1996" startWordPosition="1435" endWordPosition="1436"> of segmentation errors in a way that could be explained by theoretical properties of the two models. A system combination method that leverages the complementary strength of word-based and character-based segmentation models was also successfully explored in their work. Different from our previous focus, the diversity of different models designed with different views is utilized to construct sub-word structures in this work. We will discuss the details in the next section. 2.3 Stacked Learning Stacked generalization is a meta-learning algorithm that was first proposed in (Wolpert, 1992) and (Breiman, 1996). The idea is to include two “levels” of predictors. The first level includes one or more predictors g1, ...gK : Rd —* R; each receives input x E Rd and outputs a prediction gk(x). The second level consists of a single function h : Rd+K —* R that takes as input (x, g1(x), ..., gK(x)) and outputs a final prediction y� = h(x, g1(x), ..., gK(x)). Training is done as follows. The training data S = {(xt, yt) : t E [1, T]} is split into L equal-sized disjoint subsets S1, ..., SL. Then functions g1, ..., gL (where gl = (gl1, ..., glK)) are seperately trained on S − Sl, and are used to construct the a</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996. Stacked regressions. Mach. Learn., 24:49–64, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Vitor R Carvalho</author>
</authors>
<title>Stacked sequential learning.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th international joint conference on Artificial intelligence,</booktitle>
<pages>671--676</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="10577" citStr="Cohen and Carvalho, 2005" startWordPosition="1687" endWordPosition="1690"> the second level predictor h is trained on S. The intent of the cross-validation scheme is that ykt is similar to the prediction produced by a predictor which is learned on a sample that does not include xt. Stacked learning has been applied as a system ensemble method in several NLP tasks, such as named entity recognition (Wu et al., 2003) and dependency parsing (Nivre and McDonald, 2008). This framework is also explored as a solution for learning nonlocal features in (Torres Martins et al., 2008). In the machine learning research, stacked learning has been applied to structured prediction (Cohen and Carvalho, 2005). In this work, stacked learning is used to acquire extended training data for sub-word tagging. 3 Method 3.1 Architecture In our stacked sub-word model, joint word segmentation and POS tagging is decomposed into two steps: (1) coarse-grained word segmentation and tagging, and (2) fine-grained sub-word tagging. The workflow is shown in Figure 1. In the first phase, one word-based segmenter (SegW) and one characterbased segmenter (SegC) are trained to produce word boundaries. Additionally, a local character-based joint segmentation and tagging solver (SegTagL) is used to provide word boundaries</context>
</contexts>
<marker>Cohen, Carvalho, 2005</marker>
<rawString>William W. Cohen and Vitor R. Carvalho. 2005. Stacked sequential learning. In Proceedings of the 19th international joint conference on Artificial intelligence, pages 671–676, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>JOURNAL OF MACHINE LEARNING RESEARCH,</journal>
<pages>7--551</pages>
<contexts>
<context position="14305" citStr="Crammer et al., 2006" startWordPosition="2264" endWordPosition="2267"> described the implementation of two state-of-the-art Chinese word segmenters in word-based and character-based architectures, respectively (Sun, 2010). Our word-based segmenter is based on a discriminative joint model with a first order semi-Markov structure, and the other segmenter is based on a first order Markov model. Exact Viterbi-style search algorithms are used for decoding. Limited to the document length, we do not give the description of the features. We refer readers to read the above paper for details. For parameter estimation, our work adopt the Passive-Aggressive (PA) framework (Crammer et al., 2006), a family of margin based online learning algorithms. In this work, we introduce two simple but important refinements: (1) to shuffle the sample orders in each iteration and (2) to average the parameters in each iteration as the final parameters. Idiom In linguistics, idioms are usually presumed to be figures of speech contradicting the principle of compositionality. As a result, it is very hard to recognize out-of-vocabulary idioms for word segmentation. However, the lexicon of idioms can be taken as a close set, which helps resolve the problem well. We collect 12992 idioms1 from several onl</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. JOURNAL OF MACHINE LEARNING RESEARCH, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
<author>Wen Wang</author>
</authors>
<title>Mandarin part-of-speech tagging and discriminative reranking.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1093--1102</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3326" citStr="Huang et al. (2007)" startWordPosition="501" endWordPosition="504">ters designed with different views have complementary strength. We argue that the agreements and disagreements of different solvers can be used to construct an intermediate sub-word structure for joint segmentation and tagging. Since the sub-words are large enough in practice, the decoding for POS tagging over subwords is efficient. Finally, the Chinese language is characterized by the lack of morphology that often provides important clues for POS tagging, and the POS tags contain much syntactic information, which need context information within a large window for disambiguation. For example, Huang et al. (2007) showed the effectiveness of utilizing syntactic information to rerank POS tagging results. As a result, the capability to represent rich contextual features is crucial to a POS tagger. In this work, we use a representation-efficiency tradeoff through stacked learning, a way of approximating rich non-local fea1385 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1385–1394, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics tures. This paper describes a novel stacked sub-word model. Given multiple word segmentations</context>
<context position="21273" citStr="Huang et al., 2007" startWordPosition="3491" endWordPosition="3494">(1)=“5”; Suffix(2)=“5 5”; Suffix(3)=“355” Table 1: An example of features used in the sub-word tagging. the current token. We denote lC, lT as the sizes of the window. • Uni-gram features: C(sk) (−lC ≤ k ≤ lC), T(sk) (−lT ≤ k ≤ lT) • Bi-gram features: C(sk)C(sk+1) (−lC ≤ k ≤ lC − 1), T(sk)T(sk+1) (−lT ≤ k ≤ lT − 1) • C(si−1)C(si+1) (if lC ≥ 1), T(si−1)T(si+1) (if lT ≥ 1) • T(si−2)T(si+1) (if lT ≥ 2) • In order to better handle unknown words, we also extract morphological features: character n-gram prefixes and suffixes for n up to 3. These features have been shown useful in previous research (Huang et al., 2007). Take the sub-word “355” in Figure 2 for example, when lC and lT are both set to 1, all features used are listed in Table 1. In the following experiments, we will vary window sizes lC and lT to find out the contribution of context information for the disambiguation. A first order Max-Margin Markov Networks model is used to resolve the sequence tagging problem. We use the SVM-HMM3 implementation for the experiments in this work. We use the basic linear model without applying any kernel function. 3Available at http://www.cs.cornell.edu/ People/tj/svm—light/svm—hmm.html. Algorithm 1: The stacked</context>
</contexts>
<marker>Huang, Harper, Wang, 2007</marker>
<rawString>Zhongqiang Huang, Mary Harper, and Wen Wang. 2007. Mandarin part-of-speech tagging and discriminative reranking. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1093–1102, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan L¨u</author>
</authors>
<title>A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>897--904</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<marker>Jiang, Huang, Liu, L¨u, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan L¨u. 2008a. A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging. In Proceedings ofACL-08: HLT, pages 897–904, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Haitao Mi</author>
<author>Qun Liu</author>
</authors>
<title>Word lattice reranking for Chinese word segmentation and part-of-speech tagging.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>385--392</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2216" citStr="Jiang et al., 2008" startWordPosition="325" endWordPosition="328">ns led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation. A challenge for joint approaches is the large combined search space, which makes efficient decoding and structured learning of parameters very hard. Moreover, the representation ability of models is limited since using rich contextual word features makes the search intractable. To overcome such efficiency and effectiveness limitations, the approximate inference and reranking techniques have been explored in previous work (Zhang and Clark, 2010; Jiang et al., 2008b). In this paper, we present an effective and efficient solution for joint Chinese word segmentation and POS tagging. Our work is motivated by several characteristics of this problem. First of all, a majority of words are easy to identify in the segmentation problem. For example, a simple maximum matching segmenter can achieve an f-score of about 90. We will show that it is possible to improve the efficiency and accuracy by using different strategies for different words. Second, segmenters designed with different views have complementary strength. We argue that the agreements and disagreement</context>
<context position="6466" citStr="Jiang et al., 2008" startWordPosition="994" endWordPosition="997">rical analyses. Section 5 concludes the paper. 2 Background 2.1 Problem Definition Given a sequence of characters c = (c1, ..., c#�), the task of word segmentation and POS tagging is to predict a sequence of word and POS tag pairs y = ((w1, p1), (w#y, p#y)), where wi is a word, pi is its POS tag, and a “#” symbol denotes the number of elements in each variable. In order to avoid error propagation and make use of POS information for word segmentation, the two tasks should resolved jointly. Previous research has shown that the integrated methods outperformed pipelined systems (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008). 2.2 Character-Based and Word-Based Methods Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words. In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. Both the IOB2 representation (Ramshaw and Marcus, 1995) and the Start/End representation (Kudo and Matsumoto, 2001) are popular. For example, the label B-NN indicates that a character is located at the begging of </context>
<context position="23771" citStr="Jiang et al., 2008" startWordPosition="3919" endWordPosition="3922">tary set S − Sl is used to train three coarse solvers SegWl, SegCl and SegTagLl, which process the Sl and provide inaccurate predictions. Then the inaccurate predictions are merged into subword sequences and Sl is extended to S0l. Finally, the sub-word tagger is trained on the whole extended data set S0. 4 Experiments 4.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the Penn Chinese Treebank (CTB) in experiments. We follow this set1390 ting in this paper. We use CTB 5.0 as our main corpus and define the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010). Table 2 shows the statistics of our experimental settings. Data set CTB files # of sent. # of words Training 1-270 18,089 493,939 400-931 1001-1151 Devel. 301-325 350 6821 Test 271-300 348 8008 Table 2: Training, development and test data on CTB 5.0 Three metrics are used for evaluation: precision (P), recall (R) and balanced f-score (F) defined by 2PR/(P+R). Precision is the relative amount of correct words in the system output. Recall is the relative amount of correct words compared to the gold standard annotations. For</context>
<context position="31077" citStr="Jiang et al., 2008" startWordPosition="5139" endWordPosition="5142">rted in a majority of previous work. The final results of our system are achieved by using 10-fold crossvalidation “C:±3 T:±1” models. The left most column indicates the reference of previous systems that represent state-of-the-art results. The comparison of the accuracy between our stacked sub-word system and the state-of-the-art systems in the literature indicates that our method is competitive with the best systems. Our system obtains the highest f-score performance on both segmentation and the whole task, resulting in error reductions of 14.1% and 5.5% respectively. 1392 Test Seg Seg&amp;Tag (Jiang et al., 2008a) 97.85 93.41 (Jiang et al., 2008b) 97.74 93.37 (Kruengkrai et al., 2009) 97.87 93.67 (Zhang and Clark, 2010) 97.78 93.67 Our system 98.17 94.02 Table 7: F-score performance on the test data. 5 Conclusion and Future Work This paper has described a stacked sub-word model for joint Chinese word segmentation and POS tagging. We defined a sub-word structure which maximizes the agreement of multiple segmentations provided by different segmenters. We showed that this sub-word structure could explore the complementary strength of different systems designed with different views. Moreover, the POS tag</context>
</contexts>
<marker>Jiang, Mi, Liu, 2008</marker>
<rawString>Wenbin Jiang, Haitao Mi, and Qun Liu. 2008b. Word lattice reranking for Chinese word segmentation and part-of-speech tagging. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 385–392, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Canasai Kruengkrai</author>
<author>Kiyotaka Uchimoto</author>
<author>Jun’ichi Kazama</author>
<author>Yiou Wang</author>
<author>Kentaro Torisawa</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint Chinese word segmentation and pos tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>513--521</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="23818" citStr="Kruengkrai et al., 2009" startWordPosition="3927" endWordPosition="3930">rse solvers SegWl, SegCl and SegTagLl, which process the Sl and provide inaccurate predictions. Then the inaccurate predictions are merged into subword sequences and Sl is extended to S0l. Finally, the sub-word tagger is trained on the whole extended data set S0. 4 Experiments 4.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the Penn Chinese Treebank (CTB) in experiments. We follow this set1390 ting in this paper. We use CTB 5.0 as our main corpus and define the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010). Table 2 shows the statistics of our experimental settings. Data set CTB files # of sent. # of words Training 1-270 18,089 493,939 400-931 1001-1151 Devel. 301-325 350 6821 Test 271-300 348 8008 Table 2: Training, development and test data on CTB 5.0 Three metrics are used for evaluation: precision (P), recall (R) and balanced f-score (F) defined by 2PR/(P+R). Precision is the relative amount of correct words in the system output. Recall is the relative amount of correct words compared to the gold standard annotations. For segmentation, a token is considered to be corr</context>
<context position="31151" citStr="Kruengkrai et al., 2009" startWordPosition="5151" endWordPosition="5154"> are achieved by using 10-fold crossvalidation “C:±3 T:±1” models. The left most column indicates the reference of previous systems that represent state-of-the-art results. The comparison of the accuracy between our stacked sub-word system and the state-of-the-art systems in the literature indicates that our method is competitive with the best systems. Our system obtains the highest f-score performance on both segmentation and the whole task, resulting in error reductions of 14.1% and 5.5% respectively. 1392 Test Seg Seg&amp;Tag (Jiang et al., 2008a) 97.85 93.41 (Jiang et al., 2008b) 97.74 93.37 (Kruengkrai et al., 2009) 97.87 93.67 (Zhang and Clark, 2010) 97.78 93.67 Our system 98.17 94.02 Table 7: F-score performance on the test data. 5 Conclusion and Future Work This paper has described a stacked sub-word model for joint Chinese word segmentation and POS tagging. We defined a sub-word structure which maximizes the agreement of multiple segmentations provided by different segmenters. We showed that this sub-word structure could explore the complementary strength of different systems designed with different views. Moreover, the POS tagging could be efficiently and effectively resolved over sub-word sequences</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>Canasai Kruengkrai, Kiyotaka Uchimoto, Jun’ichi Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi Isahara. 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and pos tagging. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 513–521, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In NAACL ’01: Second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies 2001,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6968" citStr="Kudo and Matsumoto, 2001" startWordPosition="1069" endWordPosition="1072">revious research has shown that the integrated methods outperformed pipelined systems (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008). 2.2 Character-Based and Word-Based Methods Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words. In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. Both the IOB2 representation (Ramshaw and Marcus, 1995) and the Start/End representation (Kudo and Matsumoto, 2001) are popular. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. Note that word segmentation can also be formulated as a sequential classification problem to predict whether a character is located at the beginning of, inside or at the end of a word. This character-by-character method for segmentation was first proposed in (Xue, 2003), and was then further used in POS tagging in (Ng and Low, 2004). One main disadvantage of this model is the difficulty in incorporating the whole </context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In NAACL ’01: Second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies 2001, pages 1–8, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Jin Kiat Low</author>
</authors>
<title>Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based?</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>277--284</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="6446" citStr="Ng and Low, 2004" startWordPosition="990" endWordPosition="993">l results and empirical analyses. Section 5 concludes the paper. 2 Background 2.1 Problem Definition Given a sequence of characters c = (c1, ..., c#�), the task of word segmentation and POS tagging is to predict a sequence of word and POS tag pairs y = ((w1, p1), (w#y, p#y)), where wi is a word, pi is its POS tag, and a “#” symbol denotes the number of elements in each variable. In order to avoid error propagation and make use of POS information for word segmentation, the two tasks should resolved jointly. Previous research has shown that the integrated methods outperformed pipelined systems (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008). 2.2 Character-Based and Word-Based Methods Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words. In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. Both the IOB2 representation (Ramshaw and Marcus, 1995) and the Start/End representation (Kudo and Matsumoto, 2001) are popular. For example, the label B-NN indicates that a character is locate</context>
</contexts>
<marker>Ng, Low, 2004</marker>
<rawString>Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based? In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 277– 284, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>950--958</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="10345" citStr="Nivre and McDonald, 2008" startWordPosition="1650" endWordPosition="1653">L (where gl = (gl1, ..., glK)) are seperately trained on S − Sl, and are used to construct the augmented dataset S� = {((xt, k1t , ..., kKt ), yt) : kkt = glk(xt) and xt E Sl}. Finally, each gk is trained on the original dataset and the second level predictor h is trained on S. The intent of the cross-validation scheme is that ykt is similar to the prediction produced by a predictor which is learned on a sample that does not include xt. Stacked learning has been applied as a system ensemble method in several NLP tasks, such as named entity recognition (Wu et al., 2003) and dependency parsing (Nivre and McDonald, 2008). This framework is also explored as a solution for learning nonlocal features in (Torres Martins et al., 2008). In the machine learning research, stacked learning has been applied to structured prediction (Cohen and Carvalho, 2005). In this work, stacked learning is used to acquire extended training data for sub-word tagging. 3 Method 3.1 Architecture In our stacked sub-word model, joint word segmentation and POS tagging is decomposed into two steps: (1) coarse-grained word segmentation and tagging, and (2) fine-grained sub-word tagging. The workflow is shown in Figure 1. In the first phase, </context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of ACL-08: HLT, pages 950–958, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd ACL/SIGDAT Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="6908" citStr="Ramshaw and Marcus, 1995" startWordPosition="1061" endWordPosition="1064"> word segmentation, the two tasks should resolved jointly. Previous research has shown that the integrated methods outperformed pipelined systems (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008). 2.2 Character-Based and Word-Based Methods Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words. In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. Both the IOB2 representation (Ramshaw and Marcus, 1995) and the Start/End representation (Kudo and Matsumoto, 2001) are popular. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. Note that word segmentation can also be formulated as a sequential classification problem to predict whether a character is located at the beginning of, inside or at the end of a word. This character-by-character method for segmentation was first proposed in (Xue, 2003), and was then further used in POS tagging in (Ng and Low, 2004). One main disadvantage</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the 3rd ACL/SIGDAT Workshop on Very Large Corpora, Cambridge, Massachusetts, USA, pages 82–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>Word-based and character-based word segmentation models: Comparison and combination.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>1211--1219</pages>
<location>Beijing, China,</location>
<contexts>
<context position="8402" citStr="Sun, 2010" startWordPosition="1315" endWordPosition="1316">rd as well as its possible POS tag. In particular, a word-based solver reads the input sentence from left to right, predicts whether the current piece of continuous characters is a word token and which class it belongs to. Solvers may use previously predicted words and their POS information as clues to find a new word. After one word is found and classified, solvers move on and search for the next possible word. This word-by-word method for segmentation was first proposed in (Zhang and Clark, 2007), 1386 and was then further used in POS tagging in (Zhang and Clark, 2008). In our previous work(Sun, 2010), we presented a theoretical and empirical comparative analysis of character-based and word-based methods for Chinese word segmentation. We showed that the two methods produced different distributions of segmentation errors in a way that could be explained by theoretical properties of the two models. A system combination method that leverages the complementary strength of word-based and character-based segmentation models was also successfully explored in their work. Different from our previous focus, the diversity of different models designed with different views is utilized to construct sub-</context>
<context position="13835" citStr="Sun, 2010" startWordPosition="2189" endWordPosition="2190">6) described a sub-word based tagging model to resolve word segmentation. To get the pieces which are larger than characters but smaller than words, they combine a character-based segmenter and a dictionary matching segmenter. Our contributions include (1) providing a formal definition of our sub-word structure that is based on multiple segmentations and (2) proposing a stacking method to acquire sub-words. 3.2 The Coarse-grained Solvers We systematically described the implementation of two state-of-the-art Chinese word segmenters in word-based and character-based architectures, respectively (Sun, 2010). Our word-based segmenter is based on a discriminative joint model with a first order semi-Markov structure, and the other segmenter is based on a first order Markov model. Exact Viterbi-style search algorithms are used for decoding. Limited to the document length, we do not give the description of the features. We refer readers to read the above paper for details. For parameter estimation, our work adopt the Passive-Aggressive (PA) framework (Crammer et al., 2006), a family of margin based online learning algorithms. In this work, we introduce two simple but important refinements: (1) to shu</context>
</contexts>
<marker>Sun, 2010</marker>
<rawString>Weiwei Sun. 2010. Word-based and character-based word segmentation models: Comparison and combination. In Coling 2010: Posters, pages 1211–1219, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Filipe Torres Martins</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>157--166</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="10456" citStr="Martins et al., 2008" startWordPosition="1670" endWordPosition="1673"> = {((xt, k1t , ..., kKt ), yt) : kkt = glk(xt) and xt E Sl}. Finally, each gk is trained on the original dataset and the second level predictor h is trained on S. The intent of the cross-validation scheme is that ykt is similar to the prediction produced by a predictor which is learned on a sample that does not include xt. Stacked learning has been applied as a system ensemble method in several NLP tasks, such as named entity recognition (Wu et al., 2003) and dependency parsing (Nivre and McDonald, 2008). This framework is also explored as a solution for learning nonlocal features in (Torres Martins et al., 2008). In the machine learning research, stacked learning has been applied to structured prediction (Cohen and Carvalho, 2005). In this work, stacked learning is used to acquire extended training data for sub-word tagging. 3 Method 3.1 Architecture In our stacked sub-word model, joint word segmentation and POS tagging is decomposed into two steps: (1) coarse-grained word segmentation and tagging, and (2) fine-grained sub-word tagging. The workflow is shown in Figure 1. In the first phase, one word-based segmenter (SegW) and one characterbased segmenter (SegC) are trained to produce word boundaries.</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>Andr´e Filipe Torres Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Wolpert</author>
</authors>
<title>Original contribution: Stacked generalization.</title>
<date>1992</date>
<journal>Neural Netw.,</journal>
<pages>5--241</pages>
<contexts>
<context position="9196" citStr="Wolpert, 1992" startWordPosition="1432" endWordPosition="1433">ferent distributions of segmentation errors in a way that could be explained by theoretical properties of the two models. A system combination method that leverages the complementary strength of word-based and character-based segmentation models was also successfully explored in their work. Different from our previous focus, the diversity of different models designed with different views is utilized to construct sub-word structures in this work. We will discuss the details in the next section. 2.3 Stacked Learning Stacked generalization is a meta-learning algorithm that was first proposed in (Wolpert, 1992) and (Breiman, 1996). The idea is to include two “levels” of predictors. The first level includes one or more predictors g1, ...gK : Rd —* R; each receives input x E Rd and outputs a prediction gk(x). The second level consists of a single function h : Rd+K —* R that takes as input (x, g1(x), ..., gK(x)) and outputs a final prediction y� = h(x, g1(x), ..., gK(x)). Training is done as follows. The training data S = {(xt, yt) : t E [1, T]} is split into L equal-sized disjoint subsets S1, ..., SL. Then functions g1, ..., gL (where gl = (gl1, ..., glK)) are seperately trained on S − Sl, and are use</context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>David H. Wolpert. 1992. Original contribution: Stacked generalization. Neural Netw., 5:241–259, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Grace Ngai</author>
<author>Marine Carpuat</author>
</authors>
<title>A stacked, voted, stacked model for named entity recognition.</title>
<date>2003</date>
<booktitle>In Walter Daelemans and</booktitle>
<pages>200--203</pages>
<editor>Miles Osborne, editors,</editor>
<contexts>
<context position="10295" citStr="Wu et al., 2003" startWordPosition="1643" endWordPosition="1646">ts S1, ..., SL. Then functions g1, ..., gL (where gl = (gl1, ..., glK)) are seperately trained on S − Sl, and are used to construct the augmented dataset S� = {((xt, k1t , ..., kKt ), yt) : kkt = glk(xt) and xt E Sl}. Finally, each gk is trained on the original dataset and the second level predictor h is trained on S. The intent of the cross-validation scheme is that ykt is similar to the prediction produced by a predictor which is learned on a sample that does not include xt. Stacked learning has been applied as a system ensemble method in several NLP tasks, such as named entity recognition (Wu et al., 2003) and dependency parsing (Nivre and McDonald, 2008). This framework is also explored as a solution for learning nonlocal features in (Torres Martins et al., 2008). In the machine learning research, stacked learning has been applied to structured prediction (Cohen and Carvalho, 2005). In this work, stacked learning is used to acquire extended training data for sub-word tagging. 3 Method 3.1 Architecture In our stacked sub-word model, joint word segmentation and POS tagging is decomposed into two steps: (1) coarse-grained word segmentation and tagging, and (2) fine-grained sub-word tagging. The w</context>
</contexts>
<marker>Wu, Ngai, Carpuat, 2003</marker>
<rawString>Dekai Wu, Grace Ngai, and Marine Carpuat. 2003. A stacked, voted, stacked model for named entity recognition. In Walter Daelemans and Miles Osborne, editors, Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 200–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="17095" citStr="Xue et al., 2005" startWordPosition="2772" endWordPosition="2775">Answer: [P] [JJ] [ NN ] [ SegW: [] [] [ ] [ ] SegC: [] [] [ ] [ SegTagL: [P] [JJ] [ NN ] [ CD Sub-words: [P] [JJ] [ NN ] [ B-CD ] . 3 5 分 居 领 先 地 位 CD ] [M] [VV] [ JJ ] [ NN ] [ ] [ ] [ ] [ ] ] [] [ ] [ ] ] [NT] [CD] [NT] [VV] [ VV ] [ NN ] [I-CD] [NT] [CD] [NT] [VV] [ VV ] [ NN ] Figure 2: An example phrase: 以总成绩355.35分居领先地位 (Being in front with a total score of 355.35 points). not be separated in the sub-word tagging step. The intuition is that strings which are consistently segmented by the different segmenters tend to be correct predictions. In our experiment on the Penn Chinese Treebank (Xue et al., 2005), the accuracy is 98.59% on the development data which is defined in the next section. The key point for the intermediate sub-word structures is to maximize the agreement of the three coarse-grained systems. In other words, the goal is to make merged sub-words as large as possible but not overlap with any predicted word produced by the three coarse-grained solvers. In particular, if the position between two continuous characters is predicted as a word boundary by any segmenter, this position is taken as a separation position of the sub-word sequence. This strategy makes sure that it is still p</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>In International Journal of Computational Linguistics and Chinese Language Processing.</booktitle>
<contexts>
<context position="7421" citStr="Xue, 2003" startWordPosition="1145" endWordPosition="1146"> into POS tags with boundary information. Both the IOB2 representation (Ramshaw and Marcus, 1995) and the Start/End representation (Kudo and Matsumoto, 2001) are popular. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. Note that word segmentation can also be formulated as a sequential classification problem to predict whether a character is located at the beginning of, inside or at the end of a word. This character-by-character method for segmentation was first proposed in (Xue, 2003), and was then further used in POS tagging in (Ng and Low, 2004). One main disadvantage of this model is the difficulty in incorporating the whole word information. The second kind of solution is the “word-based” method, where the basic predicting units are words themselves. This kind of solver sequentially decides whether the local sequence of characters makes up a word as well as its possible POS tag. In particular, a word-based solver reads the input sentence from left to right, predicts whether the current piece of continuous characters is a word token and which class it belongs to. Solver</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. In International Journal of Computational Linguistics and Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>840--847</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="8295" citStr="Zhang and Clark, 2007" startWordPosition="1294" endWordPosition="1297">s are words themselves. This kind of solver sequentially decides whether the local sequence of characters makes up a word as well as its possible POS tag. In particular, a word-based solver reads the input sentence from left to right, predicts whether the current piece of continuous characters is a word token and which class it belongs to. Solvers may use previously predicted words and their POS information as clues to find a new word. After one word is found and classified, solvers move on and search for the next possible word. This word-by-word method for segmentation was first proposed in (Zhang and Clark, 2007), 1386 and was then further used in POS tagging in (Zhang and Clark, 2008). In our previous work(Sun, 2010), we presented a theoretical and empirical comparative analysis of character-based and word-based methods for Chinese word segmentation. We showed that the two methods produced different distributions of segmentation errors in a way that could be explained by theoretical properties of the two models. A system combination method that leverages the complementary strength of word-based and character-based segmentation models was also successfully explored in their work. Different from our pr</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840–847, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>888--896</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="6491" citStr="Zhang and Clark, 2008" startWordPosition="998" endWordPosition="1001">on 5 concludes the paper. 2 Background 2.1 Problem Definition Given a sequence of characters c = (c1, ..., c#�), the task of word segmentation and POS tagging is to predict a sequence of word and POS tag pairs y = ((w1, p1), (w#y, p#y)), where wi is a word, pi is its POS tag, and a “#” symbol denotes the number of elements in each variable. In order to avoid error propagation and make use of POS information for word segmentation, the two tasks should resolved jointly. Previous research has shown that the integrated methods outperformed pipelined systems (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008). 2.2 Character-Based and Word-Based Methods Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words. In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. Both the IOB2 representation (Ramshaw and Marcus, 1995) and the Start/End representation (Kudo and Matsumoto, 2001) are popular. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method</context>
<context position="8369" citStr="Zhang and Clark, 2008" startWordPosition="1308" endWordPosition="1311">he local sequence of characters makes up a word as well as its possible POS tag. In particular, a word-based solver reads the input sentence from left to right, predicts whether the current piece of continuous characters is a word token and which class it belongs to. Solvers may use previously predicted words and their POS information as clues to find a new word. After one word is found and classified, solvers move on and search for the next possible word. This word-by-word method for segmentation was first proposed in (Zhang and Clark, 2007), 1386 and was then further used in POS tagging in (Zhang and Clark, 2008). In our previous work(Sun, 2010), we presented a theoretical and empirical comparative analysis of character-based and word-based methods for Chinese word segmentation. We showed that the two methods produced different distributions of segmentation errors in a way that could be explained by theoretical properties of the two models. A system combination method that leverages the complementary strength of word-based and character-based segmentation models was also successfully explored in their work. Different from our previous focus, the diversity of different models designed with different vi</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Proceedings ofACL-08: HLT, pages 888–896, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and POS-tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>843--852</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="2196" citStr="Zhang and Clark, 2010" startWordPosition="321" endWordPosition="324">hown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation. A challenge for joint approaches is the large combined search space, which makes efficient decoding and structured learning of parameters very hard. Moreover, the representation ability of models is limited since using rich contextual word features makes the search intractable. To overcome such efficiency and effectiveness limitations, the approximate inference and reranking techniques have been explored in previous work (Zhang and Clark, 2010; Jiang et al., 2008b). In this paper, we present an effective and efficient solution for joint Chinese word segmentation and POS tagging. Our work is motivated by several characteristics of this problem. First of all, a majority of words are easy to identify in the segmentation problem. For example, a simple maximum matching segmenter can achieve an f-score of about 90. We will show that it is possible to improve the efficiency and accuracy by using different strategies for different words. Second, segmenters designed with different views have complementary strength. We argue that the agreeme</context>
<context position="23842" citStr="Zhang and Clark, 2010" startWordPosition="3931" endWordPosition="3934">and SegTagLl, which process the Sl and provide inaccurate predictions. Then the inaccurate predictions are merged into subword sequences and Sl is extended to S0l. Finally, the sub-word tagger is trained on the whole extended data set S0. 4 Experiments 4.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the Penn Chinese Treebank (CTB) in experiments. We follow this set1390 ting in this paper. We use CTB 5.0 as our main corpus and define the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010). Table 2 shows the statistics of our experimental settings. Data set CTB files # of sent. # of words Training 1-270 18,089 493,939 400-931 1001-1151 Devel. 301-325 350 6821 Test 271-300 348 8008 Table 2: Training, development and test data on CTB 5.0 Three metrics are used for evaluation: precision (P), recall (R) and balanced f-score (F) defined by 2PR/(P+R). Precision is the relative amount of correct words in the system output. Recall is the relative amount of correct words compared to the gold standard annotations. For segmentation, a token is considered to be correct if its boundaries ma</context>
<context position="31187" citStr="Zhang and Clark, 2010" startWordPosition="5157" endWordPosition="5160">lidation “C:±3 T:±1” models. The left most column indicates the reference of previous systems that represent state-of-the-art results. The comparison of the accuracy between our stacked sub-word system and the state-of-the-art systems in the literature indicates that our method is competitive with the best systems. Our system obtains the highest f-score performance on both segmentation and the whole task, resulting in error reductions of 14.1% and 5.5% respectively. 1392 Test Seg Seg&amp;Tag (Jiang et al., 2008a) 97.85 93.41 (Jiang et al., 2008b) 97.74 93.37 (Kruengkrai et al., 2009) 97.87 93.67 (Zhang and Clark, 2010) 97.78 93.67 Our system 98.17 94.02 Table 7: F-score performance on the test data. 5 Conclusion and Future Work This paper has described a stacked sub-word model for joint Chinese word segmentation and POS tagging. We defined a sub-word structure which maximizes the agreement of multiple segmentations provided by different segmenters. We showed that this sub-word structure could explore the complementary strength of different systems designed with different views. Moreover, the POS tagging could be efficiently and effectively resolved over sub-word sequences. To train a good sub-word tagger, w</context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 843–852, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Genichiro Kikui</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Subword-based tagging by conditional random fields for Chinese word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>193--196</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="13227" citStr="Zhang et al. (2006)" startWordPosition="2103" endWordPosition="2106">words from a subword sequence is very high. The quality of the final tagger relies on the quality of the sub-word tagger. If a high performance sub-word tagger can be constructed, the whole task can be well resolved. The statistics will also empirically show that subwords are significantly larger than characters and only slightly smaller than words. As a result, the search space of the sub-word tagging is significantly shrunken, and exact Viterbi decoding without approximately pruning can be efficiently processed. This property makes nearly all popular sequence labeling algorithms applicable. Zhang et al. (2006) described a sub-word based tagging model to resolve word segmentation. To get the pieces which are larger than characters but smaller than words, they combine a character-based segmenter and a dictionary matching segmenter. Our contributions include (1) providing a formal definition of our sub-word structure that is based on multiple segmentations and (2) proposing a stacking method to acquire sub-words. 3.2 The Coarse-grained Solvers We systematically described the implementation of two state-of-the-art Chinese word segmenters in word-based and character-based architectures, respectively (Su</context>
</contexts>
<marker>Zhang, Kikui, Sumita, 2006</marker>
<rawString>Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita. 2006. Subword-based tagging by conditional random fields for Chinese word segmentation. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 193–196, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>