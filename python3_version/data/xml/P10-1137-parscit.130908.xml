<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000280">
<title confidence="0.997972">
Multilingual Pseudo-Relevance Feedback: Performance Study of
Assisting Languages
</title>
<author confidence="0.983795">
Manoj K. Chinnakotla Karthik Raman Pushpak Bhattacharyya
</author>
<affiliation confidence="0.886806666666667">
Department of Computer Science and Engineering
Indian Institute of Technology, Bombay,
Mumbai, India
</affiliation>
<email confidence="0.998577">
{manoj,karthikr,pb}@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.997385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997939375">
In a previous work of ours Chinnakotla
et al. (2010) we introduced a novel
framework for Pseudo-Relevance Feed-
back (PRF) called MultiPRF. Given a
query in one language called Source, we
used English as the Assisting Language to
improve the performance of PRF for the
source language. MulitiPRF showed re-
markable improvement over plain Model
Based Feedback (MBF) uniformly for 4
languages, viz., French, German, Hungar-
ian and Finnish with English as the as-
sisting language. This fact inspired us
to study the effect of any source-assistant
pair on MultiPRF performance from out
of a set of languages with widely differ-
ent characteristics, viz., Dutch, English,
Finnish, French, German and Spanish.
Carrying this further, we looked into the
effect of using two assisting languages to-
gether on PRF.
The present paper is a report of these in-
vestigations, their results and conclusions
drawn therefrom. While performance im-
provement on MultiPRF is observed what-
ever the assisting language and whatever
the source, observations are mixed when
two assisting languages are used simul-
taneously. Interestingly, the performance
improvement is more pronounced when
the source and assisting languages are
closely related, e.g., French and Spanish.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963166666667">
The central problem of Information Retrieval (IR)
is to satisfy the user’s information need, which is
typically expressed through a short (typically 2-3
words) and often ambiguous query. The problem
of matching the user’s query to the documents is
rendered difficult by natural language phenomena
like morphological variations, polysemy and syn-
onymy. Relevance Feedback (RF) tries to over-
come these problems by eliciting user feedback
on the relevance of documents obtained from the
initial ranking and then uses it to automatically
refine the query. Since user input is hard to ob-
tain, Pseudo-Relevance Feedback (PRF) (Buckley
et al., 1994; Xu and Croft, 2000; Mitra et al., 1998)
is used as an alternative, wherein RF is performed
by assuming the top k documents from the initial
retrieval as being relevant to the query. Based on
the above assumption, the terms in the feedback
document set are analyzed to choose the most dis-
tinguishing set of terms that characterize the feed-
back documents and as a result the relevance of
a document. Query refinement is done by adding
the terms obtained through PRF, along with their
weights, to the actual query.
Although PRF has been shown to improve re-
trieval, it suffers from the following drawbacks:
(a) the type of term associations obtained for query
expansion is restricted to co-occurrence based re-
lationships in the feedback documents, and thus
other types of term associations such as lexical and
semantic relations (morphological variants, syn-
onyms) are not explicitly captured, and (b) due to
the inherent assumption in PRF, i.e., relevance of
top k documents, performance is sensitive to that
of the initial retrieval algorithm and as a result is
not robust.
</bodyText>
<subsubsectionHeader confidence="0.626518">
Multilingual Pseudo-Relevance Feedback
</subsubsectionHeader>
<bodyText confidence="0.9990738">
(MultiPRF) (Chinnakotla et al., 2010) is a novel
framework for PRF to overcome both the above
limitations of PRF. It does so by taking the help of
a different language called the assisting language.
In MultiPRF, given a query in source language
L1, the query is automatically translated into
the assisting language L2 and PRF performed
in the assisting language. The resultant terms
are translated back into L1 using a probabilistic
bi-lingual dictionary. The translated feedback
</bodyText>
<page confidence="0.932585">
1346
</page>
<note confidence="0.942189">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1346–1356,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999765911764706">
model, is then combined with the original feed-
back model of Li to obtain the final model which
is used to re-rank the corpus. MulitiPRF showed
remarkable improvement on standard CLEF
collections over plain Model Based Feedback
(MBF) uniformly for 4 languages, viz., French,
German, Hungarian and Finnish with English as
the assisting language. This fact inspired us to
study the effect of any source-assistant pair on
PRF performance from out of a set of languages
with widely different characteristics, viz., Dutch,
English, Finnish, French, German and Spanish.
Carrying this further, we looked into the effect of
using two assisting languages together on PRF.
The present paper is a report of these in-
vestigations, their results and conclusions drawn
therefrom. While performance improvement on
PRF is observed whatever the assisting language
and whatever the source, observations are mixed
when two assisting languages are used simulta-
neously. Interestingly, the performance improve-
ment is more pronounced when the source and as-
sisting languages are closely related, e.g., French
and Spanish.
The paper is organized as follows: Section 2,
discusses the related work. Section 3, explains the
Language Modeling (LM) based PRF approach.
Section 4, describes the MultiPRF approach. Sec-
tion 5 discusses the experimental set up. Section 6
presents the results, and studies the effect of vary-
ing the assisting language and incorporates mul-
tiple assisting languages. Finally, Section 7 con-
cludes the paper by summarizing and outlining fu-
ture work.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999978373134328">
PRF has been successfully applied in various IR
frameworks like vector space models, probabilis-
tic IR and language modeling (Buckley et al.,
1994; Jones et al., 2000; Lavrenko and Croft,
2001; Zhai and Lafferty, 2001). Several ap-
proaches have been proposed to improve the per-
formance and robustness of PRF. Some of the rep-
resentative techniques are (i) Refining the feed-
back document set (Mitra et al., 1998; Sakai et
al., 2005), (ii) Refining the terms obtained through
PRF by selecting good expansion terms (Cao et
al., 2008) and (iii) Using selective query expan-
sion (Amati et al., 2004; Cronen-Townsend et al.,
2004) and (iv) Varying the importance of docu-
ments in the feedback set (Tao and Zhai, 2006).
Another direction of work, often reported in the
TREC Robust Track, is to use a large external col-
lection like Wikipedia or the Web as a source of
expansion terms (Xu et al., 2009; Voorhees, 2006).
The intuition behind the above approach is that
if the query does not have many relevant docu-
ments in the collection then any improvements in
the modeling of PRF is bound to perform poorly
due to query drift.
Several approaches have been proposed for
including different types of lexically and se-
mantically related terms during query expansion.
Voorhees (1994) use Wordnet for query expan-
sion and report negative results. Recently, random
walk models (Lafferty and Zhai, 2001; Collins-
Thompson and Callan, 2005) have been used to
learn a rich set of term level associations by com-
bining evidence from various kinds of information
sources like WordNet, Web etc. Metzler and Croft
(2007) propose a feature based approach called la-
tent concept expansion to model term dependen-
cies.
All the above mentioned approaches use the re-
sources available within the language to improve
the performance of PRF. However, we make use of
a second language to improve the performance of
PRF. Our proposed approach is especially attrac-
tive in the case of resource-constrained languages
where the original retrieval is bad due to poor cov-
erage of the collection and/or inherent complexity
of query processing (for example term conflation)
in those languages.
Jourlin et al. (1999) use parallel blind relevance
feedback, i.e. they use blind relevance feedback on
a larger, more reliable parallel corpus, to improve
retrieval performance on imperfect transcriptions
of speech. Another related idea is by Xu et al.
(2002), where a statistical thesaurus is learned us-
ing the probabilistic bilingual dictionaries of Ara-
bic to English and English to Arabic. Meij et
al. (2009) tries to expand a query in a differ-
ent language using language models for domain-
specific retrieval, but in a very different setting.
Since our method uses a corpus in the assisting
language from a similar time period, it can be
likened to the work by Talvensaari et al. (2007)
who used comparable corpora for Cross-Lingual
Information Retrieval (CLIR). Other work pertain-
ing to document alignment in comparable corpora,
such as Braschler and Sch¨auble (1998), Lavrenko
et al. (2002), also share certain common themes
with our approach. Recent work by Gao et al.
</bodyText>
<page confidence="0.989666">
1347
</page>
<bodyText confidence="0.998636625">
(2008) uses English to improve the performance
over a subset of Chinese queries whose transla-
tions in English are unambiguous. They use inter-
document similarities across languages to improve
the ranking performance. However, cross lan-
guage document similarity measurement is in it-
self known to be an hard problem and the scale of
their experimentation is quite small.
</bodyText>
<sectionHeader confidence="0.942273" genericHeader="method">
3 PRF in the LM Framework
</sectionHeader>
<bodyText confidence="0.999837111111111">
The Language Modeling (LM) Framework allows
PRF to be modelled in a principled manner. In the
LM approach, documents and queries are modeled
using multinomial distribution over words called
document language model P(w|D) and query lan-
guage model P(w|OQ) respectively. For a given
query, the document language models are ranked
based on their proximity to the query language
model, measured using KL-Divergence.
</bodyText>
<equation confidence="0.999857333333333">
P (w|ΘQ)
P(w|ΘQ) · log
P (w|D)
</equation>
<bodyText confidence="0.999501333333333">
Since the query length is short, it is difficult to es-
timate OQ accurately using the query alone. In
PRF, the top k documents obtained through the ini-
tial ranking algorithm are assumed to be relevant
and used as feedback for improving the estima-
tion of OQ. The feedback documents contain both
relevant and noisy terms from which the feedback
language model is inferred based on a Generative
Mixture Model (Zhai and Lafferty, 2001).
Let DF = {d1, d2, ... , dkI be the top k docu-
ments retrieved using the initial ranking algorithm.
Zhai and Lafferty (Zhai and Lafferty, 2001) model
the feedback document set DF as a mixture of two
distributions: (a) the feedback language model and
(b) the collection model P(w|C). The feedback
language model is inferred using the EM Algo-
rithm (Dempster et al., 1977), which iteratively
accumulates probability mass on the most distin-
guishing terms, i.e. terms which are more fre-
quent in the feedback document set than in the
entire collection. To maintain query focus the fi-
nal converged feedback model, OF is interpolated
with the initial query model OQ to obtain the final
query model OFinal.
</bodyText>
<equation confidence="0.601034">
ΘF inal = (1 − α) · ΘQ + α · ΘF
</equation>
<bodyText confidence="0.887665666666667">
OFinal is used to re-rank the corpus using the
KL-Divergence ranking function to obtain the fi-
nal ranked list of documents. Henceforth, we refer
</bodyText>
<figureCaption confidence="0.999875">
Figure 1: Schematic of the Multilingual PRF Approach
</figureCaption>
<figure confidence="0.9480476">
Symbol Description
ΘQ Query Language Model
ΘF Feedback Language Model obtained from PRF in L1
L1
ΘF Feedback Language Model obtained from PRF in L2
L2
ΘT rans Feedback Model Translated from L2 to L1
L1
t(f|e) Probabilistic Bi-Lingual Dictionary from L2 to L1
β, γ Interpolation coefficients coefficients used in MultiPRF
</figure>
<tableCaption confidence="0.978334">
Table 2: Glossary of Symbols used in explaining MultiPRF
</tableCaption>
<bodyText confidence="0.9960145">
to the above technique as Model Based Feedback
(MBF).
</bodyText>
<sectionHeader confidence="0.998442" genericHeader="method">
4 Multilingual PRF (MultiPRF)
</sectionHeader>
<bodyText confidence="0.998182222222222">
The schematic of the MultiPRF approach is shown
in Figure 1. Given a query Q in the source lan-
guage L1, we automatically translate the query
into the assisting language L2. We then rank the
documents in the L2 collection using the query
likelihood ranking function (John Lafferty and
Chengxiang Zhai, 2003). Using the top k doc-
uments, we estimate the feedback model using
MBF as described in the previous section. Simi-
larly, we also estimate a feedback model using the
original query and the top k documents retrieved
from the initial ranking in L1. Let the resultant
feedback models be OFL� and OL1
F respectively.
The feedback model estimated in the assisting lan-
guage OFL� is translated back into language L1
using a probabilistic bi-lingual dictionary t(f|e)
from L2 —* L1 as follows:
</bodyText>
<equation confidence="0.993784">
t(f|e) · P(e|ΘFL2) (1)
</equation>
<bodyText confidence="0.950659">
The probabilistic bi-lingual dictionary t(f|e) is
</bodyText>
<figure confidence="0.992036">
Query in L1 Translated Query
Initial Retrieval
Algorithm
(LM Based Query
Likelihood)
to L2
Initial Retrieval
Algorithm
(LM Based Query
Likelihood)
Top V Results
L2 Index
PRF PRF
Feedback (Model Based Feedback (Model Based
Feedback) Feedback)
Model OL1 Model OL2
Query Feedback Relevance Model
Model Model Interpolation Translated Translation
OQ Feedback
Model
KL-Divergence
Ranking Function
Final Ranked List
Of Documents in L1
Probabilistic
Dictionary
L2 → L1
L1 Index
Top V Results
�
</figure>
<equation confidence="0.601129833333333">
KL(ΘQ||D) =
w
�
P (f|ΘT rans
L1 ) =
Vein L2
</equation>
<page confidence="0.942948">
1348
</page>
<table confidence="0.999818545454546">
Language CLEF Collection Description No. of No. of Unique CLEF Topics
Identifier Documents Terms (No. of Topics)
English EN-00+01+02 LA Times 94 113005 174669 -
EN-03+05+06 LA Times 94, Glasgow Herald 95 169477 234083 -
EN-02+03 LA Times 94, Glasgow Herald 95 169477 234083 91-200 (67)
French FR-00 Le Monde 94 44013 127065 1-40 (29)
FR-01+02 Le Monde 94, French SDA 94 87191 159809 41-140 (88)
FR-02+03 Le Monde 94, French SDA 94-95 129806 182214 91-200 (67)
FR-03+05 Le Monde 94, French SDA 94-95 129806 182214 141-200,251-300 (99)
FR-06 Le Monde 94-95, French SDA 94-95 177452 231429 301-350 (48)
German DE-00 Frankfurter Rundschau 94, Der Spiegel 94-95 153694 791093 1-40 (33)
DE-01+02 Frankfurter Rundschau 94, Der Spiegel 94-95, 225371 782304 41-140 (85)
German SDA 94
DE-02+03 294809 867072 91-200 (67)
Frankfurter Rundschau 94, Der Spiegel 94-95,
German SDA 94-95
DE-03 Frankfurter Rundschau 94, Der Spiegel 94-95, 294809 867072 141-200 (51)
German SDA 94-95
Finnish FI-02+03+04 Aamulehti 94-95 55344 531160 91-250 (119)
FI-02+03 Aamulehti 94-95 55344 531160 91-200 (67)
Dutch NL-02+03 95C Handelsblad 94-95, Algemeen Dagblad 94- 190604 575582 91-200 (67)
Spanish ES-02+03 EFE 94, EFE 95 454045 340250 91-200 (67)
</table>
<tableCaption confidence="0.998169">
Table 1: Details of the CLEF Datasets used for Evaluating the MultiPRF approach. The number shown in brackets of the final
column CLEF Topics indicate the actual number of topics used during evaluation.
</tableCaption>
<subsectionHeader confidence="0.8130485">
Source Term Top Aligned Terms in Target
French English
</subsectionHeader>
<bodyText confidence="0.993748666666667">
am´ericain american, us, united, state, america
nation nation, un, united, state, country
e´tude study, research, assess, investigate, survey
</bodyText>
<subsectionHeader confidence="0.908894">
German English
</subsectionHeader>
<bodyText confidence="0.85567">
flugzeug aircraft, plane, aeroplane, air, flight
spiele play, game, stake, role, player
verh¨altnis relationship, relate, balance, proportion
</bodyText>
<tableCaption confidence="0.858416">
Table 3: Top Translation Alternatives for some sample words
in Probabilistic Bi-Lingual Dictionary
</tableCaption>
<bodyText confidence="0.999916590909091">
learned from a parallel sentence-aligned corpora
in L1 −L2 based on word level alignments. Tiede-
mann (Tiedemann, 2001) has shown that the trans-
lation alternatives found using word alignments
could be used to infer various morphological and
semantic relations between terms. In Table 3,
we show the top translation alternatives for some
sample words. For example, the French word
am´ericain (american) brings different variants of
the translation like american, america, us, united,
state, america which are lexically and semanti-
cally related. Hence, the probabilistic bi-lingual
dictionary acts as a rich source of morphologically
and semantically related feedback terms. Thus,
during this step, of translating the feedback model
as given in Equation 1, the translation model adds
related terms in L1 which have their source as the
term from feedback model 0&apos; . The final Multi-
PRF model is obtained by interpolating the above
translated feedback model with the original query
model and the feedback model of language L1 as
given below:
</bodyText>
<equation confidence="0.996724">
®����� � (1 − p − �) - ®� + p - ®� �1 + � - ®� �an� (2)
�1 �1
</equation>
<bodyText confidence="0.999784">
Since we want to retain the query focus during
back translation the feedback model in L2 is inter-
polated with the translated query before transla-
tion of the L2 feedback model. The parameters Q
and -y control the relative importance of the orig-
inal query model, feedback model of L1 and the
translated feedback model obtained from L1 and
are tuned based on the choice of L1 and L2.
</bodyText>
<sectionHeader confidence="0.997114" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999994296296296">
We evaluate the performance of our system us-
ing the standard CLEF evaluation data in six lan-
guages, widely varying in their familial relation-
ships - Dutch, German, English, French, Span-
ish and Finnish using more than 600 topics. The
details of the collections and their corresponding
topics used for MultiPRF are given in Table 1.
Note that, in each experiment, we choose assist-
ing collections such that the topics in the source
language are covered in the assisting collection so
as to get meaningful feedback terms. In all the top-
ics, we only use the title field. We ignore the top-
ics which have no relevant documents as the true
performance on those topics cannot be evaluated.
We demonstrate the performance of MultiPRF
approach with French, German and Finnish as
source languages and Dutch, English and Span-
ish as the assisting language. We later vary the
assisting language, for each source language and
study the effects. We use the Terrier IR platform
(Ounis et al., 2005) for indexing the documents.
We perform standard tokenization, stop word re-
moval and stemming. We use the Porter Stemmer
for English and the stemmers available through the
Snowball package for other languages. Other than
these, we do not perform any language-specific
processing on the languages. In case of French,
</bodyText>
<page confidence="0.943799">
1349
</page>
<table confidence="0.999994777777778">
Collection Assist. PO5 PO10 MAP GMAP
Lang
MBF MultiPRF % Impr. MBF MultiPRF % Impr. MBF MultiPRF % Impr. MBF MultiPRF % Impr.
EN 0.5241 11.76* 0.4000 0.00 0.4393 4.10 0.3413 15.27
FR-00 ES 0.4690 0.5034 7.35* 0.4000 0.4103 2.59 0.4220 0.4418 4.69 0.2961 0.3382 14.22
NL 0.5034 7.35 0.4103 2.59 0.4451 5.47 0.3445 16.34
EN 0.4818 3.92 0.4386 7.82* 0.4535 4.43* 0.2721 13.61
FR-01+02 ES 0.4636 0.4977 7.35* 0.4068 0.4363 7.26* 0.4342 0.4416 1.70 0.2395 0.2349 -1.92
NL 0.4818 3.92 0.4409 8.38* 0.4375 0.76 0.2534 5.80
EN 0.4768 4.89* 0.4202 4* 0.3694 4.67* 0.1411 6.57
FR-03+05 ES 0.4545 0.4727 4.00 0.4040 0.4080 1.00 0.3529 0.3582 1.50 0.1324 0.1325 0.07
NL 0.4525 -0.44 0.4010 -0.75 0.3513 0.45 0.1319 -0.38
EN 0.5083 3.39 0.4729 2.25 0.4104 6.97 0.2810 29.25
FR-06 ES 0.4917 0.5083 3.39 0.4625 0.4687 1.35 0.3837 0.3918 2.12 0.2174 0.2617 20.38
NL 0.5083 3.39 0.4646 0.45 0.3864 0.71 0.2266 4.23
EN 0.3212 39.47* 0.2939 22.78* 0.2273 5.31 0.0191 730.43
DE-00 ES 0.2303 0.3212 39.47* 0.2394 0.2818 17.71* 0.2158 0.2376 10.09 0.0023 0.0123 434.78
NL 0.3151 36.82* 0.2818 17.71* 0.2331 8.00 0.0122 430.43
EN 0.6000 12.34* 0.5318 9.35* 0.4576 8.2* 0.2721 9.19
DE-01+02 ES 0.5341 0.5682 6.39* 0.4864 0.5091 4.67* 0.4229 0.4459 5.43 0.1765 0.2309 30.82
NL 0.5773 8.09* 0.5114 5.15* 0.4498 6.35* 0.2355 33.43
EN 0.5412 6.15 0.4980 4.10 0.4355 1.91 0.1771 42.48
DE-03 ES 0.5098 0.5647 10.77* 0.4784 0.4980 4.10 0.4274 0.4568 6.89* 0.1243 0.1645 32.34
NL 0.5529 8.45* 0.4941 3.27 0.4347 1.72 0.1490 19.87
EN 0.4034 6.67* 0.3319 8.52* 0.4246 7.06* 0.2272 69.05
FI-02+03+04 ES 0.3782 0.3879 2.58 0.3059 0.3267 6.81 0.3966 0.3881 -2.15 0.1344 0.1755 30.58
NL 0.3948 4.40 0.3301 7.92 0.4077 2.79 0.1839 36.83
</table>
<tableCaption confidence="0.951394">
Table 4: Results comparing the performance of MultiPRF over baseline MBF on CLEF collections with English (EN), Spanish
(ES) and Dutch (NL) as assisting languages. Results marked as $ indicate that the improvement was found to be statistically
significant over the baseline at 90% confidence level (α = 0.01) when tested using a paired two-tailed t-test.
</tableCaption>
<bodyText confidence="0.999762538461539">
since some function words like l’, d’ etc., occur as
prefixes to a word, we strip them off during index-
ing and query processing, since it significantly im-
proves the baseline performance. We use standard
evaluation measures like MAP, P@S and P@10
for evaluation. Additionally, for assessing robust-
ness, we use the Geometric Mean Average Preci-
sion (GMAP) metric (Robertson, 2006) which is
also used in the TREC Robust Track (Voorhees,
2006). The probabilistic bi-lingual dictionary used
in MultiPRF was learnt automatically by running
GIZA++: a word alignment tool (Och and Ney,
2003) on a parallel sentence aligned corpora. For
all the above language pairs we used the Europarl
Corpus (Philipp, 2005). We use Google Trans-
late as the query translation system as it has been
shown to perform well for the task (Wu et al.,
2008). We use the MBF approach explained in
Section 3 as a baseline for comparison. We use
two-stage Dirichlet smoothing with the optimal
parameters tuned based on the collection (Zhai and
Lafferty, 2004). We tune the parameters of MBF,
specifically A and α, and choose the values which
give the optimal performance on a given collec-
tion. We uniformly choose the top ten documents
for feedback. Table 4 gives the overall results.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="evaluation">
6 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999982055555556">
In Table 4, we see the performance of the Multi-
PRF approach for three assisting languages, and
how it compares with the baseline MBF meth-
ods. We find MultiPRF to consistently outperform
the baseline value on all metrics, namely MAP
(where significant improvements range from 4.4%
to 7.1%); P@5 (significant improvements range
from 4.9% to 39.5% and P@10 (where MultiPRF
has significant gains varying from 4% to 22.8%).
Additionally we also find MultiPRF to be more ro-
bust than the baseline, as indicated by the GMAP
score, where improvements vary from 4.2% to
730%. Furthermore we notice these trends hold
across different assisting languages, with Span-
ish and Dutch outperforming English as the as-
sisting language on some of the French and Ger-
man collections. On performing a more detailed
study of the results we identify the main reason
for improvements in our approach is the ability to
obtain good feedback terms in the assisting lan-
guage coupled with the introduction of lexically
and semantically related terms during the back-
translation step.
In Table 5, we see some examples, which illus-
trates the feedback terms brought by the MultiPRF
method. As can be seen by these example, the
gains achieved by MultiPRF are primarily due to
one of three reasons: (a) Good Feedback in As-
sisting Language: If the feedback model in the
assisting language contains good terms, then the
back-translation process will introduce the corre-
sponding feedback terms in the source language,
thus leading to improved performance. As an
example of this phenomena, consider the French
Query “Maladie de Creutzfeldt-Jakob”. In this
case the original feedback model also performs
</bodyText>
<page confidence="0.8875">
1350
</page>
<table confidence="0.937621126760563">
MBF-Top Representative Terms
(With Meaning) Excl. Query
Terms
exxon, million, ol (oil), tonn,
russisch (russian), olp (oil),
moskau (moscow), us
TOPIC NO ASSIST SOURCE LANGUAGE TRANSLATED QUERY MBF MPRF
LANG. QUERY QUERY MEANING MAP MAP
GERMAN &apos;01: EN Ölkatastrophe in EN Siberian Oil Engineering
TOPIC 61 ES Sibirien Bronchialasthma 0.618 0.812
GERMAN &apos;02: NL Ingénierie génétique Oil Spill in Siberia Catastrophe 0.636
TOPIC 105 EN Maladie de El asma bronquial Bronchial 0.357
FRENCH &apos;02: Creutzfeldt-Jakob Genetische 0.062 0.688
TOPIC 107 Siegerinnen von Manipulatie Asthma 0.146
FRENCH &apos;06: Wimbledon Creutzfeldt-Jakob Genetic
TOPIC 256 Champions of 0.145
GERMAN &apos;03: Wimbledon Creutzfeldt-
TOPIC 157 Jakob Disease 0.507
Wimbledon
0.074
Lady Winners
GERMAN &apos;01: ES Fusion japanischer AI in Lateinamerika 0.098
TOPIC 91 EN Banken La gripe aviar en AI in Latin 0.264
GERMAN &apos;03: NL Les droits de l&apos;enfant 0.456 0.284
TOPIC 196 América Latina America
FRENCH &apos;03: Fusion of Japanese Merger of
TOPIC 152 banks Japanese Banks 0.572
De rechten van het
Child Rights 0.479
kind
MultiPRF- Top Representative
Terms (With Meaning) Excl. Query
Terms
olverschmutz (oil pollution), ol,
russisch, erdol (petroleum), russland
(russia), olunfall(oil spill), olp
kernfusion (nuclear fusion),
zentralbank (central bank), daiwa,
weltbank (world bank),
investitionsbank (investment bank)
per (father), convent, franc, jurid
(legal), homm (man), cour (court),
biolog
daiwa, tokyo, filial (branch),
zusammenschluss (merger)
convent (convention), franc,
international, onun (united
nations), réserv (reserve)
chronisch (chronic), pet, athlet asthma, allergi, krankheit (disease),
(athlete), ekrank (ill), gesund allerg (allergenic), chronisch,
(healthy), tuberkulos hauterkrank (illness of skin), arzt
(tuberculosis), patient, reis (rice), (doctor), erkrank (ill)
person
développ (developed), évolu
genetic, gen, engineering, développ,
(evolved), product, produit
product
(product), moléculair (molecular)
malad (illness), produit (product), malad, humain (human), bovin
animal (animal), hormon (bovine), encéphalopath (suffering
(hormone) from encephalitis), scientif, recherch
(research)
telefonbuch (phone book), sieg gross (large), verfecht (champion),
(victory), titelseit (front page), sampra (sampras), 6, champion,
telekom (telecommunication), steffi, verteidigt (defendending),
graf martina, jovotna, navratilova
international, amnesty,
strassenkind (street child), karib (Caribbean), land, brasili,
kolumbi (Columbian), land, brasili schuld (blame), amerika, kalt (cold),
(Brazil), menschenrecht (human welt (world), forschung (research)
rights), polizei (police)
</table>
<tableCaption confidence="0.989202">
Table 5: Qualitative comparison of feedback terms given by MultiPRF and MBF on representative queries where positive and
negative results were observed in French and German collections.
</tableCaption>
<bodyText confidence="0.999885515151515">
quite strongly with a MAP score of 0.507. Al-
though there is no significant topic drift in this
case, there are not many relevant terms apart from
the query terms. However the same query per-
forms very well in English with all the documents
in the feedback set of the English corpus being rel-
evant, thus resulting in informative feedback terms
such as {bovin, scientif, recherch}. (b) Finding
Synonyms/Morphological Variations: Another sit-
uation in which MultiPRF leads to large improve-
ments is when it finds semantically/lexically re-
lated terms to the query terms which the origi-
nal feedback model was unable to. For example,
consider the French query “Ing´enierie g´n´tique”.
While the feedback model was unable to find
any of the synonyms of the query terms, due to
their lack of co-occurence with the query terms,
the MultiPRF model was able to get these terms,
which are introduced primarily during the back-
translation process. Thus terms like {genetic, gen,
engineering}, which are synonyms of the query
words, are found thus resulting in improved per-
formance. (c) Combination of Above Factors:
Sometimes a combination of the above two factors
causes improvements in the performance as in the
German query “ ¨Olkatastrophein Sibirien”. For
this query, MultiPRF finds good feedback terms
such as {russisch, russland} while also obtaining
semantically related terms such as {olverschmutz,
erdol, olunfall}.
Although all of the previously described exam-
ples had good quality translations of the query
in the assisting language, as mentioned in (Chin-
nakotla et al., 2010), the MultiPRF approach is
robust to suboptimal translation quality as well.
To see how MultiPRF leads to improvements even
with errors in query translation consider the Ger-
man Query “Siegerinnen von Wimbledon”. When
this is translated to English, the term “Lady” is
dropped, this causes only “Wimbledon Champi-
ons” to remain. As can be observed, this causes
terms like sampras to come up in the MultiPRF
model. However, while the MultiPRF model has
some terms pertaining to Men’s Winners of Wim-
bledon as well, the original feedback model suf-
fers from severe topic drift, with irrelevant terms
such as {telefonbuch, telekom} also amongst the
top terms. Thus we notice that despite the er-
ror in query translation MultiPRF still manages to
correct the drift of the original feedback model,
while also introducing relevant terms such as
{verfecht, steffi, martina, novotna, navratilova}
as well. Thus as shown in (Chinnakotla et al.,
2010), having a better query translation system
can only lead to better performance. We also
perform a detailed error analysis and found three
main reasons for MultiPRF failing: (i) Inaccura-
cies in query translation (including the presence of
out-of-vocabulary terms). This is seen in the Ger-
man Query AI in Lateinamerika, which wrongly
translates to Avian Flu in Latin America in Span-
ish thus affecting performance. (ii) Poor retrieval
in Assisting Language. Consider the French query
Les droits de l’enfant, for which due to topic drift
in English, MultiPRF performance reduces. (iii)
In a few rare cases inaccuracy in the back transla-
</bodyText>
<page confidence="0.984595">
1351
</page>
<figure confidence="0.9987855">
(a) Source:French (FR-01+02) Assist:Spanish (b) Source:German (DE-01+02) Assist:Dutch
(c) Source:Finnish (FI-02+03+04) Assist:English
</figure>
<figureCaption confidence="0.999727">
Figure 2: Results showing the sensitivity of MultiPRF performance to parameters Q and 7 for French, German and Finnish.
</figureCaption>
<bodyText confidence="0.987649">
tion affects performance as well.
</bodyText>
<subsectionHeader confidence="0.998228">
6.1 Parameter Sensitivity Analysis
</subsectionHeader>
<bodyText confidence="0.999983">
The MultiPRF parameters Q and -y in Equation
2 control the relative importance assigned to the
original feedback model in source language L1,
the translated feedback model obtained from as-
sisting language L2 and the original query terms.
We varied the Q and -y parameters for French, Ger-
man and Finnish collections with English, Dutch
and Spanish as assisting languages and studied its
effect on MAP of MultiPRF. The results are shown
in Figure 2. The results show that, in all the three
collections, the optimal value of the parameters
almost remains the same and lies in the range of
0.4-0.48. Due to the above reason, we arbitrarily
choose the parameters in the above range and do
not use any technique to learn these parameters.
</bodyText>
<subsectionHeader confidence="0.999961">
6.2 Effect of Assisting Language Choice
</subsectionHeader>
<bodyText confidence="0.999509488888889">
In this section, we discuss the effect of varying
the assisting language. Besides, we also study
the inter and intra familial behaviour of source-
assisting language pairs. In order to ensure that
the results are comparable across languages, we
indexed the collections from the years 2002, 2003
and use common topics from the topic range 91-
200 that have relevant documents across all the six
languages. The number of such common topics
were 67. For each source language, we use the
other languages as assisting collections and study
the performance of MultiPRF. Since query trans-
lation quality varies across language pairs, we an-
alyze the behaviour of MultiPRF in the following
two scenarios: (a) Using ideal query translation
(b) Using Google Translate for query translation.
In ideal query translation setup, in order to elim-
inate its effect, we skip the query translation step
and use the corresponding original topics for each
target language instead. The results for both the
above scenarios are given in Tables 6 and 7.
From the results, we firstly observe that besides
English, other languages such as French, Spanish,
German and Dutch act as good assisting languages
and help in improving performance over mono-
lingual MBF. We also observe that the best as-
sisting language varies with the source language.
However, the crucial factors of the assisting lan-
guage which influence the performance of Multi-
PRF are: (a) Monolingual PRF Performance: The
main motivation for using a different language was
to get good feedback terms, especially in case of
queries which fail in the source language. Hence,
an assisting language in which the monolingual
feedback performance itself is poor, is unlikely
to give any performance gains. This observation
is evident in case of Finnish, which has the low-
est Monolingual MBF performance. The results
show that Finnish is the least helpful of assist-
ing languages, with performance similar to those
of the baselines. We also observe that the three
best performing assistant languages, i.e. English,
French and Spanish, have the highest monolingual
performances as well, thus further validating the
claim. One possible reason for this is the relative
</bodyText>
<page confidence="0.987386">
1352
</page>
<subsectionHeader confidence="0.788287">
Source Assisting Language Source
</subsectionHeader>
<note confidence="0.516602">
Lang. English German Dutch Spanish French Finnish Lang.MBF
</note>
<table confidence="0.999947842105263">
MAP 0.4464 (-0.7%) 0.4471 (-0.5%) 0.4566 (+1.6%) 0.4563 (+1.5%) 0.4545 (+1.1%) 0.4495
English P@5 - 0.4925 (-0.6%) 0.5045 (+1.8%) 0.5164 (+4.2%) 0.5075 (+2.4%) 0.5194 (+4.8%) 0.4955
P@10 0.4343 (+0.4%) 0.4373 (+1.0%) 0.4537 (+4.8%) 0.4343 (+0.4%) 0.4373 (+1.0%) 0.4328
MAP 0.4229 (+4.9%) 0.4346 (+7.8%) 0.4314 (+7.0%) 0.411 (+1.9%) 0.3863 (-4.2%) 0.4033
German P@5 0.5851 (+14%) - 0.5851 (+14%) 0.5791 (+12.8%) 0.594 (+15.7%) 0.5522 (+7.6%) 0.5134
P@10 0.5284 (+11.3%) 0.5209 (+9.8%) 0.5179 (+9.1%) 0.5149 (+8.5%) 0.5075 (+6.9%) 0.4746
MAP 0.4317 (+4%) 0.4453 (+7.2%) 0.4275 (+2.9%) 0.4241 (+2.1%) 0.3971 (-4.4%) 0.4153
Dutch P@5 0.5642 (+11.8%) 0.5731 (+13.6%) - 0.5343 (+5.9%) 0.5582 (+10.6%) 0.5045 (0%) 0.5045
P@10 0.5075 (+9%) 0.4925 (+5.8%) 0.4896 (+5.1%) 0.5015 (+7.7%) 0.4806 (+3.2%) 0.4657
0.4311 (-
Spanish MAP 0.4667 (-2.9%) 0.4749 (-1.2%) 0.4744 (-1.3%) 0.4609 (-4.1%) 10.3%) 0.4805
P@5 0.62 (-2.9%) 0.6418 (+0.5%) 0.6299 (-1.4%) - 0.6269 (-1.6%) 0.6149 (-3.7%) 0.6388
P@10 0.5625 (-1.8%) 0.5806 (+1.3%) 0.5851 (+2.1%) 0.5627 (-1.8%) 0.5478 (-4.4%) 0.5731
MAP 0.4658 (+6.9%) 0.4526 (+3.9%) 0.4374 (+0.4%) 0.4634 (+6.4%) 0.4451 (+2.2%) 0.4356
French P@5 0.4925 (+3.1%) 0.4806 (+0.6%) 0.4567 (-4.4%) 0.4925 (+3.1%) - 0.4836 (+1.3%) 0.4776
P@10 0.4358 (+3.9%) 0.4239 (+1%) 0.4224 (+0.7%) 0.4388 (+4.6%) 0.4209 (+0.4%) 0.4194
MAP 0.3411 (-4.7%) 0.3796 (+6.1%) 0.3722 (+4%) 0.369 (+3.1%) 0.3553 (-0.7%) 0.3578
Finnish P@5 0.394 (+3.1%) 0.403 (+5.5%) 0.406 (+6.3%) 0.4119 (+7.8%) 0.397 (+3.9%) - 0.3821
P@10 0.3463 (+11.5%) 0.3582 (+15.4%) 0.3478 (+12%) 0.3448 (+11%) 0.3433 (+10.6%) 0.3105
</table>
<tableCaption confidence="0.998394">
Table 6: Results showing the performance of MultiPRF with different source and assisting languages using Google Translate
for query translation step. The intra-familial affinity could be observed from the elements close to the diagonal.
</tableCaption>
<bodyText confidence="0.9996449">
ease of processing in these languages. (b) Familial
Similarity Between Languages: We observe that
the performance of MultiPRF is good if the as-
sisting language is from the same language fam-
ily. Birch et al. (2008) show that the language
family is a strong predictor of machine transla-
tion performance. Hence, the query translation
and back translation quality improves if the source
and assisting languages belong to the same family.
For example, in the Germanic family, the source-
assisting language pairs German-English, Dutch-
English, Dutch-German and German-Dutch show
good performance. Similarly, in Romance family,
the performance of French-Spanish confirms this
behaviour. In some cases, we observe that Multi-
PRF scores decent improvements even when the
assisting language does not belong to the same
language family as witnessed in French-English
and English-French. This is primarily due to their
strong monolingual MBF performance.
</bodyText>
<subsectionHeader confidence="0.99948">
6.3 Effect of Language Family on Back
Translation Performance
</subsectionHeader>
<bodyText confidence="0.9999457">
As already mentioned, the performance of Multi-
PRF is good if the source and assisting languages
belong to the same family. In this section, we ver-
ify the above intuition by studying the impact of
language family on back translation performance.
The experiment designed is as follows: Given a
query in source language L1, the ideal translation
in assisting language L2 is used to compute the
query model in L2 using only the query terms.
Then, without performing PRF the query model
</bodyText>
<table confidence="0.981551571428571">
Source Assisting Language MBF MPRF
Lang.
FR ES DE NL EN FI
French - 0.3686 0.3113 0.3366 0.4338 0.3011 0.4342 0.4535
Spanish 0.3647 - 0.3440 0.3476 0.3954 0.3036 0.5000 0.4892
German 0.2729 0.2736 - 0.2951 0.2107 0.2266 0.4229 0.4576
Dutch 0.2663 0.2836 0.2902 - 0.2757 0.2372 0.3968 0.3989
</table>
<tableCaption confidence="0.972652">
Table 8: Effect of Language Family on Back Translation
Performance measured through MultiPRF MAP. 100 Topics
from years 2001 and 2002 were used for all languages.
</tableCaption>
<bodyText confidence="0.999936913043478">
is directly back translated from L2 into L1 and
finally documents are re-ranked using this trans-
lated feedback model. Since the automatic query
translation and PRF steps have been eliminated,
the only factor which influences the MultiPRF per-
formance is the back-translation step. This means
that the source-assisting language pairs for which
the back-translation is good will score a higher
performance. The results of the above experiment
is shown in Table 8. For each source language,
the best performing assisting languages have been
highlighted.
The results show that the performance of
closely related languages like French-Spanish and
German-Dutch is more when compared to other
source-assistant language pairs. This shows that
in case of closely related languages, the back-
translation step succeeds in adding good terms
which are relevant like morphological variants,
synonyms and other semantically related terms.
Hence, familial closeness of the assisting language
helps in boosting the MultiPRF performance. An
exception to this trend is English as assisting lan-
</bodyText>
<page confidence="0.950814">
1353
</page>
<subsectionHeader confidence="0.674754">
Source Assisting Language Source
</subsectionHeader>
<note confidence="0.474952">
Lang. English German Dutch Spanish French Finnish Lang.MBF
</note>
<table confidence="0.9999605">
MAP - 0.4513 (+0.4%) 0.4475 (-0.4%) 0.4695 (+4.5%) 0.4665 (+3.8%) 0.4416 (-1.7%) 0.4495
English P@5 0.5104 (+3.0%) 0.5104 (+3.0%) 0.5343 (+7.8%) 0.5403 (+9.0%) 0.4806 (-3.0%) 0.4955
P@10 0.4373 (+1.0%) 0.4358 (+0.7%) 0.4597 (+6.2%) 0.4582 (+5.9%) 0.4164 (-3.8%) 0.4328
MAP 0.4427 (+9.8%) - 0.4306 (+6.8%) 0.4404 (+9.2%) 0.4104 (+1.8%) 0.3993 (-1.0%) 0.4033
German P@5 0.606 (+18%) 0.5672 (+10.5%) 0.594 (+15.7%) 0.5761 (+12.2%) 0.5552 (+8.1%) 0.5134
P@10 0.5373 (+13.2%) 0.503 (+6.0%) 0.5299 (+11.7%) 0.494 (+4.1%) 0.5 (+5.4%) 0.4746
MAP 0.4361 (+5.0%) 0.4344 (+4.6%) 0.4227 (+1.8%) 0.4304 (+3.6%) 0.4134 (-0.5%) 0.4153
Dutch P@5 0.5761 (+14.2%) 0.5552 (+10%) - 0.5403 (+7.1%) 0.5463 (+8.3%) 0.5433 (+7.7%) 0.5045
P@10 0.5254 (+12.8%) 0.497 (+6.7%) 0.4776 (+2.6%) 0.5134 (+10.2%) 0.4925 (+5.8%) 0.4657
MAP 0.4665 (-2.9%) 0.4773 (-0.7%) 0.4733 (-1.5%) 0.4839 (+0.7%) 0.4412 (-8.2%) 0.4805
Spanish P@5 0.6507 (+1.8%) 0.6448 (+0.9%) 0.6507 (+1.8%) - 0.6478 (+1.4%) 0.597 (-6.5%) 0.6388
P@10 0.5791 (+1.0%) 0.5791 (+1.0%) 0.5761 (+0.5%) 0.5866 (+2.4%) 0.5567 (-2.9%) 0.5731
MAP 0.4591 (+5.4%) 0.4514 (+3.6%) 0.4409 (+1.2%) 0.4712 (+8.2%) 0.4354 (0%) 0.4356
French P@5 0.4925 (+3.1%) 0.4776 (0%) 0.4776 (0%) 0.4995 (+4.6%) - 0.4955 (+3.8%) 0.4776
P@10 0.4463 (+6.4%) 0.4313 (+2.8%) 0.4373 (+4.3%) 0.4448 (+6.1%) 0.4209 (+0.3%) 0.4194
MAP 0.3733 (+4.3%) 0.3559 (-0.5%) 0.3676 (+2.7%) 0.3594 (+0.4%) 0.371 (+3.7%) 0.3578
Finnish P@5 0.4149 (+8.6%) 0.385 (+0.7%) 0.388 (+1.6%) 0.388 (+1.6%) 0.3911 (+2.4%) - 0.3821
P@10 0.3567 (+14.9%) 0.31 (-0.2%) 0.3253 (+4.8%) 0.32 (+3.1%) 0.3239 (+4.3%) 0.3105
</table>
<tableCaption confidence="0.9953735">
Table 7: Results showing the performance of MultiPRF without using automatic query translation i.e. by using corresponding
original queries in assisting collection. The results show the potential of MultiPRF by establishing a performance upper bound.
</tableCaption>
<bodyText confidence="0.9990275">
guage which shows good performance across both
families.
</bodyText>
<subsectionHeader confidence="0.991749">
6.4 Multiple Assisting Languages
</subsectionHeader>
<bodyText confidence="0.9999971">
So far, we have only considered a single assist-
ing language. However, a natural extension to
the method which comes to mind, is using mul-
tiple assisting languages. In other words, com-
bining the evidence from all the feedback mod-
els of more than one assisting language, to get a
feedback model which is better than that obtained
using a single assisting language. To check how
this simple extension works, we performed exper-
iments using a pair of assisting languages. In these
experiments for a given source language (from
amongst the 6 previously mentioned languages)
we tried using all pairs of assisting languages (for
each source language, we have 10 pairs possible).
To obtain the final model, we simply interpolate all
the feedback models with the initial query model,
in a similar manner as done in MultiPRF. The re-
sults for these experiments are given in Table 9.
As we see, out of the 60 possible combinations
of source language and assisting language pairs,
we obtain improvements of greater than 3% in 16
cases. Here the improvements are with respect to
the best model amongst the two MultiPRF mod-
els corresponding to each of the two assisting lan-
guages, with the same source language. Thus we
observe that a simple linear interpolation of mod-
els is not the best way of combining evidence from
multiple assisting languages. We also observe than
when German or Spanish are used as one of the
two assisting languages, they are most likely to
</bodyText>
<subsectionHeader confidence="0.971839">
Source Assisting Language Pairs with
</subsectionHeader>
<table confidence="0.955492454545455">
Language Improvement &gt;3%
English FR-DE (4.5%), FR-ES (4.8%), DE-NL (+3.1%)
French EN-DE (4.1%), DE-ES (3.4%), NL-FI (4.8%)
German None
Spanish None
EN-DE (3.9%), DE-FR (4.1%), FR-ES (3.8%), DE-ES
Dutch (3.9%)
EN-ES (3.2%), FR-DE (4.6%), FR-ES (6.4%),
Finnish DE-ES (11.2%), DE-NL (4.4%), ES-NL (5.9%)
EN – 3 Pairs; FR – 6 Pairs; DE – 10 Pairs;
Total - 16 ES - 8 Pairs; NL – 4 Pairs; FI – 1 Pair
</table>
<tableCaption confidence="0.8743385">
Table 9: Summary of MultiPRF Results with Two Assisting
Languages. The improvements described above are with re-
spect to maximum MultiPRF MAP obtained using either L1
or L2 alone as assisting language.
</tableCaption>
<bodyText confidence="0.9887645">
lead to improvements. A more detailed study of
this observation needs to be done to explain this.
</bodyText>
<sectionHeader confidence="0.996326" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999615">
We studied the effect of different source-assistant
pairs and multiple assisting languages on the per-
formance of MultiPRF. Experiments across a wide
range of language pairs with varied degree of fa-
milial relationships show that MultiPRF improves
performance in most cases with the performance
improvement being more pronounced when the
source and assisting languages are closely related.
We also notice that the results are mixed when two
assisting languages are used simultaneously. As
part of future work, we plan to vary the model
interpolation parameters dynamically to improve
the performance in case of multiple assisting lan-
guages.
</bodyText>
<sectionHeader confidence="0.997747" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99990825">
The first author was supported by a fellowship
award from Infosys Technologies Ltd., India. We
would like to thank Mr. Vishal Vachhani for his
help in running the experiments.
</bodyText>
<page confidence="0.994131">
1354
</page>
<sectionHeader confidence="0.996275" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999955554545454">
Giambattista Amati, Claudio Carpineto, and Giovanni Ro-
mano. 2004. Query Difficulty, Robustness, and Selec-
tive Application of Query Expansion. In ECIR ’04, pages
127–137.
Alexandra Birch, Miles Osborne and Philipp Koehn. 2008.
Predicting Success in Machine Translation. In EMNLP
’08, pages 745-754, ACL.
Martin Braschler and Carol Peters. 2004. Cross-Language
Evaluation Forum: Objectives, Results, Achievements.
Inf. Retr., 7(1-2):7–31.
Martin Braschler and Peter Sch¨auble. 1998. Multilingual In-
formation Retrieval based on Document Alignment Tech-
niques. In ECDL ’98, pages 183–197, Springer-Verlag.
Chris Buckley, Gerald Salton, James Allan, and Amit Sing-
hal. 1994. Automatic Query Expansion using SMART :
TREC 3. In TREC-3, pages 69–80.
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen
Robertson. 2008. Selecting Good Expansion Terms for
Pseudo-Relevance Feedback. In SIGIR ’08, pages 243–
250. ACM.
Manoj K. Chinnakotla, Karthik Raman, and Pushpak Bhat-
tacharyya. 2010. Multilingual PRF: English Lends a
Helping Hand. In SIGIR ’10, ACM.
Kevyn Collins-Thompson and Jamie Callan. 2005. Query
Expansion Using Random Walk Models. In CIKM ’05,
pages 704–711. ACM.
Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft.
2004. A Framework for Selective Query Expansion. In
CIKM ’04, pages 236–237. ACM.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two Lan-
guages Are More Informative Than One. In ACL ’91,
pages 130–137. ACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum Like-
lihood from Incomplete Data via the EM Algorithm. Jour-
nal of the Royal Statistical Society, 39:1–38.
T. Susan Dumais, A. Todd Letsche, L. Michael Littman, and
K. Thomas Landauer. 1997. Automatic Cross-Language
Retrieval Using Latent Semantic Indexing. In AAAI ’97,
pages 18–24.
Wei Gao, John Blitzer, and Ming Zhou. 2008. Using English
Information in Non-English Web Search. In iNEWS ’08,
pages 17–24. ACM.
David Hawking, Paul Thistlewaite, and Donna Harman.
1999. Scaling Up the TREC Collection. Inf. Retr., 1(1-
2):115–137.
Hieu Hoang, Alexandra Birch, Chris Callison-burch, Richard
Zens, Rwth Aachen, Alexandra Constantin, Marcello Fed-
erico, Nicola Bertoldi, Chris Dyer, Brooke Cowan, Wade
Shen, Christine Moran, and Ondej Bojar. 2007. Moses:
Open Source Toolkit for Statistical Machine Translation.
In ACL ’07, pages 177–180.
P. Jourlin, S. E. Johnson, K. Sp¨arck Jones and P. C. Wood-
land. 1999. Improving Retrieval on Imperfect Speech
Transcriptions (Poster Abstract). In SIGIR ’99, pages
283–284. ACM.
John Lafferty and Chengxiang Zhai. 2003. Probabilistic Rel-
evance Models Based on Document and Query Genera-
tion. Language Modeling for Information Retrieval, pages
1–10. Kluwer International Series on IR.
K. Sparck Jones, S. Walker, and S. E. Robertson. 2000. A
Probabilistic Model of Information Retrieval: Develop-
ment and Comparative Experiments. Inf. Process. Man-
age., 36(6):779–808.
John Lafferty and Chengxiang Zhai. 2001. Document Lan-
guage Models, Query Models, and Risk Minimization for
Information Retrieval. In SIGIR ’01, pages 111–119.
ACM.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based
Language Models. In SIGIR ’01, pages 120–127. ACM.
Victor Lavrenko, Martin Choquette, and W. Bruce Croft.
2002. Cross-Lingual Relevance Models. In SIGIR ’02,
pages 175–182, ACM.
Edgar Meij, Dolf Trieschnigg, Maarten Rijke de, and Wessel
Kraaij. 2009. Conceptual Language Models for Domain-
specific Retrieval. Information Processing &amp; Manage-
ment, 2009.
Donald Metzler and W. Bruce Croft. 2007. Latent Concept
Expansion Using Markov Random Fields. In SIGIR ’07,
pages 311–318. ACM.
Mandar Mitra, Amit Singhal, and Chris Buckley. 1998. Im-
proving Automatic Query Expansion. In SIGIR ’98, pages
206–214. ACM.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19–51.
I.Ounis, G. Amati, Plachouras V., B. He, C. Macdonald, and
Johnson. 2005. Terrier Information Retrieval Platform.
In ECIR ’05, volume 3408 of Lecture Notes in Computer
Science, pages 517–519. Springer.
Koehn Philipp. 2005. Europarl: A Parallel Corpus for Statis-
tical Machine Translation. In MT Summit ’05.
Stephen Robertson. 2006. On GMAP: and Other Transfor-
mations. In CIKM ’06, pages 78–83. ACM.
Tetsuya Sakai, Toshihiko Manabe, and Makoto Koyama.
2005. Flexible Pseudo-Relevance Feedback Via Selective
Sampling. ACM TALIP, 4(2):111–135.
Tao Tao and ChengXiang Zhai. 2006. Regularized Esti-
mation of Mixture Models for Robust Pseudo-Relevance
Feedback. In SIGIR ’06, pages 162–169. ACM.
Tuomas Talvensaari, Jorma Laurikkala, Kalervo J¨arvelin,
Martti Juhola, and Heikki Keskustalo. 2007. Creating and
Exploiting a Comparable Corpus in Cross-language Infor-
mation Retrieval. ACM Trans. Inf. Syst., 25(1):4, 2007.
Jrg Tiedemann. 2001. The Use of Parallel Corpora in Mono-
lingual Lexicography - How word alignment can identify
morphological and semantic relations. In COMPLEX ’01,
pages 143–151.
Ellen M. Voorhees. 1994. Query Expansion Using Lexical-
Semantic Relations. In SIGIR ’94, pages 61–69. Springer-
Verlag.
</reference>
<page confidence="0.815597">
1355
</page>
<reference confidence="0.999811545454545">
Ellen Voorhees. 2006. Overview of the TREC 2005 Robust
Retrieval Track. In TREC 2005, Gaithersburg, MD. NIST.
Dan Wu, Daqing He, Heng Ji, and Ralph Grishman. 2008.
A Study of Using an Out-of-Box Commercial MT System
for Query Translation in CLIR. In iNEWS ’08, pages 71–
76. ACM.
Jinxi Xu and W. Bruce Croft. 2000. Improving the Effective-
ness of Information Retrieval with Local Context Analy-
sis. ACM Trans. Inf. Syst., 18(1):79–112.
Jinxi Xu, Alexander Fraser, and Ralph Weischedel. 2002.
Empirical Studies in Strategies for Arabic Retrieval. In
SIGIR ’02, pages 269–274. ACM.
Yang Xu, Gareth J.F. Jones, and Bin Wang. 2009.
Query Dependent Pseudo-Relevance Feedback Based on
Wikipedia. In SIGIR ’09, pages 59–66. ACM.
Chengxiang Zhai and John Lafferty. 2001. Model-based
Feedback in the Language Modeling approach to Infor-
mation Retrieval. In CIKM ’01, pages 403–410. ACM.
Chengxiang Zhai and John Lafferty. 2004. A Study of
Smoothing Methods for Language Models applied to In-
formation Retrieval. ACM Transactions on Information
Systems, 22(2):179–214.
</reference>
<page confidence="0.992844">
1356
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.582600">
<title confidence="0.9983075">Multilingual Pseudo-Relevance Feedback: Performance Study of Assisting Languages</title>
<author confidence="0.999665">Manoj K Chinnakotla Karthik Raman Pushpak Bhattacharyya</author>
<affiliation confidence="0.997998">Department of Computer Science and Engineering Indian Institute of Technology, Bombay,</affiliation>
<address confidence="0.995997">Mumbai, India</address>
<abstract confidence="0.975568787878788">In a previous work of ours Chinnakotla et al. (2010) we introduced a novel framework for Pseudo-Relevance Feed- (PRF) called Given a in one language called we English as the Language improve the performance of PRF for the source language. MulitiPRF showed remarkable improvement over plain Model Based Feedback (MBF) uniformly for 4 French, German, Hungarthe assisting language. This fact inspired us study the effect of source-assistant pair on MultiPRF performance from out of a set of languages with widely differcharacteristics, Dutch, English, French, German Carrying this further, we looked into the of using assisting languages to- PRF. The present paper is a report of these investigations, their results and conclusions drawn therefrom. While performance improvement on MultiPRF is observed whatever the assisting language and whatever the source, observations are mixed when two assisting languages are used simultaneously. Interestingly, the performance improvement is more pronounced when the source and assisting languages are e.g., French and Spanish.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Giambattista Amati</author>
<author>Claudio Carpineto</author>
<author>Giovanni Romano</author>
</authors>
<title>Query Difficulty, Robustness, and Selective Application of Query Expansion.</title>
<date>2004</date>
<booktitle>In ECIR ’04,</booktitle>
<pages>127--137</pages>
<contexts>
<context position="6106" citStr="Amati et al., 2004" startWordPosition="943" endWordPosition="946"> future work. 2 Related Work PRF has been successfully applied in various IR frameworks like vector space models, probabilistic IR and language modeling (Buckley et al., 1994; Jones et al., 2000; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001). Several approaches have been proposed to improve the performance and robustness of PRF. Some of the representative techniques are (i) Refining the feedback document set (Mitra et al., 1998; Sakai et al., 2005), (ii) Refining the terms obtained through PRF by selecting good expansion terms (Cao et al., 2008) and (iii) Using selective query expansion (Amati et al., 2004; Cronen-Townsend et al., 2004) and (iv) Varying the importance of documents in the feedback set (Tao and Zhai, 2006). Another direction of work, often reported in the TREC Robust Track, is to use a large external collection like Wikipedia or the Web as a source of expansion terms (Xu et al., 2009; Voorhees, 2006). The intuition behind the above approach is that if the query does not have many relevant documents in the collection then any improvements in the modeling of PRF is bound to perform poorly due to query drift. Several approaches have been proposed for including different types of lex</context>
</contexts>
<marker>Amati, Carpineto, Romano, 2004</marker>
<rawString>Giambattista Amati, Claudio Carpineto, and Giovanni Romano. 2004. Query Difficulty, Robustness, and Selective Application of Query Expansion. In ECIR ’04, pages 127–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<date>2008</date>
<booktitle>Predicting Success in Machine Translation. In EMNLP ’08,</booktitle>
<pages>745--754</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="33775" citStr="Birch et al. (2008)" startWordPosition="5311" endWordPosition="5314"> 0.394 (+3.1%) 0.403 (+5.5%) 0.406 (+6.3%) 0.4119 (+7.8%) 0.397 (+3.9%) - 0.3821 P@10 0.3463 (+11.5%) 0.3582 (+15.4%) 0.3478 (+12%) 0.3448 (+11%) 0.3433 (+10.6%) 0.3105 Table 6: Results showing the performance of MultiPRF with different source and assisting languages using Google Translate for query translation step. The intra-familial affinity could be observed from the elements close to the diagonal. ease of processing in these languages. (b) Familial Similarity Between Languages: We observe that the performance of MultiPRF is good if the assisting language is from the same language family. Birch et al. (2008) show that the language family is a strong predictor of machine translation performance. Hence, the query translation and back translation quality improves if the source and assisting languages belong to the same family. For example, in the Germanic family, the sourceassisting language pairs German-English, DutchEnglish, Dutch-German and German-Dutch show good performance. Similarly, in Romance family, the performance of French-Spanish confirms this behaviour. In some cases, we observe that MultiPRF scores decent improvements even when the assisting language does not belong to the same languag</context>
</contexts>
<marker>Birch, Osborne, Koehn, 2008</marker>
<rawString>Alexandra Birch, Miles Osborne and Philipp Koehn. 2008. Predicting Success in Machine Translation. In EMNLP ’08, pages 745-754, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Braschler</author>
<author>Carol Peters</author>
</authors>
<date>2004</date>
<journal>Cross-Language Evaluation Forum: Objectives, Results, Achievements. Inf. Retr.,</journal>
<pages>7--1</pages>
<marker>Braschler, Peters, 2004</marker>
<rawString>Martin Braschler and Carol Peters. 2004. Cross-Language Evaluation Forum: Objectives, Results, Achievements. Inf. Retr., 7(1-2):7–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Braschler</author>
<author>Peter Sch¨auble</author>
</authors>
<title>Multilingual Information Retrieval based on Document Alignment Techniques.</title>
<date>1998</date>
<booktitle>In ECDL ’98,</booktitle>
<pages>183--197</pages>
<publisher>Springer-Verlag.</publisher>
<marker>Braschler, Sch¨auble, 1998</marker>
<rawString>Martin Braschler and Peter Sch¨auble. 1998. Multilingual Information Retrieval based on Document Alignment Techniques. In ECDL ’98, pages 183–197, Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Buckley</author>
<author>Gerald Salton</author>
<author>James Allan</author>
<author>Amit Singhal</author>
</authors>
<title>Automatic Query Expansion using SMART : TREC 3. In</title>
<date>1994</date>
<booktitle>TREC-3,</booktitle>
<pages>69--80</pages>
<contexts>
<context position="2173" citStr="Buckley et al., 1994" startWordPosition="319" endWordPosition="322">of Information Retrieval (IR) is to satisfy the user’s information need, which is typically expressed through a short (typically 2-3 words) and often ambiguous query. The problem of matching the user’s query to the documents is rendered difficult by natural language phenomena like morphological variations, polysemy and synonymy. Relevance Feedback (RF) tries to overcome these problems by eliciting user feedback on the relevance of documents obtained from the initial ranking and then uses it to automatically refine the query. Since user input is hard to obtain, Pseudo-Relevance Feedback (PRF) (Buckley et al., 1994; Xu and Croft, 2000; Mitra et al., 1998) is used as an alternative, wherein RF is performed by assuming the top k documents from the initial retrieval as being relevant to the query. Based on the above assumption, the terms in the feedback document set are analyzed to choose the most distinguishing set of terms that characterize the feedback documents and as a result the relevance of a document. Query refinement is done by adding the terms obtained through PRF, along with their weights, to the actual query. Although PRF has been shown to improve retrieval, it suffers from the following drawba</context>
<context position="5662" citStr="Buckley et al., 1994" startWordPosition="867" endWordPosition="870">and Spanish. The paper is organized as follows: Section 2, discusses the related work. Section 3, explains the Language Modeling (LM) based PRF approach. Section 4, describes the MultiPRF approach. Section 5 discusses the experimental set up. Section 6 presents the results, and studies the effect of varying the assisting language and incorporates multiple assisting languages. Finally, Section 7 concludes the paper by summarizing and outlining future work. 2 Related Work PRF has been successfully applied in various IR frameworks like vector space models, probabilistic IR and language modeling (Buckley et al., 1994; Jones et al., 2000; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001). Several approaches have been proposed to improve the performance and robustness of PRF. Some of the representative techniques are (i) Refining the feedback document set (Mitra et al., 1998; Sakai et al., 2005), (ii) Refining the terms obtained through PRF by selecting good expansion terms (Cao et al., 2008) and (iii) Using selective query expansion (Amati et al., 2004; Cronen-Townsend et al., 2004) and (iv) Varying the importance of documents in the feedback set (Tao and Zhai, 2006). Another direction of work, often repo</context>
</contexts>
<marker>Buckley, Salton, Allan, Singhal, 1994</marker>
<rawString>Chris Buckley, Gerald Salton, James Allan, and Amit Singhal. 1994. Automatic Query Expansion using SMART : TREC 3. In TREC-3, pages 69–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guihong Cao</author>
<author>Jian-Yun Nie</author>
<author>Jianfeng Gao</author>
<author>Stephen Robertson</author>
</authors>
<title>Selecting Good Expansion Terms for Pseudo-Relevance Feedback.</title>
<date>2008</date>
<booktitle>In SIGIR ’08,</booktitle>
<pages>243--250</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6044" citStr="Cao et al., 2008" startWordPosition="932" endWordPosition="935">y, Section 7 concludes the paper by summarizing and outlining future work. 2 Related Work PRF has been successfully applied in various IR frameworks like vector space models, probabilistic IR and language modeling (Buckley et al., 1994; Jones et al., 2000; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001). Several approaches have been proposed to improve the performance and robustness of PRF. Some of the representative techniques are (i) Refining the feedback document set (Mitra et al., 1998; Sakai et al., 2005), (ii) Refining the terms obtained through PRF by selecting good expansion terms (Cao et al., 2008) and (iii) Using selective query expansion (Amati et al., 2004; Cronen-Townsend et al., 2004) and (iv) Varying the importance of documents in the feedback set (Tao and Zhai, 2006). Another direction of work, often reported in the TREC Robust Track, is to use a large external collection like Wikipedia or the Web as a source of expansion terms (Xu et al., 2009; Voorhees, 2006). The intuition behind the above approach is that if the query does not have many relevant documents in the collection then any improvements in the modeling of PRF is bound to perform poorly due to query drift. Several appr</context>
</contexts>
<marker>Cao, Nie, Gao, Robertson, 2008</marker>
<rawString>Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen Robertson. 2008. Selecting Good Expansion Terms for Pseudo-Relevance Feedback. In SIGIR ’08, pages 243– 250. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manoj K Chinnakotla</author>
<author>Karthik Raman</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Multilingual PRF: English Lends a Helping Hand.</title>
<date>2010</date>
<booktitle>In SIGIR ’10,</booktitle>
<publisher>ACM.</publisher>
<contexts>
<context position="3322" citStr="Chinnakotla et al., 2010" startWordPosition="506" endWordPosition="509">RF has been shown to improve retrieval, it suffers from the following drawbacks: (a) the type of term associations obtained for query expansion is restricted to co-occurrence based relationships in the feedback documents, and thus other types of term associations such as lexical and semantic relations (morphological variants, synonyms) are not explicitly captured, and (b) due to the inherent assumption in PRF, i.e., relevance of top k documents, performance is sensitive to that of the initial retrieval algorithm and as a result is not robust. Multilingual Pseudo-Relevance Feedback (MultiPRF) (Chinnakotla et al., 2010) is a novel framework for PRF to overcome both the above limitations of PRF. It does so by taking the help of a different language called the assisting language. In MultiPRF, given a query in source language L1, the query is automatically translated into the assisting language L2 and PRF performed in the assisting language. The resultant terms are translated back into L1 using a probabilistic bi-lingual dictionary. The translated feedback 1346 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1346–1356, Uppsala, Sweden, 11-16 July 2010. c�2010 Assoc</context>
<context position="26773" citStr="Chinnakotla et al., 2010" startWordPosition="4222" endWordPosition="4226"> {genetic, gen, engineering}, which are synonyms of the query words, are found thus resulting in improved performance. (c) Combination of Above Factors: Sometimes a combination of the above two factors causes improvements in the performance as in the German query “ ¨Olkatastrophein Sibirien”. For this query, MultiPRF finds good feedback terms such as {russisch, russland} while also obtaining semantically related terms such as {olverschmutz, erdol, olunfall}. Although all of the previously described examples had good quality translations of the query in the assisting language, as mentioned in (Chinnakotla et al., 2010), the MultiPRF approach is robust to suboptimal translation quality as well. To see how MultiPRF leads to improvements even with errors in query translation consider the German Query “Siegerinnen von Wimbledon”. When this is translated to English, the term “Lady” is dropped, this causes only “Wimbledon Champions” to remain. As can be observed, this causes terms like sampras to come up in the MultiPRF model. However, while the MultiPRF model has some terms pertaining to Men’s Winners of Wimbledon as well, the original feedback model suffers from severe topic drift, with irrelevant terms such as</context>
</contexts>
<marker>Chinnakotla, Raman, Bhattacharyya, 2010</marker>
<rawString>Manoj K. Chinnakotla, Karthik Raman, and Pushpak Bhattacharyya. 2010. Multilingual PRF: English Lends a Helping Hand. In SIGIR ’10, ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevyn Collins-Thompson</author>
<author>Jamie Callan</author>
</authors>
<title>Query Expansion Using Random Walk Models.</title>
<date>2005</date>
<booktitle>In CIKM ’05,</booktitle>
<pages>704--711</pages>
<publisher>ACM.</publisher>
<marker>Collins-Thompson, Callan, 2005</marker>
<rawString>Kevyn Collins-Thompson and Jamie Callan. 2005. Query Expansion Using Random Walk Models. In CIKM ’05, pages 704–711. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Cronen-Townsend</author>
<author>Yun Zhou</author>
<author>W Bruce Croft</author>
</authors>
<title>A Framework for Selective Query Expansion.</title>
<date>2004</date>
<booktitle>In CIKM ’04,</booktitle>
<pages>236--237</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6137" citStr="Cronen-Townsend et al., 2004" startWordPosition="947" endWordPosition="950">ted Work PRF has been successfully applied in various IR frameworks like vector space models, probabilistic IR and language modeling (Buckley et al., 1994; Jones et al., 2000; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001). Several approaches have been proposed to improve the performance and robustness of PRF. Some of the representative techniques are (i) Refining the feedback document set (Mitra et al., 1998; Sakai et al., 2005), (ii) Refining the terms obtained through PRF by selecting good expansion terms (Cao et al., 2008) and (iii) Using selective query expansion (Amati et al., 2004; Cronen-Townsend et al., 2004) and (iv) Varying the importance of documents in the feedback set (Tao and Zhai, 2006). Another direction of work, often reported in the TREC Robust Track, is to use a large external collection like Wikipedia or the Web as a source of expansion terms (Xu et al., 2009; Voorhees, 2006). The intuition behind the above approach is that if the query does not have many relevant documents in the collection then any improvements in the modeling of PRF is bound to perform poorly due to query drift. Several approaches have been proposed for including different types of lexically and semantically related</context>
</contexts>
<marker>Cronen-Townsend, Zhou, Croft, 2004</marker>
<rawString>Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. 2004. A Framework for Selective Query Expansion. In CIKM ’04, pages 236–237. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
<author>Ulrike Schwall</author>
</authors>
<title>Two Languages Are More Informative Than One.</title>
<date>1991</date>
<booktitle>In ACL ’91,</booktitle>
<pages>130--137</pages>
<publisher>ACL.</publisher>
<marker>Dagan, Itai, Schwall, 1991</marker>
<rawString>Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two Languages Are More Informative Than One. In ACL ’91, pages 130–137. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum Likelihood from Incomplete Data via the EM Algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<contexts>
<context position="10275" citStr="Dempster et al., 1977" startWordPosition="1630" endWordPosition="1633">assumed to be relevant and used as feedback for improving the estimation of OQ. The feedback documents contain both relevant and noisy terms from which the feedback language model is inferred based on a Generative Mixture Model (Zhai and Lafferty, 2001). Let DF = {d1, d2, ... , dkI be the top k documents retrieved using the initial ranking algorithm. Zhai and Lafferty (Zhai and Lafferty, 2001) model the feedback document set DF as a mixture of two distributions: (a) the feedback language model and (b) the collection model P(w|C). The feedback language model is inferred using the EM Algorithm (Dempster et al., 1977), which iteratively accumulates probability mass on the most distinguishing terms, i.e. terms which are more frequent in the feedback document set than in the entire collection. To maintain query focus the final converged feedback model, OF is interpolated with the initial query model OQ to obtain the final query model OFinal. ΘF inal = (1 − α) · ΘQ + α · ΘF OFinal is used to re-rank the corpus using the KL-Divergence ranking function to obtain the final ranked list of documents. Henceforth, we refer Figure 1: Schematic of the Multilingual PRF Approach Symbol Description ΘQ Query Language Mode</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird, and D. Rubin. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Susan Dumais</author>
<author>A Todd Letsche</author>
<author>L Michael Littman</author>
<author>K Thomas Landauer</author>
</authors>
<title>Automatic Cross-Language Retrieval Using Latent Semantic Indexing.</title>
<date>1997</date>
<booktitle>In AAAI ’97,</booktitle>
<pages>18--24</pages>
<marker>Dumais, Letsche, Littman, Landauer, 1997</marker>
<rawString>T. Susan Dumais, A. Todd Letsche, L. Michael Littman, and K. Thomas Landauer. 1997. Automatic Cross-Language Retrieval Using Latent Semantic Indexing. In AAAI ’97, pages 18–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Gao</author>
<author>John Blitzer</author>
<author>Ming Zhou</author>
</authors>
<title>Using English Information in Non-English Web Search.</title>
<date>2008</date>
<booktitle>In iNEWS ’08,</booktitle>
<pages>17--24</pages>
<publisher>ACM.</publisher>
<marker>Gao, Blitzer, Zhou, 2008</marker>
<rawString>Wei Gao, John Blitzer, and Ming Zhou. 2008. Using English Information in Non-English Web Search. In iNEWS ’08, pages 17–24. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hawking</author>
<author>Paul Thistlewaite</author>
<author>Donna Harman</author>
</authors>
<date>1999</date>
<journal>Scaling Up the TREC Collection. Inf. Retr.,</journal>
<pages>1--1</pages>
<marker>Hawking, Thistlewaite, Harman, 1999</marker>
<rawString>David Hawking, Paul Thistlewaite, and Donna Harman. 1999. Scaling Up the TREC Collection. Inf. Retr., 1(1-2):115–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-burch</author>
<author>Richard Zens</author>
<author>Rwth Aachen</author>
<author>Alexandra Constantin</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Chris Dyer</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Ondej Bojar</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In ACL ’07,</booktitle>
<pages>177--180</pages>
<marker>Hoang, Birch, Callison-burch, Zens, Aachen, Constantin, Federico, Bertoldi, Dyer, Cowan, Shen, Moran, Bojar, 2007</marker>
<rawString>Hieu Hoang, Alexandra Birch, Chris Callison-burch, Richard Zens, Rwth Aachen, Alexandra Constantin, Marcello Federico, Nicola Bertoldi, Chris Dyer, Brooke Cowan, Wade Shen, Christine Moran, and Ondej Bojar. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL ’07, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jourlin</author>
<author>S E Johnson</author>
<author>K Sp¨arck Jones</author>
<author>P C Woodland</author>
</authors>
<title>Improving Retrieval on Imperfect Speech Transcriptions (Poster Abstract).</title>
<date>1999</date>
<booktitle>In SIGIR ’99,</booktitle>
<pages>283--284</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7682" citStr="Jourlin et al. (1999)" startWordPosition="1203" endWordPosition="1206">ike WordNet, Web etc. Metzler and Croft (2007) propose a feature based approach called latent concept expansion to model term dependencies. All the above mentioned approaches use the resources available within the language to improve the performance of PRF. However, we make use of a second language to improve the performance of PRF. Our proposed approach is especially attractive in the case of resource-constrained languages where the original retrieval is bad due to poor coverage of the collection and/or inherent complexity of query processing (for example term conflation) in those languages. Jourlin et al. (1999) use parallel blind relevance feedback, i.e. they use blind relevance feedback on a larger, more reliable parallel corpus, to improve retrieval performance on imperfect transcriptions of speech. Another related idea is by Xu et al. (2002), where a statistical thesaurus is learned using the probabilistic bilingual dictionaries of Arabic to English and English to Arabic. Meij et al. (2009) tries to expand a query in a different language using language models for domainspecific retrieval, but in a very different setting. Since our method uses a corpus in the assisting language from a similar time</context>
</contexts>
<marker>Jourlin, Johnson, Jones, Woodland, 1999</marker>
<rawString>P. Jourlin, S. E. Johnson, K. Sp¨arck Jones and P. C. Woodland. 1999. Improving Retrieval on Imperfect Speech Transcriptions (Poster Abstract). In SIGIR ’99, pages 283–284. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Probabilistic Relevance Models Based on Document and Query Generation. Language Modeling for Information Retrieval,</title>
<date>2003</date>
<booktitle>International Series on IR.</booktitle>
<pages>1--10</pages>
<publisher>Kluwer</publisher>
<marker>Lafferty, Zhai, 2003</marker>
<rawString>John Lafferty and Chengxiang Zhai. 2003. Probabilistic Relevance Models Based on Document and Query Generation. Language Modeling for Information Retrieval, pages 1–10. Kluwer International Series on IR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck Jones</author>
<author>S Walker</author>
<author>S E Robertson</author>
</authors>
<title>A Probabilistic Model of Information Retrieval: Development and Comparative Experiments.</title>
<date>2000</date>
<journal>Inf. Process. Manage.,</journal>
<volume>36</volume>
<issue>6</issue>
<contexts>
<context position="5682" citStr="Jones et al., 2000" startWordPosition="871" endWordPosition="874"> is organized as follows: Section 2, discusses the related work. Section 3, explains the Language Modeling (LM) based PRF approach. Section 4, describes the MultiPRF approach. Section 5 discusses the experimental set up. Section 6 presents the results, and studies the effect of varying the assisting language and incorporates multiple assisting languages. Finally, Section 7 concludes the paper by summarizing and outlining future work. 2 Related Work PRF has been successfully applied in various IR frameworks like vector space models, probabilistic IR and language modeling (Buckley et al., 1994; Jones et al., 2000; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001). Several approaches have been proposed to improve the performance and robustness of PRF. Some of the representative techniques are (i) Refining the feedback document set (Mitra et al., 1998; Sakai et al., 2005), (ii) Refining the terms obtained through PRF by selecting good expansion terms (Cao et al., 2008) and (iii) Using selective query expansion (Amati et al., 2004; Cronen-Townsend et al., 2004) and (iv) Varying the importance of documents in the feedback set (Tao and Zhai, 2006). Another direction of work, often reported in the TREC Rob</context>
</contexts>
<marker>Jones, Walker, Robertson, 2000</marker>
<rawString>K. Sparck Jones, S. Walker, and S. E. Robertson. 2000. A Probabilistic Model of Information Retrieval: Development and Comparative Experiments. Inf. Process. Manage., 36(6):779–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Document Language Models, Query Models, and Risk Minimization for Information Retrieval.</title>
<date>2001</date>
<booktitle>In SIGIR ’01,</booktitle>
<pages>111--119</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6898" citStr="Lafferty and Zhai, 2001" startWordPosition="1076" endWordPosition="1079">REC Robust Track, is to use a large external collection like Wikipedia or the Web as a source of expansion terms (Xu et al., 2009; Voorhees, 2006). The intuition behind the above approach is that if the query does not have many relevant documents in the collection then any improvements in the modeling of PRF is bound to perform poorly due to query drift. Several approaches have been proposed for including different types of lexically and semantically related terms during query expansion. Voorhees (1994) use Wordnet for query expansion and report negative results. Recently, random walk models (Lafferty and Zhai, 2001; CollinsThompson and Callan, 2005) have been used to learn a rich set of term level associations by combining evidence from various kinds of information sources like WordNet, Web etc. Metzler and Croft (2007) propose a feature based approach called latent concept expansion to model term dependencies. All the above mentioned approaches use the resources available within the language to improve the performance of PRF. However, we make use of a second language to improve the performance of PRF. Our proposed approach is especially attractive in the case of resource-constrained languages where the</context>
</contexts>
<marker>Lafferty, Zhai, 2001</marker>
<rawString>John Lafferty and Chengxiang Zhai. 2001. Document Language Models, Query Models, and Risk Minimization for Information Retrieval. In SIGIR ’01, pages 111–119. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>W Bruce Croft</author>
</authors>
<title>Relevance Based Language Models.</title>
<date>2001</date>
<booktitle>In SIGIR ’01,</booktitle>
<pages>120--127</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5708" citStr="Lavrenko and Croft, 2001" startWordPosition="875" endWordPosition="878">lows: Section 2, discusses the related work. Section 3, explains the Language Modeling (LM) based PRF approach. Section 4, describes the MultiPRF approach. Section 5 discusses the experimental set up. Section 6 presents the results, and studies the effect of varying the assisting language and incorporates multiple assisting languages. Finally, Section 7 concludes the paper by summarizing and outlining future work. 2 Related Work PRF has been successfully applied in various IR frameworks like vector space models, probabilistic IR and language modeling (Buckley et al., 1994; Jones et al., 2000; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001). Several approaches have been proposed to improve the performance and robustness of PRF. Some of the representative techniques are (i) Refining the feedback document set (Mitra et al., 1998; Sakai et al., 2005), (ii) Refining the terms obtained through PRF by selecting good expansion terms (Cao et al., 2008) and (iii) Using selective query expansion (Amati et al., 2004; Cronen-Townsend et al., 2004) and (iv) Varying the importance of documents in the feedback set (Tao and Zhai, 2006). Another direction of work, often reported in the TREC Robust Track, is to use a lar</context>
</contexts>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based Language Models. In SIGIR ’01, pages 120–127. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>Martin Choquette</author>
<author>W Bruce Croft</author>
</authors>
<title>Cross-Lingual Relevance Models.</title>
<date>2002</date>
<booktitle>In SIGIR ’02,</booktitle>
<pages>175--182</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8555" citStr="Lavrenko et al. (2002)" startWordPosition="1343" endWordPosition="1346">tistical thesaurus is learned using the probabilistic bilingual dictionaries of Arabic to English and English to Arabic. Meij et al. (2009) tries to expand a query in a different language using language models for domainspecific retrieval, but in a very different setting. Since our method uses a corpus in the assisting language from a similar time period, it can be likened to the work by Talvensaari et al. (2007) who used comparable corpora for Cross-Lingual Information Retrieval (CLIR). Other work pertaining to document alignment in comparable corpora, such as Braschler and Sch¨auble (1998), Lavrenko et al. (2002), also share certain common themes with our approach. Recent work by Gao et al. 1347 (2008) uses English to improve the performance over a subset of Chinese queries whose translations in English are unambiguous. They use interdocument similarities across languages to improve the ranking performance. However, cross language document similarity measurement is in itself known to be an hard problem and the scale of their experimentation is quite small. 3 PRF in the LM Framework The Language Modeling (LM) Framework allows PRF to be modelled in a principled manner. In the LM approach, documents and </context>
</contexts>
<marker>Lavrenko, Choquette, Croft, 2002</marker>
<rawString>Victor Lavrenko, Martin Choquette, and W. Bruce Croft. 2002. Cross-Lingual Relevance Models. In SIGIR ’02, pages 175–182, ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edgar Meij</author>
<author>Dolf Trieschnigg</author>
<author>Maarten Rijke de</author>
<author>Wessel Kraaij</author>
</authors>
<title>Conceptual Language Models for Domainspecific Retrieval.</title>
<date>2009</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<contexts>
<context position="8072" citStr="Meij et al. (2009)" startWordPosition="1264" endWordPosition="1267"> of resource-constrained languages where the original retrieval is bad due to poor coverage of the collection and/or inherent complexity of query processing (for example term conflation) in those languages. Jourlin et al. (1999) use parallel blind relevance feedback, i.e. they use blind relevance feedback on a larger, more reliable parallel corpus, to improve retrieval performance on imperfect transcriptions of speech. Another related idea is by Xu et al. (2002), where a statistical thesaurus is learned using the probabilistic bilingual dictionaries of Arabic to English and English to Arabic. Meij et al. (2009) tries to expand a query in a different language using language models for domainspecific retrieval, but in a very different setting. Since our method uses a corpus in the assisting language from a similar time period, it can be likened to the work by Talvensaari et al. (2007) who used comparable corpora for Cross-Lingual Information Retrieval (CLIR). Other work pertaining to document alignment in comparable corpora, such as Braschler and Sch¨auble (1998), Lavrenko et al. (2002), also share certain common themes with our approach. Recent work by Gao et al. 1347 (2008) uses English to improve t</context>
</contexts>
<marker>Meij, Trieschnigg, de, Kraaij, 2009</marker>
<rawString>Edgar Meij, Dolf Trieschnigg, Maarten Rijke de, and Wessel Kraaij. 2009. Conceptual Language Models for Domainspecific Retrieval. Information Processing &amp; Management, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>W Bruce Croft</author>
</authors>
<title>Latent Concept Expansion Using Markov Random Fields.</title>
<date>2007</date>
<booktitle>In SIGIR ’07,</booktitle>
<pages>311--318</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7107" citStr="Metzler and Croft (2007)" startWordPosition="1111" endWordPosition="1114"> does not have many relevant documents in the collection then any improvements in the modeling of PRF is bound to perform poorly due to query drift. Several approaches have been proposed for including different types of lexically and semantically related terms during query expansion. Voorhees (1994) use Wordnet for query expansion and report negative results. Recently, random walk models (Lafferty and Zhai, 2001; CollinsThompson and Callan, 2005) have been used to learn a rich set of term level associations by combining evidence from various kinds of information sources like WordNet, Web etc. Metzler and Croft (2007) propose a feature based approach called latent concept expansion to model term dependencies. All the above mentioned approaches use the resources available within the language to improve the performance of PRF. However, we make use of a second language to improve the performance of PRF. Our proposed approach is especially attractive in the case of resource-constrained languages where the original retrieval is bad due to poor coverage of the collection and/or inherent complexity of query processing (for example term conflation) in those languages. Jourlin et al. (1999) use parallel blind relev</context>
</contexts>
<marker>Metzler, Croft, 2007</marker>
<rawString>Donald Metzler and W. Bruce Croft. 2007. Latent Concept Expansion Using Markov Random Fields. In SIGIR ’07, pages 311–318. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mandar Mitra</author>
<author>Amit Singhal</author>
<author>Chris Buckley</author>
</authors>
<title>Improving Automatic Query Expansion.</title>
<date>1998</date>
<booktitle>In SIGIR ’98,</booktitle>
<pages>206--214</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2214" citStr="Mitra et al., 1998" startWordPosition="327" endWordPosition="330">y the user’s information need, which is typically expressed through a short (typically 2-3 words) and often ambiguous query. The problem of matching the user’s query to the documents is rendered difficult by natural language phenomena like morphological variations, polysemy and synonymy. Relevance Feedback (RF) tries to overcome these problems by eliciting user feedback on the relevance of documents obtained from the initial ranking and then uses it to automatically refine the query. Since user input is hard to obtain, Pseudo-Relevance Feedback (PRF) (Buckley et al., 1994; Xu and Croft, 2000; Mitra et al., 1998) is used as an alternative, wherein RF is performed by assuming the top k documents from the initial retrieval as being relevant to the query. Based on the above assumption, the terms in the feedback document set are analyzed to choose the most distinguishing set of terms that characterize the feedback documents and as a result the relevance of a document. Query refinement is done by adding the terms obtained through PRF, along with their weights, to the actual query. Although PRF has been shown to improve retrieval, it suffers from the following drawbacks: (a) the type of term associations ob</context>
<context position="5924" citStr="Mitra et al., 1998" startWordPosition="912" endWordPosition="915">e results, and studies the effect of varying the assisting language and incorporates multiple assisting languages. Finally, Section 7 concludes the paper by summarizing and outlining future work. 2 Related Work PRF has been successfully applied in various IR frameworks like vector space models, probabilistic IR and language modeling (Buckley et al., 1994; Jones et al., 2000; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001). Several approaches have been proposed to improve the performance and robustness of PRF. Some of the representative techniques are (i) Refining the feedback document set (Mitra et al., 1998; Sakai et al., 2005), (ii) Refining the terms obtained through PRF by selecting good expansion terms (Cao et al., 2008) and (iii) Using selective query expansion (Amati et al., 2004; Cronen-Townsend et al., 2004) and (iv) Varying the importance of documents in the feedback set (Tao and Zhai, 2006). Another direction of work, often reported in the TREC Robust Track, is to use a large external collection like Wikipedia or the Web as a source of expansion terms (Xu et al., 2009; Voorhees, 2006). The intuition behind the above approach is that if the query does not have many relevant documents in</context>
</contexts>
<marker>Mitra, Singhal, Buckley, 1998</marker>
<rawString>Mandar Mitra, Amit Singhal, and Chris Buckley. 1998. Improving Automatic Query Expansion. In SIGIR ’98, pages 206–214. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="20007" citStr="Och and Ney, 2003" startWordPosition="3214" endWordPosition="3217">d using a paired two-tailed t-test. since some function words like l’, d’ etc., occur as prefixes to a word, we strip them off during indexing and query processing, since it significantly improves the baseline performance. We use standard evaluation measures like MAP, P@S and P@10 for evaluation. Additionally, for assessing robustness, we use the Geometric Mean Average Precision (GMAP) metric (Robertson, 2006) which is also used in the TREC Robust Track (Voorhees, 2006). The probabilistic bi-lingual dictionary used in MultiPRF was learnt automatically by running GIZA++: a word alignment tool (Och and Ney, 2003) on a parallel sentence aligned corpora. For all the above language pairs we used the Europarl Corpus (Philipp, 2005). We use Google Translate as the query translation system as it has been shown to perform well for the task (Wu et al., 2008). We use the MBF approach explained in Section 3 as a baseline for comparison. We use two-stage Dirichlet smoothing with the optimal parameters tuned based on the collection (Zhai and Lafferty, 2004). We tune the parameters of MBF, specifically A and α, and choose the values which give the optimal performance on a given collection. We uniformly choose the </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Amati I Ounis</author>
<author>V Plachouras</author>
<author>B He</author>
<author>C Macdonald</author>
<author>Johnson</author>
</authors>
<title>Terrier Information Retrieval Platform.</title>
<date>2005</date>
<booktitle>In ECIR ’05,</booktitle>
<volume>3408</volume>
<pages>517--519</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="17051" citStr="Ounis et al., 2005" startWordPosition="2736" endWordPosition="2739"> we choose assisting collections such that the topics in the source language are covered in the assisting collection so as to get meaningful feedback terms. In all the topics, we only use the title field. We ignore the topics which have no relevant documents as the true performance on those topics cannot be evaluated. We demonstrate the performance of MultiPRF approach with French, German and Finnish as source languages and Dutch, English and Spanish as the assisting language. We later vary the assisting language, for each source language and study the effects. We use the Terrier IR platform (Ounis et al., 2005) for indexing the documents. We perform standard tokenization, stop word removal and stemming. We use the Porter Stemmer for English and the stemmers available through the Snowball package for other languages. Other than these, we do not perform any language-specific processing on the languages. In case of French, 1349 Collection Assist. PO5 PO10 MAP GMAP Lang MBF MultiPRF % Impr. MBF MultiPRF % Impr. MBF MultiPRF % Impr. MBF MultiPRF % Impr. EN 0.5241 11.76* 0.4000 0.00 0.4393 4.10 0.3413 15.27 FR-00 ES 0.4690 0.5034 7.35* 0.4000 0.4103 2.59 0.4220 0.4418 4.69 0.2961 0.3382 14.22 NL 0.5034 7.</context>
</contexts>
<marker>Ounis, Plachouras, He, Macdonald, Johnson, 2005</marker>
<rawString>I.Ounis, G. Amati, Plachouras V., B. He, C. Macdonald, and Johnson. 2005. Terrier Information Retrieval Platform. In ECIR ’05, volume 3408 of Lecture Notes in Computer Science, pages 517–519. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koehn Philipp</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In MT Summit ’05.</booktitle>
<contexts>
<context position="20124" citStr="Philipp, 2005" startWordPosition="3235" endWordPosition="3236">m off during indexing and query processing, since it significantly improves the baseline performance. We use standard evaluation measures like MAP, P@S and P@10 for evaluation. Additionally, for assessing robustness, we use the Geometric Mean Average Precision (GMAP) metric (Robertson, 2006) which is also used in the TREC Robust Track (Voorhees, 2006). The probabilistic bi-lingual dictionary used in MultiPRF was learnt automatically by running GIZA++: a word alignment tool (Och and Ney, 2003) on a parallel sentence aligned corpora. For all the above language pairs we used the Europarl Corpus (Philipp, 2005). We use Google Translate as the query translation system as it has been shown to perform well for the task (Wu et al., 2008). We use the MBF approach explained in Section 3 as a baseline for comparison. We use two-stage Dirichlet smoothing with the optimal parameters tuned based on the collection (Zhai and Lafferty, 2004). We tune the parameters of MBF, specifically A and α, and choose the values which give the optimal performance on a given collection. We uniformly choose the top ten documents for feedback. Table 4 gives the overall results. 6 Results and Discussion In Table 4, we see the pe</context>
</contexts>
<marker>Philipp, 2005</marker>
<rawString>Koehn Philipp. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In MT Summit ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Robertson</author>
</authors>
<title>On GMAP: and Other Transformations.</title>
<date>2006</date>
<booktitle>In CIKM ’06,</booktitle>
<pages>78--83</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="19802" citStr="Robertson, 2006" startWordPosition="3184" endWordPosition="3185">ish (ES) and Dutch (NL) as assisting languages. Results marked as $ indicate that the improvement was found to be statistically significant over the baseline at 90% confidence level (α = 0.01) when tested using a paired two-tailed t-test. since some function words like l’, d’ etc., occur as prefixes to a word, we strip them off during indexing and query processing, since it significantly improves the baseline performance. We use standard evaluation measures like MAP, P@S and P@10 for evaluation. Additionally, for assessing robustness, we use the Geometric Mean Average Precision (GMAP) metric (Robertson, 2006) which is also used in the TREC Robust Track (Voorhees, 2006). The probabilistic bi-lingual dictionary used in MultiPRF was learnt automatically by running GIZA++: a word alignment tool (Och and Ney, 2003) on a parallel sentence aligned corpora. For all the above language pairs we used the Europarl Corpus (Philipp, 2005). We use Google Translate as the query translation system as it has been shown to perform well for the task (Wu et al., 2008). We use the MBF approach explained in Section 3 as a baseline for comparison. We use two-stage Dirichlet smoothing with the optimal parameters tuned bas</context>
</contexts>
<marker>Robertson, 2006</marker>
<rawString>Stephen Robertson. 2006. On GMAP: and Other Transformations. In CIKM ’06, pages 78–83. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Sakai</author>
<author>Toshihiko Manabe</author>
<author>Makoto Koyama</author>
</authors>
<title>Flexible Pseudo-Relevance Feedback Via Selective Sampling.</title>
<date>2005</date>
<journal>ACM TALIP,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="5945" citStr="Sakai et al., 2005" startWordPosition="916" endWordPosition="919">es the effect of varying the assisting language and incorporates multiple assisting languages. Finally, Section 7 concludes the paper by summarizing and outlining future work. 2 Related Work PRF has been successfully applied in various IR frameworks like vector space models, probabilistic IR and language modeling (Buckley et al., 1994; Jones et al., 2000; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001). Several approaches have been proposed to improve the performance and robustness of PRF. Some of the representative techniques are (i) Refining the feedback document set (Mitra et al., 1998; Sakai et al., 2005), (ii) Refining the terms obtained through PRF by selecting good expansion terms (Cao et al., 2008) and (iii) Using selective query expansion (Amati et al., 2004; Cronen-Townsend et al., 2004) and (iv) Varying the importance of documents in the feedback set (Tao and Zhai, 2006). Another direction of work, often reported in the TREC Robust Track, is to use a large external collection like Wikipedia or the Web as a source of expansion terms (Xu et al., 2009; Voorhees, 2006). The intuition behind the above approach is that if the query does not have many relevant documents in the collection then </context>
</contexts>
<marker>Sakai, Manabe, Koyama, 2005</marker>
<rawString>Tetsuya Sakai, Toshihiko Manabe, and Makoto Koyama. 2005. Flexible Pseudo-Relevance Feedback Via Selective Sampling. ACM TALIP, 4(2):111–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Tao</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Regularized Estimation of Mixture Models for Robust Pseudo-Relevance Feedback.</title>
<date>2006</date>
<booktitle>In SIGIR ’06,</booktitle>
<pages>162--169</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6223" citStr="Tao and Zhai, 2006" startWordPosition="963" endWordPosition="966">obabilistic IR and language modeling (Buckley et al., 1994; Jones et al., 2000; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001). Several approaches have been proposed to improve the performance and robustness of PRF. Some of the representative techniques are (i) Refining the feedback document set (Mitra et al., 1998; Sakai et al., 2005), (ii) Refining the terms obtained through PRF by selecting good expansion terms (Cao et al., 2008) and (iii) Using selective query expansion (Amati et al., 2004; Cronen-Townsend et al., 2004) and (iv) Varying the importance of documents in the feedback set (Tao and Zhai, 2006). Another direction of work, often reported in the TREC Robust Track, is to use a large external collection like Wikipedia or the Web as a source of expansion terms (Xu et al., 2009; Voorhees, 2006). The intuition behind the above approach is that if the query does not have many relevant documents in the collection then any improvements in the modeling of PRF is bound to perform poorly due to query drift. Several approaches have been proposed for including different types of lexically and semantically related terms during query expansion. Voorhees (1994) use Wordnet for query expansion and rep</context>
</contexts>
<marker>Tao, Zhai, 2006</marker>
<rawString>Tao Tao and ChengXiang Zhai. 2006. Regularized Estimation of Mixture Models for Robust Pseudo-Relevance Feedback. In SIGIR ’06, pages 162–169. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tuomas Talvensaari</author>
<author>Jorma Laurikkala</author>
<author>Kalervo J¨arvelin</author>
<author>Martti Juhola</author>
<author>Heikki Keskustalo</author>
</authors>
<title>Creating and Exploiting a Comparable Corpus in Cross-language Information Retrieval.</title>
<date>2007</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>25</volume>
<issue>1</issue>
<marker>Talvensaari, Laurikkala, J¨arvelin, Juhola, Keskustalo, 2007</marker>
<rawString>Tuomas Talvensaari, Jorma Laurikkala, Kalervo J¨arvelin, Martti Juhola, and Heikki Keskustalo. 2007. Creating and Exploiting a Comparable Corpus in Cross-language Information Retrieval. ACM Trans. Inf. Syst., 25(1):4, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jrg Tiedemann</author>
</authors>
<title>The Use of Parallel Corpora in Monolingual Lexicography - How word alignment can identify morphological and semantic relations.</title>
<date>2001</date>
<booktitle>In COMPLEX ’01,</booktitle>
<pages>143--151</pages>
<contexts>
<context position="14689" citStr="Tiedemann, 2001" startWordPosition="2334" endWordPosition="2335">ual number of topics used during evaluation. Source Term Top Aligned Terms in Target French English am´ericain american, us, united, state, america nation nation, un, united, state, country e´tude study, research, assess, investigate, survey German English flugzeug aircraft, plane, aeroplane, air, flight spiele play, game, stake, role, player verh¨altnis relationship, relate, balance, proportion Table 3: Top Translation Alternatives for some sample words in Probabilistic Bi-Lingual Dictionary learned from a parallel sentence-aligned corpora in L1 −L2 based on word level alignments. Tiedemann (Tiedemann, 2001) has shown that the translation alternatives found using word alignments could be used to infer various morphological and semantic relations between terms. In Table 3, we show the top translation alternatives for some sample words. For example, the French word am´ericain (american) brings different variants of the translation like american, america, us, united, state, america which are lexically and semantically related. Hence, the probabilistic bi-lingual dictionary acts as a rich source of morphologically and semantically related feedback terms. Thus, during this step, of translating the fee</context>
</contexts>
<marker>Tiedemann, 2001</marker>
<rawString>Jrg Tiedemann. 2001. The Use of Parallel Corpora in Monolingual Lexicography - How word alignment can identify morphological and semantic relations. In COMPLEX ’01, pages 143–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Query Expansion Using LexicalSemantic Relations.</title>
<date>1994</date>
<booktitle>In SIGIR ’94,</booktitle>
<pages>61--69</pages>
<publisher>SpringerVerlag.</publisher>
<contexts>
<context position="6783" citStr="Voorhees (1994)" startWordPosition="1060" endWordPosition="1061">e of documents in the feedback set (Tao and Zhai, 2006). Another direction of work, often reported in the TREC Robust Track, is to use a large external collection like Wikipedia or the Web as a source of expansion terms (Xu et al., 2009; Voorhees, 2006). The intuition behind the above approach is that if the query does not have many relevant documents in the collection then any improvements in the modeling of PRF is bound to perform poorly due to query drift. Several approaches have been proposed for including different types of lexically and semantically related terms during query expansion. Voorhees (1994) use Wordnet for query expansion and report negative results. Recently, random walk models (Lafferty and Zhai, 2001; CollinsThompson and Callan, 2005) have been used to learn a rich set of term level associations by combining evidence from various kinds of information sources like WordNet, Web etc. Metzler and Croft (2007) propose a feature based approach called latent concept expansion to model term dependencies. All the above mentioned approaches use the resources available within the language to improve the performance of PRF. However, we make use of a second language to improve the perform</context>
</contexts>
<marker>Voorhees, 1994</marker>
<rawString>Ellen M. Voorhees. 1994. Query Expansion Using LexicalSemantic Relations. In SIGIR ’94, pages 61–69. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2006</date>
<booktitle>In TREC 2005,</booktitle>
<location>Gaithersburg, MD. NIST.</location>
<contexts>
<context position="6421" citStr="Voorhees, 2006" startWordPosition="1001" endWordPosition="1002">obustness of PRF. Some of the representative techniques are (i) Refining the feedback document set (Mitra et al., 1998; Sakai et al., 2005), (ii) Refining the terms obtained through PRF by selecting good expansion terms (Cao et al., 2008) and (iii) Using selective query expansion (Amati et al., 2004; Cronen-Townsend et al., 2004) and (iv) Varying the importance of documents in the feedback set (Tao and Zhai, 2006). Another direction of work, often reported in the TREC Robust Track, is to use a large external collection like Wikipedia or the Web as a source of expansion terms (Xu et al., 2009; Voorhees, 2006). The intuition behind the above approach is that if the query does not have many relevant documents in the collection then any improvements in the modeling of PRF is bound to perform poorly due to query drift. Several approaches have been proposed for including different types of lexically and semantically related terms during query expansion. Voorhees (1994) use Wordnet for query expansion and report negative results. Recently, random walk models (Lafferty and Zhai, 2001; CollinsThompson and Callan, 2005) have been used to learn a rich set of term level associations by combining evidence fro</context>
<context position="19863" citStr="Voorhees, 2006" startWordPosition="3195" endWordPosition="3196"> as $ indicate that the improvement was found to be statistically significant over the baseline at 90% confidence level (α = 0.01) when tested using a paired two-tailed t-test. since some function words like l’, d’ etc., occur as prefixes to a word, we strip them off during indexing and query processing, since it significantly improves the baseline performance. We use standard evaluation measures like MAP, P@S and P@10 for evaluation. Additionally, for assessing robustness, we use the Geometric Mean Average Precision (GMAP) metric (Robertson, 2006) which is also used in the TREC Robust Track (Voorhees, 2006). The probabilistic bi-lingual dictionary used in MultiPRF was learnt automatically by running GIZA++: a word alignment tool (Och and Ney, 2003) on a parallel sentence aligned corpora. For all the above language pairs we used the Europarl Corpus (Philipp, 2005). We use Google Translate as the query translation system as it has been shown to perform well for the task (Wu et al., 2008). We use the MBF approach explained in Section 3 as a baseline for comparison. We use two-stage Dirichlet smoothing with the optimal parameters tuned based on the collection (Zhai and Lafferty, 2004). We tune the p</context>
</contexts>
<marker>Voorhees, 2006</marker>
<rawString>Ellen Voorhees. 2006. Overview of the TREC 2005 Robust Retrieval Track. In TREC 2005, Gaithersburg, MD. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Wu</author>
<author>Daqing He</author>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>A Study of Using an Out-of-Box Commercial MT System for Query Translation in CLIR.</title>
<date>2008</date>
<booktitle>In iNEWS ’08,</booktitle>
<pages>71--76</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="20249" citStr="Wu et al., 2008" startWordPosition="3258" endWordPosition="3261">ation measures like MAP, P@S and P@10 for evaluation. Additionally, for assessing robustness, we use the Geometric Mean Average Precision (GMAP) metric (Robertson, 2006) which is also used in the TREC Robust Track (Voorhees, 2006). The probabilistic bi-lingual dictionary used in MultiPRF was learnt automatically by running GIZA++: a word alignment tool (Och and Ney, 2003) on a parallel sentence aligned corpora. For all the above language pairs we used the Europarl Corpus (Philipp, 2005). We use Google Translate as the query translation system as it has been shown to perform well for the task (Wu et al., 2008). We use the MBF approach explained in Section 3 as a baseline for comparison. We use two-stage Dirichlet smoothing with the optimal parameters tuned based on the collection (Zhai and Lafferty, 2004). We tune the parameters of MBF, specifically A and α, and choose the values which give the optimal performance on a given collection. We uniformly choose the top ten documents for feedback. Table 4 gives the overall results. 6 Results and Discussion In Table 4, we see the performance of the MultiPRF approach for three assisting languages, and how it compares with the baseline MBF methods. We find </context>
</contexts>
<marker>Wu, He, Ji, Grishman, 2008</marker>
<rawString>Dan Wu, Daqing He, Heng Ji, and Ralph Grishman. 2008. A Study of Using an Out-of-Box Commercial MT System for Query Translation in CLIR. In iNEWS ’08, pages 71– 76. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>W Bruce Croft</author>
</authors>
<title>Improving the Effectiveness of Information Retrieval with Local Context Analysis.</title>
<date>2000</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="2193" citStr="Xu and Croft, 2000" startWordPosition="323" endWordPosition="326">al (IR) is to satisfy the user’s information need, which is typically expressed through a short (typically 2-3 words) and often ambiguous query. The problem of matching the user’s query to the documents is rendered difficult by natural language phenomena like morphological variations, polysemy and synonymy. Relevance Feedback (RF) tries to overcome these problems by eliciting user feedback on the relevance of documents obtained from the initial ranking and then uses it to automatically refine the query. Since user input is hard to obtain, Pseudo-Relevance Feedback (PRF) (Buckley et al., 1994; Xu and Croft, 2000; Mitra et al., 1998) is used as an alternative, wherein RF is performed by assuming the top k documents from the initial retrieval as being relevant to the query. Based on the above assumption, the terms in the feedback document set are analyzed to choose the most distinguishing set of terms that characterize the feedback documents and as a result the relevance of a document. Query refinement is done by adding the terms obtained through PRF, along with their weights, to the actual query. Although PRF has been shown to improve retrieval, it suffers from the following drawbacks: (a) the type of</context>
</contexts>
<marker>Xu, Croft, 2000</marker>
<rawString>Jinxi Xu and W. Bruce Croft. 2000. Improving the Effectiveness of Information Retrieval with Local Context Analysis. ACM Trans. Inf. Syst., 18(1):79–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxi Xu</author>
<author>Alexander Fraser</author>
<author>Ralph Weischedel</author>
</authors>
<date>2002</date>
<booktitle>Empirical Studies in Strategies for Arabic Retrieval. In SIGIR ’02,</booktitle>
<pages>269--274</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7920" citStr="Xu et al. (2002)" startWordPosition="1239" endWordPosition="1242">rmance of PRF. However, we make use of a second language to improve the performance of PRF. Our proposed approach is especially attractive in the case of resource-constrained languages where the original retrieval is bad due to poor coverage of the collection and/or inherent complexity of query processing (for example term conflation) in those languages. Jourlin et al. (1999) use parallel blind relevance feedback, i.e. they use blind relevance feedback on a larger, more reliable parallel corpus, to improve retrieval performance on imperfect transcriptions of speech. Another related idea is by Xu et al. (2002), where a statistical thesaurus is learned using the probabilistic bilingual dictionaries of Arabic to English and English to Arabic. Meij et al. (2009) tries to expand a query in a different language using language models for domainspecific retrieval, but in a very different setting. Since our method uses a corpus in the assisting language from a similar time period, it can be likened to the work by Talvensaari et al. (2007) who used comparable corpora for Cross-Lingual Information Retrieval (CLIR). Other work pertaining to document alignment in comparable corpora, such as Braschler and Sch¨a</context>
</contexts>
<marker>Xu, Fraser, Weischedel, 2002</marker>
<rawString>Jinxi Xu, Alexander Fraser, and Ralph Weischedel. 2002. Empirical Studies in Strategies for Arabic Retrieval. In SIGIR ’02, pages 269–274. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Xu</author>
<author>Gareth J F Jones</author>
<author>Bin Wang</author>
</authors>
<title>Query Dependent Pseudo-Relevance Feedback Based on Wikipedia.</title>
<date>2009</date>
<booktitle>In SIGIR ’09,</booktitle>
<pages>59--66</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6404" citStr="Xu et al., 2009" startWordPosition="997" endWordPosition="1000">performance and robustness of PRF. Some of the representative techniques are (i) Refining the feedback document set (Mitra et al., 1998; Sakai et al., 2005), (ii) Refining the terms obtained through PRF by selecting good expansion terms (Cao et al., 2008) and (iii) Using selective query expansion (Amati et al., 2004; Cronen-Townsend et al., 2004) and (iv) Varying the importance of documents in the feedback set (Tao and Zhai, 2006). Another direction of work, often reported in the TREC Robust Track, is to use a large external collection like Wikipedia or the Web as a source of expansion terms (Xu et al., 2009; Voorhees, 2006). The intuition behind the above approach is that if the query does not have many relevant documents in the collection then any improvements in the modeling of PRF is bound to perform poorly due to query drift. Several approaches have been proposed for including different types of lexically and semantically related terms during query expansion. Voorhees (1994) use Wordnet for query expansion and report negative results. Recently, random walk models (Lafferty and Zhai, 2001; CollinsThompson and Callan, 2005) have been used to learn a rich set of term level associations by combi</context>
</contexts>
<marker>Xu, Jones, Wang, 2009</marker>
<rawString>Yang Xu, Gareth J.F. Jones, and Bin Wang. 2009. Query Dependent Pseudo-Relevance Feedback Based on Wikipedia. In SIGIR ’09, pages 59–66. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>Model-based Feedback in the Language Modeling approach to Information Retrieval.</title>
<date>2001</date>
<booktitle>In CIKM ’01,</booktitle>
<pages>403--410</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5734" citStr="Zhai and Lafferty, 2001" startWordPosition="879" endWordPosition="882"> the related work. Section 3, explains the Language Modeling (LM) based PRF approach. Section 4, describes the MultiPRF approach. Section 5 discusses the experimental set up. Section 6 presents the results, and studies the effect of varying the assisting language and incorporates multiple assisting languages. Finally, Section 7 concludes the paper by summarizing and outlining future work. 2 Related Work PRF has been successfully applied in various IR frameworks like vector space models, probabilistic IR and language modeling (Buckley et al., 1994; Jones et al., 2000; Lavrenko and Croft, 2001; Zhai and Lafferty, 2001). Several approaches have been proposed to improve the performance and robustness of PRF. Some of the representative techniques are (i) Refining the feedback document set (Mitra et al., 1998; Sakai et al., 2005), (ii) Refining the terms obtained through PRF by selecting good expansion terms (Cao et al., 2008) and (iii) Using selective query expansion (Amati et al., 2004; Cronen-Townsend et al., 2004) and (iv) Varying the importance of documents in the feedback set (Tao and Zhai, 2006). Another direction of work, often reported in the TREC Robust Track, is to use a large external collection lik</context>
<context position="9906" citStr="Zhai and Lafferty, 2001" startWordPosition="1565" endWordPosition="1568">w|OQ) respectively. For a given query, the document language models are ranked based on their proximity to the query language model, measured using KL-Divergence. P (w|ΘQ) P(w|ΘQ) · log P (w|D) Since the query length is short, it is difficult to estimate OQ accurately using the query alone. In PRF, the top k documents obtained through the initial ranking algorithm are assumed to be relevant and used as feedback for improving the estimation of OQ. The feedback documents contain both relevant and noisy terms from which the feedback language model is inferred based on a Generative Mixture Model (Zhai and Lafferty, 2001). Let DF = {d1, d2, ... , dkI be the top k documents retrieved using the initial ranking algorithm. Zhai and Lafferty (Zhai and Lafferty, 2001) model the feedback document set DF as a mixture of two distributions: (a) the feedback language model and (b) the collection model P(w|C). The feedback language model is inferred using the EM Algorithm (Dempster et al., 1977), which iteratively accumulates probability mass on the most distinguishing terms, i.e. terms which are more frequent in the feedback document set than in the entire collection. To maintain query focus the final converged feedback </context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2001. Model-based Feedback in the Language Modeling approach to Information Retrieval. In CIKM ’01, pages 403–410. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A Study of Smoothing Methods for Language Models applied to Information Retrieval.</title>
<date>2004</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="20448" citStr="Zhai and Lafferty, 2004" startWordPosition="3290" endWordPosition="3293"> the TREC Robust Track (Voorhees, 2006). The probabilistic bi-lingual dictionary used in MultiPRF was learnt automatically by running GIZA++: a word alignment tool (Och and Ney, 2003) on a parallel sentence aligned corpora. For all the above language pairs we used the Europarl Corpus (Philipp, 2005). We use Google Translate as the query translation system as it has been shown to perform well for the task (Wu et al., 2008). We use the MBF approach explained in Section 3 as a baseline for comparison. We use two-stage Dirichlet smoothing with the optimal parameters tuned based on the collection (Zhai and Lafferty, 2004). We tune the parameters of MBF, specifically A and α, and choose the values which give the optimal performance on a given collection. We uniformly choose the top ten documents for feedback. Table 4 gives the overall results. 6 Results and Discussion In Table 4, we see the performance of the MultiPRF approach for three assisting languages, and how it compares with the baseline MBF methods. We find MultiPRF to consistently outperform the baseline value on all metrics, namely MAP (where significant improvements range from 4.4% to 7.1%); P@5 (significant improvements range from 4.9% to 39.5% and </context>
</contexts>
<marker>Zhai, Lafferty, 2004</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2004. A Study of Smoothing Methods for Language Models applied to Information Retrieval. ACM Transactions on Information Systems, 22(2):179–214.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>