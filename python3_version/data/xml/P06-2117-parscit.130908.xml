<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.798826">
Boosting Statistical Word Alignment Using
Labeled and Unlabeled Data
</title>
<author confidence="0.989224">
Hua Wu Haifeng Wang Zhanyi Liu
</author>
<affiliation confidence="0.984489">
Toshiba (China) Research and Development Center
</affiliation>
<address confidence="0.975175">
5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District
Beijing, 100738, China
</address>
<email confidence="0.997116">
{wuhua, wanghaifeng, liuzhanyi}@rdc.toshiba.com.cn
</email>
<sectionHeader confidence="0.998598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99937208">
This paper proposes a semi-supervised
boosting approach to improve statistical
word alignment with limited labeled data
and large amounts of unlabeled data. The
proposed approach modifies the super-
vised boosting algorithm to a semi-
supervised learning algorithm by incor-
porating the unlabeled data. In this algo-
rithm, we build a word aligner by using
both the labeled data and the unlabeled
data. Then we build a pseudo reference
set for the unlabeled data, and calculate
the error rate of each word aligner using
only the labeled data. Based on this semi-
supervised boosting algorithm, we inves-
tigate two boosting methods for word
alignment. In addition, we improve the
word alignment results by combining the
results of the two semi-supervised boost-
ing methods. Experimental results on
word alignment indicate that semi-
supervised boosting achieves relative er-
ror reductions of 28.29% and 19.52% as
compared with supervised boosting and
unsupervised boosting, respectively.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997078">
Word alignment was first proposed as an inter-
mediate result of statistical machine translation
(Brown et al., 1993). In recent years, many re-
searchers build alignment links with bilingual
corpora (Wu, 1997; Och and Ney, 2003; Cherry
and Lin, 2003; Wu et al., 2005; Zhang and
Gildea, 2005). These methods unsupervisedly
train the alignment models with unlabeled data.
A question about word alignment is whether
we can further improve the performances of the
word aligners with available data and available
alignment models. One possible solution is to use
the boosting method (Freund and Schapire,
1996), which is one of the ensemble methods
(Dietterich, 2000). The underlying idea of boost-
ing is to combine simple &amp;quot;rules&amp;quot; to form an en-
semble such that the performance of the single
ensemble is improved. The AdaBoost (Adaptive
Boosting) algorithm by Freund and Schapire
(1996) was developed for supervised learning.
When it is applied to word alignment, it should
solve the problem of building a reference set for
the unlabeled data. Wu and Wang (2005) devel-
oped an unsupervised AdaBoost algorithm by
automatically building a pseudo reference set for
the unlabeled data to improve alignment results.
In fact, large amounts of unlabeled data are
available without difficulty, while labeled data is
costly to obtain. However, labeled data is valu-
able to improve performance of learners. Conse-
quently, semi-supervised learning, which com-
bines both labeled and unlabeled data, has been
applied to some NLP tasks such as word sense
disambiguation (Yarowsky, 1995; Pham et al.,
2005), classification (Blum and Mitchell, 1998;
Thorsten, 1999), clustering (Basu et al., 2004),
named entity classification (Collins and Singer,
1999), and parsing (Sarkar, 2001).
In this paper, we propose a semi-supervised
boosting method to improve statistical word
alignment with both limited labeled data and
large amounts of unlabeled data. The proposed
approach modifies the supervised AdaBoost al-
gorithm to a semi-supervised learning algorithm
by incorporating the unlabeled data. Therefore, it
should address the following three problems. The
first is to build a word alignment model with
both labeled and unlabeled data. In this paper,
with the labeled data, we build a supervised
model by directly estimating the parameters in
</bodyText>
<page confidence="0.982898">
913
</page>
<note confidence="0.724275">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 913–920,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999942760869565">
the model instead of using the Expectation
Maximization (EM) algorithm in Brown et al.
(1993). With the unlabeled data, we build an un-
supervised model by estimating the parameters
with the EM algorithm. Based on these two word
alignment models, an interpolated model is built
through linear interpolation. This interpolated
model is used as a learner in the semi-supervised
AdaBoost algorithm. The second is to build a
reference set for the unlabeled data. It is auto-
matically built with a modified &amp;quot;refined&amp;quot; combi-
nation method as described in Och and Ney
(2000). The third is to calculate the error rate on
each round. Although we build a reference set
for the unlabeled data, it still contains alignment
errors. Thus, we use the reference set of the la-
beled data instead of that of the entire training
data to calculate the error rate on each round.
With the interpolated model as a learner in the
semi-supervised AdaBoost algorithm, we inves-
tigate two boosting methods in this paper to im-
prove statistical word alignment. The first
method uses the unlabeled data only in the inter-
polated model. During training, it only changes
the distribution of the labeled data. The second
method changes the distribution of both the la-
beled data and the unlabeled data during training.
Experimental results show that both of these two
methods improve the performance of statistical
word alignment.
In addition, we combine the final results of the
above two semi-supervised boosting methods.
Experimental results indicate that this combina-
tion outperforms the unsupervised boosting
method as described in Wu and Wang (2005),
achieving a relative error rate reduction of
19.52%. And it also achieves a reduction of
28.29% as compared with the supervised boost-
ing method that only uses the labeled data.
The remainder of this paper is organized as
follows. Section 2 briefly introduces the statisti-
cal word alignment model. Section 3 describes
parameter estimation method using the labeled
data. Section 4 presents our semi-supervised
boosting method. Section 5 reports the experi-
mental results. Finally, we conclude in section 6.
</bodyText>
<sectionHeader confidence="0.986797" genericHeader="method">
2 Statistical Word Alignment Model
</sectionHeader>
<bodyText confidence="0.996666">
According to the IBM models (Brown et al.,
1993), the statistical word alignment model can
be generally represented as in equation (1).
</bodyText>
<equation confidence="0.857191333333333">
Pr(a, f  |e) = ∑Pr(a&apos;, f  |e) (1)
Pr(a f  |e)
a&apos;
</equation>
<bodyText confidence="0.9742965">
Where and f represent the source sentence
e
and the target sentence, respectively.
In this paper, we use a simplified IBM model
4 (Al-Onaizan et al., 1999), which is shown in
equation (2). This simplified version does not
take into account word classes as described in
Brown et al. (1993).
</bodyText>
<equation confidence="0.9948841">
φ 0 ⎞ m − 2 φ φ
0 0
⎟ p p
0 1
⎠
m
ei ) t(f j  |eaj )
i
=1 j=
1
m
([ ( )] (
j⋅
1
=1,aj≠0
([ ( )] ( ( ))))
j
j⋅
&gt;
j=1,aj ≠0
</equation>
<bodyText confidence="0.8325682">
l, m are the lengths of the source sentence and
the target sentence respectively.
j is the position index of the target word.
a j is the position of the source word aligned to
the target word.
</bodyText>
<equation confidence="0.953449">
j thφi is the number of target words that is
e i
aligned to.
p 0 , p 1 are the fertility probabilities for , and
e 0p0+ p1 =1 .
t(fj  |eaj) is the word translation probability.
n(φi  |ei) is the fertility probability.
d j − cρ is the distortion probability for the
1 ( a j)
</equation>
<bodyText confidence="0.693812">
head word of cept1 i.
</bodyText>
<equation confidence="0.96346725">
d&gt;1 (j − p(j)) is the distortion probability for the
non-head words of cept i.
h(i) =min{k : i = a is the head of cept i.
k }
k
p j = k &lt; j k a = a
( ) max { : j k .
is the first word before
with non-zero
fertility.
ρi
ei
</equation>
<bodyText confidence="0.994056">
ci is the center of cept i.
</bodyText>
<sectionHeader confidence="0.8991565" genericHeader="method">
3 Parameter Estimation with Labeled
Data
</sectionHeader>
<bodyText confidence="0.99976625">
With the labeled data, instead of using EM algo-
rithm, we directly estimate the three main pa-
rameters in model 4: translation probability, fer-
tility probability, and distortion probability.
</bodyText>
<footnote confidence="0.4847485">
1 A cept is defined as the set of target words connected to a source word
(Brown et al., 1993).
</footnote>
<figure confidence="0.954480761904762">
Pr( a,f  |e) = ⎛ ⎜ m−
⎝ φ0
l
∏
(
j
∏
n
|
φi
(
j
⋅∏
⋅
(2)
+
m
∏
h a
1
}
</figure>
<page confidence="0.59359">
914
</page>
<figure confidence="0.684512142857143">
3.1 Translation Probability Where δ(x, y) =1 if x = y . Otherwise, δ(x, y) = 0.
The translation probability is estimated from the
4 Boosting with Labeled Data and
labeled data as described in (3).
Where count(ei, f j) is the occurring frequency of
ei aligned to in the labeled data.
fj
</figure>
<subsectionHeader confidence="0.995977">
3.2 Fertility Probability
</subsectionHeader>
<bodyText confidence="0.78249725">
The fertility probability n(φi  |ei) describes the
distribution of the numbers of words that is
ei
aligned to. It is estimated as described in (4).
</bodyText>
<subsectionHeader confidence="0.947124">
Unlabeled Data
</subsectionHeader>
<bodyText confidence="0.99988">
supervised AdaBoost algorithm for word align-
ment, which uses both the labeled data and the
unlabeled data. Based on the semi-supervised
algorithm, we describe two boosting methods for
word alignment. And then we develop a method
to combine the results of the two boosting meth-
ods.
</bodyText>
<subsectionHeader confidence="0.8619045">
4.1 Semi-Supervised AdaBoost Algorithm
for Word Alignment
</subsectionHeader>
<equation confidence="0.752452545454545">
t(fj  |ei) = ∑ count(ei , f&apos;
f
)
&apos;
count
( , )
e f
ij
(3) In this section, we first propose a semi-
i i Figure 1 shows the semi-supervised AdaBoost
count(φ&apos; , ei )
(4) algori
count e
( , )
φ
∑
thm for word alignment by using labeled
n(φi  |ei) =
&apos;
φ
p0 and describe the fertility probabilities
p 1
</equation>
<bodyText confidence="0.979897222222222">
for . And and sum to 1. We estimate
e 0 p 0 p 1
p0 directly from the labeled data, which is
shown in (5).
Where # Aligned is the occurring frequency of
the target words that have counterparts in the
source language. # Null is the occurring fre-
quency of the target words that have no counter-
parts in the source language.
</bodyText>
<subsectionHeader confidence="0.998303">
3.3 Distortion Probability
</subsectionHeader>
<bodyText confidence="0.946202">
There are two kinds of distortion probability in
model 4: one for head words and the other for
non-head words. Both of the distortion probabili-
ties describe the distribution of relative positions
Thus, if we let Δj1 = j − cρ and Δj&gt;1 = j − p(j) ,
i
the distortion probabilities for head words and
non-head words are estimated in (6) and (7) with
the labeled data, respectively.
</bodyText>
<equation confidence="0.960323333333333">
Δj1, j − cρi )
j c
, ρ i
d j
1 1
( )
Δ = ∑ ∑
δ( ,
Δ −
j j c
&apos; &apos; )
1 ρi &apos;
Δ j j c
&apos; &apos; ,
1 ρi&apos;
∑ δ (Δj &gt;1 , j − p(j))
Δ &gt;
j j p j
&apos;
&apos; , ( &apos; )
1
</equation>
<bodyText confidence="0.994481619047619">
and unlabeled data. Compared with the super-
vised Adaboost algorithm, this semi-supervised
AdaBoost algorithm mainly has five differences.
The first is the word alignment model, which
is taken as a learner in the boosting algorithm.
The word alignment model is built using both the
labeled data and the unlabeled data. With the
labeled data, we train a supervised model by di-
rectly estimating the parameters in the IBM
model as described in section 3. With the unla-
beled data, we train an unsupervised model using
the same EM algorithm in Brown et al. (1993).
Then we build an interpolation model by linearly
interpolating these two word alignment models,
which is shown in (8). This interpolated model is
used as the model Ml described in figure 1.
Where PrS (a, f  |e) and PrU (a, f  |e) are the
trained supervised model and unsupervised
model, respectively. λ is an interpolation weight.
We train the weight in equation (8) in the same
way as described in Wu et al. (2005).
</bodyText>
<subsectionHeader confidence="0.770076">
Pseudo Reference Set for Unlabeled Data
</subsectionHeader>
<bodyText confidence="0.9998515">
The second is the reference set for the unla-
beled data. For the unlabeled data, we automati-
cally build a pseudo reference set. In order to
build a reliable pseudo reference set, we perform
bi-directional word alignment on the training
data using the interpolated model trained on the
first round. Bi-directional word alignment in-
cludes alignment in two directions (source to
</bodyText>
<equation confidence="0.815202666666667">
1(Δj&gt;1) =
∑ ∑
δ(Δj&gt;1, j − p(j ))
</equation>
<page confidence="0.487034">
d&gt;
</page>
<figure confidence="0.971280176470588">
j p ( j)
,
p=
0 #Aligned
# Aligned Null
−#
(5)
(
δ
∑
Where count(φi , ei) describes the occurring fre-
quency of word ei aligned to φi target words in
the labeled data. Word Alignment Model
=λ⋅PrS(a,f  |e)+(1−λ)⋅ PrU (a,f  |e) (8)
|
Pr(a, f
e)
</figure>
<page confidence="0.944008">
915
</page>
<bodyText confidence="0.806997333333333">
Input: A training set ST including m bilingual sentence pairs;
The reference set RT for the training data;
The reference sets and (
</bodyText>
<figure confidence="0.4670632">
RL RU RL, RU ⊆ RT ) for the labeled data and the unlabeled
SL
data respectively, where
SU ST = SU ∪ SL and SU ∩ SL = NULL;
A loop count L.
</figure>
<figureCaption confidence="0.999471">
Figure 1. The Semi-Supervised Adaboost Algorithm for Word Alignment
</figureCaption>
<figure confidence="0.904592880952381">
(1) Initialize the weights:
w1(i)=1 /m, i=1,... ,m
(2) For l =1 to L , execute steps (3) to (9).
(3) For each sentence pair i, normalize the
weights on the training set:
pl(i)=wl(i) /∑wl(j), i=1,... ,m
j
(4) Update the word alignment model Ml
based on the weighted training data.
(5) Perform word alignment on the training set
with the alignment model Ml:
hl =Ml(pl)
(6) Calculate the error of with the reference
hl
εl = ∑ ⋅
p l ( i )
i
Where α(i) is calculated as in equation (9).
(7) If εl &gt; 1 / 2, then let L=l− 1, and end the
training process.
(8) Let βl = εl /(1− ε l ) .
(9) For all i, compute new weights:
wl+1(i)=wl(i)⋅ (k+(n−k)⋅ βl)/ n
where, n represents n alignment links in
the ith sentence pair. k represents the num-
ber of error links as compared with RT .
α(i)
set :
RL
Output: The final word alignment result for a source word e :
L 1
h e = = ) ( , ) ( ( ), )
⋅ WT e f h e f
⋅ δ
F ( ) argmax ( , ) argmax (log
RS e f
l l
∑= β
f f l 1 l
Where δ(x, y) =1 if x = y . Otherwise, δ(x, y) = 0. WTl (e, f) is the weight of the alignment link
(e, f) produced by the model , which is calculated as described in equation (10).
Ml
</figure>
<bodyText confidence="0.994371666666667">
target and target to source) as described in Och
and Ney (2000). Thus, we get two sets of align-
ment results and on the unlabeled data.
</bodyText>
<equation confidence="0.508558">
A1 A2
</equation>
<bodyText confidence="0.9207825">
Based on these two sets, we use a modified &amp;quot;re-
fined&amp;quot; method (Och and Ney, 2000) to construct
</bodyText>
<figure confidence="0.742704294117647">
a pseudo reference set .
RU
(1) The intersection I = A1 ∩ A2 is added to the
reference set .
RU
(2) We add (e, f)∈ A1 ∪ A2 RU
to if a) is satis-
fied or both b) and c) are satisfied.
a) Neither nor has an alignment in
e f
and p(f  |e) is greater than a threshold
Where count(e, f) is the occurring fre-
quency of the alignment link in
( e , f)
the bi-directional word alignment results.
b) has a horizontal or a vertical
( e, f)
</figure>
<bodyText confidence="0.711566">
neighbor that is already in .
RU
c) The set RU ∪ (e, f) does not contain
alignments with both horizontal and ver-
tical neighbors.
</bodyText>
<subsectionHeader confidence="0.613106">
Error of Word Aligner
</subsectionHeader>
<bodyText confidence="0.999951166666667">
The third is the calculation of the error of the
individual word aligner on each round. For word
alignment, a sentence pair is taken as a sample.
Thus, we calculate the error rate of each sentence
pair as described in (9), which is the same as de-
scribed in Wu and Wang (2005).
</bodyText>
<equation confidence="0.999218">
_ 2 |SW ∩ SR
α(i) 1  |SW|+  |SR  |(9)
</equation>
<bodyText confidence="0.958627">
Where represents the set of alignment
SW
links of a sentence pair i identified by the indi-
vidual interpolated model on each round. is
SR
the reference alignment set for the sentence pair.
With the error rate of each sentence pair, we
calculate the error of the word aligner on each
round. Although we build a pseudo reference set
RU for the unlabeled data, it contains alignment
errors. Thus, the weighted sum of the error rates
of sentence pairs in the labeled data instead of
that in the entire training data is used as the error
of the word aligner.
</bodyText>
<figure confidence="0.693668066666667">
count e f
( , )
=
p f
( |
e)
&apos;
f
count(e,
)
∑
&apos;
f
RU
δ1.
</figure>
<page confidence="0.993678">
916
</page>
<bodyText confidence="0.957784888888889">
Weights Update for Sentence Pairs
The forth is the weight update for sentence
pairs according to the error and the reference set.
In a sentence pair, there are usually several word
alignment links. Some are correct, and others
may be incorrect. Thus, we update the weights
according to the number of correct and incorrect
alignment links as compared with the reference
set, which is shown in step (9) in figure 1.
</bodyText>
<subsectionHeader confidence="0.759263">
Weights for Word Alignment Links
</subsectionHeader>
<bodyText confidence="0.870025461538461">
The fifth is the weights used when we con-
struct the final ensemble. Besides the weight
log(1 / βl) , which is the confidence measure of
the word aligner, we also use the weight
l th
to measure the confidence of each
alignment link produced by the model . The
Ml
weight WTl(e, f) is calculated as shown in (10).
Wu and Wang (2005) proved that adding this
weight improved the word alignment results.
Where count(e, f) is the occurring frequency
of the alignment link in the word align-
</bodyText>
<equation confidence="0.794987">
( e , f)
</equation>
<bodyText confidence="0.754049333333333">
ment results of the training data produced by the
model .
Ml
</bodyText>
<subsectionHeader confidence="0.990468">
4.2 Method 1
</subsectionHeader>
<bodyText confidence="0.999730214285714">
This method only uses the labeled data as train-
ing data. According to the algorithm in figure 1,
we obtain ST = SL and RT = RL. Thus, we only
change the distribution of the labeled data. How-
ever, we build an unsupervised model using the
unlabeled data. On each round, we keep this un-
supervised model unchanged, and we rebuild the
supervised model by estimating the parameters
as described in section 3 with the weighted train-
ing data. Then we interpolate the supervised
model and the unsupervised model to obtain an
interpolated model as described in section 4.1.
The interpolated model is used as the alignment
model in figure 1. Thus, in this interpolated
</bodyText>
<subsubsectionHeader confidence="0.352032">
Ml
</subsubsectionHeader>
<bodyText confidence="0.9999896">
model, we use both the labeled and unlabeled
data. On each round, we rebuild the interpolated
model using the rebuilt supervised model and the
unchanged unsupervised model. This interpo-
lated model is used to align the training data.
According to the reference set of the labeled
data, we calculate the error of the word aligner
on each round. According to the error and the
reference set, we update the weight of each sam-
ple in the labeled data.
</bodyText>
<subsectionHeader confidence="0.995607">
4.3 Method 2
</subsectionHeader>
<bodyText confidence="0.984619636363636">
This method uses both the labeled data and the
unlabeled data as training data. Thus, we set
ST = SL V SU and RT = RL V RU as described in
figure 1. With the labeled data, we build a super-
vised model, which is kept unchanged on each
round.2 With the weighted samples in the train-
ing data, we rebuild the unsupervised model with
EM algorithm on each round. Based on these two
models, we built an interpolated model as de-
scribed in section 4.1. The interpolated model is
used as the alignment model in figure 1. On
</bodyText>
<subsubsectionHeader confidence="0.346703">
Ml
</subsubsectionHeader>
<bodyText confidence="0.963425357142857">
each round, we rebuild the interpolated model
using the unchanged supervised model and the
rebuilt unsupervised model. Then the interpo-
lated model is used to align the training data.
Since the training data includes both labeled
and unlabeled data, we need to build a pseudo
reference set for the unlabeled data using the
RU
method described in section 4.1. According to
the reference set of the labeled data, we cal-
RL
culate the error of the word aligner on each
round. Then, according to the pseudo reference
set and the reference set , we update the
</bodyText>
<sectionHeader confidence="0.551053" genericHeader="method">
RU RL
</sectionHeader>
<bodyText confidence="0.995293">
weight of each sentence pair in the unlabeled
data and in the labeled data, respectively.
There are four main differences between
Method 2 and Method 1.
</bodyText>
<listItem confidence="0.913011923076923">
(1) On each round, Method 2 changes the distri-
bution of both the labeled data and the unla-
beled data, while Method 1 only changes the
distribution of the labeled data.
(2) Method 2 rebuilds the unsupervised model,
while Method 1 rebuilds the supervised
model.
(3) Method 2 uses the labeled data instead of the
entire training data to estimate the error of
the word aligner on each round.
(4) Method 2 uses an automatically built pseudo
reference set to update the weights for the
sentence pairs in the unlabeled data.
</listItem>
<subsectionHeader confidence="0.99558">
4.4 Combination
</subsectionHeader>
<bodyText confidence="0.972443666666667">
In the above two sections, we described two
semi-supervised boosting methods for word
alignment. Although we use interpolated models
</bodyText>
<footnote confidence="0.98293925">
2 In fact, we can also rebuild the supervised model accord-
ing to the weighted labeled data. In this case, as we know,
the error of the supervised model increases. Thus, we keep
the supervised model unchanged in this method.
</footnote>
<figure confidence="0.903210636363637">
WT e f
f &apos; e &apos;
l ( , ) = count(e&apos; ,f) (10)
Ycount(e, f ) +
Y
x
2
e, f)
count
WTl
(e, f)
</figure>
<page confidence="0.990609">
917
</page>
<bodyText confidence="0.999974083333333">
for word alignment in both Method 1 and
Method 2, the interpolated models are trained
with different weighted data. Thus, they perform
differently on word alignment. In order to further
improve the word alignment results, we combine
the results of the above two methods as described
in (11).
ods to calculate the precision, recall, f-measure,
and alignment error rate (AER) are shown in
equations (12), (13), (14), and (15). It can be
seen that the higher the f-measure is, the lower
the alignment error rate is.
</bodyText>
<equation confidence="0.955799909090909">
|S S |
precision = G ∩ C (12)
|S |
G
h e
3,F ( )
= arg max(λ1⋅ RS1 (e, f) + λ2⋅ RS2 (e, f))
f
 |(11) recall = SG ∩ SC |(13)
 |S |
C
</equation>
<figure confidence="0.936203458333333">
fmeasure
=
(14)
2|
× ∩
S S |
G C
AER = −
1 = −
1
+ ||
SC
fmeasure (15)
|
2 |
× SG∩ SC
|
|SG
|
|
SG
+|SC|
Where is the combined hypothesis for
h3 ,F (e)
</figure>
<figureCaption confidence="0.801288">
word alignment. RS1 (e, f) RS 2 ( e , f )
</figureCaption>
<bodyText confidence="0.958300166666667">
and are the
two ensemble results as shown in figure 1 for
Method 1 and Method 2, respectively. λ1 and λ2
are the constant weights.
In this paper, we take English to Chinese word
alignment as a case study.
</bodyText>
<subsectionHeader confidence="0.976537">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999346">
We have two kinds of training data from general
domain: Labeled Data (LD) and Unlabeled Data
(UD). The Chinese sentences in the data are
automatically segmented into words. The statis-
tics for the data is shown in Table 1. The labeled
data is manually word aligned, including 156,421
alignment links.
</bodyText>
<table confidence="0.99952825">
Data # Sentence # English # Chinese
Pairs Words Words
LD 31,069 255,504 302,470
UD 329,350 4,682,103 4,480,034
</table>
<tableCaption confidence="0.999205">
Table 1. Statistics for Training Data
</tableCaption>
<bodyText confidence="0.999609">
We use 1,000 sentence pairs as testing set,
which are not included in LD or UD. The testing
set is also manually word aligned, including
8,634 alignment links in the testing set3.
</bodyText>
<subsectionHeader confidence="0.998572">
5.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9684942">
We use the same evaluation metrics as described
in Wu et al. (2005), which is similar to those in
(Och and Ney, 2000). The difference lies in that
Wu et al. (2005) take all alignment links as sure
links.
If we use to represent the set of alignment
SG
links identified by the proposed method and SC
to denote the reference alignment set, the meth-
5.1, we get the word
an
d Ney (2000). Thus, the results in table 2 are
those of the &amp;quot;refined&amp;quot; combination. For EM
training, we use the GIZA++ toolkit4.
alignment results shown in table 2. For all of the
methods in this table, we perform bi-directional
(source to target and target to source) word
alignment, and obtain two alignment results on
the testing set. Based on the two results, we get
the &amp;quot;refined&amp;quot; combination as described in Och
</bodyText>
<subsectionHeader confidence="0.787433">
Results of Supervised Methods
</subsectionHeader>
<bodyText confidence="0.964604807692308">
Using the labeled data, we use two methods to
estimate the parameters in IBM model 4: one is
to use the EM algorithm, and the other is to esti-
mate the parameters directly from the labeled
data as described in section 3. In table 2, the
method &amp;quot;Labeled+EM&amp;quot; estimates the parameters
with the EM algorithm, which is an unsupervised
method without boosting. And the method &amp;quot;La-
beled+Direct&amp;quot; estimates the parameters directly
from the labeled data, which is a supervised
method without boosting. &amp;quot;Labeled+EM+Boost&amp;quot;
and &amp;quot;Labeled+Direct+Boost&amp;quot; represent the two
supervised boosting methods for the above two
parameter estimation methods.
Our methods that directly estimate parameters
in IBM model 4 are better than that using the EM
algorithm. &amp;quot;Labeled+Direct&amp;quot; is better than &amp;quot;La-
beled+EM&amp;quot;, achieving a relative error rate reduc-
tion of 22.97%. And &amp;quot;Labeled+Direct+Boost&amp;quot; is
better than &amp;quot;Labeled+EM+Boost&amp;quot;, achieving a
relative error rate reduction of 22.98%. In addi-
tion, the two boosting methods perform better
than their corresponding methods without
3 For a non one-to-one link, if m source words are aligned to
n target words, we take it as one alignment link instead of
m∗n alignment links. 4 It is located at http://www.fjoch.com/ GIZA++.html.
</bodyText>
<subsectionHeader confidence="0.85552">
5.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.340354">
5 Experiments With the data in section
</bodyText>
<page confidence="0.948165">
918
</page>
<table confidence="0.999948090909091">
Method Precision Recall F-Measure AER
Labeled+EM 0.6588 0.5210 0.5819 0.4181
Labeled+Direct 0.7269 0.6609 0.6924 0.3076
Labeled+EM+Boost 0.7384 0.5651 0.6402 0.3598
Labeled+Direct+Boost 0.7771 0.6757 0.7229 0.2771
Unlabeled+EM 0.7485 0.6667 0.7052 0.2948
Unlabeled+EM+Boost 0.8056 0.7070 0.7531 0.2469
Interpolated 0.7555 0.7084 0.7312 0.2688
Method 1 0.7986 0.7197 0.7571 0.2429
Method 2 0.8060 0.7388 0.7709 0.2291
Combination 0.8175 0.7858 0.8013 0.1987
</table>
<tableCaption confidence="0.99962">
Table 2. Word Alignment Results
</tableCaption>
<bodyText confidence="0.948843333333333">
boosting. For example, &amp;quot;Labeled+Direct+Boost&amp;quot;
achieves an error rate reduction of 9.92% as
compared with &amp;quot;Labeled+Direct&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.621253">
Results of Unsupervised Methods
</subsectionHeader>
<bodyText confidence="0.999992375">
With the unlabeled data, we use the EM algo-
rithm to estimate the parameters in the model.
The method &amp;quot;Unlabeled+EM&amp;quot; represents an un-
supervised method without boosting. And the
method &amp;quot;Unlabeled+EM+Boost&amp;quot; uses the same
unsupervised Adaboost algorithm as described in
Wu and Wang (2005).
The boosting method &amp;quot;Unlabeled+EM+Boost&amp;quot;
achieves a relative error rate reduction of 16.25%
as compared with &amp;quot;Unlabeled+EM&amp;quot;. In addition,
the unsupervised boosting method &amp;quot;Unla-
beled+EM+Boost&amp;quot; performs better than the su-
pervised boosting method &amp;quot;Labeled+Direct+
Boost&amp;quot;, achieving an error rate reduction of
10.90%. This is because the size of labeled data
is too small to subject to data sparseness problem.
</bodyText>
<subsectionHeader confidence="0.814943">
Results of Semi-Supervised Methods
</subsectionHeader>
<bodyText confidence="0.999363395348837">
By using both the labeled and the unlabeled
data, we interpolate the models trained by &amp;quot;La-
beled+Direct&amp;quot; and &amp;quot;Unlabeled+EM&amp;quot; to get an
interpolated model. Here, we use &amp;quot;interpolated&amp;quot;
to represent it. &amp;quot;Method 1&amp;quot; and &amp;quot;Method 2&amp;quot; rep-
resent the semi-supervised boosting methods de-
scribed in section 4.2 and section 4.3, respec-
tively. &amp;quot;Combination&amp;quot; denotes the method de-
scribed in section 4.4, which combines &amp;quot;Method
1&amp;quot; and &amp;quot;Method 2&amp;quot;. Both of the weights λ1 and
λ2 in equation (11) are set to 0.5.
&amp;quot;Interpolated&amp;quot; performs better than the meth-
ods using only labeled data or unlabeled data. It
achieves relative error rate reductions of 12.61%
and 8.82% as compared with &amp;quot;Labeled+Direct&amp;quot;
and &amp;quot;Unlabeled+EM&amp;quot;, respectively.
Using an interpolation model, the two semi-
supervised boosting methods &amp;quot;Method 1&amp;quot; and
&amp;quot;Method 2&amp;quot; outperform the supervised boosting
method &amp;quot;Labeled+Direct+Boost&amp;quot;, achieving a
relative error rate reduction of 12.34% and
17.32% respectively. In addition, the two semi-
supervised boosting methods perform better than
the unsupervised boosting method &amp;quot;Unlabeled+
EM+Boost&amp;quot;. &amp;quot;Method 1&amp;quot; performs slightly better
than &amp;quot;Unlabeled+EM+Boost&amp;quot;. This is because
we only change the distribution of the labeled
data in &amp;quot;Method 1&amp;quot;. &amp;quot;Method 2&amp;quot; achieves an er-
ror rate reduction of 7.77% as compared with
&amp;quot;Unlabeled+EM+Boost&amp;quot;. This is because we use
the interpolated model in our semi-supervised
boosting method, while &amp;quot;Unlabeled+EM+Boost&amp;quot;
only uses the unsupervised model.
Moreover, the combination of the two semi-
supervised boosting methods further improves
the results, achieving relative error rate reduc-
tions of 18.20% and 13.27% as compared with
&amp;quot;Method 1&amp;quot; and &amp;quot;Method 2&amp;quot;, respectively. It also
outperforms both the supervised boosting
method &amp;quot;Labeled+Direct+Boost&amp;quot; and the unsu-
pervised boosting method &amp;quot;Unlabeled+EM+
Boost&amp;quot;, achieving relative error rate reductions of
28.29% and 19.52% respectively.
</bodyText>
<subsectionHeader confidence="0.658772">
Summary of the Results
</subsectionHeader>
<bodyText confidence="0.999951333333333">
From the above result, it can be seen that all
boosting methods perform better than their corre-
sponding methods without boosting. The semi-
supervised boosting methods outperform the su-
pervised boosting method and the unsupervised
boosting method.
</bodyText>
<sectionHeader confidence="0.99914" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9998278">
This paper proposed a semi-supervised boosting
algorithm to improve statistical word alignment
with limited labeled data and large amounts of
unlabeled data. In this algorithm, we built an in-
terpolated model by using both the labeled data
</bodyText>
<page confidence="0.995363">
919
</page>
<bodyText confidence="0.9999692">
and the unlabeled data. This interpolated model
was employed as a learner in the algorithm. Then,
we automatically built a pseudo reference for the
unlabeled data, and calculated the error rate of
each word aligner with the labeled data. Based
on this algorithm, we investigated two methods
for word alignment. In addition, we developed a
method to combine the results of the above two
semi-supervised boosting methods.
Experimental results indicate that our semi-
supervised boosting method outperforms the un-
supervised boosting method as described in Wu
and Wang (2005), achieving a relative error rate
reduction of 19.52%. And it also outperforms the
supervised boosting method that only uses the
labeled data, achieving a relative error rate re-
duction of 28.29%. Experimental results also
show that all boosting methods outperform their
corresponding methods without boosting.
In the future, we will evaluate our method
with an available standard testing set. And we
will also evaluate the word alignment results in a
machine translation system, to examine whether
lower word alignment error rate will result in
higher translation accuracy.
</bodyText>
<sectionHeader confidence="0.999453" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999077">
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical Machine Translation
Final Report. Johns Hopkins University Workshop.
Sugato Basu, Mikhail Bilenko, and Raymond J.
Mooney. 2004. Probabilistic Framework for Semi-
Supervised Clustering. In Proc. of the 10th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD-2004), pages
59-68.
Avrim Blum and Tom Mitchell. 1998. Combing La-
beled and Unlabeled Data with Co-training. In
Proc. of the 11th Conference on Computational
Learning Theory (COLT-1998), pages1-10.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19(2): 263-311.
Colin Cherry and Dekang Lin. 2003. A Probability
Model to Improve Word Alignment. In Proc. of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2003), pages 88-95.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. In
Proc. of the Joint SIGDAT Conference on Empiri-
cal Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VLC-1999), pages
100-110.
Thomas G. Dietterich. 2000. Ensemble Methods in
Machine Learning. In Proc. of the First Interna-
tional Workshop on Multiple Classifier Systems
(MCS-2000), pages 1-15.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a New Boosting Algorithm. In Proc. of
the 13th International Conference on Machine
Learning (ICML-1996), pages 148-156.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proc. of the 38th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2000), pages 440-447.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19-51.
Thanh Phong Pham, Hwee Tou Ng, and Wee Sun Lee
2005. Word Sense Disambiguation with Semi-
Supervised Learning. In Proc. of the 20th National
Conference on Artificial Intelligence (AAAI 2005),
pages 1093-1098.
Anoop Sarkar. 2001. Applying Co-Training Methods
to Statistical Parsing. In Proc. of the 2nd Meeting of
the North American Association for Computational
Linguistics( NAACL-2001), pages 175-182.
Joachims Thorsten. 1999. Transductive Inference for
Text Classification Using Support Vector Ma-
chines. In Proc. of the 16th International Confer-
ence on Machine Learning (ICML-1999), pages
200-209.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3): 377-403.
Hua Wu and Haifeng Wang. 2005. Boosting Statisti-
cal Word Alignment. In Proc. of the 10th Machine
Translation Summit, pages 313-320.
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2005.
Alignment Model Adaptation for Domain-Specific
Word Alignment. In Proc. of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-2005), pages 467-474.
David Yarowsky. 1995. Unsupervised Word Sense
Disambiguation Rivaling Supervised Methods. In
Proc. of the 33rd Annual Meeting of the Association
for Computational Linguistics (ACL-1995), pages
189-196.
Hao Zhang and Daniel Gildea. 2005. Stochastic Lexi-
calized Inversion Transduction Grammar for
Alignment. In Proc. of the 43rd Annual Meeting of
the Association for Computational Linguistics
(ACL-2005), pages 475-482.
</reference>
<page confidence="0.996943">
920
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.788434">
<title confidence="0.9957135">Boosting Statistical Word Alignment Using Labeled and Unlabeled Data</title>
<author confidence="0.936687">Hua Wu Haifeng Wang Zhanyi Liu</author>
<affiliation confidence="0.990251">Toshiba (China) Research and Development Center</affiliation>
<address confidence="0.993843">5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District Beijing, 100738, China</address>
<email confidence="0.994069">wuhua@rdc.toshiba.com.cn</email>
<email confidence="0.994069">wanghaifeng@rdc.toshiba.com.cn</email>
<email confidence="0.994069">liuzhanyi@rdc.toshiba.com.cn</email>
<abstract confidence="0.994561884615385">This paper proposes a semi-supervised boosting approach to improve statistical word alignment with limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised boosting algorithm to a semisupervised learning algorithm by incorporating the unlabeled data. In this algorithm, we build a word aligner by using both the labeled data and the unlabeled data. Then we build a pseudo reference set for the unlabeled data, and calculate the error rate of each word aligner using only the labeled data. Based on this semisupervised boosting algorithm, we investigate two boosting methods for word alignment. In addition, we improve the word alignment results by combining the results of the two semi-supervised boosting methods. Experimental results on word alignment indicate that semisupervised boosting achieves relative error reductions of 28.29% and 19.52% as compared with supervised boosting and unsupervised boosting, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical Machine Translation Final Report.</title>
<date>1999</date>
<institution>Johns Hopkins University Workshop.</institution>
<contexts>
<context position="6253" citStr="Al-Onaizan et al., 1999" startWordPosition="975" endWordPosition="978">oduces the statistical word alignment model. Section 3 describes parameter estimation method using the labeled data. Section 4 presents our semi-supervised boosting method. Section 5 reports the experimental results. Finally, we conclude in section 6. 2 Statistical Word Alignment Model According to the IBM models (Brown et al., 1993), the statistical word alignment model can be generally represented as in equation (1). Pr(a, f |e) = ∑Pr(a&apos;, f |e) (1) Pr(a f |e) a&apos; Where and f represent the source sentence e and the target sentence, respectively. In this paper, we use a simplified IBM model 4 (Al-Onaizan et al., 1999), which is shown in equation (2). This simplified version does not take into account word classes as described in Brown et al. (1993). φ 0 ⎞ m − 2 φ φ 0 0 ⎟ p p 0 1 ⎠ m ei ) t(f j |eaj ) i =1 j= 1 m ([ ( )] ( j⋅ 1 =1,aj≠0 ([ ( )] ( ( )))) j j⋅ &gt; j=1,aj ≠0 l, m are the lengths of the source sentence and the target sentence respectively. j is the position index of the target word. a j is the position of the source word aligned to the target word. j thφi is the number of target words that is e i aligned to. p 0 , p 1 are the fertility probabilities for , and e 0p0+ p1 =1 . t(fj |eaj) is the word </context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical Machine Translation Final Report. Johns Hopkins University Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sugato Basu</author>
<author>Mikhail Bilenko</author>
<author>Raymond J Mooney</author>
</authors>
<title>Probabilistic Framework for SemiSupervised Clustering.</title>
<date>2004</date>
<booktitle>In Proc. of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004),</booktitle>
<pages>59--68</pages>
<contexts>
<context position="2964" citStr="Basu et al., 2004" startWordPosition="451" endWordPosition="454">05) developed an unsupervised AdaBoost algorithm by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Basu et al., 2004), named entity classification (Collins and Singer, 1999), and parsing (Sarkar, 2001). In this paper, we propose a semi-supervised boosting method to improve statistical word alignment with both limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data. Therefore, it should address the following three problems. The first is to build a word alignment model with both labeled and unlabeled data. In this paper, with the labeled data, we build a supervised mode</context>
</contexts>
<marker>Basu, Bilenko, Mooney, 2004</marker>
<rawString>Sugato Basu, Mikhail Bilenko, and Raymond J. Mooney. 2004. Probabilistic Framework for SemiSupervised Clustering. In Proc. of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004), pages 59-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combing Labeled and Unlabeled Data with Co-training.</title>
<date>1998</date>
<booktitle>In Proc. of the 11th Conference on Computational Learning Theory (COLT-1998),</booktitle>
<pages>1--10</pages>
<contexts>
<context position="2915" citStr="Blum and Mitchell, 1998" startWordPosition="444" endWordPosition="447"> reference set for the unlabeled data. Wu and Wang (2005) developed an unsupervised AdaBoost algorithm by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Basu et al., 2004), named entity classification (Collins and Singer, 1999), and parsing (Sarkar, 2001). In this paper, we propose a semi-supervised boosting method to improve statistical word alignment with both limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data. Therefore, it should address the following three problems. The first is to build a word alignment model with both labeled and unlabeled data. In this paper, </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combing Labeled and Unlabeled Data with Co-training. In Proc. of the 11th Conference on Computational Learning Theory (COLT-1998), pages1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="1412" citStr="Brown et al., 1993" startWordPosition="207" endWordPosition="210">e of each word aligner using only the labeled data. Based on this semisupervised boosting algorithm, we investigate two boosting methods for word alignment. In addition, we improve the word alignment results by combining the results of the two semi-supervised boosting methods. Experimental results on word alignment indicate that semisupervised boosting achieves relative error reductions of 28.29% and 19.52% as compared with supervised boosting and unsupervised boosting, respectively. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). In recent years, many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Wu et al., 2005; Zhang and Gildea, 2005). These methods unsupervisedly train the alignment models with unlabeled data. A question about word alignment is whether we can further improve the performances of the word aligners with available data and available alignment models. One possible solution is to use the boosting method (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 2000). The underlying idea of boosting is to combine simple &amp;quot;</context>
<context position="3855" citStr="Brown et al. (1993)" startWordPosition="582" endWordPosition="585">h modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data. Therefore, it should address the following three problems. The first is to build a word alignment model with both labeled and unlabeled data. In this paper, with the labeled data, we build a supervised model by directly estimating the parameters in 913 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 913–920, Sydney, July 2006. c�2006 Association for Computational Linguistics the model instead of using the Expectation Maximization (EM) algorithm in Brown et al. (1993). With the unlabeled data, we build an unsupervised model by estimating the parameters with the EM algorithm. Based on these two word alignment models, an interpolated model is built through linear interpolation. This interpolated model is used as a learner in the semi-supervised AdaBoost algorithm. The second is to build a reference set for the unlabeled data. It is automatically built with a modified &amp;quot;refined&amp;quot; combination method as described in Och and Ney (2000). The third is to calculate the error rate on each round. Although we build a reference set for the unlabeled data, it still contai</context>
<context position="5964" citStr="Brown et al., 1993" startWordPosition="923" endWordPosition="926">escribed in Wu and Wang (2005), achieving a relative error rate reduction of 19.52%. And it also achieves a reduction of 28.29% as compared with the supervised boosting method that only uses the labeled data. The remainder of this paper is organized as follows. Section 2 briefly introduces the statistical word alignment model. Section 3 describes parameter estimation method using the labeled data. Section 4 presents our semi-supervised boosting method. Section 5 reports the experimental results. Finally, we conclude in section 6. 2 Statistical Word Alignment Model According to the IBM models (Brown et al., 1993), the statistical word alignment model can be generally represented as in equation (1). Pr(a, f |e) = ∑Pr(a&apos;, f |e) (1) Pr(a f |e) a&apos; Where and f represent the source sentence e and the target sentence, respectively. In this paper, we use a simplified IBM model 4 (Al-Onaizan et al., 1999), which is shown in equation (2). This simplified version does not take into account word classes as described in Brown et al. (1993). φ 0 ⎞ m − 2 φ φ 0 0 ⎟ p p 0 1 ⎠ m ei ) t(f j |eaj ) i =1 j= 1 m ([ ( )] ( j⋅ 1 =1,aj≠0 ([ ( )] ( ( )))) j j⋅ &gt; j=1,aj ≠0 l, m are the lengths of the source sentence and the tar</context>
<context position="7570" citStr="Brown et al., 1993" startWordPosition="1264" endWordPosition="1267">ility for the 1 ( a j) head word of cept1 i. d&gt;1 (j − p(j)) is the distortion probability for the non-head words of cept i. h(i) =min{k : i = a is the head of cept i. k } k p j = k &lt; j k a = a ( ) max { : j k . is the first word before with non-zero fertility. ρi ei ci is the center of cept i. 3 Parameter Estimation with Labeled Data With the labeled data, instead of using EM algorithm, we directly estimate the three main parameters in model 4: translation probability, fertility probability, and distortion probability. 1 A cept is defined as the set of target words connected to a source word (Brown et al., 1993). Pr( a,f |e) = ⎛ ⎜ m− ⎝ φ0 l ∏ ( j ∏ n | φi ( j ⋅∏ ⋅ (2) + m ∏ h a 1 } 914 3.1 Translation Probability Where δ(x, y) =1 if x = y . Otherwise, δ(x, y) = 0. The translation probability is estimated from the 4 Boosting with Labeled Data and labeled data as described in (3). Where count(ei, f j) is the occurring frequency of ei aligned to in the labeled data. fj 3.2 Fertility Probability The fertility probability n(φi |ei) describes the distribution of the numbers of words that is ei aligned to. It is estimated as described in (4). Unlabeled Data supervised AdaBoost algorithm for word alignment, </context>
<context position="10177" citStr="Brown et al. (1993)" startWordPosition="1790" endWordPosition="1793"> &apos; &apos; , 1 ρi&apos; ∑ δ (Δj &gt;1 , j − p(j)) Δ &gt; j j p j &apos; &apos; , ( &apos; ) 1 and unlabeled data. Compared with the supervised Adaboost algorithm, this semi-supervised AdaBoost algorithm mainly has five differences. The first is the word alignment model, which is taken as a learner in the boosting algorithm. The word alignment model is built using both the labeled data and the unlabeled data. With the labeled data, we train a supervised model by directly estimating the parameters in the IBM model as described in section 3. With the unlabeled data, we train an unsupervised model using the same EM algorithm in Brown et al. (1993). Then we build an interpolation model by linearly interpolating these two word alignment models, which is shown in (8). This interpolated model is used as the model Ml described in figure 1. Where PrS (a, f |e) and PrU (a, f |e) are the trained supervised model and unsupervised model, respectively. λ is an interpolation weight. We train the weight in equation (8) in the same way as described in Wu et al. (2005). Pseudo Reference Set for Unlabeled Data The second is the reference set for the unlabeled data. For the unlabeled data, we automatically build a pseudo reference set. In order to buil</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2): 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A Probability Model to Improve Word Alignment.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003),</booktitle>
<pages>88--95</pages>
<contexts>
<context position="1543" citStr="Cherry and Lin, 2003" startWordPosition="229" endWordPosition="232">ethods for word alignment. In addition, we improve the word alignment results by combining the results of the two semi-supervised boosting methods. Experimental results on word alignment indicate that semisupervised boosting achieves relative error reductions of 28.29% and 19.52% as compared with supervised boosting and unsupervised boosting, respectively. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). In recent years, many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Wu et al., 2005; Zhang and Gildea, 2005). These methods unsupervisedly train the alignment models with unlabeled data. A question about word alignment is whether we can further improve the performances of the word aligners with available data and available alignment models. One possible solution is to use the boosting method (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 2000). The underlying idea of boosting is to combine simple &amp;quot;rules&amp;quot; to form an ensemble such that the performance of the single ensemble is improved. The AdaBoost (Adaptive Boosting) algorithm</context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>Colin Cherry and Dekang Lin. 2003. A Probability Model to Improve Word Alignment. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003), pages 88-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification.</title>
<date>1999</date>
<booktitle>In Proc. of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and</booktitle>
<contexts>
<context position="3020" citStr="Collins and Singer, 1999" startWordPosition="458" endWordPosition="461">by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Basu et al., 2004), named entity classification (Collins and Singer, 1999), and parsing (Sarkar, 2001). In this paper, we propose a semi-supervised boosting method to improve statistical word alignment with both limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data. Therefore, it should address the following three problems. The first is to build a word alignment model with both labeled and unlabeled data. In this paper, with the labeled data, we build a supervised model by directly estimating the parameters in 913 Proceedin</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised Models for Named Entity Classification. In Proc. of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and</rawString>
</citation>
<citation valid="true">
<title>Very Large Corpora</title>
<date>1999</date>
<pages>100--110</pages>
<marker>1999</marker>
<rawString>Very Large Corpora (EMNLP/VLC-1999), pages 100-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas G Dietterich</author>
</authors>
<title>Ensemble Methods in Machine Learning.</title>
<date>2000</date>
<booktitle>In Proc. of the First International Workshop on Multiple Classifier Systems (MCS-2000),</booktitle>
<pages>1--15</pages>
<contexts>
<context position="1956" citStr="Dietterich, 2000" startWordPosition="295" endWordPosition="296">termediate result of statistical machine translation (Brown et al., 1993). In recent years, many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Wu et al., 2005; Zhang and Gildea, 2005). These methods unsupervisedly train the alignment models with unlabeled data. A question about word alignment is whether we can further improve the performances of the word aligners with available data and available alignment models. One possible solution is to use the boosting method (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 2000). The underlying idea of boosting is to combine simple &amp;quot;rules&amp;quot; to form an ensemble such that the performance of the single ensemble is improved. The AdaBoost (Adaptive Boosting) algorithm by Freund and Schapire (1996) was developed for supervised learning. When it is applied to word alignment, it should solve the problem of building a reference set for the unlabeled data. Wu and Wang (2005) developed an unsupervised AdaBoost algorithm by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available with</context>
</contexts>
<marker>Dietterich, 2000</marker>
<rawString>Thomas G. Dietterich. 2000. Ensemble Methods in Machine Learning. In Proc. of the First International Workshop on Multiple Classifier Systems (MCS-2000), pages 1-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Experiments with a New Boosting Algorithm.</title>
<date>1996</date>
<booktitle>In Proc. of the 13th International Conference on Machine Learning (ICML-1996),</booktitle>
<pages>148--156</pages>
<contexts>
<context position="1899" citStr="Freund and Schapire, 1996" startWordPosition="284" endWordPosition="287">ctively. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). In recent years, many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Wu et al., 2005; Zhang and Gildea, 2005). These methods unsupervisedly train the alignment models with unlabeled data. A question about word alignment is whether we can further improve the performances of the word aligners with available data and available alignment models. One possible solution is to use the boosting method (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 2000). The underlying idea of boosting is to combine simple &amp;quot;rules&amp;quot; to form an ensemble such that the performance of the single ensemble is improved. The AdaBoost (Adaptive Boosting) algorithm by Freund and Schapire (1996) was developed for supervised learning. When it is applied to word alignment, it should solve the problem of building a reference set for the unlabeled data. Wu and Wang (2005) developed an unsupervised AdaBoost algorithm by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1996. Experiments with a New Boosting Algorithm. In Proc. of the 13th International Conference on Machine Learning (ICML-1996), pages 148-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics (ACL-2000),</booktitle>
<pages>440--447</pages>
<contexts>
<context position="4324" citStr="Och and Ney (2000)" startWordPosition="659" endWordPosition="662"> 2006. c�2006 Association for Computational Linguistics the model instead of using the Expectation Maximization (EM) algorithm in Brown et al. (1993). With the unlabeled data, we build an unsupervised model by estimating the parameters with the EM algorithm. Based on these two word alignment models, an interpolated model is built through linear interpolation. This interpolated model is used as a learner in the semi-supervised AdaBoost algorithm. The second is to build a reference set for the unlabeled data. It is automatically built with a modified &amp;quot;refined&amp;quot; combination method as described in Och and Ney (2000). The third is to calculate the error rate on each round. Although we build a reference set for the unlabeled data, it still contains alignment errors. Thus, we use the reference set of the labeled data instead of that of the entire training data to calculate the error rate on each round. With the interpolated model as a learner in the semi-supervised AdaBoost algorithm, we investigate two boosting methods in this paper to improve statistical word alignment. The first method uses the unlabeled data only in the interpolated model. During training, it only changes the distribution of the labeled</context>
<context position="12826" citStr="Och and Ney (2000)" startWordPosition="2324" endWordPosition="2327"> For all i, compute new weights: wl+1(i)=wl(i)⋅ (k+(n−k)⋅ βl)/ n where, n represents n alignment links in the ith sentence pair. k represents the number of error links as compared with RT . α(i) set : RL Output: The final word alignment result for a source word e : L 1 h e = = ) ( , ) ( ( ), ) ⋅ WT e f h e f ⋅ δ F ( ) argmax ( , ) argmax (log RS e f l l ∑= β f f l 1 l Where δ(x, y) =1 if x = y . Otherwise, δ(x, y) = 0. WTl (e, f) is the weight of the alignment link (e, f) produced by the model , which is calculated as described in equation (10). Ml target and target to source) as described in Och and Ney (2000). Thus, we get two sets of alignment results and on the unlabeled data. A1 A2 Based on these two sets, we use a modified &amp;quot;refined&amp;quot; method (Och and Ney, 2000) to construct a pseudo reference set . RU (1) The intersection I = A1 ∩ A2 is added to the reference set . RU (2) We add (e, f)∈ A1 ∪ A2 RU to if a) is satisfied or both b) and c) are satisfied. a) Neither nor has an alignment in e f and p(f |e) is greater than a threshold Where count(e, f) is the occurring frequency of the alignment link in ( e , f) the bi-directional word alignment results. b) has a horizontal or a vertical ( e, f) neigh</context>
<context position="20661" citStr="Och and Ney, 2000" startWordPosition="3805" endWordPosition="3808">segmented into words. The statistics for the data is shown in Table 1. The labeled data is manually word aligned, including 156,421 alignment links. Data # Sentence # English # Chinese Pairs Words Words LD 31,069 255,504 302,470 UD 329,350 4,682,103 4,480,034 Table 1. Statistics for Training Data We use 1,000 sentence pairs as testing set, which are not included in LD or UD. The testing set is also manually word aligned, including 8,634 alignment links in the testing set3. 5.2 Evaluation Metrics We use the same evaluation metrics as described in Wu et al. (2005), which is similar to those in (Och and Ney, 2000). The difference lies in that Wu et al. (2005) take all alignment links as sure links. If we use to represent the set of alignment SG links identified by the proposed method and SC to denote the reference alignment set, the meth5.1, we get the word an d Ney (2000). Thus, the results in table 2 are those of the &amp;quot;refined&amp;quot; combination. For EM training, we use the GIZA++ toolkit4. alignment results shown in table 2. For all of the methods in this table, we perform bi-directional (source to target and target to source) word alignment, and obtain two alignment results on the testing set. Based on th</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics (ACL-2000), pages 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="1521" citStr="Och and Ney, 2003" startWordPosition="225" endWordPosition="228">gate two boosting methods for word alignment. In addition, we improve the word alignment results by combining the results of the two semi-supervised boosting methods. Experimental results on word alignment indicate that semisupervised boosting achieves relative error reductions of 28.29% and 19.52% as compared with supervised boosting and unsupervised boosting, respectively. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). In recent years, many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Wu et al., 2005; Zhang and Gildea, 2005). These methods unsupervisedly train the alignment models with unlabeled data. A question about word alignment is whether we can further improve the performances of the word aligners with available data and available alignment models. One possible solution is to use the boosting method (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 2000). The underlying idea of boosting is to combine simple &amp;quot;rules&amp;quot; to form an ensemble such that the performance of the single ensemble is improved. The AdaBoost (Adapti</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thanh Phong Pham</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
</authors>
<title>Word Sense Disambiguation with SemiSupervised Learning.</title>
<date>2005</date>
<booktitle>In Proc. of the 20th National Conference on Artificial Intelligence (AAAI</booktitle>
<pages>1093--1098</pages>
<contexts>
<context position="2874" citStr="Pham et al., 2005" startWordPosition="439" endWordPosition="442">ould solve the problem of building a reference set for the unlabeled data. Wu and Wang (2005) developed an unsupervised AdaBoost algorithm by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Basu et al., 2004), named entity classification (Collins and Singer, 1999), and parsing (Sarkar, 2001). In this paper, we propose a semi-supervised boosting method to improve statistical word alignment with both limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data. Therefore, it should address the following three problems. The first is to build a word alignment model with both la</context>
</contexts>
<marker>Pham, Ng, Lee, 2005</marker>
<rawString>Thanh Phong Pham, Hwee Tou Ng, and Wee Sun Lee 2005. Word Sense Disambiguation with SemiSupervised Learning. In Proc. of the 20th National Conference on Artificial Intelligence (AAAI 2005), pages 1093-1098.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Applying Co-Training Methods to Statistical Parsing.</title>
<date>2001</date>
<booktitle>In Proc. of the 2nd Meeting of the North American Association for Computational Linguistics( NAACL-2001),</booktitle>
<pages>175--182</pages>
<contexts>
<context position="3048" citStr="Sarkar, 2001" startWordPosition="464" endWordPosition="465">ence set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Basu et al., 2004), named entity classification (Collins and Singer, 1999), and parsing (Sarkar, 2001). In this paper, we propose a semi-supervised boosting method to improve statistical word alignment with both limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data. Therefore, it should address the following three problems. The first is to build a word alignment model with both labeled and unlabeled data. In this paper, with the labeled data, we build a supervised model by directly estimating the parameters in 913 Proceedings of the COLING/ACL 2006 Ma</context>
</contexts>
<marker>Sarkar, 2001</marker>
<rawString>Anoop Sarkar. 2001. Applying Co-Training Methods to Statistical Parsing. In Proc. of the 2nd Meeting of the North American Association for Computational Linguistics( NAACL-2001), pages 175-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachims Thorsten</author>
</authors>
<title>Transductive Inference for Text Classification Using Support Vector Machines.</title>
<date>1999</date>
<booktitle>In Proc. of the 16th International Conference on Machine Learning (ICML-1999),</booktitle>
<pages>200--209</pages>
<contexts>
<context position="2932" citStr="Thorsten, 1999" startWordPosition="448" endWordPosition="449">labeled data. Wu and Wang (2005) developed an unsupervised AdaBoost algorithm by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Basu et al., 2004), named entity classification (Collins and Singer, 1999), and parsing (Sarkar, 2001). In this paper, we propose a semi-supervised boosting method to improve statistical word alignment with both limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data. Therefore, it should address the following three problems. The first is to build a word alignment model with both labeled and unlabeled data. In this paper, with the labeled </context>
</contexts>
<marker>Thorsten, 1999</marker>
<rawString>Joachims Thorsten. 1999. Transductive Inference for Text Classification Using Support Vector Machines. In Proc. of the 16th International Conference on Machine Learning (ICML-1999), pages 200-209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<pages>377--403</pages>
<contexts>
<context position="1502" citStr="Wu, 1997" startWordPosition="223" endWordPosition="224">we investigate two boosting methods for word alignment. In addition, we improve the word alignment results by combining the results of the two semi-supervised boosting methods. Experimental results on word alignment indicate that semisupervised boosting achieves relative error reductions of 28.29% and 19.52% as compared with supervised boosting and unsupervised boosting, respectively. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). In recent years, many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Wu et al., 2005; Zhang and Gildea, 2005). These methods unsupervisedly train the alignment models with unlabeled data. A question about word alignment is whether we can further improve the performances of the word aligners with available data and available alignment models. One possible solution is to use the boosting method (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 2000). The underlying idea of boosting is to combine simple &amp;quot;rules&amp;quot; to form an ensemble such that the performance of the single ensemble is improved. T</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3): 377-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
</authors>
<title>Boosting Statistical Word Alignment.</title>
<date>2005</date>
<booktitle>In Proc. of the 10th Machine Translation Summit,</booktitle>
<pages>313--320</pages>
<contexts>
<context position="2349" citStr="Wu and Wang (2005)" startWordPosition="359" endWordPosition="362">he performances of the word aligners with available data and available alignment models. One possible solution is to use the boosting method (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 2000). The underlying idea of boosting is to combine simple &amp;quot;rules&amp;quot; to form an ensemble such that the performance of the single ensemble is improved. The AdaBoost (Adaptive Boosting) algorithm by Freund and Schapire (1996) was developed for supervised learning. When it is applied to word alignment, it should solve the problem of building a reference set for the unlabeled data. Wu and Wang (2005) developed an unsupervised AdaBoost algorithm by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Bas</context>
<context position="5375" citStr="Wu and Wang (2005)" startWordPosition="830" endWordPosition="833">e statistical word alignment. The first method uses the unlabeled data only in the interpolated model. During training, it only changes the distribution of the labeled data. The second method changes the distribution of both the labeled data and the unlabeled data during training. Experimental results show that both of these two methods improve the performance of statistical word alignment. In addition, we combine the final results of the above two semi-supervised boosting methods. Experimental results indicate that this combination outperforms the unsupervised boosting method as described in Wu and Wang (2005), achieving a relative error rate reduction of 19.52%. And it also achieves a reduction of 28.29% as compared with the supervised boosting method that only uses the labeled data. The remainder of this paper is organized as follows. Section 2 briefly introduces the statistical word alignment model. Section 3 describes parameter estimation method using the labeled data. Section 4 presents our semi-supervised boosting method. Section 5 reports the experimental results. Finally, we conclude in section 6. 2 Statistical Word Alignment Model According to the IBM models (Brown et al., 1993), the stati</context>
<context position="13847" citStr="Wu and Wang (2005)" startWordPosition="2533" endWordPosition="2536">is greater than a threshold Where count(e, f) is the occurring frequency of the alignment link in ( e , f) the bi-directional word alignment results. b) has a horizontal or a vertical ( e, f) neighbor that is already in . RU c) The set RU ∪ (e, f) does not contain alignments with both horizontal and vertical neighbors. Error of Word Aligner The third is the calculation of the error of the individual word aligner on each round. For word alignment, a sentence pair is taken as a sample. Thus, we calculate the error rate of each sentence pair as described in (9), which is the same as described in Wu and Wang (2005). _ 2 |SW ∩ SR α(i) 1 |SW|+ |SR |(9) Where represents the set of alignment SW links of a sentence pair i identified by the individual interpolated model on each round. is SR the reference alignment set for the sentence pair. With the error rate of each sentence pair, we calculate the error of the word aligner on each round. Although we build a pseudo reference set RU for the unlabeled data, it contains alignment errors. Thus, the weighted sum of the error rates of sentence pairs in the labeled data instead of that in the entire training data is used as the error of the word aligner. count e f </context>
<context position="15274" citStr="Wu and Wang (2005)" startWordPosition="2805" endWordPosition="2808">e usually several word alignment links. Some are correct, and others may be incorrect. Thus, we update the weights according to the number of correct and incorrect alignment links as compared with the reference set, which is shown in step (9) in figure 1. Weights for Word Alignment Links The fifth is the weights used when we construct the final ensemble. Besides the weight log(1 / βl) , which is the confidence measure of the word aligner, we also use the weight l th to measure the confidence of each alignment link produced by the model . The Ml weight WTl(e, f) is calculated as shown in (10). Wu and Wang (2005) proved that adding this weight improved the word alignment results. Where count(e, f) is the occurring frequency of the alignment link in the word align( e , f) ment results of the training data produced by the model . Ml 4.2 Method 1 This method only uses the labeled data as training data. According to the algorithm in figure 1, we obtain ST = SL and RT = RL. Thus, we only change the distribution of the labeled data. However, we build an unsupervised model using the unlabeled data. On each round, we keep this unsupervised model unchanged, and we rebuild the supervised model by estimating the</context>
<context position="23596" citStr="Wu and Wang (2005)" startWordPosition="4259" endWordPosition="4262">ated 0.7555 0.7084 0.7312 0.2688 Method 1 0.7986 0.7197 0.7571 0.2429 Method 2 0.8060 0.7388 0.7709 0.2291 Combination 0.8175 0.7858 0.8013 0.1987 Table 2. Word Alignment Results boosting. For example, &amp;quot;Labeled+Direct+Boost&amp;quot; achieves an error rate reduction of 9.92% as compared with &amp;quot;Labeled+Direct&amp;quot;. Results of Unsupervised Methods With the unlabeled data, we use the EM algorithm to estimate the parameters in the model. The method &amp;quot;Unlabeled+EM&amp;quot; represents an unsupervised method without boosting. And the method &amp;quot;Unlabeled+EM+Boost&amp;quot; uses the same unsupervised Adaboost algorithm as described in Wu and Wang (2005). The boosting method &amp;quot;Unlabeled+EM+Boost&amp;quot; achieves a relative error rate reduction of 16.25% as compared with &amp;quot;Unlabeled+EM&amp;quot;. In addition, the unsupervised boosting method &amp;quot;Unlabeled+EM+Boost&amp;quot; performs better than the supervised boosting method &amp;quot;Labeled+Direct+ Boost&amp;quot;, achieving an error rate reduction of 10.90%. This is because the size of labeled data is too small to subject to data sparseness problem. Results of Semi-Supervised Methods By using both the labeled and the unlabeled data, we interpolate the models trained by &amp;quot;Labeled+Direct&amp;quot; and &amp;quot;Unlabeled+EM&amp;quot; to get an interpolated model. Her</context>
</contexts>
<marker>Wu, Wang, 2005</marker>
<rawString>Hua Wu and Haifeng Wang. 2005. Boosting Statistical Word Alignment. In Proc. of the 10th Machine Translation Summit, pages 313-320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Zhanyi Liu</author>
</authors>
<title>Alignment Model Adaptation for Domain-Specific Word Alignment.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-2005),</booktitle>
<pages>467--474</pages>
<contexts>
<context position="1560" citStr="Wu et al., 2005" startWordPosition="233" endWordPosition="236">ent. In addition, we improve the word alignment results by combining the results of the two semi-supervised boosting methods. Experimental results on word alignment indicate that semisupervised boosting achieves relative error reductions of 28.29% and 19.52% as compared with supervised boosting and unsupervised boosting, respectively. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). In recent years, many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Wu et al., 2005; Zhang and Gildea, 2005). These methods unsupervisedly train the alignment models with unlabeled data. A question about word alignment is whether we can further improve the performances of the word aligners with available data and available alignment models. One possible solution is to use the boosting method (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 2000). The underlying idea of boosting is to combine simple &amp;quot;rules&amp;quot; to form an ensemble such that the performance of the single ensemble is improved. The AdaBoost (Adaptive Boosting) algorithm by Freund and Sc</context>
<context position="10592" citStr="Wu et al. (2005)" startWordPosition="1864" endWordPosition="1867">rvised model by directly estimating the parameters in the IBM model as described in section 3. With the unlabeled data, we train an unsupervised model using the same EM algorithm in Brown et al. (1993). Then we build an interpolation model by linearly interpolating these two word alignment models, which is shown in (8). This interpolated model is used as the model Ml described in figure 1. Where PrS (a, f |e) and PrU (a, f |e) are the trained supervised model and unsupervised model, respectively. λ is an interpolation weight. We train the weight in equation (8) in the same way as described in Wu et al. (2005). Pseudo Reference Set for Unlabeled Data The second is the reference set for the unlabeled data. For the unlabeled data, we automatically build a pseudo reference set. In order to build a reliable pseudo reference set, we perform bi-directional word alignment on the training data using the interpolated model trained on the first round. Bi-directional word alignment includes alignment in two directions (source to 1(Δj&gt;1) = ∑ ∑ δ(Δj&gt;1, j − p(j )) d&gt; j p ( j) , p= 0 #Aligned # Aligned Null −# (5) ( δ ∑ Where count(φi , ei) describes the occurring frequency of word ei aligned to φi target words i</context>
<context position="20611" citStr="Wu et al. (2005)" startWordPosition="3795" endWordPosition="3798">Chinese sentences in the data are automatically segmented into words. The statistics for the data is shown in Table 1. The labeled data is manually word aligned, including 156,421 alignment links. Data # Sentence # English # Chinese Pairs Words Words LD 31,069 255,504 302,470 UD 329,350 4,682,103 4,480,034 Table 1. Statistics for Training Data We use 1,000 sentence pairs as testing set, which are not included in LD or UD. The testing set is also manually word aligned, including 8,634 alignment links in the testing set3. 5.2 Evaluation Metrics We use the same evaluation metrics as described in Wu et al. (2005), which is similar to those in (Och and Ney, 2000). The difference lies in that Wu et al. (2005) take all alignment links as sure links. If we use to represent the set of alignment SG links identified by the proposed method and SC to denote the reference alignment set, the meth5.1, we get the word an d Ney (2000). Thus, the results in table 2 are those of the &amp;quot;refined&amp;quot; combination. For EM training, we use the GIZA++ toolkit4. alignment results shown in table 2. For all of the methods in this table, we perform bi-directional (source to target and target to source) word alignment, and obtain two</context>
</contexts>
<marker>Wu, Wang, Liu, 2005</marker>
<rawString>Hua Wu, Haifeng Wang, and Zhanyi Liu. 2005. Alignment Model Adaptation for Domain-Specific Word Alignment. In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-2005), pages 467-474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>In Proc. of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL-1995),</booktitle>
<pages>189--196</pages>
<contexts>
<context position="2854" citStr="Yarowsky, 1995" startWordPosition="437" endWordPosition="438">alignment, it should solve the problem of building a reference set for the unlabeled data. Wu and Wang (2005) developed an unsupervised AdaBoost algorithm by automatically building a pseudo reference set for the unlabeled data to improve alignment results. In fact, large amounts of unlabeled data are available without difficulty, while labeled data is costly to obtain. However, labeled data is valuable to improve performance of learners. Consequently, semi-supervised learning, which combines both labeled and unlabeled data, has been applied to some NLP tasks such as word sense disambiguation (Yarowsky, 1995; Pham et al., 2005), classification (Blum and Mitchell, 1998; Thorsten, 1999), clustering (Basu et al., 2004), named entity classification (Collins and Singer, 1999), and parsing (Sarkar, 2001). In this paper, we propose a semi-supervised boosting method to improve statistical word alignment with both limited labeled data and large amounts of unlabeled data. The proposed approach modifies the supervised AdaBoost algorithm to a semi-supervised learning algorithm by incorporating the unlabeled data. Therefore, it should address the following three problems. The first is to build a word alignmen</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proc. of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL-1995), pages 189-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Stochastic Lexicalized Inversion Transduction Grammar for Alignment.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-2005),</booktitle>
<pages>475--482</pages>
<contexts>
<context position="1585" citStr="Zhang and Gildea, 2005" startWordPosition="237" endWordPosition="240"> we improve the word alignment results by combining the results of the two semi-supervised boosting methods. Experimental results on word alignment indicate that semisupervised boosting achieves relative error reductions of 28.29% and 19.52% as compared with supervised boosting and unsupervised boosting, respectively. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). In recent years, many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Wu et al., 2005; Zhang and Gildea, 2005). These methods unsupervisedly train the alignment models with unlabeled data. A question about word alignment is whether we can further improve the performances of the word aligners with available data and available alignment models. One possible solution is to use the boosting method (Freund and Schapire, 1996), which is one of the ensemble methods (Dietterich, 2000). The underlying idea of boosting is to combine simple &amp;quot;rules&amp;quot; to form an ensemble such that the performance of the single ensemble is improved. The AdaBoost (Adaptive Boosting) algorithm by Freund and Schapire (1996) was develop</context>
</contexts>
<marker>Zhang, Gildea, 2005</marker>
<rawString>Hao Zhang and Daniel Gildea. 2005. Stochastic Lexicalized Inversion Transduction Grammar for Alignment. In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-2005), pages 475-482.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>