<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.916267">
Multi-Metric Optimization Using Ensemble Tuning
</title>
<author confidence="0.873338">
Baskaran Sankaran, Anoop Sarkar
</author>
<affiliation confidence="0.654729">
Simon Fraser University
Burnaby BC. CANADA
</affiliation>
<email confidence="0.988462">
{baskaran,anoop}@cs.sfu.ca
</email>
<sectionHeader confidence="0.994561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999190458333333">
This paper examines tuning for statistical ma-
chine translation (SMT) with respect to mul-
tiple evaluation metrics. We propose several
novel methods for tuning towards multiple ob-
jectives, including some based on ensemble
decoding methods. Pareto-optimality is a nat-
ural way to think about multi-metric optimiza-
tion (MMO) and our methods can effectively
combine several Pareto-optimal solutions, ob-
viating the need to choose one. Our best
performing ensemble tuning method is a new
algorithm for multi-metric optimization that
searches for Pareto-optimal ensemble models.
We study the effectiveness of our methods
through experiments on multiple as well as
single reference(s) datasets. Our experiments
show simultaneous gains across several met-
rics (BLEU, RIBES), without any significant
reduction in other metrics. This contrasts the
traditional tuning where gains are usually lim-
ited to a single metric. Our human evaluation
results confirm that in order to produce better
MT output, optimizing multiple metrics is bet-
ter than optimizing only one.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99959575">
Tuning algorithms are used to find the weights for a
statistical machine translation (MT) model by min-
imizing error with respect to a single MT evalua-
tion metric. The tuning process improves the per-
formance of an SMT system as measured by this
metric; with BLEU (Papineni et al., 2002) being
the most popular choice. Minimum error-rate train-
ing (MERT) (Och, 2003) was the first approach in
MT to directly optimize an evaluation metric. Sev-
eral alternatives now exist: MIRA (Watanabe et al.,
2007; Chiang et al., 2008), PRO (Hopkins and May,
2011), linear regression (Bazrafshan et al., 2012)
and ORO (Watanabe, 2012) among others.
However these approaches optimize towards the
best score as reported by a single evaluation met-
ric. MT system developers typically use BLEU and
</bodyText>
<author confidence="0.545239">
Kevin Duh
</author>
<affiliation confidence="0.5430725">
Nara Institute of Science &amp; Technology
Ikoma, Nara. JAPAN
</affiliation>
<email confidence="0.804787">
kevinduh@is.naist.jp
</email>
<bodyText confidence="0.999865536585366">
ignore all the other metrics. This is done despite
the fact that other metrics model wide-ranging as-
pects of translation: from measuring the translation
edit rate (TER) in matching a translation output to a
human reference (Snover et al., 2006), to capturing
lexical choices in translation as in METEOR (Lavie
and Denkowski, 2009) to modelling semantic simi-
larity through textual entailment (Pad´o et al., 2009)
to RIBES, an evaluation metric that pays attention
to long-distance reordering (Isozaki et al., 2010).
While some of these metrics such as TER, ME-
TEOR are gaining prominence, BLEU enjoys the
status of being the de facto standard tuning metric
as it is often claimed and sometimes observed that
optimizing with BLEU produces better translations
than other metrics (Callison-Burch et al., 2011).
The gains obtained by the MT system tuned on
a particular metric do not improve performance as
measured under other metrics (Cer et al., 2010), sug-
gesting that over-fitting to a specific metric might
happen without improvements in translation quality.
In this paper we propose a new tuning framework
for jointly optimizing multiple evaluation metrics.
Pareto-optimality is a natural way to think about
multi-metric optimization and multi-metric opti-
mization (MMO) was recently explored using the
notion of Pareto optimality in the Pareto-based
Multi-objective Optimization (PMO) approach (Duh
et al., 2012). PMO provides several equivalent solu-
tions (parameter weights) having different trade-offs
between the different MT metrics. In (Duh et al.,
2012) the choice of which option to use rests with
the MT system developer and in that sense their ap-
proach is an a posteriori method to specify the pref-
erence (Marler and Arora, 2004).
In contrast to this, our tuning framework pro-
vides a principled way of using the Pareto optimal
options using ensemble decoding (Razmara et al.,
2012). We also introduce a novel method of ensem-
ble tuning for jointly tuning multiple MT evaluation
metrics and further combine this with the PMO ap-
</bodyText>
<page confidence="0.966445">
947
</page>
<note confidence="0.4743025">
Proceedings of NAACL-HLT 2013, pages 947–957,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999004333333333">
proach (Duh et al., 2012). We also introduce three
other approaches for multi-metric tuning and com-
pare their performance to the ensemble tuning. Our
experiments yield the highest metric scores across
many different metrics (that are being optimized),
something that has not been possible until now.
Our ensemble tuning method over multiple met-
rics produced superior translations than single met-
ric tuning as measured by a post-editing task.
HTER (Snover et al., 2006) scores in our human
evaluation confirm that multi-metric optimization
can lead to better MT output.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999905112903226">
In grammar induction and parsing (Spitkovsky et al.,
2011; Hall et al., 2011; Auli and Lopez, 2011) have
proposed multi-objective methods based on round-
robin iteration of single objective optimizations.
Research in SMT parameter tuning has seen a
surge of interest recently, including online/batch
learning (Watanabe, 2012; Cherry and Foster, 2012),
large-scale training (Simianer et al., 2012; He
and Deng, 2012), and new discriminative objec-
tives (Gimpel and Smith, 2012; Zheng et al., 2012;
Bazrafshan et al., 2012). However, few works
have investigated the multi-metric tuning problem in
depth. Linear combination of BLEU and TER is re-
ported in (Zaidan, 2009; Dyer et al., 2009; Servan
and Schwenk, 2011); an alternative is to optimize on
BLEU with MERT while enforcing that TER does
not degrade per iteration (He and Way, 2009). Stud-
ies on metric tunability (Liu et al., 2011; Callison-
Burch et al., 2011; Chen et al., 2012) have found
that the metric used for evaluation may not be the
best metric used for tuning. For instance, (Mauser et
al., 2008; Cer et al., 2010) report that tuning on lin-
ear combinations of BLEU-TER is more robust than
a single metric like WER.
The approach in (Devlin and Matsoukas, 2012)
modifies the optimization function to include traits
such as output length so that the hypotheses pro-
duced by the decoder have maximal score according
to one metric (BLEU) but are subject to an output
length constraint, e.g. that the output is 5% shorter.
This is done by rescoring an N-best list (forest) for
the metric combined with each trait condition and
then the different trait hypothesis are combined us-
ing a system combination step. The traits are in-
dependent of the reference (while tuning). In con-
trast, our method is able to combine multiple metrics
(each of which compares to the reference) during the
tuning step and we do not depend on N-best list (or
forest) rescoring or system combination.
Duh et. al. (2012) proposed a Pareto-based ap-
proach to SMT multi-metric tuning, where the lin-
ear combination weights do not need to be known in
advance. This is advantageous because the optimal
weighting may not be known in advance. However,
the notion of Pareto optimality implies that multiple
”best” solutions may exist, so the MT system devel-
oper may be forced to make a choice after tuning.
These approaches require the MT system devel-
oper to make a choice either before tuning (e.g. in
terms of linear combination weights) or afterwards
(e.g. the Pareto approach). Our method here is dif-
ferent in that we do not require any choice. We
use ensemble decoding (Razmara et al., 2012) (see
sec 3) to combine the different solutions resulting
from the multi-metric optimization, providing an el-
egant solution for deployment. We extend this idea
further and introduce ensemble tuning, where the
metrics have separate set of weights. The tuning
process alternates between ensemble decoding and
the update step where the weights for each metric
are optimized separately followed by joint update of
metric (meta) weights.
</bodyText>
<sectionHeader confidence="0.991274" genericHeader="method">
3 Ensemble Decoding
</sectionHeader>
<bodyText confidence="0.999952">
We now briefly review ensemble decoding (Razmara
et al., 2012) which is used as a component in the al-
gorithms we present. The prevalent model of statis-
tical MT is a log-linear framework using a vector of
feature functions 0:
</bodyText>
<equation confidence="0.8801215">
( )
p(e|f) a exp w · 0 (1)
</equation>
<bodyText confidence="0.9999562">
The idea of ensemble decoding is to combine sev-
eral models dynamically at decode time. Given mul-
tiple models, the scores are combined for each par-
tial hypothesis across the different models during
decoding using a user-defined mixture operation ®.
</bodyText>
<equation confidence="0.807506">
( )
p(e|f) a exp w1 · 01 ® w2 · 02 ® ... (2)
</equation>
<bodyText confidence="0.893715333333333">
(Razmara et al., 2012) propose several mixture
operations, such as log-wsum (simple linear mix-
ture), wsum (log-linear mixture) and max (choose lo-
</bodyText>
<page confidence="0.99685">
948
</page>
<bodyText confidence="0.999781333333333">
cally best model) among others. The different mix-
ture operations allows the user to encode the be-
liefs about the relative strengths of the models. It
has been applied successfully for domain adaptation
setting and shown to perform better approaches that
pre-compute linear mixtures of different models.
</bodyText>
<sectionHeader confidence="0.997305" genericHeader="method">
4 Multi-Metric Optimization
</sectionHeader>
<bodyText confidence="0.9930695">
In statistical MT, the multi-metric optimization
problem can be expressed as:
</bodyText>
<equation confidence="0.97728">
( )
g [M1(H), ... , Mk(H)] (3)
</equation>
<bodyText confidence="0.998795954545455">
where H = N(f; w)
where N(f; w) is the decoding function generating
a set of candidate hypotheses H based on the model
parameters w, for the source sentences f. For each
source sentence fi ∈ f there is a set of candidate
hypotheses {hi} ∈ H. The goal of the optimiza-
tion is to find the weights that maximize the func-
tion g(.) parameterized by different evaluation met-
rics M1, ... , Mk.
For the Pareto-optimal based approach such as
PMO (Duh et al., 2012), we can replace g(·) above
with gPMO(·) which returns the points in the Pareto
frontier. Alternately a weighted averaging function
gwavg(·) would result in a linear combination of the
metrics being considered, where the tuning method
would maximize the joint metric. This is similar to
the (TER-BLEU)/2 optimization (Cer et al., 2010;
Servan and Schwenk, 2011).
We introduce four methods based on the above
formulation and each method uses a different type
of g(·) function for combining different metrics and
we compare experimentally with existing methods.
</bodyText>
<subsectionHeader confidence="0.995194">
4.1 PMO Ensemble
</subsectionHeader>
<bodyText confidence="0.9999483">
PMO (Duh et al., 2012) seeks to maximize the num-
ber of points in the Pareto frontier of the metrics con-
sidered. The inner routine of the PMO-PRO tuning
is described in Algorithm 1. This routine is con-
tained within an outer loop that iterates for a fixed
number iterations of decoding the tuning set and op-
timizing the weights.
The tuning process with PMO-PRO is inde-
pendently repeated with different set of weights
for metrics1 yielding a set of equivalent solutions
</bodyText>
<footnote confidence="0.891721">
1For example Duh et al. (2012) use five different weight
</footnote>
<construct confidence="0.255891">
Algorithm 1 PMO-PRO (Inner routine for tuning)
</construct>
<listItem confidence="0.999301545454545">
1: Input: Hypotheses H = N(f; w); Weights w
2: Initialize T = {}
3: for each f in tuning set f do
4: {h} = H(f)
5: {M({h})} = ComputeMetricScore({h}, e)
6: {F} = FindParetoFrontier({M({h})})
7: for each h in {h} do
8: if h ∈ F then add (1, h) to T
9: else add (E, h) to T (see footnote 1)
10: wp ← PRO(T) (optimize using PRO)
11: Output: Pareto-optimal weights wp
</listItem>
<bodyText confidence="0.985552424242424">
{ps1, ... ,psn} which are points on the Pareto fron-
tier. The user then chooses one solution by making a
trade-off between the performance gains across dif-
ferent metrics. However, as noted earlier this a pos-
teriori choice ignores other solutions that are indis-
tinguishable from the chosen one.
We alleviate this by complementing PMO with
ensemble decoding, which we call PMO ensemble,
in which each point in the Pareto solution is a dis-
tinct component in the ensemble decoder. This idea
can also be used in other MMO approaches such as
linear combination of metrics (gwavg(.)) mentioned
above. In this view, PMO ensemble is a special case
of ensemble combination, where the decoding is per-
formed by an ensemble of optimal solutions.
The ensemble combination model introduces new
hyperparameters ,3 that are the weights of the en-
semble components (meta weights). These ensem-
ble weights could set to be uniform in a naive
implementation. Or the user can encode her be-
liefs or expectations about the individual solutions
{ps1, ... ,psn} to set the ensemble weights (based
on the relative importance of the components). Fi-
nally, one could also include a meta-level tuning step
to set the weights ,3.
The PMO ensemble approach is graphically il-
lustrated in Figure 1; we will also refer to this fig-
ure while discussing other methods.2 The orig-
settings for metrics (M1, M2), viz. (0.0, 1.0), (0.3, 0.7),
(0.5, 0.5), (0.7, 0.3) and (1.0, 0.0). They combine the met-
ric weights qi with the sentence-level metric scores Mi as
f = (Ek qkMk) /k where f is the target value for negative
examples (the else line in Alg 1) in the optimization step.
</bodyText>
<footnote confidence="0.757080333333333">
2The illustration is based on two metrics, metric-1 and
metric-2, but could be applied to any number of metrics. With-
out loss of generality we assume accuracy metrics, i.e. higher
</footnote>
<equation confidence="0.934721">
w* = arg max
w
</equation>
<page confidence="0.909551">
949
</page>
<figure confidence="0.868925">
Metric−1
</figure>
<figureCaption confidence="0.999976">
Figure 1: Illustration of different MMO approaches involving
</figureCaption>
<bodyText confidence="0.9830798">
two metrics. Solid (red) arrows indicate optimizing two met-
rics independently and the dashed (green) arrow optimize them
jointly. The Pareto frontier is indicated by the curve.
inal PMO-PRO seeks to maximize the points on
the Pareto frontier (blue curve in the figure) lead-
ing to Pareto-optimal solutions. On the other hand,
the PMO ensemble combines the different Pareto-
optimal solutions and potentially moving in the di-
rection of dashed (green) arrows to some point that
has higher score in either or both dimensions.
</bodyText>
<subsectionHeader confidence="0.993582">
4.2 Lateen MMO
</subsectionHeader>
<bodyText confidence="0.994606882352941">
Lateen EM has been proposed as a way of jointly
optimizing multiple objectives in the context of de-
pendency parsing (Spitkovsky et al., 2011). It uses
a secondary hard EM objective to move away, when
the primary soft EM objective gets stuck in a local
optima. The course correction could be performed
under different conditions leading to variations that
are based on when and how often to shift from one
objective function to another during optimization.
The lateen technique can be applied to the multi-
metric optimization in SMT by treating the differ-
ent metrics as different objective functions. While
the several lateen variants are also applicable for our
task, our objective here is to improve performance
across the different metrics (being optimized). Thus,
we restrict ourselves to the style where the search
alternates between the metrics (in round-robin fash-
ion) at each iteration. Since the notion of conver-
gence is unclear in lateen setting, we stop after a
fixed number of iterations optimizing the tuning set.
In terms of Figure 1, lateen MMO corresponds to al-
ternately maximizing the metrics along two dimen-
sions as depicted by the solid arrows.
By the very nature of lateen-alternation, the
metric score is better.
weights obtained at each iteration are likely to be
best for the metric that was optimized in that itera-
tion. Thus, one could use weights from the last k
iterations (for lateen-tuning with as many metrics)
and then decode the test set with an ensemble of
these weights as in PMO ensemble. However in
practice we find the weights to converge and we sim-
ply use the weights from the final iteration to decode
the test set in our lateen experiments.
</bodyText>
<subsectionHeader confidence="0.999974">
4.3 Union of Metrics
</subsectionHeader>
<bodyText confidence="0.999748090909091">
At each iteration lateen MMO excludes all but one
metric for optimization. An alternative would be to
consider all the metrics at each iteration so that the
optimizer could try to optimize them jointly. This
has been the general motivation for considering the
linear combination of metrics (Cer et al., 2010; Ser-
van and Schwenk, 2011) resulting in a joint metric,
which is then optimized.
However due to the scaling differences between
the scores of different metrics, the linear combi-
nation might completely suppress the metric hav-
ing scores in the lower-range. As an example, the
RIBES scores that are typically in the high 0.7-0.8
range, dominate the BLEU scores that is typically
around 0.3. While the weighted linear combination
tries to address this imbalance, they introduce ad-
ditional parameters that are manually fixed and not
separately tuned.
We avoid this linear combination pitfall by taking
the union of the metrics under which we consider
the union of training examples from all metrics and
optimize them jointly. Mathematically,
</bodyText>
<equation confidence="0.998147">
w* = arg max g(M1(H)) U ... U g(Mk(H)) (4)
w
</equation>
<bodyText confidence="0.999968">
Most of the optimization approaches involve two
phases: i) select positive and negative examples and
ii) optimize parameters to favour positive examples
while penalizing negative ones. In the union ap-
proach, we independently generate positive and neg-
ative sets of examples for all the metrics and take
their union. The optimizer now seeks to move to-
wards positive examples from all metrics, while pe-
nalizing others.
This is similar to the PMO-PRO approach except
that here the optimizer tries to simultaneously max-
imize the number of high scoring points across all
</bodyText>
<equation confidence="0.404288">
Metric−2
</equation>
<page confidence="0.95818">
950
</page>
<bodyText confidence="0.999615666666667">
metrics. Thus, instead of the entire Pareto frontier
curve in Figure 1, the union approach optimizes the
two dimensions simultaneously in each iteration.
</bodyText>
<sectionHeader confidence="0.970729" genericHeader="method">
5 Ensemble Tuning
</sectionHeader>
<bodyText confidence="0.9999844">
These methods, even though novel, under utilize the
power of ensembles as they combine the solution
only at the end of the tuning process. We would
prefer to tightly integrate the idea of ensembles into
the tuning. We thus extend the ensemble decoding
to ensemble tuning. The feature weights are repli-
cated separately for each evaluation metric, which
are treated as components in the ensemble decoding
and tuned independently in the optimization step.
Initially the ensemble decoder decodes a devset us-
ing a weighted ensemble to produce a single N-best
list. For the optimization, we employ a two-step ap-
proach of optimizing the feature weights (of each
ensemble component) followed by a step for tun-
ing the meta (component) weights. The optimized
weights are then used for decoding the devset in the
next iteration and the process is repeated for a fixed
number of iterations.
Modifying the MMO representation in Equa-
tion 3, we formulate ensemble tuning as:
</bodyText>
<equation confidence="0.925785">
( )
Hens = Nens f; {wM}; ®; A (5)
{ �
w∗ = arg max Hens  |1≤i≤k (6)
wmi
A = arg max
A
</equation>
<bodyText confidence="0.9999546875">
Here the ensemble decoder function Nens(.)
is parameterized by an ensemble of weights
wM,,..., wMk (denoted as {wM} in Eq 5) for each
metric and a mixture operation (®). A represents the
weights of the ensemble components.
Pseudo-code for ensemble tuning is shown in Al-
gorithm 2. In the beginning of each iteration (line 2),
the tuning process ensemble decodes (line 4) the
tuning set using the weights obtained from the pre-
vious iteration. Equation 5 gives the detailed expres-
sion for the ensemble decoding, where Hens denotes
the N-best list generated by the ensemble decoder.
The method now uses a dual tuning strategy in-
volving two phases to optimize the weights. In the
first step it optimizes each of the k metrics indepen-
dently (lines 6-7) along its respective dimension in
</bodyText>
<table confidence="0.332342">
Algorithm 2 Ensemble Tuning Algorithm
1: Input: Tuning set f,
Metrics M1, ... , Mk (ensemble components)
Initial weights {wM} — wM,, ... wMk and
Component (meta) weights A
</table>
<listItem confidence="0.950269818181818">
2: for j = 1,... do
3: {w(�)M } — {wM}
4: Ensemble decode the tuning set
Hens = Nens(f; {w(�)M }; ®; A)
5: {wM} = {}
6: for each metric Mi E {M} do
7: w∗Mi — PRO(Hens, wMi) (use PRO)
8: Add w∗M to {wM}
i
9: A — PMO-PRO(Hens, {wM}) (Alg 1)
10: Output: Optimal weights {wM} and A
</listItem>
<bodyText confidence="0.99963925">
the multi-metric space (as shown by the solid arrows
along the two axes in Figure 1). This yields a new
set of weights w∗ for the features in each metric.
The second tuning step (line 9) then optimizes
the meta weights (A) so as to maximize the multi-
metric objective along the joint k-dimensional space
as shown in Equation 7. This is illustrated by the
dashed arrows in the Figure 1. While g(.) could be
any function that combines multiple metrics, we use
the PMO-PRO algorithm (Alg. 1) for this step.
The main difference between ensemble tuning and
PMO ensemble is that the former is an ensemble
model over metrics and the latter is an ensemble
model over Pareto solutions. Additionally, PMO en-
semble uses the notion of ensembles only for the fi-
nal decoding after tuning has completed.
</bodyText>
<subsectionHeader confidence="0.964968">
5.1 Implementation Notes
</subsectionHeader>
<bodyText confidence="0.999203142857143">
All the proposed methods fit naturally within the
usual SMT tuning framework. However, some
changes are required in the decoder to support en-
semble decoding and in the tuning scripts for op-
timizing with multiple metrics. For ensemble de-
coding, the decoder should be able to use multiple
weight vectors and dynamically combine them ac-
cording to some desired mixture operation. Note
that, unlike Razmara et al. (2012), our approach uses
just one model but has different weight vectors for
each metric and the required decoder modifications
are simpler than full ensemble decoding.
While any of the mixture operations proposed
by Razmara et al. (2012) could be used, in this pa-
</bodyText>
<equation confidence="0.997481">
g ({Mi(Hens)|1≤i≤k} ; w∗) (7)
</equation>
<page confidence="0.971633">
951
</page>
<bodyText confidence="0.999941">
per we use log-wsum – the linear combination of the
ensemble components and log-wmax – the combina-
tion that prefers the locally best component. These
are simpler to implement and also performed com-
petitively in their domain adaptation experiments.
Unless explicitly noted otherwise, the results pre-
sented in Section 6 are based on linear mixture oper-
ation log-wsum, which empirically performed better
than the log-wmax for ensemble tuning.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.997425307692308">
We evaluate the different methods on Arabic-
English translation in single as well as multiple ref-
erences scenario. Corpus statistics are shown in
Table 1. For all the experiments in this paper,
we use Kriya, our in-house Hierarchical phrase-
based (Chiang, 2007) (Hiero) system, and inte-
grated the required changes for ensemble decoding.
Kriya performs comparably to the state of the art in
phrase-based and hierarchical phrase-based transla-
tion over a wide variety of language pairs and data
sets (Sankaran et al., 2012).
We use PRO (Hopkins and May, 2011) for op-
timizing the feature weights and PMO-PRO (Duh
et al., 2012) for optimizing meta weights, wher-
ever applicable. In both cases, we use SVM-
Rank (Joachims, 2006) as the optimizer.
We used the default parameter settings for dif-
ferent MT tuning metrics. For METEOR, we tried
both METEOR-tune and METEOR-hter settings
and found the latter to perform better in BLEU and
TER scores, even though the former was marginally
better in METEOR3 and RIBES scores. We ob-
served the margin of loss in BLEU and TER to out-
weigh the gains in METEOR and RIBES and we
chose METEOR-hter setting for both optimization
and evaluation of all our experiments.
</bodyText>
<subsectionHeader confidence="0.997492">
6.1 Evaluation on Tuning Set
</subsectionHeader>
<bodyText confidence="0.999861666666667">
Unlike conventional tuning methods, PMO (Duh et
al., 2012) was originally evaluated on the tuning set
to avoid potential mismatch with the test set. In
order to ensure robustness of evaluation, they re-
decode the devset using the optimal weights from
the last tuning iteration and report the scores on 1-
</bodyText>
<footnote confidence="0.915596333333333">
3This behaviour was also noted by Denkowski and Lavie
(2011) in their analysis of Urdu-English system for tunable met-
rics task in WMT11.
</footnote>
<table confidence="0.9316415">
best candidates.
Corpus Training size Tuning/ test set
ISI corpus 1.1 M 1664/ 1313 (MTA)
1982/ 987 (ISI)
</table>
<tableCaption confidence="0.922343">
Table 1: Corpus Statistics (# of sentences) for Arabic-English.
MTA (4-refs) and ISI (1-ref).
</tableCaption>
<bodyText confidence="0.999879515151515">
We follow the same strategy and compare our
PMO-ensemble approach with PMO-PRO (denoted
P) and a linear combination4 (denoted L) base-
line. Similar to Duh et al. (2012), we use
five different BLEU:RIBES weight settings, viz.
(0.0,1.0), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3) and
(1.0, 0.0), marked L1 through L5 or P1 through P5.
The Pareto frontier is then computed from 80 points
(5 runs and 15 iterations per run) on the devset.
Figure 2(a) shows the Pareto frontier of L and P
baselines using BLEU and RIBES as two metrics.
The frontier of the P dominates that of L for most
part showing that the PMO approach benefits from
picking Pareto points during the optimization.
We use the PMO-ensemble approach to combine
the optimized weights from the 5 tuning runs and
re-decode the devset employing ensemble decoding.
This yields the points LEns and PEns in the plot,
which obtain better scores than most of the indi-
vidual runs of L and P. This ensemble approach of
combining the final weights also generalizes to the
unseen test set as we show later.
Figure 2(b) plots the change in BLEU during tun-
ing in the multiple references and the single refer-
ence scenarios. We show for each baseline method L
and P, plots for two different weight settings that ob-
tain high BLEU and RIBES scores. In both datasets,
our ensemble tuning approach dominates the curves
of the (L and P) baselines. In summary, these results
confirm that the ensemble approach achieves results
that are competitive with previous MMO methods
on the devset Pareto curve. We now provide a more
comprehensive evaluation on the test set.
</bodyText>
<subsectionHeader confidence="0.998141">
6.2 Evaluation on Test Set
</subsectionHeader>
<bodyText confidence="0.999971666666667">
This section contains multi-metric optimization re-
sults on the unseen test sets, one test set has multi-
ple references and the other has a single-reference.
</bodyText>
<footnote confidence="0.989146">
4Linear combination is a generalized version of the com-
bined (TER-BLEU)/2 metric and its variants.
</footnote>
<page confidence="0.994016">
952
</page>
<figure confidence="0.999622978723404">
0.33 0.335 0.34 0.345 0.35
BLEU
(a) Pareto frontier and BLEU-RIBES scores: MTA 4-refs devset
Iterations
(b) Tuning BLEU scores: MTA 4-refs and ISI 1-ref devsets
PMO-PRO
Lin-Comb
P2
PEns
L4
L3
L1
P1
LEns
P3
P5
P4
L2
L5
2 4 6 8 10 12 14
BLEU
0.38
0.36
0.34
0.32
0.28
0.26
0.3
L1-mta
L5-mta
P1-mta
P5-mta
Ens-Tune-mta
L1-isi
L3-isi
P1-isi
P3-isi
Ens-Tune-isi
RIBES 0.773
0.772
0.771
0.77
0.769
0.768
0.767
0.766
0.765
</figure>
<figureCaption confidence="0.99723">
Figure 2: Devset (redecode): Comparison of Lin-comb (L) and PMO-PRO (P) with Ensemble decoding (Lens and PEns) and
</figureCaption>
<subsectionHeader confidence="0.709906">
Ensemble tuning (Ens-Tune)
</subsectionHeader>
<bodyText confidence="0.99775784375">
We plot BLEU scores against other metrics (RIBES,
METEOR and TER) and this allows us to compare
the performance of each metric relative to the de-
facto standard BLEU metric.
Baseline points are identified by single letters B
for BLEU, T for TER, etc. and the baseline (single-
metric optimized) score for each metric is indicated
by a dashed line on the corresponding axis. MMO
points use a series of single letters referring to the
metrics used, e.g. BT for BLEU-TER. The union of
metrics method is identified with the suffix ’J’ and
lateen method with suffix ’L’ (thus BT-L refers to the
lateen tuning with BLEU-TER). MMO points with-
out any suffix use the ensemble tuning approach.
Figures 3 and 4(a) plot the scores for the MTA test
set with 4-references. We see noticeable and some
statistically significant improvements in BLEU and
RIBES (see Table 2 for BLEU improvements).
All our MMO approaches, except for the union
method, show gains on both BLEU and RIBES axes.
Figures 3(b) and 4(a) show that none of the proposed
methods managed to improve the baseline scores for
METEOR and TER. However, several of our en-
semble tuning combinations work well for both ME-
TEOR (BR, BMRTB3, etc.) and TER (BMRT and
BRT) in that they improved or were close to the
baseline scores in either dimension. We again see in
these figures that the MMO approaches can improve
the BLEU-only tuning by 0.3 BLEU points, without
much drop in other metrics. This is in tune with the
finding that BLEU could be tuned easily (Callison-
Burch et al., 2011) and also explains why it remains
</bodyText>
<table confidence="0.999554315789474">
Approach and Tuning Metric(s) BLEU
MTA ISI
Single Objective Baselines
BLEU 36.06 37.20
METEOR 35.05 36.91
RIBES 33.35 36.60
TER 33.92 35.85
Ensemble Tuning: 2 Metrics
B-M 36.02 37.26
B-R 36.15 37.37
B-T 35.72 36.31
Ensemble Tuning: 3 Metrics
B-M-R 36.36 37.37
B-M-T 36.22 36.89
B-R-T 35.97 36.72
Ensemble Tuning: &gt; 3 Metrics
B-M-R-T 35.94 36.84
B-M-R-T-B3 36.16 37.12
B-M-R-T-B3-B2-B1 36.08 37.24
</table>
<tableCaption confidence="0.9718446">
Table 2: BLEU Scores on MTA (4 refs) and ISI (1 ref) test sets
using the standard mteval script. Boldface scores indicate scores
that are comparable to or better than the baseline BLEU-only
tuning. Italicized scores indicate statistically significant differ-
ences at p-value 0.05 computed with bootstrap significance test.
</tableCaption>
<bodyText confidence="0.980097363636364">
a popular choice for optimizing SMT systems.
Among the different MMO methods the ensem-
ble tuning performs better than lateen or union ap-
proaches. In terms of the number of metrics being
optimized jointly, we see substantial gains when us-
ing a small number (typically 2 or 3) of metrics. Re-
sults seem to suffer beyond this number; probably
because there might not be a space that contain so-
lution(s) optimal for all the metrics that are jointly
optimized.
We hypothesize that each metric correlates well
</bodyText>
<page confidence="0.996794">
953
</page>
<figure confidence="0.999719885245901">
0.52
0.515
0.51
METEOR
0.505
0.5
0.495
0.33 0.335 0.34 0.345 0.35 0.355 0.36 0.365 0.3
BLEU
(a) BLEU-RIBES scores
0.33 0.335 0.34 0.345 0.35 0.355 0.36 0.365 0.3
BLEU
(b) BLEU-METEOR scores
R
BMT-J
T BT PEns
BT-J
M
BR-L
BMR-L
BM-J
BMRT
MR
BR-J
BM
BR
BRT
B
BMRTB3B2B1
BM-L
LEns
BMRTB3
BMT
BMR
M
BMRTB3B2B1
BMRTB3
TMRJ
BR-L
BM BR
B
BMR
LEns(BR)
BMT
BM-L
BMR-L
BRT
BT
PEns(BR)
BR-J
T
R
BMT-J
BT-J
RIBES 0.82
0.818
0.816
0.814
0.812
0.81
0.808
</figure>
<figureCaption confidence="0.995945">
Figure 3: MTA 4-refs testset: Comparison of different MMO approaches. The dashed lines correspond to baseline scores tuned on
the respective metrics in the axes. The union of metrics method is identified with the suffix J and lateen with suffix L.
</figureCaption>
<figure confidence="0.999882762711864">
0.435
0.43
0.425
0.42
nTER
0.415
0.41
0.405
0.4
0.465
0.46
0.455
nTER
0.45
0.445
0.44
0.33 0.335 0.34 0.345 0.35 0.355 0.36 0.365 0.3
BLEU
(a) MTA (4-refs)
0.355 0.36 0.365 0.37 0.37
BLEU
(b) ISI (1-ref)
T
BT-J
BMT-J
BR-J
BMRT
PEns(BR)
BMT
BRT
BM-L
BT
R
LEns(BR)
BMR-L
MR
BMR
BR
BMRTB3
B
BM
BM-J
BR-L
M
T
BRT
R
BMRT
PEns(BR) LEns(BR)
BR-J
M
B3MT
BMRTB3B2B1
BR-L
BMRTB3
B
BM
BR
BMR
</figure>
<figureCaption confidence="0.999201">
Figure 4: BLEU-TER scores: Comparison of different MMO approaches. We plot nTER (1-TER) scores for easy reading of the
plots. The dashed lines correspond to baseline scores tuned on the respective metrics in the axes.
</figureCaption>
<bodyText confidence="0.999862375">
(in a looser sense) with few others, but not all. For
example, union optimizations BR-J and BMT-J per-
form close to or better than RIBES and TER base-
lines, but get very poor score in METEOR. On the
other hand BM-J is close to the METEOR baseline,
while doing poorly on the RIBES and TER. This be-
haviour is also evident from the single-metric base-
lines, where R and T-only settings are clearly distin-
guished from the M-only system. It is not clear if
such distinct classes of metrics could be bridged by
some optimal solution and the metric dichotomy re-
quires further study as this is key to practical multi-
metric tuning in SMT.
The lateen and union approaches appear to be
very sensitive to the number of metrics and they
generally perform well for two metrics case and
show degradation for more metrics. Unlike other
approaches, the union approach failed to improve
over the baseline BLEU and this could be attributed
to the conflict of interest among the metrics, while
choosing example points for the optimization step.
The positive example preferred by a particular met-
ric could be a negative example for the other metric.
This would only confuse the optimizer resulting in
poor solutions. Our future line of work would be to
study the effect of avoiding such of conflicting ex-
amples in the union approach.
For the single-reference (ISI) dataset, we only
plot the BLEU-TER case in Figure 4(b) due to lack
of space. The results are similar to the multiple
references set indicating that MMO approaches are
equally effective for single references5. Table 2
</bodyText>
<footnote confidence="0.968034">
5One could argue that MMO methods require multiple ref-
erences since each metric might be picking out a different ref-
</footnote>
<page confidence="0.989341">
954
</page>
<table confidence="0.999816">
Metric Single-metric Tuning Ensemble Tuning
B-only M-only B-M-R
BLEU 37.89 37.18 39.01
HBLEU 51.93 53.59 53.14
METEOR 61.31 61.56 61.68
HMETEOR 72.35 72.39 72.74
TER 0.520 0.532 0.516
HTER 0.361 0.370 0.346
</table>
<tableCaption confidence="0.9745074">
Table 3: Post-editing Human Evaluation: Regular (untargeted)
and human-targeted scores. Human targeted scores are com-
puted against the post-edited reference and regular scores are
computed with the original references. Best scores are in bold-
face and statistically significant ones (at P = 0.05) are italicized.
</tableCaption>
<bodyText confidence="0.999449">
shows the BLEU scores for our ensemble tuning
method (for various combinations) and we again see
improvements over the baseline BLEU-only tuning.
</bodyText>
<subsectionHeader confidence="0.997169">
6.3 Human Evaluation
</subsectionHeader>
<bodyText confidence="0.998411448275862">
So far we have shown that multi-metric optimiza-
tion can improve over single-metric tuning on a sin-
gle metric like BLEU and we have shown that our
methods find a tuned model that performs well with
respect to multiple metrics. Is the output that scores
higher on multiple metrics actually a better trans-
lation? To verify this, we conducted a post-editing
human evaluation experiment. We compared our en-
semble tuning approach involving BLEU, METEOR
and RIBES (B-M-R) with systems optimized for
BLEU (B-only) and METEOR (M-only).
We selected 100 random sentences (that are at
least 15 words long) from the Arabic-English MTA
(4 references) test set and translated them using the
three systems (two single metric systems and BMR
ensemble tuning). We shuffled the resulting trans-
lations and split them into 3 sets such that each set
has equal number of the translations from three sys-
tems. The translations were edited by three human
annotators in a post-editing setup, where the goal
was to edit the translations to make them as close
to the references as possible, using the Post-Editing
Tool: PET (Aziz et al., 2012). The annotators were
not Arabic-literate and relied only on the reference
translations during post-editing. The identifiers that
link each translation to the system that generated it
are removed to avoid annotator bias.
In the end we collated post-edited translations for
each system and then computed the system-level
erence sentence. Our experiment shows that even with a single
reference MMO methods can work.
human-targeted (HBLEU, HMETEOR, HTER)
scores, by using respective post-edited translations
as the reference. First comparing the HTER (Snover
et al., 2006) scores shown in Table 3, we see
that the single-metric system optimized for ME-
TEOR performs slightly worse than the one op-
timized for BLEU, despite using METEOR-hter
version (Denkowski and Lavie, 2011). Ensemble
tuning-based system optimized for three metrics (B-
M-R) improves HTER by 4% and 6.3% over BLEU
and METEOR optimized systems respectively.
The single-metric system tuned with M-only set-
ting scores high on HBLEU, closely followed by the
ensemble system. We believe this to be caused by
chance rather than any systematic gains by the M-
only tuning; the ensemble system scores high on
HMETEOR compared to the M-only system. While
HTER captures the edit distance to the targeted ref-
erence, HMETEOR and HBLEU metrics capture
missing content words or synonyms by exploiting
n-grams and paraphrase matching.
We also computed the regular variants (BLEU,
METEOR and TER), which are scored against orig-
inal references. The ensemble system outperformed
the single-metric systems in all the three metrics.
The improvements were also statistically significant
at p-value of 0.05 for BLEU and TER.
</bodyText>
<sectionHeader confidence="0.998451" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999937947368421">
We propose and present a comprehensive study of
several multi-metric optimization (MMO) methods
in SMT. First, by exploiting the idea of ensemble de-
coding (Razmara et al., 2012), we propose an effec-
tive way to combine multiple Pareto-optimal model
weights from previous MMO methods (e.g. Duh et
al. (2012)), obviating the need for manually trading
off among metrics. We also proposed two new vari-
ants: lateen-style MMO and union of metrics.
We also extended ensemble decoding to a new
tuning algorithm called ensemble tuning. This
method demonstrates statistically significant gains
for BLEU and RIBES with modest reduction in ME-
TEOR and TER. Further, in our human evaluation,
ensemble tuning obtains the best HTER among com-
peting baselines, confirming that optimizing on mul-
tiple metrics produces human-preferred translations
compared to the conventional optimization approach
involving a single metric.
</bodyText>
<page confidence="0.997542">
955
</page>
<sectionHeader confidence="0.982216" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999022796116505">
Michael Auli and Adam Lopez. 2011. Training a log-
linear parser with loss functions via softmax-margin.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 333–
343, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Wilker Aziz, Sheila Castilho Monteiro de Sousa, and
Lucia Specia. 2012. PET: a tool for post-editing
and assessing machine translation. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC’12), Istanbul,
Turkey, may. European Language Resources Associa-
tion (ELRA).
Marzieh Bazrafshan, Tagyoung Chung, and Daniel
Gildea. 2012. Tuning as linear regression. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the ACL: Human Language Technolo-
gies, pages 543–547, Montr´eal, Canada. ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22–64, Edinburgh, Scotland, July. ACL.
Daniel Cer, Christopher D. Manning, and Daniel Juraf-
sky. 2010. The best lexical metric for phrase-based
statistical mt system optimization. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the ACL, pages 555–
563. ACL.
Boxing Chen, Roland Kuhn, and Samuel Larkin. 2012.
Port: a precision-order-recall mt evaluation metric for
tuning. In Proceedings of the 50th Annual Meeting
of the ACL (Volume 1: Long Papers), pages 930–939,
Jeju Island, Korea. ACL.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the ACL: Human Language Technolo-
gies, pages 427–436, Montr´eal, Canada. ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 224–233. ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization and
Evaluation of Machine Translation Systems. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation, Edinburgh, Scotland, July. ACL.
Jacob Devlin and Spyros Matsoukas. 2012. Trait-based
hypothesis selection for machine translation. In Pro-
ceedings of the 2012 Conference of the North Ameri-
can Chapter of the ACL: Human Language Technolo-
gies, pages 528–532. ACL.
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime
Tsukada, and Masaaki Nagata. 2012. Learning to
translate with multiple objectives. In Proceedings of
the 50th Annual Meeting of the ACL, Jeju Island, Ko-
rea. ACL.
Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip
Resnik. 2009. The university of maryland statistical
machine translation system for the fourth workshop on
machine translation. In Proc. of the Fourth Workshop
on Machine Translation.
Kevin Gimpel and Noah A. Smith. 2012. Struc-
tured ramp loss minimization for machine transla-
tion. In Proceedings of the 2012 Conference of
the North American Chapter of the ACL: Human
Language Technologies, pages 221–231, Montr´eal,
Canada. ACL.
Keith Hall, Ryan T. McDonald, Jason Katz-Brown, and
Michael Ringgaard. 2011. Training dependency
parsers by jointly optimizing multiple objectives. In
Proceedings of the Empirical Methods in Natural Lan-
guage Processing, pages 1489–1499.
Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation mod-
els. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 292–301, Jeju Island, Ko-
rea. ACL.
Yifan He and Andy Way. 2009. Improving the objec-
tive function in minimum error rate training. In MT
Summit.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of the 2011 Conference on Empir-
ical Methods in Natural Language Processing, Edin-
burgh, Scotland. ACL.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic evalu-
ation of translation quality for distant language pairs.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 944–
952, Cambridge, MA. ACL.
Thorsten Joachims. 2006. Training linear svms in linear
time. In Proceedings of the 12th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, pages 217–226.
Alon Lavie and Michael J. Denkowski. 2009. The me-
teor metric for automatic evaluation of machine trans-
lation. Machine Translation, 23(2-3):105–115.
</reference>
<page confidence="0.985715">
956
</page>
<reference confidence="0.999800391891892">
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
R. T. Marler and J. S. Arora. 2004. Survey of
multi-objective optimization methods for engineer-
ing. Structural and Multidisciplinary Optimization,
26(6):369–395, April.
Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2008.
Automatic evaluation measures for statistical machine
translation system optimization. In International Con-
ference on Language Resources and Evaluation, Mar-
rakech, Morocco.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the ACL, pages 160–167. ACL.
Sebastian Pad´o, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23(2-3):181–193.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings ofAsso-
ciation of Computational Linguistics, pages 311–318.
ACL.
Majid Razmara, George Foster, Baskaran Sankaran, and
Anoop Sarkar. 2012. Mixing multiple translation
models in statistical machine translation. In Proceed-
ings of the 50th Annual Meeting of the ACL, Jeju, Re-
public of Korea. ACL.
Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.
2012. Kriya an end-to-end hierarchical phrase-based
mt system. The Prague Bulletin of Mathematical Lin-
guistics (PBML), 97(97):83–98.
Christophe Servan and Holger Schwenk. 2011. Optimis-
ing multiple metrics with mert. Prague Bull. Math.
Linguistics, 96:109–118.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 11–21, Jeju Island, Korea. ACL.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings ofAssociation forMachine Translation
in the Americas, pages 223–231.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-
sky. 2011. Lateen EM: Unsupervised training with
multiple objectives, applied to dependency grammar
induction. In Proceedings of the Empirical Methods in
Natural Language Processing, pages 1269–1280. As-
sociation of Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764–
773. ACL.
Taro Watanabe. 2012. Optimized online rank learn-
ing for machine translation. In Proceedings of the
2012 Conference of the North American Chapter of the
ACL: Human Language Technologies, pages 253–262,
Montr´eal, Canada, June. ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.
Daqi Zheng, Yifan He, Yang Liu, and Qun Liu. 2012.
Maximum rank correlation training for statistical ma-
chine translation. In MT Summit XIII.
</reference>
<page confidence="0.997577">
957
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.423153">
<title confidence="0.99993">Multi-Metric Optimization Using Ensemble Tuning</title>
<author confidence="0.896223">Baskaran Sankaran</author>
<author confidence="0.896223">Anoop Sarkar</author>
<affiliation confidence="0.838567">Simon Fraser University</affiliation>
<address confidence="0.454253">Burnaby BC. CANADA</address>
<abstract confidence="0.99821004">This paper examines tuning for statistical machine translation (SMT) with respect to multiple evaluation metrics. We propose several novel methods for tuning towards multiple obincluding some based on Pareto-optimality is a natural way to think about multi-metric optimization (MMO) and our methods can effectively combine several Pareto-optimal solutions, obviating the need to choose one. Our best tuning is a new algorithm for multi-metric optimization that searches for Pareto-optimal ensemble models. We study the effectiveness of our methods through experiments on multiple as well as single reference(s) datasets. Our experiments show simultaneous gains across several metrics (BLEU, RIBES), without any significant reduction in other metrics. This contrasts the traditional tuning where gains are usually limited to a single metric. Our human evaluation results confirm that in order to produce better MT output, optimizing multiple metrics is better than optimizing only one.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Adam Lopez</author>
</authors>
<title>Training a loglinear parser with loss functions via softmax-margin.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>333--343</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="4937" citStr="Auli and Lopez, 2011" startWordPosition="758" endWordPosition="761">aches for multi-metric tuning and compare their performance to the ensemble tuning. Our experiments yield the highest metric scores across many different metrics (that are being optimized), something that has not been possible until now. Our ensemble tuning method over multiple metrics produced superior translations than single metric tuning as measured by a post-editing task. HTER (Snover et al., 2006) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output. 2 Related Work In grammar induction and parsing (Spitkovsky et al., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwe</context>
</contexts>
<marker>Auli, Lopez, 2011</marker>
<rawString>Michael Auli and Adam Lopez. 2011. Training a loglinear parser with loss functions via softmax-margin. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 333– 343, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wilker Aziz</author>
<author>Sheila Castilho Monteiro de Sousa</author>
<author>Lucia Specia</author>
</authors>
<title>PET: a tool for post-editing and assessing machine translation.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<location>Istanbul, Turkey,</location>
<marker>Aziz, de Sousa, Specia, 2012</marker>
<rawString>Wilker Aziz, Sheila Castilho Monteiro de Sousa, and Lucia Specia. 2012. PET: a tool for post-editing and assessing machine translation. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marzieh Bazrafshan</author>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Tuning as linear regression.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the ACL: Human Language Technologies,</booktitle>
<pages>543--547</pages>
<publisher>ACL.</publisher>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="1814" citStr="Bazrafshan et al., 2012" startWordPosition="270" endWordPosition="273">imizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science &amp; Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 20</context>
<context position="5356" citStr="Bazrafshan et al., 2012" startWordPosition="819" endWordPosition="822"> our human evaluation confirm that multi-metric optimization can lead to better MT output. 2 Related Work In grammar induction and parsing (Spitkovsky et al., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the best metric used for tuning. For instance, (Mauser et al., 2008; Cer et al., 2010) report that tuning on linear combinations of</context>
</contexts>
<marker>Bazrafshan, Chung, Gildea, 2012</marker>
<rawString>Marzieh Bazrafshan, Tagyoung Chung, and Daniel Gildea. 2012. Tuning as linear regression. In Proceedings of the 2012 Conference of the North American Chapter of the ACL: Human Language Technologies, pages 543–547, Montr´eal, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<publisher>ACL.</publisher>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="2891" citStr="Callison-Burch et al., 2011" startWordPosition="438" endWordPosition="441">hing a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 2009) to modelling semantic similarity through textual entailment (Pad´o et al., 2009) to RIBES, an evaluation metric that pays attention to long-distance reordering (Isozaki et al., 2010). While some of these metrics such as TER, METEOR are gaining prominence, BLEU enjoys the status of being the de facto standard tuning metric as it is often claimed and sometimes observed that optimizing with BLEU produces better translations than other metrics (Callison-Burch et al., 2011). The gains obtained by the MT system tuned on a particular metric do not improve performance as measured under other metrics (Cer et al., 2010), suggesting that over-fitting to a specific metric might happen without improvements in translation quality. In this paper we propose a new tuning framework for jointly optimizing multiple evaluation metrics. Pareto-optimality is a natural way to think about multi-metric optimization and multi-metric optimization (MMO) was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach (Duh et al</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64, Edinburgh, Scotland, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
<author>Daniel Jurafsky</author>
</authors>
<title>The best lexical metric for phrase-based statistical mt system optimization.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL,</booktitle>
<pages>555--563</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3035" citStr="Cer et al., 2010" startWordPosition="463" endWordPosition="466">to modelling semantic similarity through textual entailment (Pad´o et al., 2009) to RIBES, an evaluation metric that pays attention to long-distance reordering (Isozaki et al., 2010). While some of these metrics such as TER, METEOR are gaining prominence, BLEU enjoys the status of being the de facto standard tuning metric as it is often claimed and sometimes observed that optimizing with BLEU produces better translations than other metrics (Callison-Burch et al., 2011). The gains obtained by the MT system tuned on a particular metric do not improve performance as measured under other metrics (Cer et al., 2010), suggesting that over-fitting to a specific metric might happen without improvements in translation quality. In this paper we propose a new tuning framework for jointly optimizing multiple evaluation metrics. Pareto-optimality is a natural way to think about multi-metric optimization and multi-metric optimization (MMO) was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach (Duh et al., 2012). PMO provides several equivalent solutions (parameter weights) having different trade-offs between the different MT metrics. In (Duh et</context>
<context position="5911" citStr="Cer et al., 2010" startWordPosition="917" endWordPosition="920"> and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the best metric used for tuning. For instance, (Mauser et al., 2008; Cer et al., 2010) report that tuning on linear combinations of BLEU-TER is more robust than a single metric like WER. The approach in (Devlin and Matsoukas, 2012) modifies the optimization function to include traits such as output length so that the hypotheses produced by the decoder have maximal score according to one metric (BLEU) but are subject to an output length constraint, e.g. that the output is 5% shorter. This is done by rescoring an N-best list (forest) for the metric combined with each trait condition and then the different trait hypothesis are combined using a system combination step. The traits a</context>
<context position="9825" citStr="Cer et al., 2010" startWordPosition="1582" endWordPosition="1585">urce sentence fi ∈ f there is a set of candidate hypotheses {hi} ∈ H. The goal of the optimization is to find the weights that maximize the function g(.) parameterized by different evaluation metrics M1, ... , Mk. For the Pareto-optimal based approach such as PMO (Duh et al., 2012), we can replace g(·) above with gPMO(·) which returns the points in the Pareto frontier. Alternately a weighted averaging function gwavg(·) would result in a linear combination of the metrics being considered, where the tuning method would maximize the joint metric. This is similar to the (TER-BLEU)/2 optimization (Cer et al., 2010; Servan and Schwenk, 2011). We introduce four methods based on the above formulation and each method uses a different type of g(·) function for combining different metrics and we compare experimentally with existing methods. 4.1 PMO Ensemble PMO (Duh et al., 2012) seeks to maximize the number of points in the Pareto frontier of the metrics considered. The inner routine of the PMO-PRO tuning is described in Algorithm 1. This routine is contained within an outer loop that iterates for a fixed number iterations of decoding the tuning set and optimizing the weights. The tuning process with PMO-PR</context>
<context position="15445" citStr="Cer et al., 2010" startWordPosition="2544" endWordPosition="2547">ations (for lateen-tuning with as many metrics) and then decode the test set with an ensemble of these weights as in PMO ensemble. However in practice we find the weights to converge and we simply use the weights from the final iteration to decode the test set in our lateen experiments. 4.3 Union of Metrics At each iteration lateen MMO excludes all but one metric for optimization. An alternative would be to consider all the metrics at each iteration so that the optimizer could try to optimize them jointly. This has been the general motivation for considering the linear combination of metrics (Cer et al., 2010; Servan and Schwenk, 2011) resulting in a joint metric, which is then optimized. However due to the scaling differences between the scores of different metrics, the linear combination might completely suppress the metric having scores in the lower-range. As an example, the RIBES scores that are typically in the high 0.7-0.8 range, dominate the BLEU scores that is typically around 0.3. While the weighted linear combination tries to address this imbalance, they introduce additional parameters that are manually fixed and not separately tuned. We avoid this linear combination pitfall by taking th</context>
</contexts>
<marker>Cer, Manning, Jurafsky, 2010</marker>
<rawString>Daniel Cer, Christopher D. Manning, and Daniel Jurafsky. 2010. The best lexical metric for phrase-based statistical mt system optimization. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 555– 563. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>Samuel Larkin</author>
</authors>
<title>Port: a precision-order-recall mt evaluation metric for tuning.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the ACL (Volume 1: Long Papers),</booktitle>
<pages>930--939</pages>
<publisher>ACL.</publisher>
<location>Jeju Island,</location>
<contexts>
<context position="5766" citStr="Chen et al., 2012" startWordPosition="890" endWordPosition="893">tanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the best metric used for tuning. For instance, (Mauser et al., 2008; Cer et al., 2010) report that tuning on linear combinations of BLEU-TER is more robust than a single metric like WER. The approach in (Devlin and Matsoukas, 2012) modifies the optimization function to include traits such as output length so that the hypotheses produced by the decoder have maximal score according to one metric (BLEU) but are subject to an output length constraint, e.g. that the output is 5% shorter. This is done by rescoring an N-best list (forest) for</context>
</contexts>
<marker>Chen, Kuhn, Larkin, 2012</marker>
<rawString>Boxing Chen, Roland Kuhn, and Samuel Larkin. 2012. Port: a precision-order-recall mt evaluation metric for tuning. In Proceedings of the 50th Annual Meeting of the ACL (Volume 1: Long Papers), pages 930–939, Jeju Island, Korea. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the ACL: Human Language Technologies,</booktitle>
<pages>427--436</pages>
<publisher>ACL.</publisher>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="5186" citStr="Cherry and Foster, 2012" startWordPosition="792" endWordPosition="795">mble tuning method over multiple metrics produced superior translations than single metric tuning as measured by a post-editing task. HTER (Snover et al., 2006) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output. 2 Related Work In grammar induction and parsing (Spitkovsky et al., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the ACL: Human Language Technologies, pages 427–436, Montr´eal, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>224--233</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1740" citStr="Chiang et al., 2008" startWordPosition="259" endWordPosition="262">oduce better MT output, optimizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science &amp; Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capt</context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 224–233. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<contexts>
<context position="21488" citStr="Chiang, 2007" startWordPosition="3571" endWordPosition="3572">on that prefers the locally best component. These are simpler to implement and also performed competitively in their domain adaptation experiments. Unless explicitly noted otherwise, the results presented in Section 6 are based on linear mixture operation log-wsum, which empirically performed better than the log-wmax for ensemble tuning. 6 Experiments We evaluate the different methods on ArabicEnglish translation in single as well as multiple references scenario. Corpus statistics are shown in Table 1. For all the experiments in this paper, we use Kriya, our in-house Hierarchical phrasebased (Chiang, 2007) (Hiero) system, and integrated the required changes for ensemble decoding. Kriya performs comparably to the state of the art in phrase-based and hierarchical phrase-based translation over a wide variety of language pairs and data sets (Sankaran et al., 2012). We use PRO (Hopkins and May, 2011) for optimizing the feature weights and PMO-PRO (Duh et al., 2012) for optimizing meta weights, wherever applicable. In both cases, we use SVMRank (Joachims, 2006) as the optimizer. We used the default parameter settings for different MT tuning metrics. For METEOR, we tried both METEOR-tune and METEOR-ht</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<publisher>ACL.</publisher>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="22813" citStr="Denkowski and Lavie (2011)" startWordPosition="3793" endWordPosition="3796">as marginally better in METEOR3 and RIBES scores. We observed the margin of loss in BLEU and TER to outweigh the gains in METEOR and RIBES and we chose METEOR-hter setting for both optimization and evaluation of all our experiments. 6.1 Evaluation on Tuning Set Unlike conventional tuning methods, PMO (Duh et al., 2012) was originally evaluated on the tuning set to avoid potential mismatch with the test set. In order to ensure robustness of evaluation, they redecode the devset using the optimal weights from the last tuning iteration and report the scores on 1- 3This behaviour was also noted by Denkowski and Lavie (2011) in their analysis of Urdu-English system for tunable metrics task in WMT11. best candidates. Corpus Training size Tuning/ test set ISI corpus 1.1 M 1664/ 1313 (MTA) 1982/ 987 (ISI) Table 1: Corpus Statistics (# of sentences) for Arabic-English. MTA (4-refs) and ISI (1-ref). We follow the same strategy and compare our PMO-ensemble approach with PMO-PRO (denoted P) and a linear combination4 (denoted L) baseline. Similar to Duh et al. (2012), we use five different BLEU:RIBES weight settings, viz. (0.0,1.0), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3) and (1.0, 0.0), marked L1 through L5 or P1 through P5.</context>
<context position="33851" citStr="Denkowski and Lavie, 2011" startWordPosition="5657" endWordPosition="5660">lation to the system that generated it are removed to avoid annotator bias. In the end we collated post-edited translations for each system and then computed the system-level erence sentence. Our experiment shows that even with a single reference MMO methods can work. human-targeted (HBLEU, HMETEOR, HTER) scores, by using respective post-edited translations as the reference. First comparing the HTER (Snover et al., 2006) scores shown in Table 3, we see that the single-metric system optimized for METEOR performs slightly worse than the one optimized for BLEU, despite using METEOR-hter version (Denkowski and Lavie, 2011). Ensemble tuning-based system optimized for three metrics (BM-R) improves HTER by 4% and 6.3% over BLEU and METEOR optimized systems respectively. The single-metric system tuned with M-only setting scores high on HBLEU, closely followed by the ensemble system. We believe this to be caused by chance rather than any systematic gains by the Monly tuning; the ensemble system scores high on HMETEOR compared to the M-only system. While HTER captures the edit distance to the targeted reference, HMETEOR and HBLEU metrics capture missing content words or synonyms by exploiting n-grams and paraphrase m</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, Scotland, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Spyros Matsoukas</author>
</authors>
<title>Trait-based hypothesis selection for machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the ACL: Human Language Technologies,</booktitle>
<pages>528--532</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="6056" citStr="Devlin and Matsoukas, 2012" startWordPosition="942" endWordPosition="945"> depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the best metric used for tuning. For instance, (Mauser et al., 2008; Cer et al., 2010) report that tuning on linear combinations of BLEU-TER is more robust than a single metric like WER. The approach in (Devlin and Matsoukas, 2012) modifies the optimization function to include traits such as output length so that the hypotheses produced by the decoder have maximal score according to one metric (BLEU) but are subject to an output length constraint, e.g. that the output is 5% shorter. This is done by rescoring an N-best list (forest) for the metric combined with each trait condition and then the different trait hypothesis are combined using a system combination step. The traits are independent of the reference (while tuning). In contrast, our method is able to combine multiple metrics (each of which compares to the refere</context>
</contexts>
<marker>Devlin, Matsoukas, 2012</marker>
<rawString>Jacob Devlin and Spyros Matsoukas. 2012. Trait-based hypothesis selection for machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the ACL: Human Language Technologies, pages 528–532. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Xianchao Wu</author>
<author>Hajime Tsukada</author>
<author>Masaaki Nagata</author>
</authors>
<title>Learning to translate with multiple objectives.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the ACL, Jeju Island,</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="3499" citStr="Duh et al., 2012" startWordPosition="528" endWordPosition="531">l., 2011). The gains obtained by the MT system tuned on a particular metric do not improve performance as measured under other metrics (Cer et al., 2010), suggesting that over-fitting to a specific metric might happen without improvements in translation quality. In this paper we propose a new tuning framework for jointly optimizing multiple evaluation metrics. Pareto-optimality is a natural way to think about multi-metric optimization and multi-metric optimization (MMO) was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach (Duh et al., 2012). PMO provides several equivalent solutions (parameter weights) having different trade-offs between the different MT metrics. In (Duh et al., 2012) the choice of which option to use rests with the MT system developer and in that sense their approach is an a posteriori method to specify the preference (Marler and Arora, 2004). In contrast to this, our tuning framework provides a principled way of using the Pareto optimal options using ensemble decoding (Razmara et al., 2012). We also introduce a novel method of ensemble tuning for jointly tuning multiple MT evaluation metrics and further combin</context>
<context position="9491" citStr="Duh et al., 2012" startWordPosition="1530" endWordPosition="1533">ures of different models. 4 Multi-Metric Optimization In statistical MT, the multi-metric optimization problem can be expressed as: ( ) g [M1(H), ... , Mk(H)] (3) where H = N(f; w) where N(f; w) is the decoding function generating a set of candidate hypotheses H based on the model parameters w, for the source sentences f. For each source sentence fi ∈ f there is a set of candidate hypotheses {hi} ∈ H. The goal of the optimization is to find the weights that maximize the function g(.) parameterized by different evaluation metrics M1, ... , Mk. For the Pareto-optimal based approach such as PMO (Duh et al., 2012), we can replace g(·) above with gPMO(·) which returns the points in the Pareto frontier. Alternately a weighted averaging function gwavg(·) would result in a linear combination of the metrics being considered, where the tuning method would maximize the joint metric. This is similar to the (TER-BLEU)/2 optimization (Cer et al., 2010; Servan and Schwenk, 2011). We introduce four methods based on the above formulation and each method uses a different type of g(·) function for combining different metrics and we compare experimentally with existing methods. 4.1 PMO Ensemble PMO (Duh et al., 2012) </context>
<context position="21849" citStr="Duh et al., 2012" startWordPosition="3629" endWordPosition="3632">evaluate the different methods on ArabicEnglish translation in single as well as multiple references scenario. Corpus statistics are shown in Table 1. For all the experiments in this paper, we use Kriya, our in-house Hierarchical phrasebased (Chiang, 2007) (Hiero) system, and integrated the required changes for ensemble decoding. Kriya performs comparably to the state of the art in phrase-based and hierarchical phrase-based translation over a wide variety of language pairs and data sets (Sankaran et al., 2012). We use PRO (Hopkins and May, 2011) for optimizing the feature weights and PMO-PRO (Duh et al., 2012) for optimizing meta weights, wherever applicable. In both cases, we use SVMRank (Joachims, 2006) as the optimizer. We used the default parameter settings for different MT tuning metrics. For METEOR, we tried both METEOR-tune and METEOR-hter settings and found the latter to perform better in BLEU and TER scores, even though the former was marginally better in METEOR3 and RIBES scores. We observed the margin of loss in BLEU and TER to outweigh the gains in METEOR and RIBES and we chose METEOR-hter setting for both optimization and evaluation of all our experiments. 6.1 Evaluation on Tuning Set </context>
<context position="23256" citStr="Duh et al. (2012)" startWordPosition="3866" endWordPosition="3869">on, they redecode the devset using the optimal weights from the last tuning iteration and report the scores on 1- 3This behaviour was also noted by Denkowski and Lavie (2011) in their analysis of Urdu-English system for tunable metrics task in WMT11. best candidates. Corpus Training size Tuning/ test set ISI corpus 1.1 M 1664/ 1313 (MTA) 1982/ 987 (ISI) Table 1: Corpus Statistics (# of sentences) for Arabic-English. MTA (4-refs) and ISI (1-ref). We follow the same strategy and compare our PMO-ensemble approach with PMO-PRO (denoted P) and a linear combination4 (denoted L) baseline. Similar to Duh et al. (2012), we use five different BLEU:RIBES weight settings, viz. (0.0,1.0), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3) and (1.0, 0.0), marked L1 through L5 or P1 through P5. The Pareto frontier is then computed from 80 points (5 runs and 15 iterations per run) on the devset. Figure 2(a) shows the Pareto frontier of L and P baselines using BLEU and RIBES as two metrics. The frontier of the P dominates that of L for most part showing that the PMO approach benefits from picking Pareto points during the optimization. We use the PMO-ensemble approach to combine the optimized weights from the 5 tuning runs and re-d</context>
<context position="35061" citStr="Duh et al. (2012)" startWordPosition="5848" endWordPosition="5851">hrase matching. We also computed the regular variants (BLEU, METEOR and TER), which are scored against original references. The ensemble system outperformed the single-metric systems in all the three metrics. The improvements were also statistically significant at p-value of 0.05 for BLEU and TER. 7 Conclusion We propose and present a comprehensive study of several multi-metric optimization (MMO) methods in SMT. First, by exploiting the idea of ensemble decoding (Razmara et al., 2012), we propose an effective way to combine multiple Pareto-optimal model weights from previous MMO methods (e.g. Duh et al. (2012)), obviating the need for manually trading off among metrics. We also proposed two new variants: lateen-style MMO and union of metrics. We also extended ensemble decoding to a new tuning algorithm called ensemble tuning. This method demonstrates statistically significant gains for BLEU and RIBES with modest reduction in METEOR and TER. Further, in our human evaluation, ensemble tuning obtains the best HTER among competing baselines, confirming that optimizing on multiple metrics produces human-preferred translations compared to the conventional optimization approach involving a single metric. </context>
</contexts>
<marker>Duh, Sudoh, Wu, Tsukada, Nagata, 2012</marker>
<rawString>Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime Tsukada, and Masaaki Nagata. 2012. Learning to translate with multiple objectives. In Proceedings of the 50th Annual Meeting of the ACL, Jeju Island, Korea. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Hendra Setiawan</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>The university of maryland statistical machine translation system for the fourth workshop on machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of the Fourth Workshop on Machine Translation.</booktitle>
<contexts>
<context position="5519" citStr="Dyer et al., 2009" startWordPosition="846" endWordPosition="849">et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the best metric used for tuning. For instance, (Mauser et al., 2008; Cer et al., 2010) report that tuning on linear combinations of BLEU-TER is more robust than a single metric like WER. The approach in (Devlin and Matsoukas, 2012) modifies the optimization function to include traits such as o</context>
</contexts>
<marker>Dyer, Setiawan, Marton, Resnik, 2009</marker>
<rawString>Chris Dyer, Hendra Setiawan, Yuval Marton, and Philip Resnik. 2009. The university of maryland statistical machine translation system for the fourth workshop on machine translation. In Proc. of the Fourth Workshop on Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Structured ramp loss minimization for machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the ACL: Human Language Technologies,</booktitle>
<pages>221--231</pages>
<publisher>ACL.</publisher>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="5310" citStr="Gimpel and Smith, 2012" startWordPosition="811" endWordPosition="814">g task. HTER (Snover et al., 2006) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output. 2 Related Work In grammar induction and parsing (Spitkovsky et al., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the best metric used for tuning. For instance, (Mauser et al., 2008; Cer et al., 2010</context>
</contexts>
<marker>Gimpel, Smith, 2012</marker>
<rawString>Kevin Gimpel and Noah A. Smith. 2012. Structured ramp loss minimization for machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the ACL: Human Language Technologies, pages 221–231, Montr´eal, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Hall</author>
<author>Ryan T McDonald</author>
<author>Jason Katz-Brown</author>
<author>Michael Ringgaard</author>
</authors>
<title>Training dependency parsers by jointly optimizing multiple objectives.</title>
<date>2011</date>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing,</booktitle>
<pages>1489--1499</pages>
<contexts>
<context position="4914" citStr="Hall et al., 2011" startWordPosition="754" endWordPosition="757">e three other approaches for multi-metric tuning and compare their performance to the ensemble tuning. Our experiments yield the highest metric scores across many different metrics (that are being optimized), something that has not been possible until now. Our ensemble tuning method over multiple metrics produced superior translations than single metric tuning as measured by a post-editing task. HTER (Snover et al., 2006) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output. 2 Related Work In grammar induction and parsing (Spitkovsky et al., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al.,</context>
</contexts>
<marker>Hall, McDonald, Katz-Brown, Ringgaard, 2011</marker>
<rawString>Keith Hall, Ryan T. McDonald, Jason Katz-Brown, and Michael Ringgaard. 2011. Training dependency parsers by jointly optimizing multiple objectives. In Proceedings of the Empirical Methods in Natural Language Processing, pages 1489–1499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Li Deng</author>
</authors>
<title>Maximum expected bleu training of phrase and lexicon translation models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>292--301</pages>
<publisher>ACL.</publisher>
<location>Jeju Island,</location>
<contexts>
<context position="5251" citStr="He and Deng, 2012" startWordPosition="802" endWordPosition="805"> than single metric tuning as measured by a post-editing task. HTER (Snover et al., 2006) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output. 2 Related Work In grammar induction and parsing (Spitkovsky et al., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the best metric used for t</context>
</contexts>
<marker>He, Deng, 2012</marker>
<rawString>Xiaodong He and Li Deng. 2012. Maximum expected bleu training of phrase and lexicon translation models. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 292–301, Jeju Island, Korea. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yifan He</author>
<author>Andy Way</author>
</authors>
<title>Improving the objective function in minimum error rate training.</title>
<date>2009</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="5670" citStr="He and Way, 2009" startWordPosition="872" endWordPosition="875">SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the best metric used for tuning. For instance, (Mauser et al., 2008; Cer et al., 2010) report that tuning on linear combinations of BLEU-TER is more robust than a single metric like WER. The approach in (Devlin and Matsoukas, 2012) modifies the optimization function to include traits such as output length so that the hypotheses produced by the decoder have maximal score according to one metric (BLEU) but are subject to an output length const</context>
</contexts>
<marker>He, Way, 2009</marker>
<rawString>Yifan He and Andy Way. 2009. Improving the objective function in minimum error rate training. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>ACL.</publisher>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="1769" citStr="Hopkins and May, 2011" startWordPosition="264" endWordPosition="267">imizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science &amp; Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in tran</context>
<context position="21783" citStr="Hopkins and May, 2011" startWordPosition="3617" endWordPosition="3620">rformed better than the log-wmax for ensemble tuning. 6 Experiments We evaluate the different methods on ArabicEnglish translation in single as well as multiple references scenario. Corpus statistics are shown in Table 1. For all the experiments in this paper, we use Kriya, our in-house Hierarchical phrasebased (Chiang, 2007) (Hiero) system, and integrated the required changes for ensemble decoding. Kriya performs comparably to the state of the art in phrase-based and hierarchical phrase-based translation over a wide variety of language pairs and data sets (Sankaran et al., 2012). We use PRO (Hopkins and May, 2011) for optimizing the feature weights and PMO-PRO (Duh et al., 2012) for optimizing meta weights, wherever applicable. In both cases, we use SVMRank (Joachims, 2006) as the optimizer. We used the default parameter settings for different MT tuning metrics. For METEOR, we tried both METEOR-tune and METEOR-hter settings and found the latter to perform better in BLEU and TER scores, even though the former was marginally better in METEOR3 and RIBES scores. We observed the margin of loss in BLEU and TER to outweigh the gains in METEOR and RIBES and we chose METEOR-hter setting for both optimization an</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, Edinburgh, Scotland. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
</authors>
<title>Automatic evaluation of translation quality for distant language pairs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>944--952</pages>
<publisher>ACL.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="2600" citStr="Isozaki et al., 2010" startWordPosition="391" endWordPosition="394">cally use BLEU and Kevin Duh Nara Institute of Science &amp; Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 2009) to modelling semantic similarity through textual entailment (Pad´o et al., 2009) to RIBES, an evaluation metric that pays attention to long-distance reordering (Isozaki et al., 2010). While some of these metrics such as TER, METEOR are gaining prominence, BLEU enjoys the status of being the de facto standard tuning metric as it is often claimed and sometimes observed that optimizing with BLEU produces better translations than other metrics (Callison-Burch et al., 2011). The gains obtained by the MT system tuned on a particular metric do not improve performance as measured under other metrics (Cer et al., 2010), suggesting that over-fitting to a specific metric might happen without improvements in translation quality. In this paper we propose a new tuning framework for joi</context>
</contexts>
<marker>Isozaki, Hirao, Duh, Sudoh, Tsukada, 2010</marker>
<rawString>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Automatic evaluation of translation quality for distant language pairs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 944– 952, Cambridge, MA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear svms in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>217--226</pages>
<contexts>
<context position="21946" citStr="Joachims, 2006" startWordPosition="3647" endWordPosition="3648">s scenario. Corpus statistics are shown in Table 1. For all the experiments in this paper, we use Kriya, our in-house Hierarchical phrasebased (Chiang, 2007) (Hiero) system, and integrated the required changes for ensemble decoding. Kriya performs comparably to the state of the art in phrase-based and hierarchical phrase-based translation over a wide variety of language pairs and data sets (Sankaran et al., 2012). We use PRO (Hopkins and May, 2011) for optimizing the feature weights and PMO-PRO (Duh et al., 2012) for optimizing meta weights, wherever applicable. In both cases, we use SVMRank (Joachims, 2006) as the optimizer. We used the default parameter settings for different MT tuning metrics. For METEOR, we tried both METEOR-tune and METEOR-hter settings and found the latter to perform better in BLEU and TER scores, even though the former was marginally better in METEOR3 and RIBES scores. We observed the margin of loss in BLEU and TER to outweigh the gains in METEOR and RIBES and we chose METEOR-hter setting for both optimization and evaluation of all our experiments. 6.1 Evaluation on Tuning Set Unlike conventional tuning methods, PMO (Duh et al., 2012) was originally evaluated on the tuning</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear svms in linear time. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 217–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Michael J Denkowski</author>
</authors>
<title>The meteor metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="2417" citStr="Lavie and Denkowski, 2009" startWordPosition="364" endWordPosition="367">azrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science &amp; Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 2009) to modelling semantic similarity through textual entailment (Pad´o et al., 2009) to RIBES, an evaluation metric that pays attention to long-distance reordering (Isozaki et al., 2010). While some of these metrics such as TER, METEOR are gaining prominence, BLEU enjoys the status of being the de facto standard tuning metric as it is often claimed and sometimes observed that optimizing with BLEU produces better translations than other metrics (Callison-Burch et al., 2011). The gains obtained by the MT system tuned on a particular metric do not improve performance as measured under other metrics </context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>Alon Lavie and Michael J. Denkowski. 2009. The meteor metric for automatic evaluation of machine translation. Machine Translation, 23(2-3):105–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Liu</author>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better evaluation metrics lead to better machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5718" citStr="Liu et al., 2011" startWordPosition="881" endWordPosition="884"> recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the best metric used for tuning. For instance, (Mauser et al., 2008; Cer et al., 2010) report that tuning on linear combinations of BLEU-TER is more robust than a single metric like WER. The approach in (Devlin and Matsoukas, 2012) modifies the optimization function to include traits such as output length so that the hypotheses produced by the decoder have maximal score according to one metric (BLEU) but are subject to an output length constraint, e.g. that the output is 5% shorter. This </context>
</contexts>
<marker>Liu, Dahlmeier, Ng, 2011</marker>
<rawString>Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011. Better evaluation metrics lead to better machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T Marler</author>
<author>J S Arora</author>
</authors>
<title>Survey of multi-objective optimization methods for engineering.</title>
<date>2004</date>
<journal>Structural and Multidisciplinary Optimization,</journal>
<volume>26</volume>
<issue>6</issue>
<contexts>
<context position="3825" citStr="Marler and Arora, 2004" startWordPosition="583" endWordPosition="586">ly optimizing multiple evaluation metrics. Pareto-optimality is a natural way to think about multi-metric optimization and multi-metric optimization (MMO) was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach (Duh et al., 2012). PMO provides several equivalent solutions (parameter weights) having different trade-offs between the different MT metrics. In (Duh et al., 2012) the choice of which option to use rests with the MT system developer and in that sense their approach is an a posteriori method to specify the preference (Marler and Arora, 2004). In contrast to this, our tuning framework provides a principled way of using the Pareto optimal options using ensemble decoding (Razmara et al., 2012). We also introduce a novel method of ensemble tuning for jointly tuning multiple MT evaluation metrics and further combine this with the PMO ap947 Proceedings of NAACL-HLT 2013, pages 947–957, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics proach (Duh et al., 2012). We also introduce three other approaches for multi-metric tuning and compare their performance to the ensemble tuning. Our experiments yield the</context>
</contexts>
<marker>Marler, Arora, 2004</marker>
<rawString>R. T. Marler and J. S. Arora. 2004. Survey of multi-objective optimization methods for engineering. Structural and Multidisciplinary Optimization, 26(6):369–395, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Saˇsa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Automatic evaluation measures for statistical machine translation system optimization.</title>
<date>2008</date>
<booktitle>In International Conference on Language Resources and Evaluation,</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="5892" citStr="Mauser et al., 2008" startWordPosition="913" endWordPosition="916">ve objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the best metric used for tuning. For instance, (Mauser et al., 2008; Cer et al., 2010) report that tuning on linear combinations of BLEU-TER is more robust than a single metric like WER. The approach in (Devlin and Matsoukas, 2012) modifies the optimization function to include traits such as output length so that the hypotheses produced by the decoder have maximal score according to one metric (BLEU) but are subject to an output length constraint, e.g. that the output is 5% shorter. This is done by rescoring an N-best list (forest) for the metric combined with each trait condition and then the different trait hypothesis are combined using a system combination</context>
</contexts>
<marker>Mauser, Hasan, Ney, 2008</marker>
<rawString>Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2008. Automatic evaluation measures for statistical machine translation system optimization. In International Conference on Language Resources and Evaluation, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL,</booktitle>
<pages>160--167</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1586" citStr="Och, 2003" startWordPosition="235" endWordPosition="236">is contrasts the traditional tuning where gains are usually limited to a single metric. Our human evaluation results confirm that in order to produce better MT output, optimizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science &amp; Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging a</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the ACL, pages 160–167. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Daniel Cer</author>
<author>Michel Galley</author>
<author>Dan Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Measuring machine translation quality as semantic equivalence: A metric based on entailment features.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<marker>Pad´o, Cer, Galley, Jurafsky, Manning, 2009</marker>
<rawString>Sebastian Pad´o, Daniel Cer, Michel Galley, Dan Jurafsky, and Christopher D. Manning. 2009. Measuring machine translation quality as semantic equivalence: A metric based on entailment features. Machine Translation, 23(2-3):181–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WieJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofAssociation of Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1508" citStr="Papineni et al., 2002" startWordPosition="221" endWordPosition="224">ross several metrics (BLEU, RIBES), without any significant reduction in other metrics. This contrasts the traditional tuning where gains are usually limited to a single metric. Our human evaluation results confirm that in order to produce better MT output, optimizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science &amp; Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WieJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings ofAssociation of Computational Linguistics, pages 311–318. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Razmara</author>
<author>George Foster</author>
<author>Baskaran Sankaran</author>
<author>Anoop Sarkar</author>
</authors>
<title>Mixing multiple translation models in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the ACL, Jeju,</booktitle>
<publisher>ACL.</publisher>
<location>Republic of</location>
<contexts>
<context position="3977" citStr="Razmara et al., 2012" startWordPosition="608" endWordPosition="611"> was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach (Duh et al., 2012). PMO provides several equivalent solutions (parameter weights) having different trade-offs between the different MT metrics. In (Duh et al., 2012) the choice of which option to use rests with the MT system developer and in that sense their approach is an a posteriori method to specify the preference (Marler and Arora, 2004). In contrast to this, our tuning framework provides a principled way of using the Pareto optimal options using ensemble decoding (Razmara et al., 2012). We also introduce a novel method of ensemble tuning for jointly tuning multiple MT evaluation metrics and further combine this with the PMO ap947 Proceedings of NAACL-HLT 2013, pages 947–957, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics proach (Duh et al., 2012). We also introduce three other approaches for multi-metric tuning and compare their performance to the ensemble tuning. Our experiments yield the highest metric scores across many different metrics (that are being optimized), something that has not been possible until now. Our ensemble tuning met</context>
<context position="7450" citStr="Razmara et al., 2012" startWordPosition="1180" endWordPosition="1183">-metric tuning, where the linear combination weights do not need to be known in advance. This is advantageous because the optimal weighting may not be known in advance. However, the notion of Pareto optimality implies that multiple ”best” solutions may exist, so the MT system developer may be forced to make a choice after tuning. These approaches require the MT system developer to make a choice either before tuning (e.g. in terms of linear combination weights) or afterwards (e.g. the Pareto approach). Our method here is different in that we do not require any choice. We use ensemble decoding (Razmara et al., 2012) (see sec 3) to combine the different solutions resulting from the multi-metric optimization, providing an elegant solution for deployment. We extend this idea further and introduce ensemble tuning, where the metrics have separate set of weights. The tuning process alternates between ensemble decoding and the update step where the weights for each metric are optimized separately followed by joint update of metric (meta) weights. 3 Ensemble Decoding We now briefly review ensemble decoding (Razmara et al., 2012) which is used as a component in the algorithms we present. The prevalent model of st</context>
<context position="20482" citStr="Razmara et al. (2012)" startWordPosition="3408" endWordPosition="3411">r metrics and the latter is an ensemble model over Pareto solutions. Additionally, PMO ensemble uses the notion of ensembles only for the final decoding after tuning has completed. 5.1 Implementation Notes All the proposed methods fit naturally within the usual SMT tuning framework. However, some changes are required in the decoder to support ensemble decoding and in the tuning scripts for optimizing with multiple metrics. For ensemble decoding, the decoder should be able to use multiple weight vectors and dynamically combine them according to some desired mixture operation. Note that, unlike Razmara et al. (2012), our approach uses just one model but has different weight vectors for each metric and the required decoder modifications are simpler than full ensemble decoding. While any of the mixture operations proposed by Razmara et al. (2012) could be used, in this pag ({Mi(Hens)|1≤i≤k} ; w∗) (7) 951 per we use log-wsum – the linear combination of the ensemble components and log-wmax – the combination that prefers the locally best component. These are simpler to implement and also performed competitively in their domain adaptation experiments. Unless explicitly noted otherwise, the results presented in</context>
<context position="34933" citStr="Razmara et al., 2012" startWordPosition="5827" endWordPosition="5830">tance to the targeted reference, HMETEOR and HBLEU metrics capture missing content words or synonyms by exploiting n-grams and paraphrase matching. We also computed the regular variants (BLEU, METEOR and TER), which are scored against original references. The ensemble system outperformed the single-metric systems in all the three metrics. The improvements were also statistically significant at p-value of 0.05 for BLEU and TER. 7 Conclusion We propose and present a comprehensive study of several multi-metric optimization (MMO) methods in SMT. First, by exploiting the idea of ensemble decoding (Razmara et al., 2012), we propose an effective way to combine multiple Pareto-optimal model weights from previous MMO methods (e.g. Duh et al. (2012)), obviating the need for manually trading off among metrics. We also proposed two new variants: lateen-style MMO and union of metrics. We also extended ensemble decoding to a new tuning algorithm called ensemble tuning. This method demonstrates statistically significant gains for BLEU and RIBES with modest reduction in METEOR and TER. Further, in our human evaluation, ensemble tuning obtains the best HTER among competing baselines, confirming that optimizing on multi</context>
</contexts>
<marker>Razmara, Foster, Sankaran, Sarkar, 2012</marker>
<rawString>Majid Razmara, George Foster, Baskaran Sankaran, and Anoop Sarkar. 2012. Mixing multiple translation models in statistical machine translation. In Proceedings of the 50th Annual Meeting of the ACL, Jeju, Republic of Korea. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baskaran Sankaran</author>
<author>Majid Razmara</author>
<author>Anoop Sarkar</author>
</authors>
<title>Kriya an end-to-end hierarchical phrase-based mt system.</title>
<date>2012</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics (PBML),</booktitle>
<pages>97--97</pages>
<contexts>
<context position="21747" citStr="Sankaran et al., 2012" startWordPosition="3610" endWordPosition="3613">ation log-wsum, which empirically performed better than the log-wmax for ensemble tuning. 6 Experiments We evaluate the different methods on ArabicEnglish translation in single as well as multiple references scenario. Corpus statistics are shown in Table 1. For all the experiments in this paper, we use Kriya, our in-house Hierarchical phrasebased (Chiang, 2007) (Hiero) system, and integrated the required changes for ensemble decoding. Kriya performs comparably to the state of the art in phrase-based and hierarchical phrase-based translation over a wide variety of language pairs and data sets (Sankaran et al., 2012). We use PRO (Hopkins and May, 2011) for optimizing the feature weights and PMO-PRO (Duh et al., 2012) for optimizing meta weights, wherever applicable. In both cases, we use SVMRank (Joachims, 2006) as the optimizer. We used the default parameter settings for different MT tuning metrics. For METEOR, we tried both METEOR-tune and METEOR-hter settings and found the latter to perform better in BLEU and TER scores, even though the former was marginally better in METEOR3 and RIBES scores. We observed the margin of loss in BLEU and TER to outweigh the gains in METEOR and RIBES and we chose METEOR-h</context>
</contexts>
<marker>Sankaran, Razmara, Sarkar, 2012</marker>
<rawString>Baskaran Sankaran, Majid Razmara, and Anoop Sarkar. 2012. Kriya an end-to-end hierarchical phrase-based mt system. The Prague Bulletin of Mathematical Linguistics (PBML), 97(97):83–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christophe Servan</author>
<author>Holger Schwenk</author>
</authors>
<title>Optimising multiple metrics with mert.</title>
<date>2011</date>
<journal>Prague Bull. Math. Linguistics,</journal>
<pages>96--109</pages>
<contexts>
<context position="5546" citStr="Servan and Schwenk, 2011" startWordPosition="850" endWordPosition="853">and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the best metric used for tuning. For instance, (Mauser et al., 2008; Cer et al., 2010) report that tuning on linear combinations of BLEU-TER is more robust than a single metric like WER. The approach in (Devlin and Matsoukas, 2012) modifies the optimization function to include traits such as output length so that the hy</context>
<context position="9852" citStr="Servan and Schwenk, 2011" startWordPosition="1586" endWordPosition="1589"> f there is a set of candidate hypotheses {hi} ∈ H. The goal of the optimization is to find the weights that maximize the function g(.) parameterized by different evaluation metrics M1, ... , Mk. For the Pareto-optimal based approach such as PMO (Duh et al., 2012), we can replace g(·) above with gPMO(·) which returns the points in the Pareto frontier. Alternately a weighted averaging function gwavg(·) would result in a linear combination of the metrics being considered, where the tuning method would maximize the joint metric. This is similar to the (TER-BLEU)/2 optimization (Cer et al., 2010; Servan and Schwenk, 2011). We introduce four methods based on the above formulation and each method uses a different type of g(·) function for combining different metrics and we compare experimentally with existing methods. 4.1 PMO Ensemble PMO (Duh et al., 2012) seeks to maximize the number of points in the Pareto frontier of the metrics considered. The inner routine of the PMO-PRO tuning is described in Algorithm 1. This routine is contained within an outer loop that iterates for a fixed number iterations of decoding the tuning set and optimizing the weights. The tuning process with PMO-PRO is independently repeated</context>
<context position="15472" citStr="Servan and Schwenk, 2011" startWordPosition="2548" endWordPosition="2552">-tuning with as many metrics) and then decode the test set with an ensemble of these weights as in PMO ensemble. However in practice we find the weights to converge and we simply use the weights from the final iteration to decode the test set in our lateen experiments. 4.3 Union of Metrics At each iteration lateen MMO excludes all but one metric for optimization. An alternative would be to consider all the metrics at each iteration so that the optimizer could try to optimize them jointly. This has been the general motivation for considering the linear combination of metrics (Cer et al., 2010; Servan and Schwenk, 2011) resulting in a joint metric, which is then optimized. However due to the scaling differences between the scores of different metrics, the linear combination might completely suppress the metric having scores in the lower-range. As an example, the RIBES scores that are typically in the high 0.7-0.8 range, dominate the BLEU scores that is typically around 0.3. While the weighted linear combination tries to address this imbalance, they introduce additional parameters that are manually fixed and not separately tuned. We avoid this linear combination pitfall by taking the union of the metrics unde</context>
</contexts>
<marker>Servan, Schwenk, 2011</marker>
<rawString>Christophe Servan and Holger Schwenk. 2011. Optimising multiple metrics with mert. Prague Bull. Math. Linguistics, 96:109–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Simianer</author>
<author>Stefan Riezler</author>
<author>Chris Dyer</author>
</authors>
<title>Joint feature selection in distributed stochastic learning for large-scale discriminative training in smt.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>11--21</pages>
<publisher>ACL.</publisher>
<location>Jeju Island,</location>
<contexts>
<context position="5231" citStr="Simianer et al., 2012" startWordPosition="798" endWordPosition="801">d superior translations than single metric tuning as measured by a post-editing task. HTER (Snover et al., 2006) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output. 2 Related Work In grammar induction and parsing (Spitkovsky et al., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the be</context>
</contexts>
<marker>Simianer, Riezler, Dyer, 2012</marker>
<rawString>Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012. Joint feature selection in distributed stochastic learning for large-scale discriminative training in smt. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11–21, Jeju Island, Korea. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation forMachine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="2331" citStr="Snover et al., 2006" startWordPosition="351" endWordPosition="354">., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science &amp; Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 2009) to modelling semantic similarity through textual entailment (Pad´o et al., 2009) to RIBES, an evaluation metric that pays attention to long-distance reordering (Isozaki et al., 2010). While some of these metrics such as TER, METEOR are gaining prominence, BLEU enjoys the status of being the de facto standard tuning metric as it is often claimed and sometimes observed that optimizing with BLEU produces better translations than other metrics (Callison-Burch et al., 2011). The gains obtained by the MT system tu</context>
<context position="4722" citStr="Snover et al., 2006" startWordPosition="723" endWordPosition="726">his with the PMO ap947 Proceedings of NAACL-HLT 2013, pages 947–957, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics proach (Duh et al., 2012). We also introduce three other approaches for multi-metric tuning and compare their performance to the ensemble tuning. Our experiments yield the highest metric scores across many different metrics (that are being optimized), something that has not been possible until now. Our ensemble tuning method over multiple metrics produced superior translations than single metric tuning as measured by a post-editing task. HTER (Snover et al., 2006) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output. 2 Related Work In grammar induction and parsing (Spitkovsky et al., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et a</context>
<context position="33649" citStr="Snover et al., 2006" startWordPosition="5624" endWordPosition="5627">sing the Post-Editing Tool: PET (Aziz et al., 2012). The annotators were not Arabic-literate and relied only on the reference translations during post-editing. The identifiers that link each translation to the system that generated it are removed to avoid annotator bias. In the end we collated post-edited translations for each system and then computed the system-level erence sentence. Our experiment shows that even with a single reference MMO methods can work. human-targeted (HBLEU, HMETEOR, HTER) scores, by using respective post-edited translations as the reference. First comparing the HTER (Snover et al., 2006) scores shown in Table 3, we see that the single-metric system optimized for METEOR performs slightly worse than the one optimized for BLEU, despite using METEOR-hter version (Denkowski and Lavie, 2011). Ensemble tuning-based system optimized for three metrics (BM-R) improves HTER by 4% and 6.3% over BLEU and METEOR optimized systems respectively. The single-metric system tuned with M-only setting scores high on HBLEU, closely followed by the ensemble system. We believe this to be caused by chance rather than any systematic gains by the Monly tuning; the ensemble system scores high on HMETEOR </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAssociation forMachine Translation in the Americas, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction.</title>
<date>2011</date>
<journal>Association of Computational Linguistics.</journal>
<booktitle>In Proceedings of the Empirical Methods in Natural Language Processing,</booktitle>
<pages>1269--1280</pages>
<contexts>
<context position="4895" citStr="Spitkovsky et al., 2011" startWordPosition="750" endWordPosition="753">, 2012). We also introduce three other approaches for multi-metric tuning and compare their performance to the ensemble tuning. Our experiments yield the highest metric scores across many different metrics (that are being optimized), something that has not been possible until now. Our ensemble tuning method over multiple metrics produced superior translations than single metric tuning as measured by a post-editing task. HTER (Snover et al., 2006) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output. 2 Related Work In grammar induction and parsing (Spitkovsky et al., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan,</context>
<context position="13580" citStr="Spitkovsky et al., 2011" startWordPosition="2230" endWordPosition="2233">two metrics independently and the dashed (green) arrow optimize them jointly. The Pareto frontier is indicated by the curve. inal PMO-PRO seeks to maximize the points on the Pareto frontier (blue curve in the figure) leading to Pareto-optimal solutions. On the other hand, the PMO ensemble combines the different Paretooptimal solutions and potentially moving in the direction of dashed (green) arrows to some point that has higher score in either or both dimensions. 4.2 Lateen MMO Lateen EM has been proposed as a way of jointly optimizing multiple objectives in the context of dependency parsing (Spitkovsky et al., 2011). It uses a secondary hard EM objective to move away, when the primary soft EM objective gets stuck in a local optima. The course correction could be performed under different conditions leading to variations that are based on when and how often to shift from one objective function to another during optimization. The lateen technique can be applied to the multimetric optimization in SMT by treating the different metrics as different objective functions. While the several lateen variants are also applicable for our task, our objective here is to improve performance across the different metrics </context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2011</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2011. Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction. In Proceedings of the Empirical Methods in Natural Language Processing, pages 1269–1280. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>764--773</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1718" citStr="Watanabe et al., 2007" startWordPosition="255" endWordPosition="258">irm that in order to produce better MT output, optimizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science &amp; Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover </context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 764– 773. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
</authors>
<title>Optimized online rank learning for machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the ACL: Human Language Technologies,</booktitle>
<pages>253--262</pages>
<publisher>ACL.</publisher>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1839" citStr="Watanabe, 2012" startWordPosition="276" endWordPosition="277">uning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science &amp; Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 2009) to modelling semantic</context>
<context position="5160" citStr="Watanabe, 2012" startWordPosition="790" endWordPosition="791">il now. Our ensemble tuning method over multiple metrics produced superior translations than single metric tuning as measured by a post-editing task. HTER (Snover et al., 2006) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output. 2 Related Work In grammar induction and parsing (Spitkovsky et al., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al.,</context>
</contexts>
<marker>Watanabe, 2012</marker>
<rawString>Taro Watanabe. 2012. Optimized online rank learning for machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the ACL: Human Language Technologies, pages 253–262, Montr´eal, Canada, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
</authors>
<title>Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--79</pages>
<contexts>
<context position="5500" citStr="Zaidan, 2009" startWordPosition="844" endWordPosition="845">., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the best metric used for tuning. For instance, (Mauser et al., 2008; Cer et al., 2010) report that tuning on linear combinations of BLEU-TER is more robust than a single metric like WER. The approach in (Devlin and Matsoukas, 2012) modifies the optimization function to inclu</context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Omar F. Zaidan. 2009. Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91:79–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daqi Zheng</author>
<author>Yifan He</author>
<author>Yang Liu</author>
<author>Qun Liu</author>
</authors>
<title>Maximum rank correlation training for statistical machine translation.</title>
<date>2012</date>
<booktitle>In MT Summit XIII.</booktitle>
<contexts>
<context position="5330" citStr="Zheng et al., 2012" startWordPosition="815" endWordPosition="818">al., 2006) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output. 2 Related Work In grammar induction and parsing (Spitkovsky et al., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009). Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the best metric used for tuning. For instance, (Mauser et al., 2008; Cer et al., 2010) report that tuning</context>
</contexts>
<marker>Zheng, He, Liu, Liu, 2012</marker>
<rawString>Daqi Zheng, Yifan He, Yang Liu, and Qun Liu. 2012. Maximum rank correlation training for statistical machine translation. In MT Summit XIII.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>