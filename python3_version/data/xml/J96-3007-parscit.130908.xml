<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999242333333333">
A Chart Re-estimation Algorithm for a
Probabilistic Recursive Transition
Network
</title>
<author confidence="0.999519">
Young S. Han. Key-Sun Choit
</author>
<affiliation confidence="0.771273">
University of Suwon Korea Advanced Institute of Science and
Technology
</affiliation>
<bodyText confidence="0.982874285714286">
A Probabilistic Recursive Transition Network is an elevated version of a Recursive Transition
Network used to model and process context-free languages in stochastic parameters. We present
a re-estimation algorithm for training probabilistic parameters, and show how efficiently it can
be implemented using charts. The complexity of the Outside algorithm we present is 0(N4G3)
where N is the input size and G is the number of states. This complexity can be significantly
overcome when the redundant computations are avoided. Experiments on the Penn tree corpus
show that re-estimation can be done more efficiently with charts.
</bodyText>
<sectionHeader confidence="0.985795" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9998895625">
Though hidden Markov models have been successful in some applications such as
corpus tagging, they are limited to the problems of regular languages. There have
been attempts to associate probabilities with context-free grammar formalisms. Re-
cently Briscoe and Carroll (1993) have reported work on generalized probabilistic LR
parsing, and others have tried different formalisms such as LTAG (Schabes, Roth, and
Osborne 1993) and Link grammar (Lafferty, Sleator, and Temperley 1992). Kupiec ex-
tended a SCFG that worked on CNF to a general CFG (Kupiec 1991). The re-estimation
algorithm presented in this paper may be seen as another version for general CFG.
One significant problem of most probabilistic approaches is the computational
burden of estimating the parameters (Lan i and Young 1990). In this paper, we consider
a probabilistic recursive transition network (PRTN) as an underlying grammar rep-
resentation, and present an algorithm for training the probabilistic parameters, then
suggest an improved version that works with reduced redundant computations. The
key point is to save intermediate results and avoid the same computation later on.
Moreover, the computation of Outside probabilities can be made only on the valid
parse space once a chart is prepared.
</bodyText>
<sectionHeader confidence="0.812346" genericHeader="method">
2. A Probabilistic Recursive Transition Network
</sectionHeader>
<bodyText confidence="0.76228">
A PRTN denoted by A is a 6-tuple.
= (A, 8, S , ,F , LT).
</bodyText>
<note confidence="0.911484">
* Computer Science Department, University of Suwon, Suwon P.O. Box 77-78, Suwon, 440-600, Korea.
</note>
<email confidence="0.955359">
E-mail: yshan@world.kaist.ac.kr
</email>
<note confidence="0.905889">
-1 Computer Science Department, Korea Advanced Institute of Science and Technology, Taejon, 305-701,
Korea. E-mail: kschoi@csking.kaist.ac.kr
</note>
<figure confidence="0.952991636363636">
C) 1996 Association for Computational Linguistics
Computational Linguistics Volume 22, Number 3
NP art AP noun
NP AP noun
NP noun
AP adj AP
AP adj
noun 0.2
iT\ AP _C) noun
1.0
art a,
the : 0.6
10
(..D ad] 10 AP 07
CFG
PRTN
Call
States
Return
States
0.8
o.
</figure>
<figureCaption confidence="0.993024">
Figure 1
</figureCaption>
<bodyText confidence="0.941122666666667">
Illustration of PRTN. A parse is composed of dark-headed transitions.
A is a transition matrix containing transition probabilities, and B is a word matrix
containing the probability distribution of the words observable at each terminal tran-
sition. F specifies the types of transitions, and E represents a stack. S and F denote
start and final states, respectively.
Stack operations are associated with transitions; transitions are classified into three
types, according to the stack operation. The first type is nonterminal transition, in
which state identification is pushed into the stack. The second type is pop transition, in
which transition is determined by the content of the stack. The third type is transitions
not committed to stack operation; these are terminal and empty transitions. In general,
the grammar expressed in PRTN consists of layers. A layer is a fragment of network
that corresponds to a nonterminal. A table of the probability distribution of words is
defined at each terminal transition. Pop transitions represent the returning of a layer
to one of its (possibly multiple) higher layers.
In this paper, parses are assumed to be sequences of dark-headed transitions (see
Figure 1). States at which pop transitions are defined are called pop states. Other
notations are listed below.
first(1) returns the first state of layer 1.
</bodyText>
<listItem confidence="0.7219485">
last(1) returns the last state of layer 1.
layer(s) returns the layer state s belongs to.
bout(1) returns the states from which layer / branches out.
bin(1) returns the states to which layer / returns.
terminal(1) returns a set of terminal edges in layer I.
nonterminal(1) returns a set of nonterminal edges in layer 1.
</listItem>
<bodyText confidence="0.413972333333333">
ij denotes the edge between states i and j.
[0] denotes the network segment between states i and j.
Wnb is a word sequence covering the ath to bill word.
</bodyText>
<page confidence="0.995981">
422
</page>
<note confidence="0.866101">
Han and Choi A Chart Re-estimation Algorithm
</note>
<sectionHeader confidence="0.949247" genericHeader="method">
3. Re-estimation Algorithm
</sectionHeader>
<bodyText confidence="0.999605428571429">
The task of a re-estimation algorithm is to assign probabilities to transitions and the
word symbols defined at each terminal transition. The Inside-Outside algorithm pro-
vides a formal basis for estimating parameters of context free languages so that the
probabilities of the word sequences (sample sentences) may be maximized. The re-
estimation algorithm for PRTN uses a variation of the Inside-Outside algorithm cus-
tomized for PRTN.
Let a word sequence of length N be denoted by:
</bodyText>
<equation confidence="0.986478">
W = W1 W2 • WN •
</equation>
<bodyText confidence="0.990381">
Now define the Inside probability.
</bodyText>
<subsectionHeader confidence="0.710954">
Definition 1
</subsectionHeader>
<bodyText confidence="0.9986145">
The Inside probability denoted by /3/(i)s_t of state i is the probability that layer(i)
generates the string positioned from s to t starting at state i given a model A.
</bodyText>
<equation confidence="0.9725825">
That is:
Pi(i)s-t = P([i,e] Ws-t1A)
</equation>
<bodyText confidence="0.971717">
where e last(layer(i)). And by definition:
</bodyText>
<equation confidence="0.989651333333333">
aikb(ik,Ws)P1(k)s-ki-t + E aijauvPi(i) P ( )
s-r- r+i-t • (1)
r=5
</equation>
<bodyText confidence="0.969859333333333">
where ik E terminal(layer(i)), ij E nonterminal(layer(i)), u = last(layer(j)),v E bin(layer(j)),
and layer(i) = layer(v).
After the last word is generated, the last state of layer(i) should be reached.
</bodyText>
<figure confidence="0.4983185">
PI (Or+ i—t f 1 if i = last(layer(i)),
0 otherwise.
</figure>
<figureCaption confidence="0.5227065">
Figure 2 is the pictorial view of the Inside probability. A valid sequence can be-
gin only at state S, thus to be strict, Pi(S) has an additional product, P(S). When
</figureCaption>
<bodyText confidence="0.9944385">
the immediate transition ij is of terminal type, the transition probability a,j and the
probability of the 9th word at the transition b(ij , Ws) are multiplied together with the
Inside probability of the rest of the sequence, Ws+i-t•
Now define the Outside probability.
</bodyText>
<subsectionHeader confidence="0.816662">
Definition 2
</subsectionHeader>
<bodyText confidence="0.901524">
The Outside probability denoted by Po(i, j)s-t is the probability that partial sequences,
W1-s-1 and Wr+i-N, are generated, provided that the partial sequence, Ws-r, is gen-
erated by MI given a model A.
And by definition:
</bodyText>
<equation confidence="0.89741675">
PO(i, i)s^-4 PUS. ij .7] Wt+1N A)
s N
EEEaxf aeyPil(f , a—s_iPi(i) t+i—bP o(x, (2)
x a=1 b=t
</equation>
<page confidence="0.996328">
423
</page>
<figure confidence="0.995776">
Computational Linguistics
layer(i)
layer(j)
Volume 22, Number 3
</figure>
<figureCaption confidence="0.562549">
Figure 2
</figureCaption>
<figure confidence="0.879264">
Illustration of Inside probability.
layer(x)
layer(i)
1 s-1 r+1
</figure>
<figureCaption confidence="0.5920755">
Figure 3
Illustration of Outside probability.
</figureCaption>
<bodyText confidence="0.953071714285714">
where x E bout(layer(0), y E bin(layer(i)), f first(layer(i)), e = last(layer(i)), layer(i) =
layer(j), and layer(x) = layer(y).
The summation on x is defined only when a 1 or b N (i.e., there are words
left to be generated). Nonterminal and its corresponding pop transitions are defined
to be 1 when a = 1 and b= N.
For a boundary case of the Outside probability where f is the first state of a layer
in the above equation:
</bodyText>
<equation confidence="0.9860365">
1 if f = S,
Po(i, j)i—N = { 0 otherwise.
</equation>
<figureCaption confidence="0.626109666666667">
Figure 3 shows the network configuration in computing the Outside probability. In
equation 2, (f , i),_i is the probability that sequence, Wa-s-i, is generated by layer(i)
left to state i, and /31(j)t±1_b is the probability that sequence Wt+ib is generated by
layer(i) right to state j.
The computation of Pi&apos;(f, i),t—a slight variation of the Inside probability in which
the P (f)a_b&apos;s in equation 1 are replaced by P(f, i)b—is done as follows:
</figureCaption>
<figure confidence="0.96347375">
13(f, i)s-t = {
131(f)s-t if s &lt; t,
1 if s &gt; t and f = i,
0 if s &gt; t and f i.
</figure>
<footnote confidence="0.9869815">
It is basically the same as the Inside probability except that it carries an i that indicates
a stop state.
Now we can derive the re-estimation algorithm for A and B using the Inside and
Outside probabilities. As the result of constrained maximization of Baum&apos;s auxiliary
</footnote>
<page confidence="0.992796">
424
</page>
<note confidence="0.786652">
Han and Choi A Chart Re-estimation Algorithm
</note>
<bodyText confidence="0.976282833333333">
function, we have the following form of re-estimation for each transition (Rabiner
1989).
= expected number of transitions from state i to state j
expected number of transitions from state i
The expectation of each transition type is computed as follows: For a terminal transi-
tion:
</bodyText>
<equation confidence="0.93348175">
E ( ) Eirv=i aiib( - Wr)P0(0),---r
P(WA)
For a nonterminal transition:
Ent(ij- )
= 2-,s=1 La=saiiPt(i)s—tauvPo(i,v)s—t
x—.1
P(WI) •
where u = last(layer(j)), v E bin(layer(j)), layer(i) layer(v), layer(j) = layer(u), and uv
is a pop transition. For a pop transition:
EsN-1 ENt—s ativP((v)s—t P0(14/
Epop( - ) =
P(W A)
</equation>
<bodyText confidence="0.8881965">
where u E bout(layer(i)), j c bin(layer(i)), v = first(layer(i)), layer(u) = layer(j), layer(v)
layer(i), and uv is a nonterminal transition.
</bodyText>
<subsectionHeader confidence="0.630616">
Since transitions of terminal and nonterminal types can occur together at a state,
</subsectionHeader>
<bodyText confidence="0.861109">
terminal transitions are estimated as follows:
</bodyText>
<equation confidence="0.733752916666667">
Et( in (3)
Ek Et(ik) + Ek Ent(ik)
For nonterminal transitions:
aii Eta(ii) (4)
Ek Et(ik) + Ek Ent(ik)
And for pop transitions, notice that only pop transitions are possible at a pop state:
aij EkEpop(ik)
Epop(ij) (5)
For a terminal transition ij and a word symbol w:
,w) ENt—laub(i Wt)Po(i, j)t—t
s., vv,„, j,Wt)Po(i, pt—t
•
</equation>
<bodyText confidence="0.9691335">
The re-estimation continues until the probability of the word sequences reaches a
certain stability.
</bodyText>
<page confidence="0.993739">
425
</page>
<figure confidence="0.994138">
Computational Linguistics Volume 22, Number 3
sentence w
Compute Inside probability
of W
Inside Table
Select valid Insides by
running Inside algorithm
topdown
Chart of
valid Insides
Run Outside algorithm
</figure>
<figureCaption confidence="0.975761">
Figure 4
</figureCaption>
<bodyText confidence="0.848382">
Outside computation with chart. Inside computation builds a table of computed Insides.
</bodyText>
<sectionHeader confidence="0.598782" genericHeader="method">
4. Chart Re-estimation Algorithm
</sectionHeader>
<bodyText confidence="0.996667290322581">
It can be shown that the complexity of the Inside algorithm is 0(N3G3) and that of the
Outside algorithm is 0(N4G3) where N is the input size and G is the number of states.
The complexity is too much for current workstations when either N or G becomes
bigger than a few 10s. A basic implementation of the algorithm is to use a chart and
avoid doing the same computations more than once. For instance, the table for storing
Inside computations takes 0(N2G2C) store, where C is the number of terminal and
nonterminal categories. A chart item is a function of five parameters, and returns an
Inside probability.
I(i, j, s, t, c) = (i,j)s—t.
A chart item is associated with categories implying that the item is valid on the
specified categories that begin the net fragment of the item. Suppose a net fragment
[i,j] begins with NP and ADJP, then given a sentence fragment Ws_t, ADJP may not
participate in generating Ws—t, while NP may. The information of valid categories is
useful when the chart is used in computing Outside probabilities.
An Outside probability is the result of computing many Inside probabilities. Com-
puting an Inside probability even in an application of moderate size can be impractical.
A naive implementation of Outside computation takes numerous Inside computations,
so estimating even a parameter will not be realistic in a serial workstation (Lan i and
Young 1990).
The proposed estimation algorithm aims at reducing the redundant Inside com-
putations in computing an Outside probability. The idea is to identify the Inside prob-
abilities used in generating an input sentence and to compute an Outside probability
using mainly those Insides. This is done first by computing an Inside probability of
the input sentence, which can return a table of Insides used in the computation. Note
that the Insides in the deepest depth are produced first, as the recursion is released,
thus there can be many Insides that are not relevant to the given sentence. The Insides
that participate in generating the input sentence can be identified by running the In-
side algorithm one more time, top-down. Figure 4 illustrates the steps of the revised
Outside computation.
The identified Insides, however, do not cover all the Insides needed in computing
an Outside probability. This is because the Inside algorithm works on a network from
</bodyText>
<page confidence="0.996025">
426
</page>
<note confidence="0.84041">
Han and Choi A Chart Re-estimation Algorithm
</note>
<bodyText confidence="0.98161875">
left to right and one transition at a time. Many Insides that are missed in the table are
compositions of smaller Insides.
Once charts of selected Insides are prepared, an Outside probability is computed
as follows:
</bodyText>
<equation confidence="0.8763415">
Po(i, j)s—t = axfaeyI(f i, a, s — 1)I(j , t b)P0 (x , Y)aI7 •
{xl1(x,y,a,b,c)&gt;O} (a,b)Ea(f,e,s,t)
</equation>
<bodyText confidence="0.98868125">
where x E bout(layer(i)), y c bin(layer(i)), f = first(layer(i)), e = last(layer(i)), c c
{nonterminal}, layer (i) = layer(j), and layer (x) = layer(y).
The function o-(f, , e, s, t) returns a set of (a, b) pairs where there are Inside items
I(f, ,e,a,b) defined at the chart such that a &lt; s and b &gt; t. In short, the items for
state f indicate the possible combinations of sentence segments inclusive of the given
fragment Ws—t because the chart contains items of all the valid sentence segments that
were generated through the layer [f, e] . When the current layer [f, e] is completed with
the two Insides computed, the computation extends to the Outside.
Useless advancements into high layers that do not lead to the successful comple-
tion of a given sentence can be avoided by making sure that [x, y] generates Wab and
the category of current layer c is defined, which can be checked by consulting the
chart items for state x.
</bodyText>
<sectionHeader confidence="0.982395" genericHeader="evaluation">
5. Experiments
</sectionHeader>
<bodyText confidence="0.999957333333333">
The goal of our experiments is to see how much saving the new estimation algorithm
achieves in computational cost. Out of 14,132 Wall Street Journal trees of the Penn tree
corpus, 1,543 trees corresponding to sentences with 10 words or less were chosen, and
the programs written in C language were run at a Sparc10 workstation.
The basic implementation of an Inside-Outside algorithm assumes tables for In-
sides and Outsides so that identical Insides and Outsides need not be recomputed.
A chart Outside or re-estimation algorithm assumes a refined table of Insides that
contains only valid Insides used in generating the input sentence as discussed earlier,
and Outside computation is done based on the refined table.
The improvement from the chart re-estimation algorithm is measured in the num-
ber of actual Inside and Outside computations done to estimate the parameters. Fig-
ure 5 shows the average counts of Insides used in estimating 50 trees randomly selected
from 1,543 samples. Before the re-estimation algorithm is applied, an RTN that faith-
fully encodes the input trees without any overgeneration is constructed from the 50
trees. The gain in Insides from the chart re-estimation algorithm is very clear, and in
the case of Outsides the gain is even more conspicuous (see Figure 6). The number of
Insides counted in chart version also includes the Insides computed in preparing the
chart.
</bodyText>
<sectionHeader confidence="0.992839" genericHeader="conclusions">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.999975833333333">
We have presented an efficient re-estimation algorithm for a PRTN that made use
of only valid Insides. The method requires the preparation of a chart by running
Inside computation twice over a whole sentence. The suggested method focuses mainly
on reducing the computational overhead of Outside computation, which is a major
portion of a re-estimation. The computation of an Inside probability may be improved
further using a similar technique introduced in this paper.
</bodyText>
<page confidence="0.98739">
427
</page>
<figure confidence="0.9891565">
Computational Linguistics Volume 22, Number 3
2000
1500
Insides 1000
500
1
without chart &apos;49-
with chart -e-
2 4 6 8 10
Sentence size
Figure 5
Gain in Insides using chart re-estimation, showing 50 randomly-chosen sentences out of 1,543
samples.
400
350
300
250
Outsides 200
150
100
50
0
-
-
-
-
- without chart -e—
with chart -e--
-
10
2 3 4 5 6 7 8 9
Sentence size
</figure>
<figureCaption confidence="0.697415">
Figure 6
</figureCaption>
<bodyText confidence="0.419026">
Gain in Outsides using chart re-estimation with the same 50 sentences as in Figure 5.
</bodyText>
<page confidence="0.990976">
428
</page>
<note confidence="0.84442">
Han and Choi A Chart Re-estimation Algorithm
</note>
<sectionHeader confidence="0.749766" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99959396875">
Briscoe, Ted, and John Carroll. 1993.
Generalized probabilistic LR parsing of
natural language (Corpora) with
unification-based grammars.&amp;quot;
Computational Linguistics 19(1): 25-57.
Kupiec, Julian. 1991. A trellis-based
algorithm for estimating the parameters
of a hidden stochastic context-free
grammar. In Proceedings of the Speech and
Natural Language Workshop, pages 241-246,
DARPA, Pacific Grove.
Lafferty, John, Daniel Sleator, and Davy
Temperley. 1992. Grammatical trigrams: A
probabilistic model of link grammar.
AAAI Fall Symposium Series: Probabilistic
Approaches to Natural Language, pages
89-97, Cambridge.
Lan, K. and S. J. Young. 1990. The
estimation of stochastic context-free
grammars using the Inside-Outside
algorithm. Computer Speech and Language
4:35-56.
Rabiner, Lawrence R. 1989. A tutorial on
hidden Markov models and selected
applications in speech recognition. In
Proceedings of the IEEE 77, Volume 2.
Schabes, Yves, Michael Roth, and Randy
Osborne. 1993. Parsing the Wall Street
Journal with the inside-outside algorithm.
Sixth Conference of the European Chapter of
the ACL, &apos;93, Utrecht, the Netherlands,
April.
</reference>
<page confidence="0.999241">
429
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.500448">
<title confidence="0.990864">A Chart Re-estimation Algorithm for a Probabilistic Recursive Transition Network</title>
<author confidence="0.993316">Key-Sun Choit</author>
<affiliation confidence="0.99951">University of Suwon Korea Advanced Institute of Science and</affiliation>
<title confidence="0.845582666666667">Technology A Probabilistic Recursive Transition Network is an elevated version of a Recursive Transition Network used to model and process context-free languages in stochastic parameters. We present</title>
<abstract confidence="0.9856124">a re-estimation algorithm for training probabilistic parameters, and show how efficiently it can implemented using charts. The complexity of the Outside algorithm we present is where N is the input size and G is the number of states. This complexity can be significantly overcome when the redundant computations are avoided. Experiments on the Penn tree corpus show that re-estimation can be done more efficiently with charts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (Corpora) with unification-based grammars.&amp;quot;</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>1</issue>
<pages>25--57</pages>
<contexts>
<context position="1096" citStr="Briscoe and Carroll (1993)" startWordPosition="157" endWordPosition="160">plemented using charts. The complexity of the Outside algorithm we present is 0(N4G3) where N is the input size and G is the number of states. This complexity can be significantly overcome when the redundant computations are avoided. Experiments on the Penn tree corpus show that re-estimation can be done more efficiently with charts. 1. Introduction Though hidden Markov models have been successful in some applications such as corpus tagging, they are limited to the problems of regular languages. There have been attempts to associate probabilities with context-free grammar formalisms. Recently Briscoe and Carroll (1993) have reported work on generalized probabilistic LR parsing, and others have tried different formalisms such as LTAG (Schabes, Roth, and Osborne 1993) and Link grammar (Lafferty, Sleator, and Temperley 1992). Kupiec extended a SCFG that worked on CNF to a general CFG (Kupiec 1991). The re-estimation algorithm presented in this paper may be seen as another version for general CFG. One significant problem of most probabilistic approaches is the computational burden of estimating the parameters (Lan i and Young 1990). In this paper, we consider a probabilistic recursive transition network (PRTN) </context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Briscoe, Ted, and John Carroll. 1993. Generalized probabilistic LR parsing of natural language (Corpora) with unification-based grammars.&amp;quot; Computational Linguistics 19(1): 25-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>A trellis-based algorithm for estimating the parameters of a hidden stochastic context-free grammar.</title>
<date>1991</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<pages>241--246</pages>
<location>DARPA, Pacific Grove.</location>
<contexts>
<context position="1377" citStr="Kupiec 1991" startWordPosition="204" endWordPosition="205">an be done more efficiently with charts. 1. Introduction Though hidden Markov models have been successful in some applications such as corpus tagging, they are limited to the problems of regular languages. There have been attempts to associate probabilities with context-free grammar formalisms. Recently Briscoe and Carroll (1993) have reported work on generalized probabilistic LR parsing, and others have tried different formalisms such as LTAG (Schabes, Roth, and Osborne 1993) and Link grammar (Lafferty, Sleator, and Temperley 1992). Kupiec extended a SCFG that worked on CNF to a general CFG (Kupiec 1991). The re-estimation algorithm presented in this paper may be seen as another version for general CFG. One significant problem of most probabilistic approaches is the computational burden of estimating the parameters (Lan i and Young 1990). In this paper, we consider a probabilistic recursive transition network (PRTN) as an underlying grammar representation, and present an algorithm for training the probabilistic parameters, then suggest an improved version that works with reduced redundant computations. The key point is to save intermediate results and avoid the same computation later on. More</context>
</contexts>
<marker>Kupiec, 1991</marker>
<rawString>Kupiec, Julian. 1991. A trellis-based algorithm for estimating the parameters of a hidden stochastic context-free grammar. In Proceedings of the Speech and Natural Language Workshop, pages 241-246, DARPA, Pacific Grove.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Grammatical trigrams: A probabilistic model of link grammar. AAAI Fall Symposium Series: Probabilistic Approaches to Natural Language,</title>
<date>1992</date>
<pages>89--97</pages>
<location>Cambridge.</location>
<marker>Lafferty, Sleator, Temperley, 1992</marker>
<rawString>Lafferty, John, Daniel Sleator, and Davy Temperley. 1992. Grammatical trigrams: A probabilistic model of link grammar. AAAI Fall Symposium Series: Probabilistic Approaches to Natural Language, pages 89-97, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lan</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the Inside-Outside algorithm.</title>
<date>1990</date>
<journal>Computer Speech and Language</journal>
<pages>4--35</pages>
<marker>Lan, Young, 1990</marker>
<rawString>Lan, K. and S. J. Young. 1990. The estimation of stochastic context-free grammars using the Inside-Outside algorithm. Computer Speech and Language 4:35-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>In Proceedings of the IEEE</booktitle>
<volume>77</volume>
<contexts>
<context position="8099" citStr="Rabiner 1989" startWordPosition="1329" endWordPosition="1330">slight variation of the Inside probability in which the P (f)a_b&apos;s in equation 1 are replaced by P(f, i)b—is done as follows: 13(f, i)s-t = { 131(f)s-t if s &lt; t, 1 if s &gt; t and f = i, 0 if s &gt; t and f i. It is basically the same as the Inside probability except that it carries an i that indicates a stop state. Now we can derive the re-estimation algorithm for A and B using the Inside and Outside probabilities. As the result of constrained maximization of Baum&apos;s auxiliary 424 Han and Choi A Chart Re-estimation Algorithm function, we have the following form of re-estimation for each transition (Rabiner 1989). = expected number of transitions from state i to state j expected number of transitions from state i The expectation of each transition type is computed as follows: For a terminal transition: E ( ) Eirv=i aiib( - Wr)P0(0),---r P(WA) For a nonterminal transition: Ent(ij- ) = 2-,s=1 La=saiiPt(i)s—tauvPo(i,v)s—t x—.1 P(WI) • where u = last(layer(j)), v E bin(layer(j)), layer(i) layer(v), layer(j) = layer(u), and uv is a pop transition. For a pop transition: EsN-1 ENt—s ativP((v)s—t P0(14/ Epop( - ) = P(W A) where u E bout(layer(i)), j c bin(layer(i)), v = first(layer(i)), layer(u) = layer(j), l</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner, Lawrence R. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. In Proceedings of the IEEE 77, Volume 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Michael Roth</author>
<author>Randy Osborne</author>
</authors>
<title>Parsing the Wall Street Journal with the inside-outside algorithm.</title>
<date>1993</date>
<booktitle>Sixth Conference of the European Chapter of the ACL, &apos;93,</booktitle>
<location>Utrecht, the Netherlands,</location>
<marker>Schabes, Roth, Osborne, 1993</marker>
<rawString>Schabes, Yves, Michael Roth, and Randy Osborne. 1993. Parsing the Wall Street Journal with the inside-outside algorithm. Sixth Conference of the European Chapter of the ACL, &apos;93, Utrecht, the Netherlands, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>