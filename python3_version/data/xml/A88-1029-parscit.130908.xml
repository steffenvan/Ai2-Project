<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000084">
<title confidence="0.76565">
THE TICC: PARSING INTERESTING TEXT.
</title>
<author confidence="0.748643">
David Allport
</author>
<affiliation confidence="0.868478">
School of Cognitive Sciences
University of Sussex,
</affiliation>
<note confidence="0.5577845">
Falmer,
Brighton BN1 9QN
</note>
<email confidence="0.987503">
davidalouk.ac.sussexxvaxa@cs.ucl.ac.uk
</email>
<sectionHeader confidence="0.979865" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999890333333333">
This paper gives an overview of the natural
language problems addressed in the Traffic
Information Collator/Condenser (TICC) pro-
ject, and describes in some detail the
&amp;quot;interesting-corner parser&amp;quot; used in the TICC&apos;s
Natural Language Summariser. The TICC is
designed to take free text input describing
local traffic incidents, and automatically out-
put local traffic information broadcasts for
motorists in appropriate geographical areas.
The &amp;quot;interesting-corner parser&amp;quot; uses both syn-
tactic and semantic information, represented
as features in a unification-based grammar, to
guide its bi-directional search for significant
phrasal groups.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="introduction">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999887033333333">
The overall goal of the TICC project is
to show the potential benefits of automati-
cally broadcasting local traffic information.
Our target system, dealing with traffic
incidents in the Sussex area, is to be com-
pleted by September 1989. The project forms
part of the Alvey Mobile Information Sys-
tems large-scale Demonstrator.
The Natural Language Summariser com-
ponent of this system is being developed at
Sussex University. Its function is to accept
a series of free text messages describing
traffic incidents, and to extract from these
messages any information that might be
relevant for broadcast to other motorists.
The Natural Language Summariser is
designed to work in a restricted domain, and
only needs to solve a subset of the problems
of text understanding. The TICC&apos;s output
messages are short and very simple assem-
blies of canned text, posing no significant
natural language generation problems. Our
main concern is that the messages should be
useful to motorists, i.e that they be reliable
indications of the state of the roads at the
time they are broadcast.
Programs such as METE° [Chevalier et
al. 1978] have demonstrated that in a res-
tricted domain with a restricted sub-language,
automatic information broadcasts can be use-
ful. Programs such as FRUMP [De Jong
1979, De Jong 1982] have also demonstrated
that expectation-driven analysers can often
successfully capture the gist of free text.
However, the top-down depth-first
confirmation of expectations based on sketchy
scripts, ignoring most of the input structure,
can lead to serious misinterpretations [Ries-
beck 82]. Our concern for accuracy of
interpretation has led us to a processing stra-
tegy in which the Natural Language Sum-
mariser analyses the input text at a far
greater level of detail than is given in the
output messages, so the system &amp;quot;knows more&amp;quot;
about the traffic incidents it is describing
than it says in its broadcasts. Our parser
uses both syntactic and semantic information
to guide its search for phrases in the input
that might be directly or indirectly relevant
to motorists, and explores alternative possible
interpretations bottom-up using an active
chart [Earley 1970, Kay 1973].
This is an ongoing research project, and
we do not claim to have solved all the
problems involved in developing a successful
system yet. The current paper considers the
particular natural language problems we are
addressing and describes the &amp;quot;interesting-
corner parser&amp;quot; that has been implemented in
the prototype system.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="method">
2. THE NATURAL LANGUAGE
SUMMARISER&apos;S TASK
</sectionHeader>
<subsectionHeader confidence="0.717787">
2.1 INPUT: Our input data comes from
</subsectionHeader>
<bodyText confidence="0.966603857142857">
the Sussex Police, who have a computer sys-
tem for storing the text of incoming traffic
messages from a variety of sources (eg.
patrol cars, emergency services, motoring
organisations). An example of the style of
this text, derived from real input but with
names etc. changed, is given in fig.l.
</bodyText>
<note confidence="0.462784">
The series of messages dealing with a
</note>
<page confidence="0.995976">
211
</page>
<bodyText confidence="0.9992674">
single incident continues over a number of
hours, depending on the severity of the
incident, and the TICC can afford to spend
up to one minute analysing an average
length message. All aspects of the police
management of the incident are described,
and many of the messages are only
indirectly relevant to motorists. For example,
if one of the vehicles involved in an
accident needs a total lift to remove it from
the road, the likely delay time given in the
broadcast message may be longer, although
the need for the total lift will not itself be
mentioned in the broadcast. Much of the
input is completely uninteresting for the
TICC&apos;s purposes, such as details of injuries
sustained by people involved, or of which
police units are dealing with the incident.
There is a great variety in prose style,
from the &amp;quot;normal&amp;quot; to the highly telegraphic,
but there is a strong tendency towards the
abbreviated. It is a non-trivial task to
correctly identify the lexical items in the
text. Parts of the input string which are not
recognised as entries in the Summariser&apos;s lex-
icon (or regular derivations from entries)
may be of four types:
U Names, numbers etc, which may be
recognised as such from the context (e.g pc
humphries requests ..., ford cortina reg
ABC123).
Other English words not in the lexi-
con, which cannot reliably be predicted to be
proper names (e.g hovis lorry b/dwn o/s bull&apos;s
head ph).
</bodyText>
<subsectionHeader confidence="0.822748">
Misspellings of items in the lexicon.
</subsectionHeader>
<bodyText confidence="0.990217948717948">
iv) Non-standard abbreviations of known
words or phrases.
Abbreviations are not always of &amp;quot;canoni-
cal&amp;quot; form, and may be derived from com-
plete words in three different ways, as fol-
lows:
D Single morpheme roots: These usu-
ally have more than one possible abbreviated
form and never include punctuation eg. gge,
grg or gar for garage. But some words do
have canonical abbreviations (eg rd for road
and st for street (or saint).
Multi-morpheme roots: These often
take only the first letter from the first root
morpheme, and then either part or all of the
second morpheme. They occasionally include
slash punctuation eg. cway, c/way for car-
riageway, mcycle, m/c for motorcycle, o/s for
outside (or offside), and ra for roundabout.
Sequences / phrases: Some sequences
of words have canonical abbreviations (e.g
bbc and not britbrdcrp). Canonical examples
seen in Fig. 1. below include rta for road
traffic accident and oic for officer in charge.
Non-canonical sequences may have a
variety of abbreviations for each of the con-
stituent words, and may or may not have
slash or period punctuation, eg. f/b for fire
brigade, eamb or esamb for east (sussex) ambu-
lance, hazchem for hazardous chemicals.
The problem is compounded for the
TICC by the fact that the input we receive
is all in upper case, hence the even the con-
vention of distinguishing proper name abbre-
viations by upper case is not applicable. In
order to cope with these different types of
input string, we need not only a &amp;quot;phrasal
lexicon&amp;quot; as advocated by Becker [Becker
1975], but also an &amp;quot;abbreviation lexicon&amp;quot;.
</bodyText>
<tableCaption confidence="0.820047529411765">
Time: 1634 Location: scaynes hill, haywards heath
1634 rta serious near top scaynes hill persons trapped rqst esamb f/b 1/2 mile
south jw freshfield rd.
1638 fm pc 123 ace inv reqd poss black oic pc 456
1639 fire and amb en route
1642 req total lift for saloon car rota garage
1654 eamb now away from scene
1655 freshileld bodyshop on way
1657 fm pc 456 req rd closed n and s of hill st crnr
1658 req two traff units to assist re closures
1709 can we inform brighton 1234 tell mr fred smith will be late due to this rta
1715 local authority required loose paving stones
1723 fm pc 234 at st george&apos;s hosp. dr in charge having examined mr Jones now
feels this is not likely to be a black. driver of lorry has arrived, will
probably be released after treatment for cuts. car 45 will be free from
hasp in about 20 min
Fig. 1. An extract from an example (fictitious) incident log.
</tableCaption>
<page confidence="0.998196">
212
</page>
<bodyText confidence="0.993643642857143">
Our aim is to have a unified process for
identifying idiomatic or fixed phrases and
abbreviated sequences as in above, so that
for example as soon as poss, asap and a.s.a.p.
are all identified as the same &amp;quot;lexical item&amp;quot;.
Work on this is, however, at a preliminary
stage, and we have not yet found any gen-
eral solution to the problem.
2.2 SUMMARISATION: Deriving a short
broadcast for motorists from a long series of
messages such as that in fig. 1 requires two
main phases. First, the Natural Language
Summariser must build up a picture of what
is happening at the scene of the incident.
Second, a Tactical Inferencer must decide
what motorists should be told regarding the
incident.
The Natural Language Summarising pro-
cess also requires two phases. In the first
phase a Message Analyser extracts interesting
information from a single message. In the
second phase an Event Recogniser puts
together the information from a series of
messages to build up a description of the
incident as a whole, or rather those aspects
of the incident relevant to other motorists
(see fig 2. below).
The Message Analyser does not build a
complete representation of the syntax and
semantics of messages such as those at 1709
and 1723 in fig. 1 above, since they have no
bearing on the progress of the traffic incident
as far as other motorists are concerned. It
just searches for phrases describing &amp;quot;interest-
ing&amp;quot; events. These fall into two classes:
Primary Events: Such as vehicles
blocking the road, substances spilling onto
the road, all or part of the road being
closed, diversions being put into operation,
garage coming to remove vehicles from the
road, services like fire brigade and county
council removing hazards, etc.
The input messages rarely describe these
events in full, so the Event Recogniser must
infer, for example, that if the local council
has been called out to remove debris from
the road, that at some time earlier debris
must have fallen on the road.
Secondary Events: These include
requests that some of the primary events
should happen, and people being informed
that primary events have happened are hap-
pening or will happen.
We will not have any model of the
beliefs of the various agents involved in
incident handling. As far as the TICC
Natural Language Summariser is concerned,
the meaning of someone being informed that
a primary event has happened is equivalent
to the statement that it has happened. But
the Tactical Inferencer will use its model of
the typical progress of traffic incidents to
predict the significance of the primary events
for other motorists. For example, if a vehicle
is stated to need a front suspended tow,
then the Tactical Inferencer will predict that
a certain amount of time will elapse before
the vehicle is towed away.
2.3 OUTPUT: Not every message input
to the system will produce an update to the
Event Recogniser&apos;s description of the incident,
because the Message Analyser may fail to
find a description of an interesting event.
But even when the Event Recogniser passes a
description of a traffic incident to the Tacti-
cal Inferencer, this will not necessarily result
in a broadcast. For example, the Event
Recogniser may recognise a series of messages
as describing a traffic light failure incident.
The Tactical Inferencer may decide to broad-
cast a message about this incident if it has
occurred on a busy main road in the rush
hour, but not if it has occurred late at night
in a small village.
</bodyText>
<figure confidence="0.848015333333333">
Road/Junction Database
Free Text Messages for
Messages Broadcaster
</figure>
<figureCaption confidence="0.85211">
Fig. 2. Part of the TICC system, showing Message Analyser,
Event Recogniser, and Tactical Inferencer.
</figureCaption>
<page confidence="0.993612">
213
</page>
<bodyText confidence="0.99997947826087">
The domain knowledge used in the the
Tactical Inferencer is non-linguistic, and con-
cerns inferences about the likely time delays
for different types of incident, the geographi-
cal areas likely to be affected by a given
incident, etc. The Transport and Road
Research Laboratory, part of the Department
of Transport, are assisting us in the develop-
ment of rules for this part of the system.
There are other components of the TICC
system which we do not detail in this paper,
such as the graphical interface, via a map of
the Sussex area, to a database of of current
traffic incidents. Although the 11CC is
designed to send its messages to a dedicated
broadcasting system, the actual broadcasting
aspect of the project is the responsibility of
RACAL research, one of our other Alvey
collaborators. In our current prototype sys-
tem, implemented on a Sun-3 workstation,
broadcasts to local geographical areas in
Sussex are simulated, and the Tactical
Inferencer is extremely simple.
</bodyText>
<sectionHeader confidence="0.999378" genericHeader="method">
3. INTERESTING CORNER PARSING
</sectionHeader>
<bodyText confidence="0.99675702631579">
The parser that has been implemented
for the Message Analyser searches bidirection-
ally for all syntactic parses associated with
semantically interesting parts of the input.
Before describing the search strategy in more
detail, we need to clarify what a syntactic
parse looks like in our grammar formalism,
and how we specify what is semantically
interesting.
3.1 THE GRAMMAR FORMALISM: We
use a unification-based grammar formalism,
with rules that look similar to context-free
phrase-structure rules. Both immediate domi-
nance and constituent ordering information
are specified by the same rule, rather than
by separate statements as in FUG [Kay
1985], LFG [Kaplan &amp; Bresnan 1982] and
GPSG [Gazdar et al 1985]. Feature-passing
between categories in rules is done explicitly
with logical variables, rather than by con-
ventions such as the HFC and FFP in GPSG
[Gazdar et al 1985]. Thus the rule format is
most similar to that used in DCG&apos;s [Pereira
&amp; Warren 19801. Categories in rules are
feature/value trees, and at each level the
value of a feature may itself be another
feature/value tree. Feature values may be
given logical names, and occurrences of
feature values having the same logical name
in a rule must unify.
The feature trees which constitute
categories in our grammar may specify both
syntactic and semantic features, so that we
can write &amp;quot;syntactic&amp;quot; rules which also iden-
tify the semantic types of their constituents.
For example, if we use the feature sf on
categories to specify a tree of semantic
features for that category, then the rule:
</bodyText>
<listItem confidence="0.567123">
(1) vp=(sf:VSF) —&gt; v=(s1=(patient:P):VSF),
np=(sf:P)
</listItem>
<bodyText confidence="0.998225695652174">
says that a verb phrase may consist of a
verb followed by a noun phrase, and that
the semantic features on the noun phrase
(labelled P) must unify with the semantic
features specified as the value of the patient
sub-feature of the verb&apos;s semantic features,
and additionally that the semantic features
on the whole verb phrase (labelled VSF)
must unify with the (complete tree of)
semantic features on the verb.
By adding domain-specific semantic
feature information to lexical categories, we
gain the power of domain-specific semantic
grammars, which have been shown to be suc-
cessful for handling ill-formed input in lim-
ited domains [Burton 1976]. But because we
use unification by extension as the basic cri-
terion for node admissability when we test
for rules to licence local trees, we can also
capture generalisations about syntactic
categories that are not domain-specific. So for
example if we had a verb-phrase rule such
as (2) and a lexical entry as in (3):
</bodyText>
<listItem confidence="0.985702">
(2) vp —&gt; v=(tr=trans), np
(3) close v=(tr=(trans),
</listItem>
<bodyText confidence="0.959210333333333">
sf=(event_type=road_closure,
agent=service,
patient=roadlocation))
then the verb feature tree specified in (2)
would unify with the verb feature tree in
(3). Hence close can be treated both as a
domain specific verb and as an instance of
the general class of transitive verbs.
Using a feature-based semantic grammar
therefore gives us a compact representation of
both domain independent and domain-specific
information in a single uniform formalism.
Syntactic generalisations are captured by
rules such as (2), and domain-specific sub-
categorisation information is expressed in
feature-trees as in (3), which states that close
has the semantic features of a road-closure
event, expecting an agent with the semantic
features of a service (eg police) and a patient
with semantic features indicating (a part of)
a road. As with all sub-languages, our
</bodyText>
<page confidence="0.993797">
214
</page>
<bodyText confidence="0.990935225225225">
lexicon also includes domain-specific meanings
for particular lexical items, eg. black mean-
ing fatal (cp messages at 1638 and 1723 in
fig. 1 above).
3.2 A GRAMMAR FOR THE TICC
DOMAIN: Writing a grammar to give ade-
quate coverage of the input that our system
must handle is a lengthy task, which will
continue over the next two years. However,
analysis of a corpus of data from police logs
of over one hundred incidents in the Sussex
area, and trials with experimental grammars,
have led us to adopt a style of grammar
which we expect will remain constant as the
grammar expands.
We do not attempt to map telegraphic
forms onto &amp;quot;fully grammatical&amp;quot; English forms
by some variant of constraint relaxation
[Kwasny &amp; Sondheimer 1981]. We simply
have a grammar with fewer constraints. This
is because it is not always easy to decide
what is missing from an elliptical sentence,
or which constraints should be relaxed. Con-
sider for example the message at 1655 from
fig. 1, repeated here:
(4) freshfield bodyshop on way
It is not at all clear what the &amp;quot;full&amp;quot; senten-
tial form of this message ought to be, since
it might also have been phrased as one of:
(5.1) freshfield bodyshop is on the way
(5.2) freshfield bodyshop is on its way
(5.3) freshfield bodyshop are on the way
(5.4) freshfield bodyshop are on their way
Each of the (5.1)-(5.4) must be allowed
to be grammatical (and each might occur in
our type of input), since noun phrases nam-
ing corporate entities can regularly be
regarded as singular or plural (cp. Ford
Motors has announced massive profits ... vs.
Ford Motors have announced massive profits).
But in each case the semantic representation
that the Message Analyser must build only
needs to represent the fact that the garage
called freshfield bodyshop are going some-
where (which the Event Recogniser will
expect to be the scene of the incident, in
order to remove the damaged vehicle). Since
the distinctions between the syntactic forms
in these examples is irrelevant for our pur-
poses, it would be a waste of the parser&apos;s
effort to introduce search and inference prob-
lems in the attempt to map the syntax of
(4) uniquely into the syntax of one or other
of the forms in (5). Indeed it is more
appropriate for our purposes to regard on
way as a domain-specific idiomatic phrase,
equivalent to en route, enrte etc (each of
which occur in similar contexts).
In keeping with this approach to ill-
formedness, our grammar contains many
categories (ie feature-trees), that would not
be recognised as syntactic categories in gram-
mars for normal English, eg. we have special
rules for phrases containing predicted unk-
nowns such as names, car registration
numbers, etc. Our parser is looking for
phrases describing events rather than sen-
tences, and we will not necessarily always
assign a structure with a single &amp;quot;S&amp;quot; label
spanning all the input message.
As we noted in 3.1 above, the lexical
entries for words that suggest interesting
events include trees of semantic features that
specify expected fillers for various roles in
these events. These feature trees provide
selectional restrictions useful for guiding the
parse, but do not themselves constitute the
semantics&amp;quot; of the lexical entries. The
semantics are represented as first-order logical
expressions in a separate field of the lexical
entry, and representations of the meaning of
phrases are built using semantic rules associ-
ated with each syntactic rule, as phrases are
completed in the bottom-up parse.
3.3 THE SEARCH STRATEGY:
Interesting-corner parsing is basically an
adaptation of bottom-up chart parsing to
allow island-driving through the input string,
whilst still parsing each individual rule uni-
directionally. This gives a maximally
efficient parse for our goal of reliably
extracting from the input all and only the
information that is relevant to other motor-
ists. This form of expectation-driven parsing
differs from that used in earlier script-based
systems such as MARGIE [Schenk 1975], ELI
[Riesbeck 1978] and FRUMP in four ways:
First, the interesting-corner parser uses
an active chart to consider bottom-up all
interesting interpretations that might be given
to an input message, rather than proceeding
left to right and filtering out later (right)
candidate interpretations on the basis of ear-
lier (left) context.
Second, if there are no interesting lexical
items in the input string, or if the only
interesting items occur at the (right) end of
the input, there is no attempt to match all
the leftmost items to a series of candidate
scripts or frames using top-down expecta-
tions.
</bodyText>
<page confidence="0.997675">
215
</page>
<bodyText confidence="0.998228584070797">
Third, the expectations themselves are
expressed declaratively in feature trees that
form part of the lexical categories, which
control the search via standard unification
with declarative rules, where previous sys-
tems used procedural &amp;quot;requests&amp;quot; in the lexi-
con.
Fourth, our parser builds an explicit
syntactic tree for the input, albeit including
semantic features, rather than by building a
semantic representation &amp;quot;directly&amp;quot;.
The interesting-corner parser checks the
semantic features on every lexical item in
the input to see if they are interesting, but
this is a far faster operation than testing
many times whether a series of lexical items
matches the expectations from a top-down
script. This does assume that the parser can
identify what the lexical items are, which is
problematic as we noted in section 2.1 above.
But as we shall see, the interesting-corner
parser does use predictions about the presence
of lexical items with particular features in
its search, and hence is in no worse a posi-
tion than a strictly top-down parser as
regards matching expectations to ill-formed
lexical items.
3.3.1 UNIDIRECTIONAL ISLAND-
DRIVING: Island-driving is useful for text
where one needs to start from clearly
identifiable (and in our case, semantically
interesting) parts of the input and extend the
analysis from there to include other parts.
But parsing rules bi-directionally is
inherently inefficient. Consider, for example, a
chart parse of the input string a b given a
single rule:
c -&gt; a b.
A standard bottom-up left-to-right active
chart parse of this input would create three
nodes (1 a 2 b 3) two active edges (an
empty one at node 1 and one from nodes 1
to 2) and one inactive edge (from node 1 to
3).
But a bi-directional parse, allowing the
rule to be indexed at any point, would build
a total of 7 active edges (one empty one at
each node, and 2 pairs with one constituent
found, built in different directions, ie 5 dis-
tinct edges). It would also build the same
inactive edge in two different directions. For
a rule with three daughters, a bidirectional
parse produces 14 active edges (9 of which
are distinct) and again 2 inactive edges.
This redundancy in structure-building
can be removed by incorporating constituents
into rules unidirectionally whilst still parsing
the text bidirectionally. We do this by
indexing each rule on either left-most or
right-most daughter, and parsing in a unique
direction away from the indexed daughter.
In order to preserve completeness in the
search, the chart must contain lists of active
and inactive edges for each direction of
expansion, although the same structure can
be shared in the inactive edge-lists for both
directions. The fundamental rule of edge-
combination must be augmented so that when
an inactive edge is added to the chart, it
combines with any appropriate active edges
at both of its ends. This process might be
called &amp;quot;indexed-corner parsing&amp;quot;, in that it
effectively combines left-corner parsing and
right-corner parsing, and the direction of
parse at any stage simply depends upon how
the individual grammar rules are indexed.
The interesting-corner parser implements
an indexed-corner chart parser, with the
addition of an agenda control mechanism and
an indexing principle for grammar rules.
3.3.2 AGENDA CONTROL: The insertion
of edges into the agenda is constrained by
the value of a &amp;quot;control-feature&amp;quot;, which
specifies where to look in the feature-trees
that constitute our categories in order to find
the semantically &amp;quot;interesting&amp;quot; features. In
our examples (1) and (2) above, this
control-feature is named sf. When a normal
bottom-up chart parse begins, all lexical
items are tested to see whether they can
spawn higher edges. But in the interesting-
corner parse, higher edges are only spawned
from lexical items that have a control-feature
specification which unifies with a pre-defined
initial value of the control feature. Thus by
assigning (sf—event_type) to be the initial
value of the control feature, we ensure that
only those edges are entered into the agenda
that have semantic feature trees that are
extensions of this tree (eg the semantic
feature tree for close in (3) above). This
effectively means that parsing must begin
from words that suggest some kind of
interesting event. Note that the initial active
edges may be proposed from any point in
the input string, and their direction of
expansion from that point is determined by
the indexing on the rules.
For all active edges proposed from lexi-
cal items that were initially recognised to be
interesting, the parser checks the list of edges
sought for &amp;quot;interesting&amp;quot; categories (ie. those
with values for the control-feature sf). If
</bodyText>
<page confidence="0.997326">
216
</page>
<bodyText confidence="0.998083987179488">
there are any, it searches, in the direction of
expansion for the current active edge, for
any lexical items that have a semantic
feature-tree which unifies with the new
specification of what is &amp;quot;interesting&amp;quot;.
For example, if the rule given in (1)
above is indexed on the left daughter, and
an active edge is proposed starting from an
inactive edge representing the lexical item
close defined as in (3) above, then via the
logical name P the features on the noun-
phrase being sought become instantiated to
(sf -Toadlocation). The parser then looks
rightwards in the input string for any lexical
items having semantic feature trees that are
extensions of this new tree. If it finds any,
it predicts more active edges from there, and
so forth.
Fig. 3 below illustrates the numerical
order in which the interesting-corner parser
incorporates nodes into the parse tree for a
very simple &amp;quot;sentence&amp;quot; (in our grammar we
allow sentences with deleted auxiliaries), but
with the details of the feature trees omitted
for legibility.
Extension unification allows one of the
structures to be unified (the target) to be an
extension of the other (the pattern), but not
vice-versa. This means that it is more res-
tricted than graph unification, and hence can
be implemented more efficiently. It is less
restricted than term unification, and hence
less efficient at parse-time, but it does allow
the grammar and lexicon to be far more
compact than they would be with term-
unification in the absence of a grammar pre-
processor. However, using extension
unification as the basic operation does also
mean that that the unification of logical
variables in rules is not order-independent,
and hence we need an indexing principle to
determine the direction in which particular
rules should be parsed.
3.3.3 THE INDEXING PRINCIPLE: Our
general principle for indexing rules is that
we must parse from categories that specify
general information (ie. that have small
feature-trees) to those that specify particular
modifications of that general information (ie.
that provide extensions to the smaller trees
by unification). This usually means that we
parse from syntactic heads to complements,
eg indexing sentences on the vp (cf. HPSG
[Proudian &amp; Pollard 19851).
In our example rule (1), we index on
the verb, because its expectations specify the
general semantic type of the object, and the
semantic feature tree of the noun-phrase will
specify a sub-type of this general type, and
therefore will be an extension of the verb&apos;s
patient semantic feature tree. In the example
shown in fig 3, the semantic tree of the np
built at node 4 is:
(sf4roadlocation.4name—huntingdon,
rtitle—lane)))
which unifies by extension with the feature
tree (sf—roadlocation)), and this as we saw
above became the expected semantic tree for
the noun-phrase when rule (1) unified with
the verb in (3).
Finally, rules for categories that have
expected unknowns as daughters are always
indexed on the known categories, even if
these are not the grammatical head (eg we
index on the policeman&apos;s title for rules han-
dling sgt smith, insp brown etc. and on the
known title of a road for cases like hunting-
don lane, markworthy avenue etc.
</bodyText>
<sectionHeader confidence="0.839978" genericHeader="method">
3.3.4 EXTENSIONS TO THE CURRENT
</sectionHeader>
<bodyText confidence="0.7704125">
SYSTEM: There are many aspects of the
TICC&apos;s Natural Language Summarisation not
dealt with in this paper, such as the seman-
tic rules used in the Message Analyser and
</bodyText>
<figure confidence="0.995324923076923">
9 np
7 poltitle
Pc
unknown
chisholrn
8 v
closing
5
np 4
n1
unknown 3 n 2
huntingdon
10
</figure>
<figureCaption confidence="0.980109">
Fig. 3. Showing the order in which the interesting-corner parser
constructs a parse tree, starting with the most interesting words.
</figureCaption>
<page confidence="0.988608">
217
</page>
<bodyText confidence="0.999934">
the Event Recogniser. There are also many
inadequacies in the current implementation of
the Message Analyser, eg in its handling of
abbreviations/phrases, and in the handling of
input that is &amp;quot;ill-formed&amp;quot; even with respect
to our relatively unconstrained grammar.
However, work is currently in progress
on these problems, and we believe that the
basic mechanisms of interesting-corner parsing
are sufficiently powerful to enable us to
achieve a practical solution, whilst being
sufficiently general to ensure that such a
solution will be theoretically interesting.
</bodyText>
<sectionHeader confidence="0.998922" genericHeader="conclusions">
4. CONCLUSION
</sectionHeader>
<bodyText confidence="0.999948318181818">
The automatic production of traffic
broadcasts, given the type of free text we
have described in this paper, poses many
difficult problems. In many ways our overall
approach to these problems follows in a long
tradition of semantically driven systems, but
the processing style of our Message Analyser
is much closer to that used in contemporary
syntax-driven systems. We make explicit use
of rules in a unification-based grammatical
formalism that express both semantic and
syntactic information declaratively, and our
interesting-corner parser provides a search of
the input messages that is both thorough and
efficient.
We believe that complete understanding
of free text messages is well beyond the
state of the art in computational linguistics,
but that we can nevertheless develop the
TICC&apos;s Natural Language Summariser to have
sufficient partial understanding to be practi-
cally useful.
</bodyText>
<sectionHeader confidence="0.999829" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999834070422535">
Becker, J.D. (1975) &amp;quot;The Phrasal Lexi-
con&amp;quot;,i n R. C. Schank and B. L. Nash-
Webber (eds.), Proceedings of the Workshop on
Theoretical Issues in Natural Language Pro-
cessing. Cambridge, Mass., Bolt, Beranek and
Newman, pp. 70-73.
Burton, R. (1976) &amp;quot;Semantic Grammar: an
Engineering Technique for Constructing
Natural Language Understanding Systems&amp;quot;,
Technical Report 3453, Cambridge, Mass.,
Bolt, Beranek and Newman.
Chevalier, M., Dansereau, J., and Poulin,
G. (1978) &amp;quot;TAUM-METEO: Description Du
Systeme.&amp;quot;, Montreal, Groupe TAUM, Univer-
site de Montreal.
DeJong, G.F. (1979) &amp;quot;Skimming Stories in
Real Time&amp;quot;, Doctoral Thesis, New Haven,
Yale University.
Dejong, G.F. (1982) &amp;quot;An Overview of the
FRUMP System&amp;quot;, in Wendy G. Lehnert and
Martin H. Ringle (eds.), Strategies for Natural
Language Processing. Hillsdale, Erlbaum, pp.
149-176.
Earley, J. (1970) &amp;quot;An Efficient Context-
free Parsing Algorithm&amp;quot;, Communications of
the ACM. vol. 6, no. 8, pp. 451-455.
Gazdar, G., Klein, E., Pullum, G., and
Sag, I. (1985) Generalised Phrase Structure
Grammar. Oxford, Blackwell.
Kaplan, R., and Bresnan, J. (1982) &amp;quot;Lexi-
cal Functional Grammar: a Formal System
for Grammatical Representation&amp;quot;, in Joan
Bresnan (ed.), The Mental Representation of
Grammatical Relations. Cambridge MA, MIT
Press, pp. 173-281.
Kay, M. (1973) &amp;quot;The MIND System&amp;quot;, in
Randall Rustin (ed.), Natural Language Pro-
cessing. New York, Algorithmics Press, pp.
155-188.
Kay, M. (1985) &amp;quot;Parsing in Functional
Unification Grammar&amp;quot;, in David R. Dowty,
Lauri Karttunen and Arnold M. Zwicky
(eds.), Natural Language Parsing. Cambridge,
Cambridge University Press, pp. 251-278.
Kwasny, S.C., and Sondheimer, N.K.
(1981) &amp;quot;Relaxation Theories for Parsing In-
formed Input&amp;quot;, American Journal of Computa-
tional Linguistics. vol. 7, no. 2, pp. 99-108.
Pereira, F.C.N., and Warren, D.H.D.
(1980) &amp;quot;Definite Clause Grammars for
Language Analysis - a Survey of the Formal-
ism and a Comparison with Augmented
Transition Networks&amp;quot;, Artificial Intelligence.
vol. 13, no. 3, pp. 231-278.
Proudian, D., and Pollard, C.J. (1985)
&amp;quot;Parsing Head-driven Phrase Structure Gram-
mar&amp;quot;, ACL Proceedings, 23rd Annual Meeting.
pp. 167-171.
Riesbeck, C.K. (1978) &amp;quot;An Expectation-
driven Production System for Natural
Language Understanding&amp;quot;, in Donald A.
Waterman and Rick Hayes-Roth (eds.),
Pattern-directed Inference Systems. New York,
Academic Press, pp. 399-414.
Riesbeck, C.K. (1982) &amp;quot;Realistic Language
Comprehension&amp;quot;, in Wendy G. Lehnert and
Martin H. Ringle (eds.), Strategies for Natural
Language Processing. Hillsdale, Erlbaum, pp.
37-54.
Schank, R.C. (1975) Conceptual Informa-
tion Processing. Amsterdam, North-Holland.
</reference>
<page confidence="0.99717">
218
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.801407">
<title confidence="0.997863">THE TICC: PARSING INTERESTING TEXT.</title>
<author confidence="0.999679">David Allport</author>
<affiliation confidence="0.999953">School of Cognitive Sciences University of Sussex,</affiliation>
<address confidence="0.9204385">Falmer, Brighton BN1 9QN</address>
<email confidence="0.996991">davidalouk.ac.sussexxvaxa@cs.ucl.ac.uk</email>
<abstract confidence="0.9973374375">This paper gives an overview of the natural language problems addressed in the Traffic Information Collator/Condenser (TICC) project, and describes in some detail the &amp;quot;interesting-corner parser&amp;quot; used in the TICC&apos;s Natural Language Summariser. The TICC is designed to take free text input describing local traffic incidents, and automatically output local traffic information broadcasts for motorists in appropriate geographical areas. The &amp;quot;interesting-corner parser&amp;quot; uses both syntactic and semantic information, represented as features in a unification-based grammar, to guide its bi-directional search for significant phrasal groups.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J D Becker</author>
</authors>
<title>The Phrasal Lexicon&amp;quot;,i n</title>
<date>1975</date>
<booktitle>Proceedings of the Workshop on Theoretical Issues in Natural Language Processing.</booktitle>
<pages>70--73</pages>
<editor>R. C. Schank and B. L. NashWebber (eds.),</editor>
<location>Cambridge, Mass., Bolt, Beranek and Newman,</location>
<contexts>
<context position="6699" citStr="Becker 1975" startWordPosition="1083" endWordPosition="1084">or officer in charge. Non-canonical sequences may have a variety of abbreviations for each of the constituent words, and may or may not have slash or period punctuation, eg. f/b for fire brigade, eamb or esamb for east (sussex) ambulance, hazchem for hazardous chemicals. The problem is compounded for the TICC by the fact that the input we receive is all in upper case, hence the even the convention of distinguishing proper name abbreviations by upper case is not applicable. In order to cope with these different types of input string, we need not only a &amp;quot;phrasal lexicon&amp;quot; as advocated by Becker [Becker 1975], but also an &amp;quot;abbreviation lexicon&amp;quot;. Time: 1634 Location: scaynes hill, haywards heath 1634 rta serious near top scaynes hill persons trapped rqst esamb f/b 1/2 mile south jw freshfield rd. 1638 fm pc 123 ace inv reqd poss black oic pc 456 1639 fire and amb en route 1642 req total lift for saloon car rota garage 1654 eamb now away from scene 1655 freshileld bodyshop on way 1657 fm pc 456 req rd closed n and s of hill st crnr 1658 req two traff units to assist re closures 1709 can we inform brighton 1234 tell mr fred smith will be late due to this rta 1715 local authority required loose pavin</context>
</contexts>
<marker>Becker, 1975</marker>
<rawString>Becker, J.D. (1975) &amp;quot;The Phrasal Lexicon&amp;quot;,i n R. C. Schank and B. L. NashWebber (eds.), Proceedings of the Workshop on Theoretical Issues in Natural Language Processing. Cambridge, Mass., Bolt, Beranek and Newman, pp. 70-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Burton</author>
</authors>
<title>Semantic Grammar: an Engineering Technique for Constructing Natural Language Understanding Systems&amp;quot;,</title>
<date>1976</date>
<tech>Technical Report 3453,</tech>
<location>Cambridge, Mass., Bolt, Beranek and Newman.</location>
<contexts>
<context position="14496" citStr="Burton 1976" startWordPosition="2395" endWordPosition="2396">t of a verb followed by a noun phrase, and that the semantic features on the noun phrase (labelled P) must unify with the semantic features specified as the value of the patient sub-feature of the verb&apos;s semantic features, and additionally that the semantic features on the whole verb phrase (labelled VSF) must unify with the (complete tree of) semantic features on the verb. By adding domain-specific semantic feature information to lexical categories, we gain the power of domain-specific semantic grammars, which have been shown to be successful for handling ill-formed input in limited domains [Burton 1976]. But because we use unification by extension as the basic criterion for node admissability when we test for rules to licence local trees, we can also capture generalisations about syntactic categories that are not domain-specific. So for example if we had a verb-phrase rule such as (2) and a lexical entry as in (3): (2) vp —&gt; v=(tr=trans), np (3) close v=(tr=(trans), sf=(event_type=road_closure, agent=service, patient=roadlocation)) then the verb feature tree specified in (2) would unify with the verb feature tree in (3). Hence close can be treated both as a domain specific verb and as an in</context>
</contexts>
<marker>Burton, 1976</marker>
<rawString>Burton, R. (1976) &amp;quot;Semantic Grammar: an Engineering Technique for Constructing Natural Language Understanding Systems&amp;quot;, Technical Report 3453, Cambridge, Mass., Bolt, Beranek and Newman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chevalier</author>
<author>J Dansereau</author>
<author>G Poulin</author>
</authors>
<title>TAUM-METEO: Description Du Systeme.&amp;quot;,</title>
<date>1978</date>
<institution>TAUM, Universite de Montreal.</institution>
<location>Montreal, Groupe</location>
<contexts>
<context position="1942" citStr="Chevalier et al. 1978" startWordPosition="287" endWordPosition="290">fic incidents, and to extract from these messages any information that might be relevant for broadcast to other motorists. The Natural Language Summariser is designed to work in a restricted domain, and only needs to solve a subset of the problems of text understanding. The TICC&apos;s output messages are short and very simple assemblies of canned text, posing no significant natural language generation problems. Our main concern is that the messages should be useful to motorists, i.e that they be reliable indications of the state of the roads at the time they are broadcast. Programs such as METE° [Chevalier et al. 1978] have demonstrated that in a restricted domain with a restricted sub-language, automatic information broadcasts can be useful. Programs such as FRUMP [De Jong 1979, De Jong 1982] have also demonstrated that expectation-driven analysers can often successfully capture the gist of free text. However, the top-down depth-first confirmation of expectations based on sketchy scripts, ignoring most of the input structure, can lead to serious misinterpretations [Riesbeck 82]. Our concern for accuracy of interpretation has led us to a processing strategy in which the Natural Language Summariser analyses</context>
</contexts>
<marker>Chevalier, Dansereau, Poulin, 1978</marker>
<rawString>Chevalier, M., Dansereau, J., and Poulin, G. (1978) &amp;quot;TAUM-METEO: Description Du Systeme.&amp;quot;, Montreal, Groupe TAUM, Universite de Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G F DeJong</author>
</authors>
<title>Skimming Stories in Real Time&amp;quot;, Doctoral Thesis,</title>
<date>1979</date>
<location>New Haven, Yale University.</location>
<marker>DeJong, 1979</marker>
<rawString>DeJong, G.F. (1979) &amp;quot;Skimming Stories in Real Time&amp;quot;, Doctoral Thesis, New Haven, Yale University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G F Dejong</author>
</authors>
<title>An Overview of the FRUMP System&amp;quot;,</title>
<date>1982</date>
<booktitle>Strategies for Natural Language Processing.</booktitle>
<pages>149--176</pages>
<editor>in Wendy G. Lehnert and Martin H. Ringle (eds.),</editor>
<location>Hillsdale, Erlbaum,</location>
<marker>Dejong, 1982</marker>
<rawString>Dejong, G.F. (1982) &amp;quot;An Overview of the FRUMP System&amp;quot;, in Wendy G. Lehnert and Martin H. Ringle (eds.), Strategies for Natural Language Processing. Hillsdale, Erlbaum, pp. 149-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An Efficient Contextfree Parsing Algorithm&amp;quot;,</title>
<date>1970</date>
<journal>Communications of the ACM.</journal>
<volume>6</volume>
<pages>451--455</pages>
<contexts>
<context position="2988" citStr="Earley 1970" startWordPosition="453" endWordPosition="454">us misinterpretations [Riesbeck 82]. Our concern for accuracy of interpretation has led us to a processing strategy in which the Natural Language Summariser analyses the input text at a far greater level of detail than is given in the output messages, so the system &amp;quot;knows more&amp;quot; about the traffic incidents it is describing than it says in its broadcasts. Our parser uses both syntactic and semantic information to guide its search for phrases in the input that might be directly or indirectly relevant to motorists, and explores alternative possible interpretations bottom-up using an active chart [Earley 1970, Kay 1973]. This is an ongoing research project, and we do not claim to have solved all the problems involved in developing a successful system yet. The current paper considers the particular natural language problems we are addressing and describes the &amp;quot;interestingcorner parser&amp;quot; that has been implemented in the prototype system. 2. THE NATURAL LANGUAGE SUMMARISER&apos;S TASK 2.1 INPUT: Our input data comes from the Sussex Police, who have a computer system for storing the text of incoming traffic messages from a variety of sources (eg. patrol cars, emergency services, motoring organisations). An </context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, J. (1970) &amp;quot;An Efficient Contextfree Parsing Algorithm&amp;quot;, Communications of the ACM. vol. 6, no. 8, pp. 451-455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalised Phrase Structure Grammar.</title>
<date>1985</date>
<location>Oxford, Blackwell.</location>
<contexts>
<context position="12951" citStr="Gazdar et al 1985" startWordPosition="2140" endWordPosition="2143"> all syntactic parses associated with semantically interesting parts of the input. Before describing the search strategy in more detail, we need to clarify what a syntactic parse looks like in our grammar formalism, and how we specify what is semantically interesting. 3.1 THE GRAMMAR FORMALISM: We use a unification-based grammar formalism, with rules that look similar to context-free phrase-structure rules. Both immediate dominance and constituent ordering information are specified by the same rule, rather than by separate statements as in FUG [Kay 1985], LFG [Kaplan &amp; Bresnan 1982] and GPSG [Gazdar et al 1985]. Feature-passing between categories in rules is done explicitly with logical variables, rather than by conventions such as the HFC and FFP in GPSG [Gazdar et al 1985]. Thus the rule format is most similar to that used in DCG&apos;s [Pereira &amp; Warren 19801. Categories in rules are feature/value trees, and at each level the value of a feature may itself be another feature/value tree. Feature values may be given logical names, and occurrences of feature values having the same logical name in a rule must unify. The feature trees which constitute categories in our grammar may specify both syntactic an</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, G., Klein, E., Pullum, G., and Sag, I. (1985) Generalised Phrase Structure Grammar. Oxford, Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
<author>J Bresnan</author>
</authors>
<title>Lexical Functional Grammar: a Formal System for Grammatical Representation&amp;quot;,</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations. Cambridge MA,</booktitle>
<pages>173--281</pages>
<editor>in Joan Bresnan (ed.),</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="12922" citStr="Kaplan &amp; Bresnan 1982" startWordPosition="2134" endWordPosition="2137">yser searches bidirectionally for all syntactic parses associated with semantically interesting parts of the input. Before describing the search strategy in more detail, we need to clarify what a syntactic parse looks like in our grammar formalism, and how we specify what is semantically interesting. 3.1 THE GRAMMAR FORMALISM: We use a unification-based grammar formalism, with rules that look similar to context-free phrase-structure rules. Both immediate dominance and constituent ordering information are specified by the same rule, rather than by separate statements as in FUG [Kay 1985], LFG [Kaplan &amp; Bresnan 1982] and GPSG [Gazdar et al 1985]. Feature-passing between categories in rules is done explicitly with logical variables, rather than by conventions such as the HFC and FFP in GPSG [Gazdar et al 1985]. Thus the rule format is most similar to that used in DCG&apos;s [Pereira &amp; Warren 19801. Categories in rules are feature/value trees, and at each level the value of a feature may itself be another feature/value tree. Feature values may be given logical names, and occurrences of feature values having the same logical name in a rule must unify. The feature trees which constitute categories in our grammar </context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>Kaplan, R., and Bresnan, J. (1982) &amp;quot;Lexical Functional Grammar: a Formal System for Grammatical Representation&amp;quot;, in Joan Bresnan (ed.), The Mental Representation of Grammatical Relations. Cambridge MA, MIT Press, pp. 173-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>The MIND System&amp;quot;,</title>
<date>1973</date>
<booktitle>Natural Language Processing.</booktitle>
<pages>155--188</pages>
<editor>in Randall Rustin (ed.),</editor>
<publisher>Press,</publisher>
<location>New York, Algorithmics</location>
<contexts>
<context position="2998" citStr="Kay 1973" startWordPosition="455" endWordPosition="456">etations [Riesbeck 82]. Our concern for accuracy of interpretation has led us to a processing strategy in which the Natural Language Summariser analyses the input text at a far greater level of detail than is given in the output messages, so the system &amp;quot;knows more&amp;quot; about the traffic incidents it is describing than it says in its broadcasts. Our parser uses both syntactic and semantic information to guide its search for phrases in the input that might be directly or indirectly relevant to motorists, and explores alternative possible interpretations bottom-up using an active chart [Earley 1970, Kay 1973]. This is an ongoing research project, and we do not claim to have solved all the problems involved in developing a successful system yet. The current paper considers the particular natural language problems we are addressing and describes the &amp;quot;interestingcorner parser&amp;quot; that has been implemented in the prototype system. 2. THE NATURAL LANGUAGE SUMMARISER&apos;S TASK 2.1 INPUT: Our input data comes from the Sussex Police, who have a computer system for storing the text of incoming traffic messages from a variety of sources (eg. patrol cars, emergency services, motoring organisations). An example of</context>
</contexts>
<marker>Kay, 1973</marker>
<rawString>Kay, M. (1973) &amp;quot;The MIND System&amp;quot;, in Randall Rustin (ed.), Natural Language Processing. New York, Algorithmics Press, pp. 155-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Parsing in Functional Unification Grammar&amp;quot;,</title>
<date>1985</date>
<pages>251--278</pages>
<editor>in David R. Dowty, Lauri Karttunen and Arnold M. Zwicky (eds.),</editor>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="12893" citStr="Kay 1985" startWordPosition="2131" endWordPosition="2132">the Message Analyser searches bidirectionally for all syntactic parses associated with semantically interesting parts of the input. Before describing the search strategy in more detail, we need to clarify what a syntactic parse looks like in our grammar formalism, and how we specify what is semantically interesting. 3.1 THE GRAMMAR FORMALISM: We use a unification-based grammar formalism, with rules that look similar to context-free phrase-structure rules. Both immediate dominance and constituent ordering information are specified by the same rule, rather than by separate statements as in FUG [Kay 1985], LFG [Kaplan &amp; Bresnan 1982] and GPSG [Gazdar et al 1985]. Feature-passing between categories in rules is done explicitly with logical variables, rather than by conventions such as the HFC and FFP in GPSG [Gazdar et al 1985]. Thus the rule format is most similar to that used in DCG&apos;s [Pereira &amp; Warren 19801. Categories in rules are feature/value trees, and at each level the value of a feature may itself be another feature/value tree. Feature values may be given logical names, and occurrences of feature values having the same logical name in a rule must unify. The feature trees which constitu</context>
</contexts>
<marker>Kay, 1985</marker>
<rawString>Kay, M. (1985) &amp;quot;Parsing in Functional Unification Grammar&amp;quot;, in David R. Dowty, Lauri Karttunen and Arnold M. Zwicky (eds.), Natural Language Parsing. Cambridge, Cambridge University Press, pp. 251-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Kwasny</author>
<author>N K Sondheimer</author>
</authors>
<title>Relaxation Theories for Parsing Informed Input&amp;quot;,</title>
<date>1981</date>
<journal>American Journal of Computational Linguistics.</journal>
<volume>7</volume>
<pages>99--108</pages>
<contexts>
<context position="16445" citStr="Kwasny &amp; Sondheimer 1981" startWordPosition="2709" endWordPosition="2712">cp messages at 1638 and 1723 in fig. 1 above). 3.2 A GRAMMAR FOR THE TICC DOMAIN: Writing a grammar to give adequate coverage of the input that our system must handle is a lengthy task, which will continue over the next two years. However, analysis of a corpus of data from police logs of over one hundred incidents in the Sussex area, and trials with experimental grammars, have led us to adopt a style of grammar which we expect will remain constant as the grammar expands. We do not attempt to map telegraphic forms onto &amp;quot;fully grammatical&amp;quot; English forms by some variant of constraint relaxation [Kwasny &amp; Sondheimer 1981]. We simply have a grammar with fewer constraints. This is because it is not always easy to decide what is missing from an elliptical sentence, or which constraints should be relaxed. Consider for example the message at 1655 from fig. 1, repeated here: (4) freshfield bodyshop on way It is not at all clear what the &amp;quot;full&amp;quot; sentential form of this message ought to be, since it might also have been phrased as one of: (5.1) freshfield bodyshop is on the way (5.2) freshfield bodyshop is on its way (5.3) freshfield bodyshop are on the way (5.4) freshfield bodyshop are on their way Each of the (5.1)-</context>
</contexts>
<marker>Kwasny, Sondheimer, 1981</marker>
<rawString>Kwasny, S.C., and Sondheimer, N.K. (1981) &amp;quot;Relaxation Theories for Parsing Informed Input&amp;quot;, American Journal of Computational Linguistics. vol. 7, no. 2, pp. 99-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>D H D Warren</author>
</authors>
<title>Definite Clause Grammars for Language Analysis - a Survey of the Formalism and a Comparison with Augmented Transition Networks&amp;quot;,</title>
<date>1980</date>
<journal>Artificial Intelligence.</journal>
<volume>13</volume>
<pages>231--278</pages>
<contexts>
<context position="13202" citStr="Pereira &amp; Warren 1980" startWordPosition="2184" endWordPosition="2187">ntically interesting. 3.1 THE GRAMMAR FORMALISM: We use a unification-based grammar formalism, with rules that look similar to context-free phrase-structure rules. Both immediate dominance and constituent ordering information are specified by the same rule, rather than by separate statements as in FUG [Kay 1985], LFG [Kaplan &amp; Bresnan 1982] and GPSG [Gazdar et al 1985]. Feature-passing between categories in rules is done explicitly with logical variables, rather than by conventions such as the HFC and FFP in GPSG [Gazdar et al 1985]. Thus the rule format is most similar to that used in DCG&apos;s [Pereira &amp; Warren 19801. Categories in rules are feature/value trees, and at each level the value of a feature may itself be another feature/value tree. Feature values may be given logical names, and occurrences of feature values having the same logical name in a rule must unify. The feature trees which constitute categories in our grammar may specify both syntactic and semantic features, so that we can write &amp;quot;syntactic&amp;quot; rules which also identify the semantic types of their constituents. For example, if we use the feature sf on categories to specify a tree of semantic features for that category, then the rule: (1) </context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, F.C.N., and Warren, D.H.D. (1980) &amp;quot;Definite Clause Grammars for Language Analysis - a Survey of the Formalism and a Comparison with Augmented Transition Networks&amp;quot;, Artificial Intelligence. vol. 13, no. 3, pp. 231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Proudian</author>
<author>C J Pollard</author>
</authors>
<title>Parsing Head-driven Phrase Structure Grammar&amp;quot;,</title>
<date>1985</date>
<booktitle>ACL Proceedings, 23rd Annual Meeting.</booktitle>
<pages>167--171</pages>
<contexts>
<context position="27200" citStr="Proudian &amp; Pollard 1985" startWordPosition="4464" endWordPosition="4467">of logical variables in rules is not order-independent, and hence we need an indexing principle to determine the direction in which particular rules should be parsed. 3.3.3 THE INDEXING PRINCIPLE: Our general principle for indexing rules is that we must parse from categories that specify general information (ie. that have small feature-trees) to those that specify particular modifications of that general information (ie. that provide extensions to the smaller trees by unification). This usually means that we parse from syntactic heads to complements, eg indexing sentences on the vp (cf. HPSG [Proudian &amp; Pollard 19851). In our example rule (1), we index on the verb, because its expectations specify the general semantic type of the object, and the semantic feature tree of the noun-phrase will specify a sub-type of this general type, and therefore will be an extension of the verb&apos;s patient semantic feature tree. In the example shown in fig 3, the semantic tree of the np built at node 4 is: (sf4roadlocation.4name—huntingdon, rtitle—lane))) which unifies by extension with the feature tree (sf—roadlocation)), and this as we saw above became the expected semantic tree for the noun-phrase when rule (1) unified w</context>
</contexts>
<marker>Proudian, Pollard, 1985</marker>
<rawString>Proudian, D., and Pollard, C.J. (1985) &amp;quot;Parsing Head-driven Phrase Structure Grammar&amp;quot;, ACL Proceedings, 23rd Annual Meeting. pp. 167-171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Riesbeck</author>
</authors>
<title>An Expectationdriven Production System for Natural Language Understanding&amp;quot;,</title>
<date>1978</date>
<pages>399--414</pages>
<editor>in Donald A. Waterman and Rick Hayes-Roth (eds.), Pattern-directed</editor>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<contexts>
<context position="19701" citStr="Riesbeck 1978" startWordPosition="3247" endWordPosition="3248">antic rules associated with each syntactic rule, as phrases are completed in the bottom-up parse. 3.3 THE SEARCH STRATEGY: Interesting-corner parsing is basically an adaptation of bottom-up chart parsing to allow island-driving through the input string, whilst still parsing each individual rule unidirectionally. This gives a maximally efficient parse for our goal of reliably extracting from the input all and only the information that is relevant to other motorists. This form of expectation-driven parsing differs from that used in earlier script-based systems such as MARGIE [Schenk 1975], ELI [Riesbeck 1978] and FRUMP in four ways: First, the interesting-corner parser uses an active chart to consider bottom-up all interesting interpretations that might be given to an input message, rather than proceeding left to right and filtering out later (right) candidate interpretations on the basis of earlier (left) context. Second, if there are no interesting lexical items in the input string, or if the only interesting items occur at the (right) end of the input, there is no attempt to match all the leftmost items to a series of candidate scripts or frames using top-down expectations. 215 Third, the expe</context>
</contexts>
<marker>Riesbeck, 1978</marker>
<rawString>Riesbeck, C.K. (1978) &amp;quot;An Expectationdriven Production System for Natural Language Understanding&amp;quot;, in Donald A. Waterman and Rick Hayes-Roth (eds.), Pattern-directed Inference Systems. New York, Academic Press, pp. 399-414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Riesbeck</author>
</authors>
<title>Realistic Language Comprehension&amp;quot;,</title>
<date>1982</date>
<booktitle>Strategies for Natural Language Processing.</booktitle>
<pages>37--54</pages>
<editor>in Wendy G. Lehnert and Martin H. Ringle (eds.),</editor>
<location>Hillsdale, Erlbaum,</location>
<marker>Riesbeck, 1982</marker>
<rawString>Riesbeck, C.K. (1982) &amp;quot;Realistic Language Comprehension&amp;quot;, in Wendy G. Lehnert and Martin H. Ringle (eds.), Strategies for Natural Language Processing. Hillsdale, Erlbaum, pp. 37-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
</authors>
<title>Conceptual Information Processing.</title>
<date>1975</date>
<location>Amsterdam, North-Holland.</location>
<marker>Schank, 1975</marker>
<rawString>Schank, R.C. (1975) Conceptual Information Processing. Amsterdam, North-Holland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>