<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002381">
<title confidence="0.9989125">
Improving A Simple Bigram HMM Part-of-Speech Tagger by Latent
Annotation and Self-Training
</title>
<author confidence="0.999558">
Zhongqiang Huang†, Vladimir Eidelman†, Mary Harper†$
</author>
<affiliation confidence="0.971617">
†Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
University of Maryland, College Park
$Human Language Technology Center of Excellence
Johns Hopkins University
</affiliation>
<email confidence="0.999446">
{zqhuang,vlad,mharper}@umiacs.umd.edu
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999870944444444">
In this paper, we describe and evaluate a bi-
gram part-of-speech (POS) tagger that uses
latent annotations and then investigate using
additional genre-matched unlabeled data for
self-training the tagger. The use of latent
annotations substantially improves the per-
formance of a baseline HMM bigram tag-
ger, outperforming a trigram HMM tagger
with sophisticated smoothing. The perfor-
mance of the latent tagger is further enhanced
by self-training with a large set of unlabeled
data, even in situations where standard bigram
or trigram taggers do not benefit from self-
training when trained on greater amounts of
labeled training data. Our best model obtains
a state-of-the-art Chinese tagging accuracy of
94.78% when evaluated on a representative
test set of the Penn Chinese Treebank 6.0.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958">
Part-of-speech (POS) tagging, the process of as-
signing every word in a sentence with a POS tag
(e.g., NN (normal noun) or JJ (adjective)), is pre-
requisite for many advanced natural language pro-
cessing tasks. Building upon the large body of re-
search to improve tagging performance for various
languages using various models (e.g., (Thede and
Harper, 1999; Brants, 2000; Tseng et al., 2005b;
Huang et al., 2007)) and the recent work on PCFG
grammars with latent annotations (Matsuzaki et al.,
2005; Petrov et al., 2006), we will investigate the use
of fine-grained latent annotations for Chinese POS
tagging. While state-of-the-art tagging systems have
achieved accuracies above 97% in English, Chinese
POS tagging (Tseng et al., 2005b; Huang et al.,
2007) has proven to be more challenging, and it is
the focus of this study.
The value of the latent variable approach for tag-
ging is that it can learn more fine grained tags to bet-
ter model the training data. Liang and Klein (2008)
analyzed the errors of unsupervised learning using
EM and found that both estimation and optimiza-
tion errors decrease as the amount of unlabeled data
increases. In our case, the learning of latent anno-
tations through EM may also benefit from a large
set of automatically labeled data to improve tagging
performance. Semi-supervised, self-labeled data has
been effectively used to train acoustic models for
speech recognition (Ma and Schwartz, 2008); how-
ever, early investigations of self-training on POS
tagging have mixed outcomes. Clark et al. (2003)
reported positive results with little labeled training
data but negative results when the amount of labeled
training data increases. Wang et al. (2007) reported
that self-training improves a trigram tagger’s accu-
racy, but this tagger was trained with only a small
amount of in-domain labeled data.
In this paper, we will investigate whether the
performance of a simple bigram HMM tagger can
be improved by introducing latent annotations and
whether self-training can further improve its perfor-
mance. To the best of our knowledge, this is the first
attempt to use latent annotations with self-training
to enhance the performance of a POS tagger.
</bodyText>
<sectionHeader confidence="0.927954" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.487218">
POS tagging using a hidden Markov model can be
considered as an instance of Bayesian inference,
</bodyText>
<page confidence="0.984934">
213
</page>
<note confidence="0.3601625">
Proceedings of NAACL HLT 2009: Short Papers, pages 213–216,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.991667">
wherein we observe a sequence of words and need
to assign them the most likely sequence of POS tags.
If ti1 denotes the tag sequence t1, · · · , ti, and wi1
denotes the word sequence w1, · · · , wi, given the
first-order Markov assumption of a bigram tagger,
the best tag sequence τ(wn1) for sentence wn1 can be
computed efficiently as1:
</bodyText>
<equation confidence="0.9911415">
τ(wn1 ) = arg maxtl p(tn1 |wn1 )
ll
≈ arg maxtn i p(ti|ti−1)p(wi|ti)
1
</equation>
<bodyText confidence="0.966069837837838">
with a set of transition parameters {p(b|a)}, for tran-
siting to tag b from tag a, and a set of emission
parameters {p(w|a)}, for generating word w from
tag a. A simple HMM tagger is trained by pulling
counts from labeled data and normalizing to get the
conditional probabilities.
It is well know that the independence assumption
of a bigram tagger is too strong in many cases. A
common practice for weakening the independence
assumption is to use a second-order Markov as-
sumption, i.e., a trigram tagger. This is similar to
explicitly annotating each POS tag with the preced-
ing tag. Rather than explicit annotation, we could
use latent annotations to split the POS tags, sim-
ilarly to the introduction of latent annotations to
PCFG grammars (Matsuzaki et al., 2005; Petrov
et al., 2006). For example, the NR tag may be
split into NR-1 and NR-2, and correspondingly the
POS tag sequence of “Mr./NR Smith/NR saw/VV
Ms./NR Smith/NR” could be refined as: “Mr./NR-2
Smith/NR-1 saw/VV-2 Ms./NR-2 Smith/NR-1”.
The objective of training a bigram tagger with la-
tent annotations is to find the transition and emission
probabilities associated with the latent tags such that
the likelihood of the training data is maximized. Un-
like training a standard bigram tagger where the POS
tags are observed, in the latent case, the latent tags
are not observable, and so a variant of EM algorithm
is used to estimate the parameters.
Given a sentence wn1 and its tag sequence tn1, con-
sider the i-th word wi and its latent tag ax E a = ti
(which means ax is a latent tag of tag a, the i-th tag
in the sequence) and the (i + 1)-th word wi+1 and
its latent tag by E b = ti+1, the forward, αi+1(by) =
p(wi+1
1 , by), and backward, βi(ax) = p(wni+1|ax),
probabilities can be computed recursively:
</bodyText>
<equation confidence="0.980928">
Eαi+1(by) = xαi(ax)p(by|ax)p(wi+1|by)
</equation>
<footnote confidence="0.851219">
1We assume that symbols exist implicitly for boundary con-
ditions.
</footnote>
<equation confidence="0.9984345">
E
βi(ax) = y p(by|ax)p(wi+1|by)βj+1(by)
</equation>
<bodyText confidence="0.848611">
In the E step, the posterior probabilities of co-
occurrence events can be computed as:
</bodyText>
<equation confidence="0.977732">
p(ax,by|w) ∝ αi(ax)p(by|ax)βi+1(by)
p(ax, wi|w) ∝ αi(ax)βi(ax)
</equation>
<bodyText confidence="0.999868333333333">
In the M step, the above posterior probabilities are
used as weighted observations to update the transi-
tion and emission probabilities2:
</bodyText>
<equation confidence="0.999793">
Ep(by|ax) = c(ax,by)/ by c(ax,by)
Ep(w|ax) = c(ax, w)/ w c(ax, w)
</equation>
<bodyText confidence="0.999668947368421">
A hierarchical split-and-merge method, similar
to (Petrov et al., 2006), is used to gradually increase
the number of latent annotations while allocating
them adaptively to places where they would pro-
duce the greatest increase in training likelihood (e.g.,
we observe heavy splitting in categories such as NN
(normal noun) and VV (verb), that cover a wide vari-
ety of words, but only minimal splitting in categories
like IJ (interjection) and ON (onomatopoeia)).
Whereas tag transition occurrences are frequent,
allowing extensive optimization using EM, word-tag
co-occurrences are sparser and more likely to suf-
fer from over-fitting. To handle this problem, we
map all words with frequency less than threshold3
λ to symbol unk and for each latent tag accumu-
late the word tag statistics of these rare words to
cr(ax, unk) = Ew:c(w)&lt;λ c(ax, w). These statistics
are redistributed among the rare words (w : c(w) &lt;
λ) to compute their emission probabilities:
</bodyText>
<equation confidence="0.934668">
c(ax, w) = cr(ax, unk) · c(a, w)/cr(a, unk)
Ep(w|ax) = c(ax, w)/ w c(ax, w)
</equation>
<bodyText confidence="0.999768875">
The impact of this rare word handling method will
be investigated in Section 3.
A character-based unknown word model, similar
to the one described in (Huang et al., 2007), is used
to handle unknown Chinese words during tagging.
A decoding method similar to the max-rule-product
method in (Petrov and Klein, 2007) is used to tag
sentences using our model.
</bodyText>
<sectionHeader confidence="0.999511" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.757477">
The Penn Chinese Treebank 6.0 (CTB6) (Xue et al.,
2005) is used as the labeled data in our study. CTB6
</bodyText>
<footnote confidence="0.98725">
2c(·) represents the count of the event.
3The value of λ is tuned on the development set.
</footnote>
<page confidence="0.997166">
214
</page>
<bodyText confidence="0.99955195">
contains news articles, which are used as the primary
source of labeled data in our experiments, as well as
broadcast news transcriptions. Since the news ar-
ticles were collected during different time periods
from different sources with a diversity of topics, in
order to obtain a representative split of train-test-
development sets, we divide them into blocks of 10
files in sorted order and for each block use the first
file for development, the second for test, and the re-
maining for training. The broadcast news data ex-
hibits many of the characteristics of newswire text
(it contains many nonverbal expressions, e.g., num-
bers and symbols, and is fully punctuated) and so is
also included in the training data set. We also uti-
lize a greater number of unlabeled sentences in the
self-training experiments. They are selected from
similar sources to the newswire articles, and are
normalized (Zhang and Kahn, 2008) and word seg-
mented (Tseng et al., 2005a). See Table 1 for a sum-
mary of the data used.
</bodyText>
<figure confidence="0.76522425">
Train Dev Test Unlabeled
sentences 24,416 1904 1975 210,000
words 678,811 51,229 52,861 6,254,947
Number of latent annotations
</figure>
<figureCaption confidence="0.998599">
Figure 1: The learning curves of the bigram tagger with
latent annotations on the development set.
</figureCaption>
<bodyText confidence="0.953329727272727">
Figure 1 plots the learning curves of two bigram
taggers with latent annotations (Bigram+LA:2 has
the special handling of rare words as described in
Section 2 while Bigram+LA:1 does not) and com-
pares its performance with a state-of-the-art trigram
HMM tagger (Huang et al., 2007) that uses trigram
transition and emission models together with bidi-
rectional decoding. Both bigram taggers initially
have much lower tagging accuracy than the trigram
tagger, due to its strong but invalid independence as-
sumption. As the number of latent annotations in-
creases, the bigram taggers are able to learn more
from the context based on the latent annotations,
and their performance improves significantly, out-
performing the trigram tagger. The performance
gap between the two bigram taggers suggests that
over-fitting occurs in the word emission model when
more latent annotations are available for optimiza-
tion; sharing the statistics among rare words alle-
viates some of the sparseness while supporting the
modeling of deeper dependencies among more fre-
quent events. In the later experiments, we use Bi-
gram+LA to denote the Bigram+LA:2 tagger.
Figure 2 compares the self-training capability of
three models (the bigram tagger w/ or w/o latent
annotations, and the aforementioned trigram tagger)
using different sizes of labeled training data and the
full set of unlabeled data. For each model, a tag-
ger is first trained on the allocated labeled training
data and is then used to tag the unlabeled data. A
new tagger is then trained on the combination4 of
the allocated labeled training data and the newly au-
tomatically labeled data.
</bodyText>
<figure confidence="0.647504777777778">
Token accuracy (%) 95 Bigram+LA+ST
94 Bigram+LA
93 Trigram+ST
92 Trigram
91 Bigram+ST
90 Bigram
89
0.1 0.2 0.4 0.6 0.8 1
Fraction of CTB6 training data
</figure>
<figureCaption confidence="0.994552333333333">
Figure 2: The performance of three taggers evaluated on
the development set, before and after self-training with
different sizes of labeled training data.
</figureCaption>
<bodyText confidence="0.999816272727273">
There are two interesting observations that distin-
guish the bigram tagger with latent annotations from
the other two taggers. First, although all of the tag-
gers improve as more labeled training data is avail-
able, the performance gap between the bigram tag-
ger with latent annotations and the other two taggers
also increases. This is because more latent annota-
tions can be used to take advantage of the additional
training data to learn deeper dependencies.
Second, the bigram tagger with latent annotations
benefits much more from self-training, although it
</bodyText>
<footnote confidence="0.99798425">
4We always balance the size of manually and automatically
labeled data through duplication (for the trigram tagger) or pos-
terior weighting (for the bigram tagger w/ or w/o latent annota-
tions), as this provides superior performance.
</footnote>
<tableCaption confidence="0.953901">
Table 1: The number of sentences and words in the data.
</tableCaption>
<figure confidence="0.97590975">
50 100 150 200 250 300 350 400
Token accuracy (%)
94.5
93.5
92.5
91.5
94
93
92
Bigram+LA:1
Bigram+LA:2
Trigram
</figure>
<page confidence="0.998714">
215
</page>
<bodyText confidence="0.999473666666667">
already has the highest performance among the three
taggers before self-training. The bigram tagger
without latent annotations benefits little from self-
training. Except for a slight improvement when
there is a small amount of labeled training, self-
training slightly hurts tagging performance as the
amount of labeled data increases. The trigram tag-
ger benefits from self-training initially but eventu-
ally has a similar pattern to the bigram tagger when
trained on the full labeled set. The performance
of the latent bigram tagger improves consistently
with self-training. Although the gain decreases for
models trained on larger training sets, since stronger
models are harder to improve, self-training still con-
tributes significantly to model accuracy.
The final tagging performance on the test set is
reported in Table 2. All of the improvements are
statistically significant (p &lt; 0.005).
</bodyText>
<table confidence="0.9760016">
Tagger Token Accuracy (%)
Bigram 92.25
Trigram 93.99
Bigram+LA 94.53
Bigram+LA+ST 94.78
</table>
<tableCaption confidence="0.999444">
Table 2: The performance of the taggers on the test set.
</tableCaption>
<bodyText confidence="0.999975857142857">
It is worth mentioning that we initially added la-
tent annotations to a trigram tagger, rather than a bi-
gram tagger, to build from a stronger starting point;
however, this did not work well. A trigram tagger re-
quires sophisticated smoothing to handle data spar-
sity, and introducing latent annotations exacerbates
the sparsity problem, especially for trigram word
emissions. The uniform extension of a bigram tag-
ger to a trigram tagger ignores whether the use of ad-
ditional context is helpful and supported by enough
data, nor is it able to use a longer context. In con-
trast, the bigram tagger with latent annotations is
able to learn different granularities for tags based on
the training data.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999957545454545">
In this paper, we showed that the accuracy of a sim-
ple bigram HMM tagger can be substantially im-
proved by introducing latent annotations together
with proper handling of rare words. We also showed
that this tagger is able to benefit from self-training,
despite the fact that other models, such as bigram or
trigram HMM taggers, do not.
In the future work, we will investigate automatic
data selection methods to choose materials that are
most suitable for self-training and evaluate the effect
of the amount of automatically labeled data.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999820833333333">
This work was supported by NSF IIS-0703859
and DARPA HR0011-06-C-0023 and HR0011-06-
2-001. Any opinions, findings and/or recommenda-
tions expressed in this paper are those of the authors
and do not necessarily reflect the views of the fund-
ing agencies.
</bodyText>
<sectionHeader confidence="0.999422" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999344025641026">
T. Brants. 2000. TnT a statistical part-of-speech tagger.
In ANLP.
S. Clark, J. R. Curran, and M. Osborne. 2003. Bootstrap-
ping pos taggers using unlabelled data. In CoNLL.
Z. Huang, M. Harper, and W. Wang. 2007. Mandarin
part-of-speech tagging and discriminative reranking.
EMNLP.
P. Liang and D. Klein. 2008. Analyzing the errors of
unsupervised learning. In ACL.
J. Ma and R. Schwartz. 2008. Factors that affect unsu-
pervised training of acoustic models. In Interspeech.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL. Association
for Computational Linguistics.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL.
S. M. Thede and M. P. Harper. 1999. A second-order
hidden markov model for part-of-speech tagging. In
ACL.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-
ning. 2005a. A conditional random field word seg-
menter. In SIGHAN Workshop on Chinese Language
Processing.
H. Tseng, D. Jurafsky, and C. Manning. 2005b. Morpho-
logical features help pos tagging of unknown words
across language varieties. In SIGHAN Workshop on
Chinese Language Processing.
W. Wang, Z. Huang, and M. Harper. 2007. Semi-
supervised learning for part-of-speech tagging of Man-
darin transcribed speech. In ICASSP.
N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The
penn chinese treebank: Phrase structure annotation of
a large corpus. Natural Language Engineering.
B. Zhang and J. G. Kahn. 2008. Evaluation of decatur
text normalizer for language model training. Technical
report, University of Washington.
</reference>
<page confidence="0.999145">
216
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.256012">
<title confidence="0.9993555">Improving A Simple Bigram HMM Part-of-Speech Tagger by Annotation and Self-Training</title>
<author confidence="0.992247">Vladimir Mary</author>
<affiliation confidence="0.9687665">for Computational Linguistics and Information Institute for Advanced Computer University of Maryland, College Language Technology Center of</affiliation>
<address confidence="0.289365">Johns Hopkins</address>
<abstract confidence="0.996682368421053">In this paper, we describe and evaluate a bigram part-of-speech (POS) tagger that uses latent annotations and then investigate using additional genre-matched unlabeled data for self-training the tagger. The use of latent annotations substantially improves the performance of a baseline HMM bigram tagger, outperforming a trigram HMM tagger with sophisticated smoothing. The performance of the latent tagger is further enhanced by self-training with a large set of unlabeled data, even in situations where standard bigram or trigram taggers do not benefit from selftraining when trained on greater amounts of labeled training data. Our best model obtains a state-of-the-art Chinese tagging accuracy of 94.78% when evaluated on a representative test set of the Penn Chinese Treebank 6.0.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In ANLP.</booktitle>
<contexts>
<context position="1578" citStr="Brants, 2000" startWordPosition="227" endWordPosition="228">rom selftraining when trained on greater amounts of labeled training data. Our best model obtains a state-of-the-art Chinese tagging accuracy of 94.78% when evaluated on a representative test set of the Penn Chinese Treebank 6.0. 1 Introduction Part-of-speech (POS) tagging, the process of assigning every word in a sentence with a POS tag (e.g., NN (normal noun) or JJ (adjective)), is prerequisite for many advanced natural language processing tasks. Building upon the large body of research to improve tagging performance for various languages using various models (e.g., (Thede and Harper, 1999; Brants, 2000; Tseng et al., 2005b; Huang et al., 2007)) and the recent work on PCFG grammars with latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006), we will investigate the use of fine-grained latent annotations for Chinese POS tagging. While state-of-the-art tagging systems have achieved accuracies above 97% in English, Chinese POS tagging (Tseng et al., 2005b; Huang et al., 2007) has proven to be more challenging, and it is the focus of this study. The value of the latent variable approach for tagging is that it can learn more fine grained tags to better model the training data. Liang and</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. TnT a statistical part-of-speech tagger. In ANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
<author>M Osborne</author>
</authors>
<title>Bootstrapping pos taggers using unlabelled data. In CoNLL.</title>
<date>2003</date>
<contexts>
<context position="2744" citStr="Clark et al. (2003)" startWordPosition="415" endWordPosition="418">ned tags to better model the training data. Liang and Klein (2008) analyzed the errors of unsupervised learning using EM and found that both estimation and optimization errors decrease as the amount of unlabeled data increases. In our case, the learning of latent annotations through EM may also benefit from a large set of automatically labeled data to improve tagging performance. Semi-supervised, self-labeled data has been effectively used to train acoustic models for speech recognition (Ma and Schwartz, 2008); however, early investigations of self-training on POS tagging have mixed outcomes. Clark et al. (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. Wang et al. (2007) reported that self-training improves a trigram tagger’s accuracy, but this tagger was trained with only a small amount of in-domain labeled data. In this paper, we will investigate whether the performance of a simple bigram HMM tagger can be improved by introducing latent annotations and whether self-training can further improve its performance. To the best of our knowledge, this is the first attempt to use latent annotations with self-trainin</context>
</contexts>
<marker>Clark, Curran, Osborne, 2003</marker>
<rawString>S. Clark, J. R. Curran, and M. Osborne. 2003. Bootstrapping pos taggers using unlabelled data. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Huang</author>
<author>M Harper</author>
<author>W Wang</author>
</authors>
<title>Mandarin part-of-speech tagging and discriminative reranking.</title>
<date>2007</date>
<publisher>EMNLP.</publisher>
<contexts>
<context position="1620" citStr="Huang et al., 2007" startWordPosition="233" endWordPosition="236">eater amounts of labeled training data. Our best model obtains a state-of-the-art Chinese tagging accuracy of 94.78% when evaluated on a representative test set of the Penn Chinese Treebank 6.0. 1 Introduction Part-of-speech (POS) tagging, the process of assigning every word in a sentence with a POS tag (e.g., NN (normal noun) or JJ (adjective)), is prerequisite for many advanced natural language processing tasks. Building upon the large body of research to improve tagging performance for various languages using various models (e.g., (Thede and Harper, 1999; Brants, 2000; Tseng et al., 2005b; Huang et al., 2007)) and the recent work on PCFG grammars with latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006), we will investigate the use of fine-grained latent annotations for Chinese POS tagging. While state-of-the-art tagging systems have achieved accuracies above 97% in English, Chinese POS tagging (Tseng et al., 2005b; Huang et al., 2007) has proven to be more challenging, and it is the focus of this study. The value of the latent variable approach for tagging is that it can learn more fine grained tags to better model the training data. Liang and Klein (2008) analyzed the errors of unsup</context>
<context position="7512" citStr="Huang et al., 2007" startWordPosition="1215" endWordPosition="1218">parser and more likely to suffer from over-fitting. To handle this problem, we map all words with frequency less than threshold3 λ to symbol unk and for each latent tag accumulate the word tag statistics of these rare words to cr(ax, unk) = Ew:c(w)&lt;λ c(ax, w). These statistics are redistributed among the rare words (w : c(w) &lt; λ) to compute their emission probabilities: c(ax, w) = cr(ax, unk) · c(a, w)/cr(a, unk) Ep(w|ax) = c(ax, w)/ w c(ax, w) The impact of this rare word handling method will be investigated in Section 3. A character-based unknown word model, similar to the one described in (Huang et al., 2007), is used to handle unknown Chinese words during tagging. A decoding method similar to the max-rule-product method in (Petrov and Klein, 2007) is used to tag sentences using our model. 3 Experiments The Penn Chinese Treebank 6.0 (CTB6) (Xue et al., 2005) is used as the labeled data in our study. CTB6 2c(·) represents the count of the event. 3The value of λ is tuned on the development set. 214 contains news articles, which are used as the primary source of labeled data in our experiments, as well as broadcast news transcriptions. Since the news articles were collected during different time peri</context>
<context position="9412" citStr="Huang et al., 2007" startWordPosition="1534" endWordPosition="1537">ized (Zhang and Kahn, 2008) and word segmented (Tseng et al., 2005a). See Table 1 for a summary of the data used. Train Dev Test Unlabeled sentences 24,416 1904 1975 210,000 words 678,811 51,229 52,861 6,254,947 Number of latent annotations Figure 1: The learning curves of the bigram tagger with latent annotations on the development set. Figure 1 plots the learning curves of two bigram taggers with latent annotations (Bigram+LA:2 has the special handling of rare words as described in Section 2 while Bigram+LA:1 does not) and compares its performance with a state-of-the-art trigram HMM tagger (Huang et al., 2007) that uses trigram transition and emission models together with bidirectional decoding. Both bigram taggers initially have much lower tagging accuracy than the trigram tagger, due to its strong but invalid independence assumption. As the number of latent annotations increases, the bigram taggers are able to learn more from the context based on the latent annotations, and their performance improves significantly, outperforming the trigram tagger. The performance gap between the two bigram taggers suggests that over-fitting occurs in the word emission model when more latent annotations are avail</context>
</contexts>
<marker>Huang, Harper, Wang, 2007</marker>
<rawString>Z. Huang, M. Harper, and W. Wang. 2007. Mandarin part-of-speech tagging and discriminative reranking. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>D Klein</author>
</authors>
<title>Analyzing the errors of unsupervised learning.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2191" citStr="Liang and Klein (2008)" startWordPosition="330" endWordPosition="333">nts, 2000; Tseng et al., 2005b; Huang et al., 2007)) and the recent work on PCFG grammars with latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006), we will investigate the use of fine-grained latent annotations for Chinese POS tagging. While state-of-the-art tagging systems have achieved accuracies above 97% in English, Chinese POS tagging (Tseng et al., 2005b; Huang et al., 2007) has proven to be more challenging, and it is the focus of this study. The value of the latent variable approach for tagging is that it can learn more fine grained tags to better model the training data. Liang and Klein (2008) analyzed the errors of unsupervised learning using EM and found that both estimation and optimization errors decrease as the amount of unlabeled data increases. In our case, the learning of latent annotations through EM may also benefit from a large set of automatically labeled data to improve tagging performance. Semi-supervised, self-labeled data has been effectively used to train acoustic models for speech recognition (Ma and Schwartz, 2008); however, early investigations of self-training on POS tagging have mixed outcomes. Clark et al. (2003) reported positive results with little labeled </context>
</contexts>
<marker>Liang, Klein, 2008</marker>
<rawString>P. Liang and D. Klein. 2008. Analyzing the errors of unsupervised learning. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ma</author>
<author>R Schwartz</author>
</authors>
<title>Factors that affect unsupervised training of acoustic models.</title>
<date>2008</date>
<booktitle>In Interspeech.</booktitle>
<contexts>
<context position="2640" citStr="Ma and Schwartz, 2008" startWordPosition="399" endWordPosition="402">us of this study. The value of the latent variable approach for tagging is that it can learn more fine grained tags to better model the training data. Liang and Klein (2008) analyzed the errors of unsupervised learning using EM and found that both estimation and optimization errors decrease as the amount of unlabeled data increases. In our case, the learning of latent annotations through EM may also benefit from a large set of automatically labeled data to improve tagging performance. Semi-supervised, self-labeled data has been effectively used to train acoustic models for speech recognition (Ma and Schwartz, 2008); however, early investigations of self-training on POS tagging have mixed outcomes. Clark et al. (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. Wang et al. (2007) reported that self-training improves a trigram tagger’s accuracy, but this tagger was trained with only a small amount of in-domain labeled data. In this paper, we will investigate whether the performance of a simple bigram HMM tagger can be improved by introducing latent annotations and whether self-training can further improve its performa</context>
</contexts>
<marker>Ma, Schwartz, 2008</marker>
<rawString>J. Ma and R. Schwartz. 2008. Factors that affect unsupervised training of acoustic models. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsuzaki</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In ACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1706" citStr="Matsuzaki et al., 2005" startWordPosition="247" endWordPosition="250">hinese tagging accuracy of 94.78% when evaluated on a representative test set of the Penn Chinese Treebank 6.0. 1 Introduction Part-of-speech (POS) tagging, the process of assigning every word in a sentence with a POS tag (e.g., NN (normal noun) or JJ (adjective)), is prerequisite for many advanced natural language processing tasks. Building upon the large body of research to improve tagging performance for various languages using various models (e.g., (Thede and Harper, 1999; Brants, 2000; Tseng et al., 2005b; Huang et al., 2007)) and the recent work on PCFG grammars with latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006), we will investigate the use of fine-grained latent annotations for Chinese POS tagging. While state-of-the-art tagging systems have achieved accuracies above 97% in English, Chinese POS tagging (Tseng et al., 2005b; Huang et al., 2007) has proven to be more challenging, and it is the focus of this study. The value of the latent variable approach for tagging is that it can learn more fine grained tags to better model the training data. Liang and Klein (2008) analyzed the errors of unsupervised learning using EM and found that both estimation and optimization errors decre</context>
<context position="4809" citStr="Matsuzaki et al., 2005" startWordPosition="764" endWordPosition="767">ord w from tag a. A simple HMM tagger is trained by pulling counts from labeled data and normalizing to get the conditional probabilities. It is well know that the independence assumption of a bigram tagger is too strong in many cases. A common practice for weakening the independence assumption is to use a second-order Markov assumption, i.e., a trigram tagger. This is similar to explicitly annotating each POS tag with the preceding tag. Rather than explicit annotation, we could use latent annotations to split the POS tags, similarly to the introduction of latent annotations to PCFG grammars (Matsuzaki et al., 2005; Petrov et al., 2006). For example, the NR tag may be split into NR-1 and NR-2, and correspondingly the POS tag sequence of “Mr./NR Smith/NR saw/VV Ms./NR Smith/NR” could be refined as: “Mr./NR-2 Smith/NR-1 saw/VV-2 Ms./NR-2 Smith/NR-1”. The objective of training a bigram tagger with latent annotations is to find the transition and emission probabilities associated with the latent tags such that the likelihood of the training data is maximized. Unlike training a standard bigram tagger where the POS tags are observed, in the latent case, the latent tags are not observable, and so a variant of </context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="7654" citStr="Petrov and Klein, 2007" startWordPosition="1237" endWordPosition="1240">ol unk and for each latent tag accumulate the word tag statistics of these rare words to cr(ax, unk) = Ew:c(w)&lt;λ c(ax, w). These statistics are redistributed among the rare words (w : c(w) &lt; λ) to compute their emission probabilities: c(ax, w) = cr(ax, unk) · c(a, w)/cr(a, unk) Ep(w|ax) = c(ax, w)/ w c(ax, w) The impact of this rare word handling method will be investigated in Section 3. A character-based unknown word model, similar to the one described in (Huang et al., 2007), is used to handle unknown Chinese words during tagging. A decoding method similar to the max-rule-product method in (Petrov and Klein, 2007) is used to tag sentences using our model. 3 Experiments The Penn Chinese Treebank 6.0 (CTB6) (Xue et al., 2005) is used as the labeled data in our study. CTB6 2c(·) represents the count of the event. 3The value of λ is tuned on the development set. 214 contains news articles, which are used as the primary source of labeled data in our experiments, as well as broadcast news transcriptions. Since the news articles were collected during different time periods from different sources with a diversity of topics, in order to obtain a representative split of train-testdevelopment sets, we divide them</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1728" citStr="Petrov et al., 2006" startWordPosition="251" endWordPosition="254">of 94.78% when evaluated on a representative test set of the Penn Chinese Treebank 6.0. 1 Introduction Part-of-speech (POS) tagging, the process of assigning every word in a sentence with a POS tag (e.g., NN (normal noun) or JJ (adjective)), is prerequisite for many advanced natural language processing tasks. Building upon the large body of research to improve tagging performance for various languages using various models (e.g., (Thede and Harper, 1999; Brants, 2000; Tseng et al., 2005b; Huang et al., 2007)) and the recent work on PCFG grammars with latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006), we will investigate the use of fine-grained latent annotations for Chinese POS tagging. While state-of-the-art tagging systems have achieved accuracies above 97% in English, Chinese POS tagging (Tseng et al., 2005b; Huang et al., 2007) has proven to be more challenging, and it is the focus of this study. The value of the latent variable approach for tagging is that it can learn more fine grained tags to better model the training data. Liang and Klein (2008) analyzed the errors of unsupervised learning using EM and found that both estimation and optimization errors decrease as the amount of u</context>
<context position="4831" citStr="Petrov et al., 2006" startWordPosition="768" endWordPosition="771">le HMM tagger is trained by pulling counts from labeled data and normalizing to get the conditional probabilities. It is well know that the independence assumption of a bigram tagger is too strong in many cases. A common practice for weakening the independence assumption is to use a second-order Markov assumption, i.e., a trigram tagger. This is similar to explicitly annotating each POS tag with the preceding tag. Rather than explicit annotation, we could use latent annotations to split the POS tags, similarly to the introduction of latent annotations to PCFG grammars (Matsuzaki et al., 2005; Petrov et al., 2006). For example, the NR tag may be split into NR-1 and NR-2, and correspondingly the POS tag sequence of “Mr./NR Smith/NR saw/VV Ms./NR Smith/NR” could be refined as: “Mr./NR-2 Smith/NR-1 saw/VV-2 Ms./NR-2 Smith/NR-1”. The objective of training a bigram tagger with latent annotations is to find the transition and emission probabilities associated with the latent tags such that the likelihood of the training data is maximized. Unlike training a standard bigram tagger where the POS tags are observed, in the latent case, the latent tags are not observable, and so a variant of EM algorithm is used t</context>
<context position="6383" citStr="Petrov et al., 2006" startWordPosition="1028" endWordPosition="1031">ni+1|ax), probabilities can be computed recursively: Eαi+1(by) = xαi(ax)p(by|ax)p(wi+1|by) 1We assume that symbols exist implicitly for boundary conditions. E βi(ax) = y p(by|ax)p(wi+1|by)βj+1(by) In the E step, the posterior probabilities of cooccurrence events can be computed as: p(ax,by|w) ∝ αi(ax)p(by|ax)βi+1(by) p(ax, wi|w) ∝ αi(ax)βi(ax) In the M step, the above posterior probabilities are used as weighted observations to update the transition and emission probabilities2: Ep(by|ax) = c(ax,by)/ by c(ax,by) Ep(w|ax) = c(ax, w)/ w c(ax, w) A hierarchical split-and-merge method, similar to (Petrov et al., 2006), is used to gradually increase the number of latent annotations while allocating them adaptively to places where they would produce the greatest increase in training likelihood (e.g., we observe heavy splitting in categories such as NN (normal noun) and VV (verb), that cover a wide variety of words, but only minimal splitting in categories like IJ (interjection) and ON (onomatopoeia)). Whereas tag transition occurrences are frequent, allowing extensive optimization using EM, word-tag co-occurrences are sparser and more likely to suffer from over-fitting. To handle this problem, we map all wor</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Thede</author>
<author>M P Harper</author>
</authors>
<title>A second-order hidden markov model for part-of-speech tagging.</title>
<date>1999</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1564" citStr="Thede and Harper, 1999" startWordPosition="223" endWordPosition="226">taggers do not benefit from selftraining when trained on greater amounts of labeled training data. Our best model obtains a state-of-the-art Chinese tagging accuracy of 94.78% when evaluated on a representative test set of the Penn Chinese Treebank 6.0. 1 Introduction Part-of-speech (POS) tagging, the process of assigning every word in a sentence with a POS tag (e.g., NN (normal noun) or JJ (adjective)), is prerequisite for many advanced natural language processing tasks. Building upon the large body of research to improve tagging performance for various languages using various models (e.g., (Thede and Harper, 1999; Brants, 2000; Tseng et al., 2005b; Huang et al., 2007)) and the recent work on PCFG grammars with latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006), we will investigate the use of fine-grained latent annotations for Chinese POS tagging. While state-of-the-art tagging systems have achieved accuracies above 97% in English, Chinese POS tagging (Tseng et al., 2005b; Huang et al., 2007) has proven to be more challenging, and it is the focus of this study. The value of the latent variable approach for tagging is that it can learn more fine grained tags to better model the training d</context>
</contexts>
<marker>Thede, Harper, 1999</marker>
<rawString>S. M. Thede and M. P. Harper. 1999. A second-order hidden markov model for part-of-speech tagging. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tseng</author>
<author>P Chang</author>
<author>G Andrew</author>
<author>D Jurafsky</author>
<author>C Manning</author>
</authors>
<title>A conditional random field word segmenter.</title>
<date>2005</date>
<booktitle>In SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="1598" citStr="Tseng et al., 2005" startWordPosition="229" endWordPosition="232">ng when trained on greater amounts of labeled training data. Our best model obtains a state-of-the-art Chinese tagging accuracy of 94.78% when evaluated on a representative test set of the Penn Chinese Treebank 6.0. 1 Introduction Part-of-speech (POS) tagging, the process of assigning every word in a sentence with a POS tag (e.g., NN (normal noun) or JJ (adjective)), is prerequisite for many advanced natural language processing tasks. Building upon the large body of research to improve tagging performance for various languages using various models (e.g., (Thede and Harper, 1999; Brants, 2000; Tseng et al., 2005b; Huang et al., 2007)) and the recent work on PCFG grammars with latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006), we will investigate the use of fine-grained latent annotations for Chinese POS tagging. While state-of-the-art tagging systems have achieved accuracies above 97% in English, Chinese POS tagging (Tseng et al., 2005b; Huang et al., 2007) has proven to be more challenging, and it is the focus of this study. The value of the latent variable approach for tagging is that it can learn more fine grained tags to better model the training data. Liang and Klein (2008) analyz</context>
<context position="8859" citStr="Tseng et al., 2005" startWordPosition="1444" endWordPosition="1447">e divide them into blocks of 10 files in sorted order and for each block use the first file for development, the second for test, and the remaining for training. The broadcast news data exhibits many of the characteristics of newswire text (it contains many nonverbal expressions, e.g., numbers and symbols, and is fully punctuated) and so is also included in the training data set. We also utilize a greater number of unlabeled sentences in the self-training experiments. They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al., 2005a). See Table 1 for a summary of the data used. Train Dev Test Unlabeled sentences 24,416 1904 1975 210,000 words 678,811 51,229 52,861 6,254,947 Number of latent annotations Figure 1: The learning curves of the bigram tagger with latent annotations on the development set. Figure 1 plots the learning curves of two bigram taggers with latent annotations (Bigram+LA:2 has the special handling of rare words as described in Section 2 while Bigram+LA:1 does not) and compares its performance with a state-of-the-art trigram HMM tagger (Huang et al., 2007) that uses trigram transition and emission mode</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Manning. 2005a. A conditional random field word segmenter. In SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tseng</author>
<author>D Jurafsky</author>
<author>C Manning</author>
</authors>
<title>Morphological features help pos tagging of unknown words across language varieties. In</title>
<date>2005</date>
<booktitle>SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="1598" citStr="Tseng et al., 2005" startWordPosition="229" endWordPosition="232">ng when trained on greater amounts of labeled training data. Our best model obtains a state-of-the-art Chinese tagging accuracy of 94.78% when evaluated on a representative test set of the Penn Chinese Treebank 6.0. 1 Introduction Part-of-speech (POS) tagging, the process of assigning every word in a sentence with a POS tag (e.g., NN (normal noun) or JJ (adjective)), is prerequisite for many advanced natural language processing tasks. Building upon the large body of research to improve tagging performance for various languages using various models (e.g., (Thede and Harper, 1999; Brants, 2000; Tseng et al., 2005b; Huang et al., 2007)) and the recent work on PCFG grammars with latent annotations (Matsuzaki et al., 2005; Petrov et al., 2006), we will investigate the use of fine-grained latent annotations for Chinese POS tagging. While state-of-the-art tagging systems have achieved accuracies above 97% in English, Chinese POS tagging (Tseng et al., 2005b; Huang et al., 2007) has proven to be more challenging, and it is the focus of this study. The value of the latent variable approach for tagging is that it can learn more fine grained tags to better model the training data. Liang and Klein (2008) analyz</context>
<context position="8859" citStr="Tseng et al., 2005" startWordPosition="1444" endWordPosition="1447">e divide them into blocks of 10 files in sorted order and for each block use the first file for development, the second for test, and the remaining for training. The broadcast news data exhibits many of the characteristics of newswire text (it contains many nonverbal expressions, e.g., numbers and symbols, and is fully punctuated) and so is also included in the training data set. We also utilize a greater number of unlabeled sentences in the self-training experiments. They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al., 2005a). See Table 1 for a summary of the data used. Train Dev Test Unlabeled sentences 24,416 1904 1975 210,000 words 678,811 51,229 52,861 6,254,947 Number of latent annotations Figure 1: The learning curves of the bigram tagger with latent annotations on the development set. Figure 1 plots the learning curves of two bigram taggers with latent annotations (Bigram+LA:2 has the special handling of rare words as described in Section 2 while Bigram+LA:1 does not) and compares its performance with a state-of-the-art trigram HMM tagger (Huang et al., 2007) that uses trigram transition and emission mode</context>
</contexts>
<marker>Tseng, Jurafsky, Manning, 2005</marker>
<rawString>H. Tseng, D. Jurafsky, and C. Manning. 2005b. Morphological features help pos tagging of unknown words across language varieties. In SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>Z Huang</author>
<author>M Harper</author>
</authors>
<title>Semisupervised learning for part-of-speech tagging of Mandarin transcribed speech.</title>
<date>2007</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="2896" citStr="Wang et al. (2007)" startWordPosition="438" endWordPosition="441">nd optimization errors decrease as the amount of unlabeled data increases. In our case, the learning of latent annotations through EM may also benefit from a large set of automatically labeled data to improve tagging performance. Semi-supervised, self-labeled data has been effectively used to train acoustic models for speech recognition (Ma and Schwartz, 2008); however, early investigations of self-training on POS tagging have mixed outcomes. Clark et al. (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. Wang et al. (2007) reported that self-training improves a trigram tagger’s accuracy, but this tagger was trained with only a small amount of in-domain labeled data. In this paper, we will investigate whether the performance of a simple bigram HMM tagger can be improved by introducing latent annotations and whether self-training can further improve its performance. To the best of our knowledge, this is the first attempt to use latent annotations with self-training to enhance the performance of a POS tagger. 2 Model POS tagging using a hidden Markov model can be considered as an instance of Bayesian inference, 21</context>
</contexts>
<marker>Wang, Huang, Harper, 2007</marker>
<rawString>W. Wang, Z. Huang, and M. Harper. 2007. Semisupervised learning for part-of-speech tagging of Mandarin transcribed speech. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F Xia</author>
<author>F Chiou</author>
<author>M Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus. Natural Language Engineering.</title>
<date>2005</date>
<contexts>
<context position="7766" citStr="Xue et al., 2005" startWordPosition="1257" endWordPosition="1260"> w). These statistics are redistributed among the rare words (w : c(w) &lt; λ) to compute their emission probabilities: c(ax, w) = cr(ax, unk) · c(a, w)/cr(a, unk) Ep(w|ax) = c(ax, w)/ w c(ax, w) The impact of this rare word handling method will be investigated in Section 3. A character-based unknown word model, similar to the one described in (Huang et al., 2007), is used to handle unknown Chinese words during tagging. A decoding method similar to the max-rule-product method in (Petrov and Klein, 2007) is used to tag sentences using our model. 3 Experiments The Penn Chinese Treebank 6.0 (CTB6) (Xue et al., 2005) is used as the labeled data in our study. CTB6 2c(·) represents the count of the event. 3The value of λ is tuned on the development set. 214 contains news articles, which are used as the primary source of labeled data in our experiments, as well as broadcast news transcriptions. Since the news articles were collected during different time periods from different sources with a diversity of topics, in order to obtain a representative split of train-testdevelopment sets, we divide them into blocks of 10 files in sorted order and for each block use the first file for development, the second for t</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>N. Xue, F. Xia, F. Chiou, and M. Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhang</author>
<author>J G Kahn</author>
</authors>
<title>Evaluation of decatur text normalizer for language model training.</title>
<date>2008</date>
<tech>Technical report,</tech>
<institution>University of Washington.</institution>
<contexts>
<context position="8820" citStr="Zhang and Kahn, 2008" startWordPosition="1436" endWordPosition="1439">ive split of train-testdevelopment sets, we divide them into blocks of 10 files in sorted order and for each block use the first file for development, the second for test, and the remaining for training. The broadcast news data exhibits many of the characteristics of newswire text (it contains many nonverbal expressions, e.g., numbers and symbols, and is fully punctuated) and so is also included in the training data set. We also utilize a greater number of unlabeled sentences in the self-training experiments. They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al., 2005a). See Table 1 for a summary of the data used. Train Dev Test Unlabeled sentences 24,416 1904 1975 210,000 words 678,811 51,229 52,861 6,254,947 Number of latent annotations Figure 1: The learning curves of the bigram tagger with latent annotations on the development set. Figure 1 plots the learning curves of two bigram taggers with latent annotations (Bigram+LA:2 has the special handling of rare words as described in Section 2 while Bigram+LA:1 does not) and compares its performance with a state-of-the-art trigram HMM tagger (Huang et al., 2007) that us</context>
</contexts>
<marker>Zhang, Kahn, 2008</marker>
<rawString>B. Zhang and J. G. Kahn. 2008. Evaluation of decatur text normalizer for language model training. Technical report, University of Washington.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>