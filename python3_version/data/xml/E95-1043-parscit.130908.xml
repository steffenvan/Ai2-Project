<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000121">
<title confidence="0.997905">
Incorporating &amp;quot;Unconscious Reanalysis&amp;quot; into an Incremental,
Monotonic Parser
</title>
<author confidence="0.999048">
Patrick Sturt*
</author>
<affiliation confidence="0.927655">
Centre for Cognitive Science
Edinburgh
</affiliation>
<address confidence="0.407605">
UK
</address>
<email confidence="0.36348">
sturt Ocogsci.ed.ac.uk
</email>
<sectionHeader confidence="0.983621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949">
This paper describes the author&apos;s imple-
mentation of a parser aimed at repro-
ducing, in a computationally explicit sys-
tem, the constraints of a particular psy-
cholinguistic model (Gorrell in press). In
Gorrell &apos;s model, &amp;quot;unconscious&amp;quot; garden
paths may be processed via the addition
of structural relations to a monotone in-
creasing set at the point of disambigua-
tion, but there is no discussion as to how
the parser decides which relations to add.
We model this decision as a search for a
node in the tree at which an explicitly
defined parsing operation, tree-lowering
may be applied. With reference to En-
glish and Japanese processing data, we
show the importance of this search for
empirical adequacy of the psycholinguis-
tic model.
</bodyText>
<sectionHeader confidence="0.944141" genericHeader="categories and subject descriptors">
1 Conscious and Unconscious
Garden Paths
</sectionHeader>
<bodyText confidence="0.99134475">
Certain researchers in the psycholinguistic com-
munity (Pritchett (1992), Gorrell (in press)), have
argued for a binary distinction between two dis-
tinct types of garden path sentences. Conscious
garden paths, such as (1) below, are locally am-
biguous sentences which give rise to reanalysis
that is both experimentally detectable and causes
a conscious sensation of difficulty or &amp;quot;surprise ef-
fect&amp;quot;. Unconscious garden paths, on the other
hand, such as (2), cause reanalysis which is exper-
imentally detectable, but which is generally not
&amp;quot;noticed&amp;quot; by the speaker or hearer.
*The work reported here was done very much in a
collaborative spirit with my supervisor, Dr. Matthew
Crocker, and thanks are due to him for innumerable
suggestions and ideas. I would also like to thank the
people who have offered insightful comments on this
work, in particular, David Milward and Martin Pick-
ering. The research was supported by ESRC grant
R00429334338
</bodyText>
<listItem confidence="0.9984245">
(1) While John was eating the ice cream melted.
(2) John knows the truth hurts.
</listItem>
<bodyText confidence="0.909628047619048">
This binary distinction has often been used to mo-
tivate a two-level architecture in the human syn-
tactic processing system, where what we will call
the &amp;quot;core parser&amp;quot; performs standard attachment,
as well as being able to reanalyse in the easy cases
(such as on reaching hurts in (2)), but where the
assistance of a higher level resolver (to use Abney&apos;s
terminology (1987, 1989)), is required to solve
the difficult cases, (such as on reaching melted in
(1)). This &amp;quot;core parser&amp;quot; has been the subject of
a number of computational implementations, in-
cluding Marcus&apos;s deterministic parser (1980), De-
scription theory (henceforth, D-theory) (Marcus
et al (1983)), and Abney&apos;s licensing based model
(1987, 1989). It has also been the subject of a
number of psycholinguistic studies on a more the-
oretical level (Pritchett (1992), Gorrell (in press)).
The implementation described in this paper is
based on the most recent model, that of (Gorrell
(in press)). This model is interesting in that it
does not allow the parser to employ delay tactics,
such as using a lookahead buffer (Marcus (1980),
Marcus et al (1983)), or waiting for the head of
a phrase to appear in the input before construct-
ing that phrase (Abney (1987, 1989), Pritchett
(1992)). Instead, processing is guided by the prin-
ciple of Incremental Licensing, which states that
&amp;quot;the parser attempts incrementally to satisfy the
principles of grammar&amp;quot;. For the purposes of this
implementation, I have interpreted this to mean
that each word must be attached into a fully-
connected phrase marker as it is found in the
input .1 The psychological desirability of such a
&apos;In fact, Gorrell conjectures that, where there is
insufficient grammatical information to postulate a
structural relation between two constituents, such as
in a sequence of two non-case marked NPs in an En-
glish centre-embedded construction, the parser may
hold these constituents unstructured in its memory
(in press, p.212). However, for the purposes of this
implementation, we have taken the most constrained
position. Note that, since we do not deal with such
</bodyText>
<page confidence="0.99637">
291
</page>
<bodyText confidence="0.999792">
Full Attachment model has been argued for, es-
pecially with regard to the processing of head-
final languages, where evidence has been found
of pre-head structuring (Inoue &amp; Fodor (1991),
Frazier (1987)). Such models have also been ex-
plored computationally (Milward (1995), Crocker
(1991)).
</bodyText>
<sectionHeader confidence="0.886738" genericHeader="keywords">
2 D-theory and Gorrell&apos;s Model
</sectionHeader>
<bodyText confidence="0.9975115">
Gorrell employs the D-theoretic device of building
up a set of dominance and precedence relations2
between nodes, where the set is intended to
be constrained by informational monotonicity, in
that once asserted to the set, no relation may be
deleted or overridden. Gorrell restricts this con-
straint to Primary structural relations (i.e. domi-
nance and precedence), while secondary relations
(e.g. thematic and case dependencies) are not so
constrained. Recall (2), repeated below:
</bodyText>
<listItem confidence="0.765293">
(2) John knows the truth hurts.
</listItem>
<bodyText confidence="0.9913015">
At the point where John knows the truth has been
processed, a complete clause will have been built:
</bodyText>
<listItem confidence="0.940536">
(3) [s [NPI John] [vp [v knows] [Np, the truth]]
</listItem>
<bodyText confidence="0.991540173913044">
The description will include the information that
the verb knows precedes NP2, and that the VP dom-
inates NP2.
{..., prec(V,NP2), dom(VP, NP2), ...}
However, on the subsequent input of hurts, the
structure can be reanalysed by asserting an extra
clausal node (call it S2) dominating NP2 (which
will then become the embedded subject), but
which is in turn dominated by the matrix VP. This
can be achieved by adding the following struc-
tural relations to the tree description fprec(V,S2),
dom(VP,S2), dom(S2,NP2)}
(4). [s [N pi John] [vP [v knows] Ls, [NI:), the
truth] [vp2 hurts]]]]
Since the description before the processing of the
disambiguating word hurts is a subset of the final
tree description, the monotonicity requirement is
satisfied. Note in particular, that, because domi-
nance is a transitive relation, and because of the
inheritance condition on trees (a node inherits
the precedence relations of its ancestors3), the
two statements dom(VP,NP2) and prec (V ,NP2) re-
main true after reanalysis.4
</bodyText>
<footnote confidence="0.810677">
constructions, none of the arguments presented here
hinge on whether or not the parser may buffer mate-
rial in this way.
2The original D-theory model did not compute
precedence relations, except between terminal nodes.
&apos;See Partee et al (1993) for a description of the
conditions on trees, with which all tree descriptions
must comply.
&apos;It will be noticed that the reanalysis here involves
</footnote>
<bodyText confidence="0.9968936">
Note also that the model will correctly fail to re-
analyse for sentence (1) above, since the reanalysis
will require the retraction of the domination rela-
tion between the VP of the adverbial clause and
the NP the ice cream.
</bodyText>
<sectionHeader confidence="0.995628" genericHeader="introduction">
3 Implementation
</sectionHeader>
<bodyText confidence="0.999914111111111">
Although Gorrell proposes a general principle to
guide initial attachment decisions (Simplicity: No
vacuous structure building), and specifies the con-
ditions under which &amp;quot;unconscious reanalysis&amp;quot; may
occur, the model leaves unspecified the problem
of how the-system may be implemented. Of par-
ticular interest is the problem of how the parser
decides which relations to add to the set at each
point in time, especially at disambiguating points.
</bodyText>
<subsectionHeader confidence="0.996777">
3.1 Lexical Representation
</subsectionHeader>
<bodyText confidence="0.998860523809524">
The basic framework on which the implementa-
tion is built is similar to Tree Adjoining Gram-
mar (Joshi et al 1975). Each lexical category is
associated with a set of structural relations, which
determine its lexical subtree. We call this set the
sub-tree projection of that lexical category. For ex-
ample, the subtree projection for verbs in the En-
glish grammar is as follows, where Lex is a variable
which will be instantiated to the actual verb found
in the input.
fdom(S,NP), dom(S,VP), dom(VP,V),
dom(V,Lex), prec(NP,VP)1
Lexical categories are also associated with lists of
left and right attachment sites. In the above case,
NP, (which will correspond to the subject of the
verb), will be unified with the left attachment site.
If a transitive verb is found in the input, then the
parser consults the verb&apos;s argument structure and
creates a new right attachment site for an NP,
asserting also that this new NP is dominated by
VP and preceded by V.
</bodyText>
<subsectionHeader confidence="0.998134">
3.2 Attachment
</subsectionHeader>
<bodyText confidence="0.954187071428572">
Simple attachment can be performed in two ways,
which are defined below, where the term current
tree description is intended to denote the the set
of structural relations built up to that point in
processing:
Intuitively, left attachment may be thought of in
terms of attaching the current tree description to
the left corner of the projection of the new word,
while right attachment corresponds to attaching
the projection of the new word to the right corner
a realignment of thematic and, on GB assumptions,
case dependencies. These are examples of what Gor-
rell calls secondary relations, which are not subject to
the monotonicity requirement.
</bodyText>
<page confidence="0.983701">
292
</page>
<bodyText confidence="0.996499">
of the current tree description. They are equiva-
lent to Abney&apos;s Attach-L and Attach respectively.
</bodyText>
<sectionHeader confidence="0.855995" genericHeader="method">
DEFINITION Left Attachment:
</sectionHeader>
<bodyText confidence="0.991848">
Let D be the current tree description, with root
node R. Let S be the subtree projection of the
new word, whose left-most attachment site, A is
of identical syntactic category as R. The updated
tree description is S U D, where A is unified with
R.
</bodyText>
<sectionHeader confidence="0.94489" genericHeader="method">
DEFINITION Right Attachment:
</sectionHeader>
<bodyText confidence="0.9793065">
Let D be the current tree description, with the
first right attachment site A. Let S be the sub-
tree projection of the new word, whose root R is
of identical syntactic category as A. The updated
tree description is S U D, where A is unified with
R.
</bodyText>
<subsectionHeader confidence="0.996708">
3.3 Tree Lowering
</subsectionHeader>
<bodyText confidence="0.999967545454545">
It should be clear that, while simple left and right
attachment will suffice for attaching arguments
without reanalysis, it will not allow us to de-
rive the reanalysis required in example (2). For
this, we intuitively require some means of insert-
ing one tree description inside another. Schemat-
ically, what we require is illustrated below, where
[1] is intended to represent the current tree de-
scription built up after John knows the truth has
been parsed, and [2] is intended to represent the
subtree description of the new word hurts.
</bodyText>
<table confidence="0.673019933333333">
[3]
/\
NP VP
/\
V S
/ \
NP VP
/\
D N
/ \
NP VP
/ \
V NP /\ = = &gt;
/ \ NP VP
D N
</table>
<bodyText confidence="0.999964352941176">
We will call this operation &amp;quot;tree-lowering&amp;quot;. In-
tuitively, the operation finds a node on the cur-
rent tree description which matches the left at-
tachment site of the projection of the new word,
and attaches it, while inserting the root of the new
projection in its place. The result is that the node
chosen is &amp;quot;lowered&amp;quot; or &amp;quot;subordinated&amp;quot;.
In order to maintain structural coherence, the new
word attached via tree-lowering must be preceded
by all other words previously attached into the
description. We can guarantee this by requiring
the lowered node to dominate the last word to be
attached. We also need to ensure that, to avoid
crossing branches, the lowered node does not dom-
inate any unsaturated attachment sites (or &amp;quot;dan-
gling nodes&amp;quot;) We therefore define accessibility for
tree-lowering as follows:
</bodyText>
<sectionHeader confidence="0.911867" genericHeader="method">
DEFINITION Accessibility:
</sectionHeader>
<bodyText confidence="0.9997488">
Let N be a node in the current tree description.
Let W be the last word to be attached into the
tree.
N is accessible if N dominates W, and N does not
dominate any unsaturated attachment sites.
</bodyText>
<sectionHeader confidence="0.954156" genericHeader="method">
DEFINITION Tree-lowering:
</sectionHeader>
<bodyText confidence="0.999868884615385">
Let D be the current tree description. Let S be
the subtree projection of the new word. The left
attachment site A of S must match a node N acces-
sible in D. The root node R of S must be licensed
by the grammar in the position occupied by N.
Let L be the set of local relations in which N par-
ticipates. Let M be the result of substituting all
instances of N in L with R. The attachment node
A is unified with N.
The updated tree-description is DUSUM5
It will be noticed that tree-lowering is similar in
spirit to the adjunction operation of Tree Adjoin-
ing Grammars (Joshi et al, 1975). The difference
is that the foot and root nodes of an auxiliary tree
in TAG, (corresponding to the &amp;quot;lowered&amp;quot; node and
the node that replaces it respectively) must be of
the same syntactic category, whereas, as we have
seen in this example, in the model proposed here,
the two nodes May be of different categories, so
long as the resulting structure is licensed by the
grammar.
In the case of example (2), at the point where the
truth has been processed, the parser must find an
accessible node which matches the category of the
left attachment site of hurts (i.e. an NP). The
only choice is NP2:
</bodyText>
<listItem confidence="0.842081">
(3) [s [Npi John] [vp [v knows] [NP2 the truth]]
</listItem>
<bodyText confidence="0.97472847826087">
Now, all the local relations in which NP2 partici-
pates are found:
{dom(VP , NP2) , prec (V , NP2) }
and NP2 is substituted with the root of the new
projection, S2 to derive two new relations:
fdam(VP,S2), prec(V,S2)}
These relations are found to be licensed, because
the verb which V dominates (&amp;quot;knows&amp;quot;) may sub-
categorise for a clause, so these new relations are
added to the sets. Now, adding the subtree pro-
jection of hurts to the set, and unifying its left
&apos;Note that Abney&apos;s STEAL operation (1987, 1989)
is more powerful than tree-lowering, since it may
change domination relations, and thus will allow sen-
tences such as (1), though it excludes reduced rela-
tive garden paths, such as The horse raced past the
barn fell. The original D-theory model (Marcus et al
(1983)) is also more powerful, because it allows the
right-most daughter of a node to be lowered under a
sibling node.
&apos;Note that the relations defining the original posi-
tion of NP2, (i.e. dom(VP,NP2) and prec(V,NP2)) are
not subtracted from the set.
</bodyText>
<page confidence="0.992525">
293
</page>
<bodyText confidence="0.998246291666667">
attachment site with NP2 results in the derived
structure with NP2 &amp;quot;subordinated&amp;quot; into the lower
clause.
[s [NP, John] [vp [v knows] [s2 [NP2 the truth]
[vp2 hurts]]]]
With the tree-lowering operation so defined, the
problem of finding which relations to add to the
set at a disambiguating point reduces to a search
for an accessible node at which to apply this oper-
ation. However, this implies that, if more than one
such node exists, the parser, must be given a pref-
erence for making the requisite decision. Consider
the following sentence fragment, for example:
(5) I know [NP, the man who believes [NP2 the
countess]]...
If the input subsequently continues with a verb,
then we have a choice of two nodes for lower-
ing, i.e. NP1 and NP2. Though no experimental
work has been done on this type of sentence, there
seems to be an intuitive preference for the lower
attachment site, NP2. In (6), binding constraints
force lowering to be applied at NP2, while in (7), it
must be applied at NPi. Of the two, most native
English speakers report (6) to be easier.
</bodyText>
<listItem confidence="0.80132875">
(6) I know the man who believes the countess
killed herself.
(7) I know the man who believes the countess
killed himself.
</listItem>
<bodyText confidence="0.999867826086956">
Note also, that, on standard X-bar assumptions,
the attachment of post-modifiers may be derived
via lowering at an X&apos; node. In this case, the low-
ered node and its replacement will be of the same
syntactic category (like the root and foot node of
a TAG auxiliary tree). Researchers have noted
a general preference for low attachment of post-
modifiers (this is accounted for by the principle
of late closure (Frazier and Rayner, 1982)). This
would suggest that a reasonable search strategy
for English would be to search the set of accessi-
ble node in a bottom-up direction for English.
The algorithm is constructed in such a way that
lowering is only attempted in cases where sim-
ple attachment fails. This means that arguments
(which are incorporated via simple attachment)
will be attached preferentially to adjuncts (which
are incorporated via lowering). This captures the
general preference for argument over adjunct at-
tachment, which is accounted for by the princi-
ple of Minimal attachment in Frazier and Rayner
(1982), and by the principle of simplicity in Gor-
rell (in press).
</bodyText>
<sectionHeader confidence="0.987813" genericHeader="method">
4 Processing Japanese
</sectionHeader>
<subsectionHeader confidence="0.999579">
4.1 Main/subordinate clause ambiguity
</subsectionHeader>
<bodyText confidence="0.998514073170732">
Japanese presents a challenge for any incremental
parsing model because, typically, it is not possible
to determine where an embedded clause begins.
Consider the following example:
(8) John ga [0i ronbun wo kaita] seitoi wo hometa.
John NOM essay ACC wrote student ACC praised
&amp;quot;John praised the student who wrote the essay&amp;quot;
Up to the first verb kaita (&amp;quot;wrote&amp;quot;), the string
is interpretable as a full clause (without a gap),
meaning &amp;quot;John wrote an essay&amp;quot;, and the incre-
mental parser builds the requisite structure. How-
ever, the appearance of the head noun seito (stu-
dent) means that at least part of the preceding
clause must be reinterpreted as a relative clause
including a gap (note that there is no overt relative
pronoun in Japanese). One way of looking at what
is happening here is to see the subject NP John
ga as being dissociated from the clause in which
it is originally attached, and reattached into the
main clause. But looking at it from a different
perspective, as Gorrell has noted (in press), one
can see the subject NP as remaining in the main
clause, and the constituent bracketed in (8), (ron-
bun wo kaita (&amp;quot;wrote an essay&amp;quot;)) as being lowered
into the relative clause. If this is possible, then we
would expect examples like (8) to be unconscious
garden paths, and this does indeed seem to be re-
flected in the intuitive data (see Mazuka and Itoh
(in press)). However, if we are to allow our parser
to handle such examples, we must expand the def-
inition of tree-lowering, since, in order to build a
relative clause, we have to assert extra material
(including the empty subject and the new S node),
which is not justified solely by the lexical require-
ments of the disambiguating word, the head noun
seaÂ°. This involves reconstructing all the clausal
structure dominating the lowering site (including
asserting empty argument positions), with refer-
ence to the verb&apos;s case frame, and attempting to
attach the result as a relative clause to the head
noun.
</bodyText>
<subsectionHeader confidence="0.998499">
4.2 Minimal Expulsion
</subsectionHeader>
<bodyText confidence="0.999901">
Inoue (1991), describes a &amp;quot;minimal expulsion
strategy&amp;quot;, which predicts a preference, on reanal-
ysis, towards expelling the minimum amount of
material from the clause. In our terms, this means
that (assuming a binary right-branching clause
structure, with the verb in its right corner) the
node selected for lowering must be as high as
possible. This means that the bottom-up search
which we use for English will wrongly predict
a Maximal expulsion strategy. In cases such as
(8), assuming the bottom-up search, when a post-
clausal noun has been reached in the input, the
</bodyText>
<page confidence="0.994642">
294
</page>
<bodyText confidence="0.999600266666667">
parser starts its search from the node immediately
dominating the last word to be incorporated, (i.e.
the verb of what will become the relative clause).
This means that, in cases such as (8), the first
preference will be to lower the verb (and therefore
&amp;quot;expel&amp;quot; both subject and object), whereas the hu-
man preference, (to lower the object and verb, and
therefore expel only the subject) is the parser&apos;s
second choice on the bottom-up search strategy.
Mazuka and Itoh (in press) note that examples
where both subject and object must be expelled
from the relative clause, as would be the first
choice in a bottom-up search, often cause a con-
scious garden path effect. An example, adapted
from Mazuka and Itoh is the following:
</bodyText>
<listItem confidence="0.8814205">
(9) Yamasita ga yuuzin wo [0 Oi houmonsita]
kaisyai de mikaketa.
</listItem>
<bodyText confidence="0.9884893">
Yamasita NOM friend ACC visited company
LOC saw
&amp;quot;Yamasita saw his friend at the company he vis-
ited.&amp;quot;
In order to capture the minimal expulsion strat-
egy in this class of Japanese examples, therefore,
search for the lowering node should be conducted
top-down. We are currently investigating the con-
sequences of changing the search strategy in this
way.
</bodyText>
<sectionHeader confidence="0.965599" genericHeader="method">
5 The Problem of Retrospective
Reanalysis
</sectionHeader>
<bodyText confidence="0.999920875">
Having formulated the constraints of Gorrell&apos;s
model in terms of the accessibility of a node for
tree-lowering, we can see that the model can be
falsified if we can find a case where the relevant
disambiguating information comes at a point in
processing where the node which is required to
be lowered is no longer accessible. Consider the
following pair of sentences:
</bodyText>
<listItem confidence="0.9297705">
(10) I saw the man with the moustache.
(11) I saw the man with the telescope.
</listItem>
<bodyText confidence="0.999930769230769">
It is familiar from the psycholinguistic literature
that there is a preference for attaching the with
phrase as an instrumental argument of the verb
(as in (11), on the reading where the telescope
is the instrument of seeing). On the assumption
that saw selects for a PP instrumental argument,
we can derive this preference in the present model
via the preference to attach as an argument as op-
posed to an adjunct. However, since we are con-
strained by incrementality, we will have to make
an attachment decision for the PP as soon as the
preposition with is encountered, and it will be at-
tached in the preferred reading as a sister of the
verb. This means that, in cases such as (10),
where, on the globally acceptable reading, the PP
is an adjunct of the NP the man, this attachment
will have to be revised, and the PP retrospectively
adjoined into the relevant N&apos; node. However, once
the preposition with has been attached, the re-
quired N&apos; node will no longer be accessible, and
a conscious garden path effect will be predicted,
which, intuitively, does not occur. Note that there
is no garden path effect even if the preposition is
separated from the disambiguating head noun by a
series of adjectives: (&amp;quot;I saw the man with the neat,
quaint, old-fashioned moustache/telescope&amp;quot;).
The same result obtains if we abstract away from
the particular implementational details of tree-
lowering, and return to the abstract level at which
Gorrell states his model. Once the PP has been
attached as an argument of the verb, it can never
be reanalysed as the adjunct of the preceding NP,
because the NP will precede the PP before re-
analysis, and dominate it after reanalysis, which
is against the &amp;quot;exclusivity condition&amp;quot; on trees (i.e.
no two nodes may stand in both a dominance and
a precedence relation).7
A similar problem concerns examples such as the
following, from Gibson et al (1993):
</bodyText>
<listItem confidence="0.792829333333333">
(12) the lamps near the paintings of the house
[that was damaged in the flood].
(13) the lamps near the painting of the houses
[that was damaged in the flood].
(14) the lamp near the paintings of the houses
[that was damaged in the flood].
</listItem>
<bodyText confidence="0.999975409090909">
in the above, Gibson et al have manipulated num-
ber agreement to force low (12), middle (13) and
high (14) attachment of the bracketed relative
clause. The results of their on- and off-line ex-
periments show clearly that the low attachment
(corresponding to 12) is easiest, but the middle
attachment (corresponding to (13)) is most diffi-
cult. This behaviour cannot be captured whether
we adopt a bottom-up or a top-down search for
tree-lowering. However, even if we can incorpo-
rate the required preferences into the parser, the
constraint of incrementality will force us to make
the decision on encountering that. This means
that, assuming we decide initially to attach low,
but number agreement on was subsequently forces
high attachment, as in (14), then a conscious gar-
den path effect will be predicted, as lowering can-
not derive the reanalysis. This is true on the ab-
stract level as well, since there will be nodes in
the description which precede the original low po-
sition of the relative clause, but are dominated by
the subsequent high position of the relative clause.
</bodyText>
<footnote confidence="0.849962">
7 Note that in Marcus et al (1983), since precedence
relations were not computed for non-terminals, lower-
ing into a predecessor was possible, thus (11) would
cause no processing difficulty. However, presumably,
their parser would overgenerate on examples such as
the horse raced past the barn fell.
</footnote>
<page confidence="0.996833">
295
</page>
<bodyText confidence="0.996133333333333">
However, intuitively, of the above sentences, it is
only (13) which causes the conscious garden path
effect .8
</bodyText>
<sectionHeader confidence="0.999127" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999954">
The current implementation shows that the suc-
cess of an abstract model such as Gorrell&apos;s de-
pends crucially on the computational details of
the processing algorithm used. The search for the
lowering site is of particular importance. In the
final section we have seen that the combination of
informational monotonicity with the assumption
of strict incrementality results in a system which
is too constrained to capture all the processing
data. Future research will be aimed at determin-
ing, firstly, how we can enrich the information to
which the search strategy is sensitive in order to
provide a better match with human preferences,
and secondly, which constraints should be relaxed
in order to avoid the problem of undergeneration.
</bodyText>
<sectionHeader confidence="0.99873" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.885134555555556">
Abney, S. P. (1987): Licensing and Parsing. Pro-
ceedings of NELS 17 p.1-15, University of Mas-
sachusetts, Amherst
Abney, S. P. (1989): A computational model of
human parsing. Journal of Psycholinguistic Re-
search 18 p.129-144
Crocker, M. W. (1991): A Logical Model of Com-
petence and Performance in the Human Sentence
Processor. PhD thesis, Dept. of Artificial Intelli-
gence, University of Edinburgh, Edinburgh, U.K.
Frazier, L. (1987): Syntactic processing: Evidence
from Dutch. In Natural Language and Linguistic
Theory 5.4 p.519-559
Frazier, L. and K. Rayner, (1982): Making and
correcting errors during sentence comprehension:
Eye movements in the analysis of structurally
ambiguous sentences. Cognitive Psychology 14
p.178-210
</reference>
<bodyText confidence="0.891446235294118">
8Preliminary findings suggest that a similar pref-
erence rating is employed in (written) production as
well as (reading) comprehension for these examples.
This can be seen in Gibson et al&apos;s (1994) study. This
shows a the LOW &gt; HIGH &gt; MID ordering in the at-
tachment of the final PP in NPs of the following form
found in the Brown corpus:
NPi Prep NP2 Prep NP3 PP
Of 105 unambiguous PP adjunct attachments, 68%
were low-attached, 26% high attached and 10% mid-
attached. However, the question of whether the syn-
tactic structures people preferentially use in produc-
tion should correspond to the syntactic structures
people preferentially assign to strings during compre-
hension is still very much an open issue, though see
Mitchell and Cuetos (1991) for a view that the expe-
rience of previous input influences parsing decisions.
</bodyText>
<reference confidence="0.981984425531915">
Gibson, E., N. Pearlmutter, E. Canesco-Gonzalez
and Greg Hickok (1993): Cross-linguistic Attach-
ment Preferences: Evidence from English and
Spanish. (ms. submitted to Cognition)
Gibson, E. and N. Pearlmutter, E. (1994): A
corpus-based account of Psycholinguistic Con-
straints on Prepositional Phrase Attachment (in
C. Clifton, L. Frazier and K. Rayner (eds)
Perspectives on Sentence Processing New York:
Lawrence Erlbaum
Correll, P. (in press): Syntax and Perception. to
be published by Cambridge University Press
Inoue, A. (1991): A comparative study of parsing
in English and Japanese. PhD thesis, University
of Conneticut.
Inoue, A. and J.D. Fodor (in press): Information-
paced parsing of Japanese. (to appear in Mazuka
Sz Nagai (eds))
Joshi, A.K., L.S. Levy, and M. Takahashi, (1975):
Tree Adjunct grammars. Journal of Computer
and System Sciences 10, p.136-163
Marcus, M. (1980): A Theory of Syntactic Recog-
nition for Natural Language Cambridge, MA:
MIT Press
Marcus, M., D. Hindle, and M. Fleck (1983): D-
theory: Talking about talking about trees. As-
sociation for Computational Linguistics 21 p.129-
136
Mazuka, R. and K. Itoh (in press): Can Japanese
be led down the garden path? (to appear in
Mazuka and Nagai)
Mazuka, R., and Nagai (eds) (to appear):
Japanese Syntactic Processing Hillsdale, NJ:
Lawrence Earlbaum
Milward, D. (1995): Incremental Interpretation
of Categorial Grammar. in Proceedings of EACL
(this volume)
Mitchell, D.C. &amp; Cuetos, F. (1991): The origins of
parsing strategies. Conference proceedings: Cur-
rent issues in natural language processing Univer-
sity of Texas at Austin, TX
Partee, B., A. ter Meulen and R. E. Wall (1993):
Mathematical methods in Linguistics Dordrecht:
Kluwer Academic Publishers
Pritchett, B. L. (1992): Grammatical Competence
and Parsing Performance. Chicago, IL: Univer-
sity of Chicago Press
</reference>
<page confidence="0.998566">
296
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.992672">Incorporating &amp;quot;Unconscious Reanalysis&amp;quot; into an Incremental, Monotonic Parser</title>
<author confidence="0.999287">Patrick Sturt</author>
<affiliation confidence="0.822621">Centre for Cognitive Science Edinburgh</affiliation>
<address confidence="0.848429">UK</address>
<email confidence="0.756321">sturtOcogsci.ed.ac.uk</email>
<abstract confidence="0.997581031531532">This paper describes the author&apos;s implementation of a parser aimed at reproducing, in a computationally explicit system, the constraints of a particular psycholinguistic model (Gorrell in press). In Gorrell &apos;s model, &amp;quot;unconscious&amp;quot; garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguabut there is no discussion as to the parser decides which relations to add. We model this decision as a search for a node in the tree at which an explicitly parsing operation, may be applied. With reference to English and Japanese processing data, we show the importance of this search for empirical adequacy of the psycholinguistic model. 1 Conscious and Unconscious Garden Paths Certain researchers in the psycholinguistic community (Pritchett (1992), Gorrell (in press)), have argued for a binary distinction between two distypes of garden path sentences. paths, as (1) below, are locally ambiguous sentences which give rise to reanalysis that is both experimentally detectable and causes a conscious sensation of difficulty or &amp;quot;surprise efgarden paths, the other hand, such as (2), cause reanalysis which is experimentally detectable, but which is generally not &amp;quot;noticed&amp;quot; by the speaker or hearer. *The work reported here was done very much in a collaborative spirit with my supervisor, Dr. Matthew Crocker, and thanks are due to him for innumerable suggestions and ideas. I would also like to thank the people who have offered insightful comments on this work, in particular, David Milward and Martin Pickering. The research was supported by ESRC grant R00429334338 (1) While John was eating the ice cream melted. (2) John knows the truth hurts. This binary distinction has often been used to motivate a two-level architecture in the human syntactic processing system, where what we will call the &amp;quot;core parser&amp;quot; performs standard attachment, as well as being able to reanalyse in the easy cases as on reaching in but where the of a level resolver use Abney&apos;s terminology (1987, 1989)), is required to solve difficult cases, (such as on reaching (1)). This &amp;quot;core parser&amp;quot; has been the subject of a number of computational implementations, including Marcus&apos;s deterministic parser (1980), Description theory (henceforth, D-theory) (Marcus et al (1983)), and Abney&apos;s licensing based model (1987, 1989). It has also been the subject of a number of psycholinguistic studies on a more theoretical level (Pritchett (1992), Gorrell (in press)). The implementation described in this paper is based on the most recent model, that of (Gorrell (in press)). This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980), Marcus et al (1983)), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987, 1989), Pritchett (1992)). Instead, processing is guided by the prinof Licensing, states that &amp;quot;the parser attempts incrementally to satisfy the principles of grammar&amp;quot;. For the purposes of this implementation, I have interpreted this to mean that each word must be attached into a fullyconnected phrase marker as it is found in the The psychological desirability of such a &apos;In fact, Gorrell conjectures that, where there is insufficient grammatical information to postulate a structural relation between two constituents, such as in a sequence of two non-case marked NPs in an English centre-embedded construction, the parser may hold these constituents unstructured in its memory (in press, p.212). However, for the purposes of this implementation, we have taken the most constrained position. Note that, since we do not deal with such 291 Attachment has been argued for, especially with regard to the processing of headfinal languages, where evidence has been found of pre-head structuring (Inoue &amp; Fodor (1991), Frazier (1987)). Such models have also been explored computationally (Milward (1995), Crocker (1991)). and Gorrell&apos;s Model Gorrell employs the D-theoretic device of building a set of dominance and precedence between nodes, where the set is intended to be constrained by informational monotonicity, in that once asserted to the set, no relation may be deleted or overridden. Gorrell restricts this conto structural relations domiand precedence), while relations (e.g. thematic and case dependencies) are not so constrained. Recall (2), repeated below: (2) John knows the truth hurts. the point where knows the truth been processed, a complete clause will have been built: [s [NPI [vp [v knows] truth]] The description will include the information that verb and that the VP domdom(VP, ...} on the subsequent input of structure can be reanalysed by asserting an extra clausal node (call it S2) dominating NP2 (which will then become the embedded subject), but which is in turn dominated by the matrix VP. This can be achieved by adding the following structural relations to the tree description fprec(V,S2), dom(VP,S2), dom(S2,NP2)} [N John] knows]Ls, [NI:), hurts]]]] Since the description before the processing of the word a subset of the final tree description, the monotonicity requirement is satisfied. Note in particular, that, because dominance is a transitive relation, and because of the inheritance condition on trees (a node inherits precedence relations of its the two statements dom(VP,NP2) and prec (V ,NP2) retrue after constructions, none of the arguments presented here hinge on whether or not the parser may buffer material in this way. original D-theory model did not compute precedence relations, except between terminal nodes. &apos;See Partee et al (1993) for a description of the conditions on trees, with which all tree descriptions must comply. &apos;It will be noticed that the reanalysis here involves Note also that the model will correctly fail to reanalyse for sentence (1) above, since the reanalysis will require the retraction of the domination relation between the VP of the adverbial clause and NP ice 3 Implementation Although Gorrell proposes a general principle to initial attachment decisions No structure building), specifies the conditions under which &amp;quot;unconscious reanalysis&amp;quot; may occur, the model leaves unspecified the problem of how the-system may be implemented. Of particular interest is the problem of how the parser to add to the set at each point in time, especially at disambiguating points. 3.1 Lexical Representation The basic framework on which the implementation is built is similar to Tree Adjoining Grammar (Joshi et al 1975). Each lexical category is associated with a set of structural relations, which its subtree. call this set the projection that lexical category. For example, the subtree projection for verbs in the English grammar is as follows, where Lex is a variable which will be instantiated to the actual verb found in the input. fdom(S,NP), dom(S,VP), dom(VP,V), dom(V,Lex), prec(NP,VP)1 Lexical categories are also associated with lists of left and right attachment sites. In the above case, NP, (which will correspond to the subject of the verb), will be unified with the left attachment site. If a transitive verb is found in the input, then the parser consults the verb&apos;s argument structure and creates a new right attachment site for an NP, asserting also that this new NP is dominated by VP and preceded by V. 3.2 Attachment Simple attachment can be performed in two ways, are defined below, where the term description intended to denote the the set of structural relations built up to that point in processing: Intuitively, left attachment may be thought of in of the current tree description to the left corner of the projection of the new word, while right attachment corresponds to attaching the projection of the new word to the right corner of thematic and, on GB assumptions, case dependencies. These are examples of what Gorrell calls secondary relations, which are not subject to the monotonicity requirement. 292 of the current tree description. They are equivato Abney&apos;s Attachment: the current tree description, with root the subtree projection of the word, whose left-most attachment site, identical syntactic category as updated description is A is unified with R. Attachment: the current tree description, with the right attachment site A. Let the subprojection of the new word, whose root identical syntactic category as updated description is U D, A is unified with R. 3.3 Tree Lowering It should be clear that, while simple left and right attachment will suffice for attaching arguments without reanalysis, it will not allow us to derive the reanalysis required in example (2). For this, we intuitively require some means of inserting one tree description inside another. Schematically, what we require is illustrated below, where intended to represent the current tree debuilt up after knows the truth parsed, and intended to represent the description of the new word [3]</abstract>
<title confidence="0.745137923076923">NP VP /\ V S / \ NP VP /\ D N / \ NP VP / \ V NP /\ = &gt; / \ NP VP D N</title>
<abstract confidence="0.999012578778135">We will call this operation &amp;quot;tree-lowering&amp;quot;. Intuitively, the operation finds a node on the current tree description which matches the left attachment site of the projection of the new word, and attaches it, while inserting the root of the new projection in its place. The result is that the node chosen is &amp;quot;lowered&amp;quot; or &amp;quot;subordinated&amp;quot;. In order to maintain structural coherence, the new word attached via tree-lowering must be preceded by all other words previously attached into the description. We can guarantee this by requiring the lowered node to dominate the last word to be attached. We also need to ensure that, to avoid crossing branches, the lowered node does not dominate any unsaturated attachment sites (or &amp;quot;dannodes&amp;quot;) We therefore define tree-lowering as follows: a node in the current tree description. Let W be the last word to be attached into the tree. accessible if W, and not dominate any unsaturated attachment sites. the current tree description. Let the subtree projection of the new word. The left site match a node accesin root node be licensed the grammar in the position occupied by the set of local relations in which par- Let the result of substituting all of attachment node unified with updated tree-description is It will be noticed that tree-lowering is similar in spirit to the adjunction operation of Tree Adjoining Grammars (Joshi et al, 1975). The difference is that the foot and root nodes of an auxiliary tree in TAG, (corresponding to the &amp;quot;lowered&amp;quot; node and the node that replaces it respectively) must be of the same syntactic category, whereas, as we have seen in this example, in the model proposed here, the two nodes May be of different categories, so long as the resulting structure is licensed by the grammar. the case of example (2), at the point where been processed, the parser must find an accessible node which matches the category of the attachment site of an NP). The choice is John] knows] [NP2the truth]] Now, all the local relations in which NP2 participates are found: , , (V , } and NP2 is substituted with the root of the new derive two new relations: These relations are found to be licensed, because verb which (&amp;quot;knows&amp;quot;) may subcategorise for a clause, so these new relations are added to the sets. Now, adding the subtree proof the set, and unifying its left that Abney&apos;s (1987, 1989) is more powerful than tree-lowering, since it may change domination relations, and thus will allow sentences such as (1), though it excludes reduced relagarden paths, such as horse raced past the fell. original D-theory model (Marcus et al (1983)) is also more powerful, because it allows the daughter of a node to be lowered sibling node. &apos;Note that the relations defining the original posiof and prec(V,NP2)) are not subtracted from the set. 293 attachment site with NP2 results in the derived structure with NP2 &amp;quot;subordinated&amp;quot; into the lower clause. [NP, knows] [NP2 the truth] hurts]]]] With the tree-lowering operation so defined, the problem of finding which relations to add to the set at a disambiguating point reduces to a search for an accessible node at which to apply this operation. However, this implies that, if more than one node exists, the be given a preference for making the requisite decision. Consider the following sentence fragment, for example: I know man who believes [NP2the countess]]... If the input subsequently continues with a verb, then we have a choice of two nodes for loweri.e. and NP2. Though no experimental work has been done on this type of sentence, there seems to be an intuitive preference for the lower attachment site, NP2. In (6), binding constraints force lowering to be applied at NP2, while in (7), it be applied at Of the two, most native English speakers report (6) to be easier. (6) I know the man who believes the countess killed herself. (7) I know the man who believes the countess killed himself. Note also, that, on standard X-bar assumptions, the attachment of post-modifiers may be derived via lowering at an X&apos; node. In this case, the lowered node and its replacement will be of the same syntactic category (like the root and foot node of a TAG auxiliary tree). Researchers have noted a general preference for low attachment of postmodifiers (this is accounted for by the principle closure and Rayner, 1982)). This would suggest that a reasonable search strategy for English would be to search the set of accessible node in a bottom-up direction for English. The algorithm is constructed in such a way that lowering is only attempted in cases where simple attachment fails. This means that arguments (which are incorporated via simple attachment) will be attached preferentially to adjuncts (which are incorporated via lowering). This captures the general preference for argument over adjunct attachment, which is accounted for by the princiof attachment Frazier and Rayner and by the principle of Gorrell (in press). 4 Processing Japanese 4.1 Main/subordinate clause ambiguity Japanese presents a challenge for any incremental parsing model because, typically, it is not possible to determine where an embedded clause begins. Consider the following example: (8) John ga [0i ronbun wo kaita] seitoi wo hometa. John NOM essay ACC wrote student ACC praised &amp;quot;John praised the student who wrote the essay&amp;quot; to the first verb the string is interpretable as a full clause (without a gap), meaning &amp;quot;John wrote an essay&amp;quot;, and the incremental parser builds the requisite structure. Howthe appearance of the head noun (student) means that at least part of the preceding clause must be reinterpreted as a relative clause including a gap (note that there is no overt relative pronoun in Japanese). One way of looking at what happening here is to see the subject NP being dissociated from the clause in which it is originally attached, and reattached into the main clause. But looking at it from a different perspective, as Gorrell has noted (in press), one can see the subject NP as remaining in the main and the constituent bracketed in (8), (ronwo kaita an essay&amp;quot;)) as being into the relative clause. If this is possible, then we would expect examples like (8) to be unconscious garden paths, and this does indeed seem to be reflected in the intuitive data (see Mazuka and Itoh (in press)). However, if we are to allow our parser to handle such examples, we must expand the definition of tree-lowering, since, in order to build a relative clause, we have to assert extra material (including the empty subject and the new S node), which is not justified solely by the lexical requirements of the disambiguating word, the head noun involves reconstructing all the clausal structure dominating the lowering site (including asserting empty argument positions), with reference to the verb&apos;s case frame, and attempting to attach the result as a relative clause to the head noun. 4.2 Minimal Expulsion Inoue (1991), describes a &amp;quot;minimal expulsion strategy&amp;quot;, which predicts a preference, on reanalysis, towards expelling the minimum amount of material from the clause. In our terms, this means that (assuming a binary right-branching clause structure, with the verb in its right corner) the node selected for lowering must be as high as possible. This means that the bottom-up search which we use for English will wrongly predict strategy. In cases such as (8), assuming the bottom-up search, when a postclausal noun has been reached in the input, the 294 parser starts its search from the node immediately dominating the last word to be incorporated, (i.e. the verb of what will become the relative clause). This means that, in cases such as (8), the first preference will be to lower the verb (and therefore &amp;quot;expel&amp;quot; both subject and object), whereas the human preference, (to lower the object and verb, and therefore expel only the subject) is the parser&apos;s second choice on the bottom-up search strategy. Mazuka and Itoh (in press) note that examples where both subject and object must be expelled from the relative clause, as would be the first choice in a bottom-up search, often cause a conscious garden path effect. An example, adapted from Mazuka and Itoh is the following: (9) Yamasita ga yuuzin wo [0 Oi houmonsita] kaisyai de mikaketa. Yamasita NOM friend ACC visited company LOC saw &amp;quot;Yamasita saw his friend at the company he visited.&amp;quot; In order to capture the minimal expulsion strategy in this class of Japanese examples, therefore, search for the lowering node should be conducted top-down. We are currently investigating the consequences of changing the search strategy in this way. 5 The Problem of Retrospective Reanalysis Having formulated the constraints of Gorrell&apos;s in terms of the a node for can see that the model can be falsified if we can find a case where the relevant disambiguating information comes at a point in processing where the node which is required to be lowered is no longer accessible. Consider the following pair of sentences: (10) I saw the man with the moustache. (11) I saw the man with the telescope. It is familiar from the psycholinguistic literature there is a preference for attaching the phrase as an instrumental argument of the verb (as in (11), on the reading where the telescope is the instrument of seeing). On the assumption for a PP instrumental argument, we can derive this preference in the present model via the preference to attach as an argument as opposed to an adjunct. However, since we are constrained by incrementality, we will have to make an attachment decision for the PP as soon as the encountered, and it will be attached in the preferred reading as a sister of the verb. This means that, in cases such as (10), where, on the globally acceptable reading, the PP an adjunct of the NP man, attachment will have to be revised, and the PP retrospectively adjoined into the relevant N&apos; node. However, once preposition been attached, the required N&apos; node will no longer be accessible, and a conscious garden path effect will be predicted, which, intuitively, does not occur. Note that there is no garden path effect even if the preposition is separated from the disambiguating head noun by a series of adjectives: (&amp;quot;I saw the man with the neat, quaint, old-fashioned moustache/telescope&amp;quot;). The same result obtains if we abstract away from the particular implementational details of treelowering, and return to the abstract level at which Gorrell states his model. Once the PP has been attached as an argument of the verb, it can never be reanalysed as the adjunct of the preceding NP, because the NP will precede the PP before reanalysis, and dominate it after reanalysis, which is against the &amp;quot;exclusivity condition&amp;quot; on trees (i.e. no two nodes may stand in both a dominance and precedence A similar problem concerns examples such as the following, from Gibson et al (1993): (12) the lamps near the paintings of the house [that was damaged in the flood]. (13) the lamps near the painting of the houses in the flood]. (14) the lamp near the paintings of the houses [that was damaged in the flood]. in the above, Gibson et al have manipulated number agreement to force low (12), middle (13) and high (14) attachment of the bracketed relative clause. The results of their onand off-line experiments show clearly that the low attachment (corresponding to 12) is easiest, but the middle attachment (corresponding to (13)) is most difficult. This behaviour cannot be captured whether we adopt a bottom-up or a top-down search for tree-lowering. However, even if we can incorporate the required preferences into the parser, the constraint of incrementality will force us to make decision on encountering means that, assuming we decide initially to attach low, number agreement on forces high attachment, as in (14), then a conscious garden path effect will be predicted, as lowering cannot derive the reanalysis. This is true on the ablevel as well, since there nodes in the description which precede the original low position of the relative clause, but are dominated by the subsequent high position of the relative clause. 7Note that in Marcus et al (1983), since precedence relations were not computed for non-terminals, lowering into a predecessor was possible, thus (11) would cause no processing difficulty. However, presumably, their parser would overgenerate on examples such as the horse raced past the barn fell. 295 However, intuitively, of the above sentences, it is only (13) which causes the conscious garden path 6 Conclusion The current implementation shows that the success of an abstract model such as Gorrell&apos;s depends crucially on the computational details of the processing algorithm used. The search for the lowering site is of particular importance. In the final section we have seen that the combination of informational monotonicity with the assumption of strict incrementality results in a system which is too constrained to capture all the processing data. Future research will be aimed at determining, firstly, how we can enrich the information to which the search strategy is sensitive in order to provide a better match with human preferences, and secondly, which constraints should be relaxed in order to avoid the problem of undergeneration.</abstract>
<title confidence="0.875022">References</title>
<author confidence="0.955893">Pro-</author>
<affiliation confidence="0.887473">of NELS University of Mas-</affiliation>
<address confidence="0.718281">sachusetts, Amherst</address>
<abstract confidence="0.761306517241379">Abney, S. P. (1989): A computational model of parsing. of Psycholinguistic Re- M. W. (1991): Logical Model of Competence and Performance in the Human Sentence thesis, Dept. of Artificial Intelligence, University of Edinburgh, Edinburgh, U.K. Frazier, L. (1987): Syntactic processing: Evidence Dutch. In Language and Linguistic Frazier, L. and K. Rayner, (1982): Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally sentences. Psychology findings suggest that a similar preference rating is employed in (written) production as well as (reading) comprehension for these examples. This can be seen in Gibson et al&apos;s (1994) study. This a the &gt; HIGH &gt; MID in the attachment of the final PP in NPs of the following form found in the Brown corpus: Prep Prep NP3 PP Of 105 unambiguous PP adjunct attachments, 68% were low-attached, 26% high attached and 10% midattached. However, the question of whether the syntactic structures people preferentially use in production should correspond to the syntactic structures people preferentially assign to strings during comprehension is still very much an open issue, though see Mitchell and Cuetos (1991) for a view that the experience of previous input influences parsing decisions.</abstract>
<note confidence="0.7136596">Gibson, E., N. Pearlmutter, E. Canesco-Gonzalez and Greg Hickok (1993): Cross-linguistic Attachment Preferences: Evidence from English and Spanish. (ms. submitted to Cognition) Gibson, E. and N. Pearlmutter, E. (1994): A corpus-based account of Psycholinguistic Constraints on Prepositional Phrase Attachment (in C. Clifton, L. Frazier and K. Rayner (eds) on Sentence Processing York: Lawrence Erlbaum P. (in press): and Perception. be published by Cambridge University Press A. (1991): comparative study of parsing English and Japanese. thesis, University of Conneticut. Inoue, A. and J.D. Fodor (in press): Informationpaced parsing of Japanese. (to appear in Mazuka Sz Nagai (eds)) Joshi, A.K., L.S. Levy, and M. Takahashi, (1975): Adjunct grammars. of Computer System Sciences M. (1980): A of Syntactic Recogfor Natural Language MA: MIT Press Marcus, M., D. Hindle, and M. Fleck (1983): D- Talking about talking about trees. Asfor Computational Linguistics p.129- 136 Mazuka, R. and K. Itoh (in press): Can Japanese be led down the garden path? (to appear in Mazuka and Nagai) Mazuka, R., and Nagai (eds) (to appear): Syntactic Processing NJ: Lawrence Earlbaum Milward, D. (1995): Incremental Interpretation Categorial Grammar. in of EACL (this volume) D.C. F. (1991): The origins of strategies. proceedings: Curissues in natural language processing University of Texas at Austin, TX ter Meulen and R. E. Wall (1993): methods in Linguistics Kluwer Academic Publishers B. L. (1992): Competence</note>
<author confidence="0.84701">Univer-</author>
<affiliation confidence="0.961592">sity of Chicago Press</affiliation>
<address confidence="0.88024">296</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S P Abney</author>
</authors>
<title>Licensing and Parsing.</title>
<date>1987</date>
<journal>Proceedings of NELS</journal>
<volume>17</volume>
<pages>1--15</pages>
<institution>University of Massachusetts,</institution>
<location>Amherst</location>
<contexts>
<context position="3168" citStr="Abney (1987" startWordPosition="511" endWordPosition="512">on theory (henceforth, D-theory) (Marcus et al (1983)), and Abney&apos;s licensing based model (1987, 1989). It has also been the subject of a number of psycholinguistic studies on a more theoretical level (Pritchett (1992), Gorrell (in press)). The implementation described in this paper is based on the most recent model, that of (Gorrell (in press)). This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980), Marcus et al (1983)), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987, 1989), Pritchett (1992)). Instead, processing is guided by the principle of Incremental Licensing, which states that &amp;quot;the parser attempts incrementally to satisfy the principles of grammar&amp;quot;. For the purposes of this implementation, I have interpreted this to mean that each word must be attached into a fullyconnected phrase marker as it is found in the input .1 The psychological desirability of such a &apos;In fact, Gorrell conjectures that, where there is insufficient grammatical information to postulate a structural relation between two constituents, such as in a sequence of two non-case marked </context>
</contexts>
<marker>Abney, 1987</marker>
<rawString>Abney, S. P. (1987): Licensing and Parsing. Proceedings of NELS 17 p.1-15, University of Massachusetts, Amherst</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Abney</author>
</authors>
<title>A computational model of human parsing.</title>
<date>1989</date>
<journal>Journal of Psycholinguistic Research</journal>
<volume>18</volume>
<pages>129--144</pages>
<marker>Abney, 1989</marker>
<rawString>Abney, S. P. (1989): A computational model of human parsing. Journal of Psycholinguistic Research 18 p.129-144</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Crocker</author>
</authors>
<title>A Logical Model of Competence and Performance in the Human Sentence Processor.</title>
<date>1991</date>
<tech>PhD thesis,</tech>
<institution>Dept. of Artificial Intelligence, University of Edinburgh,</institution>
<location>Edinburgh, U.K.</location>
<contexts>
<context position="4327" citStr="Crocker (1991)" startWordPosition="692" endWordPosition="693">tituents, such as in a sequence of two non-case marked NPs in an English centre-embedded construction, the parser may hold these constituents unstructured in its memory (in press, p.212). However, for the purposes of this implementation, we have taken the most constrained position. Note that, since we do not deal with such 291 Full Attachment model has been argued for, especially with regard to the processing of headfinal languages, where evidence has been found of pre-head structuring (Inoue &amp; Fodor (1991), Frazier (1987)). Such models have also been explored computationally (Milward (1995), Crocker (1991)). 2 D-theory and Gorrell&apos;s Model Gorrell employs the D-theoretic device of building up a set of dominance and precedence relations2 between nodes, where the set is intended to be constrained by informational monotonicity, in that once asserted to the set, no relation may be deleted or overridden. Gorrell restricts this constraint to Primary structural relations (i.e. dominance and precedence), while secondary relations (e.g. thematic and case dependencies) are not so constrained. Recall (2), repeated below: (2) John knows the truth hurts. At the point where John knows the truth has been proce</context>
</contexts>
<marker>Crocker, 1991</marker>
<rawString>Crocker, M. W. (1991): A Logical Model of Competence and Performance in the Human Sentence Processor. PhD thesis, Dept. of Artificial Intelligence, University of Edinburgh, Edinburgh, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
</authors>
<title>Syntactic processing: Evidence from Dutch.</title>
<date>1987</date>
<booktitle>In Natural Language and Linguistic Theory</booktitle>
<volume>5</volume>
<pages>519--559</pages>
<contexts>
<context position="4241" citStr="Frazier (1987)" startWordPosition="680" endWordPosition="681">sufficient grammatical information to postulate a structural relation between two constituents, such as in a sequence of two non-case marked NPs in an English centre-embedded construction, the parser may hold these constituents unstructured in its memory (in press, p.212). However, for the purposes of this implementation, we have taken the most constrained position. Note that, since we do not deal with such 291 Full Attachment model has been argued for, especially with regard to the processing of headfinal languages, where evidence has been found of pre-head structuring (Inoue &amp; Fodor (1991), Frazier (1987)). Such models have also been explored computationally (Milward (1995), Crocker (1991)). 2 D-theory and Gorrell&apos;s Model Gorrell employs the D-theoretic device of building up a set of dominance and precedence relations2 between nodes, where the set is intended to be constrained by informational monotonicity, in that once asserted to the set, no relation may be deleted or overridden. Gorrell restricts this constraint to Primary structural relations (i.e. dominance and precedence), while secondary relations (e.g. thematic and case dependencies) are not so constrained. Recall (2), repeated below: </context>
</contexts>
<marker>Frazier, 1987</marker>
<rawString>Frazier, L. (1987): Syntactic processing: Evidence from Dutch. In Natural Language and Linguistic Theory 5.4 p.519-559</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
<author>K Rayner</author>
</authors>
<title>Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences.</title>
<date>1982</date>
<journal>Cognitive Psychology</journal>
<volume>14</volume>
<pages>178--210</pages>
<contexts>
<context position="14937" citStr="Frazier and Rayner, 1982" startWordPosition="2515" endWordPosition="2518"> NPi. Of the two, most native English speakers report (6) to be easier. (6) I know the man who believes the countess killed herself. (7) I know the man who believes the countess killed himself. Note also, that, on standard X-bar assumptions, the attachment of post-modifiers may be derived via lowering at an X&apos; node. In this case, the lowered node and its replacement will be of the same syntactic category (like the root and foot node of a TAG auxiliary tree). Researchers have noted a general preference for low attachment of postmodifiers (this is accounted for by the principle of late closure (Frazier and Rayner, 1982)). This would suggest that a reasonable search strategy for English would be to search the set of accessible node in a bottom-up direction for English. The algorithm is constructed in such a way that lowering is only attempted in cases where simple attachment fails. This means that arguments (which are incorporated via simple attachment) will be attached preferentially to adjuncts (which are incorporated via lowering). This captures the general preference for argument over adjunct attachment, which is accounted for by the principle of Minimal attachment in Frazier and Rayner (1982), and by the</context>
</contexts>
<marker>Frazier, Rayner, 1982</marker>
<rawString>Frazier, L. and K. Rayner, (1982): Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. Cognitive Psychology 14 p.178-210</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
<author>N Pearlmutter</author>
<author>E Canesco-Gonzalez</author>
<author>Greg Hickok</author>
</authors>
<title>Cross-linguistic Attachment Preferences: Evidence from English and Spanish.</title>
<date>1993</date>
<note>(ms. submitted to Cognition)</note>
<contexts>
<context position="21718" citStr="Gibson et al (1993)" startWordPosition="3665" endWordPosition="3668">ustache/telescope&amp;quot;). The same result obtains if we abstract away from the particular implementational details of treelowering, and return to the abstract level at which Gorrell states his model. Once the PP has been attached as an argument of the verb, it can never be reanalysed as the adjunct of the preceding NP, because the NP will precede the PP before reanalysis, and dominate it after reanalysis, which is against the &amp;quot;exclusivity condition&amp;quot; on trees (i.e. no two nodes may stand in both a dominance and a precedence relation).7 A similar problem concerns examples such as the following, from Gibson et al (1993): (12) the lamps near the paintings of the house [that was damaged in the flood]. (13) the lamps near the painting of the houses [that was damaged in the flood]. (14) the lamp near the paintings of the houses [that was damaged in the flood]. in the above, Gibson et al have manipulated number agreement to force low (12), middle (13) and high (14) attachment of the bracketed relative clause. The results of their on- and off-line experiments show clearly that the low attachment (corresponding to 12) is easiest, but the middle attachment (corresponding to (13)) is most difficult. This behaviour ca</context>
</contexts>
<marker>Gibson, Pearlmutter, Canesco-Gonzalez, Hickok, 1993</marker>
<rawString>Gibson, E., N. Pearlmutter, E. Canesco-Gonzalez and Greg Hickok (1993): Cross-linguistic Attachment Preferences: Evidence from English and Spanish. (ms. submitted to Cognition)</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
<author>N Pearlmutter</author>
<author>E</author>
</authors>
<title>A corpus-based account of Psycholinguistic Constraints on Prepositional Phrase Attachment (in</title>
<date>1994</date>
<booktitle>Perspectives on Sentence Processing</booktitle>
<location>New York: Lawrence Erlbaum</location>
<marker>Gibson, Pearlmutter, E, 1994</marker>
<rawString>Gibson, E. and N. Pearlmutter, E. (1994): A corpus-based account of Psycholinguistic Constraints on Prepositional Phrase Attachment (in C. Clifton, L. Frazier and K. Rayner (eds) Perspectives on Sentence Processing New York: Lawrence Erlbaum</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Correll</author>
</authors>
<title>(in press): Syntax and Perception. to be published by Cambridge</title>
<publisher>University Press</publisher>
<marker>Correll, </marker>
<rawString>Correll, P. (in press): Syntax and Perception. to be published by Cambridge University Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Inoue</author>
</authors>
<title>A comparative study of parsing in English and Japanese.</title>
<date>1991</date>
<tech>PhD thesis,</tech>
<institution>University of Conneticut.</institution>
<contexts>
<context position="17648" citStr="Inoue (1991)" startWordPosition="2973" endWordPosition="2974">are to allow our parser to handle such examples, we must expand the definition of tree-lowering, since, in order to build a relative clause, we have to assert extra material (including the empty subject and the new S node), which is not justified solely by the lexical requirements of the disambiguating word, the head noun seaÂ°. This involves reconstructing all the clausal structure dominating the lowering site (including asserting empty argument positions), with reference to the verb&apos;s case frame, and attempting to attach the result as a relative clause to the head noun. 4.2 Minimal Expulsion Inoue (1991), describes a &amp;quot;minimal expulsion strategy&amp;quot;, which predicts a preference, on reanalysis, towards expelling the minimum amount of material from the clause. In our terms, this means that (assuming a binary right-branching clause structure, with the verb in its right corner) the node selected for lowering must be as high as possible. This means that the bottom-up search which we use for English will wrongly predict a Maximal expulsion strategy. In cases such as (8), assuming the bottom-up search, when a postclausal noun has been reached in the input, the 294 parser starts its search from the node </context>
</contexts>
<marker>Inoue, 1991</marker>
<rawString>Inoue, A. (1991): A comparative study of parsing in English and Japanese. PhD thesis, University of Conneticut.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Inoue</author>
<author>J D</author>
</authors>
<title>Fodor (in press): Informationpaced parsing of Japanese.</title>
<note>to appear in Mazuka Sz Nagai (eds</note>
<marker>Inoue, D, </marker>
<rawString>Inoue, A. and J.D. Fodor (in press): Informationpaced parsing of Japanese. (to appear in Mazuka Sz Nagai (eds))</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L S Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree Adjunct grammars.</title>
<date>1975</date>
<journal>Journal of Computer and System Sciences</journal>
<volume>10</volume>
<pages>136--163</pages>
<contexts>
<context position="7239" citStr="Joshi et al 1975" startWordPosition="1158" endWordPosition="1161">NP the ice cream. 3 Implementation Although Gorrell proposes a general principle to guide initial attachment decisions (Simplicity: No vacuous structure building), and specifies the conditions under which &amp;quot;unconscious reanalysis&amp;quot; may occur, the model leaves unspecified the problem of how the-system may be implemented. Of particular interest is the problem of how the parser decides which relations to add to the set at each point in time, especially at disambiguating points. 3.1 Lexical Representation The basic framework on which the implementation is built is similar to Tree Adjoining Grammar (Joshi et al 1975). Each lexical category is associated with a set of structural relations, which determine its lexical subtree. We call this set the sub-tree projection of that lexical category. For example, the subtree projection for verbs in the English grammar is as follows, where Lex is a variable which will be instantiated to the actual verb found in the input. fdom(S,NP), dom(S,VP), dom(VP,V), dom(V,Lex), prec(NP,VP)1 Lexical categories are also associated with lists of left and right attachment sites. In the above case, NP, (which will correspond to the subject of the verb), will be unified with the lef</context>
<context position="11658" citStr="Joshi et al, 1975" startWordPosition="1938" endWordPosition="1941">t sites. DEFINITION Tree-lowering: Let D be the current tree description. Let S be the subtree projection of the new word. The left attachment site A of S must match a node N accessible in D. The root node R of S must be licensed by the grammar in the position occupied by N. Let L be the set of local relations in which N participates. Let M be the result of substituting all instances of N in L with R. The attachment node A is unified with N. The updated tree-description is DUSUM5 It will be noticed that tree-lowering is similar in spirit to the adjunction operation of Tree Adjoining Grammars (Joshi et al, 1975). The difference is that the foot and root nodes of an auxiliary tree in TAG, (corresponding to the &amp;quot;lowered&amp;quot; node and the node that replaces it respectively) must be of the same syntactic category, whereas, as we have seen in this example, in the model proposed here, the two nodes May be of different categories, so long as the resulting structure is licensed by the grammar. In the case of example (2), at the point where the truth has been processed, the parser must find an accessible node which matches the category of the left attachment site of hurts (i.e. an NP). The only choice is NP2: (3)</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Joshi, A.K., L.S. Levy, and M. Takahashi, (1975): Tree Adjunct grammars. Journal of Computer and System Sciences 10, p.136-163</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language Cambridge,</title>
<date>1980</date>
<publisher>MIT Press</publisher>
<location>MA:</location>
<contexts>
<context position="3041" citStr="Marcus (1980)" startWordPosition="487" endWordPosition="488">ser&amp;quot; has been the subject of a number of computational implementations, including Marcus&apos;s deterministic parser (1980), Description theory (henceforth, D-theory) (Marcus et al (1983)), and Abney&apos;s licensing based model (1987, 1989). It has also been the subject of a number of psycholinguistic studies on a more theoretical level (Pritchett (1992), Gorrell (in press)). The implementation described in this paper is based on the most recent model, that of (Gorrell (in press)). This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980), Marcus et al (1983)), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987, 1989), Pritchett (1992)). Instead, processing is guided by the principle of Incremental Licensing, which states that &amp;quot;the parser attempts incrementally to satisfy the principles of grammar&amp;quot;. For the purposes of this implementation, I have interpreted this to mean that each word must be attached into a fullyconnected phrase marker as it is found in the input .1 The psychological desirability of such a &apos;In fact, Gorrell conjectures that, where there is insufficient gra</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Marcus, M. (1980): A Theory of Syntactic Recognition for Natural Language Cambridge, MA: MIT Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>D Hindle</author>
<author>M Fleck</author>
</authors>
<title>Dtheory: Talking about talking about trees.</title>
<date>1983</date>
<journal>Association for Computational Linguistics</journal>
<volume>21</volume>
<pages>129--136</pages>
<contexts>
<context position="2610" citStr="Marcus et al (1983)" startWordPosition="413" endWordPosition="416">een used to motivate a two-level architecture in the human syntactic processing system, where what we will call the &amp;quot;core parser&amp;quot; performs standard attachment, as well as being able to reanalyse in the easy cases (such as on reaching hurts in (2)), but where the assistance of a higher level resolver (to use Abney&apos;s terminology (1987, 1989)), is required to solve the difficult cases, (such as on reaching melted in (1)). This &amp;quot;core parser&amp;quot; has been the subject of a number of computational implementations, including Marcus&apos;s deterministic parser (1980), Description theory (henceforth, D-theory) (Marcus et al (1983)), and Abney&apos;s licensing based model (1987, 1989). It has also been the subject of a number of psycholinguistic studies on a more theoretical level (Pritchett (1992), Gorrell (in press)). The implementation described in this paper is based on the most recent model, that of (Gorrell (in press)). This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980), Marcus et al (1983)), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987, 1989), Pritchett (1992)). Instead, proce</context>
<context position="13076" citStr="Marcus et al (1983)" startWordPosition="2191" endWordPosition="2194">ction, S2 to derive two new relations: fdam(VP,S2), prec(V,S2)} These relations are found to be licensed, because the verb which V dominates (&amp;quot;knows&amp;quot;) may subcategorise for a clause, so these new relations are added to the sets. Now, adding the subtree projection of hurts to the set, and unifying its left &apos;Note that Abney&apos;s STEAL operation (1987, 1989) is more powerful than tree-lowering, since it may change domination relations, and thus will allow sentences such as (1), though it excludes reduced relative garden paths, such as The horse raced past the barn fell. The original D-theory model (Marcus et al (1983)) is also more powerful, because it allows the right-most daughter of a node to be lowered under a sibling node. &apos;Note that the relations defining the original position of NP2, (i.e. dom(VP,NP2) and prec(V,NP2)) are not subtracted from the set. 293 attachment site with NP2 results in the derived structure with NP2 &amp;quot;subordinated&amp;quot; into the lower clause. [s [NP, John] [vp [v knows] [s2 [NP2 the truth] [vp2 hurts]]]] With the tree-lowering operation so defined, the problem of finding which relations to add to the set at a disambiguating point reduces to a search for an accessible node at which to </context>
<context position="23068" citStr="Marcus et al (1983)" startWordPosition="3898" endWordPosition="3901">red preferences into the parser, the constraint of incrementality will force us to make the decision on encountering that. This means that, assuming we decide initially to attach low, but number agreement on was subsequently forces high attachment, as in (14), then a conscious garden path effect will be predicted, as lowering cannot derive the reanalysis. This is true on the abstract level as well, since there will be nodes in the description which precede the original low position of the relative clause, but are dominated by the subsequent high position of the relative clause. 7 Note that in Marcus et al (1983), since precedence relations were not computed for non-terminals, lowering into a predecessor was possible, thus (11) would cause no processing difficulty. However, presumably, their parser would overgenerate on examples such as the horse raced past the barn fell. 295 However, intuitively, of the above sentences, it is only (13) which causes the conscious garden path effect .8 6 Conclusion The current implementation shows that the success of an abstract model such as Gorrell&apos;s depends crucially on the computational details of the processing algorithm used. The search for the lowering site is o</context>
</contexts>
<marker>Marcus, Hindle, Fleck, 1983</marker>
<rawString>Marcus, M., D. Hindle, and M. Fleck (1983): Dtheory: Talking about talking about trees. Association for Computational Linguistics 21 p.129-136</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Mazuka</author>
<author>K</author>
</authors>
<title>Itoh (in press): Can Japanese be led down the garden path? (to appear in Mazuka and</title>
<location>Nagai</location>
<marker>Mazuka, K, </marker>
<rawString>Mazuka, R. and K. Itoh (in press): Can Japanese be led down the garden path? (to appear in Mazuka and Nagai)</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Mazuka</author>
</authors>
<title>and Nagai (eds) (to appear): Japanese Syntactic Processing</title>
<location>Hillsdale, NJ: Lawrence Earlbaum</location>
<marker>Mazuka, </marker>
<rawString>Mazuka, R., and Nagai (eds) (to appear): Japanese Syntactic Processing Hillsdale, NJ: Lawrence Earlbaum</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milward</author>
</authors>
<title>Incremental Interpretation of Categorial Grammar.</title>
<date>1995</date>
<booktitle>in Proceedings of EACL (this</booktitle>
<volume>volume)</volume>
<contexts>
<context position="4311" citStr="Milward (1995)" startWordPosition="690" endWordPosition="691">between two constituents, such as in a sequence of two non-case marked NPs in an English centre-embedded construction, the parser may hold these constituents unstructured in its memory (in press, p.212). However, for the purposes of this implementation, we have taken the most constrained position. Note that, since we do not deal with such 291 Full Attachment model has been argued for, especially with regard to the processing of headfinal languages, where evidence has been found of pre-head structuring (Inoue &amp; Fodor (1991), Frazier (1987)). Such models have also been explored computationally (Milward (1995), Crocker (1991)). 2 D-theory and Gorrell&apos;s Model Gorrell employs the D-theoretic device of building up a set of dominance and precedence relations2 between nodes, where the set is intended to be constrained by informational monotonicity, in that once asserted to the set, no relation may be deleted or overridden. Gorrell restricts this constraint to Primary structural relations (i.e. dominance and precedence), while secondary relations (e.g. thematic and case dependencies) are not so constrained. Recall (2), repeated below: (2) John knows the truth hurts. At the point where John knows the trut</context>
</contexts>
<marker>Milward, 1995</marker>
<rawString>Milward, D. (1995): Incremental Interpretation of Categorial Grammar. in Proceedings of EACL (this volume)</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Mitchell</author>
<author>F Cuetos</author>
</authors>
<title>The origins of parsing strategies. Conference proceedings: Current issues in natural language processing</title>
<date>1991</date>
<institution>University of Texas at Austin, TX</institution>
<marker>Mitchell, Cuetos, 1991</marker>
<rawString>Mitchell, D.C. &amp; Cuetos, F. (1991): The origins of parsing strategies. Conference proceedings: Current issues in natural language processing University of Texas at Austin, TX</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Partee</author>
<author>A ter Meulen</author>
<author>R E Wall</author>
</authors>
<date>1993</date>
<booktitle>Mathematical methods in Linguistics Dordrecht:</booktitle>
<publisher>Kluwer Academic Publishers</publisher>
<contexts>
<context position="6270" citStr="Partee et al (1993)" startWordPosition="1001" endWordPosition="1004">he disambiguating word hurts is a subset of the final tree description, the monotonicity requirement is satisfied. Note in particular, that, because dominance is a transitive relation, and because of the inheritance condition on trees (a node inherits the precedence relations of its ancestors3), the two statements dom(VP,NP2) and prec (V ,NP2) remain true after reanalysis.4 constructions, none of the arguments presented here hinge on whether or not the parser may buffer material in this way. 2The original D-theory model did not compute precedence relations, except between terminal nodes. &apos;See Partee et al (1993) for a description of the conditions on trees, with which all tree descriptions must comply. &apos;It will be noticed that the reanalysis here involves Note also that the model will correctly fail to reanalyse for sentence (1) above, since the reanalysis will require the retraction of the domination relation between the VP of the adverbial clause and the NP the ice cream. 3 Implementation Although Gorrell proposes a general principle to guide initial attachment decisions (Simplicity: No vacuous structure building), and specifies the conditions under which &amp;quot;unconscious reanalysis&amp;quot; may occur, the mod</context>
</contexts>
<marker>Partee, Meulen, Wall, 1993</marker>
<rawString>Partee, B., A. ter Meulen and R. E. Wall (1993): Mathematical methods in Linguistics Dordrecht: Kluwer Academic Publishers</rawString>
</citation>
<citation valid="true">
<authors>
<author>B L Pritchett</author>
</authors>
<title>Grammatical Competence and Parsing Performance.</title>
<date>1992</date>
<publisher>University of Chicago Press</publisher>
<location>Chicago, IL:</location>
<contexts>
<context position="1007" citStr="Pritchett (1992)" startWordPosition="154" endWordPosition="155">ious&amp;quot; garden paths may be processed via the addition of structural relations to a monotone increasing set at the point of disambiguation, but there is no discussion as to how the parser decides which relations to add. We model this decision as a search for a node in the tree at which an explicitly defined parsing operation, tree-lowering may be applied. With reference to English and Japanese processing data, we show the importance of this search for empirical adequacy of the psycholinguistic model. 1 Conscious and Unconscious Garden Paths Certain researchers in the psycholinguistic community (Pritchett (1992), Gorrell (in press)), have argued for a binary distinction between two distinct types of garden path sentences. Conscious garden paths, such as (1) below, are locally ambiguous sentences which give rise to reanalysis that is both experimentally detectable and causes a conscious sensation of difficulty or &amp;quot;surprise effect&amp;quot;. Unconscious garden paths, on the other hand, such as (2), cause reanalysis which is experimentally detectable, but which is generally not &amp;quot;noticed&amp;quot; by the speaker or hearer. *The work reported here was done very much in a collaborative spirit with my supervisor, Dr. Matthew</context>
<context position="2775" citStr="Pritchett (1992)" startWordPosition="442" endWordPosition="443">as being able to reanalyse in the easy cases (such as on reaching hurts in (2)), but where the assistance of a higher level resolver (to use Abney&apos;s terminology (1987, 1989)), is required to solve the difficult cases, (such as on reaching melted in (1)). This &amp;quot;core parser&amp;quot; has been the subject of a number of computational implementations, including Marcus&apos;s deterministic parser (1980), Description theory (henceforth, D-theory) (Marcus et al (1983)), and Abney&apos;s licensing based model (1987, 1989). It has also been the subject of a number of psycholinguistic studies on a more theoretical level (Pritchett (1992), Gorrell (in press)). The implementation described in this paper is based on the most recent model, that of (Gorrell (in press)). This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980), Marcus et al (1983)), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987, 1989), Pritchett (1992)). Instead, processing is guided by the principle of Incremental Licensing, which states that &amp;quot;the parser attempts incrementally to satisfy the principles of grammar&amp;quot;. For the purpos</context>
</contexts>
<marker>Pritchett, 1992</marker>
<rawString>Pritchett, B. L. (1992): Grammatical Competence and Parsing Performance. Chicago, IL: University of Chicago Press</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>