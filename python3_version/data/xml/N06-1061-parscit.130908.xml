<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000117">
<title confidence="0.991299">
Language Model-Based Document Clustering Using Random Walks
</title>
<author confidence="0.988887">
G¨unes¸ Erkan
</author>
<affiliation confidence="0.996588">
Department of EECS
University of Michigan
</affiliation>
<address confidence="0.986831">
Ann Arbor, MI 48109-2121
</address>
<email confidence="0.99899">
gerkan@umich.edu
</email>
<sectionHeader confidence="0.9923605" genericHeader="abstract">
Abstract
1 Introduction
</sectionHeader>
<bodyText confidence="0.999915642857143">
Document clustering is one of the oldest and most studied
problems of information retrieval (van Rijsbergen, 1979).
Almost all document clustering approaches to date have
represented documents as vectors in a bag-of-words vec-
tor space model, where each dimension of a document
vector corresponds to a term in the corpus (Salton and
McGill, 1983). General clustering algorithms are then
applied to these vectors to cluster the given corpus. There
have been attempts to use bigrams or even higher-order n-
grams to represent documents in text categorization, the
supervised counterpart of document clustering, with little
success (Caropreso et al., 2001; Tan et al., 2002).
Clustering can be viewed as partitioning a set of data
objects into groups such that the similarities between the
objects in a same group is high while inter-group simi-
larities are weaker. The fundamental assumption in this
work is that the documents that are likely to have been
generated from similar language models are likely to be
in the same cluster. Under this assumption, we propose a
new representation for document vectors specifically de-
signed for clustering purposes.
Given a corpus, we are interested in the generation
probabilities of a document based on the language models
induced by other documents in the corpus. Using these
probabilities, we propose a vector representation where
each dimension of a document vector corresponds to a
document in the corpus instead of a term in the classical
representation. In other words, our document vectors are
-dimensional, whereis the number of documents in
the corpus to be clustered. For the vectorof docu-
ment, theth element ofis closely related to the
generation probability ofbased on the language model
induced by document. The main steps of our method
are as follows:
For each ordered document pairin a given
corpus, we compute the generation probability of
from the language model induced bymak-
ing use of language-model approaches in informa-
tion retrieval (Ponte and Croft, 1998).
We represent each document by a vector of its gen-
eration probabilities based on other documents’ lan-
guage models. At this point, these vectors can be
used in any clustering algorithm instead of the tradi-
tional term-based document vectors.
Following (Kurland and Lee, 2005), our new doc-
ument vectors are used to construct the underlying
generation graph; the directed graph where docu-
ments are the nodes and link weights are propor-
tional to the generation probabilities.
We use restricted random walk probabilities to rein-
force the generation probabilities and discover hid-
den relationships in the graph that are not obvious
by the generation links. Our random walk model
is similar to the one proposed by Harel and Kohen
We propose a new document vector represen-
tation specifically designed for the document
clustering task. Instead of the traditional term-
based vectors, a document is represented as an
-dimensional vector, whereis the number of
documents in the cluster. The value at each di-
mension of the vector is closely related to the
generation probability based on the language
model of the corresponding document. In-
spired by the recent graph-based NLP methods,
we reinforce the generation probabilities by it-
erating random walks on the underlying graph
representation. Experiments with k-means and
hierarchical clustering algorithms show signif-
icant improvements over the alternative
vector representation.
</bodyText>
<page confidence="0.995579">
479
</page>
<note confidence="0.995129">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 479–486,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.8673958">
(2001) for general spatial data represented as undi-
rected graphs. We have extended their model to the
directed graph case. We use new probabilities de-
rived from random walks as the vector representa-
tion of the documents.
</bodyText>
<sectionHeader confidence="0.9497765" genericHeader="introduction">
2 Generation Probabilities as Document
Vectors
</sectionHeader>
<subsectionHeader confidence="0.988506">
2.1 Language Models
</subsectionHeader>
<bodyText confidence="0.999940470588236">
The language modeling approach to information retrieval
was first introduced by Ponte and Croft (1998) as an al-
ternative (or an improvement) to the traditional
relevance models. In the language modeling framework,
each document in the database defines a language model.
The relevance of a document to a given query is ranked
according to the generation probability of the query based
on the underlying language model of the document. To
induce a (unigram) language model from a document, we
start with the maximum likelihood (ML) estimation of
the term probabilities. For each termthat occurs in a
document, the ML estimation ofwith respect to
is defined as
whereis the number of occurences of termin
document. This estimation is often smoothed based on
the following general formula:
whereis the ML estimation ofover
an entire corpus which usuallyis a member of.is the
general smoothing parameter that takes different forms
in various smoothing methods. Smoothing has two im-
portant roles (Zhai and Lafferty, 2004). First, it accounts
for terms unseen in the document preventing zero prob-
abilities. This is similar to the smoothing effect in NLP
problems such as parsing. Second, smoothing has an-
like effect that accounts for the generation probabilities of
the common terms in the corpus. A common smoothing
technique is to use Bayesian smoothing with the Dirichlet
prior (Zhai and Lafferty, 2004; Liu and Croft, 2004):
Here,is the smoothing parameter. Higher values of
mean more aggressive smoothing.
Assuming the terms in a text are independent from
each other, the generation probability of a text sequence
given the documentis the product of the generation
probabilities of the terms of:
</bodyText>
<equation confidence="0.811046">
(1)
</equation>
<bodyText confidence="0.99996595">
In the context of information retrieval,is a query
usually composed of few terms. In this work, we are
interested in the generation probabilities of entire docu-
ments that usually have in the order of hundreds of unique
terms. If we use Equation 1, we end up having unnatural
probabilities which are irrepresentably small and cause
floating point underflow. More importantly, longer docu-
ments tend to have much smaller generation probabilities
no matter how closely related they are to the generating
language model. However, as we are interested in the
generation probabilities between all pairs of documents,
we want to be able to compare two different generation
probabilities from a fixed language model regardless of
the target document sizes. This is not a problem in the
classical document retrieval setting since the given query
is fixed, and generation probabilities for different queries
are not compared against each other. To address these
problems, following (Lavrenko et al., 2002; Kurland and
Lee, 2005), we “flatten” the probabilities by normalizing
them with respect to the document size:
</bodyText>
<equation confidence="0.75368">
(2)
flat
</equation>
<bodyText confidence="0.997063333333333">
whereis the number of terms in.flat provides
us with meaningful values which are comparable among
documents of different sizes.
</bodyText>
<subsectionHeader confidence="0.9933735">
2.2 Using Generation Probabilities as Document
Representations
</subsectionHeader>
<bodyText confidence="0.99867925">
Equation 2 suggests a representation of the relation-
ship of a document with the other documents in a
corpus. Given a corpus ofdocuments to cluster,
we form an-dimensional generation vector
</bodyText>
<equation confidence="0.7638605">
for each documentwhere
if (3)
otherwise
flat
</equation>
<bodyText confidence="0.99990494117647">
We can use these generation vectors in any clustering
algorithm we prefer instead of the classical term-based
vectors. The intuition behind this idea becomes
clearer when we consider the underlying directed graph
representation, where each document is a node and the
weight of the link fromtois equal toflat .
An appropriate analogy here is the citation graph of sci-
entific papers. The generation graph can be viewed as a
model where documents cite each other. However, un-
like real citations, the generation links are weighted and
automatically induced from the content.
The similarity function used in a clustering algorithm
over the generation vectors becomes a measure of struc-
tural similarity of two nodes in the generation graph.
Work on bibliometrics uses various similarity metrics to
assess the relatedness of scientific papers by looking at
the citation vectors (Boyack et al., 2005). Graph-based
</bodyText>
<equation confidence="0.487432">
tf
tf
</equation>
<page confidence="0.978665">
480
</page>
<bodyText confidence="0.999992777777778">
similarity metrics are also used to detect semantic simi-
larity of two documents on the Web (Maguitman et al.,
2005). Cosine, also the standard metric used in
based document clustering, is one of these metrics. In-
tuitively, the cosine of the citation vectors (i.e. vector of
outgoing link weights) of two nodes is high when they
link to similar sets of nodes with similar link weights.
Hence, the cosine of two generation vectors is a measure
of how likely two documents are generated from the same
documents’ language models.
The generation probability in Equation 2 with a
smoothed language model is never zero. This creates two
potential problems if we want to use the vector of Equa-
tion 3 directly in a clustering algorithm. First, we only
want strong generation links to contribute in the similar-
ity function since a low generation probability is not an
evidence for semantic relatedness. This intuition is sim-
ilar to throwing out the stopwords from the documents
before constructing thevectors to avoid coinci-
dental similarities between documents. Second, having
a dense vector with lots of non-zero elements will cause
efficiency problems. Vector length is assumed to be a
constant factor in analyzing the complexity of the clus-
tering algorithms. However, our generation vectors are
-dimensional, whereis the number of documents. In
other words, vector size is not a constant factor anymore,
which causes a problem of scalability to large data sets.
To address these problems, we use what Kurland and Lee
(2005) define as top generators: Given a document,
we consider onlydocuments that yield the largest gen-
eration probabilities and discard others. The resultant-
dimensional vector, denoted, has at mostnon-zero
elements, which are the largestelements of. For a
given constant, with a sparse vector representation, cer-
tain operations (e.g. cosine) on such vectors can be done
in constant time independent of.
</bodyText>
<subsectionHeader confidence="0.997704">
2.3 Reinforcing Links with Random Walks
</subsectionHeader>
<bodyText confidence="0.968044177419355">
Generation probabilities are only an approximation of se-
mantic relatedness. Using the underlying directed graph
interpretation of the generation probabilities, we aim to
get better approximations by accumulating the generation
link information in the graph. We start with some defini-
tions. We denote a (directed) graph aswhere
is the set of nodes and is the link
weight function. We formally define a generation graph
as follows:
Definition 1 Given a corpuswith
documents, and a constant, the generation graph of
is a directed graph, where.
Definition 2 A-step random walk on a graph
that starts at node is a sequence of nodes
wherefor all
. The probability of a-step random walk is defined
aswhere
is called the transition probability from nodeto
node.
For example, for a generation graph, there are at most
1-step random walks that start at a given node with
probabilities proportional to the weights of the outgoing
generation links of that node.
Suppose there are three documents,, andin a
generation graph. Suppose also that there are “strong”
generation links fromtoandto, but no link
fromto. The intuition says thatmust be semanti-
cally related toto a certain degree although there is no
generation link between them depending on’s language
model. We approximate this relation by considering the
probabilities of 2-step (or longer) random walks from
toalthough there is no 1-step random walk fromto
.
Letdenote the probability that an-step random
walk starts atand ends at. An interesting property
of random walks is that for a given node,does not
depend on. In other words, the probability of a random
walk ending up at“in the long run” does not depend
on its starting point (Seneta, 1981). This limiting prob-
ability distribution of an infinite random walk over the
nodes is called the stationary distribution of the graph.
The stationary distribution is uninteresting to us for clus-
tering purposes since it gives an information related to the
global structure of the graph. It is often used as a measure
to rank the structural importance of the nodes in a graph
(Brin and Page, 1998). For clustering, we are more inter-
ested in the local similarities inside a “cluster” of nodes
that separate them from the rest of the graph. Further-
more, the generation probabilities lose their significance
during long random walks since they get multiplied at
each step. Therefore, we computefor small values of
. Finally, we define the following:
Definition 3 The-step generation probability of docu-
mentfrom the language model of:
gen
✎gengengenis
the-step generation vector ofdocument. We will often
writeomitting the document name when we are not
talking about the vector of a specific document.
genis a measure of how likely a random walk
that starts atwill visitinor fewer steps. It helps
us to discover “hidden” similarities between documents
</bodyText>
<page confidence="0.993906">
481
</page>
<bodyText confidence="0.9998315">
that are not immediately obvious from 1-step generation
links. Note that when,is nothing but
normalized such that the sum of the elements of the vec-
tor is 1. The two are practically the same representations
since we compute the cosine of the vectors during clus-
tering.
</bodyText>
<sectionHeader confidence="0.999888" genericHeader="method">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999983125">
Our work is inspired by three main areas of research.
First, the success of language modeling approaches to
information retrieval (Ponte and Croft, 1998) is encour-
aging for a similar twist to document representation for
clustering purposes. Second, graph-based inference tech-
niques to discover “hidden” textual relationships like the
one we explored in our random walk model have been
successfully applied to other NLP problems such as sum-
marization (Erkan and Radev, 2004; Mihalcea and Ta-
rau, 2004; Zha, 2002), prepositional phrase attachment
(Toutanova et al., 2004), and word sense disambiguation
(Mihalcea, 2005). Unlike our approach, these methods
try to exploit the global structure of a graph to rank the
nodes of the graph. For example, Erkan and Radev (2004)
find the stationary distribution of the random walk on a
graph of sentences to rank the salience scores of the sen-
tences for extractive summarization. Their link weight
function is based on cosine similarity. Our graph con-
struction based on generation probabilities is inherited
from (Kurland and Lee, 2005), where authors used a sim-
ilar generation graph to rerank the documents returned
by a retrieval system based on the stationary distribu-
tion of the graph. Finally, previous research on clustering
graphs with restricted random walks inspired us to clus-
ter the generation graph using a similar approach. Our
-step random walk approach is similar to the one pro-
posed by Harel and Koren (2001). However, their algo-
rithm is proposed for “spatial data” where the nodes of
the graph are connected by undirected links that are de-
termined by a (symmetric) similarity function. Our con-
tribution in this paper is to use their approach on textual
data by using generation links, and extend the method to
directed graphs.
There is an extensive amount of research on document
clustering or clustering algorithms in general that we can
not possibly review here. After all, we do not present a
new clustering algorithm, but rather a new representation
of textual data. We explain some popular clustering algo-
rithms and evaluate our representation using them in Sec-
tion 4. Few methods have been proposed to cluster doc-
uments using a representation other than the traditional
vector space (or similar term-based vectors). Us-
ing a bipartite graph of terms and documents and then
clustering this graph based on spectral methods is one of
them (Dhillon, 2001; Zha et al., 2001). There are also
general spectral methods that start withvectors,
then map them to a new space with fewer dimensions be-
fore initiating the clustering algorithm (Ng et al., 2001).
The information-theoretic clustering algorithms are
relevant to our framework in the sense that they involve
probability distributions over words just like the language
models. However, instead of looking at the word distri-
butions at the individual document level, they make use
of the joint distribution of words and documents. For ex-
ample, given the set of documentsand the set of words
in the document collection, Slonim and Tishby (2000)
first try to find a word clusteringsuch that the mutual
informationis minimized (for good compres-
sion) while maximizing the(for preserving the
original information). Then the same procedure is used
for clustering documents using the word clusters from the
first step. Dhillon et. al. (2003) propose a co-clustering
version of this information-theoretic method where they
cluster the words and the documents concurrently.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999928714285714">
We evaluated our new vector representation by compar-
ing it against the traditionalvector space repre-
sentation. We ran k-means, single-link, average-link, and
complete-link clustering algorithms on various data sets
using both representations. These algorithms are among
the most popular ones that are used in document cluster-
ing.
</bodyText>
<subsectionHeader confidence="0.986012">
4.1 General Experimental Setting
</subsectionHeader>
<bodyText confidence="0.987641826086957">
Given a corpus, we stemmed all the documents, removed
the stopwords and constructed thevector for each
document by using the bow toolkit (McCallum, 1996).
We computed theof each term using the following
formula:
whereis the total number of documents and df is
the number of documents that the termappears in.
We computed flattened generation probabilities (Equa-
tion 2) for all ordered pairs of documents in a corpus,
and then constructed the corresponding generation graph
(Definition 1). We used Dirichlet-smoothed language
models with the smoothing parameter, which
can be considered as a typical value used in information
retrieval. While computing the generation link vectors,
we did not perform extensive parameter tuning at any
stage of our method. However, we observed the follow-
ing:
When(number of outgoing links per document)
was very small (less than 10), our methods per-
formed poorly. This is expected with such a sparse
vector representation for documents. However, the
performance got rapidly and almost monotonically
idf df
</bodyText>
<page confidence="0.983294">
482
</page>
<bodyText confidence="0.999832294117647">
better as we increaseduntil around, where
the performance stabilized and dropped after around
. We conclude that using bounded num-
ber of outgoing links per document is not only more
efficient but also necessary as we motivated in Sec-
tion 2.2.
We got the best results when the random walk pa-
rameter. When, the random walk goes
“out of the cluster” andvectors become very
dense. In other words, almost all of the graph is
reachable from a given node with 4-step or longer
random walks (assumingis around 80), which is
an indication of a “small world” effect in generation
graphs (Watts and Strogatz, 1998).
Under these observations, we will only report results us-
ing vectors,andwithregardless
of the data set and the clustering algorithm.
</bodyText>
<subsectionHeader confidence="0.9731745">
4.2 Experiments with k-means
4.2.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.999978">
k-means is a clustering algorithm popular for its sim-
plicity and efficiency. It requires, the number of clus-
ters, as input, and partitions the data set into exactly
clusters. We used a version of k-means that uses cosine
similarity to compute the distance between the vectors.
The algorithm can be summarized as follows:
</bodyText>
<listItem confidence="0.989388666666667">
1. randomly selectdocument vectors as the initial
cluster centroids;
2. assign each document to the cluster whose centroid
yields the highest cosine similarity;
3. recompute the centroid of each cluster. (centroid
vector of a cluster is the average of the vectors in
that cluster);
4. stop if none of the centroid vectors has changed at
step 3. otherwise go to step 2.
</listItem>
<subsectionHeader confidence="0.416063">
4.2.2 Data
</subsectionHeader>
<bodyText confidence="0.968556428571429">
k-means is known to work better on data sets in which
the documents are nearly evenly distributed among dif-
ferent clusters. For this reason, we tried to pick such
corpora for this experiment to be able to get a fair com-
parison between different document representations. The
first corpus we used is classic3,1 which is a collection
of technical paper abstracts in three different areas. We
used two corpora, bbc and bbcsport, that are composed
1ftp://ftp.cs.cornell.edu/pub/smart
of BBC news articles in general and sports news, respec-
tively. 2 Both corpora have 5 news classes each. 20news3
is a corpus of newsgroup articles composed of 20 classes.
Table 1 summarizes the corpora we used together with
the sizes of the smallest and largest class in each of them.
</bodyText>
<table confidence="0.998959">
Corpus Documents Classes Smallest Largest
classic3 3891 3 1033 1460
bbcsport 737 5 100 265
bbc 2225 5 386 511
20news 18846 20 628 999
</table>
<tableCaption confidence="0.999974">
Table 1: The corpora used in the k-means experiments.
</tableCaption>
<sectionHeader confidence="0.559144" genericHeader="method">
4.2.3 Results
</sectionHeader>
<bodyText confidence="0.993510842105263">
We used two different metrics to evaluate the results
of the k-means algorithm; accuracy and mutual informa-
tion. Letbe the label assigned toby the clustering
algorithm, andbe’s actual label in the corpus. Then,
map
Accuracy
whereequals 1 ifand equals zero other-
wise. mapis the function that maps the output la-
bel set of the k-means algorithm to the actual label set
of the corpus. Given the confusion matrix of the output,
best such mapping function can be efficiently found by
Munkres’s algorithm (Munkres, 1957).
Mutual information is a metric that does not require
a mapping function. Let be the
output label set of the k-means algorithm, and
be the actual label set of the corpus
with the underlying assignments of documents to these
sets. Mutual information (MI) of these two labelings is
defined as:
</bodyText>
<equation confidence="0.547555">
MI log
</equation>
<bodyText confidence="0.999962">
whereandare the probabilities that a docu-
ment is labeled asandby the algorithm and in the
actual corpus, respectively;is the probability
that these two events occur at the same time. These val-
ues can be derived from the confusion matrix. We map
the MI metric to theinterval by normalizing it with
the maximum possible MI that can be achieved with the
corpus. Normalized MI is defined as
</bodyText>
<footnote confidence="0.979379833333333">
2http://www.cs.tcd.ie/Derek.Greene/
research/datasets.html BBC corpora came in
preprocessed format so that we did not perform the processing
with the bow toolkit mentioned in Section 4.1
3http://people.csail.mit.edu/jrennie/
20Newsgroups
</footnote>
<figure confidence="0.686668333333333">
MI
MI
MI
</figure>
<page confidence="0.998943">
483
</page>
<bodyText confidence="0.999970785714286">
One disadvantage of k-means is that its performance
is very dependent on the initial selection of cluster cen-
troids. Two approaches are usually used when reporting
the performance of k-means. The algorithm is run mul-
tiple times; then either the average performance of these
runs or the best performance achieved is reported. Re-
porting the best performance is not very realistic since
we would not be clustering a corpus if we already knew
the class labels. Reporting the average may not be very
informative since the variance of multiple runs is usually
large. We adopt an approach that is somewhere in be-
tween. We use “true seeds” to initialize k-means, that is,
we randomly selectdocument vectors that belong to
each of the true classes as the initial centroids. This is
not an unrealistic assumption since we initially know the
number of classes,, in the corpus, and the cost of find-
ing one example document from each class is not usually
high. This way, we also aim to reduce the variance of the
performance of different runs for a better analysis.
Table 2 shows the results of k-means algorithm us-
ingvectors versus generation vectors(plain
flattened generation probabilities),(2-step random
walks),(3-step random walks). Taking advantage
of the relatively larger size and number of classes of
20news corpus, we randomly divided it into disjoint par-
titions with 4, 5, and 10 classes which provided us with
5, 4, and 2 new corpora, respectively. We named them
4news-1, 4news-2,, 10news-2 for clarity. We ran k-
means with 30 distinct initial seed sets for each corpus.
The first observation we draw from Table 2 is that even
vectors perform better than themodel. This is
particularly surprising given thatvectors are sparser
than therepresentation for most documents.4 All
vectors clearly outperformmodel often by
a wide margin. The performance also gets better (not al-
ways significantly though) in almost all data sets as we in-
crease the random walk length, which indicates that ran-
dom walks are useful in reinforcing generation links and
inducing new relationships. Another interesting observa-
tion is that the confidence intervals are also narrower for
generation vectors, and tend to get even narrower as we
increase.
</bodyText>
<subsectionHeader confidence="0.9986225">
4.3 Experiments with Hierarchical Clustering
4.3.1 Algorithms
</subsectionHeader>
<bodyText confidence="0.9992786">
Hierarchical clustering algorithms start with the triv-
ial clustering of the corpus where each document de-
fines a separate cluster by itself. At each iteration, two
“most similar” separate clusters are merged. The algo-
rithm stops afteriterations when all the documents
</bodyText>
<footnote confidence="0.7658855">
4Remember that we setin our experiments which
means that there can be a maximum of 80 non-zero elements
in. Most documents have more than 80 unique terms in
them.
</footnote>
<bodyText confidence="0.999340272727273">
are merged into a single cluster.
Hierarchical clustering algorithms differ in how they
define the similarity between two clusters at each merg-
ing step. We experimented with three of the most popular
algorithms using cosine as the similarity metric between
two vectors. Single-link clustering merges two clusters
whose most similar members have the highest similarity.
Complete-link clustering merges two clusters whose least
similar members have the highest similarity. Average-link
clustering merges two clusters that yield the highest av-
erage similarity between all pairs of documents.
</bodyText>
<subsectionHeader confidence="0.267022">
4.3.2 Data
</subsectionHeader>
<table confidence="0.981420666666667">
Corpus Documents Classes Smallest Largest
Reuters 8646 57 2 3735
TDT2 10160 87 2 1843
</table>
<tableCaption confidence="0.989409">
Table 3: The corpora used in the hierarchical clustering exper-
iments.
</tableCaption>
<bodyText confidence="0.999810866666667">
Although hierarchical algorithms are not very efficient,
they are useful when the documents are not evenly dis-
tributed among the classes in the corpus and some classes
exhibit a “hierarchical” nature; that is, some classes in the
data might be semantically overlapping or they might be
in a subset/superset relation with each other. We picked
two corpora that may exhibit such nature to a certain ex-
tent. Reuters-215785 is a collection of news articles from
Reuters. TDT26 is a similar corpus of news articles col-
lected from six news agencies in 1998. They contain doc-
uments labeled with zero, one or more class labels. For
each corpus, we used only the documents with exactly
one label. We also eliminated classes with only one doc-
ument since clustering such classes is trivial. We ended
up with two collections summarized in Table 3.
</bodyText>
<sectionHeader confidence="0.732436" genericHeader="evaluation">
4.3.3 Results
</sectionHeader>
<bodyText confidence="0.999878444444444">
The output of a hierarchical clustering algorithm is a
tree where leaves are the documents and each node in the
tree shows a cluster merging operation. Therefore each
subtree represents a cluster. We assume that each class of
documents in the corpus form a cluster subtree at some
point during the construction of the tree. To evaluate the
cluster tree, we use F-measure proposed in (Larsen and
Aone, 1999). F-measure for a classin the corpus and a
subtreeis defined as
</bodyText>
<footnote confidence="0.9996535">
5http://kdd.ics.uci.edu/databases/
reuters21578/reuters21578.html
6http://www.nist.gov/speech/tests/tdt/
tdt98/index.htm
</footnote>
<page confidence="0.991939">
484
</page>
<table confidence="0.993937125">
Corpus k Accuracy () Normalized Mutual Information ()
classic3 3
4news-1 4
4news-2 4
4news-3 4
4news-4 4
4news-5 4
5news-1 5
5news-2 5
5news-3 5
5news-4 5
bbc 5
bbcsport 5
10news-1 10
10news-2 10
20news 20
</table>
<tableCaption confidence="0.978001">
Table 2: Performances of different vector representations using k-means (average of 30 runsconfidence interval).
</tableCaption>
<table confidence="0.99972525">
Algorithm TDT2 Reuters-21578
single-link 65.25 82.96 84.22 83.92 59.35 59.37 65.70 66.15
average-link 90.78 93.53 94.04 94.13 78.25 79.17 77.24 81.37
complete-link 29.07 25.04 27.19 34.67 43.66 42.79 45.91 48.36
</table>
<tableCaption confidence="0.999666">
Table 4: Performances (F-measure) of different vector representations using hierarchical algorithms on two corpora.
</tableCaption>
<bodyText confidence="0.99997825">
whereandis the recall and the pre-
cision ofconsidering the class. Letbe the set
of subtrees in the output cluster tree, andbe the set
of classes. F-measure of the entire tree is the weighted
average of the maximum F-measures of all the classes:
whereis the number of documents that belong to class
.
We ran all three algorithms for both corpora. Unlike k-
means, hierarchical algorithms we used are deterministic.
Table 4 summarizes our results. An immediate observa-
tion is that average-link clustering performs much bet-
ter than other two algorithms independent of the data set
or the document representation, which is consistent with
earlier research (Zhao and Karypis, 2002). The high-
est result (shown boldface) for each algorithm and cor-
pus was achieved by using generation vectors. However,
unlike in the k-means experiments,was able to
outperformandin one or two cases.
yielded the best result instead ofin one of the six
cases.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999980545454546">
We have presented a language model inspired approach
to document clustering. Our results show that even the
simplest version of our approach with nearly no parame-
ter tuning can outperform traditionalmodels by a
wide margin. Random walk iterations on our graph-based
model have improved our results even more. Based on the
success of our model, we will investigate various graph-
based relationships for explaining semantic structure of
text collections in the future. Possible applications in-
clude information retrieval, text clustering/classification
and summarization.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9984935">
I would like to thank Dragomir Radev for his useful com-
ments. This work was partially supported by the U.S.
National Science Foundation under the following two
grants: 0329043 “Probabilistic and link-based Methods
for Exploiting Very Large Textual Repositories” admin-
istered through the IDM program and 0308024 “Collab-
orative Research: Semantic Entity and Relation Extrac-
tion from Web-Scale Text Document Collections” admin-
istered by the HLT program. All opinions, findings, con-
clusions, and recommendations in this paper are made by
the authors and do not necessarily reflect the views of the
National Science Foundation.
</bodyText>
<sectionHeader confidence="0.997034" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9108966">
Kevin W. Boyack, Richard Klavans, and Katy B¨orner.
2005. Mapping the backbone of science. Scientometrics,
64(3):351–374.
Sergey Brin and Lawrence Page. 1998. The anatomy of a large-
scale hypertextual web search engine. In Proceedings of the
</reference>
<page confidence="0.994942">
485
</page>
<reference confidence="0.998420607843137">
7th International World Wide Web Conference, pages 107–
117.
Maria Fernanda Caropreso, Stan Matwin, and Fabrizio Sebas-
tiani. 2001. A learner-independent evaluation of the use-
fulness of statistical phrases for automated text categoriza-
tion. In Amita G. Chin, editor, Text Databases and Docu-
mentManagement: Theory and Practice, pages 78–102. Idea
Group Publishing, Hershey, US.
Inderjit S. Dhillon, Subramanyam Mallela, and Dharmendra S.
Modha. 2003. Information-theoretic co-clustering. In Pe-
dro Domingos, Christos Faloutsos, Ted SEnator, Hillol Kar-
gupta, and Lise Getoor, editors, Proceedings of the ninth
ACM SIGKDD International Conference on Knowledge Dis-
covery and Data Mining (KDD-03), pages 89–98, New York,
August 24–27. ACM Press.
Inderjit S. Dhillon. 2001. Co-clustering documents and words
using bipartite spectral graph partitioning. In Proceedings of
the Seventh ACM SIGKDD Conference, pages 269–274.
G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-
based lexical centrality as salience in text summarization.
Journal ofArtificial Intelligence Research, 22:457–479.
David Harel and Yehuda Koren. 2001. Clustering spatial data
using random walks. In Proceedings of the Seventh ACM
SIGKDD Conference, pages 281–286, New York, NY, USA.
ACM Press.
Oren Kurland and Lillian Lee. 2005. PageRank without hyper-
links: Structural re-ranking using links induced by language
models. In Proceedings of SIGIR.
Bjornar Larsen and Chinatsu Aone. 1999. Fast and effective
text mining using linear-time document clustering. In KDD
’99: Proceedings of the fifth ACM SIGKDD international
conference on Knowledge discovery and data mining, pages
16–22, New York, NY, USA. ACM Press.
Victor Lavrenko, James Allan, Edward DeGuzman, Daniel
LaFlamme, Veera Pollard, and Stephen Thomas. 2002. Rel-
evance models for topic detection and tracking. In Proceed-
ings ofHLT, pages 104–110.
Xiaoyong Liu and W. Bruce Croft. 2004. Cluster-based re-
trieval using language models. In Proceedings of SIGIR,
pages 186–193.
Ana G. Maguitman, Filippo Menczer, Heather Roinestad, and
Alessandro Vespignani. 2005. Algorithmic detection of se-
mantic similarity. In WWW ’05: Proceedings of the 14th in-
ternational conference on World Wide Web, pages 107–116,
New York, NY, USA. ACM Press.
Andrew Kachites McCallum. 1996. Bow: A toolkit for sta-
tistical language modeling, text retrieval, classification and
clustering. http://www.cs.cmu.edu/ mccallum/bow.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing or-
der into texts. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings ofEMNLP 2004, pages 404–411, Barcelona, Spain,
July. Association for Computational Linguistics.
Rada Mihalcea. 2005. Unsupervised large-vocabulary word
sense disambiguation with graph-based algorithms for se-
quence data labeling. In Proceedings of Human Language
Technology Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 411–418, Van-
couver, British Columbia, Canada, October. Association for
Computational Linguistics.
James Munkres. 1957. Algorithms for the assignment and
transportation problems. Journal of the Society for Indus-
trial and Applied Mathematics, 5(1):32–38, March.
Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. 2001. On
spectral clustering: Analysis and an algorithm. In NIPS,
pages 849–856.
Jay M. Ponte and W. Bruce Croft. 1998. A language modeling
approach to information retrieval. In Proceedings of SIGIR,
pages 275–281.
G. Salton and M. J. McGill. 1983. Introduction to Modern
Information Retrieval. McGraw Hill.
E. Seneta. 1981. Non-negative matrices and markov chains.
Springer-Verlag, New York.
Noam Slonim and Naftali Tishby. 2000. Document clustering
using word clusters via the information bottleneck method.
In SIGIR, pages 208–215.
Chade-Meng Tan, Yuan-Fang Wang, and Chan-Do Lee. 2002.
The use of bigrams to enhance text categorization. Inf. Pro-
cess. Manage, 38(4):529–546.
Kristina Toutanova, Christopher D. Manning, and Andrew Y.
Ng. 2004. Learning random walk models for inducing word
dependency distributions. In ICML ’04: Proceedings of the
twenty-first international conference on Machine learning,
page 103, New York, NY, USA. ACM Press.
Cornelis J. van Rijsbergen. 1979. Information Retrieval. But-
terworths.
Duncan J. Watts and Steven H. Strogatz. 1998. Collective dy-
namics of small-world networks. Nature, 393(6684):440–
442, June 4.
Hongyuan Zha, Xiaofeng He, Chris H. Q. Ding, Ming Gu, and
Horst D. Simon. 2001. Bipartite graph partitioning and data
clustering. In Proceedings of CIKM, pages 25–32.
Hongyuan Zha. 2002. Generic Summarization and Key Phrase
Extraction Using Mutual Reinforcement Principle and Sen-
tence Clustering. Tampere, Finland.
Chengxiang Zhai and John Lafferty. 2004. A study of smooth-
ing methods for language models applied to information re-
trieval. ACM Trans. Inf. Syst. (TOIS), 22(2):179–214.
Ying Zhao and George Karypis. 2002. Evaluation of hierarchi-
cal clustering algorithms for document datasets. In CIKM
’02: Proceedings of the eleventh international conference
on Information and knowledge management, pages 515–524,
New York, NY, USA. ACM Press.
</reference>
<page confidence="0.999041">
486
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.469648">
<title confidence="0.940885">Language Model-Based Document Clustering Using Random Walks</title>
<affiliation confidence="0.991595">Department of University of</affiliation>
<address confidence="0.995834">Ann Arbor, MI 48109-2121</address>
<email confidence="0.999827">gerkan@umich.edu</email>
<intro confidence="0.509899">Abstract</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kevin W Boyack</author>
<author>Richard Klavans</author>
<author>Katy B¨orner</author>
</authors>
<title>Mapping the backbone of science.</title>
<date>2005</date>
<journal>Scientometrics,</journal>
<volume>64</volume>
<issue>3</issue>
<marker>Boyack, Klavans, B¨orner, 2005</marker>
<rawString>Kevin W. Boyack, Richard Klavans, and Katy B¨orner. 2005. Mapping the backbone of science. Scientometrics, 64(3):351–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a largescale hypertextual web search engine.</title>
<date>1998</date>
<booktitle>In Proceedings of the 7th International World Wide Web Conference,</booktitle>
<pages>107--117</pages>
<contexts>
<context position="12287" citStr="Brin and Page, 1998" startWordPosition="1937" endWordPosition="1940">ends at. An interesting property of random walks is that for a given node,does not depend on. In other words, the probability of a random walk ending up at“in the long run” does not depend on its starting point (Seneta, 1981). This limiting probability distribution of an infinite random walk over the nodes is called the stationary distribution of the graph. The stationary distribution is uninteresting to us for clustering purposes since it gives an information related to the global structure of the graph. It is often used as a measure to rank the structural importance of the nodes in a graph (Brin and Page, 1998). For clustering, we are more interested in the local similarities inside a “cluster” of nodes that separate them from the rest of the graph. Furthermore, the generation probabilities lose their significance during long random walks since they get multiplied at each step. Therefore, we computefor small values of . Finally, we define the following: Definition 3 The-step generation probability of documentfrom the language model of: gen ✎gengengenis the-step generation vector ofdocument. We will often writeomitting the document name when we are not talking about the vector of a specific document.</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The anatomy of a largescale hypertextual web search engine. In Proceedings of the 7th International World Wide Web Conference, pages 107– 117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Fernanda Caropreso</author>
<author>Stan Matwin</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>A learner-independent evaluation of the usefulness of statistical phrases for automated text categorization.</title>
<date>2001</date>
<booktitle>Text Databases and DocumentManagement: Theory and Practice,</booktitle>
<pages>78--102</pages>
<editor>In Amita G. Chin, editor,</editor>
<publisher>Idea Group Publishing,</publisher>
<location>Hershey, US.</location>
<contexts>
<context position="831" citStr="Caropreso et al., 2001" startWordPosition="118" endWordPosition="121">of the oldest and most studied problems of information retrieval (van Rijsbergen, 1979). Almost all document clustering approaches to date have represented documents as vectors in a bag-of-words vector space model, where each dimension of a document vector corresponds to a term in the corpus (Salton and McGill, 1983). General clustering algorithms are then applied to these vectors to cluster the given corpus. There have been attempts to use bigrams or even higher-order ngrams to represent documents in text categorization, the supervised counterpart of document clustering, with little success (Caropreso et al., 2001; Tan et al., 2002). Clustering can be viewed as partitioning a set of data objects into groups such that the similarities between the objects in a same group is high while inter-group similarities are weaker. The fundamental assumption in this work is that the documents that are likely to have been generated from similar language models are likely to be in the same cluster. Under this assumption, we propose a new representation for document vectors specifically designed for clustering purposes. Given a corpus, we are interested in the generation probabilities of a document based on the langua</context>
</contexts>
<marker>Caropreso, Matwin, Sebastiani, 2001</marker>
<rawString>Maria Fernanda Caropreso, Stan Matwin, and Fabrizio Sebastiani. 2001. A learner-independent evaluation of the usefulness of statistical phrases for automated text categorization. In Amita G. Chin, editor, Text Databases and DocumentManagement: Theory and Practice, pages 78–102. Idea Group Publishing, Hershey, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjit S Dhillon</author>
<author>Subramanyam Mallela</author>
<author>Dharmendra S Modha</author>
</authors>
<title>Information-theoretic co-clustering.</title>
<date>2003</date>
<booktitle>Proceedings of the ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-03),</booktitle>
<pages>89--98</pages>
<editor>In Pedro Domingos, Christos Faloutsos, Ted SEnator, Hillol Kargupta, and Lise Getoor, editors,</editor>
<publisher>ACM Press.</publisher>
<location>New York,</location>
<marker>Dhillon, Mallela, Modha, 2003</marker>
<rawString>Inderjit S. Dhillon, Subramanyam Mallela, and Dharmendra S. Modha. 2003. Information-theoretic co-clustering. In Pedro Domingos, Christos Faloutsos, Ted SEnator, Hillol Kargupta, and Lise Getoor, editors, Proceedings of the ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-03), pages 89–98, New York, August 24–27. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjit S Dhillon</author>
</authors>
<title>Co-clustering documents and words using bipartite spectral graph partitioning.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventh ACM SIGKDD Conference,</booktitle>
<pages>269--274</pages>
<contexts>
<context position="15737" citStr="Dhillon, 2001" startWordPosition="2497" endWordPosition="2498">ensive amount of research on document clustering or clustering algorithms in general that we can not possibly review here. After all, we do not present a new clustering algorithm, but rather a new representation of textual data. We explain some popular clustering algorithms and evaluate our representation using them in Section 4. Few methods have been proposed to cluster documents using a representation other than the traditional vector space (or similar term-based vectors). Using a bipartite graph of terms and documents and then clustering this graph based on spectral methods is one of them (Dhillon, 2001; Zha et al., 2001). There are also general spectral methods that start withvectors, then map them to a new space with fewer dimensions before initiating the clustering algorithm (Ng et al., 2001). The information-theoretic clustering algorithms are relevant to our framework in the sense that they involve probability distributions over words just like the language models. However, instead of looking at the word distributions at the individual document level, they make use of the joint distribution of words and documents. For example, given the set of documentsand the set of words in the docume</context>
</contexts>
<marker>Dhillon, 2001</marker>
<rawString>Inderjit S. Dhillon. 2001. Co-clustering documents and words using bipartite spectral graph partitioning. In Proceedings of the Seventh ACM SIGKDD Conference, pages 269–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graphbased lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>22--457</pages>
<contexts>
<context position="13798" citStr="Erkan and Radev, 2004" startWordPosition="2177" endWordPosition="2180">ts of the vector is 1. The two are practically the same representations since we compute the cosine of the vectors during clustering. 3 Related Work Our work is inspired by three main areas of research. First, the success of language modeling approaches to information retrieval (Ponte and Croft, 1998) is encouraging for a similar twist to document representation for clustering purposes. Second, graph-based inference techniques to discover “hidden” textual relationships like the one we explored in our random walk model have been successfully applied to other NLP problems such as summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Zha, 2002), prepositional phrase attachment (Toutanova et al., 2004), and word sense disambiguation (Mihalcea, 2005). Unlike our approach, these methods try to exploit the global structure of a graph to rank the nodes of the graph. For example, Erkan and Radev (2004) find the stationary distribution of the random walk on a graph of sentences to rank the salience scores of the sentences for extractive summarization. Their link weight function is based on cosine similarity. Our graph construction based on generation probabilities is inherited from (Kurland and Lee, 20</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexrank: Graphbased lexical centrality as salience in text summarization. Journal ofArtificial Intelligence Research, 22:457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Harel</author>
<author>Yehuda Koren</author>
</authors>
<title>Clustering spatial data using random walks.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventh ACM SIGKDD Conference,</booktitle>
<pages>281--286</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="14789" citStr="Harel and Koren (2001)" startWordPosition="2338" endWordPosition="2341">es to rank the salience scores of the sentences for extractive summarization. Their link weight function is based on cosine similarity. Our graph construction based on generation probabilities is inherited from (Kurland and Lee, 2005), where authors used a similar generation graph to rerank the documents returned by a retrieval system based on the stationary distribution of the graph. Finally, previous research on clustering graphs with restricted random walks inspired us to cluster the generation graph using a similar approach. Our -step random walk approach is similar to the one proposed by Harel and Koren (2001). However, their algorithm is proposed for “spatial data” where the nodes of the graph are connected by undirected links that are determined by a (symmetric) similarity function. Our contribution in this paper is to use their approach on textual data by using generation links, and extend the method to directed graphs. There is an extensive amount of research on document clustering or clustering algorithms in general that we can not possibly review here. After all, we do not present a new clustering algorithm, but rather a new representation of textual data. We explain some popular clustering a</context>
</contexts>
<marker>Harel, Koren, 2001</marker>
<rawString>David Harel and Yehuda Koren. 2001. Clustering spatial data using random walks. In Proceedings of the Seventh ACM SIGKDD Conference, pages 281–286, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Kurland</author>
<author>Lillian Lee</author>
</authors>
<title>PageRank without hyperlinks: Structural re-ranking using links induced by language models.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="2470" citStr="Kurland and Lee, 2005" startWordPosition="382" endWordPosition="385">y related to the generation probability ofbased on the language model induced by document. The main steps of our method are as follows: For each ordered document pairin a given corpus, we compute the generation probability of from the language model induced bymaking use of language-model approaches in information retrieval (Ponte and Croft, 1998). We represent each document by a vector of its generation probabilities based on other documents’ language models. At this point, these vectors can be used in any clustering algorithm instead of the traditional term-based document vectors. Following (Kurland and Lee, 2005), our new document vectors are used to construct the underlying generation graph; the directed graph where documents are the nodes and link weights are proportional to the generation probabilities. We use restricted random walk probabilities to reinforce the generation probabilities and discover hidden relationships in the graph that are not obvious by the generation links. Our random walk model is similar to the one proposed by Harel and Kohen We propose a new document vector representation specifically designed for the document clustering task. Instead of the traditional termbased vectors, a</context>
<context position="6803" citStr="Kurland and Lee, 2005" startWordPosition="1060" endWordPosition="1063">d to have much smaller generation probabilities no matter how closely related they are to the generating language model. However, as we are interested in the generation probabilities between all pairs of documents, we want to be able to compare two different generation probabilities from a fixed language model regardless of the target document sizes. This is not a problem in the classical document retrieval setting since the given query is fixed, and generation probabilities for different queries are not compared against each other. To address these problems, following (Lavrenko et al., 2002; Kurland and Lee, 2005), we “flatten” the probabilities by normalizing them with respect to the document size: (2) flat whereis the number of terms in.flat provides us with meaningful values which are comparable among documents of different sizes. 2.2 Using Generation Probabilities as Document Representations Equation 2 suggests a representation of the relationship of a document with the other documents in a corpus. Given a corpus ofdocuments to cluster, we form an-dimensional generation vector for each documentwhere if (3) otherwise flat We can use these generation vectors in any clustering algorithm we prefer inst</context>
<context position="9758" citStr="Kurland and Lee (2005)" startWordPosition="1531" endWordPosition="1534">is intuition is similar to throwing out the stopwords from the documents before constructing thevectors to avoid coincidental similarities between documents. Second, having a dense vector with lots of non-zero elements will cause efficiency problems. Vector length is assumed to be a constant factor in analyzing the complexity of the clustering algorithms. However, our generation vectors are -dimensional, whereis the number of documents. In other words, vector size is not a constant factor anymore, which causes a problem of scalability to large data sets. To address these problems, we use what Kurland and Lee (2005) define as top generators: Given a document, we consider onlydocuments that yield the largest generation probabilities and discard others. The resultantdimensional vector, denoted, has at mostnon-zero elements, which are the largestelements of. For a given constant, with a sparse vector representation, certain operations (e.g. cosine) on such vectors can be done in constant time independent of. 2.3 Reinforcing Links with Random Walks Generation probabilities are only an approximation of semantic relatedness. Using the underlying directed graph interpretation of the generation probabilities, we</context>
<context position="14401" citStr="Kurland and Lee, 2005" startWordPosition="2273" endWordPosition="2276">kan and Radev, 2004; Mihalcea and Tarau, 2004; Zha, 2002), prepositional phrase attachment (Toutanova et al., 2004), and word sense disambiguation (Mihalcea, 2005). Unlike our approach, these methods try to exploit the global structure of a graph to rank the nodes of the graph. For example, Erkan and Radev (2004) find the stationary distribution of the random walk on a graph of sentences to rank the salience scores of the sentences for extractive summarization. Their link weight function is based on cosine similarity. Our graph construction based on generation probabilities is inherited from (Kurland and Lee, 2005), where authors used a similar generation graph to rerank the documents returned by a retrieval system based on the stationary distribution of the graph. Finally, previous research on clustering graphs with restricted random walks inspired us to cluster the generation graph using a similar approach. Our -step random walk approach is similar to the one proposed by Harel and Koren (2001). However, their algorithm is proposed for “spatial data” where the nodes of the graph are connected by undirected links that are determined by a (symmetric) similarity function. Our contribution in this paper is</context>
</contexts>
<marker>Kurland, Lee, 2005</marker>
<rawString>Oren Kurland and Lillian Lee. 2005. PageRank without hyperlinks: Structural re-ranking using links induced by language models. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bjornar Larsen</author>
<author>Chinatsu Aone</author>
</authors>
<title>Fast and effective text mining using linear-time document clustering.</title>
<date>1999</date>
<booktitle>In KDD ’99: Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>16--22</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="26843" citStr="Larsen and Aone, 1999" startWordPosition="4276" endWordPosition="4279">s, we used only the documents with exactly one label. We also eliminated classes with only one document since clustering such classes is trivial. We ended up with two collections summarized in Table 3. 4.3.3 Results The output of a hierarchical clustering algorithm is a tree where leaves are the documents and each node in the tree shows a cluster merging operation. Therefore each subtree represents a cluster. We assume that each class of documents in the corpus form a cluster subtree at some point during the construction of the tree. To evaluate the cluster tree, we use F-measure proposed in (Larsen and Aone, 1999). F-measure for a classin the corpus and a subtreeis defined as 5http://kdd.ics.uci.edu/databases/ reuters21578/reuters21578.html 6http://www.nist.gov/speech/tests/tdt/ tdt98/index.htm 484 Corpus k Accuracy () Normalized Mutual Information () classic3 3 4news-1 4 4news-2 4 4news-3 4 4news-4 4 4news-5 4 5news-1 5 5news-2 5 5news-3 5 5news-4 5 bbc 5 bbcsport 5 10news-1 10 10news-2 10 20news 20 Table 2: Performances of different vector representations using k-means (average of 30 runsconfidence interval). Algorithm TDT2 Reuters-21578 single-link 65.25 82.96 84.22 83.92 59.35 59.37 65.70 66.15 ave</context>
</contexts>
<marker>Larsen, Aone, 1999</marker>
<rawString>Bjornar Larsen and Chinatsu Aone. 1999. Fast and effective text mining using linear-time document clustering. In KDD ’99: Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 16–22, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>James Allan</author>
<author>Edward DeGuzman</author>
<author>Daniel LaFlamme</author>
<author>Veera Pollard</author>
<author>Stephen Thomas</author>
</authors>
<title>Relevance models for topic detection and tracking.</title>
<date>2002</date>
<booktitle>In Proceedings ofHLT,</booktitle>
<pages>104--110</pages>
<contexts>
<context position="6779" citStr="Lavrenko et al., 2002" startWordPosition="1056" endWordPosition="1059">y, longer documents tend to have much smaller generation probabilities no matter how closely related they are to the generating language model. However, as we are interested in the generation probabilities between all pairs of documents, we want to be able to compare two different generation probabilities from a fixed language model regardless of the target document sizes. This is not a problem in the classical document retrieval setting since the given query is fixed, and generation probabilities for different queries are not compared against each other. To address these problems, following (Lavrenko et al., 2002; Kurland and Lee, 2005), we “flatten” the probabilities by normalizing them with respect to the document size: (2) flat whereis the number of terms in.flat provides us with meaningful values which are comparable among documents of different sizes. 2.2 Using Generation Probabilities as Document Representations Equation 2 suggests a representation of the relationship of a document with the other documents in a corpus. Given a corpus ofdocuments to cluster, we form an-dimensional generation vector for each documentwhere if (3) otherwise flat We can use these generation vectors in any clustering </context>
</contexts>
<marker>Lavrenko, Allan, DeGuzman, LaFlamme, Pollard, Thomas, 2002</marker>
<rawString>Victor Lavrenko, James Allan, Edward DeGuzman, Daniel LaFlamme, Veera Pollard, and Stephen Thomas. 2002. Relevance models for topic detection and tracking. In Proceedings ofHLT, pages 104–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoyong Liu</author>
<author>W Bruce Croft</author>
</authors>
<title>Cluster-based retrieval using language models.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>186--193</pages>
<contexts>
<context position="5508" citStr="Liu and Croft, 2004" startWordPosition="857" endWordPosition="860"> estimation ofover an entire corpus which usuallyis a member of.is the general smoothing parameter that takes different forms in various smoothing methods. Smoothing has two important roles (Zhai and Lafferty, 2004). First, it accounts for terms unseen in the document preventing zero probabilities. This is similar to the smoothing effect in NLP problems such as parsing. Second, smoothing has anlike effect that accounts for the generation probabilities of the common terms in the corpus. A common smoothing technique is to use Bayesian smoothing with the Dirichlet prior (Zhai and Lafferty, 2004; Liu and Croft, 2004): Here,is the smoothing parameter. Higher values of mean more aggressive smoothing. Assuming the terms in a text are independent from each other, the generation probability of a text sequence given the documentis the product of the generation probabilities of the terms of: (1) In the context of information retrieval,is a query usually composed of few terms. In this work, we are interested in the generation probabilities of entire documents that usually have in the order of hundreds of unique terms. If we use Equation 1, we end up having unnatural probabilities which are irrepresentably small a</context>
</contexts>
<marker>Liu, Croft, 2004</marker>
<rawString>Xiaoyong Liu and W. Bruce Croft. 2004. Cluster-based retrieval using language models. In Proceedings of SIGIR, pages 186–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana G Maguitman</author>
<author>Filippo Menczer</author>
<author>Heather Roinestad</author>
<author>Alessandro Vespignani</author>
</authors>
<title>Algorithmic detection of semantic similarity.</title>
<date>2005</date>
<booktitle>In WWW ’05: Proceedings of the 14th international conference on World Wide Web,</booktitle>
<pages>107--116</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8356" citStr="Maguitman et al., 2005" startWordPosition="1303" endWordPosition="1306">e viewed as a model where documents cite each other. However, unlike real citations, the generation links are weighted and automatically induced from the content. The similarity function used in a clustering algorithm over the generation vectors becomes a measure of structural similarity of two nodes in the generation graph. Work on bibliometrics uses various similarity metrics to assess the relatedness of scientific papers by looking at the citation vectors (Boyack et al., 2005). Graph-based tf tf 480 similarity metrics are also used to detect semantic similarity of two documents on the Web (Maguitman et al., 2005). Cosine, also the standard metric used in based document clustering, is one of these metrics. Intuitively, the cosine of the citation vectors (i.e. vector of outgoing link weights) of two nodes is high when they link to similar sets of nodes with similar link weights. Hence, the cosine of two generation vectors is a measure of how likely two documents are generated from the same documents’ language models. The generation probability in Equation 2 with a smoothed language model is never zero. This creates two potential problems if we want to use the vector of Equation 3 directly in a clusterin</context>
</contexts>
<marker>Maguitman, Menczer, Roinestad, Vespignani, 2005</marker>
<rawString>Ana G. Maguitman, Filippo Menczer, Heather Roinestad, and Alessandro Vespignani. 2005. Algorithmic detection of semantic similarity. In WWW ’05: Proceedings of the 14th international conference on World Wide Web, pages 107–116, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering.</title>
<date>1996</date>
<note>http://www.cs.cmu.edu/ mccallum/bow.</note>
<contexts>
<context position="17327" citStr="McCallum, 1996" startWordPosition="2737" endWordPosition="2738">rmation-theoretic method where they cluster the words and the documents concurrently. 4 Evaluation We evaluated our new vector representation by comparing it against the traditionalvector space representation. We ran k-means, single-link, average-link, and complete-link clustering algorithms on various data sets using both representations. These algorithms are among the most popular ones that are used in document clustering. 4.1 General Experimental Setting Given a corpus, we stemmed all the documents, removed the stopwords and constructed thevector for each document by using the bow toolkit (McCallum, 1996). We computed theof each term using the following formula: whereis the total number of documents and df is the number of documents that the termappears in. We computed flattened generation probabilities (Equation 2) for all ordered pairs of documents in a corpus, and then constructed the corresponding generation graph (Definition 1). We used Dirichlet-smoothed language models with the smoothing parameter, which can be considered as a typical value used in information retrieval. While computing the generation link vectors, we did not perform extensive parameter tuning at any stage of our method</context>
</contexts>
<marker>McCallum, 1996</marker>
<rawString>Andrew Kachites McCallum. 1996. Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering. http://www.cs.cmu.edu/ mccallum/bow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings ofEMNLP 2004,</booktitle>
<pages>404--411</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="13824" citStr="Mihalcea and Tarau, 2004" startWordPosition="2181" endWordPosition="2185">The two are practically the same representations since we compute the cosine of the vectors during clustering. 3 Related Work Our work is inspired by three main areas of research. First, the success of language modeling approaches to information retrieval (Ponte and Croft, 1998) is encouraging for a similar twist to document representation for clustering purposes. Second, graph-based inference techniques to discover “hidden” textual relationships like the one we explored in our random walk model have been successfully applied to other NLP problems such as summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Zha, 2002), prepositional phrase attachment (Toutanova et al., 2004), and word sense disambiguation (Mihalcea, 2005). Unlike our approach, these methods try to exploit the global structure of a graph to rank the nodes of the graph. For example, Erkan and Radev (2004) find the stationary distribution of the random walk on a graph of sentences to rank the salience scores of the sentences for extractive summarization. Their link weight function is based on cosine similarity. Our graph construction based on generation probabilities is inherited from (Kurland and Lee, 2005), where authors used a </context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In Dekang Lin and Dekai Wu, editors, Proceedings ofEMNLP 2004, pages 404–411, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>411--418</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="13942" citStr="Mihalcea, 2005" startWordPosition="2199" endWordPosition="2200">ur work is inspired by three main areas of research. First, the success of language modeling approaches to information retrieval (Ponte and Croft, 1998) is encouraging for a similar twist to document representation for clustering purposes. Second, graph-based inference techniques to discover “hidden” textual relationships like the one we explored in our random walk model have been successfully applied to other NLP problems such as summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Zha, 2002), prepositional phrase attachment (Toutanova et al., 2004), and word sense disambiguation (Mihalcea, 2005). Unlike our approach, these methods try to exploit the global structure of a graph to rank the nodes of the graph. For example, Erkan and Radev (2004) find the stationary distribution of the random walk on a graph of sentences to rank the salience scores of the sentences for extractive summarization. Their link weight function is based on cosine similarity. Our graph construction based on generation probabilities is inherited from (Kurland and Lee, 2005), where authors used a similar generation graph to rerank the documents returned by a retrieval system based on the stationary distribution o</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>Rada Mihalcea. 2005. Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 411–418, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Munkres</author>
</authors>
<title>Algorithms for the assignment and transportation problems.</title>
<date>1957</date>
<journal>Journal of the Society for Industrial and Applied Mathematics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="21178" citStr="Munkres, 1957" startWordPosition="3367" endWordPosition="3368"> 386 511 20news 18846 20 628 999 Table 1: The corpora used in the k-means experiments. 4.2.3 Results We used two different metrics to evaluate the results of the k-means algorithm; accuracy and mutual information. Letbe the label assigned toby the clustering algorithm, andbe’s actual label in the corpus. Then, map Accuracy whereequals 1 ifand equals zero otherwise. mapis the function that maps the output label set of the k-means algorithm to the actual label set of the corpus. Given the confusion matrix of the output, best such mapping function can be efficiently found by Munkres’s algorithm (Munkres, 1957). Mutual information is a metric that does not require a mapping function. Let be the output label set of the k-means algorithm, and be the actual label set of the corpus with the underlying assignments of documents to these sets. Mutual information (MI) of these two labelings is defined as: MI log whereandare the probabilities that a document is labeled asandby the algorithm and in the actual corpus, respectively;is the probability that these two events occur at the same time. These values can be derived from the confusion matrix. We map the MI metric to theinterval by normalizing it with the</context>
</contexts>
<marker>Munkres, 1957</marker>
<rawString>James Munkres. 1957. Algorithms for the assignment and transportation problems. Journal of the Society for Industrial and Applied Mathematics, 5(1):32–38, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
<author>Yair Weiss</author>
</authors>
<title>On spectral clustering: Analysis and an algorithm.</title>
<date>2001</date>
<booktitle>In NIPS,</booktitle>
<pages>849--856</pages>
<contexts>
<context position="15933" citStr="Ng et al., 2001" startWordPosition="2528" endWordPosition="2531">new representation of textual data. We explain some popular clustering algorithms and evaluate our representation using them in Section 4. Few methods have been proposed to cluster documents using a representation other than the traditional vector space (or similar term-based vectors). Using a bipartite graph of terms and documents and then clustering this graph based on spectral methods is one of them (Dhillon, 2001; Zha et al., 2001). There are also general spectral methods that start withvectors, then map them to a new space with fewer dimensions before initiating the clustering algorithm (Ng et al., 2001). The information-theoretic clustering algorithms are relevant to our framework in the sense that they involve probability distributions over words just like the language models. However, instead of looking at the word distributions at the individual document level, they make use of the joint distribution of words and documents. For example, given the set of documentsand the set of words in the document collection, Slonim and Tishby (2000) first try to find a word clusteringsuch that the mutual informationis minimized (for good compression) while maximizing the(for preserving the original info</context>
</contexts>
<marker>Ng, Jordan, Weiss, 2001</marker>
<rawString>Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. 2001. On spectral clustering: Analysis and an algorithm. In NIPS, pages 849–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Ponte</author>
<author>W Bruce Croft</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>275--281</pages>
<contexts>
<context position="2196" citStr="Ponte and Croft, 1998" startWordPosition="338" endWordPosition="341">ument vector corresponds to a document in the corpus instead of a term in the classical representation. In other words, our document vectors are -dimensional, whereis the number of documents in the corpus to be clustered. For the vectorof document, theth element ofis closely related to the generation probability ofbased on the language model induced by document. The main steps of our method are as follows: For each ordered document pairin a given corpus, we compute the generation probability of from the language model induced bymaking use of language-model approaches in information retrieval (Ponte and Croft, 1998). We represent each document by a vector of its generation probabilities based on other documents’ language models. At this point, these vectors can be used in any clustering algorithm instead of the traditional term-based document vectors. Following (Kurland and Lee, 2005), our new document vectors are used to construct the underlying generation graph; the directed graph where documents are the nodes and link weights are proportional to the generation probabilities. We use restricted random walk probabilities to reinforce the generation probabilities and discover hidden relationships in the g</context>
<context position="4191" citStr="Ponte and Croft (1998)" startWordPosition="646" endWordPosition="649"> over the alternative vector representation. 479 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 479–486, New York, June 2006. c�2006 Association for Computational Linguistics (2001) for general spatial data represented as undirected graphs. We have extended their model to the directed graph case. We use new probabilities derived from random walks as the vector representation of the documents. 2 Generation Probabilities as Document Vectors 2.1 Language Models The language modeling approach to information retrieval was first introduced by Ponte and Croft (1998) as an alternative (or an improvement) to the traditional relevance models. In the language modeling framework, each document in the database defines a language model. The relevance of a document to a given query is ranked according to the generation probability of the query based on the underlying language model of the document. To induce a (unigram) language model from a document, we start with the maximum likelihood (ML) estimation of the term probabilities. For each termthat occurs in a document, the ML estimation ofwith respect to is defined as whereis the number of occurences of termin d</context>
<context position="13479" citStr="Ponte and Croft, 1998" startWordPosition="2128" endWordPosition="2131">vector of a specific document. genis a measure of how likely a random walk that starts atwill visitinor fewer steps. It helps us to discover “hidden” similarities between documents 481 that are not immediately obvious from 1-step generation links. Note that when,is nothing but normalized such that the sum of the elements of the vector is 1. The two are practically the same representations since we compute the cosine of the vectors during clustering. 3 Related Work Our work is inspired by three main areas of research. First, the success of language modeling approaches to information retrieval (Ponte and Croft, 1998) is encouraging for a similar twist to document representation for clustering purposes. Second, graph-based inference techniques to discover “hidden” textual relationships like the one we explored in our random walk model have been successfully applied to other NLP problems such as summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Zha, 2002), prepositional phrase attachment (Toutanova et al., 2004), and word sense disambiguation (Mihalcea, 2005). Unlike our approach, these methods try to exploit the global structure of a graph to rank the nodes of the graph. For example, Erkan an</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>Jay M. Ponte and W. Bruce Croft. 1998. A language modeling approach to information retrieval. In Proceedings of SIGIR, pages 275–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw Hill.</publisher>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M. J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Seneta</author>
</authors>
<title>Non-negative matrices and markov chains.</title>
<date>1981</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="11892" citStr="Seneta, 1981" startWordPosition="1872" endWordPosition="1873">no link fromto. The intuition says thatmust be semantically related toto a certain degree although there is no generation link between them depending on’s language model. We approximate this relation by considering the probabilities of 2-step (or longer) random walks from toalthough there is no 1-step random walk fromto . Letdenote the probability that an-step random walk starts atand ends at. An interesting property of random walks is that for a given node,does not depend on. In other words, the probability of a random walk ending up at“in the long run” does not depend on its starting point (Seneta, 1981). This limiting probability distribution of an infinite random walk over the nodes is called the stationary distribution of the graph. The stationary distribution is uninteresting to us for clustering purposes since it gives an information related to the global structure of the graph. It is often used as a measure to rank the structural importance of the nodes in a graph (Brin and Page, 1998). For clustering, we are more interested in the local similarities inside a “cluster” of nodes that separate them from the rest of the graph. Furthermore, the generation probabilities lose their significan</context>
</contexts>
<marker>Seneta, 1981</marker>
<rawString>E. Seneta. 1981. Non-negative matrices and markov chains. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Slonim</author>
<author>Naftali Tishby</author>
</authors>
<title>Document clustering using word clusters via the information bottleneck method.</title>
<date>2000</date>
<booktitle>In SIGIR,</booktitle>
<pages>208--215</pages>
<contexts>
<context position="16376" citStr="Slonim and Tishby (2000)" startWordPosition="2597" endWordPosition="2600">001). There are also general spectral methods that start withvectors, then map them to a new space with fewer dimensions before initiating the clustering algorithm (Ng et al., 2001). The information-theoretic clustering algorithms are relevant to our framework in the sense that they involve probability distributions over words just like the language models. However, instead of looking at the word distributions at the individual document level, they make use of the joint distribution of words and documents. For example, given the set of documentsand the set of words in the document collection, Slonim and Tishby (2000) first try to find a word clusteringsuch that the mutual informationis minimized (for good compression) while maximizing the(for preserving the original information). Then the same procedure is used for clustering documents using the word clusters from the first step. Dhillon et. al. (2003) propose a co-clustering version of this information-theoretic method where they cluster the words and the documents concurrently. 4 Evaluation We evaluated our new vector representation by comparing it against the traditionalvector space representation. We ran k-means, single-link, average-link, and complet</context>
</contexts>
<marker>Slonim, Tishby, 2000</marker>
<rawString>Noam Slonim and Naftali Tishby. 2000. Document clustering using word clusters via the information bottleneck method. In SIGIR, pages 208–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chade-Meng Tan</author>
<author>Yuan-Fang Wang</author>
<author>Chan-Do Lee</author>
</authors>
<title>The use of bigrams to enhance text categorization.</title>
<date>2002</date>
<journal>Inf. Process. Manage,</journal>
<volume>38</volume>
<issue>4</issue>
<contexts>
<context position="850" citStr="Tan et al., 2002" startWordPosition="122" endWordPosition="125">tudied problems of information retrieval (van Rijsbergen, 1979). Almost all document clustering approaches to date have represented documents as vectors in a bag-of-words vector space model, where each dimension of a document vector corresponds to a term in the corpus (Salton and McGill, 1983). General clustering algorithms are then applied to these vectors to cluster the given corpus. There have been attempts to use bigrams or even higher-order ngrams to represent documents in text categorization, the supervised counterpart of document clustering, with little success (Caropreso et al., 2001; Tan et al., 2002). Clustering can be viewed as partitioning a set of data objects into groups such that the similarities between the objects in a same group is high while inter-group similarities are weaker. The fundamental assumption in this work is that the documents that are likely to have been generated from similar language models are likely to be in the same cluster. Under this assumption, we propose a new representation for document vectors specifically designed for clustering purposes. Given a corpus, we are interested in the generation probabilities of a document based on the language models induced b</context>
</contexts>
<marker>Tan, Wang, Lee, 2002</marker>
<rawString>Chade-Meng Tan, Yuan-Fang Wang, and Chan-Do Lee. 2002. The use of bigrams to enhance text categorization. Inf. Process. Manage, 38(4):529–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning random walk models for inducing word dependency distributions.</title>
<date>2004</date>
<booktitle>In ICML ’04: Proceedings of the twenty-first international conference on Machine learning,</booktitle>
<pages>103</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="13894" citStr="Toutanova et al., 2004" startWordPosition="2191" endWordPosition="2194">osine of the vectors during clustering. 3 Related Work Our work is inspired by three main areas of research. First, the success of language modeling approaches to information retrieval (Ponte and Croft, 1998) is encouraging for a similar twist to document representation for clustering purposes. Second, graph-based inference techniques to discover “hidden” textual relationships like the one we explored in our random walk model have been successfully applied to other NLP problems such as summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Zha, 2002), prepositional phrase attachment (Toutanova et al., 2004), and word sense disambiguation (Mihalcea, 2005). Unlike our approach, these methods try to exploit the global structure of a graph to rank the nodes of the graph. For example, Erkan and Radev (2004) find the stationary distribution of the random walk on a graph of sentences to rank the salience scores of the sentences for extractive summarization. Their link weight function is based on cosine similarity. Our graph construction based on generation probabilities is inherited from (Kurland and Lee, 2005), where authors used a similar generation graph to rerank the documents returned by a retriev</context>
</contexts>
<marker>Toutanova, Manning, Ng, 2004</marker>
<rawString>Kristina Toutanova, Christopher D. Manning, and Andrew Y. Ng. 2004. Learning random walk models for inducing word dependency distributions. In ICML ’04: Proceedings of the twenty-first international conference on Machine learning, page 103, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cornelis J van Rijsbergen</author>
</authors>
<date>1979</date>
<journal>Information Retrieval. Butterworths.</journal>
<marker>van Rijsbergen, 1979</marker>
<rawString>Cornelis J. van Rijsbergen. 1979. Information Retrieval. Butterworths.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duncan J Watts</author>
<author>Steven H Strogatz</author>
</authors>
<title>Collective dynamics of small-world networks.</title>
<date>1998</date>
<journal>Nature,</journal>
<volume>393</volume>
<issue>6684</issue>
<pages>442</pages>
<contexts>
<context position="18819" citStr="Watts and Strogatz, 1998" startWordPosition="2976" endWordPosition="2979">otonically idf df 482 better as we increaseduntil around, where the performance stabilized and dropped after around . We conclude that using bounded number of outgoing links per document is not only more efficient but also necessary as we motivated in Section 2.2. We got the best results when the random walk parameter. When, the random walk goes “out of the cluster” andvectors become very dense. In other words, almost all of the graph is reachable from a given node with 4-step or longer random walks (assumingis around 80), which is an indication of a “small world” effect in generation graphs (Watts and Strogatz, 1998). Under these observations, we will only report results using vectors,andwithregardless of the data set and the clustering algorithm. 4.2 Experiments with k-means 4.2.1 Algorithm k-means is a clustering algorithm popular for its simplicity and efficiency. It requires, the number of clusters, as input, and partitions the data set into exactly clusters. We used a version of k-means that uses cosine similarity to compute the distance between the vectors. The algorithm can be summarized as follows: 1. randomly selectdocument vectors as the initial cluster centroids; 2. assign each document to the </context>
</contexts>
<marker>Watts, Strogatz, 1998</marker>
<rawString>Duncan J. Watts and Steven H. Strogatz. 1998. Collective dynamics of small-world networks. Nature, 393(6684):440– 442, June 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyuan Zha</author>
<author>Xiaofeng He</author>
<author>Chris H Q Ding</author>
<author>Ming Gu</author>
<author>Horst D Simon</author>
</authors>
<title>Bipartite graph partitioning and data clustering.</title>
<date>2001</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="15756" citStr="Zha et al., 2001" startWordPosition="2499" endWordPosition="2502">f research on document clustering or clustering algorithms in general that we can not possibly review here. After all, we do not present a new clustering algorithm, but rather a new representation of textual data. We explain some popular clustering algorithms and evaluate our representation using them in Section 4. Few methods have been proposed to cluster documents using a representation other than the traditional vector space (or similar term-based vectors). Using a bipartite graph of terms and documents and then clustering this graph based on spectral methods is one of them (Dhillon, 2001; Zha et al., 2001). There are also general spectral methods that start withvectors, then map them to a new space with fewer dimensions before initiating the clustering algorithm (Ng et al., 2001). The information-theoretic clustering algorithms are relevant to our framework in the sense that they involve probability distributions over words just like the language models. However, instead of looking at the word distributions at the individual document level, they make use of the joint distribution of words and documents. For example, given the set of documentsand the set of words in the document collection, Slon</context>
</contexts>
<marker>Zha, He, Ding, Gu, Simon, 2001</marker>
<rawString>Hongyuan Zha, Xiaofeng He, Chris H. Q. Ding, Ming Gu, and Horst D. Simon. 2001. Bipartite graph partitioning and data clustering. In Proceedings of CIKM, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyuan Zha</author>
</authors>
<title>Generic Summarization and Key Phrase Extraction Using Mutual Reinforcement Principle and Sentence Clustering.</title>
<date>2002</date>
<location>Tampere, Finland.</location>
<contexts>
<context position="13836" citStr="Zha, 2002" startWordPosition="2186" endWordPosition="2187">e same representations since we compute the cosine of the vectors during clustering. 3 Related Work Our work is inspired by three main areas of research. First, the success of language modeling approaches to information retrieval (Ponte and Croft, 1998) is encouraging for a similar twist to document representation for clustering purposes. Second, graph-based inference techniques to discover “hidden” textual relationships like the one we explored in our random walk model have been successfully applied to other NLP problems such as summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Zha, 2002), prepositional phrase attachment (Toutanova et al., 2004), and word sense disambiguation (Mihalcea, 2005). Unlike our approach, these methods try to exploit the global structure of a graph to rank the nodes of the graph. For example, Erkan and Radev (2004) find the stationary distribution of the random walk on a graph of sentences to rank the salience scores of the sentences for extractive summarization. Their link weight function is based on cosine similarity. Our graph construction based on generation probabilities is inherited from (Kurland and Lee, 2005), where authors used a similar gene</context>
</contexts>
<marker>Zha, 2002</marker>
<rawString>Hongyuan Zha. 2002. Generic Summarization and Key Phrase Extraction Using Mutual Reinforcement Principle and Sentence Clustering. Tampere, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
<author>John Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to information retrieval.</title>
<date>2004</date>
<journal>ACM Trans. Inf. Syst. (TOIS),</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="5103" citStr="Zhai and Lafferty, 2004" startWordPosition="791" endWordPosition="794"> underlying language model of the document. To induce a (unigram) language model from a document, we start with the maximum likelihood (ML) estimation of the term probabilities. For each termthat occurs in a document, the ML estimation ofwith respect to is defined as whereis the number of occurences of termin document. This estimation is often smoothed based on the following general formula: whereis the ML estimation ofover an entire corpus which usuallyis a member of.is the general smoothing parameter that takes different forms in various smoothing methods. Smoothing has two important roles (Zhai and Lafferty, 2004). First, it accounts for terms unseen in the document preventing zero probabilities. This is similar to the smoothing effect in NLP problems such as parsing. Second, smoothing has anlike effect that accounts for the generation probabilities of the common terms in the corpus. A common smoothing technique is to use Bayesian smoothing with the Dirichlet prior (Zhai and Lafferty, 2004; Liu and Croft, 2004): Here,is the smoothing parameter. Higher values of mean more aggressive smoothing. Assuming the terms in a text are independent from each other, the generation probability of a text sequence giv</context>
</contexts>
<marker>Zhai, Lafferty, 2004</marker>
<rawString>Chengxiang Zhai and John Lafferty. 2004. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst. (TOIS), 22(2):179–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhao</author>
<author>George Karypis</author>
</authors>
<title>Evaluation of hierarchical clustering algorithms for document datasets.</title>
<date>2002</date>
<booktitle>In CIKM ’02: Proceedings of the eleventh international conference on Information and knowledge management,</booktitle>
<pages>515--524</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="28352" citStr="Zhao and Karypis, 2002" startWordPosition="4497" endWordPosition="4500">ing the class. Letbe the set of subtrees in the output cluster tree, andbe the set of classes. F-measure of the entire tree is the weighted average of the maximum F-measures of all the classes: whereis the number of documents that belong to class . We ran all three algorithms for both corpora. Unlike kmeans, hierarchical algorithms we used are deterministic. Table 4 summarizes our results. An immediate observation is that average-link clustering performs much better than other two algorithms independent of the data set or the document representation, which is consistent with earlier research (Zhao and Karypis, 2002). The highest result (shown boldface) for each algorithm and corpus was achieved by using generation vectors. However, unlike in the k-means experiments,was able to outperformandin one or two cases. yielded the best result instead ofin one of the six cases. 5 Conclusion We have presented a language model inspired approach to document clustering. Our results show that even the simplest version of our approach with nearly no parameter tuning can outperform traditionalmodels by a wide margin. Random walk iterations on our graph-based model have improved our results even more. Based on the success</context>
</contexts>
<marker>Zhao, Karypis, 2002</marker>
<rawString>Ying Zhao and George Karypis. 2002. Evaluation of hierarchical clustering algorithms for document datasets. In CIKM ’02: Proceedings of the eleventh international conference on Information and knowledge management, pages 515–524, New York, NY, USA. ACM Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>