<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.123600">
<title confidence="0.988837">
Copenhagen-Malm¨o: Tree Approximations of Semantic Parsing Problems
</title>
<author confidence="0.990795">
Natalie Schluter†, Jakob Elming, Sigrid Klerke, H´ector Martinez Alonso, Dirk Hovy
Barbara Plank, Anders Johannsen, and Anders Søgaard
</author>
<affiliation confidence="0.99992">
†Dpt. of Computer Science Center for Language Technology
Malm¨o University University of Copenhagen
</affiliation>
<email confidence="0.942779333333333">
natalie.schluter@mah.se {zmk867,skl,alonso}@hum.ku.dk
{dirk,bplank}@cst.dk,
{ajohannsen,soegaard}@hum.ku.dk
</email>
<sectionHeader confidence="0.99718" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999705">
In this shared task paper for SemEval-
2014 Task 8, we show that most se-
mantic structures can be approximated by
trees through a series of almost bijective
graph transformations. We transform in-
put graphs, apply off-the-shelf methods
from syntactic parsing on the resulting
trees, and retrieve output graphs. Us-
ing tree approximations, we obtain good
results across three semantic formalisms,
with a 15.9% error reduction over a state-
of-the-art semantic role labeling system on
development data. Our system came in 3/6
in the shared task closed track.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999848722222222">
Semantic analyses often go beyond tree-
structured representations, assigning multiple se-
mantic heads to nodes, some semantic formalisms
even tolerating directed cycles.1 At the same
time, syntactic parsing is a mature field with effi-
cient, highly optimised decoding and learning al-
gorithms for tree-structured representations. We
present tree approximation algorithms that in com-
bination with a state-of-the-art syntactic parser
achieve competitive performance in semantic di-
graph parsing.
We investigate two kinds of tree approximation
algorithms that we will refer to as pruning algo-
rithms and packing algorithms. Our pruning al-
gorithms simply remove and reverse edges until
the graph is a tree; edge reversals are then undone
as a postprocessing step. Our packing algorithms,
on the other hand, carry out two bijective graph
</bodyText>
<footnote confidence="0.936003166666667">
This work is licenced under a Creative Commons Attribu-
tion 4.0 International License. Page numbers and proceed-
ings footer are added by the organizers. License details:
http://creativecommons.org/licenses/by/4.0/
1For example, HPSG predicate-argument structures (Pol-
lard and Sag, 1994).
</footnote>
<bodyText confidence="0.999491578947368">
transformations to pack structural information into
new edge labels, making it possible to reconstruct
most of the structural complexity as a postprocess-
ing step. Specifically, we present a packing al-
gorithm that consists of two fully bijective graph
transformations, in addition to a further transfor-
mation that incurs only a small information loss.
We carry out experiments across three seman-
tic annotations of the Wall Street Journal section
of the Penn Treebank (Marcus et al., 1993), cor-
responding to simplified versions of the semantic
formalisms minimal recursion semantics (MRS)
(Copestake et al., 2005), Enju-style predicate-
argument structures (Miyao and Tsujii, 2003), and
Prague-style tectogrammar semantics (B¨ohmov´a
et al., 2003). We show that pruning and pack-
ing algorithms lead to state-of-the-art performance
across these semantic formalisms using an off-the-
shelf syntactic dependency parser.
</bodyText>
<sectionHeader confidence="0.999868" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999482058823529">
Sagae and Tsujii (2008) present a pruning algo-
rithm in their paper on transition-based parsing of
directed acyclic graphs (DAGs), which discards
the edges of longest span entering nodes. They
apply the dependency parser described in Sagae
and Tsujii (2007) to the tree representations. We
note that this algorithm is not sufficient to produce
trees in our case, where the input graphs are not
necessarily acyclic. It does correspond roughly to
our LONGEST-EDGE baseline, which removes the
longest edge in cycles, in addition to flow reversal.
Sagae and Tsujii (2008) also present a shift-
reduce automaton approach to parsing DAGs. In
their paper, they report a labeled F1-score of
88.7% on the PAS dataset (see Section 3), while
we obtain 89.1%, however the results are thus not
directly comparable due to different data splits.2
</bodyText>
<footnote confidence="0.988920666666667">
2We obtained code to run this as a baseline, but were un-
able to, due to memory leaks, caused by subsets of our data,
and on the subsets of data that actually parsed, recall was very
</footnote>
<page confidence="0.971671">
213
</page>
<note confidence="0.7327685">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 213–217,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999959444444444">
The shared task organizers of the Broad-
coverage Semantic Dependency Parsing task at
SemEval-20143 also presented a pruning-based
baseline system. They eliminate re-entrancies in
the graph by removing dependencies to nodes with
multiple incoming edges. Of these edges, they
again keep the shortest. They incorporate all sin-
gleton nodes by attaching nodes to the immedi-
ately following node or to a virtual root - in case
the singleton is sentence-final. Finally, they inte-
grate fragments by subordinating remaining nodes
with in-degree 0 to the root node. They apply the
parser described in Bohnet (2010), also used be-
low, to the resulting trees. This system obtained
a labeled F1-score of 54.7% on the PAS dataset.
The performance of their pruning algorithm was
also considerably lower than our algorithms on the
other datasets considered below.
</bodyText>
<sectionHeader confidence="0.991022" genericHeader="method">
3 Tree approximations
</sectionHeader>
<bodyText confidence="0.984371909090909">
This section describes two approaches to approxi-
mating graphs by trees, namely pruning and pack-
ing. Pruning optimizes the number of “good”
edges in trees (Section 3.1), while packing trans-
forms graphs into trees by means of a pipeline of
operations which are 99.6% reversible (see Fig-
ure 1); that is, almost no information from the
original graphs is lost in the trees (Section 3.2).
Under both approaches, we first introduce arti-
ficial root nodes to the graphs and append them
to the word list. Graphs may initially be discon-
nected. We connect all weakly connected com-
ponents as follows. We first identify a most im-
portant node in each weakly connected compo-
nent, which we will refer to as the root. This root
is taken to be the first node with the “top” fea-
ture from the data, if one exists. If none exists,
then the node with highest degree is chosen as the
“root”. (Note that the “root” of each non-singleton
connected component is marked as a “top” node
in the inverse transformation.) The root of each
non-singleton weakly connected component is at-
tached as a dependent of the artificial root node
with a special new label for the corresponding
edge. Also, each disconnected node is attached
as a dependent of the node to the right of it, with
a distinct special new label. It is these connected
graphs that we take to be the input in the following
low, suggesting that maybe the decoding algorithm was tuned
to a specific planarization of the complex graphs.
3http://alt.qcri.org/semeval2014/task8/
two subsections describing our graph pruning and
packing algorithms.
</bodyText>
<subsectionHeader confidence="0.999579">
3.1 Graph pruning
</subsectionHeader>
<bodyText confidence="0.99995216">
Our PRUNING algorithm removes a small number
of edges in the semantic graphs to be able to repre-
sent them as trees. The average edge counts from
the training data (see Section 4.1) indicate that the
potential edge loss in pruning is relatively small
(5.7% in the worst case). In this approach, two
transformations on the connected semantic graphs
are carried out: pruning and flow reversal.
Pruning. The input digraph may contain under-
lying undirected cycles. We break these cycles
by iteratively removing the longest edge from the
node with the fewest predecessors (lowest depth)
in the digraph. The resulting underlying undi-
rected graph is a tree.
Depth-first flow reversal. We then carry out
depth-first traversal of the resulting underlying
undirected tree, reversing the direction of edges
from the leaves upwards, as needed, until reach-
ing the root. Any reversed edge’s label is given a
special prefix, so that this reversal can be undone
in a post-processing step.
Following the above two transformations, we train
our parsers on the transformed semantic annota-
tions and output graphs such as the one in Fig-
ure 1a.
</bodyText>
<subsectionHeader confidence="0.999432">
3.2 Graph packing
</subsectionHeader>
<bodyText confidence="0.9984844">
Our PACKING algorithm consists of a pipeline of
four graph transformations. The two major trans-
formations are for coordination and generalised
long-distance dependencies, being both parallel
path inducing constructions. The transformations
are both linguistically and topologically inspired
by the f-structure annotated c-structures in Lex-
ical Functional Grammar and f-structure parsing
via off-the-shelf dependency parsers (Schluter and
Van Genabith, 2009). We further ensure the defin-
ing tree property that every node is connected by a
unique path from the root, by carrying out flow re-
versal when necessary. Finally remaining parallel
paths are broken according to an heuristic on path
locality.
Coordination. In some semantic representa-
tions of coordination, individual conjunct nodes
may all dominate a same argument, or be domi-
nated by a same head. In both these cases, paral-
lel paths are generated. The same structures may
</bodyText>
<page confidence="0.997699">
214
</page>
<figureCaption confidence="0.990759">
Figure 1: Example of pruned (top), packed (middle), and original (bottom) semantic graph. (Sentence
22002004 from the PAS dataset.)
</figureCaption>
<bodyText confidence="0.997666981481481">
be represented if the head or arguments are “fac-
tored out”. To do this, we remove all edges from
conjuncts towards a same argument (resp. from
a shared head to each conjunct), and introduce a
new edge from the root of the coordination sub-
tree towards this argument (resp. from a shared
head to the root of the coordination subtree). The
new edges receive a special prefix to facilitate ap-
plying the inverse transformation.
Breadth-first flow reversal. Unlike our pruning
algorithm, there is not yet any clear distinct path
from the root to the all nodes (as there are not
leaves yet). After carrying out the coordination
transformation, we carry out a breadth-first search
on the graph to direct flow away from the root, and
again, reversed edges’ labels are given a special
prefix. As we do this, we test resulting nodes to
see if there are any parallel paths leading to them.
If so, these paths may be transformed immediately
according to the following transformation.
Generalized long-distance dependencies.
Long-distance dependencies are represented
in f-structure annotated c-structures by path
equations. This gives a tree representation of
parallel paths, at least one of which is exactly
one edge long. Given two parallel paths p1 and
p2 in the graph, where p1 = (v1, l, vn) and p2 =
(v1, l1, v2), (v2,l2, vg), ... , (vn−1, ln−1, vn), we
remove the last edge of p2 and augment p1’s label
with the representation l1 : l2 : � � � : ln−1 of p2. p1
becomes (v1, l and l1 : l2 : : ln−1, vn), indi-
cating that vn is also the child (with dependency
label ln−1) of the node found by travelling (from
v1) down an l1 labelled edge, followed by an l2
labelled edge, and so on until the child of the ln−2
labelled edge is found.
Maximum average locality heuristic. Follow-
ing these transformations, there may still be paral-
lel paths in the graph: those not parallel to a single
edge. We remove “worst” re-entrant edges using
the simple heuristic that the path with the lowest
average edge span should be conserved entirely.
These removed edges clearly cannot be recovered
after transformation.
Our parsers are trained on the output graphs of
these four transformations such as the one in Fig-
ure 1b. We observe the main difference between
PRUNING and PACKING: coordination and long-
distance dependencies. For example, PACKING
keeps the edge between the conjunction and the
first conjunct, which is pruned away in PRUNING.
Such a difference provides a partial explanation
for the lower recall of PRUNING vis-`a-vis PACK-
ING (see Section 4.5).
</bodyText>
<sectionHeader confidence="0.99995" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99699">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.99999525">
The three datasets are semantic annotations of the
WSJ section of the Penn Treebank of English. The
average sentence length, which is also the aver-
age number of dependency edges in the tree ap-
proximations that we use to induce our semantic
parsers, is 22.93. The three semantic formalisms
are slightly richer, and the average number of
edges in the PAS-annotated treebank is 24.32. For
DM, the average number of edges is 23.77, and
for DM it is 23.33. While the pruning-based ap-
proaches thus suffers from a modest information
loss, throwing out 5.7% of the edges in the worst
</bodyText>
<page confidence="0.997674">
215
</page>
<bodyText confidence="0.999695">
case, this is not the case for packing. The re-
versibility of the packed representations is given
by the score upper bound in the last row in Ta-
ble 1. We use the dataset splits of the SemEval
2014 shared task.
</bodyText>
<subsectionHeader confidence="0.997531">
4.2 Model
</subsectionHeader>
<bodyText confidence="0.9999537">
For both our pruning and packing models, we use
the Mate parser (Bohnet, 2010)4 with default pa-
rameters to learn our parsing models. The Mate
parser is trained on the output of the transforma-
tion pipeline on Sections 00-19 of the three se-
mantically annotated WSJ datasets. Some models
use Brown clusters generated from Sections 00-
19 only. This does not solve OOV problems, but
allows of slightly better generalisation across dis-
tributionally similar words in the training data.
</bodyText>
<subsectionHeader confidence="0.999431">
4.3 Baselines
</subsectionHeader>
<bodyText confidence="0.999816923076923">
We use the SemEval 2014 shared task baseline
(SIMPLE-PRUNE; see Section 2), as well as the
LONGEST-EDGE baseline, also mentioned above.
The latter is our strongest baseline system. It is
very similar to PRUNING, in doing both edge prun-
ing and flow reversal, but the pruning step only
removes the longest edge rather than considering
node depth. Our third baseline is the Mate seman-
tic role labeling learner (SRL-DEP) (Bj¨orkelund
et al., 2009), which uses predicted syntactic parses
as input; for this, we use the syntactic parses made
available in the SemEval 2014 shared task for
replicability.
</bodyText>
<table confidence="0.98202">
Approach Cl DM PAS PCEDT Av
Systems
PRUNING NO 86.6 88.8 72.7 82.7
YES 86.9 89.1 72.5 82.8
PACKING NO 85.8 88.7 71.8 82.1
YES 86.1 88.7 72.9 82.6
Baselines
SIMPLE-PRUNE 54.7 50.9 67.8 57.8
LONGEST-EDGE 83.8 88.9 66.1 79.6
SRL-DEP 79.5 82.4 70.1 77.4
Upper bound
PACKING 99.9 99.5 99.5 99.6
</table>
<tableCaption confidence="0.975241">
Table 1: Labelled F1-score results on development
data, with and without use of Brown clusters (Cl).
</tableCaption>
<subsectionHeader confidence="0.900345">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999821">
The results are presented in Tables 1 through 3,
where the system evaluations for the SemEval task
are marked with asterisks in Table 2. We note that
all our approaches do considerably better than our
</bodyText>
<footnote confidence="0.965818">
4https://code.google.com/p/mate-tools/
</footnote>
<table confidence="0.9925864375">
Approach metric DM PAS PCEDT Av
Systems
PACKING PREC 84.8 87.7 71.2 81.2
(W/ TOP) REC 84.0 88.4 68.6 80.3
(W/O TOP)
F1 84.4 88.0 69.9 80.8*
PREC 85.4 87.9 70.8 81.4
REC 84.6 88.6 68.8 80.7
F1 85.0 88.3 69.9 81.1
PRUNING PREC 87.2 91.3 72.8 83.8
(W/ TOP) REC 80.2 81.3 62.8 74.8
(W/O TOP)
F1 83.6 86.0 67.4 79.0*
PREC 87.2 91.3 72.8 83.8
REC 85.1 85.1 68.0 79.4
F1 86.2 88.1 70.3 81.5
</table>
<tableCaption confidence="0.9840085">
Table 2: Labelled results on test data, with and
without evaluation of top nodes. The scores with
asterisks correspond to the output evaluated in the
SemEval task.
</tableCaption>
<table confidence="0.9942758125">
Approach metric DM PAS PCEDT Av
Systems
PACKING PREC 86.8 89.1 84.8 86.9
(W/ TOP) REC 86.0 89.8 81.8 85.9
(W/O TOP)
F1 86.4 89.4 83.2 86.3
PREC 87.5 89.4 85.4 87.4
REC 86.7 90.1 83.0 86.6
F1 87.1 89.7 84.2 87.0
PRUNING PREC 89.2 92.6 88.2 90.0
(W/ TOP) REC 82.0 82.5 76.1 80.2
(W/O TOP)
F1 85.4 87.3 81.7 84.8
PREC 89.2 92.6 88.2 90.0
REC 87.1 86.3 82.4 85.3
F1 88.1 89.3 85.2 87.5
</table>
<tableCaption confidence="0.956398">
Table 3: Unlabelled results on test data, with and
without evaluation of top nodes.
</tableCaption>
<bodyText confidence="0.9975894">
three baselines. The error reduction of our best
system over the SRL system across all three for-
malisms is 24.2%, and the error reduction over
the more competitive pruning baseline LONGEST-
EDGE is 15.9%. As mentioned in Section 2, these
results seem to promise better performance than
current DAG parsing models. Note from the re-
sults in Table 2 that, as expected, PRUNING leads
to higher precision than PACKING at the expense
of recall.
</bodyText>
<subsectionHeader confidence="0.853524">
4.5 Error Analysis
</subsectionHeader>
<bodyText confidence="0.9999355">
We observe that pruning leads to high precision,
while our packing algorithm gives us much bet-
ter recall. This is not surprising, since our packed
representations introduce new labels, making it
harder to generalize at training time. On the other
hand, pruning approaches suffer in recall, simply
because edges are thrown away in preprocessing
the data.
</bodyText>
<page confidence="0.998338">
216
</page>
<sectionHeader confidence="0.999804" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999885083333333">
In this paper, we experimented with using tree ap-
proximation algorithms to reduce semantic struc-
tures to trees and use off-the-shelf structured pre-
diction techniques to train semantic parsers. Our
approximation algorithms include both pruning
and packing algorithms, i.e., algorithms that try
to reduce graphs to trees optimally, as well as al-
gorithms that pack information about graphs into
trees from which we later recover the richer struc-
tures. Using these tree approximation algorithms,
we obtain 15.9% error reductions over a state-of-
the-art SRL system.
</bodyText>
<sectionHeader confidence="0.999669" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999529405405405">
Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Proc.
of CoNLL: Shared Task, pages 43–48, Boulder, CO,
USA.
Alena B¨ohmov´a, Jan Hajiˇc, Eva Hajiˇcov´a, and Barbora
Hladk´a. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeill´e,
editor, Treebanks: Building and Using Syntacti-
cally Annotated Corpora, pages 103–127. Kluwer,
Netherlands.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proc. of
COLING, pages 89–97, Beijing, China.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan
Sag. 2005. Minimal recursion semantics. Research
on Language and Computation, 3:281–332.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313–330.
Yusuke Miyao and Jun’ichi Tsujii. 2003. Probabilis-
tic modeling of argument structures including non-
local dependencies. In Proc. of RANLP, pages 79–
85, Borovets, Bulgaria.
Carl Pollard and Ivan Sag. 1994. Head-driven phrase
structure grammar. University of Chicago Press.
Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models
and parser ensembles. In Proc. of CoNLL Shared
task session of EMNLP-CoNLL, pages 1044–1050,
Prague, Czech Republic.
Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proc. of COLING,
pages 753–760, Manchester, UK.
Natalie Schluter and Josef Van Genabith. 2009. De-
pendency parsing resources for French. In Proc. of
NODALIDA, pages 166–173, Odense, Denmark.
</reference>
<page confidence="0.998393">
217
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.962650">
<title confidence="0.999931">Copenhagen-Malm¨o: Tree Approximations of Semantic Parsing Problems</title>
<author confidence="0.998939">Jakob Elming</author>
<author confidence="0.998939">Sigrid Klerke</author>
<author confidence="0.998939">H´ector Martinez Alonso</author>
<author confidence="0.998939">Dirk Barbara Plank</author>
<author confidence="0.998939">Anders Johannsen</author>
<author confidence="0.998939">Anders Søgaard</author>
<affiliation confidence="0.9892665">of Computer Science Center for Language Technology Malm¨o University University of Copenhagen</affiliation>
<abstract confidence="0.999005933333333">In this shared task paper for SemEval- 2014 Task 8, we show that most semantic structures can be approximated by trees through a series of almost bijective graph transformations. We transform input graphs, apply off-the-shelf methods from syntactic parsing on the resulting trees, and retrieve output graphs. Using tree approximations, we obtain good results across three semantic formalisms, with a 15.9% error reduction over a stateof-the-art semantic role labeling system on development data. Our system came in 3/6 in the shared task closed track.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>Multilingual semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proc. of CoNLL: Shared Task,</booktitle>
<pages>43--48</pages>
<location>Boulder, CO, USA.</location>
<marker>Bj¨orkelund, Hafdell, Nugues, 2009</marker>
<rawString>Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues. 2009. Multilingual semantic role labeling. In Proc. of CoNLL: Shared Task, pages 43–48, Boulder, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alena B¨ohmov´a</author>
<author>Jan Hajiˇc</author>
<author>Eva Hajiˇcov´a</author>
<author>Barbora Hladk´a</author>
</authors>
<title>The Prague Dependency Treebank: A three-level annotation scenario.</title>
<date>2003</date>
<booktitle>Treebanks: Building and Using Syntactically Annotated Corpora,</booktitle>
<pages>103--127</pages>
<editor>In Anne Abeill´e, editor,</editor>
<publisher>Kluwer,</publisher>
<location>Netherlands.</location>
<marker>B¨ohmov´a, Hajiˇc, Hajiˇcov´a, Hladk´a, 2003</marker>
<rawString>Alena B¨ohmov´a, Jan Hajiˇc, Eva Hajiˇcov´a, and Barbora Hladk´a. 2003. The Prague Dependency Treebank: A three-level annotation scenario. In Anne Abeill´e, editor, Treebanks: Building and Using Syntactically Annotated Corpora, pages 103–127. Kluwer, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>89--97</pages>
<location>Beijing, China.</location>
<contexts>
<context position="4777" citStr="Bohnet (2010)" startWordPosition="709" endWordPosition="710">-24, 2014. The shared task organizers of the Broadcoverage Semantic Dependency Parsing task at SemEval-20143 also presented a pruning-based baseline system. They eliminate re-entrancies in the graph by removing dependencies to nodes with multiple incoming edges. Of these edges, they again keep the shortest. They incorporate all singleton nodes by attaching nodes to the immediately following node or to a virtual root - in case the singleton is sentence-final. Finally, they integrate fragments by subordinating remaining nodes with in-degree 0 to the root node. They apply the parser described in Bohnet (2010), also used below, to the resulting trees. This system obtained a labeled F1-score of 54.7% on the PAS dataset. The performance of their pruning algorithm was also considerably lower than our algorithms on the other datasets considered below. 3 Tree approximations This section describes two approaches to approximating graphs by trees, namely pruning and packing. Pruning optimizes the number of “good” edges in trees (Section 3.1), while packing transforms graphs into trees by means of a pipeline of operations which are 99.6% reversible (see Figure 1); that is, almost no information from the ori</context>
<context position="12274" citStr="Bohnet, 2010" startWordPosition="1965" endWordPosition="1966">hree semantic formalisms are slightly richer, and the average number of edges in the PAS-annotated treebank is 24.32. For DM, the average number of edges is 23.77, and for DM it is 23.33. While the pruning-based approaches thus suffers from a modest information loss, throwing out 5.7% of the edges in the worst 215 case, this is not the case for packing. The reversibility of the packed representations is given by the score upper bound in the last row in Table 1. We use the dataset splits of the SemEval 2014 shared task. 4.2 Model For both our pruning and packing models, we use the Mate parser (Bohnet, 2010)4 with default parameters to learn our parsing models. The Mate parser is trained on the output of the transformation pipeline on Sections 00-19 of the three semantically annotated WSJ datasets. Some models use Brown clusters generated from Sections 00- 19 only. This does not solve OOV problems, but allows of slightly better generalisation across distributionally similar words in the training data. 4.3 Baselines We use the SemEval 2014 shared task baseline (SIMPLE-PRUNE; see Section 2), as well as the LONGEST-EDGE baseline, also mentioned above. The latter is our strongest baseline system. It </context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proc. of COLING, pages 89–97, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Minimal recursion semantics.</title>
<date>2005</date>
<booktitle>Research on Language and Computation,</booktitle>
<pages>3--281</pages>
<contexts>
<context position="2709" citStr="Copestake et al., 2005" startWordPosition="382" endWordPosition="385">Sag, 1994). transformations to pack structural information into new edge labels, making it possible to reconstruct most of the structural complexity as a postprocessing step. Specifically, we present a packing algorithm that consists of two fully bijective graph transformations, in addition to a further transformation that incurs only a small information loss. We carry out experiments across three semantic annotations of the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993), corresponding to simplified versions of the semantic formalisms minimal recursion semantics (MRS) (Copestake et al., 2005), Enju-style predicateargument structures (Miyao and Tsujii, 2003), and Prague-style tectogrammar semantics (B¨ohmov´a et al., 2003). We show that pruning and packing algorithms lead to state-of-the-art performance across these semantic formalisms using an off-theshelf syntactic dependency parser. 2 Related work Sagae and Tsujii (2008) present a pruning algorithm in their paper on transition-based parsing of directed acyclic graphs (DAGs), which discards the edges of longest span entering nodes. They apply the dependency parser described in Sagae and Tsujii (2007) to the tree representations. </context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan Sag. 2005. Minimal recursion semantics. Research on Language and Computation, 3:281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Mary Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2585" citStr="Marcus et al., 1993" startWordPosition="365" endWordPosition="368">cense details: http://creativecommons.org/licenses/by/4.0/ 1For example, HPSG predicate-argument structures (Pollard and Sag, 1994). transformations to pack structural information into new edge labels, making it possible to reconstruct most of the structural complexity as a postprocessing step. Specifically, we present a packing algorithm that consists of two fully bijective graph transformations, in addition to a further transformation that incurs only a small information loss. We carry out experiments across three semantic annotations of the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993), corresponding to simplified versions of the semantic formalisms minimal recursion semantics (MRS) (Copestake et al., 2005), Enju-style predicateargument structures (Miyao and Tsujii, 2003), and Prague-style tectogrammar semantics (B¨ohmov´a et al., 2003). We show that pruning and packing algorithms lead to state-of-the-art performance across these semantic formalisms using an off-theshelf syntactic dependency parser. 2 Related work Sagae and Tsujii (2008) present a pruning algorithm in their paper on transition-based parsing of directed acyclic graphs (DAGs), which discards the edges of long</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell Marcus, Mary Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic modeling of argument structures including nonlocal dependencies.</title>
<date>2003</date>
<booktitle>In Proc. of RANLP,</booktitle>
<pages>79--85</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="2775" citStr="Miyao and Tsujii, 2003" startWordPosition="390" endWordPosition="393">w edge labels, making it possible to reconstruct most of the structural complexity as a postprocessing step. Specifically, we present a packing algorithm that consists of two fully bijective graph transformations, in addition to a further transformation that incurs only a small information loss. We carry out experiments across three semantic annotations of the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993), corresponding to simplified versions of the semantic formalisms minimal recursion semantics (MRS) (Copestake et al., 2005), Enju-style predicateargument structures (Miyao and Tsujii, 2003), and Prague-style tectogrammar semantics (B¨ohmov´a et al., 2003). We show that pruning and packing algorithms lead to state-of-the-art performance across these semantic formalisms using an off-theshelf syntactic dependency parser. 2 Related work Sagae and Tsujii (2008) present a pruning algorithm in their paper on transition-based parsing of directed acyclic graphs (DAGs), which discards the edges of longest span entering nodes. They apply the dependency parser described in Sagae and Tsujii (2007) to the tree representations. We note that this algorithm is not sufficient to produce trees in </context>
</contexts>
<marker>Miyao, Tsujii, 2003</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2003. Probabilistic modeling of argument structures including nonlocal dependencies. In Proc. of RANLP, pages 79– 85, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>Head-driven phrase structure grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="2096" citStr="Pollard and Sag, 1994" startWordPosition="288" endWordPosition="292">We investigate two kinds of tree approximation algorithms that we will refer to as pruning algorithms and packing algorithms. Our pruning algorithms simply remove and reverse edges until the graph is a tree; edge reversals are then undone as a postprocessing step. Our packing algorithms, on the other hand, carry out two bijective graph This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1For example, HPSG predicate-argument structures (Pollard and Sag, 1994). transformations to pack structural information into new edge labels, making it possible to reconstruct most of the structural complexity as a postprocessing step. Specifically, we present a packing algorithm that consists of two fully bijective graph transformations, in addition to a further transformation that incurs only a small information loss. We carry out experiments across three semantic annotations of the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993), corresponding to simplified versions of the semantic formalisms minimal recursion semantics (MRS) (Copestake </context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan Sag. 1994. Head-driven phrase structure grammar. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with LR models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Proc. of CoNLL Shared task session of EMNLP-CoNLL,</booktitle>
<pages>1044--1050</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3279" citStr="Sagae and Tsujii (2007)" startWordPosition="464" endWordPosition="467">al recursion semantics (MRS) (Copestake et al., 2005), Enju-style predicateargument structures (Miyao and Tsujii, 2003), and Prague-style tectogrammar semantics (B¨ohmov´a et al., 2003). We show that pruning and packing algorithms lead to state-of-the-art performance across these semantic formalisms using an off-theshelf syntactic dependency parser. 2 Related work Sagae and Tsujii (2008) present a pruning algorithm in their paper on transition-based parsing of directed acyclic graphs (DAGs), which discards the edges of longest span entering nodes. They apply the dependency parser described in Sagae and Tsujii (2007) to the tree representations. We note that this algorithm is not sufficient to produce trees in our case, where the input graphs are not necessarily acyclic. It does correspond roughly to our LONGEST-EDGE baseline, which removes the longest edge in cycles, in addition to flow reversal. Sagae and Tsujii (2008) also present a shiftreduce automaton approach to parsing DAGs. In their paper, they report a labeled F1-score of 88.7% on the PAS dataset (see Section 3), while we obtain 89.1%, however the results are thus not directly comparable due to different data splits.2 2We obtained code to run th</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser ensembles. In Proc. of CoNLL Shared task session of EMNLP-CoNLL, pages 1044–1050, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Shift-reduce dependency DAG parsing.</title>
<date>2008</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>753--760</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="3046" citStr="Sagae and Tsujii (2008)" startWordPosition="428" endWordPosition="431">a small information loss. We carry out experiments across three semantic annotations of the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993), corresponding to simplified versions of the semantic formalisms minimal recursion semantics (MRS) (Copestake et al., 2005), Enju-style predicateargument structures (Miyao and Tsujii, 2003), and Prague-style tectogrammar semantics (B¨ohmov´a et al., 2003). We show that pruning and packing algorithms lead to state-of-the-art performance across these semantic formalisms using an off-theshelf syntactic dependency parser. 2 Related work Sagae and Tsujii (2008) present a pruning algorithm in their paper on transition-based parsing of directed acyclic graphs (DAGs), which discards the edges of longest span entering nodes. They apply the dependency parser described in Sagae and Tsujii (2007) to the tree representations. We note that this algorithm is not sufficient to produce trees in our case, where the input graphs are not necessarily acyclic. It does correspond roughly to our LONGEST-EDGE baseline, which removes the longest edge in cycles, in addition to flow reversal. Sagae and Tsujii (2008) also present a shiftreduce automaton approach to parsing</context>
</contexts>
<marker>Sagae, Tsujii, 2008</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce dependency DAG parsing. In Proc. of COLING, pages 753–760, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalie Schluter</author>
<author>Josef Van Genabith</author>
</authors>
<title>Dependency parsing resources for French.</title>
<date>2009</date>
<booktitle>In Proc. of NODALIDA,</booktitle>
<pages>166--173</pages>
<location>Odense, Denmark.</location>
<marker>Schluter, Van Genabith, 2009</marker>
<rawString>Natalie Schluter and Josef Van Genabith. 2009. Dependency parsing resources for French. In Proc. of NODALIDA, pages 166–173, Odense, Denmark.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>