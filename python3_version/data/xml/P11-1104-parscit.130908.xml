<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990925">
Reordering with Source Language Collocations
</title>
<author confidence="0.999864">
Zhanyi Liu1,2, Haifeng Wang2, Hua Wu2, Ting Liu1, Sheng Li1
</author>
<affiliation confidence="0.999498">
1Harbin Institute of Technology, Harbin, China
</affiliation>
<address confidence="0.554588">
2Baidu Inc., Beijing, China
</address>
<email confidence="0.9393375">
{liuzhanyi, wanghaifeng, wu_hua}@baidu.com
{tliu, lisheng}@hit.edu.cn
</email>
<sectionHeader confidence="0.994484" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999246941176471">
This paper proposes a novel reordering model
for statistical machine translation (SMT) by
means of modeling the translation orders of
the source language collocations. The model
is learned from a word-aligned bilingual cor-
pus where the collocated words in source sen-
tences are automatically detected. During
decoding, the model is employed to softly
constrain the translation orders of the source
language collocations, so as to constrain the
translation orders of those source phrases con-
taining these collocated words. The experi-
mental results show that the proposed method
significantly improves the translation quality,
achieving the absolute improvements of
1.1~1.4 BLEU score over the baseline me-
thods.
</bodyText>
<sectionHeader confidence="0.998088" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9868413">
Reordering for SMT is first proposed in IBM mod-
els (Brown et al., 1993), usually called IBM con-
straint model, where the movement of words
during translation is modeled. Soon after, Wu
(1997) proposed an ITG (Inversion Transduction
Grammar) model for SMT, called ITG constraint
model, where the reordering of words or phrases is
constrained to two kinds: straight and inverted. In
order to further improve the reordering perfor-
mance, many structure-based methods are pro-
posed, including the reordering model in
hierarchical phrase-based SMT systems (Chiang,
2005) and syntax-based SMT systems (Zhang et al.,
2007; Marton and Resnik, 2008; Ge, 2010; Vis-
weswariah et al., 2010). Although the sentence
structure has been taken into consideration, these
methods don‟t explicitly make use of the strong
correlations between words, such as collocations,
which can effectively indicate reordering in the
target language.
In this paper, we propose a novel method to im-
prove the reordering for SMT by estimating the
reordering score of the source-language colloca-
tions (source collocations for short in this paper).
Given a bilingual corpus, the collocations in the
source sentence are first detected automatically
using a monolingual word alignment (MWA) me-
thod without employing additional resources (Liu
et al., 2009), and then the reordering model based
on the detected collocations is learned from the
word-aligned bilingual corpus. The source colloca-
tion based reordering model is integrated into SMT
systems as an additional feature to softly constrain
the translation orders of the source collocations in
the sentence to be translated, so as to constrain the
translation orders of those source phrases contain-
ing these collocated words.
This method has two advantages: (1) it can au-
tomatically detect and leverage collocated words in
a sentence, including long-distance collocated
words; (2) such a reordering model can be inte-
grated into any SMT systems without resorting to
any additional resources.
We implemented the proposed reordering mod-
el in a phrase-based SMT system, and the evalua-
tion results show that our method significantly
improves translation quality. As compared to the
baseline systems, an absolute improvement of
1.1~1.4 BLEU score is achieved.
1036
</bodyText>
<note confidence="0.9947715">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1036–1044,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.998545625">
The paper is organized as follows: In section 2,
we describe the motivation to use source colloca-
tions for reordering, and briefly introduces the col-
location extraction method. In section 3, we
present our reordering model. And then we de-
scribe the experimental results in section 4 and 5.
In section 6, we describe the related work. Lastly,
we conclude in section 7.
</bodyText>
<sectionHeader confidence="0.993874" genericHeader="introduction">
2 Collocation
</sectionHeader>
<bodyText confidence="0.998845530612245">
A collocation is generally composed of a group of
words that occur together more often than by
chance. Collocations effectively reveal the strong
association among words in a sentence and are
widely employed in a variety of NLP tasks
(Mckeown and Radey, 2000).
Given two words in a collocation, they can be
translated in the same order as in the source lan-
guage, or in the inverted order. We name the first
case as straight, and the second inverted. Based on
the observation that some collocations tend to have
fixed translation orders such as “ItAR jin-rong `fi-
nancial‟ fl X11, wei-ji `crisis‟” (financial crisis)
whose English translation order is usually straight,
and “MO fa-lv `law‟ MM fan-wei `scope‟”
(scope of law) whose English translation order is
generally inverted, some methods have been pro-
posed to improve the reordering model for SMT
based on the collocated words crossing the neigh-
boring components (Xiong et al., 2006). We fur-
ther notice that some words are translated in
different orders when they are collocated with dif-
ferent words. For instance, when “ Y11 chao-liu
`trend‟” is collocated with “ff-,j&apos; t shi-dai `times‟”,
they are often translated into the “trend of times”;
when collocated with “rI li-shi `history‟”, the
translation usually becomes the “historical trend”.
Thus, if we can automatically detect the colloca-
tions in the sentence to be translated and their or-
ders in the target language, the reordering
information of the collocations could be used to
constrain the reordering of phrases during decod-
ing. Therefore, in this paper, we propose to im-
prove the reordering model for SMT by estimating
the reordering score based on the translation orders
of the source collocations.
In general, the collocations can be automatically
identified based on syntactic information such as
dependency trees (Lin, 1998). However these me-
thods may suffer from parsing errors. Moreover,
for many languages, no valid dependency parser
exists. Liu et al. (2009) proposed to automatically
detect the collocated words in a sentence with the
MWA method. The advantage of this method lies
in that it can identify the collocated words in a sen-
tence without additional resources. In this paper,
we employ MWA Model l~3 described in Liu et al.
(2009) to detect collocations in sentences, which
are shown in Eq. (1)~(3).
</bodyText>
<equation confidence="0.967381">
pMWA Model 3 (A |
t(wj  |wc )d(j |cj,l
j
)
</equation>
<bodyText confidence="0.997964384615385">
Where is a monolingual sentence; de-
notes the number of words collocating with ;
A = {(i, ci) I i e [l, l] &amp; c; * i} denotes the potentially
collocated words in S.
The MWA models measure the collocated
words under different constraints. MWA Model 1
only models word collocation probabilities
t(w j I w�.) . MWA Model 2 additionally employs
position collocation probabilities d(jIcj,l) . Be-
sides the features in MWA Model 2, MWA Model
3 also considers fertility probabilities n(Oi I wi)
Given a sentence, the optimal collocated words
can be obtained according to Eq. (4).
</bodyText>
<equation confidence="0.9248575">
A*  arg max pMWA Model i (A  |S) (4)
A
</equation>
<bodyText confidence="0.993094333333333">
Given a monolingual word aligned corpus, the
collocation probabilities can be estimated as fol-
lows.
</bodyText>
<equation confidence="0.951880956521739">
r(wi,wj) p(wi  |wj)p(wj |wi) (5)
, wj)
2
Where, p(wi  |w j) 
count(w
w
denotes the collocated words in the corpus and
count(wi , w j)denotes the co-occurrence frequency.
(1)
(2)
l
S)  n(i  |wi
l i 1 (3)

)


1
j

count (wi , w j )
;
1037
</equation>
<sectionHeader confidence="0.805323" genericHeader="method">
3 Reordering Model with Source Lan-
guage Collocations
</sectionHeader>
<bodyText confidence="0.950047428571429">
In this section, we first describe how to estimate
the orientation probabilities for a given collocation,
and then describe the estimation of the reordering
score during translation. Finally, we describe the
integration of the reordering model into the SMT
system.
o count(o&apos;
</bodyText>
<subsectionHeader confidence="0.999725">
3.1 Reordering probability estimation
</subsectionHeader>
<bodyText confidence="0.985781428571428">
Given a source collocation and its corres-
ponding translations in a bilingual sen-
tence pair, the reordering orientation of the
collocation can be defined as in Eq. (6).
(6)
In our method, only those collocated words in
source language that are aligned to different target
words, are taken into consideration, and those be-
ing aligned to the same target word are ignored.
Given a word-aligned bilingual corpus where
the collocations in source sentences are detected,
the probabilities of the translation orientation of
collocations in the source language can be esti-
mated, as follows:
</bodyText>
<equation confidence="0.994140571428571">
p o
count(o  inverted, fi , f j)  (8)
p(ostraight
fi,fj)  ,J= count(o  straight, f , f j
)
,fj)
(  inverted  |fi , fj) 
</equation>
<bodyText confidence="0.9993795">
Here, is collected according to
the algorithm in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999098">
3.2 Reordering model
</subsectionHeader>
<bodyText confidence="0.999985875">
Given a sentence to be translated, the col-
locations are first detected using the algorithm de-
scribed in Eq. (4). Then the reordering score is
estimated according to the reordering probability
weighted by the collocation probability of the col-
located words. Formally, for a generated transla-
tion candidate , the reordering score is calculated
as follows.
</bodyText>
<equation confidence="0.997599333333333">
( , )
i ci
PO(F,T) r(.ffii,fci) log p(oi,ci,ai,aci  |Ji,fci) (9)
</equation>
<bodyText confidence="0.92091575">
Input: A word-aligned bilingual corpus where
the source collocations are detected
Initialization: =0
for each sentence pair &lt;F, E&gt; in the corpus do
for each collocated word pair in F do
if or then
if or then
Output:
</bodyText>
<figureCaption confidence="0.9204445">
Figure 1. Algorithm of estimating
reordering frequency
</figureCaption>
<bodyText confidence="0.999241857142857">
Here, denotes the collocation probabil-
ity of and as shown in Eq. (5).
In addition to the detected collocated words in
the sentence, we also consider other possible word
pairs whose collocation probabilities are higher
than a given threshold. Thus, the reordering score
is further improved according to Eq. (10).
</bodyText>
<equation confidence="0.975358">
(i, ci)
) logp(oi,c„a„aci  |f ,fci
</equation>
<bodyText confidence="0.9970945">
Where and are two interpolation weights.
0 is the threshold of collocation probability. The
weights and the threshold can be tuned using a de-
velopment set.
</bodyText>
<subsectionHeader confidence="0.974033">
3.3 Integrated into SMT system
</subsectionHeader>
<bodyText confidence="0.9998726">
The SMT systems generally employ the log-linear
model to integrate various features (Chiang, 2005;
Koehn et al., 2007). Given an input sentence F, the
final translation E* with the highest score is chosen
from candidates, as in Eq. (11).
</bodyText>
<equation confidence="0.998669">
M
E m=1
</equation>
<bodyText confidence="0.965695">
Where hm(E, F) (m=1,...,M) denotes fea-
tures. Am is a feature weight.
Our reordering model can be integrated into the
system as one feature as shown in (10).
</bodyText>
<figure confidence="0.8729037">
(10)
,
fj
)}
PO(F,T) = a • Er(fi,fci
E *=arg max {��mhm(E,F)} (11)
(i,j)o {(i,ci) }
R
•
E
,
j&gt;s
f
&amp; r(f
Jj) logp(oi,j,a„aj  |f
)
�
r
(fi
(7)
,fi,fj)
o

count(o
1038
e4
e3
e2
e1
f1 f2 f3 f4 f5
</figure>
<figureCaption confidence="0.570107">
Figure 2. An example for reordering
</figureCaption>
<figure confidence="0.998244230769231">
4 Evaluation of Our Method
Input: Input sentenceN&apos;=j,
Initialization: Score = 0
for each uncovered word do
for each word fj ( = c; or ) do
if is covered then
if i &gt; j then
Score+= r( f.• f: l lo¢ n(o = straight I t
else
Score+=
else
Score +=
Output: Score
</figure>
<subsectionHeader confidence="0.989979">
4.1 Implementation
</subsectionHeader>
<bodyText confidence="0.997860608695652">
We implemented our method in a phrase-based
SMT system (Koehn et al., 2007). Based on the
GIZA++ package (Och and Ney, 2003), we im-
plemented a MWA tool for collocation detection.
Thus, given a sentence to be translated, we first
identify the collocations in the sentence, and then
estimate the reordering score according to the
translation hypothesis. For a translation option to
be expanded, the reordering score inside this
source phrase is calculated according to their trans-
lation orders of the collocations in the correspond-
ing target phrase. The reordering score crossing the
current translation option and the covered parts can
be calculated according to the relative position of
the collocated words. If the source phrase matched
by the current translation option is behind the cov-
ered parts in the source sentence, then
log p(o = staight I ...) is used, otherwise
log p(o = inverted I ... ) . For example, in Figure 2, the
current translation option is ( f2 f3 -+ e3 e4 ). The
collocations related to this translation option are
(f, J3) , , . The reordering scores
can be estimated as follows:
</bodyText>
<equation confidence="0.7576475">
r(f3 ,f5) log p(o  inverted  |f3, f5
)
</equation>
<bodyText confidence="0.999961714285714">
In order to improve the performance of the de-
coder, we design a heuristic function to estimate
the future score, as shown in Figure 3. For any un-
covered word and its collocates in the input sen-
tence, if the collocate is uncovered, then the higher
reordering probability is used. If the collocate has
been covered, then the reordering orientation can
</bodyText>
<figureCaption confidence="0.771387">
Figure 3. Heuristic function for estimating future
</figureCaption>
<bodyText confidence="0.964072">
score
be determined according to the relative positions of
the words and the corresponding reordering proba-
bility is employed.
</bodyText>
<subsectionHeader confidence="0.978487">
4.2 Settings
</subsectionHeader>
<bodyText confidence="0.9999675625">
We use the FBIS corpus (LDC2003E14) to train a
Chinese-to-English phrase-based translation model.
And the SRI language modeling toolkit (Stolcke,
2002) is used to train a 5-gram language model on
the English sentences of FBIS corpus.
We used the NIST evaluation set of 2002 as the
development set to tune the feature weights of the
SMT system and the interpolation parameters,
based on the minimum error rate training method
(Och, 2003), and the NIST evaluation sets of 2004
and 2008 (MT04 and MT08) as the test sets.
We use BLEU (Papineni et al., 2002) as evalua-
tion metrics. We also calculate the statistical signi-
ficance differences between our methods and the
baseline method by using the paired bootstrap re-
sample method (Koehn, 2004).
</bodyText>
<subsectionHeader confidence="0.997654">
4.3 Translation results
</subsectionHeader>
<bodyText confidence="0.999213111111111">
We compare the proposed method with various
reordering methods in previous work.
Monotone model: no reordering model is used.
Distortion based reordering (DBR) model: a
distortion based reordering method (Al-
Onaizan &amp; Papineni, 2006). In this method, the
distortion cost is defined in terms of words, ra-
ther than phrases. This method considers out-
bound, inbound, and pairwise distortions that
</bodyText>
<table confidence="0.990689272727273">
1039
Reorder models MT04 MT08
Monotone model 26.99 18.30
DBR model 26.64 17.83
MSDR model (Baseline) 28.77 18.42
MSDR+ DBR model 28.91 18.58
SCBR Model 1 29.21 19.28
SCBR Model 2 29.44 19.36
SCBR Model 3 29.50 19.44
SCBR models (1+2) 29.65 19.57
SCBR models (1+2+3) 29.75 19.61
</table>
<tableCaption confidence="0.999638">
Table 1. Translation results on various reordering models
</tableCaption>
<figure confidence="0.73572">
both-side DE basic stance also both not loose .
T1: The two sides are also the basic stand of not relaxed.
T2: The basic stance of the two sides have not relaxed.
Reference: The basic stances of both sides did not move.
</figure>
<figureCaption confidence="0.995155">
Figure 4. Translation example. (*/*) denotes (pstraight / pinverted)
</figureCaption>
<figure confidence="0.8870854">
(0.21/0.79)
(0.95/0.05)
(0.99/0.01)
Input: 3R 7 0, 41* Afk It 4 &amp;h *,&apos;k J �
shuang-fang DE ji-ben li-chang ye dou mei-you song-dong .
</figure>
<bodyText confidence="0.998332541666667">
are directly estimated by simple counting over
alignments in the word-aligned bilingual cor-
pus. This method is similar to our proposed
method. But our method considers the transla-
tion order of the collocated words.
msd-bidirectional-fe reordering (MSDR or
Baseline) model: it is one of the reordering
models in Moses. It considers three different
orientation types (monotone, swap, and discon-
tinuous) on both source phrases and target
phrases. And the translation orders of both the
next phrase and the previous phrase in respect
to the current phrase are modeled.
Source collocation based reordering (SCBR)
model: our proposed method. We investigate
three reordering models based on the corres-
ponding MWA models and their combinations.
In SCBR Model i (i=1~3), we use MWA Mod-
el i as described in section 2 to obtain the col-
located words and estimate the reordering
probabilities according to section 3.
The experiential results are shown in Table 1.
The DBR model suffers from serious data sparse-
ness. For example, the reordering cases in the
trained pairwise distortion model only covered
32~38% of those in the test sets. So its perfor-
mance is worse than that of the monotone model.
The MSDR model achieves higher BLEU scores
than the monotone model and the DBR model. Our
models further improve the translation quality,
achieving better performance than the combination
of MSDR model and DBR model. The results in
Table 1 show that “MSDR + SCBR Model 3” per-
forms the best among the SCBR models. This is
because, as compared to MWA Model 1 and 2,
MWA Model 3 takes more information into con-
sideration, including not only the co-occurrence
information of lexical tokens and the position of
words, but also the fertility of words in a sentence.
And when the three SCBR models are combined,
the performance of the SMT system is further im-
proved. As compared to other reordering models,
our models achieve an absolute improvement of
0.98~1.19 BLEU score on the test sets, which are
statistically significant (p &lt; 0.05).
Figure 4 shows an example: T1 is generated by
the baseline system and T2 is generated by the sys-
tem where the SCBR models (1+2+3)1 are used.
</bodyText>
<table confidence="0.805239363636364">
1 In the remainder of this paper, “SCBR models” means the
combination of the SCBR models (1+2+3) unless it is explicit-
ly explained.
1040
Reordering models MT04 MT08
MSDR model 28.77 18.42
MSDR+ DBR model 28.91 18.58
CBR model 28.96 18.77
WCBR model 29.15 19.10
WCBR+SCBR 29.87 19.83
models
</table>
<tableCaption confidence="0.738716">
Table 2. Translation results of co-occurrence
</tableCaption>
<table confidence="0.965624333333333">
based reordering models
CBR model SCBR
Model3
Consecutive words 77.9% 73.5%
Interrupted words 74.1% 87.8%
Total 74.3% 84.9%
</table>
<tableCaption confidence="0.948371">
Table 3. Precisions of the reordering models on
</tableCaption>
<bodyText confidence="0.976791076923077">
the development set
The input sentence contains three collocations. The
collocation (基本, 立场) is included in the same
phrase and translated together as a whole. Thus its
translation is correct in both translations. For the
other two long-distance collocations (双方, 立场)
and (立场, 松动), their translation orders are not
correctly handled by the reordering model in the
baseline system. For the collocation (双方, 立场),
since the SCBR models indicate p(o=straight|双方,
立场) &lt; p(o=inverted|双方, 立场), the system fi-
nally generates the translation T2 by constraining
their translation order with the proposed model.
</bodyText>
<sectionHeader confidence="0.401885" genericHeader="method">
5 Collocations vs. Co-occurring Words
</sectionHeader>
<bodyText confidence="0.99979275">
We compared our method with the method that
models the reordering orientations based on co-
occurring words in the source sentences, rather
than the collocations.
</bodyText>
<subsectionHeader confidence="0.919">
5.1 Co-occurrence based reordering model
</subsectionHeader>
<bodyText confidence="0.999986428571429">
We use the similar algorithm described in section 3
to train the co-occurrence based reordering (CBR)
model, except that the probability of the reordering
orientation is estimated on the co-occurring words
and the relative distance. Given an input sentence
and a translation candidate, the reordering score is
estimated as shown in Eq. (12).
</bodyText>
<equation confidence="0.971322">
0, A
PO (F, T)  Zlog p(oi,j,ai,aj  |.fi , fj , i j) (12)
</equation>
<bodyText confidence="0.998640555555556">
Here, is the relative distance of two words
in the source sentence.
We also construct the weighted co-occurrence
based reordering (WCBR) model. In this model,
the probability of the reordering orientation is ad-
ditionally weighted by the pointwise mutual infor-
mation 2 score of the two words (Manning and
Schütze, 1999), which is estimated as shown in Eq.
(13).
</bodyText>
<equation confidence="0.659845">
(13)
</equation>
<subsectionHeader confidence="0.99572">
5.2 Translation results
</subsectionHeader>
<bodyText confidence="0.999997">
Table 2 shows the translation results. It can be seen
that the performance of the SMT system is im-
proved by integrating the CBR model. The perfor-
mance of the CBR model is also better than that of
the DBR model. It is because the former is trained
based on all co-occurring aligned words, while the
latter only considers the adjacent aligned words.
When the WCBR model is used, the translation
quality is further improved. However, its perfor-
mance is still inferior to that of the SCBR models,
indicating that our method (SCBR models) of
modeling the translation orders of source colloca-
tions is more effective. Furthermore, we combine
the weighted co-occurrence based model and our
method, which outperform all the other models.
</bodyText>
<subsectionHeader confidence="0.968831">
5.3 Result analysis
Precision of prediction
</subsectionHeader>
<bodyText confidence="0.999947">
First of all, we investigate the performance of
the reordering models by calculating precisions of
the translation orders predicted by the reordering
models. Based on the source sentences and refer-
ence translations of the development set, where the
source words and target words are automatically
aligned by the bilingual word alignment method,
we construct the reference translation orders for
two words. Against the references, we calculate
three kinds of precisions as follows:
</bodyText>
<equation confidence="0.664166157894737">
i
i j i j a i a j
 o
, , , ,
} |
}|
|{ I |.i|
0a,j —

2 For occurring words extraction, the window size is set to [-6,
+6].
PCW 

j|

1&amp;o
|{|
I
(14)
</equation>
<table confidence="0.836623166666667">
1
1041
Test sets Baseline MSDR+ MSDR+
(MSDR) WCBR SCBR
MT04 78.9% 80.8% 82.5%
MT08 80.7% 83.8% 85.0%
</table>
<equation confidence="0.7801525">
(15)
(16)
</equation>
<bodyText confidence="0.994305666666667">
Here, denotes the translation order of ( f;, fj )
predicted by the reordering models. If
p(o = straight I f fj) &gt; p(o =inverted f; , fj) , then
o,j = straight , else if &lt;
p(o =inverted f; , fj ) , then o;,j =inverted .oi,.l.a,.aj
denotes the translation order derived from the word
alignments. If , then the predicted
translation order is correct, otherwise wrong.Pc,
and denote the precisions calculated on the
consecutive words and the interrupted words in the
source sentences, respectively. denotes the
precision on both cases. Here, the CBR model and
SCBR Model 3 are compared. The results are
shown in Table 3.
From the results in Table 3, it can be seen that
the CBR model has a higher precision on the con-
secutive words than the SCBR model, but lower
precisions on the interrupted words. It is mainly
because the CBR model introduces more noise
when the relative distance of words is set to a large
number, while the MWA method can effectively
detect the long-distance collocations in sentences
(Liu et al., 2009). This explains why the combina-
tion of the two models can obtain the highest
BLEU score as shown in Table 2. On the whole,
the SCBR Model 3 achieves higher precision than
the CBR model.
</bodyText>
<subsectionHeader confidence="0.823226">
Effect of the reordering model
</subsectionHeader>
<bodyText confidence="0.998884416666667">
Then we evaluate the reordering results of the
generated translations in the test sets. Using the
above method, we construct the reference transla-
tion orders of collocations in the test sets. For a
given word pair in a source sentence, if the transla-
tion order in the generated translation is the same
as that in the reference translations, then it is cor-
rect, otherwise wrong.
We compare the translations of the baseline me-
thod, the co-occurrence based method, and our me-
thod (SCBR models). The precisions calculated on
both kinds of words are shown in Table 4. From
</bodyText>
<tableCaption confidence="0.893707">
Table 4. Precisions (total) of the reordering
models on the test sets
</tableCaption>
<bodyText confidence="0.999886571428571">
the results, it can be seen that our method achieves
higher precisions than both the baseline and the
method modeling the translation orders of the co-
occurring words. It indicates that the proposed me-
thod effectively constrains the reordering of source
words during decoding and improves the transla-
tion quality.
</bodyText>
<sectionHeader confidence="0.999851" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999993625">
Reordering was first proposed in the IBM models
(Brown et al., 1993), later was named IBM con-
straint by Berger et al. (1996). This model treats
the source word sequence as a coverage set that is
processed sequentially and a source token is cov-
ered when it is translated into a new target token.
In 1997, another model called ITG constraint was
presented, in which the reordering order can be
hierarchically modeled as straight or inverted for
two nodes in a binary branching structure (Wu,
1997). Although the ITG constraint allows more
flexible reordering during decoding, Zens and Ney
(2003) showed that the IBM constraint results in
higher BLEU scores. Our method models the reor-
dering of collocated words in sentences instead of
all words in IBM models or two neighboring
blocks in ITG models.
For phrase-based SMT models, Koehn et al.
(2003) linearly modeled the distance of phrase
movements, which results in poor global reorder-
ing. More methods are proposed to explicitly mod-
el the movements of phrases (Tillmann, 2004;
Koehn et al., 2005) or to directly predict the orien-
tations of phrases (Tillmann and Zhang, 2005;
Zens and Ney, 2006), conditioned on current
source phrase or target phrase. Hierarchical phrase-
based SMT methods employ SCFG bilingual trans-
lation model and allow flexible reordering (Chiang,
2005). However, these methods ignored the corre-
lations among words in the source language or in
the target language. In our method, we automati-
cally detect the collocated words in sentences and
</bodyText>
<page confidence="0.534468">
1042
</page>
<bodyText confidence="0.999981074074074">
their translation orders in the target languages,
which are used to constrain the ordering models
with the estimated reordering (straight or inverted)
score. Moreover, our method allows flexible reor-
dering by considering both consecutive words and
interrupted words.
In order to further improve translation results,
many researchers employed syntax-based reorder-
ing methods (Zhang et al., 2007; Marton and Res-
nik, 2008; Ge, 2010; Visweswariah et al., 2010).
However these methods are subject to parsing er-
rors to a large extent. Our method directly obtains
collocation information without resorting to any
linguistic knowledge or tools, therefore is suitable
for any language pairs.
In addition, a few models employed the collo-
cation information to improve the performance of
the ITG constraints (Xiong et al., 2006). Xiong et
al. used the consecutive co-occurring words as col-
location information to constrain the reordering,
which did not lead to higher translation quality in
their experiments. In our method, we first detect
both consecutive and interrupted collocated words
in the source sentence, and then estimated the
reordering score of these collocated words, which
are used to softly constrain the reordering of source
phrases.
</bodyText>
<sectionHeader confidence="0.99907" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999980666666667">
We presented a novel model to improve SMT by
means of modeling the translation orders of source
collocations. The model was learned from a word-
aligned bilingual corpus where the potentially col-
located words in source sentences were automati-
cally detected by the MWA method. During
decoding, the model is employed to softly con-
strain the translation orders of the source language
collocations. Since we only model the reordering
of collocated words, our methods can partially al-
leviate the data sparseness encountered by other
methods directly modeling the reordering based on
source phrases or target phrases. In addition, this
kind of reordering information can be integrated
into any SMT systems without resorting to any
additional resources.
The experimental results show that the pro-
posed method significantly improves the transla-
tion quality of a phrase based SMT system,
achieving an absolute improvement of 1.1~1.4
BLEU score over the baseline methods.
</bodyText>
<sectionHeader confidence="0.995606" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9993524140625">
Yaser Al-Onaizan and Kishore Papineni. 2006. Distor-
tion Models for Statistical Machine Translation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the ACL, pp. 529-536.
Adam L. Berger, Peter F. Brown, Stephen A. Della Pie-
tra, Vincent J. Della Pietra, Andrew S. Kehler, and
Robert L. Mercer. 1996. Language Translation Appa-
ratus and Method of Using Context-Based Transla-
tion Models. United States Patent, Patent Number
5510981, April.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Del-
la Pietra, and Robert. L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Parameter
estimation. Computational Linguistics, 19(2): 263-
311.
David Chiang. 2005. A Hierarchical Phrase-based Mod-
el for Statistical Machine Translation. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics, pp. 263-270.
Niyu Ge. 2010. A Direct Syntax-Driven Reordering
Model for Phrase-Based Machine Translation. In
Proceedings of Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the ACL, pp. 849-857.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pp. 388-395.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the
North American Chapter of the Association of Com-
putational Linguistics, pp. 127-133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran Ri-
chard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation. In
Proceedings of the 45th Annual Meeting of the ACL,
Poster and Demonstration Sessions, pp. 177-180.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of International Workshop on Spoken
Language Translation.
1043
Dekang Lin. 1998. Extracting Collocations from Text
Corpora. In Proceedings of the 1st Workshop on
Computational Terminology, pp. 57-63.
Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li.
2009. Collocation Extraction Using Monolingual
Word Alignment Method. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pp. 487-495.
Christopher D. Manning and Hinrich Schütze. 1999.
Foundations of Statistical Natural Language
Processing, Cambridge, MA; London, U.K.: Brad-
ford Book &amp; MIT Press.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
Constraints for Hierarchical Phrased-based Transla-
tion. In Proceedings of the 46st Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pp. 1003-1011.
Kathleen R. McKeown and Dragomir R. Radev. 2000.
Collocations. In Robert Dale, Hermann Moisl, and
Harold Somers (Ed.), A Handbook of Natural Lan-
guage Processing, pp. 507-523.
Franz Josef Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pp. 160-167.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1) : 19-51.
Kishore Papineni, Salim Roukos, Todd Ward, and Weij-
ing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pp. 311-318.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings for the In-
ternational Conference on Spoken Language
Processing, pp. 901-904.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. In Pro-
ceedings of the Joint Conference on Human Lan-
guage Technologies and the Annual Meeting of the
North American Chapter of the Association of Com-
putational Linguistics, pp. 101-104.
Christoph Tillmann and Tong Zhang. 2005. A Localized
Prediction Model for Statistical Machine Translation.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pp. 557-564.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nanda Kambhatla.
2010. Syntax Based Reordering with Automatically
Derived Rules for Improved Statistical Machine
Translation. In Proceedings of the 23rd International
Conference on Computational Linguistics, pp. 1119-
1127.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377-403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for
Statistical Machine Translation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pp. 521-528.
Richard Zens and Herman Ney. 2003. A Comparative
Study on Reordering Constraints in Statistical Ma-
chine Translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pp. 192-202.
Richard Zens and Herman Ney. 2006. Discriminative
Reordering Models for Statistical Machine Transla-
tion. In Proceedings of the Workshop on Statistical
Machine Translation, pp. 55-63.
Dongdong Zhang, Mu Li, Chi-Ho Li, and Ming Zhou.
2007. Phrase Reordering Model Integrating Syntactic
Knowledge for SMT. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pp. 533-540.
</reference>
<page confidence="0.79453">
1044
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.755836">
<title confidence="0.999652">Reordering with Source Language Collocations</title>
<author confidence="0.992692">Haifeng Hua Ting Sheng</author>
<affiliation confidence="0.999896">Institute of Technology, Harbin,</affiliation>
<address confidence="0.998625">Inc., Beijing, China</address>
<email confidence="0.935519">liuzhanyi@hit.edu.cn</email>
<email confidence="0.935519">wanghaifeng@hit.edu.cn</email>
<email confidence="0.935519">{tliu@hit.edu.cn</email>
<email confidence="0.935519">lisheng@hit.edu.cn</email>
<abstract confidence="0.992216555555556">This paper proposes a novel reordering model for statistical machine translation (SMT) by means of modeling the translation orders of the source language collocations. The model is learned from a word-aligned bilingual corpus where the collocated words in source sentences are automatically detected. During decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. The experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 BLEU score over the baseline methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kishore Papineni</author>
</authors>
<title>Distortion Models for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>529--536</pages>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion Models for Statistical Machine Translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pp. 529-536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Andrew S Kehler</author>
<author>Robert L Mercer</author>
</authors>
<title>Language Translation Apparatus and Method of Using Context-Based Translation Models. United States Patent, Patent Number 5510981,</title>
<date>1996</date>
<contexts>
<context position="22188" citStr="Berger et al. (1996)" startWordPosition="3688" endWordPosition="3691">method (SCBR models). The precisions calculated on both kinds of words are shown in Table 4. From Table 4. Precisions (total) of the reordering models on the test sets the results, it can be seen that our method achieves higher precisions than both the baseline and the method modeling the translation orders of the cooccurring words. It indicates that the proposed method effectively constrains the reordering of source words during decoding and improves the translation quality. 6 Related Work Reordering was first proposed in the IBM models (Brown et al., 1993), later was named IBM constraint by Berger et al. (1996). This model treats the source word sequence as a coverage set that is processed sequentially and a source token is covered when it is translated into a new target token. In 1997, another model called ITG constraint was presented, in which the reordering order can be hierarchically modeled as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences in</context>
</contexts>
<marker>Berger, Brown, Pietra, Pietra, Kehler, Mercer, 1996</marker>
<rawString>Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Andrew S. Kehler, and Robert L. Mercer. 1996. Language Translation Apparatus and Method of Using Context-Based Translation Models. United States Patent, Patent Number 5510981, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<marker>Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert. L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter estimation. Computational Linguistics, 19(2): 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-based Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1545" citStr="Chiang, 2005" startWordPosition="223" endWordPosition="224">EU score over the baseline methods. 1 Introduction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source</context>
<context position="9634" citStr="Chiang, 2005" startWordPosition="1549" endWordPosition="1550">collocation probability of and as shown in Eq. (5). In addition to the detected collocated words in the sentence, we also consider other possible word pairs whose collocation probabilities are higher than a given threshold. Thus, the reordering score is further improved according to Eq. (10). (i, ci) ) logp(oi,c„a„aci |f ,fci Where and are two interpolation weights. 0 is the threshold of collocation probability. The weights and the threshold can be tuned using a development set. 3.3 Integrated into SMT system The SMT systems generally employ the log-linear model to integrate various features (Chiang, 2005; Koehn et al., 2007). Given an input sentence F, the final translation E* with the highest score is chosen from candidates, as in Eq. (11). M E m=1 Where hm(E, F) (m=1,...,M) denotes features. Am is a feature weight. Our reordering model can be integrated into the system as one feature as shown in (10). (10) , fj )} PO(F,T) = a • Er(fi,fci E *=arg max {��mhm(E,F)} (11) (i,j)o {(i,ci) } R • E , j&gt;s f &amp; r(f Jj) logp(oi,j,a„aj |f ) � r (fi (7) ,fi,fj) o  count(o 1038 e4 e3 e2 e1 f1 f2 f3 f4 f5 Figure 2. An example for reordering 4 Evaluation of Our Method Input: Input sentenceN&apos;=j, Initializati</context>
<context position="23386" citStr="Chiang, 2005" startWordPosition="3884" endWordPosition="3885">in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and 1042 their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by considering both consecutive words and interrupted words. In order to further improve translation results, many researchers employed syntax-based reordering methods (Zhang et al., 2007; Marton and </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A Hierarchical Phrase-based Model for Statistical Machine Translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 263-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
</authors>
<title>A Direct Syntax-Driven Reordering Model for Phrase-Based Machine Translation.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL,</booktitle>
<pages>849--857</pages>
<contexts>
<context position="1629" citStr="Ge, 2010" startWordPosition="237" endWordPosition="238">n IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA) </context>
<context position="24008" citStr="Ge, 2010" startWordPosition="3979" endWordPosition="3980">se methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and 1042 their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by considering both consecutive words and interrupted words. In order to further improve translation results, many researchers employed syntax-based reordering methods (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). However these methods are subject to parsing errors to a large extent. Our method directly obtains collocation information without resorting to any linguistic knowledge or tools, therefore is suitable for any language pairs. In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al., 2006). Xiong et al. used the consecutive co-occurring words as collocation information to constrain the reordering, which did not lead to higher translation quality in their experiments. In our method, we first detect</context>
</contexts>
<marker>Ge, 2010</marker>
<rawString>Niyu Ge. 2010. A Direct Syntax-Driven Reordering Model for Phrase-Based Machine Translation. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pp. 849-857.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="12865" citStr="Koehn, 2004" startWordPosition="2123" endWordPosition="2124"> toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan &amp; Papineni, 2006). In this method, the distortion cost is defined in terms of words, rather than phrases. This method considers outbound, inbound, and pairwise distortions that 1039 Reorder models MT04 MT08 Monotone model 26.99 18.30 DBR model 26.64 17.83 MSDR model (Baseline) 28.77 18.42 MSDR+ DBR model 28.91 18.58 SCBR Model 1 29.21 19.28 SCBR Model 2 29.</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pp. 388-395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="22910" citStr="Koehn et al. (2003)" startWordPosition="3808" endWordPosition="3811">rce token is covered when it is translated into a new target token. In 1997, another model called ITG constraint was presented, in which the reordering order can be hierarchically modeled as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our metho</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics, pp. 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Christine Moran Richard Zens, Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL, Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="9655" citStr="Koehn et al., 2007" startWordPosition="1551" endWordPosition="1554">obability of and as shown in Eq. (5). In addition to the detected collocated words in the sentence, we also consider other possible word pairs whose collocation probabilities are higher than a given threshold. Thus, the reordering score is further improved according to Eq. (10). (i, ci) ) logp(oi,c„a„aci |f ,fci Where and are two interpolation weights. 0 is the threshold of collocation probability. The weights and the threshold can be tuned using a development set. 3.3 Integrated into SMT system The SMT systems generally employ the log-linear model to integrate various features (Chiang, 2005; Koehn et al., 2007). Given an input sentence F, the final translation E* with the highest score is chosen from candidates, as in Eq. (11). M E m=1 Where hm(E, F) (m=1,...,M) denotes features. Am is a feature weight. Our reordering model can be integrated into the system as one feature as shown in (10). (10) , fj )} PO(F,T) = a • Er(fi,fci E *=arg max {��mhm(E,F)} (11) (i,j)o {(i,ci) } R • E , j&gt;s f &amp; r(f Jj) logp(oi,j,a„aj |f ) � r (fi (7) ,fi,fj) o  count(o 1038 e4 e3 e2 e1 f1 f2 f3 f4 f5 Figure 2. An example for reordering 4 Evaluation of Our Method Input: Input sentenceN&apos;=j, Initialization: Score = 0 for eac</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the ACL, Poster and Demonstration Sessions, pp. 177-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="23110" citStr="Koehn et al., 2005" startWordPosition="3840" endWordPosition="3843">or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and 1042 their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (s</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Extracting Collocations from Text Corpora.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st Workshop on Computational Terminology,</booktitle>
<pages>57--63</pages>
<contexts>
<context position="5666" citStr="Lin, 1998" startWordPosition="871" endWordPosition="872">story‟”, the translation usually becomes the “historical trend”. Thus, if we can automatically detect the collocations in the sentence to be translated and their orders in the target language, the reordering information of the collocations could be used to constrain the reordering of phrases during decoding. Therefore, in this paper, we propose to improve the reordering model for SMT by estimating the reordering score based on the translation orders of the source collocations. In general, the collocations can be automatically identified based on syntactic information such as dependency trees (Lin, 1998). However these methods may suffer from parsing errors. Moreover, for many languages, no valid dependency parser exists. Liu et al. (2009) proposed to automatically detect the collocated words in a sentence with the MWA method. The advantage of this method lies in that it can identify the collocated words in a sentence without additional resources. In this paper, we employ MWA Model l~3 described in Liu et al. (2009) to detect collocations in sentences, which are shown in Eq. (1)~(3). pMWA Model 3 (A | t(wj |wc )d(j |cj,l j ) Where is a monolingual sentence; denotes the number of words colloc</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Extracting Collocations from Text Corpora. In Proceedings of the 1st Workshop on Computational Terminology, pp. 57-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhanyi Liu</author>
<author>Haifeng Wang</author>
<author>Hua Wu</author>
<author>Sheng Li</author>
</authors>
<title>Collocation Extraction Using Monolingual Word Alignment Method.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>487--495</pages>
<contexts>
<context position="2293" citStr="Liu et al., 2009" startWordPosition="335" endWordPosition="338">ence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA) method without employing additional resources (Liu et al., 2009), and then the reordering model based on the detected collocations is learned from the word-aligned bilingual corpus. The source collocation based reordering model is integrated into SMT systems as an additional feature to softly constrain the translation orders of the source collocations in the sentence to be translated, so as to constrain the translation orders of those source phrases containing these collocated words. This method has two advantages: (1) it can automatically detect and leverage collocated words in a sentence, including long-distance collocated words; (2) such a reordering mo</context>
<context position="5804" citStr="Liu et al. (2009)" startWordPosition="891" endWordPosition="894">nce to be translated and their orders in the target language, the reordering information of the collocations could be used to constrain the reordering of phrases during decoding. Therefore, in this paper, we propose to improve the reordering model for SMT by estimating the reordering score based on the translation orders of the source collocations. In general, the collocations can be automatically identified based on syntactic information such as dependency trees (Lin, 1998). However these methods may suffer from parsing errors. Moreover, for many languages, no valid dependency parser exists. Liu et al. (2009) proposed to automatically detect the collocated words in a sentence with the MWA method. The advantage of this method lies in that it can identify the collocated words in a sentence without additional resources. In this paper, we employ MWA Model l~3 described in Liu et al. (2009) to detect collocations in sentences, which are shown in Eq. (1)~(3). pMWA Model 3 (A | t(wj |wc )d(j |cj,l j ) Where is a monolingual sentence; denotes the number of words collocating with ; A = {(i, ci) I i e [l, l] &amp; c; * i} denotes the potentially collocated words in S. The MWA models measure the collocated word</context>
<context position="20881" citStr="Liu et al., 2009" startWordPosition="3462" endWordPosition="3465"> on the consecutive words and the interrupted words in the source sentences, respectively. denotes the precision on both cases. Here, the CBR model and SCBR Model 3 are compared. The results are shown in Table 3. From the results in Table 3, it can be seen that the CBR model has a higher precision on the consecutive words than the SCBR model, but lower precisions on the interrupted words. It is mainly because the CBR model introduces more noise when the relative distance of words is set to a large number, while the MWA method can effectively detect the long-distance collocations in sentences (Liu et al., 2009). This explains why the combination of the two models can obtain the highest BLEU score as shown in Table 2. On the whole, the SCBR Model 3 achieves higher precision than the CBR model. Effect of the reordering model Then we evaluate the reordering results of the generated translations in the test sets. Using the above method, we construct the reference translation orders of collocations in the test sets. For a given word pair in a source sentence, if the translation order in the generated translation is the same as that in the reference translations, then it is correct, otherwise wrong. We co</context>
</contexts>
<marker>Liu, Wang, Wu, Li, 2009</marker>
<rawString>Zhanyi Liu, Haifeng Wang, Hua Wu, and Sheng Li. 2009. Collocation Extraction Using Monolingual Word Alignment Method. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pp. 487-495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Schütze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing,</booktitle>
<publisher>Bradford Book &amp; MIT Press.</publisher>
<location>Cambridge, MA; London, U.K.:</location>
<contexts>
<context position="18269" citStr="Manning and Schütze, 1999" startWordPosition="3002" endWordPosition="3005">ng (CBR) model, except that the probability of the reordering orientation is estimated on the co-occurring words and the relative distance. Given an input sentence and a translation candidate, the reordering score is estimated as shown in Eq. (12). 0, A PO (F, T)  Zlog p(oi,j,ai,aj |.fi , fj , i j) (12) Here, is the relative distance of two words in the source sentence. We also construct the weighted co-occurrence based reordering (WCBR) model. In this model, the probability of the reordering orientation is additionally weighted by the pointwise mutual information 2 score of the two words (Manning and Schütze, 1999), which is estimated as shown in Eq. (13). (13) 5.2 Translation results Table 2 shows the translation results. It can be seen that the performance of the SMT system is improved by integrating the CBR model. The performance of the CBR model is also better than that of the DBR model. It is because the former is trained based on all co-occurring aligned words, while the latter only considers the adjacent aligned words. When the WCBR model is used, the translation quality is further improved. However, its performance is still inferior to that of the SCBR models, indicating that our method (SCBR mo</context>
</contexts>
<marker>Manning, Schütze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Schütze. 1999. Foundations of Statistical Natural Language Processing, Cambridge, MA; London, U.K.: Bradford Book &amp; MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft Syntactic Constraints for Hierarchical Phrased-based Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46st Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="1619" citStr="Marton and Resnik, 2008" startWordPosition="233" endWordPosition="236">r SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignm</context>
<context position="23998" citStr="Marton and Resnik, 2008" startWordPosition="3974" endWordPosition="3978">iang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and 1042 their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by considering both consecutive words and interrupted words. In order to further improve translation results, many researchers employed syntax-based reordering methods (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). However these methods are subject to parsing errors to a large extent. Our method directly obtains collocation information without resorting to any linguistic knowledge or tools, therefore is suitable for any language pairs. In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al., 2006). Xiong et al. used the consecutive co-occurring words as collocation information to constrain the reordering, which did not lead to higher translation quality in their experiments. In our method, we fi</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrased-based Translation. In Proceedings of the 46st Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 1003-1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Dragomir R Radev</author>
</authors>
<title>Collocations. In</title>
<date>2000</date>
<booktitle>A Handbook of Natural Language Processing,</booktitle>
<pages>507--523</pages>
<marker>McKeown, Radev, 2000</marker>
<rawString>Kathleen R. McKeown and Dragomir R. Radev. 2000. Collocations. In Robert Dale, Hermann Moisl, and Harold Somers (Ed.), A Handbook of Natural Language Processing, pp. 507-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="12562" citStr="Och, 2003" startWordPosition="2071" endWordPosition="2072">ic function for estimating future score be determined according to the relative positions of the words and the corresponding reordering probability is employed. 4.2 Settings We use the FBIS corpus (LDC2003E14) to train a Chinese-to-English phrase-based translation model. And the SRI language modeling toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan &amp; Papineni, 2006). In this method, the distortion cost i</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<location></location>
<contexts>
<context position="10567" citStr="Och and Ney, 2003" startWordPosition="1734" endWordPosition="1737"> )} PO(F,T) = a • Er(fi,fci E *=arg max {��mhm(E,F)} (11) (i,j)o {(i,ci) } R • E , j&gt;s f &amp; r(f Jj) logp(oi,j,a„aj |f ) � r (fi (7) ,fi,fj) o  count(o 1038 e4 e3 e2 e1 f1 f2 f3 f4 f5 Figure 2. An example for reordering 4 Evaluation of Our Method Input: Input sentenceN&apos;=j, Initialization: Score = 0 for each uncovered word do for each word fj ( = c; or ) do if is covered then if i &gt; j then Score+= r( f.• f: l lo¢ n(o = straight I t else Score+= else Score += Output: Score 4.1 Implementation We implemented our method in a phrase-based SMT system (Koehn et al., 2007). Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. Thus, given a sentence to be translated, we first identify the collocations in the sentence, and then estimate the reordering score according to the translation hypothesis. For a translation option to be expanded, the reordering score inside this source phrase is calculated according to their translation orders of the collocations in the corresponding target phrase. The reordering score crossing the current translation option and the covered parts can be calculated according to the relative position of the collocated words. If the source p</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1) : 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="12679" citStr="Papineni et al., 2002" startWordPosition="2092" endWordPosition="2095">d the corresponding reordering probability is employed. 4.2 Settings We use the FBIS corpus (LDC2003E14) to train a Chinese-to-English phrase-based translation model. And the SRI language modeling toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Translation results We compare the proposed method with various reordering methods in previous work. Monotone model: no reordering model is used. Distortion based reordering (DBR) model: a distortion based reordering method (AlOnaizan &amp; Papineni, 2006). In this method, the distortion cost is defined in terms of words, rather than phrases. This method considers outbound, inbound, and pairwise distortions t</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pp. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings for the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="12277" citStr="Stolcke, 2002" startWordPosition="2021" endWordPosition="2022">tion to estimate the future score, as shown in Figure 3. For any uncovered word and its collocates in the input sentence, if the collocate is uncovered, then the higher reordering probability is used. If the collocate has been covered, then the reordering orientation can Figure 3. Heuristic function for estimating future score be determined according to the relative positions of the words and the corresponding reordering probability is employed. 4.2 Settings We use the FBIS corpus (LDC2003E14) to train a Chinese-to-English phrase-based translation model. And the SRI language modeling toolkit (Stolcke, 2002) is used to train a 5-gram language model on the English sentences of FBIS corpus. We used the NIST evaluation set of 2002 as the development set to tune the feature weights of the SMT system and the interpolation parameters, based on the minimum error rate training method (Och, 2003), and the NIST evaluation sets of 2004 and 2008 (MT04 and MT08) as the test sets. We use BLEU (Papineni et al., 2002) as evaluation metrics. We also calculate the statistical significance differences between our methods and the baseline method by using the paired bootstrap resample method (Koehn, 2004). 4.3 Transl</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proceedings for the International Conference on Spoken Language Processing, pp. 901-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A Unigram Orientation Model for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>101--104</pages>
<contexts>
<context position="23089" citStr="Tillmann, 2004" startWordPosition="3838" endWordPosition="3839">led as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and 1042 their translation orders in the target languages, which are used to constrain the ordering models with the es</context>
</contexts>
<marker>Tillmann, 2004</marker>
<rawString>Christoph Tillmann. 2004. A Unigram Orientation Model for Statistical Machine Translation. In Proceedings of the Joint Conference on Human Language Technologies and the Annual Meeting of the North American Chapter of the Association of Computational Linguistics, pp. 101-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A Localized Prediction Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>557--564</pages>
<contexts>
<context position="23187" citStr="Tillmann and Zhang, 2005" startWordPosition="3853" endWordPosition="3856">though the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and 1042 their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering b</context>
</contexts>
<marker>Tillmann, Zhang, 2005</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2005. A Localized Prediction Model for Statistical Machine Translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 557-564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
<author>Jiri Navratil</author>
<author>Jeffrey Sorensen</author>
<author>Vijil Chenthamarakshan</author>
<author>Nanda Kambhatla</author>
</authors>
<title>Syntax Based Reordering with Automatically Derived Rules for Improved Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>1119--1127</pages>
<contexts>
<context position="1657" citStr="Visweswariah et al., 2010" startWordPosition="239" endWordPosition="243">ls (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using a monolingual word alignment (MWA) method without employing add</context>
<context position="24036" citStr="Visweswariah et al., 2010" startWordPosition="3981" endWordPosition="3984"> ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and 1042 their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by considering both consecutive words and interrupted words. In order to further improve translation results, many researchers employed syntax-based reordering methods (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). However these methods are subject to parsing errors to a large extent. Our method directly obtains collocation information without resorting to any linguistic knowledge or tools, therefore is suitable for any language pairs. In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al., 2006). Xiong et al. used the consecutive co-occurring words as collocation information to constrain the reordering, which did not lead to higher translation quality in their experiments. In our method, we first detect both consecutive and interr</context>
</contexts>
<marker>Visweswariah, Navratil, Sorensen, Chenthamarakshan, Kambhatla, 2010</marker>
<rawString>Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, Vijil Chenthamarakshan, and Nanda Kambhatla. 2010. Syntax Based Reordering with Automatically Derived Rules for Improved Statistical Machine Translation. In Proceedings of the 23rd International Conference on Computational Linguistics, pp. 1119-1127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="1173" citStr="Wu (1997)" startWordPosition="169" endWordPosition="170">ng decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. The experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 BLEU score over the baseline methods. 1 Introduction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the </context>
<context position="22559" citStr="Wu, 1997" startWordPosition="3753" endWordPosition="3754">ly constrains the reordering of source words during decoding and improves the translation quality. 6 Related Work Reordering was first proposed in the IBM models (Brown et al., 1993), later was named IBM constraint by Berger et al. (1996). This model treats the source word sequence as a coverage set that is processed sequentially and a source token is covered when it is translated into a new target token. In 1997, another model called ITG constraint was presented, in which the reordering order can be hierarchically modeled as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phras</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>521--528</pages>
<contexts>
<context position="4760" citStr="Xiong et al., 2006" startWordPosition="726" endWordPosition="729"> in the same order as in the source language, or in the inverted order. We name the first case as straight, and the second inverted. Based on the observation that some collocations tend to have fixed translation orders such as “ItAR jin-rong `financial‟ fl X11, wei-ji `crisis‟” (financial crisis) whose English translation order is usually straight, and “MO fa-lv `law‟ MM fan-wei `scope‟” (scope of law) whose English translation order is generally inverted, some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al., 2006). We further notice that some words are translated in different orders when they are collocated with different words. For instance, when “ Y11 chao-liu `trend‟” is collocated with “ff-,j&apos; t shi-dai `times‟”, they are often translated into the “trend of times”; when collocated with “rI li-shi `history‟”, the translation usually becomes the “historical trend”. Thus, if we can automatically detect the collocations in the sentence to be translated and their orders in the target language, the reordering information of the collocations could be used to constrain the reordering of phrases during deco</context>
<context position="24396" citStr="Xiong et al., 2006" startWordPosition="4037" endWordPosition="4040">ordering by considering both consecutive words and interrupted words. In order to further improve translation results, many researchers employed syntax-based reordering methods (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). However these methods are subject to parsing errors to a large extent. Our method directly obtains collocation information without resorting to any linguistic knowledge or tools, therefore is suitable for any language pairs. In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al., 2006). Xiong et al. used the consecutive co-occurring words as collocation information to constrain the reordering, which did not lead to higher translation quality in their experiments. In our method, we first detect both consecutive and interrupted collocated words in the source sentence, and then estimated the reordering score of these collocated words, which are used to softly constrain the reordering of source phrases. 7 Conclusions We presented a novel model to improve SMT by means of modeling the translation orders of source collocations. The model was learned from a wordaligned bilingual co</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pp. 521-528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Herman Ney</author>
</authors>
<title>A Comparative Study on Reordering Constraints in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>192--202</pages>
<contexts>
<context position="22657" citStr="Zens and Ney (2003)" startWordPosition="3765" endWordPosition="3768">n quality. 6 Related Work Reordering was first proposed in the IBM models (Brown et al., 1993), later was named IBM constraint by Berger et al. (1996). This model treats the source word sequence as a coverage set that is processed sequentially and a source token is covered when it is translated into a new target token. In 1997, another model called ITG constraint was presented, in which the reordering order can be hierarchically modeled as straight or inverted for two nodes in a binary branching structure (Wu, 1997). Although the ITG constraint allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target </context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>Richard Zens and Herman Ney. 2003. A Comparative Study on Reordering Constraints in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pp. 192-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Herman Ney</author>
</authors>
<title>Discriminative Reordering Models for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>55--63</pages>
<contexts>
<context position="23208" citStr="Zens and Ney, 2006" startWordPosition="3857" endWordPosition="3860">allows more flexible reordering during decoding, Zens and Ney (2003) showed that the IBM constraint results in higher BLEU scores. Our method models the reordering of collocated words in sentences instead of all words in IBM models or two neighboring blocks in ITG models. For phrase-based SMT models, Koehn et al. (2003) linearly modeled the distance of phrase movements, which results in poor global reordering. More methods are proposed to explicitly model the movements of phrases (Tillmann, 2004; Koehn et al., 2005) or to directly predict the orientations of phrases (Tillmann and Zhang, 2005; Zens and Ney, 2006), conditioned on current source phrase or target phrase. Hierarchical phrasebased SMT methods employ SCFG bilingual translation model and allow flexible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and 1042 their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by considering both co</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Herman Ney. 2006. Discriminative Reordering Models for Statistical Machine Translation. In Proceedings of the Workshop on Statistical Machine Translation, pp. 55-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Chi-Ho Li</author>
<author>Ming Zhou</author>
</authors>
<title>Phrase Reordering Model Integrating Syntactic Knowledge for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>533--540</pages>
<contexts>
<context position="1594" citStr="Zhang et al., 2007" startWordPosition="229" endWordPosition="232">uction Reordering for SMT is first proposed in IBM models (Brown et al., 1993), usually called IBM constraint model, where the movement of words during translation is modeled. Soon after, Wu (1997) proposed an ITG (Inversion Transduction Grammar) model for SMT, called ITG constraint model, where the reordering of words or phrases is constrained to two kinds: straight and inverted. In order to further improve the reordering performance, many structure-based methods are proposed, including the reordering model in hierarchical phrase-based SMT systems (Chiang, 2005) and syntax-based SMT systems (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). Although the sentence structure has been taken into consideration, these methods don‟t explicitly make use of the strong correlations between words, such as collocations, which can effectively indicate reordering in the target language. In this paper, we propose a novel method to improve the reordering for SMT by estimating the reordering score of the source-language collocations (source collocations for short in this paper). Given a bilingual corpus, the collocations in the source sentence are first detected automatically using </context>
<context position="23973" citStr="Zhang et al., 2007" startWordPosition="3970" endWordPosition="3973">xible reordering (Chiang, 2005). However, these methods ignored the correlations among words in the source language or in the target language. In our method, we automatically detect the collocated words in sentences and 1042 their translation orders in the target languages, which are used to constrain the ordering models with the estimated reordering (straight or inverted) score. Moreover, our method allows flexible reordering by considering both consecutive words and interrupted words. In order to further improve translation results, many researchers employed syntax-based reordering methods (Zhang et al., 2007; Marton and Resnik, 2008; Ge, 2010; Visweswariah et al., 2010). However these methods are subject to parsing errors to a large extent. Our method directly obtains collocation information without resorting to any linguistic knowledge or tools, therefore is suitable for any language pairs. In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al., 2006). Xiong et al. used the consecutive co-occurring words as collocation information to constrain the reordering, which did not lead to higher translation quality in their experime</context>
</contexts>
<marker>Zhang, Li, Li, Zhou, 2007</marker>
<rawString>Dongdong Zhang, Mu Li, Chi-Ho Li, and Ming Zhou. 2007. Phrase Reordering Model Integrating Syntactic Knowledge for SMT. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 533-540.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>