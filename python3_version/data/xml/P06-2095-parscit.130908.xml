<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000673">
<title confidence="0.998145">
Using comparable corpora
to solve problems difficult for human translators
</title>
<author confidence="0.999364">
Serge Sharoff, Bogdan Babych, Anthony Hartley
</author>
<affiliation confidence="0.9971535">
Centre for Translation Studies
University of Leeds, LS2 9JT UK
</affiliation>
<email confidence="0.989197">
{s.sharoff,b.babych,a.hartley}@leeds.ac.uk
</email>
<sectionHeader confidence="0.982621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99967">
In this paper we present a tool that uses
comparable corpora to find appropriate
translation equivalents for expressions that
are considered by translators as difficult.
For a phrase in the source language the
tool identifies a range of possible expres-
sions used in similar contexts in target lan-
guage corpora and presents them to the
translator as a list of suggestions. In the
paper we discuss the method and present
results of human evaluation of the perfor-
mance of the tool, which highlight its use-
fulness when dictionary solutions are lack-
ing.
</bodyText>
<sectionHeader confidence="0.995172" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99963725862069">
There is no doubt that both professional and
trainee translators need access to authentic data
provided by corpora. With respect to polyse-
mous lexical items, bilingual dictionaries list sev-
eral translation equivalents for a headword, but
words taken in their contexts can be translated
in many more ways than indicated in dictionar-
ies. For instance, the Oxford Russian Dictionary
(ORD) lacks a translation for the Russian expres-
sion HC-1eP11bIBaiomHI3 oTBPT (‘comprehensive an-
swer’), while the Multitran Russian-English dic-
tionary suggests that it can be translated as ir-
refragable answer. Yet this expression is ex-
tremely rare in English; on the Internet it occurs
mostly in pages produced by Russian speakers.
On the other hand, translations for polysemous
words are too numerous to be listed for all pos-
sible contexts. For example, the entry for strong
in ORD already has 57 subentries and yet it fails
to mention many word combinations frequent in
the British National Corpus (BNC), such as strong
{feeling, field, opposition, sense, voice}. Strong
voice is also not listed in the Oxford French, Ger-
man or Spanish Dictionaries.
There has been surprisingly little research on
computational methods for finding translation
equivalents of words from the general lexicon.
Practically all previous studies have concerned
detection of terminological equivalence. For in-
stance, project Termight at AT&amp;T aimed to de-
velop a tool for semi-automatic acquisition of
termbanks in the computer science domain (Da-
gan and Church, 1997). There was also a study
concerning the use of multilingual webpages to
develop bilingual lexicons and termbanks (Grefen-
stette, 2002). However, neither of them concerned
translations of words from the general lexicon. At
the same time, translators often experience more
difficulty in dealing with such general expressions
because of their polysemy, which is reflected dif-
ferently in the target language, thus causing the
dependency of their translation on the correspond-
ing context. Such variation is often not captured
by dictionaries.
Because of their importance, words from the
general lexicon are studied by translation re-
searchers, and comparable corpora are increas-
ingly used in translation practice and training
(Varantola, 2003). However, such studies are
mostly confined to lexicographic exercises, which
compare the contexts and functions of potential
translation equivalents once they are known, for
instance, absolutely vs. assolutamente in Italian
(Partington, 1998). Such studies do not pro-
vide a computational model for finding appropri-
ate translation equivalents for expressions that are
not listed or are inadequate in dictionaries.
Parallel corpora, conisting of original texts and
</bodyText>
<note confidence="0.757866">
739
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 739–746,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.982197125">
their exact translations, provide a useful supple-
ment to decontextualised translation equivalents
listed in dictionaries. However, parallel corpora
are not representative. Many of them are in the
range of a few million words, which is simply too
small to account for variations in translation of
moderately frequent words. Those that are a bit
larger, such as the Europarl corpus, are restricted
in their domain. For instance, all of the 14 in-
stances of strong voice in the English section of
Europarl are used in the sense of ‘the opinion of
a political institution’. At the same time the BNC
contains 46 instances of strong voice covering sev-
eral different meanings.
In this paper we propose a computational
method for using comparable corpora to find trans-
lation equivalents for source language expressions
that are considered as difficult by trainee or pro-
fessional translators. The model is based on de-
tecting frequent multi-word expressions (MWEs)
in the source and target languages and finding a
mapping between them in comparable monolin-
gual corpora, which are designed in a similar way
in the two languages.
The described methodology is implemented in
ASSIST, a tool that helps translators to find solu-
tions for difficult translation problems. The tool
presents the results as lists of translation sugges-
tions (usually 50 to 100 items) ordered alphabeti-
cally or by their frequency in target language cor-
pora. Translators can skim through these lists and
identify an example which is most appropriate in
a given context.
In the following sections we outline our ap-
proach, evaluate the output of the prototype of AS-
SIST and discuss future work.
2 Finding translations in comparable
corpora
The proposed model finds potential translation
equivalents in four steps, which include
</bodyText>
<listItem confidence="0.999655375">
1. expansion of words in the original expression
using related words;
2. translation of the resultant set using existing
bilingual dictionaries;
3. further expansion of the set using related
words in the target language;
4. filtering of the set according to expressions
frequent in the target language corpus.
</listItem>
<bodyText confidence="0.99885952631579">
In this study we use several comparable cor-
pora for English and Russian, including large ref-
erence corpora (the BNC and the Russian Refer-
ence Corpus) and corpora of major British and
Russian newspapers. All corpora used in the study
are quite large, i.e. the size of each corpus is in
the range of 100-200 million words (MW), so that
they provide enough evidence to detect such col-
locations as strong voice and clear defiance.
Although the current study is restricted to the
English-Russian pair, the methodology does not
rely on any particular language. It can be ex-
tended to other languages for which large com-
parable corpora, POS-tagging and lemmatisation
tools, and bilingual dictionaries are available. For
example, we conducted a small study for transla-
tion between English and German using the Ox-
ford German Dictionary and a 200 MW German
corpus derived from the Internet (Sharoff, 2006).
</bodyText>
<subsectionHeader confidence="0.99355">
2.1 Query expansion
</subsectionHeader>
<bodyText confidence="0.999987566666667">
The problem with using comparable corpora to
find translation equivalents is that there is no ob-
vious bridge between the two languages. Unlike
aligned parallel corpora, comparable corpora pro-
vide a model for each individual language, while
dictionaries, which can serve as a bridge, are inad-
equate for the task in question, because the prob-
lem we want to address involves precisely transla-
tion equivalents that are not listed there.
Therefore, a specific query needs first to be
generalised in order to then retrieve a suitable
candidate from a set of candidates. One way
to generalise the query is by using similarity
classes, i.e. groups of words with lexically simi-
lar behaviour. In his work on distributional sim-
ilarity (Lin, 1998) designed a parser to identify
grammatical relationships between words. How-
ever, broad-coverage parsers suitable for process-
ing BNC-like corpora are not available for many
languages. Another, resource-light approach treats
the context as a bag of words (BoW) and detects
the similarity of contexts on the basis of colloca-
tions in a window of a certain size, typically 3-4
words, e.g. (Rapp, 2004). Even if using a parser
can increase precision in identification of contexts
in the case of long-distance dependencies (e.g. to
cook Alice a whole meal), we can find a reason-
able set of relevant terms returned using the BoW
approach, cf. the results of human evaluation for
English and German by (Rapp, 2004).
</bodyText>
<page confidence="0.547886">
740
</page>
<bodyText confidence="0.964074">
For each source word s0 we produce a list of
similar words: O(s0) = s1, ... , sN (in our tool
we use N = 20 as the cutoff). Since lists of dis-
tributionally words can contain words irrelevant to
the source word, we filter them to produce a more
reliable similarity class 5(s0) using the assump-
tion that the similarity classes of similar words
have common members:
bw E 5(s0), w E O(s0)&amp;w E U O(si)
This yields for experience the following similar-
ity class: knowledge, opportunity, life, encounter,
skill, feeling, reality, sensation, dream, vision,
learning, perception, learn.1 Even if there is no
requirement in the BoW approach that words in
the similarity class are of the same part of speech,
it happens quite frequently that most words have
the same part of speech because of the similarity
of contexts.
</bodyText>
<subsectionHeader confidence="0.998545">
2.2 Query translation and further expansion
</subsectionHeader>
<bodyText confidence="0.997808151515151">
In the next step we produce a translation class by
translating all words from the similarity class into
the target language using a bilingual dictionary
(T (w) for the translation of w). Then for Step 3
we have two options: a full translation class (TF)
and a reduced one (TR).
TF consists of similarity classes produced for
all translations: 5(T(5(s0))). However, this
causes a combinatorial explosion. If a similarity
class contains N words (the average figure is 16)
and a dictionary lists on average M equivalents
for a source word (the average figure is 11), this
procedure outputs on average M x N2 words in
the full translation class. For instance, the com-
plete translation class for experience contains 998
words. What is worse, some words from the full
translation class do not refer to the domain im-
plied in the original expression because of the am-
biguity of the translation operation. For instance,
the word dream belongs to the similarity class of
experience. Since it can be translated into Rus-
sian as CKa3Ka (‘fairy-tale’), the latter Russian word
will be expanded in the full translation class with
words referring to legends and stories. In the later
stages of the project, word sense disambiguation
in corpora could improve precision of translation
classes. However at the present stage we attempt
to trade the recall of the tool for greater precision
by translating words in the source similarity class,
1Ordered according to the score produced by the Singular
Value Decomposition method as implemented by Rapp.
and generating the similarity classes of transla-
tions only for the source word:
</bodyText>
<equation confidence="0.992339">
TR(s0) = 5(T(s0)) U T(5(s0)).
</equation>
<bodyText confidence="0.999798666666667">
This reduces the class of experience to 128 words.
This step crucially relies on a wide-coverage
machine readable dictionary. The bilingual dictio-
nary resources we use are derived from the source
file for the Oxford Russian Dictionary, provided
by OUP.
</bodyText>
<subsectionHeader confidence="0.999572">
2.3 Filtering equivalence classes
</subsectionHeader>
<bodyText confidence="0.999992875">
In the final step we check all possible combina-
tions of words from the translation classes for their
frequency in target language corpora.
The number of elements in the set of theoreti-
cally possible combinations is usually very large:
H Ti, where Ti is the number of words in the trans-
lation class of each word of the original MWE.
This number is much larger than the set of word
combinations which is found in the target lan-
guage corpora. For instance, daunting experience
has 202,594 combinations for the full translation
class of daunting experience and 6,144 for the re-
duced one. However, in the target language cor-
pora we can find only 2,256 collocations with fre-
quency &gt; 2 for the full translation class and 92 for
the reduced one.
Each theoretically possible combination is gen-
erated and looked up in a database of MWEs
(which is much faster than querying corpora for
frequencies of potential collocations). The MWE
database was pre-compiled from corpora using a
method of filtering, similar to part-of-speech fil-
tering suggested in (Justeson and Katz, 1995): in
corpora each N-gram of length 2, 3 and 4 tokens
was checked against a set of filters.
However, instead of pre-defined patterns for en-
tire expressions our filtering method uses sets of
negative constraints, which are usually applied to
the edges of expressions. This change boosts re-
call of retrieved MWEs and allows us to use the
same set of patterns for MWEs of different length.
The filter uses constraints for both lexical and
part-of-speech features, which makes configura-
tion specifications more flexible.
The idea of applying a negative feature filter
rather than a set of positive patterns is based on
the observation that it is easier to describe unde-
sirable features than to enumerate complete lists of
patterns. For example, MWEs of any length end-
ing with a preposition are undesirable (particles in
</bodyText>
<page confidence="0.492976">
741
</page>
<table confidence="0.998804666666667">
no of words British news Russian news
217,394,039 77,625,002
REs in filter 25 18
2-grams 6,361,596 5,457,848
3-grams 14,306,653 11,092,908
4-grams 19,668,956 11,514,626
</table>
<tableCaption confidence="0.999972">
Table 1: MWEs in News Corpora
</tableCaption>
<bodyText confidence="0.999847111111111">
phrasal verbs, which are desirable, are tagged dif-
ferently by the Tree Tagger, so there is no problem
with ambiguity here). Our filter captures this fact
by having a negative condition for the right edge of
the pattern (regular expression /_IN$/), rather than
enumerating all possible configurations which do
not contain a preposition at the end. In this sense
the filter is permissive: everything that is not ex-
plicitly forbidden is allowed, which makes the de-
scription more economical.
The same MWE database is used for check-
ing frequencies of multiword collocates for cor-
pus queries. For this task, candidate N-grams in
the vicinity of searched patterns are filtered us-
ing the same regular expression grammar of MWE
constraints, and then their corpus frequency is
checked in the database. Thus scores for mul-
tiword collocates can be computed from contin-
gency tables similarly to single-word collocates.
In addition, only MWEs with a frequency
higher than 1 are stored in the database. This fil-
ters out most expressions that co-occur by chance.
Table 1 gives an overview of the number of MWEs
from the news corpus which pass the filter. Other
corpora used in ASSIST (BNC and RRC) yield
similar results. MWE frequencies for each corpus
can be checked individually or joined together.
</bodyText>
<sectionHeader confidence="0.998414" genericHeader="introduction">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999979772727273">
There are several attributes of our system which
can be evaluated, and many of them are crucial
for its efficient use in the workflow of professional
translators, including: usability, quality of final so-
lutions, trade-off between adequacy and fluency
across usable examples, precision and recall of po-
tentially relevant suggestions, as well as real-text
evaluation, i.e. “What is the coverage of difficult
translation problems typically found in a text that
can be successfully tackled?”
In this paper we focus on evaluating the quality
of potentially relevant translation solutions, which
is the central point for developing and calibrat-
ing our methodology. The evaluation experiment
discussed below was specifically designed to as-
sess the usefulness of translation suggestions gen-
erated by our tool – in cases where translators
have doubts about the usefulness of dictionary so-
lutions. In this paper we do not evaluate other
equally important aspects of the system’s func-
tionality, which will be the matter of future re-
search.
</bodyText>
<subsectionHeader confidence="0.9985">
3.1 Set-up of the experiment
</subsectionHeader>
<bodyText confidence="0.999657775">
For each translation direction we collected ten ex-
amples of possibly recalcitrant translation prob-
lems – words or phrases whose translation is not
straightforward in a given context. Some of these
examples were sent to us by translators in response
to our request for difficult cases. For each exam-
ple, which we included in the evaluation kit, the
word or phrase either does not have a translation in
ORD (which is a kind of a baseline standard ref-
erence for Russian translators), or its translation
has significantly lower frequency in a target lan-
guage corpus in comparison to the frequency of
the source expression. If an MWE is not listed in
available dictionaries, we produced compositional
(word-for-word) translations using ORD. In order
to remove a possible anti-dictionary bias from our
experiment, we also checked translations in Mul-
titran, an on-line translation dictionary, which was
often quoted as one of the best resources for trans-
lation from and into Russian.
For each translation problem five solutions were
presented to translators for evaluation. One or two
of these solutions were taken from a dictionary
(usually from Multitran, and if available and dif-
ferent, from ORD). The other suggestions were
manually selected from lists of possible solutions
returned by ASSIST. Again, the criteria for se-
lection were intuitive: we included those sugges-
tions which made best sense in the given context.
Dictionary suggestions and the output of ASSIST
were indistinguishable in the questionnaires to the
evaluators. The segments were presented in sen-
tence context and translators had an option of pro-
viding their own solutions and comments. Ta-
ble 2 shows one of the questions sent to evalua-
tors. The problem example is -1eTKaa nporpamma
(‘precise programme’), which is presented in the
context of a Russian sentence with the following
(non-literal) translation This team should be put
together by responsible politicians, who have a
</bodyText>
<page confidence="0.530871">
742
</page>
<table confidence="0.99977775">
Problem example
÷åòêàÿ ïðîãðàììà, as in èç
Ñîáðàòü ýòó êîìàíäó äîëæíû îòâåòñòâåííûå
ëþäè✱ èìåþùèå ÷åòêóþ ïðîãðàììó âûõîäà
êðèçèñà✳
Translation suggestions Score
clear plan
clear policy
clear programme
clear strategy
concrete plan
Your suggestion ? (optional)
</table>
<tableCaption confidence="0.999536">
Table 2: Example of an entry in questionnaire
</tableCaption>
<bodyText confidence="0.76238408">
clear strategy for resolving the current crisis. The
third translation equivalent (clear programme) in
the table is found in the Multitran dictionary (ORD
offers no translation for ÷åòêàÿ ïðîãðàììà). The
example was included because clear programme
is much less frequent in English (2 examples in the
BNC) in comparison to ÷åòêàÿ ïðîãðàììà in Rus-
sian (70). Other translation equivalents in Table 2
are generated by ASSIST.
We then asked professional translators affiliated
to a translator’s association (identity witheld at this
stage) to rate these five potential equivalents using
a five-point scale:
5 = The suggestion is an appropriate translation
as it is.
4 = The suggestion can be used with some minor
amendment (e.g. by turning a verb into a par-
ticiple).
3 = The suggestion is useful as a hint for an-
other, appropriate translation (e.g. suggestion
elated cannot be used, but its close synonym
exhilarated can).
2 = The suggestion is not useful, even though it is
still in the same domain (e.g. fear is proposed
for a problem referring to hatred).
</bodyText>
<equation confidence="0.508229">
1 = The suggestion is totally irrelevant.
</equation>
<bodyText confidence="0.999831333333333">
We received responses from eight translators.
Some translators did not score all solutions, but
there were at least four independent judgements
for each of the 100 translation variants. An exam-
ple of the combined answer sheet for all responses
to the question from Table 2 is given in Table 3 (t1,
</bodyText>
<table confidence="0.97806675">
Translation t1 t2 t3 t4 t5 Q
clear plan 5 5 3 4 4 0.84
clear policy 5 5 3 4 4 0.84
clear programme 5 5 3 4 4 0.84
clear strategy 5 5 5 5 5 0.00
concrete plan 1 5 3 3 5 1.67
Best Dict 5 5 3 4 4 0.84
Best Syst 5 5 5 5 5 0.00
</table>
<tableCaption confidence="0.999283">
Table 3: Scores to translation equivalents
</tableCaption>
<bodyText confidence="0.971864">
t2,... denote translators; the dictionary translation
is clear programme).
</bodyText>
<subsectionHeader confidence="0.998887">
3.2 Interpretation of the results
</subsectionHeader>
<bodyText confidence="0.999970138888889">
The results were surprising in so far as for the ma-
jority of problems translators preferred very differ-
ent translation solutions and did not agree in their
scores for the same solutions. For instance, con-
crete plan in Table 3 received the score 1 from
translator t1 and 5 from t2.
In general, the translators very often picked up
on different opportunities presented by the sug-
gestions from the lists, and most suggestions were
equally legitimate ways of conveying the intended
content, cf. the study of legitimate translation vari-
ation with respect to the BLEU score in (Babych
and Hartley, 2004). In this respect it may be unfair
to compute average scores for each potential solu-
tion, since for most interesting cases the scores do
not fit into the normal distribution model. So aver-
aging scores would mask the potential usability of
really inventive solutions.
In this case it is more reasonable to evaluate
two sets of solutions – the one generated by AS-
SIST and the other found in dictionaries – but not
each solution individually. In order to do that for
each translation problem the best scores given by
each translator in each of these two sets were se-
lected. This way of generalising data characterises
the general quality of suggestion sets, and exactly
meets the needs of translators, who collectively get
ideas from the presented sets rather than from in-
dividual examples. This also allows us to mea-
sure inter-evaluator agreement on the dictionary
set and the ASSIST set, for instance, via computing
the standard deviation Q of absolute scores across
evaluators (Table 3). This appeared to be a very
informative measure for dictionary solutions.
In particular, standard deviation scores for the
dictionary set (threshold Q = 0.5) clearly split
</bodyText>
<page confidence="0.660757">
743
</page>
<table confidence="0.9992415">
Agreement: Q for dictionary &lt; 0.5
Example Dict ASSIST
Ave Q Ave Q
political upheaval 4.83 0.41 4.67 0.82
Disagreement: Q for dictionary &gt;0.5
Example Dict ASSIST
Ave Q Ave Q
clear defiance 4.14 0.90 4.60 0.55
</table>
<tableCaption confidence="0.78597">
Table 4: Examples for the two groups
</tableCaption>
<table confidence="0.99984975">
Agreement: Q for dictionary &lt; 0.5
Sub-group Dict ASSIST
Ave Q Ave Q
Agreement EAR 4.73 0.46 4.47 0.80
Agreement RAE 4.90 0.23 4.52 0.60
Agreement–All 4.81 0.34 4.49 0.70
Disagreement: Q for dictionary &gt;0.5
Sub-group Dict ASSIST
Ave Q Ave Q
Disagreement EAR 3.63 1.08 3.98 0.85
Disagreement RAE 3.90 1.02 3.96 0.73
Disagreement–All 3.77 1.05 3.97 0.79
</table>
<tableCaption confidence="0.991314">
Table 5: Averages for the two groups
</tableCaption>
<bodyText confidence="0.99977792">
our 20 problems into two distinct groups: the first
group below the threshold contains 8 examples,
for which translators typically agree on the qual-
ity of dictionary solutions; and the second group
above the threshold contains 12 examples, for
which there is less agreement. Table 4 shows some
examples from both groups and Table 5 presents
average evaluation scores and standard deviation
figures for both groups.
Overall performance on all 20 examples is the
same for the dictionary responses and for the sys-
tem’s responses: average of the mean top scores
is about 4.2 and average standard deviation of the
scores is 0.8 in both cases (for set-best responses).
This shows that ASSIST can reach the level of
performance of a combination of two authoritative
dictionaries for MWEs, while for its own transla-
tion step it uses just a subset of one-word transla-
tion equivalents from ORD. However, there is an-
other side to the evaluation experiment. In fact, we
are less interested in the system’s performance on
all of these examples than on those examples for
which there is greater disagreement among trans-
lators, i.e. where there is some degree of dissatis-
faction with dictionary suggestions.
</bodyText>
<figureCaption confidence="0.999191">
Figure 1: Agreement scores: dictionary
</figureCaption>
<bodyText confidence="0.999598513513513">
Interestingly, dictionary scores for the agree-
ment group are always higher than 4, which means
that whenever translators agreed on the dictionary
scores they were usually satisfied with the dictio-
nary solution. But they never agreed on the inap-
propriateness of the dictionary: inappropriateness
revealed itself in the form of low scores from some
translators.
This agreement/disagreement threshold can be
said to characterise two types of translation prob-
lems: those for which there exist generally ac-
cepted dictionary solutions, and those for which
translators doubt whether the solution is appropri-
ate. Best-set scores for these two groups of dic-
tionary solutions – the agreement and disagree-
ment group – are plotted on the radar charts in
Figures 1 and 2 respectively. The identifiers on
the charts are problematic source language expres-
sions as used in the questionnaire (not translation
solutions to these problems, because a problem
may have several solutions preferred by different
judges). Scores for both translation directions are
presented on the same chart, since both follow the
same pattern and receive the same interpretation.
Figure 1 shows that whenever there is little
doubt about the quality of dictionary solutions, the
radar chart approaches a circle shape near the edge
of the chart. In Figure 2 the picture is different:
the circle is disturbed, and some scores frequently
approach the centre. Therefore the disagreement
group contains those translation problems where
dictionaries provide little help.
The central problem in our evaluation experi-
ment is whether ASSIST is helpful for problems
in the second group, where translators doubt the
quality of dictionary solutions.
Firstly, it can be seen from the charts that judge-
</bodyText>
<page confidence="0.712847">
744
</page>
<figureCaption confidence="0.999545">
Figure 3: Disagreement scores: ASSIST
</figureCaption>
<bodyText confidence="0.999862518518518">
ments on the quality of the system output are more
consistent: score lines for system output are closer
to the circle shape in Figure 1 than those for dic-
tionary solutions in Figure 2 (formally: the stan-
dard deviation of evaluation scores, presented in
Table 4, is lower).
Secondly, as shown in Table 4, in this group av-
erage evaluation scores are slightly higher for AS-
SIST output than for dictionary solutions (3.97 vs
3.77) – in the eyes of human evaluators ASSIST
outperforms good dictionaries. For good dictio-
nary solutions ASSIST performance is slightly
lower: (4.49 vs 4.81), but the standard deviation
is about the same.
Having said this, solutions from our system are
really not in competition with dictionary solutions:
they provide less literal translations, which often
emerge in later stages of the translation task, when
translators correct and improve an initial draft,
where they have usually put more literal equiva-
lents (Shveitser, 1988). It is a known fact in trans-
lation studies that non-literal solutions are harder
to see and translators often find them only upon
longer reflection. Yet another fact is that non-
literal translations often require re-writing other
segments of the sentence, which may not be ob-
vious at first glance.
</bodyText>
<sectionHeader confidence="0.991574" genericHeader="conclusions">
4 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999827295454546">
The results of evaluation show that the tool is
successful in finding translation equivalents for a
range of examples. What is more, in cases where
the problem is genuinely difficult, ASSIST consis-
tently provides scores around 4 – “minor adapta-
tions needed”. The precision of the tool is low, it
suggests 50-100 examples with only 2-4 useful for
the current context. However, recall of the output
is more relevant than precision, because transla-
tors typically need just one solution for their prob-
lem, and often have to look through reasonably
large lists of dictionary translations and examples
to find something suitable for a problematic ex-
pression. Even if no immediately suitable trans-
lation can be found in the list of suggestions, it
frequently contains a hint for solving the problem
in the absence of adequate dictionary information.
The current implementation of the model is re-
stricted in several respects. First, the majority of
target language constructions mirror the syntactic
structure of the source language example. Even
if the procedure for producing similarity classes
does not impose restrictions on POS properties,
nevertheless words in the similarity class tend to
follow the POS of the original word, because of
the similarity of their contexts of use. Further-
more, dictionaries also tend to translate words
using the same POS. This means that the ex-
isting method finds mostly NPs for NPs, verb-
object pairs for verb-object pairs, etc, even if the
most natural translation uses a different syntactic
structure, e.g. I like doing X instead of I do X
gladly (when translating from German ich mache
X gerne).
Second, suggestions are generated for the query
expression independently from the context it is
used in. For instance, the words judicial, military
and religious are in the similarity class of politi-
cal, just as reform is in the simclass of upheaval.
So the following example
The plan will protect EC-based investors in Russia
from political upheavals damaging their business.
creates a list of “possible translations” evoking
various reforms and transformations.
</bodyText>
<figureCaption confidence="0.895331">
Figure 2: Disagreement scores: dictionary
</figureCaption>
<figure confidence="0.9809493">
daunting experience
passionately seek
negotiated settlement
clear defiance
recreational fear
государственное
строительство
due process
5
4
3
2
0
1
зачистка
экологичес
четк
по
вост
745
</figure>
<bodyText confidence="0.999949">
These issues can be addressed by introduc-
ing a model of the semantic context of situation,
e.g. ‘changes in business practice’ as in the ex-
ample above, or ‘unpleasant situation’ as in the
case of daunting experience. This will allow
less restrictive identification of possible transla-
tion equivalents, as well as reduction of sugges-
tions irrelevant for the context of the current ex-
ample.
Currently we are working on an option to iden-
tify semantic contexts by means of ‘semantic sig-
natures’ obtained from a broad-coverage seman-
tic parser, such as USAS (Rayson et al., 2004).
The semantic tagset used by USAS is a language-
independent multi-tier structure with 21 major dis-
course fields, subdivided into 232 sub-categories
(such as I1.1- = Money: lack; A5.1- = Eval-
uation: bad), which can be used to detect the
semantic context. Identification of semantically
similar situations can be also improved by the
use of segment-matching algorithms as employed
in Example-Based MT (EBMT) and translation
memories (Planas and Furuse, 2000; Carl and
Way, 2003).
The proposed model looks similar to some im-
plementations of statistical machine translation
(SMT), which typically uses a parallel corpus for
its translation model, and then finds the best possi-
ble recombination that fits into the target language
model (Och and Ney, 2003). Just like an MT sys-
tem, our tool can find translation equivalents for
queries which are not explicitly coded as entries
in system dictionaries. However, from the user
perspective it resembles a dynamic dictionary or
thesaurus: it translates difficult words and phrases,
not entire sentences. The main thrust of our sys-
tem is its ability to find translation equivalents for
difficult contexts where dictionary solutions do not
exist, are questionable or inappropriate.
</bodyText>
<sectionHeader confidence="0.992159" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.983894">
This research is supported by EPSRC grant
EP/C005902.
</bodyText>
<sectionHeader confidence="0.994015" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999760982142857">
Bogdan Babych and Anthony Hartley. 2004. Ex-
tending the BLEU MT evaluation method with fre-
quency weightings. In Proceedings of the 42d An-
nual Meeting of the Association for Computational
Linguistics, Barcelona.
Michael Carl and Andy Way, editors. 2003. Re-
cent advances in example-based machine transla-
tion. Kluwer, Dordrecht.
Ido Dagan and Kenneth Church. 1997. Ter-
might: Coordinating humans and machines in bilin-
gual terminology acquisition. Machine Translation,
12(1/2):89–107.
Gregory Grefenstette. 2002. Multilingual corpus-
based extraction and the very large lexicon. In Lars
Borin, editor, Language and Computers, Parallel
corpora, parallel worlds, pages 137–149. Rodopi.
John S. Justeson and Slava M. Katz. 1995. Techninal
terminology: some linguistic properties and an al-
gorithm for identification in text. Natural Language
Engineering, 1(1):9–27.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Joint COLING-ACL-98, pages
768–774, Montreal.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Alan Partington. 1998. Patterns and meanings: using
corpora for English language research and teach-
ing. John Benjamins, Amsterdam.
Emmanuel Planas and Osamu Furuse. 2000. Multi-
level similar segment matching algorithm for trans-
lation memories and example-based machine trans-
lation. In COLING, 18th International Conference
on Computational Linguistics, pages 621–627.
Reinhard Rapp. 2004. A freely available automatically
generated thesaurus of related words. In Proceed-
ings of the Forth Language Resources and Evalua-
tion Conference, LREC 2004, pages 395–398, Lis-
bon.
Paul Rayson, Dawn Archer, Scott Piao, and Tony
McEnery. 2004. The UCREL semantic analysis
system. In Proc. Beyond Named Entity Recognition
Workshop in association with LREC 2004, pages 7–
12, Lisbon.
Serge Sharoff. 2006. Creating general-purpose
corpora using automated search engine queries.
In Marco Baroni and Silvia Bernardini, editors,
WaCky! Working papers on the Web as Corpus.
Gedit, Bologna.
A.D. Shveitser. 1988. TeopHa ilepeBoga: CTaTyc, ilpo-
6.neMbI, acileKTbI. Nauka, Moskow. (In Russian:
Theory of Translation: Status, Problems, Aspects).
Krista Varantola. 2003. Translators and disposable
corpora. In Federico Zanettin, Silvia Bernardini,
and Dominic Stewart, editors, Corpora in Transla-
tor Education, pages 55–70. St Jerome, Manchester.
</reference>
<page confidence="0.92644">
746
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.689101">
<title confidence="0.98797">Using comparable corpora to solve problems difficult for human translators</title>
<author confidence="0.999631">Serge Sharoff</author>
<author confidence="0.999631">Bogdan Babych</author>
<author confidence="0.999631">Anthony Hartley</author>
<affiliation confidence="0.8854385">Centre for Translation Studies University of Leeds, LS2 9JT UK</affiliation>
<email confidence="0.995198">s.sharoff@leeds.ac.uk</email>
<email confidence="0.995198">b.babych@leeds.ac.uk</email>
<email confidence="0.995198">a.hartley@leeds.ac.uk</email>
<abstract confidence="0.994683066666667">In this paper we present a tool that uses comparable corpora to find appropriate translation equivalents for expressions that are considered by translators as difficult. For a phrase in the source language the tool identifies a range of possible expressions used in similar contexts in target language corpora and presents them to the translator as a list of suggestions. In the paper we discuss the method and present results of human evaluation of the performance of the tool, which highlight its usefulness when dictionary solutions are lacking.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bogdan Babych</author>
<author>Anthony Hartley</author>
</authors>
<title>Extending the BLEU MT evaluation method with frequency weightings.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42d Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="19977" citStr="Babych and Hartley, 2004" startWordPosition="3231" endWordPosition="3234">terpretation of the results The results were surprising in so far as for the majority of problems translators preferred very different translation solutions and did not agree in their scores for the same solutions. For instance, concrete plan in Table 3 received the score 1 from translator t1 and 5 from t2. In general, the translators very often picked up on different opportunities presented by the suggestions from the lists, and most suggestions were equally legitimate ways of conveying the intended content, cf. the study of legitimate translation variation with respect to the BLEU score in (Babych and Hartley, 2004). In this respect it may be unfair to compute average scores for each potential solution, since for most interesting cases the scores do not fit into the normal distribution model. So averaging scores would mask the potential usability of really inventive solutions. In this case it is more reasonable to evaluate two sets of solutions – the one generated by ASSIST and the other found in dictionaries – but not each solution individually. In order to do that for each translation problem the best scores given by each translator in each of these two sets were selected. This way of generalising data</context>
</contexts>
<marker>Babych, Hartley, 2004</marker>
<rawString>Bogdan Babych and Anthony Hartley. 2004. Extending the BLEU MT evaluation method with frequency weightings. In Proceedings of the 42d Annual Meeting of the Association for Computational Linguistics, Barcelona.</rawString>
</citation>
<citation valid="true">
<title>Recent advances in example-based machine translation.</title>
<date>2003</date>
<editor>Michael Carl and Andy Way, editors.</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<marker>2003</marker>
<rawString>Michael Carl and Andy Way, editors. 2003. Recent advances in example-based machine translation. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Kenneth Church</author>
</authors>
<title>Termight: Coordinating humans and machines in bilingual terminology acquisition.</title>
<date>1997</date>
<booktitle>Machine Translation,</booktitle>
<pages>12--1</pages>
<contexts>
<context position="2323" citStr="Dagan and Church, 1997" startWordPosition="354" endWordPosition="358">fails to mention many word combinations frequent in the British National Corpus (BNC), such as strong {feeling, field, opposition, sense, voice}. Strong voice is also not listed in the Oxford French, German or Spanish Dictionaries. There has been surprisingly little research on computational methods for finding translation equivalents of words from the general lexicon. Practically all previous studies have concerned detection of terminological equivalence. For instance, project Termight at AT&amp;T aimed to develop a tool for semi-automatic acquisition of termbanks in the computer science domain (Dagan and Church, 1997). There was also a study concerning the use of multilingual webpages to develop bilingual lexicons and termbanks (Grefenstette, 2002). However, neither of them concerned translations of words from the general lexicon. At the same time, translators often experience more difficulty in dealing with such general expressions because of their polysemy, which is reflected differently in the target language, thus causing the dependency of their translation on the corresponding context. Such variation is often not captured by dictionaries. Because of their importance, words from the general lexicon are</context>
</contexts>
<marker>Dagan, Church, 1997</marker>
<rawString>Ido Dagan and Kenneth Church. 1997. Termight: Coordinating humans and machines in bilingual terminology acquisition. Machine Translation, 12(1/2):89–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Multilingual corpusbased extraction and the very large lexicon.</title>
<date>2002</date>
<booktitle>Language and Computers, Parallel corpora, parallel worlds,</booktitle>
<pages>137--149</pages>
<editor>In Lars Borin, editor,</editor>
<publisher>Rodopi.</publisher>
<contexts>
<context position="2456" citStr="Grefenstette, 2002" startWordPosition="376" endWordPosition="378"> voice}. Strong voice is also not listed in the Oxford French, German or Spanish Dictionaries. There has been surprisingly little research on computational methods for finding translation equivalents of words from the general lexicon. Practically all previous studies have concerned detection of terminological equivalence. For instance, project Termight at AT&amp;T aimed to develop a tool for semi-automatic acquisition of termbanks in the computer science domain (Dagan and Church, 1997). There was also a study concerning the use of multilingual webpages to develop bilingual lexicons and termbanks (Grefenstette, 2002). However, neither of them concerned translations of words from the general lexicon. At the same time, translators often experience more difficulty in dealing with such general expressions because of their polysemy, which is reflected differently in the target language, thus causing the dependency of their translation on the corresponding context. Such variation is often not captured by dictionaries. Because of their importance, words from the general lexicon are studied by translation researchers, and comparable corpora are increasingly used in translation practice and training (Varantola, 20</context>
</contexts>
<marker>Grefenstette, 2002</marker>
<rawString>Gregory Grefenstette. 2002. Multilingual corpusbased extraction and the very large lexicon. In Lars Borin, editor, Language and Computers, Parallel corpora, parallel worlds, pages 137–149. Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Justeson</author>
<author>Slava M Katz</author>
</authors>
<title>Techninal terminology: some linguistic properties and an algorithm for identification in text.</title>
<date>1995</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="11981" citStr="Justeson and Katz, 1995" startWordPosition="1916" endWordPosition="1919">or instance, daunting experience has 202,594 combinations for the full translation class of daunting experience and 6,144 for the reduced one. However, in the target language corpora we can find only 2,256 collocations with frequency &gt; 2 for the full translation class and 92 for the reduced one. Each theoretically possible combination is generated and looked up in a database of MWEs (which is much faster than querying corpora for frequencies of potential collocations). The MWE database was pre-compiled from corpora using a method of filtering, similar to part-of-speech filtering suggested in (Justeson and Katz, 1995): in corpora each N-gram of length 2, 3 and 4 tokens was checked against a set of filters. However, instead of pre-defined patterns for entire expressions our filtering method uses sets of negative constraints, which are usually applied to the edges of expressions. This change boosts recall of retrieved MWEs and allows us to use the same set of patterns for MWEs of different length. The filter uses constraints for both lexical and part-of-speech features, which makes configuration specifications more flexible. The idea of applying a negative feature filter rather than a set of positive pattern</context>
</contexts>
<marker>Justeson, Katz, 1995</marker>
<rawString>John S. Justeson and Slava M. Katz. 1995. Techninal terminology: some linguistic properties and an algorithm for identification in text. Natural Language Engineering, 1(1):9–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Joint COLING-ACL-98,</booktitle>
<pages>768--774</pages>
<location>Montreal.</location>
<contexts>
<context position="7424" citStr="Lin, 1998" startWordPosition="1159" endWordPosition="1160">languages. Unlike aligned parallel corpora, comparable corpora provide a model for each individual language, while dictionaries, which can serve as a bridge, are inadequate for the task in question, because the problem we want to address involves precisely translation equivalents that are not listed there. Therefore, a specific query needs first to be generalised in order to then retrieve a suitable candidate from a set of candidates. One way to generalise the query is by using similarity classes, i.e. groups of words with lexically similar behaviour. In his work on distributional similarity (Lin, 1998) designed a parser to identify grammatical relationships between words. However, broad-coverage parsers suitable for processing BNC-like corpora are not available for many languages. Another, resource-light approach treats the context as a bag of words (BoW) and detects the similarity of contexts on the basis of collocations in a window of a certain size, typically 3-4 words, e.g. (Rapp, 2004). Even if using a parser can increase precision in identification of contexts in the case of long-distance dependencies (e.g. to cook Alice a whole meal), we can find a reasonable set of relevant terms re</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Joint COLING-ACL-98, pages 768–774, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Partington</author>
</authors>
<title>Patterns and meanings: using corpora for English language research and teaching. John Benjamins,</title>
<date>1998</date>
<location>Amsterdam.</location>
<contexts>
<context position="3302" citStr="Partington, 1998" startWordPosition="498" endWordPosition="499">ected differently in the target language, thus causing the dependency of their translation on the corresponding context. Such variation is often not captured by dictionaries. Because of their importance, words from the general lexicon are studied by translation researchers, and comparable corpora are increasingly used in translation practice and training (Varantola, 2003). However, such studies are mostly confined to lexicographic exercises, which compare the contexts and functions of potential translation equivalents once they are known, for instance, absolutely vs. assolutamente in Italian (Partington, 1998). Such studies do not provide a computational model for finding appropriate translation equivalents for expressions that are not listed or are inadequate in dictionaries. Parallel corpora, conisting of original texts and 739 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 739–746, Sydney, July 2006. c�2006 Association for Computational Linguistics their exact translations, provide a useful supplement to decontextualised translation equivalents listed in dictionaries. However, parallel corpora are not representative. Many of them are in the range of a few million words</context>
</contexts>
<marker>Partington, 1998</marker>
<rawString>Alan Partington. 1998. Patterns and meanings: using corpora for English language research and teaching. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Planas</author>
<author>Osamu Furuse</author>
</authors>
<title>Multilevel similar segment matching algorithm for translation memories and example-based machine translation.</title>
<date>2000</date>
<booktitle>In COLING, 18th International Conference on Computational Linguistics,</booktitle>
<pages>621--627</pages>
<contexts>
<context position="29426" citStr="Planas and Furuse, 2000" startWordPosition="4757" endWordPosition="4760"> working on an option to identify semantic contexts by means of ‘semantic signatures’ obtained from a broad-coverage semantic parser, such as USAS (Rayson et al., 2004). The semantic tagset used by USAS is a languageindependent multi-tier structure with 21 major discourse fields, subdivided into 232 sub-categories (such as I1.1- = Money: lack; A5.1- = Evaluation: bad), which can be used to detect the semantic context. Identification of semantically similar situations can be also improved by the use of segment-matching algorithms as employed in Example-Based MT (EBMT) and translation memories (Planas and Furuse, 2000; Carl and Way, 2003). The proposed model looks similar to some implementations of statistical machine translation (SMT), which typically uses a parallel corpus for its translation model, and then finds the best possible recombination that fits into the target language model (Och and Ney, 2003). Just like an MT system, our tool can find translation equivalents for queries which are not explicitly coded as entries in system dictionaries. However, from the user perspective it resembles a dynamic dictionary or thesaurus: it translates difficult words and phrases, not entire sentences. The main th</context>
</contexts>
<marker>Planas, Furuse, 2000</marker>
<rawString>Emmanuel Planas and Osamu Furuse. 2000. Multilevel similar segment matching algorithm for translation memories and example-based machine translation. In COLING, 18th International Conference on Computational Linguistics, pages 621–627.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>A freely available automatically generated thesaurus of related words.</title>
<date>2004</date>
<booktitle>In Proceedings of the Forth Language Resources and Evaluation Conference, LREC</booktitle>
<pages>395--398</pages>
<location>Lisbon.</location>
<contexts>
<context position="7820" citStr="Rapp, 2004" startWordPosition="1221" endWordPosition="1222">uitable candidate from a set of candidates. One way to generalise the query is by using similarity classes, i.e. groups of words with lexically similar behaviour. In his work on distributional similarity (Lin, 1998) designed a parser to identify grammatical relationships between words. However, broad-coverage parsers suitable for processing BNC-like corpora are not available for many languages. Another, resource-light approach treats the context as a bag of words (BoW) and detects the similarity of contexts on the basis of collocations in a window of a certain size, typically 3-4 words, e.g. (Rapp, 2004). Even if using a parser can increase precision in identification of contexts in the case of long-distance dependencies (e.g. to cook Alice a whole meal), we can find a reasonable set of relevant terms returned using the BoW approach, cf. the results of human evaluation for English and German by (Rapp, 2004). 740 For each source word s0 we produce a list of similar words: O(s0) = s1, ... , sN (in our tool we use N = 20 as the cutoff). Since lists of distributionally words can contain words irrelevant to the source word, we filter them to produce a more reliable similarity class 5(s0) using the</context>
</contexts>
<marker>Rapp, 2004</marker>
<rawString>Reinhard Rapp. 2004. A freely available automatically generated thesaurus of related words. In Proceedings of the Forth Language Resources and Evaluation Conference, LREC 2004, pages 395–398, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Rayson</author>
<author>Dawn Archer</author>
<author>Scott Piao</author>
<author>Tony McEnery</author>
</authors>
<title>The UCREL semantic analysis system.</title>
<date>2004</date>
<booktitle>In Proc. Beyond Named Entity Recognition Workshop in association with LREC</booktitle>
<pages>7--12</pages>
<location>Lisbon.</location>
<contexts>
<context position="28971" citStr="Rayson et al., 2004" startWordPosition="4687" endWordPosition="4690">4 3 2 0 1 зачистка экологичес четк по вост 745 These issues can be addressed by introducing a model of the semantic context of situation, e.g. ‘changes in business practice’ as in the example above, or ‘unpleasant situation’ as in the case of daunting experience. This will allow less restrictive identification of possible translation equivalents, as well as reduction of suggestions irrelevant for the context of the current example. Currently we are working on an option to identify semantic contexts by means of ‘semantic signatures’ obtained from a broad-coverage semantic parser, such as USAS (Rayson et al., 2004). The semantic tagset used by USAS is a languageindependent multi-tier structure with 21 major discourse fields, subdivided into 232 sub-categories (such as I1.1- = Money: lack; A5.1- = Evaluation: bad), which can be used to detect the semantic context. Identification of semantically similar situations can be also improved by the use of segment-matching algorithms as employed in Example-Based MT (EBMT) and translation memories (Planas and Furuse, 2000; Carl and Way, 2003). The proposed model looks similar to some implementations of statistical machine translation (SMT), which typically uses a </context>
</contexts>
<marker>Rayson, Archer, Piao, McEnery, 2004</marker>
<rawString>Paul Rayson, Dawn Archer, Scott Piao, and Tony McEnery. 2004. The UCREL semantic analysis system. In Proc. Beyond Named Entity Recognition Workshop in association with LREC 2004, pages 7– 12, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serge Sharoff</author>
</authors>
<title>Creating general-purpose corpora using automated search engine queries.</title>
<date>2006</date>
<booktitle>Working papers on the Web as Corpus. Gedit,</booktitle>
<editor>In Marco Baroni and Silvia Bernardini, editors, WaCky!</editor>
<location>Bologna.</location>
<contexts>
<context position="6667" citStr="Sharoff, 2006" startWordPosition="1035" endWordPosition="1036">s in the range of 100-200 million words (MW), so that they provide enough evidence to detect such collocations as strong voice and clear defiance. Although the current study is restricted to the English-Russian pair, the methodology does not rely on any particular language. It can be extended to other languages for which large comparable corpora, POS-tagging and lemmatisation tools, and bilingual dictionaries are available. For example, we conducted a small study for translation between English and German using the Oxford German Dictionary and a 200 MW German corpus derived from the Internet (Sharoff, 2006). 2.1 Query expansion The problem with using comparable corpora to find translation equivalents is that there is no obvious bridge between the two languages. Unlike aligned parallel corpora, comparable corpora provide a model for each individual language, while dictionaries, which can serve as a bridge, are inadequate for the task in question, because the problem we want to address involves precisely translation equivalents that are not listed there. Therefore, a specific query needs first to be generalised in order to then retrieve a suitable candidate from a set of candidates. One way to gen</context>
</contexts>
<marker>Sharoff, 2006</marker>
<rawString>Serge Sharoff. 2006. Creating general-purpose corpora using automated search engine queries. In Marco Baroni and Silvia Bernardini, editors, WaCky! Working papers on the Web as Corpus. Gedit, Bologna.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A D Shveitser</author>
</authors>
<title>TeopHa ilepeBoga: CTaTyc, ilpo6.neMbI, acileKTbI. Nauka, Moskow. (In Russian: Theory of Translation: Status,</title>
<date>1988</date>
<location>Problems, Aspects).</location>
<contexts>
<context position="25751" citStr="Shveitser, 1988" startWordPosition="4172" endWordPosition="4173">ores are slightly higher for ASSIST output than for dictionary solutions (3.97 vs 3.77) – in the eyes of human evaluators ASSIST outperforms good dictionaries. For good dictionary solutions ASSIST performance is slightly lower: (4.49 vs 4.81), but the standard deviation is about the same. Having said this, solutions from our system are really not in competition with dictionary solutions: they provide less literal translations, which often emerge in later stages of the translation task, when translators correct and improve an initial draft, where they have usually put more literal equivalents (Shveitser, 1988). It is a known fact in translation studies that non-literal solutions are harder to see and translators often find them only upon longer reflection. Yet another fact is that nonliteral translations often require re-writing other segments of the sentence, which may not be obvious at first glance. 4 Conclusions and future work The results of evaluation show that the tool is successful in finding translation equivalents for a range of examples. What is more, in cases where the problem is genuinely difficult, ASSIST consistently provides scores around 4 – “minor adaptations needed”. The precision</context>
</contexts>
<marker>Shveitser, 1988</marker>
<rawString>A.D. Shveitser. 1988. TeopHa ilepeBoga: CTaTyc, ilpo6.neMbI, acileKTbI. Nauka, Moskow. (In Russian: Theory of Translation: Status, Problems, Aspects).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krista Varantola</author>
</authors>
<title>Translators and disposable corpora.</title>
<date>2003</date>
<booktitle>Corpora in Translator Education,</booktitle>
<pages>55--70</pages>
<editor>In Federico Zanettin, Silvia Bernardini, and Dominic Stewart, editors,</editor>
<publisher>St</publisher>
<location>Jerome, Manchester.</location>
<contexts>
<context position="3059" citStr="Varantola, 2003" startWordPosition="466" endWordPosition="467">stette, 2002). However, neither of them concerned translations of words from the general lexicon. At the same time, translators often experience more difficulty in dealing with such general expressions because of their polysemy, which is reflected differently in the target language, thus causing the dependency of their translation on the corresponding context. Such variation is often not captured by dictionaries. Because of their importance, words from the general lexicon are studied by translation researchers, and comparable corpora are increasingly used in translation practice and training (Varantola, 2003). However, such studies are mostly confined to lexicographic exercises, which compare the contexts and functions of potential translation equivalents once they are known, for instance, absolutely vs. assolutamente in Italian (Partington, 1998). Such studies do not provide a computational model for finding appropriate translation equivalents for expressions that are not listed or are inadequate in dictionaries. Parallel corpora, conisting of original texts and 739 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 739–746, Sydney, July 2006. c�2006 Association for Computa</context>
</contexts>
<marker>Varantola, 2003</marker>
<rawString>Krista Varantola. 2003. Translators and disposable corpora. In Federico Zanettin, Silvia Bernardini, and Dominic Stewart, editors, Corpora in Translator Education, pages 55–70. St Jerome, Manchester.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>