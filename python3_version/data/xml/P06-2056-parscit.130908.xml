<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000100">
<note confidence="0.8151484">
UnsupervisedSegmentationofChineseText
byUseofBranchingEntropy
Zhihui Jin and Kumiko Tanaka-Ishii
Graduate School of Information Science and Technology
UniversityofTokyo
</note>
<sectionHeader confidence="0.655888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.992421867647059">
We propose an unsupervised segmen-
tation method based on an assumption
about language data: that the increas-
ing point of entropy of successive char-
acters is the location of a word bound-
ary. A large-scale experiment was con-
ducted by using 200 MB of unseg-
mented training data and 1 MB of test
data, and precision of 90% was attained
with recall being around 80%. More-
over, we found that the precision was
stable at around 90% independently of
the learning data size.
1Introduction
The theme of this paper is the following as-
sumption:
The uncertainty of tokens coming
after a sequence helps determine
whether a given position is at a
boundary. (A)
Intuitively, as illustrated in Figure 1, the vari-
ety of successive tokens at each character in-
side a word monotonically decreases according
to the offset length, because the longer the pre-
ceding character n-gram, the longer the pre-
ceding context and the more it restricts the
appearance of possible next tokens. For ex-
ample, it is easier to guess which character
comes after &amp;quot;natura&amp;quot; than after &amp;quot;na&amp;quot; . On the
other hand, the uncertainty at the position of
a word border becomes greater, and the com-
plexity increases, as the position is out of con-
text. With the same example, it is difficult to
guess which character comes after &amp;quot;natural &amp;quot;.
This suggests that a word border can be de-
tected by focusing on the differentials of the
uncertaintyofbranching.
In this paper, we report our study on ap-
plying this assumption to Chinese word seg-
Figure1:Intuitiveillustrationofavarietyof
successivetokensandawordboundary
mentation by formalizing the uncertainty of
successive tokens via the branching entropy
(which we mathematically define in the next
section). Our intention in this paper is above
all to study the fundamental and scientific sta-
tistical property underlying language data, so
that it can be applied to language engineering.
The above assumption (A) dates back to
the fundamental work done by Harris (Harris,
1955), where he says that when the number
of different tokens coming after every prefix of
a word marks the maximum value, then the
location corresponds to the morpheme bound-
ary. Recently, with the increasing availabil-
ity of corpora, this property underlying lan-
guage has been tested through segmentation
into words and morphemes. Kempe (Kempe,
1999) reports a preliminary experiment to de-
tect word borders in German and English texts
by monitoring the entropy of successive char-
acters for 4-grams. Also, the second author
of this paper (Tanaka-Ishii, 2005) have shown
how Japanese and Chinese can be segmented
into words by formalizing the uncertainty with
the branching entropy. Even though the test
data was limited to a small amount in this
work, the report suggested how assumption
</bodyText>
<page confidence="0.990738">
428
</page>
<note confidence="0.724698">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 428–435,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999915431372549">
(A) holds better when each of the sequence el-
ements forms a semantic unit. This motivated
our work to conduct a further, larger-scale test
in the Chinese language, which is the only hu-
man language consisting entirely of ideograms
(i.e., semantic units). In this sense, the choice
of Chinese as the language in our work is es-
sential.
If the assumption holds well, the most im-
portant and direct application is unsuper-
vised text segmentation into words. Many
works in unsupervised segmentation so far
could be interpreted as formulating assump-
tion (A) in a similar sense where branch-
ing stays low inside words but increases
at a word or morpheme border. None of
these works, however, is directly based on
(A), and they introduce other factors within
their overall methodologies. Some works are
based on in-word branching frequencies for-
mulated in an original evaluation function,
as in (Ando and Lee, 2000) (boundary pre-
cision =84.5%, recall= 78.0%, tested on 12500
Japanese ideogram words). Sun et al. (Sun
et al., 1998) uses mutual information (bound-
ary p=91.8%, no report for recall, 1588 Chi-
nese characters), and Feng(Feng et al., 2004)
incorporates branching counts in the evalua-
tion function to be optimized for obtaining
boundaries (word precision=76%, recall=78%,
2000 sentences). From the performance results
listed here, we can see that unsupervised seg-
mentation is more difficult, by far, than super-
vised segmentation; therefore, the algorithms
are complex, and previous studies have tended
to be limited in terms of both the test corpus
size and the target.
In contrast, as assumption (A) is simple, we
keep this simplicity in our formalization and
directly test the assumption on a large-scale
test corpus consisting of 1001 KB manually
segmented data with the training corpus con-
sisting of 200 MB of Chinese text.
Chinese is such an important language that
supervised segmentation methods are already
very mature. The current state-of-the-art seg-
mentation software developed by (Low et al.,
2005), which ranks as the best in the SIGHAN
bakeoff (Emerson, 2005), attains word preci-
sion and recall of 96.9% and 96.8%, respec-
tively, on the PKU track. There is also free
</bodyText>
<equation confidence="0.903466">
1 2 3 4 5 6 7 8
offset
</equation>
<bodyText confidence="0.976639647058824">
Figure2:DecreaseinH(XjXn)forChinese
characterswhennisincreased
softwaresuchas(Zhangetal.,2003)whose
performance is also high. Even then, as most
supervised methods learn on manually seg-
mented newspaper data, when the input text
is not from newspapers, the performance can
be insufficient. Given that the construction of
learning data is costly, we believe the perfor-
mance can be raised by combining the super-
vised and unsupervised methods.
Consequently, this paper verifies assump-
tion (A) in a fundamental manner for Chinese
text and addresses the questions of why and to
what extent (A) holds, when applying it to the
Chinese word segmentation problem. We first
formalize assumption (A) in a general manner.
</bodyText>
<sectionHeader confidence="0.903747" genericHeader="method">
2 The Assumption
</sectionHeader>
<bodyText confidence="0.999729777777778">
Given a set of elements X and a set of n-gram
sequences Xn formed of x, the conditional en-
tropy of an element occurring after an n-gram
sequence Xn is defined as
whereP(x)=P(X=x),P(xjxn)=P(X=
xjXn = xn), and P(X = x) indicates the prob-
ability of occurrence of x.
A well-known observation on language data
states that H(X jXn) d ecreases as n increases
(Bell et al., 1990). For example, Figure 2
shows how H(X jXn) shifts when n increases
from 1 to 8 characters, where n is the length of
a word prefix. This is calculated for all words
existing in the test corpus, with the entropy
being measured in the learning data (the learn-
ing and test data are defined in x4).
This phenomenon indicates that X will be-
come easier to estimate as the context of Xn
</bodyText>
<figure confidence="0.998564153846154">
entropy 5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
H(XjXn)=
�Y. P(xn)Y.xEx P(xjxn)logP(xjxn),(1)
xnGxn
</figure>
<page confidence="0.990202">
429
</page>
<bodyText confidence="0.992764">
getslonger.Thiscanbeintuitivelyunder-
stood:itiseasytoguessthat&amp;quot;e&amp;quot;willfollow
after &amp;quot;Hello! How ar&amp;quot; , but it is difficult to
guess what comes after the short string &amp;quot;He&amp;quot;.
The last term —log P(xjxn) in the above for-
mula indicates the information of a token of x
coming after xn, and thus the branching after
xn. The latter half of the formula, the local
entropy value for a given xn,
</bodyText>
<equation confidence="0.6265775">
H(XjXn=xn)=�X P(xjxn)logP(xjxn),
xEx
</equation>
<bodyText confidence="0.997207">
(2)
indicates the average information of branching
for a specific n-gram sequence xn. As our in-
terest in this paper is this local entropy, we
denoteH(XjXn=xn)simplyash(xn)inthe
rest of this paper.
The decrease in H(X jXn) globally indicates
that given an n-length sequence xn and an-
other (n+1)-length sequence gn+1, the follow-
ing inequality holds on average:
</bodyText>
<equation confidence="0.776004">
h(xn)&gt;h(gn+1). (3)
</equation>
<bodyText confidence="0.9998738">
One reason why inequality (3) holds for lan-
guage data is that there is context in language,
and gn+1 carries a longer context as compared
with xn. Therefore, if we suppose that xn is
the prefix of xn+1, then it is very likely that
</bodyText>
<equation confidence="0.979806">
h(xn) &gt; h(xn+1) (4)
</equation>
<bodyText confidence="0.999834181818182">
holds, because the longer the preceding n-
gram, the longer the same context. For ex-
ample, it is easier to guess what comes af-
ter x6=&amp;quot;natura&amp;quot; than what comes after x5 =
&amp;quot;natur&amp;quot;. Therefore, the decrease in H(X jXn)
can be expressed as the concept that if the con-
text is longer, the uncertainty of the branching
decreases on average. Then, taking the logical
contraposition, if the uncertainty does not de-
crease, the context is not longer, which can be
interpreted as the following:
If the entropy of successive tokens in-
creases, the location is at a context
border. (B)
For example, in the case of x7 = &amp;quot;natu-
ral&amp;quot;, the entropy h(&amp;quot;natural&amp;quot;) should be larger
than h(&amp;quot;natura&amp;quot;), because it is uncertain what
character will allow x7 to succeed. In the next
section, we utilize assumption (B) to detect
context boundaries.
Figure3:Ourmodelforboundarydetection
basedontheentropyofbranching
</bodyText>
<sectionHeader confidence="0.6536185" genericHeader="method">
3 Boundary Detection Using the
Entropy of Branching
</sectionHeader>
<bodyText confidence="0.9999686">
Assumption (B) gives a hint on how to utilize
the branching entropy as an indicator of the
context boundary. When two semantic units,
both longer than 1, are put together, the en-
tropy would appear as in the first figure of Fig-
ure 3. The first semantic unit is from offsets
0 to 4, and the second is from 4 to 8, with
each unit formed by elements of X. In the fig-
ure, one possible transition of the branching
degree is shown, where the plot at k on the
horizontal axis denotes the entropy for h(x0;k)
and xn;m denotes the substring between offsets
n and m.
Ideally, the entropy would take a maximum
at 4, because it will decrease as k is increased
in the ranges of k &lt; 4 and 4 &lt; k &lt; 8, and
at k = 4, it will rise. Therefore, the position
at k = 4 is detected as the &amp;quot;local maximum
value&amp;quot; when monitoring h(x0;k) over k. The
boundary condition after such observation can
be redefined as the following:
Bmax Boundaries are locations where the en-
tropy is locally maximized.
A similar method is proposed by Harris (Har-
ris, 1955), where morpheme borders can be
detected by using the local maximum of the
number of different tokens coming after a pre-
fix.
This only holds, however, for semantic units
longer than 1. Units often have a length of
</bodyText>
<page confidence="0.989227">
430
</page>
<bodyText confidence="0.999785384615385">
1, especially in our case with Chinese charac-
ters as elements, so that there are many one-
character words. If a unit has length 1, then
the situation will look like the second graph
in Figure 3, where three semantic units, x0;4,
x4;5, and x5;8, are present, with the middle
unit having length 1. First, at k = 4, the
value of h increases. At k = 5, the value may
increase or decrease, because the longer con-
text results in an uncertainty decrease, though
an uncertainty decrease does not necessarily
mean a longer context. When h increases at
k = 5, the situation will look like the second
graph. In this case, the condition Bmax will
not suffice, and we need a second boundary
condition:
Bincrease Boundaries are locations where the
entropy is increased.
On the other hand, when h decreases at k = 5,
then even Bincrease cannot be applied to detect
k = 5 as a boundary. We have other chances to
detect k = 5, however, by considering h(xi;k),
where 0 &lt; i &lt; k. According to inequality
(3), then, a similar trend should be present
for plots of h(xi;k), assuming that h(x0;n) &gt;
h(x0;n+1); then, we have
</bodyText>
<equation confidence="0.586022">
h(xi;n)&gt;h(xi;n+1),for0&lt;i&lt;n. (5)
</equation>
<bodyText confidence="0.9898313">
The value h(xi;k) would hopefully rise for some
i if the boundary at k = 5 is important,
although h(xi;k) can increase or decrease at
k = 5, just as in the case for h(x0;n).
Therefore, when the target language con-
sists of many one-element units, Bincrease is
crucial for collecting all boundaries. Note that
the boundaries detected by Bmax are included
in those detected by the condition Bincrease,
and also that Bincrease is a boundary condition
representing the assumption (B) more directly.
So far, we have considered only regular-
order processing: the branching degree is cal-
culated for successive elements of xn. We can
also consider the reverse order, which involves
calculating h for the previous element of xn. In
the case of the previous element, the question
is whether the head of xn forms the beginning
of a context boundary.
Next, we move on to explain how we ac-
tually applied the above formalization to the
problem of Chinese segmentation.
4Data
The whole data for training amounted to 200
MB, from the Contemporary Chinese Cor-
pus of the Center of Chinese Linguistics at
Peking University (Center for Chinese Linguis-
tics, 2006). It consists of several years of Peo-
ples&apos; Daily newspapers, contemporary Chinese
literature, and some popular Chinese maga-
zines. Note that as our method is unsuper-
vised, this learning corpus is just text without
any segmentation.
The test data were constructed by selecting
sentences from the manually segmented Peo-
ple&apos;s Daily corpus of Peking University. In to-
tal, the test data amounts to 1001 KB, consist-
ing 147026 Chinese words. The word bound-
aries indicated in the corpus were used as our
golden standard.
As punctuation is clear from text bound-
aries in Chinese text, we pre-processed the test
data by segmenting sentences at punctuation
locations to form text fragments. Then, from
all fragments, n-grams of less than 6 charac-
ters were obtained. The branching entropies
for all these n-grams existing within the test
data were obtained from the 200 MB of data.
We used 6 as the maximum n-gram length
because Chinese words with a length of more
than 5 characters are rare. Therefore, scan-
ning the n-grams up to a length of 6 was suffi-
cient. Another reason is that we actually con-
ducted the experiment up to 8-grams, but the
performance did not improve from when we
used 6-grams.
Using this list of words ranging from un-
igrams to 6-grams and their branching en-
tropies, the test data were processed so as to
obtain the word boundaries.
</bodyText>
<sectionHeader confidence="0.986078" genericHeader="method">
5 Analysis for Small Examples
</sectionHeader>
<bodyText confidence="0.997821909090909">
Figure 4 shows an actual graph of
the entropy shift for the input phrase
(wei lai fa zhan
de mu biao he zhi dao fang zhen, the aim and
guideline of future development). The upper
figure shows the entropy shift for the forward
case, and the lower figure shows the entropy
shift for the backward case. Note that for the
backward case, the branching entropy was
calculated for characters before the xn.
In the upper figure, there are two lines, one
</bodyText>
<page confidence="0.99679">
431
</page>
<bodyText confidence="0.996191377777778">
Figure4:Entropyshiftforasmallexample
(forwardandbackward)
forthebranchingentropyafterthesubstrings
startingfrom.Theleftmostlineplots
h(),h( )...h( ). There
are two increasing points, indicating that the
phrase was segmented between and
and between and.The second line
plots h( ) ... h( ). The increas-
ing locations are between and , be-
tween and , and after
The lower figure is the same. There are two
lines, one for the branching entropy before the
substring ending with suffix . The rightmost
line plots h( ), h( ) ... h(
running from back to front. We can see in-
creasing points (as seen from back to front) be-
tween and , and between and
As for the last line, it also starts from and
runs from back to front, indicating boundaries
between and between and
and just before
If we consider all the increasing points in all
four lines and take the set union of them, we
obtain the correct segmentation as follows: .
which is the 100 % correct segmentation in
terms of both recall and precision.
In fact, as there are 12 characters in this
input, there should be 12 lines starting from
each character for all substrings. For read-
ability, however, we only show two lines each
for the forward and backward cases. Also, the
maximum length of a line is 6, because we only
took 6-grams out of the learning data. If we
consider all the increasing points in all 12 lines
and take the set union, then we again obtain
100 % precision and recall. It is amazing how
all 12 lines indicate only correct word bound-
aries.
Also, note how the correct full segmenta-
tion is obtained only with partial information
from 4 lines taken from the 12 lines. Based
on this observation, we next explain the algo-
rithm that we used for a larger-scale experi-
ment.
</bodyText>
<sectionHeader confidence="0.992897" genericHeader="conclusions">
6 Algorithm for Segmentation
</sectionHeader>
<bodyText confidence="0.987062878787879">
Having determined the entropy for all n-grams
in the learning data, we could scan through
each chunk of test data in both the forward
order and the backward order to determine the
locations of segmentation.
As our intention in this paper is above all to
study the innate linguistic structure described
by assumption (B), we do not want to add any
artifacts other than this assumption. For such
exact verification, we have to scan through all
possible substrings of an input, which amounts
to O(n2) computational complexity, where n
indicates the input length of characters.
Usually, however, h(xm,n) becomes impos-
sible to measure when n — m becomes large.
Also, as noted in the previous section, words
longer than 6 characters are very rare in Chi-
nese text. Therefore, given a string x, all n-
grams of no more than 6 grams are scanned,
and the points where the boundary condition
holds are output as boundaries.
As for the boundary conditions, we have
Bmax and Bincrease, and we also utilize
Bordinary, where location n is considered as a
boundary when the branching entropy h(xn)
is simply above a given threshold. Precisely,
there are three boundary conditions:
$max h(xn) &gt; valmax,
where h(xn) takes a local maximum,
$increase h(xn+1) — h(xn) &gt; valdelta,
$ordinary h(xn) &gt; val,
where valmax, valdelta, and val are arbitrary
thresholds.
</bodyText>
<page confidence="0.994322">
432
</page>
<bodyText confidence="0.987764607142857">
recall
7Large-ScaleExperiments
7.1DefinitionofPrecisionandRecall
Usually, when precision and recall are ad-
dressed in the Chinese word segmentation do-
main, they are calculated based on the number
of words. For example, consider a correctly
segmented sequence &amp;quot;aaajbbbjcccjddd&amp;quot;, with
a,b,c,d being characters and &amp;quot;j&amp;quot; indicating a
word boundary. Suppose that the machine&apos;s
result is &amp;quot;aaabbbjcccjddd&amp;quot;; then the correct
words are only &amp;quot;ccc&amp;quot; and &amp;quot;ddd&amp;quot; , giving a value
of 2. Therefore, the precision is 2 divided
by the number of words in the results (i.e., 3
for the words &amp;quot;aaabbb&amp;quot; , &amp;quot;ccc&amp;quot; , &amp;quot;ddd&amp;quot;) , giving
67%, and the recall is 2 divided by the total
number of words in the golden standard (i.e., 4
for the words &amp;quot;aaa&amp;quot; , &amp;quot;bbb&amp;quot; , &amp;quot;ccc&amp;quot; , &amp;quot;ddd&amp;quot;) giv-
ing 50%. We call these values the word pre-
cision and recall, respectively, throughout this
paper.
In our case, we use slightly different mea-
sures for the boundary precision and recall,
which are based on the correct number of
boundaries. These scores are also utilized espe-
cially in previous works on unsupervised seg-
mentation (Ando and Lee, 2000) (Sun et al.,
1998). Precisely,
</bodyText>
<equation confidence="0.84981125">
Precision=Ncorrect
Ntest
Recall = Ncorrect where (7)
Ntrue
</equation>
<bodyText confidence="0.993571298245614">
Ncorrect is the number of correct boundaries in
the result,
Ntest is the number of boundaries in the test
result, and,
Ntrue is the number of boundaries in the
golden standard.
For example, in the case of the machine result
being &amp;quot;aaabbbjcccjddd&amp;quot;, the precision is 100%
and the recall is 75%. Thus, we consider there
to be no imprecise result as a boundary in the
output of &amp;quot;aaabbbjcccjddd&amp;quot;.
The crucial reason for using the boundary
precision and recall is that boundary detec-
tion and word extraction are not exactly the
same task. In this sense, assumption (A) or
(B) is a general assumption about a bound-
ary (of a sentence, phrase, word, morpheme).
Therefore, the boundary precision and recall
Figure5:Precisionandrecall
measure serves for directly measuring bound-
aries.
Note that all precision and recall scores from
now on in this paper are boundary precision
and recall. Even in comparing the super-
vised methods with our unsupervised method
later, the precision and recall values are all re-
calculated as boundary precision and recall.
7.2PrecisionandRecall
The precision and recall graph is shown in Fig-
ure 5. The horizontal axis is the precision
and the vertical axis is the recall. The three
lines from right to left (top to bottom) cor-
respond to Bincrease (0.0 &lt; valdelta &lt; 2.4),
Bmax (4.0 &lt; valmax &lt; 6.2), and Bordinary
(4.0 &lt; val &lt; 6.2). All are plotted with an
interval of 0.1. For every condition, the larger
the threshold, the higher the precision and the
lower the recall.
We can see how Bincrease and Bmax keep
high precision as compared with Bordinary . We
also can see that the boundary can be more
easily detected if it is judged as comprising
the proximity value of h(xn).
For Bincrease, in particular, when valdelta =
0.0, the precision and recall are still at 0.88 and
0.79, respectively. Upon increasing the thresh-
old to valdelta = 2.4, the precision is higher
than 0.96 at the cost of a low recall of 0.29. As
for Bmax, we also observe a similar tendency
but with low recall due to the smaller number
of local maximum points as compared with the
number of increasing points. Thus, we see how
Bincrease attains a better performance among
the three conditions. This shows the correct-
ness of assumption (B).
From now on, we consider only Bincrease and
proceed through our other experiments.
</bodyText>
<figure confidence="0.999226176470588">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
precision
Bincrease
Bordinary
Bmax
(6)
</figure>
<page confidence="0.997937">
433
</page>
<bodyText confidence="0.996851095238095">
Figure6:Precisionandrecalldependingon
trainingdatasize
Next, we investigated how the training data
size affects the precision and recall. This time,
the horizontal axis is the amount of learning
data, varying from 10 KB up to 200 MB, on
a log scale. The vertical axis shows the pre-
cision and recall. The boundary condition is
Bzncrease with valdelta = 0.1.
We can see how the precision always re-
mains high, whereas the recall depends on the
amount of data. The precision is stable at an
amazingly high value, even when the branch-
ing entropy is obtained from a very small cor-
pus of 10 KB. Also, the linear increase in the
recall suggests that if we had more than 200
MB of data, we would expect to have an even
higher recall. As the horizontal axis is in a log
scale, however, we would have to have giga-
bytes of data to achieve the last several per-
cent of recall.
</bodyText>
<subsectionHeader confidence="0.741427">
7.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.997984">
According to our manual error analysis, the
top-most three errors were the following:
</bodyText>
<listItem confidence="0.991233142857143">
• Numbers: dates, years, quantities (ex-
ample: 1998, written in Chinese number
characters)
• One-character words (example: (at)
(again) (toward) (and))
• Compound Chinese words (example:
(openmind)beingsegmented
</listItem>
<bodyText confidence="0.984350296296297">
(mind))
The reason for the bad results with numbers
is probably because the branching entropy for
digits is less biased than for usual ideograms.
Also, for one-character words, our method is
limited, as we explained in x3. Both of these
two problems, however, can be solved by ap-
plying special preprocessing for numbers and
one-character words, given that many of the
one-character words are functional characters,
which are limited in number. Such improve-
ments remain for our future work.
The third error type, in fact, is one that
could be judged as correct segmentation. In
the case of &amp;quot;open mind&amp;quot;, it was not segmented
into two words in the golden standard; there-
fore, our result was judged as incorrect. This
could, however, be judged as correct.
The structures of Chinese words and phrases
are very similar, and there are no clear crite-
ria for distinguishing between a word and a
phrase. The unsupervised method determines
the structure and segments words and phrases
into smaller pieces. Manual recalculation of
the accuracy comprising such cases also re-
mains for our future work.
8Conclusion
We have reported an unsupervised Chinese
segmentation method based on the branching
entropy. This method is based on an assump-
tion that &amp;quot;if the entropy of successive tokens
increases, the location is at the context bor-
der.&amp;quot; The entropies of n-grams were learned
from an unsegmented 200-MB corpus, and the
actual segmentation was conducted directly
according to the above assumption, on 1 MB
of test data. We found that the precision was
as high as 90% with recall being around 80%.
We also found an amazing tendency for the
precision to always remain high, regardless of
the size of the learning data.
There are two important considerations for
our future work. The first is to figure out how
to combine the supervised and unsupervised
methods. In particular, as the performance of
the supervised methods could be insufficient
for data that are not from newspapers, there
is the possibility of combining the supervised
and unsupervised methods to achieve a higher
accuracy for general data. The second future
work is to verify our basic assumption in other
languages. In particular, we should undertake
experimental studies in languages written with
phonogramcharacters.
</bodyText>
<figure confidence="0.998551">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10 100 1000 10000 100000 1e+06
size(KB)
recall
precision
into(open)and
</figure>
<page confidence="0.99487">
434
</page>
<sectionHeader confidence="0.95117" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999671611111111">
R.K. Ando and L. Lee. 2000. Mostly-unsupervised
statistical segmentation of Japanese: Applica-
tions to kanji. In ANLP-NAACL.
T.C. Bell, J.G. Cleary, and Witten. I.H. 1990. Text
Compression. Prentice Hall.
Center for Chinese Linguistics. 2006. Chi-
nese corpus. visited 2006, searchable from
http://ccl.pku.edu.cn/YuLiao Contents. Asp,
part of it freely available from
http://www.icl.pku.edu.cn.
T. Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In SIGHAN.
H.D. Feng, K. Chen, C.Y. Kit, and Deng. X.T.
2004. Unsupervised segmentation of chinese cor-
pus using accessor variety. In IJCNLP, pages
255{261.
S.Z. Harris. 1955. From phoneme to morpheme.
Language, pages 190-222.
A. Kempe. 1999. Experiments in unsupervised
entropy-based corpus segmentation. In Work-
shop of EACL in Computational Natural Lan-
guage Learning, pages 7-13.
J.K. Low, H.T Ng, and W. Guo. 2005. A maxi-
mum entropy approach to chinese word segmen-
tation. In SIGHAN.
M. Sun, D. Shen, and B. K. Tsou. 1998. Chi-
nese word segmentation without using lexicon
and hand-crafted training data. In COLING-
ACL.
K. Tanaka-Ishii. 2005. Entropy as an indicator of
context boundaries an experiment using a web
search engine . In IJCNLP, pages 93-105.
H.P. Zhang, Yu H.Y., Xiong D.Y., and Q Liu.
2003. Hhmm-based chinese lexical analyzer ict-
clas. In SIGHAN. visited 2006, available from
http://www.nlp.org.cn.
</reference>
<page confidence="0.999208">
435
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.365712">
<title confidence="0.666872">UnsupervisedSegmentationofChineseText byUseofBranchingEntropy</title>
<author confidence="0.99485">Zhihui Jin</author>
<author confidence="0.99485">Kumiko Tanaka-Ishii</author>
<affiliation confidence="0.889063">Graduate School of Information Science and Technology UniversityofTokyo</affiliation>
<abstract confidence="0.9995435">We propose an unsupervised segmentation method based on an assumption about language data: that the increasing point of entropy of successive characters is the location of a word boundary. A large-scale experiment was conducted by using 200 MB of unsegmented training data and 1 MB of test data, and precision of 90% was attained with recall being around 80%. Moreover, we found that the precision was stable at around 90% independently of the learning data size.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R K Ando</author>
<author>L Lee</author>
</authors>
<title>Mostly-unsupervised statistical segmentation of Japanese: Applications to kanji.</title>
<date>2000</date>
<booktitle>In ANLP-NAACL.</booktitle>
<contexts>
<context position="3982" citStr="Ando and Lee, 2000" startWordPosition="633" endWordPosition="636">hinese as the language in our work is essential. If the assumption holds well, the most important and direct application is unsupervised text segmentation into words. Many works in unsupervised segmentation so far could be interpreted as formulating assumption (A) in a similar sense where branching stays low inside words but increases at a word or morpheme border. None of these works, however, is directly based on (A), and they introduce other factors within their overall methodologies. Some works are based on in-word branching frequencies formulated in an original evaluation function, as in (Ando and Lee, 2000) (boundary precision =84.5%, recall= 78.0%, tested on 12500 Japanese ideogram words). Sun et al. (Sun et al., 1998) uses mutual information (boundary p=91.8%, no report for recall, 1588 Chinese characters), and Feng(Feng et al., 2004) incorporates branching counts in the evaluation function to be optimized for obtaining boundaries (word precision=76%, recall=78%, 2000 sentences). From the performance results listed here, we can see that unsupervised segmentation is more difficult, by far, than supervised segmentation; therefore, the algorithms are complex, and previous studies have tended to b</context>
<context position="18348" citStr="Ando and Lee, 2000" startWordPosition="3087" endWordPosition="3090">efore, the precision is 2 divided by the number of words in the results (i.e., 3 for the words &amp;quot;aaabbb&amp;quot; , &amp;quot;ccc&amp;quot; , &amp;quot;ddd&amp;quot;) , giving 67%, and the recall is 2 divided by the total number of words in the golden standard (i.e., 4 for the words &amp;quot;aaa&amp;quot; , &amp;quot;bbb&amp;quot; , &amp;quot;ccc&amp;quot; , &amp;quot;ddd&amp;quot;) giving 50%. We call these values the word precision and recall, respectively, throughout this paper. In our case, we use slightly different measures for the boundary precision and recall, which are based on the correct number of boundaries. These scores are also utilized especially in previous works on unsupervised segmentation (Ando and Lee, 2000) (Sun et al., 1998). Precisely, Precision=Ncorrect Ntest Recall = Ncorrect where (7) Ntrue Ncorrect is the number of correct boundaries in the result, Ntest is the number of boundaries in the test result, and, Ntrue is the number of boundaries in the golden standard. For example, in the case of the machine result being &amp;quot;aaabbbjcccjddd&amp;quot;, the precision is 100% and the recall is 75%. Thus, we consider there to be no imprecise result as a boundary in the output of &amp;quot;aaabbbjcccjddd&amp;quot;. The crucial reason for using the boundary precision and recall is that boundary detection and word extraction are not</context>
</contexts>
<marker>Ando, Lee, 2000</marker>
<rawString>R.K. Ando and L. Lee. 2000. Mostly-unsupervised statistical segmentation of Japanese: Applications to kanji. In ANLP-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H</author>
</authors>
<title>Text Compression.</title>
<date>1990</date>
<publisher>Prentice Hall.</publisher>
<marker>H, 1990</marker>
<rawString>T.C. Bell, J.G. Cleary, and Witten. I.H. 1990. Text Compression. Prentice Hall.</rawString>
</citation>
<citation valid="false">
<date>2006</date>
<publisher>Chi-</publisher>
<institution>Center for Chinese Linguistics.</institution>
<marker>2006</marker>
<rawString>Center for Chinese Linguistics. 2006. Chi-</rawString>
</citation>
<citation valid="false">
<authors>
<author>nese corpus</author>
</authors>
<title>visited 2006, searchable from http://ccl.pku.edu.cn/YuLiao Contents. Asp, part of it freely available from http://www.icl.pku.edu.cn.</title>
<marker>corpus, </marker>
<rawString>nese corpus. visited 2006, searchable from http://ccl.pku.edu.cn/YuLiao Contents. Asp, part of it freely available from http://www.icl.pku.edu.cn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Emerson</author>
</authors>
<title>The second international chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In SIGHAN.</booktitle>
<contexts>
<context position="5149" citStr="Emerson, 2005" startWordPosition="818" endWordPosition="819"> complex, and previous studies have tended to be limited in terms of both the test corpus size and the target. In contrast, as assumption (A) is simple, we keep this simplicity in our formalization and directly test the assumption on a large-scale test corpus consisting of 1001 KB manually segmented data with the training corpus consisting of 200 MB of Chinese text. Chinese is such an important language that supervised segmentation methods are already very mature. The current state-of-the-art segmentation software developed by (Low et al., 2005), which ranks as the best in the SIGHAN bakeoff (Emerson, 2005), attains word precision and recall of 96.9% and 96.8%, respectively, on the PKU track. There is also free 1 2 3 4 5 6 7 8 offset Figure2:DecreaseinH(XjXn)forChinese characterswhennisincreased softwaresuchas(Zhangetal.,2003)whose performance is also high. Even then, as most supervised methods learn on manually segmented newspaper data, when the input text is not from newspapers, the performance can be insufficient. Given that the construction of learning data is costly, we believe the performance can be raised by combining the supervised and unsupervised methods. Consequently, this paper verif</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>T. Emerson. 2005. The second international chinese word segmentation bakeoff. In SIGHAN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X T</author>
</authors>
<title>Unsupervised segmentation of chinese corpus using accessor variety.</title>
<date>2004</date>
<booktitle>In IJCNLP,</booktitle>
<pages>255--261</pages>
<marker>T, 2004</marker>
<rawString>H.D. Feng, K. Chen, C.Y. Kit, and Deng. X.T. 2004. Unsupervised segmentation of chinese corpus using accessor variety. In IJCNLP, pages 255{261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Z Harris</author>
</authors>
<title>From phoneme to morpheme. Language,</title>
<date>1955</date>
<pages>190--222</pages>
<contexts>
<context position="2142" citStr="Harris, 1955" startWordPosition="338" endWordPosition="339"> differentials of the uncertaintyofbranching. In this paper, we report our study on applying this assumption to Chinese word segFigure1:Intuitiveillustrationofavarietyof successivetokensandawordboundary mentation by formalizing the uncertainty of successive tokens via the branching entropy (which we mathematically define in the next section). Our intention in this paper is above all to study the fundamental and scientific statistical property underlying language data, so that it can be applied to language engineering. The above assumption (A) dates back to the fundamental work done by Harris (Harris, 1955), where he says that when the number of different tokens coming after every prefix of a word marks the maximum value, then the location corresponds to the morpheme boundary. Recently, with the increasing availability of corpora, this property underlying language has been tested through segmentation into words and morphemes. Kempe (Kempe, 1999) reports a preliminary experiment to detect word borders in German and English texts by monitoring the entropy of successive characters for 4-grams. Also, the second author of this paper (Tanaka-Ishii, 2005) have shown how Japanese and Chinese can be segm</context>
<context position="9849" citStr="Harris, 1955" startWordPosition="1625" endWordPosition="1627">gree is shown, where the plot at k on the horizontal axis denotes the entropy for h(x0;k) and xn;m denotes the substring between offsets n and m. Ideally, the entropy would take a maximum at 4, because it will decrease as k is increased in the ranges of k &lt; 4 and 4 &lt; k &lt; 8, and at k = 4, it will rise. Therefore, the position at k = 4 is detected as the &amp;quot;local maximum value&amp;quot; when monitoring h(x0;k) over k. The boundary condition after such observation can be redefined as the following: Bmax Boundaries are locations where the entropy is locally maximized. A similar method is proposed by Harris (Harris, 1955), where morpheme borders can be detected by using the local maximum of the number of different tokens coming after a prefix. This only holds, however, for semantic units longer than 1. Units often have a length of 430 1, especially in our case with Chinese characters as elements, so that there are many onecharacter words. If a unit has length 1, then the situation will look like the second graph in Figure 3, where three semantic units, x0;4, x4;5, and x5;8, are present, with the middle unit having length 1. First, at k = 4, the value of h increases. At k = 5, the value may increase or decrease</context>
</contexts>
<marker>Harris, 1955</marker>
<rawString>S.Z. Harris. 1955. From phoneme to morpheme. Language, pages 190-222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kempe</author>
</authors>
<title>Experiments in unsupervised entropy-based corpus segmentation.</title>
<date>1999</date>
<booktitle>In Workshop of EACL in Computational Natural Language Learning,</booktitle>
<pages>7--13</pages>
<contexts>
<context position="2487" citStr="Kempe, 1999" startWordPosition="393" endWordPosition="394">ur intention in this paper is above all to study the fundamental and scientific statistical property underlying language data, so that it can be applied to language engineering. The above assumption (A) dates back to the fundamental work done by Harris (Harris, 1955), where he says that when the number of different tokens coming after every prefix of a word marks the maximum value, then the location corresponds to the morpheme boundary. Recently, with the increasing availability of corpora, this property underlying language has been tested through segmentation into words and morphemes. Kempe (Kempe, 1999) reports a preliminary experiment to detect word borders in German and English texts by monitoring the entropy of successive characters for 4-grams. Also, the second author of this paper (Tanaka-Ishii, 2005) have shown how Japanese and Chinese can be segmented into words by formalizing the uncertainty with the branching entropy. Even though the test data was limited to a small amount in this work, the report suggested how assumption 428 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 428–435, Sydney, July 2006. c�2006 Association for Computational Linguistics (A) hold</context>
</contexts>
<marker>Kempe, 1999</marker>
<rawString>A. Kempe. 1999. Experiments in unsupervised entropy-based corpus segmentation. In Workshop of EACL in Computational Natural Language Learning, pages 7-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Low</author>
<author>H T Ng</author>
<author>W Guo</author>
</authors>
<title>A maximum entropy approach to chinese word segmentation.</title>
<date>2005</date>
<booktitle>In SIGHAN.</booktitle>
<contexts>
<context position="5086" citStr="Low et al., 2005" startWordPosition="805" endWordPosition="808">y far, than supervised segmentation; therefore, the algorithms are complex, and previous studies have tended to be limited in terms of both the test corpus size and the target. In contrast, as assumption (A) is simple, we keep this simplicity in our formalization and directly test the assumption on a large-scale test corpus consisting of 1001 KB manually segmented data with the training corpus consisting of 200 MB of Chinese text. Chinese is such an important language that supervised segmentation methods are already very mature. The current state-of-the-art segmentation software developed by (Low et al., 2005), which ranks as the best in the SIGHAN bakeoff (Emerson, 2005), attains word precision and recall of 96.9% and 96.8%, respectively, on the PKU track. There is also free 1 2 3 4 5 6 7 8 offset Figure2:DecreaseinH(XjXn)forChinese characterswhennisincreased softwaresuchas(Zhangetal.,2003)whose performance is also high. Even then, as most supervised methods learn on manually segmented newspaper data, when the input text is not from newspapers, the performance can be insufficient. Given that the construction of learning data is costly, we believe the performance can be raised by combining the supe</context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>J.K. Low, H.T Ng, and W. Guo. 2005. A maximum entropy approach to chinese word segmentation. In SIGHAN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sun</author>
<author>D Shen</author>
<author>B K Tsou</author>
</authors>
<title>Chinese word segmentation without using lexicon and hand-crafted training data.</title>
<date>1998</date>
<booktitle>In COLINGACL.</booktitle>
<contexts>
<context position="4097" citStr="Sun et al., 1998" startWordPosition="652" endWordPosition="655">ion is unsupervised text segmentation into words. Many works in unsupervised segmentation so far could be interpreted as formulating assumption (A) in a similar sense where branching stays low inside words but increases at a word or morpheme border. None of these works, however, is directly based on (A), and they introduce other factors within their overall methodologies. Some works are based on in-word branching frequencies formulated in an original evaluation function, as in (Ando and Lee, 2000) (boundary precision =84.5%, recall= 78.0%, tested on 12500 Japanese ideogram words). Sun et al. (Sun et al., 1998) uses mutual information (boundary p=91.8%, no report for recall, 1588 Chinese characters), and Feng(Feng et al., 2004) incorporates branching counts in the evaluation function to be optimized for obtaining boundaries (word precision=76%, recall=78%, 2000 sentences). From the performance results listed here, we can see that unsupervised segmentation is more difficult, by far, than supervised segmentation; therefore, the algorithms are complex, and previous studies have tended to be limited in terms of both the test corpus size and the target. In contrast, as assumption (A) is simple, we keep t</context>
<context position="18367" citStr="Sun et al., 1998" startWordPosition="3091" endWordPosition="3094">is 2 divided by the number of words in the results (i.e., 3 for the words &amp;quot;aaabbb&amp;quot; , &amp;quot;ccc&amp;quot; , &amp;quot;ddd&amp;quot;) , giving 67%, and the recall is 2 divided by the total number of words in the golden standard (i.e., 4 for the words &amp;quot;aaa&amp;quot; , &amp;quot;bbb&amp;quot; , &amp;quot;ccc&amp;quot; , &amp;quot;ddd&amp;quot;) giving 50%. We call these values the word precision and recall, respectively, throughout this paper. In our case, we use slightly different measures for the boundary precision and recall, which are based on the correct number of boundaries. These scores are also utilized especially in previous works on unsupervised segmentation (Ando and Lee, 2000) (Sun et al., 1998). Precisely, Precision=Ncorrect Ntest Recall = Ncorrect where (7) Ntrue Ncorrect is the number of correct boundaries in the result, Ntest is the number of boundaries in the test result, and, Ntrue is the number of boundaries in the golden standard. For example, in the case of the machine result being &amp;quot;aaabbbjcccjddd&amp;quot;, the precision is 100% and the recall is 75%. Thus, we consider there to be no imprecise result as a boundary in the output of &amp;quot;aaabbbjcccjddd&amp;quot;. The crucial reason for using the boundary precision and recall is that boundary detection and word extraction are not exactly the same t</context>
</contexts>
<marker>Sun, Shen, Tsou, 1998</marker>
<rawString>M. Sun, D. Shen, and B. K. Tsou. 1998. Chinese word segmentation without using lexicon and hand-crafted training data. In COLINGACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tanaka-Ishii</author>
</authors>
<title>Entropy as an indicator of context boundaries an experiment using a web search engine . In</title>
<date>2005</date>
<booktitle>IJCNLP,</booktitle>
<pages>93--105</pages>
<contexts>
<context position="2694" citStr="Tanaka-Ishii, 2005" startWordPosition="426" endWordPosition="427">A) dates back to the fundamental work done by Harris (Harris, 1955), where he says that when the number of different tokens coming after every prefix of a word marks the maximum value, then the location corresponds to the morpheme boundary. Recently, with the increasing availability of corpora, this property underlying language has been tested through segmentation into words and morphemes. Kempe (Kempe, 1999) reports a preliminary experiment to detect word borders in German and English texts by monitoring the entropy of successive characters for 4-grams. Also, the second author of this paper (Tanaka-Ishii, 2005) have shown how Japanese and Chinese can be segmented into words by formalizing the uncertainty with the branching entropy. Even though the test data was limited to a small amount in this work, the report suggested how assumption 428 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 428–435, Sydney, July 2006. c�2006 Association for Computational Linguistics (A) holds better when each of the sequence elements forms a semantic unit. This motivated our work to conduct a further, larger-scale test in the Chinese language, which is the only human language consisting entirel</context>
</contexts>
<marker>Tanaka-Ishii, 2005</marker>
<rawString>K. Tanaka-Ishii. 2005. Entropy as an indicator of context boundaries an experiment using a web search engine . In IJCNLP, pages 93-105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Zhang</author>
<author>H Y Yu</author>
<author>D Y Xiong</author>
<author>Q Liu</author>
</authors>
<title>Hhmm-based chinese lexical analyzer ictclas.</title>
<date>2003</date>
<booktitle>In SIGHAN. visited</booktitle>
<note>available from http://www.nlp.org.cn.</note>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>H.P. Zhang, Yu H.Y., Xiong D.Y., and Q Liu. 2003. Hhmm-based chinese lexical analyzer ictclas. In SIGHAN. visited 2006, available from http://www.nlp.org.cn.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>