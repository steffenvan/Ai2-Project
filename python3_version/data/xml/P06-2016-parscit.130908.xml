<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.974268">
Techniques to incorporate the benefits of a Hierarchy in a modified hidden
Markov model
</title>
<author confidence="0.991525">
Lin-Yi Chou
</author>
<affiliation confidence="0.993361">
University of Waikato
</affiliation>
<address confidence="0.66235">
Hamilton
New Zealand
</address>
<email confidence="0.993396">
lc55@cs.waikato.ac.nz
</email>
<sectionHeader confidence="0.99367" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.976402875">
This paper explores techniques to take ad-
vantage of the fundamental difference in
structure between hidden Markov models
(HMM) and hierarchical hidden Markov
models (HHMM). The HHMM structure
allows repeated parts of the model to be
merged together. A merged model takes
advantage of the recurring patterns within
the hierarchy, and the clusters that exist in
some sequences of observations, in order
to increase the extraction accuracy. This
paper also presents a new technique for re-
constructing grammar rules automatically.
This work builds on the idea of combining
a phrase extraction method with HHMM
to expose patterns within English text. The
reconstruction is then used to simplify the
complex structure of an HHMM
The models discussed here are evaluated
by applying them to natural language tasks
based on CoNLL-20041 and a sub-corpus
of the Lancaster Treebank2.
Keywords: information extraction, natu-
ral language, hidden Markov models.
</bodyText>
<sectionHeader confidence="0.999324" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999477833333333">
Hidden Markov models (HMMs) were introduced
in the late 1960s, and are widely used as a prob-
abilistic tool for modeling sequences of obser-
vations (Rabiner and Juang, 1986). They have
proven to be capable of assigning semantic la-
bels to tokens over a wide variety of input types.
</bodyText>
<footnote confidence="0.9862495">
1The 2004 Conference on Computational Natural Lan-
guage Learning, http://cnts.uia.ac.be/conll2004
2Lancaster/IBM Treebank,
http://www.ilc.cnr.it/EAGLES96/synlex/node23.html
</footnote>
<bodyText confidence="0.999856631578947">
This is useful for text-related tasks that involve
some uncertainty, including part-of-speech tag-
ging (Brill, 1995), text segmentation (Borkar et
al., 2001), named entity recognition (Bikel et al.,
1999) and information extraction tasks (McCal-
lum et al., 1999). However, most natural language
processing tasks are dependent on discovering a
hierarchical structure hidden within the source in-
formation. An example would be predicting se-
mantic roles from English sentences. HMMs are
less capable of reliably modeling these tasks. In
contrast hierarchical hidden Markov models (HH-
MMs) are better at capturing the underlying hier-
archy structure. While there are several difficulties
inherent in extracting information from the pat-
terns hidden within natural language information,
by discovering the hierarchical structure more ac-
curate models can be built.
HHMMs were first proposed by Fine (1998)
to resolve the complex multi-scale structures that
pervade natural language, such as speech (Rabiner
and Juang, 1986), handwriting (Nag et al., 1986),
and text. Skounakis (2003) described the HHMM
as multiple “levels” of HMM states, where lower
levels represents each individual output symbol,
and upper levels represents the combinations of
lower level sequences.
Any HHMM can be converted to a HMM by
creating a state for every possible observation,
a process called “flattening”. Flattening is per-
formed to simplify the model to a linear sequence
of Markov states, thus decreasing processing time.
But as a result of this process the model no longer
contains any hierarchical structure. To reduce the
models complexity while maintaining some hier-
archical structure, our algorithm uses a “partial
flattening” process.
In recent years, artificial intelligence re-
</bodyText>
<page confidence="0.944044">
120
</page>
<note confidence="0.722116">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 120–127,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999890083333333">
searchers have made strenuous efforts to re-
produce the human interpretation of language,
whereby patterns in grammar can be recognised
and simplified automatically. Brill (1995) de-
scribes a simple rule-based approach for learning
by rewriting the bracketing rule—a method for
presenting the structure of natural language text—
for linguistic knowledge. Similarly, Krotov (1999)
puts forward a method for eliminating redundant
grammar rules by applying a compaction algo-
rithm. This work draws upon the lessons learned
from these sources by automatically detecting sit-
uations in which the grammar structure can be re-
constructed. This is done by applying the phrase
extraction method introduced by Pantel (2001) to
rewrite the bracketing rule by calculating the de-
pendency of each possible phrase. The outcome
of this restructuring is to reduce the complexity of
the hierarchical structure and reduce the number
of levels in the hierarchy.
This paper considers the tasks of identifying
the syntactic structure of text chunking and gram-
mar parsing with previously annotated text doc-
uments. It analyses the use of HHMMs—both
before and after the application of improvement
techniques—for these tasks, then compares the re-
sults with HMMs. This paper is organised as fol-
lows: Section 2 describes the method for training
HHMMs. Section 3 describes the flattening pro-
cess for reducing the depth of hierarchical struc-
ture for HHMMs. Section 4 discusses the use of
HHMMs for the text chunking task and the gram-
mar parser. The evaluation results of the HMM,
the plain HHMM and the merged and partially flat-
tened HHMM are presented in Section 5. Finally,
Section 6 discusses the results.
</bodyText>
<sectionHeader confidence="0.962429" genericHeader="method">
2 Hierarchical Hidden Markov Model
</sectionHeader>
<bodyText confidence="0.99620475">
A HHMM is a structured multi-level stochastic
process, and can be visualised as a tree structured
HMM (see Figure 1(b)). There are two types of
states:
</bodyText>
<listItem confidence="0.9978962">
• Production state: a leaf node of the tree
structure, which contains only observations
(represented in Figure 1(b) as the empty cir-
cle Q).
• Internal state: contains several production
</listItem>
<bodyText confidence="0.940121875">
states or other internal states (represented in
Figure 1(b) as a circle with a cross inside (D).
The output of a HHMM is generated by a pro-
cess of traversing some sequence of states within
the model. At each internal state, the automa-
tion traverses down the tree, possibly through fur-
ther internal states, until it encounters a production
state where an observation is contained. Thus, as it
continues through the tree, the process generates a
sequence of observations. The process ends when
a final state is entered. The difference between a
standard HMM and a hierarchical HMM is that in-
dividual states in the hierarchical model can tra-
verse to a sequence of production states, whereas
each state in the standard model corresponds is a
production state that contains a single observation.
</bodyText>
<subsectionHeader confidence="0.896707">
2.1 Merging
</subsectionHeader>
<figureCaption confidence="0.999141">
Figure 1: Example of a HHMM
</figureCaption>
<bodyText confidence="0.9318085">
Figure 1(a) and Figure 1(b) illustrate the process
of reconstructing a HMM as a HHMM. Figure 1(a)
shows a HMM with 11 states. The two dashed
boxes (A) indicate regions of the model that have
a repeated structure. These regions are further-
more independent of the other states in the model.
Figure 1(b) models the same structure as a hier-
archical HMM, where each repeated structure is
now grouped under an internal state. This HHMM
uses a two level hierarchical structure to expose
more information about the transitions and proba-
bilities within the internal states. These states, as
discussed earlier, produce no observation of their
own. Instead, that is left to the child production
states within them. Figure 1(b) shows that each
internal state contains four production states.
In some cases, different internal states of a
HHMM correspond to exactly the same structure
in the output sequence. This is modelled by mak-
ing them share the same sub-models. Using a
HHMM allows for the merging of repeated parts
of the structure, which results in fewer states need-
ing to be identified—one of the three fundamen-
tal problems of HMM construction (Rabiner and
</bodyText>
<figure confidence="0.993052">
A A
A A
</figure>
<page confidence="0.961377">
121
</page>
<bodyText confidence="0.471553">
Juang, 1986).
</bodyText>
<subsectionHeader confidence="0.994336">
2.2 Sub-model Calculation
</subsectionHeader>
<bodyText confidence="0.987920647058824">
Estimating the parameters for multi-level HH-
MMs is a complicated process. This section de-
scribes a probability estimation method for inter-
nal states, which transforms each internal state
into three production states. Each internal state Si
in the HHMM is transformed by resolving each
child production state Si,j, into one of three trans-
formed states, Si ⇒ {s(i)
in , s(i)
stay, s(i) out}. The trans-
formation requires re-calculating the new observa-
tional and transition probabilities for each of these
transformed states. Figure 2 shows the internal
states of S2 have been transformed into s(2)
in , s(2)
stay,
sstay and s(2)
</bodyText>
<listItem confidence="0.349251">
(2) out.
</listItem>
<figureCaption confidence="0.9991465">
Figure 2: Example of a transformed HHMM with
the internal state S2.
</figureCaption>
<bodyText confidence="0.996758277777778">
The procedure to transform internal states is:
I) calculate the transformed observation (¯O) for
each internal state; II) apply the forward algorithm
to estimate the state probabilities (¯b) for the three
transformed states; III) reform the transition ma-
trix by including estimated values for additional
transformed internal states (¯A).
II. Apply forward algorithm to estimate the
transform observation value ¯b: The trans-
formed observation values are simplified to
{0 ) t , ¯b(i)
stay,t , b� )t,t }, which are then given as
the observation values for the three produc-
tions states (s(i)
in , s(i)
stay, s(i) out). The observa-
tional probability of entering state Si at time
t, i.e. production state s(i)
</bodyText>
<equation confidence="0.9068615">
in , is given by:
¯b(i) �πj × ¯Oj,t� , (2)
in,t = max
j=1..Ni
</equation>
<bodyText confidence="0.9820632">
where πj represents the transition probabil-
ities of entering child state Sj. The second
probability of staying in state Si at time t, i.e.
production state, s(i)
stay, is given by:
</bodyText>
<equation confidence="0.990036333333333">
b(t) y,t = jmaNi [Aˆj∗,j × Oj,t], (3)
= aIg .max [Aˆ* × Oj,t]
7=1..Ni j r7
</equation>
<bodyText confidence="0.996360166666667">
where ˆj∗ is the state corresponding to jˆ cal-
culated at previous time t−1, and Aˆj∗,j repre-
sents the transition probability from state Sˆj∗
to state to Sj. The third probability of exiting
state Si at time t, i.e. production state, s(i) out,
is given by:
</bodyText>
<equation confidence="0.925175">
¯b(i)
)t t = jmaNi [Aˆj∗,j × Oj,t × τj] , (4)
</equation>
<bodyText confidence="0.999708142857143">
where τj is the transition probabilities for
leaving the state Sj.
III. Reform transition probability ¯A(i): Each
internal state Si reforms a new 3 × 3 transi-
tion probability matrix ¯A, which records the
transition status for the transform matrix. The
formula for the estimated cells in A¯ are:
</bodyText>
<equation confidence="0.545353333333333">
(2) (2) (2) (2)
S 1 S in S stay S stay S out S 3
S2,1 S 2,2 S2,3 S2,4
</equation>
<bodyText confidence="0.722610666666667">
I. Calculate the observation probabilities O: ¯A(i) = Ni πj (5)
Every observation in each internal state Si is in,stay E πj (6)
re-calculated by summing up all the observa- ¯A(i) j=1 2
tion probabilities in each production state Sj in,out = Ni
as: E
j=1
</bodyText>
<equation confidence="0.672798333333333">
Oi,t = Ni Oj,t, (1) ¯A(i) = Ni,Ni Ak,j (7)
E stay,stay E
j=1 k=1,j=1
</equation>
<bodyText confidence="0.9999802">
where time t corresponds to a position in the
sequence, O is an observation sequence over
t, Oj,t is the observation probability for state
Sj at time t, and Ni represents the number of
production states for internal state Si.
</bodyText>
<equation confidence="0.86802">
¯A(i) =
</equation>
<bodyText confidence="0.767915">
stay,out
where Ni is the number of child states for
</bodyText>
<equation confidence="0.9577192">
¯A(i)
i
Ni
E τj (8)
j=1
</equation>
<bodyText confidence="0.460992">
state Si,
n stay is estimated by summing
</bodyText>
<page confidence="0.901324">
122
</page>
<bodyText confidence="0.821472869565217">
up all entry state probabilities for state Si,
¯A(i)
in,out is estimated from the observation that
50% of sequences transit from state s(i)
in di-
rectly to state s(i) out, ¯A(i)
stay,stay is the sum of
all the internal transition probabilities within
state Si, and ¯A(i)
stay,out is the sum of all exit
state probabilities. The rest of the probabili-
ties for transition matrix A¯ are set to zero to
prevent illegal transitions.
Each internal state is implemented by a bottom-
up algorithm using the values from equations (1)-
(8), where lower levels of the hierarchy tree are
calculated first to provide information for upper
level states. Once all the internal states have been
calculated, the system then need only to use the
top-level of the hierarchy tree to estimate the prob-
ability sequences. This means the model will now
become a linear HMM for the final Viterbi search
process (Viterbi, 1967).
</bodyText>
<sectionHeader confidence="0.994701" genericHeader="method">
3 Partial flattening
</sectionHeader>
<bodyText confidence="0.999931173913044">
Partial flattening is a process for reducing the
depth of hierarchical structure trees. The process
involves moving sub-trees from one node to an-
other. This section presents an interesting auto-
matic partial flattening process that makes use of
the term extractor method (Pantel and Lin, 2001).
The method discovers ways of more tightly cou-
pling observation sequences within sub-models
thus eliminating rules within the HHMM. This re-
sults in more accurate model. This process in-
volves calculating dependency values to measure
the dependency between the elements in the state
sequence (or observation sequence).
This method uses mutual information and log-
likelihood, which Dunning (1993) used to calcu-
late the dependency value between words. Where
there is a higher dependency value between words
they are more likely to be treat as a term. The pro-
cess involves collecting bigram frequencies from
a large dataset, and identifying the possible two
word candidates as terms. The first measurement
used is mutual information, which is calculated us-
ing the formula:
</bodyText>
<equation confidence="0.999396333333333">
P(x, y)
mi(x, y) = (9)
P(x)P(y)
</equation>
<bodyText confidence="0.999482">
where x and y are words adjacent to each other in
the training corpus, C(x, y) to be the frequency of
the two words, and * represents all the words in
entire training corpus. The log-likelihood ratio of
x and y is defined as:
</bodyText>
<equation confidence="0.999755555555556">
logL(x, y) = ll( k1
−ll( k1 + k2 ,k1,n1)
n1 + n2
−ll( k1 + k2
,k2, n2) (10)
n1 + n2
where k1 = C(x, y), n1 = C(x, *), k2 =
C(-,x, y), n2 = C(-,x, *) and
ll(p, k, n) = k log(p) + (n − k) log(1 − p) (11)
</equation>
<bodyText confidence="0.995885909090909">
The system computes dependency values between
states (tree nodes) or observations (tree leaves) in
the tree in the same way. The mutual informa-
tion and log-likelihood values are highest when
the words are adjacent to each other throughout
the entire corpus. By using these two values,
the method is more robust against low frequency
events.
Figure 3 is a tree representation of the HHMM,
the figure illustrates the flattening process for the
sentence:
</bodyText>
<equation confidence="0.476802">
(S (N* A AT1 graphical JJ zoo NN1 (P*
of IO (N ( strange JJ and CC peculiar JJ ) at-
tractors NN2 )))).
</equation>
<bodyText confidence="0.999796769230769">
where only the part-of-speech tags and grammar
information are considered. The left hand side of
the figure shows the original structure of the sen-
tence, and the right hand side shows the trans-
formed structure. The model’s hierarchy is re-
duced by one level, where the state P* has become
a sub-state of state S instead of N*. The process
is likely to be useful when state P* is highly de-
pendent on state N*.
The flattening process can be applied to the
model based on two types of sequence depen-
dancy; observation dependancy and state depen-
dancy.
</bodyText>
<listItem confidence="0.9937058">
• Observation dependency: The observation
dependency value is based upon the observa-
tion sequence, which in Figure 3 would be
the sequence of part-of-speech tags {AT1 JJ
NN1 IO JJ CC JJ NN21. Given observations
NN1 and IO’s as terms with a high depen-
dency value, the model then re-construct the
sub-tree at IO parent state P* moving it to the
same level as state N*, where the states of P*
and N* now share the same parent, state S.
</listItem>
<equation confidence="0.577301333333333">
n1
,k1, n1) + ll(k2 , k2, n2)
n2
</equation>
<page confidence="0.955796">
123
</page>
<figureCaption confidence="0.998284">
Figure 3: Partial flattening process for state N∗ and P∗.
</figureCaption>
<figure confidence="0.9986152">
JJ CC JJ
NN2
IO
AT1 JJ NN1
N
S
N*
P*
JJ CC JJ NN2
S
N*
AT1 JJ NN1
P*
IO
N
</figure>
<listItem confidence="0.9729465">
• State dependency : The state dependency
value is based upon the state sequence, which
in Figure 3 would be {N∗, P∗, N}. The flat-
tening process occurs when the current state
has a high dependency value with the previ-
ous state, say N∗ and P∗.
</listItem>
<table confidence="0.998370375">
term dependency value
NN1 IO 570.55
IO JJ 570.55
JJ CC 570.55
CC JJ 570.55
JJ NN2 295.24
AT1 JJ 294.25
JJ NN1 294.25
</table>
<tableCaption confidence="0.977779">
Table 1: Observation dependency values of part-
of-speech tags
</tableCaption>
<bodyText confidence="0.999425533333333">
This paper determines the high dependency val-
ues by selecting the top n values from a list of
all possible terms ranked by either observation or
state dependency values, where n is a parameter
that can be configured by the user for better per-
formance. Table 1 shows the observation depen-
dency values of terms for part-of-speech tags for
Figure 3. The term NN1 IO has a higher depen-
dency value than JJ NN1, therefore state P∗ is
joined as a sub-tree of state S. States P∗ and N
remain unchanged since state P∗ has already been
moved up a level of the tree. After the flattening
process, the state P∗ no longer belongs to the child
state of state N∗, and is instead joined as the sub-
tree to state S as shown in Figure 3.
</bodyText>
<sectionHeader confidence="0.997916" genericHeader="method">
4 Application
</sectionHeader>
<subsectionHeader confidence="0.981114">
4.1 Text Chunking
</subsectionHeader>
<bodyText confidence="0.976874677419355">
Text chunking involves producing non-
overlapping segments of low-level noun groups.
The system uses the clause information to con-
struct the hierarchical structure of text chunks,
where the clauses represent the phrases within
the sentence. The clauses can be embedded in
other clauses but cannot overlap one another.
Furthermore each clause contains one or more
text chunks.
Consider a sentence from a CoNLL-2004 cor-
pus:
(S (NP He PRP) (VP reckons VBZ) (S (NP
the DT current JJ account NN deficit NN)
(VP will MD narrow VB) (PP to TO) (NP
only RB # # 1.8 CD billion D) (PP in IN)
(NP September NNP)) (O . .))
where the part-of-speech tag associated with each
word is attached with an underscore, the clause in-
formation is identified by the S symbol and the
chunk information is identified by the rest of the
symbols NP (noun phrase), VP (verb phrase), PP
(prepositional phrase) and O (null complemen-
tizer). The brackets are in Penn Treebank II style3.
The sentence can be re-expressed just as its part-
of-speech tags thusly: {PRP VBZ DT JJ NN NN
MD VB TO RB # CD D IN NNP}, where only
the part-of-speech tags and grammar information
are to be considered for the extraction tasks. This
is done so the system can minimise the computa-
tion cost inherent in learning a large number of un-
required observation symbols. Such an approach
</bodyText>
<footnote confidence="0.996159">
3The Penn Treebank Project,
http://www.cis.upenn.edu/˜ treebank/home.html
</footnote>
<page confidence="0.996967">
124
</page>
<bodyText confidence="0.999686363636364">
also maximises the efficiency of trained data by
learning the pattern that is hidden within words
(syntax) rather than the words themselves (seman-
tics).
Figure 4 represents an example of the tree repre-
sentation of an HHMM for the text chunking task.
This example involves a hierarchy with a depth of
three. Note that state NP appears in two differ-
ent levels of the hierarchy. In order to build an
HHMM, the sentence shown above must be re-
structured as:
</bodyText>
<equation confidence="0.953336333333333">
(S (NP PRP) (VP VBZ) (S (NP DT JJ NN NN)
(VP MD VB) (PP TO) (NP RB # CD D) (PP IN)
(NP NNP)) (O . ))
</equation>
<bodyText confidence="0.9998365">
where the model makes no use of the word infor-
mation contained in the sentence.
</bodyText>
<subsectionHeader confidence="0.98661">
4.2 Grammar Parsing
</subsectionHeader>
<bodyText confidence="0.999215">
Creation of a parse tree involves describing lan-
guage grammar in a tree representation, where
each path of the tree represents a grammar rule.
Consider a sentence from the Lancaster Tree-
</bodyText>
<equation confidence="0.8888755">
bank4 :
(S (N A AT1 graphical JJ zoo NN1 (P of IO
(N ( strange JJ and CC peculiar JJ) attrac-
tors NN2))))
</equation>
<bodyText confidence="0.9997265">
where the part-of-speech tag associated with each
word is attached with an underscore, and the syn-
tactic tag for each phrase occurs immediately after
the opening square-bracket. In order to build the
</bodyText>
<figureCaption confidence="0.96379">
Figure 5: Parse tree for the HHMM
</figureCaption>
<footnote confidence="0.982933">
4Lancaster/IBM Treebank,
http://www.ilc.cnr.it/EAGLES96/synlex/node23.html
</footnote>
<bodyText confidence="0.9997096">
models from the parse tree, the system takes the
part-of-speech tags as the observation sequences,
and learns the structure of the model using the in-
formation expressed by the syntactic tags. During
construction, phrases, such as the noun phrase “(
strange JJ and CC peculiar JJ )”, are grouped
under a dummy state (N d). Figure 5 illustrates
the model in the tree representation with the struc-
ture of the model based on the previous sentence
from Lancaster Treebank.
</bodyText>
<sectionHeader confidence="0.994785" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999762066666667">
The first evaluation presents preliminary evi-
dence that the merged hierarchical hidden Markov
Model (MHHMM) is able to produce more accu-
rate results either a plain HHMM or a HMM dur-
ing the text chunking task. The results suggest
that the partial flattening process is capable of im-
proving model accuracy when the input data con-
tains complex hierarchical structures. The evalua-
tion involves analysing the results over two sets of
data. The first is a selection of data from CoNLL-
2004 and contains 8936 sentences. The second
dataset is part of the Lancaster Treebank corpus
and contains 1473 sentences. Each sentence con-
tains hand-labeled syntactic roles for natural lan-
guage text.
</bodyText>
<figure confidence="0.9908154">
0.94
0.92
F 0.90
0.88
0.86
</figure>
<figureCaption confidence="0.999552">
Figure 6: The graph of micro-average F-measure
</figureCaption>
<bodyText confidence="0.909831666666667">
against the number of training sentences during
text chunking (A: MHHMM, B: HHMM and C:
HMM)
The first finding is that the size of training data
dramatically affects the prediction accuracy. A
model with an insufficient number of observations
</bodyText>
<figure confidence="0.989648972222222">
N
AT1 JJ NN1 P
IO N
JJ CC
N_d NN2
JJ
B.200
B.400
B.600
B.800
B.1000
B.1200
B.1400
A.600
A.800
A.1400
A.200
A.400
A.1000
A.1200
C.200
C.400
C.800
C.1000
C.1200
C.600
C.1400
125
NP VP S
PRP VBZ NP
NIL
VP
PP NP
PP
O
DT JJ NN NN MD TO RB # CD D IN .
</figure>
<figureCaption confidence="0.999917">
Figure 4: HHMM for syntax roles
</figureCaption>
<bodyText confidence="0.989029">
typically has poor accuracy. In the text chunk-
ing task the number of observation symbol relies
on the number of part-of-speech tags contained in
training data. Figure 6 plots the relationship of
micro-average F-measure for three types of mod-
els (A: MHHMM, B: HHMM and C: HMM) on
10-fold cross validation with the number of train-
ing sentences ranging from 200 to 1400. The re-
sult shows that the MHHMM has the better per-
formance in accuracy over both the HHMM and
HMM, although the difference is less marked for
the latter.
number of sentences
</bodyText>
<figureCaption confidence="0.999417">
Figure 7: The average processing time for text
chunking
Figure 7 represents the average processing time
</figureCaption>
<bodyText confidence="0.998849804347826">
for testing (in seconds) for the 10-fold cross vali-
dation. The test were carried out on a dual P4-D
computer running at 3GHz and with 1Gb RAM.
The results indicate that the MHHMM gains ef-
ficiency, in terms of computation cost, by merg-
ing repeated sub-models, resulting in fewer states
in the model. In contrast the HMM has lower
efficiency as it is required to identify every sin-
gle path, leading to more states within the model
and higher computation cost. The extra costs of
constructing a HHMM, which will have the same
number of production states as the HMM, make it
the least efficient.
The second evaluation presents preliminary ev-
idence that the partially flattened hierarchical hid-
den Markov model (PFHHMM) can assign propo-
sitions to language texts (grammar parsing) at least
as accurately as the HMM. This is assignment is a
task that HHMMs are generally not well suited to.
Table 2 shows the Fl-measures of identified se-
mantic roles for each different model on the Lan-
caster Treebank data set. The models used in this
evaluation were trained with observation data from
the Lancaster Treebank training set. The training
set and testing set are sub-divided from the corpus
in proportions of 3 and 13. The PFHHMMs had ex-
tra training conditions as follows: PFHHMM obs
2000 made use of the partial flattening process,
with the high dependency parameter determined
by considering the highest 2000 dependency val-
ues from observation sequences from the corpus.
PFHHMM state 150 again uses partial flattening,
however this time the highest 150 dependency val-
ues from state sequences were utilized in discover-
ing the high dependency threshold. The n values
of 2000 and 150 were determined to be the optimal
values when applied to the training set.
The results show that applying the partial flat-
tening process to a model using observation se-
quences to determine high dependency values re-
duces the complexity of the model’s hierarchy and
consequently improves the model’s accuracy. The
state dependency method is shown to be less favor-
able for this particular task, but the micro-average
result is still comparable with the HMM’s perfor-
mance. The results also show no significant re-
</bodyText>
<figure confidence="0.9962304">
50 100 150 200
seconds
80
60
40
20
0
A: HHMM
B: HHMM−tree
C: HMM
</figure>
<page confidence="0.976578">
126
</page>
<table confidence="0.9951987">
State Count HMM HHMM PFHHMM PFHHMM
obs state
2000 150
N 16387 0.874 0.811 0.882 0.874
NULL 4670 0.794 0.035 0.744 0.743
V 4134 0.768 0.755 0.804 0.791
P 2099 0.944 0.936 0.928 0.926
Fa 180 0.525 0.814 0.687 0.457
Micro-
Average 0.793 0.701 0.809 0.792
</table>
<tableCaption confidence="0.983438">
Table 2: F1-measure of top 5 states during grammar parsing
set.
</tableCaption>
<bodyText confidence="0.975539">
lationship between the occurance count of a state
against the various models prediction accuracy.
</bodyText>
<sectionHeader confidence="0.999091" genericHeader="conclusions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999985346153846">
Due to the hierarchical structure of a HHMM, the
model has the advantage of being able to reuse
information for repeated sub-models. Thus the
HHMM can perform more accurately and requires
less computational time than the HMM in certain
situations.
The merging and flattening techniques have
been shown to be effective and could be applied
to many kinds of data with hierarchical structures.
The methods are especially appealing where the
model involves complex structure or there is a
shortage of training data. Furthermore, they ad-
dress an important issue when dealing with small
datasets: by using the hierarchical model to un-
cover less obvious structures, the model is able
to increase model performance even over more
limited source materials. The experimental re-
sults have shown the potential of the merging and
partial flattening techniques in building hierarchi-
cal models and providing better handling of states
with less observation counts. Further research in
both experimental and theoretical aspects of this
work is planned, specifically in the area of recon-
structing hierarchies where recursive formations
are present and formal analysis and testing of tech-
niques.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999823444444444">
D. M. Bikel, R. Schwartz and R. M. Weischedel. 1999.
An Algorithm that Learns What’s in a Name. Ma-
chine Learning, 34:211–231.
V. R. Borkar, K. Deshmukh and S. Sarawagi. 2001.
Automatic Segmentation of Text into Structured
Records. Proceedings of SIGMOD.
E. Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543–565.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61–74.
S. Fine , Y. Singer and N. Tishby. 1998. The Hierar-
chical Hidden Markov Model: Analysis and Appli-
cations. Machine Learning, 32:41–62.
A. Krotov, M Heple, R. Gaizauskas and Y. Wilks.
1999. Compacting the Penn Treebank Grammar.
Proceedings of COLING-98 (Montreal), pages 699-
703.
A. McCallum, K. Nigam, J. Rennie and K. Sey-
more. 1999. Building Domain-Specific Search En-
gines with Machine Learning Techniques. In AAAI-
99 Spring Symposium on Intelligent Agents in Cy-
berspace.
R. Nag, K. H. Wong, and F. Fallside. 1986. Script
Recognition Using Hidden Markov Models. Proc.
ofICASSP 86, pp. 2071-1074, Toyko.
P. Pantel and D. Lin. 2001. A Statistical Corpus-
Based Term Extractor. In Stroulia, E. and Matwin,
S. (Eds.) AI 2001. Lecture Notes in Artificial Intel-
ligence, pp. 36-46. Springer-Verlag.
L. R. Rabiner and B. H. Juang. 1986. An Introduction
to Hidden Markov Models. IEEE Acoustics Speech
and Signal Processing ASSP Magazine, ASSP-3(1):
4–16, January.
M. Skounakis, M. Craven and S. Ray. 2003. Hi-
erarchical Hidden Markov Models for Information
Extraction. In Proceedings of the 18th Interna-
tional Joint Conference on Artificial Intelligence,
Acapulco, Mexico, Morgan Kaufmann.
A. J. Viterbi. 1967. Error bounds for convolutional
codes and an asymtotically optimum decoding algo-
rithm. IEEE Transactions on Information Theory,
IT-13:260–267.
</reference>
<page confidence="0.997255">
127
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.155541">
<title confidence="0.7379535">Techniques to incorporate the benefits of a Hierarchy in a modified hidden Markov model</title>
<author confidence="0.931015">Lin-Yi Chou</author>
<affiliation confidence="0.999915">University of Waikato</affiliation>
<address confidence="0.7276135">Hamilton New Zealand</address>
<email confidence="0.71858">lc55@cs.waikato.ac.nz</email>
<abstract confidence="0.98587976">This paper explores techniques to take advantage of the fundamental difference in structure between hidden Markov models (HMM) and hierarchical hidden Markov models (HHMM). The HHMM structure allows repeated parts of the model to be merged together. A merged model takes advantage of the recurring patterns within the hierarchy, and the clusters that exist in some sequences of observations, in order to increase the extraction accuracy. This paper also presents a new technique for reconstructing grammar rules automatically. This work builds on the idea of combining a phrase extraction method with HHMM to expose patterns within English text. The reconstruction is then used to simplify the complex structure of an HHMM The models discussed here are evaluated by applying them to natural language tasks on and a sub-corpus the Lancaster extraction, natural language, hidden Markov models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>R Schwartz</author>
<author>R M Weischedel</author>
</authors>
<title>An Algorithm that Learns What’s in a Name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--211</pages>
<contexts>
<context position="1789" citStr="Bikel et al., 1999" startWordPosition="258" endWordPosition="261">troduced in the late 1960s, and are widely used as a probabilistic tool for modeling sequences of observations (Rabiner and Juang, 1986). They have proven to be capable of assigning semantic labels to tokens over a wide variety of input types. 1The 2004 Conference on Computational Natural Language Learning, http://cnts.uia.ac.be/conll2004 2Lancaster/IBM Treebank, http://www.ilc.cnr.it/EAGLES96/synlex/node23.html This is useful for text-related tasks that involve some uncertainty, including part-of-speech tagging (Brill, 1995), text segmentation (Borkar et al., 2001), named entity recognition (Bikel et al., 1999) and information extraction tasks (McCallum et al., 1999). However, most natural language processing tasks are dependent on discovering a hierarchical structure hidden within the source information. An example would be predicting semantic roles from English sentences. HMMs are less capable of reliably modeling these tasks. In contrast hierarchical hidden Markov models (HHMMs) are better at capturing the underlying hierarchy structure. While there are several difficulties inherent in extracting information from the patterns hidden within natural language information, by discovering the hierarch</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>D. M. Bikel, R. Schwartz and R. M. Weischedel. 1999. An Algorithm that Learns What’s in a Name. Machine Learning, 34:211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V R Borkar</author>
<author>K Deshmukh</author>
<author>S Sarawagi</author>
</authors>
<title>Automatic Segmentation of Text into Structured Records.</title>
<date>2001</date>
<booktitle>Proceedings of SIGMOD.</booktitle>
<contexts>
<context position="1742" citStr="Borkar et al., 2001" startWordPosition="251" endWordPosition="254">Introduction Hidden Markov models (HMMs) were introduced in the late 1960s, and are widely used as a probabilistic tool for modeling sequences of observations (Rabiner and Juang, 1986). They have proven to be capable of assigning semantic labels to tokens over a wide variety of input types. 1The 2004 Conference on Computational Natural Language Learning, http://cnts.uia.ac.be/conll2004 2Lancaster/IBM Treebank, http://www.ilc.cnr.it/EAGLES96/synlex/node23.html This is useful for text-related tasks that involve some uncertainty, including part-of-speech tagging (Brill, 1995), text segmentation (Borkar et al., 2001), named entity recognition (Bikel et al., 1999) and information extraction tasks (McCallum et al., 1999). However, most natural language processing tasks are dependent on discovering a hierarchical structure hidden within the source information. An example would be predicting semantic roles from English sentences. HMMs are less capable of reliably modeling these tasks. In contrast hierarchical hidden Markov models (HHMMs) are better at capturing the underlying hierarchy structure. While there are several difficulties inherent in extracting information from the patterns hidden within natural la</context>
</contexts>
<marker>Borkar, Deshmukh, Sarawagi, 2001</marker>
<rawString>V. R. Borkar, K. Deshmukh and S. Sarawagi. 2001. Automatic Segmentation of Text into Structured Records. Proceedings of SIGMOD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="1701" citStr="Brill, 1995" startWordPosition="247" endWordPosition="248">anguage, hidden Markov models. 1 Introduction Hidden Markov models (HMMs) were introduced in the late 1960s, and are widely used as a probabilistic tool for modeling sequences of observations (Rabiner and Juang, 1986). They have proven to be capable of assigning semantic labels to tokens over a wide variety of input types. 1The 2004 Conference on Computational Natural Language Learning, http://cnts.uia.ac.be/conll2004 2Lancaster/IBM Treebank, http://www.ilc.cnr.it/EAGLES96/synlex/node23.html This is useful for text-related tasks that involve some uncertainty, including part-of-speech tagging (Brill, 1995), text segmentation (Borkar et al., 2001), named entity recognition (Bikel et al., 1999) and information extraction tasks (McCallum et al., 1999). However, most natural language processing tasks are dependent on discovering a hierarchical structure hidden within the source information. An example would be predicting semantic roles from English sentences. HMMs are less capable of reliably modeling these tasks. In contrast hierarchical hidden Markov models (HHMMs) are better at capturing the underlying hierarchy structure. While there are several difficulties inherent in extracting information f</context>
<context position="3675" citStr="Brill (1995)" startWordPosition="538" endWordPosition="539"> processing time. But as a result of this process the model no longer contains any hierarchical structure. To reduce the models complexity while maintaining some hierarchical structure, our algorithm uses a “partial flattening” process. In recent years, artificial intelligence re120 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 120–127, Sydney, July 2006. c�2006 Association for Computational Linguistics searchers have made strenuous efforts to reproduce the human interpretation of language, whereby patterns in grammar can be recognised and simplified automatically. Brill (1995) describes a simple rule-based approach for learning by rewriting the bracketing rule—a method for presenting the structure of natural language text— for linguistic knowledge. Similarly, Krotov (1999) puts forward a method for eliminating redundant grammar rules by applying a compaction algorithm. This work draws upon the lessons learned from these sources by automatically detecting situations in which the grammar structure can be reconstructed. This is done by applying the phrase extraction method introduced by Pantel (2001) to rewrite the bracketing rule by calculating the dependency of each</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="12257" citStr="Dunning (1993)" startWordPosition="1991" endWordPosition="1992">structure trees. The process involves moving sub-trees from one node to another. This section presents an interesting automatic partial flattening process that makes use of the term extractor method (Pantel and Lin, 2001). The method discovers ways of more tightly coupling observation sequences within sub-models thus eliminating rules within the HHMM. This results in more accurate model. This process involves calculating dependency values to measure the dependency between the elements in the state sequence (or observation sequence). This method uses mutual information and loglikelihood, which Dunning (1993) used to calculate the dependency value between words. Where there is a higher dependency value between words they are more likely to be treat as a term. The process involves collecting bigram frequencies from a large dataset, and identifying the possible two word candidates as terms. The first measurement used is mutual information, which is calculated using the formula: P(x, y) mi(x, y) = (9) P(x)P(y) where x and y are words adjacent to each other in the training corpus, C(x, y) to be the frequency of the two words, and * represents all the words in entire training corpus. The log-likelihood</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Singer</author>
<author>N Tishby</author>
</authors>
<title>The Hierarchical Hidden Markov Model: Analysis and Applications.</title>
<date>1998</date>
<booktitle>Machine Learning,</booktitle>
<pages>32--41</pages>
<marker>Singer, Tishby, 1998</marker>
<rawString>S. Fine , Y. Singer and N. Tishby. 1998. The Hierarchical Hidden Markov Model: Analysis and Applications. Machine Learning, 32:41–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Krotov</author>
<author>M Heple</author>
<author>R Gaizauskas</author>
<author>Y Wilks</author>
</authors>
<title>Compacting the Penn Treebank Grammar.</title>
<date>1999</date>
<booktitle>Proceedings of COLING-98 (Montreal),</booktitle>
<pages>699--703</pages>
<marker>Krotov, Heple, Gaizauskas, Wilks, 1999</marker>
<rawString>A. Krotov, M Heple, R. Gaizauskas and Y. Wilks. 1999. Compacting the Penn Treebank Grammar. Proceedings of COLING-98 (Montreal), pages 699-703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
<author>J Rennie</author>
<author>K Seymore</author>
</authors>
<title>Building Domain-Specific Search Engines with Machine Learning Techniques.</title>
<date>1999</date>
<booktitle>In AAAI99 Spring Symposium on Intelligent Agents in Cyberspace.</booktitle>
<contexts>
<context position="1846" citStr="McCallum et al., 1999" startWordPosition="266" endWordPosition="270">robabilistic tool for modeling sequences of observations (Rabiner and Juang, 1986). They have proven to be capable of assigning semantic labels to tokens over a wide variety of input types. 1The 2004 Conference on Computational Natural Language Learning, http://cnts.uia.ac.be/conll2004 2Lancaster/IBM Treebank, http://www.ilc.cnr.it/EAGLES96/synlex/node23.html This is useful for text-related tasks that involve some uncertainty, including part-of-speech tagging (Brill, 1995), text segmentation (Borkar et al., 2001), named entity recognition (Bikel et al., 1999) and information extraction tasks (McCallum et al., 1999). However, most natural language processing tasks are dependent on discovering a hierarchical structure hidden within the source information. An example would be predicting semantic roles from English sentences. HMMs are less capable of reliably modeling these tasks. In contrast hierarchical hidden Markov models (HHMMs) are better at capturing the underlying hierarchy structure. While there are several difficulties inherent in extracting information from the patterns hidden within natural language information, by discovering the hierarchical structure more accurate models can be built. HHMMs w</context>
</contexts>
<marker>McCallum, Nigam, Rennie, Seymore, 1999</marker>
<rawString>A. McCallum, K. Nigam, J. Rennie and K. Seymore. 1999. Building Domain-Specific Search Engines with Machine Learning Techniques. In AAAI99 Spring Symposium on Intelligent Agents in Cyberspace.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nag</author>
<author>K H Wong</author>
<author>F Fallside</author>
</authors>
<title>Script Recognition Using Hidden Markov Models.</title>
<date>1986</date>
<booktitle>Proc. ofICASSP 86,</booktitle>
<pages>2071--1074</pages>
<location>Toyko.</location>
<contexts>
<context position="2629" citStr="Nag et al., 1986" startWordPosition="382" endWordPosition="385">icting semantic roles from English sentences. HMMs are less capable of reliably modeling these tasks. In contrast hierarchical hidden Markov models (HHMMs) are better at capturing the underlying hierarchy structure. While there are several difficulties inherent in extracting information from the patterns hidden within natural language information, by discovering the hierarchical structure more accurate models can be built. HHMMs were first proposed by Fine (1998) to resolve the complex multi-scale structures that pervade natural language, such as speech (Rabiner and Juang, 1986), handwriting (Nag et al., 1986), and text. Skounakis (2003) described the HHMM as multiple “levels” of HMM states, where lower levels represents each individual output symbol, and upper levels represents the combinations of lower level sequences. Any HHMM can be converted to a HMM by creating a state for every possible observation, a process called “flattening”. Flattening is performed to simplify the model to a linear sequence of Markov states, thus decreasing processing time. But as a result of this process the model no longer contains any hierarchical structure. To reduce the models complexity while maintaining some hier</context>
</contexts>
<marker>Nag, Wong, Fallside, 1986</marker>
<rawString>R. Nag, K. H. Wong, and F. Fallside. 1986. Script Recognition Using Hidden Markov Models. Proc. ofICASSP 86, pp. 2071-1074, Toyko.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>A Statistical CorpusBased Term Extractor.</title>
<date>2001</date>
<booktitle>In Stroulia, E. and Matwin, S. (Eds.) AI 2001. Lecture Notes in Artificial Intelligence,</booktitle>
<pages>36--46</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="11864" citStr="Pantel and Lin, 2001" startWordPosition="1931" endWordPosition="1934"> provide information for upper level states. Once all the internal states have been calculated, the system then need only to use the top-level of the hierarchy tree to estimate the probability sequences. This means the model will now become a linear HMM for the final Viterbi search process (Viterbi, 1967). 3 Partial flattening Partial flattening is a process for reducing the depth of hierarchical structure trees. The process involves moving sub-trees from one node to another. This section presents an interesting automatic partial flattening process that makes use of the term extractor method (Pantel and Lin, 2001). The method discovers ways of more tightly coupling observation sequences within sub-models thus eliminating rules within the HHMM. This results in more accurate model. This process involves calculating dependency values to measure the dependency between the elements in the state sequence (or observation sequence). This method uses mutual information and loglikelihood, which Dunning (1993) used to calculate the dependency value between words. Where there is a higher dependency value between words they are more likely to be treat as a term. The process involves collecting bigram frequencies fr</context>
</contexts>
<marker>Pantel, Lin, 2001</marker>
<rawString>P. Pantel and D. Lin. 2001. A Statistical CorpusBased Term Extractor. In Stroulia, E. and Matwin, S. (Eds.) AI 2001. Lecture Notes in Artificial Intelligence, pp. 36-46. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
<author>B H Juang</author>
</authors>
<title>An Introduction to Hidden Markov Models.</title>
<date>1986</date>
<journal>IEEE Acoustics Speech and Signal Processing ASSP Magazine,</journal>
<volume>3</volume>
<issue>1</issue>
<pages>4--16</pages>
<contexts>
<context position="1306" citStr="Rabiner and Juang, 1986" startWordPosition="196" endWordPosition="199">les automatically. This work builds on the idea of combining a phrase extraction method with HHMM to expose patterns within English text. The reconstruction is then used to simplify the complex structure of an HHMM The models discussed here are evaluated by applying them to natural language tasks based on CoNLL-20041 and a sub-corpus of the Lancaster Treebank2. Keywords: information extraction, natural language, hidden Markov models. 1 Introduction Hidden Markov models (HMMs) were introduced in the late 1960s, and are widely used as a probabilistic tool for modeling sequences of observations (Rabiner and Juang, 1986). They have proven to be capable of assigning semantic labels to tokens over a wide variety of input types. 1The 2004 Conference on Computational Natural Language Learning, http://cnts.uia.ac.be/conll2004 2Lancaster/IBM Treebank, http://www.ilc.cnr.it/EAGLES96/synlex/node23.html This is useful for text-related tasks that involve some uncertainty, including part-of-speech tagging (Brill, 1995), text segmentation (Borkar et al., 2001), named entity recognition (Bikel et al., 1999) and information extraction tasks (McCallum et al., 1999). However, most natural language processing tasks are depend</context>
<context position="2597" citStr="Rabiner and Juang, 1986" startWordPosition="377" endWordPosition="380">e information. An example would be predicting semantic roles from English sentences. HMMs are less capable of reliably modeling these tasks. In contrast hierarchical hidden Markov models (HHMMs) are better at capturing the underlying hierarchy structure. While there are several difficulties inherent in extracting information from the patterns hidden within natural language information, by discovering the hierarchical structure more accurate models can be built. HHMMs were first proposed by Fine (1998) to resolve the complex multi-scale structures that pervade natural language, such as speech (Rabiner and Juang, 1986), handwriting (Nag et al., 1986), and text. Skounakis (2003) described the HHMM as multiple “levels” of HMM states, where lower levels represents each individual output symbol, and upper levels represents the combinations of lower level sequences. Any HHMM can be converted to a HMM by creating a state for every possible observation, a process called “flattening”. Flattening is performed to simplify the model to a linear sequence of Markov states, thus decreasing processing time. But as a result of this process the model no longer contains any hierarchical structure. To reduce the models comple</context>
</contexts>
<marker>Rabiner, Juang, 1986</marker>
<rawString>L. R. Rabiner and B. H. Juang. 1986. An Introduction to Hidden Markov Models. IEEE Acoustics Speech and Signal Processing ASSP Magazine, ASSP-3(1): 4–16, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Skounakis</author>
<author>M Craven</author>
<author>S Ray</author>
</authors>
<title>Hierarchical Hidden Markov Models for Information Extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence,</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Acapulco, Mexico,</location>
<marker>Skounakis, Craven, Ray, 2003</marker>
<rawString>M. Skounakis, M. Craven and S. Ray. 2003. Hierarchical Hidden Markov Models for Information Extraction. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, Acapulco, Mexico, Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymtotically optimum decoding algorithm.</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>13--260</pages>
<contexts>
<context position="11549" citStr="Viterbi, 1967" startWordPosition="1884" endWordPosition="1885">he sum of all exit state probabilities. The rest of the probabilities for transition matrix A¯ are set to zero to prevent illegal transitions. Each internal state is implemented by a bottomup algorithm using the values from equations (1)- (8), where lower levels of the hierarchy tree are calculated first to provide information for upper level states. Once all the internal states have been calculated, the system then need only to use the top-level of the hierarchy tree to estimate the probability sequences. This means the model will now become a linear HMM for the final Viterbi search process (Viterbi, 1967). 3 Partial flattening Partial flattening is a process for reducing the depth of hierarchical structure trees. The process involves moving sub-trees from one node to another. This section presents an interesting automatic partial flattening process that makes use of the term extractor method (Pantel and Lin, 2001). The method discovers ways of more tightly coupling observation sequences within sub-models thus eliminating rules within the HHMM. This results in more accurate model. This process involves calculating dependency values to measure the dependency between the elements in the state seq</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>A. J. Viterbi. 1967. Error bounds for convolutional codes and an asymtotically optimum decoding algorithm. IEEE Transactions on Information Theory, IT-13:260–267.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>