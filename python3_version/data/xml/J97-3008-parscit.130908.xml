<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005252">
<title confidence="0.807095333333333">
Book Reviews
The Human Semantic Potential:
Spatial Language and Constrained Connectionism
</title>
<author confidence="0.994889">
Terry Regier
</author>
<affiliation confidence="0.589935666666667">
(University of Chicago)
Cambridge, MA: The MIT Press
(Neural Network Modeling and
</affiliation>
<bodyText confidence="0.843857">
Connectionism Series, edited by Jeffrey
L. Elman), 1996, xv+220 pp;
hardbound, ISBN 0-262-18173-8, $37.50
</bodyText>
<note confidence="0.27399">
Reviewed by
</note>
<author confidence="0.852717">
Whitney Tabor
</author>
<affiliation confidence="0.985079">
Massachusetts Institute of Technology
</affiliation>
<sectionHeader confidence="0.989279" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999903870967742">
One of the implications of connectionist work on language is that the &amp;quot;language mech-
anism&amp;quot; is not essentially different from mechanisms that perform other cognitive tasks.
This leads naturally to the notion that language learning must be understood in the
context of learning about the world more generally. It is difficult to think of how to con-
strain a modeling exercise that adopts this perspective, for such a system requires an
encoding not only of some interesting part of language but also of the corresponding
part of &amp;quot;the world.&amp;quot; If one were to undertake this rather daunting task, a natural place
to start would be with a subdomain of language in which the important semantic con-
trasts can be expressed in a two-dimensional visual world and to use low-resolution
computer graphics for the encoding of this world.
In The Human Semantic Potential, Regier takes precisely this approach, by focusing
on the domain of closed-class spatial markers (e.g., prepositions such as above, below,
left, right, in, and on in English). Bit map &amp;quot;movies&amp;quot; of a trajector that moves in relation
to a landmark for several movie frames, or that simply exists, are used as the input to
a connectionist learning model; the outputs are labels corresponding to prepositions.
The scientific game plan is to construct a model that learns the semantics of spa-
tial markers on the basis of exposure to examples in a given language; the learnable
sets of markers under the model then constitute a typology of possible sets of spatial
markers in languages of the world. Thus the model makes cross-linguistic typologi-
cal predictions. Moreover, since the model generalizes from a subset of examples in
each language to an assessment of the aptness of each word in all representable condi-
tions, it makes language-internal predictions about the meanings that particular spatial
markers can have.
Given this framework, it seems reasonable to assess the project in terms of how
much we learn about these two domains of prediction from it. In the former case—
cross-linguistic typological prediction—the results are only mildly interesting: only a
very small subset of the range of predictions that the model makes is revealed, appar-
ently because of the assumed unanalyzability of the connectionist network at the core
of the model. In the case of the second task—generalization from positive examples—
the results are more appealing: Regier shows how a linguistically-motivated variation
on a standard connectionist learning algorithm provides an effective mechanism for
</bodyText>
<page confidence="0.997723">
483
</page>
<note confidence="0.447733">
Computational Linguistics Volume 23, Number 3
</note>
<bodyText confidence="0.979233">
generalizing in the absence of negative evidence. I&apos;ll elaborate on these two assess-
ments after providing a summary of the contents.
</bodyText>
<listItem confidence="0.797059">
2. Summary of the Book
</listItem>
<bodyText confidence="0.999940307692308">
The book opens with a presentation of the modeling project. A discussion, from a
cognitive-linguistics perspective, of the contrasting semantics of spatial prepositions in
several languages provides motivation for framing the model in terms of certain prim-
itive notions such as orientation and contact. A readable introduction to connectionist
models and the back-propagation learning algorithm is then provided. &amp;quot;Constrained
connectionism&amp;quot; is introduced: models consist of some nodes that are specifically de-
signed to pick up on features relevant to the task at hand and some nodes whose role
in the representation is determined by the learning process (like the &amp;quot;hidden nodes&amp;quot; of
stereotypical connectionist networks). A simulation is described in which the model,
on the basis of limited examples, correctly generalizes the meanings of the English
prepositions above, below, left, right, in, out, on, and off without the benefit of explicit
negative evidence. Then, the structures built into the network are reviewed in detail:
there are orientation combination nodes, which measure angles between important
reference lines in the current scene (e.g., there is a node that detects the angle between
a vertical line and the line connecting the centers of mass of the trajector and land-
mark) and there are map comparison nodes, which detect features such as contact
and inclusion. The temporal structure of the scenes is detected by means of nodes that
record the maximum, minimum, and average values of other nodes in the network
(over the course of the current movie). In this regard, the model diverges from the
bulk of connectionist models of temporal processing, which seek to solve in general
the problem of detecting correlations between events that are spread out over time,
rather than positing task-specific detectors. Helpfully, Regier provides a comparison
with some of these other approaches. A sample run-through of the model&apos;s perfor-
mance is provided. At this point, the cross-linguistic predictions are discussed. Finally,
some potential extensions of the model to linguistic problems of interest—polysemy
and deixis—are considered.
</bodyText>
<sectionHeader confidence="0.964646" genericHeader="method">
3. Cross-linguistic Typology
</sectionHeader>
<bodyText confidence="0.998739117647059">
The model clearly satisfies the desideratum of providing a typology of spatial semantic
systems. The difficulty is that it is not clear what is contained in this typology. Regier
focuses mainly on two properties: (1) an intermediate sequentiality prediction, which
holds that spatial meanings are encoded in terms of the primitives source, path, and
destination only, so there can be no language that uses two different closed-class forms
to distinguish two events that differ only in the sequence of events along the path;
(2) an endpoint configuration prediction, which says that a language with a word
meaning &amp;quot;through&amp;quot; or &amp;quot;out of&amp;quot; must also have a word meaning &amp;quot;in&amp;quot; (in either the
static &amp;quot;withinness&amp;quot; sense, or the &amp;quot;into&amp;quot; sense). The first of these predictions follows
trivially, as Regier notes, from his stipulation that source, path, and destination are the
primitives that the model must work with. This prediction is a standard type of claim
about semantic universals, and as such it deserves some attention, but in the context
of a connectionist learning device that is supposed to be making typological predic-
tions, it is a bit of a nonstarter. The question one really wants an answer to is: What
constraints, if any, do the connectionist features of the model (the hidden nodes, the
gradient descent learning, the distributed representations) put on the class of learnable
languages? Similarly, Regier suggests that the endpoint configuration prediction stems
</bodyText>
<page confidence="0.994734">
484
</page>
<subsectionHeader confidence="0.851056">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999975225806452">
from the assumption that &amp;quot;the model learns to categorize based on only those spatial
features that occur at the end of some event it has seen&amp;quot; (p. 157). Again, the prediction
seems to stem trivially from one of the features of the encoding system, not from any
property of the learning mechanism.
Now, one might argue that the connectionist part of the model is not supposed
to do any constraining of the typology, and that its value lies entirely in its role as
a learning mechanism, which is, perhaps, justification enough for its inclusion. The
trouble is, the learning mechanism probably is putting some typological constraints
on the model, but no effort is made to find out what these constraints are. The end-
point configuration issue makes this point clear. Regier&apos;s implication that the endpoint
configuration prediction is a hard constraint on the model&apos;s behavior may not be pre-
cisely true. The prediction about &amp;quot;through&amp;quot; stems from the fact that in order to have
passed through the landmark, the trajector must have been in the landmark at some
earlier time frame; only if the model has recognized &amp;quot;in&amp;quot; in an earlier time frame,
Regier implies, will it succeed in storing this fact in one of its maximum-detecting
units. Suppose, however, that the random initial weights of the network happened
to be set in such a way that a certain node in its hidden layer reached its maximum
value whenever the trajector was &amp;quot;in&amp;quot; the landmark. Suppose, furthermore, that the
network with this random initial setting was trained on a language that had a word
for &amp;quot;through&amp;quot; but no word for &amp;quot;in.&amp;quot; It seems probable that such a network would
be able to preserve the useful features of its initial weight setting and learn to rec-
ognize instances of &amp;quot;through&amp;quot; despite the fact that there is no word for &amp;quot;in,&amp;quot; thus
contradicting the endpoint configuration claim. However, such a result would contra-
dict the endpoint configuration claim in only a trivial sense because the chances of
encountering such a random initial weight setting are probably very slight. What is
the moral? Connectionist networks like Regier&apos;s probably do have typological implica-
tions in virtue of their connectionism. These implications are of a probabilistic nature,
not an absolute nature, so the fact that sufficiently large networks can emulate Turing
machines (Siegelmann and Sontag 1991) is not directly relevant. The presentation of
Regier&apos;s model as a typological theory would be of much greater interest if it assessed
these implications.
</bodyText>
<sectionHeader confidence="0.963234" genericHeader="method">
4. Within-Language Generalization
</sectionHeader>
<bodyText confidence="0.999906941176471">
The treatment of learning in the absence of negative evidence, although not much more
analytical than the treatment of typological constraints, is nevertheless much more ap-
pealing because it points the way to a more in-depth treatment. Regier notes that a
principle of mutual exclusivity has been posited in several guises in the child-language
literature (pp. 62-3): a child prefers that each entity have only one name. Clearly this
cannot be an absolute constraint, since many entities have multiple names. (Likewise,
many spatial relationship situations can be described by several spatial markers—for
example, a trajector can be simultaneously outside of and above a landmark.) Regier
translates this observation into a learning system in which the positive occurrence of
a word in a situation provides a strong error signal to a network learning to use the
word, but the failure of the occurrence of that word provides only a weak signal.
Regier provides evidence that it is only a particular relationship between the strengths
of errors on occurrences and non-occurrences that produces the right generalization
performance. He finds, for example, that if this parameter is set at its highest value
(so that non-occurrences generate no error signal at all), then there is rampant over-
generalization: every position is deemed to be &amp;quot;outside&amp;quot; the landmark. On the other
hand, if the value is set too low (so that non-occurrences count quite strongly), then
</bodyText>
<page confidence="0.996444">
485
</page>
<note confidence="0.713683">
Computational Linguistics Volume 23, Number 3
</note>
<bodyText confidence="0.99982025">
only a proper subset of the actual &amp;quot;outside&amp;quot; instances is recognized. Only when the
relationship has an appropriate intermediate value does the network actually recog-
nize as instances of &amp;quot;outside&amp;quot; precisely those positions that are not contained in the
landmark. What&apos;s nice about this result is that the domain is one in which we are able
to say in topologically precise terms what &amp;quot;correct generalization&amp;quot; is. It may even be
possible to formulate principles in terms of categorical notions like inclusion, contact,
directional orientation, and so on, which will tell how to generalize from given sets of
instances. One could then, perhaps, derive the correct ratio of error strengths, rather
than having to stipulate it. In this regard, it might be of some interest to try to in-
corporate Regier&apos;s results into the developing theory of the relationship between cost
functions and generalization performance (Rumelhart et al. 1995, Vapnik 1995). The
payoff of such an endeavor, if it succeeded, would be an analysis of how topologically
precise principles of generalization can arise from a statistical approximation mecha-
nism. Such a result would go some way toward eliminating the haze surrounding the
interpretation of neural networks as scientific theories. Regier&apos;s book does a good job
of laying the groundwork for such an investigation.
</bodyText>
<sectionHeader confidence="0.985235" genericHeader="method">
5. Readership Recommendations
</sectionHeader>
<bodyText confidence="0.999978571428572">
As the previous section has indicated, the book may be of particular interest to math-
ematically minded learning theorists who are looking for a domain in which to probe
the challenging problem of generalization. On the other hand, it is also a good in-
troduction to connectionist models for anyone who does not have a mathematical
background. It should be read by cognitive linguists in general for its effective em-
ployment of the hypothesis that a linguistic theory must be simultaneously a theory
of language and a theory of perception. It will be of interest to lexical semanticists
for its treatment of spatial markers and polysemy. Although its treatment of learned
structural constraints, as opposed to pre-encoded structural constraints, leaves some-
thing to be desired, as I have noted, the issue and implementation are laid out with
appealing clarity so that anyone interested in this dichotomy should take a look at
it. Finally, the book should be of interest to anyone who believes in the value of try-
ing to weave together constraints from a variety of domains into one large computer
simulation, as well as anyone who particularly doesn&apos;t.
</bodyText>
<reference confidence="0.9974698">
Rumelhart, David E., Richard Durbin, Siegelmann, Hava T. and Eduardo
Richard Golden, and Yves Chauvin. 1995. D. Sontag. 1991. Turing computability
Backpropagation: The basic theory. In Yves with neural nets. Applied Mathematics
Chauvin and David E. Rumelhart, editors, Letters, 4(6):77-80.
Backpropagation: Theory, Architectures, and Vapnik, V. 1995. The Nature of Statistical
Applications. Lawrence Erlbaum Learning Theory. Springer, New York.
Associates, Mahwah, NJ, pages 1-34.
Whitney Tabor, a postdoctoral fellow in the Mind Articulation Project at MIT, has developed
neural network models of language change and language processing. Tabor&apos;s address is MIT,
Bldg. 20C-228, Cambridge, MA 02139; e-mail: tabor@mitedu
</reference>
<page confidence="0.999042">
486
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.440621">
<title confidence="0.997629666666667">Book Reviews The Human Semantic Potential: Spatial Language and Constrained Connectionism</title>
<author confidence="0.999982">Terry Regier</author>
<affiliation confidence="0.999351">(University of Chicago)</affiliation>
<address confidence="0.905574">Cambridge, MA: The MIT Press</address>
<note confidence="0.8658">(Neural Network Modeling and Connectionism Series, edited by Jeffrey L. Elman), 1996, xv+220 pp; hardbound, ISBN 0-262-18173-8, $37.50 Reviewed by</note>
<author confidence="0.992508">Whitney Tabor</author>
<affiliation confidence="0.99012">Massachusetts Institute of Technology</affiliation>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Richard Durbin</author>
<author>Richard Golden</author>
<author>Yves Chauvin</author>
</authors>
<title>Backpropagation: The basic theory.</title>
<date>1995</date>
<booktitle>In Yves Chauvin and</booktitle>
<pages>1--34</pages>
<editor>David E. Rumelhart, editors, Backpropagation: Theory, Architectures, and Applications. Lawrence Erlbaum Associates, Mahwah, NJ,</editor>
<contexts>
<context position="11774" citStr="Rumelhart et al. 1995" startWordPosition="1857" endWordPosition="1860">in is one in which we are able to say in topologically precise terms what &amp;quot;correct generalization&amp;quot; is. It may even be possible to formulate principles in terms of categorical notions like inclusion, contact, directional orientation, and so on, which will tell how to generalize from given sets of instances. One could then, perhaps, derive the correct ratio of error strengths, rather than having to stipulate it. In this regard, it might be of some interest to try to incorporate Regier&apos;s results into the developing theory of the relationship between cost functions and generalization performance (Rumelhart et al. 1995, Vapnik 1995). The payoff of such an endeavor, if it succeeded, would be an analysis of how topologically precise principles of generalization can arise from a statistical approximation mechanism. Such a result would go some way toward eliminating the haze surrounding the interpretation of neural networks as scientific theories. Regier&apos;s book does a good job of laying the groundwork for such an investigation. 5. Readership Recommendations As the previous section has indicated, the book may be of particular interest to mathematically minded learning theorists who are looking for a domain in wh</context>
</contexts>
<marker>Rumelhart, Durbin, Golden, Chauvin, 1995</marker>
<rawString>Rumelhart, David E., Richard Durbin, Richard Golden, and Yves Chauvin. 1995. Backpropagation: The basic theory. In Yves Chauvin and David E. Rumelhart, editors, Backpropagation: Theory, Architectures, and Applications. Lawrence Erlbaum Associates, Mahwah, NJ, pages 1-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hava T Siegelmann</author>
<author>Eduardo D Sontag</author>
</authors>
<title>Turing computability with neural nets.</title>
<date>1991</date>
<journal>Applied Mathematics Letters,</journal>
<pages>4--6</pages>
<publisher>Springer,</publisher>
<location>Vapnik, V.</location>
<contexts>
<context position="9145" citStr="Siegelmann and Sontag 1991" startWordPosition="1444" endWordPosition="1447">nstances of &amp;quot;through&amp;quot; despite the fact that there is no word for &amp;quot;in,&amp;quot; thus contradicting the endpoint configuration claim. However, such a result would contradict the endpoint configuration claim in only a trivial sense because the chances of encountering such a random initial weight setting are probably very slight. What is the moral? Connectionist networks like Regier&apos;s probably do have typological implications in virtue of their connectionism. These implications are of a probabilistic nature, not an absolute nature, so the fact that sufficiently large networks can emulate Turing machines (Siegelmann and Sontag 1991) is not directly relevant. The presentation of Regier&apos;s model as a typological theory would be of much greater interest if it assessed these implications. 4. Within-Language Generalization The treatment of learning in the absence of negative evidence, although not much more analytical than the treatment of typological constraints, is nevertheless much more appealing because it points the way to a more in-depth treatment. Regier notes that a principle of mutual exclusivity has been posited in several guises in the child-language literature (pp. 62-3): a child prefers that each entity have only </context>
</contexts>
<marker>Siegelmann, Sontag, 1991</marker>
<rawString>Siegelmann, Hava T. and Eduardo D. Sontag. 1991. Turing computability with neural nets. Applied Mathematics Letters, 4(6):77-80. Vapnik, V. 1995. The Nature of Statistical Learning Theory. Springer, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>