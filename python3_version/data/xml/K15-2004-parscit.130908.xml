<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012694">
<title confidence="0.969349">
The SoNLP-DP System in the CoNLL-2015 shared Task
</title>
<author confidence="0.999323">
Fang Kong Sheng Li Guodong Zhou
</author>
<affiliation confidence="0.925268">
School of Computer Science and Technology, Soochow University, China
Gangjiang Road 333#, Suzhou, Jiangsu of China 215006
</affiliation>
<email confidence="0.986982">
kongfang@suda.edu.cn qcl6355@gmail.com gdzhou@suda.edu.cn
</email>
<sectionHeader confidence="0.993522" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986230769231">
This paper describes the submitted dis-
course parsing system of the natural
language group of Soochow University
(SoNLP-DP) to the CoNLL 2015 shared
task. Our System classifies discourse re-
lations into explicit and non-explicit re-
lations and uses a pipeline platform to
conduct every subtask to form an end-to-
end shallow discourse parser in the Penn
Discourse Treebank (PDTB). Our system
is evaluated on the CoNLL-2015 Shared
Task closed track and achieves the 18.51%
in F1-measure on the official blind test set.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949732142857">
Discourse parsing determines the internal struc-
ture of a text via identifying the discourse relations
between its text units and plays an important role
in natural language understanding that benefits a
wide range of downstream natural language ap-
plications, such as coherence modeling (Barzilay
and Lapata, 2005; Lin et al., 2011), text summa-
rization (Lin et al., 2012), and statistical machine
translation (Meyer and Webber, 2013).
As the largest discourse corpus, the Penn Dis-
course TreeBank (PDTB) corpus (Prasad et al.,
2008) adds a layer of discourse annotations on
the top of the Penn TreeBank (PTB) corpus (Mar-
cus et al., 1993) and has been attracting more
and more attention recently (Elwell and Baldridge,
2008; Pitler and Nenkova, 2009; Prasad et al.,
2010; Ghosh et al., 2011; Kong et al., 2014;
Lin et al., 2014). Different from another famous
discourse corpus, the Rhetorical Structure The-
ory(RST) Treebank corpus(Carlson et al., 2001),
the PDTB focuses on shallow discourse relations
either lexically grounded in explicit discourse con-
nectives or associated with sentential adjacency.
This theory-neutral way makes no commitment to
any kind of higher-level discourse structure and
can work jointly with high-level topic and func-
tional structuring (Webber et al., 2012) or hierar-
chial structuring (Asher and Lascarides, 2003).
Although much research work has been con-
ducted for certain subtasks since the release of
the PDTB corpus, there is still little work on con-
structing an end-to-end shallow discourse parser.
The CoNLL 2015 shared task (Xue et al., 2015)
evaluates end-to-end shallow discourse parsing
systems for determining and classifying both ex-
plicit and non-explicit discourse relations. A par-
ticipant system needs to (1)locate all explicit (e.g.,
”because”, ”however”, ”and”.) discourse connec-
tives in the text, (2)identify the spans of text that
serve as the two arguments for each discourse con-
nective, and (3) predict the sense of the discourse
relations (e.g., ”Cause”, ”Condition”, ”Contrast”).
In this paper, we describe the system submis-
sion from the NLP group of Soochow university
(SoNLP-DP). Our shallow discourse parser con-
sists of multiple components in a pipeline archi-
tecture, including a connective classifier, argu-
ment labeler, explicit classifier, non-explicit clas-
sifier. Our system is evaluated on the CoNLL-
2015 Shared Task closed track and achieves the
18.51% in F1-measure on the official blind test set.
The remainder of this paper is organized as fol-
lows. Section 2 presents our shallow discourse
parsing system. The experimental results are de-
scribed in Section 3. Section 4 concludes the pa-
per.
</bodyText>
<sectionHeader confidence="0.94805" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.999957666666667">
In this section, after a quick overview of our sys-
tem, we describe the details involved in imple-
menting the end-to-end shallow discourse parser.
</bodyText>
<subsectionHeader confidence="0.988792">
2.1 System Overview
</subsectionHeader>
<bodyText confidence="0.998425">
A typical text consists of sentences glued together
in a systematic way to form a coherent discourse.
</bodyText>
<page confidence="0.994278">
32
</page>
<note confidence="0.832758">
Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task, pages 32–36,
Beijing, China, July 26-31, 2015. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999106235294118">
Referring to the PDTB, shallow discourse parsing
focus on shallow discourse relations either lexi-
cally grounded in explicit discourse connectives
or associated with sentential adjacency. Differ-
ent from full discourse parsing, shallow discourse
parsing transforms a piece of text into a set of
discourse relations between two adjacent or non-
adjacent discourse units, instead of connecting the
relations hierarchically to one another to form a
connected structure in the form of tree or graph.
Specifically, given a piece of text, the end-to-
end shallow discourse parser returns a set of dis-
course relations in the form of a discourse con-
nective (explicit or implicit) taking two arguments
(clauses or sentences) with a discourse sense. That
is, a complete end-to-end shallow discourse parser
includes:
</bodyText>
<listItem confidence="0.986946916666667">
• connective identification, which identifies all
connective candidates and labels them as
whether they function as discourse connec-
tives or not,
• argument labeling, which identifies the spans
of text that serve as the two arguments for
each discourse connective,
• explicit sense classification, which predicts
the sense of the explicit discourse relations
after achieving the connective and its argu-
ments,
• non-explicit sense classification, for all ad-
</listItem>
<bodyText confidence="0.950362583333333">
jacent sentence pairs within each paragraph
without explicit discourse relations, which
classify the given pair into EntRel, NoRel, or
one of the Implicit/AltLex relation senses.
Figure 1 shows the components and the rela-
tions among them. Different from the traditional
approach (i.e., Lin et al. (2014)), considering the
interaction between argument labeler and explicit
sense classifier, co-occurrence relation between
explicit and non-explicit discourse relations in a
text, our system does not employ a complete se-
quential pipeline framework.
</bodyText>
<subsectionHeader confidence="0.996627">
2.2 Connective Identification
</subsectionHeader>
<bodyText confidence="0.9957734">
Our connective identifier works in two steps.
First, the connective candidates are extracted from
the given text referring to the PDTB. There are
100 types of discourse connectives defined in
the PDTB. Then every connective candidate is
</bodyText>
<figureCaption confidence="0.9436625">
Figure 1: Framework of our end-to-end shallow
discourse parser
</figureCaption>
<bodyText confidence="0.999013111111111">
checked whether it functions as a discourse con-
nective.
Pitler and Nenkova (2009) showed that syntac-
tic features extracted from constituent parse trees
are very useful in disambiguating discourse con-
nectives. Followed their work, Lin et al. (2014)
found that a connective’s context and part-of-
speech (POS) are also helpful. Motivated by their
work, we get a set of effective features, includes:
</bodyText>
<listItem confidence="0.930935272727273">
• Lexical: connective itself, POS of the con-
nective, connective with its previous word,
connective with its next word, the location of
the connective in the sentence, i.e., start, mid-
dle and end of the sentence.
• Syntactic: the highest node in the parse
tree that covers only the connective words
(dominate node), the context of the dominate
node 1, whether the right sibling contains a
VP, the path from the parent node of the con-
nective to the root of the parse tree.
</listItem>
<bodyText confidence="0.59374925">
Besides, we observed that the syntactic class of
the connective2 and connective modifier (such as
1We use POS combination of the parent, left sibling and
right sibling of the dominate node to represent the context.
When no parent or siblings, it is marked NULL.
2All the connectives are classified into four well-defined
syntactic classes: subordinating conjunctions, coordinating
conjunctions, prepositional phrases and adverbs.
</bodyText>
<page confidence="0.997089">
33
</page>
<bodyText confidence="0.999938">
apparently, in large part, etc.) give a very strong
indication of its discourse usage. So we introduce
both as two additional features.
</bodyText>
<subsectionHeader confidence="0.999902">
2.3 Argument Labeling
</subsectionHeader>
<bodyText confidence="0.999989214285714">
The argument labeler needs to label the Arg1 and
Arg2 spans for every connective determined by
connective identifier. Following the work of Kong
et al. (2014), we employ the constituent-based ap-
proach to argument labeling by first extracting the
constituents from a parse tree are casted as argu-
ment candidates, then determining the role of ev-
ery constituent as part of Arg1, Arg2, or NULL,
and finally, merging all the constituents for Arg1
and Arg2 to obtain the Arg1 and Arg2 text spans
respectively.
Specifically, similar to semantic role labeling
(SRL), we use a simple algorithm to prune out
those constituents that are clearly not arguments to
the connective in question. The pruning algorithm
works recursively in preprocessing, starting from
the target connective node, i.e. the lowest node
dominating the connective. First, all the siblings
of the connective node are collected as candidates.
Then we move on to the parent of the connective
node and collect its siblings. This progress goes
on until we reach the root of the parse tree.
After extracting the argument candidates, a
multi-category classifier is employed to determine
the role of every argument candidate (i.e., Arg1,
Arg2, or NULL) with features reflecting the prop-
erties of the connective, the candidate constituent
and relationship between them. Features include,
</bodyText>
<listItem confidence="0.997436923076923">
• Connective related features: connective it-
self, its syntactic category, its sense class.3
• Number of left/right siblings of the connec-
tive.
• The context of the constituent. We use POS
combination of the constituent, its parent, left
sibling and right sibling to represent the con-
text. When there is no parent or siblings, it is
marked NULL.
• The path from the parent node of the connec-
tive to the node of the constituent.
• The position of the constituent relative to the
connective: left, right, or previous.
</listItem>
<footnote confidence="0.983787">
3In training stage, we extract the gold sense class from the
annotated corpus. And in testing stage, the sense classifica-
tion will be employed to get the automatic sense.
</footnote>
<subsectionHeader confidence="0.980474">
2.4 Explicit sense classification
</subsectionHeader>
<bodyText confidence="0.999962875">
After a discourse connective and its two arguments
are identified, the sense classifier is proved to de-
cide the sense that the relation conveys.
Although the same connective may carry differ-
ent semantics under different contexts, only a few
connectives are ambiguous (Pitler and Nenkova,
2009). Following the work of Lin et al. (2014),
we introduce three features to train a sense classi-
fier: the connective itself, its POS and the previous
word of the connective.
Besides, since we observed that various relative
positions (i.e., Arg1 precedes Arg2, Arg2 precedes
Arg1, Arg2 is embedded within Arg1, or Arg1 is
embedded within Arg2) are helpful for sense clas-
sification, we includes the relative position as an
additional feature.
</bodyText>
<subsectionHeader confidence="0.984267">
2.5 Non-explicit sense Classification
</subsectionHeader>
<bodyText confidence="0.999926964285714">
Referring to the PDTB, the non-explicit relations4
are annotated for all adjacent sentence pairs within
paragraphs. So non-explicit sense classification
only considers the sense of every adjacent sen-
tence pair within a paragraph without explicit dis-
course relations.
Our non-explicit sense classifier includes seven
traditional features:
Verbs: Following the work of Pitler et
al. (2009), we extract the pairs of verbs from the
given adjacent sentence pair (i.e., Arg1 and Arg2).
Besides that, the number of verb pairs which have
the same highest VerbNet verb class (Kipper et al.,
2006) is included as a feature. the average length
of verb phrases in each argument, and the POS of
main verbs are also included.
Polarity: This set of features record the number
of positive, negated positive, negative and neutral
words in both arguments and their cross-product.
The polarity of every word in arguments is de-
rived from Multi-perspective Question Answering
Opinion Corpus(MPQA) (Wilson et al., 2005). In-
tuitively, polarity features would help recognize
Comparison relations.
Modality: We include a set of features to record
the presence or absence of specific modal words
(i.e., can, may, will, shall, must, need) in Arg1
and Arg2, and their cross-product. The intuition
</bodyText>
<footnote confidence="0.9990025">
4The PDTB provides annotation for Implicit relations, Al-
tLex relations, entity transition (EntRel), and otherwise no
relation (NoRel), which are lumped together as Non-Explicit
relations.
</footnote>
<page confidence="0.998911">
34
</page>
<bodyText confidence="0.994640366666667">
behind this feature set is that the Contingency re-
lations seem to have more modal words.
Production rules: According to Lin et
al. (2009), the syntactic structure of one argument
may constrain the relation type and the syntactic
structure of the other argument. Three features are
introduced to denote the presence of syntactic pro-
ductions in Arg1, Arg2 or both. Here, these pro-
duction rules are extracted from the training data
and the rules with frequency less than 5 are ig-
nored.
Dependency rules: Similar with Production
rules, three features denoting the presence of de-
pendency productions in Arg1, Arg2 or both are
also introduced in our system.
Fisrt/Last and First 3 words: This set of fea-
tures include the first and last words of Arg1, the
first and last words of Arg2, the pair of the first
words of Arg1 and Arg2, the pair of the last words
as features, and the first three words of each argu-
ment.
Brown cluster pairs: We include the Cartesian
product of the Brown cluster values of the words in
Arg1 and Arg2. In our system, we simply take 100
Brown clusters provided by CoNLL shared task.
Besides, we introduce two features which de-
scribe the automatic determined connective list
contained by Arg1 and Arg2, respectively, to cap-
ture the co-occurrence relationship between non-
explicit and explicit discourse relations.
</bodyText>
<sectionHeader confidence="0.997002" genericHeader="method">
3 Experimentation
</sectionHeader>
<bodyText confidence="0.9998854">
We train our system on the corpora provided in the
CoNLL-2015 Shared Task and evaluate our sys-
tem on the CoNLL-2015 Shared Task closed track.
All our classifiers are trained using the OpenNLP
maximum entropy package5 with the default pa-
rameters (i.e. without smoothing and with 100 it-
erations). We firstly report the official score on
the CoNLL-2015 shared task on development, test
and blind test sets. Then, the supplementary re-
sults provided by the shared task organizes are re-
ported.
In Table 1, we present the official results of our
system performances on the CoNLL-2015 devel-
opment, test and blind test sets, respectively. From
the results, we can find that,
</bodyText>
<listItem confidence="0.984876">
• For Connective identification, our system
achieved satisfactory results.
</listItem>
<footnote confidence="0.964615">
5http://maxent.sourceforge.net/
</footnote>
<table confidence="0.999621285714286">
Development Test Blind Test
Arg1&amp;2 43.12 37.01 33.23
Arg1 57.28 52.45 46.28
Arg2 67.72 63.57 61.70
Connective 94.22 94.77 91.62
Sense 17.80 18.38 16.93
Parser 26.32 20.64 18.51
</table>
<tableCaption confidence="0.999739">
Table 1: the official F1 score of our system.
</tableCaption>
<bodyText confidence="0.734868">
• For argument labeling, the performance of
Arg2 is better than Arg1 and the performance
gaps are more than 10% in F1-measure. And
the combined results of Arg1 and Arg2 ex-
tractor reduced so much in comparison with
the performance of Arg1 or Arg2.
</bodyText>
<listItem confidence="0.998806333333333">
• For sense classification, there is a lot of room
to improve.
• For the overall parser performance, obvi-
ously, a lot of work is needed for end-to-
end discourse parsing before practical appli-
cation.
</listItem>
<table confidence="0.999599428571429">
Arg1&amp;2 Arg1 Arg2 Sense Parser
Dev Exp 34.67 38.67 74.37 20.18 29.78
nonExp 49.94 62.13 62.37 7.37 23.54
Test Exp 30.21 34.02 74.48 20.59 25.30
nonExp 42.38 57.71 54.95 6.77 16.97
Blind Exp 30.42 36.43 73.04 17.36 22.95
nonExp 35.87 49.87 51.07 5.24 14.35
</table>
<tableCaption confidence="0.8619135">
Table 2: the supplementary F1 score of our sys-
tem.
</tableCaption>
<bodyText confidence="0.9999197">
In Table 2, we reported the supplementary re-
sults provided by the shared task organizes on the
development, test and blind test sets. These ad-
ditional experiments investigate the performance
of our shallow discourse parsing for explicit and
non-explicit relations separately. From the results,
we can find that the sense classification for both
explicit and non-explicit discourse relations are
the biggest obstacles to the overall performance of
discourse parsing.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999976666666667">
We have presented the SoNLP-DP system from
the NLP group of Soochow university that par-
ticipated in the CoNLL-2015 shared task. Our
system is evaluated on the CoNLL-2015 Shared
Task closed track and achieves the 18.51% in F1-
measure on the official blind test set.
</bodyText>
<page confidence="0.998924">
35
</page>
<sectionHeader confidence="0.996531" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9993892">
This research is supported by Key project
61333018 and 61331011 under the National Natu-
ral Science Foundation of China, Project 6127320
and 61472264 under the National Natural Science
Foundation of China.
</bodyText>
<sectionHeader confidence="0.998482" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989210125">
Nicholas Asher and Alex Lascarides. 2003. Logics of
Conversation. Cambridge Unversity Press.
Regina Barzilay and Mirella Lapata. 2005. Model-
ing local coherence: An entity-based approach. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In Proceedings of Second SIGdial Workshop on Dis-
course and Dialogue.
Robert Elwell and Jason Baldridge. 2008. Discourse
connective argument identification with connective
specific rankers. In Second IEEE International Con-
ference on Semantic Computing.
Sucheta Ghosh, Richard Johansson, Giuseppe Ric-
cardi, and Sara Tonelli. 2011. Shallow discourse
parsing with conditional random fields. In Proceed-
ings of the 5th International Joint Conference on
Natural Language Processing.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extending verbnet with novel
verb classes. In Fifth Internal Conference on Lan-
guage Resources and Evaluation.
</reference>
<bodyText confidence="0.7376078">
Fang Kong, Hwee Tou Ng, and Guodong Zhou. 2014.
A constituent-based approach to argument labeling
with joint inference in discourse parsing. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing.
</bodyText>
<reference confidence="0.999263092592592">
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the Penn
Discourse Treebank. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics.
Ziheng Lin, Chang Liu, Hwee Tou Ng, and Min-Yen
Kan. 2012. Combining coherence models and ma-
chine translation evaluation metrics for summariza-
tion evaluation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20(2):151–184.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Thomas Meyer and Bonnie Webber. 2013. Implicita-
tion of discourse connectives in (machine) transla-
tion. In Proceedings of the Workshop on Discourse
in Machine Translation.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the LREC 2008 Conference.
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010. Exploiting scope for shallow discourse pars-
ing. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation.
Bonnie Webber, Marcus Egg, and Valia Kordoni. 2012.
Discourse structure and language technology. Natu-
ral Language Engineering, 18(4):437–490, 10.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing.
Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Rashmi
Prasad, Christopher Bryant, and Attapol Ruther-
ford. 2015. The conll-2015 shared task on shal-
low discourse parsing. In Proceedings of the Nine-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task.
</reference>
<page confidence="0.998939">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.251957">
<title confidence="0.99863">The SoNLP-DP System in the CoNLL-2015 shared Task</title>
<author confidence="0.998811">Fang Kong Sheng Li Guodong</author>
<affiliation confidence="0.999988">School of Computer Science and Technology, Soochow University,</affiliation>
<address confidence="0.464158">Gangjiang Road 333#, Suzhou, Jiangsu of China</address>
<email confidence="0.723773">kongfang@suda.edu.cnqcl6355@gmail.comgdzhou@suda.edu.cn</email>
<abstract confidence="0.920059714285714">This paper describes the submitted discourse parsing system of the natural language group of Soochow University (SoNLP-DP) to the CoNLL 2015 shared task. Our System classifies discourse relations into explicit and non-explicit relations and uses a pipeline platform to conduct every subtask to form an end-toend shallow discourse parser in the Penn Discourse Treebank (PDTB). Our system is evaluated on the CoNLL-2015 Shared Task closed track and achieves the 18.51% in F1-measure on the official blind test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Alex Lascarides</author>
</authors>
<title>Logics of Conversation.</title>
<date>2003</date>
<publisher>Cambridge Unversity Press.</publisher>
<contexts>
<context position="2138" citStr="Asher and Lascarides, 2003" startWordPosition="322" endWordPosition="325">ge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to any kind of higher-level discourse structure and can work jointly with high-level topic and functional structuring (Webber et al., 2012) or hierarchial structuring (Asher and Lascarides, 2003). Although much research work has been conducted for certain subtasks since the release of the PDTB corpus, there is still little work on constructing an end-to-end shallow discourse parser. The CoNLL 2015 shared task (Xue et al., 2015) evaluates end-to-end shallow discourse parsing systems for determining and classifying both explicit and non-explicit discourse relations. A participant system needs to (1)locate all explicit (e.g., ”because”, ”however”, ”and”.) discourse connectives in the text, (2)identify the spans of text that serve as the two arguments for each discourse connective, and (3</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>Nicholas Asher and Alex Lascarides. 2003. Logics of Conversation. Cambridge Unversity Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1111" citStr="Barzilay and Lapata, 2005" startWordPosition="162" endWordPosition="165">on-explicit relations and uses a pipeline platform to conduct every subtask to form an end-toend shallow discourse parser in the Penn Discourse Treebank (PDTB). Our system is evaluated on the CoNLL-2015 Shared Task closed track and achieves the 18.51% in F1-measure on the official blind test set. 1 Introduction Discourse parsing determines the internal structure of a text via identifying the discourse relations between its text units and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) </context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: An entity-based approach. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2001</date>
<booktitle>In Proceedings of Second SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="1748" citStr="Carlson et al., 2001" startWordPosition="266" endWordPosition="269">11), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to any kind of higher-level discourse structure and can work jointly with high-level topic and functional structuring (Webber et al., 2012) or hierarchial structuring (Asher and Lascarides, 2003). Although much research work has been conducted for certain subtasks since the release of the PDTB corpus, there is still little work on constructing an end-to-end shallow discourse parser. The CoNLL 2015 shar</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Proceedings of Second SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Elwell</author>
<author>Jason Baldridge</author>
</authors>
<title>Discourse connective argument identification with connective specific rankers.</title>
<date>2008</date>
<booktitle>In Second IEEE International Conference on Semantic Computing.</booktitle>
<contexts>
<context position="1519" citStr="Elwell and Baldridge, 2008" startWordPosition="230" endWordPosition="233">lations between its text units and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to any kind of higher-level discourse structure and can work jointly with high-level topic and functional structuring (Webber et al., 2012) or hierarchial structuring (Asher an</context>
</contexts>
<marker>Elwell, Baldridge, 2008</marker>
<rawString>Robert Elwell and Jason Baldridge. 2008. Discourse connective argument identification with connective specific rankers. In Second IEEE International Conference on Semantic Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sucheta Ghosh</author>
<author>Richard Johansson</author>
<author>Giuseppe Riccardi</author>
<author>Sara Tonelli</author>
</authors>
<title>Shallow discourse parsing with conditional random fields.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="1586" citStr="Ghosh et al., 2011" startWordPosition="242" endWordPosition="245">age understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to any kind of higher-level discourse structure and can work jointly with high-level topic and functional structuring (Webber et al., 2012) or hierarchial structuring (Asher and Lascarides, 2003). Although much research work has been conducted</context>
</contexts>
<marker>Ghosh, Johansson, Riccardi, Tonelli, 2011</marker>
<rawString>Sucheta Ghosh, Richard Johansson, Giuseppe Riccardi, and Sara Tonelli. 2011. Shallow discourse parsing with conditional random fields. In Proceedings of the 5th International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Anna Korhonen</author>
<author>Neville Ryant</author>
<author>Martha Palmer</author>
</authors>
<title>Extending verbnet with novel verb classes.</title>
<date>2006</date>
<booktitle>In Fifth Internal Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="10941" citStr="Kipper et al., 2006" startWordPosition="1696" endWordPosition="1699">on-explicit sense Classification Referring to the PDTB, the non-explicit relations4 are annotated for all adjacent sentence pairs within paragraphs. So non-explicit sense classification only considers the sense of every adjacent sentence pair within a paragraph without explicit discourse relations. Our non-explicit sense classifier includes seven traditional features: Verbs: Following the work of Pitler et al. (2009), we extract the pairs of verbs from the given adjacent sentence pair (i.e., Arg1 and Arg2). Besides that, the number of verb pairs which have the same highest VerbNet verb class (Kipper et al., 2006) is included as a feature. the average length of verb phrases in each argument, and the POS of main verbs are also included. Polarity: This set of features record the number of positive, negated positive, negative and neutral words in both arguments and their cross-product. The polarity of every word in arguments is derived from Multi-perspective Question Answering Opinion Corpus(MPQA) (Wilson et al., 2005). Intuitively, polarity features would help recognize Comparison relations. Modality: We include a set of features to record the presence or absence of specific modal words (i.e., can, may, </context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2006</marker>
<rawString>Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2006. Extending verbnet with novel verb classes. In Fifth Internal Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the Penn Discourse Treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="11951" citStr="Lin et al. (2009)" startWordPosition="1853" endWordPosition="1856">son et al., 2005). Intuitively, polarity features would help recognize Comparison relations. Modality: We include a set of features to record the presence or absence of specific modal words (i.e., can, may, will, shall, must, need) in Arg1 and Arg2, and their cross-product. The intuition 4The PDTB provides annotation for Implicit relations, AltLex relations, entity transition (EntRel), and otherwise no relation (NoRel), which are lumped together as Non-Explicit relations. 34 behind this feature set is that the Contingency relations seem to have more modal words. Production rules: According to Lin et al. (2009), the syntactic structure of one argument may constrain the relation type and the syntactic structure of the other argument. Three features are introduced to denote the presence of syntactic productions in Arg1, Arg2 or both. Here, these production rules are extracted from the training data and the rules with frequency less than 5 are ignored. Dependency rules: Similar with Production rules, three features denoting the presence of dependency productions in Arg1, Arg2 or both are also introduced in our system. Fisrt/Last and First 3 words: This set of features include the first and last words o</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the Penn Discourse Treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>Automatically evaluating text coherence using discourse relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1130" citStr="Lin et al., 2011" startWordPosition="166" endWordPosition="169">ses a pipeline platform to conduct every subtask to form an end-toend shallow discourse parser in the Penn Discourse Treebank (PDTB). Our system is evaluated on the CoNLL-2015 Shared Task closed track and achieves the 18.51% in F1-measure on the official blind test set. 1 Introduction Discourse parsing determines the internal structure of a text via identifying the discourse relations between its text units and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Car</context>
</contexts>
<marker>Lin, Ng, Kan, 2011</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically evaluating text coherence using discourse relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Chang Liu</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>Combining coherence models and machine translation evaluation metrics for summarization evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1169" citStr="Lin et al., 2012" startWordPosition="173" endWordPosition="176">y subtask to form an end-toend shallow discourse parser in the Penn Discourse Treebank (PDTB). Our system is evaluated on the CoNLL-2015 Shared Task closed track and achieves the 18.51% in F1-measure on the official blind test set. 1 Introduction Discourse parsing determines the internal structure of a text via identifying the discourse relations between its text units and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on</context>
</contexts>
<marker>Lin, Liu, Ng, Kan, 2012</marker>
<rawString>Ziheng Lin, Chang Liu, Hwee Tou Ng, and Min-Yen Kan. 2012. Combining coherence models and machine translation evaluation metrics for summarization evaluation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A PDTB-styled end-to-end discourse parser.</title>
<date>2014</date>
<journal>Natural Language Engineering,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="1624" citStr="Lin et al., 2014" startWordPosition="250" endWordPosition="253">range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to any kind of higher-level discourse structure and can work jointly with high-level topic and functional structuring (Webber et al., 2012) or hierarchial structuring (Asher and Lascarides, 2003). Although much research work has been conducted for certain subtasks since the releas</context>
<context position="5492" citStr="Lin et al. (2014)" startWordPosition="836" endWordPosition="839">ot, • argument labeling, which identifies the spans of text that serve as the two arguments for each discourse connective, • explicit sense classification, which predicts the sense of the explicit discourse relations after achieving the connective and its arguments, • non-explicit sense classification, for all adjacent sentence pairs within each paragraph without explicit discourse relations, which classify the given pair into EntRel, NoRel, or one of the Implicit/AltLex relation senses. Figure 1 shows the components and the relations among them. Different from the traditional approach (i.e., Lin et al. (2014)), considering the interaction between argument labeler and explicit sense classifier, co-occurrence relation between explicit and non-explicit discourse relations in a text, our system does not employ a complete sequential pipeline framework. 2.2 Connective Identification Our connective identifier works in two steps. First, the connective candidates are extracted from the given text referring to the PDTB. There are 100 types of discourse connectives defined in the PDTB. Then every connective candidate is Figure 1: Framework of our end-to-end shallow discourse parser checked whether it functio</context>
<context position="9918" citStr="Lin et al. (2014)" startWordPosition="1539" endWordPosition="1542">tion of the constituent relative to the connective: left, right, or previous. 3In training stage, we extract the gold sense class from the annotated corpus. And in testing stage, the sense classification will be employed to get the automatic sense. 2.4 Explicit sense classification After a discourse connective and its two arguments are identified, the sense classifier is proved to decide the sense that the relation conveys. Although the same connective may carry different semantics under different contexts, only a few connectives are ambiguous (Pitler and Nenkova, 2009). Following the work of Lin et al. (2014), we introduce three features to train a sense classifier: the connective itself, its POS and the previous word of the connective. Besides, since we observed that various relative positions (i.e., Arg1 precedes Arg2, Arg2 precedes Arg1, Arg2 is embedded within Arg1, or Arg1 is embedded within Arg2) are helpful for sense classification, we includes the relative position as an additional feature. 2.5 Non-explicit sense Classification Referring to the PDTB, the non-explicit relations4 are annotated for all adjacent sentence pairs within paragraphs. So non-explicit sense classification only consid</context>
</contexts>
<marker>Lin, Ng, Kan, 2014</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A PDTB-styled end-to-end discourse parser. Natural Language Engineering, 20(2):151–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1434" citStr="Marcus et al., 1993" startWordPosition="216" endWordPosition="220">ng determines the internal structure of a text via identifying the discourse relations between its text units and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to any kind of higher-level discourse structure and can work jointly with high-level topic </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Meyer</author>
<author>Bonnie Webber</author>
</authors>
<title>Implicitation of discourse connectives in (machine) translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Discourse in Machine Translation.</booktitle>
<contexts>
<context position="1231" citStr="Meyer and Webber, 2013" startWordPosition="181" endWordPosition="184">in the Penn Discourse Treebank (PDTB). Our system is evaluated on the CoNLL-2015 Shared Task closed track and achieves the 18.51% in F1-measure on the official blind test set. 1 Introduction Discourse parsing determines the internal structure of a text via identifying the discourse relations between its text units and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in expl</context>
</contexts>
<marker>Meyer, Webber, 2013</marker>
<rawString>Thomas Meyer and Bonnie Webber. 2013. Implicitation of discourse connectives in (machine) translation. In Proceedings of the Workshop on Discourse in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Using syntax to disambiguate explicit discourse connectives in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers.</booktitle>
<contexts>
<context position="1545" citStr="Pitler and Nenkova, 2009" startWordPosition="234" endWordPosition="237">ts and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to any kind of higher-level discourse structure and can work jointly with high-level topic and functional structuring (Webber et al., 2012) or hierarchial structuring (Asher and Lascarides, 2003). Altho</context>
<context position="6147" citStr="Pitler and Nenkova (2009)" startWordPosition="930" endWordPosition="933"> between argument labeler and explicit sense classifier, co-occurrence relation between explicit and non-explicit discourse relations in a text, our system does not employ a complete sequential pipeline framework. 2.2 Connective Identification Our connective identifier works in two steps. First, the connective candidates are extracted from the given text referring to the PDTB. There are 100 types of discourse connectives defined in the PDTB. Then every connective candidate is Figure 1: Framework of our end-to-end shallow discourse parser checked whether it functions as a discourse connective. Pitler and Nenkova (2009) showed that syntactic features extracted from constituent parse trees are very useful in disambiguating discourse connectives. Followed their work, Lin et al. (2014) found that a connective’s context and part-ofspeech (POS) are also helpful. Motivated by their work, we get a set of effective features, includes: • Lexical: connective itself, POS of the connective, connective with its previous word, connective with its next word, the location of the connective in the sentence, i.e., start, middle and end of the sentence. • Syntactic: the highest node in the parse tree that covers only the conne</context>
<context position="9877" citStr="Pitler and Nenkova, 2009" startWordPosition="1531" endWordPosition="1534">ective to the node of the constituent. • The position of the constituent relative to the connective: left, right, or previous. 3In training stage, we extract the gold sense class from the annotated corpus. And in testing stage, the sense classification will be employed to get the automatic sense. 2.4 Explicit sense classification After a discourse connective and its two arguments are identified, the sense classifier is proved to decide the sense that the relation conveys. Although the same connective may carry different semantics under different contexts, only a few connectives are ambiguous (Pitler and Nenkova, 2009). Following the work of Lin et al. (2014), we introduce three features to train a sense classifier: the connective itself, its POS and the previous word of the connective. Besides, since we observed that various relative positions (i.e., Arg1 precedes Arg2, Arg2 precedes Arg1, Arg2 is embedded within Arg1, or Arg1 is embedded within Arg2) are helpful for sense classification, we includes the relative position as an additional feature. 2.5 Non-explicit sense Classification Referring to the PDTB, the non-explicit relations4 are annotated for all adjacent sentence pairs within paragraphs. So non-</context>
</contexts>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the LREC 2008 Conference.</booktitle>
<contexts>
<context position="1329" citStr="Prasad et al., 2008" startWordPosition="197" endWordPosition="200">rack and achieves the 18.51% in F1-measure on the official blind test set. 1 Introduction Discourse parsing determines the internal structure of a text via identifying the discourse relations between its text units and plays an important role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes </context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse TreeBank 2.0. In Proceedings of the LREC 2008 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>Exploiting scope for shallow discourse parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="1566" citStr="Prasad et al., 2010" startWordPosition="238" endWordPosition="241">role in natural language understanding that benefits a wide range of downstream natural language applications, such as coherence modeling (Barzilay and Lapata, 2005; Lin et al., 2011), text summarization (Lin et al., 2012), and statistical machine translation (Meyer and Webber, 2013). As the largest discourse corpus, the Penn Discourse TreeBank (PDTB) corpus (Prasad et al., 2008) adds a layer of discourse annotations on the top of the Penn TreeBank (PTB) corpus (Marcus et al., 1993) and has been attracting more and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to any kind of higher-level discourse structure and can work jointly with high-level topic and functional structuring (Webber et al., 2012) or hierarchial structuring (Asher and Lascarides, 2003). Although much research wor</context>
</contexts>
<marker>Prasad, Joshi, Webber, 2010</marker>
<rawString>Rashmi Prasad, Aravind Joshi, and Bonnie Webber. 2010. Exploiting scope for shallow discourse parsing. In Proceedings of the Seventh International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
<author>Marcus Egg</author>
<author>Valia Kordoni</author>
</authors>
<title>Discourse structure and language technology.</title>
<date>2012</date>
<journal>Natural Language Engineering,</journal>
<volume>18</volume>
<issue>4</issue>
<pages>10</pages>
<contexts>
<context position="2082" citStr="Webber et al., 2012" startWordPosition="314" endWordPosition="317">e and more attention recently (Elwell and Baldridge, 2008; Pitler and Nenkova, 2009; Prasad et al., 2010; Ghosh et al., 2011; Kong et al., 2014; Lin et al., 2014). Different from another famous discourse corpus, the Rhetorical Structure Theory(RST) Treebank corpus(Carlson et al., 2001), the PDTB focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to any kind of higher-level discourse structure and can work jointly with high-level topic and functional structuring (Webber et al., 2012) or hierarchial structuring (Asher and Lascarides, 2003). Although much research work has been conducted for certain subtasks since the release of the PDTB corpus, there is still little work on constructing an end-to-end shallow discourse parser. The CoNLL 2015 shared task (Xue et al., 2015) evaluates end-to-end shallow discourse parsing systems for determining and classifying both explicit and non-explicit discourse relations. A participant system needs to (1)locate all explicit (e.g., ”because”, ”however”, ”and”.) discourse connectives in the text, (2)identify the spans of text that serve as</context>
</contexts>
<marker>Webber, Egg, Kordoni, 2012</marker>
<rawString>Bonnie Webber, Marcus Egg, and Valia Kordoni. 2012. Discourse structure and language technology. Natural Language Engineering, 18(4):437–490, 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="11351" citStr="Wilson et al., 2005" startWordPosition="1761" endWordPosition="1764">al. (2009), we extract the pairs of verbs from the given adjacent sentence pair (i.e., Arg1 and Arg2). Besides that, the number of verb pairs which have the same highest VerbNet verb class (Kipper et al., 2006) is included as a feature. the average length of verb phrases in each argument, and the POS of main verbs are also included. Polarity: This set of features record the number of positive, negated positive, negative and neutral words in both arguments and their cross-product. The polarity of every word in arguments is derived from Multi-perspective Question Answering Opinion Corpus(MPQA) (Wilson et al., 2005). Intuitively, polarity features would help recognize Comparison relations. Modality: We include a set of features to record the presence or absence of specific modal words (i.e., can, may, will, shall, must, need) in Arg1 and Arg2, and their cross-product. The intuition 4The PDTB provides annotation for Implicit relations, AltLex relations, entity transition (EntRel), and otherwise no relation (NoRel), which are lumped together as Non-Explicit relations. 34 behind this feature set is that the Contingency relations seem to have more modal words. Production rules: According to Lin et al. (2009)</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Hwee Tou Ng</author>
<author>Sameer Pradhan</author>
<author>Rashmi Prasad</author>
<author>Christopher Bryant</author>
<author>Attapol Rutherford</author>
</authors>
<title>The conll-2015 shared task on shallow discourse parsing.</title>
<date>2015</date>
<booktitle>In Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task.</booktitle>
<contexts>
<context position="2374" citStr="Xue et al., 2015" startWordPosition="362" endWordPosition="365">B focuses on shallow discourse relations either lexically grounded in explicit discourse connectives or associated with sentential adjacency. This theory-neutral way makes no commitment to any kind of higher-level discourse structure and can work jointly with high-level topic and functional structuring (Webber et al., 2012) or hierarchial structuring (Asher and Lascarides, 2003). Although much research work has been conducted for certain subtasks since the release of the PDTB corpus, there is still little work on constructing an end-to-end shallow discourse parser. The CoNLL 2015 shared task (Xue et al., 2015) evaluates end-to-end shallow discourse parsing systems for determining and classifying both explicit and non-explicit discourse relations. A participant system needs to (1)locate all explicit (e.g., ”because”, ”however”, ”and”.) discourse connectives in the text, (2)identify the spans of text that serve as the two arguments for each discourse connective, and (3) predict the sense of the discourse relations (e.g., ”Cause”, ”Condition”, ”Contrast”). In this paper, we describe the system submission from the NLP group of Soochow university (SoNLP-DP). Our shallow discourse parser consists of mult</context>
</contexts>
<marker>Xue, Ng, Pradhan, Prasad, Bryant, Rutherford, 2015</marker>
<rawString>Nianwen Xue, Hwee Tou Ng, Sameer Pradhan, Rashmi Prasad, Christopher Bryant, and Attapol Rutherford. 2015. The conll-2015 shared task on shallow discourse parsing. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning: Shared Task.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>