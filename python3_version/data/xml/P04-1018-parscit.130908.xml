<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9685295">
A Mention-Synchronous Coreference Resolution Algorithm Based on the
Bell Tree
</title>
<author confidence="0.707553">
Xiaoqiang Luo and Abe Ittycheriah
Hongyan Jing and Nanda Kambhatla and Salim Roukos
</author>
<address confidence="0.813531">
1101 Kitchawan Road
Yorktown Heights, NY 10598, U.S.A.
</address>
<email confidence="0.998861">
{xiaoluo,abei,hjing,nanda,roukos}@us.ibm.com
</email>
<sectionHeader confidence="0.995639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902416666667">
This paper proposes a new approach for
coreference resolution which uses the Bell
tree to represent the search space and casts
the coreference resolution problem as finding
the best path from the root of the Bell tree to
the leaf nodes. A Maximum Entropy model
is used to rank these paths. The coreference
performance on the 2002 and 2003 Auto-
matic Content Extraction (ACE) data will be
reported. We also train a coreference system
using the MUC6 data and competitive results
are obtained.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999845089285715">
In this paper, we will adopt the terminologies used in
the Automatic Content Extraction (ACE) task (NIST,
2003). Coreference resolution in this context is defined
as partitioning mentions into entities. A mention is an
instance of reference to an object, and the collection
of mentions referring to the same object in a document
form an entity. For example, in the following sentence,
mentions are underlined:
“The American Medical Association voted
yesterday to install the heir apparent as its
president-elect, rejecting a strong, upstart
challenge by a District doctor who argued
that the nation’s largest physicians’ group
needs stronger ethics and new leadership.”
“American Medical Association”, “its” and “group”
belong to the same entity as they refer to the same ob-
ject.
Early work of anaphora resolution focuses on find-
ing antecedents of pronouns (Hobbs, 1976; Ge et al.,
1998; Mitkov, 1998), while recent advances (Soon et
al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Itty-
cheriah et al., 2003) employ statistical machine learn-
ing methods and try to resolve reference among all
kinds of noun phrases (NP), be it a name, nominal, or
pronominal phrase – which is the scope of this paper
as well. One common strategy shared by (Soon et al.,
2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is
that a statistical model is trained to measure how likely
a pair of mentions corefer; then a greedy procedure is
followed to group mentions into entities. While this ap-
proach has yielded encouraging results, the way men-
tions are linked is arguably suboptimal in that an instant
decision is made when considering whether two men-
tions are linked or not.
In this paper, we propose to use the Bell tree to rep-
resent the process of forming entities from mentions.
The Bell tree represents the search space of the coref-
erence resolution problem– each leaf node corresponds
to a possible coreference outcome. We choose to model
the process from mentions to entities represented in the
Bell tree, and the problem of coreference resolution is
cast as finding the “best” path from the root node to
leaves. A binary maximum entropy model is trained to
compute the linking probability between a partial entity
and a mention.
The rest of the paper is organized as follows. In
Section 2, we present how the Bell tree can be used
to represent the process of creating entities from men-
tions and the search space. We use a maximum en-
tropy model to rank paths in the Bell tree, which is dis-
cussed in Section 3. After presenting the search strat-
egy in Section 4, we show the experimental results on
the ACE 2002 and 2003 data, and the Message Under-
standing Conference (MUC) (MUC, 1995) data in Sec-
tion 5. We compare our approach with some recent
work in Section 6.
</bodyText>
<sectionHeader confidence="0.740095" genericHeader="method">
2 Bell Tree: From Mention to Entity
</sectionHeader>
<bodyText confidence="0.990096451612904">
Let us consider traversing mentions in a document from
beginning (left) to end (right). The process of form-
ing entities from mentions can be represented by a tree
structure. The root node is the initial state of the pro-
cess, which consists of a partial entity containing the
first mention of a document. The second mention is
Figure 1: Bell tree representation for three mentions:
numbers in [] denote a partial entity. In-focus entities
are marked on the solid arrows, and active mentions
are marked by *. Solid arrows signify that a mention
is linked with an in-focus partial entity while dashed
arrows indicate starting of a new entity.
added in the next step by either linking to the exist-
ing entity, or starting a new entity. A second layer
of nodes are created to represent the two possible out-
comes. Subsequent mentions are added to the tree in
the same manner. The process is mention-synchronous
in that each layer of tree nodes are created by adding
one mention at a time. Since the number of tree leaves
is the number of possible coreference outcomes and it
equals the Bell Number (Bell, 1934), the tree is called
the Bell tree. The Bell Number is the num-
ber of ways of partitioning distinguishable objects
(i.e., mentions) into non-empty disjoint subsets (i.e.,
entities). The Bell Number has a “closed” formula
and it increases rapidly as in-
creases: ! Clearly, an efficient
search strategy is necessary, and it will be addressed in
Section 4.
Figure 1 illustrates how the Bell tree is created for
a document with three mentions. The initial node con-
sists of the first partial entity [1] (i.e., node (a) in Fig-
ure 1). Next, mention 2 becomes active (marked by “*”
in node (a)) and can either link with the partial entity
[1] and result in anew node (b1), or start anew entity
and create another node (b2). The partial entity which
the active mention considers linking with is said to be
in-focus. In-focus entities are highlighted on the solid
arrows in Figure 1. Similarly, mention 3 will be ac-
tive in the next stage and can take five possible actions,
which create five possible coreference results shown in
node (c1) through (c5).
Under the derivation illustrated in Figure 1, each leaf
node in the Bell tree corresponds to a possible corefer-
ence outcome, and there is no other way to form enti-
ties. The Bell tree clearly represents the search space
of the coreference resolution problem. The corefer-
ence resolution can therefore be cast equivalently as
finding the “best” leaf node. Since the search space is
large (even for a document with a moderate number of
mentions), it is difficult to estimate a distribution over
leaves directly. Instead, we choose to model the pro-
cess from mentions to entities, or in other words, score
paths from the root to leaves in the Bell tree.
A nice property of the Bell tree representation is that
the number of linking or starting steps is the same for
all the hypotheses. This makes it easy to rank them us-
ing the “local” linking and starting probabilities as the
number of factors is the same. The Bell tree represen-
tation is also incremental in that mentions are added
sequentially. This makes it easy to design a decoder
and search algorithm.
</bodyText>
<sectionHeader confidence="0.999701" genericHeader="method">
3 Coreference Model
</sectionHeader>
<subsectionHeader confidence="0.999927">
3.1 Linking and Starting Model
</subsectionHeader>
<bodyText confidence="0.985365">
We use a binary conditional model to compute the
probability that an active mention links with an in-
focus partial entity. The conditions include all the
partially-formed entities before, the focus entity index,
and the active mention.
Formally, let be mentions
in a document. Mention index represents the order
it appears in the document. Let be an entity, and
be the (many-to-one) map from mention
index to entity index . For an active mention index
, define
for some
the set of indices of the partially-established entities to
the left of (note that ), and
the set of the partially-established entities. The link
model is then
</bodyText>
<equation confidence="0.966828">
(1)
</equation>
<bodyText confidence="0.966427">
the probability linking the active mention with the
in-focus entity . The random variable takes value
from the set and signifies which entity is in focus;
takes binary value and is if links with .
As an example, for the branch from (b2) to (c4) in
Figure 1, the active mention is “3”, the set of partial
entities to the left of “3” is , the ac-
tive entity is the second partial entity “[2]”. Probability
measures how likely men-
tion “3” links with the entity “[2].”
The model only computes
how likely links with ; It does not say anything
about the possibility that starts a new entity. Fortu-
nately, the starting probability can be computed using
link probabilities (1), as shown now.
Since starting a new entity means that does not
link with any entities in , the probability of starting
</bodyText>
<figure confidence="0.9990865">
(a)
[1] 2* 3
[12] 3*
[12]
[123]
[12][3]
[1]
[1][2] 3*
[13][2]
[1] [23]
[1][2][3]
a new entity, , can be computed as
</figure>
<listItem confidence="0.516346">
(3) indicates that the probability of starting an en-
tity can be computed using the linking probabilities
</listItem>
<bodyText confidence="0.712625333333333">
,provided that the marginal
is known. In this paper,
is approximated as:
</bodyText>
<equation confidence="0.55219975">
(4)
With the approximation (4), the starting probability (3)
is
(5)
</equation>
<bodyText confidence="0.957862">
The linking model (1) and approximated starting
model (5) can be used to score paths in the Bell tree.
For example, the score for the path (a)-(b2)-(c4) in Fig-
ure 1 is the product of the start probability from (a) to
(b2) and the linking probability from (b2) to (c4).
Since (5) is an approximation, not true probability, a
constant is introduced to balance the linking proba-
bility and starting probability and the starting probabil-
ity becomes:
(6)
If , it penalizes creating new entities; Therefore,
is called start penalty. The start penalty can be
used to balance entity miss and false alarm.
</bodyText>
<subsectionHeader confidence="0.999987">
3.2 Model Training and Features
</subsectionHeader>
<bodyText confidence="0.9975056875">
The model depends on all par-
tial entities , which can be very expensive. After
making some modeling assumptions, we can approxi-
mate it as:
From (7) to (8), entities other than the one in focus,
, are assumed to have no influence on the decision
of linking with . (9) further assumes that the
entity-mention score can be obtained by the maximum
mention pair score. The model (9) is very similar to
the model in (Morton, 2000; Soon et al., 2001; Ng and
Cardie, 2002) while (8) has more conditions.
We use maximum entropy model (Berger et al.,
1996) for both the mention-pair model (9) and the
entity-mention model (8):
where is a feature and is its weight;
is a normalizing factor to ensure that (10) or (11) is a
probability. Effective training algorithm exists (Berger
et al., 1996) once the set of features is se-
lected.
The basic features used in the models are tabulated
in Table 1.
Features in the lexical category are applicable to
non-pronominal mentions only. Distance features char-
acterize how far the two mentions are, either by the
number of tokens, by the number of sentences, or by
the number of mentions in-between. Syntactic fea-
tures are derived from parse trees output from a maxi-
mum entropy parser (Ratnaparkhi,1997). The “Count”
feature calculates how many times a mention string is
seen. For pronominal mentions, attributes such as gen-
der, number, possessiveness and reflexiveness are also
used. Apart from basic features in Table 1, composite
features can be generated by taking conjunction of ba-
sic features. For example, a distance feature together
with reflexiveness of a pronoun mention can help to
capture that the antecedent of a reflexive pronoun is of-
ten closer than that of a non-reflexive pronoun.
The same set of basic features in Table 1 is used
in the entity-mention model, but feature definitions
are slightly different. Lexical features, including the
acronym features, and the apposition feature are com-
puted by testing any mention in the entity against the
active mention . Editing distance for is de-
fined as the minimum distance over any non-pronoun
mentions and the active mention. Distance features are
computed by taking minimum between mentions in the
entity and the active mention.
In the ACE data, mentions are annotated with three
levels: NAME, NOMINAL and PRONOUN. For each
ACE entity, a canonical mention is defined as the
longest NAME mention if available; or if the entity
does not have a NAME mention, the most recent NOM-
INAL mention; if there is no NAME and NOMINAL
mention, the most recent pronoun mention. In the
entity-mention model, “ncd”,“spell” and “count” fea-
tures are computed over the canonical mention of the
in-focus entity and the active mention. Conjunction
features are used in the entity-mention model too.
The mention-pair model is appealing for its simplic-
ity: features are easy to compute over a pair of men-
if
otherwise
Category Features Remark
Lexical exact_strm 1 if two mentions have the same spelling; 0 otherwise
left_subsm 1 if one mention is a left substring of the other; 0 otherwise
right_subsm 1 if one mention is a right substring of the other; 0 otherwise
acronym 1 if one mention is an acronym of the other; 0 otherwise
edit_dist quantized editing distance between two mention strings
spell pair of actual mention strings
ncd number of different capitalized words in two mentions
Distance token_dist how many tokens two mentions are apart (quantized)
sent_dist how many sentences two mentions are apart (quantized)
gap_dist how many mentions in between the two mentions in question (quantized)
Syntax POS_pair POS-pair of two mention heads
apposition 1 if two mentions are appositive; 0 otherwise
Count count pair of (quantized) numbers, each counting how many times a mention string is seen
Pronoun gender pair of attributes of {female, male, neutral, unknown }
number pair of attributes of {singular, plural, unknown}
possessive 1 if a pronoun is possessive; 0 otherwise
reflexive 1 if a pronoun is reflexive; 0 otherwise
</bodyText>
<tableCaption confidence="0.97179">
Table 1: Basic features used in the maximum entropy model.
</tableCaption>
<bodyText confidence="0.999826153846154">
tions; its drawback is that information outside the men-
tion pair is ignored. Suppose a document has three
mentions “Mr. Clinton”, “Clinton” and “she”, appear-
ing in that order. When considering the mention pair
“Clinton” and “she”, the model may tend to link them
because of their proximity; But this mistake can be
easily avoided if “Mr. Clinton” and “Clinton” have
been put into the same entity and the model knows
“Mr. Clinton” referring to a male while “she” is fe-
male. Since gender and number information is prop-
agated at the entity level, the entity-mention model is
able to check the gender consistency when considering
the active mention “she”.
</bodyText>
<subsectionHeader confidence="0.94736">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.993863733333333">
There is an in-focus entity in the condition of the link-
ing model (1) while the starting model (2) conditions
on all left entities. The disparity is intentional as the
starting action is influenced by all established entities
on the left.
(4) is not the only way can be
approximated. For example, one could use a uniform
distribution over . We experimented several schemes
of approximation, including a uniform distribution, and
(4) worked the best and is adopted here. One may con-
sider training directly and use it to
score paths in the Bell tree. The problem is that 1) the
size of from which takes value is variable; 2) the
start action depends on all entities in , which makes
it difficult to train directly.
</bodyText>
<sectionHeader confidence="0.992102" genericHeader="method">
4 Search Issues
</sectionHeader>
<bodyText confidence="0.977089739130435">
As shown in Section 2, the search space of the coref-
erence problem can be represented by the Bell tree.
Thus, the search problem reduces to creating the Bell
tree while keeping track of path scores and picking the
top-N best paths. This is exactly what is described in
Algorithm 1.
In Algorithm 1, contains all the hypotheses, or
paths from the root to the current layer of nodes. Vari-
able stores the cumulative score for a corefer-
ence result . At line 1, is initialized with a single
entity consisting of mention , which corresponds to
the root node of the Bell tree in Figure 1. Line 2 to 15
loops over the remaining mentions ( to ), and for
each mention , the algorithm extends each result
in (or a path in the Bell tree) by either linking
with an existing entity (line 5 to 10), or starting an
entity (line 11 to 14). The loop from line 2 to 12
corresponds to creating a new layer of nodes for the ac-
tive mention in the Bell tree. in line 4 and in
line 6 and 11 have to do with pruning, which will be
discussed shortly. The last line returns top results,
where denotes the result ranked by :
Algorithm 1 Search Algorithm
</bodyText>
<listItem confidence="0.8218535">
if ( ) {
8: Extend to by linking with
10: }
11: if( ) {
12: Extend to by starting .
14: }
</listItem>
<figure confidence="0.697623">
16:return
Input: mentions ;
Output: top entity results
1:Initialize:
2:for to
3: foreach node
4: compute .
5: foreach
.
</figure>
<bodyText confidence="0.966070285714285">
The complexity of the search Algorithm 1 is the total
number of nodes in the Bell tree, which is ,
where is the Bell Number. Since the Bell number
increases rapidly as a function of the number of men-
tions, pruning is necessary. We prune the search space
in the following places:
Local pruning: any children with a score below a
fixed factor of the maximum score are pruned.
This is done at line 6 and 11 in Algorithm 1. The
operation in line 4 is:
first aligns the system entities with the reference enti-
ties so that the number of common mentions is maxi-
mized. Each system entity is constrained to align with
at most one reference entity, and vice versa. For exam-
ple, suppose that a reference document contains three
entities: while a system out-
puts four entities: , then
the best alignment (from reference to system) would be
, and other entities
are not aligned. The number of common mentions of
the best alignment is (i.e.,
</bodyText>
<subsectionHeader confidence="0.926624">
5.2 Results on the ACE data
</subsectionHeader>
<bodyText confidence="0.999734888888889">
The system is first developed and tested using the ACE
data. The ACE coreference system is trained with
documents (about words) of ACE 2002 training
data. A separate documents ( words) is used as
the development-test (Devtest) set. In 2002, NIST re-
leased two test sets in February (Feb02) and September
(Sep02), respectively. Statistics of the three test sets is
summarized in Table 2. We will report coreference re-
sults on the true mentions of the three test sets.
</bodyText>
<table confidence="0.99673575">
TestSet #-docs #-words #-mentions #-entities
Devtest 90 50426 7470 2891
Feb02 97 52677 7665 3104
Sep02 186 69649 10577 4355
</table>
<tableCaption confidence="0.999042">
Table 2: Statistics of three test sets.
</tableCaption>
<figure confidence="0.92061875">
Block 8-9 is carried out only if
and block 12-13 is car-
ried out only if .
Global pruning: similar to local pruning except
</figure>
<figureCaption confidence="0.913255894736842">
that this is done using the cumulative score .
Pruning based on the global scores is carried out
at line 15 of Algorithm 1.
Limit hypotheses: we set a limit on the maxi-
mum number of live paths. This is useful when a
document contains many mentions, in which case
excessive number of paths may survive local and
global pruning.
Whenever available, we check the compatibility
of entity types between the in-focus entity and the
active mention. A hypothesis with incompatible
entity types is discarded. In the ACE annotation,
every mention has an entity type. Therefore we
can eliminate hypotheses with two mentions of
different types.
and ), which leads to
a mention recall and precision . The ECM-F mea-
sures the percentage of mentions that are in the “right”
entities.
</figureCaption>
<bodyText confidence="0.71152225">
For tests on the MUC data, we report both F-measure
using the official MUC score (Vilain et al., 1995) and
ECM-F. The MUC score counts the common links be-
tween the reference and the system output.
</bodyText>
<sectionHeader confidence="0.999705" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997218">
5.1 Performance Metrics
</subsectionHeader>
<bodyText confidence="0.999194732394366">
The official performance metric for the ACE task is
ACE-value. ACE-value is computed by first calculat-
ing the weighted cost of entity insertions, deletions and
substitutions; The cost is then normalized against the
cost of a nominal coreference system which outputs
no entities; The ACE-value is obtained by subtracting
the normalized cost from . Weights are designed to
emphasize NAME entities, while PRONOUN entities
(i.e., an entity consisting of only pronominal mentions)
carry very low weights. A perfect coreference system
will get a ACE-value while a system outputs no
entities will get a ACE-value. Thus, the ACE-value
can be interpreted as percentage of value a system has,
relative to the perfect system.
Since the ACE-value is an entity-level metric and is
weighted heavily toward NAME entities, we also mea-
sure our system’s performance by an entity-constrained
mention F-measure (henceforth “ECM-F”). The metric
For the mention-pair model, training events are gen-
erated for all compatible mention-pairs, which results
in about events, about of which are posi-
tive examples. The full mention-pair model uses about
features; Most are conjunction features. For the
entity-mention model, events are generated by walking
through the Bell tree. Only events on the true path (i.e.,
positive examples) and branches emitting from a node
on the true path to a node not on the true path (i.e.,
negative examples) are generated. For example, in Fig-
ure 1, suppose that the path (a)-(b2)-(c4) is the truth,
then positive training examples are starting event from
(a) to (b2) and linking event from (b2) to (c4); While
the negative examples are linking events from (a) to
(b1), (b2) to (c3), and the starting event from (b2) to
(c5). This scheme generates about events, out of
which about are positive training examples. The
full entity-mention model has about features, due
to less number of conjunction features and training ex-
amples.
Coreference results on the true mentions of the De-
vtest, Feb02, and Sep02 test sets are tabulated in Ta-
ble 3. These numbers are obtained with a fixed search
beam and pruning threshold (widening
the search beam or using a smaller pruning threshold
did not change results significantly).
The mention-pair model in most cases performs bet-
ter than the mention-entity model by both ACE-value
and ECM-F measure although none of the differences
is statistically significant (pair-wise t-test) at p-value
. Note that, however, the mention-pair model uses
times more features than the entity-pair model. We
also observed that, because the score between the in-
focus entity and the active mention is computed by (9)
in the mention-pair model, the mention-pair sometimes
mistakenly places a male pronoun and female pronoun
into the same entity, while the same mistake is avoided
in the entity-mention model. Using the canonical men-
tions when computing some features (e.g., “spell”) in
the entity-mention model is probably not optimal and
it is an area that needs further research.
When the same mention-pair model is used to score
the ACE 2003 evaluation data, an ACE-value is
obtained on the systems mentions. After retrained with
Chinese and Arabic data (much less training data than
English), the system got and ACE-value
on the system mentions of ACE 2003 evaluation data
for Chinese and Arabic, respectively. The results for
all three languages are among the top-tier submission
systems. Details of the mention detection and corefer-
ence system can be found in (Florian et al., 2004).
Since the mention-pair model is better, subsequent
analyses are done with the mention pair model only.
</bodyText>
<subsectionHeader confidence="0.660121">
5.2.1 Feature Impact
</subsectionHeader>
<bodyText confidence="0.999976454545454">
To see how each category of features affects the per-
formance, we start with the aforementioned mention-
pair model, incrementally remove each feature cate-
gory, retrain the system and test it on the Devtest set.
The result is summarized in Table 4. The last column
lists the number of features. The second row is the full
mention-pair model, the third through seventh row cor-
respond to models by removing the syntactic features
(i.e., POS tags and apposition features), count features,
distance features, mention type and level information,
and pair of mention-spelling features. If a basic fea-
ture is removed, conjunction features using that basic
feature are also removed. It is striking that the small-
est system consisting of only features (string and
substring match, acronym, edit distance and number of
different capitalized words) can get as much as
ACE-value. Table 4 shows clearly that these lexical
features and the distance features are the most impor-
tant. Sometimes the ACE-value increases after remov-
ing a set of features, but the ECM-F measure tracks
nicely the trend that the more features there are, the bet-
ter the performance is. This is because the ACE-value
</bodyText>
<footnote confidence="0.8736725">
1System mentions are output from a mention detection
system.
</footnote>
<figure confidence="0.912950333333333">
0.65
−2.5 −2 −1.5 −1 −0.5 0
log α
</figure>
<figureCaption confidence="0.999688">
Figure 2: Performance vs. log start penalty
</figureCaption>
<bodyText confidence="0.603662">
is a weighted metric. A small fluctuation of NAME
entities will impact the ACE-value more than many
NOMINAL or PRONOUN entities.
</bodyText>
<table confidence="0.999511285714286">
Model ACE-val(%) ECM-F(%) #-features
Full 89.8 73.20 ( 2.9) 171K
-syntax 89.0 72.6 ( 2.5) 71K
-count 89.4 72.0 ( 3.3) 70K
-dist 86.7 *66.2 ( 3.9) 24K
-type/level 86.8 65.7 ( 2.2) 5.4K
-spell 86.0 64.4 ( 1.9) 39
</table>
<tableCaption confidence="0.75448425">
Table 4: Impact of feature categories. Numbers after
are the standard deviations. * indicates that the result
is significantly (pair-wise t-test) different from the line
above at .
</tableCaption>
<subsectionHeader confidence="0.957482">
5.2.2 Effect of Start Penalty
</subsectionHeader>
<bodyText confidence="0.997576666666667">
As discussed in Section 3.1, the start penalty can
be used to balance the entity miss and false alarm. To
see this effect, we decode the Devtest set by varying the
start penalty and the result is depicted in Figure 2. The
ACE-value and ECM-F track each other fairly well.
Both achieve the optimal when .
</bodyText>
<subsectionHeader confidence="0.986851">
5.3 Experiments on the MUC data
</subsectionHeader>
<bodyText confidence="0.999761153846154">
To see how the proposed algorithm works on the MUC
data, we test our algorithm on the MUC6 data. To min-
imize the change to the coreference system, we first
map the MUC data into the ACE style. The original
MUC coreference data does not have entity types (i.e.,
“ORGANIZATION”, “LOCATION” etc), required in
the ACE style. Part of entity types can be recovered
from the corresponding named-entity annotations. The
recovered named-entity label is propagated to all men-
tions belonging to the same entity. There are 504 out of
2072 mentions of the MUC6 formal test set and 695
out of 2141 mentions of the MUC6 dry-run test set
that cannot be assigned labels by this procedure. A
</bodyText>
<figure confidence="0.945835">
0.9
0.85
ECM−F
ACE−wlue
0.8
0.75
0.7
ACE−value or ECM−F
</figure>
<table confidence="0.9940535">
Devtest Feb02 Sep02
Model ACE-val(%) ECM-F(%) ACE-val(%) ECM-F(%) ACE-val(%) ECM-F(%)
MP 89.8 73.2 ( 2.9) 90.0 73.1 ( 4.0) 88.0 73.1 ( 6.8)
EM 89.9 71.7 ( 2.4) 88.2 70.8 ( 3.9) 87.6 72.4 ( 6.2)
</table>
<tableCaption confidence="0.8651235">
Table 3: Coreference results on true mentions: MP – mention-pair model; EM – entity-mention model; ACE-val:
ACE-value; ECM-F: Entity-constrained Mention F-measure. MP uses features while EM uses only
</tableCaption>
<bodyText confidence="0.991685810810811">
features. None of the ECM-F differences between MP and EM is statistically significant at .
generic type “UNKNOWN” is assigned to these men-
tions. Mentions that can be found in the named-entity
annotation are assumed to have the ACE mention level
“NAME”; All other mentions other than English pro-
nouns are assigned the level “NOMINAL.”
After the MUC data is mapped into the ACE-style,
the same set of feature templates is used to train
a coreference system. Two coreference systems are
trained on the MUC6 data: one trained with 30 dry-run
test documents (henceforth “MUC6-small”); the other
trained with 191 “dryrun-train” documents that have
both coreference and named-entity annotations (hence-
forth “MUC6-big”) in the latest LDC release.
To use the official MUC scorer, we convert the out-
put of the ACE-style coreference system back into the
MUC format. Since MUC does not require entity label
and level, the conversion from ACE to MUC is “loss-
less.”
Table 5 tabulates the test results on the true mentions
of the MUC6 formal test set. The numbers in the ta-
ble represent the optimal operating point determined by
ECM-F. The MUC scorer cannot be used since it inher-
ently favors systems that output fewer number of enti-
ties (e.g., putting all mentions of the MUC6 formal test
set into one entity will yield a recall and
precision of links, which gives an F-measure).
The MUC6-small system compares favorably with the
similar experiment in Harabagiu et al. (2001) in which
an F-measure is reported. When measured by
the ECM-F measure, the MUC6-small system has the
same level of performance as the ACE system, while
the MUC6-big system performs better than the ACE
system. The results show that the algorithm works well
on the MUC6 data despite some information is lost in
the conversion from the MUC format to the ACE for-
mat.
</bodyText>
<table confidence="0.993268666666667">
System MUC F-measure ECM-F
MUC6-small 83.9% 72.1%
MUC6-big 85.7% 76.8%
</table>
<tableCaption confidence="0.999635">
Table 5: Results on the MUC6 formal test set.
</tableCaption>
<sectionHeader confidence="0.99976" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999957409090909">
There exists a large body of literature on the topic of
coreference resolution. We will compare this study
with some relevant work using machine learning or sta-
tistical methods only.
Soon et al. (2001) uses a decision tree model for
coreference resolution on the MUC6 and MUC7 data.
Leaves of the decision tree are labeled with “link” or
“not-link” in training. At test time, the system checks
a mention against all its preceding mentions, and the
first one labeled with “link” is picked as the antecedent.
Their work is later enhanced by (Ng and Cardie, 2002)
in several aspects: first, the decision tree returns scores
instead of a hard-decision of “link” or “not-link” so that
Ng and Cardie (2002) is able to pick the “best” candi-
date on the left, as opposed the first in (Soon et al.,
2001); Second, Ng and Cardie (2002) expands the fea-
ture sets of (Soon et al., 2001). The model in (Yang et
al., 2003) expands the conditioning scope by including
a competing candidate. Neither (Soon et al., 2001) nor
(Ng and Cardie, 2002) searches for the global optimal
entity in that they make locally independent decisions
during search. In contrast, our decoder always searches
for the best result ranked by the cumulative score (sub-
ject to pruning), and subsequent decisions depend on
earlier ones.
Recently, McCallum and Wellner (2003) proposed
to use graphical models for computing probabilities of
entities. The model is appealing in that it can poten-
tially overcome the limitation of mention-pair model in
which dependency among mentions other than the two
in question is ignored. However, models in (McCal-
lum and Wellner, 2003) compute directly the probabil-
ity of an entity configuration conditioned on mentions,
and it is not clear how the models can be factored to
do the incremental search, as it is impractical to enu-
merate all possible entities even for documents with a
moderate number of mentions. The Bell tree represen-
tation proposed in this paper, however, provides us with
a naturally incremental framework for coreference res-
olution.
Maximum entropy method has been used in coref-
erence resolution before. For example, Kehler (1997)
uses a mention-pair maximum entropy model, and two
methods are proposed to compute entity scores based
on the mention-pair model: 1) a distribution over en-
tity space is deduced; 2) the most recent mention of an
entity, together with the candidate mention, is used to
compute the entity-mention score. In contrast, in our
mention pair model, an entity-mention pair is scored
by taking the maximum score among possible mention
pairs. Our entity-mention model eliminates the need to
synthesize an entity-mention score from mention-pair
scores. Morton (2000) also uses a maximum entropy
mention-pair model, and a special “dummy” mention
is used to model the event of starting a new entity.
Features involving the dummy mention are essentially
computed with the single (normal) mention, and there-
fore the starting model is weak. In our model, the start-
ing model is obtained by “complementing” the linking
scores. The advantage is that we do not need to train
a starting model. To compensate the model inaccuracy,
we introduce a “starting penalty” to balance the linking
and starting scores.
To our knowledge, the paper is the first time the Bell
tree is used to represent the search space of the coref-
erence resolution problem.
</bodyText>
<sectionHeader confidence="0.999283" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9996972">
We propose to use the Bell tree to represent the pro-
cess of forming entities from mentions. The Bell tree
represents the search space of the coreference reso-
lution problem. We studied two maximum entropy
models, namely the mention-pair model and the entity-
mention model, both of which can be used to score
entity hypotheses. A beam search algorithm is used
to search the best entity result. State-of-the-art perfor-
mance has been achieved on the ACE coreference data
across three languages.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999955555555555">
This work was partially supported by the Defense Ad-
vanced Research Projects Agency and monitored by
SPAWAR under contract No. N66001-99-2-8916. The
views and findings contained in this material are those
of the authors and do not necessarily reflect the position
of policy of the Government and no official endorse-
ment should be inferred. We also would like to thank
the anonymous reviewers for suggestions of improving
the paper.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99948775">
E.T. Bell. 1934. Exponential numbers. Amer. Math.
Monthly, pages 411–419.
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39–71, March.
R Florian, H Hassan, A Ittycheriah, H Jing, N Kamb-
hatla, X Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 1–8, Boston, Massachusetts, USA, May
2 - May 7. Association for Computational Linguis-
tics.
Niyu Ge, John Hale, and Eugene Charniak. 1998. A
statistical approach to anaphora resolution. In Proc.
of the sixth Workshop on Very Large Corpora.
Sanda M. Harabagiu, Razvan C. Bunescu, and Steven J.
Maiorano. 2001. Text and knowledge mining for
coreference resolution. In Proc. ofNAACL.
J. Hobbs. 1976. Pronoun resolution. Technical report,
Dept. of Computer Science, CUNY, Technical Re-
port TR76-1.
A. Ittycheriah, L. Lita, N. Kambhatla, N. Nicolov,
S. Roukos, and M. Stys. 2003. Identifying and
tracking entity mentions in a maximum entropy
framework. In HLT-NAACL 2003: Short Papers,
May 27 - June 1.
Andrew Kehler. 1997. Probabilistic coreference in in-
formation extraction. In Proc. of EMNLP.
Andrew McCallum and Ben Wellner. 2003. To-
ward conditional models of identity uncertainty with
application to proper noun coreference. In IJCAI
Workshop on Information Integration on the Web.
R. Mitkov. 1998. Robust pronoun resolution with lim-
ited knowledge. In Procs. of the 17th Internaltional
Conference on Computational Linguistics, pages
869–875.
Thomas S. Morton. 2000. Coreference for NLP appli-
cations. In In Proceedings of the 38th Annual Meet-
ing ofthe Associationfor Computational Linguistics.
MUC-6. 1995. Proceedings of the Sixth Message
Understanding Conference(MUC-6), San Francisco,
CA. Morgan Kaufmann.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proc. ofACL, pages 104–111.
NIST. 2003. The ACE evaluation plan.
www.nist.gov/speech/tests/ace/index.htm.
Adwait Ratnaparkhi. 1997. A Linear Observed Time
Statistical Parser Based on Maximum Entropy Mod-
els. In Second Conference on Empirical Methods in
Natural Language Processing, pages 1 – 10.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521–544.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, , and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In In Proc. ofMUC6, pages 45–52.
Xiaofeng Yang, Guodong Zhou, Jian Su, and
Chew Lim Tan. 2003. Coreference resolution us-
ing competition learning approach. In Proc. of the
ACL.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.338136">
<title confidence="0.682946">A Mention-Synchronous Coreference Resolution Algorithm Based on the Bell Tree Xiaoqiang Luo and Abe Ittycheriah</title>
<author confidence="0.886056">Hongyan Jing</author>
<author confidence="0.886056">Nanda Kambhatla</author>
<author confidence="0.886056">Salim Roukos</author>
<address confidence="0.98184">1101 Kitchawan Road Yorktown Heights, NY 10598, U.S.A.</address>
<email confidence="0.999847">xiaoluo@us.ibm.com</email>
<email confidence="0.999847">abei@us.ibm.com</email>
<email confidence="0.999847">hjing@us.ibm.com</email>
<email confidence="0.999847">nanda@us.ibm.com</email>
<email confidence="0.999847">roukos@us.ibm.com</email>
<abstract confidence="0.999469307692308">This paper proposes a new approach for resolution which uses the tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E T Bell</author>
</authors>
<title>Exponential numbers.</title>
<date>1934</date>
<journal>Amer. Math. Monthly,</journal>
<pages>411--419</pages>
<contexts>
<context position="4671" citStr="Bell, 1934" startWordPosition="788" endWordPosition="789">e marked by *. Solid arrows signify that a mention is linked with an in-focus partial entity while dashed arrows indicate starting of a new entity. added in the next step by either linking to the existing entity, or starting a new entity. A second layer of nodes are created to represent the two possible outcomes. Subsequent mentions are added to the tree in the same manner. The process is mention-synchronous in that each layer of tree nodes are created by adding one mention at a time. Since the number of tree leaves is the number of possible coreference outcomes and it equals the Bell Number (Bell, 1934), the tree is called the Bell tree. The Bell Number is the number of ways of partitioning distinguishable objects (i.e., mentions) into non-empty disjoint subsets (i.e., entities). The Bell Number has a “closed” formula and it increases rapidly as increases: ! Clearly, an efficient search strategy is necessary, and it will be addressed in Section 4. Figure 1 illustrates how the Bell tree is created for a document with three mentions. The initial node consists of the first partial entity [1] (i.e., node (a) in Figure 1). Next, mention 2 becomes active (marked by “*” in node (a)) and can either </context>
</contexts>
<marker>Bell, 1934</marker>
<rawString>E.T. Bell. 1934. Exponential numbers. Amer. Math. Monthly, pages 411–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="9774" citStr="Berger et al., 1996" startWordPosition="1681" endWordPosition="1684">d to balance entity miss and false alarm. 3.2 Model Training and Features The model depends on all partial entities , which can be very expensive. After making some modeling assumptions, we can approximate it as: From (7) to (8), entities other than the one in focus, , are assumed to have no influence on the decision of linking with . (9) further assumes that the entity-mention score can be obtained by the maximum mention pair score. The model (9) is very similar to the model in (Morton, 2000; Soon et al., 2001; Ng and Cardie, 2002) while (8) has more conditions. We use maximum entropy model (Berger et al., 1996) for both the mention-pair model (9) and the entity-mention model (8): where is a feature and is its weight; is a normalizing factor to ensure that (10) or (11) is a probability. Effective training algorithm exists (Berger et al., 1996) once the set of features is selected. The basic features used in the models are tabulated in Table 1. Features in the lexical category are applicable to non-pronominal mentions only. Distance features characterize how far the two mentions are, either by the number of tokens, by the number of sentences, or by the number of mentions in-between. Syntactic features</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Hassan</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>X Luo</author>
<author>N Nicolov</author>
<author>S Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>1--8</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="22240" citStr="Florian et al., 2004" startWordPosition="3796" endWordPosition="3799">(e.g., “spell”) in the entity-mention model is probably not optimal and it is an area that needs further research. When the same mention-pair model is used to score the ACE 2003 evaluation data, an ACE-value is obtained on the systems mentions. After retrained with Chinese and Arabic data (much less training data than English), the system got and ACE-value on the system mentions of ACE 2003 evaluation data for Chinese and Arabic, respectively. The results for all three languages are among the top-tier submission systems. Details of the mention detection and coreference system can be found in (Florian et al., 2004). Since the mention-pair model is better, subsequent analyses are done with the mention pair model only. 5.2.1 Feature Impact To see how each category of features affects the performance, we start with the aforementioned mentionpair model, incrementally remove each feature category, retrain the system and test it on the Devtest set. The result is summarized in Table 4. The last column lists the number of features. The second row is the full mention-pair model, the third through seventh row correspond to models by removing the syntactic features (i.e., POS tags and apposition features), count f</context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>R Florian, H Hassan, A Ittycheriah, H Jing, N Kambhatla, X Luo, N Nicolov, and S Roukos. 2004. A statistical model for multilingual entity detection and tracking. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 1–8, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niyu Ge</author>
<author>John Hale</author>
<author>Eugene Charniak</author>
</authors>
<title>A statistical approach to anaphora resolution.</title>
<date>1998</date>
<booktitle>In Proc. of the sixth Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="1662" citStr="Ge et al., 1998" startWordPosition="253" endWordPosition="256">on of mentions referring to the same object in a document form an entity. For example, in the following sentence, mentions are underlined: “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” “American Medical Association”, “its” and “group” belong to the same entity as they refer to the same object. Early work of anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase – which is the scope of this paper as well. One common strategy shared by (Soon et al., 2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is that a statistical model is trained to measure how likely a pair of mentions corefer; then a greedy procedure is followed to group mentions into entities. While this approach h</context>
</contexts>
<marker>Ge, Hale, Charniak, 1998</marker>
<rawString>Niyu Ge, John Hale, and Eugene Charniak. 1998. A statistical approach to anaphora resolution. In Proc. of the sixth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda M Harabagiu</author>
<author>Razvan C Bunescu</author>
<author>Steven J Maiorano</author>
</authors>
<title>Text and knowledge mining for coreference resolution. In</title>
<date>2001</date>
<booktitle>Proc. ofNAACL.</booktitle>
<contexts>
<context position="27152" citStr="Harabagiu et al. (2001)" startWordPosition="4624" endWordPosition="4627"> the MUC format. Since MUC does not require entity label and level, the conversion from ACE to MUC is “lossless.” Table 5 tabulates the test results on the true mentions of the MUC6 formal test set. The numbers in the table represent the optimal operating point determined by ECM-F. The MUC scorer cannot be used since it inherently favors systems that output fewer number of entities (e.g., putting all mentions of the MUC6 formal test set into one entity will yield a recall and precision of links, which gives an F-measure). The MUC6-small system compares favorably with the similar experiment in Harabagiu et al. (2001) in which an F-measure is reported. When measured by the ECM-F measure, the MUC6-small system has the same level of performance as the ACE system, while the MUC6-big system performs better than the ACE system. The results show that the algorithm works well on the MUC6 data despite some information is lost in the conversion from the MUC format to the ACE format. System MUC F-measure ECM-F MUC6-small 83.9% 72.1% MUC6-big 85.7% 76.8% Table 5: Results on the MUC6 formal test set. 6 Related Work There exists a large body of literature on the topic of coreference resolution. We will compare this stu</context>
</contexts>
<marker>Harabagiu, Bunescu, Maiorano, 2001</marker>
<rawString>Sanda M. Harabagiu, Razvan C. Bunescu, and Steven J. Maiorano. 2001. Text and knowledge mining for coreference resolution. In Proc. ofNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>Pronoun resolution.</title>
<date>1976</date>
<tech>Technical report,</tech>
<institution>Dept. of Computer Science, CUNY,</institution>
<contexts>
<context position="1645" citStr="Hobbs, 1976" startWordPosition="251" endWordPosition="252"> the collection of mentions referring to the same object in a document form an entity. For example, in the following sentence, mentions are underlined: “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” “American Medical Association”, “its” and “group” belong to the same entity as they refer to the same object. Early work of anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase – which is the scope of this paper as well. One common strategy shared by (Soon et al., 2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is that a statistical model is trained to measure how likely a pair of mentions corefer; then a greedy procedure is followed to group mentions into entities. Whil</context>
</contexts>
<marker>Hobbs, 1976</marker>
<rawString>J. Hobbs. 1976. Pronoun resolution. Technical report, Dept. of Computer Science, CUNY, Technical Report TR76-1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>L Lita</author>
<author>N Kambhatla</author>
<author>N Nicolov</author>
<author>S Roukos</author>
<author>M Stys</author>
</authors>
<title>Identifying and tracking entity mentions in a maximum entropy framework.</title>
<date>2003</date>
<booktitle>In HLT-NAACL 2003: Short Papers,</booktitle>
<contexts>
<context position="1786" citStr="Ittycheriah et al., 2003" startWordPosition="274" endWordPosition="278">mentions are underlined: “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” “American Medical Association”, “its” and “group” belong to the same entity as they refer to the same object. Early work of anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase – which is the scope of this paper as well. One common strategy shared by (Soon et al., 2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is that a statistical model is trained to measure how likely a pair of mentions corefer; then a greedy procedure is followed to group mentions into entities. While this approach has yielded encouraging results, the way mentions are linked is arguably suboptimal in that an instant decision is made when </context>
</contexts>
<marker>Ittycheriah, Lita, Kambhatla, Nicolov, Roukos, Stys, 2003</marker>
<rawString>A. Ittycheriah, L. Lita, N. Kambhatla, N. Nicolov, S. Roukos, and M. Stys. 2003. Identifying and tracking entity mentions in a maximum entropy framework. In HLT-NAACL 2003: Short Papers, May 27 - June 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kehler</author>
</authors>
<title>Probabilistic coreference in information extraction.</title>
<date>1997</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="29790" citStr="Kehler (1997)" startWordPosition="5068" endWordPosition="5069">ntions other than the two in question is ignored. However, models in (McCallum and Wellner, 2003) compute directly the probability of an entity configuration conditioned on mentions, and it is not clear how the models can be factored to do the incremental search, as it is impractical to enumerate all possible entities even for documents with a moderate number of mentions. The Bell tree representation proposed in this paper, however, provides us with a naturally incremental framework for coreference resolution. Maximum entropy method has been used in coreference resolution before. For example, Kehler (1997) uses a mention-pair maximum entropy model, and two methods are proposed to compute entity scores based on the mention-pair model: 1) a distribution over entity space is deduced; 2) the most recent mention of an entity, together with the candidate mention, is used to compute the entity-mention score. In contrast, in our mention pair model, an entity-mention pair is scored by taking the maximum score among possible mention pairs. Our entity-mention model eliminates the need to synthesize an entity-mention score from mention-pair scores. Morton (2000) also uses a maximum entropy mention-pair mod</context>
</contexts>
<marker>Kehler, 1997</marker>
<rawString>Andrew Kehler. 1997. Probabilistic coreference in information extraction. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Ben Wellner</author>
</authors>
<title>Toward conditional models of identity uncertainty with application to proper noun coreference.</title>
<date>2003</date>
<booktitle>In IJCAI Workshop on Information Integration on the Web.</booktitle>
<contexts>
<context position="28978" citStr="McCallum and Wellner (2003)" startWordPosition="4936" endWordPosition="4939"> able to pick the “best” candidate on the left, as opposed the first in (Soon et al., 2001); Second, Ng and Cardie (2002) expands the feature sets of (Soon et al., 2001). The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate. Neither (Soon et al., 2001) nor (Ng and Cardie, 2002) searches for the global optimal entity in that they make locally independent decisions during search. In contrast, our decoder always searches for the best result ranked by the cumulative score (subject to pruning), and subsequent decisions depend on earlier ones. Recently, McCallum and Wellner (2003) proposed to use graphical models for computing probabilities of entities. The model is appealing in that it can potentially overcome the limitation of mention-pair model in which dependency among mentions other than the two in question is ignored. However, models in (McCallum and Wellner, 2003) compute directly the probability of an entity configuration conditioned on mentions, and it is not clear how the models can be factored to do the incremental search, as it is impractical to enumerate all possible entities even for documents with a moderate number of mentions. The Bell tree representati</context>
</contexts>
<marker>McCallum, Wellner, 2003</marker>
<rawString>Andrew McCallum and Ben Wellner. 2003. Toward conditional models of identity uncertainty with application to proper noun coreference. In IJCAI Workshop on Information Integration on the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitkov</author>
</authors>
<title>Robust pronoun resolution with limited knowledge.</title>
<date>1998</date>
<booktitle>In Procs. of the 17th Internaltional Conference on Computational Linguistics,</booktitle>
<pages>869--875</pages>
<contexts>
<context position="1677" citStr="Mitkov, 1998" startWordPosition="257" endWordPosition="258">ferring to the same object in a document form an entity. For example, in the following sentence, mentions are underlined: “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” “American Medical Association”, “its” and “group” belong to the same entity as they refer to the same object. Early work of anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase – which is the scope of this paper as well. One common strategy shared by (Soon et al., 2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is that a statistical model is trained to measure how likely a pair of mentions corefer; then a greedy procedure is followed to group mentions into entities. While this approach has yielded enco</context>
</contexts>
<marker>Mitkov, 1998</marker>
<rawString>R. Mitkov. 1998. Robust pronoun resolution with limited knowledge. In Procs. of the 17th Internaltional Conference on Computational Linguistics, pages 869–875.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Morton</author>
</authors>
<title>Coreference for NLP applications. In</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting ofthe Associationfor Computational Linguistics.</booktitle>
<contexts>
<context position="9651" citStr="Morton, 2000" startWordPosition="1661" endWordPosition="1662">omes: (6) If , it penalizes creating new entities; Therefore, is called start penalty. The start penalty can be used to balance entity miss and false alarm. 3.2 Model Training and Features The model depends on all partial entities , which can be very expensive. After making some modeling assumptions, we can approximate it as: From (7) to (8), entities other than the one in focus, , are assumed to have no influence on the decision of linking with . (9) further assumes that the entity-mention score can be obtained by the maximum mention pair score. The model (9) is very similar to the model in (Morton, 2000; Soon et al., 2001; Ng and Cardie, 2002) while (8) has more conditions. We use maximum entropy model (Berger et al., 1996) for both the mention-pair model (9) and the entity-mention model (8): where is a feature and is its weight; is a normalizing factor to ensure that (10) or (11) is a probability. Effective training algorithm exists (Berger et al., 1996) once the set of features is selected. The basic features used in the models are tabulated in Table 1. Features in the lexical category are applicable to non-pronominal mentions only. Distance features characterize how far the two mentions a</context>
<context position="30345" citStr="Morton (2000)" startWordPosition="5154" endWordPosition="5155"> coreference resolution before. For example, Kehler (1997) uses a mention-pair maximum entropy model, and two methods are proposed to compute entity scores based on the mention-pair model: 1) a distribution over entity space is deduced; 2) the most recent mention of an entity, together with the candidate mention, is used to compute the entity-mention score. In contrast, in our mention pair model, an entity-mention pair is scored by taking the maximum score among possible mention pairs. Our entity-mention model eliminates the need to synthesize an entity-mention score from mention-pair scores. Morton (2000) also uses a maximum entropy mention-pair model, and a special “dummy” mention is used to model the event of starting a new entity. Features involving the dummy mention are essentially computed with the single (normal) mention, and therefore the starting model is weak. In our model, the starting model is obtained by “complementing” the linking scores. The advantage is that we do not need to train a starting model. To compensate the model inaccuracy, we introduce a “starting penalty” to balance the linking and starting scores. To our knowledge, the paper is the first time the Bell tree is used </context>
</contexts>
<marker>Morton, 2000</marker>
<rawString>Thomas S. Morton. 2000. Coreference for NLP applications. In In Proceedings of the 38th Annual Meeting ofthe Associationfor Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference(MUC-6),</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, CA.</location>
<marker>1995</marker>
<rawString>MUC-6. 1995. Proceedings of the Sixth Message Understanding Conference(MUC-6), San Francisco, CA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="1759" citStr="Ng and Cardie, 2002" startWordPosition="270" endWordPosition="273"> following sentence, mentions are underlined: “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” “American Medical Association”, “its” and “group” belong to the same entity as they refer to the same object. Early work of anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase – which is the scope of this paper as well. One common strategy shared by (Soon et al., 2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is that a statistical model is trained to measure how likely a pair of mentions corefer; then a greedy procedure is followed to group mentions into entities. While this approach has yielded encouraging results, the way mentions are linked is arguably suboptimal in that an ins</context>
<context position="9692" citStr="Ng and Cardie, 2002" startWordPosition="1667" endWordPosition="1670">ing new entities; Therefore, is called start penalty. The start penalty can be used to balance entity miss and false alarm. 3.2 Model Training and Features The model depends on all partial entities , which can be very expensive. After making some modeling assumptions, we can approximate it as: From (7) to (8), entities other than the one in focus, , are assumed to have no influence on the decision of linking with . (9) further assumes that the entity-mention score can be obtained by the maximum mention pair score. The model (9) is very similar to the model in (Morton, 2000; Soon et al., 2001; Ng and Cardie, 2002) while (8) has more conditions. We use maximum entropy model (Berger et al., 1996) for both the mention-pair model (9) and the entity-mention model (8): where is a feature and is its weight; is a normalizing factor to ensure that (10) or (11) is a probability. Effective training algorithm exists (Berger et al., 1996) once the set of features is selected. The basic features used in the models are tabulated in Table 1. Features in the lexical category are applicable to non-pronominal mentions only. Distance features characterize how far the two mentions are, either by the number of tokens, by th</context>
<context position="28208" citStr="Ng and Cardie, 2002" startWordPosition="4806" endWordPosition="4809">able 5: Results on the MUC6 formal test set. 6 Related Work There exists a large body of literature on the topic of coreference resolution. We will compare this study with some relevant work using machine learning or statistical methods only. Soon et al. (2001) uses a decision tree model for coreference resolution on the MUC6 and MUC7 data. Leaves of the decision tree are labeled with “link” or “not-link” in training. At test time, the system checks a mention against all its preceding mentions, and the first one labeled with “link” is picked as the antecedent. Their work is later enhanced by (Ng and Cardie, 2002) in several aspects: first, the decision tree returns scores instead of a hard-decision of “link” or “not-link” so that Ng and Cardie (2002) is able to pick the “best” candidate on the left, as opposed the first in (Soon et al., 2001); Second, Ng and Cardie (2002) expands the feature sets of (Soon et al., 2001). The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate. Neither (Soon et al., 2001) nor (Ng and Cardie, 2002) searches for the global optimal entity in that they make locally independent decisions during search. In contrast, our decoder alway</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proc. ofACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The ACE evaluation plan.</title>
<date>2003</date>
<note>www.nist.gov/speech/tests/ace/index.htm.</note>
<contexts>
<context position="887" citStr="NIST, 2003" startWordPosition="134" endWordPosition="135">paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes. A Maximum Entropy model is used to rank these paths. The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported. We also train a coreference system using the MUC6 data and competitive results are obtained. 1 Introduction In this paper, we will adopt the terminologies used in the Automatic Content Extraction (ACE) task (NIST, 2003). Coreference resolution in this context is defined as partitioning mentions into entities. A mention is an instance of reference to an object, and the collection of mentions referring to the same object in a document form an entity. For example, in the following sentence, mentions are underlined: “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” “American Medical Association”, “its” and</context>
</contexts>
<marker>NIST, 2003</marker>
<rawString>NIST. 2003. The ACE evaluation plan. www.nist.gov/speech/tests/ace/index.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Linear Observed Time Statistical Parser Based on Maximum Entropy Models.</title>
<date>1997</date>
<booktitle>In Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Adwait Ratnaparkhi. 1997. A Linear Observed Time Statistical Parser Based on Maximum Entropy Models. In Second Conference on Empirical Methods in Natural Language Processing, pages 1 – 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1719" citStr="Soon et al., 2001" startWordPosition="262" endWordPosition="265">nt form an entity. For example, in the following sentence, mentions are underlined: “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” “American Medical Association”, “its” and “group” belong to the same entity as they refer to the same object. Early work of anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase – which is the scope of this paper as well. One common strategy shared by (Soon et al., 2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is that a statistical model is trained to measure how likely a pair of mentions corefer; then a greedy procedure is followed to group mentions into entities. While this approach has yielded encouraging results, the way mentions are link</context>
<context position="9670" citStr="Soon et al., 2001" startWordPosition="1663" endWordPosition="1666"> it penalizes creating new entities; Therefore, is called start penalty. The start penalty can be used to balance entity miss and false alarm. 3.2 Model Training and Features The model depends on all partial entities , which can be very expensive. After making some modeling assumptions, we can approximate it as: From (7) to (8), entities other than the one in focus, , are assumed to have no influence on the decision of linking with . (9) further assumes that the entity-mention score can be obtained by the maximum mention pair score. The model (9) is very similar to the model in (Morton, 2000; Soon et al., 2001; Ng and Cardie, 2002) while (8) has more conditions. We use maximum entropy model (Berger et al., 1996) for both the mention-pair model (9) and the entity-mention model (8): where is a feature and is its weight; is a normalizing factor to ensure that (10) or (11) is a probability. Effective training algorithm exists (Berger et al., 1996) once the set of features is selected. The basic features used in the models are tabulated in Table 1. Features in the lexical category are applicable to non-pronominal mentions only. Distance features characterize how far the two mentions are, either by the n</context>
<context position="27849" citStr="Soon et al. (2001)" startWordPosition="4744" endWordPosition="4747">6-small system has the same level of performance as the ACE system, while the MUC6-big system performs better than the ACE system. The results show that the algorithm works well on the MUC6 data despite some information is lost in the conversion from the MUC format to the ACE format. System MUC F-measure ECM-F MUC6-small 83.9% 72.1% MUC6-big 85.7% 76.8% Table 5: Results on the MUC6 formal test set. 6 Related Work There exists a large body of literature on the topic of coreference resolution. We will compare this study with some relevant work using machine learning or statistical methods only. Soon et al. (2001) uses a decision tree model for coreference resolution on the MUC6 and MUC7 data. Leaves of the decision tree are labeled with “link” or “not-link” in training. At test time, the system checks a mention against all its preceding mentions, and the first one labeled with “link” is picked as the antecedent. Their work is later enhanced by (Ng and Cardie, 2002) in several aspects: first, the decision tree returns scores instead of a hard-decision of “link” or “not-link” so that Ng and Cardie (2002) is able to pick the “best” candidate on the left, as opposed the first in (Soon et al., 2001); Secon</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
</authors>
<title>A model-theoretic coreference scoring scheme. In</title>
<date>1995</date>
<booktitle>In Proc. ofMUC6,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="18599" citStr="Vilain et al., 1995" startWordPosition="3212" endWordPosition="3215">n which case excessive number of paths may survive local and global pruning. Whenever available, we check the compatibility of entity types between the in-focus entity and the active mention. A hypothesis with incompatible entity types is discarded. In the ACE annotation, every mention has an entity type. Therefore we can eliminate hypotheses with two mentions of different types. and ), which leads to a mention recall and precision . The ECM-F measures the percentage of mentions that are in the “right” entities. For tests on the MUC data, we report both F-measure using the official MUC score (Vilain et al., 1995) and ECM-F. The MUC score counts the common links between the reference and the system output. 5 Experiments 5.1 Performance Metrics The official performance metric for the ACE task is ACE-value. ACE-value is computed by first calculating the weighted cost of entity insertions, deletions and substitutions; The cost is then normalized against the cost of a nominal coreference system which outputs no entities; The ACE-value is obtained by subtracting the normalized cost from . Weights are designed to emphasize NAME entities, while PRONOUN entities (i.e., an entity consisting of only pronominal m</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, , and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In In Proc. ofMUC6, pages 45–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Coreference resolution using competition learning approach.</title>
<date>2003</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="1738" citStr="Yang et al., 2003" startWordPosition="266" endWordPosition="269">For example, in the following sentence, mentions are underlined: “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” “American Medical Association”, “its” and “group” belong to the same entity as they refer to the same object. Early work of anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase – which is the scope of this paper as well. One common strategy shared by (Soon et al., 2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is that a statistical model is trained to measure how likely a pair of mentions corefer; then a greedy procedure is followed to group mentions into entities. While this approach has yielded encouraging results, the way mentions are linked is arguably subo</context>
<context position="28554" citStr="Yang et al., 2003" startWordPosition="4871" endWordPosition="4874">s of the decision tree are labeled with “link” or “not-link” in training. At test time, the system checks a mention against all its preceding mentions, and the first one labeled with “link” is picked as the antecedent. Their work is later enhanced by (Ng and Cardie, 2002) in several aspects: first, the decision tree returns scores instead of a hard-decision of “link” or “not-link” so that Ng and Cardie (2002) is able to pick the “best” candidate on the left, as opposed the first in (Soon et al., 2001); Second, Ng and Cardie (2002) expands the feature sets of (Soon et al., 2001). The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate. Neither (Soon et al., 2001) nor (Ng and Cardie, 2002) searches for the global optimal entity in that they make locally independent decisions during search. In contrast, our decoder always searches for the best result ranked by the cumulative score (subject to pruning), and subsequent decisions depend on earlier ones. Recently, McCallum and Wellner (2003) proposed to use graphical models for computing probabilities of entities. The model is appealing in that it can potentially overcome the limitation of mention-pair model in wh</context>
</contexts>
<marker>Yang, Zhou, Su, Tan, 2003</marker>
<rawString>Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim Tan. 2003. Coreference resolution using competition learning approach. In Proc. of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>