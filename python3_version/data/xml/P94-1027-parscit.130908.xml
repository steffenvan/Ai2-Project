<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.393533">
OPTIMIZING THE COMPUTATIONAL LEXICALIZATION OF
LARGE GRAMMARS
</title>
<author confidence="0.643115">
Christian JACQUEMIN
</author>
<affiliation confidence="0.4819705">
Institut de Recherche en Informatique de Nantes (IRIN)
JUT de Nantes —3, rue du Mardchal Joffre
</affiliation>
<address confidence="0.654067">
F-44041 NANTES Cedex 01— FRANCE
</address>
<email confidence="0.768409">
e—mail : jacquemin@ irin.iut—nantes.univ—nantesir
</email>
<sectionHeader confidence="0.989668" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999404">
The computational lexicalization of a
grammar is the optimization of the links
between lexicalized rules and lexical items in
order to improve the quality of the bottom-up
filtering during parsing. This problem is
NP -complete and untractable on large
grammars. An approximation algorithm is
presented. The quality of the suboptimal
solution is evaluated on real-world grammars as
well as on randomly generated ones.
</bodyText>
<sectionHeader confidence="0.961482" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.926292098039216">
Lexicalized grammar formalisms and more
specifically Lexicalized Tree Adjoining
Grammars (LTAGs) give a lexical account of
phenomena which cannot be considered as
purely syntactic (Schabes et al, 1990). A
formalism is said to be lexicalized if it is
composed of structures or rules associated with
each lexical item and operations to derive new
structures from these elementary ones. The
choice of the lexical anchor of a rule is
supposed to be determined on purely linguistic
grounds. This is the linguistic side of
lexicalization which links to each lexical head a
set of minimal and complete structures. But
lexicalization also has a computational aspect
because parsing algorithms for lexicalized
grammars can take advantage of lexical links
through a two-step strategy (Schabes and Joshi,
1990). The first step is the selection of the set
of rules or elementary structures associated
with the lexical items in the input sentence&apos;. In
the second step, the parser uses the rules
filtered by the first step.
The two kinds of anchors corresponding to
these two aspects of lexicalization can be
considered separately:
• The linguistic anchors are used to access the
grammar, update the data, gather together
items with similar structures, organize the
grammar into a hierarchy...
• The computational anchors are used to
select the relevant rules during the first step
of parsing and to improve computational
and conceptual tractability of the parsing
algorithm.
Unlike linguistic lexicalization, computational
anchoring concerns any of the lexical items
found in a rule and is only motivated by the
quality of the induced filtering. For example,
the systematic linguistic anchoring of the rules
describing &amp;quot;Nmetal alloy&amp;quot; to their head noun
&amp;quot;alloy&amp;quot; should be avoided and replaced by a
more distributed lexicalization. Then, only a
few rules &amp;quot;Nmetat alloy&amp;quot; will be activated when
encountering the word &amp;quot;alloy&amp;quot; in the input.
In this paper, we investigate the problem of
the optimization of computational
lexicalization. We study how to choose the
computational anchors of a lexicalized
grammar so that the distribution of the rules on
to the lexical items is the most uniform possible
</bodyText>
<footnote confidence="0.68202975">
1 The computational anchor of a rule should not be
optional (viz included in a disjunction) to make sure
that it will be encountered in any string derived from
this rule.
</footnote>
<page confidence="0.997985">
196
</page>
<bodyText confidence="0.999956805555556">
with respect to rule weights. Although
introduced with reference to LTAGs, this
optimization concerns any portion of a
grammar where rules include one or more
potential lexical anchors such as Head Driven
Phrase Structure Grammar (Pollard and Sag,
1987) or Lexicalized Context-Free Grammar
(Schabes and Waters, 1993).
This algorithm is currently used to good
effect in FASTR a unification-based parser for
terminology extraction from large corpora
(Jacquemin, 1994). In this framework, terms
are represented by rules in a lexicalized
constraint-based formalism. Due to the large
size of the grammar, the quality of the
lexicalization is a determining factor for the
computational tractability of the application.
FASTR is applied to automatic indexing on
industrial data and lays a strong emphasis on
the handling of term variations (Jacquemin and
Royaute, 1994).
The remainder of this paper is organized as
follows. In the following part, we prove that the
problem of the Lexicalization of a Grammar is
NP-complete and hence that there is no better
algorithm known to solve it than an
exponential exhaustive search. As this solution
is untractable on large data, an approximation
algorithm is presented which has a
computational-time complexity proportional to
the cubic size of the grammar. In the last part,
an evaluation of this algorithm on real-world
grammars of 6,622 and 71,623 rules as well as
on randomly generated ones confirms its
computational tractability and the quality of
the lexicalization.
</bodyText>
<subsectionHeader confidence="0.9993295">
The Problem of the
Lexicalization of a Grammar
</subsectionHeader>
<bodyText confidence="0.999284487804878">
Given a lexicalized grammar, this part describes
the problem of the optimization of the
computational lexicalization. The solution to
this problem is a lexicalization function
(henceforth a lexicalization) which associates to
each grammar rule one of the lexical items it
includes (its lexical anchor). A lexicalization is
optimized to our sense if it induces an optimal
preprocessing of the grammar. Preprocessing is
intended to activate the rules whose lexical
anchors are in the input and make all the
possible filtering of these rules before the
proper parsing algorithm. Mainly,
preprocessing discards the rules selected
through lexicalization including at least one
lexical item which is not found in the input.
The first step of the optimization of the
lexicalization is to assign a weight to each rule.
The weight is assumed to represent the cost of
the corresponding rule during the
preprocessing. For a given lexicalization, the
weight of a lexical item is the sum of the
weights of the rules linked to it. The weights
are chosen so that a uniform distribution of the
rules on to the lexical items ensures an optimal
preprocessing. Thus, the problem is to find an
anchoring which achieves such a uniform
distribution.
The weights depend on the physical
constraints of the system. For example, the
weight is the number of nodes if the memory
size is the critical point. In this case, a uniform
distribution ensures that the rules linked to an
item will not require more than a given
memory space. The weight is the number of
terminal or non-terminal nodes if the
computational cost has to be minimized.
Experimental measures can be performed on a
test set of rules in order to determine the most
accurate weight assignment.
Two simplifying assumptions are made:
</bodyText>
<listItem confidence="0.97703425">
• The weight of a rule does not depend on the
lexical item to which it is anchored.
• The weight of a rule does not depend on the
other rules simultaneously activated.
</listItem>
<bodyText confidence="0.999875307692308">
The second assumption is essential for settling
a tractable problem. The first assumption can
be avoided at the cost of a more complex
representation. In this case, instead of having a
unique weight, a rule must have as many
weights as potential lexical anchors. Apart from
this modification, the algorithm that will be
presented in the next part remains much the
same than in the case of a single weight. If the
first assumption is removed, data about the
frequency of the items in corpora can be
accounted for. Assigning smaller weights to
rules when they are anchored to rare items will
</bodyText>
<page confidence="0.990445">
197
</page>
<bodyText confidence="0.926665526315789">
make the algorithm favor the anchoring to
these items. Thus, due to their rareness, the
corresponding rules will be rarely selected.
Illustration Terms, compounds and more
generally idioms require a lexicalized syntactic
representation such as LTAGs to account for
the syntax of these lexical entries (Abellle and
Schabes, 1989). The grammars chosen to
illustrate the problem of the optimization of the
lexicalization and to evaluate the algorithm
consist of idiom rules such as 9:
9 = {from time to time, high time,
high grade, high grade steel}
Each rule is represented by a pair (wi, Ai) where
wi is the weight and Ai the set of potential
anchors. If we choose the total number of
words in an idiom as its weight and its non-
empty words as its potential anchors, 9 is
represented by the following grammar :
</bodyText>
<equation confidence="0.994124666666667">
G1= {a = (4, {time}), b = (2, (high,time)),
c = (2, {grade,high}),
d = (3, {grade,high,steel})
</equation>
<bodyText confidence="0.9997394">
We call vocabulary, the union V of all the sets
of potential anchors Ai. Here, V = {grade, high,
steel, time}. A lexicalization is a function A
associating a lexical anchor to each rule.
Given a threshold 0, the membership
problem called the Lexicalization of a
Grammar (LG) is to find a lexicalization so that
the weight of any lexical item in V is less than
or equal to 0. If 0 4 in the preceding
example, LG has a solution A:
</bodyText>
<equation confidence="0.999758">
A(a) = time, A(b) = A(c)= high,
A(d) = steel
</equation>
<bodyText confidence="0.7819255">
If 0 5. 3, LG has no solution.
Definition of the LG Problem
</bodyText>
<equation confidence="0.790796">
G = {(w1, Ai)) (wie Q+, Ai finite sets)
V= {vi} = Ai ;OE (Q+
(1) LG ( (V, G, 0, 2)1 where A : G —&gt; V is a
</equation>
<bodyText confidence="0.822255444444444">
total function anchoring the rules so that
(V(w, A)E G) A((w, , A))E A
and (VvE V) w 9)
A((w, A)). v
The associated optimization problem is to
determine the lowest value 0of the threshold
so that there exists a solution (V, G, 0opt, A) to
LG. The solution of the optimization problem
for the preceding example is 000= 4.
</bodyText>
<equation confidence="0.348439">
Lemma LG is in NP.
</equation>
<bodyText confidence="0.999048">
It is evident that checking whether a given
lexicalization is indeed a solution to LG can be
done in polynomial time. The relation R
defined by (2) is polynomially decidable:
</bodyText>
<listItem confidence="0.611982">
(2) R(V, G, 0, A) = [if A: V-+G and (V vE V)
</listItem>
<bodyText confidence="0.943668875">
w Othen true else false]
A((w, A)) = v
The weights of the items can be computed
through matrix products : a matrix for the
grammar and a matrix for the lexicalization.
The size of any lexicalization A is linear in the
size of the grammar. As (V, G, 0, A)ELG if and
only if [R(V, G, 0, A)] is true, LG is in NP. •
</bodyText>
<equation confidence="0.510976">
Theorem LG is NP-complete.
</equation>
<bodyText confidence="0.718674666666667">
Bin Packing (BP) which is NP-complete is
polynomial-time Karp reducible to LG. BP
(Baase, 1986) is the problem defined by (3) :
</bodyText>
<listItem confidence="0.651987">
(3) BP -= (R, {R1, , Rk)) I where
</listItem>
<equation confidence="0.921714">
R = , rn} is a set of n positive
</equation>
<bodyText confidence="0.831818777777778">
rational numbers less than or equal to 1
and (R1, , Rk} is a partition of R (k bins
in which the ris are packed) such that
(ViE {1, , k}) I r 1.
TER&apos;
First, any instance of BP can be represented as
an instance of LG. Let (R, {R1, , Rk}) be an
instance of BP it is transformed into the
instance (V. G, 0, A) of LG as follows:
</bodyText>
<figure confidence="0.2642786">
(4) V = {vi, ,v} a set of k symbols, 9=1,
G = {(rp 10, ,(r, V)}
and (ViE {1, , k}) (V je {1, ,n})
A((ri, v)) = v riE R.
For all 1E { 1, , k} and j€ {1, , n}, we
</figure>
<bodyText confidence="0.978662">
consider the assignment of ri to the bin R. of
BP as the anchoring of the rule (ri, V) to the
</bodyText>
<note confidence="0.371517">
item vi of LG. If (R, (R1, , RkDE BP then:
</note>
<page confidence="0.98183">
198
</page>
<listItem confidence="0.740058666666667">
(5) (ViE {1, ,k}) r 1
reRi
&lt;=&gt; (Vie {1, ... ,k}) 1
</listItem>
<equation confidence="0.7209335">
24r, vD=v;
Thus (V, G, 1, .)e LG. Conversely, given a
solution (V. G, 1, A) of LG, let R. {rie R I
A.((ri,V)) = for all ie {1, , k}. Clearly
</equation>
<bodyText confidence="0.999288727272727">
{R1, , Rk} is a partition of R because the
lexicalization is a total function and the
preceding formula ensures that each bin is
correctly loaded. Thus (R, {R1, , R}) BP. It
is also simple to verify that the transformation
from BP to LG can be performed in
polynomial time.
The optimization of an NP-complete
problem is NP-complete (Sommerhalder and
van Westrhenen, 1988), then the optimization
version of LG is NP-complete.
</bodyText>
<subsectionHeader confidence="0.9525305">
An Approximation Algorithm
for LG
</subsectionHeader>
<bodyText confidence="0.999453333333333">
This part presents and evaluates an n3-time
approximation algorithm for the LG problem
which yields a suboptimal solution close to the
optimal one. The first step is the &apos;easy&apos;
anchoring of rules including at least one rare
lexical item to one of these items. The second
step handles the `hard&apos; lexicalization of the
remaining rules including only common items
found in several other rules and for which the
decision is not straightforward. The
discrimination between these two kinds of items
is made on the basis of their global weight GW
</bodyText>
<listItem confidence="0.938377888888889">
(6) which is the sum of the weights of the rules
which are not yet anchored and which have this
lemma as potential anchor. VA and GA are
subsets of V and G which denote the items and
the rules not yet anchored. The ws and 0 are
assumed to be integers by multiplying them by
their lowest common denominator if necessary.
(6) (VvE VA) GW(v)=
(w, A) e GA,VE A
</listItem>
<bodyText confidence="0.98774375">
Step 1 : &apos;Easy&apos; Lexicalization of Rare Items
This first step of the optimization algorithm is
also the first step of the exhaustive search. The
value of the minimal threshold 0„,in given by
</bodyText>
<listItem confidence="0.464981428571429">
(7) is computed by dividing the sum of the rule
weights by the number of lemmas (1x1 stands
for the smallest integer greater than or equal to
x and I VA I stands for the size of the set VA) :
[
(7) emin = (w, A) E GA w where I VA, I *o
I Vzi
</listItem>
<bodyText confidence="0.980575051282051">
All the rules which include a lemma with a
global weight less than or equal to 0„,i,, are
anchored to this lemma. When this linking is
achieved in a non-deterministic manner, emin is
recomputed. The algorithm loops on this
lexicalization, starting it from scratch every
time, until emin remains unchanged or until all
the rules are anchored. The output value of emin
is the minimal threshold such that LG has a
solution and therefore is less than or equal to
0. After Step 1, either each rule is anchored
opt
or all the remaining items in VA, have a global
weight strictly greater than emin. The algorithm
is shown in Figure 1.
Step 2 : &apos;Hard&apos; Lexicalization of Common
Items During this step, the algorithm
repeatedly removes an item from the remaining
vocabulary and yields the anchoring of this
item. The item with the lowest global weight is
handled first because it has the smallest
combination of anchorings and hence the
probability of making a wrong choice for the
lexicalization is low. Given an item, the
candidate rules with this item as potential
anchor are ranked according to :
1 The highest priority is given to the rules
whose set of potential anchors only includes
the current item as non-anchored item.
2 The remaining candidate rules taken first
are the ones whose potential anchors have
the highest global weights (items found in
several other non-anchored rules).
The algorithm is shown in Figure 2. The
output of Step 2 is the suboptimal
computational lexicalization A. of the whole
grammar and the associated threshold 60„bopt.
Both steps can be optimized. Useless
computation is avoided by watching the capital
</bodyText>
<page confidence="0.997839">
199
</page>
<bodyText confidence="0.999379">
of weight C defined by (8) with 9- 0„,in during
Step 1 and 0 - Osubopt during Step 2:
</bodyText>
<equation confidence="0.9662405">
(8) c=19.1vAl - w
(v, A) E
</equation>
<bodyText confidence="0.999586222222222">
C corresponds to the weight which can be lost
by giving a weight W(w) which is strictly less
than the current threshold O. Every time an
anchoring to a unit to is completed, C is
reduced from 8- W(0). If C becomes negative
in either of both steps, the algorithm will fail to
make the lexicalization of the grammar and
must be started again from Step 1 with a higher
value for O.
</bodyText>
<equation confidence="0.853752666666667">
Input V, G
Output 0 • , VA, GA, A: (G - GA) -&gt; (V -VA)
1
(w, 1 w
emin 4-- A) E G
Step] repeat
GA4-- G ; A 4- V;
for each VE V such as GW(v).0 • do
for each (w, A)eG such as veA
</equation>
<bodyText confidence="0.699313">
and A((w, A)) not yet defined do
</bodyText>
<equation confidence="0.920736333333333">
A((w, A)) &lt;- v;
GA.4-- GA - {(W, A)} ;
update GW(v) ;
end
VA 4- VA- {V} ;
end
. [(w, A) E - ;
VA1
w
if( (rmin °nth/
and ( (VvE VA) GW(v)&gt; 0„,in))
or GA =0)
then exit repeat;
.0mrn4-O&apos;min ;
until( false ) ;
</equation>
<figureCaption confidence="0.97242">
Figure 1: Step 1 of the approximation algorithm.
</figureCaption>
<figure confidence="0.6951706">
Input 0„an, V, G, VA, GA,
(G-GA) (V-VA)
Output 0„bopt, A: G V
Step2 Osubopt 4-- emin ;
repeat
</figure>
<bodyText confidence="0.958236333333333">
;; anchoring the rules with only w as
;; free potential anchor (tii E VA with
;; the lowest global weight)
</bodyText>
<equation confidence="0.991757833333333">
ftv1;
G141 4-- (w,A)E GA I AnVA = {w} };
( w &lt; Osubopt)
(w, A) E G,i
then 0 • 0min+ 1; goto Stepl ;
for each (w, A)E Gal do
A((w, A)) ;
GA 4- GA - (w, A) } ;
end
0a24- {(w, A)E ; AnVA, D {0}};
W(ro) E- w;
A((w, A)) =
</equation>
<bodyText confidence="0.521047">
;; ranldng2 Ga2 and anchoring
for(i+-1;i1Grix21;i4---i+l)do
</bodyText>
<equation confidence="0.828700888888889">
(w, A) &lt;- r -1(i) ;; i ranked by r
if( W(ar) + w &gt; Omin )
then exit for;
W(J)4-W(w)+w;
A((w, A)) ;
GA GA-{(W, A)} ;
end
VA 4- VA -{VT} ;
until ( GA =0);
</equation>
<figureCaption confidence="0.87916">
Figure 2: Step 2 of the approximation algorithm.
</figureCaption>
<footnote confidence="0.934836">
2 The ranking function r: G ---&gt; (1,... I Gov! ) is
such that r((w, A)) &gt; r((w&apos;, A)
4=&gt; min W(v)Allri-)kl-(01}ilW(V)
</footnote>
<note confidence="0.504222">
vE A (IVA- {fin
</note>
<page confidence="0.995139">
200
</page>
<bodyText confidence="0.973244454545455">
Example3 The algorithm has been applied to
a test grammar G2 obtained from 41 terms with
11 potential anchors. The algorithm fails in
making the lexicalization of G2 with the
minimal threshold Omin = 12, but achieves it
with esubopt 13. This value of Osubopt can be
compared with the optimal one by running the
exhaustive search. There are 232 (:4: 4 109)
possible lexicalizations among which 35,336
are optimal ones with a threshold of 13. This
result shows that the approximation algorithm
brings forth one of the optimal solutions which
only represent a proportion of 8 10-6 of the
possible lexicalizations. In this case the optimal
and the suboptimal threshold coincide.
Time-Complexity of the Approximation
Algorithm A grammar G on a vocabulary V
can be represented by a IGIx vi -matrix of
Boolean values for the set of potential anchors
and a lx IGI -matrix for the weights. In order
to evaluate the complexity of the algorithms as
a function of the size of the grammar, we
assume that I VI and I G I are of the same order
of magnitude n. Step 1 of the algorithm
corresponds to products and sums on the
preceding matrixes and takes 0(n3) time. The
worst-case time-complexity for Step 2 of the
algorithm is also 0(n3) when using a naive
0(n2) algorithm to sort the items and the rules
by decreasing priority. In all, the time required
by the approximation algorithm is proportional
to the cubic size of the grammar.
This order of magnitude ensures that the
algorithm can be applied to large real-world
grammars such as terminological grammars.
On a Sparc 2, the lexicalization of a
terminological grammar composed of 6,622
rules and 3,256 words requires 3 seconds (real
time) and the lexicalization of a very large
terminological grammar of 71,623 rules and
38,536 single words takes 196 seconds. The
two grammars used for these experiment were
generated from two lists of terms provided by
the documentation center INIST/CNRS.
</bodyText>
<footnote confidence="0.681667666666667">
3 The exhausitve grammar and more details about this
example and the computations of the following
section are in (Jacquemin, 1991).
</footnote>
<subsectionHeader confidence="0.854879">
Evaluation of the
Approximation Algorithm
</subsectionHeader>
<bodyText confidence="0.979618555555555">
Bench Marks on Artificial Grammars In
order to check the quality of the lexicalization
on different kinds of grammars, the algorithm
has been tested on eight randomly generated
grammars of 4,000 rules having from 2 to 10
potential anchors (Table 1). The lexicon of the
first four grammars is 40 times smaller than the
grammar while the lexicon of the last four ones
is 4 times smaller than the grammar (this
proportion is close to the one of the real-world
grammar studied in the next subsection). The
eight grammars differ in their distribution of
the items on to the rules. The uniform
distribution corresponds to a uniform random
choice of the items which build the set of
potential anchors while the Gaussian one
corresponds to a choice taking more frequently
some items. The higher the parameter s, the
flatter the Gaussian distribution.
The last two columns of Table 1 give the
minimal threshold Omin after Step 1 and the
suboptimal threshold 0„bopt found by the
approximation algorithm. As mentioned when
presenting Step 1, the optimal threshold °opt is
necessarily greater than or equal to 0„,i, after
Step 1. Table 1 reports that the suboptimal
threshold 0b is not over 2 units greater than
0„,i, after Step 1. The suboptimal threshold
yielded by the approximation algorithm on
these examples has a high quality because it is
at worst 2 units greater than the optimal one.
A Comparison with Linguistic Lexicalization
on a Real-World Grammar This evaluation
consists in applying the algorithm to a natural
language grammar composed of 6,622 rules
(terms from the domain of metallurgy
provided by INIST/CNRS) and a lexicon of
3,256 items. Figure 3 depicts the distribution of
the weights with the natural linguistic
lexicalization. The frequent head words such as
alloy are heavily loaded because of the
numerous terms in N—alloy with N being a
name of metal. Conversely, in Figure 4 the
distribution of the weights from the
approximation algorithm is much more
</bodyText>
<page confidence="0.994442">
201
</page>
<bodyText confidence="0.621041">
uniform. The maximal weight of an item is 241 threshold after Step 1 being 34, the suboptimal
with the linguistic lexicalization while it is only threshold yielded by the approximation
34 with the optimized lexicalization. The algorithm is equal to the optimal one.
</bodyText>
<table confidence="0.9996317">
Lexicon size Distribution of the em„, 0,„in esubopt
items on the rules before Step 1 after Step 1 suboptimal threshold
100 uniform 143 143 143
100 Gaussian (s = 30) 141 143 144
100 Gaussian (s = 20) 141 260 261
100 Gaussian (s = 10) 141 466 468
1,000 uniform 15 15 16
1,000 Gaussian (s = 30) 14 117 118
1,000 Gaussian (s= 20) 15 237 238
1,000 Gaussian (s= 10) 14 466 467
</table>
<tableCaption confidence="0.999613">
Table 1: Bench marks of the approximation algorithm on eight randomly generated grammars.
</tableCaption>
<figure confidence="0.638267">
15 30 45 60 75 90 105 120 135 150 165 180 195 210 225 240
</figure>
<figureCaption confidence="0.998236">
Figure 3: Distribution of the weights of the lexical items with the lexicalization on head words.
</figureCaption>
<figure confidence="0.99801075">
11
Number of 3000
items
1000
(log scale)
100
1.. J. 11 iii.111 1 1 1 11 1
10
Number of 1000
items
(log scale)
100
10
1 2 3 4 5 6 7 8 910 12 14 16 18 20 22 24 26 28 30 32 34 36
10111111 illH I II
Weight
</figure>
<figureCaption confidence="0.999963">
Figure 4: Distribution of the weights of the lexical items with the optimized lexicalization.
</figureCaption>
<page confidence="0.994992">
202
</page>
<sectionHeader confidence="0.974146" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.99999684375">
As mentioned in the introduction, the
improvement of the lexicalization through an
optimization algorithm is currently used in
FASTR a parser for terminological extraction
through NLP techniques where terms are
represented by lexicalized rules. In this
framework as in top-down parsing with LTAGs
(Schabes and Joshi, 1990), the first phase of
parsing is a filtering of the rules with their
anchors in the input sentence. An unbalanced
distribution of the rules on to the lexical items
has the major computational drawback of
selecting an excessive number of rules when
the input sentence includes a common head
word such as &amp;quot;alloy&amp;quot; (127 rules have &amp;quot;alloy&amp;quot;
as head). The use of the optimized
lexicalization allows us to filter 57% of the
rules selected by the linguistic lexicalization.
This reduction is comparable to the filtering
induced by linguistic lexicalization which is
around 85% (Schabes and Joshi, 1990).
Correlatively the parsing speed is multiplied by
2.6 confirming the computational saving of the
optimization reported in this study.
There are many directions in which this
work could be refined and extended. In
particular, an optimization of this optimization
could be achieved by testing different weight
assignments in correlation with the parsing
algorithm. Thus, the computational
lexicalization would fasten both the
preprocessing and the parsing algorithm.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99990025">
I would like to thank Alain Colmerauer for his
valuable comments and a long discussion on a
draft version of my PhD dissertation. I also
gratefully acknowledge Chantal Enguehard
and two anonymous reviewers for their remarks
on earlier drafts. The experiments on industrial
data were done with term lists from the
documentation center INIST/CNRS.
</bodyText>
<sectionHeader confidence="0.999939" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.99982618367347">
Abel116, Anne, and Yves Schabes. 1989. Parsing
Idioms in Tree Adjoining Grammars. In
Proceedings, 4th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL&apos;89),
Manchester, UK.
Baase, Sara. 1978. Computer Algorithms.
Addison Wesley, Reading, MA.
Jacquemin, Christian. 1991. Transformations
des noms composes. PhD Thesis in
Computer Science, Universite of Paris 7.
Unpublished.
Jacquemin, Christian. 1994. FASTR : A
unification grammar and a parser for
terminology extraction from large corpora.
In Proceedings, IA-94, Paris, EC2, June
1994.
Jacquemin, Christian and Jean Royaute. 1994.
Retrieving terms and their variants in a
lexicalized unification-based framework. In
Proceedings, 17th Annual International
ACM SIGIR Conference (SIG1R&apos;94), Dublin,
July 1994.
Pollard, Carl and Ivan Sag. 1987. Information-
Based Syntax and Semantics. Vol 1:
Fundamentals. CSLI, Stanford, CA.
Schabes, Yves, Anne Abellle, and Aravind K.
Joshi. 1988. Parsing strategies with
lexicalized&apos; grammars: Application to tree
adjoining grammar. In Proceedings, 12th
International Conference on Computational
Linguistics (COLING &apos;88), Budapest,
Hungary.
Schabes, Yves and Aravind K. Joshi. 1990.
Parsing strategies with `lexicalized&apos;
grammars: Application to tree adjoining
grammar. In Masaru Tomita, editor, Current
Issues in Parsing Technologies. Kluwer
Academic Publishers, Dordrecht.
Schabes, Yves and Richard C. Waters. 1993.
Lexicalized Context-Free Grammars. In
Proceedings, 31st Meeting of the
Association for Computational Linguistics
(ACL&apos;93), Columbus, Ohio.
Sommerhalder, Rudolph and S. Christian van
Westrhenen. 1988. The Theory of
Computability: Programs, Machines,
Effectiveness and Feasibility. Addison-
Wesley, Reading, MA.
</reference>
<page confidence="0.999205">
203
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.829395">
<title confidence="0.9975485">OPTIMIZING THE COMPUTATIONAL LEXICALIZATION OF LARGE GRAMMARS</title>
<author confidence="0.999994">Christian JACQUEMIN</author>
<affiliation confidence="0.966493">Institut de Recherche en Informatique de Nantes (IRIN) JUT de Nantes —3, rue du Mardchal Joffre</affiliation>
<address confidence="0.995102">F-44041 NANTES Cedex 01— FRANCE</address>
<email confidence="0.908927">:jacquemin@</email>
<abstract confidence="0.998666545454546">The computational lexicalization of a grammar is the optimization of the links between lexicalized rules and lexical items in order to improve the quality of the bottom-up filtering during parsing. This problem is NP -complete and untractable on large grammars. An approximation algorithm is presented. The quality of the suboptimal solution is evaluated on real-world grammars as well as on randomly generated ones.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abel116</author>
<author>Yves Schabes</author>
</authors>
<title>Parsing Idioms in Tree Adjoining Grammars.</title>
<date>1989</date>
<booktitle>In Proceedings, 4th Conference of the European Chapter of the Association for Computational Linguistics (EACL&apos;89),</booktitle>
<location>Manchester, UK.</location>
<marker>Abel116, Schabes, 1989</marker>
<rawString>Abel116, Anne, and Yves Schabes. 1989. Parsing Idioms in Tree Adjoining Grammars. In Proceedings, 4th Conference of the European Chapter of the Association for Computational Linguistics (EACL&apos;89), Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Baase</author>
</authors>
<title>Computer Algorithms.</title>
<date>1978</date>
<publisher>Addison Wesley,</publisher>
<location>Reading, MA.</location>
<marker>Baase, 1978</marker>
<rawString>Baase, Sara. 1978. Computer Algorithms. Addison Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Jacquemin</author>
</authors>
<title>Transformations des noms composes.</title>
<date>1991</date>
<tech>PhD Thesis</tech>
<institution>in Computer Science, Universite of Paris 7. Unpublished.</institution>
<contexts>
<context position="17867" citStr="Jacquemin, 1991" startWordPosition="3185" endWordPosition="3186">ures that the algorithm can be applied to large real-world grammars such as terminological grammars. On a Sparc 2, the lexicalization of a terminological grammar composed of 6,622 rules and 3,256 words requires 3 seconds (real time) and the lexicalization of a very large terminological grammar of 71,623 rules and 38,536 single words takes 196 seconds. The two grammars used for these experiment were generated from two lists of terms provided by the documentation center INIST/CNRS. 3 The exhausitve grammar and more details about this example and the computations of the following section are in (Jacquemin, 1991). Evaluation of the Approximation Algorithm Bench Marks on Artificial Grammars In order to check the quality of the lexicalization on different kinds of grammars, the algorithm has been tested on eight randomly generated grammars of 4,000 rules having from 2 to 10 potential anchors (Table 1). The lexicon of the first four grammars is 40 times smaller than the grammar while the lexicon of the last four ones is 4 times smaller than the grammar (this proportion is close to the one of the real-world grammar studied in the next subsection). The eight grammars differ in their distribution of the ite</context>
</contexts>
<marker>Jacquemin, 1991</marker>
<rawString>Jacquemin, Christian. 1991. Transformations des noms composes. PhD Thesis in Computer Science, Universite of Paris 7. Unpublished.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Jacquemin</author>
</authors>
<title>FASTR : A unification grammar and a parser for terminology extraction from large corpora.</title>
<date>1994</date>
<booktitle>In Proceedings, IA-94,</booktitle>
<location>Paris, EC2,</location>
<contexts>
<context position="3521" citStr="Jacquemin, 1994" startWordPosition="539" endWordPosition="540">or of a rule should not be optional (viz included in a disjunction) to make sure that it will be encountered in any string derived from this rule. 196 with respect to rule weights. Although introduced with reference to LTAGs, this optimization concerns any portion of a grammar where rules include one or more potential lexical anchors such as Head Driven Phrase Structure Grammar (Pollard and Sag, 1987) or Lexicalized Context-Free Grammar (Schabes and Waters, 1993). This algorithm is currently used to good effect in FASTR a unification-based parser for terminology extraction from large corpora (Jacquemin, 1994). In this framework, terms are represented by rules in a lexicalized constraint-based formalism. Due to the large size of the grammar, the quality of the lexicalization is a determining factor for the computational tractability of the application. FASTR is applied to automatic indexing on industrial data and lays a strong emphasis on the handling of term variations (Jacquemin and Royaute, 1994). The remainder of this paper is organized as follows. In the following part, we prove that the problem of the Lexicalization of a Grammar is NP-complete and hence that there is no better algorithm known</context>
</contexts>
<marker>Jacquemin, 1994</marker>
<rawString>Jacquemin, Christian. 1994. FASTR : A unification grammar and a parser for terminology extraction from large corpora. In Proceedings, IA-94, Paris, EC2, June 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Jacquemin</author>
<author>Jean Royaute</author>
</authors>
<title>Retrieving terms and their variants in a lexicalized unification-based framework.</title>
<date>1994</date>
<booktitle>In Proceedings, 17th Annual International ACM SIGIR Conference (SIG1R&apos;94),</booktitle>
<location>Dublin,</location>
<contexts>
<context position="3918" citStr="Jacquemin and Royaute, 1994" startWordPosition="598" endWordPosition="601">lard and Sag, 1987) or Lexicalized Context-Free Grammar (Schabes and Waters, 1993). This algorithm is currently used to good effect in FASTR a unification-based parser for terminology extraction from large corpora (Jacquemin, 1994). In this framework, terms are represented by rules in a lexicalized constraint-based formalism. Due to the large size of the grammar, the quality of the lexicalization is a determining factor for the computational tractability of the application. FASTR is applied to automatic indexing on industrial data and lays a strong emphasis on the handling of term variations (Jacquemin and Royaute, 1994). The remainder of this paper is organized as follows. In the following part, we prove that the problem of the Lexicalization of a Grammar is NP-complete and hence that there is no better algorithm known to solve it than an exponential exhaustive search. As this solution is untractable on large data, an approximation algorithm is presented which has a computational-time complexity proportional to the cubic size of the grammar. In the last part, an evaluation of this algorithm on real-world grammars of 6,622 and 71,623 rules as well as on randomly generated ones confirms its computational tract</context>
</contexts>
<marker>Jacquemin, Royaute, 1994</marker>
<rawString>Jacquemin, Christian and Jean Royaute. 1994. Retrieving terms and their variants in a lexicalized unification-based framework. In Proceedings, 17th Annual International ACM SIGIR Conference (SIG1R&apos;94), Dublin, July 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>InformationBased Syntax and Semantics. Vol 1: Fundamentals.</title>
<date>1987</date>
<location>CSLI, Stanford, CA.</location>
<contexts>
<context position="3309" citStr="Pollard and Sag, 1987" startWordPosition="508" endWordPosition="511">omputational lexicalization. We study how to choose the computational anchors of a lexicalized grammar so that the distribution of the rules on to the lexical items is the most uniform possible 1 The computational anchor of a rule should not be optional (viz included in a disjunction) to make sure that it will be encountered in any string derived from this rule. 196 with respect to rule weights. Although introduced with reference to LTAGs, this optimization concerns any portion of a grammar where rules include one or more potential lexical anchors such as Head Driven Phrase Structure Grammar (Pollard and Sag, 1987) or Lexicalized Context-Free Grammar (Schabes and Waters, 1993). This algorithm is currently used to good effect in FASTR a unification-based parser for terminology extraction from large corpora (Jacquemin, 1994). In this framework, terms are represented by rules in a lexicalized constraint-based formalism. Due to the large size of the grammar, the quality of the lexicalization is a determining factor for the computational tractability of the application. FASTR is applied to automatic indexing on industrial data and lays a strong emphasis on the handling of term variations (Jacquemin and Royau</context>
</contexts>
<marker>Pollard, Sag, 1987</marker>
<rawString>Pollard, Carl and Ivan Sag. 1987. InformationBased Syntax and Semantics. Vol 1: Fundamentals. CSLI, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Anne Abellle</author>
<author>Aravind K Joshi</author>
</authors>
<title>Parsing strategies with lexicalized&apos; grammars: Application to tree adjoining grammar.</title>
<date>1988</date>
<booktitle>In Proceedings, 12th International Conference on Computational Linguistics (COLING &apos;88),</booktitle>
<location>Budapest, Hungary.</location>
<marker>Schabes, Abellle, Joshi, 1988</marker>
<rawString>Schabes, Yves, Anne Abellle, and Aravind K. Joshi. 1988. Parsing strategies with lexicalized&apos; grammars: Application to tree adjoining grammar. In Proceedings, 12th International Conference on Computational Linguistics (COLING &apos;88), Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Aravind K Joshi</author>
</authors>
<title>Parsing strategies with `lexicalized&apos; grammars: Application to tree adjoining grammar.</title>
<date>1990</date>
<booktitle>Current Issues in Parsing Technologies.</booktitle>
<editor>In Masaru Tomita, editor,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="1498" citStr="Schabes and Joshi, 1990" startWordPosition="218" endWordPosition="221">ctic (Schabes et al, 1990). A formalism is said to be lexicalized if it is composed of structures or rules associated with each lexical item and operations to derive new structures from these elementary ones. The choice of the lexical anchor of a rule is supposed to be determined on purely linguistic grounds. This is the linguistic side of lexicalization which links to each lexical head a set of minimal and complete structures. But lexicalization also has a computational aspect because parsing algorithms for lexicalized grammars can take advantage of lexical links through a two-step strategy (Schabes and Joshi, 1990). The first step is the selection of the set of rules or elementary structures associated with the lexical items in the input sentence&apos;. In the second step, the parser uses the rules filtered by the first step. The two kinds of anchors corresponding to these two aspects of lexicalization can be considered separately: • The linguistic anchors are used to access the grammar, update the data, gather together items with similar structures, organize the grammar into a hierarchy... • The computational anchors are used to select the relevant rules during the first step of parsing and to improve compu</context>
<context position="21400" citStr="Schabes and Joshi, 1990" startWordPosition="3797" endWordPosition="3800"> of 3000 items 1000 (log scale) 100 1.. J. 11 iii.111 1 1 1 11 1 10 Number of 1000 items (log scale) 100 10 1 2 3 4 5 6 7 8 910 12 14 16 18 20 22 24 26 28 30 32 34 36 10111111 illH I II Weight Figure 4: Distribution of the weights of the lexical items with the optimized lexicalization. 202 Conclusion As mentioned in the introduction, the improvement of the lexicalization through an optimization algorithm is currently used in FASTR a parser for terminological extraction through NLP techniques where terms are represented by lexicalized rules. In this framework as in top-down parsing with LTAGs (Schabes and Joshi, 1990), the first phase of parsing is a filtering of the rules with their anchors in the input sentence. An unbalanced distribution of the rules on to the lexical items has the major computational drawback of selecting an excessive number of rules when the input sentence includes a common head word such as &amp;quot;alloy&amp;quot; (127 rules have &amp;quot;alloy&amp;quot; as head). The use of the optimized lexicalization allows us to filter 57% of the rules selected by the linguistic lexicalization. This reduction is comparable to the filtering induced by linguistic lexicalization which is around 85% (Schabes and Joshi, 1990). Correl</context>
</contexts>
<marker>Schabes, Joshi, 1990</marker>
<rawString>Schabes, Yves and Aravind K. Joshi. 1990. Parsing strategies with `lexicalized&apos; grammars: Application to tree adjoining grammar. In Masaru Tomita, editor, Current Issues in Parsing Technologies. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Richard C Waters</author>
</authors>
<title>Lexicalized Context-Free Grammars.</title>
<date>1993</date>
<booktitle>In Proceedings, 31st Meeting of the Association for Computational Linguistics (ACL&apos;93),</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="3372" citStr="Schabes and Waters, 1993" startWordPosition="516" endWordPosition="519">utational anchors of a lexicalized grammar so that the distribution of the rules on to the lexical items is the most uniform possible 1 The computational anchor of a rule should not be optional (viz included in a disjunction) to make sure that it will be encountered in any string derived from this rule. 196 with respect to rule weights. Although introduced with reference to LTAGs, this optimization concerns any portion of a grammar where rules include one or more potential lexical anchors such as Head Driven Phrase Structure Grammar (Pollard and Sag, 1987) or Lexicalized Context-Free Grammar (Schabes and Waters, 1993). This algorithm is currently used to good effect in FASTR a unification-based parser for terminology extraction from large corpora (Jacquemin, 1994). In this framework, terms are represented by rules in a lexicalized constraint-based formalism. Due to the large size of the grammar, the quality of the lexicalization is a determining factor for the computational tractability of the application. FASTR is applied to automatic indexing on industrial data and lays a strong emphasis on the handling of term variations (Jacquemin and Royaute, 1994). The remainder of this paper is organized as follows.</context>
</contexts>
<marker>Schabes, Waters, 1993</marker>
<rawString>Schabes, Yves and Richard C. Waters. 1993. Lexicalized Context-Free Grammars. In Proceedings, 31st Meeting of the Association for Computational Linguistics (ACL&apos;93), Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudolph Sommerhalder</author>
<author>S Christian van Westrhenen</author>
</authors>
<date>1988</date>
<booktitle>The Theory of Computability: Programs, Machines, Effectiveness and Feasibility.</booktitle>
<publisher>AddisonWesley,</publisher>
<location>Reading, MA.</location>
<marker>Sommerhalder, van Westrhenen, 1988</marker>
<rawString>Sommerhalder, Rudolph and S. Christian van Westrhenen. 1988. The Theory of Computability: Programs, Machines, Effectiveness and Feasibility. AddisonWesley, Reading, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>