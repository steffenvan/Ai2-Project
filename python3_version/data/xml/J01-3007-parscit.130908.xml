<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003525">
<note confidence="0.297973">
Book Reviews
</note>
<title confidence="0.980629">
Advances in Probabilistic and Other Parsing Technologies
</title>
<author confidence="0.992048">
Harry Bunt and Anton Nijholt (editors)
</author>
<affiliation confidence="0.972053">
(Tilburg University and University of Twente)
Dordrecht: Kluwer Academic
</affiliation>
<bodyText confidence="0.3781995">
Publishers (Text, speech and language
technology series, edited by Nancy Ide
and Jean Veronis, volume 16), 2000,
xv+267 pp; hardbound, ISBN
0-7923-6616-6, $112.00, £71.00,
Dfl 230.00
</bodyText>
<figure confidence="0.664165">
Reviewed by
Chris Brew
</figure>
<affiliation confidence="0.645027">
The Ohio State University
</affiliation>
<bodyText confidence="0.9998">
This book is an edited selection of papers presented at the Fifth International Workshop
on Parsing Technologies, held at MIT in September 1997. Several of the papers are
already well-known and others should be. The book could easily be used as the basis
for a graduate-level advanced course on parsing. The title is unwieldy, but appropriate:
most but not all of the papers have a strong probabilistic flavor.
My favorite papers are Erik Hektoen on &amp;quot;Probabilistic parse selection based on
semantic co-occurrences,&amp;quot; Jason Eisner on &amp;quot;Bilexical grammars and their cubic-time
parsing algorithms,&amp;quot; and Chris Manning and Bob Carpenter on &amp;quot;Probabilistic parsing
using left corner language models.&amp;quot; I like these papers because they step back from
the details of parsing technology and consider its wider significance.
Manning and Carpenter offer both detail and overview. They provide a series of
probabilistic models that relax the context-freeness assumption of probabilistic context-
free grammars, measure performance in the usual way, draw appropriate conclusions,
then provide the kicker in the form of a brief section explaining &amp;quot;Why parsing the Penn
Treebank is easy.&amp;quot; As Manning and Carpenter point out, in the particular case of the
Penn Treebank, the currently accepted PARSEVAL metrics (Grishman, Macleod, and
Sterling 1992) are actually quite easy to do well on, even if the system makes systematic
errors on such things as prepositional-phrase attachment. If systems are to be deployed
into situations where such deficiencies might matter, it might be necessary to find
more appropriate evaluation methods. This issue has subsequently been addressed
by others (Carroll, Briscoe, and Sanfilippo 1998; Carroll, Minnen, and Briscoe 1999),
who argue for more obviously task-related evaluation schemes involving predicate
argument structure and/or dependency information.
Hektoen&apos;s contribution is in the same vein; it takes seriously the notion that pars-
ing is often simply a device for getting at an underlying semantics. Under his scheme,
parse selection relies on the ability to collect statistics over semantic forms. Following
this path leads Hektoen into a careful exposition of a Bayesian-estimation approach to
parse selection, which appears to be &amp;quot;a sufficient response to the high degree of sparse-
ness in the lexical co-occurrence data without the blurring associated with smoothing
and clustering&amp;quot; (p. 162). Hektoen&apos;s approach appears to work well; of course, it does
require a broad-coverage parser capable of generating semantic representations, which
may be an obstacle for many. The exposition of the method is very clear and the com-
parison with previous approaches is enlightening.
</bodyText>
<page confidence="0.991421">
459
</page>
<note confidence="0.423453">
Computational Linguistics Volume 27, Number 3
</note>
<bodyText confidence="0.999591098039216">
Mark-Jan Nederhof&apos;s &amp;quot;Regular approximation of CFLs: A grammatical view&amp;quot; is
similar to Eisner&apos;s contribution in that its focus is primarily mathematical. It describes
an attractive approach to finite-state approximation of regular grammars. The essen-
tial idea is to characterize properties that make grammars non-regular, and to develop
schemes for systematically removing such properties. This helps to keep the approx-
imation process perspicuous. Experimental work with this approximation scheme is
absent from the current article, but is reported elsewhere (Nederhof 2000).
In &amp;quot;Probabilistic GLR parsing,&amp;quot; Kentaro Inui, Virach Sornlertlamvanich, Hozumi
Tanaka, and Takenobu Tokunaga provide a careful analysis of the process of LR pars-
ing. This leads to a probabilistic parsing scheme having the desirable property, not
previously achieved for LR parsers, that the sum over all parses of the probability
is unity. Once again experimental work is not present here but is reported elsewhere
(Sornlertlamvanich, Inui, Tokunaga, Tanaku, and Takezawa 1999).
Eisner&apos;s paper does not report experiments either, but addresses a problem with
profound practical significance. It analyses the computational properties of grammars
in which potentially idiosyncratic word-to-word relationships play a key role. The
framework used is general enough to capture the essence of many recent statistical
parsers and clean enough to make it easy (and interesting) to compare one with an-
other. I like Eisner&apos;s paper for the insight it provides into the options available to
the lexically minded probabilistic modeler. This aspect is also present in &amp;quot;Encoding
frequency information in lexicalized grammars,&amp;quot; where John Carroll and David Weir,
using lexicalized tree adjoining grammar (LTAG) as an example, analyze the problem
of providing practically useful estimates of the large number of parameters that are
potentially present in lexicalized grammars. Similarly, in &amp;quot;Towards a reduced com-
mitment, D-theory style TAG parser,&amp;quot; John Chen and K. Vijay-Shanker describe an
approach to TAG parsing whose goal is to delay attachment decisions. This is a de-
sign sketch, not an implemented parser, but the design is well fleshed out, and looks
worth testing.
Several articles do have extensive evaluation data. Joshua Goodman contributes
&amp;quot;Probabilistic feature grammars,&amp;quot; developing an implemented and efficient stochastic
feature-based grammar formalism The key idea, prefigured in, for example, Stol-
cke&apos;s (1994) doctoral dissertation, is to choose a feature formalism that does not im-
pede dynamic programming implementations of the usual inside, outside, and Viterbi
probability calculations. Goodman includes extensive quantitative evaluation, which
is greatly to be welcomed. &amp;quot;A new parsing method using a global association table&amp;quot;
by Juntae Yoon, Seonho Kim, and Mansuk Song, is a description and evaluation of
a semi-deterministic parsing algorithm designed to exploit the fact that Korean is
an SOV language with many surface cues to syntactic dependency. Extensive eval-
uation is provided. Bangalore Srinavas&apos;s &amp;quot;Performance evaluation of SuperTagging
for partial parsing&amp;quot; exploits the author&apos;s SuperTagging idea (i.e., employing part-of-
speech—tagger technology to &amp;quot;almost parse,&amp;quot; using the elementary trees of lexicalized
tree adjoining grammar) for the now-standard task of partial parsing. Given the title,
the plethora of interesting performance figures is to be expected. For example, con-
necting to the discussion of the Penn Treebank above, Bangalore reports that 35% of
the sentences tested have no dependency-link errors, while 89.8% have three errors or
less.
Two papers give evaluations that are based on the measurement of run-time behav-
ior. In &amp;quot;Parsing by successive approximation,&amp;quot; Helmut Schmid describes an efficient
parsing technology that is nonetheless able to process grammars that make significant
use of features. The efficiency of this algorithm is demonstrated by appeal to a range
of empirical performance statistics. Udo Hahn, Norbert Broker, and Peter Neuhaus
</bodyText>
<page confidence="0.997373">
460
</page>
<subsectionHeader confidence="0.851088">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.9999674375">
take a similar approach to evaluation. Their contribution describes &amp;quot;Message-passing
protocols for object-oriented parsing,&amp;quot; and shows how to derive different heuristi-
cally guided parsing algorithms from variations in the communication patterns in an
object-oriented parser. They report a variety of performance statistics for a set of 41
challenging-looking sentences from German computer magazines.
Since a version of the material of the book has already been presented at a work-
shop with proceedings (Bunt and Nijholt 1997), it is relevant to ask what has been
gained (or lost) in the transition to (an expensive) book form. The articles average
20 pages—longer than the original conference presentation—and several authors have
made good use of the opportunity to update and revise their work. The editors have
selected an interesting group of papers, and provide a clear introduction with useful
summaries of the chapters, pointing out some interesting relationships between the
different lines of research.&apos; On the other hand, despite the high price of the book,
there is no evidence that a competent professional copy editor was involved in the
process of publication. This is a shame, since several of the contributions (especially
Hektoen&apos;s) deserve to be more widely known.
</bodyText>
<sectionHeader confidence="0.901932" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.990425720930233">
Bunt, Harry and Anton Nijholt. 1997.
Proceedings of the Fifth International Workshop
on Parsing Technologies. Massachusetts
Institute of Technology, Boston, MA.
Carroll, John, Ted Briscoe, and Antonio
Sanfilippo. 1998. Parser evaluation: a
survey and a new proposal. In Proceedings
of the First International Conference on
Language Resources and Evaluation,
pages 447-454, Granada, Spain.
Carroll, John, Guido Minnen, and Ted
Briscoe. 1999. Corpus annotation for
parser evaluation. In Proceedings of the
EACL-99 Post-Conference Workshop on
Linguistically Interpreted Corpora (LINC-99),
pages 35-41, Bergen, Norway.
Grishman, Ralph, Catherine Macleod,
and J. Sterling. 1992. Evaluating parsing
strategies using standardized parse files.
In Proceedings of the Third Conference on
Applied Natural Language Processing,
pages 156-161, Trento, Italy.
Nederhof, Mark-Jan. 2000. Practical
experiments with regular approximation
of context-free languages. Computational
Linguistics, 26(1): 17-44, March.
Sornlertlamvanich, Virach, Kentaro Inui,
Takenobu Tokunaga, Hozumi Tanaka, and
Toshiyuki Takezawa. 1999. Empirical
support for new probabilistic generalized
LR parsing. Journal of Natural Language
Processing, 6(2): 3-22.
Stolcke, Andreas. 1994. Bayesian Learning of
Probabilistic Language Models. Ph.D. thesis,
University of California at Berkeley.
Chris Brew is an assistant professor of computational linguistics and language technology at the
Ohio State University. His recent research has concerned the use of corpus-based methods in
psycholinguistics and in natural language generation. Brew&apos;s address is: Department of Linguis-
tics, Oxley Hall, 1712 Neil Avenue, Columbus, OH 43210; e-mail: cbrew@ling.ohio-state.edu.
1 In some cases, Bunt and Nijholt seem to be going out of their way to convince themselves that
essentially symbolic work is founded on a probabilistic approach. The papers by Chen and
Vijay-Shanker, and by Hahn, Broker and Neuhaus, in spite of the editorial claim that they fall under
&amp;quot;the development of strategies for efficient probabilistic parsing&amp;quot;, do not go into detail on this issue.
</reference>
<page confidence="0.999314">
461
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.054218">
<title confidence="0.9965815">Book Reviews Advances in Probabilistic and Other Parsing Technologies</title>
<author confidence="0.992117">Harry Bunt</author>
<author confidence="0.992117">Anton Nijholt</author>
<affiliation confidence="0.9517185">(Tilburg University and University of Twente) Dordrecht: Kluwer Academic</affiliation>
<note confidence="0.56049">Publishers (Text, speech and language technology series, edited by Nancy Ide and Jean Veronis, volume 16), 2000, xv+267 pp; hardbound, ISBN 0-7923-6616-6, $112.00, £71.00, Dfl 230.00 Reviewed by</note>
<author confidence="0.995887">Chris Brew</author>
<affiliation confidence="0.975085">The Ohio State University</affiliation>
<note confidence="0.5267855">This book is an edited selection of papers presented at the Fifth International Workshop on Parsing Technologies, held at MIT in September 1997. Several of the papers are</note>
<abstract confidence="0.993804510416666">already well-known and others should be. The book could easily be used as the basis for a graduate-level advanced course on parsing. The title is unwieldy, but appropriate: most but not all of the papers have a strong probabilistic flavor. My favorite papers are Erik Hektoen on &amp;quot;Probabilistic parse selection based on semantic co-occurrences,&amp;quot; Jason Eisner on &amp;quot;Bilexical grammars and their cubic-time parsing algorithms,&amp;quot; and Chris Manning and Bob Carpenter on &amp;quot;Probabilistic parsing using left corner language models.&amp;quot; I like these papers because they step back from the details of parsing technology and consider its wider significance. Manning and Carpenter offer both detail and overview. They provide a series of probabilistic models that relax the context-freeness assumption of probabilistic contextfree grammars, measure performance in the usual way, draw appropriate conclusions, then provide the kicker in the form of a brief section explaining &amp;quot;Why parsing the Penn Treebank is easy.&amp;quot; As Manning and Carpenter point out, in the particular case of the Penn Treebank, the currently accepted PARSEVAL metrics (Grishman, Macleod, and Sterling 1992) are actually quite easy to do well on, even if the system makes systematic errors on such things as prepositional-phrase attachment. If systems are to be deployed into situations where such deficiencies might matter, it might be necessary to find more appropriate evaluation methods. This issue has subsequently been addressed by others (Carroll, Briscoe, and Sanfilippo 1998; Carroll, Minnen, and Briscoe 1999), who argue for more obviously task-related evaluation schemes involving predicate argument structure and/or dependency information. Hektoen&apos;s contribution is in the same vein; it takes seriously the notion that parsing is often simply a device for getting at an underlying semantics. Under his scheme, parse selection relies on the ability to collect statistics over semantic forms. Following path leads Hektoen into a careful exposition of a to parse selection, which appears to be &amp;quot;a sufficient response to the high degree of sparseness in the lexical co-occurrence data without the blurring associated with smoothing and clustering&amp;quot; (p. 162). Hektoen&apos;s approach appears to work well; of course, it does require a broad-coverage parser capable of generating semantic representations, which may be an obstacle for many. The exposition of the method is very clear and the comparison with previous approaches is enlightening. 459 Computational Linguistics Volume 27, Number 3 Mark-Jan Nederhof&apos;s &amp;quot;Regular approximation of CFLs: A grammatical view&amp;quot; is similar to Eisner&apos;s contribution in that its focus is primarily mathematical. It describes an attractive approach to finite-state approximation of regular grammars. The essential idea is to characterize properties that make grammars non-regular, and to develop schemes for systematically removing such properties. This helps to keep the approximation process perspicuous. Experimental work with this approximation scheme is absent from the current article, but is reported elsewhere (Nederhof 2000). In &amp;quot;Probabilistic GLR parsing,&amp;quot; Kentaro Inui, Virach Sornlertlamvanich, Hozumi Tanaka, and Takenobu Tokunaga provide a careful analysis of the process of LR parsing. This leads to a probabilistic parsing scheme having the desirable property, not previously achieved for LR parsers, that the sum over all parses of the probability is unity. Once again experimental work is not present here but is reported elsewhere (Sornlertlamvanich, Inui, Tokunaga, Tanaku, and Takezawa 1999). Eisner&apos;s paper does not report experiments either, but addresses a problem with profound practical significance. It analyses the computational properties of grammars in which potentially idiosyncratic word-to-word relationships play a key role. The framework used is general enough to capture the essence of many recent statistical parsers and clean enough to make it easy (and interesting) to compare one with another. I like Eisner&apos;s paper for the insight it provides into the options available to the lexically minded probabilistic modeler. This aspect is also present in &amp;quot;Encoding frequency information in lexicalized grammars,&amp;quot; where John Carroll and David Weir, using lexicalized tree adjoining grammar (LTAG) as an example, analyze the problem of providing practically useful estimates of the large number of parameters that are potentially present in lexicalized grammars. Similarly, in &amp;quot;Towards a reduced commitment, D-theory style TAG parser,&amp;quot; John Chen and K. Vijay-Shanker describe an approach to TAG parsing whose goal is to delay attachment decisions. This is a design sketch, not an implemented parser, but the design is well fleshed out, and looks worth testing. Several articles do have extensive evaluation data. Joshua Goodman contributes &amp;quot;Probabilistic feature grammars,&amp;quot; developing an implemented and efficient stochastic feature-based grammar formalism The key idea, prefigured in, for example, Stolcke&apos;s (1994) doctoral dissertation, is to choose a feature formalism that does not impede dynamic programming implementations of the usual inside, outside, and Viterbi probability calculations. Goodman includes extensive quantitative evaluation, which is greatly to be welcomed. &amp;quot;A new parsing method using a global association table&amp;quot; by Juntae Yoon, Seonho Kim, and Mansuk Song, is a description and evaluation of a semi-deterministic parsing algorithm designed to exploit the fact that Korean is an SOV language with many surface cues to syntactic dependency. Extensive evaluation is provided. Bangalore Srinavas&apos;s &amp;quot;Performance evaluation of SuperTagging for partial parsing&amp;quot; exploits the author&apos;s SuperTagging idea (i.e., employing part-ofspeech—tagger technology to &amp;quot;almost parse,&amp;quot; using the elementary trees of lexicalized tree adjoining grammar) for the now-standard task of partial parsing. Given the title, the plethora of interesting performance figures is to be expected. For example, connecting to the discussion of the Penn Treebank above, Bangalore reports that 35% of the sentences tested have no dependency-link errors, while 89.8% have three errors or less. Two papers give evaluations that are based on the measurement of run-time behavior. In &amp;quot;Parsing by successive approximation,&amp;quot; Helmut Schmid describes an efficient parsing technology that is nonetheless able to process grammars that make significant use of features. The efficiency of this algorithm is demonstrated by appeal to a range empirical performance statistics. Udo Hahn, Norbert and Peter Neuhaus 460 Book Reviews take a similar approach to evaluation. Their contribution describes &amp;quot;Message-passing protocols for object-oriented parsing,&amp;quot; and shows how to derive different heuristically guided parsing algorithms from variations in the communication patterns in an object-oriented parser. They report a variety of performance statistics for a set of 41 challenging-looking sentences from German computer magazines. Since a version of the material of the book has already been presented at a workshop with proceedings (Bunt and Nijholt 1997), it is relevant to ask what has been gained (or lost) in the transition to (an expensive) book form. The articles average 20 pages—longer than the original conference presentation—and several authors have made good use of the opportunity to update and revise their work. The editors have</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Harry Bunt</author>
<author>Anton Nijholt</author>
</authors>
<date>1997</date>
<booktitle>Proceedings of the Fifth International Workshop on Parsing Technologies. Massachusetts Institute of Technology,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="7772" citStr="Bunt and Nijholt 1997" startWordPosition="1140" endWordPosition="1143"> empirical performance statistics. Udo Hahn, Norbert Broker, and Peter Neuhaus 460 Book Reviews take a similar approach to evaluation. Their contribution describes &amp;quot;Message-passing protocols for object-oriented parsing,&amp;quot; and shows how to derive different heuristically guided parsing algorithms from variations in the communication patterns in an object-oriented parser. They report a variety of performance statistics for a set of 41 challenging-looking sentences from German computer magazines. Since a version of the material of the book has already been presented at a workshop with proceedings (Bunt and Nijholt 1997), it is relevant to ask what has been gained (or lost) in the transition to (an expensive) book form. The articles average 20 pages—longer than the original conference presentation—and several authors have made good use of the opportunity to update and revise their work. The editors have selected an interesting group of papers, and provide a clear introduction with useful summaries of the chapters, pointing out some interesting relationships between the different lines of research.&apos; On the other hand, despite the high price of the book, there is no evidence that a competent professional copy e</context>
</contexts>
<marker>Bunt, Nijholt, 1997</marker>
<rawString>Bunt, Harry and Anton Nijholt. 1997. Proceedings of the Fifth International Workshop on Parsing Technologies. Massachusetts Institute of Technology, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ted Briscoe</author>
<author>Antonio Sanfilippo</author>
</authors>
<title>Parser evaluation: a survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of the First International Conference on Language Resources and Evaluation,</booktitle>
<pages>447--454</pages>
<location>Granada,</location>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>Carroll, John, Ted Briscoe, and Antonio Sanfilippo. 1998. Parser evaluation: a survey and a new proposal. In Proceedings of the First International Conference on Language Resources and Evaluation, pages 447-454, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Ted Briscoe</author>
</authors>
<title>Corpus annotation for parser evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of the EACL-99 Post-Conference Workshop on Linguistically Interpreted Corpora (LINC-99),</booktitle>
<pages>35--41</pages>
<location>Bergen,</location>
<marker>Carroll, Minnen, Briscoe, 1999</marker>
<rawString>Carroll, John, Guido Minnen, and Ted Briscoe. 1999. Corpus annotation for parser evaluation. In Proceedings of the EACL-99 Post-Conference Workshop on Linguistically Interpreted Corpora (LINC-99), pages 35-41, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Catherine Macleod</author>
<author>J Sterling</author>
</authors>
<title>Evaluating parsing strategies using standardized parse files.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<pages>156--161</pages>
<location>Trento, Italy.</location>
<marker>Grishman, Macleod, Sterling, 1992</marker>
<rawString>Grishman, Ralph, Catherine Macleod, and J. Sterling. 1992. Evaluating parsing strategies using standardized parse files. In Proceedings of the Third Conference on Applied Natural Language Processing, pages 156-161, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Practical experiments with regular approximation of context-free languages.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<pages>17--44</pages>
<contexts>
<context position="3736" citStr="Nederhof 2000" startWordPosition="551" endWordPosition="552">tational Linguistics Volume 27, Number 3 Mark-Jan Nederhof&apos;s &amp;quot;Regular approximation of CFLs: A grammatical view&amp;quot; is similar to Eisner&apos;s contribution in that its focus is primarily mathematical. It describes an attractive approach to finite-state approximation of regular grammars. The essential idea is to characterize properties that make grammars non-regular, and to develop schemes for systematically removing such properties. This helps to keep the approximation process perspicuous. Experimental work with this approximation scheme is absent from the current article, but is reported elsewhere (Nederhof 2000). In &amp;quot;Probabilistic GLR parsing,&amp;quot; Kentaro Inui, Virach Sornlertlamvanich, Hozumi Tanaka, and Takenobu Tokunaga provide a careful analysis of the process of LR parsing. This leads to a probabilistic parsing scheme having the desirable property, not previously achieved for LR parsers, that the sum over all parses of the probability is unity. Once again experimental work is not present here but is reported elsewhere (Sornlertlamvanich, Inui, Tokunaga, Tanaku, and Takezawa 1999). Eisner&apos;s paper does not report experiments either, but addresses a problem with profound practical significance. It ana</context>
</contexts>
<marker>Nederhof, 2000</marker>
<rawString>Nederhof, Mark-Jan. 2000. Practical experiments with regular approximation of context-free languages. Computational Linguistics, 26(1): 17-44, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Virach Sornlertlamvanich</author>
</authors>
<title>Kentaro Inui, Takenobu Tokunaga, Hozumi Tanaka, and Toshiyuki Takezawa.</title>
<date>1999</date>
<journal>Journal of Natural Language Processing,</journal>
<volume>6</volume>
<issue>2</issue>
<pages>3--22</pages>
<marker>Sornlertlamvanich, 1999</marker>
<rawString>Sornlertlamvanich, Virach, Kentaro Inui, Takenobu Tokunaga, Hozumi Tanaka, and Toshiyuki Takezawa. 1999. Empirical support for new probabilistic generalized LR parsing. Journal of Natural Language Processing, 6(2): 3-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Bayesian Learning of Probabilistic Language Models.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California at Berkeley.</institution>
<marker>Stolcke, 1994</marker>
<rawString>Stolcke, Andreas. 1994. Bayesian Learning of Probabilistic Language Models. Ph.D. thesis, University of California at Berkeley.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Chris Brew</author>
</authors>
<title>is an assistant professor of computational linguistics and language technology at the Ohio State University. His recent research has concerned the use of corpus-based methods in psycholinguistics and in natural language generation. Brew&apos;s address is: Department of Linguistics, Oxley Hall, 1712 Neil Avenue, Columbus, OH 43210; e-mail: cbrew@ling.ohio-state.edu.</title>
<marker>Brew, </marker>
<rawString>Chris Brew is an assistant professor of computational linguistics and language technology at the Ohio State University. His recent research has concerned the use of corpus-based methods in psycholinguistics and in natural language generation. Brew&apos;s address is: Department of Linguistics, Oxley Hall, 1712 Neil Avenue, Columbus, OH 43210; e-mail: cbrew@ling.ohio-state.edu.</rawString>
</citation>
<citation valid="false">
<title>1 In some cases, Bunt and Nijholt seem to be going out of their way to convince themselves that essentially symbolic work is founded on a probabilistic approach. The papers by Chen and Vijay-Shanker, and by Hahn, Broker and Neuhaus, in spite of the editorial claim that they fall under &amp;quot;the development of strategies for efficient probabilistic parsing&amp;quot;, do not go into detail on this issue.</title>
<marker></marker>
<rawString>1 In some cases, Bunt and Nijholt seem to be going out of their way to convince themselves that essentially symbolic work is founded on a probabilistic approach. The papers by Chen and Vijay-Shanker, and by Hahn, Broker and Neuhaus, in spite of the editorial claim that they fall under &amp;quot;the development of strategies for efficient probabilistic parsing&amp;quot;, do not go into detail on this issue.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>