<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021397">
<title confidence="0.998879">
A Semantic Approach to IE Pattern Induction
</title>
<author confidence="0.998503">
Mark Stevenson and Mark A. Greenwood
</author>
<affiliation confidence="0.997617">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.983455">
Sheffield, S1 4DP, UK
</address>
<email confidence="0.999361">
marks,m.greenwood@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999816615384615">
This paper presents a novel algorithm for
the acquisition of Information Extraction
patterns. The approach makes the assump-
tion that useful patterns will have simi-
lar meanings to those already identified
as relevant. Patterns are compared using
a variation of the standard vector space
model in which information from an on-
tology is used to capture semantic sim-
ilarity. Evaluation shows this algorithm
performs well when compared with a
previously reported document-centric ap-
proach.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997726">
Developing systems which can be easily adapted to
new domains with the minimum of human interven-
tion is a major challenge in Information Extraction
(IE). Early IE systems were based on knowledge en-
gineering approaches but suffered from a knowledge
acquisition bottleneck. For example, Lehnert et al.
(1992) reported that their system required around
1,500 person-hours of expert labour to modify for
a new extraction task. One approach to this problem
is to use machine learning to automatically learn the
domain-specific information required to port a sys-
tem (Riloff, 1996). Yangarber et al. (2000) proposed
an algorithm for learning extraction patterns for a
small number of examples which greatly reduced the
burden on the application developer and reduced the
knowledge acquisition bottleneck.
Weakly supervised algorithms, which bootstrap
from a small number of examples, have the advan-
tage of requiring only small amounts of annotated
data, which is often difficult and time-consuming
to produce. However, this also means that there
are fewer examples of the patterns to be learned,
making the learning task more challenging. Pro-
viding the learning algorithm with access to addi-
tional knowledge can compensate for the limited
number of annotated examples. This paper presents
a novel weakly supervised algorithm for IE pattern
induction which makes use of the WordNet ontology
(Fellbaum, 1998).
Extraction patterns are potentially useful for many
language processing tasks, including question an-
swering and the identification of lexical relations
(such as meronomy and hyponymy). In addition, IE
patterns encode the different ways in which a piece
of information can be expressed in text. For exam-
ple, “Acme Inc. fired Jones”, “Acme Inc. let Jones
go”, and “Jones was given notice by his employers,
Acme Inc.” are all ways of expressing the same fact.
Consequently the generation of extraction patterns is
pertinent to paraphrase identification which is cen-
tral to many language processing problems.
We begin by describing the general process of pat-
tern induction and an existing approach, based on
the distribution of patterns in a corpus (Section 2).
We then introduce a new algorithm which makes use
of WordNet to generalise extraction patterns (Sec-
tion 3) and describe an implementation (Section 4).
Two evaluation regimes are described; one based on
the identification of relevant documents and another
which aims to identify sentences in a corpus which
</bodyText>
<page confidence="0.98604">
379
</page>
<note confidence="0.991633">
Proceedings of the 43rd Annual Meeting of the ACL, pages 379–386,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.996229666666667">
are relevant for a particular IE task (Section 5). Re-
sults on each of these evaluation regimes are then
presented (Sections 6 and 7).
</bodyText>
<sectionHeader confidence="0.908" genericHeader="method">
2 Extraction Pattern Learning
</sectionHeader>
<bodyText confidence="0.99901">
We begin by outlining the general process of learn-
ing extraction patterns, similar to one presented by
(Yangarber, 2003).
</bodyText>
<listItem confidence="0.994619045454545">
1. For a given IE scenario we assume the exis-
tence of a set of documents against which the
system can be trained. The documents are
unannotated and may be either relevant (con-
tain the description of an event relevant to the
scenario) or irrelevant although the algorithm
has no access to this information.
2. This corpus is pre-processed to generate the set
of all patterns which could be used to represent
sentences contained in the corpus, call this set
S. The aim of the learning process is to identify
the subset of S representing patterns which are
relevant to the IE scenario.
3. The user provides a small set of seed patterns,
Sseed, which are relevant to the scenario. These
patterns are used to form the set of currently
accepted patterns, Sacc, so Sacc +— Sseed. The
remaining patterns are treated as candidates for
inclusion in the accepted set, these form the set
Scand(= S − Sacc).
4. A function, f, is used to assign a score to
each pattern in Scand based on those which
</listItem>
<bodyText confidence="0.816919875">
are currently in Sacc. This function as-
signs a real number to candidate patterns so
V c E Scand, f(c, Sacc) H R. A set of high
scoring patterns (based on absolute scores or
ranks after the set of patterns has been ordered
by scores) are chosen as being suitable for in-
clusion in the set of accepted patterns. These
form the set Slearn.
</bodyText>
<listItem confidence="0.998615">
5. The patterns in Slearn are added to Sacc and
removed from Scand, so Sacc +— Sacc U Slearn
and Scand +— Sacc − Slearn
6. If a suitable set of patterns has been learned
then stop, otherwise go to step 4
</listItem>
<subsectionHeader confidence="0.961838">
2.1 Document-centric approach
</subsectionHeader>
<bodyText confidence="0.99993118">
A key choice in the development of such an algo-
rithm is step 4, the process of ranking the candidate
patterns, which effectively determines which of the
candidate patterns will be learned. Yangarber et al.
(2000) chose an approach motivated by the assump-
tion that documents containing a large number of
patterns already identified as relevant to a particu-
lar IE scenario are likely to contain further relevant
patterns. This approach, which can be viewed as be-
ing document-centric, operates by associating confi-
dence scores with patterns and relevance scores with
documents. Initially seed patterns are given a maxi-
mum confidence score of 1 and all others a 0 score.
Each document is given a relevance score based on
the patterns which occur within it. Candidate pat-
terns are ranked according to the proportion of rele-
vant and irrelevant documents in which they occur,
those found in relevant documents far more than in
irrelevant ones are ranked highly. After new patterns
have been accepted all patterns’ confidence scores
are updated, based on the documents in which they
occur, and documents’ relevance according to the
accepted patterns they contain.
This approach has been shown to successfully ac-
quire useful extraction patterns which, when added
to an IE system, improved its performance (Yangar-
ber et al., 2000). However, it relies on an assump-
tion about the way in which relevant patterns are dis-
tributed in a document collection and may learn pat-
terns which tend to occur in the same documents as
relevant ones whether or not they are actually rele-
vant. For example, we could imagine an IE scenario
in which relevant documents contain a piece of in-
formation which is related to, but distinct from, the
information we aim to extract. If patterns expressing
this information were more likely to occur in rele-
vant documents than irrelevant ones the document-
centric approach would also learn the irrelevant pat-
terns.
Rather than focusing on the documents matched
by a pattern, an alternative approach is to rank pat-
terns according to how similar their meanings are
to those which are known to be relevant. This
semantic-similarity approach avoids the problem
which may be present in the document-centric ap-
proach since patterns which happen to co-occur in
the same documents as relevant ones but have dif-
ferent meanings will not be ranked highly. We now
go on to describe a new algorithm which implements
this approach.
</bodyText>
<page confidence="0.998318">
380
</page>
<sectionHeader confidence="0.991738" genericHeader="method">
3 Semantic IE Pattern Learning
</sectionHeader>
<bodyText confidence="0.999974951219512">
For these experiments extraction patterns consist of
predicate-argument structures, as proposed by Yan-
garber (2003). Under this scheme patterns consist
of triples representing the subject, verb and object
(SVO) of a clause. The first element is the “se-
mantic” subject (or agent), for example “John” is a
clausal subject in each of these sentences “John hit
Bill”, “Bill was hit by John”, “Mary saw John hit
Bill”, and “John is a bully”. The second element is
the verb in the clause and the third the object (pa-
tient) or predicate. “Bill” is a clausal object in the
first three example sentences and “bully” in the final
one. When a verb is being used intransitively, the
pattern for that clause is restricted to only the first
pair of elements.
The filler of each pattern element can be either
a lexical item or semantic category such as per-
son name, country, currency values, numerical ex-
pressions etc. In this paper lexical items are rep-
resented in lower case and semantic categories are
capitalised. For example, in the pattern COM-
PANY+fired+ceo, fired and ceo are lexical
items and COMPANY a semantic category which
could match any lexical item belonging to that type.
The algorithm described here relies on identify-
ing patterns with similar meanings. The approach
we have developed to do this is inspired by the
vector space model which is commonly used in
Information Retrieval (Salton and McGill, 1983)
and language processing in general (Pado and La-
pata, 2003). Each pattern can be represented as
a set of pattern element-filler pairs. For exam-
ple, the pattern COMPANY+fired+ceo consists
of three pairs: subject COMPANY, verb fired
and object ceo. Each pair consists of either a
lexical item or semantic category, and pattern ele-
ment. Once an appropriate set of pairs has been es-
tablished a pattern can be represented as a binary
vector in which an element with value 1 denotes that
the pattern contains a particular pair and 0 that it
does not.
</bodyText>
<subsectionHeader confidence="0.998728">
3.1 Pattern Similarity
</subsectionHeader>
<bodyText confidence="0.998312333333333">
The similarity of two pattern vectors can be com-
pared using the measure shown in Equation 1. Here
d andb are pattern vectors, bT � the transpose ofb and
</bodyText>
<figureCaption confidence="0.62406525">
Figure 1: Similarity scores and matrix for an exam-
ple vector space formed from three patterns
W a matrix that lists the similarity between each of
the possible pattern element-filler pairs.
</figureCaption>
<equation confidence="0.905594">
sim(d,b) = |a||�b|
</equation>
<bodyText confidence="0.999938428571428">
The semantic similarity matrix W contains infor-
mation about the similarity of each pattern element-
filler pair stored as non-negative real numbers and is
crucial for this measure. Assume that the set of pat-
terns, P, consists of n element-filler pairs denoted
by p1, p2, ...pn. Each row and column of W rep-
resents one of these pairs and they are consistently
labelled. So, for any i such that 1 ≤ i ≤ n, row i and
column i are both labelled with pair pi. If wij is the
element of W in row i and column j then the value
of wij represents the similarity between the pairs pi
and pj. Note that we assume the similarity of two
element-filler pairs is symmetric, so wij = wji and,
consequently, W is a symmetric matrix. Pairs with
different pattern elements (i.e. grammatical roles)
are automatically given a similarity score of 0. Di-
agonal elements of W represent the self-similarity
between pairs and have the greatest values.
Figure 1 shows an example using three patterns,
chairman+resign, ceo+quit and chair-
man+comment. This shows how these patterns are
represented as vectors and gives a sample semantic
similarity matrix. It can be seen that the first pair
of patterns are the most similar using the proposed
measure.
The measure in Equation 1 is similar to the cosine
metric, commonly used to determine the similarity
of documents in the vector space model approach
</bodyText>
<figure confidence="0.994463428571429">
Patterns Matrix labels
a.chairman+resign 1. subject chairman
b. ceo+quit 2. subject ceo
c. chairman+comment 3. verb resign
4. verb quit
5. verb comment
1 Similarity matrix 0 Similarity values
0.95 0.95 0 0 0 sim(d, b) = 0.925
1 0 0
0 0 1 0.9 0.1 sim(d, d) = 0.55
0 0 0.9 1 0.1 sim(b, ) = 0.525
0 0 0.1 0.1 1
aWbT �
(1)
</figure>
<page confidence="0.990888">
381
</page>
<bodyText confidence="0.9998796">
to Information Retrieval. However, the cosine met-
ric will not perform well for our application since it
does not take into account the similarity between el-
ements of a vector and would assign equal similarity
to each pair of patterns in the example shown in Fig-
ure 1.1 The semantic similarity matrix in Equation 1
provides a mechanism to capture semantic similar-
ity between lexical items which allows us to identify
chairman+resign and ceo+quit as the most
similar pair of patterns.
</bodyText>
<subsectionHeader confidence="0.999879">
3.2 Populating the Matrix
</subsectionHeader>
<bodyText confidence="0.990122828571429">
It is important to choose appropriate values for the
elements of W. We chose to make use of the re-
search that has concentrated on computing similar-
ity between pairs of lexical items using the WordNet
hierarchy (Resnik, 1995; Jiang and Conrath, 1997;
Patwardhan et al., 2003). We experimented with
several of the measures which have been reported
in the literature and found that the one proposed by
Jiang and Conrath (1997) to be the most effective.
The similarity measure proposed by Jiang and
Conrath (1997) relies on a technique developed by
Resnik (1995) which assigns numerical values to
each sense in the WordNet hierarchy based upon
the amount of information it represents. These val-
ues are derived from corpus counts of the words in
the synset, either directly or via the hyponym rela-
tion and are used to derive the Information Content
(IC) of a synset c thus IC(c) = − log(Pr(c)). For
two senses, s1 and s2, the lowest common subsumer,
lcs(s1, s2), is defined as the sense with the highest
information content (most specific) which subsumes
both senses in the WordNet hierarchy. Jiang and
Conrath used these elements to calculate the seman-
tic distance between a pair or words, w1 and w2, ac-
cording to this formula (where senses(w) is the set
&apos;The cosine metric for a pair of vectors is given by the cal-
culation ���
s�ss�s . Substituting the matrix multiplication in the nu-
merator of Equation 1 for the dot product of vectors a and b
would give the cosine metric. Note that taking the dot product
of a pair of vectors is equivalent to multiplying by the identity
matrix, i.e. a.b = aICT. Under our interpretation of the simi-
larity matrix, W, this equates to each pattern element-filler pair
being identical to itself but not similar to anything else.
of all possible WordNet senses for word w):
</bodyText>
<equation confidence="0.887608">
ARGMAX
s1 a senses(w1), IC(s1)+IC(s2)−2xIC(lcs(s1, s2))
s2 E senses(w2)
(2)
</equation>
<bodyText confidence="0.998301736842105">
Patwardhan et al. (2003) convert this distance
metric into a similarity measure by taking its mul-
tiplicative inverse. Their implementation was used
in the experiments described later.
As mentioned above, the second part of a pattern
element-filler pair can be either a lexical item or a
semantic category, such as company. The identifiers
used to denote these categories, i.e. COMPANY, do
not appear in WordNet and so it is not possible to
directly compare their similarity with other lexical
items. To avoid this problem these tokens are man-
ually mapped onto the most appropriate node in the
WordNet hierarchy which is then used for similar-
ity calculations. This mapping process is not partic-
ularly time-consuming since the number of named
entity types with which a corpus is annotated is usu-
ally quite small. For example, in the experiments
described in this paper just seven semantic classes
were sufficient to annotate the corpus.
</bodyText>
<subsectionHeader confidence="0.999001">
3.3 Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.999884857142857">
This pattern similarity measure can be used to create
a weakly supervised approach to pattern acquisition
following the general outline provided in Section 2.
Each candidate pattern is compared against the set
of currently accepted patterns using the measure de-
scribed in Section 3.1. We experimented with sev-
eral techniques for ranking candidate patterns based
on these scores, including using the best and aver-
age score, and found that the best results were ob-
tained when each candidate pattern was ranked ac-
cording to its score when compared against the cen-
troid vector of the set of currently accepted patterns.
We also experimented with several schemes for de-
ciding which of the scored patterns to accept (a full
description would be too long for this paper) result-
ing in a scheme where the four highest scoring pat-
terns whose score is within 0.95 of the best pattern
are accepted.
Our algorithm disregards any patterns whose cor-
pus occurrences are below a set threshold, α, since
these may be due to noise. In addition, a second
</bodyText>
<page confidence="0.992201">
382
</page>
<bodyText confidence="0.999433428571428">
threshold, Q, is used to determine the maximum
number of documents in which a pattern can occur
since these very frequent patterns are often too gen-
eral to be useful for IE. Patterns which occur in more
than Q x C, where C is the number of documents in
the collection, are not learned. For the experiments
in this paper we set α to 2 and Q to 0.3.
</bodyText>
<sectionHeader confidence="0.998101" genericHeader="method">
4 Implementation
</sectionHeader>
<bodyText confidence="0.999995517241379">
A number of pre-processing stages have to be ap-
plied to documents in order for the set of patterns to
be extracted before learning can take place. Firstly,
items belonging to semantic categories are identi-
fied by running the text through the named entity
identifier in the GATE system (Cunningham et al.,
2002). The corpus is then parsed, using a ver-
sion of MINIPAR (Lin, 1999) adapted to process
text marked with named entities, to produce depen-
dency trees from which SVO-patterns are extracted.
Active and passive voice is taken into account in
MINIPAR’s output so the sentences “COMPANY
fired their C.E.O.” and “The C.E.O. was fired by
COMPANY” would yield the same triple, COM-
PANY+fire+ceo. The indirect object of ditran-
sitive verbs is not extracted; these verbs are treated
like transitive verbs for the purposes of this analysis.
An implementation of the algorithm described
in Section 3 was completed in addition to an im-
plementation of the document-centric algorithm de-
scribed in Section 2.1. It is important to mention
that this implementation is not identical to the one
described by Yangarber et al. (2000). Their system
makes some generalisations across pattern elements
by grouping certain elements together. However,
there is no difference between the expressiveness of
the patterns learned by either approach and we do
not believe this difference has any effect on the re-
sults of our experiments.
</bodyText>
<sectionHeader confidence="0.998641" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9999805">
Various approaches have been suggested for the
evaluation of automatic IE pattern acquisition.
Riloff (1996) judged the precision of patterns
learned by reviewing them manually. Yangarber et
al. (2000) developed an indirect method which al-
lowed automatic evaluation. In addition to learning
a set of patterns, their system also notes the rele-
vance of documents based on the current set of ac-
cepted patterns. Assuming the subset of documents
relevant to a particular IE scenario is known, it is
possible to use these relevance judgements to de-
termine how accurately a given set of patterns can
discriminate the relevant documents from the irrele-
vant. This evaluation is similar to the “text-filtering”
sub-task used in the sixth Message Understanding
Conference (MUC-6) (1995) in which systems were
evaluated according to their ability to identify the
documents relevant to the extraction task. The doc-
ument filtering evaluation technique was used to al-
low comparison with previous studies.
Identifying the document containing relevant in-
formation can be considered as a preliminary stage
of an IE task. A further step is to identify the sen-
tences within those documents which are relevant.
This “sentence filtering” task is a more fine-grained
evaluation and is likely to provide more information
about how well a given set of patterns is likely to
perform as part of an IE system. Soderland (1999)
developed a version of the MUC-6 corpus in which
events are marked at the sentence level. The set of
patterns learned by the algorithm after each iteration
can be compared against this corpus to determine
how accurately they identify the relevant sentences
for this extraction task.
</bodyText>
<subsectionHeader confidence="0.955975">
5.1 Evaluation Corpus
</subsectionHeader>
<bodyText confidence="0.999996210526316">
The evaluation corpus used for the experiments was
compiled from the training and testing corpus used
in MUC-6, where the task was to extract information
about the movements of executives from newswire
texts. A document is relevant if it has a filled tem-
plate associated with it. 590 documents from a ver-
sion of the MUC-6 evaluation corpus described by
Soderland (1999) were used.
After the pre-processing stages described in Sec-
tion 4, the MUC-6 corpus produced 15,407 pattern
tokens from 11,294 different types. 10,512 patterns
appeared just once and these were effectively dis-
carded since our learning algorithm only considers
patterns which occur at least twice (see Section 3.3).
The document-centric approach benefits from a
large corpus containing a mixture of relevant and ir-
relevant documents. We provided this using a subset
of the Reuters Corpus Volume I (Rose et al., 2002)
which, like the MUC-6 corpus, consists of newswire
</bodyText>
<page confidence="0.996317">
383
</page>
<figure confidence="0.709850571428571">
COMPANY+appoint+PERSON
COMPANY+elect+PERSON
COMPANY+promote+PERSON
COMPANY+name+PERSON
PERSON+resign
PERSON+depart
PERSON+quit
</figure>
<tableCaption confidence="0.996676">
Table 1: Seed patterns for extraction task
</tableCaption>
<bodyText confidence="0.999959066666667">
texts. 3000 documents relevant to the management
succession task (identified using document meta-
data) and 3000 irrelevant documents were used to
produce the supplementary corpus. This supple-
mentary corpus yielded 126,942 pattern tokens and
79,473 types with 14,576 of these appearing more
than once. Adding the supplementary corpus to the
data set used by the document-centric approach led
to an improvement of around 15% on the document
filtering task and over 70% for sentence filtering. It
was not used for the semantic similarity algorithm
since there was no benefit.
The set of seed patterns listed in Table 1 are in-
dicative of the management succession extraction
task and were used for these experiments.
</bodyText>
<sectionHeader confidence="0.999955" genericHeader="evaluation">
6 Results
</sectionHeader>
<subsectionHeader confidence="0.99991">
6.1 Document Filtering
</subsectionHeader>
<bodyText confidence="0.999082142857143">
Results for both the document and sentence filter-
ing experiments are reported in Table 2 which lists
precision, recall and F-measure for each approach
on both evaluations. Results from the document fil-
tering experiment are shown on the left hand side
of the table and continuous F-measure scores for
the same experiment are also presented in graphi-
cal format in Figure 2. While the document-centric
approach achieves the highest F-measure of either
system (0.83 on the 33rd iteration compared against
0.81 after 48 iterations of the semantic similarity ap-
proach) it only outperforms the proposed approach
for a few iterations. In addition the semantic sim-
ilarity approach learns more quickly and does not
exhibit as much of a drop in performance after it has
reached its best value. Overall the semantic sim-
ilarity approach was found to be significantly bet-
ter than the document-centric approach (p &lt; 0.001,
Wilcoxon Signed Ranks Test).
Although it is an informative evaluation, the doc-
ument filtering task is limited for evaluating IE pat-
</bodyText>
<figure confidence="0.994134">
0 20 40 60 80 100 120
Iteration
</figure>
<figureCaption confidence="0.999615">
Figure 2: Evaluating document filtering.
</figureCaption>
<bodyText confidence="0.999928833333333">
tern learning. This evaluation indicates whether the
set of patterns being learned can identify documents
containing descriptions of events but does not pro-
vide any information about whether it can find those
events within the documents. In addition, the set of
seed patterns used for these experiments have a high
precision and low recall (Table 2). We have found
that the distribution of patterns and documents in
the corpus means that learning virtually any pattern
will help improve the F-measure. Consequently, we
believe the sentence filtering evaluation to be more
useful for this problem.
</bodyText>
<subsectionHeader confidence="0.999888">
6.2 Sentence Filtering
</subsectionHeader>
<bodyText confidence="0.999684538461538">
Results from the sentence filtering experiment are
shown in tabular format in the right hand side of
Table 22 and graphically in Figure 3. The seman-
tic similarity algorithm can be seen to outperform
the document-centric approach. This difference is
also significant (p &lt; 0.001, Wilcoxon Signed Ranks
Text).
The clear difference between these results shows
that the semantic similarity approach can indeed
identify relevant sentences while the document-
centric method identifies patterns which match rel-
evant documents, although not necessarily relevant
sentences.
</bodyText>
<tableCaption confidence="0.4651718">
2The set of seed patterns returns a precision of 0.81 for this
task. The precision is not 1 since the pattern PERSON+resign
matches sentences describing historical events (“Jones resigned
last year.”) which were not marked as relevant in this corpus
following MUC guidelines.
</tableCaption>
<figure confidence="0.997620545454545">
Semantic Similarity
Document-centric
F-measure 0.80
0.75
0.70
0.65
0.60
0.55
0.50
0.45
0.40
</figure>
<page confidence="0.992725">
384
</page>
<table confidence="0.999683454545455">
Number of Document Filtering Sentence Filtering
Iterations
Document-centric Semantic similarity Document-centric Semantic similarity
P R F P R F P R F P R F
0 1.00 0.26 0.42 1.00 0.26 0.42 0.81 0.10 0.18 0.81 0.10 0.18
20 0.75 0.68 0.71 0.77 0.78 0.77 0.30 0.29 0.29 0.61 0.49 0.54
40 0.72 0.96 0.82 0.70 0.93 0.80 0.40 0.67 0.51 0.47 0.64 0.55
60 0.65 0.96 0.78 0.68 0.96 0.80 0.32 0.70 0.44 0.42 0.73 0.54
80 0.56 0.96 0.71 0.61 0.98 0.76 0.18 0.71 0.29 0.37 0.89 0.52
100 0.56 0.96 0.71 0.58 0.98 0.73 0.18 0.73 0.28 0.28 0.92 0.42
120 0.56 0.96 0.71 0.58 0.98 0.73 0.17 0.75 0.28 0.26 0.95 0.41
</table>
<tableCaption confidence="0.999838">
Table 2: Comparison of the different approaches over 120 iterations
</tableCaption>
<figure confidence="0.997621">
0 20 40 60 80 100 120
Iteration
</figure>
<figureCaption confidence="0.999936">
Figure 3: Evaluating sentence filtering.
</figureCaption>
<bodyText confidence="0.999893888888889">
The precision scores for the sentence filtering task
in Table 2 show that the semantic similarity al-
gorithm consistently learns more accurate patterns
than the existing approach. At the same time it
learns patterns with high recall much faster than the
document-centric approach, by the 120th iteration
the pattern set covers almost 95% of relevant sen-
tences while the document-centric approach covers
only 75%.
</bodyText>
<sectionHeader confidence="0.99966" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.99996156097561">
The approach to IE pattern acquisition presented
here is related to other techniques but uses differ-
ent assumptions regarding which patterns are likely
to be relevant to a particular extraction task. Eval-
uation has showed that the semantic generalisa-
tion approach presented here performs well when
compared to a previously reported document-centric
method. Differences between the two approaches
are most obvious when the results of the sentence
filtering task are considered and it seems that this is
a more informative evaluation for this problem. The
semantic similarity approach has the additional ad-
vantage of not requiring a large corpus containing a
mixture of documents relevant and irrelevant to the
extraction task. This corpus is unannotated, and so
may not be difficult to obtain, but is nevertheless an
additional requirement.
The best score recorded by the proposed algo-
rithm on the sentence filtering task is an F-measure
of 0.58 (22nd iteration). While this result is lower
than those reported for IE systems based on knowl-
edge engineering approaches these results should be
placed in the context of a weakly supervised learning
algorithm which could be used to complement man-
ual approaches. These results could be improved by
manual filtering the patterns identified by the algo-
rithm.
The learning algorithm presented in Section 3 in-
cludes a mechanism for comparing two extraction
patterns using information about lexical similarity
derived from WordNet. This approach is not re-
stricted to this application and could be applied to
other language processing tasks such as question an-
swering, paraphrase identification and generation or
as a variant of the vector space model commonly
used in Information Retrieval. In addition, Sudo
et al. (2003) proposed representations for IE pat-
terns which extends the SVO representation used
here and, while they did not appear to significantly
improve IE, it is expected that it will be straightfor-
ward to extend the vector space model to those pat-
</bodyText>
<figure confidence="0.998821333333333">
0.55
0.50
0.45
0.30
0.25
0.20
Semantic Similarity
Document-centric
F-measure
0.40
0.35
0.15
</figure>
<page confidence="0.995101">
385
</page>
<bodyText confidence="0.999192888888889">
tern representations.
One of the reasons for the success of the approach
described here is the appropriateness of WordNet
which is constructed on paradigmatic principles,
listing the words which may be substituted for one
another, and is consequently an excellent resource
for this application. WordNet is also a generic
resource not associated with a particular domain
which means the learning algorithm can make use
of that knowledge to acquire patterns for a diverse
range of IE tasks. This work represents a step to-
wards truly domain-independent IE systems. Em-
ploying a weakly supervised learning algorithm re-
moves much of the requirement for a human anno-
tator to provide example patterns. Such approaches
are often hampered by a lack of information but the
additional knowledge in WordNet helps to compen-
sate.
</bodyText>
<sectionHeader confidence="0.996038" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999978">
This work was carried out as part of the RE-
SuLT project funded by the EPSRC (GR/T06391).
Roman Yangarber provided advice on the re-
implementation of the document-centric algorithm.
We are also grateful for the detailed comments pro-
vided by the anonymous reviewers of this paper.
</bodyText>
<sectionHeader confidence="0.999241" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999465869565218">
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: an Architecture for Devel-
opment of Robust HLT. In Proceedings of the 40th
Anniversary Meeting of the Association for Computa-
tional Linguistics (ACL-02), pages 168–175, Philadel-
phia, PA.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and some of its Applications. MIT Press,
Cambridge, MA.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings ofInternational Conference on Research in Com-
putational Linguistics, Taiwan.
W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, E. Riloff,
and S. Soderland. 1992. University of Massachusetts:
Description of the CIRCUS System used for MUC-4.
In Proceedings of the Fourth Message Understanding
Conference (MUC-4), pages 282–288, San Francisco,
CA.
D. Lin. 1999. MINIPAR: a minimalist parser. In Mary-
land Linguistics Colloquium, University of Maryland,
College Park.
MUC. 1995. Proceedings of the Sixth Message Under-
standing Conference (MUC-6), San Mateo, CA. Mor-
gan Kaufmann.
S. Pado and M. Lapata. 2003. Constructing semantic
space models from parsed corpora. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-03), pages 128–135, Sap-
poro, Japan.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Us-
ing measures of semantic relatedness for word sense
disambiguation. In Proceedings of the Fourth Inter-
national Conferences on Intelligent Text Processing
and Computational Linguistics, pages 241–257, Mex-
ico City.
P. Resnik. 1995. Using Information Content to evalu-
ate Semantic Similarity in a Taxonomy. In Proceed-
ings of the 14th International Joint Conference on Ar-
tificial Intelligence (IJCAI-95), pages 448–453, Mon-
treal, Canada.
E. Riloff. 1996. Automatically generating extraction
patterns from untagged text. In Thirteenth National
Conference on Artificial Intelligence (AAAI-96), pages
1044–1049, Portland, OR.
T. Rose, M. Stevenson, and M. Whitehead. 2002. The
Reuters Corpus Volume 1 - from Yesterday’s news to
tomorrow’s language resources. In LREC-02, pages
827–832, La Palmas, Spain.
G. Salton and M. McGill. 1983. Introduction to Modern
Information Retrieval. McGraw-Hill, New York.
S. Soderland. 1999. Learning Information Extraction
Rules for Semi-structured and free text. Machine
Learning, 31(1-3):233–272.
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Pattern Representation Model for
Automatic IE Pattern Acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-03), pages 224–231.
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic acquisition of domain
knowledge for information extraction. In Proceed-
ings of the 18th International Conference on Compu-
tational Linguistics (COLING 2000), pages 940–946,
Saarbr¨ucken, Germany.
R. Yangarber. 2003. Counter-training in the discovery of
semantic patterns. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics (ACL-03), pages 343–350, Sapporo, Japan.
</reference>
<page confidence="0.998988">
386
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.672447">
<title confidence="0.999947">A Semantic Approach to IE Pattern Induction</title>
<author confidence="0.999995">Mark Stevenson</author>
<author confidence="0.999995">Mark A Greenwood</author>
<affiliation confidence="0.999916">Department of Computer Science University of Sheffield</affiliation>
<address confidence="0.999094">Sheffield, S1 4DP, UK</address>
<email confidence="0.99562">marks,m.greenwood@dcs.shef.ac.uk</email>
<abstract confidence="0.976727428571429">This paper presents a novel algorithm for the acquisition of Information Extraction patterns. The approach makes the assumption that useful patterns will have similar meanings to those already identified as relevant. Patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity. Evaluation shows this algorithm performs well when compared with a previously reported document-centric approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
</authors>
<title>GATE: an Architecture for Development of Robust HLT.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL-02),</booktitle>
<pages>168--175</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="16697" citStr="Cunningham et al., 2002" startWordPosition="2811" endWordPosition="2814">er of documents in which a pattern can occur since these very frequent patterns are often too general to be useful for IE. Patterns which occur in more than Q x C, where C is the number of documents in the collection, are not learned. For the experiments in this paper we set α to 2 and Q to 0.3. 4 Implementation A number of pre-processing stages have to be applied to documents in order for the set of patterns to be extracted before learning can take place. Firstly, items belonging to semantic categories are identified by running the text through the named entity identifier in the GATE system (Cunningham et al., 2002). The corpus is then parsed, using a version of MINIPAR (Lin, 1999) adapted to process text marked with named entities, to produce dependency trees from which SVO-patterns are extracted. Active and passive voice is taken into account in MINIPAR’s output so the sentences “COMPANY fired their C.E.O.” and “The C.E.O. was fired by COMPANY” would yield the same triple, COMPANY+fire+ceo. The indirect object of ditransitive verbs is not extracted; these verbs are treated like transitive verbs for the purposes of this analysis. An implementation of the algorithm described in Section 3 was completed in</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan. 2002. GATE: an Architecture for Development of Robust HLT. In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL-02), pages 168–175, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database and some of its Applications.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database and some of its Applications. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>D Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings ofInternational Conference on Research in Computational Linguistics,</booktitle>
<contexts>
<context position="12394" citStr="Jiang and Conrath, 1997" startWordPosition="2075" endWordPosition="2078"> similarity between elements of a vector and would assign equal similarity to each pair of patterns in the example shown in Figure 1.1 The semantic similarity matrix in Equation 1 provides a mechanism to capture semantic similarity between lexical items which allows us to identify chairman+resign and ceo+quit as the most similar pair of patterns. 3.2 Populating the Matrix It is important to choose appropriate values for the elements of W. We chose to make use of the research that has concentrated on computing similarity between pairs of lexical items using the WordNet hierarchy (Resnik, 1995; Jiang and Conrath, 1997; Patwardhan et al., 2003). We experimented with several of the measures which have been reported in the literature and found that the one proposed by Jiang and Conrath (1997) to be the most effective. The similarity measure proposed by Jiang and Conrath (1997) relies on a technique developed by Resnik (1995) which assigns numerical values to each sense in the WordNet hierarchy based upon the amount of information it represents. These values are derived from corpus counts of the words in the synset, either directly or via the hyponym relation and are used to derive the Information Content (IC)</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J. Jiang and D. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings ofInternational Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
<author>C Cardie</author>
<author>D Fisher</author>
<author>J McCarthy</author>
<author>E Riloff</author>
<author>S Soderland</author>
</authors>
<date>1992</date>
<booktitle>University of Massachusetts: Description of the CIRCUS System used for MUC-4. In Proceedings of the Fourth Message Understanding Conference (MUC-4),</booktitle>
<pages>282--288</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="1005" citStr="Lehnert et al. (1992)" startWordPosition="146" endWordPosition="149"> already identified as relevant. Patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity. Evaluation shows this algorithm performs well when compared with a previously reported document-centric approach. 1 Introduction Developing systems which can be easily adapted to new domains with the minimum of human intervention is a major challenge in Information Extraction (IE). Early IE systems were based on knowledge engineering approaches but suffered from a knowledge acquisition bottleneck. For example, Lehnert et al. (1992) reported that their system required around 1,500 person-hours of expert labour to modify for a new extraction task. One approach to this problem is to use machine learning to automatically learn the domain-specific information required to port a system (Riloff, 1996). Yangarber et al. (2000) proposed an algorithm for learning extraction patterns for a small number of examples which greatly reduced the burden on the application developer and reduced the knowledge acquisition bottleneck. Weakly supervised algorithms, which bootstrap from a small number of examples, have the advantage of requiri</context>
</contexts>
<marker>Lehnert, Cardie, Fisher, McCarthy, Riloff, Soderland, 1992</marker>
<rawString>W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, E. Riloff, and S. Soderland. 1992. University of Massachusetts: Description of the CIRCUS System used for MUC-4. In Proceedings of the Fourth Message Understanding Conference (MUC-4), pages 282–288, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>MINIPAR: a minimalist parser. In</title>
<date>1999</date>
<institution>Maryland Linguistics Colloquium, University of Maryland, College Park.</institution>
<contexts>
<context position="16764" citStr="Lin, 1999" startWordPosition="2826" endWordPosition="2827">re often too general to be useful for IE. Patterns which occur in more than Q x C, where C is the number of documents in the collection, are not learned. For the experiments in this paper we set α to 2 and Q to 0.3. 4 Implementation A number of pre-processing stages have to be applied to documents in order for the set of patterns to be extracted before learning can take place. Firstly, items belonging to semantic categories are identified by running the text through the named entity identifier in the GATE system (Cunningham et al., 2002). The corpus is then parsed, using a version of MINIPAR (Lin, 1999) adapted to process text marked with named entities, to produce dependency trees from which SVO-patterns are extracted. Active and passive voice is taken into account in MINIPAR’s output so the sentences “COMPANY fired their C.E.O.” and “The C.E.O. was fired by COMPANY” would yield the same triple, COMPANY+fire+ceo. The indirect object of ditransitive verbs is not extracted; these verbs are treated like transitive verbs for the purposes of this analysis. An implementation of the algorithm described in Section 3 was completed in addition to an implementation of the document-centric algorithm de</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>D. Lin. 1999. MINIPAR: a minimalist parser. In Maryland Linguistics Colloquium, University of Maryland, College Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC</author>
</authors>
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA.</location>
<marker>MUC, 1995</marker>
<rawString>MUC. 1995. Proceedings of the Sixth Message Understanding Conference (MUC-6), San Mateo, CA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pado</author>
<author>M Lapata</author>
</authors>
<title>Constructing semantic space models from parsed corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03),</booktitle>
<pages>128--135</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="9076" citStr="Pado and Lapata, 2003" startWordPosition="1490" endWordPosition="1494">ame, country, currency values, numerical expressions etc. In this paper lexical items are represented in lower case and semantic categories are capitalised. For example, in the pattern COMPANY+fired+ceo, fired and ceo are lexical items and COMPANY a semantic category which could match any lexical item belonging to that type. The algorithm described here relies on identifying patterns with similar meanings. The approach we have developed to do this is inspired by the vector space model which is commonly used in Information Retrieval (Salton and McGill, 1983) and language processing in general (Pado and Lapata, 2003). Each pattern can be represented as a set of pattern element-filler pairs. For example, the pattern COMPANY+fired+ceo consists of three pairs: subject COMPANY, verb fired and object ceo. Each pair consists of either a lexical item or semantic category, and pattern element. Once an appropriate set of pairs has been established a pattern can be represented as a binary vector in which an element with value 1 denotes that the pattern contains a particular pair and 0 that it does not. 3.1 Pattern Similarity The similarity of two pattern vectors can be compared using the measure shown in Equation 1</context>
</contexts>
<marker>Pado, Lapata, 2003</marker>
<rawString>S. Pado and M. Lapata. 2003. Constructing semantic space models from parsed corpora. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03), pages 128–135, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>Using measures of semantic relatedness for word sense disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourth International Conferences on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>241--257</pages>
<location>Mexico City.</location>
<contexts>
<context position="12420" citStr="Patwardhan et al., 2003" startWordPosition="2079" endWordPosition="2082">nts of a vector and would assign equal similarity to each pair of patterns in the example shown in Figure 1.1 The semantic similarity matrix in Equation 1 provides a mechanism to capture semantic similarity between lexical items which allows us to identify chairman+resign and ceo+quit as the most similar pair of patterns. 3.2 Populating the Matrix It is important to choose appropriate values for the elements of W. We chose to make use of the research that has concentrated on computing similarity between pairs of lexical items using the WordNet hierarchy (Resnik, 1995; Jiang and Conrath, 1997; Patwardhan et al., 2003). We experimented with several of the measures which have been reported in the literature and found that the one proposed by Jiang and Conrath (1997) to be the most effective. The similarity measure proposed by Jiang and Conrath (1997) relies on a technique developed by Resnik (1995) which assigns numerical values to each sense in the WordNet hierarchy based upon the amount of information it represents. These values are derived from corpus counts of the words in the synset, either directly or via the hyponym relation and are used to derive the Information Content (IC) of a synset c thus IC(c) </context>
<context position="14049" citStr="Patwardhan et al. (2003)" startWordPosition="2360" endWordPosition="2363">r of vectors is given by the calculation ��� s�ss�s . Substituting the matrix multiplication in the numerator of Equation 1 for the dot product of vectors a and b would give the cosine metric. Note that taking the dot product of a pair of vectors is equivalent to multiplying by the identity matrix, i.e. a.b = aICT. Under our interpretation of the similarity matrix, W, this equates to each pattern element-filler pair being identical to itself but not similar to anything else. of all possible WordNet senses for word w): ARGMAX s1 a senses(w1), IC(s1)+IC(s2)−2xIC(lcs(s1, s2)) s2 E senses(w2) (2) Patwardhan et al. (2003) convert this distance metric into a similarity measure by taking its multiplicative inverse. Their implementation was used in the experiments described later. As mentioned above, the second part of a pattern element-filler pair can be either a lexical item or a semantic category, such as company. The identifiers used to denote these categories, i.e. COMPANY, do not appear in WordNet and so it is not possible to directly compare their similarity with other lexical items. To avoid this problem these tokens are manually mapped onto the most appropriate node in the WordNet hierarchy which is then</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2003</marker>
<rawString>S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Using measures of semantic relatedness for word sense disambiguation. In Proceedings of the Fourth International Conferences on Intelligent Text Processing and Computational Linguistics, pages 241–257, Mexico City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using Information Content to evaluate Semantic Similarity in a Taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95),</booktitle>
<pages>448--453</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="12369" citStr="Resnik, 1995" startWordPosition="2073" endWordPosition="2074">to account the similarity between elements of a vector and would assign equal similarity to each pair of patterns in the example shown in Figure 1.1 The semantic similarity matrix in Equation 1 provides a mechanism to capture semantic similarity between lexical items which allows us to identify chairman+resign and ceo+quit as the most similar pair of patterns. 3.2 Populating the Matrix It is important to choose appropriate values for the elements of W. We chose to make use of the research that has concentrated on computing similarity between pairs of lexical items using the WordNet hierarchy (Resnik, 1995; Jiang and Conrath, 1997; Patwardhan et al., 2003). We experimented with several of the measures which have been reported in the literature and found that the one proposed by Jiang and Conrath (1997) to be the most effective. The similarity measure proposed by Jiang and Conrath (1997) relies on a technique developed by Resnik (1995) which assigns numerical values to each sense in the WordNet hierarchy based upon the amount of information it represents. These values are derived from corpus counts of the words in the synset, either directly or via the hyponym relation and are used to derive the</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using Information Content to evaluate Semantic Similarity in a Taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95), pages 448–453, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In Thirteenth National Conference on Artificial Intelligence (AAAI-96),</booktitle>
<pages>1044--1049</pages>
<location>Portland, OR.</location>
<contexts>
<context position="1273" citStr="Riloff, 1996" startWordPosition="190" endWordPosition="191">ument-centric approach. 1 Introduction Developing systems which can be easily adapted to new domains with the minimum of human intervention is a major challenge in Information Extraction (IE). Early IE systems were based on knowledge engineering approaches but suffered from a knowledge acquisition bottleneck. For example, Lehnert et al. (1992) reported that their system required around 1,500 person-hours of expert labour to modify for a new extraction task. One approach to this problem is to use machine learning to automatically learn the domain-specific information required to port a system (Riloff, 1996). Yangarber et al. (2000) proposed an algorithm for learning extraction patterns for a small number of examples which greatly reduced the burden on the application developer and reduced the knowledge acquisition bottleneck. Weakly supervised algorithms, which bootstrap from a small number of examples, have the advantage of requiring only small amounts of annotated data, which is often difficult and time-consuming to produce. However, this also means that there are fewer examples of the patterns to be learned, making the learning task more challenging. Providing the learning algorithm with acce</context>
<context position="17920" citStr="Riloff (1996)" startWordPosition="3007" endWordPosition="3008">to an implementation of the document-centric algorithm described in Section 2.1. It is important to mention that this implementation is not identical to the one described by Yangarber et al. (2000). Their system makes some generalisations across pattern elements by grouping certain elements together. However, there is no difference between the expressiveness of the patterns learned by either approach and we do not believe this difference has any effect on the results of our experiments. 5 Evaluation Various approaches have been suggested for the evaluation of automatic IE pattern acquisition. Riloff (1996) judged the precision of patterns learned by reviewing them manually. Yangarber et al. (2000) developed an indirect method which allowed automatic evaluation. In addition to learning a set of patterns, their system also notes the relevance of documents based on the current set of accepted patterns. Assuming the subset of documents relevant to a particular IE scenario is known, it is possible to use these relevance judgements to determine how accurately a given set of patterns can discriminate the relevant documents from the irrelevant. This evaluation is similar to the “text-filtering” sub-tas</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically generating extraction patterns from untagged text. In Thirteenth National Conference on Artificial Intelligence (AAAI-96), pages 1044–1049, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Rose</author>
<author>M Stevenson</author>
<author>M Whitehead</author>
</authors>
<title>The Reuters Corpus Volume 1 - from Yesterday’s news to tomorrow’s language resources.</title>
<date>2002</date>
<booktitle>In LREC-02,</booktitle>
<pages>827--832</pages>
<location>La Palmas,</location>
<contexts>
<context position="20401" citStr="Rose et al., 2002" startWordPosition="3405" endWordPosition="3408">ted with it. 590 documents from a version of the MUC-6 evaluation corpus described by Soderland (1999) were used. After the pre-processing stages described in Section 4, the MUC-6 corpus produced 15,407 pattern tokens from 11,294 different types. 10,512 patterns appeared just once and these were effectively discarded since our learning algorithm only considers patterns which occur at least twice (see Section 3.3). The document-centric approach benefits from a large corpus containing a mixture of relevant and irrelevant documents. We provided this using a subset of the Reuters Corpus Volume I (Rose et al., 2002) which, like the MUC-6 corpus, consists of newswire 383 COMPANY+appoint+PERSON COMPANY+elect+PERSON COMPANY+promote+PERSON COMPANY+name+PERSON PERSON+resign PERSON+depart PERSON+quit Table 1: Seed patterns for extraction task texts. 3000 documents relevant to the management succession task (identified using document metadata) and 3000 irrelevant documents were used to produce the supplementary corpus. This supplementary corpus yielded 126,942 pattern tokens and 79,473 types with 14,576 of these appearing more than once. Adding the supplementary corpus to the data set used by the document-centr</context>
</contexts>
<marker>Rose, Stevenson, Whitehead, 2002</marker>
<rawString>T. Rose, M. Stevenson, and M. Whitehead. 2002. The Reuters Corpus Volume 1 - from Yesterday’s news to tomorrow’s language resources. In LREC-02, pages 827–832, La Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="9017" citStr="Salton and McGill, 1983" startWordPosition="1481" endWordPosition="1484">e either a lexical item or semantic category such as person name, country, currency values, numerical expressions etc. In this paper lexical items are represented in lower case and semantic categories are capitalised. For example, in the pattern COMPANY+fired+ceo, fired and ceo are lexical items and COMPANY a semantic category which could match any lexical item belonging to that type. The algorithm described here relies on identifying patterns with similar meanings. The approach we have developed to do this is inspired by the vector space model which is commonly used in Information Retrieval (Salton and McGill, 1983) and language processing in general (Pado and Lapata, 2003). Each pattern can be represented as a set of pattern element-filler pairs. For example, the pattern COMPANY+fired+ceo consists of three pairs: subject COMPANY, verb fired and object ceo. Each pair consists of either a lexical item or semantic category, and pattern element. Once an appropriate set of pairs has been established a pattern can be represented as a binary vector in which an element with value 1 denotes that the pattern contains a particular pair and 0 that it does not. 3.1 Pattern Similarity The similarity of two pattern ve</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soderland</author>
</authors>
<title>Learning Information Extraction Rules for Semi-structured and free text.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>31--1</pages>
<contexts>
<context position="19210" citStr="Soderland (1999)" startWordPosition="3215" endWordPosition="3216">ch systems were evaluated according to their ability to identify the documents relevant to the extraction task. The document filtering evaluation technique was used to allow comparison with previous studies. Identifying the document containing relevant information can be considered as a preliminary stage of an IE task. A further step is to identify the sentences within those documents which are relevant. This “sentence filtering” task is a more fine-grained evaluation and is likely to provide more information about how well a given set of patterns is likely to perform as part of an IE system. Soderland (1999) developed a version of the MUC-6 corpus in which events are marked at the sentence level. The set of patterns learned by the algorithm after each iteration can be compared against this corpus to determine how accurately they identify the relevant sentences for this extraction task. 5.1 Evaluation Corpus The evaluation corpus used for the experiments was compiled from the training and testing corpus used in MUC-6, where the task was to extract information about the movements of executives from newswire texts. A document is relevant if it has a filled template associated with it. 590 documents </context>
</contexts>
<marker>Soderland, 1999</marker>
<rawString>S. Soderland. 1999. Learning Information Extraction Rules for Semi-structured and free text. Machine Learning, 31(1-3):233–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sudo</author>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<title>An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03),</booktitle>
<pages>224--231</pages>
<contexts>
<context position="26966" citStr="Sudo et al. (2003)" startWordPosition="4446" endWordPosition="4449"> algorithm which could be used to complement manual approaches. These results could be improved by manual filtering the patterns identified by the algorithm. The learning algorithm presented in Section 3 includes a mechanism for comparing two extraction patterns using information about lexical similarity derived from WordNet. This approach is not restricted to this application and could be applied to other language processing tasks such as question answering, paraphrase identification and generation or as a variant of the vector space model commonly used in Information Retrieval. In addition, Sudo et al. (2003) proposed representations for IE patterns which extends the SVO representation used here and, while they did not appear to significantly improve IE, it is expected that it will be straightforward to extend the vector space model to those pat0.55 0.50 0.45 0.30 0.25 0.20 Semantic Similarity Document-centric F-measure 0.40 0.35 0.15 385 tern representations. One of the reasons for the success of the approach described here is the appropriateness of WordNet which is constructed on paradigmatic principles, listing the words which may be substituted for one another, and is consequently an excellent</context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2003</marker>
<rawString>K. Sudo, S. Sekine, and R. Grishman. 2003. An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03), pages 224–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
<author>R Grishman</author>
<author>P Tapanainen</author>
<author>S Huttunen</author>
</authors>
<title>Automatic acquisition of domain knowledge for information extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING</booktitle>
<pages>940--946</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="1298" citStr="Yangarber et al. (2000)" startWordPosition="192" endWordPosition="195">pproach. 1 Introduction Developing systems which can be easily adapted to new domains with the minimum of human intervention is a major challenge in Information Extraction (IE). Early IE systems were based on knowledge engineering approaches but suffered from a knowledge acquisition bottleneck. For example, Lehnert et al. (1992) reported that their system required around 1,500 person-hours of expert labour to modify for a new extraction task. One approach to this problem is to use machine learning to automatically learn the domain-specific information required to port a system (Riloff, 1996). Yangarber et al. (2000) proposed an algorithm for learning extraction patterns for a small number of examples which greatly reduced the burden on the application developer and reduced the knowledge acquisition bottleneck. Weakly supervised algorithms, which bootstrap from a small number of examples, have the advantage of requiring only small amounts of annotated data, which is often difficult and time-consuming to produce. However, this also means that there are fewer examples of the patterns to be learned, making the learning task more challenging. Providing the learning algorithm with access to additional knowledg</context>
<context position="5358" citStr="Yangarber et al. (2000)" startWordPosition="871" endWordPosition="874">ute scores or ranks after the set of patterns has been ordered by scores) are chosen as being suitable for inclusion in the set of accepted patterns. These form the set Slearn. 5. The patterns in Slearn are added to Sacc and removed from Scand, so Sacc +— Sacc U Slearn and Scand +— Sacc − Slearn 6. If a suitable set of patterns has been learned then stop, otherwise go to step 4 2.1 Document-centric approach A key choice in the development of such an algorithm is step 4, the process of ranking the candidate patterns, which effectively determines which of the candidate patterns will be learned. Yangarber et al. (2000) chose an approach motivated by the assumption that documents containing a large number of patterns already identified as relevant to a particular IE scenario are likely to contain further relevant patterns. This approach, which can be viewed as being document-centric, operates by associating confidence scores with patterns and relevance scores with documents. Initially seed patterns are given a maximum confidence score of 1 and all others a 0 score. Each document is given a relevance score based on the patterns which occur within it. Candidate patterns are ranked according to the proportion o</context>
<context position="17504" citStr="Yangarber et al. (2000)" startWordPosition="2943" endWordPosition="2946">d. Active and passive voice is taken into account in MINIPAR’s output so the sentences “COMPANY fired their C.E.O.” and “The C.E.O. was fired by COMPANY” would yield the same triple, COMPANY+fire+ceo. The indirect object of ditransitive verbs is not extracted; these verbs are treated like transitive verbs for the purposes of this analysis. An implementation of the algorithm described in Section 3 was completed in addition to an implementation of the document-centric algorithm described in Section 2.1. It is important to mention that this implementation is not identical to the one described by Yangarber et al. (2000). Their system makes some generalisations across pattern elements by grouping certain elements together. However, there is no difference between the expressiveness of the patterns learned by either approach and we do not believe this difference has any effect on the results of our experiments. 5 Evaluation Various approaches have been suggested for the evaluation of automatic IE pattern acquisition. Riloff (1996) judged the precision of patterns learned by reviewing them manually. Yangarber et al. (2000) developed an indirect method which allowed automatic evaluation. In addition to learning a</context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen. 2000. Automatic acquisition of domain knowledge for information extraction. In Proceedings of the 18th International Conference on Computational Linguistics (COLING 2000), pages 940–946, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
</authors>
<title>Counter-training in the discovery of semantic patterns.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03),</booktitle>
<pages>343--350</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3589" citStr="Yangarber, 2003" startWordPosition="551" endWordPosition="552">ribe an implementation (Section 4). Two evaluation regimes are described; one based on the identification of relevant documents and another which aims to identify sentences in a corpus which 379 Proceedings of the 43rd Annual Meeting of the ACL, pages 379–386, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics are relevant for a particular IE task (Section 5). Results on each of these evaluation regimes are then presented (Sections 6 and 7). 2 Extraction Pattern Learning We begin by outlining the general process of learning extraction patterns, similar to one presented by (Yangarber, 2003). 1. For a given IE scenario we assume the existence of a set of documents against which the system can be trained. The documents are unannotated and may be either relevant (contain the description of an event relevant to the scenario) or irrelevant although the algorithm has no access to this information. 2. This corpus is pre-processed to generate the set of all patterns which could be used to represent sentences contained in the corpus, call this set S. The aim of the learning process is to identify the subset of S representing patterns which are relevant to the IE scenario. 3. The user pro</context>
<context position="7723" citStr="Yangarber (2003)" startWordPosition="1260" endWordPosition="1262"> matched by a pattern, an alternative approach is to rank patterns according to how similar their meanings are to those which are known to be relevant. This semantic-similarity approach avoids the problem which may be present in the document-centric approach since patterns which happen to co-occur in the same documents as relevant ones but have different meanings will not be ranked highly. We now go on to describe a new algorithm which implements this approach. 380 3 Semantic IE Pattern Learning For these experiments extraction patterns consist of predicate-argument structures, as proposed by Yangarber (2003). Under this scheme patterns consist of triples representing the subject, verb and object (SVO) of a clause. The first element is the “semantic” subject (or agent), for example “John” is a clausal subject in each of these sentences “John hit Bill”, “Bill was hit by John”, “Mary saw John hit Bill”, and “John is a bully”. The second element is the verb in the clause and the third the object (patient) or predicate. “Bill” is a clausal object in the first three example sentences and “bully” in the final one. When a verb is being used intransitively, the pattern for that clause is restricted to onl</context>
</contexts>
<marker>Yangarber, 2003</marker>
<rawString>R. Yangarber. 2003. Counter-training in the discovery of semantic patterns. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03), pages 343–350, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>