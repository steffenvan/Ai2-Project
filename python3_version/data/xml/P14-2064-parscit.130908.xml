<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001678">
<title confidence="0.923889">
Difficult Cases: From Data to Learning, and Back
</title>
<author confidence="0.628709">
Beata Beigman Klebanov*
</author>
<affiliation confidence="0.567898">
Educational Testing Service
</affiliation>
<address confidence="0.8508555">
660 Rosedale Road
Princeton, NJ 08541
</address>
<email confidence="0.997568">
bbeigmanklebanov@ets.org
</email>
<sectionHeader confidence="0.993857" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999988722222222">
This article contributes to the ongoing dis-
cussion in the computational linguistics
community regarding instances that are
difficult to annotate reliably. Is it worth-
while to identify those? What informa-
tion can be inferred from them regarding
the nature of the task? What should be
done with them when building supervised
machine learning systems? We address
these questions in the context of a sub-
jective semantic task. In this setting, we
show that the presence of such instances
in training data misleads a machine learner
into misclassifying clear-cut cases. We
also show that considering machine lear-
ning outcomes with and without the diffi-
cult cases, it is possible to identify specific
weaknesses of the problem representation.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999696428571429">
The problem of cases that are difficult for anno-
tation received recent attention from both the the-
oretical and the applied perspectives. Such items
might receive contradictory labels, without a clear
way of settling the disagreement. Beigman and
Beigman Klebanov (2009) showed theoretically
that hard cases – items with unreliable annota-
tions – can lead to unfair benchmarking results
when found in test data, and, in worst case, to a
degradation in a machi74ne learner’s performance
on easy, uncontroversial instances if found in the
training data. Schwartz et al. (2011) provided an
empirical demonstration that the presence of such
difficult cases in dependency parsing evaluations
</bodyText>
<footnote confidence="0.99913975">
1The work presented in this paper was done when the first
author was a post-doctoral fellow at Northwestern University,
Evanston, IL and the second author was a visiting assistant
professor at Washington University, St. Louis, MO.
</footnote>
<note confidence="0.836778">
Eyal Beigman*
Liquidnet Holdings Inc.
498 Seventh Avenue
New York, NY 10018
</note>
<email confidence="0.982099">
e.beigman@gmail.com
</email>
<bodyText confidence="0.999915852941177">
leads to unstable benchmarking results, as diffe-
rent gold standards might provide conflicting an-
notations for such items. Reidsma and Carletta
(2008) demonstrated by simulation that systema-
tic disagreements between annotators negatively
impact generalization ability of classifiers built
using data from different annotators. Oosten et
al. (2011) showed that judgments of readability
of the same texts by different groups of experts
are sufficiently systematically different to hamper
cross-expert generalization of readability classi-
fiers trained on annotations from different groups.
Rehbein and Ruppenhofer (2011) discuss the ne-
gative impact of systematic simulated annotation
inconsistencies on active learning performance on
a word-sense disambiguation task.
In this paper, we address the task of classify-
ing words in a text as semantically new or old.
Using multiple annotators, we empirically identify
instances that show substantial disagreement be-
tween annotators. We then discuss those both from
the linguistic perspective, identifying some char-
acteristics of such cases, and from the perspec-
tive of machine learning, showing that the pres-
ence of difficult cases in the training data misleads
the machine learner on easy, clear-cut cases – a
phenomenon termed hard case bias in Beigman
and Beigman Klebanov (2009). The main con-
tribution of this paper is in providing additional
empricial evidence in support of the argument put
forward in the literature regarding the need to pay
attention to problematic, disagreeable instances in
annotated data – not only from the linguistic per-
spective, but also from a machine learning one.
</bodyText>
<sectionHeader confidence="0.991912" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.9994062">
The task considered here is that of classifying first
occurrences of words in a text as semantically old
or new. One of goals of the project is to inves-
tigate the relationship between various kinds of
non-novelty in text, and, in particular, the rela-
</bodyText>
<page confidence="0.96245">
390
</page>
<bodyText confidence="0.993205018867925">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 390–396,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
tionship between semantic non-novelty (conceptu-
alized as semantic association with some preced-
ing word in the text), the information structure in
terms of given and new information, and the cog-
nitive status of discourse entities (Postolache et al.,
2005; Birner and Ward, 1998; Gundel et al., 1993;
Prince, 1981). If an annotator identified an asso-
ciative tie from the target word back to some other
word in the text, the target word is thereby classi-
fied as semantically old (class 1, or positive); if no
ties were identified, it is classified as new (class 0,
or negative).
For the project, annotations were collected for
10 texts of various genres, where annotators were
asked, for every first appearance of a word in a
text, to point out previous words in the text that
are semantically or associatively related to it. All
data was annotated by 22 undergraduate and grad-
uate students in various disciplines who were re-
cruited for the task. During outlier analysis, data
from two annotators was excluded from considera-
tion, while 20 annotations were retained. This task
is fairly subjective, with inter-annotator agreement
κ=0.45 (Beigman Klebanov and Shamir, 2006).
Table 1 shows the number and proportion of in-
stances that received the “semantically old” (1) la-
bel from i annotators, for 0 &lt; i &lt; 20. The first col-
umn shows the number of annotators who gave the
label “semantically old” (1). Column 2 shows the
number and proportion of instances that received
the label 1 from the number of annotators shown in
column 1. Column 3 shows the split into item dif-
ficulty groups. We note that while about 20% of
the instances received a unanimous 0 annotation
and about 12% of the instances received just one 1
label out of 20 annotators, the remaining instances
are spread out across various values of i. Reasons
for this spread include intrinsic difficulty of some
of the items, as well as attention slips. Since anno-
tators need to consider the whole of the preceding
text when annotating a given word, maintaining
focus is a challenge, especially for words that first
appear late in the text.
Our interest being in difficult, disagreeable
cases, we group the instances into 5 bands accor-
ding to the observed level of disagreement and
the tendency in the majority of the annotations.
Thus, items with at most two label 1 annotations
are clearly semantically new, while those with at
least 17 (out of 20) are clearly semantically old.
The groups Hard 0 and Hard 1 contain instances
</bodyText>
<figure confidence="0.798559956521739">
# 1s # instances group
(proportion)
0 476 (.20) Easy 0
1 271 (.12) (.40)
2 191 (.08)
3 131 (.06) Hard 0
4 106 (.05) (.25)
5 76 (.03)
6 95 (.04)
7 85 (.04)
8 78 (.03)
9 60 (.03) Very
10 70 (.03) Hard
11 60 (.03) (.08)
12 57 (.02) Hard 1
13 63 (.03) (.13)
14 68 (.03)
15 49 (.02)
16 65 (.03)
17 60 (.03) Easy 1
18 72 (.03) (.14)
19 94 (.04)
20 99 (.04)
</figure>
<tableCaption confidence="0.961635">
Table 1: Sizes of subsets by levels of agreement.
</tableCaption>
<bodyText confidence="0.9999315">
with at least a 60% majority classification, while
the middle class – Very Hard – contains instances
for which it does not appear possible to even iden-
tify the overall tendency.
In what follows, we investigate the learnabi-
lity of the classification of semantic novelty from
various combinations of easy, hard, and very hard
data.
</bodyText>
<sectionHeader confidence="0.994339" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.998686">
3.1 Training Partitions
</subsectionHeader>
<bodyText confidence="0.999778">
The objective of the study is to determine the use-
fulness of instances of various types in the training
data for semantic novelty classification. In parti-
cular, in light of Beigman and Beigman Klebanov
(2009), we want to check whether the presence of
less reliable data (hard cases) in the training set
adversely impacts performance on the highly reli-
able data (easy cases). We therefore test separately
on easy and hard cases.
We ran 25 rounds of the following experiment.
All easy cases are randomly split 80% (train) and
20% (test), all hard cases are split into train and
test sets in the same proportions. Then various
</bodyText>
<page confidence="0.993051">
391
</page>
<bodyText confidence="0.930547142857143">
parts of the training data are used to train the 5 sys-
tems described in Table 2. We build models using
easy data; hard data; easy and hard data; easy,
hard, and very hard data; easy data and a weighted
sample of the hard data. The labels for very hard
data were assigned by flipping a fair coin.
System Easy Hard Very Hard
</bodyText>
<equation confidence="0.997791833333333">
E +
H +
E+H + +
E+H+VH + + +
E+H100 + sample1
w
</equation>
<bodyText confidence="0.966771333333333">
Table 2: The 5 training regimes used in the experi-
ment, according to the parts of the data utilized for
training.
</bodyText>
<subsectionHeader confidence="0.998331">
3.2 Machine Learning
</subsectionHeader>
<bodyText confidence="0.980183034482759">
We use linear Support Vector Machines classifier
as implemented in SVMLight (Joachims, 1999).
Apart from being a popular and powerful ma-
chine learning method, linear SVM is one of the
family of classifiers analyzed in Beigman and
Beigman Klebanov (2009), where they are theo-
retically shown to be vulnerable to hard case bias
in the worst case.
To represent the instances, we use two features
that capture semantic relatedness between words.
One feature uses Latent Semantic Analysis (Deer-
wester et al., 1990) trained on the Wall Street Jour-
nal articles to quantify the distributional similarity
of two words, the other uses an algorithm based
on WordNet (Miller, 1990) to calculate seman-
tic relatedness, combining information from both
the hierarchy and the glosses (Beigman Klebanov,
2006). For each word, we calculate LSA (Word-
Net) relatedness score for this word with each pre-
ceding word in the text, and report the highest pair-
wise score as the LSA (WordNet) feature value for
the given word. The values of the features can
be thought of as quantifying the strength of the
evidence for semantic non-newness that could be
obtained via a distributional or a dictionary-based
method.
1The weight corresponds to the number of people who
marked the item as 1, for hard cases. We take a weighted
sample of 100 hard cases.
</bodyText>
<sectionHeader confidence="0.999743" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.998577666666667">
We calculate the accuracy of every system sepa-
rately on the easy and hard test data. Table 3 shows
the results.
</bodyText>
<table confidence="0.999162">
Train Test-E Test-H
Acc Rank Acc Rank
E 0.781 1 0.643 2
E+H 0.764 2 0.654 1
E+H+VH 0.761 2 0.650 1,2
H 0.620 3 0.626 3
E+H100 0.779 1 0.645 2
w
</table>
<tableCaption confidence="0.598295">
Table 3: Accuracy and ranking for semantic no-
velty classification for systems built using various
training data and tested on easy (Test-E) and hard
(Test-H) cases. Systems with insignificant differ-
ences in performance (paired t-test, n=25, p&gt;0.05)
are given the same rank.
</tableCaption>
<bodyText confidence="0.9958109375">
We observe first the performance of the system
trained solely on hard cases (H in Table 3). This
system shows the worst performance, both on the
easy test and on the hard test. In fact, this system
failed to learn anything about the positive class in
24 out of the 25 runs, classifying all cases as nega-
tive. It is thus safe to conclude that in the feature
space used here the supervision signal in the hard
cases is too weak to guide learning.
The system trained solely on easy cases (E in
Table 3) significantly outperforms H both on the
easy and on the hard test. That is, easy cases are
more informative about the classification of hard
cases than the hard cases themselves. This shows
that at least some hard cases pattern similarly to
the easy ones in the feature space; SVM failed to
single them out when trained on hard cases alone,
but they are learnable from the easy data.
The system that trained on all cases – both easy
and hard – attains the best performance on hard
cases but yields to E on the easy test (Test-E). This
demonstrates what Beigman and Beigman Kle-
banov (2009) called hard case bias – degradation
in test performance on easy cases due to hard cases
in the training data. The negative effect of using
hard cases in training data can be mitigated if we
only use a small sample of them (system E+H100
w );
yet neither this nor other schemes we tried of
selectively incorporating hard cases into training
data produced an improvement over E when tested
on easy cases (Test-E).
</bodyText>
<page confidence="0.995693">
392
</page>
<sectionHeader confidence="0.999547" genericHeader="method">
5 Discussion
</sectionHeader>
<subsectionHeader confidence="0.991508">
5.1 Beyond worst case
</subsectionHeader>
<bodyText confidence="0.999881129032258">
Beigman and Beigman Klebanov (2009) per-
formed a theoretical analysis showing that hard
cases could lead to hard case bias where hard cases
have completely un-informative labels, with pro-
bability of p=0.5 for either label. These corre-
spond to very hard cases in our setting. According
to Table 3, it is indeed the case that adding the
very hard cases hurts performance, but not signif-
icantly so – compare results for E+H vs E+H+VH
systems.
Our results suggest that un-informative labels
are not necessary for the hard case bias to sur-
face. The instances grouped under Hard 1 have
the probability of p=0.66 for class 1 and the in-
stances grouped under Hard 0 have the probabi-
lity of p=0.71 for class 0. Thus, while the labels
are somewhat informative, it is apparently the case
that the hard instances are distributed sufficiently
differently in the feature space from the easy cases
with the same label to produce a hard case bias.
Inspecting the distribution of hard cases (Fig-
ure 1), we note that hard cases do not follow
the worst case pattern analyzed in Beigman and
Beigman Klebanov (2009), where they were con-
centrated in an area of the feature space that was
removed far from the separation plane, a malig-
nant but arguably unlikely scenario (Dligach et al.,
2010). Here, hard cases are spread both close and
far from the plane, yet their distribution is suffi-
ciently different from that of the easy cases to pro-
duce hard case bias during learning.
</bodyText>
<figureCaption confidence="0.983765">
Figure 1: Hard cases with separators learned from
easy and easy+hard training data.
</figureCaption>
<subsectionHeader confidence="0.98911">
5.2 The nature of hard cases
</subsectionHeader>
<bodyText confidence="0.99987852">
Figure 1 plots the hard instances in the two-
dimensional feature space: Latent Semantic Anal-
ysis score is shown on x-axis, and WordNet-based
score is shown on the y-axis. The red lines show
the linear separator induced when the system is
trained on easy cases only (system E in Table 3),
whereas the green line shows the separator in-
duced when the system is trained on both easy and
hard cases (system E+H).
It is apparent from the figure that the difference
in the distributions of the easy and the hard cases
lead to a lower threshold for LSA score when
WordNet score is zero and a higher threshold of
WordNet score when LSA score is zero in hard
vs easy cases. That is, the system exposed to hard
cases learned to trust LSA more and to trust Word-
Net less when determining that an instance is se-
mantically old than a system that saw only easy
cases at train time.
The tendency to trust WordNet less yields an
improvement in precision (92.1% for system E+H
on Test-E class 1 data vs 84% for system E on
Test-E class 1 data), which comes at a cost of a
drop in recall (42.2% vs 53.3%) on easy positive
cases. This suggests that high WordNet scores that
are not supported by distributional evidence are a
source of Hard 0 cases that made the system more
cautious when relying on WordNet scores.
The pattern of low LSA score and high Word-
Net score often obtains for rare senses of words:
Distributional evidence typically points away from
these senses, but they can be recovered through
dictionary definitions (glosses) in WordNet.
An example of hard 0 case involves a homony-
mous rare sense. Deck is used in the observation
deck sense in one of the texts. However, it was
found to be highly related to buy by WordNet-
based measure through the notion of illegal – buy
in the sense of bribe and deck in the sense of a
packet of illegal drugs. This is clearly a spuri-
ous connection that makes deck appear semanti-
cally associated with preceding material, whereas
annotators largely perceived it as new.
Exposure to such cases at training time leads the
system to forgo handling rare senses that lack dis-
tributional evidence, thus leading to misclassifica-
tion of easy positive cases that exhibit a similar
pattern. Thus, stall and market are both used in the
sales outlet sense in one of the text. They come out
highly related by WordNet measure; yet in the 68
</bodyText>
<figure confidence="0.997759">
Hard cases
0.7
0.6
WordNet score
0.5
0.4
0.3
0.2
0.1
LSA score
0
0 0.2 0.4 0.6 0.8
Easy Separator Hard &amp;quot;-&amp;quot;
Easy+Hard Separator Hard &amp;quot;+&amp;quot;
0.8
</figure>
<page confidence="0.998408">
393
</page>
<bodyText confidence="0.999963074074074">
instances of stall in the training data for LSA the
homonymous verbal usage predominates. Simi-
larly, partner is overwhelmingly used in the busi-
ness partner sense in the WSJ data, hence wife and
partner come out distributionally unrelated, while
the WordNet based measure successfully recovers
these connections.
Our features, while rich enough to diagnose
a rare sense (low LSA score and high WordNet
score), do not provide information regarding the
appropriateness of the rare sense in context. Short
of full scale word sense disambiguation, we expe-
rimented with the idea of taking the second highest
pairwise score as the value of the WordNet fea-
ture, under the assumption that an appropriate rare
sense is likely to be related to multiple words in
the preceding text, while a spurious rare sense is
less likely to be accidentally related to more than
one preceding word. We failed to improve per-
formance, however; it is thus left for future work
to enrich the representation of the problem so that
cases with inappropriate rare senses can be differ-
entiated from the appropriate ones. In the context
of the current article, the identification of a parti-
cular weakness in the representation is an added
value of the analysis of the machine learning per-
formance with and without the difficult cases.
</bodyText>
<sectionHeader confidence="0.999899" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999954409090909">
Reliability of annotation is a concern widely
discussed in the computational linguistics litera-
ture (Bayerl and Paul, 2011; Beigman Klebanov
and Beigman, 2009; Artstein and Poesio, 2008;
Craggs and McGee Wood, 2005; Di Eugenio and
Glass, 2004; Carletta, 1996). Ensuring high re-
liability is not always feasible, however; the ad-
vent of crowdsourcing brought about interest in
algorithms for recovering from noisy annotations:
Snow et al. (2008), Passonneau and Carpenter
(2013) and Raykar et al. (2010) discuss methods
for improving over annotator majority vote when
estimating the ground truth from multiple noisy
annotations.
A situation where learning from a small num-
ber of carefully chosen examples leads to a better
performance in classifiers is discussed in the ac-
tive learning literature (Schohn and Cohn, 2000;
Cebron and Berthold, 2009; Nguyen and Smeul-
ders, 2004; Tong and Koller, 2001). Recent work
in the proactive active learning and multi-expert
active learning paradigms incorporates considera-
tions of item difficulty and annotator expertise into
an active learning scheme (Wallace et al., 2011;
Donmez and Carbonell, 2008).
In information retrieval, one line of work con-
cerns the design of evaluation schemes that reflect
different levels of document relevance to a given
query (Kanoulas and Aslam, 2009; Sakai, 2007;
Kek¨al¨ainen, 2005; Sormunen, 2002; Voorhees,
2001; J¨arvelin and Kek¨al¨ainen, 2000; Voorhees,
2000). J¨arvelin and Kek¨al¨ainen (2000) consider,
for example, a tiered evaluation scheme, where
precision and recall are reported separately for ev-
ery level of relevance, which is quite analogous
to the idea of testing separately on easy and hard
cases as employed here. The graded notion of
relevance addressed in the information retrieval
research assumes a coding scheme where people
assign documents into one of the relevance tiers
(Kek¨al¨ainen, 2005; Sormunen, 2002). In our case,
the graded notion of semantic novelty is a possible
explanation for the observed pattern of annotator
responses.
</bodyText>
<sectionHeader confidence="0.99102" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999809851851852">
This article contributes to the ongoing discussion
in the computational linguistics community re-
garding instances that are difficult to annotate re-
liably – how to identify those, and what to do
with them once identified. We addressed this is-
sue in the context of a subjective semantic task.
In this setting, we showed that the presence of
difficult instances in training data misleads a ma-
chine learner into misclassifying clear-cut, easy
cases. We also showed that considering machine
learning outcomes with and without the difficult
cases, it is possible to identify specific weaknesses
of the problem representation. Our results align
with the literature suggesting that difficult cases
in training data can be disruptive (Beigman and
Beigman Klebanov, 2009; Schwartz et al., 2011;
Rehbein and Ruppenhofer, 2011; Reidsma and
Carletta, 2008); yet we also show that investigat-
ing their impact on the learning outcomes in some
detail can provide insight about the task at hand.
The main contribution of this paper is there-
fore in providing additional empirical evidence in
support of the argument put forward in the litera-
ture regarding the need to pay attention to prob-
lematic, disagreeable instances in annotated data
– both from the linguistic and from the machine
learning perspectives.
</bodyText>
<page confidence="0.995851">
394
</page>
<note confidence="0.645619">
Pinar Donmez and Jaime G. Carbonell. 2008. Proac-
tive learning: Cost-sensitive active learning with
multiple imperfect oracles. In Proceedings of the
17th ACM Conference on Information and Knowl-
edge Management, CIKM ’08, pages 619–628, New
York, NY, USA. ACM.
References
</note>
<bodyText confidence="0.421349666666667">
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.
</bodyText>
<reference confidence="0.993721875">
Petra Saskia Bayerl and Karsten Ingmar Paul. 2011.
What determines inter-coder agreement in manual
annotations? a meta-analytic investigation. Comput.
Linguist., 37(4):699–725, December.
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with Annotation Noise. In Proceedings
of the 47th Annual Meeting of the Association for
Computational Linguistics, pages 280–287, Singa-
pore, August.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From Annotator Agreement to Noise Models. Com-
putational Linguistics, 35(4):493–503.
Beata Beigman Klebanov and Eli Shamir. 2006.
Reader-based exploration of lexical cohesion. Lan-
guage Resources and Evaluation, 40(2):109–126.
Beata Beigman Klebanov. 2006. Measuring Seman-
tic Relatedness Using People and WordNet. In Pro-
ceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Short
Papers, pages 13–16, New York City, USA, June.
Association for Computational Linguistics.
Betty Birner and Gregory Ward. 1998. Information
Status and Non-canonical Word Order in English.
Amsterdam/Philadelphia: John Benjamins.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249–254.
Nicolas Cebron and Michael Berthold. 2009. Active
learning for object classification: From exploration
to exploitation. Data Mining and Knowledge Dis-
covery, 18:283–299.
Richard. Craggs and Mary McGee Wood. 2005. Eval-
uating Discourse and Dialogue Coding Schemes.
Computational Linguistics, 31(3):289–296.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal
of the American Society For Information Science,
41:391–407.
Barbara Di Eugenio and Michael Glass. 2004. The
kappa statistic: a second look. Computational Lin-
guistics, 30(1):95–101.
Dmitriy Dligach, Rodney Nielsen, and Martha Palmer.
2010. To Annotate More Accurately or to Annotate
More. In Proceedings of the 4th Linguistic Annota-
tion Workshop, pages 64–72, Uppsala, Sweden, July.
Jeanette Gundel, Nancy Hedberg, and Ron Zacharski.
1993. Cognitive status and the form of referring ex-
pressions in discourse. Language, 69:274–307.
Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2000. IR
Evaluation Methods for Retrieving Highly Relevant
Documents. In Proceedings of the 23th Annual In-
ternational Conference on Research and Develop-
ment in Information Retrieval, pages 41–48, Athens,
Greece, July.
Thorsten Joachims. 1999. Advances in Kernel
Methods - Support Vector Learning. In Bern-
hard Schlkopf, Christopher Burges, and Alexander
Smola, editors, Making large-scale SVM learning
practical, pages 169–184. MIT Press.
Evangelos Kanoulas and Javed Aslam. 2009. Empir-
ical Justification of the Gain and Discount Function
for nDCG . In Proceedings of the 19th ACM Confer-
ence on Information and Knowledge Management,
pages 611–620, Hong Kong, November.
Jaana Kek¨al¨ainen. 2005. Binary and graded relevance
in IR evaluations – Comparison of the effects on
ranking of IR systems. Information Processing and
Management, 41:1019–1033.
George Miller. 1990. WordNet: An on-line lexical
database. International Journal of Lexicography,
3(4):235–312.
Hieu Nguyen and Arnold Smeulders. 2004. Ac-
tive Learning Using Pre-clustering. In Proceedings
of 21st International Conference on Machine Lear-
ning, pages 623–630, Banff, Canada, July.
Philip Oosten, Vronique Hoste, and Dries Tanghe.
2011. A posteriori agreement as a quality mea-
sure for readability prediction systems. In Alexan-
der Gelbukh, editor, Computational Linguistics and
Intelligent Text Processing, volume 6609 of Lec-
ture Notes in Computer Science, pages 424–435.
Springer Berlin Heidelberg.
Rebecca J. Passonneau and Bob Carpenter. 2013. The
benefits of a model of annotation. In Proceedings of
the 7th Linguistic Annotation Workshop and Interop-
erability with Discourse, pages 187–195, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Oana Postolache, Ivana Kruijff-Korbayova, and Geert-
Jan Kruijff. 2005. Data-driven approaches for in-
formation structure identification. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 9–16, Vancouver, British
Columbia, Canada, October.
</reference>
<page confidence="0.986793">
395
</page>
<reference confidence="0.999917609375">
Ellen Prince. 1981. Toward a taxonomy of given-new
information. In Peter Cole, editor, Radical Prag-
matics, pages 223–255. Academic Press.
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, and Linda Moy. 2010. Learning from crowds.
J. Mach. Learn. Res., 11:1297–1322, August.
Ines Rehbein and Josef Ruppenhofer. 2011. Evaluat-
ing the impact of coder errors on active learning. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ’11, pages 43–
51, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Dennis Reidsma and Jean Carletta. 2008. Reliability
Measurement without Limits. Computational Lin-
guistics, 34(3):319–326.
Tetsuya Sakai. 2007. On the reliability of information
retrieval metrics based on graded relevance. Infor-
mation Processing and Management, 43:531–548.
Greg Schohn and David Cohn. 2000. Less is more:
Active Learning with Support Vector Mfachines. In
Proceedings of 17th International Conference on
Machine Learning, pages 839–846, San Francisco,
July.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ’11, pages 663–672, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast—but is it
good?: Evaluating non-expert annotations for nat-
ural language tasks. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’08, pages 254–263, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Eero Sormunen. 2002. Liberal relevance criteria of
TREC – Counting on negligible documents? In
Proceedings of the 25th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval, pages 324–330, Tampere,
Finland, August.
Simon Tong and Daphne Koller. 2001. Support Vec-
tor Machine active learning with applications to text
classification. Journal of Machine Learning Re-
search, 2:45–66.
Ellen Voorhees. 2000. Variations in relevance judge-
ments and the measurement of retrieval effective-
ness. Information Processing and Management,
36:697–716.
Ellen Voorhees. 2001. Evaluation by highly relevant
documents. In Proceedings of the 24th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 74–82,
New Orleans, LA, USA, September.
B. Wallace, K. Small, C. Brodley, and T. Trikalinos,
2011. Who Should Label What? Instance Alloca-
tion in Multiple Expert Active Learning, chapter 16,
pages 176–187.
</reference>
<page confidence="0.999087">
396
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.286874">
<title confidence="0.772838">Difficult Cases: From Data to Learning, and Back Beigman</title>
<author confidence="0.371799">Educational Testing Service</author>
<address confidence="0.9991685">660 Rosedale Road Princeton, NJ 08541</address>
<email confidence="0.995384">bbeigmanklebanov@ets.org</email>
<abstract confidence="0.999282526315789">This article contributes to the ongoing discussion in the computational linguistics community regarding instances that are difficult to annotate reliably. Is it worthwhile to identify those? What information can be inferred from them regarding the nature of the task? What should be done with them when building supervised machine learning systems? We address these questions in the context of a subjective semantic task. In this setting, we show that the presence of such instances in training data misleads a machine learner into misclassifying clear-cut cases. We also show that considering machine learning outcomes with and without the difficult cases, it is possible to identify specific weaknesses of the problem representation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Petra Saskia Bayerl</author>
<author>Karsten Ingmar Paul</author>
</authors>
<title>What determines inter-coder agreement in manual annotations? a meta-analytic investigation.</title>
<date>2011</date>
<journal>Comput. Linguist.,</journal>
<volume>37</volume>
<issue>4</issue>
<contexts>
<context position="17338" citStr="Bayerl and Paul, 2011" startWordPosition="2953" endWordPosition="2956"> be accidentally related to more than one preceding word. We failed to improve performance, however; it is thus left for future work to enrich the representation of the problem so that cases with inappropriate rare senses can be differentiated from the appropriate ones. In the context of the current article, the identification of a particular weakness in the representation is an added value of the analysis of the machine learning performance with and without the difficult cases. 6 Related Work Reliability of annotation is a concern widely discussed in the computational linguistics literature (Bayerl and Paul, 2011; Beigman Klebanov and Beigman, 2009; Artstein and Poesio, 2008; Craggs and McGee Wood, 2005; Di Eugenio and Glass, 2004; Carletta, 1996). Ensuring high reliability is not always feasible, however; the advent of crowdsourcing brought about interest in algorithms for recovering from noisy annotations: Snow et al. (2008), Passonneau and Carpenter (2013) and Raykar et al. (2010) discuss methods for improving over annotator majority vote when estimating the ground truth from multiple noisy annotations. A situation where learning from a small number of carefully chosen examples leads to a better pe</context>
</contexts>
<marker>Bayerl, Paul, 2011</marker>
<rawString>Petra Saskia Bayerl and Karsten Ingmar Paul. 2011. What determines inter-coder agreement in manual annotations? a meta-analytic investigation. Comput. Linguist., 37(4):699–725, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Beigman</author>
<author>Beata Beigman Klebanov</author>
</authors>
<title>Learning with Annotation Noise.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>280--287</pages>
<location>Singapore,</location>
<marker>Beigman, Klebanov, 2009</marker>
<rawString>Eyal Beigman and Beata Beigman Klebanov. 2009. Learning with Annotation Noise. In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics, pages 280–287, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Eyal Beigman</author>
</authors>
<title>From Annotator Agreement to Noise Models.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="17374" citStr="Klebanov and Beigman, 2009" startWordPosition="2958" endWordPosition="2961">e than one preceding word. We failed to improve performance, however; it is thus left for future work to enrich the representation of the problem so that cases with inappropriate rare senses can be differentiated from the appropriate ones. In the context of the current article, the identification of a particular weakness in the representation is an added value of the analysis of the machine learning performance with and without the difficult cases. 6 Related Work Reliability of annotation is a concern widely discussed in the computational linguistics literature (Bayerl and Paul, 2011; Beigman Klebanov and Beigman, 2009; Artstein and Poesio, 2008; Craggs and McGee Wood, 2005; Di Eugenio and Glass, 2004; Carletta, 1996). Ensuring high reliability is not always feasible, however; the advent of crowdsourcing brought about interest in algorithms for recovering from noisy annotations: Snow et al. (2008), Passonneau and Carpenter (2013) and Raykar et al. (2010) discuss methods for improving over annotator majority vote when estimating the ground truth from multiple noisy annotations. A situation where learning from a small number of carefully chosen examples leads to a better performance in classifiers is discusse</context>
</contexts>
<marker>Klebanov, Beigman, 2009</marker>
<rawString>Beata Beigman Klebanov and Eyal Beigman. 2009. From Annotator Agreement to Noise Models. Computational Linguistics, 35(4):493–503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Eli Shamir</author>
</authors>
<title>Reader-based exploration of lexical cohesion.</title>
<date>2006</date>
<journal>Language Resources and Evaluation,</journal>
<volume>40</volume>
<issue>2</issue>
<contexts>
<context position="5215" citStr="Klebanov and Shamir, 2006" startWordPosition="798" endWordPosition="801">ed as new (class 0, or negative). For the project, annotations were collected for 10 texts of various genres, where annotators were asked, for every first appearance of a word in a text, to point out previous words in the text that are semantically or associatively related to it. All data was annotated by 22 undergraduate and graduate students in various disciplines who were recruited for the task. During outlier analysis, data from two annotators was excluded from consideration, while 20 annotations were retained. This task is fairly subjective, with inter-annotator agreement κ=0.45 (Beigman Klebanov and Shamir, 2006). Table 1 shows the number and proportion of instances that received the “semantically old” (1) label from i annotators, for 0 &lt; i &lt; 20. The first column shows the number of annotators who gave the label “semantically old” (1). Column 2 shows the number and proportion of instances that received the label 1 from the number of annotators shown in column 1. Column 3 shows the split into item difficulty groups. We note that while about 20% of the instances received a unanimous 0 annotation and about 12% of the instances received just one 1 label out of 20 annotators, the remaining instances are sp</context>
</contexts>
<marker>Klebanov, Shamir, 2006</marker>
<rawString>Beata Beigman Klebanov and Eli Shamir. 2006. Reader-based exploration of lexical cohesion. Language Resources and Evaluation, 40(2):109–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
</authors>
<title>Measuring Semantic Relatedness Using People and WordNet.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>13--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="9229" citStr="Klebanov, 2006" startWordPosition="1520" endWordPosition="1521">e of the family of classifiers analyzed in Beigman and Beigman Klebanov (2009), where they are theoretically shown to be vulnerable to hard case bias in the worst case. To represent the instances, we use two features that capture semantic relatedness between words. One feature uses Latent Semantic Analysis (Deerwester et al., 1990) trained on the Wall Street Journal articles to quantify the distributional similarity of two words, the other uses an algorithm based on WordNet (Miller, 1990) to calculate semantic relatedness, combining information from both the hierarchy and the glosses (Beigman Klebanov, 2006). For each word, we calculate LSA (WordNet) relatedness score for this word with each preceding word in the text, and report the highest pairwise score as the LSA (WordNet) feature value for the given word. The values of the features can be thought of as quantifying the strength of the evidence for semantic non-newness that could be obtained via a distributional or a dictionary-based method. 1The weight corresponds to the number of people who marked the item as 1, for hard cases. We take a weighted sample of 100 hard cases. 4 Results We calculate the accuracy of every system separately on the </context>
</contexts>
<marker>Klebanov, 2006</marker>
<rawString>Beata Beigman Klebanov. 2006. Measuring Semantic Relatedness Using People and WordNet. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 13–16, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Betty Birner</author>
<author>Gregory Ward</author>
</authors>
<title>Information Status and Non-canonical Word Order in English. Amsterdam/Philadelphia:</title>
<date>1998</date>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="4323" citStr="Birner and Ward, 1998" startWordPosition="650" endWordPosition="653">goals of the project is to investigate the relationship between various kinds of non-novelty in text, and, in particular, the rela390 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 390–396, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tionship between semantic non-novelty (conceptualized as semantic association with some preceding word in the text), the information structure in terms of given and new information, and the cognitive status of discourse entities (Postolache et al., 2005; Birner and Ward, 1998; Gundel et al., 1993; Prince, 1981). If an annotator identified an associative tie from the target word back to some other word in the text, the target word is thereby classified as semantically old (class 1, or positive); if no ties were identified, it is classified as new (class 0, or negative). For the project, annotations were collected for 10 texts of various genres, where annotators were asked, for every first appearance of a word in a text, to point out previous words in the text that are semantically or associatively related to it. All data was annotated by 22 undergraduate and gradua</context>
</contexts>
<marker>Birner, Ward, 1998</marker>
<rawString>Betty Birner and Gregory Ward. 1998. Information Status and Non-canonical Word Order in English. Amsterdam/Philadelphia: John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="17475" citStr="Carletta, 1996" startWordPosition="2976" endWordPosition="2977">the representation of the problem so that cases with inappropriate rare senses can be differentiated from the appropriate ones. In the context of the current article, the identification of a particular weakness in the representation is an added value of the analysis of the machine learning performance with and without the difficult cases. 6 Related Work Reliability of annotation is a concern widely discussed in the computational linguistics literature (Bayerl and Paul, 2011; Beigman Klebanov and Beigman, 2009; Artstein and Poesio, 2008; Craggs and McGee Wood, 2005; Di Eugenio and Glass, 2004; Carletta, 1996). Ensuring high reliability is not always feasible, however; the advent of crowdsourcing brought about interest in algorithms for recovering from noisy annotations: Snow et al. (2008), Passonneau and Carpenter (2013) and Raykar et al. (2010) discuss methods for improving over annotator majority vote when estimating the ground truth from multiple noisy annotations. A situation where learning from a small number of carefully chosen examples leads to a better performance in classifiers is discussed in the active learning literature (Schohn and Cohn, 2000; Cebron and Berthold, 2009; Nguyen and Sme</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Cebron</author>
<author>Michael Berthold</author>
</authors>
<title>Active learning for object classification: From exploration to exploitation. Data Mining and Knowledge Discovery,</title>
<date>2009</date>
<pages>18--283</pages>
<contexts>
<context position="18059" citStr="Cebron and Berthold, 2009" startWordPosition="3064" endWordPosition="3067"> Eugenio and Glass, 2004; Carletta, 1996). Ensuring high reliability is not always feasible, however; the advent of crowdsourcing brought about interest in algorithms for recovering from noisy annotations: Snow et al. (2008), Passonneau and Carpenter (2013) and Raykar et al. (2010) discuss methods for improving over annotator majority vote when estimating the ground truth from multiple noisy annotations. A situation where learning from a small number of carefully chosen examples leads to a better performance in classifiers is discussed in the active learning literature (Schohn and Cohn, 2000; Cebron and Berthold, 2009; Nguyen and Smeulders, 2004; Tong and Koller, 2001). Recent work in the proactive active learning and multi-expert active learning paradigms incorporates considerations of item difficulty and annotator expertise into an active learning scheme (Wallace et al., 2011; Donmez and Carbonell, 2008). In information retrieval, one line of work concerns the design of evaluation schemes that reflect different levels of document relevance to a given query (Kanoulas and Aslam, 2009; Sakai, 2007; Kek¨al¨ainen, 2005; Sormunen, 2002; Voorhees, 2001; J¨arvelin and Kek¨al¨ainen, 2000; Voorhees, 2000). J¨arvel</context>
</contexts>
<marker>Cebron, Berthold, 2009</marker>
<rawString>Nicolas Cebron and Michael Berthold. 2009. Active learning for object classification: From exploration to exploitation. Data Mining and Knowledge Discovery, 18:283–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craggs</author>
<author>Mary McGee Wood</author>
</authors>
<date>2005</date>
<booktitle>Evaluating Discourse and Dialogue Coding Schemes. Computational Linguistics,</booktitle>
<pages>31--3</pages>
<marker>Craggs, Wood, 2005</marker>
<rawString>Richard. Craggs and Mary McGee Wood. 2005. Evaluating Discourse and Dialogue Coding Schemes. Computational Linguistics, 31(3):289–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan Dumais</author>
<author>George Furnas</author>
<author>Thomas Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society For Information Science,</journal>
<pages>41--391</pages>
<contexts>
<context position="8947" citStr="Deerwester et al., 1990" startWordPosition="1474" endWordPosition="1478">ning regimes used in the experiment, according to the parts of the data utilized for training. 3.2 Machine Learning We use linear Support Vector Machines classifier as implemented in SVMLight (Joachims, 1999). Apart from being a popular and powerful machine learning method, linear SVM is one of the family of classifiers analyzed in Beigman and Beigman Klebanov (2009), where they are theoretically shown to be vulnerable to hard case bias in the worst case. To represent the instances, we use two features that capture semantic relatedness between words. One feature uses Latent Semantic Analysis (Deerwester et al., 1990) trained on the Wall Street Journal articles to quantify the distributional similarity of two words, the other uses an algorithm based on WordNet (Miller, 1990) to calculate semantic relatedness, combining information from both the hierarchy and the glosses (Beigman Klebanov, 2006). For each word, we calculate LSA (WordNet) relatedness score for this word with each preceding word in the text, and report the highest pairwise score as the LSA (WordNet) feature value for the given word. The values of the features can be thought of as quantifying the strength of the evidence for semantic non-newne</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan Dumais, George Furnas, Thomas Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society For Information Science, 41:391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Michael Glass</author>
</authors>
<title>The kappa statistic: a second look.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<marker>Di Eugenio, Glass, 2004</marker>
<rawString>Barbara Di Eugenio and Michael Glass. 2004. The kappa statistic: a second look. Computational Linguistics, 30(1):95–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Dligach</author>
<author>Rodney Nielsen</author>
<author>Martha Palmer</author>
</authors>
<title>To Annotate More Accurately or to Annotate More.</title>
<date>2010</date>
<booktitle>In Proceedings of the 4th Linguistic Annotation Workshop,</booktitle>
<pages>64--72</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="13107" citStr="Dligach et al., 2010" startWordPosition="2211" endWordPosition="2214">r Hard 0 have the probability of p=0.71 for class 0. Thus, while the labels are somewhat informative, it is apparently the case that the hard instances are distributed sufficiently differently in the feature space from the easy cases with the same label to produce a hard case bias. Inspecting the distribution of hard cases (Figure 1), we note that hard cases do not follow the worst case pattern analyzed in Beigman and Beigman Klebanov (2009), where they were concentrated in an area of the feature space that was removed far from the separation plane, a malignant but arguably unlikely scenario (Dligach et al., 2010). Here, hard cases are spread both close and far from the plane, yet their distribution is sufficiently different from that of the easy cases to produce hard case bias during learning. Figure 1: Hard cases with separators learned from easy and easy+hard training data. 5.2 The nature of hard cases Figure 1 plots the hard instances in the twodimensional feature space: Latent Semantic Analysis score is shown on x-axis, and WordNet-based score is shown on the y-axis. The red lines show the linear separator induced when the system is trained on easy cases only (system E in Table 3), whereas the gre</context>
</contexts>
<marker>Dligach, Nielsen, Palmer, 2010</marker>
<rawString>Dmitriy Dligach, Rodney Nielsen, and Martha Palmer. 2010. To Annotate More Accurately or to Annotate More. In Proceedings of the 4th Linguistic Annotation Workshop, pages 64–72, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeanette Gundel</author>
<author>Nancy Hedberg</author>
<author>Ron Zacharski</author>
</authors>
<title>Cognitive status and the form of referring expressions in discourse.</title>
<date>1993</date>
<journal>Language,</journal>
<pages>69--274</pages>
<contexts>
<context position="4344" citStr="Gundel et al., 1993" startWordPosition="654" endWordPosition="657"> to investigate the relationship between various kinds of non-novelty in text, and, in particular, the rela390 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 390–396, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tionship between semantic non-novelty (conceptualized as semantic association with some preceding word in the text), the information structure in terms of given and new information, and the cognitive status of discourse entities (Postolache et al., 2005; Birner and Ward, 1998; Gundel et al., 1993; Prince, 1981). If an annotator identified an associative tie from the target word back to some other word in the text, the target word is thereby classified as semantically old (class 1, or positive); if no ties were identified, it is classified as new (class 0, or negative). For the project, annotations were collected for 10 texts of various genres, where annotators were asked, for every first appearance of a word in a text, to point out previous words in the text that are semantically or associatively related to it. All data was annotated by 22 undergraduate and graduate students in variou</context>
</contexts>
<marker>Gundel, Hedberg, Zacharski, 1993</marker>
<rawString>Jeanette Gundel, Nancy Hedberg, and Ron Zacharski. 1993. Cognitive status and the form of referring expressions in discourse. Language, 69:274–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalervo J¨arvelin</author>
<author>Jaana Kek¨al¨ainen</author>
</authors>
<title>IR Evaluation Methods for Retrieving Highly Relevant Documents.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23th Annual International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>41--48</pages>
<location>Athens, Greece,</location>
<marker>J¨arvelin, Kek¨al¨ainen, 2000</marker>
<rawString>Kalervo J¨arvelin and Jaana Kek¨al¨ainen. 2000. IR Evaluation Methods for Retrieving Highly Relevant Documents. In Proceedings of the 23th Annual International Conference on Research and Development in Information Retrieval, pages 41–48, Athens, Greece, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Advances in Kernel Methods - Support Vector Learning.</title>
<date>1999</date>
<booktitle>Making large-scale SVM learning practical,</booktitle>
<pages>169--184</pages>
<editor>In Bernhard Schlkopf, Christopher Burges, and Alexander Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8531" citStr="Joachims, 1999" startWordPosition="1408" endWordPosition="1409">Then various 391 parts of the training data are used to train the 5 systems described in Table 2. We build models using easy data; hard data; easy and hard data; easy, hard, and very hard data; easy data and a weighted sample of the hard data. The labels for very hard data were assigned by flipping a fair coin. System Easy Hard Very Hard E + H + E+H + + E+H+VH + + + E+H100 + sample1 w Table 2: The 5 training regimes used in the experiment, according to the parts of the data utilized for training. 3.2 Machine Learning We use linear Support Vector Machines classifier as implemented in SVMLight (Joachims, 1999). Apart from being a popular and powerful machine learning method, linear SVM is one of the family of classifiers analyzed in Beigman and Beigman Klebanov (2009), where they are theoretically shown to be vulnerable to hard case bias in the worst case. To represent the instances, we use two features that capture semantic relatedness between words. One feature uses Latent Semantic Analysis (Deerwester et al., 1990) trained on the Wall Street Journal articles to quantify the distributional similarity of two words, the other uses an algorithm based on WordNet (Miller, 1990) to calculate semantic r</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Advances in Kernel Methods - Support Vector Learning. In Bernhard Schlkopf, Christopher Burges, and Alexander Smola, editors, Making large-scale SVM learning practical, pages 169–184. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evangelos Kanoulas</author>
<author>Javed Aslam</author>
</authors>
<title>Empirical Justification of the Gain and Discount Function for nDCG .</title>
<date>2009</date>
<booktitle>In Proceedings of the 19th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>611--620</pages>
<location>Hong Kong,</location>
<contexts>
<context position="18534" citStr="Kanoulas and Aslam, 2009" startWordPosition="3136" endWordPosition="3139">xamples leads to a better performance in classifiers is discussed in the active learning literature (Schohn and Cohn, 2000; Cebron and Berthold, 2009; Nguyen and Smeulders, 2004; Tong and Koller, 2001). Recent work in the proactive active learning and multi-expert active learning paradigms incorporates considerations of item difficulty and annotator expertise into an active learning scheme (Wallace et al., 2011; Donmez and Carbonell, 2008). In information retrieval, one line of work concerns the design of evaluation schemes that reflect different levels of document relevance to a given query (Kanoulas and Aslam, 2009; Sakai, 2007; Kek¨al¨ainen, 2005; Sormunen, 2002; Voorhees, 2001; J¨arvelin and Kek¨al¨ainen, 2000; Voorhees, 2000). J¨arvelin and Kek¨al¨ainen (2000) consider, for example, a tiered evaluation scheme, where precision and recall are reported separately for every level of relevance, which is quite analogous to the idea of testing separately on easy and hard cases as employed here. The graded notion of relevance addressed in the information retrieval research assumes a coding scheme where people assign documents into one of the relevance tiers (Kek¨al¨ainen, 2005; Sormunen, 2002). In our case, </context>
</contexts>
<marker>Kanoulas, Aslam, 2009</marker>
<rawString>Evangelos Kanoulas and Javed Aslam. 2009. Empirical Justification of the Gain and Discount Function for nDCG . In Proceedings of the 19th ACM Conference on Information and Knowledge Management, pages 611–620, Hong Kong, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaana Kek¨al¨ainen</author>
</authors>
<title>Binary and graded relevance</title>
<date>2005</date>
<booktitle>in IR evaluations – Comparison of the effects on ranking of IR systems. Information Processing and Management,</booktitle>
<pages>41--1019</pages>
<marker>Kek¨al¨ainen, 2005</marker>
<rawString>Jaana Kek¨al¨ainen. 2005. Binary and graded relevance in IR evaluations – Comparison of the effects on ranking of IR systems. Information Processing and Management, 41:1019–1033.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="9107" citStr="Miller, 1990" startWordPosition="1503" endWordPosition="1504">lemented in SVMLight (Joachims, 1999). Apart from being a popular and powerful machine learning method, linear SVM is one of the family of classifiers analyzed in Beigman and Beigman Klebanov (2009), where they are theoretically shown to be vulnerable to hard case bias in the worst case. To represent the instances, we use two features that capture semantic relatedness between words. One feature uses Latent Semantic Analysis (Deerwester et al., 1990) trained on the Wall Street Journal articles to quantify the distributional similarity of two words, the other uses an algorithm based on WordNet (Miller, 1990) to calculate semantic relatedness, combining information from both the hierarchy and the glosses (Beigman Klebanov, 2006). For each word, we calculate LSA (WordNet) relatedness score for this word with each preceding word in the text, and report the highest pairwise score as the LSA (WordNet) feature value for the given word. The values of the features can be thought of as quantifying the strength of the evidence for semantic non-newness that could be obtained via a distributional or a dictionary-based method. 1The weight corresponds to the number of people who marked the item as 1, for hard </context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George Miller. 1990. WordNet: An on-line lexical database. International Journal of Lexicography, 3(4):235–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hieu Nguyen</author>
<author>Arnold Smeulders</author>
</authors>
<title>Active Learning Using Pre-clustering.</title>
<date>2004</date>
<booktitle>In Proceedings of 21st International Conference on Machine Learning,</booktitle>
<pages>623--630</pages>
<location>Banff, Canada,</location>
<contexts>
<context position="18087" citStr="Nguyen and Smeulders, 2004" startWordPosition="3068" endWordPosition="3072">arletta, 1996). Ensuring high reliability is not always feasible, however; the advent of crowdsourcing brought about interest in algorithms for recovering from noisy annotations: Snow et al. (2008), Passonneau and Carpenter (2013) and Raykar et al. (2010) discuss methods for improving over annotator majority vote when estimating the ground truth from multiple noisy annotations. A situation where learning from a small number of carefully chosen examples leads to a better performance in classifiers is discussed in the active learning literature (Schohn and Cohn, 2000; Cebron and Berthold, 2009; Nguyen and Smeulders, 2004; Tong and Koller, 2001). Recent work in the proactive active learning and multi-expert active learning paradigms incorporates considerations of item difficulty and annotator expertise into an active learning scheme (Wallace et al., 2011; Donmez and Carbonell, 2008). In information retrieval, one line of work concerns the design of evaluation schemes that reflect different levels of document relevance to a given query (Kanoulas and Aslam, 2009; Sakai, 2007; Kek¨al¨ainen, 2005; Sormunen, 2002; Voorhees, 2001; J¨arvelin and Kek¨al¨ainen, 2000; Voorhees, 2000). J¨arvelin and Kek¨al¨ainen (2000) c</context>
</contexts>
<marker>Nguyen, Smeulders, 2004</marker>
<rawString>Hieu Nguyen and Arnold Smeulders. 2004. Active Learning Using Pre-clustering. In Proceedings of 21st International Conference on Machine Learning, pages 623–630, Banff, Canada, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Oosten</author>
<author>Vronique Hoste</author>
<author>Dries Tanghe</author>
</authors>
<title>A posteriori agreement as a quality measure for readability prediction systems.</title>
<date>2011</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>6609</volume>
<pages>424--435</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="2282" citStr="Oosten et al. (2011)" startWordPosition="337" endWordPosition="340">irst author was a post-doctoral fellow at Northwestern University, Evanston, IL and the second author was a visiting assistant professor at Washington University, St. Louis, MO. Eyal Beigman* Liquidnet Holdings Inc. 498 Seventh Avenue New York, NY 10018 e.beigman@gmail.com leads to unstable benchmarking results, as different gold standards might provide conflicting annotations for such items. Reidsma and Carletta (2008) demonstrated by simulation that systematic disagreements between annotators negatively impact generalization ability of classifiers built using data from different annotators. Oosten et al. (2011) showed that judgments of readability of the same texts by different groups of experts are sufficiently systematically different to hamper cross-expert generalization of readability classifiers trained on annotations from different groups. Rehbein and Ruppenhofer (2011) discuss the negative impact of systematic simulated annotation inconsistencies on active learning performance on a word-sense disambiguation task. In this paper, we address the task of classifying words in a text as semantically new or old. Using multiple annotators, we empirically identify instances that show substantial disag</context>
</contexts>
<marker>Oosten, Hoste, Tanghe, 2011</marker>
<rawString>Philip Oosten, Vronique Hoste, and Dries Tanghe. 2011. A posteriori agreement as a quality measure for readability prediction systems. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 6609 of Lecture Notes in Computer Science, pages 424–435. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Bob Carpenter</author>
</authors>
<title>The benefits of a model of annotation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<pages>187--195</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="17691" citStr="Passonneau and Carpenter (2013)" startWordPosition="3006" endWordPosition="3009">r weakness in the representation is an added value of the analysis of the machine learning performance with and without the difficult cases. 6 Related Work Reliability of annotation is a concern widely discussed in the computational linguistics literature (Bayerl and Paul, 2011; Beigman Klebanov and Beigman, 2009; Artstein and Poesio, 2008; Craggs and McGee Wood, 2005; Di Eugenio and Glass, 2004; Carletta, 1996). Ensuring high reliability is not always feasible, however; the advent of crowdsourcing brought about interest in algorithms for recovering from noisy annotations: Snow et al. (2008), Passonneau and Carpenter (2013) and Raykar et al. (2010) discuss methods for improving over annotator majority vote when estimating the ground truth from multiple noisy annotations. A situation where learning from a small number of carefully chosen examples leads to a better performance in classifiers is discussed in the active learning literature (Schohn and Cohn, 2000; Cebron and Berthold, 2009; Nguyen and Smeulders, 2004; Tong and Koller, 2001). Recent work in the proactive active learning and multi-expert active learning paradigms incorporates considerations of item difficulty and annotator expertise into an active lear</context>
</contexts>
<marker>Passonneau, Carpenter, 2013</marker>
<rawString>Rebecca J. Passonneau and Bob Carpenter. 2013. The benefits of a model of annotation. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 187–195, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oana Postolache</author>
<author>Ivana Kruijff-Korbayova</author>
<author>GeertJan Kruijff</author>
</authors>
<title>Data-driven approaches for information structure identification.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>9--16</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="4300" citStr="Postolache et al., 2005" startWordPosition="646" endWordPosition="649">cally old or new. One of goals of the project is to investigate the relationship between various kinds of non-novelty in text, and, in particular, the rela390 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 390–396, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tionship between semantic non-novelty (conceptualized as semantic association with some preceding word in the text), the information structure in terms of given and new information, and the cognitive status of discourse entities (Postolache et al., 2005; Birner and Ward, 1998; Gundel et al., 1993; Prince, 1981). If an annotator identified an associative tie from the target word back to some other word in the text, the target word is thereby classified as semantically old (class 1, or positive); if no ties were identified, it is classified as new (class 0, or negative). For the project, annotations were collected for 10 texts of various genres, where annotators were asked, for every first appearance of a word in a text, to point out previous words in the text that are semantically or associatively related to it. All data was annotated by 22 u</context>
</contexts>
<marker>Postolache, Kruijff-Korbayova, Kruijff, 2005</marker>
<rawString>Oana Postolache, Ivana Kruijff-Korbayova, and GeertJan Kruijff. 2005. Data-driven approaches for information structure identification. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 9–16, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Prince</author>
</authors>
<title>Toward a taxonomy of given-new information.</title>
<date>1981</date>
<booktitle>Radical Pragmatics,</booktitle>
<pages>223--255</pages>
<editor>In Peter Cole, editor,</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="4359" citStr="Prince, 1981" startWordPosition="658" endWordPosition="659">elationship between various kinds of non-novelty in text, and, in particular, the rela390 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 390–396, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tionship between semantic non-novelty (conceptualized as semantic association with some preceding word in the text), the information structure in terms of given and new information, and the cognitive status of discourse entities (Postolache et al., 2005; Birner and Ward, 1998; Gundel et al., 1993; Prince, 1981). If an annotator identified an associative tie from the target word back to some other word in the text, the target word is thereby classified as semantically old (class 1, or positive); if no ties were identified, it is classified as new (class 0, or negative). For the project, annotations were collected for 10 texts of various genres, where annotators were asked, for every first appearance of a word in a text, to point out previous words in the text that are semantically or associatively related to it. All data was annotated by 22 undergraduate and graduate students in various disciplines w</context>
</contexts>
<marker>Prince, 1981</marker>
<rawString>Ellen Prince. 1981. Toward a taxonomy of given-new information. In Peter Cole, editor, Radical Pragmatics, pages 223–255. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas C Raykar</author>
<author>Shipeng Yu</author>
<author>Linda H Zhao</author>
<author>Gerardo Hermosillo Valadez</author>
<author>Charles Florin</author>
<author>Luca Bogoni</author>
<author>Linda Moy</author>
</authors>
<title>Learning from crowds.</title>
<date>2010</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>11--1297</pages>
<contexts>
<context position="17716" citStr="Raykar et al. (2010)" startWordPosition="3011" endWordPosition="3014">an added value of the analysis of the machine learning performance with and without the difficult cases. 6 Related Work Reliability of annotation is a concern widely discussed in the computational linguistics literature (Bayerl and Paul, 2011; Beigman Klebanov and Beigman, 2009; Artstein and Poesio, 2008; Craggs and McGee Wood, 2005; Di Eugenio and Glass, 2004; Carletta, 1996). Ensuring high reliability is not always feasible, however; the advent of crowdsourcing brought about interest in algorithms for recovering from noisy annotations: Snow et al. (2008), Passonneau and Carpenter (2013) and Raykar et al. (2010) discuss methods for improving over annotator majority vote when estimating the ground truth from multiple noisy annotations. A situation where learning from a small number of carefully chosen examples leads to a better performance in classifiers is discussed in the active learning literature (Schohn and Cohn, 2000; Cebron and Berthold, 2009; Nguyen and Smeulders, 2004; Tong and Koller, 2001). Recent work in the proactive active learning and multi-expert active learning paradigms incorporates considerations of item difficulty and annotator expertise into an active learning scheme (Wallace et a</context>
</contexts>
<marker>Raykar, Yu, Zhao, Valadez, Florin, Bogoni, Moy, 2010</marker>
<rawString>Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda Moy. 2010. Learning from crowds. J. Mach. Learn. Res., 11:1297–1322, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef Ruppenhofer</author>
</authors>
<title>Evaluating the impact of coder errors on active learning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>43--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2552" citStr="Rehbein and Ruppenhofer (2011)" startWordPosition="373" endWordPosition="376">man@gmail.com leads to unstable benchmarking results, as different gold standards might provide conflicting annotations for such items. Reidsma and Carletta (2008) demonstrated by simulation that systematic disagreements between annotators negatively impact generalization ability of classifiers built using data from different annotators. Oosten et al. (2011) showed that judgments of readability of the same texts by different groups of experts are sufficiently systematically different to hamper cross-expert generalization of readability classifiers trained on annotations from different groups. Rehbein and Ruppenhofer (2011) discuss the negative impact of systematic simulated annotation inconsistencies on active learning performance on a word-sense disambiguation task. In this paper, we address the task of classifying words in a text as semantically new or old. Using multiple annotators, we empirically identify instances that show substantial disagreement between annotators. We then discuss those both from the linguistic perspective, identifying some characteristics of such cases, and from the perspective of machine learning, showing that the presence of difficult cases in the training data misleads the machine l</context>
<context position="20074" citStr="Rehbein and Ruppenhofer, 2011" startWordPosition="3369" endWordPosition="3372">at to do with them once identified. We addressed this issue in the context of a subjective semantic task. In this setting, we showed that the presence of difficult instances in training data misleads a machine learner into misclassifying clear-cut, easy cases. We also showed that considering machine learning outcomes with and without the difficult cases, it is possible to identify specific weaknesses of the problem representation. Our results align with the literature suggesting that difficult cases in training data can be disruptive (Beigman and Beigman Klebanov, 2009; Schwartz et al., 2011; Rehbein and Ruppenhofer, 2011; Reidsma and Carletta, 2008); yet we also show that investigating their impact on the learning outcomes in some detail can provide insight about the task at hand. The main contribution of this paper is therefore in providing additional empirical evidence in support of the argument put forward in the literature regarding the need to pay attention to problematic, disagreeable instances in annotated data – both from the linguistic and from the machine learning perspectives. 394 Pinar Donmez and Jaime G. Carbonell. 2008. Proactive learning: Cost-sensitive active learning with multiple imperfect o</context>
</contexts>
<marker>Rehbein, Ruppenhofer, 2011</marker>
<rawString>Ines Rehbein and Josef Ruppenhofer. 2011. Evaluating the impact of coder errors on active learning. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 43– 51, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Reidsma</author>
<author>Jean Carletta</author>
</authors>
<title>Reliability Measurement without Limits.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="2085" citStr="Reidsma and Carletta (2008)" startWordPosition="312" endWordPosition="315"> training data. Schwartz et al. (2011) provided an empirical demonstration that the presence of such difficult cases in dependency parsing evaluations 1The work presented in this paper was done when the first author was a post-doctoral fellow at Northwestern University, Evanston, IL and the second author was a visiting assistant professor at Washington University, St. Louis, MO. Eyal Beigman* Liquidnet Holdings Inc. 498 Seventh Avenue New York, NY 10018 e.beigman@gmail.com leads to unstable benchmarking results, as different gold standards might provide conflicting annotations for such items. Reidsma and Carletta (2008) demonstrated by simulation that systematic disagreements between annotators negatively impact generalization ability of classifiers built using data from different annotators. Oosten et al. (2011) showed that judgments of readability of the same texts by different groups of experts are sufficiently systematically different to hamper cross-expert generalization of readability classifiers trained on annotations from different groups. Rehbein and Ruppenhofer (2011) discuss the negative impact of systematic simulated annotation inconsistencies on active learning performance on a word-sense disamb</context>
<context position="20103" citStr="Reidsma and Carletta, 2008" startWordPosition="3373" endWordPosition="3376">ied. We addressed this issue in the context of a subjective semantic task. In this setting, we showed that the presence of difficult instances in training data misleads a machine learner into misclassifying clear-cut, easy cases. We also showed that considering machine learning outcomes with and without the difficult cases, it is possible to identify specific weaknesses of the problem representation. Our results align with the literature suggesting that difficult cases in training data can be disruptive (Beigman and Beigman Klebanov, 2009; Schwartz et al., 2011; Rehbein and Ruppenhofer, 2011; Reidsma and Carletta, 2008); yet we also show that investigating their impact on the learning outcomes in some detail can provide insight about the task at hand. The main contribution of this paper is therefore in providing additional empirical evidence in support of the argument put forward in the literature regarding the need to pay attention to problematic, disagreeable instances in annotated data – both from the linguistic and from the machine learning perspectives. 394 Pinar Donmez and Jaime G. Carbonell. 2008. Proactive learning: Cost-sensitive active learning with multiple imperfect oracles. In Proceedings of the</context>
</contexts>
<marker>Reidsma, Carletta, 2008</marker>
<rawString>Dennis Reidsma and Jean Carletta. 2008. Reliability Measurement without Limits. Computational Linguistics, 34(3):319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Sakai</author>
</authors>
<title>On the reliability of information retrieval metrics based on graded relevance.</title>
<date>2007</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>43--531</pages>
<contexts>
<context position="18547" citStr="Sakai, 2007" startWordPosition="3140" endWordPosition="3141">performance in classifiers is discussed in the active learning literature (Schohn and Cohn, 2000; Cebron and Berthold, 2009; Nguyen and Smeulders, 2004; Tong and Koller, 2001). Recent work in the proactive active learning and multi-expert active learning paradigms incorporates considerations of item difficulty and annotator expertise into an active learning scheme (Wallace et al., 2011; Donmez and Carbonell, 2008). In information retrieval, one line of work concerns the design of evaluation schemes that reflect different levels of document relevance to a given query (Kanoulas and Aslam, 2009; Sakai, 2007; Kek¨al¨ainen, 2005; Sormunen, 2002; Voorhees, 2001; J¨arvelin and Kek¨al¨ainen, 2000; Voorhees, 2000). J¨arvelin and Kek¨al¨ainen (2000) consider, for example, a tiered evaluation scheme, where precision and recall are reported separately for every level of relevance, which is quite analogous to the idea of testing separately on easy and hard cases as employed here. The graded notion of relevance addressed in the information retrieval research assumes a coding scheme where people assign documents into one of the relevance tiers (Kek¨al¨ainen, 2005; Sormunen, 2002). In our case, the graded no</context>
</contexts>
<marker>Sakai, 2007</marker>
<rawString>Tetsuya Sakai. 2007. On the reliability of information retrieval metrics based on graded relevance. Information Processing and Management, 43:531–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Schohn</author>
<author>David Cohn</author>
</authors>
<title>Less is more: Active Learning with Support Vector Mfachines.</title>
<date>2000</date>
<booktitle>In Proceedings of 17th International Conference on Machine Learning,</booktitle>
<pages>839--846</pages>
<location>San Francisco,</location>
<contexts>
<context position="18032" citStr="Schohn and Cohn, 2000" startWordPosition="3060" endWordPosition="3063">nd McGee Wood, 2005; Di Eugenio and Glass, 2004; Carletta, 1996). Ensuring high reliability is not always feasible, however; the advent of crowdsourcing brought about interest in algorithms for recovering from noisy annotations: Snow et al. (2008), Passonneau and Carpenter (2013) and Raykar et al. (2010) discuss methods for improving over annotator majority vote when estimating the ground truth from multiple noisy annotations. A situation where learning from a small number of carefully chosen examples leads to a better performance in classifiers is discussed in the active learning literature (Schohn and Cohn, 2000; Cebron and Berthold, 2009; Nguyen and Smeulders, 2004; Tong and Koller, 2001). Recent work in the proactive active learning and multi-expert active learning paradigms incorporates considerations of item difficulty and annotator expertise into an active learning scheme (Wallace et al., 2011; Donmez and Carbonell, 2008). In information retrieval, one line of work concerns the design of evaluation schemes that reflect different levels of document relevance to a given query (Kanoulas and Aslam, 2009; Sakai, 2007; Kek¨al¨ainen, 2005; Sormunen, 2002; Voorhees, 2001; J¨arvelin and Kek¨al¨ainen, 200</context>
</contexts>
<marker>Schohn, Cohn, 2000</marker>
<rawString>Greg Schohn and David Cohn. 2000. Less is more: Active Learning with Support Vector Mfachines. In Proceedings of 17th International Conference on Machine Learning, pages 839–846, San Francisco, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Schwartz</author>
<author>Omri Abend</author>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>663--672</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1496" citStr="Schwartz et al. (2011)" startWordPosition="226" endWordPosition="229">ses of the problem representation. 1 Introduction The problem of cases that are difficult for annotation received recent attention from both the theoretical and the applied perspectives. Such items might receive contradictory labels, without a clear way of settling the disagreement. Beigman and Beigman Klebanov (2009) showed theoretically that hard cases – items with unreliable annotations – can lead to unfair benchmarking results when found in test data, and, in worst case, to a degradation in a machi74ne learner’s performance on easy, uncontroversial instances if found in the training data. Schwartz et al. (2011) provided an empirical demonstration that the presence of such difficult cases in dependency parsing evaluations 1The work presented in this paper was done when the first author was a post-doctoral fellow at Northwestern University, Evanston, IL and the second author was a visiting assistant professor at Washington University, St. Louis, MO. Eyal Beigman* Liquidnet Holdings Inc. 498 Seventh Avenue New York, NY 10018 e.beigman@gmail.com leads to unstable benchmarking results, as different gold standards might provide conflicting annotations for such items. Reidsma and Carletta (2008) demonstrat</context>
<context position="20043" citStr="Schwartz et al., 2011" startWordPosition="3365" endWordPosition="3368"> identify those, and what to do with them once identified. We addressed this issue in the context of a subjective semantic task. In this setting, we showed that the presence of difficult instances in training data misleads a machine learner into misclassifying clear-cut, easy cases. We also showed that considering machine learning outcomes with and without the difficult cases, it is possible to identify specific weaknesses of the problem representation. Our results align with the literature suggesting that difficult cases in training data can be disruptive (Beigman and Beigman Klebanov, 2009; Schwartz et al., 2011; Rehbein and Ruppenhofer, 2011; Reidsma and Carletta, 2008); yet we also show that investigating their impact on the learning outcomes in some detail can provide insight about the task at hand. The main contribution of this paper is therefore in providing additional empirical evidence in support of the argument put forward in the literature regarding the need to pay attention to problematic, disagreeable instances in annotated data – both from the linguistic and from the machine learning perspectives. 394 Pinar Donmez and Jaime G. Carbonell. 2008. Proactive learning: Cost-sensitive active lea</context>
</contexts>
<marker>Schwartz, Abend, Reichart, Rappoport, 2011</marker>
<rawString>Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rappoport. 2011. Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 663–672, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>254--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: Evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 254–263, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eero Sormunen</author>
</authors>
<title>Liberal relevance criteria of TREC – Counting on negligible documents?</title>
<date>2002</date>
<booktitle>In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>324--330</pages>
<location>Tampere, Finland,</location>
<contexts>
<context position="18583" citStr="Sormunen, 2002" startWordPosition="3144" endWordPosition="3145">cussed in the active learning literature (Schohn and Cohn, 2000; Cebron and Berthold, 2009; Nguyen and Smeulders, 2004; Tong and Koller, 2001). Recent work in the proactive active learning and multi-expert active learning paradigms incorporates considerations of item difficulty and annotator expertise into an active learning scheme (Wallace et al., 2011; Donmez and Carbonell, 2008). In information retrieval, one line of work concerns the design of evaluation schemes that reflect different levels of document relevance to a given query (Kanoulas and Aslam, 2009; Sakai, 2007; Kek¨al¨ainen, 2005; Sormunen, 2002; Voorhees, 2001; J¨arvelin and Kek¨al¨ainen, 2000; Voorhees, 2000). J¨arvelin and Kek¨al¨ainen (2000) consider, for example, a tiered evaluation scheme, where precision and recall are reported separately for every level of relevance, which is quite analogous to the idea of testing separately on easy and hard cases as employed here. The graded notion of relevance addressed in the information retrieval research assumes a coding scheme where people assign documents into one of the relevance tiers (Kek¨al¨ainen, 2005; Sormunen, 2002). In our case, the graded notion of semantic novelty is a possib</context>
</contexts>
<marker>Sormunen, 2002</marker>
<rawString>Eero Sormunen. 2002. Liberal relevance criteria of TREC – Counting on negligible documents? In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 324–330, Tampere, Finland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Tong</author>
<author>Daphne Koller</author>
</authors>
<title>Support Vector Machine active learning with applications to text classification.</title>
<date>2001</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--45</pages>
<contexts>
<context position="18111" citStr="Tong and Koller, 2001" startWordPosition="3073" endWordPosition="3076">h reliability is not always feasible, however; the advent of crowdsourcing brought about interest in algorithms for recovering from noisy annotations: Snow et al. (2008), Passonneau and Carpenter (2013) and Raykar et al. (2010) discuss methods for improving over annotator majority vote when estimating the ground truth from multiple noisy annotations. A situation where learning from a small number of carefully chosen examples leads to a better performance in classifiers is discussed in the active learning literature (Schohn and Cohn, 2000; Cebron and Berthold, 2009; Nguyen and Smeulders, 2004; Tong and Koller, 2001). Recent work in the proactive active learning and multi-expert active learning paradigms incorporates considerations of item difficulty and annotator expertise into an active learning scheme (Wallace et al., 2011; Donmez and Carbonell, 2008). In information retrieval, one line of work concerns the design of evaluation schemes that reflect different levels of document relevance to a given query (Kanoulas and Aslam, 2009; Sakai, 2007; Kek¨al¨ainen, 2005; Sormunen, 2002; Voorhees, 2001; J¨arvelin and Kek¨al¨ainen, 2000; Voorhees, 2000). J¨arvelin and Kek¨al¨ainen (2000) consider, for example, a </context>
</contexts>
<marker>Tong, Koller, 2001</marker>
<rawString>Simon Tong and Daphne Koller. 2001. Support Vector Machine active learning with applications to text classification. Journal of Machine Learning Research, 2:45–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
</authors>
<title>Variations in relevance judgements and the measurement of retrieval effectiveness.</title>
<date>2000</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>36--697</pages>
<contexts>
<context position="18650" citStr="Voorhees, 2000" startWordPosition="3152" endWordPosition="3153">ebron and Berthold, 2009; Nguyen and Smeulders, 2004; Tong and Koller, 2001). Recent work in the proactive active learning and multi-expert active learning paradigms incorporates considerations of item difficulty and annotator expertise into an active learning scheme (Wallace et al., 2011; Donmez and Carbonell, 2008). In information retrieval, one line of work concerns the design of evaluation schemes that reflect different levels of document relevance to a given query (Kanoulas and Aslam, 2009; Sakai, 2007; Kek¨al¨ainen, 2005; Sormunen, 2002; Voorhees, 2001; J¨arvelin and Kek¨al¨ainen, 2000; Voorhees, 2000). J¨arvelin and Kek¨al¨ainen (2000) consider, for example, a tiered evaluation scheme, where precision and recall are reported separately for every level of relevance, which is quite analogous to the idea of testing separately on easy and hard cases as employed here. The graded notion of relevance addressed in the information retrieval research assumes a coding scheme where people assign documents into one of the relevance tiers (Kek¨al¨ainen, 2005; Sormunen, 2002). In our case, the graded notion of semantic novelty is a possible explanation for the observed pattern of annotator responses. 7 C</context>
</contexts>
<marker>Voorhees, 2000</marker>
<rawString>Ellen Voorhees. 2000. Variations in relevance judgements and the measurement of retrieval effectiveness. Information Processing and Management, 36:697–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
</authors>
<title>Evaluation by highly relevant documents.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>74--82</pages>
<location>New Orleans, LA, USA,</location>
<contexts>
<context position="18599" citStr="Voorhees, 2001" startWordPosition="3146" endWordPosition="3147">tive learning literature (Schohn and Cohn, 2000; Cebron and Berthold, 2009; Nguyen and Smeulders, 2004; Tong and Koller, 2001). Recent work in the proactive active learning and multi-expert active learning paradigms incorporates considerations of item difficulty and annotator expertise into an active learning scheme (Wallace et al., 2011; Donmez and Carbonell, 2008). In information retrieval, one line of work concerns the design of evaluation schemes that reflect different levels of document relevance to a given query (Kanoulas and Aslam, 2009; Sakai, 2007; Kek¨al¨ainen, 2005; Sormunen, 2002; Voorhees, 2001; J¨arvelin and Kek¨al¨ainen, 2000; Voorhees, 2000). J¨arvelin and Kek¨al¨ainen (2000) consider, for example, a tiered evaluation scheme, where precision and recall are reported separately for every level of relevance, which is quite analogous to the idea of testing separately on easy and hard cases as employed here. The graded notion of relevance addressed in the information retrieval research assumes a coding scheme where people assign documents into one of the relevance tiers (Kek¨al¨ainen, 2005; Sormunen, 2002). In our case, the graded notion of semantic novelty is a possible explanation f</context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>Ellen Voorhees. 2001. Evaluation by highly relevant documents. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 74–82, New Orleans, LA, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wallace</author>
<author>K Small</author>
<author>C Brodley</author>
<author>T Trikalinos</author>
</authors>
<title>Who Should Label What? Instance Allocation in Multiple Expert Active Learning, chapter 16,</title>
<date>2011</date>
<pages>176--187</pages>
<contexts>
<context position="18324" citStr="Wallace et al., 2011" startWordPosition="3103" endWordPosition="3106">t al. (2010) discuss methods for improving over annotator majority vote when estimating the ground truth from multiple noisy annotations. A situation where learning from a small number of carefully chosen examples leads to a better performance in classifiers is discussed in the active learning literature (Schohn and Cohn, 2000; Cebron and Berthold, 2009; Nguyen and Smeulders, 2004; Tong and Koller, 2001). Recent work in the proactive active learning and multi-expert active learning paradigms incorporates considerations of item difficulty and annotator expertise into an active learning scheme (Wallace et al., 2011; Donmez and Carbonell, 2008). In information retrieval, one line of work concerns the design of evaluation schemes that reflect different levels of document relevance to a given query (Kanoulas and Aslam, 2009; Sakai, 2007; Kek¨al¨ainen, 2005; Sormunen, 2002; Voorhees, 2001; J¨arvelin and Kek¨al¨ainen, 2000; Voorhees, 2000). J¨arvelin and Kek¨al¨ainen (2000) consider, for example, a tiered evaluation scheme, where precision and recall are reported separately for every level of relevance, which is quite analogous to the idea of testing separately on easy and hard cases as employed here. The gr</context>
</contexts>
<marker>Wallace, Small, Brodley, Trikalinos, 2011</marker>
<rawString>B. Wallace, K. Small, C. Brodley, and T. Trikalinos, 2011. Who Should Label What? Instance Allocation in Multiple Expert Active Learning, chapter 16, pages 176–187.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>