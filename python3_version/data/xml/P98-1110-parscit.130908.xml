<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000052">
<title confidence="0.99361">
Term-list Translation
using Mono-lingual Word Co-occurrence Vectors*
</title>
<author confidence="0.93011">
Genichiro Kikui
</author>
<affiliation confidence="0.859782">
NTT Information and Communication Systems Labs.
</affiliation>
<address confidence="0.877807">
1-1 Hikarinooka, Yokosuka-Shi, Kanagawa, Japan
</address>
<email confidence="0.999835">
e-mail: kikui@isl.ntt.co.jp
</email>
<sectionHeader confidence="0.993921" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999628571428571">
A term-list is a list of content words that charac-
terize a consistent text or a concept. This paper
presents a new method for translating a term-list by
using a corpus in the target language. The method
first retrieves alternative translations for each input
word from a bilingual dictionary. It then determines
the most &apos;coherent&apos; combination of alternative trans-
lations, where the coherence of a set of words is
defined as the proximity among multi-dimensional
vectors produced from the words on the basis of
co-occurrence statistics. The method was applied
to term-lists extracted from newspaper articles and
achieved 81% translation accuracy for ambiguous
words (i.e., words with multiple translations).
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985115854166667">
A list of content words, called a term-list, is widely
used as a compact representation of documents in in-
formation retrieval and other document processing.
Automatic translation of term-lists enables this pro-
cessing to be cross-linguistic. This paper presents a
new method for translating term-lists by using co-
occurrence statistics in the target language.
Although there is little study on automatic trans-
lation of term-lists, related studies are found in the
area of target word selection (for content words) in
conventional full-text machine translation (MT).
Approaches for target word selection can be clas-
sified into two types. The first type, which has been
adopted in many commercial MT systems, is based
on hand assembled disambiguation rules, and/or dic-
tionaries. The problem with this approach is that
creating these rules requires much cost and that they
are usually domain-dependent 1.
The second type, called the statistics-based ap-
proach, learns disambiguation knowledge from large
corpora. Brown et al. presented an algorithm that
* This research was done when the author was at Center
for the Study of Language and Information(CSLI), Stanford
University.
&apos;In fact, this is partly shown by the fact that many MT
systems have substitutable domain-dependent (or &amp;quot;user&amp;quot;) dic-
tionaries .
relies on translation probabilities estimated from
large bilingual corpora (Brown et al., 1990)(Brown
et al., 1991). Dagan and Itai (1994) and Tanaka and
Iwasaki (1996) proposed algorithms for selecting tar-
get words by using word co-occurrence statistics in
the target language corpora. The latter algorithms
using mono-lingual corpora are particularly impor-
tant because, at present, we cannot always get a
sufficient amount of bilingual or parallel corpora.
Our method is closely related to (Tanaka and
Iwasaki, 1996) from the viewpoint that they both
rely on mono-lingual corpora only and do not re-
quire any syntactic analysis. The difference is that
our method uses &amp;quot;coherence scores&amp;quot;, which can cap-
ture associative relations between two words which
do not co-occur in the training corpus.
This paper is organized as follows, Section 2 de-
scribes the overall translation process. Section 3
presents a disambiguation algorithm, which is the
core part of our translation method. Section 4 and
5 give experimental results and discussion.
</bodyText>
<sectionHeader confidence="0.994695" genericHeader="method">
2 Term-list Translation
</sectionHeader>
<bodyText confidence="0.922789">
Our term-list translation method consists of two
steps called Dictionary Lookup and Disambiguation.
</bodyText>
<listItem confidence="0.460592">
1. Dictionary Lookup:
</listItem>
<bodyText confidence="0.998571636363636">
For each word in the given term-list, all the al-
ternative translations are retrieved from a bilin-
gual dictionary.
A translation candidate is defined as a combi-
nation of one translation for each input word.
For example, if the input term-list consists of
two words, say w1 and w2, and their transla-
tion include w11 for w1 and w23 for w2, then
(win w23) is a translation candidate. If w1 and
w2 have two and three alternatives respectively
then there are 6 possible translation candidates.
</bodyText>
<listItem confidence="0.645088">
2. Disambiguation:
</listItem>
<footnote confidence="0.4947655">
In this step, all possible translation candidates
are ranked according to a measure that reflects
the &apos;coherence&apos; of each candidate. The top
ranked candidate is the translated term-list.
</footnote>
<page confidence="0.99675">
670
</page>
<bodyText confidence="0.9988615">
In the following sections we concentrate on the
disambiguation step.
</bodyText>
<sectionHeader confidence="0.98921" genericHeader="method">
3 Disambiguation Algorithm
</sectionHeader>
<bodyText confidence="0.999600909090909">
The underlying hypothesis of our disambiguation
method is that a plausible combination of transla-
tion alternatives will be semantically coherent.
In order to find the most coherent combination
of words, we map words onto points in a multidi-
mensional vector space where the &apos;proximity&apos; of two
vectors represents the level of coherence of the corre-
sponding two words. The coherence of n words can
be defined as the order of spatial &apos;concentration&apos; of
the vectors.
The rest of this section formalizes this idea.
</bodyText>
<subsectionHeader confidence="0.9745645">
3.1 Co-occurrence Vector Space: WORD
SPACE
</subsectionHeader>
<bodyText confidence="0.9967973">
We employed a multi-dimensional vector space,
called WORD SPACE (Schuetze, 1997) for defin-
ing the coherence of words. The starting point of
WORD SPACE is to represent a word with an n-
dimensional vector whose i-th element is how many
times the word wi occurs close to the word. For
simplicity, we consider wi and wi to occur close in
context if and only if they appear within an m-word
distance (i.e., the words occur within a window of
m-word length), where m is a predetermined natu-
ral number.
Table 1 shows an artificial example of co-
occurrence statistics. The table shows that the
word ginko (bank, where people deposit money) co-
occurred with shikin (fund) 483 times and with hashi
(bridge) 31 times. Thus the co-occurrence vector
of ginko (money bank) contains 483 as its 89th ele-
ment and 31 as its 468th element. In short, a word
is mapped onto the row vector of the co-occurrence
table (matrix).
</bodyText>
<tableCaption confidence="0.996382">
Table 1: An example of co-occurrence statistics.
</tableCaption>
<bodyText confidence="0.987470304347826">
col. no. . . . 89 ... 468
word shikin .. . hashi
(Eng.) (fund) (bridge)
ginko .. . 483 31
(bank:money)
teibo . .. 120
(bank:river)
Using this word representation, we define the
proximity, prox, of two vectors, ii, b, as the cosine
of the angle between them, given as follows.
prox(ii, 6) â€¢ /7)/(i et II /71) (1)
If two vectors have high proximity then the corre-
sponding two words occur in similar context, and in
our terms, are coherent.
This simple definition, however, has problems,
namely its high-dimensionality and sparseness of
data. In order to solve these problems, the original
co-occurrence vector space is converted into a con-
densed low dimensional real-valued matrix by using
SVD (Singular Value Decomposition). For example,
a 20000-by-1000 matrix can be reduced to a 20000-
by-100 matrix. The resulting vector space is the
WORD SPACE 2.
</bodyText>
<subsectionHeader confidence="0.999985">
3.2 Coherence of Words
</subsectionHeader>
<bodyText confidence="0.993507714285714">
We define the coherence of words in terms of a geo-
metric relationship between the corresponding word
vectors.
As shown above, two vectors with high proximity
are coherent with respect to their associative prop-
erties. We have extended this notion to n-words.
That is, if a group of vectors are concentrated, then
the corresponding words are defined to be coherent.
Conversely, if vectors are scattered, the correspond-
ing words are in-coherent. In this paper, the concen-
tration of vectors is measured by the average prox-
imity from their centroid vector.
Formally, for a given word set W, its coherence
coh(W) is defined as follows:
</bodyText>
<equation confidence="0.9689572">
1
coh(W) = w prox(t7(w), e(W)) (2)
tirw
E fi(w) (3)
wEw
</equation>
<bodyText confidence="0.653921">
the number of words in W (4)
</bodyText>
<subsectionHeader confidence="0.99921">
3.3 Disambiguation Procedure
</subsectionHeader>
<bodyText confidence="0.999942571428572">
Our disambiguation procedure is simply selecting
the combination of translation alternatives that has
the largest coh(W) defined above. The current im-
plementation exhaustively calculates the coherence
score for each combination of translation alterna-
tives, then selects the combination with the highest
score.
</bodyText>
<subsectionHeader confidence="0.952543">
3.4 Example
</subsectionHeader>
<bodyText confidence="0.99990425">
Suppose the given term-list consists of bank and
river. Our method first retrieves translation alter-
natives from the bilingual dictionary. Let the dictio-
nary contain following translations.
</bodyText>
<footnote confidence="0.891294">
2The WORD SPACE method is closely related to La-
tent Semantic Indexing (LSI)(Deerwester et al., 1990), where
document-by-word matrices are processed by SVD instead of
word-by-word matrices. The difference between these two is
discussed in (Schuetze and Pedersen, 1007).
</footnote>
<page confidence="0.97852">
671
</page>
<table confidence="0.605468777777778">
source translations
bank ginko (bank:money),
teibo(bank:river)
interest -4 rishi (interest:money),
kyoumi(interest:feeling)
Combining these translation alternatives yields
four translation candidates:
(ginko, risoku), (ginko, kyoumi),
(teibo, risoku), (teibo, kyoumi).
</table>
<tableCaption confidence="0.982914">
Then the coherence score is calculated for each
candidate.
Table 2 shows scores calculated with the co-
occurrence data used in the translation experiment
(see. Section 4.4.2). The combination of ginko
(bank:money) and risoku(interest:money) has the
highest score. This is consistent with our intuition.
Table 2: An example of scores
</tableCaption>
<table confidence="0.992166">
rank candidate score (coh)
1 (ginko, risoku) 0.930
2 (teibo, kyoumi) 0.897
3 (ginko, kyoumi) 0.839
4 (teibo, risoku) 0.821
</table>
<sectionHeader confidence="0.997898" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999942571428571">
We conducted two types of experiments: re-
translation experiments and translation experi-
ments. Each experiment includes comparison
against the baseline algorithm, which is a unigram-
based translation algorithm. This section presents
the two types of experiments, plus the baseline al-
gorithm, followed by experimental results.
</bodyText>
<subsectionHeader confidence="0.9019">
4.1 Two Types of Experiments
4.1.1 Translation Experiment
</subsectionHeader>
<bodyText confidence="0.9992445">
In the translation experiment, term-lists in one lan-
guage, e.g., English, were translated into another
language, e.g., in Japanese. In this experiment, hu-
mans judged the correctness of outputs.
</bodyText>
<subsubsectionHeader confidence="0.90962">
4.1.2 Re-translation Experiment
</subsubsectionHeader>
<bodyText confidence="0.981934904761905">
Although the translation experiment recreates real
applications, it requires human judgment 3. Thus
we decided to conduct another type of experiment,
called a re-translation experiment. This experiment
translates given term-lists (e.g., in English) into a
second language (e.g., Japanese) and maps them
back onto the source language (e.g., in this case, En-
glish). Thus the correct translation of a term list, in
the most strict sense, is the original term-list itself.
31f a bilingual parallel corpus is available, then correspond-
ing translations could be used for correct results.
This experiment uses two bilingual dictionaries: a
forward dictionary and a backward dictionary.
In this experiment, a word in the given term-list
(e.g. in English) is first mapped to another lan-
guage (e.g., Japanese) by using the forward dictio-
nary. Each translated word is then mapped back
into original language by referring to the backward
dictionary. The union of the translations from the
backward dictionary are the translation alternatives
to be disambiguated.
</bodyText>
<subsectionHeader confidence="0.983521">
4.2 Baseline Algorithm
</subsectionHeader>
<bodyText confidence="0.999846285714286">
The baseline algorithm against which our method
was compared employs unigram probabilities for dis-
ambiguation. For each word in the given term-list,
this algorithm chooses the translation alternative
with the highest unigram probability in the target
language. Note that each word is translated inde-
pendently.
</bodyText>
<subsectionHeader confidence="0.997274">
4.3 Experimental Data
</subsectionHeader>
<bodyText confidence="0.99972435483871">
The source and the target languages of the trans-
lation experiments were English and Japanese re-
spectively. The re-translation experiments were con-
ducted for English term-lists using Japanese as the
second language.
The Japanese-
to-English dictionary was EDICT(Breen, 1995) and
the English-to-Japanese dictionary was an inversion
of the Japanese-to-English dictionary.
The co-occurrence statistics were extracted from
the 1994 New York Times (420MB) for English
and 1990 Nikkei Shinbun (Japanese newspaper)
(150MB) for Japanese. The domains of these texts
range from business to sports. Note that 400 articles
were randomly separated from the former corpus as
the test set.
The initial size of each co-occurrence matrix was
20000-by-1000, where rows and columns correspond
to the 20,000 and 1000 most frequent words in the
corpus4. Each initial matrix was then reduced by us-
ing SVD into a matrix of 20000-by-100 using SVD-
PACKC(Berry et al., 1993).
Term-lists for the experiments were automatically
generated from texts, where a term-list of a docu-
ment consists of the topmost n words ranked by their
tf-idf scores 5 . The relation between the length n of
term-list and the disambiguation accuracy was also
tested.
We prepared two test sets of term-lists: those ex-
tracted from the 400 articles from the New York
Times mentioned above, and those extracted from
</bodyText>
<footnote confidence="0.8571228">
4 Stopwords are ignored.
5The tf-idf score of a word to in a text is tfwlog(*),
where thvis the occurrence of w in the text, N is the num-
ber of documents in the collection, and Nw is the number of
documents containing to.
</footnote>
<page confidence="0.992154">
672
</page>
<bodyText confidence="0.5846165">
articles in Reuters(Reuters, 1997), called Test-NYT,
and Test-REU, respectively.
</bodyText>
<subsectionHeader confidence="0.56982">
4.4 Results
</subsectionHeader>
<subsubsectionHeader confidence="0.495803">
4.4.1 re-translation experiment
</subsubsectionHeader>
<bodyText confidence="0.99991875">
The proposed method was applied to several sets
of term-lists of different length. Results are shown
in Table 3. In this table and the following tables,
&amp;quot;ambiguous&amp;quot; and &amp;quot;success&amp;quot; correspond to the total
number of ambiguous words, not term-lists, and the
number of words that were successfully translated6.
The best results were obtained when the length of
term-lists was 4 or 6. In general, the longer a term-
list becomes, the more information it has. However,
a long term-list tends to be less coherent (i.e., con-
tain different topics). As far as our experiments are
concerned, 4 or 6 was the point of compromise.
</bodyText>
<tableCaption confidence="0.963735">
Table 3: Result of Re-translation for Test-NYT
length success/ambiguous (rate)
</tableCaption>
<figure confidence="0.994709833333333">
2 98/141 (69.5%)
4 240/329 (72.9%)
6 410/555 (73.8%)
8 559/777 (71.9%)
10 691/981 (70.4%)
12 813/1165 (69.8%)
</figure>
<bodyText confidence="0.998824222222222">
Then we compared our method against the base-
line algorithm that was trained on the same set of
articles used to create the co-occurrence matrix for
our algorithm (i.e., New York Times). Both are ap-
plied to term-lists of length 6 made from test-NYT.
The results are shown in Table 4. Although the ab-
solute value of the success rate is not satisfactory,
our method significantly outperforms the baseline
algorithm.
</bodyText>
<tableCaption confidence="0.861836">
Table 4: Result of Re-translation for Test-NYT
Method success/ambiguous (rate)
</tableCaption>
<bodyText confidence="0.894282857142857">
baseline 236/555 (42.5%)
proposed 410/555 (73.8%)
We, then, applied the same method with the same
parameters (i.e., cooccurence and unigram data) to
Test-REU. As shown in Table 5, our method did bet-
ter than the baseline algorithm although the success
rate is lower than the previous result.
</bodyText>
<tableCaption confidence="0.916113857142857">
Table 5: Result of re-translation for Test-REU
Method success/ambiguous (rate)
baseline 162/565 (28.7%)
proposed 351/565 (62.1%)
61f 100 term-lists were processed and each term-list con-
tains 2 ambiguous words, then the &amp;quot;total&amp;quot; becomes 200.
Table 6: Result of Translation for Test-NYT
</tableCaption>
<figure confidence="0.324349333333333">
Method success/ambiguous (rate)
baseline 74/125 (72.6%)
proposed 101/125 (80.8%)
</figure>
<subsubsectionHeader confidence="0.562663">
4.4.2 translation experiment
</subsubsectionHeader>
<bodyText confidence="0.999988526315789">
The translation experiment from English to
Japanese was carried out on Test-NYT. The training
corpus for both proposed and baseline methods was
the Nikkei corpus described above. Outputs were
compared against the &amp;quot;correct data&amp;quot; which were
manually created by removing incorrect alternatives
from all possible alternatives. If all the translation
alternatives in the bilingual dictionary were judged
to be correct, then we counted this word as unam-
biguous.
The accuracy of our method and baseline algo-
rithm are shown on Table6.
The accuracy of our method was 80.8%, about 8
points higher than that of the baseline method. This
shows our method is effective in improving trans-
lation accuracy when syntactic information is not
available. In this experiment, 57% of input words
were unambiguous. Thus the success rates for entire
words were 91.8% (proposed) and 82.6% (baseline).
</bodyText>
<subsectionHeader confidence="0.982228">
4.5 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999538346153846">
The following are two major failure reasons relevant
to our method 7.
The first reason is that alternatives were seman-
tically too similar to be discriminated. For ex-
ample, &amp;quot;share&amp;quot; has at least two Japanese trans-
lations: &amp;quot;shea&amp;quot;(market share) and &amp;quot;kabu&amp;quot;(stock).
Both translations frequently occur in the same con-
text in business articles, and moreover these two
words sometimes co-occur in the same text. Thus,
it is very difficult to discriminate them. In this case,
the task is difficult also for humans unless the origi-
nal text is presented.
The second reason is more complicated. Some
translation alternatives are polysemous in the target
language. If a polysemous word has a very general
meaning that co-occurs with various words, then this
word is more likely to be chosen. This is because the
corresponding vector has &amp;quot;average&amp;quot; value for each
dimension and, thus, has high proximity with the
centroid vector of multiple words.
For example, alternative translations of &amp;quot;stock&amp;quot;
includes two words: &amp;quot;kabu&amp;quot; (company share) and
&amp;quot;dash:&amp;quot; (liquid used for food). The second trans-
lation &amp;quot;dashi&amp;quot; is also a conjugation form of the
Japanese verb &amp;quot;dasu&amp;quot;, which means &amp;quot;put out&amp;quot; and
&amp;quot;start&amp;quot;. In this case, the word, &amp;quot;dashi&amp;quot;, has a cer-
</bodyText>
<footnote confidence="0.978918333333333">
70ther reasons came from errors in pre-processing includ-
ing 1) ignoring compound words, 2) incorrect handling of cap-
italized words etc.
</footnote>
<page confidence="0.998619">
673
</page>
<bodyText confidence="0.9998562">
tam n amount of proximity because of the meaning
irrelevant to the source word, e.g., stock.
This problem was pointed out by (Dagan and Itai,
1994) and they suggested two solutions 1) increas-
ing the size of the (mono-lingual) training corpora
or 2) using bilingual corpora. Another possible solu-
tion is to resolve semantic ambiguities of the training
corpora by using a mono-lingual disambiguation al-
gorithm (e.g., (?)) before making the co-occurrence
matrix.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999969047619048">
Dagan and Itai (1994) proposed a method for choos-
ing target words using mono-lingual corpora. It first
locates pairs of words in dependency relations (e.g.,
verb-object, modifier-noun, etc.), then for each pair,
it chooses the most plausible combination of trans-
lation alternatives. The plausibility of a word-pair is
measured by its co-occurence probability estimated
from corpora in the target language.
One major difference is that their method re-
lies on co-occurrence statistics between tightly and
locally related (i.e., syntactically dependent) word
pairs, whereas ours relies on associative proper-
ties of loosely and more globally related (i.e., co-
occurring within a certain distance) word groups.
Although the former statistics could provide more
accurate information for disambiguation, it requires
huge amounts of data to cover inputs (the data
sparseness problem).
Another difference, which also relates to the data
sparseness problem, is that their method uses &amp;quot;row&amp;quot;
co-occurrence statistics, whereas ours uses statistics
converted with SVD. The converted matrix has the
advantage that it represents the co-occurrence rela-
tionship between two words that share similar con-
texts but do not co-occur in the same text8. SVD
conversion may, however, weaken co-occurrence re-
lations which actually exist in the corpus.
Tanaka and Iwasaki (1996) also proposed a
method for choosing translations that solely relies on
co-occurrence statistics in the target language. The
main difference with our approach lies in the plau-
sibility measure of a translation candidate. Instead
of using a &amp;quot;coherence score&amp;quot;, their method employs
proximity, or inverse distance, between the two co-
occurrence matrices: one from the corpus (in the
target language) and the other from the translation
candidate. The distance measure of two matrices
given in the paper is the sum of the absolute dis-
tance of each corresponding element. This defini-
tion seems to lead the measure to be insensitive to
the candidate when the co-occurrence matrix is filled
with large numbers.
</bodyText>
<sectionHeader confidence="0.991302" genericHeader="method">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999912933333333">
In this paper, we have presented a method for trans-
lating term-lists using mono-lingual corpora.
The proposed method is evaluated by translation
and re-translation experiments and showed a trans-
lation accuracy of 82% for term-lists extracted from
articles ranging from business to sports.
We are planning to apply the proposed method to
cross-linguistic information retrieval (CUR). Since
the method does not rely on syntactic analysis, it
is applicable to translating users&apos; queries as well as
translating term-lists extracted from documents.
A future issue is further evaluation of the pro-
posed method using more data and various criteria
including overall performance of an application sys-
tem (e.g., CLIR).
</bodyText>
<sectionHeader confidence="0.984107" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9070115">
I am grateful to members of the Infomap project at
CSLI, Stanford for their kind support and discus-
sions. In particular I would like to thank Stanley
Peters and Raymond Flournoy.
</bodyText>
<sectionHeader confidence="0.991322" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99855135483871">
M.W. Berry, T. Do, G. O&apos;Brien, V. Krishna,
and S. Varadhan. 1993. SVDPACKC USER&apos;S
GUIDE. Tech. Rep. CS-93-194, University of Ten-
nessee, Knoxville, TN,.
J.W. Breen. 1995. EDICT, Freeware, Japanese-to-
English Dictionary.
P. Brown, J. Cocke, V. Della Pietra, F. Jelinek, R.L.
Mercer, and P. C. Roosin. 1990. A statistical
approach to language translation. Computational
Linguistics, 16(2).
P. Brown, V. Della Pietra, and R.L. Mercer. 1991.
Word sense disambiguation using statisical meth-
ods. In Proceedings of ACL-91.
I. Dagan and A. Itai. 1994. Word sense disambigua-
tion using a second language monolingual corpus.
Computational Linguistics.
S. Deerwester, S.T. Dumais, and R. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of American Society for Information Science.
Reuters. 1997. Reuters-21578, Distribution 1.0.
available at http://www.research.att.comrlewis.
H. Schuetze and Jan 0. Pedersen. 1997. A
cooccurrence-based thesaurus and two applica-
tions to information retrieval. Information Pro-
cessing &amp; Management.
H. Schuetze. 1997. Ambiguity Resolution in Lan-
guage Learning. CSLI.
K. Tanaka and H. Iwasaki. 1996. Extraction of lexi-
cal translations from non-aligned corpora. In Pro-
ceedings of COLING-96.
&amp;quot;Second order co-occurrence&amp;quot;. See (Schuetze, 1997)
</reference>
<page confidence="0.99873">
674
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925030">
<title confidence="0.9973905">Term-list Translation using Mono-lingual Word Co-occurrence Vectors*</title>
<author confidence="0.994112">Genichiro Kikui</author>
<affiliation confidence="0.999772">NTT Information and Communication Systems Labs.</affiliation>
<address confidence="0.999507">1-1 Hikarinooka, Yokosuka-Shi, Kanagawa, Japan</address>
<email confidence="0.999239">e-mail:kikui@isl.ntt.co.jp</email>
<abstract confidence="0.995619466666667">A term-list is a list of content words that characterize a consistent text or a concept. This paper presents a new method for translating a term-list by using a corpus in the target language. The method first retrieves alternative translations for each input word from a bilingual dictionary. It then determines the most &apos;coherent&apos; combination of alternative translations, where the coherence of a set of words is defined as the proximity among multi-dimensional vectors produced from the words on the basis of co-occurrence statistics. The method was applied to term-lists extracted from newspaper articles and achieved 81% translation accuracy for ambiguous words (i.e., words with multiple translations).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M W Berry</author>
<author>T Do</author>
<author>G O&apos;Brien</author>
<author>V Krishna</author>
<author>S Varadhan</author>
</authors>
<title>SVDPACKC USER&apos;S GUIDE.</title>
<date>1993</date>
<tech>Tech. Rep. CS-93-194,</tech>
<institution>University of Tennessee,</institution>
<location>Knoxville, TN,.</location>
<contexts>
<context position="11818" citStr="Berry et al., 1993" startWordPosition="1818" endWordPosition="1822">on of the Japanese-to-English dictionary. The co-occurrence statistics were extracted from the 1994 New York Times (420MB) for English and 1990 Nikkei Shinbun (Japanese newspaper) (150MB) for Japanese. The domains of these texts range from business to sports. Note that 400 articles were randomly separated from the former corpus as the test set. The initial size of each co-occurrence matrix was 20000-by-1000, where rows and columns correspond to the 20,000 and 1000 most frequent words in the corpus4. Each initial matrix was then reduced by using SVD into a matrix of 20000-by-100 using SVDPACKC(Berry et al., 1993). Term-lists for the experiments were automatically generated from texts, where a term-list of a document consists of the topmost n words ranked by their tf-idf scores 5 . The relation between the length n of term-list and the disambiguation accuracy was also tested. We prepared two test sets of term-lists: those extracted from the 400 articles from the New York Times mentioned above, and those extracted from 4 Stopwords are ignored. 5The tf-idf score of a word to in a text is tfwlog(*), where thvis the occurrence of w in the text, N is the number of documents in the collection, and Nw is the </context>
</contexts>
<marker>Berry, Do, O&apos;Brien, Krishna, Varadhan, 1993</marker>
<rawString>M.W. Berry, T. Do, G. O&apos;Brien, V. Krishna, and S. Varadhan. 1993. SVDPACKC USER&apos;S GUIDE. Tech. Rep. CS-93-194, University of Tennessee, Knoxville, TN,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Breen</author>
</authors>
<date>1995</date>
<journal>EDICT, Freeware, Japanese-toEnglish Dictionary.</journal>
<contexts>
<context position="11145" citStr="Breen, 1995" startWordPosition="1716" endWordPosition="1717">lgorithm The baseline algorithm against which our method was compared employs unigram probabilities for disambiguation. For each word in the given term-list, this algorithm chooses the translation alternative with the highest unigram probability in the target language. Note that each word is translated independently. 4.3 Experimental Data The source and the target languages of the translation experiments were English and Japanese respectively. The re-translation experiments were conducted for English term-lists using Japanese as the second language. The Japaneseto-English dictionary was EDICT(Breen, 1995) and the English-to-Japanese dictionary was an inversion of the Japanese-to-English dictionary. The co-occurrence statistics were extracted from the 1994 New York Times (420MB) for English and 1990 Nikkei Shinbun (Japanese newspaper) (150MB) for Japanese. The domains of these texts range from business to sports. Note that 400 articles were randomly separated from the former corpus as the test set. The initial size of each co-occurrence matrix was 20000-by-1000, where rows and columns correspond to the 20,000 and 1000 most frequent words in the corpus4. Each initial matrix was then reduced by u</context>
</contexts>
<marker>Breen, 1995</marker>
<rawString>J.W. Breen. 1995. EDICT, Freeware, Japanese-toEnglish Dictionary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>V Della Pietra</author>
<author>F Jelinek</author>
<author>R L Mercer</author>
<author>P C Roosin</author>
</authors>
<title>A statistical approach to language translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="2335" citStr="Brown et al., 1990" startWordPosition="344" endWordPosition="347">he problem with this approach is that creating these rules requires much cost and that they are usually domain-dependent 1. The second type, called the statistics-based approach, learns disambiguation knowledge from large corpora. Brown et al. presented an algorithm that * This research was done when the author was at Center for the Study of Language and Information(CSLI), Stanford University. &apos;In fact, this is partly shown by the fact that many MT systems have substitutable domain-dependent (or &amp;quot;user&amp;quot;) dictionaries . relies on translation probabilities estimated from large bilingual corpora (Brown et al., 1990)(Brown et al., 1991). Dagan and Itai (1994) and Tanaka and Iwasaki (1996) proposed algorithms for selecting target words by using word co-occurrence statistics in the target language corpora. The latter algorithms using mono-lingual corpora are particularly important because, at present, we cannot always get a sufficient amount of bilingual or parallel corpora. Our method is closely related to (Tanaka and Iwasaki, 1996) from the viewpoint that they both rely on mono-lingual corpora only and do not require any syntactic analysis. The difference is that our method uses &amp;quot;coherence scores&amp;quot;, which </context>
</contexts>
<marker>Brown, Cocke, Pietra, Jelinek, Mercer, Roosin, 1990</marker>
<rawString>P. Brown, J. Cocke, V. Della Pietra, F. Jelinek, R.L. Mercer, and P. C. Roosin. 1990. A statistical approach to language translation. Computational Linguistics, 16(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>V Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>Word sense disambiguation using statisical methods.</title>
<date>1991</date>
<booktitle>In Proceedings of ACL-91.</booktitle>
<contexts>
<context position="2355" citStr="Brown et al., 1991" startWordPosition="347" endWordPosition="350"> approach is that creating these rules requires much cost and that they are usually domain-dependent 1. The second type, called the statistics-based approach, learns disambiguation knowledge from large corpora. Brown et al. presented an algorithm that * This research was done when the author was at Center for the Study of Language and Information(CSLI), Stanford University. &apos;In fact, this is partly shown by the fact that many MT systems have substitutable domain-dependent (or &amp;quot;user&amp;quot;) dictionaries . relies on translation probabilities estimated from large bilingual corpora (Brown et al., 1990)(Brown et al., 1991). Dagan and Itai (1994) and Tanaka and Iwasaki (1996) proposed algorithms for selecting target words by using word co-occurrence statistics in the target language corpora. The latter algorithms using mono-lingual corpora are particularly important because, at present, we cannot always get a sufficient amount of bilingual or parallel corpora. Our method is closely related to (Tanaka and Iwasaki, 1996) from the viewpoint that they both rely on mono-lingual corpora only and do not require any syntactic analysis. The difference is that our method uses &amp;quot;coherence scores&amp;quot;, which can capture associat</context>
</contexts>
<marker>Brown, Pietra, Mercer, 1991</marker>
<rawString>P. Brown, V. Della Pietra, and R.L. Mercer. 1991. Word sense disambiguation using statisical methods. In Proceedings of ACL-91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>A Itai</author>
</authors>
<title>Word sense disambiguation using a second language monolingual corpus. Computational Linguistics.</title>
<date>1994</date>
<contexts>
<context position="2378" citStr="Dagan and Itai (1994)" startWordPosition="351" endWordPosition="354">ating these rules requires much cost and that they are usually domain-dependent 1. The second type, called the statistics-based approach, learns disambiguation knowledge from large corpora. Brown et al. presented an algorithm that * This research was done when the author was at Center for the Study of Language and Information(CSLI), Stanford University. &apos;In fact, this is partly shown by the fact that many MT systems have substitutable domain-dependent (or &amp;quot;user&amp;quot;) dictionaries . relies on translation probabilities estimated from large bilingual corpora (Brown et al., 1990)(Brown et al., 1991). Dagan and Itai (1994) and Tanaka and Iwasaki (1996) proposed algorithms for selecting target words by using word co-occurrence statistics in the target language corpora. The latter algorithms using mono-lingual corpora are particularly important because, at present, we cannot always get a sufficient amount of bilingual or parallel corpora. Our method is closely related to (Tanaka and Iwasaki, 1996) from the viewpoint that they both rely on mono-lingual corpora only and do not require any syntactic analysis. The difference is that our method uses &amp;quot;coherence scores&amp;quot;, which can capture associative relations between t</context>
<context position="16968" citStr="Dagan and Itai, 1994" startWordPosition="2646" endWordPosition="2649">th the centroid vector of multiple words. For example, alternative translations of &amp;quot;stock&amp;quot; includes two words: &amp;quot;kabu&amp;quot; (company share) and &amp;quot;dash:&amp;quot; (liquid used for food). The second translation &amp;quot;dashi&amp;quot; is also a conjugation form of the Japanese verb &amp;quot;dasu&amp;quot;, which means &amp;quot;put out&amp;quot; and &amp;quot;start&amp;quot;. In this case, the word, &amp;quot;dashi&amp;quot;, has a cer70ther reasons came from errors in pre-processing including 1) ignoring compound words, 2) incorrect handling of capitalized words etc. 673 tam n amount of proximity because of the meaning irrelevant to the source word, e.g., stock. This problem was pointed out by (Dagan and Itai, 1994) and they suggested two solutions 1) increasing the size of the (mono-lingual) training corpora or 2) using bilingual corpora. Another possible solution is to resolve semantic ambiguities of the training corpora by using a mono-lingual disambiguation algorithm (e.g., (?)) before making the co-occurrence matrix. 5 Related Work Dagan and Itai (1994) proposed a method for choosing target words using mono-lingual corpora. It first locates pairs of words in dependency relations (e.g., verb-object, modifier-noun, etc.), then for each pair, it chooses the most plausible combination of translation alt</context>
</contexts>
<marker>Dagan, Itai, 1994</marker>
<rawString>I. Dagan and A. Itai. 1994. Word sense disambiguation using a second language monolingual corpus. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of American Society for Information Science. Reuters.</journal>
<contexts>
<context position="7949" citStr="Deerwester et al., 1990" startWordPosition="1256" endWordPosition="1259">n Procedure Our disambiguation procedure is simply selecting the combination of translation alternatives that has the largest coh(W) defined above. The current implementation exhaustively calculates the coherence score for each combination of translation alternatives, then selects the combination with the highest score. 3.4 Example Suppose the given term-list consists of bank and river. Our method first retrieves translation alternatives from the bilingual dictionary. Let the dictionary contain following translations. 2The WORD SPACE method is closely related to Latent Semantic Indexing (LSI)(Deerwester et al., 1990), where document-by-word matrices are processed by SVD instead of word-by-word matrices. The difference between these two is discussed in (Schuetze and Pedersen, 1007). 671 source translations bank ginko (bank:money), teibo(bank:river) interest -4 rishi (interest:money), kyoumi(interest:feeling) Combining these translation alternatives yields four translation candidates: (ginko, risoku), (ginko, kyoumi), (teibo, risoku), (teibo, kyoumi). Then the coherence score is calculated for each candidate. Table 2 shows scores calculated with the cooccurrence data used in the translation experiment (see.</context>
</contexts>
<marker>Deerwester, Dumais, Harshman, 1990</marker>
<rawString>S. Deerwester, S.T. Dumais, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of American Society for Information Science. Reuters. 1997. Reuters-21578, Distribution 1.0. available at http://www.research.att.comrlewis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schuetze</author>
<author>Jan</author>
</authors>
<title>A cooccurrence-based thesaurus and two applications to information retrieval.</title>
<date>1997</date>
<journal>Information Processing &amp; Management.</journal>
<marker>Schuetze, Jan, 1997</marker>
<rawString>H. Schuetze and Jan 0. Pedersen. 1997. A cooccurrence-based thesaurus and two applications to information retrieval. Information Processing &amp; Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schuetze</author>
</authors>
<title>Ambiguity Resolution in Language Learning.</title>
<date>1997</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="4849" citStr="Schuetze, 1997" startWordPosition="741" endWordPosition="742">pothesis of our disambiguation method is that a plausible combination of translation alternatives will be semantically coherent. In order to find the most coherent combination of words, we map words onto points in a multidimensional vector space where the &apos;proximity&apos; of two vectors represents the level of coherence of the corresponding two words. The coherence of n words can be defined as the order of spatial &apos;concentration&apos; of the vectors. The rest of this section formalizes this idea. 3.1 Co-occurrence Vector Space: WORD SPACE We employed a multi-dimensional vector space, called WORD SPACE (Schuetze, 1997) for defining the coherence of words. The starting point of WORD SPACE is to represent a word with an ndimensional vector whose i-th element is how many times the word wi occurs close to the word. For simplicity, we consider wi and wi to occur close in context if and only if they appear within an m-word distance (i.e., the words occur within a window of m-word length), where m is a predetermined natural number. Table 1 shows an artificial example of cooccurrence statistics. The table shows that the word ginko (bank, where people deposit money) cooccurred with shikin (fund) 483 times and with h</context>
</contexts>
<marker>Schuetze, 1997</marker>
<rawString>H. Schuetze. 1997. Ambiguity Resolution in Language Learning. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tanaka</author>
<author>H Iwasaki</author>
</authors>
<title>Extraction of lexical translations from non-aligned corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING-96.</booktitle>
<contexts>
<context position="2408" citStr="Tanaka and Iwasaki (1996)" startWordPosition="356" endWordPosition="359"> much cost and that they are usually domain-dependent 1. The second type, called the statistics-based approach, learns disambiguation knowledge from large corpora. Brown et al. presented an algorithm that * This research was done when the author was at Center for the Study of Language and Information(CSLI), Stanford University. &apos;In fact, this is partly shown by the fact that many MT systems have substitutable domain-dependent (or &amp;quot;user&amp;quot;) dictionaries . relies on translation probabilities estimated from large bilingual corpora (Brown et al., 1990)(Brown et al., 1991). Dagan and Itai (1994) and Tanaka and Iwasaki (1996) proposed algorithms for selecting target words by using word co-occurrence statistics in the target language corpora. The latter algorithms using mono-lingual corpora are particularly important because, at present, we cannot always get a sufficient amount of bilingual or parallel corpora. Our method is closely related to (Tanaka and Iwasaki, 1996) from the viewpoint that they both rely on mono-lingual corpora only and do not require any syntactic analysis. The difference is that our method uses &amp;quot;coherence scores&amp;quot;, which can capture associative relations between two words which do not co-occur</context>
<context position="18645" citStr="Tanaka and Iwasaki (1996)" startWordPosition="2896" endWordPosition="2899"> statistics could provide more accurate information for disambiguation, it requires huge amounts of data to cover inputs (the data sparseness problem). Another difference, which also relates to the data sparseness problem, is that their method uses &amp;quot;row&amp;quot; co-occurrence statistics, whereas ours uses statistics converted with SVD. The converted matrix has the advantage that it represents the co-occurrence relationship between two words that share similar contexts but do not co-occur in the same text8. SVD conversion may, however, weaken co-occurrence relations which actually exist in the corpus. Tanaka and Iwasaki (1996) also proposed a method for choosing translations that solely relies on co-occurrence statistics in the target language. The main difference with our approach lies in the plausibility measure of a translation candidate. Instead of using a &amp;quot;coherence score&amp;quot;, their method employs proximity, or inverse distance, between the two cooccurrence matrices: one from the corpus (in the target language) and the other from the translation candidate. The distance measure of two matrices given in the paper is the sum of the absolute distance of each corresponding element. This definition seems to lead the me</context>
</contexts>
<marker>Tanaka, Iwasaki, 1996</marker>
<rawString>K. Tanaka and H. Iwasaki. 1996. Extraction of lexical translations from non-aligned corpora. In Proceedings of COLING-96.</rawString>
</citation>
<citation valid="true">
<title>Second order co-occurrence&amp;quot;. See (Schuetze,</title>
<date>1997</date>
<marker>1997</marker>
<rawString>&amp;quot;Second order co-occurrence&amp;quot;. See (Schuetze, 1997)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>