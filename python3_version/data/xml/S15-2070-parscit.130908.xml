<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008558">
<title confidence="0.954363">
ULisboa: Recognition and Normalization of Medical Concepts
</title>
<author confidence="0.98904">
Andr´e Leal+, Bruno Martins∗, and Francisco M. Couto+
</author>
<affiliation confidence="0.47114">
+LASIGE, Faculdade de Ciˆencias, Universidade de Lisboa, 1749-016 Lisboa, Portugal.
∗INESC-ID, Instituto Superior T´ecnico, Universidade de Lisboa, Portugal
</affiliation>
<email confidence="0.619268">
aleal@lasige.di.fc.ul.pt , bruno.g.martins@ist.ul.pt , fcouto@di.fc.ul.pt
</email>
<sectionHeader confidence="0.977838" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965434782609">
This paper describes a system developed for
the disorder identification subtask within task
14 of SemEval 2015. The developed sys-
tem is based on a chain of two modules, one
for recognition and another for normaliza-
tion. The recognition module is based on an
adapted version of the Stanford NER system
to train CRF models in order to recognize dis-
order mentions. CRF models were build based
on a novel encoding of entity spans as token
classifications to also consider non-continuous
entities, along with a rich set of features based
on (i) domain lexicons and (ii) Brown clus-
ters inferred from a large collection of clinical
texts. For disorder normalization, we (i) gen-
erated a non ambiguous dictionary of abbrevi-
ations from the labelled files, using it together
with (ii) an heuristic method based on similar-
ity search and (iii) a comparison method based
on the information content of each disorder.
The system achieved an F-measure of 0.740
(the second best), with a precision of 0.779, a
recall of 0.705.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9985945">
Clinical notes are an important source of informa-
tion recorded by medical professionals. However,
this information, when available, is not easily acces-
sible within automated procedures. Clinical notes
are inherently complex, due to their lack of struc-
ture (i.e., narrative language) and due to the need for
contextual interpretation. To address this complex-
ity, text mining approaches represent an effective so-
lution to assist the users in retrieving and extracting
the required information.
This paper presents a text mining system for pro-
cessing clinical text, that we developed for SemEval
based on a pipeline with two modules, one for entity
recognition and another for normalization.
The entity recognition module is based on the
Stanford NER tool (Finkel et al., 2005), and it uses
CRF models trained on annotated biomedical notes.
The module tags the text according to an SBIEON
encoding of entities as token classes, supporting the
recognition of non-continuous entities (Leal et al.,
2014). We relied on features based on Brown clus-
ters and domain specific lexicons. Thus, this ap-
proach combines both supervised (Stanford NER)
and unsupervised methods (Brown Clusters).
For practical applications, entity recognition is
incomplete without performing normalization, i.e.
without mapping each entity to an identifier (CUI) in
a controlled vocabulary like SNOMED CT (Cornet
and Keizer, 2008), that defines its semantic meaning.
One of the main challenges in this task consists in re-
solving the ambiguous cases, where the same entity
can have distinct semantic meanings (i.e., mapped to
distinct CUIs) depending on the context.
Our normalization module relies on the follow-
ing components: (i) a procedure for the automatic
generation of auxiliary dictionaries from the la-
belled training data (e.g, abbreviations) and from
SNOMED CT, to be used as mapping dictionaries,
(ii) an heuristic for similarity search, and (iii) an in-
formation content measure for each concept.
Our system is an extension of the one used in the
2014 edition of SemEval (Leal et al., 2014). Both
systems used the same approach for entity recogni-
tion but, in terms of the normalization component,
the system from 2014 was entirely based on a lex-
ical similarity approach using NGram, Levenstein
</bodyText>
<page confidence="0.987169">
406
</page>
<bodyText confidence="0.831498571428572">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 406–411,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
and JaroWinkler distances. The current system is in-
stead based on a pipeline were the information con-
tent was also incorporated. Besides SNOMED CT,
the current system also integrated dictionaries auto-
matically generated from the training data.
</bodyText>
<sectionHeader confidence="0.806302" genericHeader="introduction">
2 The SemEval Task
</sectionHeader>
<bodyText confidence="0.939670147058824">
Task 14 of SemEval 2015 was composed of two
subtasks: recognition and normalization of medical
concepts (subtask 1) and disorder slot filling (sub-
task 2). We only participated in subtask 1.
The recognition part of subtask 1 consisted on
performing the recognition of medical concepts,
who belong to the UMLS semantic group disor-
ders, within unstructured clinical notes. The disor-
ders group of UMLS corresponds to concepts de-
fined within SNOMED CT (Cornet and Keizer,
2008). Recognized entities can be continuous, non-
continuous or even overlapped in the text.
The normalization part consisted on the mapping
of an unique UMLS CUI (Concept Unique Identi-
fier) to each previously recognized entity, or none
at all (CUI-Less) for the cases where there is no
suitable CUI for the recognized entity within the
SNOMED CT database. Ambiguous entities repre-
sent the main challenge of this task, since identifying
the correct CUI depends on their context.
Task 14 evaluated the recognition and normaliza-
tion parts as one single task, by measuring the final
system’s precision, recall and F-measure. The eval-
uation could also be performed in a strict or relaxed
way. In strict evaluation, a predicted mention is con-
sidered a true positive if the predicted span is ex-
actly the same as the gold-standard. On the relaxed
evaluation, the predicted spans only need to overlap
the gold-standard spans to be considered a true pos-
itive. On both evaluation methods the CUI must be
correctly identified to be considered a true positive.
Thus, even with a perfect recognition system, it is
possible to achieve low results on the task, depend-
ing on the normalization performance
</bodyText>
<sectionHeader confidence="0.999005" genericHeader="method">
3 Datasets
</sectionHeader>
<bodyText confidence="0.988892076923077">
Similarly to the last edition of the competition
(Zhang et al., 2014), two sets of labelled data were
given to the participants, which were separated into
two categories (training and development). They
were used for training and testing of our system,
respectively. Unlabelled clinical notes from the
MIMIC corpus were also provided. Later, an un-
labelled test set was released to evaluate the final
system. Unlabelled clinical notes consisted on plain
text without any additional information, while la-
belled clinical notes consist on plain text together
with a list of disorder mentions contained on them.
Table 1 summarizes each dataset.
</bodyText>
<table confidence="0.990055285714286">
Train Devel Test Unlabelled
Notes 298 133 100 404k
Words 182k 154k 8k 123M
Disorder 11.5k 8k - -
Mentions
CUI-ied 8k (88%) 6k (76%) - -
CUI-less 3.5k (12%) 2k (24%) - -
</table>
<tableCaption confidence="0.999442">
Table 1: Statistical characterization of the datasets.
</tableCaption>
<sectionHeader confidence="0.985892" genericHeader="method">
4 Entity Recognition
</sectionHeader>
<bodyText confidence="0.999977363636364">
We applied the same type of approach used in our
system from last year (Leal et al., 2014) for entity
recognition. The Stanford NER software (Finkel
et al., 2005) was used to train Conditional Random
Fields (CRF) models using labelled data as input.
All input text had to be tokenized and encoded
according to a named entity recognition scheme that
encodes entities as token classifications. To be able
to recognize non-continuous entities, an SBIEON
(Leal et al., 2014) encoding was used. Besides the
tags defined in the SBIEO encoding (Ratinov and
Roth, 2009), a new tag N was added to identify
words that do not belong to the entity but are inside
the continuous span that contains the recognized en-
tity. The remain tags are used to identify Single enti-
ties, the Begin, Inside and End token of a non-single
token entity, and the Other tag for words which
are neither entities nor related to them. For over-
lapped entities we did not develop any approach, i.e.
we only recognize the first entity in an overlapping
group of entities. Thus, handling overlapping enti-
ties remains an open issue in our system.
</bodyText>
<subsectionHeader confidence="0.99836">
4.1 Recognition Features
</subsectionHeader>
<bodyText confidence="0.996858333333333">
We generated 2nd-order CRF models by using, as
training data, the labelled notes together with a rich
set of features. In 2nd-order models, the features
</bodyText>
<page confidence="0.996958">
407
</page>
<figureCaption confidence="0.999716">
Figure 1: Overview on the normalization approach.
</figureCaption>
<bodyText confidence="0.993861771428572">
are computed from representations composed by the
current class and the two previous/next classes.
Training Data: Two different sets of data were
employed: one with notes belonging to the training
set only, and another with notes from both the train-
ing and devel sets.
Brown Clusters: We inferred word representa-
tions in the form of Brown clusters (Brown et al.,
1992) from all data that was made available, i.e.
from MIMIC, train and devel. According to (Turian
et al., 2009), this technique reduces the data sparsity,
generating lower-dimensional representations of the
word vocabulary, and therefore increasing the accu-
racy. Each word cluster contains a group of words,
and clusters are formed by maximizing the mutual
information of bi-grams, according to a class-based
language model. We used a total of 404k documents,
containing an approximate total of 123M tokens, to
infer 100 different clusters using the implementation
provided by (Turian et al., 2010). The number of
clusters was chosen through a separate set of exper-
iments as the one that maximized the F-measure.
Encoding: The aforementioned SBIEON encod-
ing was employed in all recognition models.
Features: The CRF models rely on a set of fea-
tures that includes (i) word tokens within a window
of size 2, (ii) token shape (upper-cased, numeric,
etc), (iii) token position in a sentence and (iv) token
prefixes and suffixes. This basic set of features was
also extended with features based on Brown clusters,
and domain-specific lexicons.
Domain-specific lexicon: We built lexicons for
the medical domain that include (i) SNOMED CT
disorders, (ii) drugs and diseases from DBPedia and
(iii) a list of disorders from the labelled data.
</bodyText>
<sectionHeader confidence="0.996854" genericHeader="method">
5 Normalization
</sectionHeader>
<bodyText confidence="0.999948647058824">
Each recognized entity needs to be normalized, if
possible, with a unique identifier (CUI) from an ex-
isting controlled vocabulary. This way, a semantic
meaning is associated to each entity. Since ambigu-
ous entities can have multiple identifiers depending
on the context, one of the main challenges in this
task consists in the disambiguation of these cases.
To address this challenge, we developed a
pipeline framework (Figure 1) composed of several
modules. First, a recognized entity will be looked up
in an abbreviation dictionary. If it is unambiguously
present there, then the associated CUI is assigned,
otherwise the entity moves on to the next module
(i.e. lookup on the golden dictionary). The CUI-
less tag is assigned to the entity if no suitable CUI is
found at the end of this process, or if the most simi-
lar SNOMED CT candidate found is not a disorder.
</bodyText>
<subsectionHeader confidence="0.817311">
5.1 Resources
</subsectionHeader>
<bodyText confidence="0.995925866666667">
Abbreviation dictionary: This dictionary con-
tains the small (up to 4 letters) upper-cased non-
ambiguous concept descriptors found in the labelled
data. For instance, the entity ASD is an abbrevia-
tion of atrial septal defect with the CUI C0018817.
Since this descriptor is unique in SNOMED CT, it is
considered non-ambiguous.
Golden disorders dictionary: All entity spans
(ambiguous included) retrieved from the labelled
notes are used to form this dictionary. This dic-
tionary is thus composed by all concept descriptors
which were dropped by the abbreviation dictionary,
for their length or because they were ambiguous.
SNOMED CT dictionary: All concepts from
SNOMED CT are included.
</bodyText>
<page confidence="0.993413">
408
</page>
<subsectionHeader confidence="0.954459">
5.2 Methods
</subsectionHeader>
<bodyText confidence="0.999644294117647">
Similarity Search: This module was implemented
using a Lucene index (MacCandless et al., 2010).
NGram (Kondrak, 2005) and Levenshtein distances
were used to retrieve the best SNOMED CT candi-
dates. An extended Levenshtein distance, based on
a best-token-match approach, was developed. This
distance gives the similarity between a target (rec-
ognized entity descriptor) and a candidate descriptor
(SNOMED CT concept), regardless of their token’s
orders. First, both target and candidate descriptors
are split into tokens. For each target’s token, we
compute the Levenshtein distance with all candidate
tokens, and we finally pick the token corresponding
to the minimum value. Each token in the candidate
can only be compared to a single token in the tar-
get. The distance is represented by the following
formula:
</bodyText>
<equation confidence="0.996434">
Sdt = SplitTokens(dt)
Sdc = SplitTokens(dc)
−1, if|Sdt |&gt; |Sdc|
BestMatch(wdt, Sdc)
, otherwise
|Sdt|
</equation>
<bodyText confidence="0.987344041666667">
In the formula, we have that
BestMatch(wdt, Sdc) = Min{LevDist(wdt, wdc) : wdc E Sdc}
In the previous expressions, dt is the target and dc
the candidate descriptor. SplitTokens is the func-
tion responsible for splitting the descriptor into to-
kens. BestMatch returns the minimum Levenshtein
distance between the token wdt and all available to-
kens in Sdc. The token in Sdc which minimizes the
Levenshtein distance is removed from the list for
posterior iterations against the remain tokens in St.
Information Content (IC): The Information
Content (IC) was calculated for each disorder entity
using the UMLS-Similarity (McInnes et al., 2009)
software implementation. This measure enabled us
to disambiguate entities by choosing, from the list
of candidates, the ones with the lowest IC. This
assumes that more general concepts have a higher
probability to appear on a text. The intrinsic method
by (S´anchez et al., 2012) was chosen to calculate
the IC of each concept, using the following formula
where leaves(c) represents the number of leaves of
c, subsumers(c) represents the number of parents of
c, and max.leaves is the number of nodes which are
leaves in the SNOMED CT taxonomy:
</bodyText>
<equation confidence="0.97109125">
⎛ ⎞
|leaves(c) |− 1
IC (c) = −log subsumers(c) |⎠
max.leaves + 1
</equation>
<subsectionHeader confidence="0.968209">
5.3 Approach
</subsectionHeader>
<bodyText confidence="0.9999952">
We implemented a lookup method in each dictio-
nary. If the entity was found, then the associated
identifier was immediately assigned. Ambiguous
cases were resolved using the information content,
choosing the concept with the lowest IC value. For
descriptions not found in the considered dictionar-
ies, we used Lucene to retrieve the top 300 most
similar candidates from SNOMED CT and, for each
candidate, we applied the following formula to ob-
tain the final similarity measure:
</bodyText>
<equation confidence="0.763223">
Sim(dc, dt) = 0.15 * Lev(dc, dt) + 0.15 * NGram(dc, dt)
+ 0.7 * LevExt(dc, dt)
</equation>
<bodyText confidence="0.999991181818182">
In the previous expression, Sim represents the simi-
larity between the target dt and candidate dc descrip-
tor. Lev, NGram and LevExt represent the Leven-
shtein, NGram5 and Extended Levenshtein distance,
respectively. The constant values were chosen ac-
cording to a separate empirical evaluation using the
devel dataset, although in future work we intent to
use systematic approaches based on learning to rank.
For each CUI associated to the chosen candidate de-
scriptor (higher similarity with target descriptor), the
one with the lowest IC was chosen.
</bodyText>
<sectionHeader confidence="0.995138" genericHeader="method">
6 Evaluation Experiments
</sectionHeader>
<bodyText confidence="0.991784555555556">
Three runs were submitted to the SemEval 2015
competition:
Run 1: A 2nd-order CRF model was trained using
the SBIEON encoding, and a rich set of fea-
tures that includes the domain lexicons and 100
Brown clusters. For training, we only used
notes from the training set. For assigning a
UMLS identifier to each entity, we used the
framework that was previously described.
</bodyText>
<equation confidence="0.827211666666667">
Sim(dt, dc) = I
r,
wdt∈Sdt
</equation>
<page confidence="0.982363">
409
</page>
<table confidence="0.998745428571429">
Run Strict Evaluation Relaxed Evaluation
Precision Recall F-measure Precision Recall F-measure
1 0.748 0.676 0.710 0.782 0.706 0.742
2 0.749 0.681 0.713 0.780 0.709 0.743
3 0.779 0.705 0.740 0.806 0.729 0.765
Best System 0.783 0.732 0.757 0.815 0.762 0.788
SemEval 2015
</table>
<tableCaption confidence="0.999555">
Table 2: The official results for Task 1 of the SemEval 2015 challenge on clinical NLP.
</tableCaption>
<bodyText confidence="0.985322875">
Run 2: This run is identical to Run 1 with the
exception of the domain lexicon features that
were not included. Normalization followed the
same strategy as in Run 1.
Run 3: Identical to Run 1, with the exception that
both train and devel documents were used as
training data, resulting in the addition of 133
notes to the training set.
</bodyText>
<sectionHeader confidence="0.998652" genericHeader="evaluation">
7 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999982939393939">
We present our official results in Table 2, highlight-
ing our best results in comparison to those of the best
participating system in the competition.
Our best run achieved the second best F-measure
in the competition, with an F-measure of 0.740 in the
strict evaluation and 0.765 in the relaxed evaluation.
As previously said, the predicted mention can only
be correct if and only if the mapped CUI is correct.
One of the first things to notice when comparing
the runs is the difference on the results between the
third run and the others. As expected, the addition of
133 notes (devel set) to the training data produced a
better recognition model, thus improving the global
performance of the system.
The addition of domain lexicon features to the
recognition model resulted in a lower precision on
the strict evaluation. On the relaxed evaluation a
small improvement was achieved.
The small difference between the strict and re-
laxed evaluation modes can be associated to a really
precise recognition model or, more likely, with the
normalization pipeline having trouble in normaliz-
ing the concepts when they are not fully recognized.
For example, if an entity E was only partially recog-
nized, then it will be harder to normalize it.
In what concerns normalization, all runs were
produced using the same pipeline and with the same
features. Since our approach for the recognition
task is similar to the one used in the SemEval 2014
edition, and since a significant improvement in the
overall performance was obtained, we can conclude
that our recent developments in the normalization
part of the system were particularly effective.
</bodyText>
<sectionHeader confidence="0.986613" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999982">
This paper describes our participation in Task 14 of
the SemEval 2015 competition. Although this task
was divided into two subtasks, our work only ad-
dressed on the recognition and normalization of en-
tity disorders on clinical notes.
For the recognition part, we used a similar ap-
proach to the one followed in the 2014 edition of
SemEval. Specifically, a 2nd-order CRF model was
generated using the Stanford NER software, consid-
ering different sets of features. All models used the
SBIEON encoding (Leal et al., 2014) to support the
recognition of non-continuous entities. Overlapped
entities continue to be an open issue.
For the normalization part, we developed a
pipeline that takes advantage of the existing labelled
data to generate and explore auxiliary dictionaries
(e.g., an abbreviation dictionary). For the recog-
nized entities that do not match to any dictionary, we
employ a similarity search based on Lucene’s imple-
mentation of Levenshtein and NGram distances. An
extension of the Levenshtein distance was developed
to compare descriptors independently of the order
of their words. Ambiguous cases were resolved by
choosing the concepts with the lowest information
content, which was calculated using the approach
proposed by (S´anchez et al., 2012);
As expected, results show that a more compre-
hensive training set enables the generation of bet-
ter recognition models, maintaining the same set of
features. We also saw that the addition of a do-
main lexicon increased the precision, although not
</bodyText>
<page confidence="0.991872">
410
</page>
<bodyText confidence="0.999907346153846">
significantly and with almost no impact on the F-
measure. Our normalization framework was likely
the main reason for the large improvement in our re-
sults, when comparing to the results from SemEval
2014.
In our opinion, the evaluation method followed
in this year’s competition is good for evaluating the
system as a whole, but on the other hand it also lim-
its the evaluation of the two tasks separately, which
we believe would bring some advantages while de-
veloping the system and when comparing results.
For future work, we intend to evaluate both tasks
individually, to better understand which components
are performing well, and which ones need to be
improved. In the normalization task, we intend to
improve the framework that was presented, explor-
ing semantic similarity based on ontology relations
(Couto et al., 2006). By assuming that concepts
within the same text are semantically related to each
other, we intend also to disambiguate entities based
on their semantic similarity towards all other previ-
ously normalized entities (Lamurias et al., 2015).
To improve the module related to similarity search
for disambiguation, we also intend to develop a
learning to rank approach similar to the one pre-
sented by (Leaman et al., 2013).
</bodyText>
<sectionHeader confidence="0.997642" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99992025">
The authors would like to thank Fundac¸˜ao para a
Ciˆencia e Tecnologia (FCT) for the financial support
of LASIGE (UID/CEC/00408/2013) and INESC-ID
(UID/CEC/50021/2013).
</bodyText>
<sectionHeader confidence="0.998558" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999817294117647">
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based N-gram Models of Natural Language. Compu-
tational linguistics, 18(4):467–479.
Ronald Cornet and Nicolette de Keizer. 2008. Forty
years of SNOMED: A literature review. BMC Medical
Informatics and Decision Making, 8(Suppl 1:S2):1–6.
Francisco M Couto, M´ario J Silva, and Pedro M
Coutinho. 2006. Validating associations in biolog-
ical databases. In Proceedings of the 15th ACM in-
ternational conference on Information and knowledge
management, pages 142–151. ACM.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
363–370.
Grzegorz Kondrak. 2005. N-gram similarity and dis-
tance. In Proceedings of the 12th International Con-
ference String Processing and Information Retrieval,
pages 115–126.
Andre Lamurias, Jo˜ao D Ferreira, and Francisco M
Couto. 2015. Improving chemical entity recognition
through h-index based semantic similarity. Journal of
Cheminformatics, 7(Suppl 1):S13.
Andr´e Leal, Diogo Gonc¸alves, Bruno Martins, and Fran-
cisco M Couto. 2014. Ulisboa: Identification and
Classification of Medical Concepts. In Proceedings
of the 8th International Workshop on Semantic Evalu-
ation, pages 711–715.
Robert Leaman, Rezarta Islamaj Do˘gan, and Zhiyong Lu.
2013. DNorm: disease name normalization with pair-
wise learning to rank. Bioinformatics, 29(22):2909–
2917.
Michael MacCandless, Erik Hatcher, and Otis Gospod-
neti´c. 2010. Lucene in Action. Manning Publications
Company.
Bridget T McInnes, Ted Pedersen, and Serguei VS
Pakhomov. 2009. UMLS-Interface and UMLS-
Similarity: open source software for measuring paths
and semantic similarity. In Proceedings of the 2009
Annual Symposium of the American Medical Associa-
tion, pages 431–435.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the 13th Conference on Computational
Natural Language Learning, pages 147–155.
David S´anchez, Montserrat Batet, David Isern, and Aida
Valls. 2012. Ontology-based semantic similarity: A
new feature-based approach. Expert Systems with Ap-
plications, 39(9):7718 – 7728.
Joseph Turian, Lev Ratinov, Yoshua Bengio, and Dan
Roth. 2009. A preliminary evaluation of word repre-
sentations for named-entity recognition. In Proceed-
ings of the NIPS-09 Workshop on Grammar Induction,
Representation of Language and Language Learning,
pages 1–8.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A Simple and General Method
for Semi-supervised Learning. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 384–394.
Yaoyun Zhang, Jingqi Wang, Buzhou Tang, Yonghui Wu,
Min Jiang, Yukun Chen, and Hua Xu. 2014. Uth ccb:
A Report for Semeval 2014 - Task 7 Analysis of Clin-
ical Text. In Proceedings of the 8th International
Workshop on Semantic Evaluation, pages 802–806.
</reference>
<page confidence="0.998025">
411
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.438881">
<title confidence="0.998219">ULisboa: Recognition and Normalization of Medical Concepts</title>
<author confidence="0.968482">Bruno</author>
<author confidence="0.968482">M Francisco</author>
<affiliation confidence="0.943104">Faculdade de Ciˆencias, Universidade de Lisboa, 1749-016 Lisboa,</affiliation>
<address confidence="0.86089">Instituto Superior T´ecnico, Universidade de Lisboa, Portugal</address>
<abstract confidence="0.999892666666667">This paper describes a system developed for the disorder identification subtask within task 14 of SemEval 2015. The developed system is based on a chain of two modules, one for recognition and another for normalization. The recognition module is based on an adapted version of the Stanford NER system to train CRF models in order to recognize disorder mentions. CRF models were build based on a novel encoding of entity spans as token classifications to also consider non-continuous entities, along with a rich set of features based on (i) domain lexicons and (ii) Brown clusters inferred from a large collection of clinical texts. For disorder normalization, we (i) generated a non ambiguous dictionary of abbreviations from the labelled files, using it together with (ii) an heuristic method based on similarity search and (iii) a comparison method based on the information content of each disorder.</abstract>
<note confidence="0.554687666666667">The system achieved an F-measure of 0.740 (the second best), with a precision of 0.779, a recall of 0.705.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased N-gram Models of Natural Language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="8356" citStr="Brown et al., 1992" startWordPosition="1332" endWordPosition="1335">sue in our system. 4.1 Recognition Features We generated 2nd-order CRF models by using, as training data, the labelled notes together with a rich set of features. In 2nd-order models, the features 407 Figure 1: Overview on the normalization approach. are computed from representations composed by the current class and the two previous/next classes. Training Data: Two different sets of data were employed: one with notes belonging to the training set only, and another with notes from both the training and devel sets. Brown Clusters: We inferred word representations in the form of Brown clusters (Brown et al., 1992) from all data that was made available, i.e. from MIMIC, train and devel. According to (Turian et al., 2009), this technique reduces the data sparsity, generating lower-dimensional representations of the word vocabulary, and therefore increasing the accuracy. Each word cluster contains a group of words, and clusters are formed by maximizing the mutual information of bi-grams, according to a class-based language model. We used a total of 404k documents, containing an approximate total of 123M tokens, to infer 100 different clusters using the implementation provided by (Turian et al., 2010). The</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Classbased N-gram Models of Natural Language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Cornet</author>
<author>Nicolette de Keizer</author>
</authors>
<title>Forty years of SNOMED: A literature review.</title>
<date>2008</date>
<booktitle>BMC Medical Informatics and Decision Making, 8(Suppl 1:S2):1–6.</booktitle>
<marker>Cornet, de Keizer, 2008</marker>
<rawString>Ronald Cornet and Nicolette de Keizer. 2008. Forty years of SNOMED: A literature review. BMC Medical Informatics and Decision Making, 8(Suppl 1:S2):1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco M Couto</author>
<author>M´ario J Silva</author>
<author>Pedro M Coutinho</author>
</authors>
<title>Validating associations in biological databases.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th ACM international conference on Information and knowledge management,</booktitle>
<pages>142--151</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="19696" citStr="Couto et al., 2006" startWordPosition="3172" endWordPosition="3175"> our opinion, the evaluation method followed in this year’s competition is good for evaluating the system as a whole, but on the other hand it also limits the evaluation of the two tasks separately, which we believe would bring some advantages while developing the system and when comparing results. For future work, we intend to evaluate both tasks individually, to better understand which components are performing well, and which ones need to be improved. In the normalization task, we intend to improve the framework that was presented, exploring semantic similarity based on ontology relations (Couto et al., 2006). By assuming that concepts within the same text are semantically related to each other, we intend also to disambiguate entities based on their semantic similarity towards all other previously normalized entities (Lamurias et al., 2015). To improve the module related to similarity search for disambiguation, we also intend to develop a learning to rank approach similar to the one presented by (Leaman et al., 2013). Acknowledgments The authors would like to thank Fundac¸˜ao para a Ciˆencia e Tecnologia (FCT) for the financial support of LASIGE (UID/CEC/00408/2013) and INESC-ID (UID/CEC/50021/201</context>
</contexts>
<marker>Couto, Silva, Coutinho, 2006</marker>
<rawString>Francisco M Couto, M´ario J Silva, and Pedro M Coutinho. 2006. Validating associations in biological databases. In Proceedings of the 15th ACM international conference on Information and knowledge management, pages 142–151. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="2151" citStr="Finkel et al., 2005" startWordPosition="327" endWordPosition="330">le within automated procedures. Clinical notes are inherently complex, due to their lack of structure (i.e., narrative language) and due to the need for contextual interpretation. To address this complexity, text mining approaches represent an effective solution to assist the users in retrieving and extracting the required information. This paper presents a text mining system for processing clinical text, that we developed for SemEval based on a pipeline with two modules, one for entity recognition and another for normalization. The entity recognition module is based on the Stanford NER tool (Finkel et al., 2005), and it uses CRF models trained on annotated biomedical notes. The module tags the text according to an SBIEON encoding of entities as token classes, supporting the recognition of non-continuous entities (Leal et al., 2014). We relied on features based on Brown clusters and domain specific lexicons. Thus, this approach combines both supervised (Stanford NER) and unsupervised methods (Brown Clusters). For practical applications, entity recognition is incomplete without performing normalization, i.e. without mapping each entity to an identifier (CUI) in a controlled vocabulary like SNOMED CT (C</context>
<context position="6811" citStr="Finkel et al., 2005" startWordPosition="1073" endWordPosition="1076"> notes consisted on plain text without any additional information, while labelled clinical notes consist on plain text together with a list of disorder mentions contained on them. Table 1 summarizes each dataset. Train Devel Test Unlabelled Notes 298 133 100 404k Words 182k 154k 8k 123M Disorder 11.5k 8k - - Mentions CUI-ied 8k (88%) 6k (76%) - - CUI-less 3.5k (12%) 2k (24%) - - Table 1: Statistical characterization of the datasets. 4 Entity Recognition We applied the same type of approach used in our system from last year (Leal et al., 2014) for entity recognition. The Stanford NER software (Finkel et al., 2005) was used to train Conditional Random Fields (CRF) models using labelled data as input. All input text had to be tokenized and encoded according to a named entity recognition scheme that encodes entities as token classifications. To be able to recognize non-continuous entities, an SBIEON (Leal et al., 2014) encoding was used. Besides the tags defined in the SBIEO encoding (Ratinov and Roth, 2009), a new tag N was added to identify words that do not belong to the entity but are inside the continuous span that contains the recognized entity. The remain tags are used to identify Single entities, </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
</authors>
<title>N-gram similarity and distance.</title>
<date>2005</date>
<booktitle>In Proceedings of the 12th International Conference String Processing and Information Retrieval,</booktitle>
<pages>115--126</pages>
<contexts>
<context position="11398" citStr="Kondrak, 2005" startWordPosition="1820" endWordPosition="1821">atrial septal defect with the CUI C0018817. Since this descriptor is unique in SNOMED CT, it is considered non-ambiguous. Golden disorders dictionary: All entity spans (ambiguous included) retrieved from the labelled notes are used to form this dictionary. This dictionary is thus composed by all concept descriptors which were dropped by the abbreviation dictionary, for their length or because they were ambiguous. SNOMED CT dictionary: All concepts from SNOMED CT are included. 408 5.2 Methods Similarity Search: This module was implemented using a Lucene index (MacCandless et al., 2010). NGram (Kondrak, 2005) and Levenshtein distances were used to retrieve the best SNOMED CT candidates. An extended Levenshtein distance, based on a best-token-match approach, was developed. This distance gives the similarity between a target (recognized entity descriptor) and a candidate descriptor (SNOMED CT concept), regardless of their token’s orders. First, both target and candidate descriptors are split into tokens. For each target’s token, we compute the Levenshtein distance with all candidate tokens, and we finally pick the token corresponding to the minimum value. Each token in the candidate can only be comp</context>
</contexts>
<marker>Kondrak, 2005</marker>
<rawString>Grzegorz Kondrak. 2005. N-gram similarity and distance. In Proceedings of the 12th International Conference String Processing and Information Retrieval, pages 115–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Lamurias</author>
<author>Jo˜ao D Ferreira</author>
<author>Francisco M Couto</author>
</authors>
<title>Improving chemical entity recognition through h-index based semantic similarity.</title>
<date>2015</date>
<journal>Journal of Cheminformatics,</journal>
<volume>7</volume>
<pages>1--13</pages>
<marker>Lamurias, Ferreira, Couto, 2015</marker>
<rawString>Andre Lamurias, Jo˜ao D Ferreira, and Francisco M Couto. 2015. Improving chemical entity recognition through h-index based semantic similarity. Journal of Cheminformatics, 7(Suppl 1):S13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Leal</author>
<author>Diogo Gonc¸alves</author>
<author>Bruno Martins</author>
<author>Francisco M Couto</author>
</authors>
<title>Ulisboa: Identification and Classification of Medical Concepts.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation,</booktitle>
<pages>711--715</pages>
<marker>Leal, Gonc¸alves, Martins, Couto, 2014</marker>
<rawString>Andr´e Leal, Diogo Gonc¸alves, Bruno Martins, and Francisco M Couto. 2014. Ulisboa: Identification and Classification of Medical Concepts. In Proceedings of the 8th International Workshop on Semantic Evaluation, pages 711–715.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Leaman</author>
<author>Rezarta Islamaj Do˘gan</author>
<author>Zhiyong Lu</author>
</authors>
<title>DNorm: disease name normalization with pairwise learning to rank.</title>
<date>2013</date>
<journal>Bioinformatics,</journal>
<volume>29</volume>
<issue>22</issue>
<pages>2917</pages>
<marker>Leaman, Do˘gan, Lu, 2013</marker>
<rawString>Robert Leaman, Rezarta Islamaj Do˘gan, and Zhiyong Lu. 2013. DNorm: disease name normalization with pairwise learning to rank. Bioinformatics, 29(22):2909– 2917.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael MacCandless</author>
<author>Erik Hatcher</author>
<author>Otis Gospodneti´c</author>
</authors>
<title>Lucene in Action.</title>
<date>2010</date>
<publisher>Manning Publications Company.</publisher>
<marker>MacCandless, Hatcher, Gospodneti´c, 2010</marker>
<rawString>Michael MacCandless, Erik Hatcher, and Otis Gospodneti´c. 2010. Lucene in Action. Manning Publications Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bridget T McInnes</author>
<author>Ted Pedersen</author>
<author>Serguei VS Pakhomov</author>
</authors>
<title>UMLS-Interface and UMLSSimilarity: open source software for measuring paths and semantic similarity.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Annual Symposium of the American Medical Association,</booktitle>
<pages>431--435</pages>
<contexts>
<context position="12829" citStr="McInnes et al., 2009" startWordPosition="2041" endWordPosition="2044">we have that BestMatch(wdt, Sdc) = Min{LevDist(wdt, wdc) : wdc E Sdc} In the previous expressions, dt is the target and dc the candidate descriptor. SplitTokens is the function responsible for splitting the descriptor into tokens. BestMatch returns the minimum Levenshtein distance between the token wdt and all available tokens in Sdc. The token in Sdc which minimizes the Levenshtein distance is removed from the list for posterior iterations against the remain tokens in St. Information Content (IC): The Information Content (IC) was calculated for each disorder entity using the UMLS-Similarity (McInnes et al., 2009) software implementation. This measure enabled us to disambiguate entities by choosing, from the list of candidates, the ones with the lowest IC. This assumes that more general concepts have a higher probability to appear on a text. The intrinsic method by (S´anchez et al., 2012) was chosen to calculate the IC of each concept, using the following formula where leaves(c) represents the number of leaves of c, subsumers(c) represents the number of parents of c, and max.leaves is the number of nodes which are leaves in the SNOMED CT taxonomy: ⎛ ⎞ |leaves(c) |− 1 IC (c) = −log subsumers(c) |⎠ max.l</context>
</contexts>
<marker>McInnes, Pedersen, Pakhomov, 2009</marker>
<rawString>Bridget T McInnes, Ted Pedersen, and Serguei VS Pakhomov. 2009. UMLS-Interface and UMLSSimilarity: open source software for measuring paths and semantic similarity. In Proceedings of the 2009 Annual Symposium of the American Medical Association, pages 431–435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning,</booktitle>
<pages>147--155</pages>
<contexts>
<context position="7210" citStr="Ratinov and Roth, 2009" startWordPosition="1137" endWordPosition="1140">stical characterization of the datasets. 4 Entity Recognition We applied the same type of approach used in our system from last year (Leal et al., 2014) for entity recognition. The Stanford NER software (Finkel et al., 2005) was used to train Conditional Random Fields (CRF) models using labelled data as input. All input text had to be tokenized and encoded according to a named entity recognition scheme that encodes entities as token classifications. To be able to recognize non-continuous entities, an SBIEON (Leal et al., 2014) encoding was used. Besides the tags defined in the SBIEO encoding (Ratinov and Roth, 2009), a new tag N was added to identify words that do not belong to the entity but are inside the continuous span that contains the recognized entity. The remain tags are used to identify Single entities, the Begin, Inside and End token of a non-single token entity, and the Other tag for words which are neither entities nor related to them. For overlapped entities we did not develop any approach, i.e. we only recognize the first entity in an overlapping group of entities. Thus, handling overlapping entities remains an open issue in our system. 4.1 Recognition Features We generated 2nd-order CRF mo</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proceedings of the 13th Conference on Computational Natural Language Learning, pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David S´anchez</author>
<author>Montserrat Batet</author>
<author>David Isern</author>
<author>Aida Valls</author>
</authors>
<title>Ontology-based semantic similarity: A new feature-based approach.</title>
<date>2012</date>
<booktitle>Expert Systems with Applications, 39(9):7718 –</booktitle>
<pages>7728</pages>
<marker>S´anchez, Batet, Isern, Valls, 2012</marker>
<rawString>David S´anchez, Montserrat Batet, David Isern, and Aida Valls. 2012. Ontology-based semantic similarity: A new feature-based approach. Expert Systems with Applications, 39(9):7718 – 7728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
<author>Dan Roth</author>
</authors>
<title>A preliminary evaluation of word representations for named-entity recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the NIPS-09 Workshop on Grammar Induction, Representation of Language and Language Learning,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="8464" citStr="Turian et al., 2009" startWordPosition="1351" endWordPosition="1354">he labelled notes together with a rich set of features. In 2nd-order models, the features 407 Figure 1: Overview on the normalization approach. are computed from representations composed by the current class and the two previous/next classes. Training Data: Two different sets of data were employed: one with notes belonging to the training set only, and another with notes from both the training and devel sets. Brown Clusters: We inferred word representations in the form of Brown clusters (Brown et al., 1992) from all data that was made available, i.e. from MIMIC, train and devel. According to (Turian et al., 2009), this technique reduces the data sparsity, generating lower-dimensional representations of the word vocabulary, and therefore increasing the accuracy. Each word cluster contains a group of words, and clusters are formed by maximizing the mutual information of bi-grams, according to a class-based language model. We used a total of 404k documents, containing an approximate total of 123M tokens, to infer 100 different clusters using the implementation provided by (Turian et al., 2010). The number of clusters was chosen through a separate set of experiments as the one that maximized the F-measure</context>
</contexts>
<marker>Turian, Ratinov, Bengio, Roth, 2009</marker>
<rawString>Joseph Turian, Lev Ratinov, Yoshua Bengio, and Dan Roth. 2009. A preliminary evaluation of word representations for named-entity recognition. In Proceedings of the NIPS-09 Workshop on Grammar Induction, Representation of Language and Language Learning, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A Simple and General Method for Semi-supervised Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="8951" citStr="Turian et al., 2010" startWordPosition="1423" endWordPosition="1426">ters (Brown et al., 1992) from all data that was made available, i.e. from MIMIC, train and devel. According to (Turian et al., 2009), this technique reduces the data sparsity, generating lower-dimensional representations of the word vocabulary, and therefore increasing the accuracy. Each word cluster contains a group of words, and clusters are formed by maximizing the mutual information of bi-grams, according to a class-based language model. We used a total of 404k documents, containing an approximate total of 123M tokens, to infer 100 different clusters using the implementation provided by (Turian et al., 2010). The number of clusters was chosen through a separate set of experiments as the one that maximized the F-measure. Encoding: The aforementioned SBIEON encoding was employed in all recognition models. Features: The CRF models rely on a set of features that includes (i) word tokens within a window of size 2, (ii) token shape (upper-cased, numeric, etc), (iii) token position in a sentence and (iv) token prefixes and suffixes. This basic set of features was also extended with features based on Brown clusters, and domain-specific lexicons. Domain-specific lexicon: We built lexicons for the medical </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A Simple and General Method for Semi-supervised Learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaoyun Zhang</author>
<author>Jingqi Wang</author>
<author>Buzhou Tang</author>
<author>Yonghui Wu</author>
<author>Min Jiang</author>
<author>Yukun Chen</author>
<author>Hua Xu</author>
</authors>
<title>Uth ccb: A Report for Semeval</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation,</booktitle>
<pages>802--806</pages>
<contexts>
<context position="5833" citStr="Zhang et al., 2014" startWordPosition="912" endWordPosition="915"> performed in a strict or relaxed way. In strict evaluation, a predicted mention is considered a true positive if the predicted span is exactly the same as the gold-standard. On the relaxed evaluation, the predicted spans only need to overlap the gold-standard spans to be considered a true positive. On both evaluation methods the CUI must be correctly identified to be considered a true positive. Thus, even with a perfect recognition system, it is possible to achieve low results on the task, depending on the normalization performance 3 Datasets Similarly to the last edition of the competition (Zhang et al., 2014), two sets of labelled data were given to the participants, which were separated into two categories (training and development). They were used for training and testing of our system, respectively. Unlabelled clinical notes from the MIMIC corpus were also provided. Later, an unlabelled test set was released to evaluate the final system. Unlabelled clinical notes consisted on plain text without any additional information, while labelled clinical notes consist on plain text together with a list of disorder mentions contained on them. Table 1 summarizes each dataset. Train Devel Test Unlabelled N</context>
</contexts>
<marker>Zhang, Wang, Tang, Wu, Jiang, Chen, Xu, 2014</marker>
<rawString>Yaoyun Zhang, Jingqi Wang, Buzhou Tang, Yonghui Wu, Min Jiang, Yukun Chen, and Hua Xu. 2014. Uth ccb: A Report for Semeval 2014 - Task 7 Analysis of Clinical Text. In Proceedings of the 8th International Workshop on Semantic Evaluation, pages 802–806.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>