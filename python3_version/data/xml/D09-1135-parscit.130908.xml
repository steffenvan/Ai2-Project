<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.997157">
Refining Grammars for Parsing with Hierarchical Semantic Knowledge
</title>
<author confidence="0.987156">
Xiaojun Lin, Yang Fan, Meng Zhang, Xihong Wu; Huisheng Chi
</author>
<affiliation confidence="0.9585125">
Speech and Hearing Research Center
Key Laboratory of Machine Perception (Ministry of Education)
School of Electronics Engineering and Computer Science
Peking University, Beijing, 100871, China
</affiliation>
<email confidence="0.962044">
{linxj, fanyang, zhangm, wxh}@cis.pku.edu.cn, chi@pku.edu.cn
</email>
<sectionHeader confidence="0.994836" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998996">
This paper proposes a novel method to
refine the grammars in parsing by utiliz-
ing semantic knowledge from HowNet.
Based on the hierarchical state-split ap-
proach, which can refine grammars au-
tomatically in a data-driven manner, this
study introduces semantic knowledge into
the splitting process at two steps. Firstly,
each part-of-speech node will be anno-
tated with a semantic tag of its termi-
nal word. These new tags generated in
this step are semantic-related, which can
provide a good start for splitting. Sec-
ondly, a knowledge-based criterion is used
to supervise the hierarchical splitting of
these semantic-related tags, which can al-
leviate overfitting. The experiments are
carried out on both Chinese and English
Penn Treebank show that the refined gram-
mars with semantic knowledge can im-
prove parsing performance significantly.
Especially with respect to Chinese, our
parser achieves an F1 score of 87.5%,
which is the best published result we are
aware of.
</bodyText>
<sectionHeader confidence="0.998117" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999836545454545">
At present, most high-performance parsers are
based on probabilistic context-free grammars
(PCFGs) in one way or another (Collins, 1999;
Charniak and Johnson, 2005; Petrov and Klein,
2007). However, restricted by the strong context-
free assumptions, the original PCFG model which
simply takes the grammars and probabilities off a
treebank, does not perform well. Therefore, a va-
riety of techniques have been developed to enrich
and generalize the original grammar, ranging from
lexicalization to symbol annotation.
</bodyText>
<note confidence="0.941908">
Corresponding author: Xihong Wu.
</note>
<bodyText confidence="0.999905487804878">
Lexicalized PCFGs use the structural features
on the lexical head of phrasal node in a tree, and
get significant improvements for parsing (Collins,
1997; Charniak, 1997; Collins, 1999; Charniak,
2000). However, they suffer from the problem of
fundamental sparseness of the lexical dependency
information. (Klein and Manning, 2003).
In order to deal with this limitation, a variety
of unlexicalized parsing techniques have been pro-
posed. Johnson (1998) annotates each node by
its parent category in a tree, and gets significant
improvements compared with the original PCFGs
on the Penn Treebank. Then, some manual and
automatic symbol splitting methods are presented,
which get comparable performance with lexical-
ized parsers (Klein and Manning, 2003; Matsuzaki
et al., 2005). Recently, Petrov et al. (2006) in-
troduces an automatic hierarchical state-split ap-
proach to refine the grammars, which can alter-
nately split and merge the basic nonterminals by
the Expectation-Maximization (EM) algorithm. In
this method, the nonterminals are split to differ-
ent degrees, as appropriate to the actual complex-
ity in the data. The grammars refined in this way
are proved to be much more accurate and compact
than previous work on automatic annotation. This
data-driven method still suffers from the overfit-
ting problem, which may be improved by integrat-
ing other external information.
In this paper, we propose a novel method that
combines the strengths of both data-driven and
knowledge-driven strategies to refine grammars.
Based on the work proposed by Petrov et al.
(2006), we use the semantic knowledge from
HowNet (Dong and Dong, 2000) to supervise
the hierarchical state-split process at the part-of-
speech(POS) level. At first, we define the most
general hypernym in HowNet as the semantic class
of a word, and then use this semantic class to ini-
tialize the tag of each POS node. In this way, a
new set of semantic-related tags is generated, and
</bodyText>
<note confidence="0.798870666666667">
1298
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1298–1307,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999683681818182">
a good starting annotation is provided to reduce
the search space for the EM algorithm in the split-
ting process. Then, in order to mitigate the overfit-
ting risk, the hierarchical hypernym-hyponym re-
lation between hypernyms in HowNet is utilized
to supervise the splitting of these new semantic-
related tags. By introducing a knowledge-based
criterion, these new tags are decided whether or
not to split into subcategories from a semantic per-
spective. To investigate the effectiveness of the
presented approach, several experiments are con-
duced on both Chinese and English. They reveal
that the semantic knowledge is potentially useful
to parsing.
The remainder of this paper is organized as
follows. Section 2 reviews some closely related
works, including the lexical semantic related pars-
ing and the hierarchical state-split unlexicalized
parsing. In section 3, the presented method for
grammar refining is described in detail, and sev-
eral experiments are carried out for evaluation in
Section 4. Conclusions are drawn in Section 5.
</bodyText>
<sectionHeader confidence="0.981118" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9999455">
This paper tries to refine the grammars through
an improved hierarchical state-split process in-
tegrated with semantic knowledge. The related
works are reviewed as follows.
</bodyText>
<subsectionHeader confidence="0.998414">
2.1 Lexical Semantic Related Parsing
</subsectionHeader>
<bodyText confidence="0.999901212765958">
Semantic knowledge is useful to resolving syntac-
tic ambiguities, and a variety of researches focus
on how to utilize it. Especially in recent years,
a conviction arose that semantic knowledge could
be incorporated into the lexicalized parsing.
Based on the lexicalized grammars, Bikel
(2000) attempts at combining parsing and word
sense disambiguation in a unified model, using a
subset of SemCor (Miller et al., 1994). Bikel
(2000) evaluates this model in a parsing context
with sense information from WordNet, but does
not get improvements on parsing performance.
Xiong et al. (2005) combines word sense from
CiLin and HowNet (two Chinese semantic re-
sources) in a generative parsing model, which gen-
eralizes standard bilexical dependencies to word-
class dependencies, and indeed help to tackle the
sparseness problem in lexicalized parsing. The
experiments show that the parse model combined
with word sense and the most special hypernyms
achieves a significant improvement on Penn Chi-
nese Treebank. This work only considers the most
special hypernym of a word, rather than other
hypernyms at different levels of the hypernym-
hyponym hierarchy.
Then, Fujita et al. (2007) uses the Hinoki tree-
bank as training data to train a discriminative parse
selection model combining syntactic features and
word sense information. Instead of utilizing the
most special hypernym, the word sense informa-
tion in this model is embodied with more general
concepts. Based on the hand-craft sense informa-
tion, this model is proved to be effective for parse
selection.
Recently, Agirre et al. (2008) train two lexical-
ized models (Charniak, 2000; Bikel, 2004) on pre-
processed inputs, where content words are substi-
tuted with semantic classes from WordNet. By in-
tegrating the word semantic classes into the pro-
cess of parser training directly, these two models
obtain significant improvements in both parsing
and prepositional phrase attachment tasks. Zhang
(2008) does preliminary work on integrating POS
with semantic class of words directly, which can
not only alleviate the confusion in parsing, but also
infer syntax and semantic information at the same
time.
</bodyText>
<subsectionHeader confidence="0.99949">
2.2 The Hierarchical State-split Parsing
</subsectionHeader>
<bodyText confidence="0.9999595">
In order to alleviate the context-free assumptions,
Petrov et al. (2006) proposes a hierarchical state-
split approach to refine and generalize the orig-
inal grammars, and achieves state-of-the-art per-
formance. Starting with the basic nonterminals,
this method repeats the split-merge (SM) cycle to
increase the complexity of grammars. That is, it
splits every symbol into two, and then re-merges
some new subcategories based on the likelihood
computation.
</bodyText>
<subsectionHeader confidence="0.870211">
Splitting
</subsectionHeader>
<bodyText confidence="0.999923454545455">
In each splitting stage, the previous syntactic sym-
bol is split into two subcategories, and the EM al-
gorithm is adopted to learn probability of the rules
for these latent annotations to maximize the like-
lihood of trees in the training data. Finally, each
symbol generates a series of new subcategories in
a hierarchical fashion. With this method, the split-
ting strategy introduces more context information,
and the refined grammars cover more linguistic in-
formation which helps resolve the syntactic ambi-
guities.
</bodyText>
<page confidence="0.766185">
1299
</page>
<bodyText confidence="0.999471166666667">
However, it is worth noting that the EM algo-
rithm does not guarantee a global optimal solution,
and often gets stuck in a suboptimal configuration.
Therefore, a good starting annotation is expected
to help alleviate this problem, as well as reduce the
search space for EM.
</bodyText>
<subsectionHeader confidence="0.632986">
Merging
</subsectionHeader>
<bodyText confidence="0.999996533333333">
It is obvious that using more derived subcategories
can increase accuracy, but the refined grammars fit
tighter to the training data, and may lead to over-
fitting to some extent. In addition, different sym-
bols should have their specific numbers of subcat-
egories. For example, the comma POS tag should
have only one subcategory, as it always produces
the terminal comma. On the contrary, the noun
POS tag and the verb POS tag are expected to have
much more subcategories to express their context
dependencies. Therefore, it is not reasonable to
split them in the same way.
The symbol merging stage is introduced to al-
leviate this defect. This approach splits symbols
only where needed, and it is implemented by split-
ting each symbol first and then measure the loss in
likelihood incurred when removing this subcate-
gory. If the loss is small, it means that this subcate-
gory does not take enough information and should
be removed. In general it is hard to decide the
threshold of the likelihood loss, and this merging
stage is often executed by removing a certain pro-
portion of subcategories, as well as giving priority
to the most informative subcategories.
By splitting and merging alternately, this
method can refine the grammars step by step to
mitigate the overfitting risk to some extent. How-
ever, this data-driven method can not solve this
problem completely, and we need to find other ex-
ternal information to improve it.
</bodyText>
<subsectionHeader confidence="0.695794">
Analysis
</subsectionHeader>
<bodyText confidence="0.999848875">
The hierarchical state-split approach is used to
split all the symbols in the same way. Table 1 cites
the subcategories for several POS tags, along with
their two most frequent words. Results show that
the words in the same subcategory of POS tags are
semantic consistent in some cases. Therefore, it
is expected to optimize the splitting and merging
process at the POS level with semantic knowledge.
</bodyText>
<equation confidence="0.972548642857143">
NR
)CFP (Dajariver) )&amp;quot;bM/(Nepal)
j,t(Sony) f0.&apos; M(Bole Co.)
�P A(C. Hua) jCXO(T. Wen)
.&apos; WK(S. Yue) _A(Shang)
LC
_n�(middle) ti ffl(right)
14 (before) 1�4*(since)
}FAA(start) IL(end)
)(JIL(till) *(end)
P
*_n(whenever) YT(as for)
RtF(like) *Was)
#A*(look to) M*(according to)
</equation>
<tableCaption confidence="0.716583">
ffte close to) rMcontrast)
Table 1: The two most frequent words in the sub-
categories of several POS tag.
</tableCaption>
<sectionHeader confidence="0.933976" genericHeader="method">
3 Integration with Semantic Knowledge
</sectionHeader>
<bodyText confidence="0.999992615384615">
In this paper, the semantic knowledge is used to re-
fine grammars by improving the automatic hierar-
chical state-split approach. At first, in order to pro-
vide good starting annotations to reduce the search
space for the EM algorithm, we try to annotate the
tag of each POS node with the most general hyper-
nym of its terminal word. In this way, we generate
a new set of semantic-related tags. And then, in-
stead of splitting and merging all symbols together
automatically, we propose a knowledge-based cri-
terion with hierarchical semantic knowledge to su-
pervise the splitting of these new semantic-related
tags.
</bodyText>
<subsectionHeader confidence="0.996413">
3.1 HowNet
</subsectionHeader>
<bodyText confidence="0.999979066666667">
The semantic knowledge resource we use is
HowNet, which is a common sense knowledge
base unveiling concepts and inter-conceptual re-
lations in Chinese and English.
As a knowledge base of graph structure,
HowNet is devoted to demonstrating the proper-
ties of concepts through sememes and relations
between sememes. Broadly speaking, a sememe
refers to the smallest basic semantic unit that can-
not be reduced further, which can be represented
in English and their Chinese equivalents, such as
the sememe insrirurion|机构. The relations expli-
cated in HowNet include hypernym-hyponym re-
lations, location-event relations, time-event rela-
tions and so on. In this work, we mainly focus on
</bodyText>
<figure confidence="0.995975">
NR-0
NR-1
NR-2
NR-3
LC-0
LC-1
LC-2
LC-3
P-0
P-1
P-2
P-3
1300
.
.
a.
IP
o
NN VV NP
NP VP PU
IP
NP VP PU
NN-Entity VV-Event NP o
b.
OA911 An NN OA911 An NN-Attribute
The goveronment is full of The goveronment is full of
MJJ MJJ
vitality vitality
</figure>
<figureCaption confidence="0.998571">
Figure 1: The two syntax trees of the sentence &amp;quot;The government is full of vitality&amp;quot;. a. is the original
</figureCaption>
<bodyText confidence="0.986602611111111">
syntax tree, b. is the syntax tree in which each tag of the POS node is annotated with the most general
hypernym of its terminal word.
the hypernym-hyponym relations. Take the word
政府(government) as an example, its hypernyms with
the hierarchical hypernym-hyponym relations are
listed below from speciality to generality, which
we call hierarchical semantic information in this
paper.
institution|机构—*group|群体--+thing|万物--+entity|实体
It is clear that this word 政府(government) has hy-
pernyms from the most special hypernym institu-
tion|机构 to the most general hypernym entity|实体
in a hierarchical way.
In HowNet(Update 2008), there are 173535
concepts, with 2085 sememes. The sememes are
categorized into entity, event, attribute, attribute
value, etc., each corresponding to a sememe hi-
erarchy tree.
</bodyText>
<subsectionHeader confidence="0.999594">
3.2 Annotating the Training Data
</subsectionHeader>
<bodyText confidence="0.999977796296297">
One of the original motivations for the grammar
refinement is that the original symbols, especially
the POS tags, are usually too general to distin-
guish the context dependencies. Take the sentence
in Figure 1 for example, the word 政府(government)
should have different context dependencies com-
pared with the word 活力(vitality), although both of
them have the same POS tag &amp;quot;NN&amp;quot;. In fact, the
two words are defined in HowNet with different
hypernyms. The word 政府(government) is defined
as a kind of objective things, while the word 活
力(vitality) is defined as a property that is often used
to describe things. It is obvious that the different
senses can represent their different syntax struc-
tures, and we expect to refine the POS tags with
semantic knowledge.
In the automatic hierarchical state-split ap-
proach introduced above, the EM algorithm is
used to search for the maximum of the likelihood
during the splitting process, which can generate
subcategories for POS tags to express the context
dependencies. However, this method often gets
stuck in a suboptimal configuration, which varies
depending on the start point. Therefore, a good
start of the annotations is very important. As it is
displayed in Figure 1, we annotate the tag of each
POS node with the hypernym of its terminal word
as the starting annotation. There are two problems
that we have to consider in this process: a) how to
choose the appropriate semantic granularity, and
b) how to deal with the polysemous words.
As mentioned above, the semantic information
of each word can be represented as hierarchi-
cal hypernym-hyponym relations among its hyper-
nyms. In general, it is hard to decide the appro-
priate level of granularity to represent the word.
The semantic class is only used as the starting an-
notations of POS tags to reduce the search space
for EM in our method. It is followed by the hi-
erarchical state-split process to further refine the
starting annotations based on the structural infor-
mation. If more special kinds of semantic classes
are chosen, it will make the structural information
weaker. As annotations with the special hyper-
nym always defeat some of the advantage of au-
tomatically latent annotations learning, we anno-
tate the training data with the most general hyper-
nym. For example, as shown in Figure 1, the POS
tag &amp;quot;NN&amp;quot; of 政府(government) is annotated as &amp;quot;NN-
Entity&amp;quot;, and &amp;quot;NN&amp;quot; of 活力(energy) is annotated as
&amp;quot;NN-Attribute&amp;quot;.
Another problem is how to deal with the polyse-
mous words in HowNet. In fact, when we choose
the most general hypernym as the word’s semantic
</bodyText>
<figure confidence="0.996472115384615">
1301
Continue Splitting...
beast insect
beast insect
banana orange
banana orange
NN-Entity HowNet
Rf*(beast)
RA(insect)
#(banana)
P.-T(orange)
+&apos;j-(noon)
±(forenoon)
AL�V(north)
*(south)
noon forenoon
noon forenoon
north south
north south
animal|*
thing|rl*
entity|%:,W
time|RIN
fruit|**
direction|�Vn
Having hyponyms...
</figure>
<figureCaption confidence="0.972489333333333">
Figure 2: A schematic figure for the hierarchical state-split process of the semantic-related tag &amp;quot;NN-
Entity&amp;quot;. Each subcategory of this tag has its own word set, and corresponds to one hypernym at the
appropriate level in HowNet.
</figureCaption>
<bodyText confidence="0.999962777777778">
representation, this problem has been alleviated to
a large extent. In this paper we adopt the first sense
option as our word sense disambiguation (WSD)
strategy to determine the sense of each token in-
stance of a target word. That is to say, all token in-
stances of a given word are tagged with the sense
that occurs most frequently in HowNet. In addi-
tion, we keep the tag of the POS node whose ter-
minal word is not defined in HowNet unchanged.
</bodyText>
<subsectionHeader confidence="0.9924325">
3.3 Supervising the Hierarchical State-split
Process
</subsectionHeader>
<bodyText confidence="0.99997114">
With the method proposed above, we can produce
a good starting annotation with semantic knowl-
edge, which is of great use to constraining the au-
tomatic splitting process. Our parser is trained on
the good starting annotations with the automatic
hierarchical state-split process, and gets improve-
ments compared with the original training data.
However, during this process, only the most gen-
eral hypernyms are used as the semantic repre-
sentation of words, and the hierarchical semantic
knowledge is not explored. In addition, the auto-
matic process tries to refine all symbols together
through a data-driven manner, which suffers the
overfitting risk.
After annotating the training data with hyper-
nyms, a new set of semantic-related tags such as
&amp;quot;NN-Entity&amp;quot; is produced. We treat the refining
process of these semantic-related tags as the spe-
cializing process of hypernym with hierarchical
semantic knowledge. Each subcategory of these
tags corresponds to a appropriate special level of
hypernym in the HowNet. For example, every sub-
category of &amp;quot;NN-Entity&amp;quot; could corresponds to a
appropriate hyponym of entity|实体.
We integrate the hierarchical semantic knowl-
edge into the original hierarchical state-split pro-
cess to refine these semantic-related tags. First
of all, it is necessary to establish the mapping
from each subcategory of these semantic-related
tags to the hypernym at the appropriate level in
HowNet. Then, instead of likelihood judgment, a
knowledge-based criterion is proposed, to decide
whether or not to remove the new subcategories
of these tags. That is to say, once the parent tag
of this new subcategory is mapped onto the most
special hypernym without any hyponym, it should
be removed immediately.
The schematic Figure 2 demonstrates this se-
mantically supervised splitting process. The left
part of this figure is the subcategories of the
semantic-related tag &amp;quot;NN-Entity&amp;quot;, which is split
hierarchically. As expressed by the dashed line,
each subcategory corresponds to one hypernym in
the right part of this figure. If the hypernym node
has no hyponym, the corresponding subcategory
will stop splitting.
The mapping from each subcategory of these
semantic-related tags to the hypernym at the ap-
propriate level is implemented with the word set
related to this subcategory. As it is shown in Fig-
</bodyText>
<table confidence="0.935324833333333">
1302
DataSet Chinese English
Xue et al. (2002) Marcus et al. (1993)
TrainSet Art. 1-270,400-1151 Sections 2-21
DevSet Articles 301-325 Section 22
TestSet Articles 271-300 Section 23
</table>
<tableCaption confidence="0.999754">
Table 2: Experimental setup.
</tableCaption>
<bodyText confidence="0.999951076923077">
ure 2, the original tag &amp;quot;NN-Entity&amp;quot; treats all the
words it products as its word set. Once the orig-
inal category is split into two subcategories, its
word set is also split, through forcedly dividing
each word in the word set into one subcategory
which is most frequent with this word. And then,
each subcategory is mapped onto the most specific
hypernym that contains its related word set en-
tirely in HowNet. On this basis, a new knowledge-
based criterion is introduced to enrich and gener-
alize these semantic-related tags, with purpose of
fitting to the hierarchical semantic structure rather
than the training data.
</bodyText>
<sectionHeader confidence="0.999568" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999926666666667">
In this section, we designed several experiments to
investigate the validity of refining grammars with
semantic knowledge.
</bodyText>
<subsectionHeader confidence="0.97943">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999869846153846">
We did experiments on Chinese and English. In
order to make a fair comparison with previous
works, we split the standard corpora as shown
in Table 2. Our parsers were evaluated by the
EVALB parseval reference implementation1. The
Berkeley parser2 was used to train the models with
the original automatic hierarchical state-split pro-
cess. The semantic resource we used to improve
parsing was HowNet, which has been introduced
in Subsection 3.1. Statistical significance was
checked using Dan Bikel’s randomized parsing
evaluation comparator with the default setting of
10,000 iterations3.
</bodyText>
<subsectionHeader confidence="0.991783">
4.2 Semantic Representation Experiments
</subsectionHeader>
<bodyText confidence="0.99977925">
First of all, we ran experiments with different se-
mantic representation methods on Chinese. The
polysemous words in the training set were anno-
tated with the WSD strategy of first sense option,
</bodyText>
<footnote confidence="0.997726666666667">
1http://nlp.cs.nyu.edu/evalb/.
2http://code.google.com/p/berkeleyparser/.
3http://www.cis.upenn.edu/ dbikel/software.html.
</footnote>
<bodyText confidence="0.999558153846154">
which was proved to be useful in Agirre et al.
(2008).
As mentioned in Subsection 3.1, the semantic
information of each word can be represented as
a hierarchical relation among its hypernyms from
specialty to generalization in HowNet. In order to
choose the appropriate level of granularity to rep-
resent words, we annotated the training set with
different levels of granularity as semantic repre-
sentation. In our experiments, the automatic hier-
archical state-split process is used to train models
on these training sets with different level of seman-
tic representation.
We tried two kinds of semantic representations,
one is using the most general hypernym, and the
other is using the most special hypernym. Results
in Figure 3 proved the effectiveness of our method
in Subsection 3.2. When we annotated the tag of
each POS node with the most general hypernym of
its terminal word, the parser performs much bet-
ter than both the baseline and the one annotated
with the most special hypernym. Moreover, the Fi
score starts dropping after 3 training iterations on
the training set annotated with the most special hy-
pernyms, while it is still improving with the most
general one, indicating overfitting.
</bodyText>
<equation confidence="0.6205425">
1234 68 70 ��
�� eration
</equation>
<bodyText confidence="0.880634764705882">
Figure 3: Performances on Chinese with different
semantic representations: the training set without
semantic representation, the training set annotated
with the most special hypernyms, and the training
set annotated with the most general hypernyms.
When the training set was annotated with the
most general hypernyms, there were only 57 new
semantic-related tags such as &amp;quot;NN-Entity&amp;quot;, &amp;quot;NN-
Attribute&amp;quot; and so on. However, when the train-
ing set was annotated with the most special hyper-
nyms, 4313 new tags would be introduced. Ob-
Parsing accuracy (F1)
1303
viously, it introduces too many tags at once and is
difficult to refine appropriate grammars in the sub-
sequent step starting with this over-splitting train-
ing set.
</bodyText>
<subsectionHeader confidence="0.998061">
4.3 Grammar Refinement Experiments
</subsectionHeader>
<bodyText confidence="0.99996904">
Several experiments were carried out on Chinese
and English to verify the effectiveness of refining
grammars with semantic knowledge. We took the
most general hypernym as the semantic represen-
tation, and the polysemous words in the training
set were annotated with the WSD strategy of first
sense option.
In our experiments, three kinds of method were
compared. The baseline was trained on the raw
training set with the automatic hierarchical state-
split approach. Then, we improved it with the se-
mantic annotation, which annotated the raw train-
ing set with the most general hypernyms as se-
mantic representations, while keeping the train-
ing approach used in the baseline unchanged.
Further, our knowledge-based criterion was in-
troduced to supervise the automatic hierarchical
state-split process with semantic knowledge.
In this section, since most of the parsers (includ-
ing the baseline parser and our advanced parsers)
had the same behavior on development set that the
accuracy continued increasing in the five begin-
ning iterations and then dropped at the sixth iter-
ation, we chose the results at the fifth iteration as
our final test set parsing performance.
</bodyText>
<subsectionHeader confidence="0.785291">
Performances on Chinese
</subsectionHeader>
<bodyText confidence="0.961279666666667">
Figure 4 shows that refining grammars with se-
mantic knowledge can help improve parsing per-
formance significantly on Chinese (sentences of
length 40 or less). Benefitting from the good start-
ing annotations, our parser achieved significant
improvements compared with the baseline (86.8%
vs. 86.1%, p&lt;.08). It proved that the good start-
ing annotations with semantic knowledge were ef-
fective in the splitting process. Further, we su-
pervised the splitting of the new semantic-related
tags from the semantic annotations, and achieved
the best results at the fifth iteration. The best F1
score reached 87.5%, with an error rate reduction
of 10.1 %, relative to the baseline (p&lt;.004).
Table 3 compared our methods with the best
previous works on Chinese. The result showed
that refining grammars integrated with semantic
knowledge could resolve syntactic ambiguities re-
Figure 4: Performances at the fifth iteration on
Chinese (sentences of length 40 or less) with three
methods: the baseline, the parser trained on the
semantic annotations with automatic method, and
the parser trained on the semantic annotations with
knowledge-based criterion.
</bodyText>
<table confidence="0.999840285714286">
Parser &lt; 40 words LP all F1
LP LR F1 LR
Chiang and 81.1 78.8 79.9 78.0 75.2 76.6
Bikel (2002)
Petrov and 86.9 85.7 86.3 84.8 81.9 83.3
Klein (2007)
This Paper 88.9 86.0 87.5 86.0 83.1 84.5
</table>
<tableCaption confidence="0.995106">
Table 3: Our final parsing performance compared
</tableCaption>
<bodyText confidence="0.611014666666667">
with the best previous works on Chinese.
markably and achieved the state-of-the-art perfor-
mance on Chinese.
</bodyText>
<subsectionHeader confidence="0.696272">
Performances on English
</subsectionHeader>
<bodyText confidence="0.999946133333333">
In order to verify the effectiveness of our method
on other languages, we carried out some experi-
ments on English. HowNet is a common sense
knowledge base in Chinese and English, there-
fore, it was still utilized as the knowledge source
in these experiments.
The same three methods were compared on En-
glish (sentences of length 40 or less), and the re-
sults were showed in Table 4. Compared with the
baseline (90.1%), the parsers trained with the se-
mantic annotation, while using different splitting
methods introduced in Section 3, achieved an Fl
score of 90.2% and 90.3% respectively. The re-
sults showed that our methods could get a small
but stable improvements on English (p&lt;.08).
</bodyText>
<page confidence="0.536333">
1304
</page>
<table confidence="0.985271142857143">
Subcategory Refined from the Original Training Set
PN-0 援外(aid foreign), 叔婶(aunt), 自家(self), 汝(you), 予(donate), 那些(those), 相貌(appearence),
自个儿(self), 咱们(we), 彼(that), 以上(the above), 那边(there), 其他(other), 以下(below)
Subcategories Fefined from the Good Starting Annotations
PN-0 叔婶(aunt), 自家(self), 自个儿(self), 咱们(we), 汝(you)
PN-Event-0 援外(aid foreign), 予(donate)
PN-AttributeValue-2 以上(the above), 那些(those), 彼(that), 其他(other), 以下(below)
</table>
<tableCaption confidence="0.976573">
Table 5: Several subcategories that generated from the original training set and the good starting annota-
tions respectively.
</tableCaption>
<table confidence="0.9990904">
Method Fl
Baseline 90.1
Semantic Annotations 90.2
Semantic Annotations &amp; 90.3
Knowledge-based Criterion
</table>
<tableCaption confidence="0.953234">
Table 4: Performances at the fifth iteration on En-
</tableCaption>
<bodyText confidence="0.982297631578947">
glish (sentences of length 40 or less) with three
methods: the baseline, the parser trained on the
semantic annotations with automatic method, and
the parser trained on the semantic annotations with
knowledge-based criterion.
These results on English were preliminary, and
we did not introduce any language dependent op-
eration such as morphological processing. Since
only the lemma of English words can be found
in HowNet, we just annotated two kinds of POS
tags &amp;quot;VB&amp;quot;(Verb, base form) and &amp;quot;NN&amp;quot;(Noun, sin-
gular or mass) with semantic knowledge, on the
contrary, we annotated almost all POS tags whose
corresponding words could be found in HowNet
on Chinese. This might be the reason that the
improvement on the English Treebank was much
smaller than that of Chinese. It is expected to
achieve more improvements through some mor-
phological analysis in the future.
</bodyText>
<subsectionHeader confidence="0.863209">
4.4 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999962">
So far, a new strategy has been introduced to re-
fine the grammars in two steps, and achieved sig-
nificant improvements on parsing performance. In
this section, we analyze the grammars learned at
different steps, attempting to explain how the se-
mantic knowledge works.
It is hard to inspect all the grammars by hand.
Since the semantic knowledge is mainly used for
generating and splitting new semantic-related tags
in our method, we focus on the refined subcate-
gories of these tags.
First, we examine the refined subcategories of
POS tags, which are generated from the original
training set and the good starting annotations re-
spectively. Several subcategories are listed and
compared in Table 5, along with their frequent
words. It can be seen that the subcategories refined
with semantic knowledge are more consistent than
the previous one. For example, the subcategory
&amp;quot;PN-0&amp;quot;, which is refined from the original training
set, produces a lot of words without semantic con-
sistence. In contrast, we refine the subcategories
&amp;quot;PN-0&amp;quot;, &amp;quot;PN-Event-0&amp;quot; and &amp;quot;PN-AttributeValue-2&amp;quot;
from the good starting annotations. Each of them
produces a small but semantic consistent word set.
In order to inspect the difference between the
automatic splitting process and the semantic based
one, we compare the numbers of subcategories re-
fined in these two processes. Since it is hard to list
all the semantic-related tags here, three parts of
the semantic-related tags were selected and listed
in Table 6, along with the number of their subcat-
egories. The first part is the noun and verb related
tags, which are most heavily split in both two pro-
cesses. It is clear that the semantic based split-
ting process can generate more subcategories than
the automatic one, because the semantic structures
of noun and verb are sophisticated. The second
part lists the tags that have much more subcate-
gories (&gt; 4) from the automatic splitting process
than the semantic based one, and the third part
vice verse. It can be seen that most of the sub-
categories in the second part are functional cate-
gories, while most of the subcategories in the third
part are content categories. It means that the se-
mantic based splitting process is prone to generat-
ing less subcategory for the functional categories,
but more subcategories for the content categories.
This tendency is in accordance with the linguis-
tic intuition. We believe that it is the main effect
</bodyText>
<table confidence="0.991627181818182">
1305
Semantic-related Automatic split Semantic based
tag number split numebr
NN-Attribute 30 30
NN-AttributeValue 25 27
NN-Entity 32 32
NN-Event 31 30
VV-Attribute 2 2
VV-AttributeValue 27 27
VV-Entity 22 26
VV-Event 29 32
BA-event 13 5
CS-AttributeValue 29 16
CS-entity 22 15
OD-Attribute 13 7
PN-Attribute 26 22
AS-AttributeValue 2 7
JJ-event 4 8
NR-AttributeValue 9 13
NT-event 12 18
VA-AttributeValue 22 27
VA-event 7 11
</table>
<tableCaption confidence="0.948394">
Table 6: The number of subcategories learned
</tableCaption>
<bodyText confidence="0.9829085">
from two approaches: the automatic hierarchical
state-splitting, and the semantic based splitting.
of our knowledge-based criterion, because it ad-
justs the splitting results dynamically with seman-
tic knowledge, which can alleviate the overfitting
risk.
</bodyText>
<sectionHeader confidence="0.999671" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999985590909091">
In this paper, we present a novel approach to in-
tegrate semantic knowledge into the hierarchical
state-split process for grammar refinement, which
yields better accuracies on Chinese than previ-
ous methods. The improvements are mainly ow-
ing to two aspects. Firstly, the original treebank
is initialized by annotating the tag of each POS
node with the most general hypernym of its ter-
minal word, which reduces the search space for
the EM algorithm and brings an initial restrict to
the following splitting step. Secondly, the splitting
process is supervised by a knowledge-based crite-
rion with the new semantic-related tags. Benefit-
ting from the hierarchical semantic knowledge, the
proposed approach alleviates the overfitting risk in
a knowledge-driven manner. Experimental results
reveal that the semantic knowledge is of great use
to syntactic disambiguation. The further analysis
on the refined grammars shows that, our method
tends to split the content categories more often
than the baseline method and the function classes
less often.
</bodyText>
<sectionHeader confidence="0.998643" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9985664">
We thank Yaozhong Zhang for the enlighten-
ing discussions. We also thank the anony-
mous reviewers who gave very helpful com-
ments. The work was supported in part by the
National Natural Science Foundation of China
(60535030; 60605016), the National High Tech-
nology Research and Development Program of
China (2006AA010103), the National Key Ba-
sic Research Program of China (2004CB318005,
2004CB318105).
</bodyText>
<sectionHeader confidence="0.998201" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998313253731343">
E. Agirre, T. Baldwin and D. Martinez. 2008. Improv-
ing parsing and PP attachment performance with
sense information. In Proc. of ACL’08, pages 317-
325.
D. Bikel. 2000. A statistical model for pars-
ing and word-sense disambiguation. In Proc. of
EMNLP/VLC’2000, pages 155-163.
D. Bikel. 2004. Intricacies of Collins’ parsing model.
Computational Linguistics, 30(4):479-511.’
E. Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proc. of
AAAI’97, pages 598-603.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of NAACL’00, pages 132-139.
E Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxEnt discriminative reranking.
In Proc. of ACL’05, pages 173-180.
D. Chiang and D. Bikel. 2002. Recovering latent infor-
mation in treebanks. In Proc. of COLING’02, pages
183-189.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proc. ofACL’97, pages 16-
23.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, U. of Penn-
sylvania.
Z. Dong and Q. Dong. 2000. HowNet Chinese-
English conceptual database. Technical Re-
port Online Software Database, Released at ACL.
http://www.keenage.com.
1306
S. Fujita, F. Bond, S. Oepen and T. Tanaka 2007. Ex-
ploiting semantic information for HPSG parse se-
lection. In ACL 2007 Workshop on Deep Linguistic
Processing, pages 25-32.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24(4):613-
631.
D. Klein and C. Manning. 2003. Accurate unlexical-
ized parsing. In Proc. of ACL’03, pages 423-430.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313-330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Prob-
abilistic CFG with latent annotations. In Proc. of
ACL’05, pages 75-82.
George A. Miller, Martin Chodorow, Shari Landes,
Claudia Leacock, and Robert G. Thomas. 1994. Us-
ing a semantic concordance for sense identification.
In Proc. ofARPA-HLT Workshop., pages 240-243.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proc. of COLING-ACL’06, pages
443–440.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proc. of HLT-NAACL’07,
pages 404-411.
D. Xiong, S. Li, Q. Liu, S. Lin, and Y. Qian. 2005.
Parsing the Penn Chinese treebank with semantic
knowledge. In Proc. of IJCNLP’05, pages 70-81.
N. Xue, F.-D. Chiou, and M. Palmer. 2002. Building
a large scale annotated Chinese corpus. In Proc. of
COLING’02, pages 1-8.
Y. Zhang. 2008. The Study and Realization of Chinese
Parsing with Semantic and Sentence Type Informa-
tion. Master thesis, Peking University.
</reference>
<page confidence="0.795253">
1307
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.385967">
<title confidence="0.999369">Refining Grammars for Parsing with Hierarchical Semantic Knowledge</title>
<author confidence="0.979352">Yang Fan Lin</author>
<author confidence="0.979352">Meng Zhang</author>
<author confidence="0.979352">Xihong</author>
<affiliation confidence="0.76645925">Speech and Hearing Research Key Laboratory of Machine Perception (Ministry of School of Electronics Engineering and Computer Peking University, Beijing, 100871,</affiliation>
<email confidence="0.935963">fanyang,zhangm,chi@pku.edu.cn</email>
<abstract confidence="0.999209923076923">This paper proposes a novel method to refine the grammars in parsing by utilizing semantic knowledge from HowNet. Based on the hierarchical state-split approach, which can refine grammars automatically in a data-driven manner, this study introduces semantic knowledge into the splitting process at two steps. Firstly, each part-of-speech node will be annotated with a semantic tag of its terminal word. These new tags generated in this step are semantic-related, which can provide a good start for splitting. Secondly, a knowledge-based criterion is used to supervise the hierarchical splitting of these semantic-related tags, which can alleviate overfitting. The experiments are carried out on both Chinese and English Penn Treebank show that the refined grammars with semantic knowledge can improve parsing performance significantly. Especially with respect to Chinese, our achieves an of 87.5%, which is the best published result we are aware of.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>T Baldwin</author>
<author>D Martinez</author>
</authors>
<title>Improving parsing and PP attachment performance with sense information.</title>
<date>2008</date>
<booktitle>In Proc. of ACL’08,</booktitle>
<pages>317--325</pages>
<contexts>
<context position="6852" citStr="Agirre et al. (2008)" startWordPosition="1049" endWordPosition="1052">ant improvement on Penn Chinese Treebank. This work only considers the most special hypernym of a word, rather than other hypernyms at different levels of the hypernymhyponym hierarchy. Then, Fujita et al. (2007) uses the Hinoki treebank as training data to train a discriminative parse selection model combining syntactic features and word sense information. Instead of utilizing the most special hypernym, the word sense information in this model is embodied with more general concepts. Based on the hand-craft sense information, this model is proved to be effective for parse selection. Recently, Agirre et al. (2008) train two lexicalized models (Charniak, 2000; Bikel, 2004) on preprocessed inputs, where content words are substituted with semantic classes from WordNet. By integrating the word semantic classes into the process of parser training directly, these two models obtain significant improvements in both parsing and prepositional phrase attachment tasks. Zhang (2008) does preliminary work on integrating POS with semantic class of words directly, which can not only alleviate the confusion in parsing, but also infer syntax and semantic information at the same time. 2.2 The Hierarchical State-split Par</context>
<context position="21347" citStr="Agirre et al. (2008)" startWordPosition="3359" endWordPosition="3362">e parsing was HowNet, which has been introduced in Subsection 3.1. Statistical significance was checked using Dan Bikel’s randomized parsing evaluation comparator with the default setting of 10,000 iterations3. 4.2 Semantic Representation Experiments First of all, we ran experiments with different semantic representation methods on Chinese. The polysemous words in the training set were annotated with the WSD strategy of first sense option, 1http://nlp.cs.nyu.edu/evalb/. 2http://code.google.com/p/berkeleyparser/. 3http://www.cis.upenn.edu/ dbikel/software.html. which was proved to be useful in Agirre et al. (2008). As mentioned in Subsection 3.1, the semantic information of each word can be represented as a hierarchical relation among its hypernyms from specialty to generalization in HowNet. In order to choose the appropriate level of granularity to represent words, we annotated the training set with different levels of granularity as semantic representation. In our experiments, the automatic hierarchical state-split process is used to train models on these training sets with different level of semantic representation. We tried two kinds of semantic representations, one is using the most general hypern</context>
</contexts>
<marker>Agirre, Baldwin, Martinez, 2008</marker>
<rawString>E. Agirre, T. Baldwin and D. Martinez. 2008. Improving parsing and PP attachment performance with sense information. In Proc. of ACL’08, pages 317-325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
</authors>
<title>A statistical model for parsing and word-sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proc. of EMNLP/VLC’2000,</booktitle>
<pages>155--163</pages>
<contexts>
<context position="5564" citStr="Bikel (2000)" startWordPosition="848" endWordPosition="849">veral experiments are carried out for evaluation in Section 4. Conclusions are drawn in Section 5. 2 Background This paper tries to refine the grammars through an improved hierarchical state-split process integrated with semantic knowledge. The related works are reviewed as follows. 2.1 Lexical Semantic Related Parsing Semantic knowledge is useful to resolving syntactic ambiguities, and a variety of researches focus on how to utilize it. Especially in recent years, a conviction arose that semantic knowledge could be incorporated into the lexicalized parsing. Based on the lexicalized grammars, Bikel (2000) attempts at combining parsing and word sense disambiguation in a unified model, using a subset of SemCor (Miller et al., 1994). Bikel (2000) evaluates this model in a parsing context with sense information from WordNet, but does not get improvements on parsing performance. Xiong et al. (2005) combines word sense from CiLin and HowNet (two Chinese semantic resources) in a generative parsing model, which generalizes standard bilexical dependencies to wordclass dependencies, and indeed help to tackle the sparseness problem in lexicalized parsing. The experiments show that the parse model combine</context>
</contexts>
<marker>Bikel, 2000</marker>
<rawString>D. Bikel. 2000. A statistical model for parsing and word-sense disambiguation. In Proc. of EMNLP/VLC’2000, pages 155-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<pages>30--4</pages>
<contexts>
<context position="6911" citStr="Bikel, 2004" startWordPosition="1060" endWordPosition="1061">the most special hypernym of a word, rather than other hypernyms at different levels of the hypernymhyponym hierarchy. Then, Fujita et al. (2007) uses the Hinoki treebank as training data to train a discriminative parse selection model combining syntactic features and word sense information. Instead of utilizing the most special hypernym, the word sense information in this model is embodied with more general concepts. Based on the hand-craft sense information, this model is proved to be effective for parse selection. Recently, Agirre et al. (2008) train two lexicalized models (Charniak, 2000; Bikel, 2004) on preprocessed inputs, where content words are substituted with semantic classes from WordNet. By integrating the word semantic classes into the process of parser training directly, these two models obtain significant improvements in both parsing and prepositional phrase attachment tasks. Zhang (2008) does preliminary work on integrating POS with semantic class of words directly, which can not only alleviate the confusion in parsing, but also infer syntax and semantic information at the same time. 2.2 The Hierarchical State-split Parsing In order to alleviate the context-free assumptions, Pe</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>D. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30(4):479-511.’</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical parsing with a contextfree grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proc. of AAAI’97,</booktitle>
<pages>598--603</pages>
<contexts>
<context position="2085" citStr="Charniak, 1997" startWordPosition="307" endWordPosition="308"> one way or another (Collins, 1999; Charniak and Johnson, 2005; Petrov and Klein, 2007). However, restricted by the strong contextfree assumptions, the original PCFG model which simply takes the grammars and probabilities off a treebank, does not perform well. Therefore, a variety of techniques have been developed to enrich and generalize the original grammar, ranging from lexicalization to symbol annotation. Corresponding author: Xihong Wu. Lexicalized PCFGs use the structural features on the lexical head of phrasal node in a tree, and get significant improvements for parsing (Collins, 1997; Charniak, 1997; Collins, 1999; Charniak, 2000). However, they suffer from the problem of fundamental sparseness of the lexical dependency information. (Klein and Manning, 2003). In order to deal with this limitation, a variety of unlexicalized parsing techniques have been proposed. Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. Then, some manual and automatic symbol splitting methods are presented, which get comparable performance with lexicalized parsers (Klein and Manning, 2003; Matsuzaki et al.,</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997. Statistical parsing with a contextfree grammar and word statistics. In Proc. of AAAI’97, pages 598-603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. of NAACL’00,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="2117" citStr="Charniak, 2000" startWordPosition="311" endWordPosition="312">999; Charniak and Johnson, 2005; Petrov and Klein, 2007). However, restricted by the strong contextfree assumptions, the original PCFG model which simply takes the grammars and probabilities off a treebank, does not perform well. Therefore, a variety of techniques have been developed to enrich and generalize the original grammar, ranging from lexicalization to symbol annotation. Corresponding author: Xihong Wu. Lexicalized PCFGs use the structural features on the lexical head of phrasal node in a tree, and get significant improvements for parsing (Collins, 1997; Charniak, 1997; Collins, 1999; Charniak, 2000). However, they suffer from the problem of fundamental sparseness of the lexical dependency information. (Klein and Manning, 2003). In order to deal with this limitation, a variety of unlexicalized parsing techniques have been proposed. Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. Then, some manual and automatic symbol splitting methods are presented, which get comparable performance with lexicalized parsers (Klein and Manning, 2003; Matsuzaki et al., 2005). Recently, Petrov et al. </context>
<context position="6897" citStr="Charniak, 2000" startWordPosition="1058" endWordPosition="1059"> only considers the most special hypernym of a word, rather than other hypernyms at different levels of the hypernymhyponym hierarchy. Then, Fujita et al. (2007) uses the Hinoki treebank as training data to train a discriminative parse selection model combining syntactic features and word sense information. Instead of utilizing the most special hypernym, the word sense information in this model is embodied with more general concepts. Based on the hand-craft sense information, this model is proved to be effective for parse selection. Recently, Agirre et al. (2008) train two lexicalized models (Charniak, 2000; Bikel, 2004) on preprocessed inputs, where content words are substituted with semantic classes from WordNet. By integrating the word semantic classes into the process of parser training directly, these two models obtain significant improvements in both parsing and prepositional phrase attachment tasks. Zhang (2008) does preliminary work on integrating POS with semantic class of words directly, which can not only alleviate the confusion in parsing, but also infer syntax and semantic information at the same time. 2.2 The Hierarchical State-split Parsing In order to alleviate the context-free a</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A maximum-entropy-inspired parser. In Proc. of NAACL’00, pages 132-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and maxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of ACL’05,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="1533" citStr="Charniak and Johnson, 2005" startWordPosition="223" endWordPosition="226"> a knowledge-based criterion is used to supervise the hierarchical splitting of these semantic-related tags, which can alleviate overfitting. The experiments are carried out on both Chinese and English Penn Treebank show that the refined grammars with semantic knowledge can improve parsing performance significantly. Especially with respect to Chinese, our parser achieves an F1 score of 87.5%, which is the best published result we are aware of. 1 Introduction At present, most high-performance parsers are based on probabilistic context-free grammars (PCFGs) in one way or another (Collins, 1999; Charniak and Johnson, 2005; Petrov and Klein, 2007). However, restricted by the strong contextfree assumptions, the original PCFG model which simply takes the grammars and probabilities off a treebank, does not perform well. Therefore, a variety of techniques have been developed to enrich and generalize the original grammar, ranging from lexicalization to symbol annotation. Corresponding author: Xihong Wu. Lexicalized PCFGs use the structural features on the lexical head of phrasal node in a tree, and get significant improvements for parsing (Collins, 1997; Charniak, 1997; Collins, 1999; Charniak, 2000). However, they </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E Charniak and M. Johnson. 2005. Coarse-to-fine nbest parsing and maxEnt discriminative reranking. In Proc. of ACL’05, pages 173-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
<author>D Bikel</author>
</authors>
<title>Recovering latent information in treebanks.</title>
<date>2002</date>
<booktitle>In Proc. of COLING’02,</booktitle>
<pages>183--189</pages>
<marker>Chiang, Bikel, 2002</marker>
<rawString>D. Chiang and D. Bikel. 2002. Recovering latent information in treebanks. In Proc. of COLING’02, pages 183-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. ofACL’97,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="2069" citStr="Collins, 1997" startWordPosition="305" endWordPosition="306">mars (PCFGs) in one way or another (Collins, 1999; Charniak and Johnson, 2005; Petrov and Klein, 2007). However, restricted by the strong contextfree assumptions, the original PCFG model which simply takes the grammars and probabilities off a treebank, does not perform well. Therefore, a variety of techniques have been developed to enrich and generalize the original grammar, ranging from lexicalization to symbol annotation. Corresponding author: Xihong Wu. Lexicalized PCFGs use the structural features on the lexical head of phrasal node in a tree, and get significant improvements for parsing (Collins, 1997; Charniak, 1997; Collins, 1999; Charniak, 2000). However, they suffer from the problem of fundamental sparseness of the lexical dependency information. (Klein and Manning, 2003). In order to deal with this limitation, a variety of unlexicalized parsing techniques have been proposed. Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. Then, some manual and automatic symbol splitting methods are presented, which get comparable performance with lexicalized parsers (Klein and Manning, 2003; M</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proc. ofACL’97, pages 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>U. of Pennsylvania.</institution>
<contexts>
<context position="1505" citStr="Collins, 1999" startWordPosition="221" endWordPosition="222">ting. Secondly, a knowledge-based criterion is used to supervise the hierarchical splitting of these semantic-related tags, which can alleviate overfitting. The experiments are carried out on both Chinese and English Penn Treebank show that the refined grammars with semantic knowledge can improve parsing performance significantly. Especially with respect to Chinese, our parser achieves an F1 score of 87.5%, which is the best published result we are aware of. 1 Introduction At present, most high-performance parsers are based on probabilistic context-free grammars (PCFGs) in one way or another (Collins, 1999; Charniak and Johnson, 2005; Petrov and Klein, 2007). However, restricted by the strong contextfree assumptions, the original PCFG model which simply takes the grammars and probabilities off a treebank, does not perform well. Therefore, a variety of techniques have been developed to enrich and generalize the original grammar, ranging from lexicalization to symbol annotation. Corresponding author: Xihong Wu. Lexicalized PCFGs use the structural features on the lexical head of phrasal node in a tree, and get significant improvements for parsing (Collins, 1997; Charniak, 1997; Collins, 1999; Cha</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, U. of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Dong</author>
<author>Q Dong</author>
</authors>
<title>HowNet ChineseEnglish conceptual database.</title>
<date>2000</date>
<booktitle>Technical Report Online Software Database, Released at ACL. http://www.keenage.com.</booktitle>
<contexts>
<context position="3547" citStr="Dong and Dong, 2000" startWordPosition="531" endWordPosition="534">od, the nonterminals are split to different degrees, as appropriate to the actual complexity in the data. The grammars refined in this way are proved to be much more accurate and compact than previous work on automatic annotation. This data-driven method still suffers from the overfitting problem, which may be improved by integrating other external information. In this paper, we propose a novel method that combines the strengths of both data-driven and knowledge-driven strategies to refine grammars. Based on the work proposed by Petrov et al. (2006), we use the semantic knowledge from HowNet (Dong and Dong, 2000) to supervise the hierarchical state-split process at the part-ofspeech(POS) level. At first, we define the most general hypernym in HowNet as the semantic class of a word, and then use this semantic class to initialize the tag of each POS node. In this way, a new set of semantic-related tags is generated, and 1298 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1298–1307, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP a good starting annotation is provided to reduce the search space for the EM algorithm in the splitting process. Then, in order t</context>
</contexts>
<marker>Dong, Dong, 2000</marker>
<rawString>Z. Dong and Q. Dong. 2000. HowNet ChineseEnglish conceptual database. Technical Report Online Software Database, Released at ACL. http://www.keenage.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fujita</author>
<author>F Bond</author>
<author>S Oepen</author>
<author>T Tanaka</author>
</authors>
<title>Exploiting semantic information for HPSG parse selection.</title>
<date>2007</date>
<booktitle>In ACL 2007 Workshop on Deep Linguistic Processing,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="6444" citStr="Fujita et al. (2007)" startWordPosition="984" endWordPosition="987">g performance. Xiong et al. (2005) combines word sense from CiLin and HowNet (two Chinese semantic resources) in a generative parsing model, which generalizes standard bilexical dependencies to wordclass dependencies, and indeed help to tackle the sparseness problem in lexicalized parsing. The experiments show that the parse model combined with word sense and the most special hypernyms achieves a significant improvement on Penn Chinese Treebank. This work only considers the most special hypernym of a word, rather than other hypernyms at different levels of the hypernymhyponym hierarchy. Then, Fujita et al. (2007) uses the Hinoki treebank as training data to train a discriminative parse selection model combining syntactic features and word sense information. Instead of utilizing the most special hypernym, the word sense information in this model is embodied with more general concepts. Based on the hand-craft sense information, this model is proved to be effective for parse selection. Recently, Agirre et al. (2008) train two lexicalized models (Charniak, 2000; Bikel, 2004) on preprocessed inputs, where content words are substituted with semantic classes from WordNet. By integrating the word semantic cla</context>
</contexts>
<marker>Fujita, Bond, Oepen, Tanaka, 2007</marker>
<rawString>S. Fujita, F. Bond, S. Oepen and T. Tanaka 2007. Exploiting semantic information for HPSG parse selection. In ACL 2007 Workshop on Deep Linguistic Processing, pages 25-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--4</pages>
<contexts>
<context position="2368" citStr="Johnson (1998)" startWordPosition="348" endWordPosition="349">f techniques have been developed to enrich and generalize the original grammar, ranging from lexicalization to symbol annotation. Corresponding author: Xihong Wu. Lexicalized PCFGs use the structural features on the lexical head of phrasal node in a tree, and get significant improvements for parsing (Collins, 1997; Charniak, 1997; Collins, 1999; Charniak, 2000). However, they suffer from the problem of fundamental sparseness of the lexical dependency information. (Klein and Manning, 2003). In order to deal with this limitation, a variety of unlexicalized parsing techniques have been proposed. Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. Then, some manual and automatic symbol splitting methods are presented, which get comparable performance with lexicalized parsers (Klein and Manning, 2003; Matsuzaki et al., 2005). Recently, Petrov et al. (2006) introduces an automatic hierarchical state-split approach to refine the grammars, which can alternately split and merge the basic nonterminals by the Expectation-Maximization (EM) algorithm. In this method, the nonterminals are split to differe</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>M. Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613-631.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. of ACL’03,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="2247" citStr="Klein and Manning, 2003" startWordPosition="327" endWordPosition="330">original PCFG model which simply takes the grammars and probabilities off a treebank, does not perform well. Therefore, a variety of techniques have been developed to enrich and generalize the original grammar, ranging from lexicalization to symbol annotation. Corresponding author: Xihong Wu. Lexicalized PCFGs use the structural features on the lexical head of phrasal node in a tree, and get significant improvements for parsing (Collins, 1997; Charniak, 1997; Collins, 1999; Charniak, 2000). However, they suffer from the problem of fundamental sparseness of the lexical dependency information. (Klein and Manning, 2003). In order to deal with this limitation, a variety of unlexicalized parsing techniques have been proposed. Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. Then, some manual and automatic symbol splitting methods are presented, which get comparable performance with lexicalized parsers (Klein and Manning, 2003; Matsuzaki et al., 2005). Recently, Petrov et al. (2006) introduces an automatic hierarchical state-split approach to refine the grammars, which can alternately split and merge the</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Accurate unlexicalized parsing. In Proc. of ACL’03, pages 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<pages>19--2</pages>
<contexts>
<context position="19428" citStr="Marcus et al. (1993)" startWordPosition="3074" endWordPosition="3077">emantically supervised splitting process. The left part of this figure is the subcategories of the semantic-related tag &amp;quot;NN-Entity&amp;quot;, which is split hierarchically. As expressed by the dashed line, each subcategory corresponds to one hypernym in the right part of this figure. If the hypernym node has no hyponym, the corresponding subcategory will stop splitting. The mapping from each subcategory of these semantic-related tags to the hypernym at the appropriate level is implemented with the word set related to this subcategory. As it is shown in Fig1302 DataSet Chinese English Xue et al. (2002) Marcus et al. (1993) TrainSet Art. 1-270,400-1151 Sections 2-21 DevSet Articles 301-325 Section 22 TestSet Articles 271-300 Section 23 Table 2: Experimental setup. ure 2, the original tag &amp;quot;NN-Entity&amp;quot; treats all the words it products as its word set. Once the original category is split into two subcategories, its word set is also split, through forcedly dividing each word in the word set into one subcategory which is most frequent with this word. And then, each subcategory is mapped onto the most specific hypernym that contains its related word set entirely in HowNet. On this basis, a new knowledgebased criterion </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsuzaki</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proc. of ACL’05,</booktitle>
<pages>75--82</pages>
<contexts>
<context position="2691" citStr="Matsuzaki et al., 2005" startWordPosition="395" endWordPosition="398">7; Charniak, 1997; Collins, 1999; Charniak, 2000). However, they suffer from the problem of fundamental sparseness of the lexical dependency information. (Klein and Manning, 2003). In order to deal with this limitation, a variety of unlexicalized parsing techniques have been proposed. Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. Then, some manual and automatic symbol splitting methods are presented, which get comparable performance with lexicalized parsers (Klein and Manning, 2003; Matsuzaki et al., 2005). Recently, Petrov et al. (2006) introduces an automatic hierarchical state-split approach to refine the grammars, which can alternately split and merge the basic nonterminals by the Expectation-Maximization (EM) algorithm. In this method, the nonterminals are split to different degrees, as appropriate to the actual complexity in the data. The grammars refined in this way are proved to be much more accurate and compact than previous work on automatic annotation. This data-driven method still suffers from the overfitting problem, which may be improved by integrating other external information. </context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In Proc. of ACL’05, pages 75-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Martin Chodorow</author>
<author>Shari Landes</author>
<author>Claudia Leacock</author>
<author>Robert G Thomas</author>
</authors>
<title>Using a semantic concordance for sense identification.</title>
<date>1994</date>
<booktitle>In Proc. ofARPA-HLT Workshop.,</booktitle>
<pages>240--243</pages>
<contexts>
<context position="5691" citStr="Miller et al., 1994" startWordPosition="867" endWordPosition="870">per tries to refine the grammars through an improved hierarchical state-split process integrated with semantic knowledge. The related works are reviewed as follows. 2.1 Lexical Semantic Related Parsing Semantic knowledge is useful to resolving syntactic ambiguities, and a variety of researches focus on how to utilize it. Especially in recent years, a conviction arose that semantic knowledge could be incorporated into the lexicalized parsing. Based on the lexicalized grammars, Bikel (2000) attempts at combining parsing and word sense disambiguation in a unified model, using a subset of SemCor (Miller et al., 1994). Bikel (2000) evaluates this model in a parsing context with sense information from WordNet, but does not get improvements on parsing performance. Xiong et al. (2005) combines word sense from CiLin and HowNet (two Chinese semantic resources) in a generative parsing model, which generalizes standard bilexical dependencies to wordclass dependencies, and indeed help to tackle the sparseness problem in lexicalized parsing. The experiments show that the parse model combined with word sense and the most special hypernyms achieves a significant improvement on Penn Chinese Treebank. This work only co</context>
</contexts>
<marker>Miller, Chodorow, Landes, Leacock, Thomas, 1994</marker>
<rawString>George A. Miller, Martin Chodorow, Shari Landes, Claudia Leacock, and Robert G. Thomas. 1994. Using a semantic concordance for sense identification. In Proc. ofARPA-HLT Workshop., pages 240-243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL’06,</booktitle>
<pages>443--440</pages>
<contexts>
<context position="2723" citStr="Petrov et al. (2006)" startWordPosition="400" endWordPosition="403">harniak, 2000). However, they suffer from the problem of fundamental sparseness of the lexical dependency information. (Klein and Manning, 2003). In order to deal with this limitation, a variety of unlexicalized parsing techniques have been proposed. Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. Then, some manual and automatic symbol splitting methods are presented, which get comparable performance with lexicalized parsers (Klein and Manning, 2003; Matsuzaki et al., 2005). Recently, Petrov et al. (2006) introduces an automatic hierarchical state-split approach to refine the grammars, which can alternately split and merge the basic nonterminals by the Expectation-Maximization (EM) algorithm. In this method, the nonterminals are split to different degrees, as appropriate to the actual complexity in the data. The grammars refined in this way are proved to be much more accurate and compact than previous work on automatic annotation. This data-driven method still suffers from the overfitting problem, which may be improved by integrating other external information. In this paper, we propose a nove</context>
<context position="7529" citStr="Petrov et al. (2006)" startWordPosition="1152" endWordPosition="1155">4) on preprocessed inputs, where content words are substituted with semantic classes from WordNet. By integrating the word semantic classes into the process of parser training directly, these two models obtain significant improvements in both parsing and prepositional phrase attachment tasks. Zhang (2008) does preliminary work on integrating POS with semantic class of words directly, which can not only alleviate the confusion in parsing, but also infer syntax and semantic information at the same time. 2.2 The Hierarchical State-split Parsing In order to alleviate the context-free assumptions, Petrov et al. (2006) proposes a hierarchical statesplit approach to refine and generalize the original grammars, and achieves state-of-the-art performance. Starting with the basic nonterminals, this method repeats the split-merge (SM) cycle to increase the complexity of grammars. That is, it splits every symbol into two, and then re-merges some new subcategories based on the likelihood computation. Splitting In each splitting stage, the previous syntactic symbol is split into two subcategories, and the EM algorithm is adopted to learn probability of the rules for these latent annotations to maximize the likelihoo</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. of COLING-ACL’06, pages 443–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proc. of HLT-NAACL’07,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="1558" citStr="Petrov and Klein, 2007" startWordPosition="227" endWordPosition="230"> is used to supervise the hierarchical splitting of these semantic-related tags, which can alleviate overfitting. The experiments are carried out on both Chinese and English Penn Treebank show that the refined grammars with semantic knowledge can improve parsing performance significantly. Especially with respect to Chinese, our parser achieves an F1 score of 87.5%, which is the best published result we are aware of. 1 Introduction At present, most high-performance parsers are based on probabilistic context-free grammars (PCFGs) in one way or another (Collins, 1999; Charniak and Johnson, 2005; Petrov and Klein, 2007). However, restricted by the strong contextfree assumptions, the original PCFG model which simply takes the grammars and probabilities off a treebank, does not perform well. Therefore, a variety of techniques have been developed to enrich and generalize the original grammar, ranging from lexicalization to symbol annotation. Corresponding author: Xihong Wu. Lexicalized PCFGs use the structural features on the lexical head of phrasal node in a tree, and get significant improvements for parsing (Collins, 1997; Charniak, 1997; Collins, 1999; Charniak, 2000). However, they suffer from the problem o</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved inference for unlexicalized parsing. In Proc. of HLT-NAACL’07, pages 404-411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Xiong</author>
<author>S Li</author>
<author>Q Liu</author>
<author>S Lin</author>
<author>Y Qian</author>
</authors>
<title>Parsing the Penn Chinese treebank with semantic knowledge.</title>
<date>2005</date>
<booktitle>In Proc. of IJCNLP’05,</booktitle>
<pages>70--81</pages>
<contexts>
<context position="5858" citStr="Xiong et al. (2005)" startWordPosition="893" endWordPosition="896"> Lexical Semantic Related Parsing Semantic knowledge is useful to resolving syntactic ambiguities, and a variety of researches focus on how to utilize it. Especially in recent years, a conviction arose that semantic knowledge could be incorporated into the lexicalized parsing. Based on the lexicalized grammars, Bikel (2000) attempts at combining parsing and word sense disambiguation in a unified model, using a subset of SemCor (Miller et al., 1994). Bikel (2000) evaluates this model in a parsing context with sense information from WordNet, but does not get improvements on parsing performance. Xiong et al. (2005) combines word sense from CiLin and HowNet (two Chinese semantic resources) in a generative parsing model, which generalizes standard bilexical dependencies to wordclass dependencies, and indeed help to tackle the sparseness problem in lexicalized parsing. The experiments show that the parse model combined with word sense and the most special hypernyms achieves a significant improvement on Penn Chinese Treebank. This work only considers the most special hypernym of a word, rather than other hypernyms at different levels of the hypernymhyponym hierarchy. Then, Fujita et al. (2007) uses the Hino</context>
</contexts>
<marker>Xiong, Li, Liu, Lin, Qian, 2005</marker>
<rawString>D. Xiong, S. Li, Q. Liu, S. Lin, and Y. Qian. 2005. Parsing the Penn Chinese treebank with semantic knowledge. In Proc. of IJCNLP’05, pages 70-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F-D Chiou</author>
<author>M Palmer</author>
</authors>
<title>Building a large scale annotated Chinese corpus.</title>
<date>2002</date>
<booktitle>In Proc. of COLING’02,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="19407" citStr="Xue et al. (2002)" startWordPosition="3070" endWordPosition="3073">emonstrates this semantically supervised splitting process. The left part of this figure is the subcategories of the semantic-related tag &amp;quot;NN-Entity&amp;quot;, which is split hierarchically. As expressed by the dashed line, each subcategory corresponds to one hypernym in the right part of this figure. If the hypernym node has no hyponym, the corresponding subcategory will stop splitting. The mapping from each subcategory of these semantic-related tags to the hypernym at the appropriate level is implemented with the word set related to this subcategory. As it is shown in Fig1302 DataSet Chinese English Xue et al. (2002) Marcus et al. (1993) TrainSet Art. 1-270,400-1151 Sections 2-21 DevSet Articles 301-325 Section 22 TestSet Articles 271-300 Section 23 Table 2: Experimental setup. ure 2, the original tag &amp;quot;NN-Entity&amp;quot; treats all the words it products as its word set. Once the original category is split into two subcategories, its word set is also split, through forcedly dividing each word in the word set into one subcategory which is most frequent with this word. And then, each subcategory is mapped onto the most specific hypernym that contains its related word set entirely in HowNet. On this basis, a new know</context>
</contexts>
<marker>Xue, Chiou, Palmer, 2002</marker>
<rawString>N. Xue, F.-D. Chiou, and M. Palmer. 2002. Building a large scale annotated Chinese corpus. In Proc. of COLING’02, pages 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
</authors>
<title>The Study and Realization of Chinese Parsing with Semantic and Sentence Type Information. Master thesis,</title>
<date>2008</date>
<institution>Peking University.</institution>
<contexts>
<context position="7215" citStr="Zhang (2008)" startWordPosition="1106" endWordPosition="1107">utilizing the most special hypernym, the word sense information in this model is embodied with more general concepts. Based on the hand-craft sense information, this model is proved to be effective for parse selection. Recently, Agirre et al. (2008) train two lexicalized models (Charniak, 2000; Bikel, 2004) on preprocessed inputs, where content words are substituted with semantic classes from WordNet. By integrating the word semantic classes into the process of parser training directly, these two models obtain significant improvements in both parsing and prepositional phrase attachment tasks. Zhang (2008) does preliminary work on integrating POS with semantic class of words directly, which can not only alleviate the confusion in parsing, but also infer syntax and semantic information at the same time. 2.2 The Hierarchical State-split Parsing In order to alleviate the context-free assumptions, Petrov et al. (2006) proposes a hierarchical statesplit approach to refine and generalize the original grammars, and achieves state-of-the-art performance. Starting with the basic nonterminals, this method repeats the split-merge (SM) cycle to increase the complexity of grammars. That is, it splits every </context>
</contexts>
<marker>Zhang, 2008</marker>
<rawString>Y. Zhang. 2008. The Study and Realization of Chinese Parsing with Semantic and Sentence Type Information. Master thesis, Peking University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>