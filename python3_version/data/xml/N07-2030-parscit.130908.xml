<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.064047">
<title confidence="0.977925">
On using Articulatory Features for Discriminative Speaker Adaptation
</title>
<author confidence="0.965155">
Florian Metze
</author>
<affiliation confidence="0.90441">
Deutsche Telekom Laboratories
</affiliation>
<address confidence="0.81075">
Berlin; Germany
</address>
<email confidence="0.993186">
florian.metze@telekom.de
</email>
<sectionHeader confidence="0.995534" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896470588235">
This paper presents a way to perform
speaker adaptation for automatic speech
recognition using the stream weights in a
multi-stream setup, which included acous-
tic models for “Articulatory Features”
such as ROUNDED or VOICED. We
present supervised speaker adaptation ex-
periments on a spontaneous speech task
and compare the above stream-based ap-
proach to conventional approaches, in
which the models, and not stream com-
bination weights, are being adapted. In
the approach we present, stream weights
model the importance of features such as
VOICED for word discrimination, which
offers a descriptive interpretation of the
adaptation parameters.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99984353125">
Almost all approaches to automatic speech recogni-
tion (ASR) using Hidden Markov Models (HMMs)
to model the time dependency of speech are also
based on phones, or context-dependent sub-phonetic
units derived from them, as the atomic unit of speech
modeling. In phonetics, a phone is a shorthand no-
tation for a certain configuration of underlying artic-
ulatory features (AFs) (Chomsky and Halle, 1968):
/p/ is for example defined as the unvoiced, bi-labial
plosive, from which /b/ can be distinguished by its
VOICED attribute. In this sense, instead of describ-
ing speech as a single, sequential stream of sym-
bols representing sounds, we can also look at speech
as the result of a process involving several paral-
lel streams of information, each of which describes
some linguistic or articulatory property as being ei-
ther absent or present.
A multi-stream architecture is a relatively simple
approach to combining several information sources
in ASR, because it leaves the basic structure of
the Hidden Markov Model and its computational
complexity intact. Examples combining different
observations are audio-visual speech recognition
(Potamianos and Graf, 1998) and sub-band based
speech processing (Janin et al., 1999). The same
idea can also be used to combine different classi-
fiers on the same observation. In a multi-stream
HMM setup, log-linear interpolation (Beyerlein,
2000) can be derived as a framework to integrat-
ing several independent acoustic models given as
Gaussian Mixture Models (GMMs) into the speech
recognition process: given a “weight” vector A =
</bodyText>
<equation confidence="0.856197833333333">
{A0, A1, · · · , AM}, a word sequence W, and an
acoustic observation o, the posterior probability
p(W jo) one wants to optimize is written as:
M
p(Wjo) = C exp j: Ai log pi(W jo)
i=0
</equation>
<bodyText confidence="0.99256575">
C is a normalization constant, which can be ne-
glected in practice, as long as normalization EiAi =
const is observed. It is now possible to set
p(Wjo) a p(ojW) (Beyerlein, 2000) and write a
speech recognizer’s acoustic model p(ojW) in this
form, which in logarithmic representation reduces
to a simple weighted sum of so-called “scores” for
each individual stream. The Ai represent the “im-
</bodyText>
<page confidence="0.97578">
117
</page>
<note confidence="0.466243">
Proceedings of NAACL HLT 2007, Companion Volume, pages 117–120,
Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999799136363636">
portance” of the contribution of each individual in-
formation source.
Extending Kirchhoff’s (Kirchhoff, 1999) ap-
proach, the log-likelihood score combination
method to AF-based ASR can be used to combine
information from M different articulatory features
while at the same time retaining the “standard”
acoustic models as stream 0. As an example
using M = 2, the acoustic score for /z/ would
be computed as a weighted sum of the scores for
a (context-dependent sub-)phonetic model z, the
score for FRICATIVE and the score for VOICED,
while the score for /s/ would be computed as a
weighted sum of the scores for a (context-dependent
sub-) phonetic model s, the score for FRICA-
TIVE and the score for NON VOICED. The free
parameters AZ can be global (G), or they can be
made state-dependent (SD) during the optimization
process, thus changing the importance of a feature
given a specific phonetic context, as long as overall
normalization is observed. (Metze, 2005) discusses
this stream setup in more detail.
</bodyText>
<sectionHeader confidence="0.994249" genericHeader="introduction">
2 Experiments
</sectionHeader>
<bodyText confidence="0.999941304347826">
To investigate the performance of the proposed AF-
based model, we built acoustic models for 68 ar-
ticulatory features on 32h of English Spontaneous
Scheduling Task ESST data from the Verbmobil
project (Wahlster, 2000), and integrated them with
matching phone-based acoustic models.
For training robust baseline phone models, 32h
from the ESST corpus were merged with 66h Broad-
cast News ’96 data, for which manually annotated
speaker labels are available. The system is trained
using 6 iterations of ML training and uses 4000 con-
text dependent (CD) acoustic models (HMM states),
32 Gaussians per model with diagonal covariance
matrices and a global semi-tied covariance matrix
(STC) in a 40-dimensional MFCC-based feature
space after LDA. The characteristics of the training
and test sets used in the following experiments are
summarized in Table 1.
The ESST test vocabulary contains 9400 words
including pronunciation variants (7100 base forms)
while the language model perplexity is 43.5 with an
out of vocabulary (OOV) rate of 1%. The language
model is a tri-gram model trained on ESST data
</bodyText>
<table confidence="0.999690666666667">
Data Set Train 1825 Test xv2
ds2
Duration 98h 2h25 1h26 0h59
Utterances 39100 1825 1150 675
Recordings 8681 58 32 26
Speakers 423 16 9 7
</table>
<tableCaption confidence="0.996849">
Table 1: Data sets used in this work: The ESST test
</tableCaption>
<bodyText confidence="0.969229555555556">
set 1825 is the union of the development set ds2
and the evaluation set xv2.
containing manually annotated semantic classes for
most proper names (persons, locations, numbers).
Generally, systems run in less than 4 times real-time
on Pentium 4-class machines. The baseline Word
Error Rate is reported as adaptation “None” in Ta-
ble 2; the system parameters were optimized on the
ds2 data set. As the stream weight estimation pro-
cess can introduce a scaling factor for the acoustic
model, we verified that the baseline system can not
be improved by widening the beam or by readjust-
ing the weight of the language model vs. the acous-
tic model. The baseline system can also not be im-
proved significantly by varying the number of pa-
rameters, either by increasing the number of Gaus-
sians per codebook or by increasing the number of
codebooks.
</bodyText>
<subsectionHeader confidence="0.980474">
2.1 MMI Training of Stream Weights
</subsectionHeader>
<bodyText confidence="0.999812611111111">
To arrive at an optimal set of stream weights, we
used the iterative update rules presented in (Metze,
2005) to generate stream weights AZ using the Max-
imum Mutual Information (MMI) criterion (Bahl et
al., 1986).
Results after one iteration of stream weight esti-
mation on the 1825 and ds2 data sets using step
size E = 4 · 10−8, initial stream weight A0���0 =
3 · 10−3, and lattice density d = 10 are shown in
Table 2 in rows “AF (G) on 1825” and “AF (G) on
ds2”: As there are only 68 stream weights to es-
timate, adaptation works only slightly better when
adapting and testing on the same corpus (“cheat-
ing experiment”: 22.6% vs. 22.8% word error rate
(WER) on ds2). There is no loss in WER (24.9%)
on xv2 when adapting the weights on ds2 instead
of 1825, which has no overlap with xv2, so gen-
eralization on unseen test data is good for global
</bodyText>
<page confidence="0.993722">
118
</page>
<bodyText confidence="0.9990295">
stream weights, i.e. weights which do not depend
on state or context.
</bodyText>
<subsectionHeader confidence="0.997792">
2.2 Speaker-specific Stream Weights
</subsectionHeader>
<bodyText confidence="0.99999488">
The ESST test 1825 set is suitable to test speaker-
specific properties of articulatory features, because it
contains 16 speakers in 58 different recordings. As
1825 provides between 2 and 8 dialogs per speaker,
it is possible to adapt the system to individual speak-
ers in a “round-robin” or “leave-one-out” experi-
ment, i.e. to decode every test dialog with weights
adapted on all remaining dialogs from that speaker
in the 1825 test set. Using speaker-specific, but
global (G), weights computed with the above set-
tings, the resulting WER is 21.5% (row “AF (G) on
speaker” in Table 2).
Training parameters were chosen to display im-
provements after the first iteration of training with-
out convergence in further iterations. Consequently,
training a second iteration of global (i.e. context
independent) weights does not improve the perfor-
mance of the speaker adapted system. In our ex-
periments we reached best results when comput-
ing state-dependent (SD) feature weights on top of
global weights using the experimentally determined
smaller learning rate of ESD = 0.2 · E. In this case,
speaker and state dependent AF stream weights fur-
ther reduce the word error rate to 19.8% (see bottom
row of Table 2).
</bodyText>
<subsectionHeader confidence="0.904298">
2.3 ML Model Adaptation
</subsectionHeader>
<bodyText confidence="0.999301529411765">
When training speaker-dependent articulatory fea-
ture weights in Section 2.2, we were effectively per-
forming supervised speaker adaptation (on separate
adaptation data) with articulatory feature weights.
To compare the performance of AFs to other ap-
proaches to speaker adaptation, we adapted the
baseline acoustic models to the test data using
supervised maximum likelihood linear regression
(MLLR) (Leggetter and Woodland, 1994) and con-
strained MLLR, which is also known as “feature-
space adaptation” (FSA) (Gales, 1997).
The ESST data has very little channel variation
so that the performance of models that were trained
on both ESST and BN data can be improved slightly
on ESST test dialogs by using FSA, while MLLR
already leads to over-specialization (Table 2, rows
“FSA/ MLLR on ds2). The results in Table 2
</bodyText>
<table confidence="0.999797714285714">
Adaptation Test corpus
type and corpus 1825 ds2 xv2
None 25.0% 24.1% 26.1%
FSA on ds2 22.5% 25.4%
FSA on speaker 22.8% 21.6% 24.3%
MLLR on ds2 16.3% 26.4%
MLLR on speaker 20.9% 19.8% 22.4%
MMI-MAP on ds2 14.4% 26.2%
MMI-MAP on speaker 20.5% 19.5% 21.7%
AF (G) on 1825 23.7% 22.8% 24.9%
AF (G) on ds2 22.6% 24.9%
AF (SD) on ds2 22.5% 26.5%
AF (G) on speaker 21.5% 20.1% 23.6%
AF (SD) on speaker 19.8% 18.6% 21.7%
</table>
<tableCaption confidence="0.757041333333333">
Table 2: Word error rates on the ESST test sets us-
ing different kinds of adaptation. See Table 1 for a
description of data sets.
</tableCaption>
<bodyText confidence="0.999977090909091">
show that AF adaptation performs as well as FSA in
the case of supervised adaptation on the ds2 data
and better by about 1.3% absolute in the speaker
adaptation case, despite using significantly less pa-
rameters (69 for the AF case vs. 40*40=1.6k for
the FSA case). While supervised FSA is equiva-
lent to AF adaptation when adapting and decoding
on the ds2 data in a “cheating experiment” for di-
agnostic purposes (22.5% vs 22.6%, rows “FSA/
AF (G) on ds2” of Table 2), supervised FSA only
reaches a WER of 22.8% on 1825 when decod-
ing every ESST dialog with acoustic models adapted
to the other dialogs available for this speaker (row
“FSA on speaker”). AF-based adaptation reaches
21.5% for the global (G) case and 19.8% for the
state dependent (SD) case (last two rows). The AF
(SD) case has 68*4000=276k free parameters, but
decision-tree based tying using a minimum count re-
duces these to 4.3k per speaker. Per-speaker MLLR
uses 4.7k parameters in the transformation matrices
on average per speaker, but performs worse than AF-
based adaptation by about 1% absolute.
</bodyText>
<subsectionHeader confidence="0.943953">
2.4 MMI Model Adaptation
</subsectionHeader>
<bodyText confidence="0.98254">
In a non-stream setup, discriminative speaker adap-
tation approaches have been published using condi-
tional maximum likelihood linear regression (CM-
LLR) (Gunawardana and Byrne, 2001) and MMI-
</bodyText>
<page confidence="0.997922">
119
</page>
<bodyText confidence="0.9992373">
MAP (Povey et al., 2003). In supervised adapta-
tion experiments on the Switchboard corpus, which
are similar to the experiments presented in the pre-
vious section, CMLLR reduced word error rate
over the baseline, but failed to outperform conven-
tional MLLR adaptation (Gunawardana and Byrne,
2001), which was already tested in Section 2.3. We
therefore compared AF-based speaker adaptation to
MMI-MAP as described in (Povey et al., 2003).
The results are given in Table 2: using a com-
parable number of parameters for adaptation as in
the previous section, AF-based adaptation performs
slightly better than MMI-MAP (19.8% WER vs.
20.5%; rows “MMI-MAP/ AF (SD) on speaker”).
When testing on the adaptation data ds2 as a di-
agnostic experiment, MMI-MAP as well as MLLR
outperform AF based adaptation, but the gains do
not carry over to the validation set xv2, which we
attribute to over-specialization of the acoustic mod-
els (rows “MLLR/ MMI-MAP/ AF (SD) on ds2).
</bodyText>
<sectionHeader confidence="0.991941" genericHeader="conclusions">
3 Summary and Conclusion
</sectionHeader>
<bodyText confidence="0.99999212">
This paper presented a comparison between two
approaches to discriminative speaker adaptation:
speaker adaptation using articulatory features (AFs)
in the multi-stream setup presented in (Metze, 2005)
slightly outperformed model-based discriminative
approaches to speaker adaptation (Gunawardana and
Byrne, 2001; Povey et al., 2003), however at the
cost of having to evaluate additional codebooks in
the articulatory feature streams during decoding. In
our experiments, we used 68 AFs, which requires
the evaluation of 68 models for “feature present”
and 68 models for “feature absent” for each frame
during decoding, plus the computation necessary for
stream combination. In this setup however, the adap-
tation parameters, which are given by the stream
combination weights, have an intuitive meaning, as
they model the importance of phonological features
such as VOICED or ROUNDED for word discrimina-
tion for this particular speaker and phonetic context.
Context-dependent stream weights can also model
feature asynchrony to some extent, so that this ap-
proach not only improves automatic speech recogni-
tion, but might also be an interesting starting point
for future work in speaker clustering, speaker iden-
tification, or other applications in speech analysis.
</bodyText>
<sectionHeader confidence="0.990161" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999850456521739">
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and
Robert L. Mercer. 1986. Maximum mutual informa-
tion estimation of Hidden Markov Model parameters
for speech recognition. In Proc. ICASSP, volume 1,
pages 49–52, Tokyo; Japan, May. IEEE.
Peter Beyerlein. 2000. Diskriminative Modellkom-
bination in Spracherkennungssystemen mit großem
Wortschatz. Ph.D. thesis, Rheinisch-Westf¨alisch-
Technische Hochschule Aachen (RWTH), October. In
German.
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. Harper and Row, New York; USA.
Mark J. F. Gales. 1997. Maximum likelihood linear
transformations for HMM-based speech recognition.
Technical report, Cambridge University, Cambridge;
UK, May. CUED/F-INFENG/TR 291.
Asela Gunawardana and William Byrne. 2001. Discrim-
inative speaker adaptation with conditional maximum
likelihood linear regression. In Proc. Eurospeech 2001
- Scandinavia, Aalborg; Denmark, September. ISCA.
Adam Janin, Dan Ellis, and Nelson Morgan. 1999.
Multi-stream speech recognition: Ready for prime
time. In Proc. EuroSpeech 1999, Budapest; Hungary,
September. ISCA.
Katrin Kirchhoff. 1999. Robust Speech Recognition Us-
ing Articulatory Information. Ph.D. thesis, Technis-
che Fakult¨at der Universit¨at Bielefeld, Bielefeld; Ger-
many, June.
Chris J. Leggetter and Phil C. Woodland. 1994. Speaker
adaptation of HMMs using linear regression. Techni-
cal report, Cambridge University, England.
Florian Metze. 2005. Articulatory Features for Conver-
sational Speech Recognition. Ph.D. thesis, Fakult¨at
f¨ur Informatik der Universit¨at Karlsruhe (TH), Karl-
sruhe; Germany, December.
Gerasimos Potamianos and Hans-Peter Graf. 1998. Dis-
criminative training of HMM stream exponents for
audio-visual speech recognition. In Proc. ICASSP
1998, Seattle, WA; USA. IEEE.
Dan Povey, Mark J.F. Gales, Do Y. Kim, and Phil C.
Woodland. 2003. MMI-MAP and MPE-MAP for
acoustic model adaptation. In Proc. Eurospeech 2003,
Geneva; Switzerland, September. ISCA.
Wolfgang Wahlster, editor. 2000. Verbmobil: Founda-
tions of Speech-to-Speech Translation. Springer, Hei-
delberg.
</reference>
<page confidence="0.996278">
120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.548558">
<title confidence="0.999958">On using Articulatory Features for Discriminative Speaker Adaptation</title>
<author confidence="0.971165">Florian</author>
<affiliation confidence="0.824053">Deutsche Telekom</affiliation>
<address confidence="0.630841">Berlin;</address>
<email confidence="0.999375">florian.metze@telekom.de</email>
<abstract confidence="0.9996795">This paper presents a way to perform speaker adaptation for automatic speech recognition using the stream weights in a multi-stream setup, which included acoustic models for “Articulatory Features” as We present supervised speaker adaptation experiments on a spontaneous speech task and compare the above stream-based approach to conventional approaches, in which the models, and not stream combination weights, are being adapted. In the approach we present, stream weights model the importance of features such as word discrimination, which offers a descriptive interpretation of the adaptation parameters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Peter F Brown</author>
<author>Peter V de Souza</author>
<author>Robert L Mercer</author>
</authors>
<title>Maximum mutual information estimation of Hidden Markov Model parameters for speech recognition.</title>
<date>1986</date>
<booktitle>In Proc. ICASSP,</booktitle>
<volume>1</volume>
<pages>49--52</pages>
<publisher>IEEE.</publisher>
<location>Tokyo; Japan,</location>
<marker>Bahl, Brown, de Souza, Mercer, 1986</marker>
<rawString>Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and Robert L. Mercer. 1986. Maximum mutual information estimation of Hidden Markov Model parameters for speech recognition. In Proc. ICASSP, volume 1, pages 49–52, Tokyo; Japan, May. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Beyerlein</author>
</authors>
<title>Diskriminative Modellkombination in Spracherkennungssystemen mit großem Wortschatz.</title>
<date>2000</date>
<booktitle>Ph.D. thesis, Rheinisch-Westf¨alischTechnische Hochschule Aachen (RWTH), October. In German.</booktitle>
<contexts>
<context position="2197" citStr="Beyerlein, 2000" startWordPosition="326" endWordPosition="327">some linguistic or articulatory property as being either absent or present. A multi-stream architecture is a relatively simple approach to combining several information sources in ASR, because it leaves the basic structure of the Hidden Markov Model and its computational complexity intact. Examples combining different observations are audio-visual speech recognition (Potamianos and Graf, 1998) and sub-band based speech processing (Janin et al., 1999). The same idea can also be used to combine different classifiers on the same observation. In a multi-stream HMM setup, log-linear interpolation (Beyerlein, 2000) can be derived as a framework to integrating several independent acoustic models given as Gaussian Mixture Models (GMMs) into the speech recognition process: given a “weight” vector A = {A0, A1, · · · , AM}, a word sequence W, and an acoustic observation o, the posterior probability p(W jo) one wants to optimize is written as: M p(Wjo) = C exp j: Ai log pi(W jo) i=0 C is a normalization constant, which can be neglected in practice, as long as normalization EiAi = const is observed. It is now possible to set p(Wjo) a p(ojW) (Beyerlein, 2000) and write a speech recognizer’s acoustic model p(ojW</context>
</contexts>
<marker>Beyerlein, 2000</marker>
<rawString>Peter Beyerlein. 2000. Diskriminative Modellkombination in Spracherkennungssystemen mit großem Wortschatz. Ph.D. thesis, Rheinisch-Westf¨alischTechnische Hochschule Aachen (RWTH), October. In German.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>Morris Halle</author>
</authors>
<title>The Sound Pattern of English. Harper and Row,</title>
<date>1968</date>
<location>New York; USA.</location>
<contexts>
<context position="1219" citStr="Chomsky and Halle, 1968" startWordPosition="174" endWordPosition="177">ts, are being adapted. In the approach we present, stream weights model the importance of features such as VOICED for word discrimination, which offers a descriptive interpretation of the adaptation parameters. 1 Introduction Almost all approaches to automatic speech recognition (ASR) using Hidden Markov Models (HMMs) to model the time dependency of speech are also based on phones, or context-dependent sub-phonetic units derived from them, as the atomic unit of speech modeling. In phonetics, a phone is a shorthand notation for a certain configuration of underlying articulatory features (AFs) (Chomsky and Halle, 1968): /p/ is for example defined as the unvoiced, bi-labial plosive, from which /b/ can be distinguished by its VOICED attribute. In this sense, instead of describing speech as a single, sequential stream of symbols representing sounds, we can also look at speech as the result of a process involving several parallel streams of information, each of which describes some linguistic or articulatory property as being either absent or present. A multi-stream architecture is a relatively simple approach to combining several information sources in ASR, because it leaves the basic structure of the Hidden M</context>
</contexts>
<marker>Chomsky, Halle, 1968</marker>
<rawString>Noam Chomsky and Morris Halle. 1968. The Sound Pattern of English. Harper and Row, New York; USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark J F Gales</author>
</authors>
<title>Maximum likelihood linear transformations for HMM-based speech recognition.</title>
<date>1997</date>
<tech>Technical report,</tech>
<institution>Cambridge University,</institution>
<location>Cambridge; UK,</location>
<contexts>
<context position="8969" citStr="Gales, 1997" startWordPosition="1459" endWordPosition="1460">her reduce the word error rate to 19.8% (see bottom row of Table 2). 2.3 ML Model Adaptation When training speaker-dependent articulatory feature weights in Section 2.2, we were effectively performing supervised speaker adaptation (on separate adaptation data) with articulatory feature weights. To compare the performance of AFs to other approaches to speaker adaptation, we adapted the baseline acoustic models to the test data using supervised maximum likelihood linear regression (MLLR) (Leggetter and Woodland, 1994) and constrained MLLR, which is also known as “featurespace adaptation” (FSA) (Gales, 1997). The ESST data has very little channel variation so that the performance of models that were trained on both ESST and BN data can be improved slightly on ESST test dialogs by using FSA, while MLLR already leads to over-specialization (Table 2, rows “FSA/ MLLR on ds2). The results in Table 2 Adaptation Test corpus type and corpus 1825 ds2 xv2 None 25.0% 24.1% 26.1% FSA on ds2 22.5% 25.4% FSA on speaker 22.8% 21.6% 24.3% MLLR on ds2 16.3% 26.4% MLLR on speaker 20.9% 19.8% 22.4% MMI-MAP on ds2 14.4% 26.2% MMI-MAP on speaker 20.5% 19.5% 21.7% AF (G) on 1825 23.7% 22.8% 24.9% AF (G) on ds2 22.6% 2</context>
</contexts>
<marker>Gales, 1997</marker>
<rawString>Mark J. F. Gales. 1997. Maximum likelihood linear transformations for HMM-based speech recognition. Technical report, Cambridge University, Cambridge; UK, May. CUED/F-INFENG/TR 291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asela Gunawardana</author>
<author>William Byrne</author>
</authors>
<title>Discriminative speaker adaptation with conditional maximum likelihood linear regression.</title>
<date>2001</date>
<booktitle>In Proc. Eurospeech</booktitle>
<publisher>ISCA.</publisher>
<location>Scandinavia, Aalborg; Denmark,</location>
<contexts>
<context position="11074" citStr="Gunawardana and Byrne, 2001" startWordPosition="1825" endWordPosition="1828">aker”). AF-based adaptation reaches 21.5% for the global (G) case and 19.8% for the state dependent (SD) case (last two rows). The AF (SD) case has 68*4000=276k free parameters, but decision-tree based tying using a minimum count reduces these to 4.3k per speaker. Per-speaker MLLR uses 4.7k parameters in the transformation matrices on average per speaker, but performs worse than AFbased adaptation by about 1% absolute. 2.4 MMI Model Adaptation In a non-stream setup, discriminative speaker adaptation approaches have been published using conditional maximum likelihood linear regression (CMLLR) (Gunawardana and Byrne, 2001) and MMI119 MAP (Povey et al., 2003). In supervised adaptation experiments on the Switchboard corpus, which are similar to the experiments presented in the previous section, CMLLR reduced word error rate over the baseline, but failed to outperform conventional MLLR adaptation (Gunawardana and Byrne, 2001), which was already tested in Section 2.3. We therefore compared AF-based speaker adaptation to MMI-MAP as described in (Povey et al., 2003). The results are given in Table 2: using a comparable number of parameters for adaptation as in the previous section, AF-based adaptation performs slight</context>
<context position="12379" citStr="Gunawardana and Byrne, 2001" startWordPosition="2026" endWordPosition="2029">”). When testing on the adaptation data ds2 as a diagnostic experiment, MMI-MAP as well as MLLR outperform AF based adaptation, but the gains do not carry over to the validation set xv2, which we attribute to over-specialization of the acoustic models (rows “MLLR/ MMI-MAP/ AF (SD) on ds2). 3 Summary and Conclusion This paper presented a comparison between two approaches to discriminative speaker adaptation: speaker adaptation using articulatory features (AFs) in the multi-stream setup presented in (Metze, 2005) slightly outperformed model-based discriminative approaches to speaker adaptation (Gunawardana and Byrne, 2001; Povey et al., 2003), however at the cost of having to evaluate additional codebooks in the articulatory feature streams during decoding. In our experiments, we used 68 AFs, which requires the evaluation of 68 models for “feature present” and 68 models for “feature absent” for each frame during decoding, plus the computation necessary for stream combination. In this setup however, the adaptation parameters, which are given by the stream combination weights, have an intuitive meaning, as they model the importance of phonological features such as VOICED or ROUNDED for word discrimination for th</context>
</contexts>
<marker>Gunawardana, Byrne, 2001</marker>
<rawString>Asela Gunawardana and William Byrne. 2001. Discriminative speaker adaptation with conditional maximum likelihood linear regression. In Proc. Eurospeech 2001 - Scandinavia, Aalborg; Denmark, September. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Janin</author>
<author>Dan Ellis</author>
<author>Nelson Morgan</author>
</authors>
<title>Multi-stream speech recognition: Ready for prime time.</title>
<date>1999</date>
<booktitle>In Proc. EuroSpeech</booktitle>
<publisher>ISCA.</publisher>
<location>Budapest; Hungary,</location>
<contexts>
<context position="2035" citStr="Janin et al., 1999" startWordPosition="299" endWordPosition="302">eam of symbols representing sounds, we can also look at speech as the result of a process involving several parallel streams of information, each of which describes some linguistic or articulatory property as being either absent or present. A multi-stream architecture is a relatively simple approach to combining several information sources in ASR, because it leaves the basic structure of the Hidden Markov Model and its computational complexity intact. Examples combining different observations are audio-visual speech recognition (Potamianos and Graf, 1998) and sub-band based speech processing (Janin et al., 1999). The same idea can also be used to combine different classifiers on the same observation. In a multi-stream HMM setup, log-linear interpolation (Beyerlein, 2000) can be derived as a framework to integrating several independent acoustic models given as Gaussian Mixture Models (GMMs) into the speech recognition process: given a “weight” vector A = {A0, A1, · · · , AM}, a word sequence W, and an acoustic observation o, the posterior probability p(W jo) one wants to optimize is written as: M p(Wjo) = C exp j: Ai log pi(W jo) i=0 C is a normalization constant, which can be neglected in practice, a</context>
</contexts>
<marker>Janin, Ellis, Morgan, 1999</marker>
<rawString>Adam Janin, Dan Ellis, and Nelson Morgan. 1999. Multi-stream speech recognition: Ready for prime time. In Proc. EuroSpeech 1999, Budapest; Hungary, September. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Kirchhoff</author>
</authors>
<title>Robust Speech Recognition Using Articulatory Information.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Technische Fakult¨at der Universit¨at</institution>
<location>Bielefeld, Bielefeld; Germany,</location>
<contexts>
<context position="3208" citStr="Kirchhoff, 1999" startWordPosition="493" endWordPosition="494">tant, which can be neglected in practice, as long as normalization EiAi = const is observed. It is now possible to set p(Wjo) a p(ojW) (Beyerlein, 2000) and write a speech recognizer’s acoustic model p(ojW) in this form, which in logarithmic representation reduces to a simple weighted sum of so-called “scores” for each individual stream. The Ai represent the “im117 Proceedings of NAACL HLT 2007, Companion Volume, pages 117–120, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics portance” of the contribution of each individual information source. Extending Kirchhoff’s (Kirchhoff, 1999) approach, the log-likelihood score combination method to AF-based ASR can be used to combine information from M different articulatory features while at the same time retaining the “standard” acoustic models as stream 0. As an example using M = 2, the acoustic score for /z/ would be computed as a weighted sum of the scores for a (context-dependent sub-)phonetic model z, the score for FRICATIVE and the score for VOICED, while the score for /s/ would be computed as a weighted sum of the scores for a (context-dependent sub-) phonetic model s, the score for FRICATIVE and the score for NON VOICED.</context>
</contexts>
<marker>Kirchhoff, 1999</marker>
<rawString>Katrin Kirchhoff. 1999. Robust Speech Recognition Using Articulatory Information. Ph.D. thesis, Technische Fakult¨at der Universit¨at Bielefeld, Bielefeld; Germany, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris J Leggetter</author>
<author>Phil C Woodland</author>
</authors>
<title>Speaker adaptation of HMMs using linear regression. Technical report,</title>
<date>1994</date>
<institution>Cambridge University,</institution>
<location>England.</location>
<contexts>
<context position="8878" citStr="Leggetter and Woodland, 1994" startWordPosition="1442" endWordPosition="1445">ned smaller learning rate of ESD = 0.2 · E. In this case, speaker and state dependent AF stream weights further reduce the word error rate to 19.8% (see bottom row of Table 2). 2.3 ML Model Adaptation When training speaker-dependent articulatory feature weights in Section 2.2, we were effectively performing supervised speaker adaptation (on separate adaptation data) with articulatory feature weights. To compare the performance of AFs to other approaches to speaker adaptation, we adapted the baseline acoustic models to the test data using supervised maximum likelihood linear regression (MLLR) (Leggetter and Woodland, 1994) and constrained MLLR, which is also known as “featurespace adaptation” (FSA) (Gales, 1997). The ESST data has very little channel variation so that the performance of models that were trained on both ESST and BN data can be improved slightly on ESST test dialogs by using FSA, while MLLR already leads to over-specialization (Table 2, rows “FSA/ MLLR on ds2). The results in Table 2 Adaptation Test corpus type and corpus 1825 ds2 xv2 None 25.0% 24.1% 26.1% FSA on ds2 22.5% 25.4% FSA on speaker 22.8% 21.6% 24.3% MLLR on ds2 16.3% 26.4% MLLR on speaker 20.9% 19.8% 22.4% MMI-MAP on ds2 14.4% 26.2% </context>
</contexts>
<marker>Leggetter, Woodland, 1994</marker>
<rawString>Chris J. Leggetter and Phil C. Woodland. 1994. Speaker adaptation of HMMs using linear regression. Technical report, Cambridge University, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Metze</author>
</authors>
<title>Articulatory Features for Conversational Speech Recognition.</title>
<date>2005</date>
<booktitle>Ph.D. thesis, Fakult¨at f¨ur Informatik der Universit¨at</booktitle>
<location>Karlsruhe (TH), Karlsruhe; Germany,</location>
<contexts>
<context position="4061" citStr="Metze, 2005" startWordPosition="637" endWordPosition="638"> M = 2, the acoustic score for /z/ would be computed as a weighted sum of the scores for a (context-dependent sub-)phonetic model z, the score for FRICATIVE and the score for VOICED, while the score for /s/ would be computed as a weighted sum of the scores for a (context-dependent sub-) phonetic model s, the score for FRICATIVE and the score for NON VOICED. The free parameters AZ can be global (G), or they can be made state-dependent (SD) during the optimization process, thus changing the importance of a feature given a specific phonetic context, as long as overall normalization is observed. (Metze, 2005) discusses this stream setup in more detail. 2 Experiments To investigate the performance of the proposed AFbased model, we built acoustic models for 68 articulatory features on 32h of English Spontaneous Scheduling Task ESST data from the Verbmobil project (Wahlster, 2000), and integrated them with matching phone-based acoustic models. For training robust baseline phone models, 32h from the ESST corpus were merged with 66h Broadcast News ’96 data, for which manually annotated speaker labels are available. The system is trained using 6 iterations of ML training and uses 4000 context dependent </context>
<context position="6379" citStr="Metze, 2005" startWordPosition="1024" endWordPosition="1025">zed on the ds2 data set. As the stream weight estimation process can introduce a scaling factor for the acoustic model, we verified that the baseline system can not be improved by widening the beam or by readjusting the weight of the language model vs. the acoustic model. The baseline system can also not be improved significantly by varying the number of parameters, either by increasing the number of Gaussians per codebook or by increasing the number of codebooks. 2.1 MMI Training of Stream Weights To arrive at an optimal set of stream weights, we used the iterative update rules presented in (Metze, 2005) to generate stream weights AZ using the Maximum Mutual Information (MMI) criterion (Bahl et al., 1986). Results after one iteration of stream weight estimation on the 1825 and ds2 data sets using step size E = 4 · 10−8, initial stream weight A0���0 = 3 · 10−3, and lattice density d = 10 are shown in Table 2 in rows “AF (G) on 1825” and “AF (G) on ds2”: As there are only 68 stream weights to estimate, adaptation works only slightly better when adapting and testing on the same corpus (“cheating experiment”: 22.6% vs. 22.8% word error rate (WER) on ds2). There is no loss in WER (24.9%) on xv2 wh</context>
<context position="12268" citStr="Metze, 2005" startWordPosition="2016" endWordPosition="2017">on performs slightly better than MMI-MAP (19.8% WER vs. 20.5%; rows “MMI-MAP/ AF (SD) on speaker”). When testing on the adaptation data ds2 as a diagnostic experiment, MMI-MAP as well as MLLR outperform AF based adaptation, but the gains do not carry over to the validation set xv2, which we attribute to over-specialization of the acoustic models (rows “MLLR/ MMI-MAP/ AF (SD) on ds2). 3 Summary and Conclusion This paper presented a comparison between two approaches to discriminative speaker adaptation: speaker adaptation using articulatory features (AFs) in the multi-stream setup presented in (Metze, 2005) slightly outperformed model-based discriminative approaches to speaker adaptation (Gunawardana and Byrne, 2001; Povey et al., 2003), however at the cost of having to evaluate additional codebooks in the articulatory feature streams during decoding. In our experiments, we used 68 AFs, which requires the evaluation of 68 models for “feature present” and 68 models for “feature absent” for each frame during decoding, plus the computation necessary for stream combination. In this setup however, the adaptation parameters, which are given by the stream combination weights, have an intuitive meaning,</context>
</contexts>
<marker>Metze, 2005</marker>
<rawString>Florian Metze. 2005. Articulatory Features for Conversational Speech Recognition. Ph.D. thesis, Fakult¨at f¨ur Informatik der Universit¨at Karlsruhe (TH), Karlsruhe; Germany, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerasimos Potamianos</author>
<author>Hans-Peter Graf</author>
</authors>
<title>Discriminative training of HMM stream exponents for audio-visual speech recognition.</title>
<date>1998</date>
<booktitle>In Proc. ICASSP</booktitle>
<publisher>USA. IEEE.</publisher>
<location>Seattle, WA;</location>
<contexts>
<context position="1977" citStr="Potamianos and Graf, 1998" startWordPosition="290" endWordPosition="293">s sense, instead of describing speech as a single, sequential stream of symbols representing sounds, we can also look at speech as the result of a process involving several parallel streams of information, each of which describes some linguistic or articulatory property as being either absent or present. A multi-stream architecture is a relatively simple approach to combining several information sources in ASR, because it leaves the basic structure of the Hidden Markov Model and its computational complexity intact. Examples combining different observations are audio-visual speech recognition (Potamianos and Graf, 1998) and sub-band based speech processing (Janin et al., 1999). The same idea can also be used to combine different classifiers on the same observation. In a multi-stream HMM setup, log-linear interpolation (Beyerlein, 2000) can be derived as a framework to integrating several independent acoustic models given as Gaussian Mixture Models (GMMs) into the speech recognition process: given a “weight” vector A = {A0, A1, · · · , AM}, a word sequence W, and an acoustic observation o, the posterior probability p(W jo) one wants to optimize is written as: M p(Wjo) = C exp j: Ai log pi(W jo) i=0 C is a nor</context>
</contexts>
<marker>Potamianos, Graf, 1998</marker>
<rawString>Gerasimos Potamianos and Hans-Peter Graf. 1998. Discriminative training of HMM stream exponents for audio-visual speech recognition. In Proc. ICASSP 1998, Seattle, WA; USA. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Povey</author>
<author>Mark J F Gales</author>
<author>Do Y Kim</author>
<author>Phil C Woodland</author>
</authors>
<title>MMI-MAP and MPE-MAP for acoustic model adaptation.</title>
<date>2003</date>
<booktitle>In Proc. Eurospeech 2003,</booktitle>
<publisher>ISCA.</publisher>
<location>Geneva; Switzerland,</location>
<contexts>
<context position="11110" citStr="Povey et al., 2003" startWordPosition="1833" endWordPosition="1836"> the global (G) case and 19.8% for the state dependent (SD) case (last two rows). The AF (SD) case has 68*4000=276k free parameters, but decision-tree based tying using a minimum count reduces these to 4.3k per speaker. Per-speaker MLLR uses 4.7k parameters in the transformation matrices on average per speaker, but performs worse than AFbased adaptation by about 1% absolute. 2.4 MMI Model Adaptation In a non-stream setup, discriminative speaker adaptation approaches have been published using conditional maximum likelihood linear regression (CMLLR) (Gunawardana and Byrne, 2001) and MMI119 MAP (Povey et al., 2003). In supervised adaptation experiments on the Switchboard corpus, which are similar to the experiments presented in the previous section, CMLLR reduced word error rate over the baseline, but failed to outperform conventional MLLR adaptation (Gunawardana and Byrne, 2001), which was already tested in Section 2.3. We therefore compared AF-based speaker adaptation to MMI-MAP as described in (Povey et al., 2003). The results are given in Table 2: using a comparable number of parameters for adaptation as in the previous section, AF-based adaptation performs slightly better than MMI-MAP (19.8% WER vs</context>
<context position="12400" citStr="Povey et al., 2003" startWordPosition="2030" endWordPosition="2033">ation data ds2 as a diagnostic experiment, MMI-MAP as well as MLLR outperform AF based adaptation, but the gains do not carry over to the validation set xv2, which we attribute to over-specialization of the acoustic models (rows “MLLR/ MMI-MAP/ AF (SD) on ds2). 3 Summary and Conclusion This paper presented a comparison between two approaches to discriminative speaker adaptation: speaker adaptation using articulatory features (AFs) in the multi-stream setup presented in (Metze, 2005) slightly outperformed model-based discriminative approaches to speaker adaptation (Gunawardana and Byrne, 2001; Povey et al., 2003), however at the cost of having to evaluate additional codebooks in the articulatory feature streams during decoding. In our experiments, we used 68 AFs, which requires the evaluation of 68 models for “feature present” and 68 models for “feature absent” for each frame during decoding, plus the computation necessary for stream combination. In this setup however, the adaptation parameters, which are given by the stream combination weights, have an intuitive meaning, as they model the importance of phonological features such as VOICED or ROUNDED for word discrimination for this particular speaker</context>
</contexts>
<marker>Povey, Gales, Kim, Woodland, 2003</marker>
<rawString>Dan Povey, Mark J.F. Gales, Do Y. Kim, and Phil C. Woodland. 2003. MMI-MAP and MPE-MAP for acoustic model adaptation. In Proc. Eurospeech 2003, Geneva; Switzerland, September. ISCA.</rawString>
</citation>
<citation valid="true">
<title>Verbmobil: Foundations of Speech-to-Speech Translation.</title>
<date>2000</date>
<editor>Wolfgang Wahlster, editor.</editor>
<publisher>Springer,</publisher>
<location>Heidelberg.</location>
<marker>2000</marker>
<rawString>Wolfgang Wahlster, editor. 2000. Verbmobil: Foundations of Speech-to-Speech Translation. Springer, Heidelberg.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>