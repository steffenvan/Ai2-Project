<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000268">
<title confidence="0.989167">
Grammatical Error Correction with Alternating Structure Optimization
</title>
<author confidence="0.98969">
Daniel Dahlmeier&apos; and Hwee Tou Ng&apos; ,2
</author>
<affiliation confidence="0.979465">
&apos;NUS Graduate School for Integrative Sciences and Engineering
2Department of Computer Science, National University of Singapore
</affiliation>
<email confidence="0.995437">
{danielhe,nght}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.998575" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998516571428571">
We present a novel approach to grammatical
error correction based on Alternating Struc-
ture Optimization. As part of our work, we
introduce the NUS Corpus of Learner En-
glish (NUCLE), a fully annotated one mil-
lion words corpus of learner English available
for research purposes. We conduct an exten-
sive evaluation for article and preposition er-
rors using various feature sets. Our exper-
iments show that our approach outperforms
two baselines trained on non-learner text and
learner text, respectively. Our approach also
outperforms two commercial grammar check-
ing software packages.
</bodyText>
<sectionHeader confidence="0.99952" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999893960784314">
Grammatical error correction (GEC) has been rec-
ognized as an interesting as well as commercially
attractive problem in natural language process-
ing (NLP), in particular for learners of English as
a foreign or second language (EFL/ESL).
Despite the growing interest, research has been
hindered by the lack of a large annotated corpus of
learner text that is available for research purposes.
As a result, the standard approach to GEC has been
to train an off-the-shelf classifier to re-predict words
in non-learner text. Learning GEC models directly
from annotated learner corpora is not well explored,
as are methods that combine learner and non-learner
text. Furthermore, the evaluation of GEC has been
problematic. Previous work has either evaluated on
artificial test instances as a substitute for real learner
errors or on proprietary data that is not available to
other researchers. As a consequence, existing meth-
ods have not been compared on the same test set,
leaving it unclear where the current state of the art
really is.
In this work, we aim to overcome both problems.
First, we present a novel approach to GEC based
on Alternating Structure Optimization (ASO) (Ando
and Zhang, 2005). Our approach is able to train
models on annotated learner corpora while still tak-
ing advantage of large non-learner corpora. Sec-
ond, we introduce the NUS Corpus of Learner En-
glish (NUCLE), a fully annotated one million words
corpus of learner English available for research pur-
poses. We conduct an extensive evaluation for ar-
ticle and preposition errors using six different fea-
ture sets proposed in previous work. We com-
pare our proposed ASO method with two baselines
trained on non-learner text and learner text, respec-
tively. To the best of our knowledge, this is the
first extensive comparison of different feature sets
on real learner text which is another contribution
of our work. Our experiments show that our pro-
posed ASO algorithm significantly improves over
both baselines. It also outperforms two commercial
grammar checking software packages in a manual
evaluation.
The remainder of this paper is organized as fol-
lows. The next section reviews related work. Sec-
tion 3 describes the tasks. Section 4 formulates GEC
as a classification problem. Section 5 extends this to
the ASO algorithm. The experiments are presented
in Section 6 and the results in Section 7. Section 8
contains a more detailed analysis of the results. Sec-
tion 9 concludes the paper.
</bodyText>
<page confidence="0.976037">
915
</page>
<note confidence="0.984513">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 915–923,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.999127" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999179425">
In this section, we give a brief overview on related
work on article and preposition errors. For a more
comprehensive survey, see (Leacock et al., 2010).
The seminal work on grammatical error correc-
tion was done by Knight and Chander (1994) on arti-
cle errors. Subsequent work has focused on design-
ing better features and testing different classifiers,
including memory-based learning (Minnen et al.,
2000), decision tree learning (Nagata et al., 2006;
Gamon et al., 2008), and logistic regression (Lee,
2004; Han et al., 2006; De Felice, 2008). Work
on preposition errors has used a similar classifica-
tion approach and mainly differs in terms of the fea-
tures employed (Chodorow et al., 2007; Gamon et
al., 2008; Lee and Knutsson, 2008; Tetreault and
Chodorow, 2008; Tetreault et al., 2010; De Felice,
2008). All of the above works only use non-learner
text for training.
Recent work has shown that training on anno-
tated learner text can give better performance (Han
et al., 2010) and that the observed word used by
the writer is an important feature (Rozovskaya and
Roth, 2010b). However, training data has either
been small (Izumi et al., 2003), only partly anno-
tated (Han et al., 2010), or artificially created (Ro-
zovskaya and Roth, 2010b; Rozovskaya and Roth,
2010a).
Almost no work has investigated ways to combine
learner and non-learner text for training. The only
exception is Gamon (2010), who combined features
from the output of logistic-regression classifiers and
language models trained on non-learner text in a
meta-classifier trained on learner text. In this work,
we show a more direct way to combine learner and
non-learner text in a single model.
Finally, researchers have investigated GEC in
connection with web-based models in NLP (Lapata
and Keller, 2005; Bergsma et al., 2009; Yi et al.,
2008). These methods do not use classifiers, but rely
on simple n-gram counts or page hits from the Web.
</bodyText>
<sectionHeader confidence="0.994605" genericHeader="method">
3 Task Description
</sectionHeader>
<bodyText confidence="0.999131">
In this work, we focus on article and preposition er-
rors, as they are among the most frequent types of
errors made by EFL learners.
</bodyText>
<subsectionHeader confidence="0.999945">
3.1 Selection vs. Correction Task
</subsectionHeader>
<bodyText confidence="0.999987210526316">
There is an important difference between training on
annotated learner text and training on non-learner
text, namely whether the observed word can be used
as a feature or not. When training on non-learner
text, the observed word cannot be used as a feature.
The word choice of the writer is “blanked out” from
the text and serves as the correct class. A classifier
is trained to re-predict the word given the surround-
ing context. The confusion set of possible classes
is usually pre-defined. This selection task formula-
tion is convenient as training examples can be cre-
ated “for free” from any text that is assumed to be
free of grammatical errors. We define the more re-
alistic correction task as follows: given a particular
word and its context, propose an appropriate correc-
tion. The proposed correction can be identical to the
observed word, i.e., no correction is necessary. The
main difference is that the word choice of the writer
can be encoded as part of the features.
</bodyText>
<subsectionHeader confidence="0.99991">
3.2 Article Errors
</subsectionHeader>
<bodyText confidence="0.999891153846154">
For article errors, the classes are the three articles a,
the, and the zero-article. This covers article inser-
tion, deletion, and substitution errors. During train-
ing, each noun phrase (NP) in the training data is one
training example. When training on learner text, the
correct class is the article provided by the human
annotator. When training on non-learner text, the
correct class is the observed article. The context is
encoded via a set of feature functions. During test-
ing, each NP in the test set is one test example. The
correct class is the article provided by the human an-
notator when testing on learner text or the observed
article when testing on non-learner text.
</bodyText>
<subsectionHeader confidence="0.99986">
3.3 Preposition Errors
</subsectionHeader>
<bodyText confidence="0.8287708">
The approach to preposition errors is similar to ar-
ticles but typically focuses on preposition substitu-
tion errors. In our work, the classes are 36 frequent
English prepositions (about, along, among, around,
as, at, beside, besides, between, by, down, during,
except, for, from, in, inside, into, of, off, on, onto,
outside, over, through, to, toward, towards, under,
underneath, until, up, upon, with, within, without),
which we adopt from previous work. Every prepo-
sitional phrase (PP) that is governed by one of the
</bodyText>
<page confidence="0.997287">
916
</page>
<figure confidence="0.55728925">
rich set of syntactic and semantic features, in-
cluding part of speech (POS) tags, hypernyms
from WordNet (Fellbaum, 1998), and named
entities.
36 prepositions is one training or test example. We
ignore PPs governed by other prepositions.
4 Linear Classifiers for Grammatical
Error Correction
</figure>
<bodyText confidence="0.9967305">
In this section, we formulate GEC as a classification
problem and describe the feature sets for each task.
</bodyText>
<subsectionHeader confidence="0.996927">
4.1 Linear Classifiers
</subsectionHeader>
<bodyText confidence="0.866536928571428">
We use classifiers to approximate the unknown rela-
tion between articles or prepositions and their con-
texts in learner text, and their valid corrections. The
articles or prepositions and their contexts are repre-
sented as feature vectors X E X. The corrections
are the classes Y E Y.
In this work, we employ binary linear classifiers
of the form uTX where u is a weight vector. The
outcome is considered +1 if the score is positive and
−1 otherwise. A popular method for finding u is
empirical risk minimization with least square regu-
larization. Given a training set {XZ,YZjZ=1,...,n, we
aim to find the weight vector that minimizes the em-
pirical loss on the training data
</bodyText>
<equation confidence="0.985197">
L(uTXZ,YZ) + A ||u||2
) ,
(1)
</equation>
<bodyText confidence="0.999423">
where L is a loss function. We use a modification of
Huber’s robust loss function. We fix the regulariza-
tion parameter A to 10−4. A multi-class classifica-
tion problem with m classes can be cast as m binary
classification problems in a one-vs-rest arrangement.
The prediction of the classifier is the class with the
highest score Y� = arg maxY ,Y (uTY X). In earlier
experiments, this linear classifier gave comparable
or superior performance compared to a logistic re-
gression classifier.
</bodyText>
<subsectionHeader confidence="0.884417">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.959408">
We re-implement six feature extraction methods
from previous work, three for articles and three for
prepositions. The methods require different lin-
guistic pre-processing: chunking, CCG parsing, and
constituency parsing.
</bodyText>
<subsectionHeader confidence="0.559492">
4.2.1 Article Errors
</subsectionHeader>
<listItem confidence="0.9869567">
• DeFelice The system in (De Felice, 2008) for
article errors uses a CCG parser to extract a
• Han The system in (Han et al., 2006) relies on
shallow syntactic and lexical features derived
from a chunker, including the words before, in,
and after the NP, the head word, and POS tags.
• Lee The system in (Lee, 2004) uses a con-
stituency parser. The features include POS
tags, surrounding words, the head word, and
hypernyms from WordNet.
</listItem>
<subsubsectionHeader confidence="0.748946">
4.2.2 Preposition Errors
</subsubsectionHeader>
<listItem confidence="0.94458">
• DeFelice The system in (De Felice, 2008) for
preposition errors uses a similar rich set of syn-
tactic and semantic features as the system for
article errors. In our re-implementation, we do
not use a subcategorization dictionary, as this
resource was not available to us.
• TetreaultChunk The system in (Tetreault and
Chodorow, 2008) uses a chunker to extract
features from a two-word window around the
preposition, including lexical and POS n-
grams, and the head words from neighboring
constituents.
• TetreaultParse The system in (Tetreault et al.,
2010) extends (Tetreault and Chodorow, 2008)
by adding additional features derived from a
constituency and a dependency parse tree.
</listItem>
<bodyText confidence="0.998574666666667">
For each of the above feature sets, we add the ob-
served article or preposition as an additional feature
when training on learner text.
</bodyText>
<sectionHeader confidence="0.74826" genericHeader="method">
5 Alternating Structure Optimization
</sectionHeader>
<bodyText confidence="0.9998025">
This section describes the ASO algorithm and shows
how it can be used for grammatical error correction.
</bodyText>
<subsectionHeader confidence="0.977628">
5.1 The ASO algorithm
</subsectionHeader>
<bodyText confidence="0.9997044">
Alternating Structure Optimization (Ando and
Zhang, 2005) is a multi-task learning algorithm that
takes advantage of the common structure of multiple
related problems. Let us assume that we have m bi-
nary classification problems. Each classifier uZ is a
</bodyText>
<equation confidence="0.817594333333333">
1 n
u� = arg min
U n Z=1
</equation>
<page confidence="0.978323">
917
</page>
<bodyText confidence="0.999602">
weight vector of dimension p. Let O be an orthonor-
mal h x p matrix that captures the common struc-
ture of the m weight vectors. We assume that each
weight vector can be decomposed into two parts:
one part that models the particular i-th classification
problem and one part that models the common struc-
ture
</bodyText>
<equation confidence="0.982273">
ui = wi + OTvi. (2)
</equation>
<bodyText confidence="0.999188">
The parameters [{wi, vi}, O] can be learned by joint
empirical risk minimization, i.e., by minimizing the
joint empirical loss of the m problems on the train-
ing data
</bodyText>
<equation confidence="0.963898">
�L ((wl + OTvl)T Xli, Yil) + A   ||wl ||2
(3)
</equation>
<bodyText confidence="0.847589">
The key observation in ASO is that the problems
used to find O do not have to be same as the target
problems that we ultimately want to solve. Instead,
we can automatically create auxiliary problems for
the sole purpose of learning a better O.
Let us assume that we have k target problems and
m auxiliary problems. We can obtain an approxi-
mate solution to Equation 3 by performing the fol-
lowing algorithm (Ando and Zhang, 2005):
</bodyText>
<listItem confidence="0.959266818181818">
1. Learn m linear classifiers ui independently.
2. Let U = [u1, u2, ... , um] be the p x m matrix
formed from the m weight vectors.
3. Perform Singular Value Decomposition (SVD) on
U: U = V1DV2T . The first h column vectors of V1
are stored as rows of O.
4. Learn wj and vj for each of the target problems by
minimizing the empirical risk:
�L ��wj + OTvj�T Xi,Yi + A ||wj||2 .
5. The weight vector for the j-th target problem is:
uj = wj + OT vj.
</listItem>
<subsectionHeader confidence="0.959215">
5.2 ASO for Grammatical Error Correction
</subsectionHeader>
<bodyText confidence="0.999665689655173">
The key observation in our work is that the selection
task on non-learner text is a highly informative aux-
iliary problem for the correction task on learner text.
For example, a classifier that can predict the pres-
ence or absence of the preposition on can be help-
ful for correcting wrong uses of on in learner text,
e.g., if the classifier’s confidence for on is low but
the writer used the preposition on, the writer might
have made a mistake. As the auxiliary problems can
be created automatically, we can leverage the power
of very large corpora of non-learner text.
Let us assume a grammatical error correction task
with m classes. For each class, we define a bi-
nary auxiliary problem. The feature space of the
auxiliary problems is a restriction of the original
feature space X to all features except the observed
word: X\{Xobs}. The weight vectors of the aux-
iliary problems form the matrix U in Step 2 of the
ASO algorithm from which we obtain O through
SVD. Given O, we learn the vectors wj and vj,
j = 1, ... , k from the annotated learner text using
the complete feature space X.
This can be seen as an instance of transfer learn-
ing (Pan and Yang, 2010), as the auxiliary problems
are trained on data from a different domain (non-
learner text) and have a slightly different feature
space (X\{Xobs}). We note that our method is gen-
eral and can be applied to any classification problem
in GEC.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99743">
6.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.999943666666667">
The main corpus in our experiments is the NUS Cor-
pus of Learner English (NUCLE). The corpus con-
sists of about 1,400 essays written by EFL/ESL uni-
versity students on a wide range of topics, like en-
vironmental pollution or healthcare. It contains over
one million words which are completely annotated
with error tags and corrections. All annotations have
been performed by professional English instructors.
We use about 80% of the essays for training, 10% for
development, and 10% for testing. We ensure that
no sentences from the same essay appear in both the
training and the test or development data. NUCLE
is available to the community for research purposes.
On average, only 1.8% of the articles and 1.3%
of the prepositions in NUCLE contain an error.
This figure is considerably lower compared to other
learner corpora (Leacock et al., 2010, Ch. 3) and
shows that our writers have a relatively high profi-
ciency of English. We argue that this makes the task
considerably more difficult. Furthermore, to keep
the task as realistic as possible, we do not filter the
</bodyText>
<equation confidence="0.9832427">
1
n
�n
i=1
�m
l=1
1
n
�n
i=1
</equation>
<page confidence="0.966122">
918
</page>
<bodyText confidence="0.995108461538461">
test data in any way.
In addition to NUCLE, we use a subset of the
New York Times section of the Gigaword corpus1
and the Wall Street Journal section of the Penn Tree-
bank (Marcus et al., 1993) for some experiments.
We pre-process all corpora using the following tools:
We use NLTK2 for sentence splitting, OpenNLP3
for POS tagging, YamCha (Kudo and Matsumoto,
2003) for chunking, the C&amp;C tools (Clark and Cur-
ran, 2007) for CCG parsing and named entity recog-
nition, and the Stanford parser (Klein and Manning,
2003a; Klein and Manning, 2003b) for constituency
and dependency parsing.
</bodyText>
<subsectionHeader confidence="0.988907">
6.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9807996">
For experiments on non-learner text, we report ac-
curacy, which is defined as the number of correct
predictions divided by the total number of test in-
stances. For experiments on learner text, we report
Fi-measure
</bodyText>
<equation confidence="0.492018">
Precision x Recall
Precision + Recall
</equation>
<bodyText confidence="0.999873428571429">
where precision is the number of suggested correc-
tions that agree with the human annotator divided
by the total number of proposed corrections by the
system, and recall is the number of suggested cor-
rections that agree with the human annotator divided
by the total number of errors annotated by the human
annotator.
</bodyText>
<subsectionHeader confidence="0.9949525">
6.3 Selection Task Experiments on WSJ Test
Data
</subsectionHeader>
<bodyText confidence="0.99993925">
The first set of experiments investigates predicting
articles and prepositions in non-learner text. This
primarily serves as a reference point for the correc-
tion task described in the next section. We train
classifiers as described in Section 4 on the Giga-
word corpus. We train with up to 10 million train-
ing instances, which corresponds to about 37 million
words of text for articles and 112 million words of
text for prepositions. The test instances are extracted
from section 23 of the WSJ and no text from the
WSJ is included in the training data. The observed
article or preposition choice of the writer is the class
</bodyText>
<footnote confidence="0.955524333333333">
1LDC2009T13
2www.nltk.org
3opennlp.sourceforge.net
</footnote>
<bodyText confidence="0.999837666666667">
we want to predict. Therefore, the article or prepo-
sition cannot be part of the input features. Our pro-
posed ASO method is not included in these experi-
ments, as it uses the observed article or preposition
as a feature which is only applicable when testing on
learner text.
</bodyText>
<subsectionHeader confidence="0.9963705">
6.4 Correction Task Experiments on NUCLE
Test Data
</subsectionHeader>
<bodyText confidence="0.98784425">
The second set of experiments investigates the pri-
mary goal of this work: to automatically correct
grammatical errors in learner text. The test instances
are extracted from NUCLE. In contrast to the previ-
ous selection task, the observed word choice of the
writer can be different from the correct class and the
observed word is available during testing. We inves-
tigate two different baselines and our ASO method.
The first baseline is a classifier trained on the Gi-
gaword corpus in the same way as described in the
selection task experiment. We use a simple thresh-
olding strategy to make use of the observed word
during testing. The system only flags an error if the
difference between the classifier’s confidence for its
first choice and the confidence for the observed word
is higher than a threshold t. The threshold parame-
ter t is tuned on the NUCLE development data for
each feature set. In our experiments, the value for t
is between 0.7 and 1.2.
The second baseline is a classifier trained on NU-
CLE. The classifier is trained in the same way as
the Gigaword model, except that the observed word
choice of the writer is included as a feature. The cor-
rect class during training is the correction provided
by the human annotator. As the observed word is
part of the features, this model does not need an ex-
tra thresholding step. Indeed, we found that thresh-
olding is harmful in this case. During training, the
instances that do not contain an error greatly out-
number the instances that do contain an error. To re-
duce this imbalance, we keep all instances that con-
tain an error and retain a random sample of q percent
of the instances that do not contain an error. The
undersample parameter q is tuned on the NUCLE
development data for each data set. In our experi-
ments, the value for q is between 20% and 40%.
Our ASO method is trained in the following way.
We create binary auxiliary problems for articles or
prepositions, i.e., there are 3 auxiliary problems for
Fi=2x
</bodyText>
<page confidence="0.99528">
919
</page>
<bodyText confidence="0.999710818181818">
articles and 36 auxiliary problems for prepositions.
We train the classifiers for the auxiliary problems on
the complete 10 million instances from Gigaword in
the same ways as in the selection task experiment.
The weight vectors of the auxiliary problems form
the matrix U. We perform SVD to get U = V1DV2T .
We keep all columns of V1 to form O. The target
problems are again binary classification problems
for each article or preposition, but this time trained
on NUCLE. The observed word choice of the writer
is included as a feature for the target problems. We
again undersample the instances that do not contain
an error and tune the parameter q on the NUCLE de-
velopment data. The value for q is between 20% and
40%. No thresholding is applied.
We also experimented with a classifier that is
trained on the concatenated data from NUCLE and
Gigaword. This model always performed worse than
the better of the individual baselines. The reason is
that the two data sets have different feature spaces
which prevents simple concatenation of the training
data. We therefore omit these results from the paper.
</bodyText>
<sectionHeader confidence="0.975946" genericHeader="method">
7 Results
</sectionHeader>
<figure confidence="0.999784205882353">
1000 10000 100000 1e+06 1e+07
Number of training examples
(a) Articles
1000 10000 100000 1e+06 1e+07
Number of training examples
GIGAWORD DEFELICE
GIGAWORD HAN
GIGAWORD LEE
ACCURACY
0.70
0.65
0.60
0.55
0.50
0.45
0.40
0.35
0.30
0.25
GIGAWORD DEFELICE
GIGAWORD TETRAULTCHUNK
GIGAWORD TETRAULTPARSE
ACCURACY 0.88
0.86
0.84
0.82
0.80
0.78
0.76
0.74
0.72
0.70
0.68
(b) Prepositions
</figure>
<bodyText confidence="0.99965447826087">
The learning curves of the selection task experi-
ments on WSJ test data are shown in Figure 1. The
three curves in each plot correspond to different fea-
ture sets. Accuracy improves quickly in the be-
ginning but improvements get smaller as the size
of the training data increases. The best results are
87.56% for articles (Han) and 68.25% for prepo-
sitions (TetreaultParse). The best accuracy for ar-
ticles is comparable to the best reported results of
87.70% (Lee, 2004) on this data set.
The learning curves of the correction task ex-
periments on NUCLE test data are shown in Fig-
ure 2 and 3. Each sub-plot shows the curves of
three models as described in the last section: ASO
trained on NUCLE and Gigaword, the baseline clas-
sifier trained on NUCLE, and the baseline classifier
trained on Gigaword. For ASO, the x-axis shows
the number of target problem training instances. The
first observation is that high accuracy for the selec-
tion task on non-learner text does not automatically
entail high F1-measure on learner text. We also note
that feature sets with similar performance on non-
learner text can show very different performance on
</bodyText>
<figureCaption confidence="0.995105">
Figure 1: Accuracy for the selection task on WSJ
test data.
</figureCaption>
<bodyText confidence="0.952657117647059">
learner text. The second observation is that train-
ing on annotated learner text can significantly im-
prove performance. In three experiments (articles
DeFelice, Han, prepositions DeFelice), the NUCLE
model outperforms the Gigaword model trained on
10 million instances. Finally, the ASO models show
the best results. In the experiments where the NU-
CLE models already perform better than the Giga-
word baseline, ASO gives comparable or slightly
better results (articles DeFelice, Han, Lee, preposi-
tions DeFelice). In those experiments where neither
baseline shows good performance (TetreaultChunk,
TetreaultParse), ASO results in a large improvement
over either baseline. The best results are 19.29% F1-
measure for articles (Han) and 11.15% F1-measure
for prepositions (TetreaultParse) achieved by the
ASO model.
</bodyText>
<page confidence="0.97211">
920
</page>
<figure confidence="0.998495479166666">
F1
0.18
0.16
0.14
0.12
0.10
0.08
0.06
0.04
0.02
ASO
NUCLE
GIGAWORD
F1
0.20
0.18
0.16
0.14
0.12
0.10
0.08
0.06
0.04
0.02
ASO
NUCLE
GIGAWORD
F1
0.09
0.08
0.07
0.06
0.05
0.04
0.03
0.02
ASO
NUCLE
GIGAWORD
1000 10000 100000 1e+06 1e+07
Number of training examples
(a) DeFelice
1000 10000 100000 1e+06 1e+07
Number of training examples
(b) Han
1000 10000 100000 1e+06 1e+07
Number of training examples
(c) Lee
</figure>
<figureCaption confidence="0.948245">
Figure 2: F1-measure for the article correction task on NUCLE test data. Each plot shows ASO and two
baselines for a particular feature set.
</figureCaption>
<figure confidence="0.99984998">
0.09
0.08
0.07
0.06
0.05
F1
F1
F1
0.04
0.03
0.02
ASO
NUCLE
GIGAWORD
0.01
ASO
NUCLE
GIGAWORD
ASO
NUCLE
GIGAWORD
0.12
0.10
0.08
0.06
0.04
0.02
0.00
0.10
0.00
0.10
0.09
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0.00
1000 10000 100000 1e+06 1e+07
Number of training examples
(a) DeFelice
1000 10000 100000 1e+06 1e+07
Number of training examples
(b) TetreaultChunk
1000 10000 100000 1e+06 1e+07
Number of training examples
(c) TetreaultParse
</figure>
<figureCaption confidence="0.996357">
Figure 3: F1-measure for the preposition correction task on NUCLE test data. Each plot shows ASO and
</figureCaption>
<bodyText confidence="0.562307">
two baselines for a particular feature set.
</bodyText>
<sectionHeader confidence="0.959657" genericHeader="method">
8 Analysis
</sectionHeader>
<bodyText confidence="0.999953666666667">
In this section, we analyze the results in more detail
and show examples from our test set for illustration.
Table 1 shows precision, recall, and F1-measure
for the best models in our experiments. ASO
achieves a higher F1-measure than either baseline.
We use the sign-test with bootstrap re-sampling for
statistical significance testing. The sign-test is a non-
parametric test that makes fewer assumptions than
parametric tests like the t-test. The improvements in
F1-measure of ASO over either baseline are statis-
tically significant (p &lt; 0.001) for both articles and
prepositions.
The difficulty in GEC is that in many cases, more
than one word choice can be correct. Even with a
threshold, the Gigaword baseline model suggests too
many corrections, because the model cannot make
use of the observed word as a feature. This results in
low precision. For example, the model replaces as
</bodyText>
<table confidence="0.995785">
Articles
Model Prec Rec Fl
Gigaword (Han) 10.33 21.81 14.02
NUCLE (Han) 29.48 12.91 17.96
ASO (Han) 26.44 15.18 19.29
Prepositions
Model Prec Rec Fl
Gigaword (TetreaultParse ) 4.77 14.81 7.21
NUCLE (DeFelice) 13.84 5.55 7.92
ASO (TetreaultParse) 18.30 8.02 11.15
</table>
<tableCaption confidence="0.983434">
Table 1: Best results for the correction task on NU-
</tableCaption>
<bodyText confidence="0.987255875">
CLE test data. Improvements for ASO over either
baseline are statistically significant (p &lt; 0.001) for
both tasks.
with by in the sentence “This group should be cate-
gorized as the vulnerable group”, which is wrong.
In contrast, the NUCLE model learns a bias to-
wards the observed word and therefore achieves
higher precision. However, the training data is
</bodyText>
<page confidence="0.992753">
921
</page>
<bodyText confidence="0.999955411764706">
smaller and therefore recall is low as the model has
not seen enough examples during training. This is
especially true for prepositions which can occur in a
large variety of contexts. For example, the preposi-
tion in should be on in the sentence “... psychology
had an impact in the way we process and manage
technology”. The phrase “impact on the way” does
not appear in the NUCLE training data and the NU-
CLE baseline fails to detect the error.
The ASO model is able to take advantage of both
the annotated learner text and the large non-learner
text, thus achieving overall high Fl-measure. The
phrase “impact on the way”, for example, appears
many times in the Gigaword training data. With the
common structure learned from the auxiliary prob-
lems, the ASO model successfully finds and corrects
this mistake.
</bodyText>
<subsectionHeader confidence="0.976839">
8.1 Manual Evaluation
</subsectionHeader>
<bodyText confidence="0.999979206896552">
We carried out a manual evaluation of the best ASO
models and compared their output with two com-
mercial grammar checking software packages which
we call System A and System B. We randomly sam-
pled 1000 test instances for articles and 2000 test
instances for prepositions and manually categorized
each test instance into one of the following cate-
gories: (1) Correct means that both human and sys-
tem flag an error and suggest the same correction.
If the system’s correction differs from the human
but is equally acceptable, it is considered (2) Both
Ok. If the system identifies an error but fails to cor-
rect it, we consider it (3) Both Wrong, as both the
writer and the system are wrong. (4) Other Error
means that the system’s correction does not result
in a grammatical sentence because of another gram-
matical error that is outside the scope of article or
preposition errors, e.g., a noun number error as in
“all the dog”. If the system corrupts a previously
correct sentence it is a (5) False Flag. If the hu-
man flags an error but the system does not, it is a
(6) Miss. (7) No Flag means that neither the human
annotator nor the system flags an error. We calculate
precision by dividing the count of category (1) by the
sum of counts of categories (1), (3), and (5), and re-
call by dividing the count of category (1) by the sum
of counts of categories (1), (3), and (6). The results
are shown in Table 2. Our ASO method outperforms
both commercial software packages. Our evalua-
</bodyText>
<table confidence="0.998726">
Articles
ASO System A System B
(1) Correct 4 1 1
(2) Both Ok 16 12 18
(3) Both Wrong 0 1 0
(4) Other Error 1 0 0
(5) False Flag 1 0 4
(6) Miss 3 5 6
(7) No Flag 975 981 971
Precision 80.00 50.00 20.00
Recall 57.14 14.28 14.28
F1 66.67 22.21 16.67
Prepositions
ASO System A System B
(1) Correct 3 3 0
(2) Both Ok 35 39 24
(3) Both Wrong 0 2 0
(4) Other Error 0 0 0
(5) False Flag 5 11 1
(6) Miss 12 11 15
(7) No Flag 1945 1934 1960
Precision 37.50 18.75 0.00
Recall 20.00 18.75 0.00
F1 26.09 18.75 0.00
</table>
<tableCaption confidence="0.945289">
Table 2: Manual evaluation and comparison with
commercial grammar checking software.
</tableCaption>
<bodyText confidence="0.997326333333333">
tion shows that even commercial software packages
achieve low Fl-measure for article and preposition
errors, which confirms the difficulty of these tasks.
</bodyText>
<sectionHeader confidence="0.998634" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999996333333333">
We have presented a novel approach to grammati-
cal error correction based on Alternating Structure
Optimization. We have introduced the NUS Corpus
of Learner English (NUCLE), a fully annotated cor-
pus of learner text. Our experiments for article and
preposition errors show the advantage of our ASO
approach over two baseline methods. Our ASO ap-
proach also outperforms two commercial grammar
checking software packages in a manual evaluation.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9957856">
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore.
</bodyText>
<page confidence="0.995455">
922
</page>
<sectionHeader confidence="0.998338" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999172">
R.K. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and un-
labeled data. Journal of Machine Learning Research,
6.
S. Bergsma, D. Lin, and R. Goebel. 2009. Web-scale n-
gram models for lexical disambiguation. In Proceed-
ings of IJCAI.
M. Chodorow, J. Tetreault, and N.R. Han. 2007. De-
tection of grammatical errors involving prepositions.
In Proceedings of the 4th ACL-SIGSEM Workshop on
Prepositions.
S. Clark and J.R. Curran. 2007. Wide-coverage effi-
cient statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4).
R. De Felice. 2008. Automatic Error Detection in Non-
native English. Ph.D. thesis, University of Oxford.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press, Cambridge,MA.
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W.B.
Dolan, D. Belenko, and L. Vanderwende. 2008. Using
contextual speller techniques and language modeling
for ESL error correction. In Proceedings of IJCNLP.
M. Gamon. 2010. Using mostly native data to correct
errors in learners’ writing: A meta-classifier approach.
In Proceedings of HLT-NAACL.
N.R. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Natural Language Engineering, 12(02).
N.R. Han, J. Tetreault, S.H. Lee, and J.Y. Ha. 2010.
Using an error-annotated learner corpus to develop an
ESL/EFL error correction system. In Proceedings of
LREC.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-
hara. 2003. Automatic error detection in the Japanese
learners’ English spoken data. In Companion Volume
to the Proceedings of ACL.
D. Klein and C.D. Manning. 2003a. Accurate unlexical-
ized parsing. In Proceedings of ACL.
D. Klein and C.D. Manning. 2003b. Fast exact inference
with a factored model for natural language processing.
Advances in Neural Information Processing Systems
(NIPS 2002), 15.
K. Knight and I. Chander. 1994. Automated postediting
of documents. In Proceedings of AAAI.
T Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL.
M. Lapata and F. Keller. 2005. Web-based models for
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1).
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated Grammatical Error Detection for
Language Learners. Morgan &amp; Claypool Publishers,
San Rafael,CA.
J. Lee and O. Knutsson. 2008. The role of PP attachment
in preposition generation. In Proceedings of CICLing.
J. Lee. 2004. Automatic article restoration. In Proceed-
ings of HLT-NAACL.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
G. Minnen, F. Bond, and A. Copestake. 2000. Memory-
based learning for article generation. In Proceedings
of CoNLL.
R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006.
A feedback-augmented method for detecting errors in
the writing of learners of English. In Proceedings of
COLING-ACL.
S.J. Pan and Q. Yang. 2010. A survey on transfer learn-
ing. IEEE Transactions on Knowledge and Data En-
gineering, 22(10).
A. Rozovskaya and D. Roth. 2010a. Generating con-
fusion sets for context-sensitive error correction. In
Proceedings of EMNLP.
A. Rozovskaya and D. Roth. 2010b. Training paradigms
for correcting errors in grammar and usage. In Pro-
ceedings of HLT-NAACL.
J. Tetreault and M. Chodorow. 2008. The ups and downs
of preposition error detection in ESL writing. In Pro-
ceedings of COLING.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using
parse features for preposition selection and error de-
tection. In Proceedings ofACL.
X. Yi, J. Gao, and W.B. Dolan. 2008. A web-based En-
glish proofing system for English as a second language
users. In Proceedings of IJCNLP.
</reference>
<page confidence="0.998926">
923
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.629736">
<title confidence="0.997661">Grammatical Error Correction with Alternating Structure Optimization</title>
<author confidence="0.78883">Tou</author>
<affiliation confidence="0.8196545">Graduate School for Integrative Sciences and of Computer Science, National University of</affiliation>
<abstract confidence="0.999594133333333">We present a novel approach to grammatical error correction based on Alternating Structure Optimization. As part of our work, we the Corpus of Learner Ena fully annotated one million words corpus of learner English available for research purposes. We conduct an extensive evaluation for article and preposition errors using various feature sets. Our experiments show that our approach outperforms two baselines trained on non-learner text and learner text, respectively. Our approach also outperforms two commercial grammar checking software packages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R K Ando</author>
<author>T Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>6</volume>
<contexts>
<context position="2065" citStr="Ando and Zhang, 2005" startWordPosition="312" endWordPosition="315">rpora is not well explored, as are methods that combine learner and non-learner text. Furthermore, the evaluation of GEC has been problematic. Previous work has either evaluated on artificial test instances as a substitute for real learner errors or on proprietary data that is not available to other researchers. As a consequence, existing methods have not been compared on the same test set, leaving it unclear where the current state of the art really is. In this work, we aim to overcome both problems. First, we present a novel approach to GEC based on Alternating Structure Optimization (ASO) (Ando and Zhang, 2005). Our approach is able to train models on annotated learner corpora while still taking advantage of large non-learner corpora. Second, we introduce the NUS Corpus of Learner English (NUCLE), a fully annotated one million words corpus of learner English available for research purposes. We conduct an extensive evaluation for article and preposition errors using six different feature sets proposed in previous work. We compare our proposed ASO method with two baselines trained on non-learner text and learner text, respectively. To the best of our knowledge, this is the first extensive comparison o</context>
<context position="11224" citStr="Ando and Zhang, 2005" startWordPosition="1815" endWordPosition="1818">including lexical and POS ngrams, and the head words from neighboring constituents. • TetreaultParse The system in (Tetreault et al., 2010) extends (Tetreault and Chodorow, 2008) by adding additional features derived from a constituency and a dependency parse tree. For each of the above feature sets, we add the observed article or preposition as an additional feature when training on learner text. 5 Alternating Structure Optimization This section describes the ASO algorithm and shows how it can be used for grammatical error correction. 5.1 The ASO algorithm Alternating Structure Optimization (Ando and Zhang, 2005) is a multi-task learning algorithm that takes advantage of the common structure of multiple related problems. Let us assume that we have m binary classification problems. Each classifier uZ is a 1 n u� = arg min U n Z=1 917 weight vector of dimension p. Let O be an orthonormal h x p matrix that captures the common structure of the m weight vectors. We assume that each weight vector can be decomposed into two parts: one part that models the particular i-th classification problem and one part that models the common structure ui = wi + OTvi. (2) The parameters [{wi, vi}, O] can be learned by joi</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>R.K. Ando and T. Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bergsma</author>
<author>D Lin</author>
<author>R Goebel</author>
</authors>
<title>Web-scale ngram models for lexical disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="5335" citStr="Bergsma et al., 2009" startWordPosition="840" endWordPosition="843"> al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamon (2010), who combined features from the output of logistic-regression classifiers and language models trained on non-learner text in a meta-classifier trained on learner text. In this work, we show a more direct way to combine learner and non-learner text in a single model. Finally, researchers have investigated GEC in connection with web-based models in NLP (Lapata and Keller, 2005; Bergsma et al., 2009; Yi et al., 2008). These methods do not use classifiers, but rely on simple n-gram counts or page hits from the Web. 3 Task Description In this work, we focus on article and preposition errors, as they are among the most frequent types of errors made by EFL learners. 3.1 Selection vs. Correction Task There is an important difference between training on annotated learner text and training on non-learner text, namely whether the observed word can be used as a feature or not. When training on non-learner text, the observed word cannot be used as a feature. The word choice of the writer is “blank</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2009</marker>
<rawString>S. Bergsma, D. Lin, and R. Goebel. 2009. Web-scale ngram models for lexical disambiguation. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chodorow</author>
<author>J Tetreault</author>
<author>N R Han</author>
</authors>
<title>Detection of grammatical errors involving prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions.</booktitle>
<contexts>
<context position="4229" citStr="Chodorow et al., 2007" startWordPosition="661" endWordPosition="664">and preposition errors. For a more comprehensive survey, see (Leacock et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has </context>
</contexts>
<marker>Chodorow, Tetreault, Han, 2007</marker>
<rawString>M. Chodorow, J. Tetreault, and N.R. Han. 2007. Detection of grammatical errors involving prepositions. In Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>Wide-coverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="15846" citStr="Clark and Curran, 2007" startWordPosition="2659" endWordPosition="2663">ly high proficiency of English. We argue that this makes the task considerably more difficult. Furthermore, to keep the task as realistic as possible, we do not filter the 1 n �n i=1 �m l=1 1 n �n i=1 918 test data in any way. In addition to NUCLE, we use a subset of the New York Times section of the Gigaword corpus1 and the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) for some experiments. We pre-process all corpora using the following tools: We use NLTK2 for sentence splitting, OpenNLP3 for POS tagging, YamCha (Kudo and Matsumoto, 2003) for chunking, the C&amp;C tools (Clark and Curran, 2007) for CCG parsing and named entity recognition, and the Stanford parser (Klein and Manning, 2003a; Klein and Manning, 2003b) for constituency and dependency parsing. 6.2 Evaluation Metrics For experiments on non-learner text, we report accuracy, which is defined as the number of correct predictions divided by the total number of test instances. For experiments on learner text, we report Fi-measure Precision x Recall Precision + Recall where precision is the number of suggested corrections that agree with the human annotator divided by the total number of proposed corrections by the system, and </context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>S. Clark and J.R. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R De Felice</author>
</authors>
<title>Automatic Error Detection in Nonnative English.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Oxford.</institution>
<marker>De Felice, 2008</marker>
<rawString>R. De Felice. 2008. Automatic Error Detection in Nonnative English. Ph.D. thesis, University of Oxford.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press, Cambridge,MA.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An electronic lexical database. MIT Press, Cambridge,MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>J Gao</author>
<author>C Brockett</author>
<author>A Klementiev</author>
<author>W B Dolan</author>
<author>D Belenko</author>
<author>L Vanderwende</author>
</authors>
<title>Using contextual speller techniques and language modeling for ESL error correction.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<contexts>
<context position="4010" citStr="Gamon et al., 2008" startWordPosition="624" endWordPosition="627">n for Computational Linguistics, pages 915–923, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work In this section, we give a brief overview on related work on article and preposition errors. For a more comprehensive survey, see (Leacock et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth,</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>M. Gamon, J. Gao, C. Brockett, A. Klementiev, W.B. Dolan, D. Belenko, and L. Vanderwende. 2008. Using contextual speller techniques and language modeling for ESL error correction. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
</authors>
<title>Using mostly native data to correct errors in learners’ writing: A meta-classifier approach.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="4935" citStr="Gamon (2010)" startWordPosition="780" endWordPosition="781">2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamon (2010), who combined features from the output of logistic-regression classifiers and language models trained on non-learner text in a meta-classifier trained on learner text. In this work, we show a more direct way to combine learner and non-learner text in a single model. Finally, researchers have investigated GEC in connection with web-based models in NLP (Lapata and Keller, 2005; Bergsma et al., 2009; Yi et al., 2008). These methods do not use classifiers, but rely on simple n-gram counts or page hits from the Web. 3 Task Description In this work, we focus on article and preposition errors, as th</context>
</contexts>
<marker>Gamon, 2010</marker>
<rawString>M. Gamon. 2010. Using mostly native data to correct errors in learners’ writing: A meta-classifier approach. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N R Han</author>
<author>M Chodorow</author>
<author>C Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>02</issue>
<contexts>
<context position="4064" citStr="Han et al., 2006" startWordPosition="633" endWordPosition="636"> Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work In this section, we give a brief overview on related work on article and preposition errors. For a more comprehensive survey, see (Leacock et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small </context>
<context position="9855" citStr="Han et al., 2006" startWordPosition="1596" endWordPosition="1599">nt. The prediction of the classifier is the class with the highest score Y� = arg maxY ,Y (uTY X). In earlier experiments, this linear classifier gave comparable or superior performance compared to a logistic regression classifier. 4.2 Features We re-implement six feature extraction methods from previous work, three for articles and three for prepositions. The methods require different linguistic pre-processing: chunking, CCG parsing, and constituency parsing. 4.2.1 Article Errors • DeFelice The system in (De Felice, 2008) for article errors uses a CCG parser to extract a • Han The system in (Han et al., 2006) relies on shallow syntactic and lexical features derived from a chunker, including the words before, in, and after the NP, the head word, and POS tags. • Lee The system in (Lee, 2004) uses a constituency parser. The features include POS tags, surrounding words, the head word, and hypernyms from WordNet. 4.2.2 Preposition Errors • DeFelice The system in (De Felice, 2008) for preposition errors uses a similar rich set of syntactic and semantic features as the system for article errors. In our re-implementation, we do not use a subcategorization dictionary, as this resource was not available to </context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>N.R. Han, M. Chodorow, and C. Leacock. 2006. Detecting errors in English article usage by non-native speakers. Natural Language Engineering, 12(02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N R Han</author>
<author>J Tetreault</author>
<author>S H Lee</author>
<author>J Y Ha</author>
</authors>
<title>Using an error-annotated learner corpus to develop an ESL/EFL error correction system.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="4518" citStr="Han et al., 2010" startWordPosition="710" endWordPosition="713">memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamon (2010), who combined features from the output of logistic-regression classifiers and language models trained on non-learner text in a meta-classifier trained on learner text. In this work, </context>
</contexts>
<marker>Han, Tetreault, Lee, Ha, 2010</marker>
<rawString>N.R. Han, J. Tetreault, S.H. Lee, and J.Y. Ha. 2010. Using an error-annotated learner corpus to develop an ESL/EFL error correction system. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>T Saiga</author>
<author>T Supnithi</author>
<author>H Isahara</author>
</authors>
<date>2003</date>
<booktitle>Automatic error detection in the Japanese learners’ English spoken data. In Companion Volume to the Proceedings of ACL.</booktitle>
<contexts>
<context position="4684" citStr="Izumi et al., 2003" startWordPosition="738" endWordPosition="741"> De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamon (2010), who combined features from the output of logistic-regression classifiers and language models trained on non-learner text in a meta-classifier trained on learner text. In this work, we show a more direct way to combine learner and non-learner text in a single model. Finally, researchers have investigated GEC in connection with web-based models in</context>
</contexts>
<marker>Izumi, Uchimoto, Saiga, Supnithi, Isahara, 2003</marker>
<rawString>E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isahara. 2003. Automatic error detection in the Japanese learners’ English spoken data. In Companion Volume to the Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="15941" citStr="Klein and Manning, 2003" startWordPosition="2676" endWordPosition="2679">Furthermore, to keep the task as realistic as possible, we do not filter the 1 n �n i=1 �m l=1 1 n �n i=1 918 test data in any way. In addition to NUCLE, we use a subset of the New York Times section of the Gigaword corpus1 and the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) for some experiments. We pre-process all corpora using the following tools: We use NLTK2 for sentence splitting, OpenNLP3 for POS tagging, YamCha (Kudo and Matsumoto, 2003) for chunking, the C&amp;C tools (Clark and Curran, 2007) for CCG parsing and named entity recognition, and the Stanford parser (Klein and Manning, 2003a; Klein and Manning, 2003b) for constituency and dependency parsing. 6.2 Evaluation Metrics For experiments on non-learner text, we report accuracy, which is defined as the number of correct predictions divided by the total number of test instances. For experiments on learner text, we report Fi-measure Precision x Recall Precision + Recall where precision is the number of suggested corrections that agree with the human annotator divided by the total number of proposed corrections by the system, and recall is the number of suggested corrections that agree with the human annotator divided by th</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C.D. Manning. 2003a. Accurate unlexicalized parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language processing.</title>
<date>2003</date>
<booktitle>Advances in Neural Information Processing Systems (NIPS</booktitle>
<pages>15</pages>
<contexts>
<context position="15941" citStr="Klein and Manning, 2003" startWordPosition="2676" endWordPosition="2679">Furthermore, to keep the task as realistic as possible, we do not filter the 1 n �n i=1 �m l=1 1 n �n i=1 918 test data in any way. In addition to NUCLE, we use a subset of the New York Times section of the Gigaword corpus1 and the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) for some experiments. We pre-process all corpora using the following tools: We use NLTK2 for sentence splitting, OpenNLP3 for POS tagging, YamCha (Kudo and Matsumoto, 2003) for chunking, the C&amp;C tools (Clark and Curran, 2007) for CCG parsing and named entity recognition, and the Stanford parser (Klein and Manning, 2003a; Klein and Manning, 2003b) for constituency and dependency parsing. 6.2 Evaluation Metrics For experiments on non-learner text, we report accuracy, which is defined as the number of correct predictions divided by the total number of test instances. For experiments on learner text, we report Fi-measure Precision x Recall Precision + Recall where precision is the number of suggested corrections that agree with the human annotator divided by the total number of proposed corrections by the system, and recall is the number of suggested corrections that agree with the human annotator divided by th</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C.D. Manning. 2003b. Fast exact inference with a factored model for natural language processing. Advances in Neural Information Processing Systems (NIPS 2002), 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>I Chander</author>
</authors>
<title>Automated postediting of documents.</title>
<date>1994</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="3779" citStr="Knight and Chander (1994)" startWordPosition="589" endWordPosition="592">O algorithm. The experiments are presented in Section 6 and the results in Section 7. Section 8 contains a more detailed analysis of the results. Section 9 concludes the paper. 915 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 915–923, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work In this section, we give a brief overview on related work on article and preposition errors. For a more comprehensive survey, see (Leacock et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use </context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>K. Knight and I. Chander. 1994. Automated postediting of documents. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="15793" citStr="Kudo and Matsumoto, 2003" startWordPosition="2650" endWordPosition="2653">2010, Ch. 3) and shows that our writers have a relatively high proficiency of English. We argue that this makes the task considerably more difficult. Furthermore, to keep the task as realistic as possible, we do not filter the 1 n �n i=1 �m l=1 1 n �n i=1 918 test data in any way. In addition to NUCLE, we use a subset of the New York Times section of the Gigaword corpus1 and the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) for some experiments. We pre-process all corpora using the following tools: We use NLTK2 for sentence splitting, OpenNLP3 for POS tagging, YamCha (Kudo and Matsumoto, 2003) for chunking, the C&amp;C tools (Clark and Curran, 2007) for CCG parsing and named entity recognition, and the Stanford parser (Klein and Manning, 2003a; Klein and Manning, 2003b) for constituency and dependency parsing. 6.2 Evaluation Metrics For experiments on non-learner text, we report accuracy, which is defined as the number of correct predictions divided by the total number of test instances. For experiments on learner text, we report Fi-measure Precision x Recall Precision + Recall where precision is the number of suggested corrections that agree with the human annotator divided by the tot</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>T Kudo and Y. Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>F Keller</author>
</authors>
<title>Web-based models for natural language processing.</title>
<date>2005</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="5313" citStr="Lapata and Keller, 2005" startWordPosition="836" endWordPosition="839"> partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamon (2010), who combined features from the output of logistic-regression classifiers and language models trained on non-learner text in a meta-classifier trained on learner text. In this work, we show a more direct way to combine learner and non-learner text in a single model. Finally, researchers have investigated GEC in connection with web-based models in NLP (Lapata and Keller, 2005; Bergsma et al., 2009; Yi et al., 2008). These methods do not use classifiers, but rely on simple n-gram counts or page hits from the Web. 3 Task Description In this work, we focus on article and preposition errors, as they are among the most frequent types of errors made by EFL learners. 3.1 Selection vs. Correction Task There is an important difference between training on annotated learner text and training on non-learner text, namely whether the observed word can be used as a feature or not. When training on non-learner text, the observed word cannot be used as a feature. The word choice o</context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>M. Lapata and F. Keller. 2005. Web-based models for natural language processing. ACM Transactions on Speech and Language Processing, 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
<author>M Gamon</author>
<author>J Tetreault</author>
</authors>
<title>Automated Grammatical Error Detection for Language Learners.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool Publishers,</publisher>
<location>San Rafael,CA.</location>
<contexts>
<context position="3691" citStr="Leacock et al., 2010" startWordPosition="574" endWordPosition="577">ction 4 formulates GEC as a classification problem. Section 5 extends this to the ASO algorithm. The experiments are presented in Section 6 and the results in Section 7. Section 8 contains a more detailed analysis of the results. Section 9 concludes the paper. 915 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 915–923, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work In this section, we give a brief overview on related work on article and preposition errors. For a more comprehensive survey, see (Leacock et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Ch</context>
<context position="15172" citStr="Leacock et al., 2010" startWordPosition="2536" endWordPosition="2539">lthcare. It contains over one million words which are completely annotated with error tags and corrections. All annotations have been performed by professional English instructors. We use about 80% of the essays for training, 10% for development, and 10% for testing. We ensure that no sentences from the same essay appear in both the training and the test or development data. NUCLE is available to the community for research purposes. On average, only 1.8% of the articles and 1.3% of the prepositions in NUCLE contain an error. This figure is considerably lower compared to other learner corpora (Leacock et al., 2010, Ch. 3) and shows that our writers have a relatively high proficiency of English. We argue that this makes the task considerably more difficult. Furthermore, to keep the task as realistic as possible, we do not filter the 1 n �n i=1 �m l=1 1 n �n i=1 918 test data in any way. In addition to NUCLE, we use a subset of the New York Times section of the Gigaword corpus1 and the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) for some experiments. We pre-process all corpora using the following tools: We use NLTK2 for sentence splitting, OpenNLP3 for POS tagging, YamCha (Kudo</context>
</contexts>
<marker>Leacock, Chodorow, Gamon, Tetreault, 2010</marker>
<rawString>C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault. 2010. Automated Grammatical Error Detection for Language Learners. Morgan &amp; Claypool Publishers, San Rafael,CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
<author>O Knutsson</author>
</authors>
<title>The role of PP attachment in preposition generation.</title>
<date>2008</date>
<booktitle>In Proceedings of</booktitle>
<contexts>
<context position="4273" citStr="Lee and Knutsson, 2008" startWordPosition="669" endWordPosition="672">nsive survey, see (Leacock et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non</context>
</contexts>
<marker>Lee, Knutsson, 2008</marker>
<rawString>J. Lee and O. Knutsson. 2008. The role of PP attachment in preposition generation. In Proceedings of CICLing. J. Lee. 2004. Automatic article restoration. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="15620" citStr="Marcus et al., 1993" startWordPosition="2624" endWordPosition="2627"> only 1.8% of the articles and 1.3% of the prepositions in NUCLE contain an error. This figure is considerably lower compared to other learner corpora (Leacock et al., 2010, Ch. 3) and shows that our writers have a relatively high proficiency of English. We argue that this makes the task considerably more difficult. Furthermore, to keep the task as realistic as possible, we do not filter the 1 n �n i=1 �m l=1 1 n �n i=1 918 test data in any way. In addition to NUCLE, we use a subset of the New York Times section of the Gigaword corpus1 and the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) for some experiments. We pre-process all corpora using the following tools: We use NLTK2 for sentence splitting, OpenNLP3 for POS tagging, YamCha (Kudo and Matsumoto, 2003) for chunking, the C&amp;C tools (Clark and Curran, 2007) for CCG parsing and named entity recognition, and the Stanford parser (Klein and Manning, 2003a; Klein and Manning, 2003b) for constituency and dependency parsing. 6.2 Evaluation Metrics For experiments on non-learner text, we report accuracy, which is defined as the number of correct predictions divided by the total number of test instances. For experiments on learner t</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>F Bond</author>
<author>A Copestake</author>
</authors>
<title>Memorybased learning for article generation.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="3944" citStr="Minnen et al., 2000" startWordPosition="613" endWordPosition="616">paper. 915 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 915–923, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work In this section, we give a brief overview on related work on article and preposition errors. For a more comprehensive survey, see (Leacock et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed wor</context>
</contexts>
<marker>Minnen, Bond, Copestake, 2000</marker>
<rawString>G. Minnen, F. Bond, and A. Copestake. 2000. Memorybased learning for article generation. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nagata</author>
<author>A Kawai</author>
<author>K Morihiro</author>
<author>N Isu</author>
</authors>
<title>A feedback-augmented method for detecting errors in the writing of learners of English.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="3989" citStr="Nagata et al., 2006" startWordPosition="620" endWordPosition="623">ing of the Association for Computational Linguistics, pages 915–923, Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics 2 Related Work In this section, we give a brief overview on related work on article and preposition errors. For a more comprehensive survey, see (Leacock et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature </context>
</contexts>
<marker>Nagata, Kawai, Morihiro, Isu, 2006</marker>
<rawString>R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. A feedback-augmented method for detecting errors in the writing of learners of English. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Pan</author>
<author>Q Yang</author>
</authors>
<title>A survey on transfer learning.</title>
<date>2010</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>22</volume>
<issue>10</issue>
<contexts>
<context position="14062" citStr="Pan and Yang, 2010" startWordPosition="2349" endWordPosition="2352">rpora of non-learner text. Let us assume a grammatical error correction task with m classes. For each class, we define a binary auxiliary problem. The feature space of the auxiliary problems is a restriction of the original feature space X to all features except the observed word: X\{Xobs}. The weight vectors of the auxiliary problems form the matrix U in Step 2 of the ASO algorithm from which we obtain O through SVD. Given O, we learn the vectors wj and vj, j = 1, ... , k from the annotated learner text using the complete feature space X. This can be seen as an instance of transfer learning (Pan and Yang, 2010), as the auxiliary problems are trained on data from a different domain (nonlearner text) and have a slightly different feature space (X\{Xobs}). We note that our method is general and can be applied to any classification problem in GEC. 6 Experiments 6.1 Data Sets The main corpus in our experiments is the NUS Corpus of Learner English (NUCLE). The corpus consists of about 1,400 essays written by EFL/ESL university students on a wide range of topics, like environmental pollution or healthcare. It contains over one million words which are completely annotated with error tags and corrections. Al</context>
</contexts>
<marker>Pan, Yang, 2010</marker>
<rawString>S.J. Pan and Q. Yang. 2010. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Generating confusion sets for context-sensitive error correction.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4615" citStr="Rozovskaya and Roth, 2010" startWordPosition="727" endWordPosition="730"> Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamon (2010), who combined features from the output of logistic-regression classifiers and language models trained on non-learner text in a meta-classifier trained on learner text. In this work, we show a more direct way to combine learner and non-learner text in a single model. Finally, res</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>A. Rozovskaya and D. Roth. 2010a. Generating confusion sets for context-sensitive error correction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Training paradigms for correcting errors in grammar and usage.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="4615" citStr="Rozovskaya and Roth, 2010" startWordPosition="727" endWordPosition="730"> Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamon (2010), who combined features from the output of logistic-regression classifiers and language models trained on non-learner text in a meta-classifier trained on learner text. In this work, we show a more direct way to combine learner and non-learner text in a single model. Finally, res</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>A. Rozovskaya and D. Roth. 2010b. Training paradigms for correcting errors in grammar and usage. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>M Chodorow</author>
</authors>
<title>The ups and downs of preposition error detection in ESL writing.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="4303" citStr="Tetreault and Chodorow, 2008" startWordPosition="673" endWordPosition="676">ck et al., 2010). The seminal work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. Th</context>
<context position="10520" citStr="Tetreault and Chodorow, 2008" startWordPosition="1706" endWordPosition="1709">ical features derived from a chunker, including the words before, in, and after the NP, the head word, and POS tags. • Lee The system in (Lee, 2004) uses a constituency parser. The features include POS tags, surrounding words, the head word, and hypernyms from WordNet. 4.2.2 Preposition Errors • DeFelice The system in (De Felice, 2008) for preposition errors uses a similar rich set of syntactic and semantic features as the system for article errors. In our re-implementation, we do not use a subcategorization dictionary, as this resource was not available to us. • TetreaultChunk The system in (Tetreault and Chodorow, 2008) uses a chunker to extract features from a two-word window around the preposition, including lexical and POS ngrams, and the head words from neighboring constituents. • TetreaultParse The system in (Tetreault et al., 2010) extends (Tetreault and Chodorow, 2008) by adding additional features derived from a constituency and a dependency parse tree. For each of the above feature sets, we add the observed article or preposition as an additional feature when training on learner text. 5 Alternating Structure Optimization This section describes the ASO algorithm and shows how it can be used for gramm</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>J. Tetreault and M. Chodorow. 2008. The ups and downs of preposition error detection in ESL writing. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>J Foster</author>
<author>M Chodorow</author>
</authors>
<title>Using parse features for preposition selection and error detection.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4327" citStr="Tetreault et al., 2010" startWordPosition="677" endWordPosition="680">work on grammatical error correction was done by Knight and Chander (1994) on article errors. Subsequent work has focused on designing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classification approach and mainly differs in terms of the features employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training. Recent work has shown that training on annotated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly annotated (Han et al., 2010), or artificially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamo</context>
<context position="10742" citStr="Tetreault et al., 2010" startWordPosition="1741" endWordPosition="1744">he head word, and hypernyms from WordNet. 4.2.2 Preposition Errors • DeFelice The system in (De Felice, 2008) for preposition errors uses a similar rich set of syntactic and semantic features as the system for article errors. In our re-implementation, we do not use a subcategorization dictionary, as this resource was not available to us. • TetreaultChunk The system in (Tetreault and Chodorow, 2008) uses a chunker to extract features from a two-word window around the preposition, including lexical and POS ngrams, and the head words from neighboring constituents. • TetreaultParse The system in (Tetreault et al., 2010) extends (Tetreault and Chodorow, 2008) by adding additional features derived from a constituency and a dependency parse tree. For each of the above feature sets, we add the observed article or preposition as an additional feature when training on learner text. 5 Alternating Structure Optimization This section describes the ASO algorithm and shows how it can be used for grammatical error correction. 5.1 The ASO algorithm Alternating Structure Optimization (Ando and Zhang, 2005) is a multi-task learning algorithm that takes advantage of the common structure of multiple related problems. Let us </context>
</contexts>
<marker>Tetreault, Foster, Chodorow, 2010</marker>
<rawString>J. Tetreault, J. Foster, and M. Chodorow. 2010. Using parse features for preposition selection and error detection. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yi</author>
<author>J Gao</author>
<author>W B Dolan</author>
</authors>
<title>A web-based English proofing system for English as a second language users.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP.</booktitle>
<contexts>
<context position="5353" citStr="Yi et al., 2008" startWordPosition="844" endWordPosition="847">cially created (Rozovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a). Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamon (2010), who combined features from the output of logistic-regression classifiers and language models trained on non-learner text in a meta-classifier trained on learner text. In this work, we show a more direct way to combine learner and non-learner text in a single model. Finally, researchers have investigated GEC in connection with web-based models in NLP (Lapata and Keller, 2005; Bergsma et al., 2009; Yi et al., 2008). These methods do not use classifiers, but rely on simple n-gram counts or page hits from the Web. 3 Task Description In this work, we focus on article and preposition errors, as they are among the most frequent types of errors made by EFL learners. 3.1 Selection vs. Correction Task There is an important difference between training on annotated learner text and training on non-learner text, namely whether the observed word can be used as a feature or not. When training on non-learner text, the observed word cannot be used as a feature. The word choice of the writer is “blanked out” from the t</context>
</contexts>
<marker>Yi, Gao, Dolan, 2008</marker>
<rawString>X. Yi, J. Gao, and W.B. Dolan. 2008. A web-based English proofing system for English as a second language users. In Proceedings of IJCNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>