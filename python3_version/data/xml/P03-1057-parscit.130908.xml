<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998217">
Feedback Cleaning of Machine Translation Rules
Using Automatic Evaluation
</title>
<author confidence="0.803147">
Kenji Imamura, Eiichiro Sumita
</author>
<affiliation confidence="0.442267">
ATR Spoken Language Translation
Research Laboratories
Seika-cho, Soraku-gun, Kyoto, Japan
</affiliation>
<email confidence="0.993962">
{kenji.imamura,eiichiro.sumita}@atr.co.jp
</email>
<author confidence="0.991018">
Yuji Matsumoto
</author>
<affiliation confidence="0.823383666666667">
Nara Institute of
Science and Technology
Ikoma-shi, Nara, Japan
</affiliation>
<email confidence="0.998811">
matsu@is.aist-nara.ac.jp
</email>
<sectionHeader confidence="0.998597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99976645">
When rules of transfer-based machine
translation (MT) are automatically ac-
quired from bilingual corpora, incor-
rect/redundant rules are generated due to
acquisition errors or translation variety in
the corpora. As a new countermeasure
to this problem, we propose a feedback
cleaning method using automatic evalua-
tion of MT quality, which removes incor-
rect/redundant rules as a way to increase
the evaluation score. BLEU is utilized
for the automatic evaluation. The hill-
climbing algorithm, which involves fea-
tures of this task, is applied to searching
for the optimal combination of rules. Our
experiments show that the MT quality im-
proves by 10% in test sentences according
to a subjective evaluation. This is consid-
erable improvement over previous meth-
ods.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997828357142857">
Along with the efforts made in accumulating bilin-
gual corpora for many language pairs, quite a few
machine translation (MT) systems that automati-
cally acquire their knowledge from corpora have
been proposed. However, knowledge for transfer-
based MT acquired from corpora contains many in-
correct/redundant rules due to acquisition errors or
translation variety in the corpora. Such rules con-
flict with other existing rules and cause implausible
MT results or increase ambiguity. If incorrect rules
could be avoided, MT quality would necessarily im-
prove.
There are two approaches to overcoming incor-
rect/redundant rules:
</bodyText>
<listItem confidence="0.99917425">
• Selecting appropriate rules in a disambiguation
process during the translation (on-line process-
ing, (Meyers et al., 2000)).
• Cleaning incorrect/redundant rules after
</listItem>
<bodyText confidence="0.938903">
automatic acquisition (off-line processing,
(Menezes and Richardson, 2001; Imamura,
2002)).
We employ the second approach in this paper.
The cutoff by frequency (Menezes and Richardson,
2001) and the hypothesis test (Imamura, 2002) have
been applied to clean the rules. The cutoff by fre-
quency can slightly improve MT quality, but the im-
provement is still insufficient from the viewpoint of
the large number of redundant rules. The hypothesis
test requires very large corpora in order to obtain a
sufficient number of rules that are statistically confi-
dent.
Another current topic of machine translation is
automatic evaluation of MT quality (Papineni et al.,
2002; Yasuda et al., 2001; Akiba et al., 2001). These
methods aim to replace subjective evaluation in or-
der to speed up the development cycle of MT sys-
tems. However, they can be utilized not only as de-
velopers’ aids but also for automatic tuning of MT
systems (Su et al., 1992).
We propose feedback cleaning that utilizes
an automatic evaluation for removing incor-
rect/redundant translation rules as a tuning method
</bodyText>
<figure confidence="0.996338875">
Automatic
Acquisition
Training
Corpus
Rule
Selection/Deletion
Translation
Rules
Feedback Cleaning
Evaluation
Corpus
Automatic
Evaluation
MT
Engine
MT Results
</figure>
<figureCaption confidence="0.999996">
Figure 1: Structure of Feedback Cleaning
</figureCaption>
<bodyText confidence="0.999874625">
(Figure 1). Our method evaluates the contribution
of each rule to the MT results and removes inap-
propriate rules as a way to increase the evaluation
scores. Since the automatic evaluation correlates
with a subjective evaluation, MT quality will im-
prove after cleaning.
Our method only evaluates MT results and does
not consider various conditions of the MT engine,
such as parameters, interference in dictionaries, dis-
ambiguation methods, and so on. Even if an MT
engine avoids incorrect/redundant rules by on-line
processing, errors inevitably remain. Our method
cleans the rules in advance by only focusing on the
remaining errors. Thus, our method complements
on-line processing and adapts translation rules to the
given conditions of the MT engine.
</bodyText>
<sectionHeader confidence="0.9969895" genericHeader="introduction">
2 MT System and Problems of Automatic
Acquisition
</sectionHeader>
<subsectionHeader confidence="0.999078">
2.1 MT Engine
</subsectionHeader>
<bodyText confidence="0.999965923076923">
We use the Hierarchical Phrase Alignment-based
Translator (HPAT) (Imamura, 2002) as a transfer-
based MT system. The most important knowledge in
HPAT is transfer rules, which define the correspon-
dences between source and target language expres-
sions. An example of English-to-Japanese transfer
rules is shown in Figure 2. The transfer rules are
regarded as a synchronized context-free grammar.
When the system translates an input sentence, the
sentence is first parsed by using source patterns of
the transfer rules. Next, a tree structure of the tar-
get language is generated by mapping the source
patterns to the corresponding target patterns. When
non-terminal symbols remain in the target tree, tar-
get words are inserted by referring to a translation
dictionary.
Ambiguities, which occur during parsing or map-
ping, are resolved by selecting the rules that mini-
mize the semantic distance between the input words
and source examples (real examples in the training
corpus) of the transfer rules (Furuse and Iida, 1994).
For instance, when the input phrase “leave at 11
a.m.” is translated into Japanese, Rule 2 in Figure
2 is selected because the semantic distance from the
source example (arrive, p.m.) is the shortest to the
head words of the input phrase (leave, a.m.).
</bodyText>
<subsectionHeader confidence="0.999851">
2.2 Problems of Automatic Acquisition
</subsectionHeader>
<bodyText confidence="0.999966">
HPAT automatically acquires its transfer rules from
parallel corpora by using Hierarchical Phrase Align-
ment (Imamura, 2001). However, the rule set con-
tains many incorrect/redundant rules. The reasons
for this problem are roughly classified as follows.
</bodyText>
<listItem confidence="0.9999415">
• Errors in automatic rule acquisition
• Translation variety in corpora
</listItem>
<bodyText confidence="0.9894934375">
– The acquisition process cannot generalize
the rules because bilingual sentences de-
pend on the context or the situation.
– Corpora contain multiple (paraphrasable)
translations of the same source expres-
sion.
In the experiment of Imamura (2002), about
92,000 transfer rules were acquired from about
120,000 bilingual sentences 1. Most of these rules
are low-frequency. They reported that MT quality
slightly improved, even though the low-frequency
rules were removed to a level of about 1/9 the pre-
vious number. However, since some of them, such
as idiomatic rules, are necessary for translation, MT
quality cannot be dramatically improved by only re-
moving low-frequency rules.
</bodyText>
<sectionHeader confidence="0.985523" genericHeader="method">
3 Automatic Evaluation of MT Quality
</sectionHeader>
<bodyText confidence="0.99988375">
We utilize BLEU (Papineni et al., 2002) for the au-
tomatic evaluation of MT quality in this paper.
BLEU measures the similarity between MT re-
sults and translation results made by humans (called
</bodyText>
<footnote confidence="0.9768685">
1In this paper, the number of rules denotes the number of
unique pairs of source patterns and target patterns.
</footnote>
<table confidence="0.8517514">
Rule No. Syn. Cat. Source Pattern Target Pattern Source Example
1 VP XIP at YNP ⇒ Y’ de X’ ((present, conference) ...)
2 VP XIP at YNP ⇒ Y’ ni X’ ((stay, hotel), (arrive, p.m) ...)
3 VP XIP at YNP ⇒ Y’ wo X’ ((look, it) ...)
4 NP XNP at YNP ⇒ Y’ no X’ ((man, front desk) ...)
</table>
<figureCaption confidence="0.999187">
Figure 2: Example of HPAT Transfer Rules
</figureCaption>
<bodyText confidence="0.999924956521739">
references). This similarity is measured by N-gram
precision scores. Several kinds of N-grams can be
used in BLEU. We use from 1-gram to 4-gram in
this paper, where a 1-gram precision score indicates
the adequacy of word translation and longer N-gram
(e.g., 4-gram) precision scores indicate fluency of
sentence translation. The BLEU score is calculated
from the product of N-gram precision scores, so this
measure combines adequacy and fluency.
Note that a sizeable set of MT results is necessary
in order to calculate an accurate BLEU score. Al-
though it is possible to calculate the BLEU score of a
single MT result, it contains errors from the subjec-
tive evaluation. BLEU cancels out individual errors
by summing the similarities of MT results. There-
fore, we need all of the MT results from the evalua-
tion corpus in order to calculate an accurate BLEU
score.
One feature of BLEU is its use of multiple ref-
erences for a single source sentence. However, one
reference per sentence is used in this paper because
an already existing bilingual corpus is applied to the
cleaning.
</bodyText>
<sectionHeader confidence="0.99054" genericHeader="method">
4 Feedback Cleaning
</sectionHeader>
<bodyText confidence="0.999983714285714">
In this section, we introduce the proposed method,
called feedback cleaning. This method is carried out
by selecting or removing translation rules to increase
the BLEU score of the evaluation corpus (Figure 1).
Thus, this task is regarded as a combinatorial op-
timization problem of translation rules. The hill-
climbing algorithm, which involves the features of
this task, is applied to the optimization. The fol-
lowing sections describe the reasons for using this
method and its procedure. The hill-climbing al-
gorithm often falls into locally optimal solutions.
However, we believe that a locally optimal solution
is more effective in improving MT quality than the
previous methods.
</bodyText>
<subsectionHeader confidence="0.999615">
4.1 Costs of Combinatorial Optimization
</subsectionHeader>
<bodyText confidence="0.9999489375">
Most combinatorial optimization methods iterate
changes in the combination and the evaluation. In
the machine translation task, the evaluation process
requires the longest time. For example, in order to
calculate the BLEU score of a combination (solu-
tion), we have to translate C times, where C denotes
the size of the evaluation corpus. Furthermore, in
order to find the nearest neighbor solution, we have
to calculate all BLEU scores of the neighborhood.
If the number of rules is R and the neighborhood
is regarded as consisting of combinations made by
changing only one rule, we have to translate C × R
times to find the nearest neighbor solution. Assume
that C = 10, 000 and R = 100, 000, the number
of sentence translations (sentences to be translated)
becomes one billion. It is infeasible to search for
the optimal solution without reducing the number of
sentence translations.
A feature of this task is that removing rules is eas-
ier than adding rules. The rules used for translating
a sentence can be identified during the translation.
Conversely, the source sentence set 5[r], where a
rule r is used for the translation, is determined once
the evaluation corpus is translated. When r is re-
moved, only the MT results of 5[r] will change,
so we do not need to re-translate other sentences.
Assuming that five rules on average are applied to
translate a sentence, the number of sentence trans-
lations becomes 5 × C + C = 60, 000 for testing
all rules. On the contrary, to add a rule, the entire
corpus must be re-translated because it is unknown
which MT results will change by adding a rule.
</bodyText>
<subsectionHeader confidence="0.981695">
4.2 Cleaning Procedure
</subsectionHeader>
<bodyText confidence="0.99170275">
Based on the above discussion, we utilize the hill-
climbing algorithm, in which the initial solution
contains all rules (called the base rule set) and the
search for a combination is done by only removing
static: Ceval, an evaluation corpus
Rbase, a rule set acquired from the entire training corpus (the base rule set)
R, a current rule set, a subset of the base rule set
5[r], a source sentence set where the rule r is used for the translation
</bodyText>
<equation confidence="0.9166273">
Dociter, an MT result set of the evaluation corpus translated with the current rule set
procedure CLEAN-RULESET ()
R +— Rbase
repeat
Riter +— R
Rremove +— 0
scoreiter +— SET-TRANSLATION()
for each r in Riter do
if 5[r] =� 0 then
R +— Riter — {r}
</equation>
<bodyText confidence="0.487274">
translate all sentences in 5[r], and obtain the MT results T[r]
Doc[r] +— the MT result set that T [r] is replaced from Dociter
the rule contribution contrib[r] +— scoreiter — BLEU-SCORE(Doc[r])
if contrib[r] &lt; 0 then add r to Rremove
</bodyText>
<equation confidence="0.936371">
end
R +— Riter — Rremove
until Rremove = 0
function SET-TRANSLATION () returns a BLEU score of the evaluation corpus translated with R
Dociter +— 0
for each r in Rbase do 5[r] +— 0 end
</equation>
<bodyText confidence="0.8829372">
for each s in Ceval do
translate s and obtain the MT result t
obtain the rule set R[s] that is used for translating s
for each r in R[s] do add s to 5[r] end
add t to Dociter
</bodyText>
<equation confidence="0.311341">
end
return BLEU-SCORE(Dociter)
</equation>
<figureCaption confidence="0.997441">
Figure 3: Feedback Cleaning Algorithm
</figureCaption>
<bodyText confidence="0.9898305">
rules. The algorithm is shown in Figure 3. This al-
gorithm can be summarized as follows.
</bodyText>
<listItem confidence="0.997767363636364">
• Translate the evaluation corpus first and then
obtain the rules used for the translation and the
BLEU score before removing rules.
• For each rule one-by-one, calculate the BLEU
score after removing the rule and obtain the dif-
ference between this score and the score before
the rule was removed. This difference is called
the rule contribution.
• If the rule contribution is negative (i.e., the
BLUE score increases after removing the rule),
remove the rule.
</listItem>
<bodyText confidence="0.99874875">
In order to achieve faster convergence, this algo-
rithm removes all rules whose rule contribution is
negative in one iteration. This assumes that the re-
moved rules are independent from one another.
</bodyText>
<sectionHeader confidence="0.992429" genericHeader="method">
5 N-fold Cross-cleaning
</sectionHeader>
<bodyText confidence="0.987538">
In general, most evaluation corpora are smaller than
training corpora. Therefore, omissions of cleaning
</bodyText>
<figureCaption confidence="0.9981845">
Figure 4: Structure of Cross-cleaning
(In the case of three-fold cross-cleaning)
</figureCaption>
<bodyText confidence="0.8713302">
will remain because not all rules can be tested by the
evaluation corpus. In order to avoid this problem, we
propose an advanced method called cross-cleaning
(Figure 4), which is similar to cross-validation.
The procedure of cross-cleaning is as follows.
</bodyText>
<listItem confidence="0.938724647058824">
1. First, create the base rule set from the entire
training corpus.
2. Next, divide the training corpus into N pieces
uniformly.
3. Leave one piece for the evaluation, acquire
rules from the rest (N − 1) of the pieces, and
repeat them N times. Thus, we obtain N pairs
of rule set and evaluation sub-corpus. Each rule
set is a subset of the base rule set.
4. Apply the feedback cleaning algorithm to each
of the N pairs and record the rule contributions
even if the rules are removed. The purpose of
this step is to obtain the rule contributions.
5. For each rule in the base rule set, sum up the
rule contributions obtained from the rule sub-
sets. If the sum is negative, remove the rule
from the base rule set.
</listItem>
<bodyText confidence="0.9965535">
The major difference of this method from cross-
validation is Step 5. In the case of cross-cleaning,
</bodyText>
<note confidence="0.990120571428571">
Set Name Feature English Japanese
Training # of Sentences 149,882
Corpus # of Words 868,087 984,197
Evaluation # of Sentences 10,145
Corpus # of Words 59,533 67,554
Test # of Sentences 10,150
Corpus # of Words 59,232 67,193
</note>
<tableCaption confidence="0.997322">
Table 1: Corpus Size
</tableCaption>
<bodyText confidence="0.99981525">
the rule subsets cannot be directly merged because
some rules have already been removed in Step 4.
Therefore, we only obtain the rule contributions
from the rule subsets and sum them up. The summed
contribution is an approximate value of the rule
contribution to the entire training corpus. Cross-
cleaning removes the rules from the base rule set
based on this approximate contribution.
Cross-cleaning uses all sentences in the training
corpus, so it is nearly equivalent to applying a large
evaluation corpus to feedback cleaning, even though
it does not require specific evaluation corpora.
</bodyText>
<sectionHeader confidence="0.999232" genericHeader="method">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.9994385">
In this section, the effects of feedback cleaning are
evaluated by using English-to-Japanese translation.
</bodyText>
<subsectionHeader confidence="0.990264">
6.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.9997765">
Bilingual Corpora The corpus used in the fol-
lowing experiments is the Basic Travel Expression
Corpus (Takezawa et al., 2002). This is a collec-
tion of Japanese sentences and their English trans-
lations based on expressions that are usually found
in phrasebooks for foreign tourists. We divided it
into sub-corpora for training, evaluation, and test as
shown in Table 1. The number of rules acquired
from the training corpus (the base rule set size) was
105,588.
Evaluation Methods of MT Quality We used the
following two methods to evaluate MT quality.
</bodyText>
<sectionHeader confidence="0.658724" genericHeader="method">
1. Test Corpus BLEU Score
</sectionHeader>
<bodyText confidence="0.996473">
The BLUE score was calculated with the test
corpus. The number of references was one for
each sentence, in the same way used for the
feedback cleaning.
</bodyText>
<figure confidence="0.9997762">
Rule Contributions
Divide
Training
Evaluation
Training
Evaluation
Training
Evaluation
Training
Rule
Subset 1
Rule
Subset 2
Rule
Subset 3
Feedback
Cleaning
Feedback
Cleaning
Feedback
Cleaning
Rule
Deletion
Base
Rule Set
Cleaned
Rule Set
Training
Corpus
Number of Iterations
</figure>
<figureCaption confidence="0.811506">
Figure 5: Relationship between Number of Itera-
tions and BLEU Scores/Number of Rules
2. Subjective Quality
</figureCaption>
<bodyText confidence="0.999779909090909">
A total of 510 sentences from the test corpus
were evaluated by paired comparison. Specif-
ically, the source sentences were translated us-
ing the base rule set, and the same sources were
translated using the rules after the cleaning.
One-by-one, a Japanese native speaker judged
which MT result was better or that they were
of the same quality. Subjective quality is repre-
sented by the following equation, where I de-
notes the number of improved sentences and D
denotes the number of degraded sentences.
</bodyText>
<equation confidence="0.467787333333333">
I − D
Subj. Quality = (1)
# of test sentences
</equation>
<subsectionHeader confidence="0.996226">
6.2 Feedback Cleaning Using Evaluation
Corpus
</subsectionHeader>
<bodyText confidence="0.999989827586207">
In order to observe the characteristics of feedback
cleaning, cleaning of the base rule set was carried
out by using the evaluation corpus. The results are
shown in Figure 5. This graph shows changes in
the test corpus BLEU score, the evaluation corpus
BLEU score, and the number of rules along with the
number of iterations.
Consequently, the removed rules converged at
nine iterations, and 6,220 rules were removed. The
evaluation corpus BLEU score was improved by in-
creasing the number of iterations, demonstrating that
the combinatorial optimization by the hill-climbing
algorithm worked effectively. The test corpus BLEU
score reached a peak score of 0.245 at the second
iteration and slightly decreased after the third itera-
tion due to overfitting. However, the final score was
0.244, which is almost the same as the peak score.
The test corpus BLEU score was lower than
the evaluation corpus BLEU score because the
rules used in the test corpus were not exhaustively
checked by the evaluation corpus. If the evaluation
corpus size could be expanded, the test corpus score
would improve.
About 37,000 sentences were translated on aver-
age in each iteration. This means that the time for
an iteration is estimated at about ten hours if trans-
lation speed is one second per sentence. This is a
short enough time for us because our method does
not require real-time processing. 2
</bodyText>
<subsectionHeader confidence="0.996251">
6.3 MT Quality vs. Cleaning Methods
</subsectionHeader>
<bodyText confidence="0.999914666666667">
Next, in order to compare the proposed methods
with the previous methods, the MT quality achieved
by each of the following five methods was measured.
</bodyText>
<sectionHeader confidence="0.877472" genericHeader="method">
1. Baseline
</sectionHeader>
<bodyText confidence="0.91744">
The MT results using the base rule set.
</bodyText>
<sectionHeader confidence="0.477435" genericHeader="method">
2. Cutoff by Frequency
</sectionHeader>
<bodyText confidence="0.9995486">
Low-frequency rules that appeared in the train-
ing corpus less often than twice were removed
from the base rule set. This threshold was
experimentally determined by the test corpus
BLEU score.
</bodyText>
<sectionHeader confidence="0.79232" genericHeader="method">
3. k2 Test
</sectionHeader>
<bodyText confidence="0.9981855">
The k2 test was performed in the same manner
as in Imamura (2002)’s experiment. We intro-
duced rules with more than 95 percent confi-
dence (k2 ≥ 3.841).
</bodyText>
<sectionHeader confidence="0.648284" genericHeader="method">
4. Simple Feedback Cleaning
</sectionHeader>
<bodyText confidence="0.789892">
Feedback cleaning was carried out using the
evaluation corpus in Table 1.
</bodyText>
<sectionHeader confidence="0.608649" genericHeader="method">
5. Cross-cleaning
</sectionHeader>
<bodyText confidence="0.8749194">
N-fold cross-cleaning was carried out. We ap-
plied five-fold cross-cleaning in this experi-
ment.
The results are shown in Table 2. This table shows
that the test corpus BLEU score and the subjective
</bodyText>
<footnote confidence="0.927049">
2In this experiment, it took about 80 hours until convergence
using a Pentium 4 2-GHz computer.
</footnote>
<figure confidence="0.968387882352941">
0 1 2 3 4 5 6 7 8 9
0.32
0.28
0.26
0.24
0.22
0.3
Evaluation Corpus BLEU Score
Test Corpus BLEU Score
Number of Rules
90k
80k
120k
110k
100k
Number of Rules
BLEU Score
</figure>
<table confidence="0.995898222222222">
Baseline Previous Methods Proposed Methods
Cutoff by Freq. χ2 Test Simple FC Cross-cleaning
# of Rules 105,588 26,053 1,499 99,368 82,462
Test Corpus BLEU Score 0.232 0.234 0.157 0.244 0.277
Subjective Quality +1.77% -6.67% +6.67% +10.0%
# of Improved Sentences 83 115 83 100
# of Same Quality 353 246 378 361
(Same Results) (257) (114) (266) (234)
# of Degraded Sentences 74 149 49 49
</table>
<tableCaption confidence="0.998434">
Table 2: MT Quality vs. Cleaning Methods
</tableCaption>
<bodyText confidence="0.99984915">
quality of the proposed methods (simple feedback
cleaning and cross-cleaning) are considerably im-
proved over those of the previous methods.
Focusing on the subjective quality of the proposed
methods, some MT results were degraded from the
baseline due to the removal of rules. However, the
subjective quality levels were relatively improved
because our methods aim to increase the portion of
correct MT results.
Focusing on the number of the rules, the rule
set of the simple feedback cleaning is clearly a lo-
cally optimal solution, since the number of rules
is more than that of cross-cleaning, although the
BLEU score is lower. In comparing the number of
rules in cross-cleaning with that in the cutoff by fre-
quency, the former is three times higher than the lat-
ter. We assume that the solution of cross-cleaning
is also the locally optimal solution. If we could find
the globally optimal solution, the MT quality would
certainly improve further.
</bodyText>
<sectionHeader confidence="0.999949" genericHeader="method">
7 Discussion
</sectionHeader>
<subsectionHeader confidence="0.995303">
7.1 Other Automatic Evaluation Methods
</subsectionHeader>
<bodyText confidence="0.999931217391305">
The idea of feedback cleaning is independent of
BLEU. Some automatic evaluation methods of MT
quality other than BLEU have been proposed. For
example, Su et al. (1992), Yasuda et al. (2001), and
Akiba et al. (2001) measure similarity between MT
results and the references by DP matching (edit dis-
tances) and then output the evaluation scores. These
automatic evaluation methods that output scores are
applicable to feedback cleaning.
The characteristics common to these methods, in-
cluding BLEU, is that the similarity to references
are measured for each sentence, and the evaluation
score of an MT system is calculated by aggregating
the similarities. Therefore, MT results of the eval-
uation corpus are necessary to evaluate the system,
and reducing the number of sentence translations is
an important technique for all of these methods.
The effects of feedback cleaning depend on the
characteristics of objective measures. DP-based
measures and BLEU have different characteristics
(Yasuda et al., 2003). The exploration of several
measures for feedback cleaning remains an interest-
ing future work.
</bodyText>
<subsectionHeader confidence="0.984164">
7.2 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999991066666667">
When applying corpus-based machine translation to
a different domain, bilingual corpora of the new do-
main are necessary. However, the sizes of the new
corpora are generally smaller than that of the orig-
inal corpus because the collection of bilingual sen-
tences requires a high cost.
The feedback cleaning proposed in this paper can
be interpreted as adapting the translation rules so
that the MT results become similar to the evaluation
corpus. Therefore, if we regard the bilingual corpus
of the new domain as the evaluation corpus and carry
out feedback cleaning, the rule set will be adapted to
the new domain. In other words, our method can be
applied to adaptation of an MT system by using a
smaller corpus of the new domain.
</bodyText>
<sectionHeader confidence="0.99964" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999938666666667">
In this paper, we proposed a feedback cleaning
method that utilizes automatic evaluation to remove
incorrect/redundant translation rules. BLEU was
utilized for the automatic evaluation of MT qual-
ity, and the hill-climbing algorithm was applied to
searching for the combinatorial optimization. Uti-
lizing features of this task, incorrect/redundant rules
were removed from the initial solution, which con-
tains all rules acquired from the training corpus. In
addition, we proposed N-fold cross-cleaning to re-
duce the influence of the evaluation corpus size. Our
experiments show that the MT quality was improved
by 10% in paired comparison and by 0.045 in the
BLEU score. This is considerable improvement over
the previous methods.
</bodyText>
<sectionHeader confidence="0.986531" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9999394">
The research reported here is supported in part by
a contract with the Telecommunications Advance-
ment Organization of Japan entitled, “A study of
speech dialogue translation technology based on a
large corpus.”
</bodyText>
<sectionHeader confidence="0.999453" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99812">
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001. Using multiple edit distances to automatically
rank machine translation output. In Proceedings of
Machine Translation Summit VIII, pages 15–20.
Osamu Furuse and Hitoshi Iida. 1994. Constituent
boundary parsing for example-based machine transla-
tion. In Proceedings of COLING-94, pages 105–111.
Kenji Imamura. 2001. Hierarchical phrase alignment
harmonized with parsing. In Proceedings of the 6th
Natural Language Processing Pacific Rim Symposium
(NLPRS 2001), pages 377–384.
Kenji Imamura. 2002. Application of translation knowl-
edge acquired by hierarchical phrase alignment for
pattern-based MT. In Proceedings of the 9th Confer-
ence on Theoretical and Methodological Issues in Ma-
chine Translation (TMI-2002), pages 74–84.
Arul Menezes and Stephen D. Richardson. 2001. A
best first alignment algorithm for automatic extrac-
tion of transfer mappings from bilingual corpora. In
Proceedings of the ‘Workshop on Example-Based Ma-
chine Translation’ in MT Summit VIII, pages 35–42.
Adam Meyers, Michiko Kosaka, and Ralph Grishman.
2000. Chart-based translation rule application in ma-
chine translation. In Proceedings of COLING-2000,
pages 537–543.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 311–318.
Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang. 1992.
A new quantitative quality measure for machine trans-
lation systems. In Proceedings of COLING-92, pages
433–439.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech
translation of travel conversations in the real world.
In Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC 2002),
pages 147–152.
Keiji Yasuda, Fumiaki Sugaya, Toshiyuki Takezawa, Sei-
ichi Yamamoto, and Masuzo Yanagida. 2001. An au-
tomatic evaluation method of translation quality using
translation answer candidates queried from a parallel
corpus. In Proceedings of Machine Translation Sum-
mit VIII, pages 373–378.
Keiji Yasuda, Fumiaki Sugaya, Toshiyuki Takezawa, Sei-
ichi Yamamoto, and Masuzo Yanagida. 2003. Appli-
cations of automatic evaluation methods to measuring
a capability of speech translation system. In Proceed-
ings of the 10th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL 2003), pages 371–378.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.153720">
<title confidence="0.7359116">Feedback Cleaning of Machine Translation Rules Using Automatic Evaluation Kenji Imamura, Eiichiro Sumita ATR Spoken Language Translation Research Laboratories</title>
<author confidence="0.525468">Soraku-gun Seika-cho</author>
<author confidence="0.525468">Japan Yuji Matsumoto Kyoto</author>
<affiliation confidence="0.989282">Nara Institute of Science and Technology</affiliation>
<address confidence="0.89882">Ikoma-shi, Nara, Japan</address>
<email confidence="0.977894">matsu@is.aist-nara.ac.jp</email>
<abstract confidence="0.991228571428571">When rules of transfer-based machine translation (MT) are automatically acquired from bilingual corpora, incorrect/redundant rules are generated due to acquisition errors or translation variety in the corpora. As a new countermeasure to this problem, we propose a feedback cleaning method using automatic evaluation of MT quality, which removes incorrect/redundant rules as a way to increase the evaluation score. BLEU is utilized for the automatic evaluation. The hillclimbing algorithm, which involves features of this task, is applied to searching for the optimal combination of rules. Our experiments show that the MT quality improves by 10% in test sentences according to a subjective evaluation. This is considerable improvement over previous methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yasuhiro Akiba</author>
<author>Kenji Imamura</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Using multiple edit distances to automatically rank machine translation output.</title>
<date>2001</date>
<booktitle>In Proceedings of Machine Translation Summit VIII,</booktitle>
<pages>15--20</pages>
<contexts>
<context position="2615" citStr="Akiba et al., 2001" startWordPosition="380" endWordPosition="383">02)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Automatic Acquisition Training Corpus Rule Selection/Deletion Translation Rules Feedback Cleaning Evaluation Corpus Automatic Evaluation MT Engine MT Results Figure 1: Structure of Feedback Cleaning (Figure 1). Our method evaluate</context>
<context position="20685" citStr="Akiba et al. (2001)" startWordPosition="3397" endWordPosition="3400">although the BLEU score is lower. In comparing the number of rules in cross-cleaning with that in the cutoff by frequency, the former is three times higher than the latter. We assume that the solution of cross-cleaning is also the locally optimal solution. If we could find the globally optimal solution, the MT quality would certainly improve further. 7 Discussion 7.1 Other Automatic Evaluation Methods The idea of feedback cleaning is independent of BLEU. Some automatic evaluation methods of MT quality other than BLEU have been proposed. For example, Su et al. (1992), Yasuda et al. (2001), and Akiba et al. (2001) measure similarity between MT results and the references by DP matching (edit distances) and then output the evaluation scores. These automatic evaluation methods that output scores are applicable to feedback cleaning. The characteristics common to these methods, including BLEU, is that the similarity to references are measured for each sentence, and the evaluation score of an MT system is calculated by aggregating the similarities. Therefore, MT results of the evaluation corpus are necessary to evaluate the system, and reducing the number of sentence translations is an important technique fo</context>
</contexts>
<marker>Akiba, Imamura, Sumita, 2001</marker>
<rawString>Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita. 2001. Using multiple edit distances to automatically rank machine translation output. In Proceedings of Machine Translation Summit VIII, pages 15–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Osamu Furuse</author>
<author>Hitoshi Iida</author>
</authors>
<title>Constituent boundary parsing for example-based machine translation.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING-94,</booktitle>
<pages>105--111</pages>
<contexts>
<context position="5016" citStr="Furuse and Iida, 1994" startWordPosition="753" endWordPosition="756">en the system translates an input sentence, the sentence is first parsed by using source patterns of the transfer rules. Next, a tree structure of the target language is generated by mapping the source patterns to the corresponding target patterns. When non-terminal symbols remain in the target tree, target words are inserted by referring to a translation dictionary. Ambiguities, which occur during parsing or mapping, are resolved by selecting the rules that minimize the semantic distance between the input words and source examples (real examples in the training corpus) of the transfer rules (Furuse and Iida, 1994). For instance, when the input phrase “leave at 11 a.m.” is translated into Japanese, Rule 2 in Figure 2 is selected because the semantic distance from the source example (arrive, p.m.) is the shortest to the head words of the input phrase (leave, a.m.). 2.2 Problems of Automatic Acquisition HPAT automatically acquires its transfer rules from parallel corpora by using Hierarchical Phrase Alignment (Imamura, 2001). However, the rule set contains many incorrect/redundant rules. The reasons for this problem are roughly classified as follows. • Errors in automatic rule acquisition • Translation va</context>
</contexts>
<marker>Furuse, Iida, 1994</marker>
<rawString>Osamu Furuse and Hitoshi Iida. 1994. Constituent boundary parsing for example-based machine translation. In Proceedings of COLING-94, pages 105–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Imamura</author>
</authors>
<title>Hierarchical phrase alignment harmonized with parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium (NLPRS</booktitle>
<pages>377--384</pages>
<contexts>
<context position="5432" citStr="Imamura, 2001" startWordPosition="821" endWordPosition="822">e resolved by selecting the rules that minimize the semantic distance between the input words and source examples (real examples in the training corpus) of the transfer rules (Furuse and Iida, 1994). For instance, when the input phrase “leave at 11 a.m.” is translated into Japanese, Rule 2 in Figure 2 is selected because the semantic distance from the source example (arrive, p.m.) is the shortest to the head words of the input phrase (leave, a.m.). 2.2 Problems of Automatic Acquisition HPAT automatically acquires its transfer rules from parallel corpora by using Hierarchical Phrase Alignment (Imamura, 2001). However, the rule set contains many incorrect/redundant rules. The reasons for this problem are roughly classified as follows. • Errors in automatic rule acquisition • Translation variety in corpora – The acquisition process cannot generalize the rules because bilingual sentences depend on the context or the situation. – Corpora contain multiple (paraphrasable) translations of the same source expression. In the experiment of Imamura (2002), about 92,000 transfer rules were acquired from about 120,000 bilingual sentences 1. Most of these rules are low-frequency. They reported that MT quality </context>
</contexts>
<marker>Imamura, 2001</marker>
<rawString>Kenji Imamura. 2001. Hierarchical phrase alignment harmonized with parsing. In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium (NLPRS 2001), pages 377–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Imamura</author>
</authors>
<title>Application of translation knowledge acquired by hierarchical phrase alignment for pattern-based MT.</title>
<date>2002</date>
<booktitle>In Proceedings of the 9th Conference on Theoretical and Methodological Issues in Machine Translation (TMI-2002),</booktitle>
<pages>74--84</pages>
<contexts>
<context position="1999" citStr="Imamura, 2002" startWordPosition="281" endWordPosition="282">corpora contains many incorrect/redundant rules due to acquisition errors or translation variety in the corpora. Such rules conflict with other existing rules and cause implausible MT results or increase ambiguity. If incorrect rules could be avoided, MT quality would necessarily improve. There are two approaches to overcoming incorrect/redundant rules: • Selecting appropriate rules in a disambiguation process during the translation (on-line processing, (Meyers et al., 2000)). • Cleaning incorrect/redundant rules after automatic acquisition (off-line processing, (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Aki</context>
<context position="4081" citStr="Imamura, 2002" startWordPosition="606" endWordPosition="607">y evaluates MT results and does not consider various conditions of the MT engine, such as parameters, interference in dictionaries, disambiguation methods, and so on. Even if an MT engine avoids incorrect/redundant rules by on-line processing, errors inevitably remain. Our method cleans the rules in advance by only focusing on the remaining errors. Thus, our method complements on-line processing and adapts translation rules to the given conditions of the MT engine. 2 MT System and Problems of Automatic Acquisition 2.1 MT Engine We use the Hierarchical Phrase Alignment-based Translator (HPAT) (Imamura, 2002) as a transferbased MT system. The most important knowledge in HPAT is transfer rules, which define the correspondences between source and target language expressions. An example of English-to-Japanese transfer rules is shown in Figure 2. The transfer rules are regarded as a synchronized context-free grammar. When the system translates an input sentence, the sentence is first parsed by using source patterns of the transfer rules. Next, a tree structure of the target language is generated by mapping the source patterns to the corresponding target patterns. When non-terminal symbols remain in th</context>
<context position="5877" citStr="Imamura (2002)" startWordPosition="888" endWordPosition="889"> a.m.). 2.2 Problems of Automatic Acquisition HPAT automatically acquires its transfer rules from parallel corpora by using Hierarchical Phrase Alignment (Imamura, 2001). However, the rule set contains many incorrect/redundant rules. The reasons for this problem are roughly classified as follows. • Errors in automatic rule acquisition • Translation variety in corpora – The acquisition process cannot generalize the rules because bilingual sentences depend on the context or the situation. – Corpora contain multiple (paraphrasable) translations of the same source expression. In the experiment of Imamura (2002), about 92,000 transfer rules were acquired from about 120,000 bilingual sentences 1. Most of these rules are low-frequency. They reported that MT quality slightly improved, even though the low-frequency rules were removed to a level of about 1/9 the previous number. However, since some of them, such as idiomatic rules, are necessary for translation, MT quality cannot be dramatically improved by only removing low-frequency rules. 3 Automatic Evaluation of MT Quality We utilize BLEU (Papineni et al., 2002) for the automatic evaluation of MT quality in this paper. BLEU measures the similarity be</context>
<context position="18377" citStr="Imamura (2002)" startWordPosition="3007" endWordPosition="3008">short enough time for us because our method does not require real-time processing. 2 6.3 MT Quality vs. Cleaning Methods Next, in order to compare the proposed methods with the previous methods, the MT quality achieved by each of the following five methods was measured. 1. Baseline The MT results using the base rule set. 2. Cutoff by Frequency Low-frequency rules that appeared in the training corpus less often than twice were removed from the base rule set. This threshold was experimentally determined by the test corpus BLEU score. 3. k2 Test The k2 test was performed in the same manner as in Imamura (2002)’s experiment. We introduced rules with more than 95 percent confidence (k2 ≥ 3.841). 4. Simple Feedback Cleaning Feedback cleaning was carried out using the evaluation corpus in Table 1. 5. Cross-cleaning N-fold cross-cleaning was carried out. We applied five-fold cross-cleaning in this experiment. The results are shown in Table 2. This table shows that the test corpus BLEU score and the subjective 2In this experiment, it took about 80 hours until convergence using a Pentium 4 2-GHz computer. 0 1 2 3 4 5 6 7 8 9 0.32 0.28 0.26 0.24 0.22 0.3 Evaluation Corpus BLEU Score Test Corpus BLEU Score </context>
</contexts>
<marker>Imamura, 2002</marker>
<rawString>Kenji Imamura. 2002. Application of translation knowledge acquired by hierarchical phrase alignment for pattern-based MT. In Proceedings of the 9th Conference on Theoretical and Methodological Issues in Machine Translation (TMI-2002), pages 74–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arul Menezes</author>
<author>Stephen D Richardson</author>
</authors>
<title>A best first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the ‘Workshop on Example-Based Machine Translation’ in MT Summit VIII,</booktitle>
<pages>35--42</pages>
<contexts>
<context position="1983" citStr="Menezes and Richardson, 2001" startWordPosition="277" endWordPosition="280">ransferbased MT acquired from corpora contains many incorrect/redundant rules due to acquisition errors or translation variety in the corpora. Such rules conflict with other existing rules and cause implausible MT results or increase ambiguity. If incorrect rules could be avoided, MT quality would necessarily improve. There are two approaches to overcoming incorrect/redundant rules: • Selecting appropriate rules in a disambiguation process during the translation (on-line processing, (Meyers et al., 2000)). • Cleaning incorrect/redundant rules after automatic acquisition (off-line processing, (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda e</context>
</contexts>
<marker>Menezes, Richardson, 2001</marker>
<rawString>Arul Menezes and Stephen D. Richardson. 2001. A best first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora. In Proceedings of the ‘Workshop on Example-Based Machine Translation’ in MT Summit VIII, pages 35–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Michiko Kosaka</author>
<author>Ralph Grishman</author>
</authors>
<title>Chart-based translation rule application in machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING-2000,</booktitle>
<pages>537--543</pages>
<contexts>
<context position="1864" citStr="Meyers et al., 2000" startWordPosition="264" endWordPosition="267">T) systems that automatically acquire their knowledge from corpora have been proposed. However, knowledge for transferbased MT acquired from corpora contains many incorrect/redundant rules due to acquisition errors or translation variety in the corpora. Such rules conflict with other existing rules and cause implausible MT results or increase ambiguity. If incorrect rules could be avoided, MT quality would necessarily improve. There are two approaches to overcoming incorrect/redundant rules: • Selecting appropriate rules in a disambiguation process during the translation (on-line processing, (Meyers et al., 2000)). • Cleaning incorrect/redundant rules after automatic acquisition (off-line processing, (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confide</context>
</contexts>
<marker>Meyers, Kosaka, Grishman, 2000</marker>
<rawString>Adam Meyers, Michiko Kosaka, and Ralph Grishman. 2000. Chart-based translation rule application in machine translation. In Proceedings of COLING-2000, pages 537–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="2573" citStr="Papineni et al., 2002" startWordPosition="372" endWordPosition="375">, (Menezes and Richardson, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Automatic Acquisition Training Corpus Rule Selection/Deletion Translation Rules Feedback Cleaning Evaluation Corpus Automatic Evaluation MT Engine MT Results Figure 1: Structure of Feedbac</context>
<context position="6387" citStr="Papineni et al., 2002" startWordPosition="966" endWordPosition="969">contain multiple (paraphrasable) translations of the same source expression. In the experiment of Imamura (2002), about 92,000 transfer rules were acquired from about 120,000 bilingual sentences 1. Most of these rules are low-frequency. They reported that MT quality slightly improved, even though the low-frequency rules were removed to a level of about 1/9 the previous number. However, since some of them, such as idiomatic rules, are necessary for translation, MT quality cannot be dramatically improved by only removing low-frequency rules. 3 Automatic Evaluation of MT Quality We utilize BLEU (Papineni et al., 2002) for the automatic evaluation of MT quality in this paper. BLEU measures the similarity between MT results and translation results made by humans (called 1In this paper, the number of rules denotes the number of unique pairs of source patterns and target patterns. Rule No. Syn. Cat. Source Pattern Target Pattern Source Example 1 VP XIP at YNP ⇒ Y’ de X’ ((present, conference) ...) 2 VP XIP at YNP ⇒ Y’ ni X’ ((stay, hotel), (arrive, p.m) ...) 3 VP XIP at YNP ⇒ Y’ wo X’ ((look, it) ...) 4 NP XNP at YNP ⇒ Y’ no X’ ((man, front desk) ...) Figure 2: Example of HPAT Transfer Rules references). This </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Yih Su</author>
<author>Ming-Wen Wu</author>
<author>Jing-Shin Chang</author>
</authors>
<title>A new quantitative quality measure for machine translation systems.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92,</booktitle>
<pages>433--439</pages>
<contexts>
<context position="2846" citStr="Su et al., 1992" startWordPosition="423" endWordPosition="426">ality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Automatic Acquisition Training Corpus Rule Selection/Deletion Translation Rules Feedback Cleaning Evaluation Corpus Automatic Evaluation MT Engine MT Results Figure 1: Structure of Feedback Cleaning (Figure 1). Our method evaluates the contribution of each rule to the MT results and removes inappropriate rules as a way to increase the evaluation scores. Since the automatic evaluation correlates with a subjective evaluation, MT quality will improve after cle</context>
<context position="20638" citStr="Su et al. (1992)" startWordPosition="3388" endWordPosition="3391"> rules is more than that of cross-cleaning, although the BLEU score is lower. In comparing the number of rules in cross-cleaning with that in the cutoff by frequency, the former is three times higher than the latter. We assume that the solution of cross-cleaning is also the locally optimal solution. If we could find the globally optimal solution, the MT quality would certainly improve further. 7 Discussion 7.1 Other Automatic Evaluation Methods The idea of feedback cleaning is independent of BLEU. Some automatic evaluation methods of MT quality other than BLEU have been proposed. For example, Su et al. (1992), Yasuda et al. (2001), and Akiba et al. (2001) measure similarity between MT results and the references by DP matching (edit distances) and then output the evaluation scores. These automatic evaluation methods that output scores are applicable to feedback cleaning. The characteristics common to these methods, including BLEU, is that the similarity to references are measured for each sentence, and the evaluation score of an MT system is calculated by aggregating the similarities. Therefore, MT results of the evaluation corpus are necessary to evaluate the system, and reducing the number of sen</context>
</contexts>
<marker>Su, Wu, Chang, 1992</marker>
<rawString>Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang. 1992. A new quantitative quality measure for machine translation systems. In Proceedings of COLING-92, pages 433–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiyuki Takezawa</author>
<author>Eiichiro Sumita</author>
<author>Fumiaki Sugaya</author>
<author>Hirofumi Yamamoto</author>
<author>Seiichi Yamamoto</author>
</authors>
<title>Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>147--152</pages>
<contexts>
<context position="14890" citStr="Takezawa et al., 2002" startWordPosition="2426" endWordPosition="2429">he rule contribution to the entire training corpus. Crosscleaning removes the rules from the base rule set based on this approximate contribution. Cross-cleaning uses all sentences in the training corpus, so it is nearly equivalent to applying a large evaluation corpus to feedback cleaning, even though it does not require specific evaluation corpora. 6 Evaluation In this section, the effects of feedback cleaning are evaluated by using English-to-Japanese translation. 6.1 Experimental Settings Bilingual Corpora The corpus used in the following experiments is the Basic Travel Expression Corpus (Takezawa et al., 2002). This is a collection of Japanese sentences and their English translations based on expressions that are usually found in phrasebooks for foreign tourists. We divided it into sub-corpora for training, evaluation, and test as shown in Table 1. The number of rules acquired from the training corpus (the base rule set size) was 105,588. Evaluation Methods of MT Quality We used the following two methods to evaluate MT quality. 1. Test Corpus BLEU Score The BLUE score was calculated with the test corpus. The number of references was one for each sentence, in the same way used for the feedback clean</context>
</contexts>
<marker>Takezawa, Sumita, Sugaya, Yamamoto, Yamamoto, 2002</marker>
<rawString>Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya, Hirofumi Yamamoto, and Seiichi Yamamoto. 2002. Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002), pages 147–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Yasuda</author>
<author>Fumiaki Sugaya</author>
<author>Toshiyuki Takezawa</author>
<author>Seiichi Yamamoto</author>
<author>Masuzo Yanagida</author>
</authors>
<title>An automatic evaluation method of translation quality using translation answer candidates queried from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of Machine Translation Summit VIII,</booktitle>
<pages>373--378</pages>
<contexts>
<context position="2594" citStr="Yasuda et al., 2001" startWordPosition="376" endWordPosition="379">on, 2001; Imamura, 2002)). We employ the second approach in this paper. The cutoff by frequency (Menezes and Richardson, 2001) and the hypothesis test (Imamura, 2002) have been applied to clean the rules. The cutoff by frequency can slightly improve MT quality, but the improvement is still insufficient from the viewpoint of the large number of redundant rules. The hypothesis test requires very large corpora in order to obtain a sufficient number of rules that are statistically confident. Another current topic of machine translation is automatic evaluation of MT quality (Papineni et al., 2002; Yasuda et al., 2001; Akiba et al., 2001). These methods aim to replace subjective evaluation in order to speed up the development cycle of MT systems. However, they can be utilized not only as developers’ aids but also for automatic tuning of MT systems (Su et al., 1992). We propose feedback cleaning that utilizes an automatic evaluation for removing incorrect/redundant translation rules as a tuning method Automatic Acquisition Training Corpus Rule Selection/Deletion Translation Rules Feedback Cleaning Evaluation Corpus Automatic Evaluation MT Engine MT Results Figure 1: Structure of Feedback Cleaning (Figure 1)</context>
<context position="20660" citStr="Yasuda et al. (2001)" startWordPosition="3392" endWordPosition="3395">n that of cross-cleaning, although the BLEU score is lower. In comparing the number of rules in cross-cleaning with that in the cutoff by frequency, the former is three times higher than the latter. We assume that the solution of cross-cleaning is also the locally optimal solution. If we could find the globally optimal solution, the MT quality would certainly improve further. 7 Discussion 7.1 Other Automatic Evaluation Methods The idea of feedback cleaning is independent of BLEU. Some automatic evaluation methods of MT quality other than BLEU have been proposed. For example, Su et al. (1992), Yasuda et al. (2001), and Akiba et al. (2001) measure similarity between MT results and the references by DP matching (edit distances) and then output the evaluation scores. These automatic evaluation methods that output scores are applicable to feedback cleaning. The characteristics common to these methods, including BLEU, is that the similarity to references are measured for each sentence, and the evaluation score of an MT system is calculated by aggregating the similarities. Therefore, MT results of the evaluation corpus are necessary to evaluate the system, and reducing the number of sentence translations is </context>
</contexts>
<marker>Yasuda, Sugaya, Takezawa, Yamamoto, Yanagida, 2001</marker>
<rawString>Keiji Yasuda, Fumiaki Sugaya, Toshiyuki Takezawa, Seiichi Yamamoto, and Masuzo Yanagida. 2001. An automatic evaluation method of translation quality using translation answer candidates queried from a parallel corpus. In Proceedings of Machine Translation Summit VIII, pages 373–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Yasuda</author>
<author>Fumiaki Sugaya</author>
<author>Toshiyuki Takezawa</author>
<author>Seiichi Yamamoto</author>
<author>Masuzo Yanagida</author>
</authors>
<title>Applications of automatic evaluation methods to measuring a capability of speech translation system.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<pages>371--378</pages>
<contexts>
<context position="21474" citStr="Yasuda et al., 2003" startWordPosition="3516" endWordPosition="3519">output scores are applicable to feedback cleaning. The characteristics common to these methods, including BLEU, is that the similarity to references are measured for each sentence, and the evaluation score of an MT system is calculated by aggregating the similarities. Therefore, MT results of the evaluation corpus are necessary to evaluate the system, and reducing the number of sentence translations is an important technique for all of these methods. The effects of feedback cleaning depend on the characteristics of objective measures. DP-based measures and BLEU have different characteristics (Yasuda et al., 2003). The exploration of several measures for feedback cleaning remains an interesting future work. 7.2 Domain Adaptation When applying corpus-based machine translation to a different domain, bilingual corpora of the new domain are necessary. However, the sizes of the new corpora are generally smaller than that of the original corpus because the collection of bilingual sentences requires a high cost. The feedback cleaning proposed in this paper can be interpreted as adapting the translation rules so that the MT results become similar to the evaluation corpus. Therefore, if we regard the bilingual </context>
</contexts>
<marker>Yasuda, Sugaya, Takezawa, Yamamoto, Yanagida, 2003</marker>
<rawString>Keiji Yasuda, Fumiaki Sugaya, Toshiyuki Takezawa, Seiichi Yamamoto, and Masuzo Yanagida. 2003. Applications of automatic evaluation methods to measuring a capability of speech translation system. In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2003), pages 371–378.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>