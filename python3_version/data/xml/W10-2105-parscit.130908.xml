<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007949">
<title confidence="0.671192">
Grammar-driven versus Data-driven: Which Parsing System is More
Affected by Domain Shifts?
</title>
<author confidence="0.987924">
Barbara Plank
</author>
<affiliation confidence="0.8735725">
University of Groningen
The Netherlands
</affiliation>
<email confidence="0.989758">
b.plank@rug.nl
</email>
<sectionHeader confidence="0.997293" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967615384615">
In the past decade several parsing systems
for natural language have emerged, which
use different methods and formalisms. For
instance, systems that employ a hand-
crafted grammar and a statistical disam-
biguation component versus purely sta-
tistical data-driven systems. What they
have in common is the lack of portabil-
ity to new domains: their performance
might decrease substantially as the dis-
tance between test and training domain in-
creases. Yet, to which degree do they suf-
fer from this problem, i.e. which kind of
parsing system is more affected by domain
shifts? Intuitively, grammar-driven sys-
tems should be less affected by domain
changes. To investigate this hypothesis,
an empirical investigation on Dutch is car-
ried out. The performance variation of
a grammar-driven versus two data-driven
systems across domains is evaluated, and a
simple measure to quantify domain sensi-
tivity proposed. This will give an estimate
of which parsing system is more affected
by domain shifts, and thus more in need
for adaptation techniques.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995472181818182">
Most modern Natural Language Processing (NLP)
systems are subject to the wellknown problem of
lack of portability to new domains: there is a sub-
stantial drop in their performance when the sys-
tem gets input from another text domain (Gildea,
2001). This is the problem of domain adapta-
tion. Although the problem exists ever since the
emergence of supervised Machine Learning, it has
started to get attention only in recent years.
Studies on supervised domain adaptation
(where there are limited amounts of annotated
</bodyText>
<note confidence="0.819311666666667">
Gertjan van Noord
University of Groningen
The Netherlands
</note>
<email confidence="0.952642">
G.J.M.van.Noord@rug.nl
</email>
<bodyText confidence="0.999959536585366">
resources in the new domain) have shown that
straightforward baselines (e.g. models based on
source only, target only, or the union of the data)
achieve a relatively high performance level and are
“surprisingly difficult to beat” (Daum´e III, 2007).
In contrast, semi-supervised adaptation (i.e. no
annotated resources in the new domain) is a much
more realistic situation but is clearly also consid-
erably more difficult. Current studies on semi-
supervised approaches show very mixed results.
Dredze et al. (2007) report on “frustrating” re-
sults on the CoNLL 2007 semi-supervised adap-
tation task for dependency parsing, i.e. “no team
was able to improve target domain performance
substantially over a state-of-the-art baseline”. On
the other hand, there have been positive results as
well. For instance, McClosky et al. (2006) im-
proved a statistical parser by self-training. Struc-
tural Correspondence Learning (Blitzer et al.,
2006) was effective for PoS tagging and Sentiment
Analysis (Blitzer et al., 2006; Blitzer et al., 2007),
while only modest gains were obtained for struc-
tured output tasks like parsing.
For parsing, most previous work on do-
main adaptation has focused on data-driven sys-
tems (Gildea, 2001; McClosky et al., 2006;
Dredze et al., 2007), i.e. systems employing (con-
stituent or dependency based) treebank gram-
mars. Only few studies examined the adaptation of
grammar-based systems (Hara et al., 2005; Plank
and van Noord, 2008), i.e. systems employing
a hand-crafted grammar with a statistical disam-
biguation component. This may be motivated by
the fact that potential gains for this task are inher-
ently bound by the grammar. Yet, domain adap-
tation poses a challenge for both kinds of pars-
ing systems. But to what extent do these differ-
ent kinds of systems suffer from the problem? We
test the hypothesis that grammar-driven systems
are less affected by domain changes. We empir-
ically investigate this in a case-study on Dutch.
</bodyText>
<page confidence="0.983508">
25
</page>
<note confidence="0.987185">
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 25–33,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.999085" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999975481481481">
Most previous work has focused on a single pars-
ing system in isolation (Gildea, 2001; Hara et
al., 2005; McClosky et al., 2006). However,
there is an observable trend towards combining
different parsing systems to exploit complemen-
tary strengths. For instance, Nivre and McDon-
ald (2008) combine two data-driven systems to im-
prove dependency accuracy. Similarly, two studies
successfully combined grammar-based and data-
driven systems: Sagae et al. (2007) incorporate
data-driven dependencies as soft-constraint in a
HPSG-based system for parsing the Wallstreet
Journal. In the same spirit (but the other di-
rection), Zhang and Wang (2009) use a deep-
grammar based backbone to improve data-driven
parsing accuracy. They incorporate features from
the grammar-based backbone into the data-driven
system to achieve better generalization across do-
mains. This is the work most closest to ours.
However, which kind of system (hand-crafted
versus purely statistical) is more affected by the
domain, and thus more sensitive to domain shifts?
To the best of our knowledge, no study has yet ad-
dressed this issue. We thus assess the performance
variation of three dependency parsing systems for
Dutch across domains, and propose a simple mea-
sure to quantify domain sensitivity.
</bodyText>
<sectionHeader confidence="0.91854" genericHeader="method">
3 Parsing Systems
</sectionHeader>
<bodyText confidence="0.984196611111111">
The parsing systems used in this study are: a
grammar-based system for Dutch (Alpino) and
two data-driven systems (MST and Malt), all de-
scribed next.
(1) Alpino is a parser for Dutch which has
been developed over the last ten years, on the ba-
sis of a domain-specific HPSG-grammar that was
used in the OVIS spoken dialogue system. The
OVIS parser was shown to out-perform a statisti-
cal (DOP) parser, in a contrastive formal evalua-
tion (van Zanten et al., 1999). In the ten years af-
ter this evaluation, the system has developed into a
generic parser for Dutch. Alpino consists of more
than 800 grammar rules in the tradition of HPSG,
and a large hand-crafted lexicon. It produces de-
pendency structures as ouput, where more than a
single head per token is allowed. For words that
are not in the lexicon, the system applies a large
variety of unknown word heuristics (van Noord,
2006), which deal with number-like expressions,
compounds, proper names, etc. Coverage of the
grammar and lexicon has been extended over the
years by paying careful attention to the results of
parsing large corpora, by means of error mining
techniques (van Noord, 2004; de Kok et al., 2009).
Lexical ambiguity is reduced by means of a
POS-tagger, described in (Prins and van No-
ord, 2003). This POS-tagger is trained on large
amounts of parser output, and removes unlikely
lexical categories. Some amount of lexical am-
biguity remains. A left-corner parser constructs
a parse-forest for an input sentence. Based on
large amounts of parsed data, the parser considers
only promising parse step sequences, by filtering
out sequences of parse steps which were not pre-
viously used to construct a best parse for a given
sentence. The parse step filter improves efficiency
considerably (van Noord, 2009).
A best-first beam-search algorithm retrieves the
best parse(s) from that forest by consulting a Max-
imum Entropy disambiguation component. Fea-
tures for the disambiguation component include
non-local features. For instance, there are features
that can be used to learn a preference for local ex-
traction over long-distance extraction, and a pref-
erence for subject fronting rather than direct ob-
ject fronting, and a preference for certain types of
orderings in the ”mittelfeld” of a Dutch sentence.
The various features that we use for disambigua-
tion, as well as the best-first algorithm is described
in (van Noord, 2006). The model now also con-
tains features which implement selection restric-
tions, trained on the basis of large parsed corpora
(van Noord, 2007). The maximum entropy dis-
ambiguation component is trained on the Alpino
treebank, described below.
To illustrate the role of the disambiguation com-
ponent, we provide some results for the first 536
sentences of one of the folds of the training data
(of course, the model used in this experiment is
trained on the remaining folds of training data).
In this setup, the POS-tagger and parse step filter
already filter out many, presumably bad, parses.
This table indicates that a very large amount of
parses can be constructed for some sentences. Fur-
thermore, the maximum entropy disambiguation
component does a good job in selecting good
parses from those. Accuracy is given here in terms
of f-score of named dependencies.
sents parses oracle arbitrary model
536 45011 95.74 76.56 89.39
(2) MST Parser (McDonald et al., 2005) is a
</bodyText>
<page confidence="0.995221">
26
</page>
<bodyText confidence="0.997023086956522">
data-driven graph-based dependency parser. The
system couples a minimum spanning tree search
procedure with a separate second stage classifier
to label the dependency edges.
(3) MALT Parser (Nivre et al., 2007) is a data-
driven transition-based dependency parser. Malt
parser uses SVMs to learn a classifier that predicts
the next parsing action. Instances represent parser
configurations and the label to predict determines
the next parser action.
Both data-driven parsers (MST and Malt) are
thus not specific for the Dutch Language, however,
they can be trained on a variety of languages given
that the training corpus complies with the column-
based format introduced in the 2006 CoNLL
shared task (Buchholz and Marsi, 2006). Ad-
ditionally, both parsers implement projective and
non-projective parsing algorithms, where the latter
will be used in our experiments on the relatively
free word order language Dutch. Despite that, we
train the data-driven parsers using their default set-
tings (e.g. first order features for MST, SVM with
polynomial kernel for Malt).
</bodyText>
<sectionHeader confidence="0.993553" genericHeader="method">
4 Datasets and experimental setup
</sectionHeader>
<bodyText confidence="0.99964719047619">
The source domain on which all parsers are trained
is cdb, the Alpino Treebank (van Noord, 2006).
For our cross-domain evaluation, we consider
Wikipedia and DPC (Dutch Parallel Corpus) as
target data. All datasets are described next.
Source: Cdb The cdb (Alpino Treebank) con-
sists of 140,000 words (7,136 sentences) from the
Eindhoven corpus (newspaper text). It is a col-
lection of text fragments from 6 Dutch newspa-
pers. The collection has been annotated accord-
ing to the guidelines of CGN (Oostdijk, 2000) and
stored in XML format. It is the standard treebank
used to train the disambiguation component of the
Alpino parser. Note that cdb is a subset of the
training corpus used in the CoNLL 2006 shared
task (Buchholz and Marsi, 2006). The CoNLL
training data additionally contained a mix of non-
newspaper text,1 which we exclude here on pur-
pose to keep a clean baseline.
Target: Wikipedia and DPC We use the
Wikipedia and DPC subpart of the LASSY cor-
</bodyText>
<table confidence="0.998900444444444">
Wikipedia Example articles #a #w ASL
LOC (location) Belgium, Antwerp (city) 31 25259 11.5
KUN (arts) Tervuren school 11 17073 17.1
POL (politics) Belgium elections 2003 16 15107 15.4
SPO (sports) Kim Clijsters 9 9713 11.1
HIS (history) History of Belgium 3 8396 17.9
BUS (business) Belgium Labour Federation 9 4440 11.0
NOB (nobility) Albert II 6 4179 15.1
COM (comics) Suske and Wiske 3 4000 10.5
MUS (music) Sandra Kim, Urbanus 3 1296 14.6
HOL (holidays) Flemish Community Day 4 524 12.2
Total 95 89987 13.4
DPC Description/Example #a #words ASL
Science medicine, oeanography 69 60787 19.2
Institutions political speeches 21 28646 16.1
Communication ICT/Internet 29 26640 17.5
Welfare state pensions 22 20198 17.9
Culture darwinism 11 16237 20.5
Economy inflation 9 14722 18.5
Education education in Flancers 2 11980 16.3
Home affairs presentation (Brussel) 1 9340 17.3
Foreign affairs European Union 7 9007 24.2
Environment threats/nature 6 8534 20.4
Finance banks (education banker) 6 6127 22.3
Leisure various (drugscandal) 2 2843 20.3
Consumption toys from China 1 1310 22.6
Total 186 216371 18.5
</table>
<tableCaption confidence="0.945739">
Table 1: Overview Wikipedia and DPC corpus (#a
articles, #w words, ASL average sentence length)
</tableCaption>
<bodyText confidence="0.991952366666667">
pus2 as target domains. These corpora contain sev-
eral domains, e.g. sports, locations, science. On
overview of the corpora is given in Table 1. Note
that both consist of hand-corrected data labeled by
Alpino, thus all domains employ the same anno-
tation scheme. This might introduce a slight bias
towards Alpino, however it has the advantage that
all domains employ the same annotation scheme –
which was the major source of error in the CoNLL
task on domain adaptation (Dredze et al., 2007).
CoNLL2006 This is the testfile for Dutch that
was used in the CoNLL 2006 shared task on multi-
lingual dependency parsing. The file consists
of 386 sentences from an institutional brochure
(about youth healthcare). We use this file to check
our data-driven models against state-of-the-art.
Alpino to CoNLL format In order to train the
MST and Malt parser and evaluate it on the var-
ious Wikipedia and DPC articles, we needed to
convert the Alpino Treebank format into the tab-
ular CoNLL format. To this end, we adapted the
treebank conversion software developed by Erwin
Marsi for the CoNLL 2006 shared task on multi-
lingual dependency parsing. Instead of using the
PoS tagger and tagset used in the shared task (to
which we did not have access to), we replaced the
PoS tags with more fine-grained tags obtained by
1Namely, a large amount of questions (from CLEF,
roughly 4k sentences) and hand-crafted sentences used dur-
ing the development of the grammar (1.5k).
</bodyText>
<footnote confidence="0.998084333333333">
2LASSY (Large Scale Syntactic Annotation of written
Dutch), ongoing project. Corpus version 17905, obtained
from http://www.let.rug.nl/vannoord/Lassy/corpus/
</footnote>
<page confidence="0.998504">
27
</page>
<bodyText confidence="0.999404172413793">
parsing the data with the Alpino parser.3 At testing
time, the data-driven parsers are given PoS tagged
input, while Alpino gets plain sentences.
Evaluation In all experiments, unless otherwise
specified, performance is measured as Labeled
Attachment Score (LAS), the percentage of to-
kens with the correct dependency edge and label.
To compute LAS, we use the CoNLL 2007 eval-
uation script4 with punctuation tokens excluded
from scoring (as was the default setting in CoNLL
2006). We thus evaluate all parsers using the same
evaluation metric. Note that the standard metric
for Alpino would be a variant of LAS, which al-
lows for a discrepancy between expected and re-
turned dependencies. Such a discrepancy can oc-
cur, for instance, because the syntactic annotation
of Alpino allows words to be dependent on more
than a single head (’secondary edges’) (van No-
ord, 2006). However, such edges are ignored in
the CoNLL format; just a single head per token
is allowed. Furthermore, there is another simpli-
fication. As the Dutch tagger used in the CoNLL
2006 shared task did not have the concept of multi-
words, the organizers chose to treat them as a sin-
gle token (Buchholz and Marsi, 2006). We here
follow the CoNLL 2006 task setup. To determine
whether results are significant, we us the Approx-
imate Randomization Test (see Yeh (2000)) with
1000 random shuffles.
</bodyText>
<sectionHeader confidence="0.978064" genericHeader="method">
5 Domain sensitivity
</sectionHeader>
<bodyText confidence="0.999937857142857">
The problem of domain dependence poses a chal-
lenge for both kinds of parsing systems, data-
driven and grammar-driven. However, to what ex-
tent? Which kind of parsing system is more af-
fected by domain shifts? We may rephrase our
question as: Which parsing system is more robust
to different input texts? To answer this question,
we will examine the robustness of the different
parsing systems in terms of variation of accuracy
on a variety of domains.
A measure of domain sensitivity Given a pars-
ing system (p) trained on some source domain
and evaluated on a set of N target domains, the
most intuitive measure would be to simply calcu-
</bodyText>
<footnote confidence="0.912642833333333">
3As discussed later (Section 6, cf. Table 2), using Alpino
tags actually improves the performance of the data-driven
parsers. We could perform this check as we recently got ac-
cess to the tagger and tagset used in the CoNLL shared task
(Mbt with wotan tagset; thanks to Erwin Marsi).
4http://nextens.uvt.nl/depparse-wiki/SoftwarePage
</footnote>
<bodyText confidence="0.983898333333333">
late mean (p) and standard deviation (sd) of the
performance on the target domains:
= accuracy of parser p on target domain i
</bodyText>
<equation confidence="0.6397215">
PN 1 V ASP sdtparget = s PN 1(LASip − µ�arget)2
N − 1
</equation>
<bodyText confidence="0.999987888888889">
However, standard deviation is highly influenced
by outliers. Furthermore, this measure does not
take the source domain performance (baseline)
into consideration nor the size of the target domain
itself. We thus propose to measure the domain
sensitivity of a system, i.e. its average domain
variation (adv), as weighted average difference
from the baseline (source) mean, where weights
represents the size of the various domains:
</bodyText>
<equation confidence="0.997713">
N
adv = P 1 wi * Qip, with
PN i
i=1 w
−LASbaseline and wi = size(wi)
p PNi=1 size(wi)
</equation>
<bodyText confidence="0.9999265">
In more detail, we measure average domain
variation (adv) relative to the baseline (source do-
main) performance by considering non-squared
differences from the out-of-domain mean and
weigh it by domain size. The adv measure can
thus take on positive or negative values. Intu-
itively, it will indicate the average weighted gain
or loss in performance, relative to the source do-
main. As alternative, we may want to just cal-
culate a straight, unweighted average: uadv =
PNi=1 Qip/N. However, this assumes that domains
have a representative size, and a threshold might
be needed to disregard domains that are presum-
ably too small.
We will use adv in the empirical result section
to evaluate the domain sensitivity of the parsers,
where size will be measured in terms of number of
words. We additionally provide values for the un-
weighted version using domains with at least 4000
words (cf. Table 1).
</bodyText>
<sectionHeader confidence="0.997519" genericHeader="method">
6 Empirical results
</sectionHeader>
<bodyText confidence="0.999721125">
First of all, we performed several sanity checks.
We trained the MST parser on the entire original
CoNLL training data as well as the cdb subpart
only, and evaluated it on the original CoNLL test
data. As shown in Table 2 (row 1-2) the accura-
cies of both models falls slightly below state-of-
the-art performance (row 5), most probably due to
the fact that we used standard parsing settings (e.g.
</bodyText>
<figure confidence="0.737483">
LASip
target =
µp
Qi
p
= LASip
</figure>
<page confidence="0.991949">
28
</page>
<bodyText confidence="0.925644333333333">
no second-order features for MST). More impor-
tantly, there was basically no difference in perfor-
mance when trained on the entire data or cdb only.
</bodyText>
<table confidence="0.999633222222222">
Model LAS UAS
MST (original CoNLL) 78.35 82.89
MST (original CoNLL, cdb subpart) 78.37 82.71
MST (cdb retagged with Alpino) 82.14 85.51
Malt (cdb retagged with Alpino) 80.64 82.66
MST (Nivre and McDonald, 2008) 79.19 83.6
Malt (Nivre and McDonald, 2008) 78.59 n/a
MST (cdb retagged with Mbt) 78.73 82.66
Malt (cdb retagged with Mbt) 75.34 78.29
</table>
<tableCaption confidence="0.996357">
Table 2: Performance of data-driven parsers ver-
</tableCaption>
<bodyText confidence="0.972971405405405">
sus state-of-the-art on the CoNLL 2006 testset (in
Labeled/Unlabeled Attachment Score).
We then trained the MST and Malt parser on
the cdb corpus converted into the retagged CoNLL
format, and tested on CoNLL 2006 test data (also
retagged with Alpino). As seen in Table 2, by
using Alpino tags the performance level signifi-
cantly improves (with p &lt; 0.002 using Approx-
imate Randomization Test with 1000 iterations).
This increase in performance can be attributed to
two sources: (a) improvements in the Alpino tree-
bank itself over the course of the years, and (b) the
more fine-grained PoS tagset obtained by parsing
the data with the deep grammar. To examine the
contribution of each source, we trained an addi-
tional MST model on the cdb data but tagged with
the same tagger as in the CoNLL shared task (Mbt,
cf. Table 2 last row): the results show that the
major source of improvement actually comes from
using the more fine-grained Alpino tags (78.73 -*
82.14 = +3.41 LAS), rather than the changes in
the treebank (78.37 -* 78.73 = +0.36 LAS).
Thus, despite the rather limited training data and
use of standard training settings, we are in line
with, and actually above, current results of data-
driven parsing for Dutch.
Baselines To establish our baselines, we per-
form 5-fold cross validation for each parser on the
source domain (cdb corpus, newspaper text). The
baselines for each parser are given in Table 3. The
grammar-driven parser Alpino achieves a baseline
that is significantly higher (90.75% LAS) com-
pared to the baselines of the data-driven systems
(around 80-83% LAS).
Cross-domain results As our goal is to assess
performance variation across domains, we evalu-
ate each parser on the Wikipedia and DPC corpora
</bodyText>
<table confidence="0.998720666666667">
Model Alpino MST Malt
Baseline (LAS) 90.76 83.63 79.95
Baseline (UAS) 92.47 88.12 83.31
</table>
<tableCaption confidence="0.904502">
Table 3: Baseline (5-fold cross-validation). All
differences are significant at p &lt; 0.001.
</tableCaption>
<bodyText confidence="0.999524888888889">
that cover a variety of domains (described in Ta-
ble 1). Figure 1 and Figure 2 summarizes the re-
sults for each corpus, respectively. In more detail,
the figures depict for each parser the baseline per-
formance as given in Table 3 (straight lines) and
the performance on every domain (bars). Note that
domains are ordered by size (number of words), so
that the largest domains appear as bars on the left.
Similar graphs come up if we replace labeled at-
tachment score with its unlabeled variant.
Figure 1 depicts parser performance on the
Wikipedia domains with respect to the source
domain baseline. The figure indicates that the
grammar-driven parser does not suffer much from
domain shifts. Its performance falls even above
baseline for several Wikipedia domains. In con-
trast, the MST parser suffers the most from the
domain changes; on most domains a substantial
performance drop can be observed. The transition-
based parser scores on average significantly lower
than the graph-based counterpart and Alpino, but
seems to be less affected by the domain shifts.
We can summarize this findings by our pro-
posed average domain variation measure (un-
weighted scores are given in the Figure): On av-
erage (over all Wikipedia domains), Alpino suf-
fers the least (adv = +0.81), followed by Malt
(+0.59) and MST (−2.2), which on average loses
2.2 absolute LAS. Thus, the graph-based data-
driven dependency parser MST suffers the most.
We evaluate the parsers also on the more var-
ied DPC corpus. It contains a broader set of do-
mains, amongst others science texts (medical texts
from the European Medicines Agency as well as
texts about oceanography) and articles with more
technical vocabulary (Communication, i.e. Inter-
net/ICT texts). The results are depicted in Fig-
ure 2. Both Malt (adv = 0.4) and Alpino (adv =
0.22) achieve on average a gain over the baseline,
with this time Malt being slightly less domain af-
fected than Alpino (most probably because Malt
scores above average on the more influential/larger
domains). Nevertheless, Alpino’s performance
level is significantly higher compared to both data-
driven counterparts. The graph-based data-driven
</bodyText>
<page confidence="0.995176">
29
</page>
<figure confidence="0.999918522727272">
LOC
KUN
POL
SPO
HIS
BUS
NOB
COM
MUS
HOL
Labeled Attachment Score (LAS)
75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98
Alpino
adv= 0.81 (+/− 3.7 )
uadv (&gt;4k)= 2 (+/− 2.1 )
Alpino
MST
Malt
MST
adv = −2.2 (+/− 9 )
Malt
uadv (&gt;4k)= −1.8 (+/− 4 )
adv = 0.59 (+/− 9.4 )
uadv(&gt;4k)= 1.3 (+/− 3 )
LOC
KUN
POL
SPO
HIS
BUS
NOB
COM
MUS
HOL
LOC
KUN
POL
SPO
HIS
BUS
NOB
COM
MUS
HOL
</figure>
<figureCaption confidence="0.929605333333333">
Figure 1: Performance on Wikipedia domains with respect to the source baseline (newspaper text) in-
cluding average domain variation (adv) score and its unweighted alternative (uadv). Domains are ordered
by size (largest on left). Full-colored bars indicate domains where performance lies below the baseline.
</figureCaption>
<bodyText confidence="0.994204390243902">
parser MST is the most domain-sensitive parser
also on DPC (adv = −0.27).
In contrast, if we would take only the deviation
on the target domains into consideration (with-
out considering the baseline, cf. Section 5), we
would get a completely opposite ranking on DPC,
where the Malt parser would actually be consid-
ered the most domain-sensitive (here higher sd
means higher sensitivity): Malt (sd = 1.20), MST
(sd = 1.14), Alpino (sd = 1.05). However, by
looking at Figure 2, intuitively, MST suffers more
from the domain shifts than Malt, as most bars lie
below the baseline. Moreover, the standard devia-
tion measure neither gives a sense of whether the
parser on average suffers a loss or gain over the
new domains, nor incorporates the information of
domain size. We thus believe our proposed aver-
age domain variation is a better suited measure.
To check whether the differences in perfor-
mance variation are statistically significant, we
performed an Approximate Randomization Test
over the performance differences (deltas) on the
23 domains (DPC and Wikipedia). The results
show that the difference between Alpino and MST
is significant. The same goes for the difference
between MST and Malt. Thus Alpino is signifi-
cantly more robust than MST. However, the dif-
ference between Alpino and Malt is not signif-
icant. These findings hold for differences mea-
sured in both labeled and unlabeled attachments
scores. Furthermore, all differences in absolute
performance across domains are significant.
To summarize, our empirical evaluation shows
that the grammar-driven system Alpino is rather
robust across domains. It is the best perform-
ing system and it is significantly more robust than
MST. In constrast, the transition-based parser Malt
scores the lowest across all domains, but its vari-
ation turned out not to be different from Alpino.
Over all domains, MST is the most domain-
sensitive parser.
</bodyText>
<page confidence="0.987329">
30
</page>
<figure confidence="0.995487924528302">
Alpino
adv = 0.22 (+/− 0.823 )
uadv (&gt;4k)= 0.4 (+/− 0.8 )
Labeled Attachment Score (LAS)
78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95
MST
adv = −0.27 (+/− 0.56 )
uadv (&gt;4k)= −0.21 (+/− 1 )
Alpino
MST
Malt
Malt
adv = 0.4 (+/− 0.54 )
uadv (&gt;4k)= 0.41 (+/− 0.9 )
Science
Institutions
Communication
Welfare_state
Culture
Economy
Education
Home_affairs
Foreign_affairs
Environment
Finance
Leisure
Consumption
Science
Institutions
Communication
Welfare_state
Culture
Economy
Education
Home_affairs
Foreign_affairs
Environment
Finance
Leisure
Consumption
Science
Institutions
Communication
Welfare_state
Culture
Economy
Education
Home_affairs
Foreign_affairs
Environment
Finance
Leisure
Consumption
</figure>
<figureCaption confidence="0.999965">
Figure 2: Performance on DPC domains with respect to the source baseline (newspaper text).
</figureCaption>
<bodyText confidence="0.99835788">
Excursion: Lexical information Both kinds
of parsing systems rely on lexical information
(words/stems) when learning their parsing (or
parse disambiguation) model. However, how
much influence does lexical information have?
To examine this issue, we retrain all parsing sys-
tems by excluding lexical information. As all pars-
ing systems rely on a feature-based representa-
tion, we remove all feature templates that include
words and thus train models on a reduced fea-
ture space (original versus reduced space: Alpino
24k/7k features; MST 14M/1.9M features; Malt
17/13 templates). The result of evaluating the
unlexicaled models on Wikipedia are shown in
Figure 3. Clearly, performance drops for for all
parsers in all domains. However, for the data-
driven parsers to a much higher degree. For in-
stance, MST loses on average 11 absolute points
in performance (adv = −11) and scores below
baseline on all Wikipedia domains. In contrast,
the grammar-driven parser Alpino suffers far less,
still scores above baseline on some domains.5 The
Malt parser lies somewhere in between, also suf-
fers from the missing lexical information, but to a
lesser degree than the graph-based parser MST.
</bodyText>
<sectionHeader confidence="0.998007" genericHeader="conclusions">
7 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.960752363636364">
We examined a grammar-based system cou-
pled with a statistical disambiguation component
(Alpino) and two data-driven statistical parsing
systems (MST and Malt) for dependency parsing
of Dutch. By looking at the performance variation
across a large variety of domains, we addressed
the question of how sensitive the parsing systems
are to the text domain. This, to gauge which kind
5Note that the parser has still access to its lexicon here;
for now we removed lexicalized features from the trainable
part of Alpino, the statistical disambiguation component.
</bodyText>
<page confidence="0.999785">
31
</page>
<figure confidence="0.999869886363636">
LOC
KUN
POL
SPO
HIS
BUS
NOB
COM
MUS
HOL
LOC
KUN
POL
SPO
HIS
BUS
NOB
COM
MUS
HOL
LOC
KUN
POL
SPO
HIS
BUS
NOB
COM
MUS
HOL
Alpino
MST
Malt
MST
adv = −11 (+/− 11 )
uadv (&gt;4k)= −11 (+/− 2.1 )
Labeled Attachment Score (LAS)
66 68 70 72 74 76 78 80 82 84 86 88 90 92 94 96
Malt
adv = −4.9 (+/− 9 )
uadv (&gt;4k)= −4.8 (+/− 3 )
Alpino
adv= −0.63 (+/− 3.6 )
uadv (&gt;4k)= 0.1 (+/− 2 )
</figure>
<figureCaption confidence="0.999985">
Figure 3: Performance of unlexical parsers on Wikipedia domains with respect to the source baseline.
</figureCaption>
<bodyText confidence="0.999895625">
of system (data-driven versus grammar-driven) is
more affected by domain shifts, and thus more in
need for adaptation techniques. We also proposed
a simple measure to quantify domain sensitivity.
The results show that the grammar-based sys-
tem Alpino is the best performing system, and it
is robust across domains. In contrast, MST, the
graph-based approach to data-driven parsing is the
most domain-sensitive parser. The results for Malt
indicate that its variation across domains is lim-
ited, but this parser is outperformed by both other
systems on all domains. In general, data-driven
systems heavily rely on the training data to esti-
mate their models. This becomes apparent when
we exclude lexical information from the train-
ing process, which results in a substantial perfor-
mance drop for the data-driven systems, MST and
Malt. The grammar-driven model was more robust
against the missing lexical information. Grammar-
driven systems try to encode domain independent
linguistic knowledge, but usually suffer from cov-
erage problems. The Alpino parser successfully
implements a set of unknown word heuristics and
a partial parsing strategy (in case no full parse can
be found) to overcome this problem. This makes
the system rather robust across domains, and, as
shown in this study, significantly more robust than
MST. This is not to say that domain dependence
does not consitute a problem for grammar-driven
parsers at all. As also noted by Zhang and Wang
(2009), the disambiguation component and lexi-
cal coverage of grammar-based systems are still
domain-dependent. Thus, domain dependence is a
problem for both types of parsing systems, though,
as shown in this study, to a lesser extent for the
grammar-based system Alpino. Of course, these
results are specific for Dutch; however, it’s a first
step. As the proposed methods are indepedent of
language and parsing system, they can be applied
to another system or language.
In future, we would like to (a) perform an error
analysis (e.g. why for some domains the parsers
outperform their baseline; what are typical in-
domain and out-domain errors), (a) examine why
there is such a difference in performance variation
between Malt and MST, and (c) investigate what
part(s) of the Alpino parser are responsible for the
differences with the data-driven parsers.
</bodyText>
<page confidence="0.998957">
32
</page>
<sectionHeader confidence="0.998339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999857742857143">
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Conference on Empirical Meth-
ods in Natural Language Processing, Sydney.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In ACL, Prague, Czech Republic.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
In Proc. of CoNLL, pages 149–164.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In ACL, Prague, Czech Republic.
Dani¨el de Kok, Jianqiang Ma, and Gertjan van Noord.
2009. A generalized method for iterative error min-
ing in parsing results. In Proceedings of the 2009
Workshop on Grammar Engineering Across Frame-
works (GEAF 2009), pages 71–79, Suntec, Singa-
pore, August.
Mark Dredze, John Blitzer, Pratha Pratim Taluk-
dar, Kuzman Ganchev, Joao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adaptation
for parsing. In Proceedings of the CoNLL Shared
Task Session, Prague, Czech Republic.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Tadayoshi Hara, Miyao Yusuke, and Jun’ichi Tsu-
jii. 2005. Adapting a probabilistic disambiguation
model of an hpsg parser to a new domain. In Pro-
ceedings of the International Joint Conference on
Natural Language Processing.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152–159, New York City.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 523–530.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950–958, Columbus, Ohio, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13:95–135.
Nelleke Oostdijk. 2000. The Spoken Dutch Corpus:
Overview and first evaluation. In Proceedings of
LREC, pages 887–894.
Barbara Plank and Gertjan van Noord. 2008. Ex-
ploring an auxiliary distribution based approach to
domain adaptation of a syntactic disambiguation
model. In Proceedings of the Workshop on Cross-
Framework and Cross-Domain Parser Evaluation
(PE), Manchester, August.
Robbert Prins and Gertjan van Noord. 2003. Reinforc-
ing parser preferences through tagging. Traitement
Automatique des Langues, 44(3):121–139.
Kenji Sagae, Yusuke Miyao, and Jun’ichi Tsujii. 2007.
Hpsg parsing with shallow dependency constraints.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 624–
631, Prague, Czech Republic, June.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In ACL2004,
Barcelona. ACL.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement
Automatique des Langues naturelles, pages 20–42,
Leuven.
Gertjan van Noord. 2007. Using self-trained bilexical
preferences to improve disambiguation accuracy. In
Proceedings of the International Workshop on Pars-
ing Technology (IWPT), ACL 2007 Workshop, pages
1–10, Prague. ACL.
Gertjan van Noord. 2009. Learning efficient parsing.
In EACL 2009, The 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 817–825, Athens, Greece.
Gert Veldhuijzen van Zanten, Gosse Bouma, Khalil
Sima’an, Gertjan van Noord, and Remko Bonnema.
1999. Evaluation of the NLP components of the
OVIS2 spoken dialogue system. In Frank van
Eynde, Ineke Schuurman, and Ness Schelkens, ed-
itors, Computational Linguistics in the Netherlands
1998, pages 213–229. Rodopi Amsterdam.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In ACL,
pages 947–953, Morristown, NJ, USA.
Yi Zhang and Rui Wang. 2009. Cross-domain depen-
dency parsing using a deep linguistic grammar. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 378–386, Suntec, Singapore,
August.
</reference>
<page confidence="0.999375">
33
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.501656">
<title confidence="0.886835">Grammar-driven versus Data-driven: Which Parsing System is Affected by Domain Shifts?</title>
<author confidence="0.935491">Barbara</author>
<affiliation confidence="0.7939775">University of The</affiliation>
<email confidence="0.954421">b.plank@rug.nl</email>
<abstract confidence="0.999140777777778">In the past decade several parsing systems for natural language have emerged, which use different methods and formalisms. For instance, systems that employ a handcrafted grammar and a statistical disambiguation component versus purely statistical data-driven systems. What they have in common is the lack of portability to new domains: their performance might decrease substantially as the distance between test and training domain increases. Yet, to which degree do they suffer from this problem, i.e. which kind of parsing system is more affected by domain shifts? Intuitively, grammar-driven systems should be less affected by domain changes. To investigate this hypothesis, an empirical investigation on Dutch is carried out. The performance variation of a grammar-driven versus two data-driven systems across domains is evaluated, and a simple measure to quantify domain sensitivity proposed. This will give an estimate of which parsing system is more affected by domain shifts, and thus more in need for adaptation techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sydney.</location>
<contexts>
<context position="2743" citStr="Blitzer et al., 2006" startWordPosition="416" endWordPosition="419">esources in the new domain) is a much more realistic situation but is clearly also considerably more difficult. Current studies on semisupervised approaches show very mixed results. Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. “no team was able to improve target domain performance substantially over a state-of-the-art baseline”. On the other hand, there have been positive results as well. For instance, McClosky et al. (2006) improved a statistical parser by self-training. Structural Correspondence Learning (Blitzer et al., 2006) was effective for PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), while only modest gains were obtained for structured output tasks like parsing. For parsing, most previous work on domain adaptation has focused on data-driven systems (Gildea, 2001; McClosky et al., 2006; Dredze et al., 2007), i.e. systems employing (constituent or dependency based) treebank grammars. Only few studies examined the adaptation of grammar-based systems (Hara et al., 2005; Plank and van Noord, 2008), i.e. systems employing a hand-crafted grammar with a statistical disambiguation co</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Conference on Empirical Methods in Natural Language Processing, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2841" citStr="Blitzer et al., 2007" startWordPosition="432" endWordPosition="435">re difficult. Current studies on semisupervised approaches show very mixed results. Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. “no team was able to improve target domain performance substantially over a state-of-the-art baseline”. On the other hand, there have been positive results as well. For instance, McClosky et al. (2006) improved a statistical parser by self-training. Structural Correspondence Learning (Blitzer et al., 2006) was effective for PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), while only modest gains were obtained for structured output tasks like parsing. For parsing, most previous work on domain adaptation has focused on data-driven systems (Gildea, 2001; McClosky et al., 2006; Dredze et al., 2007), i.e. systems employing (constituent or dependency based) treebank grammars. Only few studies examined the adaptation of grammar-based systems (Hara et al., 2005; Plank and van Noord, 2008), i.e. systems employing a hand-crafted grammar with a statistical disambiguation component. This may be motivated by the fact that potential gains for this task are inherently bound</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In ACL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing. In</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>149--164</pages>
<contexts>
<context position="9330" citStr="Buchholz and Marsi, 2006" startWordPosition="1478" endWordPosition="1481">ure with a separate second stage classifier to label the dependency edges. (3) MALT Parser (Nivre et al., 2007) is a datadriven transition-based dependency parser. Malt parser uses SVMs to learn a classifier that predicts the next parsing action. Instances represent parser configurations and the label to predict determines the next parser action. Both data-driven parsers (MST and Malt) are thus not specific for the Dutch Language, however, they can be trained on a variety of languages given that the training corpus complies with the columnbased format introduced in the 2006 CoNLL shared task (Buchholz and Marsi, 2006). Additionally, both parsers implement projective and non-projective parsing algorithms, where the latter will be used in our experiments on the relatively free word order language Dutch. Despite that, we train the data-driven parsers using their default settings (e.g. first order features for MST, SVM with polynomial kernel for Malt). 4 Datasets and experimental setup The source domain on which all parsers are trained is cdb, the Alpino Treebank (van Noord, 2006). For our cross-domain evaluation, we consider Wikipedia and DPC (Dutch Parallel Corpus) as target data. All datasets are described </context>
<context position="14648" citStr="Buchholz and Marsi, 2006" startWordPosition="2347" endWordPosition="2350">standard metric for Alpino would be a variant of LAS, which allows for a discrepancy between expected and returned dependencies. Such a discrepancy can occur, for instance, because the syntactic annotation of Alpino allows words to be dependent on more than a single head (’secondary edges’) (van Noord, 2006). However, such edges are ignored in the CoNLL format; just a single head per token is allowed. Furthermore, there is another simplification. As the Dutch tagger used in the CoNLL 2006 shared task did not have the concept of multiwords, the organizers chose to treat them as a single token (Buchholz and Marsi, 2006). We here follow the CoNLL 2006 task setup. To determine whether results are significant, we us the Approximate Randomization Test (see Yeh (2000)) with 1000 random shuffles. 5 Domain sensitivity The problem of domain dependence poses a challenge for both kinds of parsing systems, datadriven and grammar-driven. However, to what extent? Which kind of parsing system is more affected by domain shifts? We may rephrase our question as: Which parsing system is more robust to different input texts? To answer this question, we will examine the robustness of the different parsing systems in terms of va</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In In Proc. of CoNLL, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In ACL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianqiang Ma Dani¨el de Kok</author>
<author>Gertjan van Noord</author>
</authors>
<title>A generalized method for iterative error mining in parsing results.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks (GEAF</booktitle>
<pages>71--79</pages>
<location>Suntec, Singapore,</location>
<marker>Dani¨el de Kok, van Noord, 2009</marker>
<rawString>Dani¨el de Kok, Jianqiang Ma, and Gertjan van Noord. 2009. A generalized method for iterative error mining in parsing results. In Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks (GEAF 2009), pages 71–79, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>John Blitzer</author>
</authors>
<title>Pratha Pratim Talukdar, Kuzman Ganchev, Joao Graca, and Fernando Pereira.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Dredze, Blitzer, 2007</marker>
<rawString>Mark Dredze, John Blitzer, Pratha Pratim Talukdar, Kuzman Ganchev, Joao Graca, and Fernando Pereira. 2007. Frustratingly hard domain adaptation for parsing. In Proceedings of the CoNLL Shared Task Session, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1462" citStr="Gildea, 2001" startWordPosition="225" endWordPosition="226"> empirical investigation on Dutch is carried out. The performance variation of a grammar-driven versus two data-driven systems across domains is evaluated, and a simple measure to quantify domain sensitivity proposed. This will give an estimate of which parsing system is more affected by domain shifts, and thus more in need for adaptation techniques. 1 Introduction Most modern Natural Language Processing (NLP) systems are subject to the wellknown problem of lack of portability to new domains: there is a substantial drop in their performance when the system gets input from another text domain (Gildea, 2001). This is the problem of domain adaptation. Although the problem exists ever since the emergence of supervised Machine Learning, it has started to get attention only in recent years. Studies on supervised domain adaptation (where there are limited amounts of annotated Gertjan van Noord University of Groningen The Netherlands G.J.M.van.Noord@rug.nl resources in the new domain) have shown that straightforward baselines (e.g. models based on source only, target only, or the union of the data) achieve a relatively high performance level and are “surprisingly difficult to beat” (Daum´e III, 2007). </context>
<context position="3024" citStr="Gildea, 2001" startWordPosition="464" endWordPosition="465">ependency parsing, i.e. “no team was able to improve target domain performance substantially over a state-of-the-art baseline”. On the other hand, there have been positive results as well. For instance, McClosky et al. (2006) improved a statistical parser by self-training. Structural Correspondence Learning (Blitzer et al., 2006) was effective for PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), while only modest gains were obtained for structured output tasks like parsing. For parsing, most previous work on domain adaptation has focused on data-driven systems (Gildea, 2001; McClosky et al., 2006; Dredze et al., 2007), i.e. systems employing (constituent or dependency based) treebank grammars. Only few studies examined the adaptation of grammar-based systems (Hara et al., 2005; Plank and van Noord, 2008), i.e. systems employing a hand-crafted grammar with a statistical disambiguation component. This may be motivated by the fact that potential gains for this task are inherently bound by the grammar. Yet, domain adaptation poses a challenge for both kinds of parsing systems. But to what extent do these different kinds of systems suffer from the problem? We test th</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadayoshi Hara</author>
<author>Miyao Yusuke</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Adapting a probabilistic disambiguation model of an hpsg parser to a new domain.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="3231" citStr="Hara et al., 2005" startWordPosition="494" endWordPosition="497"> McClosky et al. (2006) improved a statistical parser by self-training. Structural Correspondence Learning (Blitzer et al., 2006) was effective for PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), while only modest gains were obtained for structured output tasks like parsing. For parsing, most previous work on domain adaptation has focused on data-driven systems (Gildea, 2001; McClosky et al., 2006; Dredze et al., 2007), i.e. systems employing (constituent or dependency based) treebank grammars. Only few studies examined the adaptation of grammar-based systems (Hara et al., 2005; Plank and van Noord, 2008), i.e. systems employing a hand-crafted grammar with a statistical disambiguation component. This may be motivated by the fact that potential gains for this task are inherently bound by the grammar. Yet, domain adaptation poses a challenge for both kinds of parsing systems. But to what extent do these different kinds of systems suffer from the problem? We test the hypothesis that grammar-driven systems are less affected by domain changes. We empirically investigate this in a case-study on Dutch. 25 Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the</context>
</contexts>
<marker>Hara, Yusuke, Tsujii, 2005</marker>
<rawString>Tadayoshi Hara, Miyao Yusuke, and Jun’ichi Tsujii. 2005. Adapting a probabilistic disambiguation model of an hpsg parser to a new domain. In Proceedings of the International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>152--159</pages>
<location>New York City.</location>
<contexts>
<context position="2637" citStr="McClosky et al. (2006)" startWordPosition="401" endWordPosition="404">isingly difficult to beat” (Daum´e III, 2007). In contrast, semi-supervised adaptation (i.e. no annotated resources in the new domain) is a much more realistic situation but is clearly also considerably more difficult. Current studies on semisupervised approaches show very mixed results. Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. “no team was able to improve target domain performance substantially over a state-of-the-art baseline”. On the other hand, there have been positive results as well. For instance, McClosky et al. (2006) improved a statistical parser by self-training. Structural Correspondence Learning (Blitzer et al., 2006) was effective for PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), while only modest gains were obtained for structured output tasks like parsing. For parsing, most previous work on domain adaptation has focused on data-driven systems (Gildea, 2001; McClosky et al., 2006; Dredze et al., 2007), i.e. systems employing (constituent or dependency based) treebank grammars. Only few studies examined the adaptation of grammar-based systems (Hara et al., 2005; Plan</context>
<context position="4092" citStr="McClosky et al., 2006" startWordPosition="637" endWordPosition="640">adaptation poses a challenge for both kinds of parsing systems. But to what extent do these different kinds of systems suffer from the problem? We test the hypothesis that grammar-driven systems are less affected by domain changes. We empirically investigate this in a case-study on Dutch. 25 Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 25–33, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 2 Related work Most previous work has focused on a single parsing system in isolation (Gildea, 2001; Hara et al., 2005; McClosky et al., 2006). However, there is an observable trend towards combining different parsing systems to exploit complementary strengths. For instance, Nivre and McDonald (2008) combine two data-driven systems to improve dependency accuracy. Similarly, two studies successfully combined grammar-based and datadriven systems: Sagae et al. (2007) incorporate data-driven dependencies as soft-constraint in a HPSG-based system for parsing the Wallstreet Journal. In the same spirit (but the other direction), Zhang and Wang (2009) use a deepgrammar based backbone to improve data-driven parsing accuracy. They incorporate</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms. In</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>523--530</pages>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>950--958</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="4251" citStr="Nivre and McDonald (2008)" startWordPosition="659" endWordPosition="663">hypothesis that grammar-driven systems are less affected by domain changes. We empirically investigate this in a case-study on Dutch. 25 Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 25–33, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 2 Related work Most previous work has focused on a single parsing system in isolation (Gildea, 2001; Hara et al., 2005; McClosky et al., 2006). However, there is an observable trend towards combining different parsing systems to exploit complementary strengths. For instance, Nivre and McDonald (2008) combine two data-driven systems to improve dependency accuracy. Similarly, two studies successfully combined grammar-based and datadriven systems: Sagae et al. (2007) incorporate data-driven dependencies as soft-constraint in a HPSG-based system for parsing the Wallstreet Journal. In the same spirit (but the other direction), Zhang and Wang (2009) use a deepgrammar based backbone to improve data-driven parsing accuracy. They incorporate features from the grammar-based backbone into the data-driven system to achieve better generalization across domains. This is the work most closest to ours. H</context>
<context position="18205" citStr="Nivre and McDonald, 2008" startWordPosition="2951" endWordPosition="2954">ated it on the original CoNLL test data. As shown in Table 2 (row 1-2) the accuracies of both models falls slightly below state-ofthe-art performance (row 5), most probably due to the fact that we used standard parsing settings (e.g. LASip target = µp Qi p = LASip 28 no second-order features for MST). More importantly, there was basically no difference in performance when trained on the entire data or cdb only. Model LAS UAS MST (original CoNLL) 78.35 82.89 MST (original CoNLL, cdb subpart) 78.37 82.71 MST (cdb retagged with Alpino) 82.14 85.51 Malt (cdb retagged with Alpino) 80.64 82.66 MST (Nivre and McDonald, 2008) 79.19 83.6 Malt (Nivre and McDonald, 2008) 78.59 n/a MST (cdb retagged with Mbt) 78.73 82.66 Malt (cdb retagged with Mbt) 75.34 78.29 Table 2: Performance of data-driven parsers versus state-of-the-art on the CoNLL 2006 testset (in Labeled/Unlabeled Attachment Score). We then trained the MST and Malt parser on the cdb corpus converted into the retagged CoNLL format, and tested on CoNLL 2006 test data (also retagged with Alpino). As seen in Table 2, by using Alpino tags the performance level significantly improves (with p &lt; 0.002 using Approximate Randomization Test with 1000 iterations). This</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of ACL-08: HLT, pages 950–958, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<pages>13--95</pages>
<contexts>
<context position="8816" citStr="Nivre et al., 2007" startWordPosition="1397" endWordPosition="1400">, presumably bad, parses. This table indicates that a very large amount of parses can be constructed for some sentences. Furthermore, the maximum entropy disambiguation component does a good job in selecting good parses from those. Accuracy is given here in terms of f-score of named dependencies. sents parses oracle arbitrary model 536 45011 95.74 76.56 89.39 (2) MST Parser (McDonald et al., 2005) is a 26 data-driven graph-based dependency parser. The system couples a minimum spanning tree search procedure with a separate second stage classifier to label the dependency edges. (3) MALT Parser (Nivre et al., 2007) is a datadriven transition-based dependency parser. Malt parser uses SVMs to learn a classifier that predicts the next parsing action. Instances represent parser configurations and the label to predict determines the next parser action. Both data-driven parsers (MST and Malt) are thus not specific for the Dutch Language, however, they can be trained on a variety of languages given that the training corpus complies with the columnbased format introduced in the 2006 CoNLL shared task (Buchholz and Marsi, 2006). Additionally, both parsers implement projective and non-projective parsing algorithm</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13:95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nelleke Oostdijk</author>
</authors>
<title>The Spoken Dutch Corpus: Overview and first evaluation.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>887--894</pages>
<contexts>
<context position="10209" citStr="Oostdijk, 2000" startWordPosition="1619" endWordPosition="1620">(e.g. first order features for MST, SVM with polynomial kernel for Malt). 4 Datasets and experimental setup The source domain on which all parsers are trained is cdb, the Alpino Treebank (van Noord, 2006). For our cross-domain evaluation, we consider Wikipedia and DPC (Dutch Parallel Corpus) as target data. All datasets are described next. Source: Cdb The cdb (Alpino Treebank) consists of 140,000 words (7,136 sentences) from the Eindhoven corpus (newspaper text). It is a collection of text fragments from 6 Dutch newspapers. The collection has been annotated according to the guidelines of CGN (Oostdijk, 2000) and stored in XML format. It is the standard treebank used to train the disambiguation component of the Alpino parser. Note that cdb is a subset of the training corpus used in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). The CoNLL training data additionally contained a mix of nonnewspaper text,1 which we exclude here on purpose to keep a clean baseline. Target: Wikipedia and DPC We use the Wikipedia and DPC subpart of the LASSY corWikipedia Example articles #a #w ASL LOC (location) Belgium, Antwerp (city) 31 25259 11.5 KUN (arts) Tervuren school 11 17073 17.1 POL (politics) Belgium </context>
</contexts>
<marker>Oostdijk, 2000</marker>
<rawString>Nelleke Oostdijk. 2000. The Spoken Dutch Corpus: Overview and first evaluation. In Proceedings of LREC, pages 887–894.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Gertjan van Noord</author>
</authors>
<title>Exploring an auxiliary distribution based approach to domain adaptation of a syntactic disambiguation model.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on CrossFramework and Cross-Domain Parser Evaluation (PE),</booktitle>
<location>Manchester,</location>
<marker>Plank, van Noord, 2008</marker>
<rawString>Barbara Plank and Gertjan van Noord. 2008. Exploring an auxiliary distribution based approach to domain adaptation of a syntactic disambiguation model. In Proceedings of the Workshop on CrossFramework and Cross-Domain Parser Evaluation (PE), Manchester, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robbert Prins</author>
<author>Gertjan van Noord</author>
</authors>
<title>Reinforcing parser preferences through tagging.</title>
<date>2003</date>
<booktitle>Traitement Automatique des Langues,</booktitle>
<volume>44</volume>
<issue>3</issue>
<marker>Prins, van Noord, 2003</marker>
<rawString>Robbert Prins and Gertjan van Noord. 2003. Reinforcing parser preferences through tagging. Traitement Automatique des Langues, 44(3):121–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Hpsg parsing with shallow dependency constraints.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>624--631</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4418" citStr="Sagae et al. (2007)" startWordPosition="683" endWordPosition="686">NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 25–33, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 2 Related work Most previous work has focused on a single parsing system in isolation (Gildea, 2001; Hara et al., 2005; McClosky et al., 2006). However, there is an observable trend towards combining different parsing systems to exploit complementary strengths. For instance, Nivre and McDonald (2008) combine two data-driven systems to improve dependency accuracy. Similarly, two studies successfully combined grammar-based and datadriven systems: Sagae et al. (2007) incorporate data-driven dependencies as soft-constraint in a HPSG-based system for parsing the Wallstreet Journal. In the same spirit (but the other direction), Zhang and Wang (2009) use a deepgrammar based backbone to improve data-driven parsing accuracy. They incorporate features from the grammar-based backbone into the data-driven system to achieve better generalization across domains. This is the work most closest to ours. However, which kind of system (hand-crafted versus purely statistical) is more affected by the domain, and thus more sensitive to domain shifts? To the best of our know</context>
</contexts>
<marker>Sagae, Miyao, Tsujii, 2007</marker>
<rawString>Kenji Sagae, Yusuke Miyao, and Jun’ichi Tsujii. 2007. Hpsg parsing with shallow dependency constraints. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 624– 631, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Error mining for widecoverage grammar engineering.</title>
<date>2004</date>
<booktitle>In ACL2004,</booktitle>
<publisher>ACL.</publisher>
<location>Barcelona.</location>
<marker>van Noord, 2004</marker>
<rawString>Gertjan van Noord. 2004. Error mining for widecoverage grammar engineering. In ACL2004, Barcelona. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>At Last Parsing Is Now Operational.</title>
<date>2006</date>
<booktitle>In TALN 2006 Verbum Ex Machina, Actes De La 13e Conference sur Le Traitement Automatique des Langues naturelles,</booktitle>
<pages>20--42</pages>
<location>Leuven.</location>
<marker>van Noord, 2006</marker>
<rawString>Gertjan van Noord. 2006. At Last Parsing Is Now Operational. In TALN 2006 Verbum Ex Machina, Actes De La 13e Conference sur Le Traitement Automatique des Langues naturelles, pages 20–42, Leuven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Using self-trained bilexical preferences to improve disambiguation accuracy.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technology (IWPT), ACL 2007 Workshop,</booktitle>
<pages>1--10</pages>
<publisher>ACL.</publisher>
<location>Prague.</location>
<marker>van Noord, 2007</marker>
<rawString>Gertjan van Noord. 2007. Using self-trained bilexical preferences to improve disambiguation accuracy. In Proceedings of the International Workshop on Parsing Technology (IWPT), ACL 2007 Workshop, pages 1–10, Prague. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Learning efficient parsing.</title>
<date>2009</date>
<booktitle>In EACL 2009, The 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>817--825</pages>
<location>Athens, Greece.</location>
<marker>van Noord, 2009</marker>
<rawString>Gertjan van Noord. 2009. Learning efficient parsing. In EACL 2009, The 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 817–825, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gert Veldhuijzen van Zanten</author>
<author>Gosse Bouma</author>
<author>Khalil Sima’an</author>
<author>Gertjan van Noord</author>
<author>Remko Bonnema</author>
</authors>
<date>1999</date>
<booktitle>Evaluation of the NLP components of the OVIS2 spoken dialogue system. In Frank van Eynde, Ineke Schuurman, and Ness Schelkens, editors, Computational Linguistics in the Netherlands</booktitle>
<pages>213--229</pages>
<location>Rodopi Amsterdam.</location>
<marker>van Zanten, Bouma, Sima’an, van Noord, Bonnema, 1999</marker>
<rawString>Gert Veldhuijzen van Zanten, Gosse Bouma, Khalil Sima’an, Gertjan van Noord, and Remko Bonnema. 1999. Evaluation of the NLP components of the OVIS2 spoken dialogue system. In Frank van Eynde, Ineke Schuurman, and Ness Schelkens, editors, Computational Linguistics in the Netherlands 1998, pages 213–229. Rodopi Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In ACL,</booktitle>
<pages>947--953</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="14794" citStr="Yeh (2000)" startWordPosition="2373" endWordPosition="2374">r instance, because the syntactic annotation of Alpino allows words to be dependent on more than a single head (’secondary edges’) (van Noord, 2006). However, such edges are ignored in the CoNLL format; just a single head per token is allowed. Furthermore, there is another simplification. As the Dutch tagger used in the CoNLL 2006 shared task did not have the concept of multiwords, the organizers chose to treat them as a single token (Buchholz and Marsi, 2006). We here follow the CoNLL 2006 task setup. To determine whether results are significant, we us the Approximate Randomization Test (see Yeh (2000)) with 1000 random shuffles. 5 Domain sensitivity The problem of domain dependence poses a challenge for both kinds of parsing systems, datadriven and grammar-driven. However, to what extent? Which kind of parsing system is more affected by domain shifts? We may rephrase our question as: Which parsing system is more robust to different input texts? To answer this question, we will examine the robustness of the different parsing systems in terms of variation of accuracy on a variety of domains. A measure of domain sensitivity Given a parsing system (p) trained on some source domain and evaluate</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In ACL, pages 947–953, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Rui Wang</author>
</authors>
<title>Cross-domain dependency parsing using a deep linguistic grammar.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>378--386</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="4601" citStr="Zhang and Wang (2009)" startWordPosition="710" endWordPosition="713">ork has focused on a single parsing system in isolation (Gildea, 2001; Hara et al., 2005; McClosky et al., 2006). However, there is an observable trend towards combining different parsing systems to exploit complementary strengths. For instance, Nivre and McDonald (2008) combine two data-driven systems to improve dependency accuracy. Similarly, two studies successfully combined grammar-based and datadriven systems: Sagae et al. (2007) incorporate data-driven dependencies as soft-constraint in a HPSG-based system for parsing the Wallstreet Journal. In the same spirit (but the other direction), Zhang and Wang (2009) use a deepgrammar based backbone to improve data-driven parsing accuracy. They incorporate features from the grammar-based backbone into the data-driven system to achieve better generalization across domains. This is the work most closest to ours. However, which kind of system (hand-crafted versus purely statistical) is more affected by the domain, and thus more sensitive to domain shifts? To the best of our knowledge, no study has yet addressed this issue. We thus assess the performance variation of three dependency parsing systems for Dutch across domains, and propose a simple measure to qu</context>
<context position="29525" citStr="Zhang and Wang (2009)" startWordPosition="4828" endWordPosition="4831">-driven model was more robust against the missing lexical information. Grammardriven systems try to encode domain independent linguistic knowledge, but usually suffer from coverage problems. The Alpino parser successfully implements a set of unknown word heuristics and a partial parsing strategy (in case no full parse can be found) to overcome this problem. This makes the system rather robust across domains, and, as shown in this study, significantly more robust than MST. This is not to say that domain dependence does not consitute a problem for grammar-driven parsers at all. As also noted by Zhang and Wang (2009), the disambiguation component and lexical coverage of grammar-based systems are still domain-dependent. Thus, domain dependence is a problem for both types of parsing systems, though, as shown in this study, to a lesser extent for the grammar-based system Alpino. Of course, these results are specific for Dutch; however, it’s a first step. As the proposed methods are indepedent of language and parsing system, they can be applied to another system or language. In future, we would like to (a) perform an error analysis (e.g. why for some domains the parsers outperform their baseline; what are typ</context>
</contexts>
<marker>Zhang, Wang, 2009</marker>
<rawString>Yi Zhang and Rui Wang. 2009. Cross-domain dependency parsing using a deep linguistic grammar. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 378–386, Suntec, Singapore, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>