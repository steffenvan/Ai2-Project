<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.979766">
A Bootstrapping Approach to Unsupervised Detection of Cue Phrase
Variants
</title>
<note confidence="0.511587666666667">
Rashid M. Abdalla and Simone Teufel
Computer Laboratory, University of Cambridge
15 JJ Thomson Avenue, Cambridge CB3 OFD, UK
</note>
<email confidence="0.994622">
rma33@cam.ac.uk, sht25@cam.ac.uk
</email>
<sectionHeader confidence="0.997334" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938055555556">
We investigate the unsupervised detection
of semi-fixed cue phrases such as “This
paper proposes a novel approach... 1”
from unseen text, on the basis of only a
handful of seed cue phrases with the de-
sired semantics. The problem, in contrast
to bootstrapping approaches for Question
Answering and Information Extraction, is
that it is hard to find a constraining context
for occurrences of semi-fixed cue phrases.
Our method uses components of the cue
phrase itself, rather than external con-
text, to bootstrap. It successfully excludes
phrases which are different from the tar-
get semantics, but which look superficially
similar. The method achieves 88% ac-
curacy, outperforming standard bootstrap-
ping approaches.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998982240740741">
Cue phrases such as “This paper proposes a novel
approach to... ”, “no method for ... exists” or even
“you will hear from my lawyer” are semi-fixed in
that they constitute a formulaic pattern with a clear
semantics, but with syntactic and lexical variations
which are hard to predict and thus hard to detect
in unseen text (e.g. “a new algorithm for ... is
suggested in the current paper” or “I envisage le-
gal action”). In scientific discourse, such meta-
discourse (Myers, 1992; Hyland, 1998) abounds
and plays an important role in marking the dis-
course structure of the texts.
Finding these variants can be useful for many
text understanding tasks because semi-fixed cue
phrases act as linguistic markers indicating the im-
portance and/or the rhetorical role of some ad-
jacent text. For the summarisation of scientific
&apos;In contrast to standard work in discourse linguistics,
which mostly considers sentence connectives and adverbials
as cue phrases, our definition includes longer phrases, some-
times even entire sentences.
papers, cue phrases such as “Our paper deals
with... ” are commonly used as indicators of
extraction-worthiness of sentences (Kupiec et al.,
1995). Re-generative (rather than extractive) sum-
marisation methods may want to go further than
that and directly use the knowledge that a certain
sentence contains the particular research aim of a
paper, or a claimed gap in the literature. Similarly,
in the task of automatic routing of customer emails
and automatic answering of some of these, the de-
tection of threats of legal action could be useful.
However, systems that use cue phrases usually
rely on manually compiled lists, the acquisition
of which is time-consuming and error-prone and
results in cue phrases which are genre-specific.
Methods for finding cue phrases automatically in-
clude Hovy and Lin (1998) (using the ratio of
word frequency counts in summaries and their cor-
responding texts), Teufel (1998) (using the most
frequent n-grams), and Paice (1981) (using a pat-
tern matching grammar and a lexicon of manu-
ally collected equivalence classes). The main is-
sue with string-based pattern matching techniques
is that they cannot capture syntactic generalisa-
tions such as active/passive constructions, differ-
ent tenses and modification by adverbial, adjecti-
val or prepositional phrases, appositions and other
parenthetical material.
For instance, we may be looking for sentences
expressing the goal or main contribution of a pa-
per; Fig. 1 shows candidates of such sentences.
Cases a)–e), which do indeed describe the authors’
goal, display a wide range of syntactic variation.
</bodyText>
<figure confidence="0.9692571">
a) In this paper, we introduce a method for similarity-
based estimation of ...
b) We introduce and justify a method.. .
c) A method (described in section 1) is introduced
d) The method introduced here is a variation...
e) We wanted to introduce a method.. .
f) We do not introduce a method.. .
g) We introduce and adopt the method given in [1]...
h) Previously we introduced a similar method.. .
i) They introduce a similar method.. .
</figure>
<figureCaption confidence="0.967035">
Figure 1: Goal statements and syntactic variation – cor-
rect matches (a-e) and incorrect matches (f-i)
</figureCaption>
<page confidence="0.932669">
921
</page>
<note confidence="0.5805235">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 921–928,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.998275538461538">
Cases f)–i) in contrast are false matches: they do
not express the authors’ goals, although they are
superficially similar to the correct contexts. While
string-based approaches (Paice, 1981; Teufel,
1998) are too restrictive to cover the wide varia-
tion within the correct contexts, bag-of-words ap-
proaches such as Agichtein and Gravano’s (2000)
are too permissive and would miss many of the
distinctions between correct and incorrect con-
texts.
Lisacek et al. (2005) address the task of iden-
tifying “paradigm shift” sentences in the biomed-
ical literature, i.e. statements of thwarted expec-
tation. This task is somewhat similar to ours in
its definition by rhetorical context. Their method
goes beyond string-based matching: In order for a
sentence to qualify, the right set of concepts must
be present in a sentence, with any syntactic re-
lationship holding between them. Each concept
set is encoded as a fixed, manually compiled lists
of strings. Their method covers only one particu-
lar context (the paradigm shift one), whereas we
are looking for a method where many types of cue
phrases can be acquired. Whereas it relies on man-
ually assembled lists, we advocate data-driven ac-
quisition of new contexts. This is generally pre-
ferrable to manual definition, as language use is
changing, inventive and hard to predict and as
many of the relevant concepts in a domain may be
infrequent (cf. the formulation “be cursed”, which
was used in our corpus as a way of describing a
method’s problems). It also allows the acquisition
of cue phrases in new domains, where the exact
prevalent meta-discourse might not be known.
Riloff’s (1993) method for learning information
extraction (IE) patterns uses a syntactic parse and
correspondences between the text and filled MUC-
style templates to learn context in terms of lexico-
semantic patterns. However, it too requires sub-
stantial hand-crafted knowledge: 1500 filled tem-
plates as training material, and a lexicon of se-
mantic features for roughly 3000 nouns for con-
straint checking. Unsupervised methods for simi-
lar tasks include Agichtein and Gravano’s (2000)
work, which shows that clusters of vector-space-
based patterns can be successfully employed to
detect specific IE relationships (companies and
their headquarters), and Ravichandran and Hovy’s
(2002) algorithm for finding patterns for a Ques-
tion Answering (QA) task. Based on training ma-
terial in the shape of pairs of question and answer
terms – e.g., (e.g. {Mozart, 1756}), they learn the
</bodyText>
<figure confidence="0.986947333333333">
a) In this paper, we introduce a method for similarity-
based estimation of ...
b) Here, we present a similarity-based approach for esti-
mation of.. .
c) In this paper, we propose an algorithm which is ...
d) We will here define a technique for similarity-based.. .
</figure>
<figureCaption confidence="0.999147">
Figure 2: Context around cue phrases (lexical variants)
</figureCaption>
<bodyText confidence="0.9723845">
semantics holding between these terms (“birth
year”) via frequent string patterns occurring in the
context, such as “A was born in B”, by consider-
ing n-grams of all repeated substrings. What is
common to these three works is that bootstrapping
relies on constraints between the context external
to the extracted material and the extracted mate-
rial itself, and that the target extraction material is
defined by real-world relations.
Our task differs in that the cue phrases we ex-
tract are based on general rhetorical relations hold-
ing in all scientific discourse. Our approach for
finding semantically similar variants in an unsu-
pervised fashion relies on bootstrapping of seeds
from within the cue phrase. The assumption is that
every semi-fixed cue phrase contains at least two
main concepts whose syntax and semantics mutu-
ally constrain each other (e.g. verb and direct ob-
ject in phrases such as “(we) present an approach
for”). The expanded cue phrases are recognised
in various syntactic contexts using a parser2. Gen-
eral semantic constraints valid for groups of se-
mantically similar cue phrases are then applied to
model, e.g., the fact that it must be the authors who
present the method, not somebody else.
We demonstrate that such an approach is more
appropriate for our task than IE/QA bootstrapping
mechanisms based on cue phrase-external con-
text. Part of the reason for why normal boot-
strapping does not work for our phrases is the dif-
ficulty of finding negatives contexts, essential in
bootstrapping to evaluate the quality of the pat-
terns automatically. IE and QA approaches, due
to uniqueness assumptions of the real-world rela-
tions that these methods search for, have an auto-
matic definition of negative contexts by hard con-
straints (i.e., all contexts involving Mozart and any
other year are by definition of the wrong seman-
tics; so are all contexts involving Microsoft and
a city other than Redmond). As our task is not
grounded in real-world relations but in rhetorical
ones, constraints found in the context tend to be
2Thus, our task shows some parallels to work in para-
phrasing (Barzilay and Lee, 2002) and syntactic variant gen-
eration (Jacquemin et al., 1997), but the methods are very
different.
</bodyText>
<page confidence="0.996054">
922
</page>
<bodyText confidence="0.99996804">
soft rather than hard (cf. Fig 2): while it it possible
that strings such as “we” and “in this paper” occur
more often in the context of a given cue phrase,
they also occur in many other places in the paper
where the cue phrase is not present. Thus, it is
hard to define clear negative contexts for our task.
The novelty of our work is thus the new pattern
extraction task (finding variants of semi-fixed cue
phrases), a task for which it is hard to directly use
the context the patterns appear in, and an iterative
unsupervised bootstrapping algorithm for lexical
variants, using phrase-internal seeds and ranking
similar candidates based on relation strength be-
tween the seeds.
While our method is applicable to general cue
phrases, we demonstrate it here with transitive
verb–direct object pairs, namely a) cue phrases in-
troducing a new methodology (and thus the main
research goal of the scientific article; e.g. “In
this paper, we propose a novel algorithm... ”) –
we call those goal-type cue phrases; and b) cue
phrases indicating continuation of previous other
research (e.g. “Therefore, we adopt the approach
presented in [1]... ”) – continuation-type cue
phrases.
</bodyText>
<sectionHeader confidence="0.833689" genericHeader="method">
2 Lexical Bootstrapping Algorithm
</sectionHeader>
<bodyText confidence="0.97732035483871">
The task of this module is to find lexical vari-
ants of the components of the seed cue phrases.
Given the seed phrases “we introduce a method”
and “we propose a model”, the algorithm starts
by finding all direct objects of “introduce” in a
given corpus and, using an appropriate similar-
ity measure, ranks them according to their dis-
tributional similarity to the nouns “method” and
“model”. Subsequently, the noun “method” is used
to find transitive verbs and rank them according to
their similarity to “introduce” and “propose”. In
both cases, the ranking step retains variants that
preserve the semantics of the cue phrase (e.g. “de-
velop” and “approach”) and filters irrelevant terms
that change the phrase semantics (e.g. “need” and
“example”).
Stopping at this point would limit us to those
terms that co-occur with the seed words in the
training corpus. Therefore additional iterations us-
ing automatically generated verbs and nouns are
applied in order to recover more and more vari-
ants. The full algorithm is given in Fig. 3.
The algorithm requires corpus data for the steps
Hypothesize (producing a list of potential candi-
dates) and Rank (testing them for similarity). We
Input: Tuples {A1, A2, ... , A.1 and {B1, B2, ... , Bn1.
Initialisation: Set the concept-A reference set to
{A1, A2, ... , A,,,,1 and the concept-B reference set to
{B1, B2, ... , Bn1. Set the concept-A active element to A1
and the concept-B active element to B1.
Recursion:
</bodyText>
<listItem confidence="0.9383505">
1. Concept B retrieval:
(i) Hypothesize: Find terms in the corpus which
are in the desired relationship with the concept-A
active element (e.g. direct objects of a verb active
element). This results in the concept-B candidate
set.
(ii) Rank: Rank the concept-B candidate set using
a suitable ranking methodology that may make use
of the concept-B reference set. In this process, each
member of the candidate set is assigned a score.
(iii) Accumulate: Add the top s items of the
concept-B candidate set to the concept-B accumu-
</listItem>
<bodyText confidence="0.7438408">
lator list (based on empirical results, s is the rank of
the candidate set during the initial iteration and 50
for the remaining iterations). If an item is already
on the accumulator list, add its ranking score to the
existing item’s score.
</bodyText>
<listItem confidence="0.624414142857143">
2. Concept A retrieval: as above, with concepts A and
B swapped.
3. Updating active elements:
(i) Set the concept-B active element to the highest
ranked instance in the concept-B accumulator list
which has not been used as an active element be-
fore.
</listItem>
<bodyText confidence="0.971982608695652">
(ii) Set the concept-A active element to the highest
ranked instance in the concept-A accumulator list
which has not been used as an active element be-
fore.
Repeat steps 1-3 for k iterations
Output: top M words of concept-A (verb) accumulator list
and top N words of concept-B (noun) accumulator list
Reference set: a set of seed words which define the col-
lective semantics of the concept we are looking for in this
iteration
Active element: the instance of the concept used in the cur-
rent iteration for retrieving instances of the other concept.
If we are finding lexical variants of Concept A by exploit-
ing relationships between Concepts A and B, then the active
element is from Concept B.
Candidate set: the set of candidate terms for one concept
(eg. Concept A) obtained using an active element from the
other concept (eg. Concept B). The more semantically simi-
lar a term in the candidate set is to the members of the refer-
ence set, the higher its ranking should be. This set contains
verbs if the active element is a noun and vice versa.
Accumulator list: a sorted list that accumulates the ranked
members of the candidate set.
</bodyText>
<figureCaption confidence="0.966529">
Figure 3: Lexical variant bootstrapping algorithm
</figureCaption>
<bodyText confidence="0.999981">
estimate frequencies for the Rank step from the
written portion of the British National Corpus
(BNC, Burnard (1995)), 90 Million words. For
the Hypothesize step, we experiment with two
data sets: First, the scientific subsection of the
BNC (24 Million words), which we parse using
RASP (Briscoe and Carroll, 2002); we then ex-
amine the grammatical relations (GRs) for transi-
tive verb constructions, both in active and passive
voice. This method guarantees that we find al-
most all transitive verb constructions cleanly; Car-
roll et al. (1999) report an accuracy of .85 for
</bodyText>
<page confidence="0.996921">
923
</page>
<construct confidence="0.9562195">
DOs, Active: &amp;quot;AGENT STRING AUX active-verb-element DETERMINER * POSTMOD&amp;quot;
DOs, Passive: &amp;quot;DETERMINER * AUX active-verb-element element&amp;quot;
TVs, Active: &amp;quot;AGENT STRING AUX * DETERMINER active-noun- element POSTMOD&amp;quot;
TVs, Passive:&amp;quot;DET active-noun-element AUX * POSTMOD&amp;quot;
</construct>
<figureCaption confidence="0.983738">
Figure 4: Query patterns for retrieving direct objects (DOs) and transitive verbs (TVs) in the Hypothesize step.
</figureCaption>
<bodyText confidence="0.999973739130435">
newspaper articles for this relation. Second, in
order to obtain larger coverage and more current
data we also experiment with Google Scholar3, an
automatic web-based indexer of scientific litera-
ture (mainly peer-reviewed papers, technical re-
ports, books, pre-prints and abstracts). Google
Scholar snippets are often incomplete fragments
which cannot be parsed. For practical reasons, we
decided against processing the entire documents,
and obtain an approximation to direct objects and
transitive verbs with regular expressions over the
result snippets in both active and passive voice
(cf. Fig. 4), designed to be high-precision4. The
amount of data available from BNC and Google
Scholar is not directly comparable: harvesting
Google Scholar snippets for both active and pas-
sive constructions gives around 2000 sentences per
seed (Google Scholar returns up to 1000 results
per query), while the number of BNC sentences
containing seed words in active and passive form
varies from 11 (“formalism”) to 5467 (“develop”)
with an average of 1361 sentences for the experi-
mental seed pairs.
</bodyText>
<subsectionHeader confidence="0.477017">
Ranking
</subsectionHeader>
<bodyText confidence="0.999991">
Having obtained our candidate sets (either from
the scientific subsection of the BNC or from
Google Scholar), the members are ranked using
BNC frequencies. We investigate two ranking
methodologies: frequency-based and context-
based. Frequency-based ranking simply ranks
each member of the candidate set by how many
times it is retrieved together with the current active
element. Context-based ranking uses a similarity
measure for computing the scores, giving a higher
score to those words that share sufficiently similar
contexts with the members of the reference set.
We consider similarity measures in a vector space
defined either by a fixed window, by the sentence
window, or by syntactic relationships. The score
assigned to each word in the candidate set is the
sum of its semantic similarity values computed
with respect to each member in the reference set.
</bodyText>
<footnote confidence="0.9456376">
3http://scholar.google.com
4The capitalised words in these patterns are replaced by
actual words (e.g. AGENT STRING: We/I, DETERMINER:
a/ an/our), and the extracted words (indicated by “*”) are lem-
matised.
</footnote>
<bodyText confidence="0.9983289375">
Syntactic contexts, as opposed to window-based
contexts, constrain the context of a word to only
those words that are grammatically related to
it. We use verb-object relations in both active
and passive voice constructions as did Pereira et
al. (1993) and Lee (1999), among others. We
use the cosine similarity measure for window-
based contexts and the following commonly
used similarity measures for the syntactic vector
space: Hindle’s (1990) measure, the weighted Lin
measure (Wu and Zhou, 2003), the α-Skew diver-
gence measure (Lee, 1999), the Jensen-Shannon
(JS) divergence measure (Lin, 1991), Jaccard’s
coefficient (van Rijsbergen, 1979) and the Con-
fusion probability (Essen and Steinbiss, 1992).
The Jensen-Shannon measure JS (x1, x2) =
</bodyText>
<equation confidence="0.997657">
(Px log P
XE{X,X21 (YI ) g 2 ( (YX)+P 1)+P(Y X2))
</equation>
<bodyText confidence="0.999979086956522">
subsequently performed best for our task. We
compare the different ranking methodologies
and data sets with respect to a manually-defined
gold standard list of 20 goal-type verbs and 20
nouns. This list was manually assembled from
Teufel (1999); WordNet synonyms and other
plausible verbs and nouns found via Web searches
on scientific articles were added. We ensured by
searches on the ACL anthology that there is good
evidence that the gold-standard words indeed
occur in the right contexts, i.e. in goal statement
sentences. As we want to find similarity metrics
and data sources which result in accumulator lists
with many of these gold members at high ranks,
we need a measure that rewards exactly those
lists. We use non-interpolated Mean Average
Precision (MAP), a standard measure for eval-
uating ranked information retrieval runs, which
combines precision and recall and ranges from 0
to 15.
We use 8 pairs of 2-tuples as input (e.g. [in-
troduce, study] &amp; [approach, method]), randomly
selected from the gold standard list. MAP was cal-
</bodyText>
<equation confidence="0.447853">
5MAP = N Pj=1 APj = N PN, 1 M PM1 P (gi)
</equation>
<bodyText confidence="0.9666368">
where P (gi) = n&amp;quot;&apos; r&amp;quot;&apos;if gi is retrieved and 0 otherwise, N is
the number of seed combinations, M is the size of the golden
list, gi is the it member of the golden list and rij is its rank
in the retrieved list of combination j while nij is the number
of golden members found up to and including rank rij.
</bodyText>
<equation confidence="0.8769775">
P
y�Y
</equation>
<page confidence="0.958041">
924
</page>
<table confidence="0.9999032">
Ranking scheme BNC Google Scholar
Frequency-based 0.123 0.446
Sentence-window 0.200 0.344
Fixedsize-window 0.184 0.342
Hindle 0.293 0.416
Weighted Lin 0.358 0.509
α-Skew 0.361 0.486
Jensen-Shannon 0.404 0.550
Jaccard’s coef. 0.301 0.436
Confusion prob. 0.171 0.293
</table>
<figureCaption confidence="0.996425">
Figure 5: MAPs after the first iteration
</figureCaption>
<bodyText confidence="0.99998728125">
culated over the verbs and nouns retrieved using
our algorithm and averaged. Fig. 5 summarises the
MAP scores for the first iteration, where Google
Scholar significantly outperformed the BNC. The
best result for this iteration (MAP=.550) was
achieved by combining Google Scholar and the
Jensen-Shannon measure. The algorithm stops to
iterate when no more improvement can be ob-
tained, in this case after 4 iterations, resulting in
a final MAP of .619.
Although α-Skew outperforms the simpler mea-
sures in ranking nouns, its performance on verbs
is worse than the performance of Weighted Lin.
While Lee (1999) argues that α-Skew’s asymme-
try can be advantageous for nouns, this probably
does not hold for verbs: verb hierarchies have
much shallower structure than noun hierarchies
with most verbs concentrated on one level (Miller
et al., 1990). This would explain why JS, which
is symmetric compared to the α-Skew metric, per-
formed better in our experiments.
In the evaluation presented here we therefore
use Google Scholar data and the JS measure. An
additional improvement (MAP=.630) is achieved
when we incorporate a filter based on the follow-
ing hypothesis: goal-type verbs should be more
likely to have their direct objects preceded by in-
definite articles rather than definite articles or pos-
sessive determiners (because a new method is in-
troduced) whereas continuation-type verbs should
prefer definite articles with their direct objects (as
an existing method is involved).
</bodyText>
<sectionHeader confidence="0.930273" genericHeader="method">
3 Syntactic variants and semantic filters
</sectionHeader>
<bodyText confidence="0.9999115">
The syntactic variant extractor takes as its input
the raw text and the lists of verbs and nouns gen-
erated by the lexical bootstrapper. After RASP-
parsing the input text, all instances of the input
verbs are located and, based on the grammatical
relations output by RASP6, a set of relevant en-
</bodyText>
<footnote confidence="0.715001">
6The grammatical relations used are nsubj, dobj, iobj,
aux, argmod, detmod, ncmod and mod.
</footnote>
<bodyText confidence="0.999276461538462">
The agent of the verb (e.g., “We adopt...
... adopted by the author”), the agent’s determiner and
related adjectives.
The direct object of the verb, the object’s determiner
and adjectives, in addition to any post-modifiers (e.g.,
“... apply a method proposed by (1] ... ” , “... follow
an approach of (1] ... ”
Auxiliaries of the verb (e.g., “In a similar manner, we
may propose a ... ”)
Adverbial modification of the verb (e.g., “We have pre-
viously presented a ....”)
Prepositional phrases related to the verb (e.g., “In this
paper we present... ”, “... adopted from their work”)
</bodyText>
<figureCaption confidence="0.97182">
Figure 6: Grammatical relations considered
</figureCaption>
<bodyText confidence="0.976397142857143">
tities and modifiers for each verb is constructed,
grouped into five categories (cf. Fig. 6).
Next, semantic filters are applied to each of the
potential candidates (represented by the extracted
entities and modifiers), and a fitness score is cal-
culated. These constraints encode semantic princi-
ples that will apply to all cue phrases of that rhetor-
ical category. Examples for constraints are: if
work is referred to as being done in previous own
work, it is probably not a goal statement; the work
in a goal statement must be presented here or in the
current paper (the concept of ‘here-ness”); and the
agents of a goal statement have to be the authors,
not other people. While these filters are manually
defined, they are modular, encode general princi-
ples, and can be combined to express a wide range
of rhetorical contexts. We verified that around 20
semantic constraints are enough to cover a large
sets of different cue phrases (the 1700 cue phrases
from Teufel (1999)), though not all of these are
implemented yet.
A nice side-effect of our approach is the simple
characterisation of a cue phrase (by a syntactic re-
lationship, some seed words for each concept, and
some general, reusable semantic constraints). This
characterisation is more informative and specific
than string-based approaches, yet it has the poten-
tial for generalisation (useful if the cue phrases are
ever manually assessed and put into a lexicon).
Fig. 7 shows successful extraction examples
from our corpus7, illustrating the difficulty of
the task: the system correctly identified sen-
tences with syntactically complex goal-type and
continuation-type cue phrases, and correctly re-
jected deceptive variants$.
</bodyText>
<footnote confidence="0.7952435">
7Numbers after examples give CmpLg archive numbers,
followed by sentence numbers according to our preprocess-
ing.
8The seeds in this example were [analyse, present] &amp; [ar-
chitecture, method] (for goal) and [improve, adopt] &amp; [model,
method] (for continuation).
</footnote>
<page confidence="0.990583">
925
</page>
<table confidence="0.3712475">
Correctly found:
Goal-type:
</table>
<tableCaption confidence="0.886687444444444">
What we aim in this paper is to propose a paradigm
that enables partial/local generation through de-
compositions and reorganizations of tentative local
structures. (9411021, S-5)
Continuation-type:
In this paper we have discussed how the lexico-
graphical concept of lexical functions, introduced
by Melcuk to describe collocations, can be used as
an interlingual device in the machine translation of
</tableCaption>
<figure confidence="0.4799029">
such structures. (9410009, S-126)
Correctly rejected:
Goal-type:
Perhaps the method proposed by Pereira et al.
(1993) is the most relevant in our context.
(9605014, S-76)
Continuation-type:
Neither Kamp nor Kehler extend their copying/ sub-
stitution mechanism to anything besides pronouns,
as we have done. (9502014, S-174)
</figure>
<figureCaption confidence="0.997761">
Figure 7: Sentences correctly processed by our system
</figureCaption>
<sectionHeader confidence="0.856246" genericHeader="method">
4 Gold standard evaluation
</sectionHeader>
<bodyText confidence="0.959590684210527">
We evaluated the quality of the extracted phrases
in two ways: by comparing our system output to
gold standard annotation, and by human judge-
ment of the quality of the returned sentences. In
both cases bootstrapping was done using the seed
tuples [analyse, present] &amp; [architecture, method].
For the gold standard-evaluation, we ran our sys-
tem on a test set of 121 scientific articles drawn
from the CmpLg corpus (Teufel, 1999) – en-
tirely different texts from the ones the system was
trained on. Documents were manually annotated
by the second author for (possibly more than one)
goal-type sentence; annotation of that type has
been previously shown to be reliable at K=.71
(Teufel, 1999). Our evaluation recorded how often
the system’s highest-ranked candidate was indeed
a goal-type sentence; as this is a precision-critical
task, we do not measure recall here.
We compared our system against our reimple-
mentation of Ravichandran and Hovy’s (2002)
paraphrase learning. The seed words were of the
form {goal-verb, goal-noun}, and we submitted
each of the 4 combinations of the seed pair to
Google Scholar. From the top 1000 documents for
each query, we harvested 3965 sentences contain-
ing both the goal-verb and the goal-noun. By con-
sidering all possible substrings, an extensive list of
candidate patterns was assembled. Patterns with
single occurrences were discarded, leaving a list
of 5580 patterns (examples in Fig. 8). In order
to rank the patterns by precision, the goal-verbs
were submitted as queries and the top 1000 doc-
uments were downloaded for each. From these,
we &lt;verb&gt; a &lt;noun&gt; for
of a new &lt;noun&gt; to &lt;verb&gt; the
In this section, we &lt;verb&gt; the &lt;noun&gt; of
the &lt;noun&gt; &lt;verb&gt; in this paper
is to &lt;verb&gt; the &lt;noun&gt; after
</bodyText>
<figureCaption confidence="0.5961155">
Figure 8: Examples of patterns extracted using
Ravichandran and Hovy’s (2002) method
</figureCaption>
<table confidence="0.996868666666667">
Method Correct
sentences
Our system with bootstrapping 88 (73%)
Ravichandran and Hovy (2002) 58 (48%)
Our system, no bootstrapping, WordNet 50 (41%)
Our system, no bootstrapping, seeds only 37 (30%)
</table>
<figureCaption confidence="0.980107">
Figure 9: Gold standard evaluation: results
</figureCaption>
<bodyText confidence="0.940257076923077">
the precision of each pattern was calculated by di-
viding the number of strings matching the pattern
instantiated with both the goal-verb and all Word-
Net synonyms of the goal-noun, by the number
of strings matching the patterns instantiated with
the goal-verb only. An important point here is
that while the tight semantic coupling between the
question and answer terms in the original method
accurately identifies all the positive and negative
examples, we can only approximate this by using a
sensible synonym set for the seed goal-nouns. For
each document in the test set, the sentence contain-
ing the pattern with the highest precision (if any)
was extracted as the goal sentence.
We also compared our system to two baselines.
We replaced the lists obtained from the lexical
bootstrapping module with a) just the seed pair
and b) the seed pair and all the WordNet synonyms
of the components of the seed pair9.
The results of these experiments are given
in Fig. 9. All differences are statistically
significant with the x2 test at p=.01 (except
those between Ravichandran/Hovy and our non-
bootstrapping/WordNet system). Our bootstrap-
ping system outperforms the Ravichandran and
Hovy algorithm by 34%. This is not surprising,
because this algorithm was not designed to per-
form well in tasks where there is no clear negative
context. The results also show that bootstrapping
outperforms a general thesaurus such as WordNet.
Out of the 33 articles where our system’s
favourite was not an annotated goal-type sentence,
only 15 are due to bootstrapping errors (i.e., to
an incorrect ranking of the lexical variants), corre-
9Bootstrapping should in principle do better than a the-
saurus, as some of our correctly identified variants are not
true synonyms (e.g., theory vs. method), and as noise through
overgeneration of unrelated senses might occur unless auto-
matic word sense diambiguation is performed.
</bodyText>
<page confidence="0.984422">
926
</page>
<table confidence="0.99455975">
System chose: but should have chosen:
derive set compare model
illustrate algorithm present formalisation
discuss measures present variations
describe modifications
propose measures
accommodate material describe approach
examine material present study
Figure 10: Wrong bootstrapping decisions
Ceiling System Baseline
Exp. A 3.91 3.08 1.58
Exp.B 4.33 3.67 2.50
</table>
<figureCaption confidence="0.951431">
Figure 11: Extrinsic evaluation: judges’ scores
</figureCaption>
<bodyText confidence="0.999938363636364">
sponding to a 88% accuracy of the bootstrapping
module. Examples from those 15 error cases are
given in Fig. 10. The other errors were due to the
cue phrase not being a transitive verb–direct ob-
ject pattern (e.g. we show that, our goal is and
we focus on), so the system could not have found
anything (11 cases, or an 80% accuracy), ungram-
matical English or syntactic construction too com-
plex, resulting in a lack of RASP detection of the
crucial grammatical relation (2) and failure of the
semantic filter to catch non-goal contexts (5).
</bodyText>
<sectionHeader confidence="0.985813" genericHeader="method">
5 Human evaluation
</sectionHeader>
<bodyText confidence="0.999895088888889">
We next perform two human experiments to in-
directly evaluate the quality of the automatically
generated cue phrase variants. Given an abstract of
an article and a sentence extracted from the article,
judges are asked to assign a score ranging from 1
(low) to 5 (high) depending on how well the sen-
tence expresses the goal of that article (Exp. A),
or the continuation of previous work (Exp. B).
Each experiment involves 24 articles drawn ran-
domly from a subset of 80 articles in the CmpLg
corpus that contain manual annotation for goal-
type and continuation-type sentences. The experi-
ments use three external judges (graduate students
in computational linguistics), and a Latin Square
experimental design with three conditions: Base-
line (see below), System-generated and Ceiling
(extracted from the gold standard annotation used
in Teufel (1999)). Judges were not told how the
sentences were generated, and no judge saw an
item in more than one condition.
The baseline for Experiment A was a random
selection of sentences with the highest TF*IDF
scores, because goal-type sentences typically con-
tain many content-words. The baseline for ex-
periment B (continuation-type) were randomly se-
lected sentences containing citations, because they
often co-occur with statements of continuation. In
both cases, the length of the baseline sentence was
controlled for by the average lengths of the gold
standard and the system-extracted sentences in the
document.
Fig. 11 shows that judges gave an average score
of 3.08 to system-extracted sentences in Exp. A,
compared with a baseline of 1.58 and a ceiling of
3.9110; in Exp. B, the system scored 3.67, with
a higher baseline of 2.50 and a ceiling of 4.33.
According to the Wilcoxon signed-ranks test at
α = .01, the system is indistinguishable from
the gold standard, but significantly different from
the baseline, in both experiments. Although this
study is on a small scale, it indicates that humans
judged sentences obtained with our method as al-
most equally characteristic of their rhetorical func-
tion as human-chosen sentences, and much better
than non-trivial baselines.
</bodyText>
<sectionHeader confidence="0.999579" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99719975">
In this paper we have investigated the automatic
acquisition of semi-fixed cue phrases as a boot-
strapping task which requires very little manual
input for each cue phrase and yet generalises to
a wide range of syntactic and lexical variants in
running text. Our system takes a few seeds of the
type of cue phrase as input, and bootstraps lex-
ical variants from a large corpus. It filters out
many semantically invalid contexts, and finds cue
phrases in various syntactic variants. The system
achieved 80% precision of goal-type phrases of
the targeted syntactic shape (88% if only the boot-
strapping module is evaluated), and good quality
ratings from human judges. We found Google
Scholar to perform better than BNC as source for
finding hypotheses for lexical variants, which may
be due to the larger amount of data available to
Google Scholar. This seems to outweigh the dis-
advantage of only being able to use POS patterns
with Google Scholar, as opposed to robust parsing
with the BNC.
In the experiments reported, we bootstrap only
from one type of cue phrase (transitive verbs and
direct objects). This type covers a large propor-
tion of the cue phrases needed practically, but our
algorithm should in principle work for any kind of
semi-fixed cue phrase, as long as they have two
core concepts and a syntactic and semantic
</bodyText>
<footnote confidence="0.875058333333333">
10This score seems somewhat low, considering that these
were the best sentences available as goal descriptions, accord-
ing to the gold standard.
</footnote>
<page confidence="0.975359">
927
</page>
<table confidence="0.983184583333333">
CUE PHRASE: “(previous) methods fail” (Subj–Verb)
VARIANTS SEED 1: methodology, approach,
technique...
VARIANTS SEED 2: be cursed, be incapable of, be
restricted to, be troubled, degrade, fall prey to, .. .
CUE PHRASE: “advantage over previous methods”
(NP–PP postmod + adj–noun premod.)
VARIANTS SEED 1: benefit, breakthrough, edge,
improvement, innovation, success, triumph...
VARIANTS SEED 2: available, better-known,
cited, classic, common, conventional, current, cus-
tomary, established, existing, extant,...
</table>
<figureCaption confidence="0.904461">
Figure 12: Cues with other syntactic relationships
</figureCaption>
<bodyText confidence="0.999955045454545">
relation between them. Examples for such other
types of phrases are given in Fig. 12; the second
cue phrase involves a complex syntactic relation-
ship between the two seeds (or possibly it could
be considered as a cue phrase with three seeds).
We will next investigate if the positive results pre-
sented here can be maintained for other syntactic
contexts and for cue phrases with more than two
seeds.
The syntactic variant extractor could be en-
hanced in various ways, eg. by resolving anaphora
in cue phrases. A more sophisticated model of
syntactically weighted vector space (Pado and La-
pata, 2003) may help improve the lexical acquisi-
tion phase. Another line for future work is boot-
strapping meaning across cue phrases within the
same rhetorical class, e.g. to learn that we propose
a method for X and we aim to do X are equivalent.
As some papers will contain both variants of the
cue phrase, with very similar material (X) in the
vicinity, they could be used as starting point for
experiments to validate cue phrase equivalence.
</bodyText>
<sectionHeader confidence="0.999091" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9990238">
This work was funded by the EPSRC projects CIT-
RAZ (GR/S27832/01, “Rhetorical Citation Maps
and Domain-independent Argumentative Zon-
ing”) and SCIBORG (EP/C010035/1, “Extracting
the Science from Scientific Publications”).
</bodyText>
<sectionHeader confidence="0.999355" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999289324675325">
Eugene Agichtein and Luis Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections. In Pro-
ceedings of the 5th ACM International Conference on Dig-
ital Libraries.
Regina Barzilay and Lillian Lee. 2002. Bootstrapping lex-
ical choice via multiple-sequence alignment. In Proc. of
EMNLP.
Ted Briscoe and John Carroll. 2002. Robust accurate statis-
tical annotation of general text. In Proc. of LREC.
Lou Burnard, 1995. Users Reference Guide, British National
Corpus Version 1.0. Oxford University, UK.
John Carroll, Guido Minnen, and Ted Briscoe. 1999. Cor-
pus annotation for parser evaluation. In Proceedings
of Linguistically Interpreted Corpora (LINC-99), EACL-
workshop.
Ute Essen and Volker Steinbiss. 1992. Co-occurrence
smoothing for stochastic language modelling. In Proc. of
ICASSP.
Donald Hindle. 1990. Noun classification from predicate-
argument structures. In Proc. of the ACL.
Edvard Hovy and Chin-Yew Lin. 1998. Automated text sum-
marization and the Summarist system. In Proc. of the TIP-
STER Text Program.
Ken Hyland. 1998. Persuasion and context: The pragmat-
ics of academic metadiscourse. Journal of Pragmatics,
30(4):437–455.
Christian Jacquemin, Judith Klavans, and Evelyn Tzouker-
mann. 1997. Expansion of multi-word terms for indexing
and retrieval using morphology and syntax. In Proc. of the
ACL.
Julian Kupiec, Jan O. Pedersen, and Francine Chen. 1995. A
trainable document summarizer. In Proc. of SIGIR-95.
Lillian Lee. 1999. Measures of distributional similarity. In
Proc. of the ACL.
Jianhua Lin. 1991. Divergence measures based on the Shan-
non entropy. IEEE transactions on Information Theory,
37(1):145–151.
Frederique Lisacek, Christine Chichester, Aaron Kaplan, and
Sandor Agnes. 2005. Discovering paradigm shift patterns
in biomedical abstracts: Application to neurodegenerative
diseases. In Proc. of the SMBM.
George Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine Miller. 1990. Five papers
on WordNet. Technical report, Cognitive Science Labora-
tory, Princeton University.
Greg Myers. 1992. In this paper we report...—speech acts
and scientific facts. Journal of Pragmatics, 17(4):295–
313.
Sebastian Pado and Mirella Lapata. 2003. Constructing se-
mantic space models from parsed corpora. In Proc. of
ACL.
Chris D. Paice. 1981. The automatic generation of lit-
erary abstracts: an approach based on the identifica-
tion of self-indicating phrases. In Robert Norman Oddy,
Stephen E. Robertson, Cornelis Joost van Rijsbergen, and
P. W. Williams, editors, Information Retrieval Research,
Butterworth, London, UK.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Dis-
tributional clustering of English words. In Proc. of the
ACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system. In
Proc. of the ACL.
Ellen Riloff. 1993. Automatically constructing a dictionary
for information extraction tasks. In Proc. of AAAI-93.
Simone Teufel. 1998. Meta-discourse markers and problem-
structuring in scientific articles. In Proceedings of the
ACL-98 Workshop on Discourse Structure and Discourse
Markers.
Simone Teufel. 1999. Argumentative Zoning: Information
Extraction from Scientific Text. Ph.D. thesis, School of
Cognitive Science, University of Edinburgh, UK.
Cornelis Joost van Rijsbergen. 1979. Information Retrieval.
Butterworth, London, UK, 2nd edition.
Hua Wu and Ming Zhou. 2003. Synonymous collocation
extraction using translation information. In Proc. of the
ACL.
</reference>
<page confidence="0.996989">
928
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.900965">
<title confidence="0.993914">A Bootstrapping Approach to Unsupervised Detection of Cue Phrase Variants</title>
<author confidence="0.999957">Rashid M Abdalla</author>
<author confidence="0.999957">Simone Teufel</author>
<affiliation confidence="0.999956">Computer Laboratory, University of Cambridge</affiliation>
<address confidence="0.99746">15 JJ Thomson Avenue, Cambridge CB3 OFD, UK</address>
<email confidence="0.96215">rma33@cam.ac.uk,sht25@cam.ac.uk</email>
<abstract confidence="0.997234894736842">We investigate the unsupervised detection semi-fixed cue phrases such as proposes a novel approach... from unseen text, on the basis of only a handful of seed cue phrases with the desired semantics. The problem, in contrast to bootstrapping approaches for Question Answering and Information Extraction, is that it is hard to find a constraining context for occurrences of semi-fixed cue phrases. Our method uses components of the cue phrase itself, rather than external context, to bootstrap. It successfully excludes phrases which are different from the target semantics, but which look superficially similar. The method achieves 88% accuracy, outperforming standard bootstrapping approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Luis Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the 5th ACM International Conference on Digital Libraries.</booktitle>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In Proceedings of the 5th ACM International Conference on Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Bootstrapping lexical choice via multiple-sequence alignment.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="9237" citStr="Barzilay and Lee, 2002" startWordPosition="1478" endWordPosition="1481">otstrapping to evaluate the quality of the patterns automatically. IE and QA approaches, due to uniqueness assumptions of the real-world relations that these methods search for, have an automatic definition of negative contexts by hard constraints (i.e., all contexts involving Mozart and any other year are by definition of the wrong semantics; so are all contexts involving Microsoft and a city other than Redmond). As our task is not grounded in real-world relations but in rhetorical ones, constraints found in the context tend to be 2Thus, our task shows some parallels to work in paraphrasing (Barzilay and Lee, 2002) and syntactic variant generation (Jacquemin et al., 1997), but the methods are very different. 922 soft rather than hard (cf. Fig 2): while it it possible that strings such as “we” and “in this paper” occur more often in the context of a given cue phrase, they also occur in many other places in the paper where the cue phrase is not present. Thus, it is hard to define clear negative contexts for our task. The novelty of our work is thus the new pattern extraction task (finding variants of semi-fixed cue phrases), a task for which it is hard to directly use the context the patterns appear in, a</context>
</contexts>
<marker>Barzilay, Lee, 2002</marker>
<rawString>Regina Barzilay and Lillian Lee. 2002. Bootstrapping lexical choice via multiple-sequence alignment. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="14506" citStr="Briscoe and Carroll, 2002" startWordPosition="2366" endWordPosition="2369"> term in the candidate set is to the members of the reference set, the higher its ranking should be. This set contains verbs if the active element is a noun and vice versa. Accumulator list: a sorted list that accumulates the ranked members of the candidate set. Figure 3: Lexical variant bootstrapping algorithm estimate frequencies for the Rank step from the written portion of the British National Corpus (BNC, Burnard (1995)), 90 Million words. For the Hypothesize step, we experiment with two data sets: First, the scientific subsection of the BNC (24 Million words), which we parse using RASP (Briscoe and Carroll, 2002); we then examine the grammatical relations (GRs) for transitive verb constructions, both in active and passive voice. This method guarantees that we find almost all transitive verb constructions cleanly; Carroll et al. (1999) report an accuracy of .85 for 923 DOs, Active: &amp;quot;AGENT STRING AUX active-verb-element DETERMINER * POSTMOD&amp;quot; DOs, Passive: &amp;quot;DETERMINER * AUX active-verb-element element&amp;quot; TVs, Active: &amp;quot;AGENT STRING AUX * DETERMINER active-noun- element POSTMOD&amp;quot; TVs, Passive:&amp;quot;DET active-noun-element AUX * POSTMOD&amp;quot; Figure 4: Query patterns for retrieving direct objects (DOs) and transitive ve</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Ted Briscoe and John Carroll. 2002. Robust accurate statistical annotation of general text. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>Users Reference Guide, British National Corpus Version 1.0.</title>
<date>1995</date>
<publisher>Oxford University, UK.</publisher>
<contexts>
<context position="14308" citStr="Burnard (1995)" startWordPosition="2336" endWordPosition="2337">cept B. Candidate set: the set of candidate terms for one concept (eg. Concept A) obtained using an active element from the other concept (eg. Concept B). The more semantically similar a term in the candidate set is to the members of the reference set, the higher its ranking should be. This set contains verbs if the active element is a noun and vice versa. Accumulator list: a sorted list that accumulates the ranked members of the candidate set. Figure 3: Lexical variant bootstrapping algorithm estimate frequencies for the Rank step from the written portion of the British National Corpus (BNC, Burnard (1995)), 90 Million words. For the Hypothesize step, we experiment with two data sets: First, the scientific subsection of the BNC (24 Million words), which we parse using RASP (Briscoe and Carroll, 2002); we then examine the grammatical relations (GRs) for transitive verb constructions, both in active and passive voice. This method guarantees that we find almost all transitive verb constructions cleanly; Carroll et al. (1999) report an accuracy of .85 for 923 DOs, Active: &amp;quot;AGENT STRING AUX active-verb-element DETERMINER * POSTMOD&amp;quot; DOs, Passive: &amp;quot;DETERMINER * AUX active-verb-element element&amp;quot; TVs, Ac</context>
</contexts>
<marker>Burnard, 1995</marker>
<rawString>Lou Burnard, 1995. Users Reference Guide, British National Corpus Version 1.0. Oxford University, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Ted Briscoe</author>
</authors>
<title>Corpus annotation for parser evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of Linguistically Interpreted Corpora (LINC-99),</booktitle>
<pages>EACLworkshop.</pages>
<contexts>
<context position="14732" citStr="Carroll et al. (1999)" startWordPosition="2402" endWordPosition="2406">d members of the candidate set. Figure 3: Lexical variant bootstrapping algorithm estimate frequencies for the Rank step from the written portion of the British National Corpus (BNC, Burnard (1995)), 90 Million words. For the Hypothesize step, we experiment with two data sets: First, the scientific subsection of the BNC (24 Million words), which we parse using RASP (Briscoe and Carroll, 2002); we then examine the grammatical relations (GRs) for transitive verb constructions, both in active and passive voice. This method guarantees that we find almost all transitive verb constructions cleanly; Carroll et al. (1999) report an accuracy of .85 for 923 DOs, Active: &amp;quot;AGENT STRING AUX active-verb-element DETERMINER * POSTMOD&amp;quot; DOs, Passive: &amp;quot;DETERMINER * AUX active-verb-element element&amp;quot; TVs, Active: &amp;quot;AGENT STRING AUX * DETERMINER active-noun- element POSTMOD&amp;quot; TVs, Passive:&amp;quot;DET active-noun-element AUX * POSTMOD&amp;quot; Figure 4: Query patterns for retrieving direct objects (DOs) and transitive verbs (TVs) in the Hypothesize step. newspaper articles for this relation. Second, in order to obtain larger coverage and more current data we also experiment with Google Scholar3, an automatic web-based indexer of scientific li</context>
</contexts>
<marker>Carroll, Minnen, Briscoe, 1999</marker>
<rawString>John Carroll, Guido Minnen, and Ted Briscoe. 1999. Corpus annotation for parser evaluation. In Proceedings of Linguistically Interpreted Corpora (LINC-99), EACLworkshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ute Essen</author>
<author>Volker Steinbiss</author>
</authors>
<title>Co-occurrence smoothing for stochastic language modelling.</title>
<date>1992</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="18006" citStr="Essen and Steinbiss, 1992" startWordPosition="2894" endWordPosition="2897">text of a word to only those words that are grammatically related to it. We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: Hindle’s (1990) measure, the weighted Lin measure (Wu and Zhou, 2003), the α-Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard’s coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). The Jensen-Shannon measure JS (x1, x2) = (Px log P XE{X,X21 (YI ) g 2 ( (YX)+P 1)+P(Y X2)) subsequently performed best for our task. We compare the different ranking methodologies and data sets with respect to a manually-defined gold standard list of 20 goal-type verbs and 20 nouns. This list was manually assembled from Teufel (1999); WordNet synonyms and other plausible verbs and nouns found via Web searches on scientific articles were added. We ensured by searches on the ACL anthology that there is good evidence that the gold-standard words indeed occur in the right contexts, i.e. in goal </context>
</contexts>
<marker>Essen, Steinbiss, 1992</marker>
<rawString>Ute Essen and Volker Steinbiss. 1992. Co-occurrence smoothing for stochastic language modelling. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
</authors>
<title>Noun classification from predicateargument structures.</title>
<date>1990</date>
<booktitle>In Proc. of the ACL.</booktitle>
<marker>Hindle, 1990</marker>
<rawString>Donald Hindle. 1990. Noun classification from predicateargument structures. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edvard Hovy</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Automated text summarization and the Summarist system.</title>
<date>1998</date>
<booktitle>In Proc. of the TIPSTER Text Program.</booktitle>
<contexts>
<context position="2798" citStr="Hovy and Lin (1998)" startWordPosition="434" endWordPosition="437">marisation methods may want to go further than that and directly use the knowledge that a certain sentence contains the particular research aim of a paper, or a claimed gap in the literature. Similarly, in the task of automatic routing of customer emails and automatic answering of some of these, the detection of threats of legal action could be useful. However, systems that use cue phrases usually rely on manually compiled lists, the acquisition of which is time-consuming and error-prone and results in cue phrases which are genre-specific. Methods for finding cue phrases automatically include Hovy and Lin (1998) (using the ratio of word frequency counts in summaries and their corresponding texts), Teufel (1998) (using the most frequent n-grams), and Paice (1981) (using a pattern matching grammar and a lexicon of manually collected equivalence classes). The main issue with string-based pattern matching techniques is that they cannot capture syntactic generalisations such as active/passive constructions, different tenses and modification by adverbial, adjectival or prepositional phrases, appositions and other parenthetical material. For instance, we may be looking for sentences expressing the goal or m</context>
</contexts>
<marker>Hovy, Lin, 1998</marker>
<rawString>Edvard Hovy and Chin-Yew Lin. 1998. Automated text summarization and the Summarist system. In Proc. of the TIPSTER Text Program.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Hyland</author>
</authors>
<title>Persuasion and context: The pragmatics of academic metadiscourse.</title>
<date>1998</date>
<journal>Journal of Pragmatics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1459" citStr="Hyland, 1998" startWordPosition="227" endWordPosition="228"> superficially similar. The method achieves 88% accuracy, outperforming standard bootstrapping approaches. 1 Introduction Cue phrases such as “This paper proposes a novel approach to... ”, “no method for ... exists” or even “you will hear from my lawyer” are semi-fixed in that they constitute a formulaic pattern with a clear semantics, but with syntactic and lexical variations which are hard to predict and thus hard to detect in unseen text (e.g. “a new algorithm for ... is suggested in the current paper” or “I envisage legal action”). In scientific discourse, such metadiscourse (Myers, 1992; Hyland, 1998) abounds and plays an important role in marking the discourse structure of the texts. Finding these variants can be useful for many text understanding tasks because semi-fixed cue phrases act as linguistic markers indicating the importance and/or the rhetorical role of some adjacent text. For the summarisation of scientific &apos;In contrast to standard work in discourse linguistics, which mostly considers sentence connectives and adverbials as cue phrases, our definition includes longer phrases, sometimes even entire sentences. papers, cue phrases such as “Our paper deals with... ” are commonly us</context>
</contexts>
<marker>Hyland, 1998</marker>
<rawString>Ken Hyland. 1998. Persuasion and context: The pragmatics of academic metadiscourse. Journal of Pragmatics, 30(4):437–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Jacquemin</author>
<author>Judith Klavans</author>
<author>Evelyn Tzoukermann</author>
</authors>
<title>Expansion of multi-word terms for indexing and retrieval using morphology and syntax.</title>
<date>1997</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="9295" citStr="Jacquemin et al., 1997" startWordPosition="1487" endWordPosition="1490">tically. IE and QA approaches, due to uniqueness assumptions of the real-world relations that these methods search for, have an automatic definition of negative contexts by hard constraints (i.e., all contexts involving Mozart and any other year are by definition of the wrong semantics; so are all contexts involving Microsoft and a city other than Redmond). As our task is not grounded in real-world relations but in rhetorical ones, constraints found in the context tend to be 2Thus, our task shows some parallels to work in paraphrasing (Barzilay and Lee, 2002) and syntactic variant generation (Jacquemin et al., 1997), but the methods are very different. 922 soft rather than hard (cf. Fig 2): while it it possible that strings such as “we” and “in this paper” occur more often in the context of a given cue phrase, they also occur in many other places in the paper where the cue phrase is not present. Thus, it is hard to define clear negative contexts for our task. The novelty of our work is thus the new pattern extraction task (finding variants of semi-fixed cue phrases), a task for which it is hard to directly use the context the patterns appear in, and an iterative unsupervised bootstrapping algorithm for l</context>
</contexts>
<marker>Jacquemin, Klavans, Tzoukermann, 1997</marker>
<rawString>Christian Jacquemin, Judith Klavans, and Evelyn Tzoukermann. 1997. Expansion of multi-word terms for indexing and retrieval using morphology and syntax. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan O Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proc. of SIGIR-95.</booktitle>
<contexts>
<context position="2135" citStr="Kupiec et al., 1995" startWordPosition="328" endWordPosition="331">ourse structure of the texts. Finding these variants can be useful for many text understanding tasks because semi-fixed cue phrases act as linguistic markers indicating the importance and/or the rhetorical role of some adjacent text. For the summarisation of scientific &apos;In contrast to standard work in discourse linguistics, which mostly considers sentence connectives and adverbials as cue phrases, our definition includes longer phrases, sometimes even entire sentences. papers, cue phrases such as “Our paper deals with... ” are commonly used as indicators of extraction-worthiness of sentences (Kupiec et al., 1995). Re-generative (rather than extractive) summarisation methods may want to go further than that and directly use the knowledge that a certain sentence contains the particular research aim of a paper, or a claimed gap in the literature. Similarly, in the task of automatic routing of customer emails and automatic answering of some of these, the detection of threats of legal action could be useful. However, systems that use cue phrases usually rely on manually compiled lists, the acquisition of which is time-consuming and error-prone and results in cue phrases which are genre-specific. Methods fo</context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan O. Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Proc. of SIGIR-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="17572" citStr="Lee (1999)" startWordPosition="2833" endWordPosition="2834">to each word in the candidate set is the sum of its semantic similarity values computed with respect to each member in the reference set. 3http://scholar.google.com 4The capitalised words in these patterns are replaced by actual words (e.g. AGENT STRING: We/I, DETERMINER: a/ an/our), and the extracted words (indicated by “*”) are lemmatised. Syntactic contexts, as opposed to window-based contexts, constrain the context of a word to only those words that are grammatically related to it. We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: Hindle’s (1990) measure, the weighted Lin measure (Wu and Zhou, 2003), the α-Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard’s coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). The Jensen-Shannon measure JS (x1, x2) = (Px log P XE{X,X21 (YI ) g 2 ( (YX)+P 1)+P(Y X2)) subsequently performed best for our task. We compare the different rankin</context>
<context position="20410" citStr="Lee (1999)" startWordPosition="3302" endWordPosition="3303">ion culated over the verbs and nouns retrieved using our algorithm and averaged. Fig. 5 summarises the MAP scores for the first iteration, where Google Scholar significantly outperformed the BNC. The best result for this iteration (MAP=.550) was achieved by combining Google Scholar and the Jensen-Shannon measure. The algorithm stops to iterate when no more improvement can be obtained, in this case after 4 iterations, resulting in a final MAP of .619. Although α-Skew outperforms the simpler measures in ranking nouns, its performance on verbs is worse than the performance of Weighted Lin. While Lee (1999) argues that α-Skew’s asymmetry can be advantageous for nouns, this probably does not hold for verbs: verb hierarchies have much shallower structure than noun hierarchies with most verbs concentrated on one level (Miller et al., 1990). This would explain why JS, which is symmetric compared to the α-Skew metric, performed better in our experiments. In the evaluation presented here we therefore use Google Scholar data and the JS measure. An additional improvement (MAP=.630) is achieved when we incorporate a filter based on the following hypothesis: goal-type verbs should be more likely to have t</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence measures based on the Shannon entropy.</title>
<date>1991</date>
<journal>IEEE transactions on Information Theory,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="17902" citStr="Lin, 1991" startWordPosition="2882" endWordPosition="2883">e lemmatised. Syntactic contexts, as opposed to window-based contexts, constrain the context of a word to only those words that are grammatically related to it. We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: Hindle’s (1990) measure, the weighted Lin measure (Wu and Zhou, 2003), the α-Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard’s coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). The Jensen-Shannon measure JS (x1, x2) = (Px log P XE{X,X21 (YI ) g 2 ( (YX)+P 1)+P(Y X2)) subsequently performed best for our task. We compare the different ranking methodologies and data sets with respect to a manually-defined gold standard list of 20 goal-type verbs and 20 nouns. This list was manually assembled from Teufel (1999); WordNet synonyms and other plausible verbs and nouns found via Web searches on scientific articles were added. We ensured by searches on the ACL anthology th</context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Jianhua Lin. 1991. Divergence measures based on the Shannon entropy. IEEE transactions on Information Theory, 37(1):145–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederique Lisacek</author>
<author>Christine Chichester</author>
<author>Aaron Kaplan</author>
<author>Sandor Agnes</author>
</authors>
<title>Discovering paradigm shift patterns in biomedical abstracts: Application to neurodegenerative diseases.</title>
<date>2005</date>
<booktitle>In Proc. of the SMBM.</booktitle>
<contexts>
<context position="4775" citStr="Lisacek et al. (2005)" startWordPosition="747" endWordPosition="750">erence on Computational Linguistics and 44th Annual Meeting of the ACL, pages 921–928, Sydney, July 2006. c�2006 Association for Computational Linguistics Cases f)–i) in contrast are false matches: they do not express the authors’ goals, although they are superficially similar to the correct contexts. While string-based approaches (Paice, 1981; Teufel, 1998) are too restrictive to cover the wide variation within the correct contexts, bag-of-words approaches such as Agichtein and Gravano’s (2000) are too permissive and would miss many of the distinctions between correct and incorrect contexts. Lisacek et al. (2005) address the task of identifying “paradigm shift” sentences in the biomedical literature, i.e. statements of thwarted expectation. This task is somewhat similar to ours in its definition by rhetorical context. Their method goes beyond string-based matching: In order for a sentence to qualify, the right set of concepts must be present in a sentence, with any syntactic relationship holding between them. Each concept set is encoded as a fixed, manually compiled lists of strings. Their method covers only one particular context (the paradigm shift one), whereas we are looking for a method where man</context>
</contexts>
<marker>Lisacek, Chichester, Kaplan, Agnes, 2005</marker>
<rawString>Frederique Lisacek, Christine Chichester, Aaron Kaplan, and Sandor Agnes. 2005. Discovering paradigm shift patterns in biomedical abstracts: Application to neurodegenerative diseases. In Proc. of the SMBM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Five papers on WordNet.</title>
<date>1990</date>
<tech>Technical report,</tech>
<institution>Cognitive Science Laboratory, Princeton University.</institution>
<contexts>
<context position="20644" citStr="Miller et al., 1990" startWordPosition="3337" endWordPosition="3340">ration (MAP=.550) was achieved by combining Google Scholar and the Jensen-Shannon measure. The algorithm stops to iterate when no more improvement can be obtained, in this case after 4 iterations, resulting in a final MAP of .619. Although α-Skew outperforms the simpler measures in ranking nouns, its performance on verbs is worse than the performance of Weighted Lin. While Lee (1999) argues that α-Skew’s asymmetry can be advantageous for nouns, this probably does not hold for verbs: verb hierarchies have much shallower structure than noun hierarchies with most verbs concentrated on one level (Miller et al., 1990). This would explain why JS, which is symmetric compared to the α-Skew metric, performed better in our experiments. In the evaluation presented here we therefore use Google Scholar data and the JS measure. An additional improvement (MAP=.630) is achieved when we incorporate a filter based on the following hypothesis: goal-type verbs should be more likely to have their direct objects preceded by indefinite articles rather than definite articles or possessive determiners (because a new method is introduced) whereas continuation-type verbs should prefer definite articles with their direct objects</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. Five papers on WordNet. Technical report, Cognitive Science Laboratory, Princeton University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Myers</author>
</authors>
<title>In this paper we report...—speech acts and scientific facts.</title>
<date>1992</date>
<journal>Journal of Pragmatics,</journal>
<volume>17</volume>
<issue>4</issue>
<pages>313</pages>
<contexts>
<context position="1444" citStr="Myers, 1992" startWordPosition="225" endWordPosition="226">ut which look superficially similar. The method achieves 88% accuracy, outperforming standard bootstrapping approaches. 1 Introduction Cue phrases such as “This paper proposes a novel approach to... ”, “no method for ... exists” or even “you will hear from my lawyer” are semi-fixed in that they constitute a formulaic pattern with a clear semantics, but with syntactic and lexical variations which are hard to predict and thus hard to detect in unseen text (e.g. “a new algorithm for ... is suggested in the current paper” or “I envisage legal action”). In scientific discourse, such metadiscourse (Myers, 1992; Hyland, 1998) abounds and plays an important role in marking the discourse structure of the texts. Finding these variants can be useful for many text understanding tasks because semi-fixed cue phrases act as linguistic markers indicating the importance and/or the rhetorical role of some adjacent text. For the summarisation of scientific &apos;In contrast to standard work in discourse linguistics, which mostly considers sentence connectives and adverbials as cue phrases, our definition includes longer phrases, sometimes even entire sentences. papers, cue phrases such as “Our paper deals with... ” </context>
</contexts>
<marker>Myers, 1992</marker>
<rawString>Greg Myers. 1992. In this paper we report...—speech acts and scientific facts. Journal of Pragmatics, 17(4):295– 313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Mirella Lapata</author>
</authors>
<title>Constructing semantic space models from parsed corpora.</title>
<date>2003</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="34805" citStr="Pado and Lapata, 2003" startWordPosition="5598" endWordPosition="5602">syntactic relationships relation between them. Examples for such other types of phrases are given in Fig. 12; the second cue phrase involves a complex syntactic relationship between the two seeds (or possibly it could be considered as a cue phrase with three seeds). We will next investigate if the positive results presented here can be maintained for other syntactic contexts and for cue phrases with more than two seeds. The syntactic variant extractor could be enhanced in various ways, eg. by resolving anaphora in cue phrases. A more sophisticated model of syntactically weighted vector space (Pado and Lapata, 2003) may help improve the lexical acquisition phase. Another line for future work is bootstrapping meaning across cue phrases within the same rhetorical class, e.g. to learn that we propose a method for X and we aim to do X are equivalent. As some papers will contain both variants of the cue phrase, with very similar material (X) in the vicinity, they could be used as starting point for experiments to validate cue phrase equivalence. 7 Acknowledgements This work was funded by the EPSRC projects CITRAZ (GR/S27832/01, “Rhetorical Citation Maps and Domain-independent Argumentative Zoning”) and SCIBOR</context>
</contexts>
<marker>Pado, Lapata, 2003</marker>
<rawString>Sebastian Pado and Mirella Lapata. 2003. Constructing semantic space models from parsed corpora. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris D Paice</author>
</authors>
<title>The automatic generation of literary abstracts: an approach based on the identification of self-indicating phrases.</title>
<date>1981</date>
<booktitle>Information Retrieval Research,</booktitle>
<editor>In Robert Norman Oddy, Stephen E. Robertson, Cornelis Joost van Rijsbergen, and P. W. Williams, editors,</editor>
<location>Butterworth, London, UK.</location>
<contexts>
<context position="2951" citStr="Paice (1981)" startWordPosition="460" endWordPosition="461"> claimed gap in the literature. Similarly, in the task of automatic routing of customer emails and automatic answering of some of these, the detection of threats of legal action could be useful. However, systems that use cue phrases usually rely on manually compiled lists, the acquisition of which is time-consuming and error-prone and results in cue phrases which are genre-specific. Methods for finding cue phrases automatically include Hovy and Lin (1998) (using the ratio of word frequency counts in summaries and their corresponding texts), Teufel (1998) (using the most frequent n-grams), and Paice (1981) (using a pattern matching grammar and a lexicon of manually collected equivalence classes). The main issue with string-based pattern matching techniques is that they cannot capture syntactic generalisations such as active/passive constructions, different tenses and modification by adverbial, adjectival or prepositional phrases, appositions and other parenthetical material. For instance, we may be looking for sentences expressing the goal or main contribution of a paper; Fig. 1 shows candidates of such sentences. Cases a)–e), which do indeed describe the authors’ goal, display a wide range of </context>
<context position="4499" citStr="Paice, 1981" startWordPosition="705" endWordPosition="706">pt the method given in [1]... h) Previously we introduced a similar method.. . i) They introduce a similar method.. . Figure 1: Goal statements and syntactic variation – correct matches (a-e) and incorrect matches (f-i) 921 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 921–928, Sydney, July 2006. c�2006 Association for Computational Linguistics Cases f)–i) in contrast are false matches: they do not express the authors’ goals, although they are superficially similar to the correct contexts. While string-based approaches (Paice, 1981; Teufel, 1998) are too restrictive to cover the wide variation within the correct contexts, bag-of-words approaches such as Agichtein and Gravano’s (2000) are too permissive and would miss many of the distinctions between correct and incorrect contexts. Lisacek et al. (2005) address the task of identifying “paradigm shift” sentences in the biomedical literature, i.e. statements of thwarted expectation. This task is somewhat similar to ours in its definition by rhetorical context. Their method goes beyond string-based matching: In order for a sentence to qualify, the right set of concepts must</context>
</contexts>
<marker>Paice, 1981</marker>
<rawString>Chris D. Paice. 1981. The automatic generation of literary abstracts: an approach based on the identification of self-indicating phrases. In Robert Norman Oddy, Stephen E. Robertson, Cornelis Joost van Rijsbergen, and P. W. Williams, editors, Information Retrieval Research, Butterworth, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="17557" citStr="Pereira et al. (1993)" startWordPosition="2828" endWordPosition="2831">ships. The score assigned to each word in the candidate set is the sum of its semantic similarity values computed with respect to each member in the reference set. 3http://scholar.google.com 4The capitalised words in these patterns are replaced by actual words (e.g. AGENT STRING: We/I, DETERMINER: a/ an/our), and the extracted words (indicated by “*”) are lemmatised. Syntactic contexts, as opposed to window-based contexts, constrain the context of a word to only those words that are grammatically related to it. We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: Hindle’s (1990) measure, the weighted Lin measure (Wu and Zhou, 2003), the α-Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard’s coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). The Jensen-Shannon measure JS (x1, x2) = (Px log P XE{X,X21 (YI ) g 2 ( (YX)+P 1)+P(Y X2)) subsequently performed best for our task. We compare the d</context>
<context position="24824" citStr="Pereira et al. (1993)" startWordPosition="4001" endWordPosition="4004">od] (for goal) and [improve, adopt] &amp; [model, method] (for continuation). 925 Correctly found: Goal-type: What we aim in this paper is to propose a paradigm that enables partial/local generation through decompositions and reorganizations of tentative local structures. (9411021, S-5) Continuation-type: In this paper we have discussed how the lexicographical concept of lexical functions, introduced by Melcuk to describe collocations, can be used as an interlingual device in the machine translation of such structures. (9410009, S-126) Correctly rejected: Goal-type: Perhaps the method proposed by Pereira et al. (1993) is the most relevant in our context. (9605014, S-76) Continuation-type: Neither Kamp nor Kehler extend their copying/ substitution mechanism to anything besides pronouns, as we have done. (9502014, S-174) Figure 7: Sentences correctly processed by our system 4 Gold standard evaluation We evaluated the quality of the extracted phrases in two ways: by comparing our system output to gold standard annotation, and by human judgement of the quality of the returned sentences. In both cases bootstrapping was done using the seed tuples [analyse, present] &amp; [architecture, method]. For the gold standard</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="27020" citStr="Ravichandran and Hovy (2002)" startWordPosition="4355" endWordPosition="4358">list of candidate patterns was assembled. Patterns with single occurrences were discarded, leaving a list of 5580 patterns (examples in Fig. 8). In order to rank the patterns by precision, the goal-verbs were submitted as queries and the top 1000 documents were downloaded for each. From these, we &lt;verb&gt; a &lt;noun&gt; for of a new &lt;noun&gt; to &lt;verb&gt; the In this section, we &lt;verb&gt; the &lt;noun&gt; of the &lt;noun&gt; &lt;verb&gt; in this paper is to &lt;verb&gt; the &lt;noun&gt; after Figure 8: Examples of patterns extracted using Ravichandran and Hovy’s (2002) method Method Correct sentences Our system with bootstrapping 88 (73%) Ravichandran and Hovy (2002) 58 (48%) Our system, no bootstrapping, WordNet 50 (41%) Our system, no bootstrapping, seeds only 37 (30%) Figure 9: Gold standard evaluation: results the precision of each pattern was calculated by dividing the number of strings matching the pattern instantiated with both the goal-verb and all WordNet synonyms of the goal-noun, by the number of strings matching the patterns instantiated with the goal-verb only. An important point here is that while the tight semantic coupling between the question and answer terms in the original method accurately identifies all the positive and negative examp</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proc. of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically constructing a dictionary for information extraction tasks.</title>
<date>1993</date>
<booktitle>In Proc. of AAAI-93.</booktitle>
<marker>Riloff, 1993</marker>
<rawString>Ellen Riloff. 1993. Automatically constructing a dictionary for information extraction tasks. In Proc. of AAAI-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
</authors>
<title>Meta-discourse markers and problemstructuring in scientific articles.</title>
<date>1998</date>
<booktitle>In Proceedings of the ACL-98 Workshop on Discourse Structure and Discourse Markers.</booktitle>
<contexts>
<context position="2899" citStr="Teufel (1998)" startWordPosition="452" endWordPosition="453">contains the particular research aim of a paper, or a claimed gap in the literature. Similarly, in the task of automatic routing of customer emails and automatic answering of some of these, the detection of threats of legal action could be useful. However, systems that use cue phrases usually rely on manually compiled lists, the acquisition of which is time-consuming and error-prone and results in cue phrases which are genre-specific. Methods for finding cue phrases automatically include Hovy and Lin (1998) (using the ratio of word frequency counts in summaries and their corresponding texts), Teufel (1998) (using the most frequent n-grams), and Paice (1981) (using a pattern matching grammar and a lexicon of manually collected equivalence classes). The main issue with string-based pattern matching techniques is that they cannot capture syntactic generalisations such as active/passive constructions, different tenses and modification by adverbial, adjectival or prepositional phrases, appositions and other parenthetical material. For instance, we may be looking for sentences expressing the goal or main contribution of a paper; Fig. 1 shows candidates of such sentences. Cases a)–e), which do indeed </context>
<context position="4514" citStr="Teufel, 1998" startWordPosition="707" endWordPosition="708"> given in [1]... h) Previously we introduced a similar method.. . i) They introduce a similar method.. . Figure 1: Goal statements and syntactic variation – correct matches (a-e) and incorrect matches (f-i) 921 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 921–928, Sydney, July 2006. c�2006 Association for Computational Linguistics Cases f)–i) in contrast are false matches: they do not express the authors’ goals, although they are superficially similar to the correct contexts. While string-based approaches (Paice, 1981; Teufel, 1998) are too restrictive to cover the wide variation within the correct contexts, bag-of-words approaches such as Agichtein and Gravano’s (2000) are too permissive and would miss many of the distinctions between correct and incorrect contexts. Lisacek et al. (2005) address the task of identifying “paradigm shift” sentences in the biomedical literature, i.e. statements of thwarted expectation. This task is somewhat similar to ours in its definition by rhetorical context. Their method goes beyond string-based matching: In order for a sentence to qualify, the right set of concepts must be present in </context>
</contexts>
<marker>Teufel, 1998</marker>
<rawString>Simone Teufel. 1998. Meta-discourse markers and problemstructuring in scientific articles. In Proceedings of the ACL-98 Workshop on Discourse Structure and Discourse Markers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
</authors>
<title>Argumentative Zoning: Information Extraction from Scientific Text.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Cognitive Science, University of Edinburgh, UK.</institution>
<contexts>
<context position="18343" citStr="Teufel (1999)" startWordPosition="2953" endWordPosition="2954">ce: Hindle’s (1990) measure, the weighted Lin measure (Wu and Zhou, 2003), the α-Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard’s coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). The Jensen-Shannon measure JS (x1, x2) = (Px log P XE{X,X21 (YI ) g 2 ( (YX)+P 1)+P(Y X2)) subsequently performed best for our task. We compare the different ranking methodologies and data sets with respect to a manually-defined gold standard list of 20 goal-type verbs and 20 nouns. This list was manually assembled from Teufel (1999); WordNet synonyms and other plausible verbs and nouns found via Web searches on scientific articles were added. We ensured by searches on the ACL anthology that there is good evidence that the gold-standard words indeed occur in the right contexts, i.e. in goal statement sentences. As we want to find similarity metrics and data sources which result in accumulator lists with many of these gold members at high ranks, we need a measure that rewards exactly those lists. We use non-interpolated Mean Average Precision (MAP), a standard measure for evaluating ranked information retrieval runs, which</context>
<context position="23305" citStr="Teufel (1999)" startWordPosition="3780" endWordPosition="3781">y. Examples for constraints are: if work is referred to as being done in previous own work, it is probably not a goal statement; the work in a goal statement must be presented here or in the current paper (the concept of ‘here-ness”); and the agents of a goal statement have to be the authors, not other people. While these filters are manually defined, they are modular, encode general principles, and can be combined to express a wide range of rhetorical contexts. We verified that around 20 semantic constraints are enough to cover a large sets of different cue phrases (the 1700 cue phrases from Teufel (1999)), though not all of these are implemented yet. A nice side-effect of our approach is the simple characterisation of a cue phrase (by a syntactic relationship, some seed words for each concept, and some general, reusable semantic constraints). This characterisation is more informative and specific than string-based approaches, yet it has the potential for generalisation (useful if the cue phrases are ever manually assessed and put into a lexicon). Fig. 7 shows successful extraction examples from our corpus7, illustrating the difficulty of the task: the system correctly identified sentences wit</context>
<context position="25538" citStr="Teufel, 1999" startWordPosition="4116" endWordPosition="4117">nd their copying/ substitution mechanism to anything besides pronouns, as we have done. (9502014, S-174) Figure 7: Sentences correctly processed by our system 4 Gold standard evaluation We evaluated the quality of the extracted phrases in two ways: by comparing our system output to gold standard annotation, and by human judgement of the quality of the returned sentences. In both cases bootstrapping was done using the seed tuples [analyse, present] &amp; [architecture, method]. For the gold standard-evaluation, we ran our system on a test set of 121 scientific articles drawn from the CmpLg corpus (Teufel, 1999) – entirely different texts from the ones the system was trained on. Documents were manually annotated by the second author for (possibly more than one) goal-type sentence; annotation of that type has been previously shown to be reliable at K=.71 (Teufel, 1999). Our evaluation recorded how often the system’s highest-ranked candidate was indeed a goal-type sentence; as this is a precision-critical task, we do not measure recall here. We compared our system against our reimplementation of Ravichandran and Hovy’s (2002) paraphrase learning. The seed words were of the form {goal-verb, goal-noun}, </context>
<context position="30883" citStr="Teufel (1999)" startWordPosition="4973" endWordPosition="4974">ore ranging from 1 (low) to 5 (high) depending on how well the sentence expresses the goal of that article (Exp. A), or the continuation of previous work (Exp. B). Each experiment involves 24 articles drawn randomly from a subset of 80 articles in the CmpLg corpus that contain manual annotation for goaltype and continuation-type sentences. The experiments use three external judges (graduate students in computational linguistics), and a Latin Square experimental design with three conditions: Baseline (see below), System-generated and Ceiling (extracted from the gold standard annotation used in Teufel (1999)). Judges were not told how the sentences were generated, and no judge saw an item in more than one condition. The baseline for Experiment A was a random selection of sentences with the highest TF*IDF scores, because goal-type sentences typically contain many content-words. The baseline for experiment B (continuation-type) were randomly selected sentences containing citations, because they often co-occur with statements of continuation. In both cases, the length of the baseline sentence was controlled for by the average lengths of the gold standard and the system-extracted sentences in the doc</context>
</contexts>
<marker>Teufel, 1999</marker>
<rawString>Simone Teufel. 1999. Argumentative Zoning: Information Extraction from Scientific Text. Ph.D. thesis, School of Cognitive Science, University of Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cornelis Joost van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<location>Butterworth, London, UK,</location>
<note>2nd edition.</note>
<marker>van Rijsbergen, 1979</marker>
<rawString>Cornelis Joost van Rijsbergen. 1979. Information Retrieval. Butterworth, London, UK, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Ming Zhou</author>
</authors>
<title>Synonymous collocation extraction using translation information.</title>
<date>2003</date>
<booktitle>In Proc. of the ACL.</booktitle>
<contexts>
<context position="17803" citStr="Wu and Zhou, 2003" startWordPosition="2866" endWordPosition="2869">ctual words (e.g. AGENT STRING: We/I, DETERMINER: a/ an/our), and the extracted words (indicated by “*”) are lemmatised. Syntactic contexts, as opposed to window-based contexts, constrain the context of a word to only those words that are grammatically related to it. We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: Hindle’s (1990) measure, the weighted Lin measure (Wu and Zhou, 2003), the α-Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard’s coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). The Jensen-Shannon measure JS (x1, x2) = (Px log P XE{X,X21 (YI ) g 2 ( (YX)+P 1)+P(Y X2)) subsequently performed best for our task. We compare the different ranking methodologies and data sets with respect to a manually-defined gold standard list of 20 goal-type verbs and 20 nouns. This list was manually assembled from Teufel (1999); WordNet synonyms and other plausible verbs and nouns found</context>
</contexts>
<marker>Wu, Zhou, 2003</marker>
<rawString>Hua Wu and Ming Zhou. 2003. Synonymous collocation extraction using translation information. In Proc. of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>