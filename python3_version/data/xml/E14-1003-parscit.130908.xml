<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.994269">
Minimum Translation Modeling with Recurrent Neural Networks
</title>
<author confidence="0.999677">
Yuening Hu Michael Auli, Qin Gao, Jianfeng Gao
</author>
<affiliation confidence="0.9994465">
Department of Computer Science Microsoft Research
University of Maryland, College Park Redmond, WA, USA
</affiliation>
<email confidence="0.993656">
ynhu@cs.umd.edu {michael.auli,qigao,jfgao}@microsoft.com
</email>
<sectionHeader confidence="0.997335" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984705882353">
We introduce recurrent neural network-
based Minimum Translation Unit (MTU)
models which make predictions based on
an unbounded history of previous bilin-
gual contexts. Traditional back-off n-gram
models suffer under the sparse nature of
MTUs which makes estimation of high-
order sequence models challenging. We
tackle the sparsity problem by modeling
MTUs both as bags-of-words and as a
sequence of individual source and target
words. Our best results improve the out-
put of a phrase-based statistical machine
translation system trained on WMT 2012
French-English data by up to 1.5 BLEU,
and we outperform the traditional n-gram
based MTU approach by up to 0.8 BLEU.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999855693548387">
Classical phrase-based translation models rely
heavily on the language model and the re-
ordering model to capture dependencies between
phrases. Sequence models over Minimum Trans-
lation Units (MTUs) have been shown to com-
plement both syntax-based (Quirk and Menezes,
2006) as well as phrase-based (Zhang et al., 2013)
models by explicitly modeling relationships be-
tween phrases. MTU models have been tradi-
tionally estimated using standard back-off n-gram
techniques (Quirk and Menezes, 2006; Crego and
Yvon, 2010; Zhang et al., 2013), similar to word-
based language models (§2).
However, the estimation of higher-order n-gram
models becomes increasingly difficult due to data
sparsity issues associated with large n-grams, even
when training on over one hundred billion words
(Heafield et al., 2013); bilingual units are much
sparser than words and are therefore even harder
to estimate. Another drawback of n-gram mod-
els is that future predictions are based on a limited
amount of previous context that is often not suf-
ficient to capture important aspects of human lan-
guage (Rastrow et al., 2012).
Recently, several feed-forward neural network-
based models have achieved impressive improve-
ments over traditional back-off n-gram models in
language modeling (Bengio et al., 2003; Schwenk
et al., 2007; Schwenk et al., 2012; Vaswani et al.,
2013), as well as translation modeling (Allauzen et
al., 2011; Le et al., 2012; Gao et al., 2013). These
models tackle the data sparsity problem by rep-
resenting words in continuous space rather than
as discrete units. Similar words are grouped in
the same sub-space rather than being treated as
separate entities. Neural network models can be
seen as functions over continuous representations
exploiting the similarity between words, thereby
making the estimation of probabilities over higher-
order n-grams easier.
However, feed-forward networks do not directly
address the limited context issue either, since pre-
dictions are based on a fixed-size context, similar
to back-off n-gram models. We therefore focus
in this paper on recurrent neural network architec-
tures, which address the limited context issue by
basing predictions on an unbounded history of pre-
vious events which allows to capture long-span de-
pendencies. Recurrent architectures have recently
advanced the state of the art in language model-
ing (Mikolov et al., 2010; Mikolov et al., 2011a;
Mikolov, 2012) outperforming multi-layer feed-
forward based networks in perplexity and word er-
ror rate for speech recognition (Arisoy et al., 2012;
Sundermeyer et al., 2013). Recent work has also
shown successful applications to machine transla-
tion (Mikolov, 2012; Auli et al., 2013; Kalchbren-
ner and Blunsom, 2013). We extend this work by
modeling Minimum Translation Units with recur-
rent neural networks.
Specifically, we introduce two recurrent neu-
ral network-based MTU models to address the is-
</bodyText>
<page confidence="0.944071">
20
</page>
<note confidence="0.9973475">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20–29,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.526121">
Words MTUs
Tokens 34,769,416 14,853,062
Types 143,524 1,315,512
Singleton types 34.9% 80.1%
</table>
<tableCaption confidence="0.961185">
Table 1: Token and type counts for both source
</tableCaption>
<bodyText confidence="0.6406825">
and target words as well as MTUs based on the
WMT 2006 German to English data set (cf. §5).
</bodyText>
<equation confidence="0.4700535">
M1 M2 M3 M4 M5
T 111 *ff T A__A
</equation>
<author confidence="0.192376">
Yu ZuoTian JuXing Le null HuiTan
</author>
<figure confidence="0.964222333333333">
held the meeting null yesterday
M1: Yu =&gt; null
M2: ZuoTian =&gt; Yesterday
M3: JuXing_Le =&gt; held
M4: null =&gt; the
M5: HuiTan =&gt; meeting
</figure>
<figureCaption confidence="0.974409">
Figure 1: Example Minimum Translation Unit
partitioning based on Zhang et al. (2013).
</figureCaption>
<bodyText confidence="0.999367785714286">
sues regarding data sparsity and limited context
sizes by leveraging continuous representations and
the unbounded history of the recurrent architec-
ture. Our first approach frames the problem as a
sequence modeling task over minimal units (§3).
The second model improves over the first by mod-
eling an MTU as a bag-of-words, thereby allow-
ing us to learn representations over sub-structures
of minimal units that are shared across MTUs
(§4). Our models significantly outperform the tra-
ditional back-off n-gram based approach and we
show that they act complementary to a very strong
recurrent neural network-based language model
based solely on target words (§5).
</bodyText>
<sectionHeader confidence="0.970907" genericHeader="method">
2 Minimum Translation Units
</sectionHeader>
<bodyText confidence="0.993562317073171">
Banchs et al. (2005) introduced the idea of framing
translation as a sequence modeling problem where
a sentence pair is generated in left-to-right order as
a sequence of bilingual n-grams. Minimum Trans-
lation Units (Quirk and Menezes, 2006; Zhang
et al., 2013) are an extension which additionally
permit tuples with empty source or target sides,
thereby allowing insertion or deletion phrase pairs.
The two basic requirements for MTUs are that
there are no overlapping word alignment links be-
tween phrase pairs and it should not be possible to
extract smaller phrase pairs without violating the
word alignment constraints. Informally, we can
think of MTUs as small phrase pairs that cannot
be broken down any further without violating the
two requirements.
Minimum Translation Units partition a sentence
pair into a set of minimal bilingual units or tu-
ples obtained by an algorithm similar to phrase-
extraction (Koehn et al., 2003). Figure 1 illus-
trates such a partitioning. Modeling minimal units
has two advantages over considering larger phrase
pairs that are effectively composed of MTUs:
First, minimal units result in a unique partition-
ing of a sentence pair. This has the advantage that
we avoid modeling spurious derivations, that is,
multiple derivations generating the same sentence
pair. Second, minimal units result in smaller mod-
els with a smoother distribution than models based
on composed units (Zhang et al., 2013).
Sentence pairs can be generated in multiple or-
ders, such as left-to-right or right-to-left, either in
source or target order. For example, the source
left-to-right order of the sentence pair in Figure 1
is simply M1, M2, M3, M4, M5, while the tar-
get left-to-right order is M3, M4, M5, M1, M2.
We deal with inserted or deleted words similar to
Zhang et al. (2013): The source side null token of
an inserted target phrase is placed next to the last
source word aligned to the closest preceding non-
null aligned target phrase; a similar rule is applied
</bodyText>
<figureCaption confidence="0.83936325">
to null tokens on the target side. For example, in
Figure 1 we place M4 straight after M3 because
“the”, the aligned target phrase, is after “held”, the
previous non-null aligned target phrase.
</figureCaption>
<bodyText confidence="0.999557166666667">
We can straightforwardly estimate an n-gram
model over MTUs to estimate the probability
of a sentence pair using standard back-off tech-
niques commonly employed in language mod-
eling. For example, a trigram model in tar-
get left-to-right order factors the sentence pair in
</bodyText>
<figureCaption confidence="0.636203">
Figure 1 as p(M3) p(M4|M3) p(M5|M3, M4)
p(M1|M4, M5)p(M2|M5, M1).
</figureCaption>
<bodyText confidence="0.999901333333333">
If we would like to model larger contexts, then
we quickly run into data sparsity issues. To illus-
trate this point, consider the parameter growth of
an n-gram model which is driven by the vocabu-
lary size |V  |and the n-gram order n: O(|V |n).
Clearly, the exact estimation of higher-order n-
</bodyText>
<page confidence="0.998464">
21
</page>
<bodyText confidence="0.999808939393939">
gram probabilities becomes more difficult with
large n, leading to the estimation of events with
increasingly sparse statistics, or having to rely
on statistics from lower-order events with back-
off models, which is less desirable. Even word-
based language models rarely ventured so far
much beyond 5-gram statistics as demonstrated
by Heafield et al. (2013) who trained a, by to-
day’s standards, very large 5-gram model on 130B
words. Data sparsity is therefore an even more sig-
nificant issue for MTU models relying on much
larger vocabularies. In our setting, the MTU vo-
cabulary is an order of magnitude larger than a
word vocabulary obtained from the same data (Ta-
ble 1). Furthermore, most MTUs are observed
only once making the reliable estimation of prob-
abilities very challenging.
Neural network-based sequence models tackle
the data sparsity problem by learning continuous
word representations, that group similar words to-
gether in continuous space. For example, the
distributional representations induced by recurrent
neural networks have been found to have interest-
ing syntactic and semantic regularities (Mikolov
et al., 2013). Furthermore, these representations
can be exploited to estimate more reliable statis-
tics over higher-order n-grams than with discrete
word units. Recurrent neural networks go beyond
fixed-size contexts and allow the model to keep
track of long-span dependencies that are important
for future predictions. In the next sections we will
present Minimum Translation Unit models based
on recurrent architectures.
</bodyText>
<sectionHeader confidence="0.997701" genericHeader="method">
3 Atomic MTU RNN Model
</sectionHeader>
<bodyText confidence="0.961700647058824">
The first model we introduce is based on the recur-
rent neural network language model of Mikolov
et al. (2010). We frame the problem as a tradi-
tional sequence modeling task which treats MTUs
as atomic units, similar to the approach taken by
the traditional back-off n-gram models.
The model is factored into an input layer, a hid-
den layer with recurrent connections, and an out-
put layer (Figure 2). The input layer encodes the
MTU at time t as a 1-of-N vector mt with all val-
ues being zero except for the entry representing
the MTU. The output layer yt represents a proba-
bility distribution over possible next MTUs; both
the input and output layers are of size |V |, the size
of the MTU vocabulary. The hidden layer state ht
encodes the history of all MTUs observed in the
mt
</bodyText>
<figureCaption confidence="0.919813666666667">
Figure 2: Structure of the atomic recurrent neu-
ral network MTU model following the word-based
RNN model of Mikolov (2012).
</figureCaption>
<bodyText confidence="0.990546214285714">
sequence up to time step t.
The state of the hidden layer is determined by
the input layer and the hidden layer configuration
of the previous time step ht−1. The weights of the
connections between the layers are summarized in
a number of matrices: U represents weights from
the input layer to the hidden layer, and W repre-
sents connections from the previous hidden layer
to the current hidden layer. Matrix V contains
weights between the current hidden layer and the
output layer.
The hidden and output layers are computed
via a series of matrix-vector products and non-
linearities:
</bodyText>
<equation confidence="0.970603">
ht = s(Umt + Wht−1)
yt = 9(Vht)
</equation>
<bodyText confidence="0.628545">
where
</bodyText>
<equation confidence="0.999284">
1 _ exp {zm}
s(z) = 1 + exp {−z}, 9(zm) Ek exp {zk}
</equation>
<bodyText confidence="0.999331625">
are sigmoid and softmax functions, respectively.
Additionally, the network is interpolated with a
maximum entropy model of sparse n-gram fea-
tures over input MTUs (Mikolov et al., 2011a).
The maximum entropy weights D are added to
the output activations before applying the softmax
function and are estimated jointly with all other
parameters (Figure 3).1
</bodyText>
<footnote confidence="0.636026">
1While these features depend on multiple input MTUs, we
</footnote>
<figure confidence="0.9705465625">
yt
0
0
1
0
0
0
0
0
W
U
ht
V
ht-1
22
Mt
</figure>
<figureCaption confidence="0.85427725">
Figure 3: Structure of atomic recurrent neural net-
work MTU model with classing layer ct and direct
connections D between the input and output lay-
ers (cf. Figure 2).
</figureCaption>
<bodyText confidence="0.986383535714286">
The model is optimized via a maximum likeli-
hood objective function using stochastic gradient
descent. Training is based on the truncated back
propagation through time algorithm, which unrolls
the network and then computes error gradients
over multiple time steps (Rumelhart et al., 1986);
we use a cross entropy criterion to obtain the error
vector with respect to the output activations and
the desired prediction. After training, the output
layer represents posteriors p(mt+1|mtt−n+1, ht),
the probability of the next MTU given the previ-
ous ninput MTUs mtt−n+1 = mt, . . . , mt−n+1
and the current hidden layer configuration ht.
Naive computation of the probability distribu-
tion over the next MTU is very expensive for large
vocabularies, such as commonly encountered for
MTU models (Table 1). A well established ef-
ficiency trick assigns each possible output to a
unique class and then uses a two-step process to
find the probability of an MTU, instead of comput-
ing the probability of all possible outputs (Good-
man, 2001; Emami and Jelinek, 2005; Mikolov et
al., 2011b). Under this scheme we compute the
probability of an MTU by multiplying the prob-
ability of its class cit with the probability of the
depicted them for simplicity as a connection between the
current input vector mt and the output layer.
minimal unit conditioned on the class:
</bodyText>
<equation confidence="0.973155">
p(mt+1|mtt−n+1, ht) =
p(cit|mtt−n+1, ht) p(mt+1|cit, mtt−n+1, ht)
</equation>
<bodyText confidence="0.981889416666667">
This factorization reduces the complexity of com-
puting the output probabilities from O(|V |) to
O(|C |+ maxi |ci|) where |C |is the number of
classes and |ci |is the number of minimal units
�
in class ci. The best case complexity O( |V |)
requires the number of classes and MTUs to be
evenly balanced, i.e., each class contains exactly
as many minimal units as there are classes.
Figure 3 illustrates how classing changes the
structure of the network by adding an additional
output layer for the class probabilities.
</bodyText>
<sectionHeader confidence="0.996521" genericHeader="method">
4 Bag-of-words MTU RNN Model
</sectionHeader>
<bodyText confidence="0.99997675">
The previous model treats MTUs as atomic sym-
bols which leads to large vocabularies requir-
ing large parameter sets and expensive inference.
However, similar MTUs may share the same
words, or words which are related in continuous
space. The atomic MTU model does not exploit
this since it cannot access the internal structure of
a minimal unit.
The approach we pursue next is to break MTUs
into individual source and target words (Le et al.,
2012) in order to exploit structural similarities be-
tween infrequently observed minimal units. Sin-
gletons represent the vast majority of our MTU
vocabulary (Table 1). This resembles the word-
hashing trick of Huang et al. (2013) who repre-
sented individual words as a bag-of-character n-
grams to reduce the vocabulary size of a neural
network-based model in an information retrieval
setting.2
We first describe a theoretically appealing but
computationally expensive model and then discuss
a more practical variation. The input layer of this
model accepts the current minimal unit as a K-of-
N vector representing K source and target words
as opposed to the 1-of-N encoding of entire MTUs
in the previous model (Figure 4). Larger MTUs
may contain the same word more than once and we
simply adjust their count to one.3 Different to the
</bodyText>
<footnote confidence="0.7326222">
2Applying the same technique would likely result in too many
collisions since we are dealing with multi-word units instead
of single words.
3We found no effect on accuracy when using the unmodified
count in initial experiments.
</footnote>
<figure confidence="0.998416941176471">
D
Yt
0
0
1
0
0
0
0
0
ht
U
V
W
ht-1
T
Ct
</figure>
<page confidence="0.797288">
23
</page>
<figureCaption confidence="0.980705">
Figure 4: Structure of MTU bag-of-words recur-
</figureCaption>
<bodyText confidence="0.99724335">
rent neural network model. The input layer rep-
resents a minimal unit as a bag-of-words and the
output layer yt is a probability distribution over
possible next MTUs depending on the activations
of the word layer wt representing source and tar-
get words of minimal units.
previous model, the input vector has now multiple
active entries whose signals are absorbed into the
new hidden layer configuration.
This bag-of-words encoding of minimal units
dramatically reduces the vocabulary size but it in-
evitably maps different MTUs to the same encod-
ing. On our data set, we observe less than 0.2% of
minimal units that are involved in collisions, a rate
that is similar to Huang et al. (2013). In practice
collisions are unlikely to affect accuracy in our set-
ting because MTUs that are mapped to the same
encoding usually do not differ much in semantic
meaning as illustrated by the following examples:
erfolg haben → succeed collides with haben er-
folg → succeed, or damit , → to and, damit → to;
in both examples either the auxiliary verb haben or
the comma changes position, neither of which sig-
nificantly changes the meaning for this particular
pair of MTUs.
The structure of the bag-of-words MTU RNN
models is shown in Figure 4. Similar to the atomic
MTU RNN model (§3), the hidden layer combines
the signal from the input layer and the previous
hidden layer configuration. The hidden layer acti-
vations feed into a word layer wt representing the
source and target words that part of all possible
MTUs; it is of the same size as the input layer. The
word layer is connected to a convolutional out-
put layer yt by weights summarized in the sparse
matrix C. The output layer represents all possi-
ble next minimal units, where each MTU entry is
only connected to neurons in the word layer repre-
senting its source and target words. The word and
MTU layers are then computed as follows:
</bodyText>
<equation confidence="0.996237">
wt = s(Vht)
yt = g(Cwt)
</equation>
<bodyText confidence="0.999982944444444">
However, there are a number of computational
issues with this model: First, we cannot efficiently
factor the word layer wt into classes such as for
the atomic MTU RNN model because we require
all its activations to compute the MTU output
layer yt. This reduces the best case complex-
ity of computing the word layer from O(V|V |)
back to linear in the number of source and tar-
get words |V |. In practice this results in between
200-1000 more activations that need to be com-
puted, depending on the word vocabulary size.
Second, turning the MTU output layer into a con-
volutional layer is not enough to sufficiently re-
duce the computational effort to compute the out-
put activations since the number of connections
between the word and MTU layers is very imbal-
anced. This is because frequent words, such as
function words, are part of many MTUs and there-
fore have a very high out-degree, e.g., the neuron
representing “the” has over 82K outgoing edges.
On the other hand, infrequent words, have a very
low out-degree. This imbalance makes it hard
to efficiently compute activations and error gradi-
ents, even on a GPU, since some neurons require
substantially more work than others.4
For these reasons we decided to design a sim-
pler, more tractable version of this model (Fig-
ure 5). The simplified model still represents an
input MTU as a bag-of-words but minimal units
are generated word-by-word, first emitting source
words and then target words. This is in contrast
to the original model which predicted an MTU as
a single unit. Decomposing the next MTU into
individual words dramatically reduces the size of
the output layer, thereby resulting in faster com-
putation of the outputs and making normalization
</bodyText>
<footnote confidence="0.75246">
4In initial experiments we found this model to be over twenty
times slower than the atomic MTU RNN model with esti-
mated training times of over 6 weeks. This was despite us-
ing a vastly smaller vocabulary and by computing the word
layer on a, by current standards, high-end GPU (NVIDIA
Tesla K20c) using sparse matrix optimizations (cuSPARSE)
for the convolutional layer.
</footnote>
<figure confidence="0.987085391304348">
yt
D
xt
tgt
1
0
1
1
0 U
1
0
0
W
ht
V
wt
C
...
...
ht-
...
src
MTU
24
mt 5 Experiments
src
MTU
tgt
ht-1
1
0
1
1
0
1
0
0
W
U
D
ht
T
V
ct
yt
mt+1
</figure>
<figureCaption confidence="0.7884355">
Figure 5: Simplified MTU bag-of-words recurrent
neural network model (cf. Figure 4). An MTU is
</figureCaption>
<bodyText confidence="0.991337416666667">
input as bag-of-words and the next MTU is pre-
dicted as a sequence of both source and target
words.
into probabilities easier. Furthermore, the output
layer can be factorized into classes requiring only
a fraction of the neurons to be computed, a much
more efficient solution compared to the original
model which required calculation of the entire out-
put layer.
The simplified model computes the probability
of the next MTU mt+1 as a product of individual
word probabilities:
</bodyText>
<equation confidence="0.996817">
p(mt+1|mtt−n+1, ht) = (1)
ri p(ck|mtt−n+1, ht)
a1,...,au∈mt+1
p(ak|ck, mtt−n+1, ht)
</equation>
<bodyText confidence="0.999984523809524">
where we predict a sequence of source and target
words a1, ... , au E mt+1 with a class-structured
output layer, similar to the atomic model (§3).
Training still uses a cross entropy criterion and
back propagation through time, however, error
vectors are computed on a per-word basis, instead
of a per-MTU basis. Direct connections between
the input and output layers are based on source and
target words which is less sparse than basing direct
features on entire MTUs such as for the original
bag-of-words model.
Overall, the simplified model retains the bag-of-
words input representation of the original model,
while permitting the efficient factorization of the
word-output layer into classes.
We evaluate the effectiveness of both the atomic
MTU RNN model (§3) and the simplified bag-of-
words MTU RNN model (§4) in an n-best rescor-
ing setting, comparing against a trigram back-off
MTU model as well as the phrasal decoder 1-best
output which we denote as the baseline.
</bodyText>
<subsectionHeader confidence="0.98432">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999949857142857">
Baselines. We experiment with an in-house
phrase-based system similar to Moses (Koehn et
al., 2007), scoring translations by a set of common
features including maximum likelihood estimates
of source given target mappings pMLE(e|f) and
vice versa pMLE(f|e), as well as lexical weight-
ing estimates pLW (e|f) and pLW (f|e), word and
phrase-penalties, a linear distortion feature and
a lexicalized reordering feature. The baseline
includes a standard modified Kneser-Ney word-
based language model trained on the target-side of
the parallel corpora described below. Log-linear
weights are estimated with minimum error rate
training (MERT; Och, 2003).
The 1-best output by the phrase-based decoder
is the baseline accuracy. As a second baseline we
experiment with a trigram back-off MTU model
trained on all extracted MTUs, denoted as n-gram
MTU. The trigram MTU model is estimated with
the same modified Kneser-Ney framework as the
target side language model. All MTU models are
trained in target left-to-right MTU order which
performed well in initial experiments.
Evaluation. We test our approach on two differ-
ent data sets. First, we train a German to English
system based on the data of the WMT 2006 shared
task (Koehn and Monz, 2006). The parallel corpus
includes about 35M words of parliamentary pro-
ceedings for training, a development set and two
test sets with 2000 sentences each.
Second, we experiment with a French to En-
glish system based on 102M words of training data
from the WMT 2012 campaign. The majority of
the training data set is parliamentary proceedings
except for about 5m words which are newswire; all
MTU models are trained on the newswire subset
since we found similar accuracy to using all data in
initial experiments. We evaluate on four newswire
domain test sets from 2008, 2010 and 2011 as well
as the 2010 system combination test set contain-
ing between 2034 to 3003 sentences. Log-linear
weights are estimated on the 2009 data set com-
</bodyText>
<page confidence="0.995282">
25
</page>
<bodyText confidence="0.99956825">
prising 2525 sentences. We evaluate all systems
in a single reference BLEU setting.
Rescoring Setup. We rescore the 1000-best out-
put of the baseline phrase-based decoder by ei-
ther the trigram back-off MTU model or the
RNN models. The baseline accuracy is obtained
by choosing the 1-best decoder output. We re-
estimate the log-linear weights for rescoring by
running a further iteration of MERT with the ad-
ditional feature values; we initialize the rescoring
feature weight to zero and try 20 random restarts.
At test time we use the new set of log-linear
weights to rescore the test set n-best list.
Neural Network Setup. We trained the recur-
rent neural network models on between 88% and
93% of each data set and used the remainder as
validation data. The vocabulary of the atomic
MTU RNN model is comprised of all MTU types
which were observed more than once in the train-
ing data.5 Similarly, we modeled all non-singleton
words for the bag-of-words MTU RNN model.
We obtain classes for words or MTUs using a
version of Brown-Clustering with an additional
regularization term to optimize the runtime of
the language model (Brown et al., 1992; Zweig
and Makarychev, 2013). Direct connections use
features over unigrams, bigrams and trigrams of
words or MTUs, depending on the model. Fea-
tures are hashed to a table with at most 500 million
values following Mikolov et al. (2011a). We use
the standard settings for the model with the default
learning rate α = 0.1 that decays exponentially if
the validation set entropy does not decrease. Back
propagation through time computes error gradi-
ents over the past twenty time steps. Training
is stopped after 20 epochs or when the valida-
tion entropy does not decrease over two epochs.
Throughout, we use a hidden layer size of 100
which provided a good trade-off between time and
accuracy in initial experiments.
</bodyText>
<subsectionHeader confidence="0.925412">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.997473571428571">
We first report the decoder 1-best output as the
first baseline and then rescore our two data sets
(Table 2 and Table 3) with the n-gram back-off
MTU model to establish a second baseline (n-
gram MTU). The n-gram model improves by 0.4
BLEU over the decoder 1-best on all test sets for
German to English. On French-English accuracy
</bodyText>
<footnote confidence="0.9209995">
5We tried modeling all MTUs which did not contain a single-
ton word but observed no significant effect on accuracy.
</footnote>
<table confidence="0.956811714285714">
dev test1 test2
Baseline 25.8 26.0 26.0
n-gram MTU 26.3 26.6 26.4
atomic MTU RNN 26.5 26.8 26.5
BoW MTU RNN 26.5 27.0 26.9
word RNNLM 26.5 27.1 26.8
Combined 26.8 27.3 27.1
</table>
<tableCaption confidence="0.986206">
Table 2: German to English BLEU results for
</tableCaption>
<bodyText confidence="0.996791317073171">
the decoder 1-best output (Baseline) compared to
rescoring with a target left-to-right trigram MTU
model (n-gram MTU), our two recurrent neural
network-based MTU models, a word-based RNN-
based language model (word RNNLM), as well
as a combination of the three RNN-based models
(Combined).
improves on three out of five sets by up to 0.7
BLEU.
Next, we evaluate the accuracy of the MTU
RNN models. The atomic MTU RNN model im-
proves over the n-gram MTU model on all test sets
for German to English, however, for French to En-
glish the back-off model performs better on two
out of four test sets.
The next question we answer is if breaking
MTUs into individual units to leverage similarities
in the internal structure can help accuracy. The re-
sults (Table 2 and Table 3) for the bag-of-words
model (BoW MTU RNN) clearly show that this is
the case for both language pairs. We significantly
improve over the n-gram MTU model as well as
the atomic RNN model on all test sets. We observe
gains of up to 0.5 BLEU over the n-gram MTU
model for German to English as well as French to
English; improvements over the decoder baseline
are up to 1.2 BLEU for French to English.
How do our models compare to other neural net-
work approaches that rely only on target side in-
formation? To answer this question we compare
to the strong language model of Mikolov (2012;
RNNLM) which has recently improved the state-
of-the-art in language modeling perplexity. The
results (Table 2 and Table 3) show that RNNLM
performs competitively. However, our approaches
model translation since we use both source and tar-
get information as opposed to scoring only the flu-
ency of the target side, such as done by RNNLM.
Can our models act complementary to a strong
RNN language model? Our final experiment com-
bines the atomic MTU RNN model, the BoW
</bodyText>
<page confidence="0.99465">
26
</page>
<table confidence="0.999809142857143">
dev news2008 news2010 news2011 newssyscomb2010
Baseline 24.3 20.5 24.4 25.1 24.3
n-gram MTU 24.6 20.8 24.4 25.8 24.3
atomic MTU RNN 24.6 20.7 24.4 25.5 24.3
BoW MTU RNN 25.2 21.2 24.8 26.3 24.6
word RNNLM 25.1 21.4 25.1 26.4 24.9
Combined 25.4 21.4 25.1 26.6 24.9
</table>
<tableCaption confidence="0.967834">
Table 3: French to English BLEU results for the decoder 1-best output (Baseline) compared to various
MTU models (cf. Table 2).
</tableCaption>
<bodyText confidence="0.998314416666667">
MTU RNN model, and the RNNLM (Combined).
The results (Table 2 and Table 3) confirm that this
is the case. For German to English translation
accuracy improves by 0.2 to 0.3 BLEU over the
RNNLM alone, with gains of up to 1.3 BLEU over
the baseline and up to 0.7 BLEU over the n-gram
MTU model. Improvements for French to English
are lower but we can see some gains on news2011
and on the dev set. Overall, we improve accuracy
on the French to English task by up to 1.5 BLEU
over the decoder 1-best, and by up to 0.8 BLEU
over the n-gram MTU model.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="evaluation">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999967583333333">
Our approach of modeling Minimum Translation
Units is very much in line with recent work on n-
gram-based translation models (Crego and Yvon,
2010), and more recently, continuous space-based
translation models (Le et al., 2012). The mod-
els presented in this paper differ in a number of
key aspects: We use a recurrent architecture repre-
senting an unbounded history of MTUs rather than
a feed-forward style network. Feed-forward net-
works as well as back-off n-gram models rely on a
finite history which results in predictions indepen-
dent of anything but a short context of words. A
recent side-by-side comparison between recurrent
and feed-forward style neural networks (Sunder-
meyer et al., 2013) has shown that recurrent ar-
chitectures outperform feed-forward networks in
a language modeling task, a similar problem to
modeling sequences over Minimum Translation
Units.
Furthermore, the input of our best model is a
bag-of-words representation of an MTU, unlike
the ordered source and target word n-grams used
by Crego and Yvon (2010) as well as Le et al.
(2012). Finally, we model both source and target
words in a single recurrent neural network. The
approach of Le et al. (2012) factorizes the joint
probability over an MTU sequence in a way that
suggests the use of separate neural network mod-
els for the source and the target sides, where each
model generates words on the respective side only.
Other work on applying recurrent neural net-
works to machine translation (Mikolov, 2012; Auli
et al., 2013; Kalchbrenner and Blunsom, 2013)
concentrated on word-based language and transla-
tion models, whereas we model Minimum Trans-
lation Units.
</bodyText>
<sectionHeader confidence="0.995767" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999664363636364">
Minimum Translation Unit models based on recur-
rent neural networks lead to substantial gains over
their classical n-gram back-off models. We intro-
duced two models of which the best improves ac-
curacy by up to 1.5 BLEU over the 1-best decoder
output, and by 0.8 BLEU over a trigram MTU
model in an n-best rescoring setting.
Our experiments have shown that representing
MTUs as bags-of-words leads to better accuracy
since this exploits similarities in the internal struc-
ture of Minimum Translation Units, which is not
possible when modeling them as atomic symbols.
We have also shown that our models are comple-
mentary to a very strong RNN language model
(Mikolov, 2012).
In future work, we would like to make the initial
version of the bag-of-words model computation-
ally more tractable using a better GPU implemen-
tation. This model combines the efficient bag-of-
words input representation with the ability to pre-
dict MTUs as single units while explicitly model-
ing the constituent words in an intermediate layer.
</bodyText>
<sectionHeader confidence="0.998827" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997305">
We would like to thank Kristina Toutanova for
providing a dataset and for helpful discussions re-
lated to this work. We also thank the four anony-
mous reviewers for their comments.
</bodyText>
<page confidence="0.997394">
27
</page>
<sectionHeader confidence="0.996238" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999930504761905">
Alexandre Allauzen, H´el`ene Bonneau-Maynard, Hai-
Son Le, Aur´elien Max, Guillaume Wisniewski,
Franc¸ois Yvon, Gilles Adda, Josep Maria Crego,
Adrien Lardilleux, Thomas Lavergne, and Artem
Sokolov. 2011. LIMSI @ WMT11. In Proc. of
WMT, pages 309–315, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and
Bhuvana Ramabhadran. 2012. Deep Neural Net-
work Language Models. In NAACL-HLT Work-
shop on the Future of Language Modeling for HLT,
pages 20–28, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Proc.
of EMNLP, October.
Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert,
Patrik Lambert, and Jos´e B. Mari˜no. 2005. Statis-
tical Machine Translation of Euparl Data by Using
bilingual n-grams. In Proc. of ACL Workshop on
Building and Using Parallel Texts, pages 133–136,
Jun.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Re-
search, 3:1137–1155.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467–479,
Dec.
Josep Crego and Franc¸ois Yvon. 2010. Factored bilin-
gual n-gram language models for statistical machine
translation. Machine Translation, 24(2):159–175.
Ahmad Emami and Frederick Jelinek. 2005. A Neu-
ral Syntactic Language Model. Machine Learning,
60(1-3):195–227, September.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning Semantic Representations
for the Phrase Translation Model. Technical Report
MSR-TR-2013-88, Microsoft Research, September.
Joshua Goodman. 2001. Classes for Fast Maximum
Entropy Training. In Proc. of ICASSP.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modified
Kneser-Ney Language Model Estimation. In Proc.
of ACL, August.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning Deep
Structured Semantic Models for Web Search using
Clickthrough Data. In Proc. of CIKM, October.
Nal Kalchbrenner and Phil Blunsom. 2013. Re-
current Continuous Translation Models. In Proc.
of EMNLP, pages 1700–1709, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In Proc. of NAACL Workshop
on Statistical Machine Translation, pages 102–121.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In
Proc. of HLT-NAACL, pages 127–133, Edmonton,
Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL Demo and Poster Sessions, pages
177–180, Prague, Czech Republic, Jun.
Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Proc. of HLT-NAACL, pages
39–48, Montr´eal, Canada. Association for Compu-
tational Linguistics.
Tom´a&amp;quot;s Mikolov, Karafi´at Martin, Luk´a&amp;quot;s Burget, Jan
Cernock´y, and Sanjeev Khudanpur. 2010. Recur-
rent Neural Network based Language Model. In
Proc. of INTERSPEECH, pages 1045–1048.
Tom´a&amp;quot;s Mikolov, Anoop Deoras, Daniel Povey, Luk´a&amp;quot;s
Burget, and Jan &amp;quot;Cernock´y. 2011a. Strategies
for Training Large Scale Neural Network Language
Models. In Proc. of ASRU, pages 196–201.
Tom´a&amp;quot;s Mikolov, Stefan Kombrink, Luk´a&amp;quot;s Burget, Jan
Cernock´y, and Sanjeev Khudanpur. 2011b. Ex-
tensions of Recurrent Neural Network Language
Model. In Proc. of ICASSP, pages 5528–5531.
Tom´a&amp;quot;s Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic Regularities in Continuous Space-
Word Representations. In Proc. of NAACL, pages
746–751, Stroudsburg, PA, USA, June. Association
for Computational Linguistics.
Tom´a&amp;quot;s Mikolov. 2012. Statistical Language Models
based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
pages 160–167, Sapporo, Japan, July.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in
Statistical Machine Translation. In Proc. of NAACL,
pages 8–16, New York, Jun.
</reference>
<page confidence="0.976641">
28
</page>
<reference confidence="0.999902463414634">
Ariya Rastrow, Sanjeev Khudanpur, and Mark Dredze.
2012. Revisiting the Case for Explicit Syntactic
Information in Language Models. In NAACL-HLT
Workshop on the Future of Language Modeling for
HLT, pages 50–58. Association for Computational
Linguistics.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1986. Learning Internal Representations
by Error Propagation. In Symposium on Parallel and
Distributed Processing.
Holger Schwenk, Marta R. Costa-juss`a, and Jos´e A. R.
Fonollosa. 2007. Smooth Bilingual N-Gram Trans-
lation. In Proc. of EMNLP, pages 430–438, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, Pruned or Continuous Space
Language Models on a GPU for Statistical Machine
Translation. In NAACL-HLT Workshop on the Fu-
ture of Language Modeling for HLT, pages 11–19.
Association for Computational Linguistics.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schl¨uter, and Hermann Ney.
2013. Comparison of Feedforward and Recurrent
Neural Network Language Models. In IEEE In-
ternational Conference on Acoustics, Speech, and
Signal Processing, pages 8430–8434, Vancouver,
Canada, May.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with Large-scale
Neural Language Models improves Translation. In
Proc. of EMNLP. Association for Computational
Linguistics, October.
Hui Zhang, Kristina Toutanova, Chris Quirk, and Jian-
feng Gao. 2013. Beyond left-to-right: Multiple de-
composition structures for smt. In Proc. of NAACL,
pages 12–21, Atlanta, Georgia, June. Association
for Computational Linguistics.
Geoff Zweig and Konstantin Makarychev. 2013.
Speed Regularization and Optimality in Word Class-
ing. In Proc. of ICASSP.
</reference>
<page confidence="0.999117">
29
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.537594">
<title confidence="0.999816">Minimum Translation Modeling with Recurrent Neural Networks</title>
<author confidence="0.999636">Yuening Hu Michael Auli</author>
<author confidence="0.999636">Qin Gao</author>
<author confidence="0.999636">Jianfeng Gao</author>
<affiliation confidence="0.888558">Department of Computer Science Microsoft Research University of Maryland, College Park Redmond, WA, USA</affiliation>
<abstract confidence="0.982401666666667">We introduce recurrent neural networkbased Minimum Translation Unit (MTU) models which make predictions based on an unbounded history of previous bilingual contexts. Traditional back-off n-gram models suffer under the sparse nature of MTUs which makes estimation of highorder sequence models challenging. We tackle the sparsity problem by modeling MTUs both as bags-of-words and as a sequence of individual source and target words. Our best results improve the output of a phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.5 BLEU, and we outperform the traditional n-gram based MTU approach by up to 0.8 BLEU.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandre Allauzen</author>
<author>H´el`ene Bonneau-Maynard</author>
<author>HaiSon Le</author>
<author>Aur´elien Max</author>
<author>Guillaume Wisniewski</author>
<author>Franc¸ois Yvon</author>
<author>Gilles Adda</author>
<author>Josep Maria Crego</author>
<author>Adrien Lardilleux</author>
<author>Thomas Lavergne</author>
<author>Artem Sokolov</author>
</authors>
<date>2011</date>
<booktitle>LIMSI @ WMT11. In Proc. of WMT,</booktitle>
<pages>309--315</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="2351" citStr="Allauzen et al., 2011" startWordPosition="349" endWordPosition="352">eafield et al., 2013); bilingual units are much sparser than words and are therefore even harder to estimate. Another drawback of n-gram models is that future predictions are based on a limited amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (Bengio et al., 2003; Schwenk et al., 2007; Schwenk et al., 2012; Vaswani et al., 2013), as well as translation modeling (Allauzen et al., 2011; Le et al., 2012; Gao et al., 2013). These models tackle the data sparsity problem by representing words in continuous space rather than as discrete units. Similar words are grouped in the same sub-space rather than being treated as separate entities. Neural network models can be seen as functions over continuous representations exploiting the similarity between words, thereby making the estimation of probabilities over higherorder n-grams easier. However, feed-forward networks do not directly address the limited context issue either, since predictions are based on a fixed-size context, simil</context>
</contexts>
<marker>Allauzen, Bonneau-Maynard, Le, Max, Wisniewski, Yvon, Adda, Crego, Lardilleux, Lavergne, Sokolov, 2011</marker>
<rawString>Alexandre Allauzen, H´el`ene Bonneau-Maynard, HaiSon Le, Aur´elien Max, Guillaume Wisniewski, Franc¸ois Yvon, Gilles Adda, Josep Maria Crego, Adrien Lardilleux, Thomas Lavergne, and Artem Sokolov. 2011. LIMSI @ WMT11. In Proc. of WMT, pages 309–315, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ebru Arisoy</author>
<author>Tara N Sainath</author>
<author>Brian Kingsbury</author>
<author>Bhuvana Ramabhadran</author>
</authors>
<title>Deep Neural Network Language Models.</title>
<date>2012</date>
<booktitle>In NAACL-HLT Workshop on the Future of Language Modeling for HLT,</booktitle>
<pages>20--28</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3490" citStr="Arisoy et al., 2012" startWordPosition="524" endWordPosition="527">context issue either, since predictions are based on a fixed-size context, similar to back-off n-gram models. We therefore focus in this paper on recurrent neural network architectures, which address the limited context issue by basing predictions on an unbounded history of previous events which allows to capture long-span dependencies. Recurrent architectures have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012) outperforming multi-layer feedforward based networks in perplexity and word error rate for speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). Recent work has also shown successful applications to machine translation (Mikolov, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013). We extend this work by modeling Minimum Translation Units with recurrent neural networks. Specifically, we introduce two recurrent neural network-based MTU models to address the is20 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20–29, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Words MTUs Tokens 34,769,416 14,853,0</context>
</contexts>
<marker>Arisoy, Sainath, Kingsbury, Ramabhadran, 2012</marker>
<rawString>Ebru Arisoy, Tara N. Sainath, Brian Kingsbury, and Bhuvana Ramabhadran. 2012. Deep Neural Network Language Models. In NAACL-HLT Workshop on the Future of Language Modeling for HLT, pages 20–28, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint Language and Translation Modeling with Recurrent Neural Networks.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<contexts>
<context position="3626" citStr="Auli et al., 2013" startWordPosition="545" endWordPosition="548">aper on recurrent neural network architectures, which address the limited context issue by basing predictions on an unbounded history of previous events which allows to capture long-span dependencies. Recurrent architectures have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012) outperforming multi-layer feedforward based networks in perplexity and word error rate for speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). Recent work has also shown successful applications to machine translation (Mikolov, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013). We extend this work by modeling Minimum Translation Units with recurrent neural networks. Specifically, we introduce two recurrent neural network-based MTU models to address the is20 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20–29, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Words MTUs Tokens 34,769,416 14,853,062 Types 143,524 1,315,512 Singleton types 34.9% 80.1% Table 1: Token and type counts for both source and target words as well as MTUs b</context>
<context position="29871" citStr="Auli et al., 2013" startWordPosition="4994" endWordPosition="4997">nput of our best model is a bag-of-words representation of an MTU, unlike the ordered source and target word n-grams used by Crego and Yvon (2010) as well as Le et al. (2012). Finally, we model both source and target words in a single recurrent neural network. The approach of Le et al. (2012) factorizes the joint probability over an MTU sequence in a way that suggests the use of separate neural network models for the source and the target sides, where each model generates words on the respective side only. Other work on applying recurrent neural networks to machine translation (Mikolov, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013) concentrated on word-based language and translation models, whereas we model Minimum Translation Units. 7 Conclusion and Future Work Minimum Translation Unit models based on recurrent neural networks lead to substantial gains over their classical n-gram back-off models. We introduced two models of which the best improves accuracy by up to 1.5 BLEU over the 1-best decoder output, and by 0.8 BLEU over a trigram MTU model in an n-best rescoring setting. Our experiments have shown that representing MTUs as bags-of-words leads to better accuracy since this exploits</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint Language and Translation Modeling with Recurrent Neural Networks. In Proc. of EMNLP, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adri`a de Gispert</author>
<author>Patrik Lambert</author>
<author>Jos´e B Mari˜no</author>
</authors>
<title>Statistical Machine Translation of Euparl Data by Using bilingual n-grams.</title>
<date>2005</date>
<booktitle>In Proc. of ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>133--136</pages>
<marker>Banchs, Crego, de Gispert, Lambert, Mari˜no, 2005</marker>
<rawString>Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, and Jos´e B. Mari˜no. 2005. Statistical Machine Translation of Euparl Data by Using bilingual n-grams. In Proc. of ACL Workshop on Building and Using Parallel Texts, pages 133–136, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2228" citStr="Bengio et al., 2003" startWordPosition="328" endWordPosition="331">ficult due to data sparsity issues associated with large n-grams, even when training on over one hundred billion words (Heafield et al., 2013); bilingual units are much sparser than words and are therefore even harder to estimate. Another drawback of n-gram models is that future predictions are based on a limited amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (Bengio et al., 2003; Schwenk et al., 2007; Schwenk et al., 2012; Vaswani et al., 2013), as well as translation modeling (Allauzen et al., 2011; Le et al., 2012; Gao et al., 2013). These models tackle the data sparsity problem by representing words in continuous space rather than as discrete units. Similar words are grouped in the same sub-space rather than being treated as separate entities. Neural network models can be seen as functions over continuous representations exploiting the similarity between words, thereby making the estimation of probabilities over higherorder n-grams easier. However, feed-forward ne</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A Neural Probabilistic Language Model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="24217" citStr="Brown et al., 1992" startWordPosition="4011" endWordPosition="4014">e use the new set of log-linear weights to rescore the test set n-best list. Neural Network Setup. We trained the recurrent neural network models on between 88% and 93% of each data set and used the remainder as validation data. The vocabulary of the atomic MTU RNN model is comprised of all MTU types which were observed more than once in the training data.5 Similarly, we modeled all non-singleton words for the bag-of-words MTU RNN model. We obtain classes for words or MTUs using a version of Brown-Clustering with an additional regularization term to optimize the runtime of the language model (Brown et al., 1992; Zweig and Makarychev, 2013). Direct connections use features over unigrams, bigrams and trigrams of words or MTUs, depending on the model. Features are hashed to a table with at most 500 million values following Mikolov et al. (2011a). We use the standard settings for the model with the default learning rate α = 0.1 that decays exponentially if the validation set entropy does not decrease. Back propagation through time computes error gradients over the past twenty time steps. Training is stopped after 20 epochs or when the validation entropy does not decrease over two epochs. Throughout, we </context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep Crego</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Factored bilingual n-gram language models for statistical machine translation.</title>
<date>2010</date>
<journal>Machine Translation,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="1465" citStr="Crego and Yvon, 2010" startWordPosition="210" endWordPosition="213">up to 1.5 BLEU, and we outperform the traditional n-gram based MTU approach by up to 0.8 BLEU. 1 Introduction Classical phrase-based translation models rely heavily on the language model and the reordering model to capture dependencies between phrases. Sequence models over Minimum Translation Units (MTUs) have been shown to complement both syntax-based (Quirk and Menezes, 2006) as well as phrase-based (Zhang et al., 2013) models by explicitly modeling relationships between phrases. MTU models have been traditionally estimated using standard back-off n-gram techniques (Quirk and Menezes, 2006; Crego and Yvon, 2010; Zhang et al., 2013), similar to wordbased language models (§2). However, the estimation of higher-order n-gram models becomes increasingly difficult due to data sparsity issues associated with large n-grams, even when training on over one hundred billion words (Heafield et al., 2013); bilingual units are much sparser than words and are therefore even harder to estimate. Another drawback of n-gram models is that future predictions are based on a limited amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, seve</context>
<context position="28514" citStr="Crego and Yvon, 2010" startWordPosition="4772" endWordPosition="4775">at this is the case. For German to English translation accuracy improves by 0.2 to 0.3 BLEU over the RNNLM alone, with gains of up to 1.3 BLEU over the baseline and up to 0.7 BLEU over the n-gram MTU model. Improvements for French to English are lower but we can see some gains on news2011 and on the dev set. Overall, we improve accuracy on the French to English task by up to 1.5 BLEU over the decoder 1-best, and by up to 0.8 BLEU over the n-gram MTU model. 6 Related Work Our approach of modeling Minimum Translation Units is very much in line with recent work on ngram-based translation models (Crego and Yvon, 2010), and more recently, continuous space-based translation models (Le et al., 2012). The models presented in this paper differ in a number of key aspects: We use a recurrent architecture representing an unbounded history of MTUs rather than a feed-forward style network. Feed-forward networks as well as back-off n-gram models rely on a finite history which results in predictions independent of anything but a short context of words. A recent side-by-side comparison between recurrent and feed-forward style neural networks (Sundermeyer et al., 2013) has shown that recurrent architectures outperform f</context>
</contexts>
<marker>Crego, Yvon, 2010</marker>
<rawString>Josep Crego and Franc¸ois Yvon. 2010. Factored bilingual n-gram language models for statistical machine translation. Machine Translation, 24(2):159–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmad Emami</author>
<author>Frederick Jelinek</author>
</authors>
<title>A Neural Syntactic Language Model.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="12856" citStr="Emami and Jelinek, 2005" startWordPosition="2071" endWordPosition="2074">aining, the output layer represents posteriors p(mt+1|mtt−n+1, ht), the probability of the next MTU given the previous ninput MTUs mtt−n+1 = mt, . . . , mt−n+1 and the current hidden layer configuration ht. Naive computation of the probability distribution over the next MTU is very expensive for large vocabularies, such as commonly encountered for MTU models (Table 1). A well established efficiency trick assigns each possible output to a unique class and then uses a two-step process to find the probability of an MTU, instead of computing the probability of all possible outputs (Goodman, 2001; Emami and Jelinek, 2005; Mikolov et al., 2011b). Under this scheme we compute the probability of an MTU by multiplying the probability of its class cit with the probability of the depicted them for simplicity as a connection between the current input vector mt and the output layer. minimal unit conditioned on the class: p(mt+1|mtt−n+1, ht) = p(cit|mtt−n+1, ht) p(mt+1|cit, mtt−n+1, ht) This factorization reduces the complexity of computing the output probabilities from O(|V |) to O(|C |+ maxi |ci|) where |C |is the number of classes and |ci |is the number of minimal units � in class ci. The best case complexity O( |V</context>
</contexts>
<marker>Emami, Jelinek, 2005</marker>
<rawString>Ahmad Emami and Frederick Jelinek. 2005. A Neural Syntactic Language Model. Machine Learning, 60(1-3):195–227, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Wen-tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning Semantic Representations for the Phrase Translation Model.</title>
<date>2013</date>
<tech>Technical Report MSR-TR-2013-88,</tech>
<institution>Microsoft Research,</institution>
<contexts>
<context position="2387" citStr="Gao et al., 2013" startWordPosition="357" endWordPosition="360">re much sparser than words and are therefore even harder to estimate. Another drawback of n-gram models is that future predictions are based on a limited amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (Bengio et al., 2003; Schwenk et al., 2007; Schwenk et al., 2012; Vaswani et al., 2013), as well as translation modeling (Allauzen et al., 2011; Le et al., 2012; Gao et al., 2013). These models tackle the data sparsity problem by representing words in continuous space rather than as discrete units. Similar words are grouped in the same sub-space rather than being treated as separate entities. Neural network models can be seen as functions over continuous representations exploiting the similarity between words, thereby making the estimation of probabilities over higherorder n-grams easier. However, feed-forward networks do not directly address the limited context issue either, since predictions are based on a fixed-size context, similar to back-off n-gram models. We the</context>
</contexts>
<marker>Gao, He, Yih, Deng, 2013</marker>
<rawString>Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2013. Learning Semantic Representations for the Phrase Translation Model. Technical Report MSR-TR-2013-88, Microsoft Research, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Classes for Fast Maximum Entropy Training.</title>
<date>2001</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="12831" citStr="Goodman, 2001" startWordPosition="2068" endWordPosition="2070">ction. After training, the output layer represents posteriors p(mt+1|mtt−n+1, ht), the probability of the next MTU given the previous ninput MTUs mtt−n+1 = mt, . . . , mt−n+1 and the current hidden layer configuration ht. Naive computation of the probability distribution over the next MTU is very expensive for large vocabularies, such as commonly encountered for MTU models (Table 1). A well established efficiency trick assigns each possible output to a unique class and then uses a two-step process to find the probability of an MTU, instead of computing the probability of all possible outputs (Goodman, 2001; Emami and Jelinek, 2005; Mikolov et al., 2011b). Under this scheme we compute the probability of an MTU by multiplying the probability of its class cit with the probability of the depicted them for simplicity as a connection between the current input vector mt and the output layer. minimal unit conditioned on the class: p(mt+1|mtt−n+1, ht) = p(cit|mtt−n+1, ht) p(mt+1|cit, mtt−n+1, ht) This factorization reduces the complexity of computing the output probabilities from O(|V |) to O(|C |+ maxi |ci|) where |C |is the number of classes and |ci |is the number of minimal units � in class ci. The b</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua Goodman. 2001. Classes for Fast Maximum Entropy Training. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Ivan Pouzyrevsky</author>
<author>Jonathan H Clark</author>
<author>Philipp Koehn</author>
</authors>
<title>Scalable Modified Kneser-Ney Language Model Estimation.</title>
<date>2013</date>
<booktitle>In Proc. of ACL,</booktitle>
<contexts>
<context position="1751" citStr="Heafield et al., 2013" startWordPosition="253" endWordPosition="256">anslation Units (MTUs) have been shown to complement both syntax-based (Quirk and Menezes, 2006) as well as phrase-based (Zhang et al., 2013) models by explicitly modeling relationships between phrases. MTU models have been traditionally estimated using standard back-off n-gram techniques (Quirk and Menezes, 2006; Crego and Yvon, 2010; Zhang et al., 2013), similar to wordbased language models (§2). However, the estimation of higher-order n-gram models becomes increasingly difficult due to data sparsity issues associated with large n-grams, even when training on over one hundred billion words (Heafield et al., 2013); bilingual units are much sparser than words and are therefore even harder to estimate. Another drawback of n-gram models is that future predictions are based on a limited amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (Bengio et al., 2003; Schwenk et al., 2007; Schwenk et al., 2012; Vaswani et al., 2013), as well as translation modeling (Allauzen et al., 2011</context>
<context position="8414" citStr="Heafield et al. (2013)" startWordPosition="1325" endWordPosition="1328">larger contexts, then we quickly run into data sparsity issues. To illustrate this point, consider the parameter growth of an n-gram model which is driven by the vocabulary size |V |and the n-gram order n: O(|V |n). Clearly, the exact estimation of higher-order n21 gram probabilities becomes more difficult with large n, leading to the estimation of events with increasingly sparse statistics, or having to rely on statistics from lower-order events with backoff models, which is less desirable. Even wordbased language models rarely ventured so far much beyond 5-gram statistics as demonstrated by Heafield et al. (2013) who trained a, by today’s standards, very large 5-gram model on 130B words. Data sparsity is therefore an even more significant issue for MTU models relying on much larger vocabularies. In our setting, the MTU vocabulary is an order of magnitude larger than a word vocabulary obtained from the same data (Table 1). Furthermore, most MTUs are observed only once making the reliable estimation of probabilities very challenging. Neural network-based sequence models tackle the data sparsity problem by learning continuous word representations, that group similar words together in continuous space. Fo</context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable Modified Kneser-Ney Language Model Estimation. In Proc. of ACL, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Po-Sen Huang</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
<author>Li Deng</author>
<author>Alex Acero</author>
<author>Larry Heck</author>
</authors>
<title>Learning Deep Structured Semantic Models for Web Search using Clickthrough Data.</title>
<date>2013</date>
<booktitle>In Proc. of CIKM,</booktitle>
<contexts>
<context position="14433" citStr="Huang et al. (2013)" startWordPosition="2336" endWordPosition="2339">s which leads to large vocabularies requiring large parameter sets and expensive inference. However, similar MTUs may share the same words, or words which are related in continuous space. The atomic MTU model does not exploit this since it cannot access the internal structure of a minimal unit. The approach we pursue next is to break MTUs into individual source and target words (Le et al., 2012) in order to exploit structural similarities between infrequently observed minimal units. Singletons represent the vast majority of our MTU vocabulary (Table 1). This resembles the wordhashing trick of Huang et al. (2013) who represented individual words as a bag-of-character ngrams to reduce the vocabulary size of a neural network-based model in an information retrieval setting.2 We first describe a theoretically appealing but computationally expensive model and then discuss a more practical variation. The input layer of this model accepts the current minimal unit as a K-ofN vector representing K source and target words as opposed to the 1-of-N encoding of entire MTUs in the previous model (Figure 4). Larger MTUs may contain the same word more than once and we simply adjust their count to one.3 Different to t</context>
<context position="16038" citStr="Huang et al. (2013)" startWordPosition="2613" endWordPosition="2616">t as a bag-of-words and the output layer yt is a probability distribution over possible next MTUs depending on the activations of the word layer wt representing source and target words of minimal units. previous model, the input vector has now multiple active entries whose signals are absorbed into the new hidden layer configuration. This bag-of-words encoding of minimal units dramatically reduces the vocabulary size but it inevitably maps different MTUs to the same encoding. On our data set, we observe less than 0.2% of minimal units that are involved in collisions, a rate that is similar to Huang et al. (2013). In practice collisions are unlikely to affect accuracy in our setting because MTUs that are mapped to the same encoding usually do not differ much in semantic meaning as illustrated by the following examples: erfolg haben → succeed collides with haben erfolg → succeed, or damit , → to and, damit → to; in both examples either the auxiliary verb haben or the comma changes position, neither of which significantly changes the meaning for this particular pair of MTUs. The structure of the bag-of-words MTU RNN models is shown in Figure 4. Similar to the atomic MTU RNN model (§3), the hidden layer </context>
</contexts>
<marker>Huang, He, Gao, Deng, Acero, Heck, 2013</marker>
<rawString>Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning Deep Structured Semantic Models for Web Search using Clickthrough Data. In Proc. of CIKM, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent Continuous Translation Models.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1700--1709</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="3659" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="549" endWordPosition="553">eural network architectures, which address the limited context issue by basing predictions on an unbounded history of previous events which allows to capture long-span dependencies. Recurrent architectures have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012) outperforming multi-layer feedforward based networks in perplexity and word error rate for speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). Recent work has also shown successful applications to machine translation (Mikolov, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013). We extend this work by modeling Minimum Translation Units with recurrent neural networks. Specifically, we introduce two recurrent neural network-based MTU models to address the is20 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 20–29, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Words MTUs Tokens 34,769,416 14,853,062 Types 143,524 1,315,512 Singleton types 34.9% 80.1% Table 1: Token and type counts for both source and target words as well as MTUs based on the WMT 2006 German to En</context>
<context position="29904" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="4998" endWordPosition="5001">del is a bag-of-words representation of an MTU, unlike the ordered source and target word n-grams used by Crego and Yvon (2010) as well as Le et al. (2012). Finally, we model both source and target words in a single recurrent neural network. The approach of Le et al. (2012) factorizes the joint probability over an MTU sequence in a way that suggests the use of separate neural network models for the source and the target sides, where each model generates words on the respective side only. Other work on applying recurrent neural networks to machine translation (Mikolov, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013) concentrated on word-based language and translation models, whereas we model Minimum Translation Units. 7 Conclusion and Future Work Minimum Translation Unit models based on recurrent neural networks lead to substantial gains over their classical n-gram back-off models. We introduced two models of which the best improves accuracy by up to 1.5 BLEU over the 1-best decoder output, and by 0.8 BLEU over a trigram MTU model in an n-best rescoring setting. Our experiments have shown that representing MTUs as bags-of-words leads to better accuracy since this exploits similarities in the internal str</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent Continuous Translation Models. In Proc. of EMNLP, pages 1700–1709, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between european languages.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<contexts>
<context position="22347" citStr="Koehn and Monz, 2006" startWordPosition="3692" endWordPosition="3695"> rate training (MERT; Och, 2003). The 1-best output by the phrase-based decoder is the baseline accuracy. As a second baseline we experiment with a trigram back-off MTU model trained on all extracted MTUs, denoted as n-gram MTU. The trigram MTU model is estimated with the same modified Kneser-Ney framework as the target side language model. All MTU models are trained in target left-to-right MTU order which performed well in initial experiments. Evaluation. We test our approach on two different data sets. First, we train a German to English system based on the data of the WMT 2006 shared task (Koehn and Monz, 2006). The parallel corpus includes about 35M words of parliamentary proceedings for training, a development set and two test sets with 2000 sentences each. Second, we experiment with a French to English system based on 102M words of training data from the WMT 2012 campaign. The majority of the training data set is parliamentary proceedings except for about 5m words which are newswire; all MTU models are trained on the newswire subset since we found similar accuracy to using all data in initial experiments. We evaluate on four newswire domain test sets from 2008, 2010 and 2011 as well as the 2010 s</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Philipp Koehn and Christof Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In Proc. of NAACL Workshop on Statistical Machine Translation, pages 102–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="6185" citStr="Koehn et al., 2003" startWordPosition="955" endWordPosition="958">with empty source or target sides, thereby allowing insertion or deletion phrase pairs. The two basic requirements for MTUs are that there are no overlapping word alignment links between phrase pairs and it should not be possible to extract smaller phrase pairs without violating the word alignment constraints. Informally, we can think of MTUs as small phrase pairs that cannot be broken down any further without violating the two requirements. Minimum Translation Units partition a sentence pair into a set of minimal bilingual units or tuples obtained by an algorithm similar to phraseextraction (Koehn et al., 2003). Figure 1 illustrates such a partitioning. Modeling minimal units has two advantages over considering larger phrase pairs that are effectively composed of MTUs: First, minimal units result in a unique partitioning of a sentence pair. This has the advantage that we avoid modeling spurious derivations, that is, multiple derivations generating the same sentence pair. Second, minimal units result in smaller models with a smoother distribution than models based on composed units (Zhang et al., 2013). Sentence pairs can be generated in multiple orders, such as left-to-right or right-to-left, either</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proc. of HLT-NAACL, pages 127–133, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="21214" citStr="Koehn et al., 2007" startWordPosition="3514" endWordPosition="3517">as for the original bag-of-words model. Overall, the simplified model retains the bag-ofwords input representation of the original model, while permitting the efficient factorization of the word-output layer into classes. We evaluate the effectiveness of both the atomic MTU RNN model (§3) and the simplified bag-ofwords MTU RNN model (§4) in an n-best rescoring setting, comparing against a trigram back-off MTU model as well as the phrasal decoder 1-best output which we denote as the baseline. 5.1 Experimental Setup Baselines. We experiment with an in-house phrase-based system similar to Moses (Koehn et al., 2007), scoring translations by a set of common features including maximum likelihood estimates of source given target mappings pMLE(e|f) and vice versa pMLE(f|e), as well as lexical weighting estimates pLW (e|f) and pLW (f|e), word and phrase-penalties, a linear distortion feature and a lexicalized reordering feature. The baseline includes a standard modified Kneser-Ney wordbased language model trained on the target-side of the parallel corpora described below. Log-linear weights are estimated with minimum error rate training (MERT; Och, 2003). The 1-best output by the phrase-based decoder is the b</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proc. of ACL Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous Space Translation Models with Neural Networks.</title>
<date>2012</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>39--48</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="2368" citStr="Le et al., 2012" startWordPosition="353" endWordPosition="356">bilingual units are much sparser than words and are therefore even harder to estimate. Another drawback of n-gram models is that future predictions are based on a limited amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (Bengio et al., 2003; Schwenk et al., 2007; Schwenk et al., 2012; Vaswani et al., 2013), as well as translation modeling (Allauzen et al., 2011; Le et al., 2012; Gao et al., 2013). These models tackle the data sparsity problem by representing words in continuous space rather than as discrete units. Similar words are grouped in the same sub-space rather than being treated as separate entities. Neural network models can be seen as functions over continuous representations exploiting the similarity between words, thereby making the estimation of probabilities over higherorder n-grams easier. However, feed-forward networks do not directly address the limited context issue either, since predictions are based on a fixed-size context, similar to back-off n-</context>
<context position="14212" citStr="Le et al., 2012" startWordPosition="2301" endWordPosition="2304">ses. Figure 3 illustrates how classing changes the structure of the network by adding an additional output layer for the class probabilities. 4 Bag-of-words MTU RNN Model The previous model treats MTUs as atomic symbols which leads to large vocabularies requiring large parameter sets and expensive inference. However, similar MTUs may share the same words, or words which are related in continuous space. The atomic MTU model does not exploit this since it cannot access the internal structure of a minimal unit. The approach we pursue next is to break MTUs into individual source and target words (Le et al., 2012) in order to exploit structural similarities between infrequently observed minimal units. Singletons represent the vast majority of our MTU vocabulary (Table 1). This resembles the wordhashing trick of Huang et al. (2013) who represented individual words as a bag-of-character ngrams to reduce the vocabulary size of a neural network-based model in an information retrieval setting.2 We first describe a theoretically appealing but computationally expensive model and then discuss a more practical variation. The input layer of this model accepts the current minimal unit as a K-ofN vector representi</context>
<context position="28594" citStr="Le et al., 2012" startWordPosition="4783" endWordPosition="4786">3 BLEU over the RNNLM alone, with gains of up to 1.3 BLEU over the baseline and up to 0.7 BLEU over the n-gram MTU model. Improvements for French to English are lower but we can see some gains on news2011 and on the dev set. Overall, we improve accuracy on the French to English task by up to 1.5 BLEU over the decoder 1-best, and by up to 0.8 BLEU over the n-gram MTU model. 6 Related Work Our approach of modeling Minimum Translation Units is very much in line with recent work on ngram-based translation models (Crego and Yvon, 2010), and more recently, continuous space-based translation models (Le et al., 2012). The models presented in this paper differ in a number of key aspects: We use a recurrent architecture representing an unbounded history of MTUs rather than a feed-forward style network. Feed-forward networks as well as back-off n-gram models rely on a finite history which results in predictions independent of anything but a short context of words. A recent side-by-side comparison between recurrent and feed-forward style neural networks (Sundermeyer et al., 2013) has shown that recurrent architectures outperform feed-forward networks in a language modeling task, a similar problem to modeling </context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous Space Translation Models with Neural Networks. In Proc. of HLT-NAACL, pages 39–48, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´as Mikolov</author>
<author>Karafi´at Martin</author>
<author>Luk´as Burget</author>
<author>Jan Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent Neural Network based Language Model.</title>
<date>2010</date>
<booktitle>In Proc. of INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Martin, Burget, Cernock´y, Khudanpur, 2010</marker>
<rawString>Tom´a&amp;quot;s Mikolov, Karafi´at Martin, Luk´a&amp;quot;s Burget, Jan Cernock´y, and Sanjeev Khudanpur. 2010. Recurrent Neural Network based Language Model. In Proc. of INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tom´as Mikolov</author>
<author>Anoop Deoras</author>
<author>Daniel Povey</author>
<author>Luk´as Burget</author>
<author>Jan</author>
</authors>
<title>Cernock´y. 2011a. Strategies for Training Large Scale Neural Network Language Models.</title>
<booktitle>In Proc. of ASRU,</booktitle>
<pages>196--201</pages>
<marker>Mikolov, Deoras, Povey, Burget, Jan, </marker>
<rawString>Tom´a&amp;quot;s Mikolov, Anoop Deoras, Daniel Povey, Luk´a&amp;quot;s Burget, and Jan &amp;quot;Cernock´y. 2011a. Strategies for Training Large Scale Neural Network Language Models. In Proc. of ASRU, pages 196–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´as Mikolov</author>
<author>Stefan Kombrink</author>
<author>Luk´as Burget</author>
<author>Jan Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of Recurrent Neural Network Language Model.</title>
<date>2011</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<pages>5528--5531</pages>
<marker>Mikolov, Kombrink, Burget, Cernock´y, Khudanpur, 2011</marker>
<rawString>Tom´a&amp;quot;s Mikolov, Stefan Kombrink, Luk´a&amp;quot;s Burget, Jan Cernock´y, and Sanjeev Khudanpur. 2011b. Extensions of Recurrent Neural Network Language Model. In Proc. of ICASSP, pages 5528–5531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´as Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic Regularities in Continuous SpaceWord Representations.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>746--751</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="9191" citStr="Mikolov et al., 2013" startWordPosition="1446" endWordPosition="1449">g on much larger vocabularies. In our setting, the MTU vocabulary is an order of magnitude larger than a word vocabulary obtained from the same data (Table 1). Furthermore, most MTUs are observed only once making the reliable estimation of probabilities very challenging. Neural network-based sequence models tackle the data sparsity problem by learning continuous word representations, that group similar words together in continuous space. For example, the distributional representations induced by recurrent neural networks have been found to have interesting syntactic and semantic regularities (Mikolov et al., 2013). Furthermore, these representations can be exploited to estimate more reliable statistics over higher-order n-grams than with discrete word units. Recurrent neural networks go beyond fixed-size contexts and allow the model to keep track of long-span dependencies that are important for future predictions. In the next sections we will present Minimum Translation Unit models based on recurrent architectures. 3 Atomic MTU RNN Model The first model we introduce is based on the recurrent neural network language model of Mikolov et al. (2010). We frame the problem as a traditional sequence modeling </context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tom´a&amp;quot;s Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic Regularities in Continuous SpaceWord Representations. In Proc. of NAACL, pages 746–751, Stroudsburg, PA, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´as Mikolov</author>
</authors>
<title>Statistical Language Models based on Neural Networks.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="3359" citStr="Mikolov, 2012" startWordPosition="506" endWordPosition="507">timation of probabilities over higherorder n-grams easier. However, feed-forward networks do not directly address the limited context issue either, since predictions are based on a fixed-size context, similar to back-off n-gram models. We therefore focus in this paper on recurrent neural network architectures, which address the limited context issue by basing predictions on an unbounded history of previous events which allows to capture long-span dependencies. Recurrent architectures have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012) outperforming multi-layer feedforward based networks in perplexity and word error rate for speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). Recent work has also shown successful applications to machine translation (Mikolov, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013). We extend this work by modeling Minimum Translation Units with recurrent neural networks. Specifically, we introduce two recurrent neural network-based MTU models to address the is20 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages </context>
<context position="10520" citStr="Mikolov (2012)" startWordPosition="1673" endWordPosition="1674">e model is factored into an input layer, a hidden layer with recurrent connections, and an output layer (Figure 2). The input layer encodes the MTU at time t as a 1-of-N vector mt with all values being zero except for the entry representing the MTU. The output layer yt represents a probability distribution over possible next MTUs; both the input and output layers are of size |V |, the size of the MTU vocabulary. The hidden layer state ht encodes the history of all MTUs observed in the mt Figure 2: Structure of the atomic recurrent neural network MTU model following the word-based RNN model of Mikolov (2012). sequence up to time step t. The state of the hidden layer is determined by the input layer and the hidden layer configuration of the previous time step ht−1. The weights of the connections between the layers are summarized in a number of matrices: U represents weights from the input layer to the hidden layer, and W represents connections from the previous hidden layer to the current hidden layer. Matrix V contains weights between the current hidden layer and the output layer. The hidden and output layers are computed via a series of matrix-vector products and nonlinearities: ht = s(Umt + Wht</context>
<context position="26949" citStr="Mikolov (2012" startWordPosition="4494" endWordPosition="4495">he results (Table 2 and Table 3) for the bag-of-words model (BoW MTU RNN) clearly show that this is the case for both language pairs. We significantly improve over the n-gram MTU model as well as the atomic RNN model on all test sets. We observe gains of up to 0.5 BLEU over the n-gram MTU model for German to English as well as French to English; improvements over the decoder baseline are up to 1.2 BLEU for French to English. How do our models compare to other neural network approaches that rely only on target side information? To answer this question we compare to the strong language model of Mikolov (2012; RNNLM) which has recently improved the stateof-the-art in language modeling perplexity. The results (Table 2 and Table 3) show that RNNLM performs competitively. However, our approaches model translation since we use both source and target information as opposed to scoring only the fluency of the target side, such as done by RNNLM. Can our models act complementary to a strong RNN language model? Our final experiment combines the atomic MTU RNN model, the BoW 26 dev news2008 news2010 news2011 newssyscomb2010 Baseline 24.3 20.5 24.4 25.1 24.3 n-gram MTU 24.6 20.8 24.4 25.8 24.3 atomic MTU RNN </context>
<context position="29852" citStr="Mikolov, 2012" startWordPosition="4992" endWordPosition="4993">thermore, the input of our best model is a bag-of-words representation of an MTU, unlike the ordered source and target word n-grams used by Crego and Yvon (2010) as well as Le et al. (2012). Finally, we model both source and target words in a single recurrent neural network. The approach of Le et al. (2012) factorizes the joint probability over an MTU sequence in a way that suggests the use of separate neural network models for the source and the target sides, where each model generates words on the respective side only. Other work on applying recurrent neural networks to machine translation (Mikolov, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013) concentrated on word-based language and translation models, whereas we model Minimum Translation Units. 7 Conclusion and Future Work Minimum Translation Unit models based on recurrent neural networks lead to substantial gains over their classical n-gram back-off models. We introduced two models of which the best improves accuracy by up to 1.5 BLEU over the 1-best decoder output, and by 0.8 BLEU over a trigram MTU model in an n-best rescoring setting. Our experiments have shown that representing MTUs as bags-of-words leads to better accuracy </context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tom´a&amp;quot;s Mikolov. 2012. Statistical Language Models based on Neural Networks. Ph.D. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="21758" citStr="Och, 2003" startWordPosition="3595" endWordPosition="3596">in-house phrase-based system similar to Moses (Koehn et al., 2007), scoring translations by a set of common features including maximum likelihood estimates of source given target mappings pMLE(e|f) and vice versa pMLE(f|e), as well as lexical weighting estimates pLW (e|f) and pLW (f|e), word and phrase-penalties, a linear distortion feature and a lexicalized reordering feature. The baseline includes a standard modified Kneser-Ney wordbased language model trained on the target-side of the parallel corpora described below. Log-linear weights are estimated with minimum error rate training (MERT; Och, 2003). The 1-best output by the phrase-based decoder is the baseline accuracy. As a second baseline we experiment with a trigram back-off MTU model trained on all extracted MTUs, denoted as n-gram MTU. The trigram MTU model is estimated with the same modified Kneser-Ney framework as the target side language model. All MTU models are trained in target left-to-right MTU order which performed well in initial experiments. Evaluation. We test our approach on two different data sets. First, we train a German to English system based on the data of the WMT 2006 shared task (Koehn and Monz, 2006). The paral</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of ACL, pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
</authors>
<title>Do we need phrases? Challenging the conventional wisdom in Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>8--16</pages>
<location>New York,</location>
<contexts>
<context position="1225" citStr="Quirk and Menezes, 2006" startWordPosition="174" endWordPosition="177">ity problem by modeling MTUs both as bags-of-words and as a sequence of individual source and target words. Our best results improve the output of a phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.5 BLEU, and we outperform the traditional n-gram based MTU approach by up to 0.8 BLEU. 1 Introduction Classical phrase-based translation models rely heavily on the language model and the reordering model to capture dependencies between phrases. Sequence models over Minimum Translation Units (MTUs) have been shown to complement both syntax-based (Quirk and Menezes, 2006) as well as phrase-based (Zhang et al., 2013) models by explicitly modeling relationships between phrases. MTU models have been traditionally estimated using standard back-off n-gram techniques (Quirk and Menezes, 2006; Crego and Yvon, 2010; Zhang et al., 2013), similar to wordbased language models (§2). However, the estimation of higher-order n-gram models becomes increasingly difficult due to data sparsity issues associated with large n-grams, even when training on over one hundred billion words (Heafield et al., 2013); bilingual units are much sparser than words and are therefore even harde</context>
<context position="5494" citStr="Quirk and Menezes, 2006" startWordPosition="844" endWordPosition="847">as a bag-of-words, thereby allowing us to learn representations over sub-structures of minimal units that are shared across MTUs (§4). Our models significantly outperform the traditional back-off n-gram based approach and we show that they act complementary to a very strong recurrent neural network-based language model based solely on target words (§5). 2 Minimum Translation Units Banchs et al. (2005) introduced the idea of framing translation as a sequence modeling problem where a sentence pair is generated in left-to-right order as a sequence of bilingual n-grams. Minimum Translation Units (Quirk and Menezes, 2006; Zhang et al., 2013) are an extension which additionally permit tuples with empty source or target sides, thereby allowing insertion or deletion phrase pairs. The two basic requirements for MTUs are that there are no overlapping word alignment links between phrase pairs and it should not be possible to extract smaller phrase pairs without violating the word alignment constraints. Informally, we can think of MTUs as small phrase pairs that cannot be broken down any further without violating the two requirements. Minimum Translation Units partition a sentence pair into a set of minimal bilingua</context>
</contexts>
<marker>Quirk, Menezes, 2006</marker>
<rawString>Chris Quirk and Arul Menezes. 2006. Do we need phrases? Challenging the conventional wisdom in Statistical Machine Translation. In Proc. of NAACL, pages 8–16, New York, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariya Rastrow</author>
<author>Sanjeev Khudanpur</author>
<author>Mark Dredze</author>
</authors>
<title>Revisiting the Case for Explicit Syntactic Information in Language Models.</title>
<date>2012</date>
<booktitle>In NAACL-HLT Workshop on the Future of Language Modeling for HLT,</booktitle>
<pages>50--58</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2049" citStr="Rastrow et al., 2012" startWordPosition="304" endWordPosition="307">d Menezes, 2006; Crego and Yvon, 2010; Zhang et al., 2013), similar to wordbased language models (§2). However, the estimation of higher-order n-gram models becomes increasingly difficult due to data sparsity issues associated with large n-grams, even when training on over one hundred billion words (Heafield et al., 2013); bilingual units are much sparser than words and are therefore even harder to estimate. Another drawback of n-gram models is that future predictions are based on a limited amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (Bengio et al., 2003; Schwenk et al., 2007; Schwenk et al., 2012; Vaswani et al., 2013), as well as translation modeling (Allauzen et al., 2011; Le et al., 2012; Gao et al., 2013). These models tackle the data sparsity problem by representing words in continuous space rather than as discrete units. Similar words are grouped in the same sub-space rather than being treated as separate entities. Neural network models can be seen as function</context>
</contexts>
<marker>Rastrow, Khudanpur, Dredze, 2012</marker>
<rawString>Ariya Rastrow, Sanjeev Khudanpur, and Mark Dredze. 2012. Revisiting the Case for Explicit Syntactic Information in Language Models. In NAACL-HLT Workshop on the Future of Language Modeling for HLT, pages 50–58. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning Internal Representations by Error Propagation. In</title>
<date>1986</date>
<booktitle>Symposium on Parallel and Distributed Processing.</booktitle>
<contexts>
<context position="12096" citStr="Rumelhart et al., 1986" startWordPosition="1944" endWordPosition="1947">x function and are estimated jointly with all other parameters (Figure 3).1 1While these features depend on multiple input MTUs, we yt 0 0 1 0 0 0 0 0 W U ht V ht-1 22 Mt Figure 3: Structure of atomic recurrent neural network MTU model with classing layer ct and direct connections D between the input and output layers (cf. Figure 2). The model is optimized via a maximum likelihood objective function using stochastic gradient descent. Training is based on the truncated back propagation through time algorithm, which unrolls the network and then computes error gradients over multiple time steps (Rumelhart et al., 1986); we use a cross entropy criterion to obtain the error vector with respect to the output activations and the desired prediction. After training, the output layer represents posteriors p(mt+1|mtt−n+1, ht), the probability of the next MTU given the previous ninput MTUs mtt−n+1 = mt, . . . , mt−n+1 and the current hidden layer configuration ht. Naive computation of the probability distribution over the next MTU is very expensive for large vocabularies, such as commonly encountered for MTU models (Table 1). A well established efficiency trick assigns each possible output to a unique class and then</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1986. Learning Internal Representations by Error Propagation. In Symposium on Parallel and Distributed Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Marta R Costa-juss`a</author>
<author>Jos´e A R Fonollosa</author>
</authors>
<title>Smooth Bilingual N-Gram Translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>430--438</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Schwenk, Costa-juss`a, Fonollosa, 2007</marker>
<rawString>Holger Schwenk, Marta R. Costa-juss`a, and Jos´e A. R. Fonollosa. 2007. Smooth Bilingual N-Gram Translation. In Proc. of EMNLP, pages 430–438, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Anthony Rousseau</author>
<author>Mohammed Attik</author>
</authors>
<title>Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In NAACL-HLT Workshop on the Future of Language Modeling for HLT,</booktitle>
<pages>11--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2272" citStr="Schwenk et al., 2012" startWordPosition="336" endWordPosition="339">ed with large n-grams, even when training on over one hundred billion words (Heafield et al., 2013); bilingual units are much sparser than words and are therefore even harder to estimate. Another drawback of n-gram models is that future predictions are based on a limited amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (Bengio et al., 2003; Schwenk et al., 2007; Schwenk et al., 2012; Vaswani et al., 2013), as well as translation modeling (Allauzen et al., 2011; Le et al., 2012; Gao et al., 2013). These models tackle the data sparsity problem by representing words in continuous space rather than as discrete units. Similar words are grouped in the same sub-space rather than being treated as separate entities. Neural network models can be seen as functions over continuous representations exploiting the similarity between words, thereby making the estimation of probabilities over higherorder n-grams easier. However, feed-forward networks do not directly address the limited c</context>
</contexts>
<marker>Schwenk, Rousseau, Attik, 2012</marker>
<rawString>Holger Schwenk, Anthony Rousseau, and Mohammed Attik. 2012. Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation. In NAACL-HLT Workshop on the Future of Language Modeling for HLT, pages 11–19. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Ilya Oparin</author>
<author>Jean-Luc Gauvain</author>
<author>Ben Freiberg</author>
<author>Ralf Schl¨uter</author>
<author>Hermann Ney</author>
</authors>
<title>Comparison of Feedforward and Recurrent Neural Network Language Models.</title>
<date>2013</date>
<booktitle>In IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>8430--8434</pages>
<location>Vancouver, Canada,</location>
<marker>Sundermeyer, Oparin, Gauvain, Freiberg, Schl¨uter, Ney, 2013</marker>
<rawString>Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain, Ben Freiberg, Ralf Schl¨uter, and Hermann Ney. 2013. Comparison of Feedforward and Recurrent Neural Network Language Models. In IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 8430–8434, Vancouver, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with Large-scale Neural Language Models improves Translation.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="2295" citStr="Vaswani et al., 2013" startWordPosition="340" endWordPosition="343"> even when training on over one hundred billion words (Heafield et al., 2013); bilingual units are much sparser than words and are therefore even harder to estimate. Another drawback of n-gram models is that future predictions are based on a limited amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (Bengio et al., 2003; Schwenk et al., 2007; Schwenk et al., 2012; Vaswani et al., 2013), as well as translation modeling (Allauzen et al., 2011; Le et al., 2012; Gao et al., 2013). These models tackle the data sparsity problem by representing words in continuous space rather than as discrete units. Similar words are grouped in the same sub-space rather than being treated as separate entities. Neural network models can be seen as functions over continuous representations exploiting the similarity between words, thereby making the estimation of probabilities over higherorder n-grams easier. However, feed-forward networks do not directly address the limited context issue either, si</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with Large-scale Neural Language Models improves Translation. In Proc. of EMNLP. Association for Computational Linguistics, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zhang</author>
<author>Kristina Toutanova</author>
<author>Chris Quirk</author>
<author>Jianfeng Gao</author>
</authors>
<title>Beyond left-to-right: Multiple decomposition structures for smt.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>12--21</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="1270" citStr="Zhang et al., 2013" startWordPosition="182" endWordPosition="185"> and as a sequence of individual source and target words. Our best results improve the output of a phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.5 BLEU, and we outperform the traditional n-gram based MTU approach by up to 0.8 BLEU. 1 Introduction Classical phrase-based translation models rely heavily on the language model and the reordering model to capture dependencies between phrases. Sequence models over Minimum Translation Units (MTUs) have been shown to complement both syntax-based (Quirk and Menezes, 2006) as well as phrase-based (Zhang et al., 2013) models by explicitly modeling relationships between phrases. MTU models have been traditionally estimated using standard back-off n-gram techniques (Quirk and Menezes, 2006; Crego and Yvon, 2010; Zhang et al., 2013), similar to wordbased language models (§2). However, the estimation of higher-order n-gram models becomes increasingly difficult due to data sparsity issues associated with large n-grams, even when training on over one hundred billion words (Heafield et al., 2013); bilingual units are much sparser than words and are therefore even harder to estimate. Another drawback of n-gram mod</context>
<context position="4565" citStr="Zhang et al. (2013)" startWordPosition="700" endWordPosition="703">stics, pages 20–29, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Words MTUs Tokens 34,769,416 14,853,062 Types 143,524 1,315,512 Singleton types 34.9% 80.1% Table 1: Token and type counts for both source and target words as well as MTUs based on the WMT 2006 German to English data set (cf. §5). M1 M2 M3 M4 M5 T 111 *ff T A__A Yu ZuoTian JuXing Le null HuiTan held the meeting null yesterday M1: Yu =&gt; null M2: ZuoTian =&gt; Yesterday M3: JuXing_Le =&gt; held M4: null =&gt; the M5: HuiTan =&gt; meeting Figure 1: Example Minimum Translation Unit partitioning based on Zhang et al. (2013). sues regarding data sparsity and limited context sizes by leveraging continuous representations and the unbounded history of the recurrent architecture. Our first approach frames the problem as a sequence modeling task over minimal units (§3). The second model improves over the first by modeling an MTU as a bag-of-words, thereby allowing us to learn representations over sub-structures of minimal units that are shared across MTUs (§4). Our models significantly outperform the traditional back-off n-gram based approach and we show that they act complementary to a very strong recurrent neural ne</context>
<context position="6685" citStr="Zhang et al., 2013" startWordPosition="1033" endWordPosition="1036">to a set of minimal bilingual units or tuples obtained by an algorithm similar to phraseextraction (Koehn et al., 2003). Figure 1 illustrates such a partitioning. Modeling minimal units has two advantages over considering larger phrase pairs that are effectively composed of MTUs: First, minimal units result in a unique partitioning of a sentence pair. This has the advantage that we avoid modeling spurious derivations, that is, multiple derivations generating the same sentence pair. Second, minimal units result in smaller models with a smoother distribution than models based on composed units (Zhang et al., 2013). Sentence pairs can be generated in multiple orders, such as left-to-right or right-to-left, either in source or target order. For example, the source left-to-right order of the sentence pair in Figure 1 is simply M1, M2, M3, M4, M5, while the target left-to-right order is M3, M4, M5, M1, M2. We deal with inserted or deleted words similar to Zhang et al. (2013): The source side null token of an inserted target phrase is placed next to the last source word aligned to the closest preceding nonnull aligned target phrase; a similar rule is applied to null tokens on the target side. For example, i</context>
</contexts>
<marker>Zhang, Toutanova, Quirk, Gao, 2013</marker>
<rawString>Hui Zhang, Kristina Toutanova, Chris Quirk, and Jianfeng Gao. 2013. Beyond left-to-right: Multiple decomposition structures for smt. In Proc. of NAACL, pages 12–21, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoff Zweig</author>
<author>Konstantin Makarychev</author>
</authors>
<title>Speed Regularization and Optimality in Word Classing.</title>
<date>2013</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="24246" citStr="Zweig and Makarychev, 2013" startWordPosition="4015" endWordPosition="4018"> log-linear weights to rescore the test set n-best list. Neural Network Setup. We trained the recurrent neural network models on between 88% and 93% of each data set and used the remainder as validation data. The vocabulary of the atomic MTU RNN model is comprised of all MTU types which were observed more than once in the training data.5 Similarly, we modeled all non-singleton words for the bag-of-words MTU RNN model. We obtain classes for words or MTUs using a version of Brown-Clustering with an additional regularization term to optimize the runtime of the language model (Brown et al., 1992; Zweig and Makarychev, 2013). Direct connections use features over unigrams, bigrams and trigrams of words or MTUs, depending on the model. Features are hashed to a table with at most 500 million values following Mikolov et al. (2011a). We use the standard settings for the model with the default learning rate α = 0.1 that decays exponentially if the validation set entropy does not decrease. Back propagation through time computes error gradients over the past twenty time steps. Training is stopped after 20 epochs or when the validation entropy does not decrease over two epochs. Throughout, we use a hidden layer size of 10</context>
</contexts>
<marker>Zweig, Makarychev, 2013</marker>
<rawString>Geoff Zweig and Konstantin Makarychev. 2013. Speed Regularization and Optimality in Word Classing. In Proc. of ICASSP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>