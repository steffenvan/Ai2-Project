<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<sectionHeader confidence="0.684793" genericHeader="method">
ABSTRACTS OF CURRENT LITERATURE
</sectionHeader>
<subsectionHeader confidence="0.703136">
Selected Dissertation Abstracts
</subsectionHeader>
<bodyText confidence="0.819982863636364">
Compiled by:
Susanne M. Humphrey, National Library of Medicine, Bethesda, MD 20209
Bob Krovetz, University of Massachusetts, Amherst, MA 01002
The following are citations selected by title and abstract as being related to computational linguistics or knowledge
representation, resulting from a computer search, using the BRS Information Technologies retrieval service, of the
Dissertation Abstracts International (DAI) database produced by University Microfilms International.
Included are the UM order number and year-month of entry into the database; author; university, degree, and,
if available, number of pages; title; DAI subject category chosen by the author of the dissertation; and abstract.
References are sorted first by DAI subject category and second by author. Citations denoted by an MAI reference
do not yet have abstracts in the database and refer to abstracts in the published Masters Abstracts International.
Unless otherwise specified, paper or microform copies of dissertations may be ordered from:
University Microfilms International
Dissertation Copies
Post Office Box 1764
Ann Arbor, MI 48106
telephone for U.S. (except Michigan, Hawaii, Alaska): 1-800-521-3042
for Canada: 1-800-268-6090.
Price lists and other ordering and shipping information are in the introduction to the published DAI. An alternate
source for copies is sometimes provided at the end of the abstract.
The dissertation titles and abstracts contained here are published with permission of University Microfilms
International, publishers of Dissertation Abstracts International (copyright by University Microfilms International),
and may not be reproduced without their prior permission.
</bodyText>
<sectionHeader confidence="0.751321" genericHeader="method">
NEW REPORTS AND MEMOS
</sectionHeader>
<keyword confidence="0.3478605">
Net-Based Plan Synthesis
Bahler, Dennis Rex
</keyword>
<sectionHeader confidence="0.109352" genericHeader="method">
University Microfilms International
ADG89-01237
</sectionHeader>
<bodyText confidence="0.97492356">
This work is concerned with the synthesis, characterization, and
verification of plans. Planning is the problem of synthesizing a
set of actions to accomplish a goal. In this thesis, components
of a planning system—situations, operator schemata, and
operators—are defined and discussed. Schemata may be used to
form a plan grammar, and operators may be considered as
labels on its productions. A taxonomic classification of plan
grammars is established depending on their structure, and the
functionality of such grammars is discussed for attacking
important problems in plan synthesis, particularly resource
acquisition and/or release and simple forms of spatial reasoning.
When situational rules are appended to a plan grammar, a
generic planning problem can be defined.
Representations of situations, schemata, and operators are
defined employing a new class of net called the generalized
condition/event net (GC/Enet), and the relationship of plans to
nets is discussed with reference to two important new net
properties—mutual persistence of event sequences and liveness
with respect to a set of places.
Next, the taxonomy of plan grammars is extended into a
taxonomy of planning problems. Along with well-known
algorithms for solving problems in the simpler classes, two new
algorithms are presented for problems in the most general class.
All the algorithms operate by growing a net from two initially
Computational Linguistics, Volume 15, Number 4, December 1989 269
</bodyText>
<subsectionHeader confidence="0.778192">
Abstracts of Current Literature
</subsectionHeader>
<bodyText confidence="0.9998666">
unconnected components. Embedded in this net is a partially
ordered set of events representing the occurrence of operators.
The most basic search problem to be solved by a planning
system is identified for the most general class of problem and a
complexity result about it is proved.
Finally, this thesis describes a means of abstractly
characterizing multi-agent plans as a strict partial ordering
defined over a multiset of operators. This ordering, called an
operator occurrence ordering, makes it possible to define
rigorously the notion of plan and plan execution and to prove
formally certain important properties of nonlinear plans and
executions, including plan correctness and validity of a plan
with respect to a problem. A theorem is proved about
necessary and sufficient conditions under which an operator
ordering terminates execution having achieved a desired goal.
</bodyText>
<figure confidence="0.90482925">
Protos: A Unified Approach to
Concept Representation, Classification,
and Learning
Bareiss, Ellis Raymond, Jr.
University Microfilms International
ADG89-01270
A Massively Parallel Natural Language
Processing Architecture with
Distributed Control
Berg, Howard George Jr.
University Microfilms International
ADG89-02614
</figure>
<bodyText confidence="0.9998334">
The primary contribution of this research is a unified approach
to concept representation, classification, and concept learning.
This approach has been implemented as a computer program,
Protos, which learns concepts as it performs classification under
the guidance of a teacher. The soundness of the approach has
been demonstrated by successfully applying Protos to the task
of acquiring knowledge for performing heuristic classification at
an expert level of proficiency.
The Protos approach addresses the complexities of
representing, using, and learning natural concepts. These
concepts are polymorphic and ill-defined. Most machine learning
research is based on inductive learning and deductive
classification, which are more suitable for artificial domains
(e.g., mathematics) than natural domains (e.g., medicine). In
contrast, Protos takes an exemplar-based approach. It represents
concepts extensionally as sets of retained exemplars, classifies a
new instance by recalling a similar exemplar and explaining its
similarity to the instance, and learns when a classification
failure indicates that knowledge is missing. Because Protos
learns as a byproduct of classification, its performance
continually improves.
Protos has been experimentally evaluated by training it to
diagnose hearing disorders. An expert audiologist trained Protos
with 200 cases of hearing disorder. Through this small amount
of training, Protos evolved into an expert system whose
classification performance was comparable to that of
experienced human clinicians.
This research describes a new massively parallel model for
natural language processing and knowledge representation. This
model, Autonomous Semantic Networks (ASNs), addresses
shortcomings in previous approaches. The serial-processor
models of natural language processing become bogged down in
the searching and manipulation of the large amounts of world
knowledge they need to understand the texts they read. The
existing massively parallel models have the potential to avoid
that bottleneck. However, the current state of the art makes
necessary operations such as variable binding and the use of
ordered relations extremely difficult. The ASN model represents
a middle road—a massively parallel model that can easily build
and manipulate representations of new and changed concepts
</bodyText>
<page confidence="0.93666">
270 Computational Linguistics, Volume 15, Number 4, December 1989
</page>
<subsectionHeader confidence="0.95486">
Abstracts of Current Literature
</subsectionHeader>
<bodyText confidence="0.999942052631579">
resulting from the reading of the input. This is an essential
ability for a natural language processing system to have to
handle inference and anaphoric reference.
The ASN model has as its basis a spreading activation-based
network of units and the connections between them. These
form the knowledge representation portion of the model. In
addition, ASNs have two other classes of nodes. The first
class, WTA nodes, allows the search and selection among units
based upon their activation. The second class of nodes is
composed of the construction nodes. Construction nodes
manipulate the units in the knowledge representation. Depending
on the exact type of node, it can either add a connection
between units, delete a connection, or allocate a new unit. The
notion of indirection in the operations of construction nodes
allows them to implement variable-like constructs.
The usefulness of ASNs is shown by a network that
processes a simple multiple-sentence text. The implementation
of schemata, disambiguation, and discourse processing in ASN-
based systems is also discussed.
</bodyText>
<figure confidence="0.9846615">
Large Vocabulary Speaker—Independent
Continuous Speech the SPHINX
System
Lee, Kai—Fu
University Microfilms International
ADG88-26533
</figure>
<bodyText confidence="0.988465820512821">
Speaker independence, continuous speech, and large vocabulary
are three of the greatest problems in automatic speech
recognition. Previous accurate speech recognizers avoided
dealing with all three problems simultaneously. This dissertation
describes SPHINX, the first system to demonstrate the
feasibility of accurate large-vocabulary speaker-independent
continuous speech recognition.
SPHINX is based on four important principles: use of a
sophisticated yet tractable model of speech, incorporation of
human speech knowledge, utilization of speech units that are
trainable, well understood, and context-insensitive, and ability to
learn and to adapt to individual speakers.
Hidden Markov models (HMMs) are used to represent speech
in SPHINX. Hidden Markov modeling is a powerful technique
capable of robust and succinct modeling of speech. With their
efficient maximum likelihood training and recognition algorithms,
HMMs have already been successfully applied to more
constrained tasks.
Within the framework of hidden Markov modeling, SPHINX
uses human knowledge in several ways. Perceptually motivated
parameters were incorporated using frame and segment level
integration techniques. Also, an optimized set of phones and
word pronunciations were derived from human phonetic/lexical
knowledge and tuning experiments. The incorporation of
knowledge resulted in substantial improvements in recognition
accuracy.
It is well known that the same phone in different contexts
has different realizations. A good unit of speech must be
trainable, yet models context-dependent and word-dependent
effects. Two novel units, function word-dependent phone model
and the generalized triphone model, are introduced. These units
led to very substantial improvements in performance.
Finally, to improve the system given some knowledge of the
speaker, two learning algorithms are introduced to modify the
system to adapt to an input speaker. The first algorithm is
based on speaker cluster selection, and the other involves
deleted interpolation of various speaker-independent and
speaker-dependent HMM parameters.
Computational Linguistics, Volume 15, Number 4, December 1989 271
</bodyText>
<subsectionHeader confidence="0.938896">
Abstracts of Current Literature
</subsectionHeader>
<bodyText confidence="0.999844285714286">
SPHINX attained a word accuracy of 96% on the 997-word
resource management task. It is much more accurate than any
previously reported results on similar tasks. In fact, it is
comparable to the best speaker-dependent systems. By using
sophisticated modeling techniques to exploit abundant training
data, SPHINX has bridged the gap between speaker-independent
and speaker-dependent recognition.
</bodyText>
<figure confidence="0.986170333333333">
Knowledge Intensive Planning
Luria, Marc A.
University Microfilms International
ADG89-02194
Building and Maintaining Hierarchical
Semantic Nets
Mili, Hafedh
University Microfilms International
ADG88-25145
</figure>
<bodyText confidence="0.999834612244898">
This thesis describes a program called KIP (Knowledge
Intensive Planner). KIP is a general, commonsense planner that
can reason about planning situations in the real world for which
it is provided information. KIP is the planning component of
the UC (UNIX consultant) system. KIP is used to solve the
problems the user poses to the UC. KIP has knowledge about
UNIX commands, including the effects of those commands and
under what conditions those commands can and should be
issued. The best plan is reported to the user after determining
the goals of the user, selecting and specifying a plan that fulfills
the goals of the user (plan determination), testing if the plan
will work in this particular problem situation without causing
unacceptable consequences, and modifying this plan if
necessary.
A major problem in commonsense planning is the focus of
attention on relevant knowledge. In particular, the problem of
identifying potential plan failures in a plan is difficult, since
there are often many sources of plan failure, both for failures
due to an unsatisfied condition of a plan and failures due to
goal conflict. This problem is further complicated because many
values of conditions in a particular planning problem may be
unknown. To address the problem of identifying potential plan
failures, a new idea, called a concern, has been introduced.
Concerns identify which aspects of a plan are most likely to
fail.
Knowledge-intensive Al applications require the development
and maintenance of increasingly large, consistent, and up-to-date
knowledge bases. Knowledge engineering remains largely
manual, presenting a major bottleneck to the development of
intelligent systems. This research explores methods to build
automatically and maintain hierarchical semantic nets. Such
semantic nets that are manually built and laboriously updated
exist according to some rules that are imprecise at best. The
approach followed is one of taking advantage of the structure
existing in readily available knowledge sources to build and/or
maintain hierarchical semantic nets.
Three methods were investigated, dealing with knowledge
sources of varying structural complexity. The first two methods,
briefly summarized in this thesis, proved useful in the context
of an intelligent information retrieval system that used a
hierarchical semantic net to match documents to queries.
However, both methods suffered, to varying degrees, from the
vagueness and lack of clear semantics of hierarchical relations
in human-made hierarchies. The third method, DK method, is
based on a precise characterization of hierarchical relationships.
The DK method is based on the observation that human-
made hierarchies are often built in a way such that concepts&apos;
properties follow clear patterns, similar to the patterns implied
by inheritance in taxonomies. These patterns are described by
</bodyText>
<page confidence="0.856238">
272 Computational Linguistics, Volume 15, Number 4, December 1989
</page>
<subsectionHeader confidence="0.839028">
Abstracts of Current Literature
</subsectionHeader>
<bodyText confidence="0.9998021875">
regularity, a generalized form of inheritance. It is argued that
regularity is a fundamental property of hierarchies, and a model
of hierarchies (DK model of hierarchies) that embodies
regularity is proposed. In this model, hierarchical relations are
described by predicates that represent the particular
relationships that hold between the properties of a concept and
the properties of its descendants. Adding a concept to a DK
hierarchy is then reduced to a problem of classifying that
concept using the hierarchy as a hierarchical classifier. The
cognitive and mathematical foundations of this approach are
discussed. In particular, a fuzzy model of DK hierarchies and
its associated classification algorithm are proposed to handle the
uncertainties and exceptions to regularity that often plague
human-made hierarchies. The fuzzy models are then tested on
two human-made hierarchies, and the results show the validity
of our approach.
</bodyText>
<figure confidence="0.788711833333333">
Learning Effective Search Control
Knowledge: An Explanation-Based
Approach
Minton, Steven
University Microfilms International
ADG88-26537
</figure>
<bodyText confidence="0.999324447368421">
To solve problems more effectively with accumulating
experience, a problem solver must be able to learn and exploit
search control knowledge. Although previous research has
demonstrated that Explanation-Based Learning (EBL) is a viable
approach for acquiring control knowledge, in practice the
learned control knowledge may not be useful. For control
knowledge to be effective, the cumulative benefits of applying
the knowledge must outweigh the cumulative costs of-testing
whether the knowledge is applicable. Previous research in EBL
has ignored this issue, which I refer to as the utility problem.
Most researchers have simply demonstrated that EBL can
improve performance on particular examples without analyzing
exactly when performance improvement will occur. In practice,
it is much more difficult to improve performance over a
population of examples than it is to improve performance on
isolated examples.
One answer to the utility problem is to search for &amp;quot;good&amp;quot;
explanations—explanations that can be profitably employed to
control problem solving. Instead of simply adding control
knowledge haphazardly, a learning system must be sensitive to
the problem solver&apos;s computational architecture and the
potential costs and benefits of adding knowledge. This thesis
analyzes the utility of EBL, and describes a method for
searching for good explanations. The method, implemented in
the PRODIGY/EBL system, consists of a three-stage heuristic
search. Given a problem solving trace, PRODIGY first selects
what to learn. The system chooses from a variety of target
concepts, each representing a different strategy for optimizing
performance. Second, after creating an initial explanation from
the trace, PRODIGY searches for a representation of the
explanation that is efficient to match. Finally the system
empirically tests the effectiveness of the learned control
knowledge to determine whether it is actually worth keeping.
The thesis includes a set of comprehensive experiments
testing the performance of the PRODIGY/EBL system and its
components in several domains. In addition, a formal
description of EBL is presented, together with a correctness
proof for PRODIGY&apos;s generalization method.
</bodyText>
<table confidence="0.776908857142857">
Computational Linguistics, Volume 15, Number 4, December 1989 273
Abstracts of Current Literature
Syntactic and Thematic Contributions
To On-Line Sentence Comprehension
Speer, Shari Rae
University Microfilms International
ADG89-01396
A Knowledge-Based Message
Generation System for Motor and
Speech Disabled Persons: Design
Methodology and Prototype Testing
Sy, Bon Kiem
University Microfilms International
ADG88-17749
</table>
<bodyText confidence="0.999959631578948">
Current linguistic theories assume a rich lexicon, in which
individual lexical entries are associated with syntactic
subcategorization information and semantic argument structures.
Verbs in particular are associated with sets of thematic roles
that indicate the types of sentential context in which they may
appear. Five experiments explore the ways that thematic
relations and syntactic phrase structure contribute to the parsing
process in human language comprehension. Two on-line tasks,
phoneme monitoring and self-paced reading, are used to
measure sentence processing during the comprehension of NP-
V-NP-PP sentences.
The first experiment provides empirical evidence from a
sentence completion task to confirm linguistic intuitions about
the thematic roles likely to be associated with the arguments of
agentive transitive verbs. S-V-0 contexts tested in the first
experiment are extended to create materials for the subsequent
experiments. Experiments two and three show processing effects
due to syntactic complexity (verb-attached vs. verbal object-
attached prepositional phrases) and due to the thematic role
preferences of the verbs. These effects replicate across the
auditory and visual modalities. Experiments four and five
examine the contributions of preposition (IN and WITH), verb
type, verb-preposition pairing, and preposition-thematic-relation
pairing on the parsing process for sentences of the same
syntactic structure (verb-attached prepositional phrases). Results
indicate that verb type and preposition-thematic relation pairing
influence on-line decisions of the human language parser.
A computer-based nonvocal communication device is an aid to
assist nonvocal, motor disabled persons in generating written
and spoken messages through an &amp;quot;interrogation&amp;quot; process. This
process is executed by suggesting a list of probable messages
for the user to affirm, deny, or select from. The message
search strategy used in existing devices is usually based on
fixed statistical information (such as the occurrence frequency
of a message). This existing approach is ineffective when a user
attempts to use the device to generate infrequently used
messages (i.e., messages with low likelihood or low occurrence
frequency).
This ineffectiveness is overcome, at least in part, through the
development of a Knowledge-based Message Generation System
(KMGS), which can &amp;quot;reason out&amp;quot; a desired message. The
reasoning process is based on the embedded knowledge
concerning the likelihood and the grammatical structure of the
message, as well as any sources of information (such as
partially understandable speech) provided to the system. The
new approach can effectively seek out the desired message even
if it is infrequently used, and thus improves the communication
rate of the device.
The current design of KMGS makes use of an entropy
measurement in the selection of message elements for the
knowledge base, which optimizes both the articulateness and the
fluency of the system. The knowledge base of the system is a
language graph encoded with English grammatical rules and
message elements. The search for the message elements is
conceptualized as a path search in the language graph, and a
special frame architecture is used to construct and partition the
graph. Bayesian belief reasoning from the Dempster-Shafer
</bodyText>
<page confidence="0.96619">
274 Computational Linguistics, Volume 15, Number 4, December 1989
</page>
<figure confidence="0.963092">
Abstracts of Current Literature
A Study of the Article System in
English
Chen, Pi-Fen Liu
University Microfilms International
ADG89-00019
A Semantics for Groups and Events
Lasersohn, Peter Nathan
University Microfilms International
ADG88-24557
</figure>
<bodyText confidence="0.987519854545454">
Theory of Evidence is augmented to cope with the time varying
evidence so that both the information from external sources
(such as a speech recognition system) and the embedded
knowledge can be used to optimize the process of message
search. An &amp;quot;information fusion&amp;quot; strategy is introduced to
integrate various forms of external information. Experimental
testing results of the prototype system are reported.
This dissertation aims to answer the following questions
frequently asked by adult second language learners of English:
(a) when do we use the? (b) when do we use a(n)? and (c)
when do we use neither the nor a(n)? This study discusses
mass and count nouns in English, and what makes an NP
definite and what makes it indefinite. It also discusses the
generic use of English articles. It is argued that not every mass
noun can be converted into a count noun and vice versa. Four
principles are given for mass/count conversion. For (in)definite-
ness, three requirements—existence, uniqueness, and
familiarity—are posited for the use of the. Subtle differences
among the generic use of the, a(n) and 0, the zero article, are
discussed, and their respective distributions and restrictions are
presented. An overall system of article usage is presented for
second language learning.
This dissertation provides a model-theoretic semantics for
English sentences attributing a property or action to a group of
objects, either collectively or distributively. It is shown that
certain adverbial expressions select for collective predicates;
therefore collective and distributive predicates must be
distinguishable. This finding is problematic for recent accounts
of distributive predicates that analyze such predicates as taking
group-level arguments, and hence as not distinguishable from
collective predicates.
A group-level treatment of distributives is possible, however,
if predicate denotations are relativized to a set of events for
which a part/whole relation is defined. An event in which a
group performs an action distributively will have subevents in
which each of the group&apos;s members perform the same action;
an event in which the group performs the action collectively
will not.
This analysis also makes possible an account of the fact that
adverbials expressing collective action commonly have an
additional use expressing spatial proximity, both in English and
cross-linguistically. (Compare John and Mary lifted a piano
together with John and Mary sat together.) A spatial &amp;quot;trace&amp;quot;
function on the set of events allows formal definitions for the
spatial uses of such adverbials to exactly parallel the definitions
for the collectivizing uses.
The dissertation also provides arguments for a set-theoretic
(as opposed to lattice-theoretic or merological) model for
plurality, in which the group membership relation is distinct
from the subgroup relation.
Certain quantifiers are shown sensitive to distinction between
different sorts of group-level event. To accommodate this fact,
it is suggested that verbal denotations provide, for each event,
both an &amp;quot;inclusion set&amp;quot; and an &amp;quot;exclusion ser—corresponding
roughly to positive and negative denotations. If the inclusion
</bodyText>
<table confidence="0.86094375">
Computational Linguistics, Volume 15, Number 4, December 1989 275
Abstracts of Current Literature
Information Structure in Planned,
Written and Unplanned, Spoken
Discourse
Lee, Chingkwei Adrienne
University Microfilms International
ADG88-17573
</table>
<bodyText confidence="0.999913843137255">
and exclusion sets are allowed under certain circumstances not
to complement each other, correct results obtain.
The splitting of verbal denotations into inclusion and
exclusion sets also allows the solution of certain problems in
previous accounts of the semantics of subject-verb agreement
for number. The dissertation closes with a defense of the
hypothesis that agreement (in English) is conditioned primarily
by the semantics.
A taxonomy of discourse entities based on the reconsideration
of Prince&apos;s 1981 trichotomy of given-new information and
Minsky&apos;s 1975 &apos;frame&apos; conception is created to analyze four
thousand discourse entities from lectures and writings of four
linguistics profes*sors on the same or closely related subject
matter to determine the differences of information structure in
planned, written and unplanned, spoken discourse.
Two discourse models based on entities are proposed to
account for planned, written and unplanned, spoken discourse
respectively. Based on these two models, research hypotheses
concerning the differences between the two types of discourse
are formulated. The procedures of data analyses are to code
each token according to six factor groups, input the coded
tokens into a computer to run the sorting routines of the
VARBRUL 2 program to obtain the frequencies of seven
aspects of entities in the two types of discourse, use four
statistical tests, the X2 test, matched t-test, coefficient
correlation test, and Ryan-Einot-Gabriel-Welsch multiple range
test, to determine if the differences are significant.
It is found that there are significantly more local entities,
partial-referent entities, modified noun entities, and longer
entities in planned, written discourse than in unplanned, spoken
discourse, and that there are significantly more pronoun entities,
animate entities, and shorter entities in unplanned, spoken
discourse than in planned, written discourse. These findings
suggest that those entities occurring in writing need more time
to process, while those occurring in speaking need less time to
process. When the major categories of entity types are
examined, the hierarchies of their occurrence in the two types
of discourse are the same. Based on this finding, an entity-
based model of communication (the E. B. Model of
Communication) that depicts the way people write and speak is
proposed: messages are originally conceived in the form of
framed/slotted entities, which are used most in discourse;
situationally evoked entities, which are called forth from the
situation involved to facilitate or to ease communication, are
used least; in the process of delivering the text, textually
dependent entities, which are used more than situationally
evoked entities and less than framed entities, are derived from
framed entities. The present study makes it possible to explain
and predict communication breakdown quantitatively in terms of
entities. Implications and applications of the present study are
given.
</bodyText>
<page confidence="0.951704">
276 Computational Linguistics, Volume 15, Number 4, December 1989
</page>
<figure confidence="0.789749538461539">
Abstracts of Current Literature
Scripts as Knowledge Representation:
Evidence from Two Case Studies of
Aphasia
Speicher, Barbara Lynn
University Microfilms Order Number
ADG89-02700
From French to English: A Look at
the Translation Process in Students,
Bilinguals, and Professional Translators
Gerloff, Pamela Ann
University Microfilms International
ADG88-23316
</figure>
<bodyText confidence="0.995027672413793">
Scripts, a theoretical model of knowledge representation, is rich
in linguistic and extralinguistic context. Scripts are situational
routines, such as going to a bank or a restaurant. They contain
information about settings, roles, props, sequences of actions,
and typical vocabulary. Scripts, which provide predictions and
expectations about an exchange, allow people to function
automatically in such routines. Examining language-disordered
populations makes possible our disentangling of linguistic
knowledge from real world knowledge. Aphasic subjects offer
insights impossible to capture from normal subjects.
In this dissertation, two nonfluent aphasics participated in
two sets of tasks based on the restaurant script and scripts
they had developed for therapy. Each task set included scriptal
tasks manipulating setting, roles, and props; metascriptal task(s)
manipulating sequence and requiring verbal descriptions; and
non-scriptal task(s) utilizing contexts unrelated to the scripts
while eliciting the same vocabulary (target lexemes) that the
aphasics had used in the scriptal tasks. The tasks usually
elicited verbal production and occasionally non-verbal
manipulation of scriptal elements.
The restaurant script findings support the paradigm of scripts
as underlying representations of knowledge because scriptal
tasks elicit the highest percentage of appropriate responses.
These aphasics demonstrate knowledge of the sequence of
activities within the script and sensitivity to the various scriptal
features. Specifically, setting and roles prove highly facilitative
of scriptal vocabulary. A strong interdependency between scripts
and vocabulary emerges. Scripts help these aphasics access
more language. Also, typical vocabulary seems to trigger a
script. The subjects occasionally invoke scripts to aid in their
retrieval of words.
Scripts may have both beneficial and detrimental effects on
application from therapy to other contexts. Communication
strategies compensate for linguistic limitations. These aphasics
demonstrate two types of circumlocution, scriptal and semantic.
Serial naming, classified as a reemergence of egocentric speech,
allows these aphasics to answer questions they would have
been unable to answer otherwise. The scriptal paradigm enables
us to explore and reanalyze several aphasic behaviors.
A study was conducted using think-aloud protocols to
investigate the translation processes of students, bilingual
speakers, and professional translators. The study consisted of
twelve subjects: four intermediate level college students learning
French as a second language; four bilingual speakers of French
and English, none of whom had significant prior experience
with translation; and four professional translators, none of
whom had grown up bilingually. Subjects were given a French
magazine article and asked to &amp;quot;think out loud&amp;quot; as they
translated it into English. Participants had access to dictionaries
and a thesaurus. The think-aloud protocols were audio- and
video-tape recorded and transcribed verbatim. They were then
coded for problem-solving strategies and behaviors and the size
of language units worked with (e.g., word, phrase, clause,
sentence). The data were analyzed to determine differences in
processing among the groups, the range of individual variation
within groups, and different &amp;quot;types&amp;quot; of processors that
emerged.
Computational Linguistics, Volume 15, Number 4, December 1989 277
</bodyText>
<subsectionHeader confidence="0.825959">
Abstracts of Current Literature
</subsectionHeader>
<bodyText confidence="0.999981269230769">
Central findings were that translation gets neither &amp;quot;easier&amp;quot;
nor faster as one becomes more experienced with the language
and more practiced with translation. Problems simply become
more complex, and experienced language users hold themselves
to higher standards than do novices, leading them to find more
problems with the text and to spend more time and effort on
those they find. Bilinguals and translators engaged in more total
problem-solving activity and made more solution attempts per
problem than the students did. They also generated more
possible translation choices, did more editing and continuous
monitoring, and worked through the text a greater number of
times.
All participants worked mostly in small syntactic units, but
bilinguals and translators also worked in larger discourse
chunks, demonstrating greater range and flexibility in their
processing styles.
Experience with translation was found to be a more reliable
predictor of processing style than degree of language pro-
ficiency.
The study discusses the structure of the translation process,
the importance of context building in translation, and differences
in processing styles within and among the groups, identifying
various &amp;quot;types&amp;quot; of processors that emerged. Some hypotheses
are offered concerning processes that are most likely to produce
good translations. Implications for both education and research
are presented.
</bodyText>
<page confidence="0.939428">
278 Computational Linguistics, Volume 15, Number 4, December 1989
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.094306">
<title confidence="0.7162485">ABSTRACTS OF CURRENT LITERATURE Selected Dissertation Abstracts</title>
<note confidence="0.843125818181818">Compiled by: Susanne M. Humphrey, National Library of Medicine, Bethesda, MD 20209 Bob Krovetz, University of Massachusetts, Amherst, MA 01002 The following are citations selected by title and abstract as being related to computational linguistics or knowledge representation, resulting from a computer search, using the BRS Information Technologies retrieval service, of the Dissertation Abstracts International (DAI) database produced by University Microfilms International. Included are the UM order number and year-month of entry into the database; author; university, degree, and, if available, number of pages; title; DAI subject category chosen by the author of the dissertation; and abstract. References are sorted first by DAI subject category and second by author. Citations denoted by an MAI reference do not yet have abstracts in the database and refer to abstracts in the published Masters Abstracts International. Unless otherwise specified, paper or microform copies of dissertations may be ordered from:</note>
<affiliation confidence="0.777391">University Microfilms International Dissertation Copies</affiliation>
<address confidence="0.8011835">Post Office Box 1764 Ann Arbor, MI 48106</address>
<phone confidence="0.414779">telephone for U.S. (except Michigan, Hawaii, Alaska): 1-800-521-3042</phone>
<note confidence="0.915588">for Canada: 1-800-268-6090.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>