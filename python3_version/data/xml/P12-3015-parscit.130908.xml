<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007615">
<title confidence="0.882151">
CSNIPER
Annotation-by-query for non-canonical constructions in large corpora
</title>
<author confidence="0.969713">
Richard Eckart de Castilho, Iryna Gurevych Sabine Bartsch
</author>
<affiliation confidence="0.928172666666667">
Ubiquitous Knowledge Processing Lab (UKP-TUDA) English linguistics
Department of Computer Science Department of Linguistics and Literary Studies
Technische Universit¨at Darmstadt Technische Universit¨at Darmstadt
</affiliation>
<email confidence="0.840353">
http://www.ukp.tu-darmstadt.de http://www.linglit.tu-darmstadt.de
</email>
<sectionHeader confidence="0.997419" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892818181818">
We present CSNIPER (Corpus Sniper), a
tool that implements (i) a web-based multi-
user scenario for identifying and annotating
non-canonical grammatical constructions in
large corpora based on linguistic queries and
(ii) evaluation of annotation quality by mea-
suring inter-rater agreement. This annotation-
by-query approach efficiently harnesses expert
knowledge to identify instances of linguistic
phenomena that are hard to identify by means
of existing automatic annotation tools.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998215">
Linguistic annotation by means of automatic pro-
cedures, such as part-of-speech (POS) tagging, is
a backbone of modern corpus linguistics; POS
tagged corpora enhance the possibilities of corpus
query. However, many linguistic phenomena are
not amenable to automatic annotation and are not
readily identifiable on the basis of surface features.
Non-canonical constructions (NCCs), which are the
use-case of the tool presented in this paper, are a
case in point. NCCs, of which cleft-sentences are
a well-known example, raise a number of issues that
prevent their reliable automatic identification in cor-
pora. Yet, they warrant corpus study due to the rel-
atively low frequency of individual instances, their
deviation from canonical construction patterns and
frequent ambiguity. This makes them hard to distin-
guish from other, seemingly similar constructions.
Expert knowledge is thus required to reliably iden-
tify and annotate such phenomena in sufficiently
large corpora like the 100 mil. word British National
</bodyText>
<page confidence="0.996379">
85
</page>
<bodyText confidence="0.955046058823529">
Corpus (BNC Consortium, 2007). This necessitates
manual annotation which is time-consuming and
error-prone when carried out by individual linguists.
To overcome these issues, CSNIPER implements
a web-based multi-user annotation scenario in which
linguists formulate and refine queries that identify
a given linguistic construction in a corpus and as-
sess the query results to distinguish instances of the
phenomenon under study (true positives) from such
examples that are wrongly identified by the query
(false positives). Each expert linguist thus acts as a
rater rather than an annotator. The tool records as-
sessments made by each rater. A subsequent evalua-
tion step measures the inter-rater agreement. The ac-
tual annotation step is deferred until after this evalu-
ation in order to achieve high annotation confidence.
Annotate
</bodyText>
<figureCaption confidence="0.99732">
Figure 1: Annotation-by-query workflow
</figureCaption>
<bodyText confidence="0.993489875">
CSNIPER implements an annotation-by-query ap-
proach which entails the following interlinking func-
tionalities (see fig. 1):
Query development: Corpus queries can be de-
veloped and refined within the tool. Based on query
results which are assessed and labeled by the user,
queries can be systematically evaluated and refined
for precision. This transfers some of the ideas of
</bodyText>
<figure confidence="0.903807571428571">
review
assessments
refine
query
Evaluate
Assess
Query
</figure>
<note confidence="0.3684465">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 85–90,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999916219512196">
relevance feedback, which is a common method of
improving search results in information retrieval, to
a linguistic corpus query system.
Assessment: Query results are presented to the
user as a list of sentences with optional additional
context; the user assesses and labels each sentence
as representing or not representing an instance of the
linguistic phenomenon under study. The tool imple-
ments a function that allows the user to comment
on decisions and to temporarily mark sentences with
uncertain assessments for later review.
Evaluation: Evaluation is a central functional-
ity of CSNIPElt serving three purposes. 1) It in-
tegrates with the query development by providing
feedback to refine queries and improve query pre-
cision. 2) It provides information on sentences not
labeled consistently by all users, which can be used
to review the assessments. 3) It calculates the inter-
rater agreement which is used in the corpus annota-
tion step to ensure high annotation confidence.
Corpus annotation: By assessing and labeling
query results as correct or wrong, raters provide the
tool with their annotation decisions. CSNIPElt anno-
tates the corpus with those annotation decisions that
exceed a certain inter-rater agreement threshold.
This annotation-by-query approach of querying,
assessing, evaluating and annotating allows multiple
distributed raters to incrementally improve query re-
sults and achieve high quality annotations. In this
paper, we show how such an approach is well-suited
for annotation tasks that require manual analysis
over large corpora. The approach is generalizable
to any kind of linguistic phenomena that can be lo-
cated in corpora on the basis of queries and require
manual assessment by multiple expert raters.
In the next two sections, we are providing a more
detailed description of the use-case driving the de-
velopment of CSNIPElt (sect. 2) and discuss why ex-
isting tools do not provide viable solutions (sect. 3).
Sect. 4 discusses CSNIPElt and sect. 5 draws some
conclusions and offers an outlook on the next steps.
</bodyText>
<sectionHeader confidence="0.989638" genericHeader="method">
2 Non-canonical grammatical
constructions
</sectionHeader>
<bodyText confidence="0.995113">
The initial purpose of CSNIPElt is the corpus-based
study of so-called non-canonical grammatical con-
structions (NCC) (examples (2) - (5) below):
</bodyText>
<listItem confidence="0.999617">
1. The media was now calling Reagan the front-
runner. (canonical)
2. It was Reagan whom the media was now calling
the frontrunner. (it-cleft)
3. It was the media who was now calling Reagan
the frontrunner. (it-cleft)
4. It was now that the media were calling Reagan
the frontrunner. (it-cleft)
5. Reagan the media was not calling the front-
runner. (inversion)
</listItem>
<bodyText confidence="0.983523394736842">
NCCs are linguistic constructions that deviate
in characteristic ways from the unmarked lexico-
grammatical patterning and informational ordering
in the sentence. This is exemplified by the con-
structions of sentences (2) - (5) above. While ex-
pressing the same propositional content, the order
of information units available through the permissi-
ble grammatical constructions offers interesting in-
sights into the constructional inventory of a lan-
guage. It also opens up the possibility of comparing
seemingly closely related languages in terms of the
sets of available related constructions as well as the
relations between instances of canonical and non-
canonical constructions.
In linguistics, a cleft sentence is defined as a com-
plex sentence that expresses a single proposition
where the clefted element is co-referential with the
following clause. E.g., it-clefts are comprised of the
following constituents:
clefted clause
element
The NCCs under study pose interesting chal-
lenges both from a linguistic and a natural language
processing perspective. Due to their deviation from
the canonical constructions, they come in a vari-
ety of potential construction patterns as exemplified
above. Non-canonical constructions can be expected
to be individually rarer in any given corpus than their
canonical counterparts. Their patterns of usage and
their discourse functions have not yet been described
exhaustively, especially not in representative corpus
studies because they are notoriously hard to identify
without suitable software. Their empirical distribu-
tion in corpora is thus largely unknown.
A major task in recognizing NCCs is distin-
guishing them from structurally similar construc-
dummy main verb
subject it to be
</bodyText>
<page confidence="0.87731">
86
</page>
<bodyText confidence="0.99981675">
tions with default logical and propositional content.
An example of a particular difficulty from the do-
main of it-clefts are anaphoric uses of it as in (6) be-
low that do not refer forward to the following clause,
but are the antecedents of entities previously intro-
duced in the context of preceding sentences. Other
issues arise in cases of true relative clauses as exem-
plified in (7) below:
</bodyText>
<listItem confidence="0.990259714285714">
6. London will be the only capital city in Eu-
rope where rail services are expected to make
a profit,’ he added. It is a policy that could lead
to economic and environmental chaos. [BNC:
A9N-s400]
7. It is a legal manoeuvre that declined in cur-
rency in the ’80s. [BNC: B1L-s576]
</listItem>
<bodyText confidence="0.999568576923077">
Further examples of NCCs apart from the it-clefts
addressed in this paper are wh-clefts and their sub-
types, all-clefts, there-clefts, if-because-clefts and
demonstrative clefts as well as inversions. All of
these are as hard to identify in a corpus as it-clefts.
The linguistic aim of our research is a comparison
of non-canonical constructions in English and Ger-
man. Research on these requires very large corpora
due to the relatively low frequency of the individ-
ual instances. Due to the ambiguous nature of many
NCC candidates, automatically finding them in cor-
pora is difficult. Therefore, multiple experts have to
manually assess candidates in corpora.
Our approach does not aim at the exhaustive an-
notation of all NCCs. The major goal is to improve
the understanding of the linguistic properties and us-
age of NCCs. Furthermore, we define a gold stan-
dard to evaluate algorithms for automatic NCC iden-
tification. In our task, the total number of NCCs in
any given corpus is unknown. Thus, while we can
measure the precision of queries, we cannot mea-
sure their recall. To address this, we exhaustively
annotate a small part of the corpus and extrapolate
the estimated number of total NCC candidates.
In summary, the requirements for a tool to support
multi-user annotation of NCCs are as follows:
</bodyText>
<listItem confidence="0.990584666666667">
1. querying large linguistically pre-processed
corpora and query refinement
2. assessment of sentences that are true instances
of NCCs in a multi-user setting
3. evaluation of inter-rater agreement and query
precision
</listItem>
<bodyText confidence="0.9731585">
In the following section, we review previous work
to support linguistic annotation tasks.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="method">
3 Related work
</sectionHeader>
<bodyText confidence="0.999615527777778">
We differentiate three categories of linguistic tools
which all partially fulfill our requirements: querying
tools, annotation tools, and transformation tools.
Linguistic query tools: Such tools allow to query
a corpus using linguistic features, e.g. part-of-
speech tags. Examples are ANNIS2 (Zeldes et al.,
2009) and the IMS Open Corpus Workbench (CWB)
(Christ, 1994). Both tools provide powerful query
engines designed for large linguistically annotated
corpora. Both are server-based tools that can be used
concurrently by multiple users. However, they do
not allow to assess the query results.
Linguistic annotation tools: Such tools allow
the user to add linguistic annotations to a corpus.
Examples are MMAX2 (M¨uller and Strube, 2006)
and the UIMA CAS Editor1. These tools typically
display a full document for the user to annotate. As
NCCs appear only occasionally in a text, such tools
cannot be effectively applied to our task, as they of-
fer no linguistic query capabilities to quickly locate
potential NCCs in a large corpus.
Linguistic transformation tools: Such tools al-
low the creation of annotations using transforma-
tion rules. Examples are TextMarker (Kluegl et al.,
2009) and the UAM CorpusTool (O’Donnell, 2008).
A rule has the form category := pattern and creates
new annotation of the type category on any part of
a text matching pattern. A rule for the annotation
of passive clauses in the UAM CorpusTool could be
passive-clause := clause + containing be% partici-
ple. These tools do not support the assessment of
the results, though. In contrast to the querying tools,
transformation tools are not specifically designed to
operate efficiently on large corpora. Thus, they are
hardly productive for our task, which requires the
analysis of large corpora.
</bodyText>
<sectionHeader confidence="0.999551" genericHeader="evaluation">
4 CSNIPER
</sectionHeader>
<bodyText confidence="0.996888">
We present CSNIPER, an annotation tool for non-
canonical constructions. Its main features are:
</bodyText>
<footnote confidence="0.989297">
1http://uima.apache.org/
</footnote>
<page confidence="0.999126">
87
</page>
<figureCaption confidence="0.999617">
Figure 2: Search form
</figureCaption>
<bodyText confidence="0.999580461538461">
Annotation-by-query – Sentences potentially
containing a particular type of NCC are retrieved us-
ing a query. If the sentence contains the NCC of
interest, the user manually labels it as correct and
otherwise wrong. Annotations are generated based
on the users’ assessments.
Distributed multi-user setting – Our web-based
tool supports multiple users concurrently assessing
query results. Each user can only see and edit their
own assessments and has a personal query history.
Evaluation – The evaluation module provides in-
formation on assessments, number of annotated in-
stances, query precision and inter-rater agreement.
</bodyText>
<subsectionHeader confidence="0.998317">
4.1 Implementation and data
</subsectionHeader>
<bodyText confidence="0.999956818181818">
CSNIPER is implemented in Java and uses the CWB
as its linguistic search engine (cf. sect. 3). Assess-
ments are stored in a MySQL database. Currently,
the British National Corpus (BNC) is used in our
study. Apache UIMA and DKPro Core2 are used
for linguistic pre-processing, format conversion, and
to drive the indexing of the corpora. In particular,
DKPro Core includes a reader for the BNC and a
writer for the CWB. As the BNC does not carry
lemma annotations, we add them using the DKPro
TreeTagger (Schmid, 1994) module.
</bodyText>
<subsectionHeader confidence="0.993439">
4.2 Query (Figure 2)
</subsectionHeader>
<bodyText confidence="0.999989333333333">
The user begins by selecting a Q corpus and a
Q construction type (e.g. It-Cleft). A query can be
chosen from a Q list of examples, from the Q per-
sonal query history, or a new Q query can be en-
tered. The query is applied to find instances of that
construction (e.g. “It” /VCC[] /PP[] /RC[]). Af-
ter pressing the Q Submit query button, the tool
presents the user with a KWIC view of the query
results (fig. 3). At this point, the user may choose to
</bodyText>
<footnote confidence="0.9515495">
2http://www.ukp.tu-darmstadt.de/
research/current-projects/dkpro/
</footnote>
<bodyText confidence="0.997089375">
refine and re-run the query.
As each user may use different queries, they will
typically assess different sets of query results. This
can yield a set of sentences labeled by a single user
only. Therefore, the tool can display those sentences
for assessment that other users have assessed, but the
current user has not. This allows getting labels from
all users for every NCC candidate.
</bodyText>
<subsectionHeader confidence="0.999671">
4.3 Assessment (Figure 3)
</subsectionHeader>
<bodyText confidence="0.999982896551724">
If the query results match the expectation, the user
can switch to the assessment mode by clicking the
0 Begin assessment button. At this point, an An-
notationCandidate record is created in the database
for each sentence unless a record is already present.
These records contain the offsets of the sentence in
the original text, the sentence text and the construc-
tion type. In addition, an AnnotationCandidateLabel
record is created for each sentence to hold the as-
sessment to be provided by the user.
In the assessment mode, an additional Q Label
column appears in the KWIC view. Clicking in this
column cycles through the labels correct, wrong,
check and nothing. When the user is uncertain, the
label check can be used to mark candidates for later
review. The view can be Q filtered for those sen-
tences that need to be assessed, those that have been
assessed, or those that have been labeled with check.
A , o comment can be left to further describe difficult
cases or to justify decisions. All changes are imme-
diately saved to the database, so the user can stop
assessing at any time and resume the process later.
The proper assessment of a sentence as an in-
stance of a particular construction type sometimes
depends on the context found in the preceding and
following sentences. For this purpose, clicking on
the „ book icon in the KWIC view displays the
sentence in its larger context (fig. 4). POS tags are
shown in the sentence to facilitate query refinement.
</bodyText>
<subsectionHeader confidence="0.996893">
4.4 Evaluation (Figure 5)
</subsectionHeader>
<bodyText confidence="0.998859857142857">
The evaluation function provides an overview of the
current assessment state (fig. 5). We support two
evaluation views: by construction type and by query.
By construction type: In this view, one or more
,z corpora, ,s types, and ,a users can be selected
for evaluation. For these, all annotation candidates
and the respective statistics are displayed. It is pos-
</bodyText>
<page confidence="0.998836">
88
</page>
<figureCaption confidence="0.999445">
Figure 3: KWIC view of query results and assessments
</figureCaption>
<bodyText confidence="0.97739628">
sible to ,s filter for correct, wrong, disputed, incom-
pletely assessed, and unassessed candidates. A can-
didate is disputed if it is not labeled consistently by
all selected users. A candidate is incompletely as-
sessed if at least one of the selected users labeled
it and at least one other did not. Investigating dis-
puted cases and ,s inter-rater agreement per type
using Fleiss’ Kappa (Fleiss, 1971) are the main uses
of this view. The inter-rater agreement is calculated
using only candidates labeled by all selected users.
By query: In this view, query precision and as-
sessment completeness are calculated for a set of
n queries and ,s users. The query precision is cal-
culated from the labeled candidates as:
precision = |TP |+ |FP|
|TP|
We treat a candidate as a true positive (TP) if:
1) the number of correct labels is larger than the
number of wrong labels; 2) the ratio of correct labels
compared to the number of raters exceeds a given
,s threshold. Candidates are conversely treated as
false positives (FPs) if the number of wrong labels
is larger and the threshold is exceeded. The thresh-
old controls the confidence of the TP and, thus, of
the annotations generated from them (cf. sect. 4.5).
</bodyText>
<figureCaption confidence="0.996756">
Figure 4: Sentence context view with POS tags
</figureCaption>
<bodyText confidence="0.999827875">
If a candidate is neither TP nor FP, it is unknown
(UNK). When calculating precision, UNK candi-
dates are counted as FP. The estimated precision is
the precision to be expected if TP and FP are equally
distributed over the set of candidates. It takes into
account only the currently known TP and FP and ig-
nores the UNK candidates. Both values are the same
once all candidates have been labeled by all users.
</bodyText>
<subsectionHeader confidence="0.965094">
4.5 Annotation
</subsectionHeader>
<bodyText confidence="0.9999468">
When the assessment process is complete, corpus
annotations can be generated from the assessed can-
didates. Here, we employ the thresholded major-
ity vote approach that we also use to determine the
TP/FP in sect. 4.4. Annotations for the respective
NCC type are added directly to the corpus. The aug-
mented corpus can be used in further exploratory
work. Alternatively, a file with all assessed candi-
dates can be generated to serve as training data for
identification methods based on machine learning.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999794076923077">
We have presented CSNIPER, a tool for the an-
notation of linguistic phenomena whose investiga-
tion requires the analysis of large corpora due to
a relatively low frequency of instances and whose
identification requires expert knowledge to distin-
guish them from other similar constructions. Our
tool integrates the complete functionality needed for
the annotation-by-query workflow. It provides dis-
tributed multi-user annotation and evaluation. The
feedback provided by the integrated evaluation mod-
ule can be used to systematically refine queries and
improve assessments. Finally, high-confidence an-
notations can be generated from the assessments.
</bodyText>
<page confidence="0.99938">
89
</page>
<figureCaption confidence="0.999019">
Figure 5: Evaluation by query and by NCC type
</figureCaption>
<bodyText confidence="0.99976275">
The annotation-by-query approach can be gener-
alized beyond non-canonical constructions to other
linguistic phenomena with similar properties. An
example could be metaphors, which typically also
appear with comparatively low frequency and re-
quire expert knowledge to be annotated. We plan
to integrate further automatic annotations and query
possibilities to support such further use-cases.
</bodyText>
<sectionHeader confidence="0.999197" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998256">
We would like to thank Erik-Lˆan Do Dinh, who assisted
in implementing CSNIPER as well as Gert Webelhuth and
Janina Rado for testing and providing valuable feedback.
This work has been supported by the Hessian research
excellence program “Landes-Offensive zur Entwicklung
Wissenschaftlich-¨okonomischer Exzellenz” (LOEWE) as
part of the research center “Digital Humanities” and by
the Volkswagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806.
Data cited herein have been extracted from the British
National Corpus, distributed by Oxford University Com-
puting Services on behalf of the BNC Consortium. All
rights in the texts cited are reserved.
</bodyText>
<sectionHeader confidence="0.999592" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999667325">
BNC Consortium. 2007. The British National Corpus,
version 3 (BNC XML Edition). Distributed by Oxford
University Computing Services p.p. the BNC Consor-
tium,http://www.natcorp.ox.ac.uk/.
Oliver Christ. 1994. A modular and flexible architec-
ture for an integrated corpus query system. In Proc.
of the 3rd Conference on Computational Lexicography
and Text Research (COMPLEX’94), pages 23–32, Bu-
dapest, Hungary, Jul.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. In Psychological Bulletin,
volume 76 (5), pages 378–381. American Psychologi-
cal Association, Washington, DC.
Peter Kluegl, Martin Atzmueller, and Frank Puppe.
2009. TextMarker: A tool for rule-based informa-
tion extraction. In Christian Chiarcos, Richard Eckart
de Castilho, and Manfred Stede, editors, Proc. of the
Biennial GSCL Conference 2009, 2nd UIMA@GSCL
Workshop, pages 233–240. Gunter Narr Verlag, Sep.
Christoph M¨uller and Michael Strube. 2006. Multi-level
annotation of linguistic data with MMAX2. In Sabine
Braun, Kurt Kohn, and Joybrato Mukherjee, editors,
Corpus Technology and Language Pedagogy: New Re-
sources, New Tools, New Methods, pages 197–214. Pe-
ter Lang, Frankfurt am Main, Germany, Aug.
Mick O’Donnell. 2008. The UAM CorpusTool: Soft-
ware for corpus annotation and exploration. In Car-
men M. et al. Bretones Callejas, editor, Applied Lin-
guistics Now: Understanding Language and Mind
/ La Ling¨uistica Aplicada Hoy: Comprendiendo el
Lenguaje y la Mente, pages 1433–1447. Almeria: Uni-
versidad de Almeria.
Helmut Schmid. 1994. Improvements in part-of-speech
tagging with an application to German. In Proc. of Int.
Conference on New Methods in Language Processing,
pages 44–49, Manchester, UK, Sep.
Amir Zeldes, Julia Ritz, Anke L¨udeling, and Christian
Chiarcos. 2009. ANNIS: A search tool for multi-
layer annotated corpora. In Proc. of Corpus Linguis-
tics 2009, Liverpool, UK, Jul.
</reference>
<page confidence="0.998634">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.635434">
<title confidence="0.998834">Annotation-by-query for non-canonical constructions in large corpora</title>
<author confidence="0.8292135">Richard Eckart de_Castilho</author>
<author confidence="0.8292135">Iryna Gurevych Sabine Bartsch Ubiquitous Knowledge Processing Lab English linguistics</author>
<affiliation confidence="0.980039">Department of Computer Science Department of Linguistics and Literary Studies Technische Universit¨at Darmstadt Technische Universit¨at Darmstadt</affiliation>
<web confidence="0.996871">http://www.ukp.tu-darmstadt.de http://www.linglit.tu-darmstadt.de</web>
<abstract confidence="0.999731916666667">present Sniper), a tool that implements (i) a web-based multiuser scenario for identifying and annotating non-canonical grammatical constructions in large corpora based on linguistic queries and (ii) evaluation of annotation quality by measuring inter-rater agreement. This annotationby-query approach efficiently harnesses expert knowledge to identify instances of linguistic phenomena that are hard to identify by means of existing automatic annotation tools.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>BNC Consortium</author>
</authors>
<date>2007</date>
<booktitle>The British National Corpus, version 3 (BNC XML Edition). Distributed by Oxford University Computing Services p.p. the BNC Consortium,http://www.natcorp.ox.ac.uk/.</booktitle>
<contexts>
<context position="1961" citStr="Consortium, 2007" startWordPosition="264" endWordPosition="265">n this paper, are a case in point. NCCs, of which cleft-sentences are a well-known example, raise a number of issues that prevent their reliable automatic identification in corpora. Yet, they warrant corpus study due to the relatively low frequency of individual instances, their deviation from canonical construction patterns and frequent ambiguity. This makes them hard to distinguish from other, seemingly similar constructions. Expert knowledge is thus required to reliably identify and annotate such phenomena in sufficiently large corpora like the 100 mil. word British National 85 Corpus (BNC Consortium, 2007). This necessitates manual annotation which is time-consuming and error-prone when carried out by individual linguists. To overcome these issues, CSNIPER implements a web-based multi-user annotation scenario in which linguists formulate and refine queries that identify a given linguistic construction in a corpus and assess the query results to distinguish instances of the phenomenon under study (true positives) from such examples that are wrongly identified by the query (false positives). Each expert linguist thus acts as a rater rather than an annotator. The tool records assessments made by e</context>
</contexts>
<marker>Consortium, 2007</marker>
<rawString>BNC Consortium. 2007. The British National Corpus, version 3 (BNC XML Edition). Distributed by Oxford University Computing Services p.p. the BNC Consortium,http://www.natcorp.ox.ac.uk/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Christ</author>
</authors>
<title>A modular and flexible architecture for an integrated corpus query system.</title>
<date>1994</date>
<booktitle>In Proc. of the 3rd Conference on Computational Lexicography and Text Research (COMPLEX’94),</booktitle>
<pages>23--32</pages>
<location>Budapest, Hungary,</location>
<contexts>
<context position="10390" citStr="Christ, 1994" startWordPosition="1572" endWordPosition="1573">. assessment of sentences that are true instances of NCCs in a multi-user setting 3. evaluation of inter-rater agreement and query precision In the following section, we review previous work to support linguistic annotation tasks. 3 Related work We differentiate three categories of linguistic tools which all partially fulfill our requirements: querying tools, annotation tools, and transformation tools. Linguistic query tools: Such tools allow to query a corpus using linguistic features, e.g. part-ofspeech tags. Examples are ANNIS2 (Zeldes et al., 2009) and the IMS Open Corpus Workbench (CWB) (Christ, 1994). Both tools provide powerful query engines designed for large linguistically annotated corpora. Both are server-based tools that can be used concurrently by multiple users. However, they do not allow to assess the query results. Linguistic annotation tools: Such tools allow the user to add linguistic annotations to a corpus. Examples are MMAX2 (M¨uller and Strube, 2006) and the UIMA CAS Editor1. These tools typically display a full document for the user to annotate. As NCCs appear only occasionally in a text, such tools cannot be effectively applied to our task, as they offer no linguistic qu</context>
</contexts>
<marker>Christ, 1994</marker>
<rawString>Oliver Christ. 1994. A modular and flexible architecture for an integrated corpus query system. In Proc. of the 3rd Conference on Computational Lexicography and Text Research (COMPLEX’94), pages 23–32, Budapest, Hungary, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>In Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<pages>378--381</pages>
<publisher>American Psychological Association,</publisher>
<location>Washington, DC.</location>
<contexts>
<context position="16380" citStr="Fleiss, 1971" startWordPosition="2563" endWordPosition="2564">,z corpora, ,s types, and ,a users can be selected for evaluation. For these, all annotation candidates and the respective statistics are displayed. It is pos88 Figure 3: KWIC view of query results and assessments sible to ,s filter for correct, wrong, disputed, incompletely assessed, and unassessed candidates. A candidate is disputed if it is not labeled consistently by all selected users. A candidate is incompletely assessed if at least one of the selected users labeled it and at least one other did not. Investigating disputed cases and ,s inter-rater agreement per type using Fleiss’ Kappa (Fleiss, 1971) are the main uses of this view. The inter-rater agreement is calculated using only candidates labeled by all selected users. By query: In this view, query precision and assessment completeness are calculated for a set of n queries and ,s users. The query precision is calculated from the labeled candidates as: precision = |TP |+ |FP| |TP| We treat a candidate as a true positive (TP) if: 1) the number of correct labels is larger than the number of wrong labels; 2) the ratio of correct labels compared to the number of raters exceeds a given ,s threshold. Candidates are conversely treated as fals</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. In Psychological Bulletin, volume 76 (5), pages 378–381. American Psychological Association, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Kluegl</author>
<author>Martin Atzmueller</author>
<author>Frank Puppe</author>
</authors>
<title>TextMarker: A tool for rule-based information extraction.</title>
<date>2009</date>
<booktitle>Proc. of the Biennial GSCL Conference 2009, 2nd UIMA@GSCL Workshop,</booktitle>
<pages>233--240</pages>
<editor>In Christian Chiarcos, Richard Eckart de Castilho, and Manfred Stede, editors,</editor>
<publisher>Gunter Narr Verlag, Sep.</publisher>
<contexts>
<context position="11210" citStr="Kluegl et al., 2009" startWordPosition="1699" endWordPosition="1702"> to assess the query results. Linguistic annotation tools: Such tools allow the user to add linguistic annotations to a corpus. Examples are MMAX2 (M¨uller and Strube, 2006) and the UIMA CAS Editor1. These tools typically display a full document for the user to annotate. As NCCs appear only occasionally in a text, such tools cannot be effectively applied to our task, as they offer no linguistic query capabilities to quickly locate potential NCCs in a large corpus. Linguistic transformation tools: Such tools allow the creation of annotations using transformation rules. Examples are TextMarker (Kluegl et al., 2009) and the UAM CorpusTool (O’Donnell, 2008). A rule has the form category := pattern and creates new annotation of the type category on any part of a text matching pattern. A rule for the annotation of passive clauses in the UAM CorpusTool could be passive-clause := clause + containing be% participle. These tools do not support the assessment of the results, though. In contrast to the querying tools, transformation tools are not specifically designed to operate efficiently on large corpora. Thus, they are hardly productive for our task, which requires the analysis of large corpora. 4 CSNIPER We </context>
</contexts>
<marker>Kluegl, Atzmueller, Puppe, 2009</marker>
<rawString>Peter Kluegl, Martin Atzmueller, and Frank Puppe. 2009. TextMarker: A tool for rule-based information extraction. In Christian Chiarcos, Richard Eckart de Castilho, and Manfred Stede, editors, Proc. of the Biennial GSCL Conference 2009, 2nd UIMA@GSCL Workshop, pages 233–240. Gunter Narr Verlag, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph M¨uller</author>
<author>Michael Strube</author>
</authors>
<title>Multi-level annotation of linguistic data with MMAX2.</title>
<date>2006</date>
<booktitle>Corpus Technology and Language Pedagogy:</booktitle>
<pages>197--214</pages>
<editor>In Sabine Braun, Kurt Kohn, and Joybrato Mukherjee, editors,</editor>
<location>New Resources, New Tools, New Methods,</location>
<marker>M¨uller, Strube, 2006</marker>
<rawString>Christoph M¨uller and Michael Strube. 2006. Multi-level annotation of linguistic data with MMAX2. In Sabine Braun, Kurt Kohn, and Joybrato Mukherjee, editors, Corpus Technology and Language Pedagogy: New Resources, New Tools, New Methods, pages 197–214. Peter Lang, Frankfurt am Main, Germany, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mick O’Donnell</author>
</authors>
<title>The UAM CorpusTool: Software for corpus annotation and exploration.</title>
<date>2008</date>
<booktitle>Applied Linguistics Now: Understanding Language and Mind / La Ling¨uistica Aplicada Hoy: Comprendiendo el Lenguaje y la Mente,</booktitle>
<pages>1433--1447</pages>
<editor>In Carmen M. et al. Bretones Callejas, editor,</editor>
<institution>Universidad de Almeria.</institution>
<location>Almeria:</location>
<marker>O’Donnell, 2008</marker>
<rawString>Mick O’Donnell. 2008. The UAM CorpusTool: Software for corpus annotation and exploration. In Carmen M. et al. Bretones Callejas, editor, Applied Linguistics Now: Understanding Language and Mind / La Ling¨uistica Aplicada Hoy: Comprendiendo el Lenguaje y la Mente, pages 1433–1447. Almeria: Universidad de Almeria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Improvements in part-of-speech tagging with an application to German.</title>
<date>1994</date>
<booktitle>In Proc. of Int. Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="13116" citStr="Schmid, 1994" startWordPosition="2004" endWordPosition="2005">er of annotated instances, query precision and inter-rater agreement. 4.1 Implementation and data CSNIPER is implemented in Java and uses the CWB as its linguistic search engine (cf. sect. 3). Assessments are stored in a MySQL database. Currently, the British National Corpus (BNC) is used in our study. Apache UIMA and DKPro Core2 are used for linguistic pre-processing, format conversion, and to drive the indexing of the corpora. In particular, DKPro Core includes a reader for the BNC and a writer for the CWB. As the BNC does not carry lemma annotations, we add them using the DKPro TreeTagger (Schmid, 1994) module. 4.2 Query (Figure 2) The user begins by selecting a Q corpus and a Q construction type (e.g. It-Cleft). A query can be chosen from a Q list of examples, from the Q personal query history, or a new Q query can be entered. The query is applied to find instances of that construction (e.g. “It” /VCC[] /PP[] /RC[]). After pressing the Q Submit query button, the tool presents the user with a KWIC view of the query results (fig. 3). At this point, the user may choose to 2http://www.ukp.tu-darmstadt.de/ research/current-projects/dkpro/ refine and re-run the query. As each user may use differe</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Improvements in part-of-speech tagging with an application to German. In Proc. of Int. Conference on New Methods in Language Processing, pages 44–49, Manchester, UK, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Zeldes</author>
<author>Julia Ritz</author>
<author>Anke L¨udeling</author>
<author>Christian Chiarcos</author>
</authors>
<title>ANNIS: A search tool for multilayer annotated corpora.</title>
<date>2009</date>
<booktitle>In Proc. of Corpus Linguistics 2009,</booktitle>
<location>Liverpool, UK,</location>
<marker>Zeldes, Ritz, L¨udeling, Chiarcos, 2009</marker>
<rawString>Amir Zeldes, Julia Ritz, Anke L¨udeling, and Christian Chiarcos. 2009. ANNIS: A search tool for multilayer annotated corpora. In Proc. of Corpus Linguistics 2009, Liverpool, UK, Jul.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>