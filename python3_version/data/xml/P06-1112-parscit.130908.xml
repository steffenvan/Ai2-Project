<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000881">
<title confidence="0.9985115">
Exploring Correlation of Dependency Relation Paths
for Answer Extraction
</title>
<author confidence="0.999418">
Dan Shen
</author>
<affiliation confidence="0.833008333333333">
Department of Computational Linguistics
Saarland University
Saarbruecken, Germany
</affiliation>
<email confidence="0.991225">
dshen@coli.uni-sb.de
</email>
<author confidence="0.988324">
Dietrich Klakow
</author>
<affiliation confidence="0.822203333333333">
Spoken Language Systems
Saarland University
Saarbruecken, Germany
</affiliation>
<email confidence="0.995157">
klakow@lsv.uni-saarland.de
</email>
<sectionHeader confidence="0.993821" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969833333333">
In this paper, we explore correlation of
dependency relation paths to rank candi-
date answers in answer extraction. Using
the correlation measure, we compare de-
pendency relations of a candidate answer
and mapped question phrases in sentence
with the corresponding relations in ques-
tion. Different from previous studies, we
propose an approximate phrase mapping
algorithm and incorporate the mapping
score into the correlation measure. The
correlations are further incorporated into
a Maximum Entropy-based ranking model
which estimates path weights from train-
ing. Experimental results show that our
method significantly outperforms state-of-
the-art syntactic relation-based methods
by up to 20% in MRR.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9960861">
Answer Extraction is one of basic modules in open
domain Question Answering (QA). It is to further
process relevant sentences extracted with Passage /
Sentence Retrieval and pinpoint exact answers us-
ing more linguistic-motivated analysis. Since QA
turns to find exact answers rather than text snippets
in recent years, answer extraction becomes more
and more crucial.
Typically, answer extraction works in the fol-
lowing steps:
</bodyText>
<listItem confidence="0.999876571428571">
• Recognize expected answer type of a ques-
tion.
• Annotate relevant sentences with various
types of named entities.
• Regard the phrases annotated with the ex-
pected answer type as candidate answers.
• Rank candidate answers.
</listItem>
<bodyText confidence="0.999936205882353">
In the above work flow, answer extraction heav-
ily relies on named entity recognition (NER). On
one hand, NER reduces the number of candidate
answers and eases answer ranking. On the other
hand, the errors from NER directly degrade an-
swer extraction performance. To our knowledge,
most top ranked QA systems in TREC are sup-
ported by effective NER modules which may iden-
tify and classify more than 20 types of named en-
tities (NE), such as abbreviation, music, movie,
etc. However, developing such named entity rec-
ognizer is not trivial. Up to now, we haven’t found
any paper relevant to QA-specific NER develop-
ment. So, it is hard to follow their work. In this pa-
per, we just use a general MUC-based NER, which
makes our results reproducible.
A general MUC-based NER can’t annotate a
large number of NE classes. In this case, all
noun phrases in sentences are regarded as candi-
date answers, which makes candidate answer sets
much larger than those filtered by a well devel-
oped NER. The larger candidate answer sets result
in the more difficult answer extraction. Previous
methods working on surface word level, such as
density-based ranking and pattern matching, may
not perform well. Deeper linguistic analysis has
to be conducted. This paper proposes a statisti-
cal method which exploring correlation of depen-
dency relation paths to rank candidate answers. It
is motivated by the observation that relations be-
tween proper answers and question phrases in can-
didate sentences are always similar to the corre-
sponding relations in question. For example, the
question ”What did Alfred Nobel invent?” and the
</bodyText>
<page confidence="0.981514">
889
</page>
<note confidence="0.5587195">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 889–896,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.996460714285714">
candidate sentence ”... in the will of Swedish in-
dustrialist Alfred Nobel, who invented dynamite.”
For each question, firstly, dependency relation
paths are defined and extracted from the question
and each of its candidate sentences. Secondly,
the paths from the question and the candidate sen-
tence are paired according to question phrase map-
ping score. Thirdly, correlation between two paths
of each pair is calculated by employing Dynamic
Time Warping algorithm. The input of the cal-
culation is correlations between dependency re-
lations, which are estimated from a set of train-
ing path pairs. Lastly, a Maximum Entropy-based
ranking model is proposed to incorporate the path
correlations and rank candidate answers. Further-
more, sentence supportive measure are presented
according to correlations of relation paths among
question phrases. It is applied to re-rank the can-
didate answers extracted from the different candi-
date sentences. Considering phrases may provide
more accurate information than individual words,
we extract dependency relations on phrase level
instead of word level.
The experiment on TREC questions shows that
our method significantly outperforms a density-
based method by 50% in MRR and three state-
of-the-art syntactic-based methods by up to 20%
in MRR. Furthermore, we classify questions by
judging whether NER is used. We investigate
how these methods perform on the two question
sets. The results indicate that our method achieves
better performance than the other syntactic-based
methods on both question sets. Especially for
more difficult questions, for which NER may not
help, our method improves MRR by up to 31%.
The paper is organized as follows. Section 2
discusses related work and clarifies what is new in
this paper. Section 3 presents relation path corre-
lation in detail. Section 4 and 5 discuss how to in-
corporate the correlations for answer ranking and
re-ranking. Section 6 reports experiment and re-
sults.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999931655172414">
In recent years’ TREC Evaluation, most top
ranked QA systems use syntactic information in
answer extraction. Next, we will briefly discuss
the main usages.
(Kaisser and Becker, 2004) match a question
into one of predefined patterns, such as ”When
did Jack Welch retire from GE?” to the pattern
”When+did+NP+Verb+NPorPP”. For each ques-
tion pattern, there is a set of syntactic structures for
potential answer. Candidate answers are ranked
by matching the syntactic structures. This method
worked well on TREC questions. However, it
is costing to manually construct question patterns
and syntactic structures of the patterns.
(Shen et al., 2005) classify question words into
four classes target word, head word, subject word
and verb. For each class, syntactic relation pat-
terns which contain one question word and one
proper answer are automatically extracted and
scored from training sentences. Then, candidate
answers are ranked by partial matching to the syn-
tactic relation patterns using tree kernel. However,
the criterion to classify the question words is not
clear in their paper. Proper answers may have ab-
solutely different relations with different subject
words in sentences. They don’t consider the cor-
responding relations in questions.
(Tanev et al., 2004; Wu et al., 2005) compare
syntactic relations in questions and those in an-
swer sentences. (Tanev et al., 2004) reconstruct
a basic syntactic template tree for a question, in
which one of the nodes denotes expected answer
position. Then, answer candidates for this ques-
tion are ranked by matching sentence syntactic
tree to the question template tree. Furthermore, the
matching is weighted by lexical variations. (Wu et
al., 2005) combine n-gram proximity search and
syntactic relation matching. For syntactic rela-
tion matching, question tree and sentence subtree
around a candidate answer are matched from node
to node.
Although the above systems apply the different
methods to compare relations in question and an-
swer sentences, they follow the same hypothesis
that proper answers are more likely to have same
relations in question and answer sentences. For
example, in question ”Who founded the Black Pan-
thers organization?”, where, the question word
”who” has the dependency relations ”subj” with
”found” and ”subj obj nn” with ”Black Panthers
organization”, in sentence ”Hilliard introduced
Bobby Seale, who co-founded the Black Panther
Party here ...”, the proper answer ”Bobby Seale”
has the same relations with most question phrases.
These methods achieve high precision, but poor
recall due to relation variations. One meaning
is often represented as different relation combi-
nations. In the above example, appositive rela-
</bodyText>
<page confidence="0.993962">
890
</page>
<bodyText confidence="0.99994734375">
tion frequently appears in answer sentences, such
as ”Black Panther Party co-founder Bobby Seale
is ordered bound and gagged ...” and indicates
proper answer Bobby Seale although it is asked in
different way in the question.
(Cui et al., 2004) propose an approximate de-
pendency relation matching method for both pas-
sage retrieval and answer extraction. The simi-
larity between two relations is measured by their
co-occurrence rather than exact matching. They
state that their method effectively overcomes the
limitation of the previous exact matching meth-
ods. Lastly, they use the sum of similarities of
all path pairs to rank candidate answers, which is
based on the assumption that all paths have equal
weights. However, it might not be true. For ex-
ample, in question ”What book did Rachel Carson
write in 1962?”, the phrase ”Rachel Carson” looks
like more important than ”1962” since the former
is question topic and the latter is a constraint for
expected answer. In addition, lexical variations
are not well considered and a weak relation path
alignment algorithm is used in their work.
Based on the previous works, this paper ex-
plores correlation of dependency relation paths be-
tween questions and candidate sentences. Dy-
namic time warping algorithm is adapted to cal-
culate path correlations and approximate phrase
mapping is proposed to cope with phrase varia-
tions. Finally, maximum entropy-based ranking
model is developed to incorporate the correlations
and rank candidate answers.
</bodyText>
<sectionHeader confidence="0.992826" genericHeader="method">
3 Dependency Relation Path Correlation
</sectionHeader>
<bodyText confidence="0.999771">
In this section, we discuss how the method per-
forms in detail.
</bodyText>
<subsectionHeader confidence="0.998242">
3.1 Dependency Relation Path Extraction
</subsectionHeader>
<bodyText confidence="0.99993">
We parse questions and candidate sentences with
N iniPar (Lin, 1994), a fast and robust parser for
grammatical dependency relations. Then, we ex-
tract relation paths from dependency trees.
Dependency relation path is defined as a struc-
ture P =&lt; N1, R, N2 &gt; where, N1, N2 are
two phrases and R is a relation sequence R =&lt;
r1, ���,ri &gt; in which ri is one of the predefined de-
pendency relations. Totally, there are 42 relations
defined in N iniPar. A relation sequence R be-
tween two phrases N1, N2 is extracted by travers-
ing from the N1 node to the N2 node in a depen-
dency tree.
</bodyText>
<figure confidence="0.992994043478261">
Q: What book did Rachel Carson write in 1962?
Paths for Answer Ranking
N1 (EAP) R N2
What det book
What det obj subj Rachel Carson
What det obj write
What det obj mod pcomp-n 1962
Paths for Answer Re-ranking
book obj subj Rachel Carson
book obj write
book obj mod pcomp-n 1962
...
S: Rachel Carson ’s 1962 book &amp;quot; Silent Spring &amp;quot; said
dieldrin causes mania.
Paths for Answer Ranking
N1 (CA) R N2
Silent Spring title book
Silent Spring title gen Rachel Carson
Silent Spring title num 1962
Paths for Answer Re-ranking
book gen Rachel Carson
book num 1962
...
</figure>
<figureCaption confidence="0.898044">
Figure 1: Relation Paths for sample question and
sentence. EAP indicates expected answer position;
CA indicates candidate answer
</figureCaption>
<bodyText confidence="0.999704652173913">
For each question, we extract relation paths
among noun phrases, main verb and question
word. The question word is further replaced with
”EAP”, which indicates the expected answer po-
sition. For each candidate sentence, we firstly
extract relation paths between answer candidates
and mapped question phrases. These paths will
be used for answer ranking (Section 4). Secondly,
we extract relation paths among mapped question
phrases. These paths will be used for answer re-
ranking (Section 5). Question phrase mapping will
be discussed in Section 3.4. Figure 1 shows some
relation paths extracted for an example question
and candidate sentence.
Next, the relation paths in a question and each
of its candidate sentences are paired according to
their phrase similarity. For any two relation path
Pi and Pj which are extracted from the ques-
tion and the candidate sentence respectively, if
Sim(Ni1, Nj1) &gt; 0 and Sim(Ni2, Nj2) &gt; 0,
Pi and Pj are paired as &lt; Pi, Pj &gt;. The ques-
tion phrase ”EAP” is mapped to candidate answer
phrase in the sentence. The similarity between two
</bodyText>
<page confidence="0.987993">
891
</page>
<figure confidence="0.8229455">
Path Pairs for Answer Ranking
N1 (EAP / CA) Rq Rs N2
Silent Spring det title book
Silent Spring det obj subj title gen Rachel Carson
Silent Spring det obj mod pcomp-n title num 1962
Path Pairs for Answer Re-ranking
N1 Rq Rs N2
book obj subj gen Rachel Carson
book obj mod pcomp-n num 1962
...
</figure>
<figureCaption confidence="0.998788">
Figure 2: Paired Relation Path
</figureCaption>
<bodyText confidence="0.998341666666667">
phrases will be discussed in Section 3.4. Figure 2
further shows the paired relation paths which are
presented in Figure 1.
</bodyText>
<subsectionHeader confidence="0.992936">
3.2 Dependency Relation Path Correlation
</subsectionHeader>
<bodyText confidence="0.998237777777778">
Comparing a proper answer and other wrong can-
didate answers in each sentence, we assume that
relation paths between the proper answer and
question phrases in the sentence are more corre-
lated to the corresponding paths in question. So,
for each path pair &lt; P1, P2 &gt;, we measure the
correlation between its two paths P1 and P2.
We derive the correlations between paths by
adapting dynamic time warping (DTW) algorithm
(Rabiner et al., 1978). DTW is to find an optimal
alignment between two sequences which maxi-
mizes the accumulated correlation between two
sequences. A sketch of the adapted algorithm is
as follows.
Let R1 =&lt; r11, ..., r1n &gt;, (n = 1, ..., N)
and R2 =&lt; r21, ..., r2m &gt;, (m = 1, ..., M) de-
note two relation sequences. R1 and R2 consist
of N and M relations respectively. R1(n) =
r1n and R2(m) = r2m. Cor(r1,r2) denotes
the correlation between two individual relations
r1, r2, which is estimated by a statistical model
during training (Section 3.3). Given the corre-
lations Cor(r1n, r2m) for each pair of relations
(r1n, r2m) within R1 and R2, the goal of DTW is
to find a path, m = map(n), which map n onto the
corresponding m such that the accumulated corre-
lation Cor* along the path is maximized.
</bodyText>
<equation confidence="0.95800525">
� N
Cor* = max Cor(R1(n), R2(map(n))
map(n)
n=1
</equation>
<bodyText confidence="0.99972">
A dynamic programming method is used to de-
termine the optimum path map(n). The accumu-
lated correlation CorA to any grid point (n, m)
can be recursively calculated as
</bodyText>
<equation confidence="0.995562333333333">
CorA(n, m) = Cor(r1n, r2m) + max
q&lt;m
Cor* = CorA(N, M)
</equation>
<bodyText confidence="0.998738">
The overall correlation measure has to be nor-
malized as longer sequences normally give higher
correlation value. So, the correlation between two
sequences R1 and R2 is calculated as
</bodyText>
<equation confidence="0.780727">
Cor(R1, R2) = Cor*/ max(N, M)
</equation>
<bodyText confidence="0.998398">
Finally, we define the correlation between two
relation paths P1 and P2 as
</bodyText>
<equation confidence="0.991483">
Cor(P1, P2) = Cor(R1, R2) X Sim(N11, N21)
X Sim(N12, N22)
</equation>
<bodyText confidence="0.978454333333333">
Where, Sim(N11, N21) and Sim(N12, N22)
are the phrase mapping score when pairing
two paths, which will be described in Section
3.4. If two phrases are absolutely different
Cor(N11, N21) = 0 or Cor(N12, N22) = 0, the
paths may not be paired since Cor(P1, P2) = 0.
</bodyText>
<subsectionHeader confidence="0.979257">
3.3 Relation Correlation Estimation
</subsectionHeader>
<bodyText confidence="0.998106833333334">
In the above section, we have described how to
measure path correlations. The measure requires
relation correlations Cor(r1, r2) as inputs. We
apply a statistical method to estimate the relation
correlations from a set of training path pairs. The
training data collecting will be described in Sec-
tion 6.1.
For each question and its answer sentences in
training data, we extract relation paths between
”EAP” and other phrases in the question and
paths between proper answer and mapped ques-
tion phrases in the sentences. After pairing the
question paths and the corresponding sentence
paths, correlation of two relations is measured by
their bipartite co-occurrence in all training path
pairs. Mutual information-based measure (Cui et
al., 2004) is employed to calculate the relation cor-
relations.
</bodyText>
<equation confidence="0.996263333333333">
� � X �(rQ � , rS j )
Cor(rQ � , rS j ) = log
fQ(rQ� ) X fS(rSj )
</equation>
<bodyText confidence="0.984828888888889">
where, rQ� and rS � are two relations in question
paths and sentence paths respectively. fQ(rQ� ) and
fS(rS� ) are the numbers of occurrences of rQ� in
question paths and rS �in sentence paths respec-
tively. S(rQ, rS� ) is 1 when rQ� and rS� co-occur
in a path pair, and 0 otherwise. α is a factor to
discount the co-occurrence value for long paths. It
is set to the inverse proportion of the sum of path
lengths of the path pair.
</bodyText>
<equation confidence="0.812986">
CorA(n − 1, q)
</equation>
<page confidence="0.983683">
892
</page>
<subsectionHeader confidence="0.947956">
3.4 Approximate Question Phrase Mapping
</subsectionHeader>
<bodyText confidence="0.981721571428572">
Basic noun phrases (BNP) and verbs in questions
are mapped to their candidate sentences. A BNP
is defined as the smallest noun phrase in which
there are no noun phrases embedded. To address
lexical and format variations between phrases, we
propose an approximate phrase mapping strategy.
A BNP is separated into a set of heads
H = {h1, ..., hi} and a set of modifiers M =
{m1,...mj}. Some heuristic rules are applied to
judge heads and modifiers: 1. If BNP is a named
entity, all words are heads. 2. The last word of
BNP is head. 3. Rest words are modifiers.
The similarity between two BNPs
Sim(BNPQ, BNP,) is defined as:
</bodyText>
<equation confidence="0.983940333333333">
Sim(BNPQ, BNP.) = ASim(HQ, H.)
+ (1 − A)Sim(MQ, M.)
|Mq UMs|
</equation>
<bodyText confidence="0.9913365">
Furthermore, the similarity between two heads
Sim(hi, hj) are defined as:
</bodyText>
<listItem confidence="0.999534">
• Sim = 1, if hi = hj after stemming;
• Sim = 1, if hi = hj after format alternation;
• Sim = SemSim(hi, hj)
</listItem>
<bodyText confidence="0.999697380952381">
These items consider morphological, format
and semantic variations respectively. 1. The mor-
phological variations match words after stemming,
such as ”Rhodes scholars” and ”Rhodes scholar-
ships”. 2. The format alternations cope with
special characters, such as ”-” for ”Ice-T” and
”Ice T”, ”&amp;” for ”Abercrombie and Fitch” and
”Abercrombie &amp; Fitch”. 3. The semantic simi-
larity SemSim(hi, hj) is measured using Word-
Net and eXtended WordNet. We use the same
semantic path finding algorithm, relation weights
and semantic similarity measure as (Moldovan and
Novischi, 2002). For efficiency, only hypernym,
hyponym and entailment relations are considered
and search depth is set to 2 in our experiments.
Particularly, the semantic variations are not con-
sidered for NE heads and modifiers. Modifier sim-
ilarity Sim(mi, mj) only consider the morpho-
logical and format variations. Moreover, verb sim-
ilarity measure Sim(v1, v2) is the same as head
similarity measure Sim(hi, hj).
</bodyText>
<sectionHeader confidence="0.9751" genericHeader="method">
4 Candidate Answer Ranking
</sectionHeader>
<bodyText confidence="0.99998504">
According to path correlations of candidate an-
swers, a Maximum Entropy (ME)-based model is
applied to rank candidate answers. Unlike (Cui et
al., 2004), who rank candidate answers with the
sum of the path correlations, ME model may es-
timate the optimal weights of the paths based on
a training data set. (Berger et al., 1996) gave a
good description of ME model. The model we
use is similar to (Shen et al., 2005; Ravichandran
et al., 2003), which regard answer extraction as a
ranking problem instead of a classification prob-
lem. We apply Generalized Iterative Scaling for
model parameter estimation and Gaussian Prior
for smoothing.
If expected answer type is unknown during
question processing or corresponding type of
named entities isn’t recognized in candidate sen-
tences, we regard all basic noun phrases as can-
didate answers. Since a MUC-based NER loses
many types of named entities, we have to handle
larger candidate answer sets. Orthographic fea-
tures, similar to (Shen et al., 2005), are extracted to
capture word format information of candidate an-
swers, such as capitalizations, digits and lengths,
etc. We expect they may help to judge what proper
answers look like since most NER systems work
on these features.
Next, we will discuss how to incorporate path
correlations. Two facts are considered to affect
path weights: question phrase type and path
length. For each question, we divide question
phrases into four types: target, topic, constraint
and verb. Target is a kind of word which indicates
the expected answer type of the question, such as
”party” in ”What party led Australia from 1983 to
1996?”. Topic is the event/person that the ques-
tion talks about, such as ”Australia”. Intuitively, it
is the most important phrase of the question. Con-
straint are the other phrases of the question ex-
cept topic, such as ”1983” and ”1996”. Verb is
the main verb of the question, such as ”lead”. Fur-
thermore, since shorter path indicates closer rela-
tion between two phrases, we discount path corre-
lation in long question path by dividing the corre-
lation by the length of the question path. Lastly,
we sum the discounted path correlations for each
type of question phrases and fire it as a feature,
such as ”Target Cor=c, where c is the correla-
tion value for question target. ME-based rank-
ing model incorporate the orthographic and path
</bodyText>
<figure confidence="0.968473">
E
hiEHq
Sim(hi,hj )
E
hjEHs
Sim(HQ, H.) =
|Hq UHs|
Sim(MQ, M.) =
E
miEMq
Sim(mi,mj )
E
mjEMs
</figure>
<page confidence="0.998325">
893
</page>
<bodyText confidence="0.9686825">
correlation features to rank candidate answers for
each of candidate sentences.
</bodyText>
<sectionHeader confidence="0.835596" genericHeader="method">
5 Candidate Answer Re-ranking
</sectionHeader>
<bodyText confidence="0.999976">
After ranking candidate answers, we select the
highest ranked one from each candidate sentence.
In this section, we are to re-rank them according
to sentence supportive degree. We assume that a
candidate sentence supports an answer if relations
between mapped question phrases in the candidate
sentence are similar to the corresponding ones in
question. Relation paths between any two ques-
tion phrases are extracted and paired. Then, corre-
lation of each pair is calculated. Re-rank formula
is defined as follows:
</bodyText>
<equation confidence="0.99979">
�Score(answer) = � � Cor(Pd1, Pd2)
</equation>
<bodyText confidence="0.999517">
where, α is answer ranking score. It is the nor-
malized prediction value of the ME-based ranking
model described in Section 4.
</bodyText>
<equation confidence="0.608411">
i Cor(Pi1, Pi2) is
</equation>
<bodyText confidence="0.998037">
the sum of correlations of all path pairs. Finally,
the answer with the highest score is returned.
</bodyText>
<sectionHeader confidence="0.99877" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999557">
In this section, we set up experiments on TREC
factoid questions and report evaluation results.
</bodyText>
<subsectionHeader confidence="0.998061">
6.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999669189189189">
The goal of answer extraction is to identify ex-
act answers from given candidate sentence col-
lections for questions. The candidate sentences
are regarded as the most relevant sentences to the
questions and retrieved by IR techniques. Quali-
ties of the candidate sentences have a strong im-
pact on answer extraction. It is meaningless to
evaluate the questions of which none candidate
sentences contain proper answer in answer extrac-
tion experiment. To our knowledge, most of cur-
rent QA systems lose about half of questions in
sentence retrieval stage. To make more questions
evaluated in our experiments, for each of ques-
tions, we automatically build a candidate sentence
set from TREC judgements rather than use sen-
tence retrieval output.
We use TREC99-03 questions for training and
TREC04 questions for testing. As to build training
data, we retrieve all of the sentences which con-
tain proper answers from relevant documents ac-
cording to TREC judgements and answer patterns.
Then, We manually check the sentences and re-
move those in which answers cannot be supported.
As to build candidate sentence sets for testing, we
retrieve all of the sentences from relevant docu-
ments in judgements and keep those which contain
at least one question key word. Therefore, each
question has at least one proper candidate sentence
which contains proper answer in its candidate sen-
tence set.
There are 230 factoid questions (27 NIL ques-
tions) in TREC04. NIL questions are excluded
from our test set because TREC doesn’t supply
relevant documents and answer patterns for them.
Therefore, we will evaluate 203 TREC04 ques-
tions. Five answer extraction methods are evalu-
ated for comparison:
</bodyText>
<listItem confidence="0.998249238095238">
• Density: Density-based method is used as
baseline, in which we choose candidate an-
swer with the shortest surface distance to
question phrases.
• SynPattern: Syntactic relation patterns
(Shen et al., 2005) are automatically ex-
tracted from training set and are partially
matched using tree kernel.
• StrictMatch: Strict relation matching fol-
lows the assumption in (Tanev et al., 2004;
Wu et al., 2005). We implement it by adapt-
ing relation correlation score. In stead of
learning relation correlations during training,
we predefine them as: Cor(r1, r2) = 1 if
r1 = r2; 0, otherwise.
• ApprMatch: Approximate relation match-
ing (Cui et al., 2004) aligns two relation paths
using fuzzy matching and ranks candidates
according to the sum of all path similarities.
• CorME: It is the method proposed in this pa-
per. Different from ApprMatch, ME-based
</listItem>
<bodyText confidence="0.919995818181818">
ranking model is implemented to incorpo-
rate path correlations which assigns different
weights for different paths respectively. Fur-
thermore, phrase mapping score is incorpo-
rated into the path correlation measure.
These methods are briefly described in Section
2. Performance is evaluated with Mean Reciprocal
Rank (MRR). Furthermore, we list percentages of
questions correctly answered in terms of top 5 an-
swers and top 1 answer returned respectively. No
answer validations are used to adjust answers.
</bodyText>
<page confidence="0.999011">
894
</page>
<tableCaption confidence="0.999525">
Table 1: Overall performance
</tableCaption>
<table confidence="0.99971375">
Density SynPattern StrictMatch ApprMatch CorME
MRR 0.45 0.56 0.57 0.60 0.67
Top1 0.36 0.53 0.49 0.53 0.62
Top5 0.56 0.60 0.67 0.70 0.74
</table>
<subsectionHeader confidence="0.726413">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.91233936">
Table 1 shows the overall performance of the five
methods. The main observations from the table
are as follows:
1. The methods SynPattern, StrictMatch, Ap-
prMatch and CorME significantly improve
MRR by 25.0%, 26.8%, 34.5% and 50.1%
over the baseline method Density. The im-
provements may benefit from the various ex-
plorations of syntactic relations.
2. The performance of SynPattern (0.56MRR)
and StrictMatch (0.57MRR) are close. Syn-
Pattern matches relation sequences of can-
didate answers with the predefined relation
sequences extracted from a training data
set, while StrictMatch matches relation se-
quences of candidate answers with the cor-
responding relation sequences in questions.
But, both of them are based on the assump-
tion that the more number of same rela-
tions between two sequences, the more sim-
ilar the sequences are. Furthermore, since
most TREC04 questions only have one or two
phrases and many questions have similar ex-
pressions, SynPattern and StrictMatch don’t
make essential difference.
</bodyText>
<listItem confidence="0.967158">
3. ApprMatch and CorME outperform SynPat-
tern and StrictMatch by about 6.1% and
18.4% improvement in MRR. Strict matching
often fails due to various relation representa-
tions in syntactic trees. However, such vari-
ations of syntactic relations may be captured
by ApprMatch and CorME using a MI-based
statistical method.
4. CorME achieves the better performance by
11.6% than ApprMatch. The improvement
may benefit from two aspects: 1) ApprMatch
</listItem>
<bodyText confidence="0.97244948">
assigns equal weights to the paths of a can-
didate answer and question phrases, while
CorME estimate the weights according to
phrase type and path length. After training a
ME model, the weights are assigned, such as
5.72 for topic path ; 3.44 for constraints path
and 1.76 for target path. 2) CorME incorpo-
rates approximate phrase mapping scores into
path correlation measure.
We further divide the questions into two classes
according to whether NER is used in answer ex-
traction. If the expected answer type of a ques-
tion is unknown, such as ”How did James Dean
die?” or the type cannot be annotated by NER,
such as ”What ethnic group/race are Crip mem-
bers?”, we put the question in Qw/oNE set, oth-
erwise, we put it in QwNE. For the questions in
Qw/oNE, we extract all basic noun phrases and
verb phrases as candidate answers. Then, answer
extraction module has to work on the larger can-
didate sets. Using a MUC-based NER, the rec-
ognized types include person, location, organiza-
tion, date, time and money. In TREC04 questions,
123 questions are put in QwNE and 80 questions
in Qw/oNE.
</bodyText>
<tableCaption confidence="0.963026">
Table 2: Performance on two question sets QwNE
and Qw/oNE
</tableCaption>
<table confidence="0.998378">
QwNE Qw/oNE
Density 0.66 0.11
SynPattern 0.71 0.36
StrictMatch 0.70 0.36
ApprMatch 0.72 0.42
CorME 0.79 0.47
</table>
<bodyText confidence="0.99992925">
We evaluate the performance on QwNE and
Qw/oNE respectively, as shown in Table 2.
The density-based method Density (0.11MRR)
loses many questions in Qw/oNE, which indi-
cates that using only surface word information
is not sufficient for large candidate answer sets.
On the contrary, SynPattern(0.36MRR), Strict-
Pattern(0.36MRR), ApprMatch(0.42MRR) and
CorME (0.47MRR) which capture syntactic infor-
mation, perform much better than Density. Our
method CorME outperforms the other syntactic-
based methods on both QwNE and Qw/oNE. Es-
</bodyText>
<page confidence="0.996178">
895
</page>
<bodyText confidence="0.999968944444444">
pecially for more difficult questions Qw/oNE, the
improvements (up to 31% in MRR) are more ob-
vious. It indicates that our method can be used to
further enhance state-of-the-art QA systems even
if they have a good NER.
In addition, we evaluate component contribu-
tions of our method based on the main idea of
relation path correlation. Three components are
tested: 1. Appr. Mapping (Section 3.4). We re-
place approximate question phrase mapping with
exact phrase mapping and withdraw the phrase
mapping scores from path correlation measure. 2.
Answer Ranking (Section 4). Instead of using
ME model, we sum all of the path correlations to
rank candidate answers, which is similar to (Cui
et al., 2004). 3. Answer Re-ranking (Section
5). We disable this component and select top 5
answers according to answer ranking scores.
</bodyText>
<tableCaption confidence="0.998017">
Table 3: Component Contributions
</tableCaption>
<table confidence="0.9928164">
MRR
Overall 0.67
- Appr. Mapping 0.63
- Answer Ranking 0.62
- Answer Re-ranking 0.66
</table>
<bodyText confidence="0.999964384615385">
The contribution of each component is evalu-
ated with the overall performance degradation af-
ter it is removed or replaced. Some findings are
concluded from Table 3. Performances degrade
when replacing approximate phrase mapping or
ME-based answer ranking, which indicates that
both of them have positive effects on the systems.
This may be also used to explain why CorME out-
performs ApprMatch in Table 1. However, remov-
ing answer re-ranking doesn’t affect much. Since
short questions, such as ”What does AARP stand
for?”, frequently occur in TREC04, exploring the
phrase relations for such questions isn’t helpful.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999965590909091">
In this paper, we propose a relation path
correlation-based method to rank candidate an-
swers in answer extraction. We extract and pair
relation paths from questions and candidate sen-
tences. Next, we measure the relation path cor-
relation in each pair based on approximate phrase
mapping score and relation sequence alignment,
which is calculated by DTW algorithm. Lastly,
a ME-based ranking model is proposed to incor-
porate the path correlations and rank candidate
answers. The experiment on TREC questions
shows that our method significantly outperforms
a density-based method by 50% in MRR and three
state-of-the-art syntactic-based methods by up to
20% in MRR. Furthermore, the method is espe-
cially effective for difficult questions, for which
NER may not help. Therefore, it may be used to
further enhance state-of-the-art QA systems even
if they have a good NER. In the future, we are to
further evaluate the method based on the overall
performance of a QA system and adapt it to sen-
tence retrieval task.
</bodyText>
<sectionHeader confidence="0.999179" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99995672972973">
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguisitics, 22:39–71.
Hang Cui, Keya Li, Renxu Sun, Tat-Seng Chua, and
Min-Yen Kan. 2004. National university of singa-
pore at the trec-13 question answering. In Proceed-
ings of TREC2004, NIST.
M. Kaisser and T. Becker. 2004. Question answering
by searching large corpora with linguistic methods.
In Proceedings of TREC2004, NIST.
Dekang Lin. 1994. Principar—an efficient, broad-
coverage, principle-based parser. In Proceedings of
COLING1994, pages 42–488.
Dan Moldovan and Adrian Novischi. 2002. Lexical
chains for question answering. In Proceedings of
COLING2002.
L. R. Rabiner, A. E. Rosenberg, and S. E. Levinson.
1978. Considerations in dynamic time warping al-
gorithms for discrete word recognition. In Proceed-
ings of IEEE Transactions on acoustics, speech and
signal processing.
Deepak Ravichandran, Eduard Hovy, and Franz Josef
Och. 2003. Statistical qa - classifier vs. re-ranker:
What’s the difference? In Proceedings of ACL2003
workshop on Multilingual Summarization and Ques-
tion Answering.
Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow.
2005. Exploring syntactic relation patterns for ques-
tion answering. In Proceedings of IJCNLP2005.
H. Tanev, M. Kouylekov, and B. Magnini. 2004. Com-
bining linguisitic processing and web mining for
question answering: Itc-irst at trec-2004. In Pro-
ceedings of TREC2004, NIST.
M. Wu, M. Y. Duan, S. Shaikh, S. Small, and T. Strza-
lkowski. 2005. University at albany’s ilqua in trec
2005. In Proceedings of TREC2005, NIST.
</reference>
<page confidence="0.999013">
896
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.847063">
<title confidence="0.999819">Exploring Correlation of Dependency Relation Paths for Answer Extraction</title>
<author confidence="0.999999">Dan Shen</author>
<affiliation confidence="0.999963">Department of Computational Linguistics Saarland University</affiliation>
<address confidence="0.981814">Saarbruecken, Germany</address>
<email confidence="0.999007">dshen@coli.uni-sb.de</email>
<author confidence="0.993236">Dietrich Klakow</author>
<affiliation confidence="0.9731595">Spoken Language Systems Saarland University</affiliation>
<address confidence="0.97461">Saarbruecken, Germany</address>
<email confidence="0.999414">klakow@lsv.uni-saarland.de</email>
<abstract confidence="0.996701210526316">In this paper, we explore correlation of dependency relation paths to rank candidate answers in answer extraction. Using the correlation measure, we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question. Different from previous studies, we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure. The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training. Experimental results show that our method significantly outperforms state-ofthe-art syntactic relation-based methods by up to 20% in MRR.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguisitics,</journal>
<pages>22--39</pages>
<contexts>
<context position="18250" citStr="Berger et al., 1996" startWordPosition="2990" endWordPosition="2993">rticularly, the semantic variations are not considered for NE heads and modifiers. Modifier similarity Sim(mi, mj) only consider the morphological and format variations. Moreover, verb similarity measure Sim(v1, v2) is the same as head similarity measure Sim(hi, hj). 4 Candidate Answer Ranking According to path correlations of candidate answers, a Maximum Entropy (ME)-based model is applied to rank candidate answers. Unlike (Cui et al., 2004), who rank candidate answers with the sum of the path correlations, ME model may estimate the optimal weights of the paths based on a training data set. (Berger et al., 1996) gave a good description of ME model. The model we use is similar to (Shen et al., 2005; Ravichandran et al., 2003), which regard answer extraction as a ranking problem instead of a classification problem. We apply Generalized Iterative Scaling for model parameter estimation and Gaussian Prior for smoothing. If expected answer type is unknown during question processing or corresponding type of named entities isn’t recognized in candidate sentences, we regard all basic noun phrases as candidate answers. Since a MUC-based NER loses many types of named entities, we have to handle larger candidate</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguisitics, 22:39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Keya Li</author>
<author>Renxu Sun</author>
<author>Tat-Seng Chua</author>
<author>Min-Yen Kan</author>
</authors>
<title>National university of singapore at the trec-13 question answering.</title>
<date>2004</date>
<booktitle>In Proceedings of TREC2004,</booktitle>
<pages>NIST.</pages>
<contexts>
<context position="8380" citStr="Cui et al., 2004" startWordPosition="1289" endWordPosition="1292">”, in sentence ”Hilliard introduced Bobby Seale, who co-founded the Black Panther Party here ...”, the proper answer ”Bobby Seale” has the same relations with most question phrases. These methods achieve high precision, but poor recall due to relation variations. One meaning is often represented as different relation combinations. In the above example, appositive rela890 tion frequently appears in answer sentences, such as ”Black Panther Party co-founder Bobby Seale is ordered bound and gagged ...” and indicates proper answer Bobby Seale although it is asked in different way in the question. (Cui et al., 2004) propose an approximate dependency relation matching method for both passage retrieval and answer extraction. The similarity between two relations is measured by their co-occurrence rather than exact matching. They state that their method effectively overcomes the limitation of the previous exact matching methods. Lastly, they use the sum of similarities of all path pairs to rank candidate answers, which is based on the assumption that all paths have equal weights. However, it might not be true. For example, in question ”What book did Rachel Carson write in 1962?”, the phrase ”Rachel Carson” l</context>
<context position="15457" citStr="Cui et al., 2004" startWordPosition="2499" endWordPosition="2502">2) as inputs. We apply a statistical method to estimate the relation correlations from a set of training path pairs. The training data collecting will be described in Section 6.1. For each question and its answer sentences in training data, we extract relation paths between ”EAP” and other phrases in the question and paths between proper answer and mapped question phrases in the sentences. After pairing the question paths and the corresponding sentence paths, correlation of two relations is measured by their bipartite co-occurrence in all training path pairs. Mutual information-based measure (Cui et al., 2004) is employed to calculate the relation correlations. � � X �(rQ � , rS j ) Cor(rQ � , rS j ) = log fQ(rQ� ) X fS(rSj ) where, rQ� and rS � are two relations in question paths and sentence paths respectively. fQ(rQ� ) and fS(rS� ) are the numbers of occurrences of rQ� in question paths and rS �in sentence paths respectively. S(rQ, rS� ) is 1 when rQ� and rS� co-occur in a path pair, and 0 otherwise. α is a factor to discount the co-occurrence value for long paths. It is set to the inverse proportion of the sum of path lengths of the path pair. CorA(n − 1, q) 892 3.4 Approximate Question Phrase </context>
<context position="18076" citStr="Cui et al., 2004" startWordPosition="2958" endWordPosition="2961">easure as (Moldovan and Novischi, 2002). For efficiency, only hypernym, hyponym and entailment relations are considered and search depth is set to 2 in our experiments. Particularly, the semantic variations are not considered for NE heads and modifiers. Modifier similarity Sim(mi, mj) only consider the morphological and format variations. Moreover, verb similarity measure Sim(v1, v2) is the same as head similarity measure Sim(hi, hj). 4 Candidate Answer Ranking According to path correlations of candidate answers, a Maximum Entropy (ME)-based model is applied to rank candidate answers. Unlike (Cui et al., 2004), who rank candidate answers with the sum of the path correlations, ME model may estimate the optimal weights of the paths based on a training data set. (Berger et al., 1996) gave a good description of ME model. The model we use is similar to (Shen et al., 2005; Ravichandran et al., 2003), which regard answer extraction as a ranking problem instead of a classification problem. We apply Generalized Iterative Scaling for model parameter estimation and Gaussian Prior for smoothing. If expected answer type is unknown during question processing or corresponding type of named entities isn’t recogniz</context>
<context position="23715" citStr="Cui et al., 2004" startWordPosition="3888" endWordPosition="3891">thod is used as baseline, in which we choose candidate answer with the shortest surface distance to question phrases. • SynPattern: Syntactic relation patterns (Shen et al., 2005) are automatically extracted from training set and are partially matched using tree kernel. • StrictMatch: Strict relation matching follows the assumption in (Tanev et al., 2004; Wu et al., 2005). We implement it by adapting relation correlation score. In stead of learning relation correlations during training, we predefine them as: Cor(r1, r2) = 1 if r1 = r2; 0, otherwise. • ApprMatch: Approximate relation matching (Cui et al., 2004) aligns two relation paths using fuzzy matching and ranks candidates according to the sum of all path similarities. • CorME: It is the method proposed in this paper. Different from ApprMatch, ME-based ranking model is implemented to incorporate path correlations which assigns different weights for different paths respectively. Furthermore, phrase mapping score is incorporated into the path correlation measure. These methods are briefly described in Section 2. Performance is evaluated with Mean Reciprocal Rank (MRR). Furthermore, we list percentages of questions correctly answered in terms of t</context>
<context position="28523" citStr="Cui et al., 2004" startWordPosition="4661" endWordPosition="4664">) are more obvious. It indicates that our method can be used to further enhance state-of-the-art QA systems even if they have a good NER. In addition, we evaluate component contributions of our method based on the main idea of relation path correlation. Three components are tested: 1. Appr. Mapping (Section 3.4). We replace approximate question phrase mapping with exact phrase mapping and withdraw the phrase mapping scores from path correlation measure. 2. Answer Ranking (Section 4). Instead of using ME model, we sum all of the path correlations to rank candidate answers, which is similar to (Cui et al., 2004). 3. Answer Re-ranking (Section 5). We disable this component and select top 5 answers according to answer ranking scores. Table 3: Component Contributions MRR Overall 0.67 - Appr. Mapping 0.63 - Answer Ranking 0.62 - Answer Re-ranking 0.66 The contribution of each component is evaluated with the overall performance degradation after it is removed or replaced. Some findings are concluded from Table 3. Performances degrade when replacing approximate phrase mapping or ME-based answer ranking, which indicates that both of them have positive effects on the systems. This may be also used to explain</context>
</contexts>
<marker>Cui, Li, Sun, Chua, Kan, 2004</marker>
<rawString>Hang Cui, Keya Li, Renxu Sun, Tat-Seng Chua, and Min-Yen Kan. 2004. National university of singapore at the trec-13 question answering. In Proceedings of TREC2004, NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kaisser</author>
<author>T Becker</author>
</authors>
<title>Question answering by searching large corpora with linguistic methods.</title>
<date>2004</date>
<booktitle>In Proceedings of TREC2004,</booktitle>
<pages>NIST.</pages>
<contexts>
<context position="5619" citStr="Kaisser and Becker, 2004" startWordPosition="861" endWordPosition="864">oth question sets. Especially for more difficult questions, for which NER may not help, our method improves MRR by up to 31%. The paper is organized as follows. Section 2 discusses related work and clarifies what is new in this paper. Section 3 presents relation path correlation in detail. Section 4 and 5 discuss how to incorporate the correlations for answer ranking and re-ranking. Section 6 reports experiment and results. 2 Related Work In recent years’ TREC Evaluation, most top ranked QA systems use syntactic information in answer extraction. Next, we will briefly discuss the main usages. (Kaisser and Becker, 2004) match a question into one of predefined patterns, such as ”When did Jack Welch retire from GE?” to the pattern ”When+did+NP+Verb+NPorPP”. For each question pattern, there is a set of syntactic structures for potential answer. Candidate answers are ranked by matching the syntactic structures. This method worked well on TREC questions. However, it is costing to manually construct question patterns and syntactic structures of the patterns. (Shen et al., 2005) classify question words into four classes target word, head word, subject word and verb. For each class, syntactic relation patterns which</context>
</contexts>
<marker>Kaisser, Becker, 2004</marker>
<rawString>M. Kaisser and T. Becker. 2004. Question answering by searching large corpora with linguistic methods. In Proceedings of TREC2004, NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Principar—an efficient, broadcoverage, principle-based parser.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING1994,</booktitle>
<pages>42--488</pages>
<contexts>
<context position="9837" citStr="Lin, 1994" startWordPosition="1521" endWordPosition="1522"> Based on the previous works, this paper explores correlation of dependency relation paths between questions and candidate sentences. Dynamic time warping algorithm is adapted to calculate path correlations and approximate phrase mapping is proposed to cope with phrase variations. Finally, maximum entropy-based ranking model is developed to incorporate the correlations and rank candidate answers. 3 Dependency Relation Path Correlation In this section, we discuss how the method performs in detail. 3.1 Dependency Relation Path Extraction We parse questions and candidate sentences with N iniPar (Lin, 1994), a fast and robust parser for grammatical dependency relations. Then, we extract relation paths from dependency trees. Dependency relation path is defined as a structure P =&lt; N1, R, N2 &gt; where, N1, N2 are two phrases and R is a relation sequence R =&lt; r1, ���,ri &gt; in which ri is one of the predefined dependency relations. Totally, there are 42 relations defined in N iniPar. A relation sequence R between two phrases N1, N2 is extracted by traversing from the N1 node to the N2 node in a dependency tree. Q: What book did Rachel Carson write in 1962? Paths for Answer Ranking N1 (EAP) R N2 What det</context>
</contexts>
<marker>Lin, 1994</marker>
<rawString>Dekang Lin. 1994. Principar—an efficient, broadcoverage, principle-based parser. In Proceedings of COLING1994, pages 42–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Adrian Novischi</author>
</authors>
<title>Lexical chains for question answering.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING2002.</booktitle>
<contexts>
<context position="17498" citStr="Moldovan and Novischi, 2002" startWordPosition="2868" endWordPosition="2871"> if hi = hj after format alternation; • Sim = SemSim(hi, hj) These items consider morphological, format and semantic variations respectively. 1. The morphological variations match words after stemming, such as ”Rhodes scholars” and ”Rhodes scholarships”. 2. The format alternations cope with special characters, such as ”-” for ”Ice-T” and ”Ice T”, ”&amp;” for ”Abercrombie and Fitch” and ”Abercrombie &amp; Fitch”. 3. The semantic similarity SemSim(hi, hj) is measured using WordNet and eXtended WordNet. We use the same semantic path finding algorithm, relation weights and semantic similarity measure as (Moldovan and Novischi, 2002). For efficiency, only hypernym, hyponym and entailment relations are considered and search depth is set to 2 in our experiments. Particularly, the semantic variations are not considered for NE heads and modifiers. Modifier similarity Sim(mi, mj) only consider the morphological and format variations. Moreover, verb similarity measure Sim(v1, v2) is the same as head similarity measure Sim(hi, hj). 4 Candidate Answer Ranking According to path correlations of candidate answers, a Maximum Entropy (ME)-based model is applied to rank candidate answers. Unlike (Cui et al., 2004), who rank candidate a</context>
</contexts>
<marker>Moldovan, Novischi, 2002</marker>
<rawString>Dan Moldovan and Adrian Novischi. 2002. Lexical chains for question answering. In Proceedings of COLING2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
<author>A E Rosenberg</author>
<author>S E Levinson</author>
</authors>
<title>Considerations in dynamic time warping algorithms for discrete word recognition.</title>
<date>1978</date>
<booktitle>In Proceedings of IEEE Transactions on</booktitle>
<contexts>
<context position="13028" citStr="Rabiner et al., 1978" startWordPosition="2079" endWordPosition="2082">red Relation Path phrases will be discussed in Section 3.4. Figure 2 further shows the paired relation paths which are presented in Figure 1. 3.2 Dependency Relation Path Correlation Comparing a proper answer and other wrong candidate answers in each sentence, we assume that relation paths between the proper answer and question phrases in the sentence are more correlated to the corresponding paths in question. So, for each path pair &lt; P1, P2 &gt;, we measure the correlation between its two paths P1 and P2. We derive the correlations between paths by adapting dynamic time warping (DTW) algorithm (Rabiner et al., 1978). DTW is to find an optimal alignment between two sequences which maximizes the accumulated correlation between two sequences. A sketch of the adapted algorithm is as follows. Let R1 =&lt; r11, ..., r1n &gt;, (n = 1, ..., N) and R2 =&lt; r21, ..., r2m &gt;, (m = 1, ..., M) denote two relation sequences. R1 and R2 consist of N and M relations respectively. R1(n) = r1n and R2(m) = r2m. Cor(r1,r2) denotes the correlation between two individual relations r1, r2, which is estimated by a statistical model during training (Section 3.3). Given the correlations Cor(r1n, r2m) for each pair of relations (r1n, r2m) w</context>
</contexts>
<marker>Rabiner, Rosenberg, Levinson, 1978</marker>
<rawString>L. R. Rabiner, A. E. Rosenberg, and S. E. Levinson. 1978. Considerations in dynamic time warping algorithms for discrete word recognition. In Proceedings of IEEE Transactions on acoustics, speech and signal processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
<author>Franz Josef Och</author>
</authors>
<title>Statistical qa - classifier vs. re-ranker: What’s the difference?</title>
<date>2003</date>
<booktitle>In Proceedings of ACL2003 workshop on Multilingual Summarization and Question Answering.</booktitle>
<contexts>
<context position="18365" citStr="Ravichandran et al., 2003" startWordPosition="3012" endWordPosition="3015">, mj) only consider the morphological and format variations. Moreover, verb similarity measure Sim(v1, v2) is the same as head similarity measure Sim(hi, hj). 4 Candidate Answer Ranking According to path correlations of candidate answers, a Maximum Entropy (ME)-based model is applied to rank candidate answers. Unlike (Cui et al., 2004), who rank candidate answers with the sum of the path correlations, ME model may estimate the optimal weights of the paths based on a training data set. (Berger et al., 1996) gave a good description of ME model. The model we use is similar to (Shen et al., 2005; Ravichandran et al., 2003), which regard answer extraction as a ranking problem instead of a classification problem. We apply Generalized Iterative Scaling for model parameter estimation and Gaussian Prior for smoothing. If expected answer type is unknown during question processing or corresponding type of named entities isn’t recognized in candidate sentences, we regard all basic noun phrases as candidate answers. Since a MUC-based NER loses many types of named entities, we have to handle larger candidate answer sets. Orthographic features, similar to (Shen et al., 2005), are extracted to capture word format informati</context>
</contexts>
<marker>Ravichandran, Hovy, Och, 2003</marker>
<rawString>Deepak Ravichandran, Eduard Hovy, and Franz Josef Och. 2003. Statistical qa - classifier vs. re-ranker: What’s the difference? In Proceedings of ACL2003 workshop on Multilingual Summarization and Question Answering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Geert-Jan M Kruijff</author>
<author>Dietrich Klakow</author>
</authors>
<title>Exploring syntactic relation patterns for question answering.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP2005.</booktitle>
<contexts>
<context position="6080" citStr="Shen et al., 2005" startWordPosition="932" endWordPosition="935">luation, most top ranked QA systems use syntactic information in answer extraction. Next, we will briefly discuss the main usages. (Kaisser and Becker, 2004) match a question into one of predefined patterns, such as ”When did Jack Welch retire from GE?” to the pattern ”When+did+NP+Verb+NPorPP”. For each question pattern, there is a set of syntactic structures for potential answer. Candidate answers are ranked by matching the syntactic structures. This method worked well on TREC questions. However, it is costing to manually construct question patterns and syntactic structures of the patterns. (Shen et al., 2005) classify question words into four classes target word, head word, subject word and verb. For each class, syntactic relation patterns which contain one question word and one proper answer are automatically extracted and scored from training sentences. Then, candidate answers are ranked by partial matching to the syntactic relation patterns using tree kernel. However, the criterion to classify the question words is not clear in their paper. Proper answers may have absolutely different relations with different subject words in sentences. They don’t consider the corresponding relations in questio</context>
<context position="18337" citStr="Shen et al., 2005" startWordPosition="3008" endWordPosition="3011">r similarity Sim(mi, mj) only consider the morphological and format variations. Moreover, verb similarity measure Sim(v1, v2) is the same as head similarity measure Sim(hi, hj). 4 Candidate Answer Ranking According to path correlations of candidate answers, a Maximum Entropy (ME)-based model is applied to rank candidate answers. Unlike (Cui et al., 2004), who rank candidate answers with the sum of the path correlations, ME model may estimate the optimal weights of the paths based on a training data set. (Berger et al., 1996) gave a good description of ME model. The model we use is similar to (Shen et al., 2005; Ravichandran et al., 2003), which regard answer extraction as a ranking problem instead of a classification problem. We apply Generalized Iterative Scaling for model parameter estimation and Gaussian Prior for smoothing. If expected answer type is unknown during question processing or corresponding type of named entities isn’t recognized in candidate sentences, we regard all basic noun phrases as candidate answers. Since a MUC-based NER loses many types of named entities, we have to handle larger candidate answer sets. Orthographic features, similar to (Shen et al., 2005), are extracted to c</context>
<context position="23277" citStr="Shen et al., 2005" startWordPosition="3815" endWordPosition="3818">each question has at least one proper candidate sentence which contains proper answer in its candidate sentence set. There are 230 factoid questions (27 NIL questions) in TREC04. NIL questions are excluded from our test set because TREC doesn’t supply relevant documents and answer patterns for them. Therefore, we will evaluate 203 TREC04 questions. Five answer extraction methods are evaluated for comparison: • Density: Density-based method is used as baseline, in which we choose candidate answer with the shortest surface distance to question phrases. • SynPattern: Syntactic relation patterns (Shen et al., 2005) are automatically extracted from training set and are partially matched using tree kernel. • StrictMatch: Strict relation matching follows the assumption in (Tanev et al., 2004; Wu et al., 2005). We implement it by adapting relation correlation score. In stead of learning relation correlations during training, we predefine them as: Cor(r1, r2) = 1 if r1 = r2; 0, otherwise. • ApprMatch: Approximate relation matching (Cui et al., 2004) aligns two relation paths using fuzzy matching and ranks candidates according to the sum of all path similarities. • CorME: It is the method proposed in this pap</context>
</contexts>
<marker>Shen, Kruijff, Klakow, 2005</marker>
<rawString>Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow. 2005. Exploring syntactic relation patterns for question answering. In Proceedings of IJCNLP2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tanev</author>
<author>M Kouylekov</author>
<author>B Magnini</author>
</authors>
<title>Combining linguisitic processing and web mining for question answering: Itc-irst at trec-2004.</title>
<date>2004</date>
<booktitle>In Proceedings of TREC2004,</booktitle>
<pages>NIST.</pages>
<contexts>
<context position="6703" citStr="Tanev et al., 2004" startWordPosition="1028" endWordPosition="1031">sify question words into four classes target word, head word, subject word and verb. For each class, syntactic relation patterns which contain one question word and one proper answer are automatically extracted and scored from training sentences. Then, candidate answers are ranked by partial matching to the syntactic relation patterns using tree kernel. However, the criterion to classify the question words is not clear in their paper. Proper answers may have absolutely different relations with different subject words in sentences. They don’t consider the corresponding relations in questions. (Tanev et al., 2004; Wu et al., 2005) compare syntactic relations in questions and those in answer sentences. (Tanev et al., 2004) reconstruct a basic syntactic template tree for a question, in which one of the nodes denotes expected answer position. Then, answer candidates for this question are ranked by matching sentence syntactic tree to the question template tree. Furthermore, the matching is weighted by lexical variations. (Wu et al., 2005) combine n-gram proximity search and syntactic relation matching. For syntactic relation matching, question tree and sentence subtree around a candidate answer are matche</context>
<context position="23454" citStr="Tanev et al., 2004" startWordPosition="3843" endWordPosition="3846">. NIL questions are excluded from our test set because TREC doesn’t supply relevant documents and answer patterns for them. Therefore, we will evaluate 203 TREC04 questions. Five answer extraction methods are evaluated for comparison: • Density: Density-based method is used as baseline, in which we choose candidate answer with the shortest surface distance to question phrases. • SynPattern: Syntactic relation patterns (Shen et al., 2005) are automatically extracted from training set and are partially matched using tree kernel. • StrictMatch: Strict relation matching follows the assumption in (Tanev et al., 2004; Wu et al., 2005). We implement it by adapting relation correlation score. In stead of learning relation correlations during training, we predefine them as: Cor(r1, r2) = 1 if r1 = r2; 0, otherwise. • ApprMatch: Approximate relation matching (Cui et al., 2004) aligns two relation paths using fuzzy matching and ranks candidates according to the sum of all path similarities. • CorME: It is the method proposed in this paper. Different from ApprMatch, ME-based ranking model is implemented to incorporate path correlations which assigns different weights for different paths respectively. Furthermor</context>
</contexts>
<marker>Tanev, Kouylekov, Magnini, 2004</marker>
<rawString>H. Tanev, M. Kouylekov, and B. Magnini. 2004. Combining linguisitic processing and web mining for question answering: Itc-irst at trec-2004. In Proceedings of TREC2004, NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wu</author>
<author>M Y Duan</author>
<author>S Shaikh</author>
<author>S Small</author>
<author>T Strzalkowski</author>
</authors>
<title>University at albany’s ilqua in trec</title>
<date>2005</date>
<booktitle>In Proceedings of TREC2005,</booktitle>
<pages>NIST.</pages>
<contexts>
<context position="6721" citStr="Wu et al., 2005" startWordPosition="1032" endWordPosition="1035">into four classes target word, head word, subject word and verb. For each class, syntactic relation patterns which contain one question word and one proper answer are automatically extracted and scored from training sentences. Then, candidate answers are ranked by partial matching to the syntactic relation patterns using tree kernel. However, the criterion to classify the question words is not clear in their paper. Proper answers may have absolutely different relations with different subject words in sentences. They don’t consider the corresponding relations in questions. (Tanev et al., 2004; Wu et al., 2005) compare syntactic relations in questions and those in answer sentences. (Tanev et al., 2004) reconstruct a basic syntactic template tree for a question, in which one of the nodes denotes expected answer position. Then, answer candidates for this question are ranked by matching sentence syntactic tree to the question template tree. Furthermore, the matching is weighted by lexical variations. (Wu et al., 2005) combine n-gram proximity search and syntactic relation matching. For syntactic relation matching, question tree and sentence subtree around a candidate answer are matched from node to nod</context>
<context position="23472" citStr="Wu et al., 2005" startWordPosition="3847" endWordPosition="3850">excluded from our test set because TREC doesn’t supply relevant documents and answer patterns for them. Therefore, we will evaluate 203 TREC04 questions. Five answer extraction methods are evaluated for comparison: • Density: Density-based method is used as baseline, in which we choose candidate answer with the shortest surface distance to question phrases. • SynPattern: Syntactic relation patterns (Shen et al., 2005) are automatically extracted from training set and are partially matched using tree kernel. • StrictMatch: Strict relation matching follows the assumption in (Tanev et al., 2004; Wu et al., 2005). We implement it by adapting relation correlation score. In stead of learning relation correlations during training, we predefine them as: Cor(r1, r2) = 1 if r1 = r2; 0, otherwise. • ApprMatch: Approximate relation matching (Cui et al., 2004) aligns two relation paths using fuzzy matching and ranks candidates according to the sum of all path similarities. • CorME: It is the method proposed in this paper. Different from ApprMatch, ME-based ranking model is implemented to incorporate path correlations which assigns different weights for different paths respectively. Furthermore, phrase mapping </context>
</contexts>
<marker>Wu, Duan, Shaikh, Small, Strzalkowski, 2005</marker>
<rawString>M. Wu, M. Y. Duan, S. Shaikh, S. Small, and T. Strzalkowski. 2005. University at albany’s ilqua in trec 2005. In Proceedings of TREC2005, NIST.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>