<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.970752">
Forest-Based Translation
</title>
<author confidence="0.995594">
Haitao Mi† Liang Huang$ Qun Liu†
</author>
<affiliation confidence="0.980675666666667">
†Key Lab. of Intelligent Information Processing $Department of Computer &amp; Information Science
Institute of Computing Technology University of Pennsylvania
Chinese Academy of Sciences Levine Hall, 3330 Walnut Street
</affiliation>
<address confidence="0.755512">
P.O. Box 2704, Beijing 100190, China Philadelphia, PA 19104, USA
</address>
<email confidence="0.999344">
{htmi,liuqun}@ict.ac.cn lhuang3@cis.upenn.edu
</email>
<sectionHeader confidence="0.995645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999765941176471">
Among syntax-based translation models, the
tree-based approach, which takes as input a
parse tree of the source sentence, is a promis-
ing direction being faster and simpler than
its string-based counterpart. However, current
tree-based systems suffer from a major draw-
back: they only use the 1-best parse to direct
the translation, which potentially introduces
translation mistakes due to parsing errors. We
propose a forest-based approach that trans-
lates a packed forest of exponentially many
parses, which encodes many more alternatives
than standard n-best lists. Large-scale exper-
iments show an absolute improvement of 1.7
BLEU points over the 1-best baseline. This
result is also 0.8 points higher than decoding
with 30-best parses, and takes even less time.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99984452173913">
Syntax-based machine translation has witnessed
promising improvements in recent years. Depend-
ing on the type of input, these efforts can be di-
vided into two broad categories: the string-based
systems whose input is a string to be simultane-
ously parsed and translated by a synchronous gram-
mar (Wu, 1997; Chiang, 2005; Galley et al., 2006),
and the tree-based systems whose input is already a
parse tree to be directly converted into a target tree
or string (Lin, 2004; Ding and Palmer, 2005; Quirk
et al., 2005; Liu et al., 2006; Huang et al., 2006).
Compared with their string-based counterparts, tree-
based systems offer some attractive features: they
are much faster in decoding (linear time vs. cubic
time, see (Huang et al., 2006)), do not require a
binary-branching grammar as in string-based mod-
els (Zhang et al., 2006), and can have separate gram-
mars for parsing and translation, say, a context-free
grammar for the former and a tree substitution gram-
mar for the latter (Huang et al., 2006). However, de-
spite these advantages, current tree-based systems
suffer from a major drawback: they only use the 1-
best parse tree to direct the translation, which po-
tentially introduces translation mistakes due to pars-
ing errors (Quirk and Corston-Oliver, 2006). This
situation becomes worse with resource-poor source
languages without enough Treebank data to train a
high-accuracy parser.
One obvious solution to this problem is to take as
input k-best parses, instead of a single tree. This k-
best list postpones some disambiguation to the de-
coder, which may recover from parsing errors by
getting a better translation from a non 1-best parse.
However, a k-best list, with its limited scope, of-
ten has too few variations and too many redundan-
cies; for example, a 50-best list typically encodes
a combination of 5 or 6 binary ambiguities (since
25 &lt; 50 &lt; 26), and many subtrees are repeated
across different parses (Huang, 2008). It is thus inef-
ficient either to decode separately with each of these
very similar trees. Longer sentences will also aggra-
vate this situation as the number of parses grows ex-
ponentially with the sentence length.
We instead propose a new approach, forest-based
translation (Section 3), where the decoder trans-
lates a packed forest of exponentially many parses,1
</bodyText>
<footnote confidence="0.9703705">
1There has been some confusion in the MT literature regard-
ing the term forest: the word “forest” in “forest-to-string rules”
</footnote>
<page confidence="0.926445">
192
</page>
<note confidence="0.711493">
Proceedings of ACL-08: HLT, pages 192–199,
</note>
<page confidence="0.463649">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<figure confidence="0.7422115">
VP
→ held x2 with x1
</figure>
<figureCaption confidence="0.999758">
Figure 1: An example translation rule (r3 in Fig. 2).
</figureCaption>
<bodyText confidence="0.9999603">
which compactly encodes many more alternatives
than k-best parses. This scheme can be seen as
a compromise between the string-based and tree-
based methods, while combining the advantages of
both: decoding is still fast, yet does not commit to
a single parse. Large-scale experiments (Section 4)
show an improvement of 1.7 BLEU points over the
1-best baseline, which is also 0.8 points higher than
decoding with 30-best trees, and takes even less time
thanks to the sharing of common subtrees.
</bodyText>
<sectionHeader confidence="0.970115" genericHeader="method">
2 Tree-based systems
</sectionHeader>
<bodyText confidence="0.999959571428571">
Current tree-based systems perform translation in
two separate steps: parsing and decoding. A parser
first parses the source language input into a 1-best
tree T, and the decoder then searches for the best
derivation (a sequence of translation steps) d* that
converts source tree T into a target-language string
among all possible derivations D:
</bodyText>
<equation confidence="0.9151855">
d* = arg max P(d|T). (1)
dED
</equation>
<bodyText confidence="0.9996435">
We will now proceed with a running example
translating from Chinese to English:
</bodyText>
<figure confidence="0.980720108108108">
r4 ⇓ r5 ⇓
(e) Bush [held a talk]2 [with Sharon]1
yu
NR
jˇuxing
le
NN
Sh¯al´ong
r1 ⇓
huit´an
PP
VPB
NR
(a) B`ushi[yˇu Sh¯al´ong ]1 [juxing le huit´an ]2
⇓ 1-best parser
(b) IP
NPB
VP
with
(d) Bush held
Sh¯al´ong
huit´an
r2 ⇓ r3 ⇓
Sh¯al´ong
NPB
NN
huit´an
NPB
NR
B`ushi
P
NPB
NPB
VV
AS
NPB
NR
B`ushi
NN
le
NR
yu
juxing
VP
PP
VPB
P
NPB
VV
AS
NPB
PP
P x1:NPB
yu
jˇuxing
VV
VPB
AS
le
x2:NPB
(2) ihTt
B`ushiBush
Æ ��
yu Sh¯al´ong
with/and Sharon1
�f7
jˇuxing
hold
��
huit´an
talk2
T
le
pass.
</figure>
<figureCaption confidence="0.998393333333333">
Figure 2: An example derivation of tree-to-string trans-
lation. Shaded regions denote parts of the tree that is
pattern-matched with the rule being applied.
</figureCaption>
<bodyText confidence="0.995779666666667">
“Bush held a talk2 with Sharon1”
Figure 2 shows how this process works. The Chi-
nese sentence (a) is first parsed into tree (b), which
will be converted into an English string in 5 steps.
First, at the root node, we apply rule r1 preserving
top-level word-order between English and Chinese,
</bodyText>
<equation confidence="0.619708">
(r1) IP(x1:NPB x2:VP) → x1 x2
</equation>
<bodyText confidence="0.9498948">
(Liu et al., 2007) was a misnomer which actually refers to a set
of several unrelated subtrees over disjoint spans, and should not
be confused with the standard concept ofpackedforest.
which results in two unfinished subtrees in (c). Then
rule r2 grabs the B`ushisubtree and transliterate it
</bodyText>
<equation confidence="0.650583">
(r2) NPB(NR(B`ush´ı)) → Bush.
</equation>
<bodyText confidence="0.999886166666667">
Similarly, rule r3 shown in Figure 1 is applied to
the VP subtree, which swaps the two NPBs, yielding
the situation in (d). This rule is particularly interest-
ing since it has multiple levels on the source side,
which has more expressive power than synchronous
context-free grammars where rules are flat.
</bodyText>
<page confidence="0.995663">
193
</page>
<bodyText confidence="0.998181466666667">
More formally, a (tree-to-string) translation rule
(Huang et al., 2006) is a tuple (t, s, 0), where t is the
source-side tree, whose internal nodes are labeled by
nonterminal symbols in N, and whose frontier nodes
are labeled by source-side terminals in E or vari-
ables from a set X = {x1, x2, ...1; s E (X U A)* is
the target-side string where A is the target language
terminal set; and 0 is a mapping from X to nonter-
minals in N. Each variable xi E X occurs exactly
once in t and exactly once in s. We denote R to be
the translation rule set. A similar formalism appears
in another form in (Liu et al., 2006). These rules are
in the reverse direction of the original string-to-tree
transducer rules defined by Galley et al. (2004).
Finally, from step (d) we apply rules r4 and r5
</bodyText>
<equation confidence="0.537852">
(r4) NPB(NN(huit´an)) —* a talk
(r5) NPB(NR(Sh¯al´ong)) —* Sharon
</equation>
<bodyText confidence="0.825071666666667">
which perform phrasal translations for the two re-
maining subtrees, respectively, and get the Chinese
translation in (e).
</bodyText>
<sectionHeader confidence="0.996064" genericHeader="method">
3 Forest-based translation
</sectionHeader>
<bodyText confidence="0.9999979375">
We now extend the tree-based idea from the previ-
ous section to the case of forest-based translation.
Again, there are two steps, parsing and decoding.
In the former, a (modified) parser will parse the in-
put sentence and output a packed forest (Section 3.1)
rather than just the 1-best tree. Such a forest is usu-
ally huge in size, so we use the forest pruning algo-
rithm (Section 3.4) to reduce it to a reasonable size.
The pruned parse forest will then be used to direct
the translation.
In the decoding step, we first convert the parse for-
est into a translation forest using the translation rule
set, by similar techniques of pattern-matching from
tree-based decoding (Section 3.2). Then the decoder
searches for the best derivation on the translation
forest and outputs the target string (Section 3.3).
</bodyText>
<subsectionHeader confidence="0.997239">
3.1 Parse Forest
</subsectionHeader>
<bodyText confidence="0.999264538461538">
Informally, a packed parse forest, or forest in short,
is a compact representation of all the derivations
(i.e., parse trees) for a given sentence under a
context-free grammar (Billot and Lang, 1989). For
example, consider the Chinese sentence in Exam-
ple (2) above, which has (at least) two readings de-
pending on the part-of-speech of the word yˇu, which
can be either a preposition (P “with”) or a conjunc-
tion (CC “and”). The parse tree for the preposition
case is shown in Figure 2(b) as the 1-best parse,
while for the conjunction case, the two proper nouns
(B`ushiand Sh¯al´ong) are combined to form a coordi-
nated NP
</bodyText>
<equation confidence="0.998757">
NPB0,1 CC1,2 NPB2,3
NP0,3 (*)
</equation>
<bodyText confidence="0.995939">
which functions as the subject of the sentence. In
this case the Chinese sentence is translated into
</bodyText>
<listItem confidence="0.657258">
(3) “ [Bush and Sharon] held a talk”.
</listItem>
<bodyText confidence="0.979197565217391">
Shown in Figure 3(a), these two parse trees can
be represented as a single forest by sharing common
subtrees such as NPB0,1 and VPB3,6. Such a forest
has a structure of a hypergraph (Klein and Manning,
2001; Huang and Chiang, 2005), where items like
NP0,3 are called nodes, and deductive steps like (*)
correspond to hyperedges.
More formally, a forest is a pair (V, E), where V
is the set of nodes, and E the set of hyperedges. For
a given sentence w1:l = w1 ... wl, each node v E V
is in the form of Xi,j, which denotes the recogni-
tion of nonterminal X spanning the substring from
positions i through j (that is, wi+1 ... wj). Each hy-
peredge e E E is a pair (tails(e), head(e)), where
head(e) E V is the consequent node in the deduc-
tive step, and tails(e) E V * is the list of antecedent
nodes. For example, the hyperedge for deduction (*)
is notated:
((NPB0,1, CC1,2, NPB2,3), NP0,3).
There is also a distinguished root node TOP in
each forest, denoting the goal item in parsing, which
is simply S0,l where S is the start symbol and l is the
sentence length.
</bodyText>
<subsectionHeader confidence="0.994208">
3.2 Translation Forest
</subsectionHeader>
<bodyText confidence="0.9999488">
Given a parse forest and a translation rule set R, we
can generate a translation forest which has a simi-
lar hypergraph structure. Basically, just as the depth-
first traversal procedure in tree-based decoding (Fig-
ure 2), we visit in top-down order each node v in the
</bodyText>
<page confidence="0.992634">
194
</page>
<sectionHeader confidence="0.578346" genericHeader="method">
4 translation rule set R
</sectionHeader>
<bodyText confidence="0.580866">
translation hyperedge translation rule
</bodyText>
<equation confidence="0.997769833333333">
e1 r1 IP(x1:NPB x2:VP) → x1 x2
e2 rs IP(x1:NP x2:VPB) → x1 x2
e3 r3 VP(PP(P(yˇu) x1:NPB) VPB(VV(fiˇuxing) AS(le) x2:NPB)) → held x2 with x1
e4 r7 VP(PP(P(yˇu) x1:NPB) x2:VPB) → x2 with x1
e5 r8 NP(x1:NPB CC(yˇu) x2:NPB) → x1 and x2
es r9 VPB(VV(fiˇuxing) AS(le) x1:NPB) → held x1
</equation>
<figureCaption confidence="0.9985594">
Figure 3: (a) the parse forest of the example sentence; solid hyperedges denote the 1-best parse in Figure 2(b) while
dashed hyperedges denote the alternative parse due to Deduction (*). (b) the corresponding translation forest after
applying the translation rules (lexical rules not shown); the derivation shown in bold solid lines (e1 and e3) corresponds
to the derivation in Figure 2; the one shown in dashed lines (e2, e5, and es) uses the alternative parse and corresponds
to the translation in Example (3). (c) the correspondence between translation hyperedges and translation rules.
</figureCaption>
<figure confidence="0.882306848484849">
NP0,3
VP1,6
IP0,6
VPB3,6
PP1,3
NPB0,1
NR0,1
B`ushi
CC1,2
yˇu
P1,2 NPB2,3
NR2,3
Sh¯al´ong
VV3,4
fiˇux´ıng
NPB5,6
NN5,6
huit´an
AS4,5
le
IP0,6
e1
e2
NP0,3
VP1,6
e5
e4 e3
VPB3,6
PP1,3
NPB0,1 CC1,2
P1,2 NPB2,3
VV3,4 AS4,5 NPB5,6
e6
</figure>
<bodyText confidence="0.999386833333333">
parse forest, and try to pattern-match each transla-
tion rule r against the local sub-forest under node v.
For example, in Figure 3(a), at node VP1,6, two rules
r3 and r7 both matches the local subforest, and will
thus generate two translation hyperedges e3 and e4
(see Figure 3(b-c)).
More formally, we define a function match(r, v)
which attempts to pattern-match rule r at node v in
the parse forest, and in case of success, returns a
list of descendent nodes of v that are matched to the
variables in r, or returns an empty list if the match
fails. Note that this procedure is recursive and may
</bodyText>
<page confidence="0.994906">
195
</page>
<bodyText confidence="0.533593">
Pseudocode 1 The conversion algorithm.
</bodyText>
<listItem confidence="0.99685075">
1: Input: parse forest Hp and rule set R
2: Output: translation forest Ht
3: for each node v E Vp in top-down order do
4: for each translation rule r E R do
5: vars +— match(r, v) ⊲ variables
6: if vars is not empty then
7: e +— (vars, v, s(r))
8: add translation hyperedge e to Ht
</listItem>
<bodyText confidence="0.792152">
involve multiple parse hyperedges. For example,
</bodyText>
<equation confidence="0.79371">
match(r3, VP1,6) = (NPB2,3, NPB5,6),
</equation>
<bodyText confidence="0.951341058823529">
which covers three parse hyperedges, while nodes
in gray do not pattern-match any rule (although they
are involved in the matching of other nodes, where
they match interior nodes of the source-side tree
fragments in a rule). We can thus construct a transla-
tion hyperedge from match(r, v) to v for each node
v and rule r. In addition, we also need to keep track
of the target string s(r) specified by rule r, which in-
cludes target-language terminals and variables. For
example, s(r3) = “held x2 with x1”. The subtrans-
lations of the matched variable nodes will be sub-
stituted for the variables in s(r) to get a complete
translation for node v. So a translation hyperedge e
is a triple (tails(e), head(e), s) where s is the target
string from the rule, for example,
e3 = ((NPB2,3, NPB5,6), VP1,6, “held x2 with x1”).
This procedure is summarized in Pseudocode 1.
</bodyText>
<subsectionHeader confidence="0.998129">
3.3 Decoding Algorithms
</subsectionHeader>
<bodyText confidence="0.9998768125">
The decoder performs two tasks on the translation
forest: 1-best search with integrated language model
(LM), and k-best search with LM to be used in min-
imum error rate training. Both tasks can be done ef-
ficiently by forest-based algorithms based on k-best
parsing (Huang and Chiang, 2005).
For 1-best search, we use the cube pruning tech-
nique (Chiang, 2007; Huang and Chiang, 2007)
which approximately intersects the translation forest
with the LM. Basically, cube pruning works bottom
up in a forest, keeping at most k +LM items at each
node, and uses the best-first expansion idea from the
Algorithm 2 of Huang and Chiang (2005) to speed
up the computation. An +LM item of node v has the
form (va*b), where a and b are the target-language
boundary words. For example, (VP held * Sharon) is an
</bodyText>
<page confidence="0.658158">
1,6
</page>
<bodyText confidence="0.99984">
+LM item with its translation starting with “held”
and ending with “Sharon”. This scheme can be eas-
ily extended to work with a general n-gram by stor-
ing n − 1 words at both ends (Chiang, 2007).
For k-best search after getting 1-best derivation,
we use the lazy Algorithm 3 of Huang and Chiang
(2005) that works backwards from the root node,
incrementally computing the second, third, through
the kth best alternatives. However, this time we work
on a finer-grained forest, called translation+LMfor-
est, resulting from the intersection of the translation
forest and the LM, with its nodes being the +LM
items during cube pruning. Although this new forest
is prohibitively large, Algorithm 3 is very efficient
with minimal overhead on top of 1-best.
</bodyText>
<subsectionHeader confidence="0.995669">
3.4 Forest Pruning Algorithm
</subsectionHeader>
<bodyText confidence="0.99981775">
We use the pruning algorithm of (Jonathan Graehl,
p.c.; Huang, 2008) that is very similar to the method
based on marginal probability (Charniak and John-
son, 2005), except that it prunes hyperedges as well
as nodes. Basically, we use an Inside-Outside algo-
rithm to compute the Viterbi inside cost Q(v) and the
Viterbi outside cost α(v) for each node v, and then
compute the merit αQ(e) for each hyperedge:
</bodyText>
<equation confidence="0.9931525">
αQ(e) = α(head(e)) + � Q(ui) (4)
uiEtails(e)
</equation>
<bodyText confidence="0.959403833333333">
Intuitively, this merit is the cost of the best derivation
that traverses e, and the difference S(e) = αQ(e) −
Q(TOP) can be seen as the distance away from the
globally best derivation. We prune away a hyper-
edge e if S(e) &gt; p for a threshold p. Nodes with
all incoming hyperedges pruned are also pruned.
</bodyText>
<sectionHeader confidence="0.999703" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.981186">
We can extend the simple model in Equation 1 to a
log-linear one (Liu et al., 2006; Huang et al., 2006):
</bodyText>
<equation confidence="0.990524">
d* = arg max
dED P(d  |T)λ0 · eλ1|d |· Plm(s)λ2 · eλ3|3|
</equation>
<bodyText confidence="0.981617">
(5)
where T is the 1-best parse, eλ1|d |is the penalty term
on the number of rules in a derivation, Plm(s) is the
language model and eλ3|3 |is the length penalty term
</bodyText>
<page confidence="0.997189">
196
</page>
<bodyText confidence="0.9066975">
BLEU score
on target translation. The derivation probability con-
ditioned on 1-best tree, P(d  |T), should now be
replaced by P(d  |Hp) where Hp is the parse forest,
which decomposes into the product of probabilities
of translation rules r E d:
</bodyText>
<equation confidence="0.993703">
P(d  |Hp) = � P(r) (6)
rEd
where each P(r) is the product of five probabilities:
P(r) = P(t  |s)A4 · Plex(t  |s)A5·
P(s  |t)A6 · Plex(s  |t)A&apos; · P(t  |Hp)
</equation>
<bodyText confidence="0.9999656">
Here t and s are the source-side tree and target-
side string of rule r, respectively, P(t  |s) and
P(s  |t) are the two translation probabilities, and
Plex(·) are the lexical probabilities. The only extra
term in forest-based decoding is P(t  |Hp) denot-
ing the source side parsing probability of the current
translation rule r in the parse forest, which is the
product of probabilities of each parse hyperedge ep
covered in the pattern-match of t against Hp (which
can be recorded at conversion time):
</bodyText>
<equation confidence="0.867796">
P(t  |Hp) = � P(ep). (8)
epEHp, ep covered by t
</equation>
<subsectionHeader confidence="0.992808">
4.1 Data preparation
</subsectionHeader>
<bodyText confidence="0.999979421052632">
Our experiments are on Chinese-to-English transla-
tion, and we use the Chinese parser of Xiong et al.
(2005) to parse the source side of the bitext. Follow-
ing Huang (2008), we modify the parser to output a
packed forest for each sentence.
Our training corpus consists of 31,011 sentence
pairs with 0.8M Chinese words and 0.9M English
words. We first word-align them by GIZA++ refined
by “diagand” from Koehn et al. (2003), and apply
the tree-to-string rule extraction algorithm (Galley et
al., 2006; Liu et al., 2006), which resulted in 346K
translation rules. Note that our rule extraction is still
done on 1-best parses, while decoding is on k-best
parses or packed forests. We also use the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train a
trigram language model with Kneser-Ney smooth-
ing on the English side of the bitext.
We use the 2002 NIST MT Evaluation test set as
our development set (878 sentences) and the 2005
</bodyText>
<figure confidence="0.5587665">
0 5 10 15 20 25 30 35
average decoding time (secs/sentence)
</figure>
<figureCaption confidence="0.9987755">
Figure 4: Comparison of decoding on forests with decod-
ing on k-best trees.
</figureCaption>
<bodyText confidence="0.9981138125">
NIST MT Evaluation test set as our test set (1082
sentences), with on average 28.28 and 26.31 words
per sentence, respectively. We evaluate the transla-
tion quality using the case-sensitive BLEU-4 met-
ric (Papineni et al., 2002). We use the standard min-
imum error-rate training (Och, 2003) to tune the fea-
ture weights to maximize the system’s BLEU score
on the dev set. On dev and test sets, we prune the
Chinese parse forests by the forest pruning algo-
rithm in Section 3.4 with a threshold of p = 12, and
then convert them into translation forests using the
algorithm in Section 3.2. To increase the coverage
of the rule set, we also introduce a default transla-
tion hyperedge for each parse hyperedge by mono-
tonically translating each tail node, so that we can
always at least get a complete translation in the end.
</bodyText>
<subsectionHeader confidence="0.689609">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999933769230769">
The BLEU score of the baseline 1-best decoding is
0.2325, which is consistent with the result of 0.2302
in (Liu et al., 2007) on the same training, develop-
ment and test sets, and with the same rule extrac-
tion procedure. The corresponding BLEU score of
Pharaoh (Koehn, 2004) is 0.2182 on this dataset.
Figure 4 compares forest decoding with decoding
on k-best trees in terms of speed and quality. Us-
ing more than one parse tree apparently improves the
BLEU score, but at the cost of much slower decod-
ing, since each of the top-k trees has to be decoded
individually although they share many common sub-
trees. Forest decoding, by contrast, is much faster
</bodyText>
<figure confidence="0.989261615384615">
0.250
0.248
0.246
0.244
0.242
0.240
0.238
0.236
0.234
0.232
0.230
1-best
p=5
k=10
p=12
k-best trees
forests decoding
k=30
k=100
A8
.
(7)
197
Percentage of sentences (%)
0 10 20 30 40 50 60 70 80 90 100
i (rank of the parse tree picked by the decoder)
</figure>
<figureCaption confidence="0.962252333333333">
Figure 5: Percentage of the i-th best parse tree being
picked in decoding. 32% of the distribution for forest de-
coding is beyond top-100 and is not shown on this plot.
</figureCaption>
<bodyText confidence="0.996851260869565">
and produces consistently better BLEU scores. With
pruning threshold p = 12, it achieved a BLEU
score of 0.2485, which is an absolute improvement
of 1.6% points over the 1-best baseline, and is statis-
tically significant using the sign-test of Collins et al.
(2005) (p &lt; 0.01).
We also investigate the question of how often the
ith-best parse tree is picked to direct the translation
(i = 1, 2, ...), in both k-best and forest decoding
schemes. A packed forest can be roughly viewed as
a (virtual) ∞-best list, and we can thus ask how of-
ten is a parse beyond top-k used by a forest, which
relates to the fundamental limitation of k-best lists.
Figure 5 shows that, the 1-best parse is still preferred
25% of the time among 30-best trees, and 23% of
the time by the forest decoder. These ratios decrease
dramatically as i increases, but the forest curve has a
much longer tail in large i. Indeed, 40% of the trees
preferred by a forest is beyond top-30, 32% is be-
yond top-100, and even 20% beyond top-1000. This
confirms the fact that we need exponentially large k-
best lists with the explosion of alternatives, whereas
a forest can encode these information compactly.
</bodyText>
<subsectionHeader confidence="0.999366">
4.3 Scaling to large data
</subsectionHeader>
<bodyText confidence="0.998848">
We also conduct experiments on a larger dataset,
which contains 2.2M training sentence pairs. Be-
sides the trigram language model trained on the En-
glish side of these bitext, we also use another tri-
gram model trained on the first 1/3 of the Xinhua
portion of Gigaword corpus. The two LMs have dis-
</bodyText>
<table confidence="0.9961975">
approach \ ruleset TR TR+BP
1-best tree 0.2666 0.2939
30-best trees 0.2755 0.3084
forest (p = 12) 0.2839 0.3149
</table>
<tableCaption confidence="0.999932">
Table 1: BLEU score results from training on large data.
</tableCaption>
<bodyText confidence="0.99983824">
tinct weights tuned by minimum error rate training.
The dev and test sets remain the same as above.
Furthermore, we also make use of bilingual
phrases to improve the coverage of the ruleset. Fol-
lowing Liu et al. (2006), we prepare a phrase-table
from a phrase-extractor, e.g. Pharaoh, and at decod-
ing time, for each node, we construct on-the-fly flat
translation rules from phrases that match the source-
side span of the node. These phrases are called syn-
tactic phrases which are consistent with syntactic
constituents (Chiang, 2005), and have been shown to
be helpful in tree-based systems (Galley et al., 2006;
Liu et al., 2006).
The final results are shown in Table 1, where TR
denotes translation rule only, and TR+BP denotes
the inclusion of bilingual phrases. The BLEU score
of forest decoder with TR is 0.2839, which is a 1.7%
points improvement over the 1-best baseline, and
this difference is statistically significant (p &lt; 0.01).
Using bilingual phrases further improves the BLEU
score by 3.1% points, which is 2.1% points higher
than the respective 1-best baseline. We suspect this
larger improvement is due to the alternative con-
stituents in the forest, which activates many syntac-
tic phrases suppressed by the 1-best parse.
</bodyText>
<sectionHeader confidence="0.974185" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999972230769231">
We have presented a novel forest-based translation
approach which uses a packed forest rather than the
1-best parse tree (or k-best parse trees) to direct the
translation. Forest provides a compact data-structure
for efficient handling of exponentially many tree
structures, and is shown to be a promising direc-
tion with state-of-the-art translation results and rea-
sonable decoding speed. This work can thus be
viewed as a compromise between string-based and
tree-based paradigms, with a good trade-off between
speed and accuarcy. For future work, we would like
to use packed forests not only in decoding, but also
for translation rule extraction during training.
</bodyText>
<figure confidence="0.98157275">
25
20
15
10
5
0
forest decoding
30-best trees
</figure>
<page confidence="0.984475">
198
</page>
<sectionHeader confidence="0.979801" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9998271">
Part of this work was done while L. H. was visit-
ing CAS/ICT. The authors were supported by Na-
tional Natural Science Foundation of China, Con-
tracts 60736014 and 60573188, and 863 State Key
Project No. 2006AA010108 (H. M and Q. L.), and
by NSF ITR EIA-0205456 (L. H.). We would also
like to thank Chris Quirk for inspirations, Yang
Liu for help with rule extraction, Mark Johnson for
posing the question of virtual ∞-best list, and the
anonymous reviewers for suggestions.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999879630434783">
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Proceedings
ofACL ’89, pages 143–151.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine-grained n-best parsing and discriminative rerank-
ing. In Proceedings of the 43rd ACL.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263–270, Ann Arbor, Michigan, June.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Comput. Linguist., 33(2):201–228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL, pages 531–540,
Ann Arbor, Michigan, June.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings ofACL, pages 541–
548, Ann Arbor, Michigan, June.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In HLT-
NAACL, pages 273–280, Boston, MA.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL, pages 961–968, Sydney, Aus-
tralia, July.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of Ninth International Work-
shop on Parsing Technologies (IWPT-2005), Vancou-
ver, Canada.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL, pages 144–151, Prague, Czech
Republic, June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings ofAMTA, Boston,
MA, August.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL, Columbus, OH.
Dan Klein and Christopher D. Manning. 2001. Parsing
and Hypergraphs. In Proceedings of the Seventh In-
ternational Workshop on Parsing Technologies (IWPT-
2001), 17-19 October 2001, Beijing, China.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings ofHLT-NAACL, Edmonton, AB, Canada.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings ofAMTA, pages 115–124.
Dekang Lin. 2004. A path-based transfer model for ma-
chine translation. In Proceedings ofthe 20th COLING,
Barcelona, Spain.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING-ACL, pages 609–
616, Sydney, Australia, July.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of ACL, pages 704–711, Prague, Czech Re-
public, June.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311–318, Philadephia, USA, July.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed statis-
tical machine translation. In Proceedings of EMNLP.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically informed
phrasal SMT. In Proceedings ofACL, pages 271–279,
Ann Arbor, Michigan, June.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings ofICSLP, vol-
ume 30, pages 901–904.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin.
2005. Parsing the Penn Chinese Treebank with seman-
tic knowledge. In Proceedings ofIJCNLP 2005, pages
70–81, Jeju Island, South Korea.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of HLT-NAACL,
New York, NY.
</reference>
<page confidence="0.998911">
199
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.913535">
<title confidence="0.980779">Forest-Based Translation</title>
<affiliation confidence="0.979998">Lab. of Intelligent Information Processing of Computer &amp; Information Science Institute of Computing Technology University of Pennsylvania</affiliation>
<address confidence="0.9988295">Chinese Academy of Sciences Levine Hall, 3330 Walnut Street P.O. Box 2704, Beijing 100190, China Philadelphia, PA 19104, USA</address>
<email confidence="0.998615">lhuang3@cis.upenn.edu</email>
<abstract confidence="0.998363611111111">Among syntax-based translation models, the which takes as input a parse tree of the source sentence, is a promising direction being faster and simpler than its string-based counterpart. However, current tree-based systems suffer from a major drawback: they only use the 1-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors. We a that translates a packed forest of exponentially many parses, which encodes many more alternatives standard lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sylvie Billot</author>
<author>Bernard Lang</author>
</authors>
<title>The structure of shared forests in ambiguous parsing.</title>
<date>1989</date>
<booktitle>In Proceedings ofACL ’89,</booktitle>
<pages>143--151</pages>
<contexts>
<context position="8361" citStr="Billot and Lang, 1989" startWordPosition="1388" endWordPosition="1391"> a reasonable size. The pruned parse forest will then be used to direct the translation. In the decoding step, we first convert the parse forest into a translation forest using the translation rule set, by similar techniques of pattern-matching from tree-based decoding (Section 3.2). Then the decoder searches for the best derivation on the translation forest and outputs the target string (Section 3.3). 3.1 Parse Forest Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Billot and Lang, 1989). For example, consider the Chinese sentence in Example (2) above, which has (at least) two readings depending on the part-of-speech of the word yˇu, which can be either a preposition (P “with”) or a conjunction (CC “and”). The parse tree for the preposition case is shown in Figure 2(b) as the 1-best parse, while for the conjunction case, the two proper nouns (B`ushiand Sh¯al´ong) are combined to form a coordinated NP NPB0,1 CC1,2 NPB2,3 NP0,3 (*) which functions as the subject of the sentence. In this case the Chinese sentence is translated into (3) “ [Bush and Sharon] held a talk”. Shown in </context>
</contexts>
<marker>Billot, Lang, 1989</marker>
<rawString>Sylvie Billot and Bernard Lang. 1989. The structure of shared forests in ambiguous parsing. In Proceedings ofACL ’89, pages 143–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine-grained n-best parsing and discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context position="15097" citStr="Charniak and Johnson, 2005" startWordPosition="2570" endWordPosition="2574">backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LMforest, resulting from the intersection of the translation forest and the LM, with its nodes being the +LM items during cube pruning. Although this new forest is prohibitively large, Algorithm 3 is very efficient with minimal overhead on top of 1-best. 3.4 Forest Pruning Algorithm We use the pruning algorithm of (Jonathan Graehl, p.c.; Huang, 2008) that is very similar to the method based on marginal probability (Charniak and Johnson, 2005), except that it prunes hyperedges as well as nodes. Basically, we use an Inside-Outside algorithm to compute the Viterbi inside cost Q(v) and the Viterbi outside cost α(v) for each node v, and then compute the merit αQ(e) for each hyperedge: αQ(e) = α(head(e)) + � Q(ui) (4) uiEtails(e) Intuitively, this merit is the cost of the best derivation that traverses e, and the difference S(e) = αQ(e) − Q(TOP) can be seen as the distance away from the globally best derivation. We prune away a hyperedge e if S(e) &gt; p for a threshold p. Nodes with all incoming hyperedges pruned are also pruned. 4 Experi</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine-grained n-best parsing and discriminative reranking. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1486" citStr="Chiang, 2005" startWordPosition="216" endWordPosition="217">tially many parses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar f</context>
<context position="9183" citStr="Chiang, 2005" startWordPosition="1536" endWordPosition="1537">on (CC “and”). The parse tree for the preposition case is shown in Figure 2(b) as the 1-best parse, while for the conjunction case, the two proper nouns (B`ushiand Sh¯al´ong) are combined to form a coordinated NP NPB0,1 CC1,2 NPB2,3 NP0,3 (*) which functions as the subject of the sentence. In this case the Chinese sentence is translated into (3) “ [Bush and Sharon] held a talk”. Shown in Figure 3(a), these two parse trees can be represented as a single forest by sharing common subtrees such as NPB0,1 and VPB3,6. Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like NP0,3 are called nodes, and deductive steps like (*) correspond to hyperedges. More formally, a forest is a pair (V, E), where V is the set of nodes, and E the set of hyperedges. For a given sentence w1:l = w1 ... wl, each node v E V is in the form of Xi,j, which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, wi+1 ... wj). Each hyperedge e E E is a pair (tails(e), head(e)), where head(e) E V is the consequent node in the deductive step, and tails(e) E V * is the list of antecedent nodes. For example, the hyperedge for ded</context>
<context position="13648" citStr="Chiang, 2005" startWordPosition="2327" endWordPosition="2328">be substituted for the variables in s(r) to get a complete translation for node v. So a translation hyperedge e is a triple (tails(e), head(e), s) where s is the target string from the rule, for example, e3 = ((NPB2,3, NPB5,6), VP1,6, “held x2 with x1”). This procedure is summarized in Pseudocode 1. 3.3 Decoding Algorithms The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and k-best search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing (Huang and Chiang, 2005). For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Huang and Chiang (2005) to speed up the computation. An +LM item of node v has the form (va*b), where a and b are the target-language boundary words. For example, (VP held * Sharon) is an 1,6 +LM item with its translation starting with “held” and ending with “Sharon”. This scheme </context>
<context position="22142" citStr="Chiang, 2005" startWordPosition="3828" endWordPosition="3829"> (p = 12) 0.2839 0.3149 Table 1: BLEU score results from training on large data. tinct weights tuned by minimum error rate training. The dev and test sets remain the same as above. Furthermore, we also make use of bilingual phrases to improve the coverage of the ruleset. Following Liu et al. (2006), we prepare a phrase-table from a phrase-extractor, e.g. Pharaoh, and at decoding time, for each node, we construct on-the-fly flat translation rules from phrases that match the sourceside span of the node. These phrases are called syntactic phrases which are consistent with syntactic constituents (Chiang, 2005), and have been shown to be helpful in tree-based systems (Galley et al., 2006; Liu et al., 2006). The final results are shown in Table 1, where TR denotes translation rule only, and TR+BP denotes the inclusion of bilingual phrases. The BLEU score of forest decoder with TR is 0.2839, which is a 1.7% points improvement over the 1-best baseline, and this difference is statistically significant (p &lt; 0.01). Using bilingual phrases further improves the BLEU score by 3.1% points, which is 2.1% points higher than the respective 1-best baseline. We suspect this larger improvement is due to the alterna</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of ACL, pages 263–270, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="13716" citStr="Chiang, 2007" startWordPosition="2339" endWordPosition="2340">n for node v. So a translation hyperedge e is a triple (tails(e), head(e), s) where s is the target string from the rule, for example, e3 = ((NPB2,3, NPB5,6), VP1,6, “held x2 with x1”). This procedure is summarized in Pseudocode 1. 3.3 Decoding Algorithms The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and k-best search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing (Huang and Chiang, 2005). For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Huang and Chiang (2005) to speed up the computation. An +LM item of node v has the form (va*b), where a and b are the target-language boundary words. For example, (VP held * Sharon) is an 1,6 +LM item with its translation starting with “held” and ending with “Sharon”. This scheme can be easily extended to work with a general n-gram by storing n − </context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>531--540</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="20218" citStr="Collins et al. (2005)" startWordPosition="3488" endWordPosition="3491">230 1-best p=5 k=10 p=12 k-best trees forests decoding k=30 k=100 A8 . (7) 197 Percentage of sentences (%) 0 10 20 30 40 50 60 70 80 90 100 i (rank of the parse tree picked by the decoder) Figure 5: Percentage of the i-th best parse tree being picked in decoding. 32% of the distribution for forest decoding is beyond top-100 and is not shown on this plot. and produces consistently better BLEU scores. With pruning threshold p = 12, it achieved a BLEU score of 0.2485, which is an absolute improvement of 1.6% points over the 1-best baseline, and is statistically significant using the sign-test of Collins et al. (2005) (p &lt; 0.01). We also investigate the question of how often the ith-best parse tree is picked to direct the translation (i = 1, 2, ...), in both k-best and forest decoding schemes. A packed forest can be roughly viewed as a (virtual) ∞-best list, and we can thus ask how often is a parse beyond top-k used by a forest, which relates to the fundamental limitation of k-best lists. Figure 5 shows that, the 1-best parse is still preferred 25% of the time among 30-best trees, and 23% of the time by the forest decoder. These ratios decrease dramatically as i increases, but the forest curve has a much l</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of ACL, pages 531–540, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>541--548</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1660" citStr="Ding and Palmer, 2005" startWordPosition="245" endWordPosition="248"> 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006). However, despite these advantages, current tree-based systems suffer from a major drawback:</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proceedings ofACL, pages 541– 548, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule? In</title>
<date>2004</date>
<booktitle>HLTNAACL,</booktitle>
<pages>273--280</pages>
<location>Boston, MA.</location>
<contexts>
<context position="7077" citStr="Galley et al. (2004)" startWordPosition="1176" endWordPosition="1179">urce-side tree, whose internal nodes are labeled by nonterminal symbols in N, and whose frontier nodes are labeled by source-side terminals in E or variables from a set X = {x1, x2, ...1; s E (X U A)* is the target-side string where A is the target language terminal set; and 0 is a mapping from X to nonterminals in N. Each variable xi E X occurs exactly once in t and exactly once in s. We denote R to be the translation rule set. A similar formalism appears in another form in (Liu et al., 2006). These rules are in the reverse direction of the original string-to-tree transducer rules defined by Galley et al. (2004). Finally, from step (d) we apply rules r4 and r5 (r4) NPB(NN(huit´an)) —* a talk (r5) NPB(NR(Sh¯al´ong)) —* Sharon which perform phrasal translations for the two remaining subtrees, respectively, and get the Chinese translation in (e). 3 Forest-based translation We now extend the tree-based idea from the previous section to the case of forest-based translation. Again, there are two steps, parsing and decoding. In the former, a (modified) parser will parse the input sentence and output a packed forest (Section 3.1) rather than just the 1-best tree. Such a forest is usually huge in size, so we </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In HLTNAACL, pages 273–280, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1508" citStr="Galley et al., 2006" startWordPosition="218" endWordPosition="221">rses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tr</context>
<context position="17489" citStr="Galley et al., 2006" startWordPosition="3001" endWordPosition="3004">gainst Hp (which can be recorded at conversion time): P(t |Hp) = � P(ep). (8) epEHp, ep covered by t 4.1 Data preparation Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Following Huang (2008), we modify the parser to output a packed forest for each sentence. Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words. We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules. Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests. We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram language model with Kneser-Ney smoothing on the English side of the bitext. We use the 2002 NIST MT Evaluation test set as our development set (878 sentences) and the 2005 0 5 10 15 20 25 30 35 average decoding time (secs/sentence) Figure 4: Comparison of decoding on forests with decoding on k-best trees. NIST MT Evaluation test set as ou</context>
<context position="22220" citStr="Galley et al., 2006" startWordPosition="3840" endWordPosition="3843">ge data. tinct weights tuned by minimum error rate training. The dev and test sets remain the same as above. Furthermore, we also make use of bilingual phrases to improve the coverage of the ruleset. Following Liu et al. (2006), we prepare a phrase-table from a phrase-extractor, e.g. Pharaoh, and at decoding time, for each node, we construct on-the-fly flat translation rules from phrases that match the sourceside span of the node. These phrases are called syntactic phrases which are consistent with syntactic constituents (Chiang, 2005), and have been shown to be helpful in tree-based systems (Galley et al., 2006; Liu et al., 2006). The final results are shown in Table 1, where TR denotes translation rule only, and TR+BP denotes the inclusion of bilingual phrases. The BLEU score of forest decoder with TR is 0.2839, which is a 1.7% points improvement over the 1-best baseline, and this difference is statistically significant (p &lt; 0.01). Using bilingual phrases further improves the BLEU score by 3.1% points, which is 2.1% points higher than the respective 1-best baseline. We suspect this larger improvement is due to the alternative constituents in the forest, which activates many syntactic phrases suppre</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of COLING-ACL, pages 961–968, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of Ninth International Workshop on Parsing Technologies (IWPT-2005),</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="9183" citStr="Huang and Chiang, 2005" startWordPosition="1534" endWordPosition="1537"> conjunction (CC “and”). The parse tree for the preposition case is shown in Figure 2(b) as the 1-best parse, while for the conjunction case, the two proper nouns (B`ushiand Sh¯al´ong) are combined to form a coordinated NP NPB0,1 CC1,2 NPB2,3 NP0,3 (*) which functions as the subject of the sentence. In this case the Chinese sentence is translated into (3) “ [Bush and Sharon] held a talk”. Shown in Figure 3(a), these two parse trees can be represented as a single forest by sharing common subtrees such as NPB0,1 and VPB3,6. Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like NP0,3 are called nodes, and deductive steps like (*) correspond to hyperedges. More formally, a forest is a pair (V, E), where V is the set of nodes, and E the set of hyperedges. For a given sentence w1:l = w1 ... wl, each node v E V is in the form of Xi,j, which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, wi+1 ... wj). Each hyperedge e E E is a pair (tails(e), head(e)), where head(e) E V is the consequent node in the deductive step, and tails(e) E V * is the list of antecedent nodes. For example, the hyperedge for ded</context>
<context position="13648" citStr="Huang and Chiang, 2005" startWordPosition="2325" endWordPosition="2328">odes will be substituted for the variables in s(r) to get a complete translation for node v. So a translation hyperedge e is a triple (tails(e), head(e), s) where s is the target string from the rule, for example, e3 = ((NPB2,3, NPB5,6), VP1,6, “held x2 with x1”). This procedure is summarized in Pseudocode 1. 3.3 Decoding Algorithms The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and k-best search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing (Huang and Chiang, 2005). For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Huang and Chiang (2005) to speed up the computation. An +LM item of node v has the form (va*b), where a and b are the target-language boundary words. For example, (VP held * Sharon) is an 1,6 +LM item with its translation starting with “held” and ending with “Sharon”. This scheme </context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of Ninth International Workshop on Parsing Technologies (IWPT-2005), Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>144--151</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="13741" citStr="Huang and Chiang, 2007" startWordPosition="2341" endWordPosition="2344">So a translation hyperedge e is a triple (tails(e), head(e), s) where s is the target string from the rule, for example, e3 = ((NPB2,3, NPB5,6), VP1,6, “held x2 with x1”). This procedure is summarized in Pseudocode 1. 3.3 Decoding Algorithms The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and k-best search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing (Huang and Chiang, 2005). For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Huang and Chiang (2005) to speed up the computation. An +LM item of node v has the form (va*b), where a and b are the target-language boundary words. For example, (VP held * Sharon) is an 1,6 +LM item with its translation starting with “held” and ending with “Sharon”. This scheme can be easily extended to work with a general n-gram by storing n − 1 words at both ends (Chi</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proceedings of ACL, pages 144–151, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proceedings ofAMTA,</booktitle>
<location>Boston, MA,</location>
<contexts>
<context position="1719" citStr="Huang et al., 2006" startWordPosition="257" endWordPosition="260">decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006). However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1- best parse tree to direct the transla</context>
<context position="6417" citStr="Huang et al., 2006" startWordPosition="1048" endWordPosition="1051">ted subtrees over disjoint spans, and should not be confused with the standard concept ofpackedforest. which results in two unfinished subtrees in (c). Then rule r2 grabs the B`ushisubtree and transliterate it (r2) NPB(NR(B`ush´ı)) → Bush. Similarly, rule r3 shown in Figure 1 is applied to the VP subtree, which swaps the two NPBs, yielding the situation in (d). This rule is particularly interesting since it has multiple levels on the source side, which has more expressive power than synchronous context-free grammars where rules are flat. 193 More formally, a (tree-to-string) translation rule (Huang et al., 2006) is a tuple (t, s, 0), where t is the source-side tree, whose internal nodes are labeled by nonterminal symbols in N, and whose frontier nodes are labeled by source-side terminals in E or variables from a set X = {x1, x2, ...1; s E (X U A)* is the target-side string where A is the target language terminal set; and 0 is a mapping from X to nonterminals in N. Each variable xi E X occurs exactly once in t and exactly once in s. We denote R to be the translation rule set. A similar formalism appears in another form in (Liu et al., 2006). These rules are in the reverse direction of the original str</context>
<context position="15806" citStr="Huang et al., 2006" startWordPosition="2701" endWordPosition="2704">lgorithm to compute the Viterbi inside cost Q(v) and the Viterbi outside cost α(v) for each node v, and then compute the merit αQ(e) for each hyperedge: αQ(e) = α(head(e)) + � Q(ui) (4) uiEtails(e) Intuitively, this merit is the cost of the best derivation that traverses e, and the difference S(e) = αQ(e) − Q(TOP) can be seen as the distance away from the globally best derivation. We prune away a hyperedge e if S(e) &gt; p for a threshold p. Nodes with all incoming hyperedges pruned are also pruned. 4 Experiments We can extend the simple model in Equation 1 to a log-linear one (Liu et al., 2006; Huang et al., 2006): d* = arg max dED P(d |T)λ0 · eλ1|d |· Plm(s)λ2 · eλ3|3| (5) where T is the 1-best parse, eλ1|d |is the penalty term on the number of rules in a derivation, Plm(s) is the language model and eλ3|3 |is the length penalty term 196 BLEU score on target translation. The derivation probability conditioned on 1-best tree, P(d |T), should now be replaced by P(d |Hp) where Hp is the parse forest, which decomposes into the product of probabilities of translation rules r E d: P(d |Hp) = � P(r) (6) rEd where each P(r) is the product of five probabilities: P(r) = P(t |s)A4 · Plex(t |s)A5· P(s |t)A6 · Plex</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings ofAMTA, Boston, MA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="3096" citStr="Huang, 2008" startWordPosition="484" endWordPosition="485">e languages without enough Treebank data to train a high-accuracy parser. One obvious solution to this problem is to take as input k-best parses, instead of a single tree. This kbest list postpones some disambiguation to the decoder, which may recover from parsing errors by getting a better translation from a non 1-best parse. However, a k-best list, with its limited scope, often has too few variations and too many redundancies; for example, a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 25 &lt; 50 &lt; 26), and many subtrees are repeated across different parses (Huang, 2008). It is thus inefficient either to decode separately with each of these very similar trees. Longer sentences will also aggravate this situation as the number of parses grows exponentially with the sentence length. We instead propose a new approach, forest-based translation (Section 3), where the decoder translates a packed forest of exponentially many parses,1 1There has been some confusion in the MT literature regarding the term forest: the word “forest” in “forest-to-string rules” 192 Proceedings of ACL-08: HLT, pages 192–199, Columbus, Ohio, USA, June 2008. c�2008 Association for Computatio</context>
<context position="15003" citStr="Huang, 2008" startWordPosition="2557" endWordPosition="2558"> derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LMforest, resulting from the intersection of the translation forest and the LM, with its nodes being the +LM items during cube pruning. Although this new forest is prohibitively large, Algorithm 3 is very efficient with minimal overhead on top of 1-best. 3.4 Forest Pruning Algorithm We use the pruning algorithm of (Jonathan Graehl, p.c.; Huang, 2008) that is very similar to the method based on marginal probability (Charniak and Johnson, 2005), except that it prunes hyperedges as well as nodes. Basically, we use an Inside-Outside algorithm to compute the Viterbi inside cost Q(v) and the Viterbi outside cost α(v) for each node v, and then compute the merit αQ(e) for each hyperedge: αQ(e) = α(head(e)) + � Q(ui) (4) uiEtails(e) Intuitively, this merit is the cost of the best derivation that traverses e, and the difference S(e) = αQ(e) − Q(TOP) can be seen as the distance away from the globally best derivation. We prune away a hyperedge e if S</context>
<context position="17162" citStr="Huang (2008)" startWordPosition="2950" endWordPosition="2951">tion probabilities, and Plex(·) are the lexical probabilities. The only extra term in forest-based decoding is P(t |Hp) denoting the source side parsing probability of the current translation rule r in the parse forest, which is the product of probabilities of each parse hyperedge ep covered in the pattern-match of t against Hp (which can be recorded at conversion time): P(t |Hp) = � P(ep). (8) epEHp, ep covered by t 4.1 Data preparation Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Following Huang (2008), we modify the parser to output a packed forest for each sentence. Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words. We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules. Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests. We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram language mode</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of ACL, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing and Hypergraphs.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventh International Workshop on Parsing Technologies (IWPT2001),</booktitle>
<pages>17--19</pages>
<location>Beijing, China.</location>
<contexts>
<context position="9158" citStr="Klein and Manning, 2001" startWordPosition="1530" endWordPosition="1533">eposition (P “with”) or a conjunction (CC “and”). The parse tree for the preposition case is shown in Figure 2(b) as the 1-best parse, while for the conjunction case, the two proper nouns (B`ushiand Sh¯al´ong) are combined to form a coordinated NP NPB0,1 CC1,2 NPB2,3 NP0,3 (*) which functions as the subject of the sentence. In this case the Chinese sentence is translated into (3) “ [Bush and Sharon] held a talk”. Shown in Figure 3(a), these two parse trees can be represented as a single forest by sharing common subtrees such as NPB0,1 and VPB3,6. Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like NP0,3 are called nodes, and deductive steps like (*) correspond to hyperedges. More formally, a forest is a pair (V, E), where V is the set of nodes, and E the set of hyperedges. For a given sentence w1:l = w1 ... wl, each node v E V is in the form of Xi,j, which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, wi+1 ... wj). Each hyperedge e E E is a pair (tails(e), head(e)), where head(e) E V is the consequent node in the deductive step, and tails(e) E V * is the list of antecedent nodes. For examp</context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Christopher D. Manning. 2001. Parsing and Hypergraphs. In Proceedings of the Seventh International Workshop on Parsing Technologies (IWPT2001), 17-19 October 2001, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL,</booktitle>
<location>Edmonton, AB,</location>
<contexts>
<context position="17412" citStr="Koehn et al. (2003)" startWordPosition="2990" endWordPosition="2993"> probabilities of each parse hyperedge ep covered in the pattern-match of t against Hp (which can be recorded at conversion time): P(t |Hp) = � P(ep). (8) epEHp, ep covered by t 4.1 Data preparation Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Following Huang (2008), we modify the parser to output a packed forest for each sentence. Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words. We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules. Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests. We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram language model with Kneser-Ney smoothing on the English side of the bitext. We use the 2002 NIST MT Evaluation test set as our development set (878 sentences) and the 2005 0 5 10 15 20 25 30 35 average decoding time (secs/sentence) Figure 4: Comparison of decodin</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings ofHLT-NAACL, Edmonton, AB, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings ofAMTA,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="19156" citStr="Koehn, 2004" startWordPosition="3298" endWordPosition="3299"> of p = 12, and then convert them into translation forests using the algorithm in Section 3.2. To increase the coverage of the rule set, we also introduce a default translation hyperedge for each parse hyperedge by monotonically translating each tail node, so that we can always at least get a complete translation in the end. 4.2 Results The BLEU score of the baseline 1-best decoding is 0.2325, which is consistent with the result of 0.2302 in (Liu et al., 2007) on the same training, development and test sets, and with the same rule extraction procedure. The corresponding BLEU score of Pharaoh (Koehn, 2004) is 0.2182 on this dataset. Figure 4 compares forest decoding with decoding on k-best trees in terms of speed and quality. Using more than one parse tree apparently improves the BLEU score, but at the cost of much slower decoding, since each of the top-k trees has to be decoded individually although they share many common subtrees. Forest decoding, by contrast, is much faster 0.250 0.248 0.246 0.244 0.242 0.240 0.238 0.236 0.234 0.232 0.230 1-best p=5 k=10 p=12 k-best trees forests decoding k=30 k=100 A8 . (7) 197 Percentage of sentences (%) 0 10 20 30 40 50 60 70 80 90 100 i (rank of the pars</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings ofAMTA, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A path-based transfer model for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings ofthe 20th COLING,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1637" citStr="Lin, 2004" startWordPosition="243" endWordPosition="244">ts over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006). However, despite these advantages, current tree-based systems suffer</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Dekang Lin. 2004. A path-based transfer model for machine translation. In Proceedings ofthe 20th COLING, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-tostring alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>609--616</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1698" citStr="Liu et al., 2006" startWordPosition="253" endWordPosition="256">oints higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006). However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1- best parse tree </context>
<context position="6955" citStr="Liu et al., 2006" startWordPosition="1157" endWordPosition="1160">at. 193 More formally, a (tree-to-string) translation rule (Huang et al., 2006) is a tuple (t, s, 0), where t is the source-side tree, whose internal nodes are labeled by nonterminal symbols in N, and whose frontier nodes are labeled by source-side terminals in E or variables from a set X = {x1, x2, ...1; s E (X U A)* is the target-side string where A is the target language terminal set; and 0 is a mapping from X to nonterminals in N. Each variable xi E X occurs exactly once in t and exactly once in s. We denote R to be the translation rule set. A similar formalism appears in another form in (Liu et al., 2006). These rules are in the reverse direction of the original string-to-tree transducer rules defined by Galley et al. (2004). Finally, from step (d) we apply rules r4 and r5 (r4) NPB(NN(huit´an)) —* a talk (r5) NPB(NR(Sh¯al´ong)) —* Sharon which perform phrasal translations for the two remaining subtrees, respectively, and get the Chinese translation in (e). 3 Forest-based translation We now extend the tree-based idea from the previous section to the case of forest-based translation. Again, there are two steps, parsing and decoding. In the former, a (modified) parser will parse the input sentenc</context>
<context position="15785" citStr="Liu et al., 2006" startWordPosition="2697" endWordPosition="2700">n Inside-Outside algorithm to compute the Viterbi inside cost Q(v) and the Viterbi outside cost α(v) for each node v, and then compute the merit αQ(e) for each hyperedge: αQ(e) = α(head(e)) + � Q(ui) (4) uiEtails(e) Intuitively, this merit is the cost of the best derivation that traverses e, and the difference S(e) = αQ(e) − Q(TOP) can be seen as the distance away from the globally best derivation. We prune away a hyperedge e if S(e) &gt; p for a threshold p. Nodes with all incoming hyperedges pruned are also pruned. 4 Experiments We can extend the simple model in Equation 1 to a log-linear one (Liu et al., 2006; Huang et al., 2006): d* = arg max dED P(d |T)λ0 · eλ1|d |· Plm(s)λ2 · eλ3|3| (5) where T is the 1-best parse, eλ1|d |is the penalty term on the number of rules in a derivation, Plm(s) is the language model and eλ3|3 |is the length penalty term 196 BLEU score on target translation. The derivation probability conditioned on 1-best tree, P(d |T), should now be replaced by P(d |Hp) where Hp is the parse forest, which decomposes into the product of probabilities of translation rules r E d: P(d |Hp) = � P(r) (6) rEd where each P(r) is the product of five probabilities: P(r) = P(t |s)A4 · Plex(t |s</context>
<context position="17508" citStr="Liu et al., 2006" startWordPosition="3005" endWordPosition="3008">be recorded at conversion time): P(t |Hp) = � P(ep). (8) epEHp, ep covered by t 4.1 Data preparation Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Following Huang (2008), we modify the parser to output a packed forest for each sentence. Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words. We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules. Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests. We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram language model with Kneser-Ney smoothing on the English side of the bitext. We use the 2002 NIST MT Evaluation test set as our development set (878 sentences) and the 2005 0 5 10 15 20 25 30 35 average decoding time (secs/sentence) Figure 4: Comparison of decoding on forests with decoding on k-best trees. NIST MT Evaluation test set as our test set (1082 se</context>
<context position="21828" citStr="Liu et al. (2006)" startWordPosition="3777" endWordPosition="3780">.2M training sentence pairs. Besides the trigram language model trained on the English side of these bitext, we also use another trigram model trained on the first 1/3 of the Xinhua portion of Gigaword corpus. The two LMs have disapproach \ ruleset TR TR+BP 1-best tree 0.2666 0.2939 30-best trees 0.2755 0.3084 forest (p = 12) 0.2839 0.3149 Table 1: BLEU score results from training on large data. tinct weights tuned by minimum error rate training. The dev and test sets remain the same as above. Furthermore, we also make use of bilingual phrases to improve the coverage of the ruleset. Following Liu et al. (2006), we prepare a phrase-table from a phrase-extractor, e.g. Pharaoh, and at decoding time, for each node, we construct on-the-fly flat translation rules from phrases that match the sourceside span of the node. These phrases are called syntactic phrases which are consistent with syntactic constituents (Chiang, 2005), and have been shown to be helpful in tree-based systems (Galley et al., 2006; Liu et al., 2006). The final results are shown in Table 1, where TR denotes translation rule only, and TR+BP denotes the inclusion of bilingual phrases. The BLEU score of forest decoder with TR is 0.2839, w</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-tostring alignment template for statistical machine translation. In Proceedings of COLING-ACL, pages 609– 616, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yun Huang</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Forest-to-string statistical translation rules.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>704--711</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5734" citStr="Liu et al., 2007" startWordPosition="939" endWordPosition="942">ng VV VPB AS le x2:NPB (2) ihTt B`ushiBush Æ �� yu Sh¯al´ong with/and Sharon1 �f7 jˇuxing hold �� huit´an talk2 T le pass. Figure 2: An example derivation of tree-to-string translation. Shaded regions denote parts of the tree that is pattern-matched with the rule being applied. “Bush held a talk2 with Sharon1” Figure 2 shows how this process works. The Chinese sentence (a) is first parsed into tree (b), which will be converted into an English string in 5 steps. First, at the root node, we apply rule r1 preserving top-level word-order between English and Chinese, (r1) IP(x1:NPB x2:VP) → x1 x2 (Liu et al., 2007) was a misnomer which actually refers to a set of several unrelated subtrees over disjoint spans, and should not be confused with the standard concept ofpackedforest. which results in two unfinished subtrees in (c). Then rule r2 grabs the B`ushisubtree and transliterate it (r2) NPB(NR(B`ush´ı)) → Bush. Similarly, rule r3 shown in Figure 1 is applied to the VP subtree, which swaps the two NPBs, yielding the situation in (d). This rule is particularly interesting since it has multiple levels on the source side, which has more expressive power than synchronous context-free grammars where rules ar</context>
<context position="19008" citStr="Liu et al., 2007" startWordPosition="3271" endWordPosition="3274">tem’s BLEU score on the dev set. On dev and test sets, we prune the Chinese parse forests by the forest pruning algorithm in Section 3.4 with a threshold of p = 12, and then convert them into translation forests using the algorithm in Section 3.2. To increase the coverage of the rule set, we also introduce a default translation hyperedge for each parse hyperedge by monotonically translating each tail node, so that we can always at least get a complete translation in the end. 4.2 Results The BLEU score of the baseline 1-best decoding is 0.2325, which is consistent with the result of 0.2302 in (Liu et al., 2007) on the same training, development and test sets, and with the same rule extraction procedure. The corresponding BLEU score of Pharaoh (Koehn, 2004) is 0.2182 on this dataset. Figure 4 compares forest decoding with decoding on k-best trees in terms of speed and quality. Using more than one parse tree apparently improves the BLEU score, but at the cost of much slower decoding, since each of the top-k trees has to be decoded individually although they share many common subtrees. Forest decoding, by contrast, is much faster 0.250 0.248 0.246 0.244 0.242 0.240 0.238 0.236 0.234 0.232 0.230 1-best </context>
</contexts>
<marker>Liu, Huang, Liu, Lin, 2007</marker>
<rawString>Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007. Forest-to-string statistical translation rules. In Proceedings of ACL, pages 704–711, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="18343" citStr="Och, 2003" startWordPosition="3151" endWordPosition="3152">train a trigram language model with Kneser-Ney smoothing on the English side of the bitext. We use the 2002 NIST MT Evaluation test set as our development set (878 sentences) and the 2005 0 5 10 15 20 25 30 35 average decoding time (secs/sentence) Figure 4: Comparison of decoding on forests with decoding on k-best trees. NIST MT Evaluation test set as our test set (1082 sentences), with on average 28.28 and 26.31 words per sentence, respectively. We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002). We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on the dev set. On dev and test sets, we prune the Chinese parse forests by the forest pruning algorithm in Section 3.4 with a threshold of p = 12, and then convert them into translation forests using the algorithm in Section 3.2. To increase the coverage of the rule set, we also introduce a default translation hyperedge for each parse hyperedge by monotonically translating each tail node, so that we can always at least get a complete translation in the end. 4.2 Results The BLEU score of the baseline 1-best decoding is 0.2325, wh</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<location>Philadephia, USA,</location>
<contexts>
<context position="18282" citStr="Papineni et al., 2002" startWordPosition="3139" endWordPosition="3142">orests. We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram language model with Kneser-Ney smoothing on the English side of the bitext. We use the 2002 NIST MT Evaluation test set as our development set (878 sentences) and the 2005 0 5 10 15 20 25 30 35 average decoding time (secs/sentence) Figure 4: Comparison of decoding on forests with decoding on k-best trees. NIST MT Evaluation test set as our test set (1082 sentences), with on average 28.28 and 26.31 words per sentence, respectively. We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002). We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on the dev set. On dev and test sets, we prune the Chinese parse forests by the forest pruning algorithm in Section 3.4 with a threshold of p = 12, and then convert them into translation forests using the algorithm in Section 3.2. To increase the coverage of the rule set, we also introduce a default translation hyperedge for each parse hyperedge by monotonically translating each tail node, so that we can always at least get a complete translation in the end. 4.2 Results</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318, Philadephia, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Simon Corston-Oliver</author>
</authors>
<title>The impact of parse quality on syntactically-informed statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2429" citStr="Quirk and Corston-Oliver, 2006" startWordPosition="370" endWordPosition="373">ttractive features: they are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006). However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1- best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). This situation becomes worse with resource-poor source languages without enough Treebank data to train a high-accuracy parser. One obvious solution to this problem is to take as input k-best parses, instead of a single tree. This kbest list postpones some disambiguation to the decoder, which may recover from parsing errors by getting a better translation from a non 1-best parse. However, a k-best list, with its limited scope, often has too few variations and too many redundancies; for example, a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 25 &lt; 50 &lt; 26), a</context>
</contexts>
<marker>Quirk, Corston-Oliver, 2006</marker>
<rawString>Chris Quirk and Simon Corston-Oliver. 2006. The impact of parse quality on syntactically-informed statistical machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>271--279</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1680" citStr="Quirk et al., 2005" startWordPosition="249" endWordPosition="252">result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006). However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings ofACL, pages 271–279, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings ofICSLP,</booktitle>
<volume>30</volume>
<pages>901--904</pages>
<contexts>
<context position="17729" citStr="Stolcke, 2002" startWordPosition="3044" endWordPosition="3045">urce side of the bitext. Following Huang (2008), we modify the parser to output a packed forest for each sentence. Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words. We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules. Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests. We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram language model with Kneser-Ney smoothing on the English side of the bitext. We use the 2002 NIST MT Evaluation test set as our development set (878 sentences) and the 2005 0 5 10 15 20 25 30 35 average decoding time (secs/sentence) Figure 4: Comparison of decoding on forests with decoding on k-best trees. NIST MT Evaluation test set as our test set (1082 sentences), with on average 28.28 and 26.31 words per sentence, respectively. We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002). We use the standard minimum error-rate traini</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings ofICSLP, volume 30, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1472" citStr="Wu, 1997" startWordPosition="214" endWordPosition="215">of exponentially many parses, which encodes many more alternatives than standard n-best lists. Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline. This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time. 1 Introduction Syntax-based machine translation has witnessed promising improvements in recent years. Depending on the type of input, these efforts can be divided into two broad categories: the string-based systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Shuanglong Li</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Parsing the Penn Chinese Treebank with semantic knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings ofIJCNLP 2005,</booktitle>
<pages>70--81</pages>
<location>Jeju Island, South</location>
<contexts>
<context position="17099" citStr="Xiong et al. (2005)" startWordPosition="2936" endWordPosition="2939">tring of rule r, respectively, P(t |s) and P(s |t) are the two translation probabilities, and Plex(·) are the lexical probabilities. The only extra term in forest-based decoding is P(t |Hp) denoting the source side parsing probability of the current translation rule r in the parse forest, which is the product of probabilities of each parse hyperedge ep covered in the pattern-match of t against Hp (which can be recorded at conversion time): P(t |Hp) = � P(ep). (8) epEHp, ep covered by t 4.1 Data preparation Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext. Following Huang (2008), we modify the parser to output a packed forest for each sentence. Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words. We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules. Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests. We also use the SRI Language Mo</context>
</contexts>
<marker>Xiong, Li, Liu, Lin, 2005</marker>
<rawString>Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin. 2005. Parsing the Penn Chinese Treebank with semantic knowledge. In Proceedings ofIJCNLP 2005, pages 70–81, Jeju Island, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<location>New York, NY.</location>
<contexts>
<context position="1995" citStr="Zhang et al., 2006" startWordPosition="300" endWordPosition="303"> input is a string to be simultaneously parsed and translated by a synchronous grammar (Wu, 1997; Chiang, 2005; Galley et al., 2006), and the tree-based systems whose input is already a parse tree to be directly converted into a target tree or string (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006). Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006). However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1- best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). This situation becomes worse with resource-poor source languages without enough Treebank data to train a high-accuracy parser. One obvious solution to this problem </context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proceedings of HLT-NAACL, New York, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>