<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016302">
<title confidence="0.9971645">
Computing Term Translation Probabilities with Generalized Latent
Semantic Analysis
</title>
<author confidence="0.953445">
Irina Matveeva
</author>
<affiliation confidence="0.962321">
Department of Computer Science
University of Chicago
</affiliation>
<address confidence="0.749654">
Chicago, IL 60637
</address>
<email confidence="0.998061">
matveeva@cs.uchicago.edu
</email>
<author confidence="0.854637">
Gina-Anne Levow
</author>
<affiliation confidence="0.947495">
Department of Computer Science
University of Chicago
</affiliation>
<address confidence="0.739634">
Chicago, IL 60637
</address>
<email confidence="0.998246">
levow@cs.uchicago.edu
</email>
<sectionHeader confidence="0.995627" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999844933333333">
Term translation probabilities proved an
effective method of semantic smoothing in
the language modelling approach to infor-
mation retrieval tasks. In this paper, we
use Generalized Latent Semantic Analysis
to compute semantically motivated term
and document vectors. The normalized
cosine similarity between the term vec-
tors is used as term translation probabil-
ity in the language modelling framework.
Our experiments demonstrate that GLSA-
based term translation probabilities cap-
ture semantic relations between terms and
improve performance on document classi-
fication.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999627">
Many recent applications such as document sum-
marization, passage retrieval and question answer-
ing require a detailed analysis of semantic rela-
tions between terms since often there is no large
context that could disambiguate words’s meaning.
Many approaches model the semantic similarity
between documents using the relations between
semantic classes of words, such as representing
dimensions of the document vectors with distri-
butional term clusters (Bekkerman et al., 2003)
and expanding the document and query vectors
with synonyms and related terms as discussed
in (Levow et al., 2005). They improve the per-
formance on average, but also introduce some in-
stability and thus increased variance (Levow et al.,
2005).
The language modelling approach (Ponte and
Croft, 1998; Berger and Lafferty, 1999) proved
very effective for the information retrieval task.
Berger et. al (Berger and Lafferty, 1999) used
translation probabilities between terms to account
for synonymy and polysemy. However, their
model of such probabilities was computationally
demanding.
Latent Semantic Analysis (LSA) (Deerwester et
al., 1990) is one of the best known dimensionality
reduction algorithms. Using a bag-of-words docu-
ment vectors (Salton and McGill, 1983), it com-
putes a dual representation for terms and docu-
ments in a lower dimensional space. The resulting
document vectors reside in the space of latent se-
mantic concepts which can be expressed using dif-
ferent words. The statistical analysis of the seman-
tic relatedness between terms is performed implic-
itly, in the course of a matrix decomposition.
In this project, we propose to use a combi-
nation of dimensionality reduction and language
modelling to compute the similarity between doc-
uments. We compute term vectors using the Gen-
eralized Latent Semantic Analysis (Matveeva et
al., 2005). This method uses co-occurrence based
measures of semantic similarity between terms
to compute low dimensional term vectors in the
space of latent semantic concepts. The normalized
cosine similarity between the term vectors is used
as term translation probability.
</bodyText>
<sectionHeader confidence="0.931567" genericHeader="method">
2 Term Translation Probabilities in
</sectionHeader>
<subsectionHeader confidence="0.803667">
Language Modelling
</subsectionHeader>
<bodyText confidence="0.998240625">
The language modelling approach (Ponte and
Croft, 1998) proved very effective for the infor-
mation retrieval task. This method assumes that
every document defines a multinomial probabil-
ity distribution p(w1d) over the vocabulary space.
Thus, given a query q = (qi, ..., qm), the like-
lihood of the query is estimated using the docu-
ment’s distribution: p(qld) = fIm1 p(qi|d), where
</bodyText>
<page confidence="0.982552">
151
</page>
<bodyText confidence="0.972569">
qi are query terms. Relevant documents maximize
p(d|q) ∝ p(q|d)p(d).
Many relevant documents may not contain the
same terms as the query. However, they may
contain terms that are semantically related to the
query terms and thus have high probability of
being “translations”, i.e. re-formulations for the
query words.
Berger et. al (Berger and Lafferty, 1999) in-
troduced translation probabilities between words
into the document-to-query model as a way of se-
mantic smoothing of the conditional word proba-
bilities. Thus, they query-document similarity is
computed as
</bodyText>
<equation confidence="0.959759">
t(qi|w)p(w|d). (1)
</equation>
<bodyText confidence="0.999892818181818">
Each document word w is a translation of a query
term qi with probability t(qi|w). This approach
showed improvements over the baseline language
modelling approach (Berger and Lafferty, 1999).
The estimation of the translation probabilities is,
however, a difficult task. Lafferty and Zhai used
a Markov chain on words and documents to es-
timate the translation probabilities (Lafferty and
Zhai, 2001). We use the Generalized Latent Se-
mantic Analysis to compute the translation proba-
bilities.
</bodyText>
<subsectionHeader confidence="0.99567">
2.1 Document Similarity
</subsectionHeader>
<bodyText confidence="0.999989411764706">
We propose to use low dimensional term vectors
for inducing the translation probabilities between
terms. We postpone the discussion of how the term
vectors are computed to section 2.2. To evaluate
the validity of this approach, we applied it to doc-
ument classification.
We used two methods of computing the sim-
ilarity between documents. First, we computed
the language modelling score using term transla-
tion probabilities. Once the term vectors are com-
puted, the document vectors are generated as lin-
ear combinations of term vectors. Therefore, we
also used the cosine similarity between the docu-
ments to perform classificaiton.
We computed the language modelling score of
a test document d relative to a training document
di as
</bodyText>
<equation confidence="0.993773">
p(d|di) = Y X t(v|w)p(w|di). (2)
</equation>
<bodyText confidence="0.958001347826087">
vEd wEdi
Appropriately normalized values of the cosine
similarity measure between pairs of term vectors
cos(V, w) are used as the translation probability
between the corresponding terms t(v|w).
In addition, we used the cosine similarity be-
tween the document vectors
αdi w βdj
v hw,), (3)
where αdiw and βdj
v represent the weight of the
terms w and v with respect to the documents di
and dj, respectively.
In this case, the inner products between the term
vectors are also used to compute the similarity be-
tween the document vectors. Therefore, the cosine
similarity between the document vectors also de-
pends on the relatedness between pairs of terms.
We compare these two document similarity
scores to the cosine similarity between bag-of-
word document vectors. Our experiments show
that these two methods offer an advantage for doc-
ument classification.
</bodyText>
<subsectionHeader confidence="0.986472">
2.2 Generalized Latent Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.999911">
We use the Generalized Latent Semantic Analy-
sis (GLSA) (Matveeva et al., 2005) to compute se-
mantically motivated term vectors.
The GLSA algorithm computes the term vectors
for the vocabulary of the document collection C
with vocabulary V using a large corpus W. It has
the following outline:
</bodyText>
<listItem confidence="0.9771514">
1. Construct the weighted term document ma-
trix D based on C
2. For the vocabulary words in V , obtain a ma-
trix of pair-wise similarities, 5, using the
large corpus W
3. Obtain the matrix UT of low dimensional
vector space representation of terms that pre-
serves the similarities in 5, UT ∈ Rk&amp;quot;|V |
4. Compute document vectors by taking linear
combinations of term vectors Dˆ = UT D
</listItem>
<bodyText confidence="0.9997501">
The columns of Dˆ are documents in the k-
dimensional space.
In step 2 we used point-wise mutual informa-
tion (PMI) as the co-occurrence based measure of
semantic associations between pairs of the vocab-
ulary terms. PMI has been successfully applied to
semantic proximity tests for words (Turney, 2001;
Terra and Clarke, 2003) and was also success-
fully used as a measure of term similarity to com-
pute document clusters (Pantel and Lin, 2002). In
</bodyText>
<equation confidence="0.995333166666667">
p(q|d) = Ym X
i wEd
�dji = X X
wEdi vEdj
h
di,
</equation>
<page confidence="0.991834">
152
</page>
<bodyText confidence="0.999911166666667">
our preliminary experiments, the GLSA with PMI
showed a better performance than with other co-
occurrence based measures such as the likelihood
ratio, and x2 test.
PMI between random variables representing
two words, w1 and w2, is computed as
</bodyText>
<equation confidence="0.836610333333333">
P(W1 = 1, W2 = 1)
PMI(w1, w2) = log P (W1 = 1)P (W2 = 1).
(4)
</equation>
<bodyText confidence="0.999941">
We used the singular value decomposition
(SVD) in step 3 to compute GLSA term vectors.
LSA (Deerwester et al., 1990) and some other
related dimensionality reduction techniques, e.g.
Locality Preserving Projections (He and Niyogi,
2003) compute a dual document-term representa-
tion. The main advantage of GLSA is that it fo-
cuses on term vectors which allows for a greater
flexibility in the choice of the similarity matrix.
</bodyText>
<sectionHeader confidence="0.999755" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999975761904762">
The goal of the experiments was to understand
whether the GLSA term vectors can be used to
model the term translation probabilities. We used
a simple k-NN classifier and a basic baseline to
evalute the performance. We used the GLSA-
based term translation probabilities within the lan-
guage modelling framework and GLSA document
vectors.
We used the 20 news groups data set because
previous studies showed that the classification per-
formance on this document collection can notice-
ably benefit from additional semantic informa-
tion (Bekkerman et al., 2003). For the GLSA
computations we used the terms that occurred in
at least 15 documents, and had a vocabulary of
9732 terms. We removed documents with fewer
than 5 words. Here we used 2 sets of 6 news
groups. Groupd contained documents from dis-
similar news groups1, with a total of 5300 docu-
ments. Groups contained documents from more
similar news groups2 and had 4578 documents.
</bodyText>
<subsectionHeader confidence="0.979934">
3.1 GLSA Computation
</subsectionHeader>
<bodyText confidence="0.9939595">
To collect the co-occurrence statistics for the sim-
ilarities matrix S we used the English Gigaword
collection (LDC). We used 1,119,364 New York
Times articles labeled “story” with 771,451 terms.
</bodyText>
<footnote confidence="0.6764245">
1os.ms, sports.baseball, rec.autos, sci.space, misc.forsale,
religion-christian
2politics.misc, politics.mideast, politics.guns, reli-
gion.misc, religion.christian, atheism
</footnote>
<table confidence="0.999540857142857">
Groupd Groups
#L tf Glsa LM tf Glsa LM
100 0.58 0.75 0.69 0.42 0.48 0.48
200 0.65 0.78 0.74 0.47 0.52 0.51
400 0.69 0.79 0.76 0.51 0.56 0.55
1000 0.75 0.81 0.80 0.58 0.60 0.59
2000 0.78 0.83 0.83 0.63 0.64 0.63
</table>
<tableCaption confidence="0.998654">
Table 1: k-NN classification accuracy for 20NG.
</tableCaption>
<figureCaption confidence="0.894842">
Figure 1: k-NN with 400 training documents.
</figureCaption>
<bodyText confidence="0.999978083333333">
We used the Lemur toolkit3 to tokenize and in-
dex the document; we used stemming and a list of
stop words. Unless stated otherwise, for the GLSA
methods we report the best performance over dif-
ferent numbers of embedding dimensions.
The co-occurrence counts can be obtained using
either term co-occurrence within the same docu-
ment or within a sliding window of certain fixed
size. In our experiments we used the window-
based approach which was shown to give better
results (Terra and Clarke, 2003). We used the win-
dow of size 4.
</bodyText>
<subsectionHeader confidence="0.999762">
3.2 Classification Experiments
</subsectionHeader>
<bodyText confidence="0.999878733333333">
We ran the k-NN classifier with k=5 on ten ran-
dom splits of training and test sets, with different
numbers of training documents. The baseline was
to use the cosine similarity between the bag-of-
words document vectors weighted with term fre-
quency. Other weighting schemes such as max-
imum likelihood and Laplace smoothing did not
improve results.
Table 1 shows the results. We computed the
score between the training and test documents us-
ing two approaches: cosine similarity between the
GLSA document vectors according to Equation 3
(denoted as GLSA), and the language modelling
score which included the translation probabilities
between the terms as in Equation 2 (denoted as
</bodyText>
<footnote confidence="0.914834">
3http://www.lemurproject.org/
</footnote>
<page confidence="0.998646">
153
</page>
<bodyText confidence="0.999779153846154">
LM). We used the term frequency as an estimate
for p(w1d). To compute the matrix of translation
probabilities P, where P[i][j] = t(tj ti) for the
LMCLSA approach, we first obtained the matrix
Pˆ[i][j] = cos(f4*, t�). We set the negative and zero
entries in Pˆ to a small positive value. Finally, we
normalized the rows of Pˆ to sum up to one.
Table 1 shows that for both settings GLSA and
LM outperform the tf document vectors. As ex-
pected, the classification task was more difficult
for the similar news groups. However, in this
case both GLSA-based approaches outperform the
baseline. In both cases, the advantage is more
significant with smaller sizes of the training set.
GLSA and LM performance usually peaked at
around 300-500 dimensions which is in line with
results for other SVD-based approaches (Deer-
wester et al., 1990). When the highest accuracy
was achieved at higher dimensions, the increase
after 500 dimensions was rather small, as illus-
trated in Figure 1.
These results illustrate that the pair-wise simi-
larities between the GLSA term vectors add im-
portant semantic information which helps to go
beyond term matching and deal with synonymy
and polysemy.
</bodyText>
<sectionHeader confidence="0.995607" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99998152173913">
We used the GLSA to compute term translation
probabilities as a measure of semantic similarity
between documents. We showed that the GLSA
term-based document representation and GLSA-
based term translation probabilities improve per-
formance on document classification.
The GLSA term vectors were computed for all
vocabulary terms. However, different measures of
similarity may be required for different groups of
terms such as content bearing general vocabulary
words and proper names as well as other named
entities. Furthermore, different measures of sim-
ilarity work best for nouns and verbs. To extend
this approach, we will use a combination of sim-
ilarity measures between terms to model the doc-
ument similarity. We will divide the vocabulary
into general vocabulary terms and named entities
and compute a separate similarity score for each
of the group of terms. The overall similarity score
is a function of these two scores. In addition, we
will use the GLSA-based score together with syn-
tactic similarity to compute the similarity between
the general vocabulary terms.
</bodyText>
<sectionHeader confidence="0.996409" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999764930232558">
Ron Bekkerman, Ran El-Yaniv, and Naftali Tishby.
2003. Distributional word clusters vs. words for text
categorization.
Adam Berger and John Lafferty. 1999. Information re-
trieval as statistical translation. In Proc. of the 22rd
ACM SIGIR.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391–407.
Xiaofei He and Partha Niyogi. 2003. Locality preserv-
ing projections. In Proc. of NIPS.
John Lafferty and Chengxiang Zhai. 2001. Document
language models, query models, and risk minimiza-
tion for information retrieval. In Proc. of the 24th
ACM SIGIR, pages 111–119, New York, NY, USA.
ACM Press.
Gina-Anne Levow, Douglas W. Oard, and Philip
Resnik. 2005. Dictionary-based techniques for
cross-language information retrieval. Information
Processing and Management: Special Issue on
Cross-language Information Retrieval.
Irina Matveeva, Gina-Anne Levow, Ayman Farahat,
and Christian Royer. 2005. Generalized latent se-
mantic analysis for term representation. In Proc. of
RANLP.
Patrick Pantel and Dekang Lin. 2002. Document clus-
tering with committees. In Proc. of the 25th ACM
SIGIR, pages 199–206. ACM Press.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Proc.
of the 21st ACM SIGIR, pages 275–281, New York,
NY, USA. ACM Press.
Gerard Salton and Michael J. McGill. 1983. Intro-
duction to Modern Information Retrieval. McGraw-
Hill.
Egidio L. Terra and Charles L. A. Clarke. 2003. Fre-
quency estimates for statistical word similarity mea-
sures. In Proc.of HLT-NAACL.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI–IR versus LSA on TOEFL. Lecture Notes in
Computer Science, 2167:491–502.
</reference>
<page confidence="0.999771">
154
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.673656">
<title confidence="0.999392">Computing Term Translation Probabilities with Generalized Latent Semantic Analysis</title>
<author confidence="0.999445">Irina Matveeva</author>
<affiliation confidence="0.9999645">Department of Computer Science University of Chicago</affiliation>
<address confidence="0.979627">Chicago, IL 60637</address>
<email confidence="0.999531">matveeva@cs.uchicago.edu</email>
<author confidence="0.959363">Gina-Anne Levow</author>
<affiliation confidence="0.999965">Department of Computer Science University of Chicago</affiliation>
<address confidence="0.993167">Chicago, IL 60637</address>
<email confidence="0.999773">levow@cs.uchicago.edu</email>
<abstract confidence="0.982590875">Term translation probabilities proved an effective method of semantic smoothing in the language modelling approach to information retrieval tasks. In this paper, we use Generalized Latent Semantic Analysis to compute semantically motivated term and document vectors. The normalized cosine similarity between the term vectors is used as term translation probability in the language modelling framework. Our experiments demonstrate that GLSAbased term translation probabilities capture semantic relations between terms and improve performance on document classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Bekkerman</author>
<author>Ran El-Yaniv</author>
<author>Naftali Tishby</author>
</authors>
<title>Distributional word clusters vs. words for text categorization.</title>
<date>2003</date>
<contexts>
<context position="1370" citStr="Bekkerman et al., 2003" startWordPosition="183" endWordPosition="186">GLSAbased term translation probabilities capture semantic relations between terms and improve performance on document classification. 1 Introduction Many recent applications such as document summarization, passage retrieval and question answering require a detailed analysis of semantic relations between terms since often there is no large context that could disambiguate words’s meaning. Many approaches model the semantic similarity between documents using the relations between semantic classes of words, such as representing dimensions of the document vectors with distributional term clusters (Bekkerman et al., 2003) and expanding the document and query vectors with synonyms and related terms as discussed in (Levow et al., 2005). They improve the performance on average, but also introduce some instability and thus increased variance (Levow et al., 2005). The language modelling approach (Ponte and Croft, 1998; Berger and Lafferty, 1999) proved very effective for the information retrieval task. Berger et. al (Berger and Lafferty, 1999) used translation probabilities between terms to account for synonymy and polysemy. However, their model of such probabilities was computationally demanding. Latent Semantic A</context>
<context position="8638" citStr="Bekkerman et al., 2003" startWordPosition="1357" endWordPosition="1360">eater flexibility in the choice of the similarity matrix. 3 Experiments The goal of the experiments was to understand whether the GLSA term vectors can be used to model the term translation probabilities. We used a simple k-NN classifier and a basic baseline to evalute the performance. We used the GLSAbased term translation probabilities within the language modelling framework and GLSA document vectors. We used the 20 news groups data set because previous studies showed that the classification performance on this document collection can noticeably benefit from additional semantic information (Bekkerman et al., 2003). For the GLSA computations we used the terms that occurred in at least 15 documents, and had a vocabulary of 9732 terms. We removed documents with fewer than 5 words. Here we used 2 sets of 6 news groups. Groupd contained documents from dissimilar news groups1, with a total of 5300 documents. Groups contained documents from more similar news groups2 and had 4578 documents. 3.1 GLSA Computation To collect the co-occurrence statistics for the similarities matrix S we used the English Gigaword collection (LDC). We used 1,119,364 New York Times articles labeled “story” with 771,451 terms. 1os.ms,</context>
</contexts>
<marker>Bekkerman, El-Yaniv, Tishby, 2003</marker>
<rawString>Ron Bekkerman, Ran El-Yaniv, and Naftali Tishby. 2003. Distributional word clusters vs. words for text categorization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Information retrieval as statistical translation.</title>
<date>1999</date>
<booktitle>In Proc. of the 22rd ACM SIGIR.</booktitle>
<contexts>
<context position="1695" citStr="Berger and Lafferty, 1999" startWordPosition="235" endWordPosition="238"> there is no large context that could disambiguate words’s meaning. Many approaches model the semantic similarity between documents using the relations between semantic classes of words, such as representing dimensions of the document vectors with distributional term clusters (Bekkerman et al., 2003) and expanding the document and query vectors with synonyms and related terms as discussed in (Levow et al., 2005). They improve the performance on average, but also introduce some instability and thus increased variance (Levow et al., 2005). The language modelling approach (Ponte and Croft, 1998; Berger and Lafferty, 1999) proved very effective for the information retrieval task. Berger et. al (Berger and Lafferty, 1999) used translation probabilities between terms to account for synonymy and polysemy. However, their model of such probabilities was computationally demanding. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is one of the best known dimensionality reduction algorithms. Using a bag-of-words document vectors (Salton and McGill, 1983), it computes a dual representation for terms and documents in a lower dimensional space. The resulting document vectors reside in the space of latent semantic </context>
<context position="3784" citStr="Berger and Lafferty, 1999" startWordPosition="556" endWordPosition="559">his method assumes that every document defines a multinomial probability distribution p(w1d) over the vocabulary space. Thus, given a query q = (qi, ..., qm), the likelihood of the query is estimated using the document’s distribution: p(qld) = fIm1 p(qi|d), where 151 qi are query terms. Relevant documents maximize p(d|q) ∝ p(q|d)p(d). Many relevant documents may not contain the same terms as the query. However, they may contain terms that are semantically related to the query terms and thus have high probability of being “translations”, i.e. re-formulations for the query words. Berger et. al (Berger and Lafferty, 1999) introduced translation probabilities between words into the document-to-query model as a way of semantic smoothing of the conditional word probabilities. Thus, they query-document similarity is computed as t(qi|w)p(w|d). (1) Each document word w is a translation of a query term qi with probability t(qi|w). This approach showed improvements over the baseline language modelling approach (Berger and Lafferty, 1999). The estimation of the translation probabilities is, however, a difficult task. Lafferty and Zhai used a Markov chain on words and documents to estimate the translation probabilities </context>
</contexts>
<marker>Berger, Lafferty, 1999</marker>
<rawString>Adam Berger and John Lafferty. 1999. Information retrieval as statistical translation. In Proc. of the 22rd ACM SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="2009" citStr="Deerwester et al., 1990" startWordPosition="278" endWordPosition="281">the document and query vectors with synonyms and related terms as discussed in (Levow et al., 2005). They improve the performance on average, but also introduce some instability and thus increased variance (Levow et al., 2005). The language modelling approach (Ponte and Croft, 1998; Berger and Lafferty, 1999) proved very effective for the information retrieval task. Berger et. al (Berger and Lafferty, 1999) used translation probabilities between terms to account for synonymy and polysemy. However, their model of such probabilities was computationally demanding. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is one of the best known dimensionality reduction algorithms. Using a bag-of-words document vectors (Salton and McGill, 1983), it computes a dual representation for terms and documents in a lower dimensional space. The resulting document vectors reside in the space of latent semantic concepts which can be expressed using different words. The statistical analysis of the semantic relatedness between terms is performed implicitly, in the course of a matrix decomposition. In this project, we propose to use a combination of dimensionality reduction and language modelling to compute the similarity </context>
<context position="7767" citStr="Deerwester et al., 1990" startWordPosition="1220" endWordPosition="1223">ney, 2001; Terra and Clarke, 2003) and was also successfully used as a measure of term similarity to compute document clusters (Pantel and Lin, 2002). In p(q|d) = Ym X i wEd �dji = X X wEdi vEdj h di, 152 our preliminary experiments, the GLSA with PMI showed a better performance than with other cooccurrence based measures such as the likelihood ratio, and x2 test. PMI between random variables representing two words, w1 and w2, is computed as P(W1 = 1, W2 = 1) PMI(w1, w2) = log P (W1 = 1)P (W2 = 1). (4) We used the singular value decomposition (SVD) in step 3 to compute GLSA term vectors. LSA (Deerwester et al., 1990) and some other related dimensionality reduction techniques, e.g. Locality Preserving Projections (He and Niyogi, 2003) compute a dual document-term representation. The main advantage of GLSA is that it focuses on term vectors which allows for a greater flexibility in the choice of the similarity matrix. 3 Experiments The goal of the experiments was to understand whether the GLSA term vectors can be used to model the term translation probabilities. We used a simple k-NN classifier and a basic baseline to evalute the performance. We used the GLSAbased term translation probabilities within the l</context>
<context position="11803" citStr="Deerwester et al., 1990" startWordPosition="1874" endWordPosition="1878">(f4*, t�). We set the negative and zero entries in Pˆ to a small positive value. Finally, we normalized the rows of Pˆ to sum up to one. Table 1 shows that for both settings GLSA and LM outperform the tf document vectors. As expected, the classification task was more difficult for the similar news groups. However, in this case both GLSA-based approaches outperform the baseline. In both cases, the advantage is more significant with smaller sizes of the training set. GLSA and LM performance usually peaked at around 300-500 dimensions which is in line with results for other SVD-based approaches (Deerwester et al., 1990). When the highest accuracy was achieved at higher dimensions, the increase after 500 dimensions was rather small, as illustrated in Figure 1. These results illustrate that the pair-wise similarities between the GLSA term vectors add important semantic information which helps to go beyond term matching and deal with synonymy and polysemy. 4 Conclusion and Future Work We used the GLSA to compute term translation probabilities as a measure of semantic similarity between documents. We showed that the GLSA term-based document representation and GLSAbased term translation probabilities improve perf</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofei He</author>
<author>Partha Niyogi</author>
</authors>
<title>Locality preserving projections.</title>
<date>2003</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="7886" citStr="He and Niyogi, 2003" startWordPosition="1235" endWordPosition="1238">ers (Pantel and Lin, 2002). In p(q|d) = Ym X i wEd �dji = X X wEdi vEdj h di, 152 our preliminary experiments, the GLSA with PMI showed a better performance than with other cooccurrence based measures such as the likelihood ratio, and x2 test. PMI between random variables representing two words, w1 and w2, is computed as P(W1 = 1, W2 = 1) PMI(w1, w2) = log P (W1 = 1)P (W2 = 1). (4) We used the singular value decomposition (SVD) in step 3 to compute GLSA term vectors. LSA (Deerwester et al., 1990) and some other related dimensionality reduction techniques, e.g. Locality Preserving Projections (He and Niyogi, 2003) compute a dual document-term representation. The main advantage of GLSA is that it focuses on term vectors which allows for a greater flexibility in the choice of the similarity matrix. 3 Experiments The goal of the experiments was to understand whether the GLSA term vectors can be used to model the term translation probabilities. We used a simple k-NN classifier and a basic baseline to evalute the performance. We used the GLSAbased term translation probabilities within the language modelling framework and GLSA document vectors. We used the 20 news groups data set because previous studies sho</context>
</contexts>
<marker>He, Niyogi, 2003</marker>
<rawString>Xiaofei He and Partha Niyogi. 2003. Locality preserving projections. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Chengxiang Zhai</author>
</authors>
<title>Document language models, query models, and risk minimization for information retrieval.</title>
<date>2001</date>
<booktitle>In Proc. of the 24th ACM SIGIR,</booktitle>
<pages>111--119</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4409" citStr="Lafferty and Zhai, 2001" startWordPosition="649" endWordPosition="652">introduced translation probabilities between words into the document-to-query model as a way of semantic smoothing of the conditional word probabilities. Thus, they query-document similarity is computed as t(qi|w)p(w|d). (1) Each document word w is a translation of a query term qi with probability t(qi|w). This approach showed improvements over the baseline language modelling approach (Berger and Lafferty, 1999). The estimation of the translation probabilities is, however, a difficult task. Lafferty and Zhai used a Markov chain on words and documents to estimate the translation probabilities (Lafferty and Zhai, 2001). We use the Generalized Latent Semantic Analysis to compute the translation probabilities. 2.1 Document Similarity We propose to use low dimensional term vectors for inducing the translation probabilities between terms. We postpone the discussion of how the term vectors are computed to section 2.2. To evaluate the validity of this approach, we applied it to document classification. We used two methods of computing the similarity between documents. First, we computed the language modelling score using term translation probabilities. Once the term vectors are computed, the document vectors are </context>
</contexts>
<marker>Lafferty, Zhai, 2001</marker>
<rawString>John Lafferty and Chengxiang Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In Proc. of the 24th ACM SIGIR, pages 111–119, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina-Anne Levow</author>
<author>Douglas W Oard</author>
<author>Philip Resnik</author>
</authors>
<title>Dictionary-based techniques for cross-language information retrieval.</title>
<date>2005</date>
<booktitle>Information Processing and Management: Special Issue on Cross-language Information Retrieval.</booktitle>
<contexts>
<context position="1484" citStr="Levow et al., 2005" startWordPosition="202" endWordPosition="205">classification. 1 Introduction Many recent applications such as document summarization, passage retrieval and question answering require a detailed analysis of semantic relations between terms since often there is no large context that could disambiguate words’s meaning. Many approaches model the semantic similarity between documents using the relations between semantic classes of words, such as representing dimensions of the document vectors with distributional term clusters (Bekkerman et al., 2003) and expanding the document and query vectors with synonyms and related terms as discussed in (Levow et al., 2005). They improve the performance on average, but also introduce some instability and thus increased variance (Levow et al., 2005). The language modelling approach (Ponte and Croft, 1998; Berger and Lafferty, 1999) proved very effective for the information retrieval task. Berger et. al (Berger and Lafferty, 1999) used translation probabilities between terms to account for synonymy and polysemy. However, their model of such probabilities was computationally demanding. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is one of the best known dimensionality reduction algorithms. Using a bag-</context>
</contexts>
<marker>Levow, Oard, Resnik, 2005</marker>
<rawString>Gina-Anne Levow, Douglas W. Oard, and Philip Resnik. 2005. Dictionary-based techniques for cross-language information retrieval. Information Processing and Management: Special Issue on Cross-language Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irina Matveeva</author>
<author>Gina-Anne Levow</author>
<author>Ayman Farahat</author>
<author>Christian Royer</author>
</authors>
<title>Generalized latent semantic analysis for term representation.</title>
<date>2005</date>
<booktitle>In Proc. of RANLP.</booktitle>
<contexts>
<context position="2722" citStr="Matveeva et al., 2005" startWordPosition="393" endWordPosition="396">t vectors (Salton and McGill, 1983), it computes a dual representation for terms and documents in a lower dimensional space. The resulting document vectors reside in the space of latent semantic concepts which can be expressed using different words. The statistical analysis of the semantic relatedness between terms is performed implicitly, in the course of a matrix decomposition. In this project, we propose to use a combination of dimensionality reduction and language modelling to compute the similarity between documents. We compute term vectors using the Generalized Latent Semantic Analysis (Matveeva et al., 2005). This method uses co-occurrence based measures of semantic similarity between terms to compute low dimensional term vectors in the space of latent semantic concepts. The normalized cosine similarity between the term vectors is used as term translation probability. 2 Term Translation Probabilities in Language Modelling The language modelling approach (Ponte and Croft, 1998) proved very effective for the information retrieval task. This method assumes that every document defines a multinomial probability distribution p(w1d) over the vocabulary space. Thus, given a query q = (qi, ..., qm), the l</context>
<context position="6260" citStr="Matveeva et al., 2005" startWordPosition="946" endWordPosition="949">with respect to the documents di and dj, respectively. In this case, the inner products between the term vectors are also used to compute the similarity between the document vectors. Therefore, the cosine similarity between the document vectors also depends on the relatedness between pairs of terms. We compare these two document similarity scores to the cosine similarity between bag-ofword document vectors. Our experiments show that these two methods offer an advantage for document classification. 2.2 Generalized Latent Semantic Analysis We use the Generalized Latent Semantic Analysis (GLSA) (Matveeva et al., 2005) to compute semantically motivated term vectors. The GLSA algorithm computes the term vectors for the vocabulary of the document collection C with vocabulary V using a large corpus W. It has the following outline: 1. Construct the weighted term document matrix D based on C 2. For the vocabulary words in V , obtain a matrix of pair-wise similarities, 5, using the large corpus W 3. Obtain the matrix UT of low dimensional vector space representation of terms that preserves the similarities in 5, UT ∈ Rk&amp;quot;|V | 4. Compute document vectors by taking linear combinations of term vectors Dˆ = UT D The c</context>
</contexts>
<marker>Matveeva, Levow, Farahat, Royer, 2005</marker>
<rawString>Irina Matveeva, Gina-Anne Levow, Ayman Farahat, and Christian Royer. 2005. Generalized latent semantic analysis for term representation. In Proc. of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Document clustering with committees.</title>
<date>2002</date>
<booktitle>In Proc. of the 25th ACM SIGIR,</booktitle>
<pages>199--206</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="7292" citStr="Pantel and Lin, 2002" startWordPosition="1127" endWordPosition="1130">dimensional vector space representation of terms that preserves the similarities in 5, UT ∈ Rk&amp;quot;|V | 4. Compute document vectors by taking linear combinations of term vectors Dˆ = UT D The columns of Dˆ are documents in the kdimensional space. In step 2 we used point-wise mutual information (PMI) as the co-occurrence based measure of semantic associations between pairs of the vocabulary terms. PMI has been successfully applied to semantic proximity tests for words (Turney, 2001; Terra and Clarke, 2003) and was also successfully used as a measure of term similarity to compute document clusters (Pantel and Lin, 2002). In p(q|d) = Ym X i wEd �dji = X X wEdi vEdj h di, 152 our preliminary experiments, the GLSA with PMI showed a better performance than with other cooccurrence based measures such as the likelihood ratio, and x2 test. PMI between random variables representing two words, w1 and w2, is computed as P(W1 = 1, W2 = 1) PMI(w1, w2) = log P (W1 = 1)P (W2 = 1). (4) We used the singular value decomposition (SVD) in step 3 to compute GLSA term vectors. LSA (Deerwester et al., 1990) and some other related dimensionality reduction techniques, e.g. Locality Preserving Projections (He and Niyogi, 2003) compu</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Document clustering with committees. In Proc. of the 25th ACM SIGIR, pages 199–206. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Ponte</author>
<author>W Bruce Croft</author>
</authors>
<title>A language modeling approach to information retrieval.</title>
<date>1998</date>
<booktitle>In Proc. of the 21st ACM SIGIR,</booktitle>
<pages>275--281</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1667" citStr="Ponte and Croft, 1998" startWordPosition="231" endWordPosition="234">tween terms since often there is no large context that could disambiguate words’s meaning. Many approaches model the semantic similarity between documents using the relations between semantic classes of words, such as representing dimensions of the document vectors with distributional term clusters (Bekkerman et al., 2003) and expanding the document and query vectors with synonyms and related terms as discussed in (Levow et al., 2005). They improve the performance on average, but also introduce some instability and thus increased variance (Levow et al., 2005). The language modelling approach (Ponte and Croft, 1998; Berger and Lafferty, 1999) proved very effective for the information retrieval task. Berger et. al (Berger and Lafferty, 1999) used translation probabilities between terms to account for synonymy and polysemy. However, their model of such probabilities was computationally demanding. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is one of the best known dimensionality reduction algorithms. Using a bag-of-words document vectors (Salton and McGill, 1983), it computes a dual representation for terms and documents in a lower dimensional space. The resulting document vectors reside in t</context>
<context position="3098" citStr="Ponte and Croft, 1998" startWordPosition="446" endWordPosition="449">ition. In this project, we propose to use a combination of dimensionality reduction and language modelling to compute the similarity between documents. We compute term vectors using the Generalized Latent Semantic Analysis (Matveeva et al., 2005). This method uses co-occurrence based measures of semantic similarity between terms to compute low dimensional term vectors in the space of latent semantic concepts. The normalized cosine similarity between the term vectors is used as term translation probability. 2 Term Translation Probabilities in Language Modelling The language modelling approach (Ponte and Croft, 1998) proved very effective for the information retrieval task. This method assumes that every document defines a multinomial probability distribution p(w1d) over the vocabulary space. Thus, given a query q = (qi, ..., qm), the likelihood of the query is estimated using the document’s distribution: p(qld) = fIm1 p(qi|d), where 151 qi are query terms. Relevant documents maximize p(d|q) ∝ p(q|d)p(d). Many relevant documents may not contain the same terms as the query. However, they may contain terms that are semantically related to the query terms and thus have high probability of being “translations</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>Jay M. Ponte and W. Bruce Croft. 1998. A language modeling approach to information retrieval. In Proc. of the 21st ACM SIGIR, pages 275–281, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGrawHill.</publisher>
<contexts>
<context position="2135" citStr="Salton and McGill, 1983" startWordPosition="297" endWordPosition="300">nce on average, but also introduce some instability and thus increased variance (Levow et al., 2005). The language modelling approach (Ponte and Croft, 1998; Berger and Lafferty, 1999) proved very effective for the information retrieval task. Berger et. al (Berger and Lafferty, 1999) used translation probabilities between terms to account for synonymy and polysemy. However, their model of such probabilities was computationally demanding. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is one of the best known dimensionality reduction algorithms. Using a bag-of-words document vectors (Salton and McGill, 1983), it computes a dual representation for terms and documents in a lower dimensional space. The resulting document vectors reside in the space of latent semantic concepts which can be expressed using different words. The statistical analysis of the semantic relatedness between terms is performed implicitly, in the course of a matrix decomposition. In this project, we propose to use a combination of dimensionality reduction and language modelling to compute the similarity between documents. We compute term vectors using the Generalized Latent Semantic Analysis (Matveeva et al., 2005). This method</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and Michael J. McGill. 1983. Introduction to Modern Information Retrieval. McGrawHill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Egidio L Terra</author>
<author>Charles L A Clarke</author>
</authors>
<title>Frequency estimates for statistical word similarity measures.</title>
<date>2003</date>
<booktitle>In Proc.of HLT-NAACL.</booktitle>
<contexts>
<context position="7177" citStr="Terra and Clarke, 2003" startWordPosition="1106" endWordPosition="1109">y words in V , obtain a matrix of pair-wise similarities, 5, using the large corpus W 3. Obtain the matrix UT of low dimensional vector space representation of terms that preserves the similarities in 5, UT ∈ Rk&amp;quot;|V | 4. Compute document vectors by taking linear combinations of term vectors Dˆ = UT D The columns of Dˆ are documents in the kdimensional space. In step 2 we used point-wise mutual information (PMI) as the co-occurrence based measure of semantic associations between pairs of the vocabulary terms. PMI has been successfully applied to semantic proximity tests for words (Turney, 2001; Terra and Clarke, 2003) and was also successfully used as a measure of term similarity to compute document clusters (Pantel and Lin, 2002). In p(q|d) = Ym X i wEd �dji = X X wEdi vEdj h di, 152 our preliminary experiments, the GLSA with PMI showed a better performance than with other cooccurrence based measures such as the likelihood ratio, and x2 test. PMI between random variables representing two words, w1 and w2, is computed as P(W1 = 1, W2 = 1) PMI(w1, w2) = log P (W1 = 1)P (W2 = 1). (4) We used the singular value decomposition (SVD) in step 3 to compute GLSA term vectors. LSA (Deerwester et al., 1990) and some </context>
<context position="10200" citStr="Terra and Clarke, 2003" startWordPosition="1611" endWordPosition="1614">9 2000 0.78 0.83 0.83 0.63 0.64 0.63 Table 1: k-NN classification accuracy for 20NG. Figure 1: k-NN with 400 training documents. We used the Lemur toolkit3 to tokenize and index the document; we used stemming and a list of stop words. Unless stated otherwise, for the GLSA methods we report the best performance over different numbers of embedding dimensions. The co-occurrence counts can be obtained using either term co-occurrence within the same document or within a sliding window of certain fixed size. In our experiments we used the windowbased approach which was shown to give better results (Terra and Clarke, 2003). We used the window of size 4. 3.2 Classification Experiments We ran the k-NN classifier with k=5 on ten random splits of training and test sets, with different numbers of training documents. The baseline was to use the cosine similarity between the bag-ofwords document vectors weighted with term frequency. Other weighting schemes such as maximum likelihood and Laplace smoothing did not improve results. Table 1 shows the results. We computed the score between the training and test documents using two approaches: cosine similarity between the GLSA document vectors according to Equation 3 (deno</context>
</contexts>
<marker>Terra, Clarke, 2003</marker>
<rawString>Egidio L. Terra and Charles L. A. Clarke. 2003. Frequency estimates for statistical word similarity measures. In Proc.of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the web for synonyms:</title>
<date>2001</date>
<booktitle>PMI–IR versus LSA on TOEFL. Lecture Notes in Computer Science,</booktitle>
<pages>2167--491</pages>
<contexts>
<context position="7152" citStr="Turney, 2001" startWordPosition="1104" endWordPosition="1105"> the vocabulary words in V , obtain a matrix of pair-wise similarities, 5, using the large corpus W 3. Obtain the matrix UT of low dimensional vector space representation of terms that preserves the similarities in 5, UT ∈ Rk&amp;quot;|V | 4. Compute document vectors by taking linear combinations of term vectors Dˆ = UT D The columns of Dˆ are documents in the kdimensional space. In step 2 we used point-wise mutual information (PMI) as the co-occurrence based measure of semantic associations between pairs of the vocabulary terms. PMI has been successfully applied to semantic proximity tests for words (Turney, 2001; Terra and Clarke, 2003) and was also successfully used as a measure of term similarity to compute document clusters (Pantel and Lin, 2002). In p(q|d) = Ym X i wEd �dji = X X wEdi vEdj h di, 152 our preliminary experiments, the GLSA with PMI showed a better performance than with other cooccurrence based measures such as the likelihood ratio, and x2 test. PMI between random variables representing two words, w1 and w2, is computed as P(W1 = 1, W2 = 1) PMI(w1, w2) = log P (W1 = 1)P (W2 = 1). (4) We used the singular value decomposition (SVD) in step 3 to compute GLSA term vectors. LSA (Deerweste</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the web for synonyms: PMI–IR versus LSA on TOEFL. Lecture Notes in Computer Science, 2167:491–502.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>